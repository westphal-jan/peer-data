{"id": "1708.04968", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Fault in your stars: An Analysis of Android App Reviews", "abstract": "On mobile app distribution platforms such as the Google Play Store, users can share feedback on downloaded apps in the form of a review and star rating. Typically, star ratings range from one to five stars, with one star indicating a high level of dissatisfaction with the app and five stars indicating a high level of satisfaction.", "histories": [["v1", "Wed, 16 Aug 2017 16:37:52 GMT  (136kb,D)", "http://arxiv.org/abs/1708.04968v1", "Under review in CoDS-COMAD 2018. Preprint"]], "COMMENTS": "Under review in CoDS-COMAD 2018. Preprint", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["rahul aralikatte", "giriprasad sridhara", "neelamadhav gantayat", "senthil mani"], "accepted": false, "id": "1708.04968"}, "pdf": {"name": "1708.04968.pdf", "metadata": {"source": "META", "title": "Fault in your stars: An Analysis of Android App Reviews", "authors": ["Rahul Aralika\u008ae", "Giriprasad Sridhara", "Neelamadhav Gantayat", "Senthil Mani"], "emails": ["rahul.a.r@in.ibm.com", "girisrid@in.ibm.com", "neelamadhav@in.ibm.com", "sentmani@in.ibm.com"], "sections": [{"heading": null, "text": "Unfortunately, due to a variety of reasons, o en the star rating provided by a user is inconsistent with the opinion expressed in the review. For example, consider the following review for the Facebook App on Android; \u201cAwesome App\u201d. One would reasonably expect the rating for this review to be ve stars, but the actual rating is one star!\nSuch inconsistent ratings can lead to a de ated (or in ated) overall average rating of an app which can a ect user downloads, as typically users look at the average star ratings while making a decision on downloading an app. Also, the app developers receive a biased feedback about the application that does not represent ground reality. is is especially signi cant for small apps with a few thousand downloads as even a small number of mismatched reviews can bring down the average rating drastically.\nIn this paper, we conducted a study on this review-rating mismatch problem. We manually examined 8600 reviews from 10 popular Android apps and found that 20% of the ratings in our dataset were inconsistent with the review. Further, we developed three systems; two of which were based on traditional machine learning and one on deep learning to automatically identify reviews whose rating did not match with the opinion expressed in the review. Our deep learning system performed the best and had an accuracy of 92% in identifying the correct star rating to be associated with a given review.\nIn another evaluation, we asked 23 end users to write reviews for any 5 apps that they had used recently. We got 115 reviews from 66 di erent mobile apps. Our deep learning system had an accuracy of 87%.\nFurther, our study suggests that this problem is quite prevalent among apps. Across the ten apps used in our study, the mismatch percentage ranged from 16% to 26%.\nKEYWORDS Android; Mobile apps; Convolutional Neural Networks; Machine Learning; Deep Learning"}, {"heading": "1 INTRODUCTION", "text": "Mobile apps are typically available for download at digital distribution platforms like Google Play Store and Apple Store. Once a user has downloaded and used an app, these distribution platforms also allow the user to enter feedback about the app. e feedback is received in the form of a review comment and an associated star rating. e star rating ranges from one to ve stars with one star denoting extreme dissatisfaction with an app and ve stars denoting high satisfaction.\ne review comments and star ratings are very important as studies and our survey show that users typically download an app based on these factors [9].\nAs ratings are an important factor in determining the download of an app, it is imperative that ratings be accurate i.e., a rating accurately re ects the experience of the user with the app.\nHowever, a study [3] and our investigations suggest that o en the star rating given by a user is not consistent with the opinion expressed in the review comment. Consider the following review text for the Instagram app on Android.\n\u201cLove instagram it\u2019s the best in the world Love it it\u2019s the best in the world\u201d (sic)\nOne would reasonably expect that due to the highly positive sentiment expressed in the review, the associated star rating would be ve stars, but the actual star rating is one star! Such mismatches bring down the average rating of an app, which can adversely a ect future downloads of the app (especially for small and upcoming apps without many downloads).\nReview rating mismatches can occur due to a variety of reasons; one reason could be that novice end users may simply be confused about the di erence between one and ve stars [3]. On the other end, a negative opinion accompanied by ve stars could happen due to the following reason: A user may initially provide a rating of ve stars to an app due to a positive experience. Review systems allow users to simply rate without an explicit review comment. Later on, the user may have a negative experience with the app, (usually a er an update). He may then write about his problems with the app, but may forget to update the rating to accurately re ect his current review text. us, a review with a largely negative opinion can have a high rating of ve stars. is hypothesis is in fact con rmed by our survey responses in Section 2\nIn this paper, we perform a study of this review rating mismatch problem. We rst establish by means of a user survey and manual study that the review rating mismatch problem is prevalent across\nar X\niv :1\n70 8.\n04 96\n8v 1\n[ cs\n.L G\n] 1\n6 A\nug 2\n01 7\npopular apps on Android. We also establish the need for a system which can automatically detect inconsistencies between reviews and ratings. We then show that the development of such an automated system is non-trivial i.e., simple techniques such as natural language sentiment analysis do not su ce.\nWe then empirically establish that our system performs well i.e., can accurately identify reviews whose ratings do not match with the opinion expressed in the review text.\nWe use our automated system to nd the prevalence of reviewrating mismatches across 10 popular apps on Android and discover that 16% to 26% of the ratings do not match with their reviews. We nally show the generalizability of our system by analyzing mismatches on datasets of completely di erent domains.\nTo summarize, the main contributions of this paper are as follows:\n\u2022 A survey of Android app end users and developers which suggests that: \u2013 Review text and star rating should match \u2013 It is useful to have an automated system to detect\nmismatches \u2013 Users do not update their rating when they change\nthe review text \u2022 A manual investigation of 8600 reviews from 10 popular\napps on Android. is study shows that about 20% of the reviews have inconsistent ratings and this inconsistency is distributed across apps. \u2022 Machine learning and deep learning techniques to automatically detect mismatched review-ratings. \u2022 A deep learning model which achieves a cross-validation accuracy of 92% in identifying reviews with inconsistent ratings. \u2022 An evaluation with 23 independent human evaluators on a test set of 115 reviews drawn from 66 diverse mobile apps. e accuracy ranged from 84% to 87% \u2022 An estimate of the prevalence of review-rating mismatches across 10 popular Android apps using our deep learning system. e mismatch ranges from 16% to 26%\ne remainder of this paper is organized as follows: Section 2 provides the motivation, Section 3 describes our approach to solving this review rating mismatch problem, Section 4 describes the evaluation of our approach. Section 5 discusses the implications of our work and Section 6 describes the related work (which has mainly focused on extracting feature requests and bugs from reviews and not on detecting inconsistent review-ratings). We conclude in Section 7."}, {"heading": "2 MOTIVATION", "text": "In this section, we provide motivation for our work using two methods:\n\u2022 A survey of Android app end users and developers \u2022 A manual annotation of reviews from popular Android\napps"}, {"heading": "2.1 Motivating Survey", "text": "In our survey, we primarily wanted to know whether users believed that a mobile app star rating and associated review text should\nmatch, whether an automated system to detect mismatched reviews is useful and whether users update the star rating when they update the review text.\nWe hosted the survey questions on Google Forms and posted the link on di erent platforms such as Android forums, mailing lists, bulletin boards of the Computer Science Department at two premier universities in our country and organization.\nNo compensation was provided to any of the survey participants. ey were not told about our hypothesis about review-rating mismatch. e survey had two branches based on whether the respondent was an Android app developer or only an end user. e end users had seven questions while the developers had four questions.\nWe received 109 responses to our survey with 82% being end users and 18% developers. e survey responses are shown in Tables 1, 2, 3 and 4.\ne fundamental premise of our work that the star rating and the associated review text should correspond is strongly supported by the responses shown in Table 1. Further, Table 2 suggests that both end-users and developers feel that an automated system to detect mismatched review-ratings is useful.\nTable 3 shows the other survey questions to end users and their responses. It suggests that users base their download decision on existing reviews and average rating. us, if we have a number of ratings that are inconsistent with the review text (say, the correct rating should have been ve, but the user rated as one), the average rating of the app may decrease which will in turn will a ect app downloads. For widely popular apps with millions of downloads, it is possible that inconsistent ratings do not a ect the average rating in a signi cant manner; however, for small and upcoming apps, the average rating will be a ected leading to decreased downloads.\ne last row in Table 3 is very instructive. It suggests that users typically do not update the rating a er updating their review text. We believe this is one of the major causes of review-rating mismatch.\nTable 4 shows the other survey questions to app developers and their responses. It suggests that developers believe that reviewrating mismatch is prevalent and importantly, a ects app development.\nTo conclude, both Android app end users and developers agree that the review text and associated star rating must match. Further, they consider an automated system to detect mismatched reviewsratings is useful.\nOne issue with surveys is that \u201cwhat people say\u201d could be di erent from \u201cwhat people do\u201d [6]. To overcome such issues, typically, a triangulation approach is used to con rm a survey\u2019s ndings [6]. us, we also manually annotated review text from popular Android apps and rated them to con rm that the review-rating mismatch problem is indeed prevalent. We describe our procedure in the next sub-section."}, {"heading": "2.2 Manual Annotation of Reviews", "text": "We chose 8600 random reviews from 10 very popular Android apps. Some characteristics of the apps are shown in Table 5. As can be seen from the table, we have diversity in the sample with apps drawn from di erent categories such as social media, e-mail, games and so on.\nOnce we chose the reviews, three annotators set about manually annotating them. e annotation task is to read the review and assign a star rating ranging from one to ve stars, without having seen the original star rating. Assigning a star rating is a somewhat subjective process; hence we formulated the following guidelines about star ratings corresponding to review text.\n\u2022 Five Stars: A ve star rating is assigned to review text which are entirely complimentary without any reports of a problem or even a feature request. For example, the Facebook review,\n\u201cAwesome!\u201d\n\u2022 Four stars: A four star rating is assigned to reviews which are almost like ve star rated reviews but which express a\nPPPPPPAnn Org 1 2 3 4 5 Total\n1 1337 367 238 84 73 2099 2 314 313 257 168 89 1141 3 74 57 397 275 229 1032 4 15 5 52 434 510 1016 5 38 15 71 438 2750 3312\nTotal 1778 757 1015 1399 3651 8600\nArmed with the above guidelines, we set about manually annotating reviews. Note that this is a tedious process and the annotators could annotate only about 3 reviews per minute. is time consumption further illustrates the need for an automated solution.\nWe measured the agreement among the annotators through the Fleiss\u2019s Kappa, a standard inter-annotator agreeent measure when there are multiple annotators [1]. e Kappa score was 0.7 indicating a substantial level of agreement [1].\nFor reviews where annotators were not in agreement, we took the majority rating if at least two annotators agreed, else we took the average as the nal rating.\nTable 6 shows the distribution of the original rating and the annotator rating. e diagonal elements represent agreement between the original reviewer and the annotators while the non-diagonal elements represent the disagreements.\nWe then proceeded to count the mismatches between the original rating and the rating assigned by the annotators. We classi ed ratings 5-4 as Good, 3 as Neutral and 1-2 as Bad. We counted the mismatches only when the rating for a review text moved from one\ncategory to another. For example, if the original rating was 5 and the annotator rating was 3, we considered it as a mismatch. However, if the annotated rating was 4, we did not consider it as a mismatch.\nSuch a scheme ensures that ne di erences in opinion are tolerated and we do not get an in ated sense of mismatches.\nUsing the above methodology, we found about 20% mismatches in the 8600 reviews. e highlighted cells in Table 6 show the mismatch count. e mismatch percentage per app are shown in Table 7 indicating that the review-rating mismatch problem is fairly prevalent and also occurs across di erent apps. A sample of review-rating mismatches are shown in Table 8.\nTo conclude, the survey of end-users and app developers suggested that the problem of review text and star rating mismatch was fairly prevalent. Our manual annotation of reviews across popular apps also suggest the same.\nWe also found during the manual annotation of review text, identifying mismatches is a tedious process which supports the survey ndings of the need for an automated solution. us, the manual annotation has reinforced the survey ndings.\nMoving on, a simple or naive approach for automation is to use natural language sentiment analysis. We describe in the next subsection as to why this approach does not su ce and hence motivate the need for sophisticated automated solutions."}, {"heading": "2.3 Sentiment Analysis Based Rating Prediction is insu cient", "text": "In Natural Language Processing, sentiment analysis research deals with automatically analyzing the sentiment expressed in a sentence. Usually, the sentence analyzed is categorized into one of 5 categories viz., highly negative, negative, neutral, positive, and highly positive.\nIntuitively, it appears that one can thus apply sentiment analysis to review text, obtain a category such as highly positive and map\nit to a numerical star rating, 5. However, this approach does not work in practice as described below.\nSentiment Score Calculation: For each review in our set of 8600 reviews, we rst applied a natural language tokenizer from the Stanford NLP toolkit [18] to obtain individual sentences. We then applied sentiment analysis to each sentence and computed the overall average sentiment as follows: e ve sentiment categories were mapped to an ordinal scale ranging from 1 to 5, with the category highly negative mapped to 1 and the category highly positive mapped to 5. Let si be the sentiment score for the ith sentence of the review text. en the average sentiment score for the entire review text, S, is given by\nS = round ( 1 n \u2211 i si ) (1)\nFor each review, we then found the correlation between the rating and S. We used both the Pearson and Spearman correlation. e Pearson correlation is a measure of the linear correlation between two variables X and Y , giving a value between +1 and \u22121 inclusive, where +1 is total positive correlation, 0 is no correlation, and \u22121 is total negative correlation. Similarly, the Spearman correlation assesses how well the relationship between two variables can be described using a monotonic function. If there are no repeated data values, a perfect Spearman correlation of +1 or \u22121 occur when each of the variables is a perfect monotone function of the other.\nResults: We obtained Pearson and Spearman correlation values of around 0.5 each, for correlation between the ratings and the average sentiment scores. is indicates that there is some correlation (as expected) but not a very high degree, which precludes the use of sentiment analysis alone to solve the problem of review-rating mismatch.\nDiscussion: Our hypothesis as to why sentiment analysis does not work is as follows: Consider the review text for Facebook on Android, \u201cfreezes a er last update\u201d. ere is nothing intrinsically negative about this sentence if one looks at it from a typical English sentence perspective. It is only in the domain of mobile apps that words like \u2018freezes\u2019 have a very negative connotation. Sentiment analysis tools which are trained on standard English text will not be able to work accurately in this domain, thus preventing their use.\nSince simple techniques like sentiment analysis do not su ce in automatically detecting review-rating mismatch, we explore more advanced solutions and describe them next."}, {"heading": "3 APPROACH", "text": "In this section, we describe our approach towards automatically detecting review-rating mismatches. We use three di erent approaches, two of which are based on traditional machine learning and one which is based on deep learning. e manually annotated set of 8600 reviews from 10 popular apps, served as the training data for these approaches."}, {"heading": "3.1 Machine Learning", "text": "Since the problem at hand is a ve class classi cation problem, we rst train di erent standard machine learning classi ers as\nbaselines. We mention the di erent classi ers and the features used to train them below.\n3.1.1 Machine Learning algorithms used.\n\u2022 Naive Bayes Classi er \u2022 Decision Trees (J48) \u2022 Decision Stump (One-level decision tree) \u2022 Decision Table (Majority classi er) \u2022 AdaBoost (AdaBoost.M1, LogitBoost) \u2022 K-nearest neighbors (IBk) \u2022 Support Vector Machines (SMO) \u2022 Holte\u2019s 1R\nDue to space constraints, we do not provide details of these classi ers, but interested readers can refer to [20] for more details.\n3.1.2 Features formachine learning. In supervised learning, along with manually annotated training data, we need to identify a proper set of features and extract feature values for each data point in the training data. As we are dealing with text data, it is natural to use TF-IDF scores as one of the features. But when we analyzed the 8600 reviews in our training data, there were some pa erns which we felt would help represent the reviews be er. We now describe the various features we extracted from our training set (apart from TF-IDF) to train the above mentioned classi ers and the intuition behind them:\nHasAllCapitalWords: When a user is unhappy, he tends to use all capital le ered words which is a norm on the world wide web. For example, consider the review from Facebook, \u201cNOTIFICATIONS STOPPED WORKING\u201d. e presence of such terms indicate frustration and disappointment and act as cue for a lower star rating. is is a binary 0 or 1 feature.\nHasNegativeCueWords: Similar to the above, this feature is also helpful in identifying reviews that should have a low star rating. Cue words such as crash, freeze, hang, slow, annoying, etc. express a negative opinion. We manually constructed a dictionary of such negative cue words.\nHas estions: is feature is also helpful in identifying reviews that should have a low star rating. We observed that reviews which had a question typically indicated unhappiness and hence had lower star ratings. For example,\u2018Why are there so many updates?\u2019. Another way to identify such questions is to check for the presence of words like why, when, where, what, etc. is is helpful in situations where users may have not used a question mark in the review text.\nHasExclamation: In contrast with the above features which deal with cues about identifying low star ratings, this feature deals with identifying high star ratings. We noticed that review text which were correctly rated high o en had exclamation(s). For example, the Facebook review,\u2018awesome app!\u2019.\nHasPositiveCueWords: is is a counterpart of the HasNegativeCueWords and is helpful in identifying reviews that should have a high star rating. Cue words that express a positive opinion such as great, excellent, awesome, etc. are used here. We manually constructed a dictionary of such positive cue words.\nReviewLength: We observed that reviews which are correctly rated high tend to be short with only a few words. In contrast, reviews which have been correctly rated low tend to be long (with\nmany words) as the user typically is complaining about certain things. For example, \u2018Noti cations stopped working. e only one I get is for primary or priority inbox. No longer able to use the sorting feature and get noti cations for each group. Doesn\u2019t allow turning on label noti cations. Pop up appears, but does not work\u2019.\nSentimentScore: Although our empirical experiments suggest that sentiment analysis alone cannot be used to accurately predict star ratings, we believe, that in conjunction with other features, it can help in accurately identifying review-rating mismatch.\nReadabilityScore: Our intuition is that, reviews that are correctly rated high are more readable [14] than those which are correctly rated low. is is because, when users are unhappy or confronted with a problem, they may be agitated and hence may not write clearly."}, {"heading": "3.2 Deep Learning", "text": "Usually feature engineering requires domain expertise which is o en hard or expensive to come by. Also, when features are handcra ed, some important correlations may be missed which result in a poor representation of data which in turn decreases the accuracy of classi cation. is can be overcome by unsupervised feature learning/deep learning where the best feature representations are automatically learned from raw data.\nRNNs with LSTM units [10] have become the defacto standard for unsupervised extraction of features from text. However, recently CNNs have been used to get state of the art results on problems involving small pieces of text [23]. e app reviews which we are currently dealing with can o en be short. erefore we use a modi ed version of CNNs called Dependency based CNN (DCNN) for our classi cation problem. We brie y introduce CNNs below which is then followed by the internal working of the DCNN.\n3.2.1 Convolutional Neural Networks. CNN [15] is a type of feedforward arti cial neural network whose simplest form consists of 2 types of layers; the convolutional layer and the pooling layer. Neurons in each layer pass on their outputs to the next layer a er they undergo a non-linear transformation (typically recti ed linear or tanh). e neurons in a convolutional layer are connected to a small part of the adjacent layers which help in capturing spatiallylocal correlations. e pooling layer is used for non-linear down sampling of the inputs [5]. It also provides translation invariance; for example, it can identify a car, no ma er in which way it is oriented.\n3.2.2 Dependency based Convolutional Neural Networks. Since CNNs were designed to operate mainly on images, they inherently apply convolution on continuous areas of inputs. If it is applied as is to language tasks, the convolution operates on the words in a sequential order. Let xi \u2208 Rd be a d-dimensional representation of a word (can either be a one-hot representation or word2vec [19]). If \u2295 is the concatenation operator,\nxi j = xi \u2295 xi+1 \u2295 ... \u2295 xi+j (2) where xi j is the concatenated word vector from the ith to the jth word on which the convolution is applied. is is similar to n-gram models which feed local information to the convolution operations. In some cases, parts of a phrase maybe separated by several other\nwords. erefore we need a way to capture relationships among words even when they are not contiguous.\nDCNN is similar to the model proposed in [13], but it also considers the ancestor and sibling words in a sentence\u2019s dependency tree. Here, we consider two types of convolutions: Convolutions of ancestor paths and Convolutions on Siblings.\nConvolution of ancestor paths and siblings: In this case, we concatenate word vectors as follows:\nxik = xi \u2295 xp(i) \u2295 ... \u2295 xpk\u22121(i) (3) where pk (i) is a function which returns the kth ancestor of the\nith word. Mathematically, pk (i) = { p(pk\u22121(i)) if k > 0\ni if k = 0 (4)\nFor a given xik , we apply a convolutional lter w \u2208 Rk\u00d7d with a bias term b.\nci = \u03d5(w \u00b7 xik + b) (5) where \u03d5 is a non-linearity such as tanh or ReLu [27]. When this lter is applied on all the words in a sentence, we get a feature map c \u2208 Rl , where l is the length of the sentence.\nc = [c1, c2, \u00b7 \u00b7 \u00b7 , cl ] (6) We repeat the same process for performing convolutions of sib-\nlings as well. e only di erence is that, here pk (i) returns the kth sibling(s) rather than the kth ancestor.\n3.2.3 Our Model. : When using CNNs with text data, we use max-over-time pooling [13] to get the maximum activation over the feature map c . In DCNNs, we want the maximum activation from the feature map across the whole dependency tree (whole sentence). is is also called \u2018max-over-tree\u2019 pooling [17]. us, each lter outputs only one feature vector a er pooling. e nal representation of a sentence will be many such features, one from each of the lters in the network. ese feature vectors are nally passed on to a fully connected layer for classi cation.\ne model we built is similar to what is done in [17]. We concatenated the feature maps obtained from ancestor path and sibling convolutions with the sequential n-gram representation. e concatenation with the sequential n-gram representation was done because the app reviews contains grammatical aws which will result in parsing errors during dependency tree construction, but these parsing errors do not a ect the sequential representation. e concatenated representation is shown in equation 7. Here c(i)a represent the ancestor path feature map, c(i)s represent the sibling feature map and c(i) represent the sequential feature map. We used 100 lters (Na = Ns = N = 100) for each representation.\nc = [ ancestors\ufe37 \ufe38\ufe38 \ufe37 c (1) a , \u00b7 \u00b7 \u00b7 , c (Na ) a ; siblings\ufe37 \ufe38\ufe38 \ufe37 c (1) s , \u00b7 \u00b7 \u00b7 , c (Ns ) s ; sequential\ufe37 \ufe38\ufe38 \ufe37 c(1), \u00b7 \u00b7 \u00b7 , c(N )] (7)\nIn our model, we used a dropout probability of 0.5 and a learning rate of 0.95 which was decayed using adadelta update rule [28]. We used 100 dimensional word embedding which were learned from scratch during training."}, {"heading": "4 EVALUATION", "text": "In this section, we describe our evaluation. We designed our evaluation to answer the following research questions (RQ):\nRQ1: Accuracy on manually annotated data: What is the accuracy of our machine learning and deep learning techniques on manually annotated data?\nRQ2: Accuracy on data from the wild: With what accuracy are we able to automatically predict the star rating of any given review text?\nRQ3: Mismatch Prevalence: How prevalent are the reviewrating mismatches across popular mobile apps for Android?"}, {"heading": "4.1 Metric", "text": "As explained in Section 2.2 we categorized the ratings 5-4 as Good, rating 3 as Neutral and ratings 1-2 as Bad. We considered our prediction to be accurate i the predicted rating and the correct rating fell in the same category. erefore our accuracy is given by equation 8 where pi is the predicted rating category, ci is the correct rating category and n is the total number of reviews in the evaluation data.\nNote that have not used \u201dprecision/recall\u201d etc. as they make sense only when there are multiple correct answers and a subset of them is returned by an approach. Here we have only a binary answer as result (i.e., match/mismatch) and thus we strongly believe \u201daccuracy\u201d is the most appropriate measure.\naccuracy = 1 n \u2211 i zi { zi = 1 i f pi = ci zi = 0 i f pi , ci\n(8)"}, {"heading": "4.2 RQ1: Accuracy on manually annotated data", "text": "In this section, we answer the question about the accuracy of our machine learning and deep learning techniques on manually annotated data of 8600 reviews.\n4.2.1 Construction of the models. Model with handcra ed features: For each of the 8600 reviews in our training set, we extract the features using the rules described in Section 3.1.2. For the ReviewLength and ReadabilityScore we transformed the score values into a scale of 0 to 1 as follows.\nFor ReviewLength we automatically got the count of words in all the reviews. We then sort and nd the percentile to which a review belongs. If a review is in the 60th percentile, the feature\u2019s value will be 0.6 and so on.\nWe obtained the Flesch-Kincaid readability score [14] for each review text and transformed the scores into a scale of 0 to 1 as above.\nWe then built 9 di erent traditional machine learning models mentioned previously and trained them using the extracted feature values with Weka 3.8 [24].\nModel withWord Vectors: We constructed a vocabulary of all the words present in our review corpus by removing stop words and converting the rest into lowercase. We then use 100 dimensional glove vectors [22] pre-trained on Wikipedia-14 and Gigaword 5 datasets to represent each word in our vocabulary. Finally, these word vectors were used to train the machine learning classi ers mentioned previously.\nModel using DCNN: For DCNNs, no pre-processing of data is required. erefore, we only removed all special characters in the review text and used it for training.\n4.2.2 Results. To evaluate all these models, we used ten fold cross validation. Here the 8600 reviews were divided into ten equal sets. In a single iteration, nine sets were used for training and one set was used to test. We had ten such iterations, with each of the ten sets serving as the test data once.\ne cross-validation accuracy of the models with di erent traditional machine learning techniques is shown in Figure 1. e lightly shaded bars i.e., the bars that appear on the le of each pair of bars represent the cross-validation accuracy of each machine learning classi er using the features described in Section 3.1.2. e best accuracy of 74.9% was obtained with the IBk classi er i.e., the Instance Based classi er, followed by the J48 (Decision Tree) classi er which had an accuracy of 73.6%.\ne darkly shaded bars i.e., the bars that appear on the right of each pair of bars represent the cross-validation accuracy of each machine learning classi er using word vectors. e best accuracy of 84.52% was obtained with the One R classi er, followed by the J48 (Decision Tree) classi er which had an accuracy of 83.22%.\ne DCNN outperformed all the models shown in Figure 1. It had an accuracy of 92%.\n4.2.3 Discussion. e deep learning approach appears to perform the best among the three techniques. e time taken by all three approaches is of the same order with the entire model building (i.e., learning) and ten fold cross validation nishing in a few minutes. e feature engineering based approach did not perform as well due to limitations in identifying all possible features which can accurately help in identifying the rating. DCNN outperformed the word vector based model as it captures additional non-sequential context of a word, which may not have been captured by the word vector model. is additional context appears to help the model to classify the review in a more accurate fashion.\nus, for answering the next two research questions, we consider only DCNN as the automated solution for predicting review mismatches, as it has performed the best on the manually annotated data set."}, {"heading": "4.3 RQ2: Accuracy on data from the wild", "text": "Evaluation with users: We recruited 23 users within our organization for this evaluation. All of them have advanced degrees in computer science. We asked them to write reviews for 5 mobile apps of their choice and provide a suitable rating corresponding to their reviews. At a high level, we instructed the users not to choose all positive or all negative experience apps to ensure diversity of review-ratings. us we have a total of 23 \u00d7 5 = 115 mobile app reviews with associated ratings in our test set. e users were not compensated in anyway for their work. ey were also not told about the intention behind this exercise.\nIn total, the 23 di erent users provided reviews for 66 di erent apps out of which four apps had gured in our training set viz., Facebook, Twi er, Gmail and LinkedIn. e distribution of 115 reviews among the star ratings were as follows: 9 reviews were rated as one star, 15 as two stars, 34 as three stars, 35 as four stars and 22 as ve stars.\nFor each review, we then used our DCNN model trained on our manually annotated dataset to predict the rating using only the provided review text.\nResults: We computed the accuracy according to equation 8. We got an accuracy of 87% if we consider only reviews from those apps which were present in our training set. Whereas, if we consider the entire review set from all the 66 apps, the accuracy was 84%. We believe that both these accuracy values are fairly good.\nDiscussion: e accuracy values and the vast number of apps suggest that the DCNN based approach is fairly good in assigning an appropriate star rating to a review text and can also be used adequately on apps that have not featured in the training set. e\nFigure 2: RQ3 : Distribution of number of reviews for which DCNN predicted the ratings as Match or Mismatch with user ratings across 10 applications\nFa ce\nbo ok\nSu bw\nay S\nur fe\nrs\nAn gr\ny Bi\nrd s\nGo og\nle P\nlu s\nIn st\nag ra\nm\nTw itt\ner\nLi nk\ned In Gm ai l\nTe m\npl e\nRu n\nQu or\na 0\n20\n40\n60\n80\n100\nM a tc\nh e s\na n d M\nis m\na tc\nh e s\n%\n1 2\n1 1\n0\n6 9\n3 0\n3 0\n1 8\n8 4\n2 2\n3 6\n7 4\n7 9\n0 6\n9 1\n0 5\n8 4\n0 4\n3 1\n1 4\n2 4\n0 2\n2 8\n4 0\n1 3\n2 0\n9 0\n2\n2 6\n6 0\n1 2\n9 1\n2 7\n7 7\n1 7\n3 4\n2 3\n7 0\n8 2\n8\n4 5\n7\nMatch Mismatch\nApp Number of Reviews App Number of Reviews Angry Birds 3920 LinkedIn 10839\nFacebook 14950 ora 2859 Gmail 10744 Subway Surfers 8250 Google Plus 11082 Temple Run 3942 Instagram 4965 Twi er 10683\nTable 9: Ten Android apps and the reviews per application for identifying mismatch prevalence. 82234 reviews in total\nreview-rating prediction accuracy for an app can be further improved if the DCNN is trained on review text from the same app. is is due to the fact that sometimes, we see app speci c terms in the review text which were not seen by our model previously.\nWe also asked the users if they would use a system that would automatically help suggest an appropriate rating for their reviews. 18 of the 23 users said that they would use such a system, indicating a need for an automated solution. Note that, this reinforces our survey results presented in Section 2.\nGeneralizability to other domains: We now discuss about the generalizability of our approach i.e., what would be the accuracy of our approach when applied to non Android app reviews?\nWe gathered 1000 random product reviews apiece from the publicly available Amazon MP3 and Trip Advisor datasets [26] and applied our DCNN model to obtain star ratings for these reviews. We then manually veri ed the results and found that our approach had an accuracy of 88% on the Amazon MP3 reviews and 86% on the Trip Advisor reviews. is shows that, our model works well on di erent domains even without any ne tuning [21] and being trained on a relatively small dataset of an entirely di erent domain (8600 Android app reviews). us, our model generalizes fairly well and is not over ed to the training data."}, {"heading": "4.4 RQ3: Mismatch Prevalence", "text": "We now answer the question as to how prevalent are the reviewrating mismatches across popular mobile apps for Android on Google Play Store.\nData: In order to obtain a count of the review-rating mismatches for an app, we rst need to retrieve all the reviews of that app. Unfortunately, Google Play Store does not allow one to download\nall reviews for an app (unless one is a developer of the app). us, the next option is to crawl Google Play Store and retrieve the reviews. Here again, it puts restrictions on crawling and blocks requests if too many of them are sent in a short period of time. erefore, we decided to retrieve only the reviews from the last few months for each app. We obtained a total of 82234 reviews of 10 di erent apps. e apps and the number of retrieved reviews per app are shown in Table 9. We then ran our DCNN model on these reviews.\nResults: Accuracy was computed as before using equation 8. e percentage of reviews for which the original and the predicted star rating categories matched and did not match are shown in the stacked bar chart in Figure 2. e results suggest that a substantial number of reviews (17179 or 20%) have inconsistent star ratings. is phenomenon is not restricted to a few apps but appears across all the apps, ranging from 16% (for ora, LinkedIn and Subway Surfers) to 26% (Instagram and Twi er). A sample of the mismatched reviews are shown in Table 10.\nChange of average rating due to mismatches: For each app in Table 9, we computed the average rating, using the original star rating for the reviews shown in Table 9. We then computed the average rating using the predicted ratings of our DCNN model for each app. e original average rating and the predicted rating a er accounting for mismatches is shown in Table 11.\ne average rating decreases for some apps but increases for others when we re-calculate a er correcting the mismatches. e average rating variation ranges from 0.3 to 0.7.\nGeneral Discussion: Note that, it is possible that such mismatched reviews are not really mismatches but rather mis-predictions by our system. To validate this, we manually analyzed the mismatches by examining 760 random reviews, with 76 apiece from each of the ten apps. Of the 760 reviews, there were 192 which were classi ed as mismatch by our system (i.e., 25.26%). Among these, we had an accuracy of 90.62% i.e., 174 of the 192 were indeed mismatches. us, overall, 174 of the 760 reviews were genuine mismatches (i.e., 23%)\nFurther, our qualitative observations are as follows: Errors in mismatch prediction happen when reviews contain non-English representations of English words like \u201cwaaaaahoooo\u201d and \u201cwoooorked\u203c!\u201d etc. In apps like ora some reviews talked about the content (questions and answers) served by the app rather than about the app as such. is lead to the detection of a few mismatches incorrectly."}, {"heading": "4.5 Replication Package", "text": "Our tool is available at h p://mismatch.mybluemix.net. e user survey responses, data sets for manual annotation and the evaluations are also provided there. Further, model parameters for the DCNN are also mentioned in detail."}, {"heading": "4.6 reats to Validity", "text": "Our study is focused on applications from Android with reviews from Google Play Store, and hence it might not generalize to other distribution platforms like Apple Store. Due to constraints imposed by Google Play Store on downloading all reviews of an app, we had to perforce evaluate on a smaller subset of the latest 82234 reviews from 10 popular Android apps. us, our results may not generalize to all reviews, especially when the review text contain app speci c terminologies not seen by the model beforehand. To mitigate this we downloaded as many reviews as possible and in future, plan to evaluate our approach on reviews from the Apple Store.\nUser surveys are generally prone to various threats[6] such as being unrepresentative, exhibiting bias and idealistic responses (what people say in a survey can be di erent from what they actually do in practice). To mitigate these threats, we tried to ensure that we obtained responses from a representative sample by posting noti cations about the survey on many diverse platforms. We also tried to avoid bias in the responses by not revealing the intentions behind our exercise and avoiding any kind of compensation. Finally, we used the manual annotations to reinforce some of the survey\u2019s ndings.\nIn constructing our training set, the rating given by annotators might not be accurate since they are not aware of the circumstances under which the review was wri en and the original rating provided. To mitigate this we had fairly large sample of 8600 reviews from diverse apps and three independent annotators.\nIn the user evaluation, there is a possibility for certain users to not rate their reviews in accordance to what is expected. To mitigate this, we conducted the evaluation with a fairly large pool of 23 participants with each reviewing 5 di erent apps of their choice."}, {"heading": "5 IMPLICATIONS OF OURWORK", "text": "We believe this is a foundational work and can be used in several prior research works such as [2, 16]. Prior research that use reviews and rating, make an assumption that the rating and reviews match and thus can be used as is (i.e, the average rating truly represents the experience of the end-users; a low star rating implies a negative opinion and so on).\nOur work strongly suggests that we need to be more careful in dealing with reviews and rating. (For ex: a heuristic for automatically nding negative reviews could assume a low star rating of 1 or 2. However, this heuristic may not be very accurate and would miss a number of negative reviews which have been erroneously rated 4 or 5 stars).\nFurther as shown in Table 11, review-rating mismatch will a ect the overall average rating of an app and this can impact research that uses average rating [16] to determine success of an app.\nere is a strong correlation between average rating and downloads [9]. Mismatched review-ratings can de ate average rating leading to fewer downloads and consequently a loss of revenue. Small and upcoming apps with less number of downloads and reviews are especially a ected by these inconsistent review-ratings.\nOur future work will focus on determining the improvement in existing research that can be obtained by identifying review rating mismatches."}, {"heading": "6 RELATEDWORK", "text": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].\nTo help developers prioritize the devices to test their app, [12] examined reviews from di erent devices for the same app and found that some devices gave signi cantly lower ratings.\nDave et al. [4] use information retrieval techniques to distinguish between positive and negative product reviews. ey state that the performance of their method is a ected due to rating inconsistencies, which they de ne as \u201csimilar qualitative descriptions yield very di erent quantitative reviews from reviewers. In the most extreme case, reviewers do not understand the rating system and give a 1 instead of a 5\u201d. Our work in contrast addresses the rating inconsistency problem directly.\nFu et al. [7] try to understand why people might dislike an app. As part of this, they allude to the presence of reviews with inconsistent ratings. ey propose a simple approach of using regression on words with a certain frequency (viz., > 10). ey unfortunately do not provide an accuracy assessment of their approach. e rst of our three approaches can be seen as a generalization of their approach as it uses TF-IDF and further allows the use of di erent classi ers apart from regression. Also, our DCNN approach performs be er than the mentioned approach."}, {"heading": "7 CONCLUSION", "text": "In this paper, we performed a study of the Android app reviewrating mismatch problem. We conducted a survey of Android app end users and developers. e survey responses suggest that: (1) review text and corresponding star ratings should match; (2) it is useful to have an automated system to detect mismatches; (3) end users do not update the star ratings when they update their review text; (4) developers believe mismatches are prevalent and a ects overall app development.\nWe manually analyzed 8600 reviews from 10 mobile apps available for Android. ese apps include Facebook, Gmail and other popular apps. We found that about 20% of the reviews had ratings which did not match with the review text. Further, our study suggested that manually analyzing reviews to detect inconsistent ratings can be tedious and time consuming, thus, warranting automated solutions.\nWe developed multiple automated systems to detect reviews with inconsistent ratings. ese systems are based on machine and deep learning methods. We then empirically established that our Dependency based Convolutional Neural Network model performs well in practice i.e., can accurately identify reviews whose rating does not match with the opinion expressed in the review text. Our system achieved an accuracy of 92% on the manually annotated data.\nFurther, we performed an end user evaluation. We recruited 23 Android app end-users and asked them to write reviews for any ve mobile apps used by them, along with providing a rating ranging from one to ve stars. We predicted the star rating for these user reviews using DCNN and compared with the user provided rating. Our system achieved an upward accuracy of 87%.\nWe nally used our system to detect review-rating mismatches across 10 popular apps on Android (available on Google Play Store) and found that mismatched review-ratings are fairly prevalent across apps ranging from 16% to 26%."}], "references": [{"title": "Beyond kappa: A review of interrater agreement measures", "author": ["Mousumi Banerjee", "Michelle Capozzoli", "Laura McSweeney", "Debajyoti Sinha"], "venue": "Canadian Journal of Statistics 27,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "AR-miner: Mining Informative Reviews for Developers from Mobile App Marketplace", "author": ["Ning Chen", "Jialiu Lin", "Steven C.H. Hoi", "Xiaokui Xiao", "Boshen Zhang"], "venue": "In Proceedings of the 36th International Conference on So\u0087ware Engineering (ICSE 2014). ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Mining the Peanut Gallery: Opinion Extraction and Semantic Classi\u0080cation of Product Reviews", "author": ["Kushal Dave", "Steve Lawrence", "David M. Pennock"], "venue": "In Proceedings of the 12th International Conference on World Wide Web (WWW \u201903)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Mining the Peanut Gallery: Opinion Extraction and Semantic Classi\u0080cation of Product Reviews", "author": ["Kushal Dave", "Steve Lawrence", "David M. Pennock"], "venue": "In Proceedings of the 12th International Conference on World Wide Web (WWW \u201903)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Selecting Empirical Methods for So\u0089ware Engineering Research", "author": ["Steve Easterbrook", "Janice Singer", "Margaret-Anne Storey", "Daniela Damian"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Why People Hate Your App: Making Sense of User Feedback in a Mobile App Store", "author": ["Bin Fu", "Jialiu Lin", "Lei Li", "Christos Faloutsos", "Jason Hong", "Norman Sadeh"], "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \u201913)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Analysis of User Comments: An Approach for So\u0089ware Requirements Evolution", "author": ["Laura V. Galvis Carre\u00f1o", "Kristina Winbladh"], "venue": "In Proceedings of the  2013 International Conference on So\u0087ware Engineering (ICSE", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "App store mining and analysis: MSR for app stores", "author": ["M. Harman", "Y. Jia", "Y. Zhang"], "venue": "In Mining So\u0087ware Repositories (MSR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Retrieving and Analyzing Mobile Apps Feature Requests from Online Reviews", "author": ["Claudia Iacob", "Rachel Harrison"], "venue": "In Proceedings of the 10th Working Conference on Mining So\u0087ware Repositories (MSR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Prioritizing the Devices to Test Your App on: A Case Study of Android Game Apps", "author": ["Hammad Khalid", "Meiyappan Nagappan", "Emad Shihab", "Ahmed E. Hassan"], "venue": "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of So\u0087ware Engineering (FSE 2014). ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classi\u0080cation", "author": ["Yoon Kim"], "venue": "CoRR abs/1408.5882", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Comparison of Learning Algorithms for Handwri\u008aen Digit Recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bo\u008aou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Mller", "E. Sckinger", "P. Simard", "V. Vapnik"], "venue": "In INTERNATIONAL CONFERENCE ON ARTIFICIAL NEURAL NETWORKS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "API Change and Fault Proneness: A \u008creat to the Success of Android Apps", "author": ["Mario Linares-V\u00e1squez", "Gabriele Bavota", "Carlos Bernal-C\u00e1rdenas", "Massimiliano Di Penta", "Rocco Oliveto", "Denys Poshyvanyk"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of So\u0087ware Engineering (ESEC/FSE", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Tree-based Convolution for Sentence Modeling", "author": ["Mingbo Ma", "Liang Huang", "Bowen Zhou", "Bing Xiang"], "venue": "CoRR abs/1507.01839", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "\u008ce Stanford CoreNLP Natural Language Processing Toolkit. In Association for Computational Linguistics (ACL) System Demonstrations", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "E\u0081cient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Je\u0082rey Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Machine Learning (1 ed.)", "author": ["\u008comas M. Mitchell"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Statistical-Mechanical Analysis of Pre-training and Fine Tuning in Deep Learning", "author": ["M. Ohzeki"], "venue": "Journal of the Physical Society of Japan", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Je\u0082rey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP). 1532\u20131543", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship A\u008aribution", "author": ["S. Ruder", "P. Gha\u0082ari", "J.G. Breslin"], "venue": "ArXiv e-prints (Sept. 2016)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Statistical Genomics: Methods and Protocols. Springer, New York, NY, Chapter Introducing Machine Learning Concepts with WEKA, 353\u2013378", "author": ["Tony C. Smith", "Eibe Frank"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Mining User Opinions in Mobile App Reviews: A Keyword-Based Approach (T)", "author": ["Phong Minh Vu", "Tam \u008ce Nguyen", "Hung Viet Pham", "Tung \u008canh Nguyen"], "venue": "In Proceedings of the 2015 30th IEEE/ACM International Conference on Automated So\u0087ware Engineering (ASE) (ASE \u201915)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Latent Aspect Rating Analysis Without Aspect Keyword Supervision", "author": ["Hongning Wang", "Yue Lu", "ChengXiang Zhai"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \u201911)", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Empirical Evaluation of Recti\u0080ed Activations in Convolutional Network", "author": ["Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li"], "venue": "CoRR abs/1505.00853", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Ma\u008ahew D. Zeiler"], "venue": "CoRR abs/1212.5701", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "\u008ce review comments and star ratings are very important as studies and our survey show that users typically download an app based on these factors [9].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "However, a study [3] and our investigations suggest that o\u0089en the star rating given by a user is not consistent with the opinion expressed in the review comment.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Review rating mismatches can occur due to a variety of reasons; one reason could be that novice end users may simply be confused about the di\u0082erence between one and \u0080ve stars [3].", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "One issue with surveys is that \u201cwhat people say\u201d could be di\u0082erent from \u201cwhat people do\u201d [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "To overcome such issues, typically, a triangulation approach is used to con\u0080rm a survey\u2019s \u0080ndings [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "We measured the agreement among the annotators through the Fleiss\u2019s Kappa, a standard inter-annotator agreeent measure when there are multiple annotators [1].", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "7 indicating a substantial level of agreement [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 15, "context": "Sentiment Score Calculation: For each review in our set of 8600 reviews, we \u0080rst applied a natural language tokenizer from the Stanford NLP toolkit [18] to obtain individual sentences.", "startOffset": 148, "endOffset": 152}, {"referenceID": 17, "context": "Due to space constraints, we do not provide details of these classi\u0080ers, but interested readers can refer to [20] for more details.", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "RNNs with LSTM units [10] have become the defacto standard for unsupervised extraction of features from text.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "However, recently CNNs have been used to get state of the art results on problems involving small pieces of text [23].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "CNN [15] is a type of feedforward arti\u0080cial neural network whose simplest form consists of 2 types of layers; the convolutional layer and the pooling layer.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "Let xi \u2208 Rd be a d-dimensional representation of a word (can either be a one-hot representation or word2vec [19]).", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "DCNN is similar to the model proposed in [13], but it also considers the ancestor and sibling words in a sentence\u2019s dependency tree.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "where \u03c6 is a non-linearity such as tanh or ReLu [27].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": ": When using CNNs with text data, we use max-over-time pooling [13] to get the maximum activation over the feature map c .", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "\u008cis is also called \u2018max-over-tree\u2019 pooling [17].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "\u008ce model we built is similar to what is done in [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "95 which was decayed using adadelta update rule [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "8 [24].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "We then use 100 dimensional glove vectors [22] pre-trained on Wikipedia-14 and Gigaword 5 datasets to represent each word in our vocabulary.", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": ", what would be the accuracy of our approach when applied to non Android app reviews? We gathered 1000 random product reviews apiece from the publicly available Amazon MP3 and Trip Advisor datasets [26] and applied our DCNN model to obtain star ratings for these reviews.", "startOffset": 198, "endOffset": 202}, {"referenceID": 18, "context": "\u008cis shows that, our model works well on di\u0082erent domains even without any \u0080ne tuning [21] and being trained on a relatively small dataset of an entirely di\u0082erent domain (8600 Android app reviews).", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "User surveys are generally prone to various threats[6] such as being unrepresentative, exhibiting bias and idealistic responses (what people say in a survey can be di\u0082erent from what they actually do in practice).", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "We believe this is a foundational work and can be used in several prior research works such as [2, 16].", "startOffset": 95, "endOffset": 102}, {"referenceID": 13, "context": "We believe this is a foundational work and can be used in several prior research works such as [2, 16].", "startOffset": 95, "endOffset": 102}, {"referenceID": 13, "context": "Further as shown in Table 11, review-rating mismatch will a\u0082ect the overall average rating of an app and this can impact research that uses average rating [16] to determine success of an app.", "startOffset": 155, "endOffset": 159}, {"referenceID": 7, "context": "\u008cere is a strong correlation between average rating and downloads [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 6, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 9, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 22, "context": "Broadly, most of the related work has focused on analyzing the content of the review text using techniques such as topic modeling to identify bugs and feature requests [2, 8, 11, 25].", "startOffset": 168, "endOffset": 182}, {"referenceID": 10, "context": "To help developers prioritize the devices to test their app, [12] examined reviews from di\u0082erent devices for the same app and found that some devices gave signi\u0080cantly lower ratings.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "[4] use information retrieval techniques to distinguish between positive and negative product reviews.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] try to understand why people might dislike an app.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Mobile app distribution platforms such as Google Play Store allow users to share their feedback about downloaded apps in the form of a review comment and a corresponding star rating. Typically, the star rating ranges from one to \u0080ve stars, with one star denoting a high sense of dissatisfaction with the app and \u0080ve stars denoting a high sense of satisfaction. Unfortunately, due to a variety of reasons, o\u0089en the star rating provided by a user is inconsistent with the opinion expressed in the review. For example, consider the following review for the Facebook App on Android; \u201cAwesome App\u201d. One would reasonably expect the rating for this review to be \u0080ve stars, but the actual rating is one star! Such inconsistent ratings can lead to a de\u0083ated (or in\u0083ated) overall average rating of an app which can a\u0082ect user downloads, as typically users look at the average star ratings while making a decision on downloading an app. Also, the app developers receive a biased feedback about the application that does not represent ground reality. \u008cis is especially signi\u0080cant for small apps with a few thousand downloads as even a small number of mismatched reviews can bring down the average rating drastically. In this paper, we conducted a study on this review-rating mismatch problem. We manually examined 8600 reviews from 10 popular Android apps and found that 20% of the ratings in our dataset were inconsistent with the review. Further, we developed three systems; two of which were based on traditional machine learning and one on deep learning to automatically identify reviews whose rating did not match with the opinion expressed in the review. Our deep learning system performed the best and had an accuracy of 92% in identifying the correct star rating to be associated with a given review. In another evaluation, we asked 23 end users to write reviews for any 5 apps that they had used recently. We got 115 reviews from 66 di\u0082erent mobile apps. Our deep learning system had an accuracy of 87%. Further, our study suggests that this problem is quite prevalent among apps. Across the ten apps used in our study, the mismatch percentage ranged from 16% to 26%.", "creator": "LaTeX with hyperref package"}}}