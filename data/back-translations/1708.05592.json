{"id": "1708.05592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Future Word Contexts in Neural Network Language Models", "abstract": "Lately, bi-directional recursive network language models (bi-RNNLMs) have proven to be better than standard, unidirectional, recursive network language models (uni-RNNLMs) in a number of speech recognition tasks, suggesting that future word context information can be useful beyond the word history. However, bi-RNNLMs pose a number of challenges in that they use complete previous and future word context information, affecting both training efficiency and their use within a framework for rescoring grids. In this paper, these problems are addressed by proposing a novel neural network structure that supersedes word contexts (su-RNLMs). Instead of using a recursive unit to capture complete future word contexts within a framework for rescoring grids, a feedback unit will be used to show a limited number of successful future NNNNLMs modelling NNNNLMs based on NNI model results (this model can also be much more efficient for NI modelling NI modelling results).", "histories": [["v1", "Fri, 18 Aug 2017 13:11:22 GMT  (617kb,D)", "http://arxiv.org/abs/1708.05592v1", "Submitted to ASRU2017"]], "COMMENTS": "Submitted to ASRU2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xie chen", "xunying liu", "anton ragni", "yu wang", "mark gales"], "accepted": false, "id": "1708.05592"}, "pdf": {"name": "1708.05592.pdf", "metadata": {"source": "CRF", "title": "FUTURE WORD CONTEXTS IN NEURAL NETWORK LANGUAGE MODELS", "authors": ["X. Chen", "X. Liu", "A. Ragni", "Y. Wang", "M.J.F. Gales"], "emails": ["xc257@eng.cam.ac.uk,", "ar527@eng.cam.ac.uk,", "yw396@eng.cam.ac.uk,", "mjfg@eng.cam.ac.uk,", "xyliu@se.cuhk.edu.hk"], "sections": [{"heading": null, "text": "Index Terms\u2014 Bidirectional recurrent neural network, language model, succeeding words, speech recognition"}, {"heading": "1. INTRODUCTION", "text": "Language models (LMs) are crucial components in many applications, such as speech recognition and machine translation. The aim of language models is to compute the probability of any given sentenceW = (w1, w2, ..., wL), which can be calculated as\nP (W) = P (w1, w2, ..., wL) = L\u220f t=1 P (wt|wt\u221211 ) (1)\nThe task of LMs is to calculate the probability of word wt given its previous history wt\u221211 = w1, w2, ..., wt\u22121. n-gram LMs [1] and neural network based language mdoels (NNLMs) [2, 3] are two widely used language models. In n-gram LMs, the most recent n\u22121 words are used as an approximation of the complete history, thus\nP (wt|wt\u221211 ) \u2248 P (wt|w t\u22121 t\u2212n+1) (2)\nThis n-gram assumption can also be used to construct a n-gram feedforward NNLMs [2]. In contrast, recurrent neural network LMs (RNNLMs) model the complete history via a recurrent connection.\nThis research was funded under the ALTA Institute, University of Cambridge. Thanks to Cambridge English, University of Cambridge, for supporting this research. Xunying Liu is funded by MSRA grant no. 6904412 and CUHK grant no. 4055065.\nMost of previous work on language models has focused on utilising history information, the future word context information has not been extensively investigated. There have been several attempts to incorporate future context information into recurrent neural network language models. Individual forward and backward RNNLMs can be built, and these two LMs combined with a log-linear interpolation [4]. In [5], succeeding words were incorporated into RNNLM within a Maximum Entropy framework. [6] investigated the use of bidirectional RNNLMs (bi-RNNLMs) for speech recognition. For a broadcast news task, sigmoid based RNNLMs gave small gains, while no performance improvement was obtained when using long short-term memory (LSTM) based RNNLMs. More recently, bi-RNNLMs can produce consistent, and significant, performance improvements over unidirectional RNNLMs (uni-RNNLMs) on a range of speech recognition tasks [7].\nThough they can yield performance gain, bi-RNNLMs pose several challenges for both model training and inference as they require the complete previous and future word context information to be taken into account. It is difficult to parallelise training efficiently. Lattice rescoring is also complicated for these LMs as future context needs to be incorporated. This means that the form of approximation used for uni-RNNLMs [8] is not suitable to apply. Hence, N-best rescoring is normally used [5, 6, 7]. However, the ability to manipulate lattices is very important in many speech applications. Lattices can be used for a wide range of downstream applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11]. In order to address these issues, a novel model structure, succeeding word RNNLMs (su-RNNLMs), is proposed in this paper. Instead of using a recurrent unit to capture the complete future word context as in bi-RNNLMs, a feedforward unit is used to model a small, fixed-length number of succeeding words. This allows existing efficient training [12] and lattice rescoring [8] algorithms developed for uni-RNNLMs to be extended to the proposed su-RNNLMs. Using these extended algorithms, compact lattices can be generated with su-RNNLMs supporting lattice based downstream processing.\nThe rest of this paper is organized as follows. Section 2 gives a brief review of RNNLMs, including both unidirectional and bidirectional RNNLMs. The proposed model with succeeding words (su-RNNLMs) is introduced in Section 3, followed by a description of the lattice rescoring algorithm in Section 4. Section 5 discusses the interpolation of language models. The experimental results are presented in Section 6 and conclusions are drawn in Section 7."}, {"heading": "2. UNI- AND BI-DIRECTIONAL RNNLMS", "text": ""}, {"heading": "2.1. Unidirectional RNNLMs", "text": "In contrast to feedforward NNLMs, where only modeling the previous n \u2212 1 words, recurrent NNLMs [13] represent the full non-\nar X\niv :1\n70 8.\n05 59\n2v 1\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 7\ntruncated history wt\u221211 = w1, w2, ..., wt\u22121 for word wt using the 1-of-K encoding of the previous word wt\u22121 and a continuous vector ht\u22122 as a compact representation of the remaining context wt\u221221 . Figure 1 shows an example of this unidirectional RNNLM (uniRNNLM). The most recent word wt\u22121 is used as input and projected into a low-dimensional, continuous, space via a linear projection layer. A recurrent hidden layer is used after this projection layer. The form of the recurrent layer can be based on a standard sigmoid based recurrent unit, with sigmoid activations [3], or more complicated forms such as gated recurrent unit (GRU) [14] and long short-term memory (LSTM) units [15]. A continuous vector ht\u22121 representing the complete history information wt\u221211 can be obtained using ht\u22122 and previous word wt\u22121. This vector is used as input of recurrent layer for the estimation of next word. An output layer with softmax function is used to calculate the probability P (wt|wt\u221211 ). An additional node is often added at the output layer to model the probability mass of out-of-shortlist (OOS) words to speed up softmax computation by limiting vocabulary size [16]. Similarly, an out-of-vocabulary (OOV) node can be added in the input layer to model OOV words. The probability of word sequenceW = wL1 is calculated as,\nPu(w L 1 ) = L\u220f t=1 P (wt|wt\u221211 ) (3)\nPerplexity (PPL) is a metric used widely to evaluate the quality of language models. According to the definition in [17], the perplexity can be computed based on sentence probability with,\nPPL = exp ( \u2212 1 N J\u2211 j=1 logPu(Wj) )\n= exp ( \u2212 1 N J\u2211 j=1 logPu(w Lj 1 ) ) = exp ( \u2212 1 N J\u2211 j=1 Lj\u2211 t=1 logP (wt|wt\u221211 ) )\n(4)\nWhere N is the total number of words and J is the number of sentence in the evaluation corpus. Lj is the number of word in jth sentence. From the above equation, the PPL is calculated based on the average log probability of each word, which for unidirectional LMs, yields the average sentence log probability.\nUni-RNNLMs can be trained efficiently on Graphics Processing Units (GPUs) by using spliced sentence bunch (i.e. minibatch) mode [12]. Multiple sentences can be concatenated together to form a longer sequence and sets of these long sequences can then be aligned in parallel from left to right. This data structure is more efficient for minibatch based training as they have comparable sequence\nlength [12]. When using these forms of language models for tasks like speech recognition, N-best rescoring is the most straightforward way to apply uni-RNNLMs. Lattice rescoring is also possible by introducing approximations [8] to control merging and expansion of different paths in lattice. This will be described in more detail in Section 4."}, {"heading": "2.2. Bidirectional RNNLMs", "text": "Figure 2 illustrates an example of bidirectional RNNLMs (biRNNLMs). Unlike uni-RNNLMs, both the history word context wt\u221211 and future word context w L t+1 are used to estimate the probability of current word P (wt|wt\u221211 , wLt+1). Two recurrent units are used to capture the previous and future information respectively. In the same fashion as uni-RNNLMs, ht\u22121 is a compact continuous vector of the history information wt\u221211 . While h\u0303t+1 is another continuous vector to encode the future information wLt+1. This future context vector is computed from the next word wt+1 and the previous future context vector h\u0303t+2 containing information of wLt+2. The concatenation of ht\u22121 and h\u0303t+1 is then fed into the output layer, with softmax function, to calculate the output probability. In order to reduce the number of parameter, the projection layer for the previous and future words are often shared.\nThe probability of word sequence W = wL1 can be computed using bi-RNNLMs as,\nPb(w L 1 ) =\n1\nZb P\u0302b(W) =\n1\nZb L\u220f t=1 P (wt|wt\u221211 , w L t+1) (5)\nP\u0302b(W) is the unnormalized sentence probability computed from the individual word probabilities of the bi-RNNLM. Zb is a sentencelevel normalization term to ensure the sentence probability is appropriately normalized. This is defined as,\nZb = \u2211 W\u2208\u0398 P\u0302b(W) (6)\nwhere \u0398 is the set of all possible sentences. Unfortunately, this normalization term is impractical to calculate for most tasks.\nIn a similar form to Equation 4, the PPL of bi-RNNLMs can be\ncalculated based on sentence probability as, PPL = exp ( \u2212 1 N J\u2211 j=1 logPb(w Lj 1 ) )\n= exp ( \u2212 1 N J\u2211 j=1 log 1 Zb P\u0302b(w Lj 1 ) )\n(7)\n= exp ( J N log(Zb)\u2212 1 N J\u2211 j=1 Lj\u2211 t=1 logP (wt|wt\u221211 , w Lj t+1) ) However, Zb is often infeasible to obtain. As a result, it is not possible to compute a valid perplexity from bi-RNNLMs. Nevertheless, the average log probability of each word can be used to get a \u201cpseudo\u201d perplexity (PPL).\nPPLpseudo = exp ( \u2212 1 N J\u2211 j=1 Lj\u2211 t=1 logP (wt|wt\u221211 , w Lj t+1) ) (8)\nThis is the second term of the valid PPL of bi-RNNLMs shown in Equation 7. It is a \u201cpseudo\u201d PPL because the normalized sentence probability Pb(W) is impossible to obtain and the unnormalized sentence probability P\u0302b(W) is used instead. Hence, the \u201cpseudo\u201d PPL of bi-RNNLMs is not comparable with the valid PPL of uniRNNLMs. However, the value of \u201cpseudo\u201d PPL provides information on the average word probability from bi-RNNLMs since it is obtained using the word probability.\nIn order to achieve good performance for speech recognition, [7] proposed an additional smoothing of the bi-RNNLM probability at test time. The probability of bi-RNNLMs is smoothed as,\nP (wi|wt\u221211 , w L t+1) = exp(\u03b1yi)\u2211V j exp(\u03b1yj)\n(9)\nwhere yi is the activation before softmax function for node i in the output layer. \u03b1 is an empirical smoothing factor, which is chosen as 0.7 in this paper.\nThe use of both preceding and following context information in bi-RNNLMs presents challenges to both model training and inference. First, N-best rescoring is normally used for speech recognition [7]. Lattice rescoring is impractical for bi-RNNLMs as the computation of word probabilities requires information from the complete sentence.\nAnother drawback of bi-RNNLMs is the difficulty in training. The complete previous and future context information is required to predict the probability of each word. It is expensive to directly training bi-RNNLMs sentence by sentence, and difficult to parallelise the training for efficiency. In [6], all sentences in the training corpus were concatenated together to form a single sequence to facilitate minibatch based training. This sequence was then \u201cchopped\u201d into sub-sequences with the average sentence length. Bi-RNNLMs were then trained on GPU by processing multiple sequences at the same time. This allows bi-RNNLMs to be efficiently trained. However, issues can arise from the random cutting of sentences, history and future context vectors may be reset in the middle of a sentence. In [7], the bi-RNNLMs were trained in a more consistent fashion. Multiple sentences were aligned from left to right to form minibatches during bi-RNNLM training. In order to handle issues caused by variable sentence length, NULL tokens were appended to the ends of sentences to ensure that the aligned sentences had the same length. These NULL tokens were not used for parameter update. In this paper, this approach is adopted to train bi-RNNLMs as it gave better performance."}, {"heading": "3. RNNLMS WITH SUCCEEDING WORDS", "text": "As discussed above, bi-RNNLMs are slow to train and difficult to use in lattice rescoring. In order to address these issues, a novel structure, the su-RNNLM, is proposed in this paper to incorporate future context information. The model structure is illustrated in Figure 3. In the same fashion as bi-RNNLMs, the previous history wt\u221211 is modeled with recurrent units (e.g. LSTM, GRU). However, instead of modeling the complete future context information, wLt+1, using recurrent units, feedforward units are used to capture a finite number of succeeding words, wt+kt+1 . The softmax function is again applied at the output layer to obtain the probability of the current word P (wt|wt\u221211 , w t+k t+1 ). The word embedding in the projection layer are shared for all input words. When the succeeding words are beyond the sentence boundary, a vector of 0 is used as the word embedding vector. This is similar to the zero padding of the feedforward forward NNLMs at the beginning of each sentence [13].\nAs the number of succeeding words is finite and fixed for each word, its succeeding words can be organized as a n-gram future context and used for minibatch mode training as in feedforward NNLMs [13]. Su-RNNLMs can then be trained efficiently in a similar fashion to uni-RNNLMs in a spliced sentence bunch mode [12].\nCompared with equations 3 and 5, the probability of word sequence wL1 can be computed as\nPs(w L 1 ) =\n1\nZs L\u220f t=1 P (wt|wt\u221211 , w t+k t+1 ) (10)\nAgain, the sentence level normalization term Zs is difficult to compute and only \u201cpseudo\u201d PPL can be obtained. The probabilities of su-RNNLMs are also very sharp, which can be seen from the \u201cpseudo\u201d PPLs in Table 2 in Section 6. Hence, the bi-RNNLM probability smoothing given in Equation 9 is also required for suRNNLMs to achieve good performance at evaluation time."}, {"heading": "4. LATTICE RESCORING", "text": "Lattice rescoring with feedforward NNLMs is straightforward [13] whereas approximations are required for uni-RNNLMs lattice rescoring [8, 18]. As mentioned in Section 2.2, N-best rescoring has previously been used for bi-RNNLMs. It is not practical for\nbi-RNNLMs to be used for lattice rescoring and generation as both the complete previous and future context information are required. However, lattices are very useful in many applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11]. In contrast, su-RNNLMs require a fixed number of succeeding words, instead of the complete future context information. From Figure 3, su-RNNLMs can be viewed as a combination of uni-RNNLMs for history information and feedforward NNLMs for future context information. Hence, lattice rescoring is feasible for su-RNNLMs by extending the lattice rescoring algorithm of uni-RNNLMs by considering additional fixed length future contexts."}, {"heading": "4.1. Lattice rescoring of uni-RNNLMs", "text": "In this paper, the n-gram approximation [8] based approach is used for uni-RNNLMs lattice rescoring. When considering merging of two paths, if their previous n\u2212 1 words are identical, the two paths are viewed as \u201cequivalent\u201d and can be merged. This is illustrated in Figure 5 for the start node of wordw4. The history information from the best path is kept for the following RNNLM probability computation and the histories of all other paths are discarded. For example, the path (w0, w2, w3) is kept and the other path (w1, w2, w3) is discarded given arc w4.\nThere are two types of approximation involved for uni-RNNLM lattice rescoring, which are the merge and cache approximations. The merge approximation controls the merging of two paths. In [8], the first path reaching the node was kept and all other paths with the same n-gram history were discarded irrespective of the associated scores. This introduces inaccuracies in the RNNLM probability calculation. The merge approximation can be improved by keeping the path with the highest accumulated score. This is the approach adopted in this work. For fast probability lookup in lattice rescoring, n-gram probabilities can be cached using n \u2212 1 words as a key. A similar approach can be used with RNNLM probabilities. In [8], RNNLM probabilities were cached based on the previous n\u2212 1 words, which is referred as cache approximation. Thus a word probability obtained from the cache may be derived from another history sharing the same n \u2212 1 previous words. This introduces another inaccuracy. In order to avoid this inaccuracy yet maintain the efficiency, the cache approximation used in [8] is improved by adopting the complete history as key for caching RNNLM probabilities. Both modifications yielt small but consistent improvements over [8] on a range of tasks."}, {"heading": "4.2. Lattice rescoring of su-RNNLMs", "text": "For lattice rescoring with su-RNNLMs, the n-gram approximation can be adopted and extended to support the future word context. In order to handle succeeding words correctly, paths will be merged only if the following succeeding words are identical. In this way, the path expansion is carried out in both directions. Any two paths with the same succeeding words and n\u2212 1 previous words are merged.\nFigure 4 shows part of an example lattice generated by a 2-gram LM. In order to apply uni-RNNLM lattice rescoring using a 3-gram approximation, the grey shaded node in Figure 4 needs to be duplicated as word w3 has two distinct 3-gram histories, which are (w0, w2) and (w1, w2) respectively. Figure 5 shows the lattice after rescoring using a uni-RNNLM with 3-gram approximation. In order to apply su-RNNLMs for lattice rescoring, the succeeding words also need to be taken into account. Figure 6 is the expanded lattice using a su-RNNLM with 1 succeeding word. The grey shaded nodes\nin Figure 5 need to be expanded further as they have distinct succeeding words. The blue shaded nodes in Figure 6 are the expanded node in the resulting lattice.\nUsing the n-gram history approximation and given k succeeding words, the lattice expansion process is effectively a n+ k-gram lattice expansion for uni-RNNLMs. For larger value of n and k, the resulting lattices can be very large. This can be addressed by pruning the lattice and doing initial lattice expansion with a uni-RNNLM."}, {"heading": "5. LANGUAGE MODEL INTERPOLATION", "text": "For unidirectional language models, such as n-gram model and uniRNNLMs, the word probabilities are normally combined using linear interpolation,\nPu(wt|wt\u221211 ) = (11) (1\u2212 \u03bb1)Pn(wt|wt\u221211 ) + \u03bb1Pr(wt|w t\u22121 1 )\nwhere Pn and Pr are the probabilities from n-gram and uni-RNN LMs respectively, \u03bb1 is the interpolation weight of uni-RNNLMs.\nHowever, it is not valid to directly combine uni-LMs (e.g unidirectional n-gram LMs or RNNLMs) and bi-LMs (or su-LMs) using linear interpolation due to the sentence level normalisation term required for bi-LMs (or su-LMs) in Equation 5. As described in [7], uni-LMs can be log-linearly interpolated with bi-LMs for speech recognition using,\nP (wt|wt\u221211 , w L t+1) = (12)\n1 Z Pu(wt|wt\u221211 ) (1\u2212\u03bb2)Pb(wt|wt\u221211 , w L t+1) \u03bb2\nwhere Z is the appropriate normalisation term. The normalisation term can be discarded for speech recognition as it does not affect the hypothesis ranking. Pu and Pb are the probabilities from uniLMs and bi-RNNLMs respectively. \u03bb2 is the log-linear interpolation weight of bi-RNNLMs. The issue of normalisation term in suRNLMs is similar to that of bi-RNNLMs, as shown in Equation 10. Hence, log-linear interpolation can also be applied for the combination of su-RNNLMs and uni-LMs and is the approach used in this paper.\nBy default, linear interpolation is used to combine uni-RNNLMs and n-gram LMs. A two-stage interpolation is used when including bi-RNNLMs and su-RNNLMs. The uni-RNNLMs and n-gram LMs are first interpolated using linear interpolation. These linearly interpolated probabilities are then log-linearly interpolated with those of bi-RNNLMs (or su-RNNLMs)."}, {"heading": "6. EXPERIMENTS", "text": "Experiments were conducted using the AMI IHM meeting corpus [19] to evaluated the speech recognition performance of various language models. The Kaldi training data configuration was used. A total of 78 hours of speech was used in acoustic model training. This consists of about 1M words of acoustic transcription. Eight meetings were excluded from the training set and used as the development and test sets.\nThe Kaldi acoustic model training recipe [20] featuring sequence training [21] was applied for deep neural network (DNN) training. CMLLR transformed MFCC features [22] were used as the input and 4000 clustered context dependent states were used as targets. The DNN was trained with 6 hidden layers, and each layer has 2048 hidden nodes.\nThe first part of the Fisher corpus, 13M words, was used for additional language modeling training data. A 49k word decoding vocabulary was used for all experiments. All LMs were trained on the combined (AMI+Fisher), 14M word in total. A 4-gram KN smoothed back-off LM without pruning was trained and used for lattices generation. GRU based recurrent units were used for all unidirectional and bidirectional RNNLMs 1. 512 hidden nodes were used in the hidden layer. An extended version of CUED-RNNLM [23] was developed for the training of uni-RNNLMs, bi-RNNLMs and su-RNNLMs. The related code and recipe will be available online 2. The linear interpolation weight \u03bb1 between 4-gram LMs and uni-RNNLMs was set to be 0.75 as it gave the best performance on the development data. The log-linear interpolation weight \u03bb2 for bi-RNNLMs (or su-RNNLMs) was 0.3. The probabilities of biRNNLMs and su-RNNLMs were smoothed with a smoothing factor 0.7 as suggested in [7]. The 3-gram approximation was applied for the history merging of uni-RNNLMs and su-RNNLMs during lattice rescoring and generation [8].\nTable 1 shows the word error rates of the baseline system with 4-gram and uni-RNN LMs. Lattice rescoring and 100-best rescoring are applied to lattices generated by the 4-gram LM. As expected, uniRNNLMs yield a significant performance improvement over 4-gram LMs. Lattice rescoring gives a comparable performance with 100- best rescoring. Confusion network (CN) decoding can be applied to lattices generated by uni-RNNLM lattice rescoring and additional performance improvements can be achieved. However, it is difficult to apply confusion network decoding to the 100-best 3.\nTable 2 gives the training speed measured with word per second (w/s) and (\u201cpseudo\u201d) PPLs of various RNNLMs with difference\n1GRU and LSTM gave similar performance for this task, while GRU LMs\namounts of future word context. When the number of succeeding words is 0, this is the baseline uni-RNNLMs. When the number of succeeding words is set to \u221e, a bi-RNNLM with complete future context information is used. It can be seen that su-RNNLMs give a comparable training speed as uni-RNNLMs. The additional computational load of the su-RNNLMs mainly come from the feedforward unit for succeeding words as shown in Figure 3. The computation in this part is much less than that of other parts such as output layer and GRU layers. However, the training of su-RNNLMs is much faster than bi-RNNLMs as it is difficult to parallelise the training of bi-RNNLMs efficiently [7]. It is worth mentioning again that the PPLs of uni-RNNLMs can not be compared directly with the \u201cpseudo\u201d PPLs of bi-RNNLMs and su-RNNLMs. But both PPLs and \u201cpseudo\u201d PPLs reflect the average log probability of each word. From Table 2, with increasing number of succeeding words, the \u201cpseudo\u201d PPLs of the su-RNNLMs keeps decreasing, yielding comparable value as bi-RNNLMs.\nTable 3 gives the WER results of 100-best rescoring with various language models. For bi-RNNLMs (or su-RNNLMs), it is not possible to use linear interpolation. Thus a two stage approach is adopted as described in Section 5. This results in slight differences, second decimal place, between the uni-RNNLM case and the 0 future context su-RNNLM. The increasing number of the succeeding words consistently reduces the WER. With 1 succeeding word, the WERs were reduced by 0.2% absolutely. Su-RNNLMs with more than 2 succeeding words gave about 0.5% absolute WER reduction. Bi-RNNLMs (shown in the bottom line of Table 3) outperform suRNNLMs by 0.1% to 0.2%, as it is able to incorporate the complete future context information with recurrent connection.\nTable 4 shows the WERs of lattice rescoring using su-RNNLMs. The lattice rescoring algorithm described in Section 4 was applied. Su-RNNLMs with 1 and 3 succeeding words were used for lattice rescoring. From Table 4, su-RNNLMs with 1 succeeding words give 0.2% WER reduction and using 3 succeeding words gives about 0.5% WER reduction. These results are consistent with the 100-best rescoring result in Table 3. Confusion network decoding can be applied on the rescored lattices and additional 0.3-0.4% WER performance improvements are obtained on dev and eval test sets.\nare faster for training and evaluation 2http://mi.eng.cam.ac.uk/projects/cued-rnnlm/ 3N-best list can be converted to lattice and CN decoding then can be applied, but it requires a much larger N-best list, such as 10K used in [8]."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, the use of future context information on neural network language models has been explored. A novel model structure is proposed to address the issues associated with bi-RNNLMs, such as slow train speed and difficulties in lattice rescoring. Instead of using a recurrent unit to capture the complete future information, a feedforward unit was used to model a finite number of succeeding words. The existing training and lattice rescoring algorithms for uni-RNNLMs are extended for the proposed su-RNNLMs. Experimental results show that su-RNNLMs achieved a slightly worse performances than bi-RNNLMs, but with much faster training speed. Furthermore, additional performance improvements can be obtained from lattice rescoring and subsequent confusion network decoding. Future work will examine improved pruning scheme to address the lattice expansion issues associated with larger future context."}, {"heading": "8. REFERENCES", "text": "[1] Stanley Chen and Joshua Goodman, \u201cAn empirical study of smoothing techniques for language modeling,\u201d Computer Speech & Language, vol. 13, no. 4, pp. 359\u2013393, 1999.\n[2] Yoshua Bengio, Re\u0301jean Ducharme, Pascal Vincent, and Christian Jauvin, \u201cA neural probabilistic language model,\u201d Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.\n[3] Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur, \u201cRecurrent neural network based language model.,\u201d in Proc. ISCA INTERSPEECH, 2010.\n[4] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig, \u201cAchieving human parity in conversational speech recognition,\u201d arXiv preprint arXiv:1610.05256, 2016.\n[5] Yangyang Shi, Martha Larson, Pascal Wiggers, and Catholijn Jonker, \u201cExploiting the succeeding words in recurrent neural network language models.,\u201d in Proc. ICSA INTERSPEECH, 2013.\n[6] Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran, and Stanley Chen, \u201cBidirectional recurrent neural network language models for automatic speech recognition,\u201d in Proc. ICASSP. IEEE, 2015, pp. 5421\u20135425.\n[7] Xie Chen, Anton Ragni, Xunying Liu, and Mark Gales, \u201cInvestigating bidirectional recurrent neural network language models for speech recognition.,\u201d in Proc. ICSA INTERSPEECH, 2017.\n[8] Xunying Liu, Xie Chen, Yongqiang Wang, Mark Gales, and Phil Woodland, \u201cTwo efficient lattice rescoring methods using recurrent neural network language models,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1438\u20131449, 2016.\n[9] Frank Wessel, Ralf Schluter, Klaus Macherey, and Hermann Ney, \u201cConfidence measures for large vocabulary continuous speech recognition,\u201d Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 3, pp. 288\u2013298, 2001.\n[10] Xie Chen, Anton Ragni, Jake Vasilakes, Xunying Liu, Kate Knill, and Mark Gales, \u201cRecurrent neural network language models for keyword search,\u201d in Proc. ICASSP. IEEE, 2017, pp. 5775\u20135779.\n[11] Lidia Mangu, Eric Brill, and Andreas Stolcke, \u201cFinding consensus in speech recognition: word error minimization and other applications of confusion networks,\u201d Computer Speech & Language, vol. 14, no. 4, pp. 373\u2013400, 2000.\n[12] Xie Chen, Xunying Liu, Yongqiang Wang, Mark Gales, and Phil Woodland, \u201cEfficient training and evaluation of recurrent neural network language models for automatic speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.\n[13] Holger Schwenk, \u201cContinuous space language models,\u201d Computer Speech & Language, vol. 21, no. 3, pp. 492\u2013518, 2007.\n[14] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.\n[15] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[16] Junho Park, Xunying Liu, Mark Gales, and Phil Woodland, \u201cImproved neural network based language modelling and adaptation,\u201d in Proc. ISCA INTERSPEECH, 2010.\n[17] Frederick Jelinek, \u201cThe dawn of statistical asr and mt,\u201d Computational Linguistics, vol. 35, no. 4, pp. 483\u2013494, 2009.\n[18] Martin Sundermeyer, Hermann Ney, and Ralf Schluter, \u201cFrom feedforward to recurrent lstm neural networks for language modeling,\u201d Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 3, pp. 517\u2013529, 2015.\n[19] Jean Carletta et al., \u201cThe AMI meeting corpus: A preannouncement,\u201d in Machine learning for multimodal interaction, pp. 28\u201339. Springer, 2006.\n[20] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., \u201cThe Kaldi speech recognition toolkit,\u201d in ASRU, IEEE Workshop on, 2011.\n[21] Karel Vesely\u0300, Arnab Ghoshal, Luka\u0301s Burget, and Daniel Povey, \u201cSequence-discriminative training of deep neural networks.,\u201d in Proc. ICSA INTERSPEECH, 2013.\n[22] Mark Gales, \u201cMaximum likelihood linear transformations for HMM-based speech recognition,\u201d Computer Speech & Language, vol. 12, no. 2, pp. 75\u201398, 1998.\n[23] Xie Chen, Xunying Liu, Mark Gales, and Phil Woodland, \u201cCUED-RNNLM an open-source toolkit for efficient training and evaluation of recurrent neural network language models,\u201d in Proc. ICASSP. IEEE, 2015."}], "references": [{"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley Chen", "Joshua Goodman"], "venue": "Computer Speech & Language, vol. 13, no. 4, pp. 359\u2013393, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Proc. ISCA INTERSPEECH, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Achieving human parity in conversational speech recognition", "author": ["Wayne Xiong", "Jasha Droppo", "Xuedong Huang", "Frank Seide", "Mike Seltzer", "Andreas Stolcke", "Dong Yu", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1610.05256, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting the succeeding words in recurrent neural network language models", "author": ["Yangyang Shi", "Martha Larson", "Pascal Wiggers", "Catholijn Jonker"], "venue": "Proc. ICSA INTERSPEECH, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Bidirectional recurrent neural network language models for automatic speech recognition", "author": ["Ebru Arisoy", "Abhinav Sethy", "Bhuvana Ramabhadran", "Stanley Chen"], "venue": "Proc. ICASSP. IEEE, 2015, pp. 5421\u20135425.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating bidirectional recurrent neural network language models for speech recognition", "author": ["Xie Chen", "Anton Ragni", "Xunying Liu", "Mark Gales"], "venue": "Proc. ICSA INTERSPEECH, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Two efficient lattice rescoring methods using recurrent neural network language models", "author": ["Xunying Liu", "Xie Chen", "Yongqiang Wang", "Mark Gales", "Phil Woodland"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1438\u20131449, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Confidence measures for large vocabulary continuous speech recognition", "author": ["Frank Wessel", "Ralf Schluter", "Klaus Macherey", "Hermann Ney"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 3, pp. 288\u2013298, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Recurrent neural network language models for keyword search", "author": ["Xie Chen", "Anton Ragni", "Jake Vasilakes", "Xunying Liu", "Kate Knill", "Mark Gales"], "venue": "Proc. ICASSP. IEEE, 2017, pp. 5775\u20135779.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Finding consensus in speech recognition: word error minimization and other applications of confusion networks", "author": ["Lidia Mangu", "Eric Brill", "Andreas Stolcke"], "venue": "Computer Speech & Language, vol. 14, no. 4, pp. 373\u2013400, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient training and evaluation of recurrent neural network language models for automatic speech recognition", "author": ["Xie Chen", "Xunying Liu", "Yongqiang Wang", "Mark Gales", "Phil Woodland"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech & Language, vol. 21, no. 3, pp. 492\u2013518, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Improved neural network based language modelling and adaptation", "author": ["Junho Park", "Xunying Liu", "Mark Gales", "Phil Woodland"], "venue": "Proc. ISCA INTERSPEECH, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "The dawn of statistical asr and mt", "author": ["Frederick Jelinek"], "venue": "Computational Linguistics, vol. 35, no. 4, pp. 483\u2013494, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schluter"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 3, pp. 517\u2013529, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The AMI meeting corpus: A preannouncement", "author": ["Jean Carletta"], "venue": "Machine learning for multimodal interaction, pp. 28\u201339. Springer, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "The Kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz"], "venue": "ASRU, IEEE Workshop on, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["Karel Vesel\u1ef3", "Arnab Ghoshal", "Luk\u00e1s Burget", "Daniel Povey"], "venue": "Proc. ICSA INTERSPEECH, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["Mark Gales"], "venue": "Computer Speech & Language, vol. 12, no. 2, pp. 75\u201398, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "CUED-RNNLM an open-source toolkit for efficient training and evaluation of recurrent neural network language models", "author": ["Xie Chen", "Xunying Liu", "Mark Gales", "Phil Woodland"], "venue": "Proc. ICASSP. IEEE, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "n-gram LMs [1] and neural network based language mdoels (NNLMs) [2, 3] are two widely used language models.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "n-gram LMs [1] and neural network based language mdoels (NNLMs) [2, 3] are two widely used language models.", "startOffset": 64, "endOffset": 70}, {"referenceID": 2, "context": "n-gram LMs [1] and neural network based language mdoels (NNLMs) [2, 3] are two widely used language models.", "startOffset": 64, "endOffset": 70}, {"referenceID": 1, "context": "This n-gram assumption can also be used to construct a n-gram feedforward NNLMs [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "Individual forward and backward RNNLMs can be built, and these two LMs combined with a log-linear interpolation [4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "In [5], succeeding words were incorporated into RNNLM within a Maximum Entropy framework.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "[6] investigated the use of bidirectional RNNLMs (bi-RNNLMs) for speech recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "More recently, bi-RNNLMs can produce consistent, and significant, performance improvements over unidirectional RNNLMs (uni-RNNLMs) on a range of speech recognition tasks [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "This means that the form of approximation used for uni-RNNLMs [8] is not suitable to apply.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Hence, N-best rescoring is normally used [5, 6, 7].", "startOffset": 41, "endOffset": 50}, {"referenceID": 5, "context": "Hence, N-best rescoring is normally used [5, 6, 7].", "startOffset": 41, "endOffset": 50}, {"referenceID": 6, "context": "Hence, N-best rescoring is normally used [5, 6, 7].", "startOffset": 41, "endOffset": 50}, {"referenceID": 8, "context": "Lattices can be used for a wide range of downstream applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "Lattices can be used for a wide range of downstream applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "Lattices can be used for a wide range of downstream applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 11, "context": "This allows existing efficient training [12] and lattice rescoring [8] algorithms developed for uni-RNNLMs to be extended to the proposed su-RNNLMs.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "This allows existing efficient training [12] and lattice rescoring [8] algorithms developed for uni-RNNLMs to be extended to the proposed su-RNNLMs.", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "In contrast to feedforward NNLMs, where only modeling the previous n \u2212 1 words, recurrent NNLMs [13] represent the full nonar X iv :1 70 8.", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "The form of the recurrent layer can be based on a standard sigmoid based recurrent unit, with sigmoid activations [3], or more complicated forms such as gated recurrent unit (GRU) [14] and long short-term memory (LSTM) units [15].", "startOffset": 114, "endOffset": 117}, {"referenceID": 13, "context": "The form of the recurrent layer can be based on a standard sigmoid based recurrent unit, with sigmoid activations [3], or more complicated forms such as gated recurrent unit (GRU) [14] and long short-term memory (LSTM) units [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "The form of the recurrent layer can be based on a standard sigmoid based recurrent unit, with sigmoid activations [3], or more complicated forms such as gated recurrent unit (GRU) [14] and long short-term memory (LSTM) units [15].", "startOffset": 225, "endOffset": 229}, {"referenceID": 15, "context": "An additional node is often added at the output layer to model the probability mass of out-of-shortlist (OOS) words to speed up softmax computation by limiting vocabulary size [16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "According to the definition in [17], the perplexity can be computed based on sentence probability with,", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "minibatch) mode [12].", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "length [12].", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Lattice rescoring is also possible by introducing approximations [8] to control merging and expansion of different paths in lattice.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "In order to achieve good performance for speech recognition, [7] proposed an additional smoothing of the bi-RNNLM probability at test time.", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "First, N-best rescoring is normally used for speech recognition [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "In [6], all sentences in the training corpus were concatenated together to form a single sequence to facilitate minibatch based training.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], the bi-RNNLMs were trained in a more consistent fashion.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "This is similar to the zero padding of the feedforward forward NNLMs at the beginning of each sentence [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "As the number of succeeding words is finite and fixed for each word, its succeeding words can be organized as a n-gram future context and used for minibatch mode training as in feedforward NNLMs [13].", "startOffset": 195, "endOffset": 199}, {"referenceID": 11, "context": "Su-RNNLMs can then be trained efficiently in a similar fashion to uni-RNNLMs in a spliced sentence bunch mode [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "Lattice rescoring with feedforward NNLMs is straightforward [13] whereas approximations are required for uni-RNNLMs lattice rescoring [8, 18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "Lattice rescoring with feedforward NNLMs is straightforward [13] whereas approximations are required for uni-RNNLMs lattice rescoring [8, 18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 17, "context": "Lattice rescoring with feedforward NNLMs is straightforward [13] whereas approximations are required for uni-RNNLMs lattice rescoring [8, 18].", "startOffset": 134, "endOffset": 141}, {"referenceID": 8, "context": "However, lattices are very useful in many applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "However, lattices are very useful in many applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "However, lattices are very useful in many applications, such as confidence score estimation [9], keyword search [10] and confusion network decoding [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 7, "context": "In this paper, the n-gram approximation [8] based approach is used for uni-RNNLMs lattice rescoring.", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "In [8], the first path reaching the node was kept and all other paths with the same n-gram history were discarded irrespective of the associated scores.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [8], RNNLM probabilities were cached based on the previous n\u2212 1 words, which is referred as cache approximation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In order to avoid this inaccuracy yet maintain the efficiency, the cache approximation used in [8] is improved by adopting the complete history as key for caching RNNLM probabilities.", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "Both modifications yielt small but consistent improvements over [8] on a range of tasks.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "As described in [7], uni-LMs can be log-linearly interpolated with bi-LMs for speech recognition using,", "startOffset": 16, "endOffset": 19}, {"referenceID": 18, "context": "Experiments were conducted using the AMI IHM meeting corpus [19] to evaluated the speech recognition performance of various language models.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "The Kaldi acoustic model training recipe [20] featuring sequence training [21] was applied for deep neural network (DNN) training.", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "The Kaldi acoustic model training recipe [20] featuring sequence training [21] was applied for deep neural network (DNN) training.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "CMLLR transformed MFCC features [22] were used as the input and 4000 clustered context dependent states were used as targets.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "An extended version of CUED-RNNLM [23] was developed for the training of uni-RNNLMs, bi-RNNLMs and su-RNNLMs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "7 as suggested in [7].", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "The 3-gram approximation was applied for the history merging of uni-RNNLMs and su-RNNLMs during lattice rescoring and generation [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "However, the training of su-RNNLMs is much faster than bi-RNNLMs as it is difficult to parallelise the training of bi-RNNLMs efficiently [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "uk/projects/cued-rnnlm/ 3N-best list can be converted to lattice and CN decoding then can be applied, but it requires a much larger N-best list, such as 10K used in [8].", "startOffset": 165, "endOffset": 168}], "year": 2017, "abstractText": "Recently, bidirectional recurrent network language models (biRNNLMs) have been shown to outperform standard, unidirectional, recurrent neural network language models (uni-RNNLMs) on a range of speech recognition tasks. This indicates that future word context information beyond the word history can be useful. However, bi-RNNLMs pose a number of challenges as they make use of the complete previous and future word context information. This impacts both training efficiency and their use within a lattice rescoring framework. In this paper these issues are addressed by proposing a novel neural network structure, succeeding word RNNLMs (suRNNLMs). Instead of using a recurrent unit to capture the complete future word contexts, a feedforward unit is used to model a finite number of succeeding, future, words. This model can be trained much more efficiently than bi-RNNLMs and can also be used for lattice rescoring. Experimental results on a meeting transcription task (AMI) show the proposed model consistently outperformed uni-RNNLMs and yield only a slight degradation compared to bi-RNNLMs in N-best rescoring. Additionally, performance improvements can be obtained using lattice rescoring and subsequent confusion network decoding.", "creator": "LaTeX with hyperref package"}}}