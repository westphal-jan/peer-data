{"id": "1708.05655", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Multi-objective Contextual Multi-armed Bandit Problem with a Dominant Objective", "abstract": "In this essay, we propose a new multi-objective contextual multi-armed bandit problem with two objectives, in which one of the objectives dominates the other. In contrast to one of the objective bandit problems, in which the learner receives a random scalar reward for each arm, in the proposed problem he chooses a random reward vector, in which each component of the reward vector corresponds to one of the objectives and the distribution of the reward depends on the context provided to the learner at the beginning of each round. We call this problem a contextual multi-armed bandit with a dominant target (CMAB-DO). In the CMAB-DO, the learner's goal is to maximize his total reward in the non-dominant target while ensuring that he maximizes his total reward in the dominant target. In this case, the optimal arm given in a context is the one that does not maximize the expected reward in the dominant target.", "histories": [["v1", "Fri, 18 Aug 2017 15:41:58 GMT  (512kb,D)", "http://arxiv.org/abs/1708.05655v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cem tekin", "eralp turgay"], "accepted": false, "id": "1708.05655"}, "pdf": {"name": "1708.05655.pdf", "metadata": {"source": "CRF", "title": "Multi-objective Contextual Multi-armed Bandit Problem with a Dominant Objective", "authors": ["Cem Tekin", "Eralp Turgay"], "emails": ["cemtekin@ee.bilkent.edu.tr,", "turgay@ee.bilkent.edu.tr."], "sections": [{"heading": null, "text": "Index Terms\u2014Online learning, contextual bandits, multiobjective bandits, dominant objective, multi-dimensional regret, Pareto regret.\nI. INTRODUCTION With the rapid increase in the generation speed of the streaming data, online learning methods are becoming increasingly valuable for sequential decision making problems. Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives. In this work, we propose a multi-objective contextual bandit problem with dominant and non-dominant objectives. For this problem, we construct a multi-objective contextual bandit algorithm named MOC-MAB, which maximizes long-term reward of the nondominant objective conditioned on the fact that it maximizes the long-term reward of the dominant objective.\nC. Tekin and E. Turgay are with the Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey, 06800. Email: cemtekin@ee.bilkent.edu.tr, turgay@ee.bilkent.edu.tr.\nThis work is supported by TUBITAK 2232 Grant 116C043 and supported in part by TUBITAK 3501 Grant 116E229.\nA preliminary version of this work is accepted to IEEE MLSP 2017.\nIn this problem, the learner observes a multi-dimensional context in the beginning of each round. Then, it selects one of the available arms and receives a random reward vector, which is drawn from a fixed distribution that depends on the context and the selected arm. No statistical assumptions are made on the way the contexts arrive, and the learner does not have any a priori information on the reward distributions. The optimal arm is defined as the one that maximizes the expected reward of the non-dominant objective among all arms that maximize the expected reward of the dominant objective given the context.\nThe learner\u2019s performance is measured in terms of its regret, which measures the loss that the learner accumulates due to not knowing the reward distributions beforehand. We introduce two new notions of regret: the 2D regret and the Pareto regret. The 2D regret is a vector whose ith component corresponds to the difference between the expected total reward of an oracle in objective i that selects optimal arm for each context and that of the learner by time T . On the other hand, the Pareto regret measures sum of the distances of the arms selected by the learner to the Pareto front. For this, we extend the Pareto regret proposed in [7] to take into account the dependence of the Pareto front on the context.\nWe prove that MOC-MAB achieves O\u0303(T (2\u03b1+d)/(3\u03b1+d)) 2D regret, where d is the dimension of the context and \u03b1 is a constant that depends on the similarity information that relates the distances between contexts to the distances between expected rewards of an arm. This shows that MOC-MAB is average-reward optimal in the limit T \u2192\u221e in both objectives. We also show that the optimal arm lies in the Pareto front, and MOC-MAB also achieves O\u0303(T (2\u03b1+d)/(3\u03b1+d)) Pareto regret. Then, we argue that it is possible to make the Pareto regret of MOC-MAB O\u0303(T (\u03b1+d)/(2\u03b1+d)) by adjusting its parameters, such that the Pareto regret becomes order optimal up to a logarithmic factor [8], but this comes at an expense of making the regret in the non-dominant objective of MOC-MAB linear in the number of rounds.\nTo the best of our knowledge, our work is the first to formulate a contextual multi-objective bandit problem and prove sublinear bounds on the 2D regret and the Pareto regret. In addition, we also evaluate the performance of MOC-MAB through simulations and compare it with other single and multi-objective bandit algorithms and various offline methods. Our results show that MOC-MAB outperforms its competitors, which are not specifically designed to deal with problems involving dominant and non-dominant objectives.\nThe rest of the paper is organized as follows. Related work is given in Section II. Problem formulation, definitions of the 2D regret and the Pareto regret, and possible applications of\nar X\niv :1\n70 8.\n05 65\n5v 1\n[ cs\n.L G\n] 1\n8 A\nug 2\n01 7\n2 CMAB-DO are given in Section III. MOC-MAB is introduced in Section IV, and its regrets are analyzed in Section V. An extension of MOC-MAB that deals with dynamically changing reward distributions is proposed in Section VI. Numerical results are presented in Section VII, and concluding remarks are provided in Section VIII."}, {"heading": "II. RELATED WORK", "text": "In the past decade, many variants of the classical multiarmed problem have been introduced (see [11] for a comprehensive discussion). Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7]. While these examples have been studied separately in prior works, in this paper we aim to fuse contextual bandits and multiobjective bandits together. Below, we discuss the related work on the classical multi-armed bandit problem, contextual bandits and multi-objective bandits. The differences between our work and related works are summarized in Table I."}, {"heading": "A. The Classical Multi-armed Bandit Problem", "text": "The classical multi-armed bandit problem involves K arms with unknown reward distributions. The learner sequentially selects arms and observes noisy reward samples from the selected arms. The goal of the learner is to use the knowledge it obtains through these observations to maximize its longterm reward. For this, the learner needs to identify arms with high rewards without wasting too much time on arms with low rewards. In conclusion, it needs to strike the balance between exploration and exploitation.\nA through technical analysis of the classical multi-armed bandit problem is given in [14], where it is shown that O(log T ) regret is achieved asymptotically by index policies that use upper confidence bounds (UCBs) for the rewards. This result is tight in the sense that there is a matching asymptotic lower bound. Later on, it is shown in [15] that it is possible to achieve O(log T ) regret by using index policies constructed using the sample means of the arm rewards. The first finitetime logarithmic regret bound is given in [16]. Strikingly, the algorithm that achieves this bound computes the arm indices using only the information about the current round, the sample mean arm rewards and the number of times each arm is selected. This line of research has been followed by many others, and new algorithms with tighter regret bounds have been proposed [17]."}, {"heading": "B. The Contextual Bandit Problem", "text": "In the contextual bandit problem, different from the classical multi-armed bandit problem, the learner observes a context (side information) at the beginning of each round, which gives a hint about the expected arm rewards in that round. The context naturally arises in many practical applications such as social recommender systems [18], medical diagnosis [19] and big data stream mining [20]. Existing work on contextual bandits can be categorized into three based on how the contexts arrive and how they are related to the arm rewards.\nThe first category assumes the existence of similarity information (usually provided in terms of a metric) that relates the\nvariation in the expected reward of an arm as a function of the context to the distance between the contexts. For this category, no statistical assumptions are made on how the contexts arrive. However, given a particular context, the arm rewards come from a fixed distribution parameterized by the context.\nThis problem is considered in [8], and the Query-AdClustering algorithm that achieves O(T 1\u22121/(2+dc)+ ) regret for any > 0 is proposed, where dc is the covering dimension of the similarity space. In addition, \u2126(T 1\u22121/(2+dp)\u2212 ) lower bound on the regret, where dp is the packing dimension of the similarity space, is also proposed in this work. The main idea behind Query-Ad-Clustering is to partition the context space into disjoint sets and to estimate the expected arm rewards for each set in the partition separately. A parallel work [9] proposes the contextual zooming algorithm which partitions the similarity space non-uniformly, according to both sampling frequency and rewards obtained from different regions of the similarity space. It is shown that contextual zooming achieves O\u0303(T 1\u22121/(2+dz)) regret, where dz is the zooming dimension of the similarity space, which is an optimistic version of the covering dimension that depends on the size of the set of nearoptimal arms.\nIn this category of contextual bandits, reward estimates are accurate as long as the contexts that lie in the same set of the context space partition are similar to each other. However, when dimension of the context is high, the regret bound becomes almost linear. This issue is addressed in [21], where it is assumed that the arm rewards depend on an unknown subset of the contexts, and it is shown that the regret in this case only depends on the number of relevant context dimensions.\nThe second category assumes that the expected reward of an arm is a linear combination of the elements of the context. For this model, Li et al. [1] proposed the LinUCB algorithm. A modified version of this algorithm, named SupLinUCB, is considered by Chu et al. [10], and is shown to achieve O\u0303( \u221a Td) regret, where d is the dimension of the context. Valko et al. [22] mixed LinUCB and SupLinUCB with kernel functions and proposed an algorithm whose regret is O\u0303( \u221a T d\u0303) where d\u0303 is the effective dimension of the kernel feature space. The third category assumes that the contexts and arm rewards are jointly drawn from a fixed but unknown distribution. For this case, Langford et al. [12] proposed the epoch greedy algorithm with O(T 2/3) regret and later works [13], [23] proposed more efficient learning algorithms with O\u0303(T 1/2) regret.\nOur problem is similar to the problems in the first category in terms of the context arrivals and existence of the similarity information."}, {"heading": "C. The Multi-objective Bandit Problem", "text": "In the multi-objective bandit problem, the learner receives a multi-dimensional reward in each round. Since the rewards are no longer scalar, the definition of a benchmark to compare the learner against becomes obscure. Existing work on multiobjective bandits can be categorized into two: Pareto approach and scalarized approach.\n3\nIn the Pareto approach, the main idea is to estimate the Pareto front set which consists of the arms that are not dominated by any other arm. Dominance relationship is defined such that if the expected reward of an arm a\u2217 is greater than the expected reward of another arm a in at least one objective, and the expected reward of the arm a is not greater than the expected reward of the arm a\u2217 in any objective, then the arm a\u2217 dominates the arm a. This approach is proposed in [7], and a learning algorithm called Pareto-UCB1 that achieves O(log T ) Pareto regret is proposed. Essentially, this algorithm computes UCB indices for each objective-arm pair, and then, uses these indices to estimate the Pareto front arm set, after which it selects an arm randomly from the Pareto front set. A modified version of this algorithm where the indices depend on both the estimated mean and the estimated standard deviation is proposed in [24]. Numerous other variants are also considered in prior works, including the Pareto Thompson sampling algorithm in [25] and the Annealing Pareto algorithm in [26].\nOn the other hand, in the scalarized approach [7], [27], a random weight is assigned to each objective at each round, from which for each arm a weighted sum of the indices of the objectives are calculated. In short, this method turns the multiobjective bandit problem into a single-objective problem. For instance, Scalarized UCB1 in [7] achieves O(S\u2032 log(T/S\u2032)) scalarized regret where S\u2032 is the number of scalarization functions used by the algorithm.\nThe regret notion used in the Pareto and scalarized approaches are very different from our 2D regret notion. In the Pareto approach, the regret at round t is defined as the minimum distance that should be added to expected reward vector of the chosen arm at round t to move the chosen arm to the Pareto front. On the other hand, scalarized regret is the difference between scalarized expected rewards of the optimal arm and the chosen arm. Different from these definitions, which define the regret as a scalar quantity, we define the 2D regret as a two-dimensional vector. Hence, our goal is to minimize a multi-dimensional regret measure conditioned on the fact that we minimize the regret in the dominant objective. We show that by achieving this, we also minimize the Pareto regret."}, {"heading": "III. PROBLEM DESCRIPTION", "text": ""}, {"heading": "A. System Model", "text": "The system operates in a sequence of rounds indexed by t \u2208 {1, 2, . . .}. At the beginning of round t, the learner observes a d-dimensional context denoted by xt. Without loss of generality, we assume that xt lies in the context space X := [0, 1]d. After observing xt the learner selects an arm at from a finite set A, and then, observes a two dimensional random reward rt = (r1t , r 2 t ) that depends both on xt and at. Here, r1t and r 2 t denote the rewards in the dominant and the non-dominant objectives, respectively, and are given by r1t = \u00b5 1 at(xt) + \u03ba 1 t and r 2 t = \u00b5 2 at(xt) + \u03ba 2 t , where \u00b5 i a(x), i \u2208 {1, 2} denotes the expected reward of arm a in objective i given context x, and the noise process {(\u03ba1t , \u03ba2t )} is such that the marginal distribution of \u03bait, i \u2208 {1, 2} is conditionally 1-sub-Gaussian,1 i.e.,\n\u2200\u03bb \u2208 R E[e\u03bb\u03ba i t |a1:t,x1:t,\u03ba11:t\u22121,\u03ba21:t\u22121] \u2264 exp(\u03bb2/2)\nwhere b1:t := (b1, . . . , bt). The expected reward vector for context-arm pair (x, a) is denoted by \u00b5a(x) := (\u00b51a(x), \u00b5 2 a(x)).\nThe set of arms that maximize the expected reward for the dominant objective for context x is given as A\u2217(x) := arg maxa\u2208A \u00b5 1 a(x). The set of optimal arms is given as the set of arms in A\u2217(x) with the highest expected rewards for the non-dominant objective. Without loss of generality, we assume that there is a single optimal arm, and denote it by a\u2217(x). Hence, we have a\u2217(x) = arg maxa\u2208A\u2217(x) \u00b5 2 a(x). Let \u00b5 1 \u2217(x) and \u00b52\u2217(x) denote the expected rewards of arm a \u2217(x) in the dominant and the non-dominant objectives, respectively, given context x. We assume that the expected rewards are Ho\u0308lder continuous in the context, which is a common assumption in the contextual bandit literature [8], [19], [20].\nAssumption 1. There exists L > 0, \u03b1 > 0 such that for all i \u2208 {1, 2} , a \u2208 A and x, x\u2032 \u2208 X , we have\n|\u00b5ia(x)\u2212 \u00b5ia(x\u2032)| < L \u2016x\u2212 x\u2032\u2016 \u03b1 .\nAnother common way to compare arms when the rewards are multi-dimensional is to use the notion of Pareto optimality, which is described below.\n1Examples of 1-sub-Gaussian distributions include the Gaussian distribution with zero mean and unit variance, and any distribution defined over an interval of length 2 with zero mean [28]. Moreover, our results generalize to the case when \u03bait is conditionally R-sub-Gaussian for R \u2265 1. This only changes the constant terms that appear in our regret bounds.\n4 Definition 1 (Pareto Optimality). (i) An arm a is weakly dominated by arm a\u2032 given context x, denoted by \u00b5a(x) \u00b5a\u2032(x) or \u00b5a\u2032(x) \u00b5a(x), if \u00b5ia(x) \u2264 \u00b5ia\u2032(x),\u2200i \u2208 {1, 2}. (ii) An arm a is dominated by arm a\u2032 given context x, denoted by \u00b5a(x) \u227a \u00b5a\u2032(x) or \u00b5a\u2032(x) \u00b5a(x), if it is weakly dominated and \u2203i \u2208 {1, 2} such that \u00b5ia(x) < \u00b5ia\u2032(x). (iii) Two arms a and a\u2032 are incomparable given context x, denoted by \u00b5a(x)||\u00b5a\u2032(x), if neither arm dominates the other. (iv) An arm is Pareto optimal given context x if it is not dominated by any other arm given context x. Given a particular context x, the set of all Pareto optimal arms is called the Pareto front, and is denoted by O(x).\nRemark 1. Note that a\u2217(x) \u2208 O(x) for all x \u2208 X since a\u2217(x) is not dominated by any other arm. For all a \u2208 A, we have \u00b51\u2217(x) \u2265 \u00b51a(x). By definition of a\u2217(x) if there exists an arm a for which \u00b52a(x) > \u00b5 2 \u2217(x), then we must have \u00b5 1 a(x) < \u00b5 1 \u2217(x). Such an arm will be incomparable with a\u2217(x)."}, {"heading": "B. Definitions of the 2D Regret and the Pareto Regret", "text": "Initially, the learner does not know the expected rewards; it learns them over time. The goal of the learner is to compete with an oracle, which knows the expected rewards of the arms for every context and chooses the optimal arm given the current context. Hence, the 2D regret of the learner by round T is defined as the tuple (Reg1(T ),Reg2(T )), where\nRegi(T ) := T\u2211 t=1 \u00b5i\u2217(xt)\u2212 T\u2211 t=1 \u00b5iat(xt), i \u2208 {1, 2} (1)\nfor an arbitrary sequence of contexts x1, . . . , xT . Another interesting performance measure is the Pareto regret [7], which measures the loss of the learner with respect to arms in the Pareto front. To define the Pareto regret, we first define the Pareto suboptimality gap (PSG).\nDefinition 2 (PSG of an arm). The PSG of an arm a \u2208 A given context x, denoted by \u2206a(x), is defined as the minimum scalar \u2265 0 that needs to be added to all entries of \u00b5a(x) such that a becomes a member of the Pareto front. Formally,\n\u2206a(x) := inf \u22650\ns.t. (\u00b5a(x) + ) || \u00b5a\u2032(x),\u2200a\u2032 \u2208 O(x)\nwhere is a 2-dimensional vector, whose entries are .\nBased on the above definition, the Pareto regret of the learner by round T is given by\nPR(T ) := T\u2211 t=1 \u2206at(xt). (2)"}, {"heading": "C. Applications of CMAB-DO", "text": "In this subsection we describe two possible applications of CMAB-DO.\n1) Multichannel Communication: Consider a multi-channel communication scenario in which a user chooses a channel Q \u2208 Q and a transmission rate R \u2208 R in each round after receiving context xt := {SNRQ,t}Q\u2208Q, where SNRQ,t is the signal to noise ratio of channel Q in round t.\nIn this setup, each arm corresponds to a transmission ratechannel pair denoted by aR,Q. Hence, the set of arms is A = R\u00d7Q. When the user completes its transmission at the end of round t, it receives a two dimensional reward where the dominant one is related to throughput and the non-dominant one is related to reliability. Here, r2t \u2208 {0, 1} where 0 and 1 correspond to failed and successful transmission, respectively. Moreover, the success rate of aR,Q is equal to \u00b52aR,Q(xt) = 1\u2212pout(R,Q, xt), where pout(\u00b7) denotes the outage probability. Here, pout(R,Q, xt) also depends on the gain on channel Q whose distribution is unknown to the user. On the other hand, for aR,Q, r1t \u2208 {0, R} and \u00b51aR,Q(xt) = R(1\u2212pout(R,Q, xt)). It is usually the case that the outage probability increases with R, so maximizing the throughput and reliability are usually conflicting objectives.2\n2) Online Binary Classification: Consider a medical diagnosis problem where a patient with context xt (including features such as age, gender, medical test results etc.) arrives in round t. Then, this patient is assigned to one of the experts in A who will diagnose the patient. In reality, these experts can either be clinical decision support systems or humans, but the classification performance of these experts are context dependent and unknown a priori. In this problem, the dominant objective can correspond to accuracy while the non-dominant objective can correspond to false negative rate. For this case, the rewards in both objectives are binary, and depend on whether the classification is correct and a positive case is correctly identified."}, {"heading": "IV. THE LEARNING ALGORITHM", "text": "We introduce MOC-MAB in this section. Its pseudocode is given in Algorithm 1. MOC-MAB uniformly partitions X into md hypercubes with edge lengths 1/m. This partition is denoted by P . For each p \u2208 P and a \u2208 A it keeps: (i) a counter Na,p that counts the number of times the context was in p and arm a was selected before the current round, (ii) the sample mean of the rewards obtained from rounds prior to the current round in which the context was in p and arm a was selected, i.e., \u00b5\u03021a,p and \u00b5\u0302 2 a,p for the dominant and non-dominant objectives, respectively. The idea behind partitioning is to utilize the similarity of arm rewards given in Assumption 1 to learn together for groups of similar contexts. Basically, when the number of sets in the partition is small, the number past samples that fall into a specific set is large; however, the similarity of the past samples that fall into the same set is small. The optimal partitioning should balance the inaccuracy in arm reward estimates that results form these two conflicting facts.\nAt round t, MOC-MAB first identifies the hypercube in P that contains xt, which is denoted by p\u2217. Then, it calculates the following indices for the rewards in dominant and nondominant objectives:\ngia,p\u2217 := \u00b5\u0302 i a,p\u2217 + ua,p\u2217 , i \u2208 {1, 2} (3)\n2Note that in this example, given that arm aR,Q is selected, we have \u03ba1t = r1t \u2212 \u00b51aR,Q (xt) and \u03ba 2 t = r 2 t \u2212 \u00b52aR,Q (xt). Clearly, both \u03ba 1 t and \u03ba 2 t are zero mean. Moreover, \u03ba1t \u2208 [\u2212R,R] and \u03ba2t \u2208 [\u22121, 1]. Hence, \u03ba1t is R-subGaussian and \u03ba2t is 1-sub-Gaussian.\n5 Algorithm 1 MOC-MAB 1: Input: T , d, L, \u03b1, m, \u03b2 2: Initialize sets: Create partition P of X into md identical\nhypercubes 3: Initialize counters: Na,p = 0, \u2200a \u2208 A, \u2200p \u2208 P , t = 1 4: Initialize estimates: \u00b5\u03021a,p = \u00b5\u0302 2 a,p = 0, \u2200a \u2208 A, \u2200p \u2208 P 5: while 1 \u2264 t \u2264 T do 6: Find p\u2217 \u2208 P such that xt \u2208 p\u2217 7: Compute gia,p\u2217 for a \u2208 A, i \u2208 {1, 2} as given in (3) 8: Set a\u22171 = arg maxa\u2208A g 1 a,p\u2217 .\n9: if ua\u22171 ,p\u2217 > \u03b2v then 10: Select arm at = a\u22171 11: else 12: Find set of candidate optimal arms A\u0302\u2217 given in (4) 13: Select arm at = arg maxa\u2208A\u0302\u2217 g 2 a,p\u2217 14: end if 15: Observe rt = (r1t , r 2 t ) 16: \u00b5\u0302iat,p\u2217 \u2190 (\u00b5\u0302 i at,p\u2217Nat,p\u2217 + r i t)/(Nat,p\u2217 + 1), i \u2208 {1, 2} 17: Nat,p\u2217 \u2190 Nat,p\u2217 + 1 18: t\u2190 t+ 1 19: end while\nwhere the uncertainty level ua,p := \u221a\n2Am,T /Na,p, Am,T := (1 + 2 log(4|A|mdT 3/2)) represents the uncertainty over the sample mean estimate of the reward due to the number of instances that are used to compute \u00b5\u0302ia,p\u2217 .\n3 Hence, a UCB for \u00b5ia(x) is g i a,p + v for x \u2208 p, where v := Ld\u03b1/2m\u2212\u03b1 denotes the non-vanishing uncertainty term due to context space partitioning. Since this term is non-vanishing, we also name it the margin of tolerance. The main learning principle in such a setting is called optimism under the face of uncertainty. The idea is to inflate the reward estimates from arms that are not selected often by a certain level, such that the inflated reward estimate becomes an upper confidence bound for the true expected reward with a very high probability. This way, arms that are not selected frequently are explored, and this exploration potentially helps the learner to discover arms that are better than the arm with the highest estimated reward. As expected, the uncertainty level vanishes as an arm gets selected more often.\nAfter calculating the UCBs, MOC-MAB judiciously determines the arm to select based on these UCBs. It is important to note that the choice a\u22171 := arg maxa\u2208A g 1 a,p\u2217 can be highly suboptimal for the non-dominant objective. To see this, consider a very simple setting, where A = {a, b}, \u00b51a(x) = \u00b5 1 b(x) = 0.5, \u00b5 2 a(x) = 1 and \u00b5 2 b(x) = 0 for all x \u2208 X . For an algorithm that always selects at = a\u22171 and that randomly chooses one of the arms with the highest index in the dominant objective in case of a tie, both arms will be equally selected in expectation. Hence, due to the noisy rewards, there are sample paths in which arm 2 is selected more than half of the time. For these sample paths, the expected regret in the non-dominant objective is at least T/2. MOC-MAB overcomes the effect of the noise mentioned above due to\n3Although MOC-MAB requires T as input, it can run without the knowledge of T beforehand by applying a method called the doubling-trick. See [29] and [19] for a discussion on the doubling-trick.\nthe randomness in the rewards and the partitioning of X by creating a safety margin below the maximal index g1a\u22171 ,p\u2217 for the dominant objective, when its confidence for a\u22171 is high, i.e., when ua\u22171 ,p\u2217 \u2264 \u03b2v, where \u03b2 > 0 is a constant. For this, it calculates the set of candidate optimal arms given as\nA\u0302\u2217 := { a \u2208 A : g1a,p\u2217 \u2265 \u00b5\u03021a\u22171 ,p\u2217 \u2212 ua\u22171 ,p\u2217 \u2212 2v } (4)\n= { a \u2208 A : \u00b5\u03021a,p\u2217 \u2265 \u00b5\u03021a\u22171 ,p\u2217 \u2212 ua\u22171 ,p\u2217 \u2212 ua,p\u2217 \u2212 2v } .\nHere, the term \u2212ua\u22171 ,p\u2217 \u2212 ua,p\u2217 \u2212 2v accounts for the joint uncertainty over the sample mean rewards of arms a and a\u22171. Then, MOC-MAB selects at = arg maxa\u2208A\u0302\u2217 g 2 a,p\u2217 .\nOn the other hand, when its confidence for a\u22171 is low, i.e., when ua\u22171 ,p\u2217 > \u03b2v, it has a little hope even in selecting an optimal arm for the dominant objective. In this case it just selects at = a\u22171 to improve its confidence for a \u2217 1. After its arm selection, it receives the random reward vector rt, which is then used to update the counters and the sample mean rewards for p\u2217.\nRemark 2. At each round, finding the set in P that xt belongs to requires d computations. Moreover, each of the following processes requires |A| computations: (i) finding maximum value among the indices of the dominant objective, (ii) creating a candidate set and finding maximum value among the indices of the non-dominant objective. Hence, MOC-MAB requires dT+3|A|T computations in T rounds. In addition, the memory complexity of MOC-MAB is O(md)."}, {"heading": "V. REGRET ANALYSIS", "text": "In this section we prove that both the 2D regret and the Pareto regret of MOC-MAB are sublinear functions of T . Hence, MOC-MAB is average reward optimal in both regrets. First, we introduce the following as preliminaries.\nFor an event F , let Fc denote the complement of that event. For all the parameters defined in Section IV, we explicitly use the round index t, when referring to the value of that parameter at the beginning of round t. For instance, Na,p(t) denotes the value of Na,p at the beginning of round t. Let Np(t) denote the number of context arrivals to p \u2208 P by round t, \u03c4p(t) denote the round in which a context arrives to p \u2208 P for the tth time, and Ria(t) denote the random reward of arm a in objective i at round t. Let x\u0303p(t) := x\u03c4p(t), R\u0303 i a,p(t) := Ria(\u03c4p(t)), N\u0303a,p(t) := Na,p(\u03c4p(t)), \u00b5\u0303 i a,p(t) := \u00b5\u0302 i a,p(\u03c4p(t)), a\u0303p(t) := a\u03c4p(t), \u03ba\u0303 i p(t) := \u03ba i \u03c4p(t)\nand u\u0303a,p(t) := ua,p(\u03c4p(t)). Let Tp := {t \u2208 {1, . . . , T} : xt \u2208 p} denote the set of rounds for which the context is in p \u2208 P .\nNext, we define the following lower and upper bounds: Lia,p(t) := \u00b5\u0303 i a,p(t)\u2212 u\u0303a,p(t) and U ia,p(t) := \u00b5\u0303ia,p(t) + u\u0303a,p(t) for i \u2208 {1, 2}. Let\nUCia,p := Np(T )\u22c3 t=1 {\u00b5ia(x\u0303p(t)) /\u2208 [Lia,p(t)\u2212 v, U ia,p(t) + v]}\ndenote the event that the learner is not confident about its reward estimate in objective i for at least once in rounds in which the context is in p by time T . Here Lia,p(t) \u2212 v and U ia,p(t) + v are the lower confidence bound (LCB) and UCB\n6 for \u00b5ia(x\u0303p(t)), respectively. Also, let UC i p := \u222aa\u2208AUCia,p, UCp := \u222ai\u2208{1,2}UCip and UC := \u222ap\u2208PUCp, and for each i \u2208 {1, 2}, p \u2208 P and a \u2208 A, let\n\u00b5ia,p = sup x\u2208p \u00b5ia(x) and \u00b5 i a,p = inf x\u2208p \u00b5ia(x).\nLet\nRegip(T ) := Np(T )\u2211 t=1 \u00b5i\u2217(x\u0303p(t))\u2212 Np(T )\u2211 t=1 \u00b5ia\u0303p(t)(x\u0303p(t))\ndenote the regret incurred in objective i for rounds in Tp (regret incurred in p \u2208 P). Then, the total regret in objective i can be written as\nRegi(T ) = \u2211 p\u2208P Regip(T ). (5)\nThus, the expected regret in objective i becomes E[Regi(T )] = \u2211 p\u2208P E[Regip(T )]. (6)\nNext, we bound E[Regip(T )]. We have\nE[Regip(T )] = E[Regip(T ) | UC] Pr(UC) + E[Reg i p(T ) | UC c] Pr(UCc) \u2264 CimaxNp(T ) Pr(UC) + E[Regip(T ) | UC c] (7)\nwhere Cimax is the maximum difference in the expected reward of the optimal arm and any other arm for objective i.\nHaving obtained the decomposition in (7), we proceed by bounding the terms in (7). For this, we first bound Pr(UCp) in the next lemma.\nLemma 1. For any p \u2208 P , we have Pr(UCp) \u2264 1/(mdT ).\nProof. The proof is given in Appendix A.\nUsing the result of Lemma 1, we obtain\nPr(UC) \u2264 1/T and Pr(UCc) \u2265 1\u2212 1/T. (8)\nTo prove the lemma above, we use the concentration inequality given in Lemma 6 in [28] to bound the probability of UCia,p. However, a direct application of this inequality is not possible to our problem, due to the fact that the context sequence x\u0303p(1), . . . , x\u0303p(Np(t)) does not have identical elements, which makes the mean values of R\u0303ia,p(1), . . . , R\u0303 i a,p(Np(t)) different. To overcome this problem, we use the sandwich technique proposed in [19] in order to bound the rewards sampled from actual context arrivals between the rewards sampled from two specific processes that are related to the original process, where each process has a fixed mean value.\nAfter bounding the probability of the event Pr(UCp), we bound the instantaneous (single round) regret on event Pr(UCc). For simplicity of notation, in the following lemmas we use a\u2217(t) := a\u2217(x\u0303p(t)) to denote the optimal arm, a\u0303(t) := a\u0303p(t) to denote the arm selected at round \u03c4p(t) and a\u0302\u22171(t) to denote the arm whose first index is highest at round \u03c4p(t), when the set p \u2208 P that the context belongs to is obvious.\nThe following lemma shows that on event UCcp the regret incurred in a round \u03c4p(t) for the dominant objective can be\nbounded as function of the difference between the upper and lower confidence bounds plus the margin of tolerance.\nLemma 2. When MOC-MAB is run, on event UCcp, we have\n\u00b51a\u2217(t)(x\u0303p(t))\u2212 \u00b5 1 a\u0303(t)(x\u0303p(t)) \u2264U 1 a\u0303(t),p(t)\u2212 L 1 a\u0303(t),p(t)\n+ 2(\u03b2 + 2)v\nfor all t \u2208 {1, . . . , Np(T )}.\nProof. We consider two cases. When u\u0303a\u0302\u22171(t),p(t) \u2264 \u03b2v, we have\nU1a\u0303(t),p(t) \u2265 L 1 a\u0302\u22171(t),p (t)\u2212 2v \u2265 U1a\u0302\u22171(t),p(t)\u2212 2u\u0303a\u0302\u22171(t),p(t)\u2212 2v \u2265 U1a\u0302\u22171(t),p(t)\u2212 2(\u03b2 + 1)v.\nOn the other hand, when u\u0303a\u0302\u22171(t),p(t) > \u03b2v, the selected arm is a\u0303(t) = a\u0302\u22171(t). Hence, we obtain\nU1a\u0303(t),p(t) = U 1 a\u0302\u22171(t),p (t) \u2265 U1a\u0302\u22171(t),p(t)\u2212 2(\u03b2 + 1)v.\nThus, for both cases, we have\nU1a\u0303(t),p(t) \u2265 U 1 a\u0302\u22171(t),p (t)\u2212 2(\u03b2 + 1)v (9)\nand\nU1a\u0302\u22171(t),p(t) \u2265 U 1 a\u2217(t),p(t). (10)\nOn event UCcp, we also have\n\u00b51a\u2217(t)(x\u0303p(t)) \u2264 U 1 a\u2217(t),p(t) + v (11)\nand\n\u00b51a\u0303(t)(x\u0303p(t)) \u2265 L 1 a\u0303(t),p(t)\u2212 v. (12)\nBy combining (9)-(12), we obtain\n\u00b51a\u2217(t)(x\u0303p(t))\u2212 \u00b5 1 a\u0303(t)(x\u0303p(t)) \u2264U 1 a\u0303(t),p(t)\u2212 L 1 a\u0303(t),p(t)\n+ 2(\u03b2 + 2)v.\nThe lemma below bounds the regret incurred in a round \u03c4p(t) for the non-dominant objective on event UCcp when the uncertainty level of the arm with the highest index in the dominant objective is low.\nLemma 3. When MOC-MAB is run, on event UCcp, for t \u2208 {1, . . . , Np(T )} if\nu\u0303a\u0302\u22171(t),p(t) \u2264 \u03b2v\nholds, then we have\n\u00b52a\u2217(t)(x\u0303p(t))\u2212 \u00b5 2 a\u0303(t)(x\u0303p(t)) \u2264 U 2 a\u0303(t),p(t)\u2212 L 2 a\u0303(t),p + 2v.\nProof. When u\u0303a\u0302\u22171(t),p(t) \u2264 \u03b2v holds all arms that are selected as candidate optimal arms have their index for objective 1 in the interval [L1a\u0302\u22171(t),p(t)\u2212 2v, U 1 a\u0302\u22171(t),p\n(t)]. Next, we show that U1a\u2217(t),p(t) is also in this interval.\nOn event UCcp, we have\n\u00b51a\u2217(t)(x\u0303p(t)) \u2208 [L 1 a\u2217(t),p(t)\u2212 v, U 1 a\u2217(t),p(t) + v] \u00b51a\u0302\u22171(t)(x\u0303p(t)) \u2208 [L 1 a\u0302\u22171(t),p (n)\u2212 v, U1a\u0302\u22171(t),p(t) + v].\n7 We also know that\n\u00b51a\u2217(t)(x\u0303p(t)) \u2265 \u00b5 1 a\u0302\u22171(t) (x\u0303p(t)).\nUsing the inequalities above, we obtain\nU1a\u2217(t),p(t) \u2265 \u00b5 1 a\u2217(t)(x\u0303p(t))\u2212 v \u2265 \u00b5 1 a\u0302\u22171(t) (x\u0303p(t))\u2212 v \u2265 L1a\u0302\u22171(t),p(t)\u2212 2v.\nSince the selected arm has the maximum index for the non-dominant objective among all arms whose indices for the dominant objective are in [L1a\u0302\u22171(t),p(t) \u2212 2v, U 1 a\u0302\u22171(t),p\n(t)], we have U2a\u0303(t),p(t) \u2265 U 2 a\u2217(t),p(t). Combining this with the fact that UCcp holds, we get\n\u00b52a\u0303(t)(x\u0303p(t)) \u2265 L 2 a\u0303(t),p(t)\u2212 v (13)\nand\n\u00b52a\u2217(t)(x\u0303p(t)) \u2264 U 2 a\u2217(t),p(t) + v \u2264 U 2 a\u0303(t),p(t) + v. (14)\nFinally, by combining (13) and (14), we obtain\n\u00b52a\u2217(t)(x\u0303p(t))\u2212 \u00b5 2 a\u0303(t)(x\u0303p(t)) \u2264 U 2 a\u0303(t),p(t)\u2212 L 2 a\u0303(t),p(t) + 2v.\nFor any p \u2208 P , we also need to bound the regret of the non-dominant objective for rounds in which u\u0303a\u0302\u22171(t),p(t) > \u03b2v, t \u2208 {1, . . . , Np(T )}.\nLemma 4. When MOC-MAB is run, the number of rounds in Tp for which u\u0303a\u0302\u22171(t),p(t) > \u03b2v happens is bounded above by\n|A| (\n2Am,T \u03b22v2 + 1\n) .\nProof. This event happens when N\u0303a\u0302\u22171(t),p(t) < 2Am,T /(\u03b2\n2v2). Every such event will result in an increase in the value of Na\u0302\u22171(t),p by one. Hence, for p \u2208 P and a \u2208 A, the number of times u\u0303a,p(t) > \u03b2v can happen is bounded above by 2Am,T /(\u03b22v2) + 1. The final result is obtained by summing over all arms.\nIn the next lemmas, we bound Reg1p(t) and Reg 2 p(t) given\nthat UCc holds.\nLemma 5. When MOC-MAB is run, on event UCc, we have for all p \u2208 P\nReg1p(t) \u2264 |A|C1max + 2Bm,T \u221a |A|Np(t) + 2(\u03b2 + 2)vNp(t).\nwhere Bm,T := 2 \u221a 2Am,T .\nProof. The proof is given in Appendix B.\nLemma 6. When MOC-MAB is run, on event UCc we have for all p \u2208 P\nReg2p(t) \u2264C2max|A| (\n2Am,T \u03b22v2 + 1\n) + 2vNp(t)\n+ 2Bm,T \u221a |A|Np(t).\nProof. The proof is given in Appendix C.\nNext, we use the result of Lemmas 1, 5 and 6 to find a bound on Regi(t) that holds for all t \u2264 T with probability at least 1\u2212 1/T .\nTheorem 1. When MOC-MAB is run, we have for any i \u2208 {1, 2}\nPr(Regi(t) < i(t) \u2200t \u2208 {1, . . . , T}) \u2265 1\u2212 1/T\nwhere\n1(t) = m d|A|C1max + 2Bm,T\n\u221a |A|mdt+ 2(\u03b2 + 2)vt\nand\n2(t) =m d|A|C2max +mdC2max|A| ( 2Am,T \u03b22v2 ) + 2Bm,T \u221a |A|mdt+ 2vt.\nProof. By (5) and Lemmas 5 and 6, we have on event UCc: Reg1(t) \u2264md|A|C1max + 2Bm,T \u2211 p\u2208P \u221a |A|Np(t)\n+ 2(\u03b2 + 2)vt \u2264md|A|C1max + 2Bm,T \u221a |A|mdt\n+ 2(\u03b2 + 2)vt.\nand Reg2(t) \u2264md|A|C2max +mdC2max|A| (\n2Am,T \u03b22v2 ) + 2Bm,T\n\u2211 p\u2208P \u221a |A|Np(t) + 2vt\n\u2264md|A|C2max +mdC2max|A| (\n2Am,T \u03b22v2 ) + 2Bm,T \u221a |A|mdt+ 2vt\nfor all t \u2264 T . The result follows from the fact that UCc holds with probability at least 1\u2212 1/T .\nThe following theorem shows that the expected 2D regret of MOC-MAB by time T is O\u0303(T 2\u03b1+d 3\u03b1+d ).\nTheorem 2. When MOC-MAB is run with inputs m = dT 1/(3\u03b1+d)e and \u03b2 > 0, we have\nE[Reg1(T )] \u2264C1max + 2d|A|C1maxT d 3\u03b1+d\n+2(\u03b2 + 2)Ld\u03b1/2T 2\u03b1+d 3\u03b1+d +2d/2+1Bm,T \u221a |A|T 1.5\u03b1+d 3\u03b1+d\nand E[Reg2(T )] \u2264 2d/2+1Bm,T \u221a |A|T 1.5\u03b1+d 3\u03b1+d + C2max\n+ ( 2Ld\u03b1/2 +\nC2max|A|21+2\u03b1+dAm,T \u03b22L2d\u03b1\n) T 2\u03b1+d 3\u03b1+d\n+2dC2max|A|T d 3\u03b1+d .\nProof. E[Regi(T )] is bounded by using the result of Theorem 1 and (7):\nE[Regi(T )] \u2264 E[Regi(T ) | UCc] + \u2211 p\u2208P CimaxNp(T ) Pr(UC)\n8 \u2264 E[Regi(T ) | UCc] + \u2211 p\u2208P CimaxNp(T )/T = E[Regi(T ) | UCc] + Cimax.\nTherefore, we have\nE[Reg1(T )] \u2264 1(T ) + C1max E[Reg2(T )] \u2264 2(T ) + C2max.\nIt can be shown that when we set m = dT 1/(2\u03b1+d)e regret bound of the dominant objective becomes O\u0303(T (\u03b1+d)/(2\u03b1+d)) and regret bound of the non-dominant objective becomes O(T ). The optimal value for m that makes both regrets sublinear is m = dT 1/(3\u03b1+d)e. With this value of m, we obtain\nE[Reg1(T )] \u22642d|A|C1maxT d 3\u03b1+d + 2(\u03b2 + 2)Ld\u03b1/2T 2\u03b1+d 3\u03b1+d + 2d/2+1Bm,T \u221a |A|T 1.5\u03b1+d 3\u03b1+d + C1max\nand E[Reg2(T )] \u2264 ( 2Ld\u03b1/2 + C2max|A|21+2\u03b1+dAm,T\n\u03b22L2d\u03b1\n) T 2\u03b1+d 3\u03b1+d\n+ C2max + 2 dC2max|A|T\nd 3\u03b1+d + 2d/2+1Bm,T \u221a |A|T 1.5\u03b1+d 3\u03b1+d .\nFrom the results above we conclude that both regrets are O\u0303(T (2\u03b1+d)/(3\u03b1+d)), where for the first regret bound the constant that multiplies the highest order of the regret does not depend on A, while the dependence on this term is linear for the second regret bound.\nNext, we show that the expected value of the Pareto regret of MOC-MAB given in (2) is also O\u0303(T (2\u03b1+d)/(3\u03b1+d)).\nTheorem 3. When MOC-MAB is run with inputs m = dT 1/(3\u03b1+d)e and \u03b2 > 0, we have\nPr(PR(t) < 1(t) \u2200t \u2208 {1, . . . , T}) \u2265 1\u2212 1/T\nwhere 1(t) is given in Theorem 1 and\nE[PR(T )] \u2264C1max + 2d|A|C1maxT d 3\u03b1+d\n+2(\u03b2 + 2)Ld\u03b1/2T 2\u03b1+d 3\u03b1+d +2d/2+1Bm,T \u221a |A|T 1.5\u03b1+d 3\u03b1+d .\nProof. Consider any p \u2208 P and t \u2208 {1, . . . , Np(T )}. By definition \u2206a\u0303(t)(x\u0303p(t)) \u2264 \u00b51a\u2217(t)(x\u0303p(t))\u2212\u00b5 1 a\u0303(t)(x\u0303p(t)). This holds since for any > 0, adding \u00b51a\u2217(t)(x\u0303p(t)) \u2212 \u00b5 1 a\u0303(t)(x\u0303p(t)) + to \u00b51a\u0303(t)(x\u0303p(t)) will either make it (i) dominate the arms in O(x\u0303p(t)) or (ii) incomparable with the arms in O(x\u0303p(t)). Hence, using the result in Lemma 2, we have on event UCc\n\u2206a\u0303(t)(x\u0303p(t)) \u2264 U1a\u0303(t),p(t)\u2212 L 1 a\u0303(t),p(t) + 2(\u03b2 + 2)v.\nLet PRp(T ) := \u2211Np(T ) t=1 \u2206a\u0303(t)(x\u0303p(t)). Hence, PR(T ) =\u2211\np\u2208P PRp(T ). Due to this, the results derived for Reg 1(t) and Reg1(T ) in Theorems 1 and 2 also hold for PRp(t) and PRp(T ).\nTheorem 3 shows that the regret measures E[Reg1(T )], E[Reg2(T )] and E[PR(T )] for MOC-MAB are all\nO\u0303(T (2\u03b1+d)/(3\u03b1+d)) when it is run with m = dT 1/(3\u03b1+d)e. This implies that MOC-MAB is average reward optimal in all regret measures as T \u2192\u221e. The growth rate of the Pareto regret can be further decreased by setting m = dT 1/(2\u03b1+d)e. This will make the Pareto regret O\u0303(T (\u03b1+d)/(2\u03b1+d)) (which matches with the lower bound in [8] for the single-objective contextual bandit problem with similarity information up to a logaritmic factor) but will also make the regret in the non-dominant objective linear."}, {"heading": "VI. LEARNING UNDER PERIODICALLY CHANGING REWARD DISTRIBUTIONS", "text": "In many practical cases, the reward distribution of an arm changes periodically over time even under the same context. For instance, in a recommender system the probability that a user clicks to an ad may change with the time of the day, but the pattern of change can be periodical on a daily basis and this can be known by the system. Moreover, this change is usually gradual over time. In this section, we extend MOC-MAB such that it can deal with such settings.\nFor this, let Ts denote the period. For the d-dimensional context xt = (x1,t, x2,t, ..., xd,t) received at round t let x\u0302t := (x1,t, x2,t, ..., xd+1,t) denote the extended context where xd+1,t := (t mod Ts)/Ts is the time context. Let X\u0302 denote the d + 1 dimensional extended context space constructed by adding the time dimension to X . It is assumed that the following holds for the extended contexts.\nAssumption 2. Given any x\u0302, x\u0302\u2032 \u2208 X\u0302 , there exists L\u0302 > 0 and \u03b1\u0302 > 0 such that for all i \u2208 {1, 2} and a \u2208 A, we have\n|\u00b5ia(x\u0302)\u2212 \u00b5ia(x\u0302\u2032)| \u2264 L\u0302||x\u0302\u2212 x\u0302\u2032||\u03b1\u0302.\nNote that Assumption 2 implies Assumption 1 with L = L\u0302 and \u03b1 = \u03b1\u0302 when x\u0302d+1 = x\u0302\u2032d+1. Moreover, for two contexts (x1, . . . , xd, xd+1) and (x1, . . . , xd, x\u2032d+1), we have\n|\u00b5ia(x\u0302)\u2212 \u00b5ia(x\u0302\u2032)| \u2264 L\u0302|xd+1 \u2212 x\u2032d+1|\u03b1\u0302\nwhich implies that the change in the expected rewards is gradual. Under Assumption 2, the performance of MOC-MAB is bounded as follows.\nCorollary 1. When MOC-MAB is run with inputs L\u0302, \u03b1\u0302, m = dT 1/(3\u03b1\u0302+d+1)e, and \u03b2 > 0 by using the extended context space X\u0302 instead of the original context space X , we have\nE[Regi(T )] = O\u0303(T (2\u03b1\u0302+d+1)/(3\u03b1\u0302+d+1)) for i \u2208 {1, 2}.\nProof. The proof simply follows from the proof of Theorem 2 by extending the dimension of the context space by one.\nVII. ILLUSTRATIVE RESULTS\nIn order to evaluate the performance of MOC-MAB, we run three different experiments both with synthetic and real-world datasets. In the first two experiments, the task is classification. Hence, we compare MOC-MAB with well known classifiers. In the third experiment, we compare MOC-MAB with other bandit algorithms.\n9"}, {"heading": "A. Experiment 1", "text": "In this experiment, we evaluate the performance of MOCMAB on the breast cancer dataset from the UCI Machine Learning Repository [30]. The dataset contains 569 instances and 30 features such as radius, texture, perimeter, area, smoothness, compactness etc. 357 instances are labeled as benign. The others are labeled as malignant.\nFirst, we apply PCA to reduce the dimension of the feature vector to 3. We use these features in the rest of the experiment. The benchmarks we compare MOC-MAB against are support vector machines, multilayer perceptron and logistic regression. For each run the dataset is randomly split into 284 training and 285 test instances. All of the reported results are averaged over 50 runs. The test phase is carried out by randomly sampling data instances from the test instances. This allows us to increase the number of test rounds beyond the number of test instances.\nThe benchmarks are trained offline and are kept fixed during the test phase. Since MOC-MAB is an online method, it is not trained before testing. MOC-MAB has two arms: one arm always predicts benign and the other arm always predicts malignant. Note that false negative results miss the malignant tumor and have dangerous consequences for the patients. Hence, hyper-parameters of the methods are adjusted such that the false negative rate (FNR) is kept below 4%. This is achieved by shifting the hyperplanes in SVM, modifying the loss function in multilayer perceptron, changing the prior in logistic regression and adjusting \u03b2 in MOC-MAB. Also, the uncertainty level of MOC-MAB is multiplied by 1/10 to reduce the number of explorations. The loss (negative of the reward) of MOC-MAB in the dominant and the non-dominant objectives are set as the misclassification and the false negative events, respectively.\nAccuracy and false negative rate of the learning methods are presented in Fig. 1 as a function of the size of the test phase. This figure shows that the performance of MOC-MAB improves as it learns during the test phase, and it beats the other methods in both objectives as the size of the test phase gets larger."}, {"heading": "B. Experiment 2", "text": "In this experiment, we use the dataset and competitor algorithms from Experiment 1. The test phase size is fixed to be T = 105. The purpose of this experiment is to highlight the advantage of online learning over offline learning, when the distribution of the training and test sets are different. For this purpose offline benchmarks are trained with training sets that have different benign patient rates. Fig. 2 reports the error rate at T = 105 averaged over 10 runs. As expected, the error rate of the offline methods depend highly on the training set decomposition, and hence, they perform much worse than MOC-MAB."}, {"heading": "C. Experiment 3", "text": "In this experiment, we compare MOC-MAB with other bandit algorithms on a synthetic multi-objective dataset. We take X = [0, 1]2 and assume that the context at each time\n10\nstep is chosen uniformly at random from X . We consider 4 arms and the time horizon is set as T = 105. The expected arm rewards for 3 of the arms are generated as follows: We generate 3 multivariate Gaussian distributions for the dominant objective and 3 multivariate Gaussian distributions for the non-dominant objective. For the dominant objective, the mean vectors of the first two distributions are set as [0.3, 0.5], and the mean vector of the third distribution is set as [0.7, 0.5]. Similarly, for the non-dominant objective, the mean vectors of the distributions are set as [0.3, 0.7], [0.3, 0.3] and [0.7, 0.5], respectively. For all the Gaussian distributions the covariance matrix is given by 0.3\u2217 I where I is the 2 by 2 identity matrix. Then, each Gaussian distribution is normalized by multiplying it with a constant, such that its maximum value becomes 1. These normalized distributions form the expected arm rewards. In addition, the expected reward of the fourth arm for the dominant objective is set as 0, and its expected reward for the non-dominant objective is set as the normalized multivariate Gaussian distribution with mean vector [0.7, 0.5]. We assume that the reward of an arm in an objective given a context x is a Bernoulli random variable whose parameter is equal to the magnitude of the corresponding normalized distribution at context x.\nWe compare MOC-MAB with the following bandit algorithms: Pareto UCB1 (P-UCB1): This is the Empirical Pareto UCB1 algorithm proposed in [7]. Scalarized UCB1 (S-UCB1): This is the Scalarized Multiobjective UCB1 algorithm proposed in [7]. Contextual Pareto UCB1 (CP-UCB1): This is the contextual version of P-UCB1 which partitions the context space in the same way as MOC-MAB does, and uses a different instance of P-UCB1 in each set of the partition. Contextual Scalarized UCB1 (CS-UCB1): This is the contextual version of S-UCB1, which partitions the context space in the same way as MOC-MAB does, and uses a different instance of S-UCB1 in each set of the partition. Contextual Dominant UCB1 (CD-UCB1): This is the contextual version of UCB1 [16], which partitions the context space in the same way as MOC-MAB does, and uses a different instance of UCB1 in each set of the partition. This algorithm only uses the rewards from the dominant objective to update the indices of the arms.\nFor S-UCB1 and CS-UCB1, the weights of the linear scalarization functions are chosen as [1, 0], [0.5, 0.5] and [0, 1]. For all contextual algorithms, the partition of the context space is formed by choosing m according to Theorem 2. For MOC-MAB, \u03b2 is chosen as 1. In addition, we scaled down the uncertainty level (also known as the confidence term or the inflation term) of all the algorithms by a constant chosen from {1, 1/5, 1/10, 1/15, 1/20, 1/25, 1/30}, since we observed that the regrets of all algorithms become smaller when the uncertainty level is scaled down. For MOC-MAB the optimal scale factor for the dominant objective is 1/15, for CS-UCB1 and CD-UCB1, it is 1/5, for CP-UCB1, it is 1/30 and for the non-contextual algorithms, it is 1/20. The regret results are obtained by using the optimal scale factor\nfor each algorithm.\nEvery algorithm is run 100 times and the results are averaged over these runs. Simulation results given in Fig. 3 show the change in the regret of the algorithms in both objectives as a function of time (rounds). As observed from the results, MOC-MAB beats all other algorithms in both objectives except CD-UCB1. While the regret of CD-UCB1 in the dominant objective is slightly better than that of MOCMAB, its regret is much worse than MOC-MAB in the non-dominant objective. This is expected since it only aims to maximize the reward in the dominant objective without considering the other objective. In the dominant objective, obtained total reward of MOC-MAB is 5.5% higher than that of CP-UCB1, 8.7% higher than that of CS-UCB1, 49.9% higher than that of P-UCB1 and 51.5% higher than that of S-UCB1 but 1% smaller than that of CD-UCB1. In the nondominant objective, obtained total reward of MOC-MAB is 3.8% higher than that of CP-UCB1, 7% higher than that of CS-UCB1, 20.2% higher than that of CD-UCB1, and 68.6% higher than that of P-UCB1 and 69.4% higher than that of S-UCB1.\n11"}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we propose a new contextual bandit problem with two objectives in which one objective is dominant and the other is non-dominant. According to this definition, we propose two performance metrics: the 2D regret (which is multi-dimensional) and the Pareto regret (which is scalar). Then, we propose an online learning algorithm called MOCMAB and show that it achieves sublinear 2D regret and Pareto regret. To the best of our knowledge, our work is the first to consider a multi-objective contextual bandit problem where the expected arm rewards and contexts are related through similarity information. We also evaluate the performance of MOC-MAB on both real-world and synthetic datasets and compare it with offline methods and other bandit algorithms. Our results demonstrate that MOC-MAB outperforms its competitors, which are not specifically designed to deal with problems involving dominant and non-dominant objectives."}, {"heading": "APPENDIX A OOF OF LEMMA 1", "text": "Proof. From the definitions of Lia,p(t), U i a,p(t) and UC i a,p, it can be observed that the event UCia,p happens when \u00b5 i a(x\u0303p(t)) does not fall into the confidence interval [Lia,p(t)\u2212v, U ia,p(t)+ v] for some t. The probability of this event could be easily bounded by using the concentration inequality given in Appendix D, if the expected reward from the same arm did not change over rounds. However, this is not the case in our model since the elements of {x\u0303p(t)} Np(T ) t=1 are not identical which makes the distributions of R\u0303ia,p(t), t \u2208 {1, . . . , Np(T )} different.\nIn order to resolve this issue, we propose the following: Recall that\nR\u0303ia,p(t) = \u00b5 i a(x\u0303p(t)) + \u03ba\u0303 i p(t)\nand\n\u00b5\u0303ia,p(t) =\n\u2211t\u22121 l=1 R\u0303 i a,p(l)I(a\u0303p(l) = a)\nN\u0303a,p(t) .\nWe define two new sequences of random variables, whose sample mean values will lower and upper bound \u00b5\u0303ia,p(t). The best sequence is defined as {Ria,p(t)} Np(T ) t=1 where\nR i\na,p(t) = \u00b5 i a,p + \u03ba\u0303 i p(t)\nand the worst sequence is defined as {Ria,p(t)} Np(T ) t=1 where\nRia,p(t) = \u00b5 i a,p + \u03ba\u0303ip(t).\nLet\n\u00b5ia,p(t) := t\u22121\u2211 l=1 R i a,p(l)I(a\u0303p(l) = a)/N\u0303a,p(t)\n\u00b5i a,p (t) := t\u22121\u2211 l=1 Ria,p(l)I(a\u0303p(l) = a)/N\u0303a,p(t).\nWe have\n\u00b5i a,p (t) \u2264 \u00b5\u0303ia,p(t) \u2264 \u00b5ia,p(t) \u2200t \u2208 {1, . . . , Np(T )}\nalmost surely. Let\nL i\na,p(t) := \u00b5 i a,p(t)\u2212 u\u0303a,p(t)\nU i\na,p(t) := \u00b5 i a,p(t) + u\u0303a,p(t)\nLia,p(t) := \u00b5 i a,p (t)\u2212 u\u0303a,p(t) U ia,p(t) := \u00b5 i a,p (t) + u\u0303a,p(t).\nIt can be shown that\n{\u00b5ia(x\u0303p(t)) /\u2208 [Lia,p(t)\u2212 v, U ia,p(t) + v]}\n\u2282 {\u00b5ia(x\u0303p(t)) /\u2208 [L i a,p(t)\u2212 v, U i\na,p(t) + v]} \u222a {\u00b5ia(x\u0303p(t)) /\u2208 [L i a,p(t)\u2212 v, U i a,p(t) + v]}. (15)\nThe following inequalities can be obtained from the Ho\u0308lder continuity assumption:\n\u00b5ia(x\u0303p(t)) \u2264 \u00b5ia,p \u2264 \u00b5ia(x\u0303p(t)) + L\n(\u221a d\nm\n)\u03b1 (16)\n\u00b5ia(x\u0303p(t))\u2212 L\n(\u221a d\nm\n)\u03b1 \u2264 \u00b5i\na,p \u2264 \u00b5ia(x\u0303p(t)). (17)\nSince v = L (\u221a d/m )\u03b1\n, using (16) and (17) it can be shown that\n(i) {\u00b5ia(x\u0303p(t)) /\u2208[L i a,p(t)\u2212 v, U i a,p(t) + v]}\n\u2282 {\u00b5ia,p /\u2208 [L i a,p(t), U i\na,p(t)]}, (ii) {\u00b5ia(x\u0303p(t)) /\u2208[L i a,p(t)\u2212 v, U i a,p(t) + v]}\n\u2282 {\u00b5i a,p /\u2208 [Lia,p(t), U i a,p(t)]}.\nPlugging these into (15), we get\n{\u00b5ia(x\u0303p(t)) /\u2208 [Lia,p(t)\u2212 v, U ia,p(t) + v]}\n\u2282 {\u00b5ia,p /\u2208 [L i a,p(t), U i a,p(t)]} \u222a {\u00b5ia,p /\u2208 [L i a,p(t), U i a,p(t)]}.\nThen, using the equation above and the union bound, we obtain\nPr(UCia,p) \u2264 Pr Np(T )\u22c3 t=1 {\u00b5ia,p /\u2208 [L i a,p(t), U i a,p(t)]}  + Pr\nNa,p(T )\u22c3 t=1 {\u00b5i a,p /\u2208 [Lia,p(t), U i a,p(t)]}  . Both terms on the right-hand side of the inequality above can be bounded using the concentration inequality in Appendix D. Using \u03b4 = 1/(4|A|mdT ) in Appendix D gives\nPr(UCia,p) \u2264 1\n2|A|mdT since 1 + Na,p(T ) \u2264 T . Then, using the union bound, we obtain\nPr(UCip) \u2264 1\n2mdT\nand\nPr(UCp) \u2264 1\nmdT .\n12"}, {"heading": "APPENDIX B PROOF OF LEMMA 5", "text": "Let Ta,p := {1 \u2264 l \u2264 Np(t) : a\u0303p(l) = a} and T\u0303a,p := {l \u2208 Ta,p : N\u0303a,p(l) \u2265 1}. By Lemma 2, we have\nReg1p(t) = \u2211 a\u2208A \u2211 l\u2208Ta,p ( \u00b51\u2217(x\u0303p(l))\u2212 \u00b51a\u0303p(l)(x\u0303p(l)) ) \u2264 \u2211 a\u2208A \u2211 l\u2208T\u0303a,p ( U1a\u0303p(l),p(l)\u2212 L 1 a\u0303p(l),p (l) + 2(\u03b2 + 2)v )\n+ |A|C1max = \u2211 a\u2208A \u2211 l\u2208T\u0303a,p ( U1a\u0303p(l),p(l)\u2212 L 1 a\u0303p(l),p (l) ) + 2(\u03b2 + 2)vNp(t) + |A|C1max. (18)\nWe also have\u2211 a\u2208A \u2211 l\u2208T\u0303a,p ( U1a\u0303p(l),p(l)\u2212 L 1 a\u0303p(l),p (l) )\n\u2264 \u2211 a\u2208A Bm,T \u2211 l\u2208T\u0303a,p \u221a 1 N\u0303a,p(l)  \u2264 Bm,T\n\u2211 a\u2208A Na,p(t)\u22121\u2211 k=0\n\u221a 1\n1 + k \u2264 2Bm,T \u2211 a\u2208A \u221a Na,p(t) (19)\n\u2264 2Bm,T \u221a |A|Np(t) (20)\nwhere Bm,T = 2 \u221a 2Am,T , and (19) follows from the fact that\nNa,p(t)\u22121\u2211 k=0\n\u221a 1 1 + k \u2264 \u222b Na,p(t) x=0 1\u221a x dx = 2 \u221a Na,p(t).\nCombining (18) and (20), we obtain that on event UCc Reg1p(t) \u2264 |A|C1max + 2Bm,T \u221a |A|Np(t) + 2(\u03b2 + 2)vNp(t)."}, {"heading": "APPENDIX C PROOF OF LEMMA 6", "text": "Using the result of Lemma 4, the contribution to the regret of the non-dominant objective in rounds for which u\u0303a\u0302\u22171(t),p(t) > \u03b2v is bounded by\nC2max|A| (\n2Am,T \u03b22v2 + 1\n) . (21)\nLet T 2a,p := {l \u2264 Np(t) : a\u0303p(t) = a and N\u0303a,p(l) \u2265 2Am,T /(\u03b2\n2v2)}. By Lemma 3, we have\u2211 a\u2208A \u2211 l\u2208T 2a,p ( \u00b52\u2217(x\u0303p(l))\u2212 \u00b52a\u0303p(l)(x\u0303p(l)) ) \u2264 \u2211 a\u2208A \u2211 l\u2208T 2a,p ( U2a\u0303p(l),p(l)\u2212 L 2 a\u0303p(l),p (l) + 2v )\n= \u2211 a\u2208A \u2211 l\u2208T 2a,p ( U2a\u0303p(l),p(l)\u2212 L 2 a\u0303p(l),p (l) ) + 2vNp(t) (22)\nWe have on event UCc\u2211 a\u2208A \u2211 l\u2208T 2a,p ( U2a\u0303p(l),p(l)\u2212 L 2 a\u0303p(l),p (l) )\n\u2264 \u2211 a\u2208A Bm,T \u2211 l\u2208T 2a,p \u221a 1 N\u0303a,p(l)  \u2264 Bm,T\n\u2211 a\u2208A Na,p(t)\u22121\u2211 k=0\n\u221a 1\n1 + k \u2264 2Bm,T \u2211 a\u2208A \u221a Na,p(t)\n\u2264 2Bm,T \u221a |A|Np(t). (23)\nwhere Bm,T = 2 \u221a\n2Am,T . Combining (21), (22) and (23), we obtain\nReg2p(t) \u2264C2max|A| (\n2Am,T \u03b22v2 + 1\n) + 2vNp(t)\n+ 2Bm,T \u221a |A|Np(t)."}, {"heading": "APPENDIX D", "text": "CONCENTRATION INEQUALITY [28], [31]\nConsider an arm a for which the rewards of objective i are generated by a process {Ria(t)}Tt=1 with \u00b5ia = E[Ria(t)], where the noise Ria(t) \u2212 \u00b5ia is conditionally 1-sub-Gaussian. Let Na(T ) \u2265 1 denote the number of times a is selected by the end of time T . Let \u00b5\u0302a(T ) = \u2211T t=1 I(a(t) = a)R i a(t)/Na(T ). For any \u03b4 > 0 with probability at least 1\u2212 \u03b4 we have\n|\u00b5\u0302a(T )\u2212 \u00b5a|\n\u2264\n\u221a 2\nNa(T )\n( 1 + 2 log ( (1 +Na(T ))1/2\n\u03b4\n)) \u2200T \u2208 N."}], "references": [{"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire"], "venue": "Proc. 19th International Conference on World Wide Web, 2010, pp. 661\u2013670.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Personalized course sequence recommendations", "author": ["Jie Xu", "Tianwei Xing", "Mihaela Van Der Schaar"], "venue": "IEEE Trans. Signal Process., vol. 64, no. 20, pp. 5340\u20135352, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Using contextual learning to improve diagnostic accuracy: Application in breast cancer screening", "author": ["Linqi Song", "William Hsu", "Jie Xu", "Mihaela van der Schaar"], "venue": "IEEE J. Biomed. Health Inform., vol. 20, no. 3, pp. 902\u2013914, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiuser channel allocations in cognitive radio networks: A combinatorial multiarmed bandit formulation", "author": ["Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain"], "venue": "Proc. IEEE Symposium on New Frontiers in Dynamic Spectrum, 2010, pp. 1\u20139.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed stochastic online learning policies for opportunistic spectrum access", "author": ["Yi Gai", "Bhaskar Krishnamachari"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 23, pp. 6184\u20136193, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequential learning for multi-channel wireless network monitoring with channel switching costs", "author": ["Thanh Le", "Csaba Szepesvari", "Rong Zheng"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 22, pp. 5919\u20135929, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Designing multi-objective multiarmed bandits algorithms: A study", "author": ["Madalina M Drugan", "Ann Nowe"], "venue": "Proc. International Joint Conference on Neural Networks (IJCNN), 2013, pp. 1\u20138.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Contextual multi-armed bandits", "author": ["Tyler Lu", "D\u00e1vid P\u00e1l", "Martin P\u00e1l"], "venue": "Proc. AISTATS, 2010, pp. 485\u2013492.  13", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Contextual bandits with similarity information", "author": ["Aleksandrs Slivkins"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 2533\u20132568, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Li", "Lev Reyzin", "Robert E Schapire"], "venue": "Proc. AISTATS, 2011, pp. 208\u2013214.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 1, pp. 1\u2013122, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["John Langford", "Tong Zhang"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 20, pp. 1096\u20131103, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert Schapire"], "venue": "Proc. International Conference on Machine Learning (ICML), 2014, pp. 1638\u20131646.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze L Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics, vol. 6, pp. 4\u201322, 1985.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1985}, {"title": "Sample mean based index policies with O(logn) regret for the multi-armed bandit problem", "author": ["Rajeev Agrawal"], "venue": "Advances in Applied Probability, vol. 27, no. 4, pp. 1054\u20131078, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Aur\u00e9lien Garivier", "Olivier Capp\u00e9"], "venue": "Proc. 24th Annual Conference on Learning Theory (COLT), 2011, pp. 359\u2013376.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed online learning in social recommender systems", "author": ["Cem Tekin", "Simpson Zhang", "Mihaela van der Schaar"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 8, no. 4, pp. 638\u2013652, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive ensemble learning with confidence bounds", "author": ["Cem Tekin", "Jinsung Yoon", "Mihaela van der Schaar"], "venue": "IEEE Trans. Signal Process., vol. 65, no. 4, pp. 888\u2013903, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed online learning via cooperative contextual bandits", "author": ["Cem Tekin", "Mihaela van der Schaar"], "venue": "IEEE Trans. Signal Process., vol. 63, no. 14, pp. 3700\u20133714, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "RELEAF: An algorithm for learning and exploiting relevance", "author": ["Cem Tekin", "Mihaela van der Schaar"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 9, no. 4, pp. 716\u2013727, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of kernelised contextual bandits", "author": ["Michal Valko", "Nathan Korda", "R\u00e9mi Munos", "Ilias Flaounas", "Nello Cristianini"], "venue": "Proc. 29th Conference on Uncertainty in Artificial Intelligence (UAI), 2013, pp. 654\u2013663.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "Proc. 27th Conference on Uncertainty in Artificial Intelligence (UAI), 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge gradient for multi-objective multi-armed bandit algorithms", "author": ["Saba Q Yahyaa", "Madalina M Drugan", "Bernard Manderick"], "venue": "Proc. ICAART (1), 2014, pp. 74\u201383.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Thompson sampling for multiobjective multi-armed bandits problem", "author": ["Saba Q Yahyaa", "Bernard Manderick"], "venue": "Proc. European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. 47\u201352.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Annealing-Pareto multi-objective multi-armed bandit algorithm", "author": ["Saba Q Yahyaa", "Madalina M Drugan", "Bernard Manderick"], "venue": "Proc. IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2014, pp. 1\u20138.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalarization based Pareto optimal set of arms identification algorithms", "author": ["Madalina M Drugan", "Ann Now\u00e9"], "venue": "Proc. International Joint Conference on Neural Networks (IJCNN), 2014, pp. 2690\u20132697.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), 2011, pp. 2312\u20132320.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "How to use expert advice", "author": ["Nicolo Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P Helmbold", "Robert E Schapire", "Manfred K Warmuth"], "venue": "Journal of the ACM (JACM), vol. 44, no. 3, pp. 427\u2013485, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to optimize via posterior sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "Mathematics of Operations Research, vol. 39, no. 4, pp. 1221\u20131243, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "Many of these problems, including recommender systems [1], [2], medical screening [3], cognitive radio networks [4], [5] and wireless network monitoring [6] may involve multiple and possibly conflicting objectives.", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "For this, we extend the Pareto regret proposed in [7] to take into account the dependence of the Pareto front on the context.", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "Then, we argue that it is possible to make the Pareto regret of MOC-MAB \u00d5(T ) by adjusting its parameters, such that the Pareto regret becomes order optimal up to a logarithmic factor [8], but this comes at an expense of making the regret in the non-dominant objective of MOC-MAB linear in the number of rounds.", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "RELATED WORK In the past decade, many variants of the classical multiarmed problem have been introduced (see [11] for a comprehensive discussion).", "startOffset": 109, "endOffset": 113}, {"referenceID": 8, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Two notable examples are contextual bandits [9], [12], [13] and multi-objective bandits [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 13, "context": "A through technical analysis of the classical multi-armed bandit problem is given in [14], where it is shown that O(log T ) regret is achieved asymptotically by index policies that use upper confidence bounds (UCBs) for the rewards.", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "Later on, it is shown in [15] that it is possible to achieve O(log T ) regret by using index policies constructed using the sample means of the arm rewards.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "The first finitetime logarithmic regret bound is given in [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "This line of research has been followed by many others, and new algorithms with tighter regret bounds have been proposed [17].", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "The context naturally arises in many practical applications such as social recommender systems [18], medical diagnosis [19] and big data stream mining [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "The context naturally arises in many practical applications such as social recommender systems [18], medical diagnosis [19] and big data stream mining [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The context naturally arises in many practical applications such as social recommender systems [18], medical diagnosis [19] and big data stream mining [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "This problem is considered in [8], and the Query-AdClustering algorithm that achieves O(T 1\u22121/(2+dc)+ ) regret for any > 0 is proposed, where dc is the covering dimension of the similarity space.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "A parallel work [9] proposes the contextual zooming algorithm which partitions the similarity space non-uniformly, according to both sampling frequency and rewards obtained from different regions of the similarity space.", "startOffset": 16, "endOffset": 19}, {"referenceID": 20, "context": "This issue is addressed in [21], where it is assumed that the arm rewards depend on an unknown subset of the contexts, and it is shown that the regret in this case only depends on the number of relevant context dimensions.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "[1] proposed the LinUCB algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], and is shown to achieve \u00d5( \u221a Td) regret, where d is the dimension of the context.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] mixed LinUCB and SupLinUCB with kernel functions and proposed an algorithm whose regret is \u00d5( \u221a T d\u0303) where d\u0303 is the effective dimension of the kernel feature space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] proposed the epoch greedy algorithm with O(T ) regret and later works [13], [23] proposed more efficient learning algorithms with \u00d5(T ) regret.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] proposed the epoch greedy algorithm with O(T ) regret and later works [13], [23] proposed more efficient learning algorithms with \u00d5(T ) regret.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "[12] proposed the epoch greedy algorithm with O(T ) regret and later works [13], [23] proposed more efficient learning algorithms with \u00d5(T ) regret.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 73, "endOffset": 76}, {"referenceID": 9, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "Contextual Zooming [9] \u00d5(T 1\u22121/(2+dz)) No Yes No Yes Query-Ad-Clustering [8] \u00d5(T 1\u22121/(2+dc)) No Yes No Yes SupLinUCB [10] \u00d5( \u221a T ) No Yes Yes No Pareto-UCB1 [7] O(log(T )) Yes No No No Scalarized-UCB1 [7] O(log(T )) Yes No No No MOC-MAB (our work) \u00d5(T (2\u03b1+d)/(3\u03b1+d)) (2D and Pareto regrets) Yes Yes No Yes \u00d5(T (\u03b1+d)/(2\u03b1+d)) (Pareto regret only)", "startOffset": 201, "endOffset": 204}, {"referenceID": 6, "context": "This approach is proposed in [7], and a learning algorithm called Pareto-UCB1 that achieves O(log T ) Pareto regret is proposed.", "startOffset": 29, "endOffset": 32}, {"referenceID": 23, "context": "A modified version of this algorithm where the indices depend on both the estimated mean and the estimated standard deviation is proposed in [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "Numerous other variants are also considered in prior works, including the Pareto Thompson sampling algorithm in [25] and the Annealing Pareto algorithm in [26].", "startOffset": 112, "endOffset": 116}, {"referenceID": 25, "context": "Numerous other variants are also considered in prior works, including the Pareto Thompson sampling algorithm in [25] and the Annealing Pareto algorithm in [26].", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "On the other hand, in the scalarized approach [7], [27], a random weight is assigned to each objective at each round, from which for each arm a weighted sum of the indices of the objectives are calculated.", "startOffset": 46, "endOffset": 49}, {"referenceID": 26, "context": "On the other hand, in the scalarized approach [7], [27], a random weight is assigned to each objective at each round, from which for each arm a weighted sum of the indices of the objectives are calculated.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "For instance, Scalarized UCB1 in [7] achieves O(S\u2032 log(T/S\u2032)) scalarized regret where S\u2032 is the number of scalarization functions used by the algorithm.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "Without loss of generality, we assume that xt lies in the context space X := [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 7, "context": "We assume that the expected rewards are H\u00f6lder continuous in the context, which is a common assumption in the contextual bandit literature [8], [19], [20].", "startOffset": 139, "endOffset": 142}, {"referenceID": 18, "context": "We assume that the expected rewards are H\u00f6lder continuous in the context, which is a common assumption in the contextual bandit literature [8], [19], [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "We assume that the expected rewards are H\u00f6lder continuous in the context, which is a common assumption in the contextual bandit literature [8], [19], [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 27, "context": "1Examples of 1-sub-Gaussian distributions include the Gaussian distribution with zero mean and unit variance, and any distribution defined over an interval of length 2 with zero mean [28].", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "Another interesting performance measure is the Pareto regret [7], which measures the loss of the learner with respect to arms in the Pareto front.", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "See [29] and [19] for a discussion on the doubling-trick.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "See [29] and [19] for a discussion on the doubling-trick.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "To prove the lemma above, we use the concentration inequality given in Lemma 6 in [28] to bound the probability of UCa,p.", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "To overcome this problem, we use the sandwich technique proposed in [19] in order to bound the rewards sampled from actual context arrivals between the rewards sampled from two specific processes that are related to the original process, where each process has a fixed mean value.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "This will make the Pareto regret \u00d5(T ) (which matches with the lower bound in [8] for the single-objective contextual bandit problem with similarity information up to a logaritmic factor) but will also make the regret in the non-dominant objective linear.", "startOffset": 78, "endOffset": 81}, {"referenceID": 29, "context": "Experiment 1 In this experiment, we evaluate the performance of MOCMAB on the breast cancer dataset from the UCI Machine Learning Repository [30].", "startOffset": 141, "endOffset": 145}, {"referenceID": 0, "context": "We take X = [0, 1] and assume that the context at each time 1 2 3 4 5 6 7 8 9 10 Test size 104 0.", "startOffset": 12, "endOffset": 18}, {"referenceID": 6, "context": "We compare MOC-MAB with the following bandit algorithms: Pareto UCB1 (P-UCB1): This is the Empirical Pareto UCB1 algorithm proposed in [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "Scalarized UCB1 (S-UCB1): This is the Scalarized Multiobjective UCB1 algorithm proposed in [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "Contextual Dominant UCB1 (CD-UCB1): This is the contextual version of UCB1 [16], which partitions the context space in the same way as MOC-MAB does, and uses a different instance of UCB1 in each set of the partition.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "For S-UCB1 and CS-UCB1, the weights of the linear scalarization functions are chosen as [1, 0], [0.", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": "5] and [0, 1].", "startOffset": 7, "endOffset": 13}, {"referenceID": 27, "context": "APPENDIX D CONCENTRATION INEQUALITY [28], [31] Consider an arm a for which the rewards of objective i are generated by a process {R a(t)}t=1 with \u03bca = E[R a(t)], where the noise R a(t) \u2212 \u03bca is conditionally 1-sub-Gaussian.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "APPENDIX D CONCENTRATION INEQUALITY [28], [31] Consider an arm a for which the rewards of objective i are generated by a process {R a(t)}t=1 with \u03bca = E[R a(t)], where the noise R a(t) \u2212 \u03bca is conditionally 1-sub-Gaussian.", "startOffset": 42, "endOffset": 46}], "year": 2017, "abstractText": "In this paper, we propose a new multi-objective contextual multi-armed bandit problem with two objectives, where one of the objectives dominates the other objective. Unlike single-objective bandit problems in which the learner obtains a random scalar reward for each arm it selects, in the proposed problem, the learner obtains a random reward vector, where each component of the reward vector corresponds to one of the objectives and the distribution of the reward depends on the context that is provided to the learner at the beginning of each round. We call this problem contextual multi-armed bandit with a dominant objective (CMAB-DO). In CMAB-DO, the goal of the learner is to maximize its total reward in the non-dominant objective while ensuring that it maximizes its total reward in the dominant objective. In this case, the optimal arm given a context is the one that maximizes the expected reward in the non-dominant objective among all arms that maximize the expected reward in the dominant objective. First, we show that the optimal arm lies in the Pareto front. Then, we propose the multi-objective contextual multi-armed bandit algorithm (MOCMAB), and define two performance measures: the 2-dimensional (2D) regret and the Pareto regret. We show that both the 2D regret and the Pareto regret of MOC-MAB are sublinear in the number of rounds. We also compare the performance of the proposed algorithm with other state-of-the-art methods in synthetic and real-world datasets. The proposed model and the algorithm have a wide range of real-world applications that involve multiple and possibly conflicting objectives ranging from wireless communication to medical diagnosis and recommender systems.", "creator": "LaTeX with hyperref package"}}}