{"id": "1708.05729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Neural machine translation for low-resource languages", "abstract": "Neural Machine Translation (NMT) approaches have improved the state of the art in many machine translation settings in recent years, but they require large amounts of training data to achieve meaningful results. We show that NMT can also be used for low-resource languages by introducing more local dependencies and using word alignments to learn sentence orders during translation. In addition to our novel model, we also present an empirical assessment of resource-poor phrase-based Statistical Machine Translation (SMT) and NMT to examine the lower limits of each technology. We note that while SMT remains the best option for low-resource settings, our method can produce acceptable translations with only 70,000 characters of training data, a level at which the basic NMT system completely fails.", "histories": [["v1", "Fri, 18 Aug 2017 18:16:23 GMT  (16kb)", "http://arxiv.org/abs/1708.05729v1", "rejected from EMNLP 2017"]], "COMMENTS": "rejected from EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert \\\"ostling", "j\\\"org tiedemann"], "accepted": false, "id": "1708.05729"}, "pdf": {"name": "1708.05729.pdf", "metadata": {"source": "CRF", "title": "Neural machine translation for low-resource languages", "authors": ["Robert \u00d6stling"], "emails": ["robert@ling.su.se,", "jorg.tiedemann@helsinki.fi"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n05 72\n9v 1\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 7\nproaches have improved the state of the art in many machine translation settings over the last couple of years, but they require large amounts of training data to produce sensible output. We demonstrate that NMT can be used for low-resource languages as well, by introducing more local dependencies and using word alignments to learn sentence reordering during translation. In addition to our novel model, we also present an empirical evaluation of low-resource phrase-based statistical machine translation (SMT) and NMT to investigate the lower limits of the respective technologies. We find that while SMT remains the best option for low-resource settings, our method can produce acceptable translations with only 70 000 tokens of training data, a level where the baseline NMT system fails completely."}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT) has made rapid progress over the last few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016), emerging as a serious alternative to phrasebased statistical machine translation (Koehn et al., 2003). Most of the previous literature perform empirical evaluations using training data in the order of millions to tens of millions of parallel sentence pairs. In contrast, we want to see how low you can push the training data requirement for neural machine translation.1 To the best of our knowledge, there is no previous systematic treatment of\n1The code of our implementation is available at http://www.example.com (redacted for review, code submitted as nmt.tgz in the review system)\nthis question. Zoph et al. (2016) did explore lowresource NMT, but by assuming the existence of large amounts of data from related languages."}, {"heading": "2 Low-resource model", "text": "The current de-facto standard approach to NMT (Bahdanau et al., 2014) has two components: a target-side RNN language model (Mikolov et al., 2010; Sundermeyer et al., 2012), and an encoder plus attention mechanism that is used to condition on the source sentence. While an acceptable language model can be trained using relatively small amounts of data, more data is required to train the attention mechanism and encoder. This has the effect that a standard NMT system trained on too little data essentially becomes an elaborate language model, capable of generating sentences in the target language that have little to do with the source sentences.\nWe approach this problem by reversing the mode of operation of the translation model. Instead of setting loose a target-side language model with a weak coupling to the source sentence, our model steps through the source sentence token by token, generating one (possibly empty) chunk of the target sentence at a time. The generated chunk is then inserted into the partial target sentence into a position predicted by a reordering mechanism. Table 1 demonstrates this procedure. While reordering is a difficult problem, word order errors are preferable to a nonsensical output. We translate each token using a character-to-character model conditioned on the local source context, which is a relatively simple problem since data is not so sparse at the token level. This also results in open vocabularies for the source and target languages.\nOur model consists of the following components, whose relation are also summarized in Al-\ngorithm 1:\n\u2022 Source token encoder. A bidirectional Long Short-Term Memory (LSTM) en-\ncoder (Hochreiter and Schmidhuber, 1997) that takes a source token ws as a sequence of (embedded) characters and produces a fixeddimensional vector SRC-TOK-ENC(ws).\n\u2022 Source sentence encoder. A bidirectional LSTM encoder that takes a sequence of en-\ncoded source tokens and produces an encoded sequence es1...N . Thus we use the same two-level source sentence encoding scheme as Luong and Manning (2016) except that we do not use separate embeddings for common words.\n\u2022 Target token encoder. A bidirectional LSTM encoder that takes a target token2\nwt and produces a fixed-dimensional vector TRG-TOK-ENC(wt).\n\u2022 Target state encoder. An LSTM which at step i produces a target state\nhi = TRG-ENC(e s i \u2016TRG-TOK-ENC(w t i\u22121)) given the i:th encoded source position and the i\u2212 1:th encoded target token.\n\u2022 Target token decoder. A standard characterlevel LSTM language model conditioned on\nthe current target state hi, which produces a target token wti = TRG-TOK-DEC(hi) as a sequence of characters.\n\u2022 Target position predictor. A two-layer feedforward network that outputs (af-\nter softmax) the probability of wti being\n2Note that on the target side, a token may contain the empty string as well as multi-word expressions with spaces. We use the term token to reflect that we seek to approximate a 1-to-1 correspondence between source and target tokens\nAlgorithm 1 Our proposed translation model.\nfunction TRANSLATE(ws1...N )\n\u22b2 Encode each source token for all i \u2208 1 . . . N do esi \u2190 SRC-TOK-ENC(w s i ) end for \u22b2 Encode source token sequence s1...N \u2190 SRC-ENC(e s 1...N ) \u22b2 Translate one source token at a time for all i \u2208 1 . . . N do \u22b2 Encode previous target token\neti \u2190 TRG-TOK-ENC(w t i\u22121) \u22b2 Generate target state vector hi \u2190 TRG-ENC(e t i\u2016si) \u22b2 Generate target token p(wti) \u223c TRG-TOK-DEC(hi) \u22b2 Predict insertion position\np(ki) \u223c POSITION(h1...i, k1...i\u22121) end for \u22b2 Search for a high-probability target \u22b2 sequence and ordering, and return it\nend function\ninserted at position ki = j of the partial target sentence: P (ki = j) \u221d exp(POSITION(hpos(j)\u2016hpos(j+1)\u2016hi)). Here, pos(j) is the position i of the source token that generated the j:th token of the partial target hypothesis. This is akin to the attention model of traditional NMT, but less critical to the translation process because it does not directly influence the generated token."}, {"heading": "3 Training", "text": "Since our intended application is translation of low-resource languages, we rely on word alignments to provide supervision for the reordering model. We use the EFMARAL aligner (O\u0308stling and Tiedemann, 2016), which uses a Bayesian model with priors that generate good results even for rather small corpora. From this, we get estimates of the alignment probabilities Pf (ai = j) and Pb(aj = i) in the forward and backward directions, respectively.\nOur model requires a sequence of source tokens and a sequence of target tokens of the same length. We extract this by first finding the most confident 1-to-1 word alignments, that is, the set of consistent pairs (i, j) with maximum \u220f\n(i,j) Pf (ai =\nj)\u00b7Pb(aj = i). Then we use the source sentence as a fixed point, so that the final training sequence is the same length of the source sentence. Unaligned source tokens are assumed to generate the empty string, and source tokens aligned to a target token followed by unaligned target tokens are assumed to generate the whole sequence (with spaces between tokens). While this prohibits a single source token to generate multiple dislocated target words (e.g., standard negation in French), our intention is that this constraint will result in overall better translations when data is sparse.\nOnce the aligned token sequences have been extracted, we train our model using backpropagation with stochastic gradient descent. For this we use the Chainer library (Tokui et al., 2015), with Adam (Kingma and Ba, 2015) for optimization. We use early stopping based on cross-entropy from held-out sentences in the training data. For the smaller Watchtower data, we stopped after about 10 epochs, while about 25\u201340 were required for the 20% Bible data. We use a dimensionality of 256 for all layers except the character embeddings (which are of size 64)."}, {"heading": "4 Baseline systems", "text": "In addition to our proposed model, we two public translation systems: Moses (Koehn et al., 2007) for phrase-based statistical machine translation (SMT), and HNMT3 for neural machine translation (NMT). For comparability, we use the same word alignment method (O\u0308stling and Tiedemann, 2016) withMoses as with our proposed model (see Section 3).\nFor the SMT system, we use 5-gram modified Kneser-Ney language models estimated with KenLM (Heafield et al., 2013). We symmetrize the word alignments using the GROW-DIAG-FINAL heuristic. Otherwise, standard settings are used for phrase extraction and estimation of translation probabilities and lexical weights. Parameters are tuned using MERT (Och, 2003) with 200-best lists.\nThe baseline NMT system, HNMT, uses standard attention-based translation with the hybrid source encoder architecture of Luong and Manning (2016) and a characterbased decoder. We run it with parameters comparable to those of our proposed model: 256-dimensional word embeddings and encoder\n3 https://github.com/robertostling/hnmt\nLSTM, 64-dimensional character embeddings, and an attention hidden layer size of 256. We used a decoder LSTM size of 512 to account for the fact that this model generates whole sentences, as opposed to our model which generates only small chunks."}, {"heading": "5 Data", "text": "The most widely translated publicly available parallel text is the Bible, which has been used previously for multilingual NLP (e.g. Yarowsky et al., 2001; Agic\u0301 et al., 2015). In addition to this, the Watchtower magazine is also publicly available and translated into a large number of languages. Although generally containing less text than the Bible, Agic\u0301 et al. (2016) showed that its more modern style and consistent translations can outweigh this disadvantage for out-of-domain tasks. The Bible and Watchtower texts are quite similar, so we also evaluate on data from the WMT shared tasks from the news domain (newstest2016 for Czech and German, newstest2008 for French and Spanish). These are four languages that occur in all three data sets, and we use them with English as the target language in all experiments.\nThe Watchtower texts are the shortest, after removing 1000 random sentences each for development and test, we have 62\u201371 thousand tokens in each language for training. For the Bible, we used every 5th sentence in order to get a subset similar in size to the New Testament.4 After removing 1000 sentences for development and test, this yielded 130\u2013175 thousand tokens per language for training."}, {"heading": "6 Results", "text": "Table 3 summarizes the results of our evaluation, and Table 2 shows some example translations. For evaluation we use the BLEU metric (Papineni et al., 2002). To summarize, it is clear that SMT remains the best method for low-resource machine translation, but that current methods are not able to produce acceptable general machine translation systems given the parallel data available for low-resource languages.\nOur model manages to reduce the gap between phrase-based and neural machine translation, with\n4The reason we did not simply use the New Testament is because it consists largely of four redundant gospels, which makes it difficult to use for machine translation evaluation.\nBLEU scores of 9\u201317% (in-domain) using only about 70 000 tokens of training data, a condition where the traditional NMT system is unable to produce any sensible output at all. It should be noted that due to time constraints, we performed inference with greedy search for our model, whereas the NMT baseline used a beam search with a beam size of 10."}, {"heading": "7 Discussion", "text": "We have demonstrated a possible road towards better neural machine translation for low-resource languages, where we can assume no data beyond a small parallel text. In our evaluation, we see that it outperforms a standard NMT baseline, but is not currently better than the SMT system. In the future, we hope to use the insights gained from this work to further explore the possibility of constraining NMT models to perform better under severe data sparsity. In particular, we would like to explore models that preserve more of the fluency characteristic of NMT, while ensuring that adequacy does not suffer too much when data is sparse."}], "references": [{"title": "If all you have is a bit of the bible: Learning pos taggers for truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Dirk Hovy", "Anders S\u00f8gaard."], "venue": "Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Agi\u0107 et al\\.,? 2015", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2015}, {"title": "Multilingual projection for parsing truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "H\u00e9ctor Martnez Alonso", "Natalie Schluter", "Anders S\u00f8gaard."], "venue": "Transactions of the Association for Computational Linguistics 4:301\u2013", "citeRegEx": "Agi\u0107 et al\\.,? 2016", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scalable modified Kneser-Ney languagemodel estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of ACL. pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Vol-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010. pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of ACL. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Efficient word alignment with Markov Chain Monte Carlo", "author": ["Robert \u00d6stling", "J\u00f6rg Tiedemann."], "venue": "Prague Bulletin of Mathematical Linguistics 106:125\u2013146.", "citeRegEx": "\u00d6stling and Tiedemann.,? 2016", "shortCiteRegEx": "\u00d6stling and Tiedemann.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "INTERSPEECH 2012. pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton."], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Deniz Yuret", "Jonathan May", "Kevin Knight"], "venue": "Stroudsburg, PA, USA,", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation (NMT) has made rapid progress over the last few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016),", "startOffset": 81, "endOffset": 145}, {"referenceID": 2, "context": "Neural machine translation (NMT) has made rapid progress over the last few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016),", "startOffset": 81, "endOffset": 145}, {"referenceID": 5, "context": "emerging as a serious alternative to phrasebased statistical machine translation (Koehn et al., 2003).", "startOffset": 81, "endOffset": 101}, {"referenceID": 14, "context": "Zoph et al. (2016) did explore low-", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "The current de-facto standard approach to NMT (Bahdanau et al., 2014) has two components: a target-side RNN language model (Mikolov et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 7, "context": ", 2014) has two components: a target-side RNN language model (Mikolov et al., 2010; Sundermeyer et al., 2012), and an encoder plus attention mechanism that is used to condition on the source sentence.", "startOffset": 61, "endOffset": 109}, {"referenceID": 11, "context": ", 2014) has two components: a target-side RNN language model (Mikolov et al., 2010; Sundermeyer et al., 2012), and an encoder plus attention mechanism that is used to condition on the source sentence.", "startOffset": 61, "endOffset": 109}, {"referenceID": 6, "context": "Thus we use the same two-level source sentence encoding scheme as Luong and Manning (2016) except that we do not use separate embeddings for common words.", "startOffset": 66, "endOffset": 91}, {"referenceID": 9, "context": "We use the EFMARAL aligner (\u00d6stling and Tiedemann, 2016), which uses a", "startOffset": 27, "endOffset": 56}, {"referenceID": 13, "context": "For this we use the Chainer library (Tokui et al., 2015), with Adam (Kingma and Ba, 2015) for optimization.", "startOffset": 36, "endOffset": 56}, {"referenceID": 4, "context": ", 2015), with Adam (Kingma and Ba, 2015) for optimization.", "startOffset": 19, "endOffset": 40}, {"referenceID": 9, "context": "For comparability, we use the same word alignment method (\u00d6stling and Tiedemann, 2016) withMoses as with our proposed model (see Section 3).", "startOffset": 57, "endOffset": 86}, {"referenceID": 3, "context": "For the SMT system, we use 5-gram modified Kneser-Ney language models estimated with KenLM (Heafield et al., 2013).", "startOffset": 91, "endOffset": 114}, {"referenceID": 8, "context": "Parameters are tuned using MERT (Och, 2003) with 200-best lists.", "startOffset": 32, "endOffset": 43}, {"referenceID": 6, "context": "The baseline NMT system, HNMT, uses standard attention-based translation with the hybrid source encoder architecture of Luong and Manning (2016) and a characterbased decoder.", "startOffset": 120, "endOffset": 145}, {"referenceID": 0, "context": "allel text is the Bible, which has been used previously for multilingual NLP (e.g. Yarowsky et al., 2001; Agi\u0107 et al., 2015).", "startOffset": 77, "endOffset": 124}, {"referenceID": 0, "context": "Although generally containing less text than the Bible, Agi\u0107 et al. (2016) showed that its more modern style and consistent translations can outweigh this disadvantage for out-of-domain tasks.", "startOffset": 56, "endOffset": 75}, {"referenceID": 10, "context": "For evaluation we use the BLEU metric (Papineni et al., 2002).", "startOffset": 38, "endOffset": 61}], "year": 2017, "abstractText": "Neural machine translation (NMT) approaches have improved the state of the art in many machine translation settings over the last couple of years, but they require large amounts of training data to produce sensible output. We demonstrate that NMT can be used for low-resource languages as well, by introducing more local dependencies and using word alignments to learn sentence reordering during translation. In addition to our novel model, we also present an empirical evaluation of low-resource phrase-based statistical machine translation (SMT) and NMT to investigate the lower limits of the respective technologies. We find that while SMT remains the best option for low-resource settings, our method can produce acceptable translations with only 70 000 tokens of training data, a level where the baseline NMT system fails completely.", "creator": "LaTeX with hyperref package"}}}