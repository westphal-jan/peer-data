{"id": "1708.06510", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Handling Homographs in Neural Machine Translation", "abstract": "Homographies, words with different meanings but the same interface shape, have long been a source of difficulty for machine translation systems, as it is difficult to select the correct translation from context. However, with the advent of neural machine translation systems (NMT), which can theoretically take into account the global sentence context, it can be assumed that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems do in fact still have significant problems with the correct translation of ambiguous words. Then, inspired by the literature on the meaning of disambiguity, we describe methods that model the context of the input word with context-aware word embeddings that help to differentiate the meaning of the word before it is fed into the coder. Experiments on three language pairs show that such models improve the performance of NMT systems both in terms of BLEU score and in terms of accuracy of homographic translation.", "histories": [["v1", "Tue, 22 Aug 2017 06:48:27 GMT  (1031kb,D)", "http://arxiv.org/abs/1708.06510v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["frederick liu", "han lu", "graham neubig"], "accepted": false, "id": "1708.06510"}, "pdf": {"name": "1708.06510.pdf", "metadata": {"source": "CRF", "title": "Handling Homographs in Neural Machine Translation", "authors": ["Frederick Liu", "Graham Neubig"], "emails": ["fliu1@cs.cmu.edu", "hlu2@cs.cmu.edu", "gneubig@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT; Sutskever et al. (2014); Bahdanau et al. (2015), \u00a72), a method for MT that performs translation in an end-toend fashion using neural networks, is quickly becoming the de-facto standard in MT applications due to its impressive empirical results. One of the drivers behind these results is the ability of NMT to capture long-distance context using recurrent neural networks in both the encoder, which takes the input and turns it into a continuous-space representation, and the decoder, which tracks the\ntarget-sentence state, deciding which word to output next. As a result of this ability to capture long-distance dependencies, NMT has achieved great improvements in a number of areas that have bedeviled traditional methods such as phrasebased MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016).\nOne other phenomenon that was poorly handled by PBMT was homographs \u2013 words that have the same surface form but multiple senses. As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b) or phrasesense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena. Thus, we may wonder: do NMT systems suffer from the same problems when translating homographs? Or are the recurrent nets applied in the encoding step, and the strong language model in the decoding step enough to alleviate all problems of word sense ambiguity?\nIn \u00a73 we first attempt to answer this question\nar X\niv :1\n70 8.\n06 51\n0v 1\n[ cs\n.C L\n] 2\n2 A\nug 2\n01 7\nquantitatively by examining the word translation accuracy of a baseline NMT system as a function of the number of senses that each word has. Results demonstrate that standard NMT systems make a significant number of errors on homographs, a few of which are shown in Fig. 1.\nWith this result in hand, we propose a method for more directly capturing contextual information that may help disambiguate difficult-to-translate homographs. Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; Ka\u030ageba\u0308ck and Salomonsson, 2016; Yuan et al., 2016; S\u030custer et al., 2016), examining three methods inspired by these literatures (\u00a74). In order to incorporate this information into NMT, we examine two methods: gating the word-embeddings in the model (similarly to Choi et al. (2017)), and concatenating the context-aware representation to the word embedding (\u00a75).\nTo evaluate the effectiveness of our method, we compare our context-aware models with a strong baseline (Luong et al., 2015) on the EnglishGerman, English-French, and English-Chinese WMT dataset. We show that our proposed model outperforms the baseline in the overall BLEU score across three different language pairs. Quantitative analysis demonstrates that our model performs better on translating homographs. Lastly, we show sample translations of the baseline system and our proposed model."}, {"heading": "2 Neural Machine Translation", "text": "We follow the global-general-attention NMT architecture with input-feeding proposed by Luong et al. (2015), which we will briefly summarize here. The neural network models the conditional distribution over translations Y = (y1, y2, . . . , ym) given a sentence in source language X = (x1, x2, . . . xn) as P (Y |X). A NMT system consists of an encoder that summarizes the source sentence X as a vector representation h, and a decoder that generates a target word at each time step condition on both h and previous words. The conditional distribution is optimized with cross-entropy loss at each decoder output.\nThe encoder is usually a uni-directional or bidirectional RNN that reads the input sentence word by word. In the more standard bi-directional case, before being read by the RNN unit, each word in X is mapped to a embedding in contin-\nuous vector space by a function fe.\nfe(xt) = Me > \u00b7 1(xt) (1)\nMe \u2208 R|Vs|\u00d7d is a matrix that maps a one-hot representation of xt, 1(xt) to a d-dimensional vector space, and Vs is the source vocabulary. We call the word embedding computed this way Lookup embedding. The word embeddings are then read by a bi-directional RNN\n\u2212\u2192 h t = \u2212\u2212\u2192 RNNe( \u2212\u2192 h t\u22121, fe(xt)) (2) \u2190\u2212 h t = \u2190\u2212\u2212 RNNe( \u2190\u2212 h t+1, fe(xt)) (3)\nAfter being read by both RNNs we can compute the actual hidden state at step t, ht = [ \u2212\u2192 h t; \u2190\u2212 h t], and the encoder summarized representation h = hn. The recurrent units \u2212\u2212\u2192 RNNe and \u2190\u2212\u2212 RNNe are usually either LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al., 2014).\nThe decoder is a uni-directional RNN that decodes tth target word conditioned on (1) previous decoder hidden state gt\u22121, (2) previous word yt\u22121 , and (3) the weighted sum of encoder hidden states at. The decoder maintains the tth hidden state gt as follows,\ngt = \u2212\u2212\u2192 RNNd(gt\u22121, fd(yt\u22121),at) (4)\nAgain, \u2212\u2212\u2192 RNNd is either LSTM or GRU, and fd is a mapping function in target language space. The general attention mechanism for computing the weighted encoder hidden states at first computes the similarity between gt\u22121 and ht\u2032 for t\u2032 = 1, 2, . . . , n.\nscore(gt\u22121,ht\u2032) = gt\u22121W atth > t\u2032 (5)\nThe similarities are then normalized through a softmax layer , which results in the weights for encoder hidden states.\n\u03b1t,t\u2032 = score(gt\u22121,ht\u2032)\u2211n k=1 score(gt\u22121,hk)\n(6)\nWe can then compute at as follows,\nat = n\u2211 k=1 \u03b1t,khk (7)\nFinally, we compute the distribution over yt as,\ng\u0302t = tanh(W 1[gt;at]) (8)\np(yt|y<t, X) = softmax(W 2g\u0302t) (9)"}, {"heading": "3 NMT\u2019s Problems with Homographs", "text": "As described in Eqs. (2) and (3), NMT models encode the target words using recurrent encoders, theoretically endowing them with the ability to handle homographs through global sentential context. However, despite the fact that they have this ability, our qualitative observation of NMT results revealed a significant number of ambiguous words being translated incorrectly, casting doubt on whether the standard NMT setup is able to appropriately learn parameters that disambiguate these word choices.\nTo demonstrate this more concretely, in Fig. 2 we show the translation accuracy of an NMT system with respect to words of varying levels of ambiguity. Specifically, we use the best baseline NMT system to translate three different language pairs from WMT test set (detailed in \u00a76) and plot the F1-score of word translations by the number of senses that they have. The number of senses for a word is acquired from the Cambridge English dictionary,1 after excluding stop words.2\nWe evaluate the translation performance of words in the source side by aligning them to the target side using fast-align (Dyer et al., 2013). The aligner outputs a set of target words to which the source words aligns for both the reference translation and the model translations. F1 score is calculated between the two sets of words.\nAfter acquiring the F1 score for each word, we bucket the F1 scores by the number of senses, and plot the average score of four consecutive buckets as shown in Fig. 2. As we can see from the re-\n1http://dictionary.cambridge.org/us/ dictionary/english/\n2We use the stop word list from NLTK (Bird et al., 2009).\nsults, the F1 score for words decreases as the number of senses increases for three different language pairs. This demonstrates that the translation performance of current NMT systems on words with more senses is significantly decreased from that for words with fewer senses. From this result, it is evident that modern NMT architectures are not enough to resolve the problem of homographs on their own."}, {"heading": "4 Neural Word Sense Disambiguation", "text": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words. Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (Ka\u030ageba\u0308ck and Salomonsson, 2016; Yuan et al., 2016). In this section, we will summarize three methods for WSD which we will further utilize as three different context networks to improve NMT.\nNeural bag-of-words (NBOW) Kalchbrenner et al. (2014); Iyyer et al. (2015) have shown success by representing full sentences with a context vector, which is the average of the Lookup embeddings of the input sequence\nct = 1\nn n\u2211 k=1 M>c 1(xk) (10)\nThis is a simple way to model sentences, but has the potential to capture the global topic of the sentence in a straightforward and coherent way. However, in this case, the context vector would be the same for every word in the input sequence.\nBi-directional LSTM (BiLSTM) Ka\u030ageba\u0308ck and Salomonsson (2016) leveraged a bidirectional LSTM that learns a context vector for the target word in the input sequence and predict the word sense with a multi-layer perceptron. Specifically, we can compute the context vector ct for tth word similarly to bi-directional encoder as follows,\n\u2212\u2192c t = \u2212\u2212\u2192 RNNc(\u2212\u2192c t\u22121, fc(xt)) (11)\n\u2190\u2212c t = \u2190\u2212\u2212 RNNc(\u2190\u2212c t+1, fc(xt)) (12)\nct = [ \u2212\u2192c t;\u2190\u2212c t] (13)\n\u2212\u2212\u2192 RNNc, \u2190\u2212\u2212 RNNc are forward and backward LSTMs repectively, and fc(xt) = M>c 1(xt) is a function that maps a word to continous embedding space.\nHeld-out LSTM (HoLSTM) Yuan et al. (2016) trained a LSTM language model, which predicts a held-out word given the surrounding context, with a large amount of unlabeled text as training data. Given the context vector from this language model, they predict the word sense with a WSD classifier. Specifically, we can compute the context vector ct for tth word by first replacing tth word with a special symbol (e.g. <$>). We then feed the replaced sequence to a uni-directional LSTM:\nc\u0303i = \u2212\u2212\u2192 RNNc(c\u0303i\u22121, fc(xi)) (14)\nFinally, we can get context vector for the tth word\nct = c\u0303n (15)\n\u2212\u2212\u2192 RNNc and fc are defined in BiLSTM paragraph, and n is the length of the sequence. Despite the fact that the context vector is always the last hidden state of the LSTM no matter which word we are targeting, the input sequence read by the HoLSTM is actually different every time."}, {"heading": "5 Adding Context to NMT", "text": "Now that we have several methods to incorporate global context regarding a single word, it is necessary to incorporate this context with NMT. Specifically, we propose two methods to either Gate or Concatenate a context vector ct with the Lookup embedding M>e \u00b7 1(xt) to form a context-aware word embedding before feeding it into the encoder as shown in Fig. 3. The detail of these methods is described below.\nGate Inspired by Choi et al. (2017), as our first method for integration of context-aware word embeddings, we use a gating function as follows:\nf \u2032e(xt) = fe(xt) \u03c3(ct) (16) = M>e 1(xt) \u03c3(ct) (17)\nThe symbol represents element-wise multiplication, and \u03c3 is element-wise sigmoid function. Choi et al. (2017) use this method in concert with averaged embeddings from words in source language like the NBOW model above, which naturally uses the same context vectors for all time steps. In this paper, we additionally test this function with context vectors calculated using the BiLSTM and HoLSTM .\nConcatenate We also propose another way for incorporating context: by concatenating the context vector with the word embeddings. This is expressed as below:\nf \u2032e(xt) = W 3[fe(xt); ct] (18)\n= W 3[M > e 1(xt); ct] (19)\nW 3 is used to project the concatenated vector back to the original d-dimensional space.\nFor each method can compute context vector ct with either the NBOW, BiLSTM, or HoLSTM described in \u00a74. We share the parameters in fe with fc (i.e. M e = M c) since the vocabulary space is the same for context network and encoder. As a result, our context network only slightly increases the number of model parameters. Details about the\nnumber of parameters of each model we use in the experiments are shown in Table 1."}, {"heading": "6 Experiments", "text": "We evaluate our model on three different language pairs: English-French (WMT\u201914), and EnglishGerman (WMT\u201915), English-Chinese (WMT\u201917) with English as the source side. For German and French, we use a combination of Europarl v7, Common Crawl, and News Commentary as training set. For development set, newstest2013 is used for German and newstest2012 is used for French. For Chinese, we use a combination of News Commentary v12 and the CWMT Corpus as the training set and held out 2357 sentences as the development set. Translation performances are reported in case-sensitive BLEU on newstest2014 (2737 sentences), newstest2015 (2169 sentences) for German, newstest2013 (3000 sentences), newstest2014 (3003 sentences) for French, and newsdev2017 (2002 sentences) for Chinese.3 Details about tokenization are as follows. For German, we use the tokenized dataset from Luong et al. (2015); for French, we used the moses (Koehn et al., 2007) tokenization script with the \u201c-a\u201d flag; for Chinese, we split sequences of Chinese characters, but keeps sequences of non-Chinese characters as they are, using the script from IWSLT Evaluation 2015.4\nWe compare our context-aware NMT systems with strong baseline models on each dataset."}, {"heading": "6.1 Training Details", "text": "We limit our vocabularies to be the top 50K most frequent words for both source and target language. Words not in these shortlisted vocabularies are converted into an \u3008unk\u3009 token.\nWhen training our NMT systems, following Bahdanau et al. (2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle minibatches as we proceed. We train our model with the following settings. (1) We start with a learning rate of 1 and we begin to halve the learning rate every epoch once it overfits. (2) We train until the model converges. (i.e. the difference between the perplexity for the current epoch and the previous epoch is less than 0.01) (3) We batched the instances with the same length and our maximum\n3We use the development set as testing data because the official test set hasn\u2019t been released.\n4https://sites.google.com/site/ iwsltevaluation2015/mt-track\nmini-batch size is 256, and (4) the normalized gradient is rescaled whenever its norm exceeds 5. (6) Dropout is applied between vertical RNN stacks with probability 0.3. Additionally, the context network is trained jointly with the encoder-decoder architecture. Our model is built upon OpenNMT (Klein et al., 2017) with the default settings unless otherwise noted."}, {"heading": "6.2 Experimental Results", "text": "In this section, we compare our proposed contextaware NMT models with baseline models on English-German dataset. Our baseline models are encoder-decoder models using global-general attention and input feeding on the decoder side as described in \u00a72, varying the settings on the encoder side. Our proposed model builds upon baseline models by concatenating or gating different types of context vectors. We use LSTM for encoder, decoder, and context network. The decoder is the same across baseline models and proposed models, having 500 hidden units. During testing, we use beam search with a beam size of 5. The dimension for input word embedding d is set 500 across encoder, decoder, and context network. Settings for three different baselines are listed below.\nBaseline 1: An uni-directional LSTM with 500 hidden units and 2 layers of stacking LSTM.\nBaseline 2: A bi-directional LSTM with 250 hidden units and 2 layers of stacking LSTM. Each state is summarized by concatenating the hidden states of forward and backward encoder into 500 hidden units.\nBaseline 3: A bi-directional LSTM with 250 hidden units and 3 layers of stacking LSTM. This can be compared with the proposed method, which adds an extra layer of computation before the word embeddings, essentially adding an extra layer.\nThe context network uses the below settings.\nNBOW: Average word embedding of the input sequence.\nBiLSTM: A single-layer bi-directional LSTM with 250 hidden units. The context vector is represented by concatenating the hidden states of forward and backward LSTM into a 500 dimensional vector.\nHoLSTM: A single-layer uni-directional LSTM with 500 hidden units.\nThe results are shown in Table 1. The first thing we observe is that for all settings except NBOW+Concat, the proposed method is able to improve over the respective baseline methods with 2 layers. Comparing the best baseline model with the best context-aware model (results in bold in the table), we can see that we achieved improvements of around 0.7 BLEU on both WMT14 and WMT15. This is in contrast to simply using a 3-layer network, which actually degrades performance, perhaps due to the vanishing gradients problem it increases the difficulty in learning.\nNext, comparing different methods for incor-\nporating context, we can see that BiLSTM performs best across all settings. HoLSTM performs slightly better than NBOW, and NBOW obviously suffers from having the same context vector for every word in the input sequence failing to outperform the corresponding baselines. Comparing the two integration methods that incorporate context into word embeddings. Both methods improve over the baseline with BiLSTM as the context network. Concatenating the context vector and the word embedding performed better than gating. Finally, in contrast to the baseline, it is not obvious whether using uni-directional or bi-directional as the encoder is better for our proposed models, particularly when BiLSTM is used for calculating the context network. This is likely due to the fact that bi-directional information is already captured by the context network, and may not be necessary in the encoder itself.\nWe further compared the two systems on two different languages, French and Chinese. We achieved 0.5-0.8 BLEU improvement, showing our proposed models are stable and consistent across different language pairs. The results are shown in Table 2.\nTo show that our 3-layer models are properly trained, we ran a 3-layer bidirectional encoder with residual networks on En-Fr and got 27.45 for WMT13 and 30.60 for WMT14, which is similarly lower than the two layer result. It should be noted that previous work such as Britz et al. (2017) have\nalso noted that the gains for encoders beyond two layers is minimal."}, {"heading": "6.3 Targeted Analysis", "text": "In order to examine whether our proposed model can better translate words with multiple senses, we evaluate our context-aware model on a list of homographs extracted from Wikipedia5 compared to the baseline model on three different language pairs. For the baseline model, we choose the bestperforming model, as described in \u00a76.2.\nTo do so, we first acquire the translation of homographs in the source language using fast-align (Dyer et al., 2013). We run fast-align on all the parallel corpora including training data and testing data6 because the unsupervised nature of the algorithm requires it to have a large amount of training data to obtain accurate alignments. Since there might be multiple aligned words in the target language given a word in source language, we use F1, precision, and recall as our metrics, and take the micro-average across all the sentence pairs. 7 We calculated the scores for the 50000 words/characters from our source vocabulary using only English words. The results are shown in Table 3. The table shows two interesting results: (1) The score for the homographs is lower than the score obtained from all the words in the vocabulary. This shows that words with more meanings are harder to translate with Chinese as the only exception.8 (2) The improvement of our proposed model over baseline model is larger on the homographs compared to all the words in vocabulary. This shows that although\n5 https://en.wikipedia.org/wiki/List_ of_English_homographs\n6Reference translation, and all the system generated translations.\n7The link to the evaluation script \u2013 https://goo.gl/oHYR8E\n8One potential explanation for Chinese is that because the Chinese results are generated on the character level, the automatic alignment process was less accurate.\nour context-aware model is better overall, the improvements are particularly focused on words with multiple senses, which matches the intuition behind the design of the model."}, {"heading": "6.4 Qualitative Analysis", "text": "We show sample translations on English-Chinese WMT\u201917 dataset in Table 4 with three kinds of examples. We highlighted the English homograph in bold, correctly translated words in blue, and wrongly translated words in red. (1) Target homographs are translated into the correct sense with the help of context network. For the first sample translation, \u201cmeets\u201d is correctly translated to \u201c\u201d by our model, and wrongly translated to \u201c\u7b26\u5408\u201d by baseline model. In fact, \u201c\u201d is closer to the definition \u201ccome together intentionally\u201d and \u201c\u7b26\u5408\u201d is closer to \u201dsatisfy\u201d in the English dictionary. (2) Target homographs are translated into different but similar senses for both models in the forth example. Both models translate the word \u201cbelieved\u201d to common translations \u201c\u88ab \u201d or \u201c\u76f8\u4fe1\u201d, but these meaning are both close to reference translation \u201c\u636e\u4fe1\u201d. (3) Target homograph is translated into the wrong sense for the baseline model, but is not translated in our model in the fifth example."}, {"heading": "7 Related Work", "text": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015). Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (Ka\u030ageba\u0308ck and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word embeddings that capture context (Ka\u030ageba\u0308ck and Sa-\nlomonsson, 2016; Yuan et al., 2016). S\u030custer et al. (2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD.\nIn contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation. Following this reformulation, Chan et al. (2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems. Xiong and Zhang (2014) breaks the process into two stages. First predicts the sense of the ambiguous source word. The predicted word senses together with other context features are then used to predict possible target translation.\nWithin the framework of Neural MT, there is one work that has similar motivation to ours. Choi et al. (2017) leverage the NBOW as context and gate the word-embedding on both encoder and decoder side. However, their work does not distinguish context vectors for words in the same sequence, in contrast to the method in this paper,\nand our results demonstrate that this is an important feature of methods that handle homographs in NMT. In addition, our quantitative analysis of the problems that homographs pose to NMT and evaluation of how context-aware models fix them was not covered in this previous work."}, {"heading": "8 Conclusion", "text": "Theoretically, NMT systems should be able to handle homographs if the encoder captures the clues to translate them correctly. In this paper, we empirically show that this may not be the case; the performance of word level translation degrades as the number of senses for each word increases. We hypothesize that this is due to the fact that each word is mapped to a word vector despite them being in different contexts, and propose to integrate methods from neural WSD systems into an NMT system to alleviate this problem. We concatenated the context vector computed from the context network with the word embedding to form a contextaware word embedding, successfully improving the NMT system. We evaluated our model on\nthree different language pairs and outperformed a strong baseline model according to BLEU score in all of them. We further evaluated our results targeting the translation of homographs, and our model performed better in terms of F1 score.\nWhile the architectures proposed in this work do not solve the problem of homographs, our empirical results in Table 3 demonstrate that they do yield improvements (larger than those on other varieties of words). We hope that this paper will spark discussion on the topic, and future work will propose even more focused architectures."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["References Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "EMNLP. pages 257\u2013267.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "\u201d O\u2019Reilly Media, Inc.\u201d.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Massive exploration of neural machine translation architectures", "author": ["Denny Britz", "Anna Goldie", "Thang Luong", "Quoc Le."], "venue": "arXiv:1703.03906 .", "citeRegEx": "Britz et al\\.,? 2017", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "A unified multilingual semantic representation of concepts", "author": ["Jos\u00e9 Camacho-Collados", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "ACL. pages 741\u2013751.", "citeRegEx": "Camacho.Collados et al\\.,? 2015", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation", "author": ["Marine Carpuat", "Dekai Wu."], "venue": "TMI pages 43\u201352.", "citeRegEx": "Carpuat and Wu.,? 2007a", "shortCiteRegEx": "Carpuat and Wu.", "year": 2007}, {"title": "Improving statistical machine translation using word sense disambiguation", "author": ["Marine Carpuat", "Dekai Wu."], "venue": "EMNLP-CoNLL. pages 61\u201372.", "citeRegEx": "Carpuat and Wu.,? 2007b", "shortCiteRegEx": "Carpuat and Wu.", "year": 2007}, {"title": "Word sense disambiguation improves statistical machine translation", "author": ["Yee Seng Chan", "Hwee Tou Ng", "David Chiang."], "venue": "ACL. volume 45, pages 33\u201340.", "citeRegEx": "Chan et al\\.,? 2007", "shortCiteRegEx": "Chan et al\\.", "year": 2007}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP. pages 1025\u20131035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Context-dependent word representation for neural machine translation", "author": ["Heeyoul Choi", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1607.00578 .", "citeRegEx": "Choi et al\\.,? 2017", "shortCiteRegEx": "Choi et al\\.", "year": 2017}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Clustering and diversifying web search results with graphbased word sense induction", "author": ["Antonio Di Marco", "Roberto Navigli."], "venue": "Computational Linguistics 39(3):709\u2013754.", "citeRegEx": "Marco and Navigli.,? 2013", "shortCiteRegEx": "Marco and Navigli.", "year": 2013}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A Smith."], "venue": "NAACL-HLT, pages 644\u2013648.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daum\u00e9 III."], "venue": "ACL. pages 1681\u20131691.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Word sense disambiguation using a bidirectional lstm", "author": ["Mikael K\u00e5geb\u00e4ck", "Hans Salomonsson."], "venue": "COLING 2016 page 51.", "citeRegEx": "K\u00e5geb\u00e4ck and Salomonsson.,? 2016", "shortCiteRegEx": "K\u00e5geb\u00e4ck and Salomonsson.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "ACL pages 212\u2013217.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Learning to represent words in context with multilingual supervision", "author": ["Kazuya Kawakami", "Chris Dyer."], "venue": "ICLR workshop .", "citeRegEx": "Kawakami and Dyer.,? 2016", "shortCiteRegEx": "Kawakami and Dyer.", "year": 2016}, {"title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "author": ["G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush."], "venue": "arXiv:1701.02810 .", "citeRegEx": "Klein et al\\.,? 2017", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "EMNLP. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "NAACL. pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Senselearner: Minimally supervised word sense disambiguation for all words in open text", "author": ["Rada Mihalcea", "Ehsanul Faruque."], "venue": "ACL/SIGLEX. volume 3, pages 155\u2013158.", "citeRegEx": "Mihalcea and Faruque.,? 2004", "shortCiteRegEx": "Mihalcea and Faruque.", "year": 2004}, {"title": "Word sense disambiguation: A survey", "author": ["Roberto Navigli."], "venue": "ACM Computing Surveys (CSUR) 41(2):10.", "citeRegEx": "Navigli.,? 2009", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "Neural reranking improves subjective quality of machine translation: Naist at wat2015", "author": ["Graham Neubig", "Makoto Morishita", "Satoshi Nakamura."], "venue": "WAT .", "citeRegEx": "Neubig et al\\.,? 2015", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach", "author": ["Hwee Tou Ng", "Hian Beng Lee."], "venue": "ACL. pages 40\u201347.", "citeRegEx": "Ng and Lee.,? 1996", "shortCiteRegEx": "Ng and Lee.", "year": 1996}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders", "author": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord."], "venue": "NAACL-HLT pages 1346\u2013 1356.", "citeRegEx": "\u0160uster et al\\.,? 2016", "shortCiteRegEx": "\u0160uster et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word-sense disambiguation for machine translation", "author": ["David Vickrey", "Luke Biewald", "Marc Teyssier", "Daphne Koller."], "venue": "HLT-EMNLP. pages 771\u2013778.", "citeRegEx": "Vickrey et al\\.,? 2005", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}, {"title": "A sense-based translation model for statistical machine translation", "author": ["Deyi Xiong", "Min Zhang."], "venue": "ACL. pages 1459\u20131469.", "citeRegEx": "Xiong and Zhang.,? 2014", "shortCiteRegEx": "Xiong and Zhang.", "year": 2014}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky."], "venue": "ACL. pages 189\u2013196.", "citeRegEx": "Yarowsky.,? 1995", "shortCiteRegEx": "Yarowsky.", "year": 1995}, {"title": "Semi-supervised word sense disambiguation with neural models", "author": ["Dayu Yuan", "Julian Richardson", "Ryan Doherty", "Colin Evans", "Eric Altendorf."], "venue": "arXiv:1603.07012 .", "citeRegEx": "Yuan et al\\.,? 2016", "shortCiteRegEx": "Yuan et al\\.", "year": 2016}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng."], "venue": "ACL. pages 78\u201383.", "citeRegEx": "Zhong and Ng.,? 2010", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}], "referenceMentions": [{"referenceID": 27, "context": "Neural machine translation (NMT; Sutskever et al. (2014); Bahdanau et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2015), \u00a72), a method for MT that performs translation in an end-toend fashion using neural networks, is quickly becoming the de-facto standard in MT applications due to its impressive empirical results.", "startOffset": 8, "endOffset": 31}, {"referenceID": 25, "context": "(2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016).", "startOffset": 70, "endOffset": 116}, {"referenceID": 1, "context": "(2003)), including agreement and long-distance syntactic dependencies (Neubig et al., 2015; Bentivogli et al., 2016).", "startOffset": 70, "endOffset": 116}, {"referenceID": 18, "context": "based MT (PBMT; Koehn et al. (2003)), including agreement and long-distance syntactic dependencies (Neubig et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b) or phrasesense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena.", "startOffset": 117, "endOffset": 140}, {"referenceID": 5, "context": "As a result, PBMT systems required specific separate modules to incorporate long-term context, performing word-sense (Carpuat and Wu, 2007b) or phrasesense (Carpuat and Wu, 2007a) disambiguation to improve their handling of these phenomena.", "startOffset": 156, "endOffset": 179}, {"referenceID": 16, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 14, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 15, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 32, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 27, "context": "Specifically, we learn from neural models for word sense disambiguation (Kalchbrenner et al., 2014; Iyyer et al., 2015; K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016; \u0160uster et al., 2016), examining three methods inspired by these literatures (\u00a74).", "startOffset": 72, "endOffset": 191}, {"referenceID": 9, "context": "In order to incorporate this information into NMT, we examine two methods: gating the word-embeddings in the model (similarly to Choi et al. (2017)), and concatenating the context-aware representation to the word embedding (\u00a75).", "startOffset": 129, "endOffset": 148}, {"referenceID": 22, "context": "To evaluate the effectiveness of our method, we compare our context-aware models with a strong baseline (Luong et al., 2015) on the EnglishGerman, English-French, and English-Chinese WMT dataset.", "startOffset": 104, "endOffset": 124}, {"referenceID": 22, "context": "We follow the global-general-attention NMT architecture with input-feeding proposed by Luong et al. (2015), which we will briefly summarize here.", "startOffset": 87, "endOffset": 107}, {"referenceID": 13, "context": "The recurrent units \u2212\u2212\u2192 RNNe and \u2190\u2212\u2212 RNNe are usually either LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al.", "startOffset": 67, "endOffset": 101}, {"referenceID": 10, "context": "The recurrent units \u2212\u2212\u2192 RNNe and \u2190\u2212\u2212 RNNe are usually either LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al., 2014).", "startOffset": 110, "endOffset": 130}, {"referenceID": 12, "context": "We evaluate the translation performance of words in the source side by aligning them to the target side using fast-align (Dyer et al., 2013).", "startOffset": 121, "endOffset": 140}, {"referenceID": 2, "context": "dictionary/english/ We use the stop word list from NLTK (Bird et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 26, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 23, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 33, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 8, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 4, "context": "Word sense disambiguation (WSD) is the task of resolving the ambiguity of homographs (Ng and Lee, 1996; Mihalcea and Faruque, 2004; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015), and we hypothesize that by learning from these models we can improve the ability of the NMT model to choose the correct translation for these ambiguous words.", "startOffset": 85, "endOffset": 229}, {"referenceID": 15, "context": "Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016).", "startOffset": 111, "endOffset": 162}, {"referenceID": 32, "context": "Recent research tackles this problem with neural models and has shown state-of-the art results on WSD datasets (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016).", "startOffset": 111, "endOffset": 162}, {"referenceID": 15, "context": "Neural bag-of-words (NBOW) Kalchbrenner et al. (2014); Iyyer et al.", "startOffset": 27, "endOffset": 54}, {"referenceID": 14, "context": "(2014); Iyyer et al. (2015) have shown success by representing full sentences with a context vector, which is the average of the Lookup embeddings of the input sequence", "startOffset": 8, "endOffset": 28}, {"referenceID": 15, "context": "Bi-directional LSTM (BiLSTM) K\u00e5geb\u00e4ck and Salomonsson (2016) leveraged a bidirectional LSTM that learns a context vector for the target word in the input sequence and predict the word sense with a multi-layer perceptron.", "startOffset": 29, "endOffset": 61}, {"referenceID": 32, "context": "Held-out LSTM (HoLSTM) Yuan et al. (2016) trained a LSTM language model, which predicts a held-out word given the surrounding context, with a large amount of unlabeled text as training data.", "startOffset": 23, "endOffset": 42}, {"referenceID": 9, "context": "Gate Inspired by Choi et al. (2017), as our first method for integration of context-aware word embeddings, we use a gating function as follows:", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "Choi et al. (2017) use this method in concert with averaged embeddings from words in source language like the NBOW model above, which naturally uses the same context vectors for all time steps.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "(2015); for French, we used the moses (Koehn et al., 2007) tokenization script with the \u201c-a\u201d flag; for Chinese, we split sequences of Chinese characters, but keeps sequences of non-Chinese characters as they are, using the script from IWSLT Evaluation 2015.", "startOffset": 38, "endOffset": 58}, {"referenceID": 19, "context": "For German, we use the tokenized dataset from Luong et al. (2015); for French, we used the moses (Koehn et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "When training our NMT systems, following Bahdanau et al. (2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle minibatches as we proceed.", "startOffset": 41, "endOffset": 64}, {"referenceID": 18, "context": "Our model is built upon OpenNMT (Klein et al., 2017) with the default settings unless otherwise noted.", "startOffset": 32, "endOffset": 52}, {"referenceID": 19, "context": "001) than baseline models using paired bootstrap resampling (Koehn, 2004).", "startOffset": 60, "endOffset": 73}, {"referenceID": 3, "context": "It should be noted that previous work such as Britz et al. (2017) have", "startOffset": 46, "endOffset": 66}, {"referenceID": 12, "context": "To do so, we first acquire the translation of homographs in the source language using fast-align (Dyer et al., 2013).", "startOffset": 97, "endOffset": 116}, {"referenceID": 31, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 26, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 23, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 24, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 33, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 8, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 4, "context": "Word sense disambiguation (WSD), the task of determining the correct meaning or sense of a word in context is a long standing task in NLP (Yarowsky, 1995; Ng and Lee, 1996; Mihalcea and Faruque, 2004; Navigli, 2009; Zhong and Ng, 2010; Di Marco and Navigli, 2013; Chen et al., 2014; Camacho-Collados et al., 2015).", "startOffset": 138, "endOffset": 313}, {"referenceID": 15, "context": "Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word embeddings that capture context (K\u00e5geb\u00e4ck and Sa-", "startOffset": 89, "endOffset": 140}, {"referenceID": 32, "context": "Recent research on tackling WSD and capturing multi-senses includes work leveraging LSTM (K\u00e5geb\u00e4ck and Salomonsson, 2016; Yuan et al., 2016), which we extended as a context network in our paper and predicting senses with word embeddings that capture context (K\u00e5geb\u00e4ck and Sa-", "startOffset": 89, "endOffset": 140}, {"referenceID": 26, "context": "\u0160uster et al. (2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "(2016); Kawakami and Dyer (2016) also showed that bilingual data improves WSD.", "startOffset": 8, "endOffset": 33}, {"referenceID": 26, "context": "In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation.", "startOffset": 45, "endOffset": 67}, {"referenceID": 5, "context": "Following this reformulation, Chan et al. (2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems.", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": "(2007); Carpuat and Wu (2007a,b) integrated WSD systems into phrase-based systems. Xiong and Zhang (2014) breaks the process into two stages.", "startOffset": 8, "endOffset": 106}, {"referenceID": 9, "context": "Choi et al. (2017) leverage the NBOW as context and gate the word-embedding on both encoder and decoder side.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context. However, with the advent of neural machine translation (NMT) systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense before feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.", "creator": "LaTeX with hyperref package"}}}