{"id": "1708.08959", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "A Simple LSTM model for Transition-based Dependency Parsing", "abstract": "We present a simple LSTM-based transition-based dependency parser. Our model consists of a single hidden LSTM layer that replaces the hidden layer in the common feed-forward network architecture. We also propose a new initialization method that uses the pre-trained weights of an upstream neural network to initialize our LSTM-based model. We also show that using dropouts on the input layer has a positive effect on performance. Our final parser achieves a score of 93.06% blank and 91.01% labeled attachments on the Penn treebank. In addition, we replace LSTMs in our model with GRUs and Elman units and examine the effectiveness of our initialization method on individual gates, which account for all three types of RNN units.", "histories": [["v1", "Tue, 29 Aug 2017 18:25:35 GMT  (131kb)", "http://arxiv.org/abs/1708.08959v1", null], ["v2", "Fri, 8 Sep 2017 22:46:59 GMT  (131kb)", "http://arxiv.org/abs/1708.08959v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mohab elkaref", "bernd bohnet"], "accepted": false, "id": "1708.08959"}, "pdf": {"name": "1708.08959.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mxe346@cs.bham.ac.uk", "bohnetbd@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n08 95\n9v 1\n[ cs\n.C L\n] 2\n9 A\nug 2\n01 7\ntransition-based dependency parser. Our model is composed of a single LSTM hidden layer replacing the hidden layer in the usual feed-forward network architecture. We also propose a new initialization method that uses the pre-trained weights from a feed-forward neural network to initialize our LSTM-based model. We also show that using dropout on the input layer has a positive effect on performance. Our final parser achieves a 93.06% unlabeled and 91.01% labeled attachment score on the Penn Treebank. We additionally replace LSTMs with GRUs and Elman units in our model and explore the effectiveness of our initialization method on individual gates constituting all three types of RNN units."}, {"heading": "1 Introduction", "text": "Neural Networks have become the backbone of some of the top performing transitionbased dependency parsing systems. The use of a simple feed-forward network (FFN) by Chen and Manning (2014), kickstarted a string of improvements upon this approach. Weiss et al. (2015) trained a larger deeper network, and used a final structured perceptron layer on top of this. Andor et al. (2016) used global normalization and a large beam to achieve the state of the art results for dependency parsers of this type.\nOn the other hand, recurrent neural network (RNN) models have also started recieving more attention. Kiperwasser and Goldberg (2016a) used heirarchical tree LSTMs to model the dependency tree itself, and then passed on the extracted information to a feed-forward/output layer structure\nsimilar to that in Chen and Manning (2014)\u2019s original model. Dyer et al. (2015) used stack-LSTMs to model the current states of the different structures of a transition-based system, with separate stack-LSTMs modeling the stack, the buffer, and the history of transitions made so far.\nChen et al. (2015) used two Gated Recurrent Unit (GRUs) networks to represent the dependency tree, while Zhang et al. (2015) developed TreeLSTMs to estimate the probability that a certain dependency tree is generated given a sentence.\nKiperwasser and Goldberg (2016b) used a conceptually simpler approach, by running a bidirectional LSTM (biLSTM) over the sentence. The inputs to these biLSTMs were various features describing the word, its part-of-speech (pos) tag, and various other structural information about each token in the sentence. The output of the biLSTMs was again passed onto a feed-forward layer to compute a hidden state before the final output layer.\nThese LSTM-based methods, however, all attempt to replace the original embeddings layer used by Chen and Manning (2014) with more sophisticated feature representation but, as we pointed out, they keep the main structure of the model largely the same. That is, a hidden layer encoding the input features at the current time-step before a final output layer scores possible transitions. These hidden layers can be seen as encoding the current configuration of inputs in a manner useful only for the decision made at that point in the transition sequence.\nIn contrast to these approaches, Kuncoro et. al. (2016) extended the basic model of Chen & Manning (2014) by replacing the hidden layer with an LSTM, thus allowing the network to model sequences of transitions instead of only immediate input/transition pairs.\nIn this work we build on Kuncoro et. al.(2016)\u2019s\napproach by initialising the weights of an LSTMbased dependency parser with weights of a pretrained Feed-Forward network. We show that this method produces a substantial improvement in accuracy scores, and is also applicable to different kinds of RNNs. An additional contribution of this paper is a refinement of the basic training model of Chen & Manning (2014) producing a more accurate Feed Forward model as a baseline for our experiments 1.\nWe begin with a brief overview of transitionbased dependency parsing, followed by an explanation of our baseline models; the basic FFN and LSTM-based models that are the center of this work. We then explain our proposed method for the alternative initialization of the LSTM weights, and then present the results of our experiments with a comparison with other state-of-theart parsers. Finally we explore the use of GRUs and Elman networks in place of LSTMs, and show the effect of initializing individual gates using our proposed method on the overall performance."}, {"heading": "2 Transition-based Dependency Parsing", "text": "A transition-based parsing system considers a given sentence one word at a time. The parser then makes a decision to either join this word to a word encountered previously with a dependency relation, or to store this word until it can be attached at a later point in the sentence. In this way the parser requires only a single pass through the sentence to produce a dependency tree.\nIn this work we use the arc-standard transition system (Nivre, 2004), which maintains two data structures, the stack (S), which holds the words that the parser has already seen and wishes to remember, and the buffer (B), containing all the words that it has yet to consider, in the order in which they appear in the sentence. In addition the parser keeps a list of all dependency arcs (A) produced throughout the parse. Together, the state of the stack, the buffer, and the list of arcs are referred to as the configuration (x) of the parser. In their initial states, the buffer contains all the words of a sentence in order, with the stack containing the ROOT token, which is typically attached to the main verb in the sentence. The parser can then perform one of 3 transitions:\n\u2022 SHIFT removes the front word, b0 from B,\n1Our tensorflow implementation can be found here link will be here upon acceptance of paper\nand pushes it onto S. \u2022 LEFT-ARC adds an arc between the top two items, s0 and s1, on S with s0 being the head. s1 is then removed from S. \u2022 RIGHT-ARC adds an arc between the top two items, s0 and s1, on S with s1 being the head. s0 is then popped from S.\nEach of these transitions changes the state of one or more of the structures in the parser, and therefore produces a new x."}, {"heading": "3 Baseline Models", "text": "Our proposed approach makes use of a simple feed-forward model to improve the performance of an LSTM-based model. We show that the final network surpasses both of our baselines, which are the original feed-forward network, and an LSTM model trained with randomly initialized weights. In this section we will describe the structure of both baselines."}, {"heading": "3.1 Input Layer, Selected Features, & Output Layer", "text": "The Embeddings layer is a concatenation of the embedding vectors of select raw features of the parser configuration. The resulting layer is a dense feature representation of x. The features used in our implementation are shown in Table 1.\nWe represent the configuration of the parser at a particular timestep as a number of raw features extracted from the data structures of x. We use vector embeddings to represent each of the raw features.\nEach word (w), part of speech tag (t), and arc label (l) is represented as a d-dimensional vector ew \u2208 R dw , et \u2208 R dt , and el \u2208 R dl respectively. And so the embedding matrices for the\ndifferent types of features are Ew \u2208 Rdw\u00d7Vw , Et \u2208 Rdt\u00d7Vt , and El \u2208 Rdl\u00d7Vl , where d\u2217 is the dimensionality of the embedding vector for a feature type, and V\u2217 is the vocabulary size. We add additional vectors for \u201dROOT\u201d and \u201dNULL\u201d for all feature types, as well as \u201dUNK\u201d (unknown), for unknown/infrequent words.\nThis embeddings layer is used as the input layer in all models described in this work. For all models we use dropout (Hinton et al., 2012) on the input layer. We find that this improves the final accuracy of all the networks trained.\nThe output layer y consists of nodes representing every possible transition, with one node representing Shift, and a node for every possible pair of arc transitions (Left/Right-Arc) and dependency labels. This makes the size of the output layer constant at 2Vl + 1, regardless of the structure of the network."}, {"heading": "3.2 Feed-Forward Model", "text": "For our FFN model we use the same basic structure of Chen and Manning (2014) with a single hidden layer and a final softmax output layer. We however follow Weiss et. al. (2015) in using rectified linear units (ReLUs) (Nair and Hinton, 2010) as hidden neuron activation functions. Finally, we use dropout on the hidden layer similar to the input layer. The structure of the FFN is specified below.\nh = max{0,Wx+ bh}\ny = softmax(Whh)\nFollowing Weiss et al. (2015) we set the initial bias of the hidden layer to 0.02 in order to avoid having any dead ReLUs at the start of training."}, {"heading": "3.3 RNN-based Model", "text": "Our RNN-based model is an extension of the basic feed forward model, with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) standing in for the traditional feed forward hidden layers.\nThe change allows for the information in the parser configuration to be shared as needed with future time-steps. This lets the network at any point in the sequence of transitions make a decision based on a more informative context, that is not only based on the current configuration, or the present state of the dependency tree, but also on the changes made to them.\nIn their standard forms, RNNs are affected by both exploding and vanishing gradients (Bengio et al., 1994), making them notoriously hard to train despite their expressive ability. LSTMs are a variety of RNNs that maintains an internal state ct that forms the basis for the recurrence, and is passed from time-step to the next. This direct connection is not interrupted by any weight matrixes, as would be the case in simpler RNN architectures such as Elman networks (Elman, 1990), but is instead scaled and added to by a number of gates that handle extracting and scaling information from the input data, and computing a final hidden state ht at each time step to pass on to deeper layers. This uninterrupted connection of internal states throughout the sequence is an important part of how LSTMs address the shortcomings of RNNs.\nThere have been a variety of architectures in literature referred to as LSTMs, all bearing slight differences to the basic LSTM unit. The definition of the LSTM we use in this work is shown\nbelow.\nit = \u03c3(Wxixt +Whiht\u22121 + bi)\njt = tanh(Wxjxt +Whjht\u22121 + bj)\nft = \u03c3(Wxfxt +Whfht\u22121 + bf )\not = \u03c3(Wxoxt +Whoht\u22121 + bo)\nct = ct\u22121 \u2299 ft + it \u2299 jt\nht = tanh(ct)\u2299 ot\nWith the final softmax output layer, just as with the FNN model.\ny = softmax (Whht)\nUnlike Kuncoro et. al. (2016), we do not use peephole connections like those suggested by Graves (2013). Additionally, we add a bias of 1 to the LSTM\u2019s forget gate following Gers et al. (2000). Finally, we also apply a dropout similar to that in Zaremba et. al. (2014).\nAs shown in this definition, the LSTM cell maintains an internal state ct, where the previous internal state ct\u22121 is modulated at each time-step by the forget gate ft, and then added to by a scaled selection of the current input xt by the input gates it and jt. This new ct is then used for the external state ht and passed on to the next time-step. All gates rely on weighted activations of the current input xt and the previous external state ht\u22121.\nThis pair of hidden states allows the LSTM to contribute to long-term decisions with ct, while still being able to make immediate or short-term decisions with ht, and it is this final calculation of ht, along with ot, that is the focus of our contribution in this work."}, {"heading": "4 Initializing LSTM gates", "text": "Much has been written about the need for careful initialization of weights, often done to complement certain optimisation methods such as gradient descent with momentum in (Sutskever et al., 2013). For deep networks, Hinton et. al. (2006) and later Bengio et. al. (2007) approached initialization differently by using a greedy layer-wise unsupervised learning algorithm, which trains each layer sequentially, before fine-tuning the entire network as a whole.\nLe et. al. (2015) suggested replacing traditional tanh units in a simple RNN with ReLUs, in addition to initializing the weights with an identity matrix.\nAs previously mentioned, Gers et al. (2000) suggested initializing the bias of the forget gate bf of an LSTM to 1. This allowed the LSTM unit to learn which information it needed to forget as opposed to detecting the opposite. This was later shown by Jozefowicz et. al. (2015) to improve performance of an LSTM on a variety of tasks.\nAlternatively, it has become increasingly common to use tuned outputs from one network as initialization for another. For example the use of pretrained embeddings as initialization for word vectors has become de facto standard procedure for tasks such as dependency parsing, language modelling and question answering.\nFollowing this approach, we propose initializing the LSTM weights, specifically the Wx\u2217 and\nbias b\u2217 of all LSTM gates, with the weight matrix Wx and hidden bias bh of a pre-trained, similarly structured feed-forward neural network. We also initialize the embedding matrices used Ew,t,l and the weights of the final softmax layer Why with those of the pre-trained feed-forward network.\nTo illustrate this idea we reproduce a modified version of the LSTM architecture diagram appearing in (Jozefowicz et al., 2015) in Figure 2, with the addition of the final softmax layer yt. The flow of information from the current input xt to yt (as shown by the bold arrows) is almost identical to that in an FFN, except for the addition of ht\u22121 as input to o, and the \u201cinterference\u201d of information from ct to produce ht.\nThis approach rests on the 2 hidden states of the LSTM requiring different information from the same input data. Since htis more concerned with immediate decisions, it would strongly benefit from the trained weights of a feed-forward network, which are tuned to extract the maximum relevant information from the input of the current time-step, since it has no access to prior information.\nThe various LSTM gates would still be able to learn to use information from ht\u22121 but would be in a better position to do so with the biases and input weights closer to an optimum configuration.\nMoreover, the internal state ct would receive less severe errors early on in the training process, owing to a better contribution from ot in the calculation of ht, and a less disruptive result from ct due to the input and forget gates initially behaving more similarly to the regular hidden layer of the original FFN.\nThis would mean less pressure on the weights of the input and forget gates to adapt to immediate decisions while the internal state would be more capable of gradually learning longer term patterns.\nWe will henceforth differentiate networks initialized in the manner described in this section by referring to them as bootstrapped models, while we refer to the usual randomly initialized networks as baselines models."}, {"heading": "5 Experiments", "text": "We begin by comparing the performance of our FFN and LSTM baseline networks with our bootstrapped model. For all networks we ran a model with a single hidden layer 256 neurons/LSTM units wide. The embeddings dimensions used\nwere dw = dt = dl = 100. Weiss et al. (2015) showed that large gains can be made with a grid search to tune learning hyperparameters as well embeddings sizes and hidden layer dimensions, which we did not perform due to its very high computational cost. We use the GloVe pre-trained embeddings produced by Pennington et al. (2014) to initialize the word vectors.\nLearning is done with mini-batch stochastic gradient descent (SGD) with momentum to minimise logistic loss with the learning rate \u03b1 = 0.05 and momentum \u00b5 = 0.9. We also use an additional l2 regularization cost (\u03bb = 10 \u22128).\nL(\u03b8) = \u2212 \u2211\ni\nlog(yi) + \u03bb\n2 \u2016\u03b8\u20162\nWhere \u03b8 represents all weight, biases, and embeddings matrices. We also set the dropout rate to 0.3 for the embeddings layers and hidden layer for both the baselines and bootstrapped model, and initialise all baseline weights randomly in the range [\u22120.01, 0.01].\nFor LSTM-based models we used truncated backpropagation throught time (BPTT), with a truncation limit \u03c4 = 5. This means that errors are propagated backwards to layers in previous time steps until a limit \u03c4 is reached. In our experiments varying \u03c4 between 5 and full back propagation had a negligible effect on the final accuracy of the networks, while using a truncation limit produced a significant speed up in training. We stress that this insignificant difference is most likely a task and architecture specific issue, and would probably be much more pronounced in other tasks and neural network set-ups.\nFor our experiments we use the Wall Street Journal (WSJ) section from the Penn Treebank (Marcus et al., 1993). We use \u00a72-21 for training, \u00a722 for development, and \u00a723 for testing. We use Stanford Dependencies (SD) (De Marneffe et al., 2006) converted from constituency trees using version 3.3.0 of the converter. As is standard we use predicted POS tags for the train, dev, and test sets. We report unlabeled attachment score (UAS) and labeled attachment score (LAS), with punctuation excluded.\nThe results in Table 2 show the effect of applying dropout on the input layer for our FFN baseline, when compared to the similarly sized Chen and Manning (2014) model which has 200 neurons in its hidden layer. This is in addition\nto achieving very close dev score accuracy results with only a single 256 neuron hidden layer when compared to the significantly larger models of Weiss et al. (2015) with 2 layers of size 2048, and Andor et al. (2016) with 2 layers of size 1024 layers.\nComparing our 2 baseline models shows that the LSTM-based model performs much better than the FFN model, with an almost 0.5% gain in dev score accuracy. Our main result is our bootstrapped model, which not only surpassed the original FFN baseline, but also the LSTM baseline.\nWe note that our LSTM-baseline achieves a substantial improvement over the similar architecture of Kuncoro et al. (2016). The main differences in this case are a slightly larger model and using LSTMs without peephole connections.\nIn addition, our bootstrapped model produces better results than all the mentioned feed forward models in addition to most of the LSTMbased approaches in Table 2, with the exception of Kiperwasser and Goldberg (2016b), despite only having a single hidden layer of LSTM units and making no use of biLSTMs, TreeLSTMs, or Stack LSTMs."}, {"heading": "6 Discussion", "text": "The results of our experiments seem to lend credence to the idea that learning short and long-term patterns separately is useful to the performance of an LSTM. To generalize this further, one could say that a sequence modelling task where a 1-to-1 relation between input/output pairs can be learned should first attempt this with an FFN, and then transfer that knowledge to an LSTM as described in section 4, so sequence specific information can be further modelled.\nAn additional benefit of this approach is that it can be applied to previously trained FFNs and can improve any of the models that we have compared our results with in Table 2. This is also true of the LSTM-based models, where the strength of their contributions lies in their innovative approaches to feature extraction while keeping the rest of the network essentially the same.\nFor example, we can merge our work with that of Kiperwasser and Goldberg (2016b), by first training their model; a biLSTM input layer going to a feed-forward hidden layer followed by an output layer, and then replacing the hidden layer with an LSTM initialized with the weights of that hidden layer.\nFinally, our addition of applying dropout to the input layer can also be used here to further strengthen the performance of this example."}, {"heading": "7 Alternative Recurrent Units", "text": "So far we have shown how to improve the performance of LSTMs by drawing parallels between the functions of certain gates and the traditional feed-forward network. In this section we attempted to do the same for 2 other popular forms of RNNs, the Simple Recurrent Network, otherwise known as the Elman network (Elman, 1990), and the Gated Recurrent Unit (GRU) (Cho et al., 2014)."}, {"heading": "7.1 Elman networks", "text": "The Elman network is one of the earliest and simplest RNNs found in literature. It was the subject of much study and suffered from all the original problems of vanishing and exploding gradients mentioned before, which later motivated the development and adoption of more sophisticated units such as LSTMs and GRUs.\nNevertheless there have been examples where Elman networks were capable of performing rel-\natively well, notably the work of Mikolov et al. (2010) on language modelling and an extended memory version of Elman networks in (Mikolov et al., 2014).\nElman networks themselves are only a simple addition to the architecture of the traditional feedforward network. Whereas an FFN has a hidden layer, and Elman network has an additional context layer, that represents the output of the hidden layer in the previous time-step. In a way, it can be compared to output gate of an LSTM, without any additional tools to model the sequence.\nIn our experiment we use the ReLU activation function once more for the hidden layer similar to Le et al. (2015), but without their initialization\nstrategy. The precise definition of the Elman network that we use is shown below.\nh = max{0,Wxx+Whht\u22121 + bh}\ny = softmax (Whh)\nIn Figure 3a we illustrate the structure of this network. The simplicity of the addition here makes it far easier to draw parallels between the function of the weight matrices in the Elman network and in the FFN as shown in 2b."}, {"heading": "7.2 Gated Recurrent Units", "text": "Introduced by Cho et al. (2014), GRUs are an architecture often compared to LSTMs. It also attempts to solve the gradient vanishing problem in a similar way, by keeping the modulation and addition of information in separate gates, and avoiding any weighted obstructions between the hidden states of one time-step and the next. A notable difference however is the lack of an internal state. All modifications are done directly to the external hidden state ht, potentially complicating the learning process with conflicting information about short and long-term dependencies.\nDespite this apparently simpler structure, Chung et al. (2014) found GRUs to outperform LSTMs on a number of tasks, and Jozefowicz et al. (2015) also found that GRUs can beat LSTMs except in language modelling. However, Jozefowicz et al. (2015) also found that initializing the LSTM forget gate bias bf to 1 allowed the LSTM to almost match the performance of the GRU on other tasks.\nrt = \u03c3(Wxrxt +Whrht\u22121 + br)\nzt = \u03c3(Wxzxt +Whzht\u22121 + bz)\nh\u0303t = tanh(Wxh\u0303xt +Whh\u0303(rt \u2299 ht\u22121) + bh) ht = zt \u2299 ht\u22121 + (1\u2212 zt)\u2299 h\u0303t\ny = softmax (Whht)\nThe internal architecture of a GRU consists of a reset gate rt modulating the previous state ht\u22121, a candidate gate h\u0303 computing the next addition to ht, and an update gate zt controlling how much of the candidate h\u0303t is added to ht.\nIn this case the candidate gate h\u0303t is the most analogous to the hidden layer in an FFN. As shown by the bold lines in Figure 3b, this flow of information appears similar to that of the output gate ot\nin an LSTM, except that the additional input here is modulated by the rt instead of receiving ht\u22121, in addition to dealing with further interference from the update gate."}, {"heading": "7.3 Comparison & Results", "text": "For the experiments in this section we used the same network dimensions as in section 5, as well as the same training parameters and procedure.\nFor each RNN type we trained 2 FFN and RNN baselines, one with GloVe pre-trained word embeddings(Pennington et al., 2014) and another with randomly initialized embeddings. We then trained bootstrapped models initialized with the FFN baselines. The results are shown in Table 3.\nAs in section 5, this initialization method shows a positive effect on an LSTM-based model, again surpassing both its baselines. The Elman network is stronger than expected and benefits greatly from this approach. Indeed, the bootstrapped Elman model is comparable in accuracy to some of the results in Table 2.\nThis cannot be said of GRUs, however, where its baselines perform significantly worse than\nother RNNs. Moreover, bootstrapped GRU models perform even worse than their baselines, even failing to match the accuracy of the FFNs used to initialize them. This disparity in accuracy compared to LSTMs seems to lend credence to our earlier hypothesis that learning long-term sequences can interfere with learning to make immediate decisions based on the input from the current time step. The architecture of an LSTM which maintains a long-term internal state ct separate from a short-term external state ht, and the additional improvement gained from learning these separately, as opposed to the single common hidden state ht in GRUs appears to provide a distinct advantage here.\nThe improvement achieved by a bootstrapped Elmanmodel can thus be explained by the fact that it suffers from gradient vanishing (Bengio et al., 1994), and so sequence specific information does not affect training to the extent that it does in GRUs."}, {"heading": "8 Initializing Individual Gates", "text": "Our final set of experiments is to investigate whether or not individual gates of LSTMs and GRUs can benefit from this initialization technique. We follow the same initialization and training procedures described previously, and for every gate we also initialize its corresponding bias vectors. We keep the same size and parameters as in section 7.3, and also train baselines with and without pre-trained embeddings.\nBootstrapping individual LSTM gates produces mixed results, especially when considering the difference in performance between the random and pre-trained embeddings experiments.\nFull bootstrapping, bootstrapping the j gate or bootstrapping the o gate seem to be the most reliable options based on these results.\nResults for bootstrapping individual GRU gates vary drastically, with individual gates performing very differently in their random and pre-trained embedding experiments.\nSurprisingly, bootstrapping all GRU gates achieves better results than the GRU baseline for random embeddings, while severely hurting accuracy with pre-trained embeddings. All GRU experiments, bootstrapped or not, still do not perform better than the FFN baseline."}, {"heading": "9 Conclusion", "text": "In this paper we have presented a simple and effective LSTM transition-based dependency parser. Its performance rivals that of far more complicated approaches, while still being capable of integrating with minimal changes to their architecture.\nAdditionally, we showed that the application of dropout to the input layer can improve the performance of a network. Like our other contributions here this is simple to apply to other models and is not only limited to the architectures presented in this work.\nFinally, we proposed a method of using pretrained FFNs as initializations for an RNN-based model. We showed that this approach can produce gains in accuracy for both LSTMs and Elman networks, with the final LSTM model surpassing or matching most state-of-the-art LSTM-based models.\nThis initialization method can potentially be applied to any LSTM-based task, where a 1-to-1 relation between inputs can first be modelled using an FFN. Exploring the effects of this method on other tasks is left for future work."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "arXiv preprint arXiv:1603.06042 .", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Greedy layer-wise training of deep networks. Advances in neural information processing systems 19:153", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE transactions on neural networks 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP. pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Transition-based dependency parsing using two heterogeneous gated recursive neural networks", "author": ["Xinchi Chen", "Yaqian Zhou", "Chenxi Zhu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "EMNLP. pages 1879\u2013 1889.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of LREC", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "arXiv preprint arXiv:1505.08075 .", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins."], "venue": "Neural computation 12(10):2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh."], "venue": "Neural computation 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580 .", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Journal of Machine Learning Research .", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Easyfirst dependency parsing with hierarchical tree lstms", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.00375 .", "citeRegEx": "Kiperwasser and Goldberg.,? 2016a", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.04351 .", "citeRegEx": "Kiperwasser and Goldberg.,? 2016b", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Dependency parsing with lstms: An empirical evaluation", "author": ["Adhiguna Kuncoro", "Yuichiro Sawai", "Kevin Duh", "Yuji Matsumoto."], "venue": "arXiv preprint arXiv:1604.06529 .", "citeRegEx": "Kuncoro et al\\.,? 2016", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton."], "venue": "arXiv preprint arXiv:1504.00941 .", "citeRegEx": "Le et al\\.,? 2015", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10). pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton."], "venue": "International conference on machine learning. pages 1139\u2013 1147.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "arXiv preprint arXiv:1506.06158 .", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Top-down tree long short-term memory networks", "author": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1511.00060 .", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "The use of a simple feed-forward network (FFN) by Chen and Manning (2014), kickstarted a string of improvements upon this approach.", "startOffset": 50, "endOffset": 74}, {"referenceID": 2, "context": "The use of a simple feed-forward network (FFN) by Chen and Manning (2014), kickstarted a string of improvements upon this approach. Weiss et al. (2015) trained a larger deeper network, and used a final structured perceptron layer on top of this.", "startOffset": 50, "endOffset": 152}, {"referenceID": 0, "context": "Andor et al. (2016) used global normalization and a large beam to achieve the state of the art results for dependency parsers of this type.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Andor et al. (2016) used global normalization and a large beam to achieve the state of the art results for dependency parsers of this type. On the other hand, recurrent neural network (RNN) models have also started recieving more attention. Kiperwasser and Goldberg (2016a) used heirarchical tree LSTMs to model the dependency tree itself, and then passed on the extracted information to a feed-forward/output layer structure similar to that in Chen and Manning (2014)\u2019s original model.", "startOffset": 0, "endOffset": 274}, {"referenceID": 0, "context": "Andor et al. (2016) used global normalization and a large beam to achieve the state of the art results for dependency parsers of this type. On the other hand, recurrent neural network (RNN) models have also started recieving more attention. Kiperwasser and Goldberg (2016a) used heirarchical tree LSTMs to model the dependency tree itself, and then passed on the extracted information to a feed-forward/output layer structure similar to that in Chen and Manning (2014)\u2019s original model.", "startOffset": 0, "endOffset": 469}, {"referenceID": 0, "context": "Andor et al. (2016) used global normalization and a large beam to achieve the state of the art results for dependency parsers of this type. On the other hand, recurrent neural network (RNN) models have also started recieving more attention. Kiperwasser and Goldberg (2016a) used heirarchical tree LSTMs to model the dependency tree itself, and then passed on the extracted information to a feed-forward/output layer structure similar to that in Chen and Manning (2014)\u2019s original model. Dyer et al. (2015) used stack-LSTMs to model the current states of the different structures of a transition-based system, with separate stack-LSTMs modeling the stack, the buffer, and the history of transitions made so far.", "startOffset": 0, "endOffset": 506}, {"referenceID": 3, "context": "These LSTM-based methods, however, all attempt to replace the original embeddings layer used by Chen and Manning (2014) with more sophisticated feature representation but, as we pointed out, they keep the main structure of the model largely the same.", "startOffset": 96, "endOffset": 120}, {"referenceID": 13, "context": "For all models we use dropout (Hinton et al., 2012) on the input layer.", "startOffset": 30, "endOffset": 51}, {"referenceID": 23, "context": "(2015) in using rectified linear units (ReLUs) (Nair and Hinton, 2010) as hidden neuron activation functions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 3, "context": "For our FFN model we use the same basic structure of Chen and Manning (2014) with a single hidden layer and a final softmax output layer.", "startOffset": 53, "endOffset": 77}, {"referenceID": 3, "context": "For our FFN model we use the same basic structure of Chen and Manning (2014) with a single hidden layer and a final softmax output layer. We however follow Weiss et. al. (2015) in using rectified linear units (ReLUs) (Nair and Hinton, 2010) as hidden neuron activation functions.", "startOffset": 53, "endOffset": 177}, {"referenceID": 26, "context": "Following Weiss et al. (2015) we set the initial bias of the hidden layer to 0.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "Our RNN-based model is an extension of the basic feed forward model, with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) standing in for the traditional feed forward hidden layers.", "startOffset": 110, "endOffset": 144}, {"referenceID": 2, "context": "In their standard forms, RNNs are affected by both exploding and vanishing gradients (Bengio et al., 1994), making them notoriously hard to train despite their expressive ability.", "startOffset": 85, "endOffset": 106}, {"referenceID": 9, "context": "This direct connection is not interrupted by any weight matrixes, as would be the case in simpler RNN architectures such as Elman networks (Elman, 1990), but is instead scaled and added to by a number of gates that handle extracting and scaling information from the input data, and computing a final hidden state ht at each time step to pass on to deeper layers.", "startOffset": 139, "endOffset": 152}, {"referenceID": 10, "context": "(2016), we do not use peephole connections like those suggested by Graves (2013). Additionally, we add a bias of 1 to the LSTM\u2019s forget gate following Gers et al.", "startOffset": 67, "endOffset": 81}, {"referenceID": 10, "context": "Additionally, we add a bias of 1 to the LSTM\u2019s forget gate following Gers et al. (2000). Finally, we also apply a dropout similar to that in Zaremba et.", "startOffset": 69, "endOffset": 88}, {"referenceID": 10, "context": "Additionally, we add a bias of 1 to the LSTM\u2019s forget gate following Gers et al. (2000). Finally, we also apply a dropout similar to that in Zaremba et. al. (2014).", "startOffset": 69, "endOffset": 164}, {"referenceID": 25, "context": "Much has been written about the need for careful initialization of weights, often done to complement certain optimisation methods such as gradient descent with momentum in (Sutskever et al., 2013).", "startOffset": 172, "endOffset": 196}, {"referenceID": 25, "context": "Much has been written about the need for careful initialization of weights, often done to complement certain optimisation methods such as gradient descent with momentum in (Sutskever et al., 2013). For deep networks, Hinton et. al. (2006) and later Bengio et.", "startOffset": 173, "endOffset": 239}, {"referenceID": 25, "context": "Much has been written about the need for careful initialization of weights, often done to complement certain optimisation methods such as gradient descent with momentum in (Sutskever et al., 2013). For deep networks, Hinton et. al. (2006) and later Bengio et. al. (2007) approached initialization differently by using a greedy layer-wise unsupervised learning algorithm, which trains each layer sequentially, before fine-tuning the entire network as a whole.", "startOffset": 173, "endOffset": 271}, {"referenceID": 10, "context": "As previously mentioned, Gers et al. (2000) suggested initializing the bias of the forget gate bf of an LSTM to 1.", "startOffset": 25, "endOffset": 44}, {"referenceID": 10, "context": "As previously mentioned, Gers et al. (2000) suggested initializing the bias of the forget gate bf of an LSTM to 1. This allowed the LSTM unit to learn which information it needed to forget as opposed to detecting the opposite. This was later shown by Jozefowicz et. al. (2015) to improve performance of an LSTM on a variety of tasks.", "startOffset": 25, "endOffset": 277}, {"referenceID": 15, "context": "To illustrate this idea we reproduce a modified version of the LSTM architecture diagram appearing in (Jozefowicz et al., 2015) in Figure 2, with the addition of the final softmax layer yt.", "startOffset": 102, "endOffset": 127}, {"referenceID": 25, "context": "Weiss et al. (2015) showed that large gains can be made with a grid search to tune learning hyperparameters as well embeddings sizes and hidden layer dimensions, which we did not perform due to its very high computational cost.", "startOffset": 0, "endOffset": 20}, {"referenceID": 24, "context": "We use the GloVe pre-trained embeddings produced by Pennington et al. (2014) to initialize the word vectors.", "startOffset": 52, "endOffset": 77}, {"referenceID": 20, "context": "For our experiments we use the Wall Street Journal (WSJ) section from the Penn Treebank (Marcus et al., 1993).", "startOffset": 88, "endOffset": 109}, {"referenceID": 3, "context": "The results in Table 2 show the effect of applying dropout on the input layer for our FFN baseline, when compared to the similarly sized Chen and Manning (2014) model which has 200 neurons in its hidden layer.", "startOffset": 137, "endOffset": 161}, {"referenceID": 26, "context": "Zhang et al. (2015) do not use pre-trained word vectors for their final result.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "The values given for Andor et al. (2016) and Weiss et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 0, "context": "The values given for Andor et al. (2016) and Weiss et al. (2015) reflect only the performance of the greedy FFN models produced in their work, with other improvements made explained breifly in section 1.", "startOffset": 21, "endOffset": 65}, {"referenceID": 25, "context": "to achieving very close dev score accuracy results with only a single 256 neuron hidden layer when compared to the significantly larger models of Weiss et al. (2015) with 2 layers of size 2048, and Andor et al.", "startOffset": 146, "endOffset": 166}, {"referenceID": 0, "context": "(2015) with 2 layers of size 2048, and Andor et al. (2016) with 2 layers of size 1024 layers.", "startOffset": 39, "endOffset": 59}, {"referenceID": 18, "context": "We note that our LSTM-baseline achieves a substantial improvement over the similar architecture of Kuncoro et al. (2016). The main differences in this case are a slightly larger model and using LSTMs without peephole connections.", "startOffset": 99, "endOffset": 121}, {"referenceID": 16, "context": "In addition, our bootstrapped model produces better results than all the mentioned feed forward models in addition to most of the LSTMbased approaches in Table 2, with the exception of Kiperwasser and Goldberg (2016b), despite only having a single hidden layer of LSTM units and making no use of biLSTMs, TreeLSTMs, or Stack LSTMs.", "startOffset": 185, "endOffset": 218}, {"referenceID": 16, "context": "For example, we can merge our work with that of Kiperwasser and Goldberg (2016b), by first training their model; a biLSTM input layer going to a feed-forward hidden layer followed by an output layer, and then replacing the hidden layer with an LSTM initialized with the weights of that hidden layer.", "startOffset": 48, "endOffset": 81}, {"referenceID": 9, "context": "In this section we attempted to do the same for 2 other popular forms of RNNs, the Simple Recurrent Network, otherwise known as the Elman network (Elman, 1990), and the Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 146, "endOffset": 159}, {"referenceID": 5, "context": "In this section we attempted to do the same for 2 other popular forms of RNNs, the Simple Recurrent Network, otherwise known as the Elman network (Elman, 1990), and the Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 196, "endOffset": 214}, {"referenceID": 21, "context": "(2010) on language modelling and an extended memory version of Elman networks in (Mikolov et al., 2014).", "startOffset": 81, "endOffset": 103}, {"referenceID": 19, "context": "atively well, notably the work of Mikolov et al. (2010) on language modelling and an extended memory version of Elman networks in (Mikolov et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 9, "context": "(2010) on language modelling and an extended memory version of Elman networks in (Mikolov et al., 2014). Elman networks themselves are only a simple addition to the architecture of the traditional feedforward network. Whereas an FFN has a hidden layer, and Elman network has an additional context layer, that represents the output of the hidden layer in the previous time-step. In a way, it can be compared to output gate of an LSTM, without any additional tools to model the sequence. In our experiment we use the ReLU activation function once more for the hidden layer similar to Le et al. (2015), but without their initialization strategy.", "startOffset": 63, "endOffset": 599}, {"referenceID": 5, "context": "Introduced by Cho et al. (2014), GRUs are an architecture often compared to LSTMs.", "startOffset": 14, "endOffset": 32}, {"referenceID": 6, "context": "Despite this apparently simpler structure, Chung et al. (2014) found GRUs to outperform LSTMs on a number of tasks, and Jozefowicz et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 6, "context": "Despite this apparently simpler structure, Chung et al. (2014) found GRUs to outperform LSTMs on a number of tasks, and Jozefowicz et al. (2015) also found that GRUs can beat LSTMs except in language modelling.", "startOffset": 43, "endOffset": 145}, {"referenceID": 6, "context": "Despite this apparently simpler structure, Chung et al. (2014) found GRUs to outperform LSTMs on a number of tasks, and Jozefowicz et al. (2015) also found that GRUs can beat LSTMs except in language modelling. However, Jozefowicz et al. (2015) also found that initializing the LSTM forget gate bias bf to 1 allowed the LSTM to almost match the performance of the GRU on other tasks.", "startOffset": 43, "endOffset": 245}, {"referenceID": 24, "context": "For each RNN type we trained 2 FFN and RNN baselines, one with GloVe pre-trained word embeddings(Pennington et al., 2014) and another with randomly initialized embeddings.", "startOffset": 96, "endOffset": 121}, {"referenceID": 2, "context": "The improvement achieved by a bootstrapped Elmanmodel can thus be explained by the fact that it suffers from gradient vanishing (Bengio et al., 1994), and so sequence specific information does not affect training to the extent that it does in GRUs.", "startOffset": 128, "endOffset": 149}], "year": 2017, "abstractText": "We present a simple LSTM-based transition-based dependency parser. Our model is composed of a single LSTM hidden layer replacing the hidden layer in the usual feed-forward network architecture. We also propose a new initialization method that uses the pre-trained weights from a feed-forward neural network to initialize our LSTM-based model. We also show that using dropout on the input layer has a positive effect on performance. Our final parser achieves a 93.06% unlabeled and 91.01% labeled attachment score on the Penn Treebank. We additionally replace LSTMs with GRUs and Elman units in our model and explore the effectiveness of our initialization method on individual gates constituting all three types of RNN units.", "creator": "LaTeX with hyperref package"}}}