{"id": "1708.09121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Interpretable Categorization of Heterogeneous Time Series Data", "abstract": "Explaining heterogeneous multivariate time series data is a key issue in many applications, and the problem requires addressing two major data mining challenges at the same time: learning models that can be interpreted by humans, and extracting heterogeneous multivariate time series data. The intersection of these two areas is not sufficiently explored in existing literature. To bridge this gap, we propose grammar-based decision trees and an algorithm for learning them. Grammar-based decision trees add a grammatical framework to decision trees. Logical expressions derived from context-free grammar are used instead of simple thresholds for attributes. The added expressivity allows support for a wide range of data types while maintaining the interpretability of decision trees. By choosing a grammar based on time logic, we show that gram-based decision trees can be used for interpretable classification of high-dimensional heterogeneous and time series data.", "histories": [["v1", "Wed, 30 Aug 2017 05:21:26 GMT  (5015kb,D)", "http://arxiv.org/abs/1708.09121v1", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ritchie lee", "mykel j kochenderfer", "ole j mengshoel", "joshua silbermann"], "accepted": false, "id": "1708.09121"}, "pdf": {"name": "1708.09121.pdf", "metadata": {"source": "CRF", "title": "Interpretable Categorization of Heterogeneous Time Series Data", "authors": ["Ritchie Lee", "Mykel J. Kochenderfer", "Ole J. Mengshoel", "Joshua Silbermann"], "emails": ["ole.mengshoel}@sv.cmu.edu", "mykel@stanford.edu", "joshua.silbermann@jhuapl.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nAirborne collision avoidance systems are mandated worldwide on all large transport and cargo aircraft to help prevent mid-air collisions. Their operation have played a crucial role in the high level of safety in the national airspace [1]. To address the growing needs of the national airspace, the Federal Aviation Administration (FAA) has decided to develop a new aircraft collision avoidance system. The next-generation Airborne Collision Avoidance System (ACAS X) is currently being developed and tested and promises a number of potential improvements over current systems including a reduction in collision risk while simultaneously reducing the number of unnecessary alerts [2].\nOne of the primary safety metrics of airborne collision avoidance systems is the likelihood of near mid-air collision (NMAC), defined as two aircraft coming closer than 500 feet horizontally and 100 feet vertically. Efficient algorithms have been developed to generate large datasets of NMAC and non-NMAC instances in simulation [3]. However, while it is straightforward to observe that an NMAC has occurred, discovering and categorizing relevant properties of NMACs is much more challenging. Understanding how NMAC events occur is important for both validation and informing development of ACAS X.\nThe analysis of NMAC encounters faces two major challenges that need to be addressed simultaneously: Learning models that are human-interpretable and mining of heterogeneous multivariate time series data. Aircraft data is naturally a high-dimensional heterogeneous time series, where the variables are a mix of numeric, Boolean, and categorical types. However, there is a dearth of algorithms that can handle this class of data. The second challenge is the need for interpretability. That is, humans must be able to understand and reason about the information captured by the model. Interpretability is essential for domain experts to validate the model\u2019s output, understand its limitations, and build trust in the system. In knowledge discovery applications, such as analysis of NMAC encounters, the explanation is the reason for the analysis. While the above problems have been addressed separately in the literature, we are not aware of any solutions that address both problems simultaneously.\nIn this paper, we present grammar-based decision tree (GBDT), a framework that extends decision trees with a grammar framework for interpretable classification. Traditional decision trees partition the input space using simple thresholds on attributes, such as (x1 < 2) [9][10]. However, these rules have limited expressiveness and cannot be used to express more complex logical relationships, such as those between het-\nar X\niv :1\n70 8.\n09 12\n1v 1\n[ cs\n.L G\n] 3\nerogeneous attributes or across time. To address this problem, GBDT allows arbitrary logical expressions as decision rules. Any logical expression can be used so long as the evaluation of it produces a Boolean result. The domain of the logical expressions is specified using a context-free grammar (CFG) where the user can also introduce domain knowledge and application-specific tailoring. By choosing a grammar based on temporal logic, GBDTs can be used for classifying and explaining high-dimensional and heterogeneous time series data. Moreover, GBDTs can also be used for categorization, which is the combined task of clustering data into similar groups and providing intuitive labels for them. Categorizations are produced naturally by the decision tree at no additional cost.\nWe present an induction algorithm for GBDTs that employs grammar-based expression search (GBES) as a subroutine to search for good partitioning expressions. Four existing GBES algorithms are considered resulting in four variations of the GBDT induction algorithm.\nWe show the results of GBDT on two applications. To validate our approach, we apply GBDT on the classic Australian Sign Language dataset from the UC Irvine (UCI) machine learning repository [41] and show that the generated explanations are reasonable. Finally, we apply GBDT to our motivating application. We analyze a dataset of simulated aircraft encounters to explain why certain encounters end in near mid-air collision while others do not. We first compare all four GBDT variants to see which one performs best on this dataset. Then we discuss the categories and their properties found by the algorithm.\nThe remainder of the paper is organized as follows. Section II reviews related literature on interpretable models and time series analysis. Section III reviews notation and terminology, CFGs as well as four GBES algorithms. Section IV presents GBDT and shows how the model can be used for both classification and categorization. Section V presents a general induction algorithm for GBDTs. Section VI describes our experiments in applying GBDT to the classic Australian Sign Language dataset. Finally, Section VII evaluates four variants of the GBDT induction algorithm on a simulated aircraft encounter dataset and presents results of applying GBDT to study near mid-air collisions."}, {"heading": "II. RELATED WORK", "text": "The authors are not aware of any existing interpretable data mining approaches for multivariate heterogenous time series datasets. We review the relevant works in the most related topics of learning interpretable models and multivariate time series analysis.\nA variety of interpretable models for static data have been proposed in the literature. Regression models [4], generalized additive models [5][6], and Bayesian case models [7] have been recently proposed as models with interpretability. These models improve interpretability by stating decision boundaries in terms of basis functions or representative instances (proto-\ntypes). Bayesian networks have also been used for prediction and data understanding [8].\nRule-based models, such as decision trees [9][10], decision lists [11], and decision sets [12], are easy to understand because decision boundaries are stated in terms of input attributes and simple logical operators. Decision trees partition the data using a tree structure and decision lists use an if-then-else branching structure. Decision sets use independent decision rules to reduce coupling between rules. Efficient algorithms have been proposed for inducing these models by first using associative rule mining (ARM) to mine a set of interesting rules from the data, then optimizing over combinations of these rules to induce classifiers [13][14][12].\nGenetic programming (GP) has been studied extensively for classification problems [15]. GP is particularly well-suited to evolve tree structures [16][17]. A number of studies have used GP to evolve interpretable models for analyzing medical datasets [18][19]. Grammar-guided genetic programming (GGGP) uses a grammar to guide the evolution of genetic programs [20][21]. A variety of classification structures, including decision trees, have been evolved using GGGP [22].\nInductive logic programming (ILP) uses logic programming to find a set of logical implications (Horn clauses) that best explains the data given a set of known facts [23][24]. ILP has been previously used for interpretable classification of static data.\nTime series analysis has focused on dynamic time warping, hidden Markov models, dictionary-based approaches, and recurrent neural networks [25][26][27]. However, these methods do not provide interpretable output.\nShapelets [28] and subsequence search [29] have been proposed for univariate time series classification. The approach searches for simple patterns that are most correlated with the class label. Interpretability comes from identifying a prototype of the recurring subsequence pattern. Implication rules [30] and simple logical combinations of shapelets [31] have been proposed to extend the shapelets approach.\nThe combination of decision trees and GGGP has been proposed in the past [32][33]. However, there are a number of important differences between these works and GBDT. These prior works use GGGP to optimize entire traditional decision trees where the splits are simple thresholds over individual attributes. In contrast, the splits in GBDT are temporal logic expressions and GGGP (or more generally GBES) is used to optimize individual splits. This distinction is crucially important because the added expressivity of the logical expression is what allows support for multivariate heterogeneous time series datasets."}, {"heading": "III. PRELIMINARIES", "text": ""}, {"heading": "A. Notation and Terminology", "text": "A multi-dimensional time series dataset D consists of m records, where each record is a two-dimensional matrix of n attributes by T time steps. A trace of an attribute xi is denoted ~xi and is a vector of length T that represents the time series of that attribute. A label is associated with each data\nrecord. Logical and comparison operators are given broadcast semantics where appropriate. For example, the comparison operator in ~xi < c compares each element of ~xi to c and returns a vector of the same size as ~xi. Similarly, the logical operator in ~xi\u2227~xj operates elementwise. The temporal operators F and G are eventually and globally, respectively. Eventually returns true if any value in the input vector is true. Globally returns true if all values in the input vector are true. A trial is a run of an experiment initialized with a unique random seed."}, {"heading": "B. Context-Free Grammar", "text": "A context-free grammar (CFG) defines a set of rules that govern how to form expressions in a formal language, such as linear temporal logic (LTL) [34]. The grammar defines the syntax of the language as well as provides a direct means to generate valid expressions.\nA CFG G is defined by a 4-tuple (N , T ,P,S), where \u2022 N is a set of nonterminal symbols, which are symbols\nthat can be replaced by other symbols, \u2022 T is a set of terminal symbols, which are symbols\nthat will appear in the final expression generated by the grammar, \u2022 P is a set of productions, which are rules for replacing a nonterminal symbol with other nonterminal or terminal symbols, and \u2022 S is the start symbol, which is a special nonterminal symbol that serves as the starting expression of a derivation.\nTo generate an expression from the grammar, we begin with the start symbol, then repeatedly apply production rules to rewrite the expression until no more nonterminals remain. When applying a rule, a nonterminal in the expression is replaced with a substitution defined in a production rule. The rules can be applied any number of times and in any order. The expression is complete when the expression consists of only terminal symbols. The derivation is commonly represented as a tree structure called a derivation tree.\nFormally, grammars define only the syntax and not the semantics of expressions. However, for convenience, we shall assume that the semantics of the language are defined and use the term \u201cgrammar\u201d loosely to refer to both the grammar and its associated semantics."}, {"heading": "C. Grammar-Based Expression Search", "text": "Grammar-based expression search (GBES) is the problem of finding expressions from a grammar that minimize a given fitness function [21]. The formulation is extremely general due to the flexibility and expressiveness of grammars and the arbitrary choice of fitness function. Owing to this generality, the GBES approach has been applied to a wide variety of applications, including image and signal processing, modeling of medical and economic data, and industrial process control [35]. A number of GBES algorithms have been proposed in the literature. We review four of these algorithms in the following sections.\n1) Monte Carlo: Monte Carlo is a simple algorithm that generates expressions by repeatedly selecting nonterminals in the expression and randomly choosing substitutions to apply. Substitutions are chosen uniformly from all available possibilities. When no nonterminals remain, the fitness of the generated expression is evaluated and the expression with the best fitness is reported.\nSince Monte Carlo search is undirected, identical expressions may be evaluated many times, which can be expensive. Furthermore, due to the nature of the sampling, shallower paths will get sampled more frequently than deeper paths. Deep paths may have a vanishingly small probability of being reached [35].\n2) Monte Carlo Tree Search: Monte Carlo tree search (MCTS) is a heuristic search algorithm that is used to optimize certain sequential decision-making problems [36][37]. MCTS is based on reinforcement learning, the idea of improving behavior through experience and interaction with an environment [38].\nExpression search is formulated as a sequential decisionmaking problem by transforming the decisions in the derivation tree so that they occur in sequential order, for example, by assuming depth-first traversal order [39]. The \u201cstate\u201d of the system is the expression derived so far and the \u201caction\u201d is the selection of which substitution to apply to the selected nonterminal. MCTS is then applied to optimize the resulting sequential decision-making problem [39].\n3) Grammatical Evolution: Grammatical evolution (GE) [40] is a GGGP algorithm that is based on a sequential representation of the derivation tree. Specifically, GE defines a transformation from a variable-length binary string to a sequence of rule selections in a CFG. Then it uses a standard genetic algorithm (GA) to search over binary strings [35]. Our implementation uses one-point crossover and uniform mutation [35].\n4) Genetic Programming: Genetic programming (GP) is an evolutionary algorithm for optimizing trees [20]. Genetic operators are defined specifically for trees and thus do not require any transformations of the derivation tree. Our implementation uses a crossover operator that exchanges compatible subtrees between two individuals and a mutation operator that replaces entire subtrees with randomly-generated ones. We also use tournament selection for selecting individuals."}, {"heading": "IV. GRAMMAR-BASED DECISION TREES", "text": "Grammar-based decision tree (GBDT) extends decision trees with a grammar framework to allow general logical expressions to be used as the branching rules in a decision tree. The domain of the logical expressions is constrained using a context-free grammar (CFG). In this paper, we consider grammars based on temporal logic for the classification of high-dimensional and heterogeneous time series data. The grammar can be easily adapted to the characteristics of the data or the application."}, {"heading": "A. Prediction and Categorization", "text": "As in a traditional decision tree, the logical expressions are organized as nodes in a tree where each child branch corresponds to one outcome of the parent\u2019s expression. Class label prediction is performed in the traditional manner. Starting at the root of the tree, we recursively evaluate each logical expression. The result of the expression dictates which child branch to follow and the process continues. When a leaf node is reached, the most frequently seen class label in the training data at that leaf node is returned. Predictions for both previously seen and unseen data can be made in this manner.\nAn added benefit of using a decision tree structure is the ability to extract a categorization from the model at no additional cost. Categorization can be helpful in building intuition and explaining data. A GBDT can be used to categorize data by considering each leaf node of the tree to be a separate category. To describe the category, a global expression for a node can be derived by forming the conjunction of all branch expressions along the path that connects the root and the node of interest. The members of the cluster are the records where the cluster\u2019s global expression holds. Since partitions are mutually exclusive, the clusters do not overlap.\nFigure 1 shows an example of a GBDT with accompanying CFG expressed in Backus-Naur Form (BNF). The CFG in this example describes a simple temporal logic. It assumes that the data has four attributes ~x1, ~x2, ~x3, ~x4, where the attributes ~x1 and ~x2 are Boolean time-series vectors and ~x3 and ~x4 are time-series vectors of real numbers. The grammar contains two types of rules. Expression rules, such as EV ::= F (VB), contain partially-formed expressions that contain terminal and non-terminal symbols. Non-terminal symbols are substituted further using the appropriate production rules. Or Rules, such as B ::= EV | GL, contain the symbol |, which is used to delineate the different possible substitutions.\nEach non-leaf node of the decision tree contains a logical expression that governs which child branch is followed. Leaf nodes show the predicted class label. For example, a data record where F (~x1\u2227~x2) is true and G(~x3 < 5) is false would be predicted to have class label 2. The example also shows the unique cluster numbers for each leaf node. For example, the righmost leaf node in Figure 1a is labeled as cluster 4. The global expression for cluster 4 is \u00acF (~x1\u2227~x2)\u2227\u00acF (~x1\u2227(~x3 < 2)) and the members of cluster 4 are those records in the data where that global expression holds."}, {"heading": "B. Grammars for Heterogeneous Time Series", "text": "Grammar design is crucial to the effectiveness of GBDT. The ideal grammar should produce expressions that are interpretable, tailored to the application, and support the attribute types present in the dataset. This section aims to offer design suggestions to the user to achieve these goals. The exact details of the grammar will depend on the specific needs of the user.\nIn the GBDT framework, expressions are evaluated on a data record and produce a Boolean output. The symbols of the expression can refer to fields of the record, constants, or functions. We adopt a subset of linear temporal logic (LTL), a\nformal logic with temporal operators often used in the analysis of time-series data [34]. We have found grammars similar to the one presented in Figure 1 to be particularly effective for heterogeneous time series data. The grammar produces expressions that integrate Boolean, categorical, and real-valued data types nicely and the expressions use simple well-known operators, which are very intuitive.\nWhen designing a grammar, it is important to manage data types. For example, the grammar in Figure 1 is organized by the data type of its non-terminals. The production rule B selects amongst functions that return a Boolean, whereas the production rule VB selects amongst functions and expressions that return a vector of Booleans. Similarly, the non-terminal VR represents a vector of real values while C represents a constant. Organization of types is not only important to provide functions with valid inputs but also key to meaningfully nest and combine heterogeneous types.\nTo keep the example simple, Figure 1 included only a small number of operators. The grammar can be readily extended to include a much wider set of operators including disjunct \u2228, greater than >, equals =, where equality is important for categorical attributes, and even arbitrary user-defined functions."}, {"heading": "C. Natural Language Descriptions", "text": "Logical expressions can sometimes be dense and hard to parse. In many cases, we can improve interpretability by additionally providing natural language descriptions of the expressions to the user. One method to automatically translate expressions into English sentences is to map expression rules and terminal symbols in the CFG to corresponding sentence fragments and then use the structure of the expression\u2019s derivation tree to assemble the fragments. Figure 2 shows an example of a mapping from expressions and symbols to sentence fragments that could be used with the grammar in Figure 1. As an example, we have given English descriptions to each data attribute as well.\nApplying this mapping, the decision rules in Figure 1 can be transformed into the following natural language descriptions: F (~x1\u2227~x2) is translated to \u201cat some point, [advisory is active] and [pilot is responding]\u201d; G(~x3 < 5) is translated to \u201cfor\nall time, [vertical rate] is less than 5\u201d; and F (~x1 \u2227 (~x3 < 2)) is translated to \u201cat some point, [advisory is active] and [[vertical rate] is less than 2]\u201d. We include square brackets in the sentence to help the reader disambiguate nested sentence components.\nV. INDUCTION OF GBDTS\nInduction of a GBDT is performed top-down as in traditional decision tree learning. However, the major difference is the use of grammar-based expression search (GBES) as a subroutine to find the best partition expression. The induction algorithm begins with a single (root) node containing all data records. GBES is then used to search a CFG for the partitioning expression that yields the best fitness. The expression is evaluated on each record and the data is partitioned into two child nodes according to the results of the evaluation. The process is applied recursively to each child until all data records at the node are either correctly classified or a maximum tree depth is reached. The mode of the training labels is used for class label prediction at a leaf node. The GBDT induction algorithm is shown in Algorithm 1.\nAlgorithm 1 Grammar-Based Decision Tree Induction 1: . Inputs: CFG G, Fitness Function F , Data D, Labels L, 2: Depth d 3: function GBDT(G,F,D,L, d) 4: R\u2190 SPLIT(G,F,D,L, d) 5: return TREE(R) 6: function SPLIT(G,F,D,L, d) 7: if ISHOMOGENEOUS(L) or d = 0 then 8: return LEAFNODE(MODE(L)) 9: E \u2190 EXPRESSIONSEARCH(G,F,D,L) 10: (D+, D\u2212)\u2190 SPLITDATA(D,E) 11: (L+, L\u2212)\u2190 TRUTHLABELS(D+, D\u2212) 12: child+ \u2190 SPLIT(G,F,D+, L+, d\u2212 1) 13: child\u2212 \u2190 SPLIT(G,F,D\u2212, L\u2212, d\u2212 1) 14: return NODE(E, child+, child\u2212)\nGBDT is the main entry point to the induction algorithm. It returns a TREE object containing the root node of the induced decision tree. SPLIT attempts to partition the data into two\nparts. It first tests whether the terminal conditions are met and if so returns a LEAFNODE object that predicts the mode of the labels. The partitioning terminates if the maximum depth has been reached or if all class labels are the same, which is tested by the ISHOMOGENEOUS function. EXPRESSIONSEARCH uses GBES to search for the expression that induces the best split. SPLITDATA evaluates the expression on each data record and partitions the data into two parts according to whether the expression holds. TRUTHLABELS performs a lookup for the class labels. Then, SPLIT is called recursively on each part. SPLIT returns a NODE object containing the decision expression and the children of the node."}, {"heading": "A. Fitness Function", "text": "We evaluate the desirability, or fitness, of an expression according to two competing objectives. On one hand, we want expressions that split the data so that the resulting clusters have the same ground truth class labels. Splits that induce high homogeneity tend to produce shallower trees and thus shorter global expressions at leaf nodes. They also produce classifiers with better predictive accuracy when the maximum tree depth is limited. To quantify homogeneity, we use the Gini impurity metric following the classification and regression tree (CART) framework [9]. On the other hand, we want to encourage interpretability by minimizing the length and complexity of the expression. Shorter and simpler expressions are generally easier to interpret. We use the number of nodes in the derivation tree as a proxy for the complexity of an expression. The two objectives are combined linearly into a single (minimizing) fitness function given by\nF (E) = w1IG(E) + w2NE\nwhere F is the fitness function, w1 and w2 are weights, and NE is the number of nodes in the derivation tree of E. The total Gini impurity, IG(E), is the sum of the Gini impurity of each group that results from splitting the data using expression E. It is given by\nIG(E) = \u2211\nL\u2208{L+,L\u2212} \u2211 b\u2208B f bL(1\u2212 f bL)\nwhere f bL is the fraction of elements in L that are equal to b, L+ are the labels of the records where E evaluates to true, L\u2212 are the labels of the records where E evaluates to false, and B = {True, False}."}, {"heading": "B. Computational Complexity", "text": "The most computationally expensive part of GBDT is evaluating the fitness of an expression since it involves visiting each record in the dataset then computing statistics. Furthermore, GBES requires a large number of expression evaluations to optimize the decision expression at each decision node. The deeper the tree, the more nodes that need to be optimized. However, as the tree gets deeper, the nodes operate on increasingly smaller fractions of the dataset. In fact, while the number of decision nodes grows exponentially with tree depth, the number of records that must be evaluated at each level remains\nconstant (the size of the dataset). Overall, the computational complexity of GBDT induction is O(|D| \u00b7NGBES \u00b7 d), where |D| is the number of records in the dataset, NGBES is the number of evaluations used in GBES, and d is the depth of the decision tree."}, {"heading": "VI. AUSTRALIAN SIGN LANGUAGE", "text": "To test our approach, we analyze the classic Australian Sign Language (\u201cAuslan\u201d) dataset from the UC Irvine (UCI) repository [41][25]. The data originates from participants wearing instrumented gloves while signing specific words in Australian sign language. The dataset is a multivariate time series of varying episode length containing 22 continuous features, 95 words with 27 examples each.\nWe use a generic temporal logic grammar that includes all features and the following operators: F , G, =\u21d2 , \u00ac, \u2228, \u2227, =, <, \u2264, >, \u2265. Since features may have different ranges, constant values used in comparison operators must be specialized to each feature. To address this issue, we consider the range of each feature in the dataset and discretize uniformly. Features are compared only with their corresponding set of discretized values.\nWe extract the following eight classes from the dataset hello, please, yes, no, right, wrong, same, different. These words were chosen because videos of the words being signed were readily available to the author. We performed 20 random trials using 70% of the data for training and 30% for testing. GBDT achieved an average testing accuracy of 96.3%. Full training accuracy is always achieved due to the nature of the decision tree. The average number of rules needed to fully partition the data is 8.6. We use genetic programming (GP) to optimize the expressions in the GBDT.\nThe strength of GBDT lies in its ability to categorize and explain data. GBDT not only provides interpretable expressions about the data\u2019s distinguishing properties but also a hierarchical organization of the properties. Figure 3 shows the resulting GBDT from the best-performing trial. We compare the learned model to the videos of the signed words to intuitively verify correctness. At the root node, GBDT partitions same and different from the other words identifying the fully bent left pinky as the distinguishing property. Subsequently, different is distinguished from same by looking at whether the right palm is ever facing upwards. Of the remaining words, hello is isolated using the fully straight middle finger and then wrong is identified using the straight right pinky. The remaining four words are grouped right with yes and please with no using the yaw position of the right hand. The word yes is distinguished from right by the bent right thumb. Lastly, no is distinguished from please using the combined property on right forefinger bend, right yaw angle, and x position of the right hand."}, {"heading": "VII. COLLISION AVOIDANCE APPLICATION", "text": "Airborne Collision Avoidance System (ACAS X) monitors the airspace of an aircraft and issues alerts to the pilot if a conflict is detected. A resolution advisory (RA) is issued to\nhelp the pilot resolve the conflict, for example, instructing the pilot to climb at 1500 feet per minute. The collision avoidance system may revise an RA as the encounter unfolds. The pilot delays for five seconds before responding to an initial RA and three seconds before responding to subsequent RAs [3].\nWe apply GBDT to analyze simulated aircraft encounters to discover the most predictive properties of near mid-air collisions (NMACs) and categorize encounters according to those properties. The results of our study are used to help the ACAS X development team better organize and understand the NMACs and inform development."}, {"heading": "A. Dataset", "text": "We analyze a dataset that contains simulation logs from an aircraft encounter simulator modeling a two-aircraft mid-air encounter [3]. Components in the simulator include sensors, pilot response, aircraft dynamics, and a development prototype of ACAS X. The dataset contains 10,000 encounters with 863 NMACs and 9137 non-NMACs. The class imbalance is due to the rarity of NMACs and the difficulty in generating NMAC encounters. We provide GBDT with the entire dataset so that the algorithm can learn from a larger set of examples. Each encounter has 38 attributes collected at 1Hz for 50 time steps. The attributes are of mixed type that include numeric, categorical, and Boolean types. The attributes include the state of the aircraft, pilot commands, and the state and output of the collision avoidance system for each aircraft.\nSince the NMAC condition (horizontal separation < 500 feet and vertical separation < 100 feet) is directly observable in the data, GBDT will split on this rule because it has the best fitness. While correct, the result does not provide useful information about the NMACs. We address the issue by filtering the NMAC event from the encounters. For each encounter, we compute the time of closest approach, which\nis the point where the separation of the two aircraft reaches a minimum, then trim the encounter to retain only data from the start of the encounter to five seconds prior to that time. Five seconds strike a good balance between removing NMAC information and leaving sufficient encounter data for prediction."}, {"heading": "B. Grammar", "text": "We crafted a custom CFG for the ACAS X dataset building on the grammar presented in Figure 1. We include temporal logic operators eventually F and globally G; elementwise logical operators conjunct \u2227, disjunct \u2228, negation \u00ac, and implies =\u21d2 ; comparison operators less than <, less than or equal to \u2264, greater than >, greater than or equal to \u2265, and equal =; mathematical functions absolute value |x|, difference \u2212, and sign SIGN; and computing function count COUNT (which returns the number of true values in a vector of Booleans).\nIn addition to dividing the attributes by data type, the ACAS X grammar further subdivides the attributes by their physical representations. The reason is to be able to compare attributes with constant values that have appropriate scale and resolution. For example, even though aircraft heading and vertical rate are both real-valued attributes, aircraft heading should be compared to values in the range of \u2212180\u25e6 to 180\u25e6, whereas vertical rate should be compared to values in the range of \u221280 feet per second to 80 feet per second."}, {"heading": "C. Comparison of Induction Algorithms", "text": "We study the performance of GBDT when used together with Monte Carlo, Monte Carlo tree search (MCTS), grammatical evolution (GE), and GP as subroutines. We evaluate the algorithms based on classification performance and interpretability of the produced models.\n1) Classification Performance: We evaluate the classification performance of the models on the ACAS X dataset and report accuracy, precision, recall, and F1-score. A decision tree can always achieve perfect classification performance on a training set given a sufficient number of splits. However, large and deep trees are harder to interpret than smaller ones. These experiments limit the maximum tree depth to four.\n2) Interpretability Metrics: We consider various metrics for quantifying the interpretability of the models. These metrics aim to capture the size and complexity of various parts of the model. Intuitively, large and complex models tend to be less interpretable than smaller ones. \u2022 Average rule length. The average length of a rule\nmeasured in number of characters. In general, shorter rules are easier to interpret than longer ones. \u2022 Average rule nodes. The average number of nodes in the derivation trees of the decision expressions. This metric aims to capture the complexity of a rule rather than only its representation length. In general, derivations with smaller number of nodes in its tree are less complex and thus easier to interpret.\nWe performed 20 random trials for each algorithm and report the mean of the results in Table I. The best performing entry for each metric is highlighted. We are unaware of any competing methods that can both handle heterogeneous multivariate time series data and learn an interpretable model. As a result, we compare our results to \u201cflat\u201d GP search. That is, we perform GBES search without the decision tree structure, but augment the grammar to allow conjuncts, disjuncts, and negations of temporal expressions. To be fair, we also allocate the same total number of expression evaluations to the flat search as to GBDT.\nAccounting for the class imbalance, we observe that the flat search performs quite poorly without the help of the decision tree structure. This is not surprising as the number of expressions grows exponentially as more logical expressions are chained.\nIn our experiments, GBDT-GP produced decision trees with the highest classification accuracy and F1-score, while GBDT-MCTS produced decision trees that were shorter and less complex. Overall, we selected GBDT-GP for its better classification performance yet competitive interpretability performance."}, {"heading": "D. Categorizing Aircraft Encounters", "text": "We apply GBDT-GP to learn a categorization for the ACAS X data. A maximum tree depth of four is used. Figure 4 shows an example of the categorization results extracted from the leaf nodes of the tree. The figure shows plots of altitude versus time, which, while cannot fully capture the highdimensionality of the encounter data, is generally most visually informative in this ACAS X application since ACAS X issues RAs only in the vertical direction. Since we are primarily interested in categorizing NMACs, we consider only NMAC categories. Figure 4 shows the first five encounters for each of the six NMAC categories identified, where each row is a separate category. The categories are labeled in ascending order starting at cluster 1 at the top row. The first row has only two plots because category 1 only contained two encounters.\nThe following sections summarize the properties that distinguish each category and discuss their relevance. Because the categories are derived from leaf nodes at depth four, each category is described by the conjunction of four expressions. The hierarchical nature of GBDT means that some decision expressions are shared between categories while others are not. Due to space constraints, we restrict our discussion to a select subset of categories.\n1\n1) Category 1: In this category, the aircraft maintain altitude separation for most of the encounter, then one aircraft accelerates rapidly toward the other aircraft as they approach. The aggressive maneuvering causes vertical separation to be lost rapidly and results in an NMAC. Figure 5 shows the spike in climb rate at 34 seconds, just 5 seconds before NMAC. Textual labels in figure indicate the issued RAs. An asterisk at a time step indicates that the pilot is following active RA and a dash indicates that the pilot is following the previous RA.\nAggressive, last-minute maneuvering is known to be problematic. With the pilot\u2019s five-second response time, there is insufficient time remaining for the collision avoidance system to resolve the conflict. It is unlikely that any reasonable collision avoidance system would be able to resolve such NMACs. Fortunately, these encounters are extremely rare outside of simulation.\n2) Category 5: In these encounters, the aircraft cross in altitude early in the encounter without active advisories, then maneuver back toward each other to cause an NMAC. Since the aircraft are estimated to cross safely and appear to vertically diverge following the crossing, the collision avoidance system witholds an RA to reduce the number of unnecessary alerts. However, after crossing, the aircraft make a sudden maneuver toward each other that results in an NMAC. Because the aircraft are already close in altitude, the vertical maneuvering in these encounters is less aggressive than those in previous categories. Due to the late maneuvering and the pilot response delay, pilot 2 does not start to comply with the issued RA until within five seconds of NMAC. Figure 6 shows the vertical profile of an encounter in this category where the aircraft cross in altitude at 19 seconds and then turn back to NMAC at 38 seconds.\n3) Category 6: In these encounters, an initial RA is issued early in the encounter and the aircraft cross in altitude during the pilot\u2019s initial response delay. As the pilots start responding to their advisories, their maneuvers actually result in bringing them closer together rather than further apart. While a late revision to the advisory is given, the encounter ultimately results in an NMAC. Figure 7 shows the vertical profile of an encounter in this category. The aircraft cross in altitude during pilot 2\u2019s response delay period at 21 seconds and NMAC occurs at 39 seconds.\nIssuing advisories can be tricky in cases where the aircraft are approximately co-altitude due to the uncertainty in the aircraft\u2019s estimated positions and future intentions. In these encounters, the problem arises as one aircraft maneuvers to cross in altitude immediately after an initial RA is issued to the other aircraft."}, {"heading": "VIII. CONCLUSION", "text": "This paper introduced GBDT, a framework that combines decision trees and grammar-based expression search for interpretable classification and categorization. GBDT was developed specifically to address the need for interpretable models that can support high-dimensional and heterogeneous time\nseries data. To further improve interpretability, we showed a method to automatically generate English sentences by providing a map of subexpressions to sentence fragments. We applied GBDT to categorize an aircraft encounter dataset and showed that the method produces interpretable and insightful categories. Our approach not only partitions a dataset into similar groups, but also explains the relevant properties of each group. Our GBDT tool is used to help the ACAS X team automate the organization of large NMAC datasets and the discovery of their relevant properties. The source code for GBDT and the experiments in this paper are available online [42]."}], "references": [{"title": "The traffic alert and collision avoidance system", "author": ["J.K. Kuchar", "A.C. Drumm"], "venue": "Lincoln Laboratory Journal, vol. 16, no. 2, pp. 277\u2013296, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Nextgeneration airborne collision avoidance system", "author": ["M.J. Kochenderfer", "J.E. Holland", "J.P. Chryssanthacopoulos"], "venue": "Lincoln Laboratory Journal, vol. 19, no. 1, pp. 17\u201333, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive stress testing of airborne collision avoidance systems", "author": ["R. Lee", "M.J. Kochenderfer", "O.J. Mengshoel", "G.P. Brat", "M.P. Owen"], "venue": "Digital Avionics Systems Conference (DASC), Prague, Czech Republic, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple means to improve the interpretability of regression coefficients", "author": ["H. Schielzeth"], "venue": "Methods in Ecology and Evolution, vol. 1, no. 2, pp. 103\u2013113, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Intelligible models for classification and regression", "author": ["Y. Lou", "R. Caruana", "J. Gehrke"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, pp. 150\u2013158.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification", "author": ["B. Kim", "C. Rudin", "J.A. Shah"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1952\u2013 1960.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable causal learning for predicting adverse events in smart buildings", "author": ["A. Basak", "O. Mengshoel", "S. Hosein", "R. Martin"], "venue": "Workshops at the Thirtieth AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning, vol. 1, no. 1, pp. 81\u2013106, 1986.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning decision lists", "author": ["R.L. Rivest"], "venue": "Machine Learning, vol. 2, no. 3, pp. 229\u2013246, 1987.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "Interpretable decision sets: A joint framework for description and prediction", "author": ["H. Lakkaraju", "S. Bach", "J. Leskovec"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Orc: Ordered rules for classification a discrete optimization approach to associative classification", "author": ["D. Bertsimas", "A. Chang", "C. Rudin"], "venue": "2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "author": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "venue": "Annals of Applied Statistics, vol. 9, no. 3, pp. 1350\u20131371, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on the application of genetic programming to classification", "author": ["P.G. Espejo", "S. Ventura", "F. Herrera"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C, vol. 40, no. 2, pp. 121\u2013144, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection", "author": ["J.R. Koza"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "A study on efficient generation of decision trees using genetic programming", "author": ["T. Tanigawa", "Q. Zhao"], "venue": "Conference on Genetic and Evolutionary Computation. Morgan Kaufmann, 2000, pp. 1047\u20131052.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "Using grammar based genetic programming for data mining of medical knowledge", "author": ["P.S. Ngan", "M.L. Wong", "K.S. Leung", "J.C. Cheng"], "venue": "Genetic Programming, pp. 254\u2013259, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Gpdti: A genetic programming decision tree induction method to find epistatic effects in common complex diseases", "author": ["J.K. Estrada-Gil", "J.C. Fern\u00e1ndez-L\u00f3pez", "E. Hern\u00e1ndez-Lemus", "I. Silva- Zolezzi", "A. Hidalgo-Miranda", "G. Jim\u00e9nez-S\u00e1nchez", "E.E. Vallejo- Clemente"], "venue": "Bioinformatics, vol. 23, no. 13, pp. i167\u2013i174, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Grammatically-based genetic programming", "author": ["P.A. Whigham"], "venue": "Workshop on Genetic Programming: From Theory to Real-World Applications, no. 3, 1995, pp. 33\u201341.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Grammar-based genetic programming: A survey", "author": ["R.I. Mckay", "N.X. Hoai", "P.A. Whigham", "Y. Shan", "M. O\u2019Neill"], "venue": "Genetic Programming and Evolvable Machines, vol. 11, no. 3-4, pp. 365\u2013396, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "A comparison of classification accuracy of four genetic programming-evolved intelligent structures", "author": ["A. Tsakonas"], "venue": "Information Sciences, vol. 176, no. 6, pp. 691\u2013724, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Inverse entailment and progol", "author": ["S. Muggleton"], "venue": "New Generation Computing, vol. 13, no. 3-4, pp. 245\u2013286, 1995.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Ilp turns 20", "author": ["S. Muggleton", "L. De Raedt", "D. Poole", "I. Bratko", "P. Flach", "K. Inoue", "A. Srinivasan"], "venue": "Machine Learning, vol. 86, no. 1, pp. 3\u201323, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal classification: Extending the classification paradigm to multivariate time series", "author": ["M.W. Kadous"], "venue": "Ph.D. dissertation, The University of New South Wales, 2002.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "The great time series classification bake off: A review and experimental evaluation of recent algorithmic advances", "author": ["A. Bagnall", "J. Lines", "A. Bostrom", "J. Large", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery, vol. 31, no. 3, pp. 606\u2013660, 2017. [Online]. Available: http://dx.doi.org/10.1007/s10618-016-0483-9", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Convolutional neural networks for human activity recognition using mobile sensors", "author": ["M. Zeng", "L.T. Nguyen", "B. Yu", "O.J. Mengshoel", "J. Zhu", "P. Wu", "J. Zhang"], "venue": "Mobile Computing, Applications and Services (MobiCASE), 2014 6th International Conference on. IEEE, 2014, pp. 197\u2013205.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Time series shapelets: A novel technique that allows accurate, interpretable and fast classification", "author": ["L. Ye", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery, vol. 22, no. 1, pp. 149\u2013182, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Sax-vsm: Interpretable time series classification using sax and vector space model", "author": ["P. Senin", "S. Malinchik"], "venue": "Data Mining (ICDM). IEEE, 2013, pp. 1175\u20131180.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovery of meaningful rules in time series", "author": ["M. Shokoohi-Yekta", "Y. Chen", "B. Campana", "B. Hu", "J. Zakaria", "E. Keogh"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 1085\u20131094.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Logical-shapelets: An expressive primitive for time series classification", "author": ["A. Mueen", "E. Keogh", "N. Young"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2011, pp. 1154\u20131162.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Data Mining using Grammar Based Genetic Programming and Applications", "author": ["M.L. Wong", "K.S. Leung"], "venue": "Springer Science & Business Media,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "Grammatical evolution decision trees for detecting gene-gene interactions", "author": ["A.A. Motsinger-Reif", "S. Deodhar", "S.J. Winham", "N.E. Hardison"], "venue": "BioData mining, vol. 3, no. 1, p. 8, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "On the temporal analysis of fairness", "author": ["D.M. Gabbay", "A. Pnueli", "S. Shelah", "J. Stavi"], "venue": "ACM Symposium on Principles of Programming Languages, 1980, pp. 163\u2013173.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1980}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "European Conference on Machine Learning (ECML), 2006, pp. 282\u2013 293.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "A survey of Monte Carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1998}, {"title": "Monte-carlo expression discovery", "author": ["T. Cazenave"], "venue": "International Journal on Artificial Intelligence Tools, vol. 22, no. 01, p. 1250035, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Grammatical evolution", "author": ["M. O\u2019Neil", "C. Ryan"], "venue": "Grammatical Evolution. Springer, 2003, pp. 33\u201347.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Grammar-based decision trees (gbdts) julia package", "author": ["R. Lee"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Their operation have played a crucial role in the high level of safety in the national airspace [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "collision risk while simultaneously reducing the number of unnecessary alerts [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "Efficient algorithms have been developed to generate large datasets of NMAC and non-NMAC instances in simulation [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "on attributes, such as (x1 < 2) [9][10].", "startOffset": 35, "endOffset": 39}, {"referenceID": 37, "context": "To validate our approach, we apply GBDT on the classic Australian Sign Language dataset from the UC Irvine (UCI) machine learning repository [41] and show that the generated explanations are reasonable.", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "Regression models [4], generalized additive models [5][6], and Bayesian case models [7] have", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "Regression models [4], generalized additive models [5][6], and Bayesian case models [7] have", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Regression models [4], generalized additive models [5][6], and Bayesian case models [7] have", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "Bayesian networks have also been used for prediction and data understanding [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Rule-based models, such as decision trees [9][10], decision lists [11], and decision sets [12], are easy to understand be-", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "Rule-based models, such as decision trees [9][10], decision lists [11], and decision sets [12], are easy to understand be-", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "Rule-based models, such as decision trees [9][10], decision lists [11], and decision sets [12], are easy to understand be-", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Efficient algorithms have been proposed for inducing these models by first using associative rule mining (ARM) to mine a set of interesting rules from the data, then optimizing over combinations of these rules to induce classifiers [13][14][12].", "startOffset": 232, "endOffset": 236}, {"referenceID": 11, "context": "Efficient algorithms have been proposed for inducing these models by first using associative rule mining (ARM) to mine a set of interesting rules from the data, then optimizing over combinations of these rules to induce classifiers [13][14][12].", "startOffset": 236, "endOffset": 240}, {"referenceID": 9, "context": "Efficient algorithms have been proposed for inducing these models by first using associative rule mining (ARM) to mine a set of interesting rules from the data, then optimizing over combinations of these rules to induce classifiers [13][14][12].", "startOffset": 240, "endOffset": 244}, {"referenceID": 12, "context": "Genetic programming (GP) has been studied extensively for classification problems [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "GP is particularly well-suited to evolve tree structures [16][17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "GP is particularly well-suited to evolve tree structures [16][17].", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "A number of studies have used GP to evolve interpretable models for analyzing medical datasets [18][19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "A number of studies have used GP to evolve interpretable models for analyzing medical datasets [18][19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "Grammar-guided genetic programming (GGGP) uses a grammar to guide the evolution of genetic programs [20][21].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "Grammar-guided genetic programming (GGGP) uses a grammar to guide the evolution of genetic programs [20][21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "A variety of classification structures, including decision trees, have been evolved using GGGP [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "Inductive logic programming (ILP) uses logic programming to find a set of logical implications (Horn clauses) that best explains the data given a set of known facts [23][24].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "Inductive logic programming (ILP) uses logic programming to find a set of logical implications (Horn clauses) that best explains the data given a set of known facts [23][24].", "startOffset": 169, "endOffset": 173}, {"referenceID": 22, "context": "Time series analysis has focused on dynamic time warping, hidden Markov models, dictionary-based approaches, and recurrent neural networks [25][26][27].", "startOffset": 139, "endOffset": 143}, {"referenceID": 23, "context": "Time series analysis has focused on dynamic time warping, hidden Markov models, dictionary-based approaches, and recurrent neural networks [25][26][27].", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "Time series analysis has focused on dynamic time warping, hidden Markov models, dictionary-based approaches, and recurrent neural networks [25][26][27].", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "Shapelets [28] and subsequence search [29] have been proposed for univariate time series classification.", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "Shapelets [28] and subsequence search [29] have been proposed for univariate time series classification.", "startOffset": 38, "endOffset": 42}, {"referenceID": 27, "context": "Implication rules [30] and simple logical combinations of shapelets [31] have been proposed to extend the shapelets approach.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "Implication rules [30] and simple logical combinations of shapelets [31] have been proposed to extend the shapelets approach.", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "The combination of decision trees and GGGP has been proposed in the past [32][33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "The combination of decision trees and GGGP has been proposed in the past [32][33].", "startOffset": 77, "endOffset": 81}, {"referenceID": 31, "context": "A context-free grammar (CFG) defines a set of rules that govern how to form expressions in a formal language, such as linear temporal logic (LTL) [34].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "Grammar-based expression search (GBES) is the problem of finding expressions from a grammar that minimize a given fitness function [21].", "startOffset": 131, "endOffset": 135}, {"referenceID": 32, "context": "2) Monte Carlo Tree Search: Monte Carlo tree search (MCTS) is a heuristic search algorithm that is used to optimize certain sequential decision-making problems [36][37].", "startOffset": 160, "endOffset": 164}, {"referenceID": 33, "context": "2) Monte Carlo Tree Search: Monte Carlo tree search (MCTS) is a heuristic search algorithm that is used to optimize certain sequential decision-making problems [36][37].", "startOffset": 164, "endOffset": 168}, {"referenceID": 34, "context": "MCTS is based on reinforcement learning, the idea of improving behavior through experience and interaction with an environment [38].", "startOffset": 127, "endOffset": 131}, {"referenceID": 35, "context": "Expression search is formulated as a sequential decisionmaking problem by transforming the decisions in the derivation tree so that they occur in sequential order, for example, by assuming depth-first traversal order [39].", "startOffset": 217, "endOffset": 221}, {"referenceID": 35, "context": "MCTS is then applied to optimize the resulting sequential decision-making problem [39].", "startOffset": 82, "endOffset": 86}, {"referenceID": 36, "context": "3) Grammatical Evolution: Grammatical evolution (GE) [40] is a GGGP algorithm that is based on a sequential representation of the derivation tree.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "4) Genetic Programming: Genetic programming (GP) is an evolutionary algorithm for optimizing trees [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "formal logic with temporal operators often used in the analysis of time-series data [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 37, "context": "To test our approach, we analyze the classic Australian Sign Language (\u201cAuslan\u201d) dataset from the UC Irvine (UCI) repository [41][25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 22, "context": "To test our approach, we analyze the classic Australian Sign Language (\u201cAuslan\u201d) dataset from the UC Irvine (UCI) repository [41][25].", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "The pilot delays for five seconds before responding to an initial RA and three seconds before responding to subsequent RAs [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "We analyze a dataset that contains simulation logs from an aircraft encounter simulator modeling a two-aircraft mid-air encounter [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 38, "context": "The source code for GBDT and the experiments in this paper are available online [42].", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "The explanation of heterogeneous multivariate time series data is a central problem in many applications. The problem requires two major data mining challenges to be addressed simultaneously: Learning models that are humaninterpretable and mining of heterogeneous multivariate time series data. The intersection of these two areas is not adequately explored in the existing literature. To address this gap, we propose grammar-based decision trees and an algorithm for learning them. Grammar-based decision tree extends decision trees with a grammar framework. Logical expressions, derived from context-free grammar, are used for branching in place of simple thresholds on attributes. The added expressivity enables support for a wide range of data types while retaining the interpretability of decision trees. By choosing a grammar based on temporal logic, we show that grammar-based decision trees can be used for the interpretable classification of high-dimensional and heterogeneous time series data. In addition to classification, we show how grammar-based decision trees can also be used for categorization, which is a combination of clustering and generating interpretable explanations for each cluster. We apply grammar-based decision trees to analyze the classic Australian Sign Language dataset as well as categorize and explain near midair collisions to support the development of a prototype aircraft collision avoidance system.", "creator": "LaTeX with hyperref package"}}}