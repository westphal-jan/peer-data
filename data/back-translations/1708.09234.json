{"id": "1708.09234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Fighting with the Sparsity of Synonymy Dictionaries", "abstract": "Graph-based synset induction methods, such as MaxMax and Watset, induce synsets by performing a global clustering of a synonymic graph. However, such methods are sensitive to the structure of the input synonym graph: the input dictionary's incompleteness can significantly reduce the quality of the extracted synsets. In this article, we propose two different approaches to reduce the incompleteness of the input dictionaries: the first performs pre-processing of the graph by adding missing edges, while the second performs post-processing by merging similar synset clusters. We evaluate these approaches using two datasets for the Russian language and discuss their impact on the performance of the synset induction methods. Finally, we conduct a comprehensive error analysis of each approach and discuss prominent alternative methods to address the problem of synonymic dictionaries \"insufficiency.", "histories": [["v1", "Wed, 30 Aug 2017 12:29:04 GMT  (33kb)", "http://arxiv.org/abs/1708.09234v1", "In Proceedings of the 6th Conference on Analysis of Images, Social Networks, and Texts (AIST'2017): Springer Lecture Notes in Computer Science (LNCS)"]], "COMMENTS": "In Proceedings of the 6th Conference on Analysis of Images, Social Networks, and Texts (AIST'2017): Springer Lecture Notes in Computer Science (LNCS)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dmitry ustalov", "mikhail chernoskutov", "chris biemann", "alexander panchenko"], "accepted": false, "id": "1708.09234"}, "pdf": {"name": "1708.09234.pdf", "metadata": {"source": "CRF", "title": "Fighting with the Sparsity of Synonymy Dictionaries for Automatic Synset Induction", "authors": ["Dmitry Ustalov", "Mikhail Chernoskutov", "Chris Biemann", "Alexander Panchenko"], "emails": ["dmitry.ustalov@urfu.ru", "mikhail.chernoskutov@urfu.ru", "biemann@informatik.uni-hamburg.de", "panchenko@informatik.uni-hamburg.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n09 23\n4v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\nKeywords: lexical semantics \u00b7 word embeddings \u00b7 synset induction \u00b7 synonyms \u00b7 word sense induction \u00b7 synset induction \u00b7 sense embeddings"}, {"heading": "1 Introduction", "text": "A synonymy dictionary, representing synonymy relations between the individual words, can be modeled as an undirected graph where nodes are words and edges are synonymy relations.4 Such a graph, called a synonymy graph or a synonymy network, tends to have a clustered structure [7]. This property is exploited by various graph-based word sense induction (WSI) methods, such as [22]. The goal of such WSI methods is to build a word sense inventory from various networks, such as synonymy graphs, co-occurrence graphs, graphs of distributionally related words, etc. (see a survey by Navigli [19]).\nThe clusters are densely connected subgraphs of synonymy graph that correspond to the groups of semantically equivalent words or synsets (sets of synonyms). Synsets are building blocks for WordNet [5] and similar lexical databases\n4 In the context of this work, we assume that synonymy is a relation of lexical semantic equivalence which is context-independent, as opposed to \u201ccontextual synonyms\u201d [32].\nused in various applications, such as information retrieval [14]. Graph-based WSI combined with graph clustering makes it possible to induce synsets in an unsupervised way [12, 30]. However, these methods are highly sensitive to the structure of the input synonymy graph [30], which motivates the development of synonymy graph expansion methods.\nIn this paper, we are focused on the data sparseness reduction problem in the synonymy graphs. This problem is inherent to the majority of manually constructed lexical-semantic graphs due to the Zipf\u2019s law of word frequencies [33]: the long tail of rare words is inherently underrepresented. In this work, given a synonymy graph and a graph clustering algorithm, we compare the performance of two methods designed to improve synset induction. The goal of each method is to improve the final synset cluster structures. Both methods are based on the assumption that synonymy is a symmetric relation. We run our experiments on the Russian language using the Watset state-of-the-art unsupervised synset induction method [30].\nThe contribution of this paper is a study of two principally different methods for dealing with the sparsity of the input synonymy graphs. The former, relation transitivity method, is based on expansion of the synonymy graph. The latter, synset merging method, is based on the mutual similarity of synsets."}, {"heading": "2 Related Work", "text": "Hope and Keller [12] introduced the MaxMax clustering algorithm particularly designed for the word sense induction task. In a nutshell, pairs of nodes are grouped if they have a maximal mutual affinity. The algorithm starts by converting the undirected input graph into a directed graph by keeping the maximal affinity nodes of each node. Next, all nodes are marked as root nodes. Finally, for each root node, the following procedure is repeated: all transitive children of this root form a cluster and the root are marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster.\nVan Dongen [3] presented the Markov Clustering (MCL) algorithm for graphs based on simulation of stochastic flow in graphs. MCL simulates random walks within a graph by alternation of two operators called expansion and inflation, which recompute the class labels. This approch has been successfully used for the word sense induction task [4].\nBiemann [1] introduced Chinese Whispers, a clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority labels among the neighboring nodes. The author showed usefulness of the algorithm for induction of word senses based on corpus-induced graphs.\nThe ECO approach [8] was applied to induce a WordNet of the Portuguese language.5 In its core, ECO is based on a clustering algorithm that was used\n5 http://ontopt.dei.uc.pt\nto induce synsets from synonymy dictionaries. The algorithm starts by adding random noise to edge weights. Then, the approach applies Markov Clustering of this graph several times to estimate the probability of each word pair being in the same synset. Finally, candidate pairs over a certain threshold are added to output synsets.\nIn our experiments, we rely on the Watset synset induction method [30] based on a graph meta-clustering algorithm that combines local and global hard clustering to obtain a fuzzy graph clustering. The authors shown that this approach outperforms all methods mentioned above on the synset induction task and therefore we use it as the strongest baseline to date.\nMeyer and Gurevich [17] presented an approach for construction of an ontologized version of Wiktionary, by formation of ontological concepts and relationships between them from the ambiguous input dictionary, yet their approach does not involve graph clustering."}, {"heading": "3 Two Approaches to Cope with Dictionary Sparseness", "text": "We propose two approaches for dealing with the incompleteness of the input synonymy dictionaries of a graph-based synset induction method, such as Watset or MaxMax. First, we describe a graph-based approach that preprocesses the input graph by adding new edges. This step is applied before the synset induction clustering. Second, we describe an approach that post-processes the synsets by merging highly semantically related synsets. This step is applied after the synset induction clustering step, refining its results."}, {"heading": "3.1 Expansion of Synonymy Graph via Relation Transitivity", "text": "Assuming that synonymy is an equivalence relation due to its reflexiveness, symmetry, and transitivity, we can insert additional edges into the synonymy graph between nodes that are transitively, synonymous, i.e. are connected by a short path of synonymy links. We assume that if an edge for a pair of synonyms is missing, the graph still contains several relatively short paths connecting the nodes corresponding to these words.\nFirstly, for each vertex, we extract its neighbors and the neighbors of these neighbors. Secondly, we compute the set of candidate edges by connecting the disconnected vertices. Then, we compute the number of simple paths between the vertices in candidate edges. Finally, we add an edge into the graph if there are at least k such paths which lengths are in the range [i, j].\nParticularly, the algorithm works as follows:\n1. extract a first-order ego network N1 and a second-order ego network N2 for each node; 2. generate the set of candidate edges that connect the disconnected nodes in N1, i.e., the total number of the candidates is C\n2 |N1| \u2212 |EN1 |, where C 2 |N1| is\nthe number of all 2-combinations over the |N1|-element set and EN1 is the set of edges in N1;\n3. keep only those edge candidates that satisfy two conditions: 1) there are at least k paths p in N2, so no path contains the initial ego node, and 2) the length of each path belongs to the interval [i, j].\nThe approach has two parameters: the minimal number of paths to consider k and the path length interval [i, j]. It should be noted that this approach processes the input synonymy graph without taking the polysemous words into account. Such words are then handled by the Watset algorithm that induces word senses based on the expanded synonymy graph."}, {"heading": "3.2 Synset Merging based on Synset Vector Representations", "text": "We assume that closely related synsets carry equivalent meanings and use the following procedure to merge near-duplicate synsets:\n1. learn synset embeddings for each synset using the SenseGram method by simply averaging word vectors that correspond to the words in the synset [26]; 2. identify the closely related synsets using the m-kNN algorithm [20] that considers two objects as closely related if they are mutual neighbors of each other; 3. merge the closely related synsets in a specific order: the smallest synsets are merged first, the largest are merged later; every synset can be merged only once in order to avoid giant merged clusters.\nThis approach has two parameters: the number of nearest neighbors to consider k (fixed to 10 in our experiments)6 and the maximal number of merged synsets t, e.g., if t = 1 then only the first mutual nearest neighbor is merged. It should be noted that this approach operates on synsets that which are already have been discovered by Watset. Therefore, the merged synsets are composed of disambiguated word senses."}, {"heading": "4 Evaluation", "text": "We evaluate the performance of the proposed approaches using the Watset graph clustering method that shows state-of-the-art results on synset induction [30]. Watset is a meta-clustering algorithm that disambiguates a (word) graph by first performing ego-network clustering to split nodes (words) into (word) senses. Then a global clustering is used to form (syn)sets of senses. For both clustering steps, any graph clustering algorithm can be employed; in [30], it was shown that combinations of Chinese Whispers [1] (CW) and Markov Clustering [3] (MCL) provide the best results. We also evaluated the same approaches with the MaxMax [12] method, but the results were virtually the same, so we omitted them for brevity.\n6 In general, the m-kNN method can be parametrized by two different parameters: kij \u2013 the number of nearest neighbors from the word i to the word j and kji \u2013 the number of nearest neighbors from the word j to the word i. In our case, for simplicity, we set kij = kji = k."}, {"heading": "4.1 Datasets", "text": "We evaluate the proposed augmentation approaches on two gold standard datasets for Russian: RuWordNet [15] and YARN [2]. Both are analogues of the original English WordNet [5].\nWe used the same input graph as in [30]; the graph is based on three synonymy dictionaries, the Russian Wiktionary, the Abramov\u2019s dictionary and the UNLDC dictionary. The graph is weighted using the similarities from Russian Distributional Thesaurus (RDT) [23].7. To construct synset embeddings, we used word vectors from the RDT.\nThe lexicon of the input dictionary is different from the lexicon of RuWordNet [15], which includes a lot of domain-specific synsets. At the same time, the input dataset is the same as the data sources used for boostrapping YARN [2].\nThe summary of the datasets is shown in Table 1: the \u201c# words\u201d column specifies the number of lexical units in the dataset (nodes of the input graph), the \u201c# synonyms\u201d column indicates the number of synonymy pairs appearing in the dataset (edges of the input graph). The problem of dictionary sparsity is the fact that some edges (synonyms) are missing in the input resource. Finally, the \u201c# synsets\u201d column specifies the number of resulting synsets (if applicable)."}, {"heading": "4.2 Quality Measures", "text": "We report results according to standard word sense induction evaluation measures: paired precision, recall and F-score [16], i.e., each cluster of n words yields n(n\u22121)\n2 synonymy pairs. The exact same evaluation protocol was used in the original Watset publication. We perform evaluation on the intersection of gold standard lexicon and the lexicon of the induced resource."}, {"heading": "4.3 Results", "text": "The evaluation results are shown in Fig. 1. As one may observe, in the case of the RuWordNet dataset, the method based on the transitivity expansion rendered almost no improvements in terms of recall while dramatically dropping the\n7 http://russe.nlpub.ru/downloads\nprecision. The second method, based on synset embeddings shows much better results on this dataset: It substantially improves recall, yet at the cost of a drop in precision.\nIn case of the YARN dataset, the results are similar with the graph-based method significantly lagging behind the vector-based method. However, in this case, the difference in the observed performance is smaller with some configurations of the graph-based methods approaching the performance of the vectorbased method. Similarly to the first dataset, both methods trade off gains in recall for the drops in precision. Note, however, that the vector-based method can perform a shift of the \u201csweet spot\u201d of the clustering approach. While the Fmeasure remains at the same level, it is possible to obtain higher levels of recall, which can be useful for some applications. In the following section, we perform error analysis for each method."}, {"heading": "5 Discussion", "text": "Perfect synonyms are very rare, which is confirmed by the precision-recall plot in Fig. 1. Both methods insert relations of other types, such as association, co-\nhyponymy, hypernymy, etc. Recall increases with the level of inclusiveness of the configuration; this also causes significant drops in precision. The expansion methods presented in this paper could therefore be more useful for generation of other types of symmetric semantic relations, such as co-hyponymy."}, {"heading": "5.1 Error Analysis: Synonymy Transitivity", "text": "We tried the following configurations of the approach: 2 \u2264 i \u2264 j \u2264 3, k \u2264 10. However, only the variations with a small allowed length i = j = 2 and a high number of found simple paths k \u2265 5 yielded viable results.\nWe explain the quick drops in precision by the fact that no word is a perfect synonym of another [10]. This results in the potential loss of the synonymy relation on each additional transitive node. While having a lot of pertinent edge insertions like \u201c\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c\u0441\u044f\u2013\u043f\u043e\u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f (show up \u2013 appear)\u201d or \u201c\u043f\u043e\u0434\u0442\u0440\u0443\u043d\u0438\u0432\u0430\u0442\u044c\u2013 \u0441\u0442\u0435\u0431\u0430\u0442\u044c\u0441\u044f (prank \u2013 make jokes)\u201d, this method introduces such false positives like \u201c\u043a\u0438\u0439\u2013\u0445\u043b\u044b\u0441\u0442 (cue \u2013 whip)\u201d, \u201c\u0448\u0435\u0444\u2013\u0446\u0430\u0440\u044c (boss \u2013 tsar)\u201d, \u201c\u0441\u043e\u043b\u0438\u0434\u043d\u044b\u0439\u2013\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u044b\u0439 (solid \u2013 correct)\u201d, etc.\nOne of the reasons of this outcome is that adding new edges increases the size of the communities. They capture neighboring vertices and edges belonging to other communities in the initial graph. Hence, on the one hand, we obtain communities with excess elements, while on the other hand, we observe depleted communities."}, {"heading": "5.2 Error Analysis: Synset Merging", "text": "The different configurations of the vector-based method in this plot correspond to the following values of the t parameter (the maximum number of merged synsets): 1, 2, 3, 5, 10. Merging more than one synset at a time provides a substantial gain in the recall, yet again at the cost of the precision drop.\nTable 2 presents an example of correct merging of synsets. The results of the clustering generate multiple small synsets that refer to the same meaning. Such synsets tend to be mutual nearest neighbors. Top ten most similar synsets to the synset \u201ccynicism\u201d are depicted. In this table, we also indicate whether each neighbor is the mutual nearest neighbor or not. In this example, the method of mutual nearest neighbors perfectly achieves its goal of merging synonymous synsets.\nTable 3 presents an example of a wrong merging of synsets on the example of the synset \u201czinc, Zn\u201d. This sample illustrates the reasons behind the drops in precision. While different chemical elements, such as zinc and cobalt are strongly semantically related, they are co-hyponyms of the common hypernym \u201cchemical element\u201d, and not synonyms, i.e. terms with equivalent meanings. This result is in line with the prior results showing that the majority of the nearest neighbors delivered by the distributional semantic models, such as the skip-grammodel [18] used in our experiments, tend to be co-hyponyms as shown in prior studies [11, 24, 25, 31]. The results presented in both Table 2 and 3 have been manually annotated by a single expert."}, {"heading": "5.3 Other Ways to Deal with Sparseness of the Input Dictionary", "text": "In this section, we discuss avenues for future work: the prominent approaches that might be useful in addressing the sparseness of the synonym dictionaries.\nLexical-syntactic patterns for extraction of synonyms. Hearst patterns are widely used to mine hypernymy relations from text [27]. Such patterns can be also learned automatically [28,29]. In [21], seven patterns for extraction of synonyms were proposed, which function in the same was as the Hearst patterns for hypernymy extraction. Such synonymy extraction patterns can be also learned automatically in the same fashion as patterns for hypernymy extraction from text [29]. Finally, hypernyms, antonyms, and other relations extracted from text can be used to filter our non-synonymous candidates.\nGlobal clustering of synsets. It is possible to find groups of semantically related words (clique-like structures) and expand synonyms only within such communities with a graph clustering algorithm, such as Chinese Whispers [1]. The current graph-based transitivity expansion method does not consider the structure of the communities of the synonymy graph.\nSynonymy detection as an anaphora resolution problem. Another observation is that synonyms are often not used in the same sentence, but instead used to ensure linguistic variance of the text. In this respect, synonymy extraction task is similar to the anaphora resolution task [13]. This line of work is related to prior work of [6] in detecting \u201cbridging mentions\u201d.\nCrowdsourcing. Finally, the last option is simply to improve the quality of the input dictionaries by the means of crowdsourcing [9]. Namely, involving more people to edit Wiktionary that we use as the input data will increase the coverage of the extracted synsets, but large-scale crowdsourcing requires a set of elaborated quality control measures."}, {"heading": "6 Conclusion", "text": "In this paper, we explored two alternative strategies for coping with the problem of inherent sparsity and incompleteness of the synonymy dictionaries. These sparsity issues hamper performance of the methods for automatic induction of synsets, such as MaxMax [12] and Watset [30]. One of the proposed methods performs pre-processing of the graph of synonyms, while the second one performs post-processing of the induced synsets.\nOur experiments on two large scale datasets show that (1) both methods are able to substantially improve recall, but at the cost of substantial drops of precision; (2) the post-processing approach yields better results overall. We conclude our study with an overview of prominent alternative approaches for expansion of incomplete synonymy dictionaries.\nWe believe the results of our study will be useful for both enriching the available lexical semantic resources like OntoWiktionary [17] as well as for increasing the lexical coverage of the input data for the graph-based word sense induction methods.\nAcknowledgements. We acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG) under the \u201cJOIN-T\u201d project, the DAAD, the RFBR under the projects no. 16-37-00203 \u043c\u043e\u043b_\u0430 and no. 16-37-00354 \u043c\u043e\u043b_\u0430, and the RFH under the project no. 16-04-12019. The calculations were carried out using the supercomputer \u201cUran\u201d at the Krasovskii Institute of Mathematics and Mechanics. Finally, we also thank four anonymous reviewers for their helpful comments."}], "references": [{"title": "Chinese Whispers: An Efficient Graph Clustering Algorithm and Its Application to Natural Language Processing Problems", "author": ["C. Biemann"], "venue": "Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing. pp. 73\u201380. TextGraphs-1, Association for Computational Linguistics, New York, NY, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "YARN: Spinning-in-Progress", "author": ["P. Braslavski", "D. Ustalov", "M. Mukhin", "Y. Kiselev"], "venue": "Proceedings of the 8th Global WordNet Conference. pp. 58\u201365. GWC 2016, Global WordNet Association, Bucharest, Romania", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph Clustering by Flow Simulation", "author": ["S. van Dongen"], "venue": "Ph.D. thesis, University of Utrecht", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Discovering Corpus-Specific Word Senses", "author": ["B. Dorow", "D. Widdows"], "venue": "Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 2. pp. 79\u201382. EACL \u201903, Association for Computational Linguistics, Budapest, Hungary", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "WordNet: An Electronic Database", "author": ["C. Fellbaum"], "venue": "MIT Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Distributional Semantics for Resolving Bridging Mentions", "author": ["T. Feuerbach", "M. Riedl", "C. Biemann"], "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing. pp. 192\u2013199. INCOMA Ltd. Shoumen, BULGARIA, Hissar, Bulgaria", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Synonym Dictionary Improvement through Markov Clustering and Clustering Stability", "author": ["D. Gfeller", "J.C. Chappelier", "P. De Los Rios"], "venue": "Proceedings of the International Symposium on Applied Stochastic Models and Data Analysis. pp. 106\u2013113", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "ECO and Onto.PT: a flexible approach for creating a Portuguese wordnet automatically. Language Resources and Evaluation", "author": ["H. Gon\u00e7alo Oliveira", "P. Gomes"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "The People\u2019s Web Meets NLP", "author": ["I. Gurevych", "Kim", "J. (eds."], "venue": "Springer Berlin Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "An old problem for the new psycho-semantics: Synonymity", "author": ["D.J. Herrmann"], "venue": "Psychological Bulletin 85(3), 490\u2013512", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1978}, {"title": "Modelling Word Similarity: an Evaluation of Automatic Synonymy Extraction Algorithms", "author": ["K. Heylen", "Y. Peirsman", "D. Geeraerts", "D. Speelman"], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation. pp. 3243\u20133249. LREC 2008, European Language Resources Association, Marrakech, Morocco", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "MaxMax: A Graph-Based Soft Clustering Algorithm Applied to Word Sense Induction", "author": ["D. Hope", "B. Keller"], "venue": "Computational Linguistics and Intelligent Text Processing: 14th International Conference, CICLing 2013, Samos, Greece, March 24-30, 2013, Proceedings, Part I, pp. 368\u2013381. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "An Algorithm for Pronominal Anaphora Resolution", "author": ["S. Lappin", "H.J. Leass"], "venue": "Computational Linguistics 20(4), 535\u2013561", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Thesauri in information retrieval", "author": ["N.V. Loukachevitch"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Creating Russian WordNet by Conversion", "author": ["N.V. Loukachevitch", "G. Lashevich", "A.A. Gerasimova", "V.V. Ivanov", "B.V. Dobrov"], "venue": "Computational Linguistics and Intellectual Technologies: papers from the Annual conference \u201cDialogue\u201d. pp. 405\u2013415. RSUH, Moscow, Russia", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "SemEval-2010 Task 14: Word Sense Induction & Disambiguation", "author": ["S. Manandhar", "I. Klapaftis", "D. Dligach", "S. Pradhan"], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 63\u201368. Association for Computational Linguistics, Uppsala, Sweden", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "OntoWiktionary: Constructing an Ontology from the Collaborative Online Dictionary Wiktionary, pp", "author": ["C.M. Meyer", "I. Gurevyich"], "venue": "131\u2013161. IGI Global, Hershey, PA, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26, pp. 3111\u20133119. Curran Associates, Inc., Harrahs and Harveys, NV, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "A Quick Tour of Word Sense Disambiguation, Induction and Related Approaches", "author": ["R. Navigli"], "venue": "Proceedings of the 38th International Conference on Current Trends in Theory and Practice of Computer Science, pp. 115\u2013129. SOFSEM\u201912, Springer-Verlag, Berlin, Heidelberg", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Extraction of Semantic Relations between Concepts with KNN Algorithms on Wikipedia", "author": ["A. Panchenko", "S. Adeykin", "A. Romanov", "P. Romanov"], "venue": "Proceedings of the 2nd International Workshop on Concept Discovery in Unstructured Data. pp. 78\u201386. No. 871 in CEUR Workshop Proceedings, Leuven, Belgium", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "A Semantic Similarity Measure Based on Lexico-Syntactic Patterns", "author": ["A. Panchenko", "O. Morozova", "H. Naets"], "venue": "Proceedings of KONVENS 2012. pp. 174\u2013178. \u00d6GAI", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Noun Sense Induction and Disambiguation using Graph-Based Distributional Semantics", "author": ["A. Panchenko", "J. Simon", "M. Riedl", "C. Biemann"], "venue": "Proceedings of the 13th Conference on Natural Language Processing. pp. 192\u2013202. KONVENS 2016, Bochumer Linguistische Arbeitsberichte", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Human and Machine Judgements for Russian Semantic Relatedness, pp", "author": ["A. Panchenko", "D. Ustalov", "N. Arefyev", "D. Paperno", "N. Konstantinova", "N. Loukachevitch", "C. Biemann"], "venue": "221\u2013235. Springer International Publishing, Cham, Switzerland", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Comparison of the baseline knowledge-, corpus-, and web-based similarity measures for semantic relations extraction", "author": ["A. Panchenko"], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics. pp. 11\u201321. Association for Computational Linguistics, Edinburgh, UK", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Putting things in order", "author": ["Y. Peirsman", "K. Heylen", "D. Speelman"], "venue": "First and second order context models for the calculation of semantic similarity. In: Proceedings of the 9th Journ\u00e9es internationales d\u2019Analyse statistique des Donn\u00e9es Textuelles. pp. 907\u2013916. JADT 2008, Lyon, France", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Making Sense of Word Embeddings", "author": ["M. Pelevina", "N. Arefyev", "C. Biemann", "A. Panchenko"], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP. pp. 174\u2013183. Association for Computational Linguistics, Berlin, Germany", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "A Large Database of Hypernymy Relations Extracted from the Web", "author": ["J. Seitner", "C. Bizer", "K. Eckert", "S. Faralli", "R. Meusel", "H. Paulheim", "S.P. Ponzetto"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation. pp. 360\u2013367. LREC 2016, European Language Resources Association (ELRA), Portoro\u017e, Slovenia", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method", "author": ["V. Shwartz", "Y. Goldberg", "I. Dagan"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2389\u20132398. Association for Computational Linguistics, Berlin, Germany", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning Syntactic Patterns for Automatic Hypernym Discovery", "author": ["R. Snow", "D. Jurafsky", "A.Y. Ng"], "venue": "Proceedings of the 17th International Conference on Neural Information Processing Systems. pp. 1297\u20131304. NIPS\u201904, MIT Press, Vancouver, British Columbia, Canada", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Watset: Automatic Induction of Synsets from a Graph of Synonyms", "author": ["D. Ustalov", "A. Panchenko", "C. Biemann"], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1579\u2013 1590. Association for Computational Linguistics, Vancouver, Canada", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "How semantic is Latent Semantic Analysis? In: Proceedings of R\u00c9CITAL 2005", "author": ["T. Wandmacher"], "venue": "pp. 525\u2013534. Dourdan, France", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Semantic relationships between contextual synonyms", "author": ["X.M. Zeng"], "venue": "ERIC 4(9), 33\u2013 37", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "The psycho-biology off language", "author": ["G.K. Zipf"], "venue": "Boston: I-Ioughton-Mifflin", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1935}], "referenceMentions": [{"referenceID": 6, "context": "Such a graph, called a synonymy graph or a synonymy network, tends to have a clustered structure [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "This property is exploited by various graph-based word sense induction (WSI) methods, such as [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "(see a survey by Navigli [19]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "Synsets are building blocks for WordNet [5] and similar lexical databases", "startOffset": 40, "endOffset": 43}, {"referenceID": 31, "context": "4 In the context of this work, we assume that synonymy is a relation of lexical semantic equivalence which is context-independent, as opposed to \u201ccontextual synonyms\u201d [32].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "used in various applications, such as information retrieval [14].", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "Graph-based WSI combined with graph clustering makes it possible to induce synsets in an unsupervised way [12, 30].", "startOffset": 106, "endOffset": 114}, {"referenceID": 29, "context": "Graph-based WSI combined with graph clustering makes it possible to induce synsets in an unsupervised way [12, 30].", "startOffset": 106, "endOffset": 114}, {"referenceID": 29, "context": "However, these methods are highly sensitive to the structure of the input synonymy graph [30], which motivates the development of synonymy graph expansion methods.", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "This problem is inherent to the majority of manually constructed lexical-semantic graphs due to the Zipf\u2019s law of word frequencies [33]: the long tail of rare words is inherently underrepresented.", "startOffset": 131, "endOffset": 135}, {"referenceID": 29, "context": "We run our experiments on the Russian language using the Watset state-of-the-art unsupervised synset induction method [30].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Hope and Keller [12] introduced the MaxMax clustering algorithm particularly designed for the word sense induction task.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": "Van Dongen [3] presented the Markov Clustering (MCL) algorithm for graphs based on simulation of stochastic flow in graphs.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "This approch has been successfully used for the word sense induction task [4].", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "Biemann [1] introduced Chinese Whispers, a clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "The ECO approach [8] was applied to induce a WordNet of the Portuguese language.", "startOffset": 17, "endOffset": 20}, {"referenceID": 29, "context": "In our experiments, we rely on the Watset synset induction method [30] based on a graph meta-clustering algorithm that combines local and global hard clustering to obtain a fuzzy graph clustering.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Meyer and Gurevich [17] presented an approach for construction of an ontologized version of Wiktionary, by formation of ontological concepts and relationships between them from the ambiguous input dictionary, yet their approach does not involve graph clustering.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "learn synset embeddings for each synset using the SenseGram method by simply averaging word vectors that correspond to the words in the synset [26]; 2.", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "identify the closely related synsets using the m-kNN algorithm [20] that considers two objects as closely related if they are mutual neighbors of each other; 3.", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "We evaluate the performance of the proposed approaches using the Watset graph clustering method that shows state-of-the-art results on synset induction [30].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "For both clustering steps, any graph clustering algorithm can be employed; in [30], it was shown that combinations of Chinese Whispers [1] (CW) and Markov Clustering [3] (MCL) provide the best results.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "For both clustering steps, any graph clustering algorithm can be employed; in [30], it was shown that combinations of Chinese Whispers [1] (CW) and Markov Clustering [3] (MCL) provide the best results.", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "For both clustering steps, any graph clustering algorithm can be employed; in [30], it was shown that combinations of Chinese Whispers [1] (CW) and Markov Clustering [3] (MCL) provide the best results.", "startOffset": 166, "endOffset": 169}, {"referenceID": 11, "context": "We also evaluated the same approaches with the MaxMax [12] method, but the results were virtually the same, so we omitted them for brevity.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "We evaluate the proposed augmentation approaches on two gold standard datasets for Russian: RuWordNet [15] and YARN [2].", "startOffset": 102, "endOffset": 106}, {"referenceID": 1, "context": "We evaluate the proposed augmentation approaches on two gold standard datasets for Russian: RuWordNet [15] and YARN [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Both are analogues of the original English WordNet [5].", "startOffset": 51, "endOffset": 54}, {"referenceID": 29, "context": "We used the same input graph as in [30]; the graph is based on three synonymy dictionaries, the Russian Wiktionary, the Abramov\u2019s dictionary and the UNLDC dictionary.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "The graph is weighted using the similarities from Russian Distributional Thesaurus (RDT) [23].", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "The lexicon of the input dictionary is different from the lexicon of RuWordNet [15], which includes a lot of domain-specific synsets.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "At the same time, the input dataset is the same as the data sources used for boostrapping YARN [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 15, "context": "We report results according to standard word sense induction evaluation measures: paired precision, recall and F-score [16], i.", "startOffset": 119, "endOffset": 123}, {"referenceID": 9, "context": "We explain the quick drops in precision by the fact that no word is a perfect synonym of another [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "This result is in line with the prior results showing that the majority of the nearest neighbors delivered by the distributional semantic models, such as the skip-grammodel [18] used in our experiments, tend to be co-hyponyms as shown in prior studies [11, 24, 25, 31].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "This result is in line with the prior results showing that the majority of the nearest neighbors delivered by the distributional semantic models, such as the skip-grammodel [18] used in our experiments, tend to be co-hyponyms as shown in prior studies [11, 24, 25, 31].", "startOffset": 252, "endOffset": 268}, {"referenceID": 23, "context": "This result is in line with the prior results showing that the majority of the nearest neighbors delivered by the distributional semantic models, such as the skip-grammodel [18] used in our experiments, tend to be co-hyponyms as shown in prior studies [11, 24, 25, 31].", "startOffset": 252, "endOffset": 268}, {"referenceID": 24, "context": "This result is in line with the prior results showing that the majority of the nearest neighbors delivered by the distributional semantic models, such as the skip-grammodel [18] used in our experiments, tend to be co-hyponyms as shown in prior studies [11, 24, 25, 31].", "startOffset": 252, "endOffset": 268}, {"referenceID": 30, "context": "This result is in line with the prior results showing that the majority of the nearest neighbors delivered by the distributional semantic models, such as the skip-grammodel [18] used in our experiments, tend to be co-hyponyms as shown in prior studies [11, 24, 25, 31].", "startOffset": 252, "endOffset": 268}, {"referenceID": 26, "context": "Hearst patterns are widely used to mine hypernymy relations from text [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Such patterns can be also learned automatically [28,29].", "startOffset": 48, "endOffset": 55}, {"referenceID": 28, "context": "Such patterns can be also learned automatically [28,29].", "startOffset": 48, "endOffset": 55}, {"referenceID": 20, "context": "In [21], seven patterns for extraction of synonyms were proposed, which function in the same was as the Hearst patterns for hypernymy extraction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Such synonymy extraction patterns can be also learned automatically in the same fashion as patterns for hypernymy extraction from text [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "It is possible to find groups of semantically related words (clique-like structures) and expand synonyms only within such communities with a graph clustering algorithm, such as Chinese Whispers [1].", "startOffset": 194, "endOffset": 197}, {"referenceID": 12, "context": "In this respect, synonymy extraction task is similar to the anaphora resolution task [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "This line of work is related to prior work of [6] in detecting \u201cbridging mentions\u201d.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "Finally, the last option is simply to improve the quality of the input dictionaries by the means of crowdsourcing [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 11, "context": "These sparsity issues hamper performance of the methods for automatic induction of synsets, such as MaxMax [12] and Watset [30].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "These sparsity issues hamper performance of the methods for automatic induction of synsets, such as MaxMax [12] and Watset [30].", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "We believe the results of our study will be useful for both enriching the available lexical semantic resources like OntoWiktionary [17] as well as for increasing the lexical coverage of the input data for the graph-based word sense induction methods.", "startOffset": 131, "endOffset": 135}], "year": 2017, "abstractText": "Graph-based synset induction methods, such as MaxMax and Watset, induce synsets by performing a global clustering of a synonymy graph. However, such methods are sensitive to the structure of the input synonymy graph: sparseness of the input dictionary can substantially reduce the quality of the extracted synsets. In this paper, we propose two different approaches designed to alleviate the incompleteness of the input dictionaries. The first one performs a pre-processing of the graph by adding missing edges, while the second one performs a post-processing by merging similar synset clusters. We evaluate these approaches on two datasets for the Russian language and discuss their impact on the performance of synset induction methods. Finally, we perform an extensive error analysis of each approach and discuss prominent alternative methods for coping with the problem of sparsity of the synonymy dictionaries.", "creator": "LaTeX with hyperref package"}}}