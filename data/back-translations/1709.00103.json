{"id": "1709.00103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "abstract": "We propose Seq2SQL, a deep neural network for translating natural language questions into corresponding SQL queries. Our model uses the structure of SQL queries to significantly reduce the output space of generated queries. In addition, we will use rewards from executing queries within the database to learn a strategy for generating disordered portions of queries that we show are less suitable for optimization through cross-entropy loss. In addition, we will publish WikiSQL, a dataset of 87726 hand-annotated examples of queries and SQL queries spread across 26375 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying guideline-based learning with a QL execution of 6Q375 tables and 4Q3 models on 2Q3 queries, from 29.9% to 23.9% accuracy.", "histories": [["v1", "Thu, 31 Aug 2017 23:12:15 GMT  (1543kb,D)", "http://arxiv.org/abs/1709.00103v1", "10 pages, 5 figures"], ["v2", "Wed, 20 Sep 2017 19:14:04 GMT  (223kb,D)", "http://arxiv.org/abs/1709.00103v2", "12 pages, 5 figures"], ["v3", "Wed, 18 Oct 2017 01:34:46 GMT  (195kb,D)", "http://arxiv.org/abs/1709.00103v3", "12 pages, 5 figures"], ["v4", "Thu, 26 Oct 2017 21:13:56 GMT  (270kb,D)", "http://arxiv.org/abs/1709.00103v4", "12 pages, 5 figures"], ["v5", "Tue, 31 Oct 2017 20:49:31 GMT  (270kb,D)", "http://arxiv.org/abs/1709.00103v5", "12 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["victor zhong", "caiming xiong", "richard socher"], "accepted": false, "id": "1709.00103"}, "pdf": {"name": "1709.00103.pdf", "metadata": {"source": "CRF", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "authors": ["Victor Zhong", "Caiming Xiong"], "emails": ["vzhong@salesforce.com", "cmxiong@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": "1 Introduction", "text": "A vast amount of today\u2019s information is stored in relational databases, which provide the foundation of crucial applications such as medical records [Hillestad et al., 2005], financial markets [Beck et al., 2000], and customer relations management [Ngai et al., 2009]. However, accessing information in relational databases requires an understanding of query languages such as SQL, which, while powerful, is difficult to master. A prominent research area at the intersection of natural language processing and human-computer interactions is the study of natural language interfaces (NLI) [Androutsopoulos et al., 1995], which seek to provide means for humans to interact with computers through the use of natural language. We investigate one particular aspect of NLI applied to relational databases: translating natural language questions to SQL queries.\nOur main contributions in this work are two-fold. First, we introduce Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Seq2SQL, shown in Figure 1, consists of three components that leverage the structure of a SQL query to greatly reduce the output space of generated queries. Moreover, it uses policy-based reinforcement learning (RL) to generate the conditions of the query, which we show are unsuitable for optimization using cross entropy loss due to their unordered nature. Seq2SQL is trained using a mixed objective combining cross entropy losses and RL rewards from in-the-loop query execution on a database. These characteristics allow the model to achieve much improved results on query generation.\nSecond, we release WikiSQL, a corpus of 87726 hand-annotated instances of natural language questions, SQL queries, and SQL tables extracted from 26375 HTML tables from Wikipedia. WikiSQL is an order of magnitude larger than comparable datasets that also provide logical forms along with natural language utterances. We release the tables used in WikiSQL both in raw JSON format as well\nar X\niv :1\n70 9.\n00 10\n3v 1\n[ cs\n.C L\n] 3\n1 A\nug 2\nas in the form of a SQL database. Along with WikiSQL, we also release a query execution engine for the database, which we use for in-the-loop query execution to learn the policy. On WikiSQL, we show that Seq2SQL significantly outperforms an attentional sequence to sequence baseline, which obtains 35.9% execution accuracy, as well as a pointer network baseline, which obtains 52.8% execution accuracy. By leveraging the inherent structure of SQL queries and applying policy gradient methods using the reward signals from live query execution during training, Seq2SQL achieves a marked improvement on WikiSQL, obtaining 60.3% execution accuracy."}, {"heading": "2 Related Work", "text": "Learning logical forms from natural language utterances. In semantic parsing for question answering (QA), natural language questions are parsed into logical forms, which are then executed on a knowledge graph [Zelle and Mooney, 1996, Wong and Mooney, 2007, Zettlemoyer and Collins, 2005, 2007]. Other work in semantic parsing has focused on learning parsers without relying on annotated logical forms by leveraging conversational logs [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], distant supervision [Cai and Yates, 2013, Reddy et al., 2014], and questionanswer pairs [Liang et al., 2011]. Recently, Pasupat and Liang [2015] introduced a floating parser to address the large amount of variation in utterances and table schema for semantic parsing on web tables. Our approach is different from these semantic parsing techniques in that it does not rely on a high quality grammar and lexicon, and instead relies on representation learning.\nSemantic parsing datasets. Previous semantic parsing systems were typically designed to answer complex and compositional questions over closed-domain, fixed-schema datasets such as GeoQuery [Tang and Mooney, 2001] and ATIS [Price, 1990]. Researchers have also investigated QA over subsets of large-scale knowledge graphs such as DBPedia [Starc and Mladenic, 2017] and Freebase [Cai and Yates, 2013, Berant et al., 2013]. The dataset \u201cOvernight\u201d [Wang et al., 2015] uses a similar crowd-sourcing process to build a dataset of (natural language question, logical form) pairs, but it is comprised of only 8 domains. WikiTableQuestions [Pasupat and Liang, 2015] is a collection of question and answers, also over a large quantity of tables extracted from Wikipedia. However, it does not provide logical forms whereas WikiSQL does. WikiTableQuestions is focused on the task of QA over potentially noisy web tables, whereas WikiSQL is focused on generating SQL queries for questions over relational database tables. Our intent is to build a natural language interface for databases.\nRepresentation learning for sequence generation. Our baseline model is based on the attentional sequence to sequence model (Seq2Seq) proposed by [Bahdanau et al., 2015]. Seq2SQL is inspired by pointer networks [Vinyals et al., 2015, Merity et al., 2017], which, instead of generating words from a fixed vocabulary like Seq2Seq, generates by selecting words from the input sequence. This class of models has been successfully applied to tasks such as language modeling [Merity et al., 2017], summarization [Gu et al., 2016], combinatorial optimization [Bello et al., 2017], and question answering [Seo et al., 2017, Xiong et al., 2017]. Neural methods have also been applied to semantic parsing [Dong and Lapata, 2016, Neelakantan et al., 2017]. However, unlike [Dong and Lapata, 2016], we use pointer based generation instead of Seq2Seq generation and show that our approach achieves higher performance, especially for rare words and column names. Unlike [Neelakantan et al., 2017], our model does not require access to the table content during inference, which may be unavailable due to privacy concerns. In contrast to both methods, we train our model using policy-based RL, which again improves performance.\nNatural language interface for databases. The practical applications of WikiSQL have much to do with Natural Language Interfaces (NLI). One of the prominent works in this area is PRE-\nCISE [Popescu et al., 2003], which translates questions to SQL queries and identifies questions that it is not confident about. Giordani and Moschitti [2012] proposed a system to translate questions to SQL by first generating candidate queries from a grammar then ranking them using tree kernels. Both of these approaches rely on a high quality grammar and are not suitable for tasks that require generalization to new schema. Iyer et al. [2017] also proposed a system to translate to SQL, but with a Seq2Seq model that is further improved with human feedback. Compared to this model, we show that Seq2SQL substantially outperforms Seq2Seq and uses reinforcement learning instead of human feedback during training."}, {"heading": "3 Model", "text": "The Seq2SQL model generates a SQL query from a natural language question and table schema.1 One powerful baseline model is the Seq2Seq model utilized for machine translation [Bahdanau et al., 2015]. However, the output space of the standard softmax in a Seq2Seq model is unnecessarily large for this task. In particular, we note that the problem can be dramatically simplified by limiting the output space of the generated sequence to the union of the table schema, the question utterance, and SQL key words. The resulting model is similar to a pointer network [Vinyals et al., 2015] with augmented inputs. We first show this augmented pointer network model, then address its limitations, particularly with respect to generating unordered query conditions, in our description of Seq2SQL."}, {"heading": "3.1 Augmented Pointer Network", "text": "The augmented pointer network generates the SQL query token-by-token by selecting from an input sequence. In our case, the input sequence is the concatenation of the column names, required for the selection column and the condition columns of the query, the question, required for the conditions of the query, and the limited vocabulary of the SQL language such as SELECT, COUNT etc. In the example shown in Figure 2, the column name tokens consist of \u201cPick\u201d, \u201c#\u201d, \u201cCFL\u201d, \u201cTeam\u201d etc.; the question tokens consist of \u201cHow\u201d, \u201cmany\u201d, \u201cCFL\u201d, \u201cteams\u201d etc.; the SQL tokens consist of SELECT, WHERE, COUNT, MIN, MAX etc. With this augmented input sequence, the pointer network is able to fully produce the SQL query by selecting exclusively from the input.\nSuppose we are given a list of N table columns and a question such as in Figure 2, and asked to produce the corresponding SQL query. Let xcj = [x c j,1, x c j,2, ...x c j,Tj\n] denote the sequence of words in the name of the jth column, where xcj,i represents the ith word in the jth column and Tj represents the total number of words in the jth column. Similarly, let xq and xs respectively denote the sequence of words in the question and the set of all unique words in the SQL vocabulary.\nWe define the input sequence x as the concatenation of all the column names, the question, and the SQL vocabulary:\nx = [<col>;xc1;x c 2; ...;x c N ; <sql>;x s; <question>;xq] (1)\nwhere [a; b] denotes the concatenation between the sequences a and b and we add special sentinel tokens between neighbouring sequences to demarcate the boundaries.\nThe network first encodes x using a two-layer, bidirectional Long Short-Term Memory network [Hochreiter and Schmidhuber, 1997]. The input to the encoder are the embeddings corresponding to words in the input sequence. We denote the output of the encoder by h, where ht is the state of the encoder corresponding to the tth word in the input sequence. For brevity, we do not write\n1For simplicity, we define table schema as the names of the columns in the table.\nout the LSTM equations, which are described by Hochreiter and Schmidhuber [1997]. We then apply a pointer network similar to that proposed by Vinyals et al. [2015] to the input encodings h.\nThe decoder network uses a two layer, unidirectional LSTM. During each decoder step s, the decoder LSTM takes as input ys\u22121, the query token generated during the previous decoding step, and outputs the state gs. Next, the decoder produces a scalar attention score \u03b1 ptr s,t for each position t of the input sequence:\n\u03b1ptrs,t =W ptrtanh ( Uptrgs +V ptrht )\n(2)\nThe input token with the highest score is then chosen by the decoder to be the next token of the SQL query, ys = argmax(\u03b1ptrs ).\n3.2 Seq2SQL\nWhile the augmented pointer model is able to solve the SQL generation problem, it does not leverage the structure inherent in SQL queries. Normally, a SQL query such as that shown in Figure 3 consists of three components. The first component is the aggregation operator, in this case COUNT, which produces a summary of the rows selected by the SQL query. Alternatively the query may request no summary statistics, in which case an aggregation operator is not provided. The second component is the SELECT column(s), in this case Engine, which identifies the column(s) that are to\nbe included in the returned results. The third component is the WHERE clause of the query, in this cases WHERE Driver = Val Musetti, which contains conditions by which to filter the rows. Here, we want only rows in which the driver is \u201cVal Musetti\u201d.\nTo leverage the structure present in SQL queries, we introduce Seq2SQL. Seq2SQL, as shown in Figure 3, is composed of three parts that correspond to the aggregation operator, the SELECT column, and the WHERE clause. First, the network classifies an aggregation operation for the query, with the addition of a null operation that corresponds to no aggregation. Next, the network points to a column in the input table corresponding to the SELECT column. Finally, the network generates the conditions for the query using a pointer network. The first two components are supervised using cross entropy loss, whereas the third generation component is trained using policy gradient reinforcement learning in order to address the unordered nature of query conditions (we explain this in detail in Section 3.2). Utilizing the structure of a SQL query allows Seq2SQL to further reduce the output space of SQL queries, which we show leads to a higher performance compared to both the Seq2Seq model and the pointer model.\nAggregation Operation. The aggregation operation depends on the question. For the example shown in Figure 3, the correct operator is COUNT because the question asks for \u201cHow many\u201d.\nTo compute the aggregation operation, we first compute an input representation \u03baagg. Similar to Equation 2, we first compute the scalar attention score, \u03b1inpt = W\ninphenct , for each tth token in the input sequence. The vector of all scores, \u03b1inp = [\u03b1inp1 , \u03b1 inp 2 , ...], is then normalized to\nproduce a distribution over all the tokens in the input sequence x, \u03b2inp = softmax ( \u03b1inp ) . The input representation, \u03baagg, is the sum over the column encodings weighted by the corresponding normalized score:\n\u03baagg = \u2211 t \u03b2inpt h enc t (3)\nLet \u03b1agg denote the scores over the aggregation operations such as COUNT, MIN, MAX, and the noaggregation operation NULL. We compute \u03b1agg by applying a multi-layer perceptron to the input representation \u03baagg:\n\u03b1agg =W agg tanh (V agg\u03baagg + bagg) + cagg (4)\nThe distribution over the set of possible aggregation operations, \u03b2agg = softmax (\u03b1agg), is obtained by applying the softmax function. We use cross entropy loss Lagg for the aggregation operation.\nSELECT Column. The selection column depends on the table columns as well as the question. Namely, for the example in Figure 3, we see from the \u201cHow many engine types\u201d part of the question that the query is asking for the \u201cEngine\u201d column. SELECT column prediction is then a matching problem, solvable using a pointer: given the list of column representations and a question representation, we select the column that best matches the question.\nIn order to produce the representations for the columns, we first encode each column name with a LSTM. The representation of a particular column j, ecj , is given by:\nhcj,t = LSTM ( emb ( xcj,t ) , hcj,t\u22121 ) ecj = h c j,Tj (5)\nHere, hcj,t denotes the tth encoder state of the jth column. e c j , column j\u2019s representation, is then taken to be the last encoder state.\nTo construct a representation for the question, we compute another input representation \u03basel using the same architecture as for \u03baagg (Equation 3) but with untied weights. Finally, we apply a multi-layer perceptron over the column representations for the pointer estimation, conditioned on the input representation, to compute the a score for each column j:\n\u03b1selj =W sel tanh ( V sel\u03basel + V cecj ) (6)\nWe normalize the scores with a softmax function to produce a distribution over the possible SELECT columns \u03b2sel = softmax ( \u03b1sel ) . For the example shown in Figure 3, the distribution would be over the columns \u201cEntrant\u201d, \u201cConstructor\u201d, \u201cChassis\u201d, \u201cEngine\u201d, \u201cNo\u201d, and the ground truth SELECT column \u201cDriver\u201d. We train the SELECT network using cross entropy loss Lsel.\nWHERE Clause. The WHERE clause can be trained using a pointer decoder similar to that described in Section 3.1. However, there is a crucial limitation in using the cross entropy loss to optimize the network: the WHERE conditions of a query can be arbitrarily swapped and the query still yield the same result. Suppose we have the question \u201cwhich men are older than 18\u201d and the queries SELECT name FROM insurance WHERE age > 18 AND gender = \"male\" and SELECT name FROM insurance WHERE gender = \"male\" AND age > 18. Both queries obtain the correct execution result despite not having exact string match. If the former query is provided as the ground truth, using cross entropy loss to supervise the generation would then wrongly penalize the latter query. To address this problem, we apply reinforcement learning to learn a policy to directly optimize the expected \u201ccorrectness\u201d of the execution result (Equation 7).\nInstead of teacher forcing at each step, we sample from the output distribution to obtain the next token. At the end of the generation procedure, we execute the generated SQL query against the database to obtain a reward. Let q (y) denote the query generated by the model and qg denote the ground truth query corresponding to the question. The reward R (q (y) , qg) is defined as:\nR (q (y) , qg) =  \u22122, if q (y) is not a valid SQL query \u22121, if q (y) is a valid SQL query and executes to an incorrect result +1, if q (y) is a valid SQL query and executes to the correct result\n(7)\nLet y = [y1, y2, ..., yT ] denote the sequence of generated tokens in the WHERE clause of a SQL query q (y) that resulted in an execution reward of R (q (y) , qg). The loss, Lwhe = \u2212Ey[R (q (y) , qg)], is the negative expected reward over possible WHERE clauses.\nAs described by Schulman et al. [2015], the act of sampling during the forward pass of the network can be followed by the corresponding injection of a synthetic gradient, which is a function of the reward, during the backward pass in order to compute estimated parameter gradient. Specifically, if we let Pt(yt) denote the probability of choosing token yt during time step t, the corresponding synthetic gradient is R (q (y) , qg) logPt(yt).\nMixed Objective Function. The model is trained using gradient descent to minimize the objective function L = Lagg+Lsel+Lwhe. The total gradient is then the equally weighted sum of the gradients\nfrom the cross entropy loss in predicting the SELECT column, the gradients from the cross entropy loss in predicting the aggregation operation, and the gradient estimate from the policy learning reward described above.\n4 WikiSQL\nWikiSQL is a collection of questions, corresponding SQL queries, and SQL tables. It is the largest available corpus of its kind to date. A single example in WikiSQL, shown in Figure 2, contains a table, a SQL query, and the natural language question corresponding to the SQL query.\nTable 1 shows how WikiSQL compares to related datasets. Namely, we note that WikiSQL is the largest handannotated semantic parsing dataset to date - it is an order of magnitude larger than other datasets that have logical forms, either in terms of the number of examples or the number of table schemas. Moreover, the queries in Wik-\niSQL span over a large number of table schemas and cover a large diversity of data. The large quantity of schemas presents a challenge compared to other datasets: the model must be able to not only generalize to new queries, but to new schemas as well. Finally, WikiSQL contains challenging, realistic data extracted from the web. This is evident in the distributions of the number of columns, the lengths of questions, and the length of queries, respectively shown in Figure 5. Another indicator of the variety of questions in the dataset is the distribution of question types, shown in Figure 4.\nWikiSQL is collected using crowd-sourcing on Amazon Mechanical Turk (AMT) in two phases. First, a worker is asked to paraphrase a generated question for a table. The generated question itself is formed using a template, filled using a randomly generated SQL query. We ensure the validity and complexity of the tables by keeping only those that are legitimate database tables and sufficiently large in the number of rows and columns. Next, two other workers are asked to verify that the paraphrase has the same meaning as the generated question. We discard paraphrases that do not show enough variation, as measured by the character edit distance from the generated question, as well as those that are deemed incorrect by both workers during verification. More details on how WikiSQL was collected is shown in Section 1 of the Appendix. We make available examples of the interface used during the paraphrase phase and during the verification phase in the supplementary materials. The dataset is available for download at https://github.com/salesforce/wikisql.\nThe tables, their paraphrases, and SQL queries are randomly slotted into train, dev, and test splits, such that each table is present in exactly one split. In addition to the raw tables, queries, results, and natural utterances, we also release a corresponding SQL database and query execution engine."}, {"heading": "4.1 Evaluation", "text": "Let N denote the total number of examples in the dataset, Nex the number of queries that, when executed, result in the correct result, and Nlf the number of queries has exact string match with the ground truth query used to collect the paraphrase.\nWe evaluate using the execution accuracy metric Accex = NexN . One downside of Accex is that it is possible to construct a SQL query that does not correspond to the question but nevertheless obtains the same result. For example, the two queries SELECT COUNT(name) WHERE SSN = 123 and SELECT COUNT(SSN) WHERE SSN = 123 produce the same result if no two people with different names share the SSN 123.\nHence, we also use the logical form accuracy Acclf = NlfN . However, as we showed in Section 3.2, Acclf incorrectly penalizes queries that achieve the correct result but do not have exact string match with the ground truth query. Due to these observations, we use both metrics to evaluate our models."}, {"heading": "5 Experiments", "text": "The dataset is tokenized using Stanford CoreNLP [Manning et al., 2014]. In particular, we use the normalized tokens for training and reverse them into their original gloss before outputting the query. This way, the queries are composed of tokens from the original gloss and are hence executable in the database.\nWe use, and keep fixed, GloVe word embeddings [Pennington et al., 2014] and character n-gram embeddings [Hashimoto et al., 2016]. Let wgx denote the GloVe embedding and w c x the character embedding for word x. Here, wcx is the mean of the embeddings of all the character n-grams in x. For a given word x, we define its embedding wx = [wgx;w c x]. For words that have neither word nor character embeddings, we assign the zero vector. All networks are run for a maximum of 100 epochs with early stopping on dev split execution accuracy. We train using ADAM [Kingma and Ba, 2014] and regularize using dropout [Srivastava et al., 2014]. We implement all models using PyTorch 2. All recurrent layers have a hidden size of 200 units and are followed by a dropout of 0.3.\nTo train Seq2SQL, we first pretrain a version in which the WHERE clause is supervised via teacher forcing (i.e. the sampling procedure is not trained from scratch) and then continue training using reinforcement learning. In order to obtain the rewards described in Section 3.2, we use the query execution engine described in Section 4."}, {"heading": "5.1 Result", "text": "We compare results against an attentional sequence to sequence baseline based on the neural semantic parser proposed by Dong and Lapata [2016], which achieved state of the art results on a variety of semantic parsing datasets. We make one modification to the model by augmenting the input with the table schema, such that the model can generalize to new tables. This model is described in detail in Section 2 of the Appendix. The performance of the three models are shown in Table 2.\nReducing the output space by utilizing the augmented pointer network significantly improves upon the attentional sequence to sequence model by 16.9%. Moreover, leveraging the structure of SQL queries leads to another significant improvement of 4.8%, as is shown by the performance of Seq2SQL without RL compared to the augmented pointer network. Finally, the full Seq2SQL model, trained using reinforcement learning based on rewards from in-the-loop query executions on a database, leads to yet another significant performance increase of 2.7%.\n2https://pytorch.org"}, {"heading": "5.2 Analysis", "text": "Limiting the output space lead to more accurate conditions. Compared to Seq2Seq, the augmented pointer model generates much higher quality WHERE clause conditions. For example, for the question \u201cin how many districts was a successor seated on march 4, 1850?\u201d, Seq2Seq generates WHERE Date successor seated = seated march 4 whereas Seq2SQL generates WHERE Date successor seated = seated march 4 1850. Similarly, for the question \u201cwhat\u2019s doug battaglia\u2019s pick number?\u201d, the Seq2Seq produces WHERE Player = doug whereas Seq2SQL produces WHERE Player = doug battaglia. A reason for this is that conditions tend to be comprised of rare words (e.g. the infrequent \u201c1850\u201d). The Seq2Seq model is then inclined to produce a condition that contains frequently occurring tokens in the training corpus, such as \u201cmarch 4\u201d for date, or \u201cdoug\u201d for player name. This problem does not affect the the pointer network as much, since the output space is constructed from the input sequence and thus orders of magnitude smaller.\nIncorporating structure reduces invalid queries. As expected, incorporating the simple selection and aggregation structure into the model reduces the percentage of invalid SQL queries generated from 7.9% to 4.8%. Generally, we find that a large quantity of\ninvalid queries result from invalid column names. That is, the model generates a query that refers to columns that are not present in the table. This is particularly helpful when the names of columns contain many tokens, such as \u201cMiles (km)\u201d, which is comprised of 4 tokens after tokenization. Introducing a specialized classifier for the aggregation also reduces the error rate due to having the incorrect aggregation operator. Table 3 shows that adding the aggregation classifier substantially improves the recall and F1 for predicting the COUNT operator. For more queries produced by the different models, please see Section 3 of the Appendix.\nReinforcement learning generates higher quality WHERE clause that are ordered differently than ground truth. As expected, training with policy-based RL obtains correct results in which the order of the conditions is different than the order in the ground truth query. For example, for the question \u201cin what district was the democratic candidate first elected in 1992?\u201d, the ground truth conditions are WHERE First elected = 1992 AND Party = Democratic whereas Seq2SQL generates WHERE Party = Democratic AND First elected = 1992. We also note that when Seq2SQL is correct and Seq2SQL without policy learning is not, the latter tends to produce an incorrect WHERE clause. For example, for the rather complex question \u201cwhat is the race name of the 12th round trenton, new jersey race where a.j. foyt had the pole position?\u201d, Seq2SQL trained without RL generates WHERE rnd = 12 and track = a.j. foyt AND pole position = a.j. foyt whereas Seq2SQL trained with RL correctly generates WHERE rnd = 12 AND pole position = a.j. foyt."}, {"heading": "6 Conclusion", "text": "We proposed Seq2SQL, a deep neural network for translating questions to SQL queries. Our model leverages the structure of SQL queries to reduce the output space of the model. As a part of Seq2SQL, we applied in-the-loop query execution to learn a policy for generating the conditions of the SQL query, which is unordered in nature and unsuitable for optimization via cross entropy loss. We also introduced WikiSQL, a dataset of questions and SQL queries that is an order of magnitude larger than comparable datasets. Finally, we showed that Seq2SQL outperforms attentional sequence to sequence models on WikiSQL, improving execution accuracy from 35.9% to 60.3% and logical form accuracy from 23.4% to 49.2%."}, {"heading": "A Collection of WikiSQL", "text": "WikiSQL is collected in a paraphrase phases as well as a verification phase. In the paraphrase phase, we use tables extracted from Wikipedia by Bhagavatula et al [Bhagavatula et al., 2013] and remove small tables according to the following criteria:\n\u2022 the number of cells in each row is not the same \u2022 the content in a cell exceed 50 characters \u2022 a header cell is empty \u2022 the table has less than 5 rows or 5 columns \u2022 over 40% of the cells of a row contain identical content\nWe also remove the last row of a table because a large quantity of HTML tables tend to have summary statistics in the last row, and hence the last row does not adhere to the table schema defined by the header row.\nFor each of the table that passes the above criteria, we randomly generate 6 SQL queries according to the following rules:\n\u2022 the query follows the format SELECT agg_op agg_col from table where cond1_col cond1_op cond1 AND cond2_col cond2_op cond2 ... \u2022 the aggregation operator agg_op can be empty or COUNT. In the event that the aggregation column agg_col is numeric, agg_op can additionally be one of MAX and MIN \u2022 the condition operator cond_op is =. In the event that the corresponding condition column cond_col is numeric, cond_op can additionally be one of > and < \u2022 the condition cond can be any possible value present in the table under the corresponding cond_col. In the event that cond_col is numerical, cond can be any numerical value sampled from the range from the minimum value in the column to the maximum value in the column.\nWe only generate queries that produce a non-empty result set. To enforce succinct queries, we remove conditions from the generated queries if doing so does not change the execution result.\nFor each query, we generate a crude question using a template and obtain a human paraphrase via crowdsourcing on Amazon Mechanical Turk. In each Amazon Mechanical Turk HIT, a worker is shown a table as well as its generated questions and asked to provide paraphrases for each question.\nAfter obtaining natural language utterances from the paraphrase phase, we give each questionparaphrase pair to two other workers in the verification phase to verify that the paraphrase and the original question contain the same meaning.\nWe then filter the initial collection of paraphrases using the following criteria:\n\u2022 the paraphrase must be deemed correct by at least one worker during the verification phrase \u2022 the paraphrase must be sufficiently different from the generated question, with a character-\nlevel edit distance greater than 10"}, {"heading": "B Attentional Seq2Seq Baseline", "text": "For our baseline, we employ the attentional sequence to sequence model. This model was proposed by Dong et al. for neural semantic parsing, and achieved state of the art results on a variety of semantic parsing datasets. We implement a variant using OpenNMT and a global attention encoder-decoder architecture (with input feeding) described by Luong et al.\nWe use the same two-layer, bidirectional, stacked LSTM encoder as described previously. The decoder is a two-layer, unidirectional, stacked LSTM. The decoder LSTM is almost identical to that described by Equation 2 of the paper, with the sole difference coming from input feeding.\ngs = LSTM ([ emb (ys\u22121) ;\u03ba dec s\u22121 ] , gs\u22121 ) (8)\nwhere the dhid dimensional \u03badecs is the attentional context over the input sequence during the sth decoding step, computed as\n\u03b1decs,t = h dec s ( W dechenct )\u1d40 \u03b2decs = softmax ( \u03b1decs ) (9)\n\u03bas = \u2211 t \u03b2s,th enc t (10)\nTo produce the output token during the sth decoder step, the concatenation of the decoder state and the attention context is given to a final linear layer to produce a distribution \u03b1dec over words in the target vocabulary\n\u03b1dec = softmax ( Udec[hdecs ;\u03ba dec s ] )\n(11)\nwhere Udec has dimensions Nvocab \u00d7 dhid and Nvocab is the size of the output vocabulary. During training, teacher forcing is used. During inference, a beam size of 5 is used and generated unknown words are replaced by the input words with the highest attention weight."}, {"heading": "C Predictions by Seq2SQL", "text": ""}], "references": [{"title": "Natural language interfaces to databases - an introduction", "author": ["I. Androutsopoulos", "G. Ritchie", "P. Thanisch"], "venue": null, "citeRegEx": "Androutsopoulos et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Androutsopoulos et al\\.", "year": 1995}, {"title": "Bootstrapping semantic parsers from conversations", "author": ["Y. Artzi", "L.S. Zettlemoyer"], "venue": "In EMNLP,", "citeRegEx": "Artzi and Zettlemoyer.,? \\Q2011\\E", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2011}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L.S. Zettlemoyer"], "venue": "TACL, 1:49\u201362,", "citeRegEx": "Artzi and Zettlemoyer.,? \\Q2013\\E", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A new database on the structure and development of the financial sector", "author": ["T. Beck", "A. Demirg\u00fc\u00e7-Kunt", "R. Levine"], "venue": "The World Bank Economic Review,", "citeRegEx": "Beck et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2000}, {"title": "Neural combinatorial optimization with reinforcement learning", "author": ["I. Bello", "H. Pham", "Q.V. Le", "M. Norouzi", "S. Bengio"], "venue": "ICRL,", "citeRegEx": "Bello et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bello et al\\.", "year": 2017}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "In EMNLP,", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Methods for exploring and mining tables on wikipedia", "author": ["C. Bhagavatula", "T. Noraset", "D. Downey"], "venue": "In IDEA@KDD,", "citeRegEx": "Bhagavatula et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bhagavatula et al\\.", "year": 2013}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Q. Cai", "A. Yates"], "venue": "In ACL,", "citeRegEx": "Cai and Yates.,? \\Q2013\\E", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": null, "citeRegEx": "Dong and Lapata.,? \\Q2016\\E", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Translating questions to SQL queries with generative parsers discriminatively reranked", "author": ["A. Giordani", "A. Moschitti"], "venue": "In COLING,", "citeRegEx": "Giordani and Moschitti.,? \\Q2012\\E", "shortCiteRegEx": "Giordani and Moschitti.", "year": 2012}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["J. Gu", "Z. Lu", "H. Li", "V.O.K. Li"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "author": ["K. Hashimoto", "C. Xiong", "Y. Tsuruoka", "R. Socher"], "venue": "arXiv, cs.CL", "citeRegEx": "Hashimoto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2016}, {"title": "Can electronic medical record systems transform health care? potential health benefits, savings, and costs", "author": ["R. Hillestad", "J. Bigelow", "A. Bower", "F. Girosi", "R. Meili", "R. Scoville", "R. Taylor"], "venue": "Health affairs,", "citeRegEx": "Hillestad et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hillestad et al\\.", "year": 2005}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning a neural semantic parser from user feedback", "author": ["S. Iyer", "I. Konstas", "A. Cheung", "J. Krishnamurthy", "L. Zettlemoyer"], "venue": null, "citeRegEx": "Iyer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "In Association for Computational Linguistics (ACL) System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Pointer sentinel mixture models", "author": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "venue": null, "citeRegEx": "Merity et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2017}, {"title": "Learning a natural language interface with neural programmer", "author": ["A. Neelakantan", "Q.V. Le", "M. Abadi", "A. McCallum", "D. Amodei"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2017}, {"title": "Application of data mining techniques in customer relationship management: A literature review and classification", "author": ["E.W. Ngai", "L. Xiu", "D.C. Chau"], "venue": "Expert systems with applications,", "citeRegEx": "Ngai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ngai et al\\.", "year": 2009}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "In ACL,", "citeRegEx": "Pasupat and Liang.,? \\Q2015\\E", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Towards a theory of natural language interfaces to databases", "author": ["A.-M. Popescu", "O. Etzioni", "H. Kautz"], "venue": "In Proceedings of the 8th International Conference on Intelligent User Interfaces,", "citeRegEx": "Popescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Popescu et al\\.", "year": 2003}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P.J. Price"], "venue": null, "citeRegEx": "Price.,? \\Q1990\\E", "shortCiteRegEx": "Price.", "year": 1990}, {"title": "Large-scale semantic parsing without question-answer", "author": ["S. Reddy", "M. Lapata", "M. Steedman"], "venue": "pairs. TACL,", "citeRegEx": "Reddy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["J. Schulman", "N. Heess", "T. Weber", "P. Abbeel"], "venue": "In NIPS,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["M.J. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": null, "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Joint learning of ontology and semantic parser from text", "author": ["J. Starc", "D. Mladenic"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Starc and Mladenic.,? \\Q2017\\E", "shortCiteRegEx": "Starc and Mladenic.", "year": 2017}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["L.R. Tang", "R.J. Mooney"], "venue": "In ECML,", "citeRegEx": "Tang and Mooney.,? \\Q2001\\E", "shortCiteRegEx": "Tang and Mooney.", "year": 2001}, {"title": "Building a semantic parser overnight", "author": ["Y. Wang", "J. Berant", "P. Liang"], "venue": "In ACL,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney"], "venue": "In ACL,", "citeRegEx": "Wong and Mooney.,? \\Q2007\\E", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Dynamic coattention networks for question answering", "author": ["C. Xiong", "V. Zhong", "R. Socher"], "venue": "ICRL,", "citeRegEx": "Xiong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2017}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["J.M. Zelle", "R.J. Mooney"], "venue": "In AAAI/IAAI,", "citeRegEx": "Zelle and Mooney.,? \\Q1996\\E", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Zettlemoyer and Collins.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Zettlemoyer and Collins.,? \\Q2007\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "A vast amount of today\u2019s information is stored in relational databases, which provide the foundation of crucial applications such as medical records [Hillestad et al., 2005], financial markets [Beck et al.", "startOffset": 149, "endOffset": 173}, {"referenceID": 4, "context": ", 2005], financial markets [Beck et al., 2000], and customer relations management [Ngai et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 21, "context": ", 2000], and customer relations management [Ngai et al., 2009].", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "A prominent research area at the intersection of natural language processing and human-computer interactions is the study of natural language interfaces (NLI) [Androutsopoulos et al., 1995], which seek to provide means for humans to interact with computers through the use of natural language.", "startOffset": 159, "endOffset": 189}, {"referenceID": 1, "context": "Other work in semantic parsing has focused on learning parsers without relying on annotated logical forms by leveraging conversational logs [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], distant supervision [Cai and Yates, 2013, Reddy et al.", "startOffset": 140, "endOffset": 169}, {"referenceID": 2, "context": "Other work in semantic parsing has focused on learning parsers without relying on annotated logical forms by leveraging conversational logs [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], distant supervision [Cai and Yates, 2013, Reddy et al.", "startOffset": 186, "endOffset": 215}, {"referenceID": 17, "context": ", 2014], and questionanswer pairs [Liang et al., 2011].", "startOffset": 34, "endOffset": 54}, {"referenceID": 31, "context": "Previous semantic parsing systems were typically designed to answer complex and compositional questions over closed-domain, fixed-schema datasets such as GeoQuery [Tang and Mooney, 2001] and ATIS [Price, 1990].", "startOffset": 163, "endOffset": 186}, {"referenceID": 25, "context": "Previous semantic parsing systems were typically designed to answer complex and compositional questions over closed-domain, fixed-schema datasets such as GeoQuery [Tang and Mooney, 2001] and ATIS [Price, 1990].", "startOffset": 196, "endOffset": 209}, {"referenceID": 30, "context": "Researchers have also investigated QA over subsets of large-scale knowledge graphs such as DBPedia [Starc and Mladenic, 2017] and Freebase [Cai and Yates, 2013, Berant et al.", "startOffset": 99, "endOffset": 125}, {"referenceID": 32, "context": "The dataset \u201cOvernight\u201d [Wang et al., 2015] uses a similar crowd-sourcing process to build a dataset of (natural language question, logical form) pairs, but it is comprised of only 8 domains.", "startOffset": 24, "endOffset": 43}, {"referenceID": 22, "context": "WikiTableQuestions [Pasupat and Liang, 2015] is a collection of question and answers, also over a large quantity of tables extracted from Wikipedia.", "startOffset": 19, "endOffset": 44}, {"referenceID": 3, "context": "Our baseline model is based on the attentional sequence to sequence model (Seq2Seq) proposed by [Bahdanau et al., 2015].", "startOffset": 96, "endOffset": 119}, {"referenceID": 19, "context": "This class of models has been successfully applied to tasks such as language modeling [Merity et al., 2017], summarization [Gu et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 11, "context": ", 2017], summarization [Gu et al., 2016], combinatorial optimization [Bello et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 5, "context": ", 2016], combinatorial optimization [Bello et al., 2017], and question answering [Seo et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 9, "context": "However, unlike [Dong and Lapata, 2016], we use pointer based generation instead of Seq2Seq generation and show that our approach achieves higher performance, especially for rare words and column names.", "startOffset": 16, "endOffset": 39}, {"referenceID": 20, "context": "Unlike [Neelakantan et al., 2017], our model does not require access to the table content during inference, which may be unavailable due to privacy concerns.", "startOffset": 7, "endOffset": 33}, {"referenceID": 1, "context": "Other work in semantic parsing has focused on learning parsers without relying on annotated logical forms by leveraging conversational logs [Artzi and Zettlemoyer, 2011], demonstrations [Artzi and Zettlemoyer, 2013], distant supervision [Cai and Yates, 2013, Reddy et al., 2014], and questionanswer pairs [Liang et al., 2011]. Recently, Pasupat and Liang [2015] introduced a floating parser to address the large amount of variation in utterances and table schema for semantic parsing on web tables.", "startOffset": 141, "endOffset": 362}, {"referenceID": 24, "context": "CISE [Popescu et al., 2003], which translates questions to SQL queries and identifies questions that it is not confident about.", "startOffset": 5, "endOffset": 27}, {"referenceID": 10, "context": "Giordani and Moschitti [2012] proposed a system to translate questions to SQL by first generating candidate queries from a grammar then ranking them using tree kernels.", "startOffset": 0, "endOffset": 30}, {"referenceID": 10, "context": "Giordani and Moschitti [2012] proposed a system to translate questions to SQL by first generating candidate queries from a grammar then ranking them using tree kernels. Both of these approaches rely on a high quality grammar and are not suitable for tasks that require generalization to new schema. Iyer et al. [2017] also proposed a system to translate to SQL, but with a Seq2Seq model that is further improved with human feedback.", "startOffset": 0, "endOffset": 318}, {"referenceID": 3, "context": "1 One powerful baseline model is the Seq2Seq model utilized for machine translation [Bahdanau et al., 2015].", "startOffset": 84, "endOffset": 107}, {"referenceID": 14, "context": "The network first encodes x using a two-layer, bidirectional Long Short-Term Memory network [Hochreiter and Schmidhuber, 1997].", "startOffset": 92, "endOffset": 126}, {"referenceID": 14, "context": "out the LSTM equations, which are described by Hochreiter and Schmidhuber [1997]. We then apply a pointer network similar to that proposed by Vinyals et al.", "startOffset": 47, "endOffset": 81}, {"referenceID": 14, "context": "out the LSTM equations, which are described by Hochreiter and Schmidhuber [1997]. We then apply a pointer network similar to that proposed by Vinyals et al. [2015] to the input encodings h.", "startOffset": 47, "endOffset": 164}, {"referenceID": 27, "context": "As described by Schulman et al. [2015], the act of sampling during the forward pass of the network can be followed by the corresponding injection of a synthetic gradient, which is a function of the reward, during the backward pass in order to compute estimated parameter gradient.", "startOffset": 16, "endOffset": 39}, {"referenceID": 31, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 25, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al.", "startOffset": 59, "endOffset": 72}, {"referenceID": 8, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 32, "context": "The datasets are GeoQuery880 [Tang and Mooney, 2001], ATIS [Price, 1990], Free917 [Cai and Yates, 2013], Overnight [Wang et al., 2015], WebQuestions [Berant et al.", "startOffset": 115, "endOffset": 134}, {"referenceID": 6, "context": ", 2015], WebQuestions [Berant et al., 2013], and WikiTableQuestions [Pasupat and Liang, 2015].", "startOffset": 22, "endOffset": 43}, {"referenceID": 22, "context": ", 2013], and WikiTableQuestions [Pasupat and Liang, 2015].", "startOffset": 32, "endOffset": 57}, {"referenceID": 18, "context": "The dataset is tokenized using Stanford CoreNLP [Manning et al., 2014].", "startOffset": 48, "endOffset": 70}, {"referenceID": 23, "context": "We use, and keep fixed, GloVe word embeddings [Pennington et al., 2014] and character n-gram embeddings [Hashimoto et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 12, "context": ", 2014] and character n-gram embeddings [Hashimoto et al., 2016].", "startOffset": 40, "endOffset": 64}, {"referenceID": 16, "context": "We train using ADAM [Kingma and Ba, 2014] and regularize using dropout [Srivastava et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 9, "context": "We compare results against an attentional sequence to sequence baseline based on the neural semantic parser proposed by Dong and Lapata [2016], which achieved state of the art results on a variety of semantic parsing datasets.", "startOffset": 120, "endOffset": 143}], "year": 2017, "abstractText": "A significant amount of the world\u2019s knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-theloop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 87726 hand-annotated examples of questions and SQL queries distributed across 26375 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 60.3% and logical form accuracy from 23.4% to 49.2%.", "creator": "LaTeX with hyperref package"}}}