{"id": "1709.00813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "From Review to Rating: Exploring Dependency Measures for Text Classification", "abstract": "There are various text analysis techniques that attempt to uncover unstructured information from the text. In this paper, we examine the use of statistical dependency measures for text classification, which represent text as word vectors. Study satisfaction scores on a 3-point scale and their free-text comments on university subjects are used as a dataset. We compared two text representations: a frequency word representation and a term frequency relationship to word vectors and found that word vectors provide greater accuracy. However, these word vectors have a large number of features that exacerbate the burden of computational complexity. Therefore, we investigated the use of a nonlinear dependence measure for feature selection, maximizing the dependence between text reviews and the corresponding results. Our quantitative and qualitative analysis of a dataset to student satisfaction shows that our approach achieves comparable accuracy with the full feature vector, while being faster in the order of magnitude test.", "histories": [["v1", "Mon, 4 Sep 2017 05:38:35 GMT  (70kb,D)", "http://arxiv.org/abs/1709.00813v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel cunningham-nelson", "mahsa baktashmotlagh", "wageeh boles"], "accepted": false, "id": "1709.00813"}, "pdf": {"name": "1709.00813.pdf", "metadata": {"source": "CRF", "title": "From Review to Rating: Exploring Dependency Measures for Text Classification", "authors": ["Samuel Cunningham-Nelson", "Mahsa Baktashmotlagh", "Wageeh Boles"], "emails": ["samuel.cunninghamnelson@qut.edu.au", "m.baktashmotlagh@qut.edu.au", "w.boles@qut.edu.au"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nStatistical analysis techniques are used to find patterns in actual data. Statistical dependence measures such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and the Randomized dependence coefficient (RDC) [3] have been extensively used in the areas of pattern recognition and computer vision to find correlation between the random variables. Here, we make use of the state-of-theart non-linear dependency measures for text classification. We first perform feature selection via maximising the dependence of the text comments and the correlated scores, and then train the classifier on the reduced features.\nText analysis methods and techniques are applied to many actual examples of data to obtain meaningful examples and results. Textual data can be used, when looking at product reviews to determine if that product has a positive or negative satisfaction score. Movie reviews consisting of solely text data have also been used to predict ratings or scores [4]. In this work, we use student comments, paired with satisfaction. In this study, machine learning is used to provide a link between the satisfaction score given to a university subject, and free text comment.\nStudent evaluations of teaching are an important part of assessing student satisfaction in a particular class. The feed-\nback from these student evaluations often consists of a score or ranking for several questions, and also the possibility of free text comments. Part of teaching is fulfilling students\u2019 expectations, and feedback and ratings from evaluation allows that to be done [5]. From feedback, pointers and information can be determined and hopefully influence the teaching style and method of delivery.\nIn its most basic form, student satisfaction for a unit (subject) can be rated as either good, neutral or bad. Maas et al [6] show a generic approach to using words as a vector and their application to sentiment analysis. The methods mentioned in this work look at algorithms and search to find patterns and common connections.\nIn this work we aim to investigate the following two research questions,\n1) How can dimensions of data that provide the highest influence be determined? 2) What is the correlation between the text a student writes and the satisfaction score a student gives?\nOur contribution in this work is three-fold. We examine whether there is a dependence between students\u2019 free text comments and the satisfaction score which they give. We introduced a data-set, consisting of many scores and free text comments. We finally use statistical dependency methods, performing a quantitative and qualitative analysis on the results."}, {"heading": "II. LITERATURE REVIEW", "text": "Text analysis is a large area with several different methods and techniques being commonly applied. The literature review discusses several common techniques, focussing on the ones used in this work."}, {"heading": "A. Text Analysis and Bag of Words Representation", "text": "Text data contains a plethora of ingrained or implied information and meaning, which humans are naturally adept at interpreting. This is however harder for machines to extract, often resulting in misinterpreted or entirely lost information which may be crucial to the context. One example of text data that is used often as a bench mark is movie reviews. A study, that provides valuable insights into text data analysis, looks at predicting movie revenue from ratings written by critics [7].\nar X\niv :1\n70 9.\n00 81\n3v 1\n[ cs\n.C L\n] 4\nS ep\n2 01\n7\nTraditionally, a bag of words representation is used to model textual data. This involves transforming the corpus of text into an N \u00d7 M sparse matrix, where N is the number of text responses and M is the number of unique words.\nAnother common enhancement used alongside the bag of words representation is Term Frequency - Inverse Document Frequency (TF-IDF). TF-IDF calculates values for every word which is a proportion of the word frequency in one document with respect to the frequency percentage of all documents the word appears in [8]. The higher this number, suggests a greater importance for this word in the current document. Common words would receive a lower weighting. This can be also expressed in the following expression,\nwx,y = tfx,y \u00d7 log ( N\ndfx\n) (1)\nWhere, wx,y is the weight of term x within document y, tfx,y is the frequency of x in y, dfx is the number of documents containing x, and N is the total number of documents.\nB. Vector Space Models\nAn alternative way to represent textual data, is to use a vector of features for each individual word. These feature vectors have many more dimensions than just a single word, and when developed, are trained from many documents.\n1) Word2Vec: Developed in 2013, Word2Vec is one method of modelling words as vectors [9]. Other versions developed using similar processes exist, however this model was developed using a neural network training approach, and a negative skip gram model [10]. Each word is projected into a 300\u00d7 1 vector. This Word2Vec model has been trained on Google News articles.\n2) Glove: Another alternative to Word2Vec is the word vector representation Glove [11]. This method uses a technique (global matrix factorization) to implement the word vectors differently to Word2Vec. This representation can sometimes outperform Word2Vec, depending on the situation.\n3) t-SNE: Visualising high-dimensional data is an important problem and needs to be considered carefully [12]. Using t-Distributed Stochastic Neighbour Embedding (t-SNE), data which previously has many dimensions can be reduced to just two or three for visualisation purposes [13]. It is useful when high dimensional data has common or important themes across only a few of the dimensions. t-SNE works well with larger amounts of data, as opposed to just a few vectors.\n4) Summary: Word2Vec and Glove are two examples of word vectorisation methods which are commonly used for text analysis. Representing words as vectors allows further embedded information to be uncovered, compared to more simplistic analysis techniques."}, {"heading": "III. BACKGROUND", "text": "In this work, we are interested in measuring the distance between distributions of the student\u2019s comments and their corresponding scores. Generally, the two probability distributions can be compared either through non-parametric models\n(e.g., kernel density estimation), or parametric ones (e.g., using Gaussian Mixture Models).\nHere, we exploit two non-parametric approaches to compute the distribution difference between multiple sources of data: MMD (Maximum Mean Discrepancy), and RDC (Randomised Dependence Coefficient)."}, {"heading": "A. MMD (Maximum Mean Discrepancy)", "text": "Let Xp = {x1p, \u00b7 \u00b7 \u00b7 ,xnp} and Yq = {y1q , \u00b7 \u00b7 \u00b7 ,ynq } be the two sets of i.i.d. observations from two different sources p and q, with n samples, respectively. Using the MMD criterion, we can determine whether p = q.\nDefinition 1: [14] Let F be a class of functions f : X \u2192 R. Then the MMD and its empirical estimate are defined as:\nMMD(F, p, q) = sup f\u2208F\n(Ex\u223cp[f(x)]\u2212 Ey\u223cq[f(y)]) ,\nMMD(F,Xp,Yq) = sup f\u2208F  1 n n\u2211 i=1 f(xip)\u2212 1 n n\u2211 j=1 f(yjq)  . Theorem 1: [14] Let F be a unit ball in a Reproducing Kernel Hilbert Space (RKHS), defined on a compact metric space X with associated kernel k(\u00b7, \u00b7). Then MMD(F, p, q) = 0 if and only if p = q.\nAn empirical estimate of the MMD can be written\nas\n( n\u2211\ni,j=1\nk(xip,x j p)\nn2 + n\u2211 i,j=1 k(yiq,y j q) n2 \u2212 2 n,n\u2211 i,j=1 k(xip,y j q) n2\n) 1 2\nwhere k(\u00b7, \u00b7) is the universal or more general form of the characteristic kernel of the mapping:\nMMD = \u2016 1 n n\u2211 i=1 \u03c6(xip)\u2212 1 n m\u2211 j=1 \u03c6(xjq)\u20162\n= 1\nn2 n\u2211 i,j=1 exp\n( \u2212 (xip \u2212 xjp)T (xip \u2212 xjp)\n\u03c3\n)\n+ 1\nn2 n\u2211 i,j=1 exp\n( \u2212 (yiq \u2212 yjq)T (yiq \u2212 yjq)\n\u03c3\n) (2)\n\u2212 2 n2 n,n\u2211 i,j=1 exp\n( \u2212 (xip \u2212 yjq)T (xip \u2212 yjq)\n\u03c3\n) .\nIn summary, MMD is a powerful non-parametric method that compares the distribution difference between different sources of data by computing the difference between the means of the two sets mapped into a RKHS."}, {"heading": "B. RDC (Randomized Dependence Coefficient)", "text": "The Randomized Dependence Coefficient (RDC) statistic measures the dependence between random samples X \u2208 Rp\u00d7n and Y \u2208 Rq\u00d7n by first applying a copula-transformation on the random samples and projecting the copulas through k randomly chosen non-linear projections, and then finding the largest canonical correlation between these non-linear projections. Given the random samples X \u2208 Rp\u00d7n and Y \u2208 Rq\u00d7n\nand the parameters k \u2208 N+ and s \u2208 R+, the Randomized Dependence Coefficient between X and Y is defined as:\nrdc(X,Y ; k, s) := sup \u03b1,\u03b2\n\u03c1 ( \u03b1T\u03a6(P (X); k, s),\u03b2T\u03a6(P (Y ); k, s) ) .\n(3) where \u03a6(X; k, s) is the k\u2212th order random non-linear projection from X \u2208 Rd\u00d7n to \u03a6(X; k, s) \u2208 Rk\u00d7n:\n\u03a6(X; k, s) :=  \u03c6(w T 1 x1 + b1) \u00b7 \u00b7 \u00b7 \u03c6(wTk x1 + bk) ... ...\n... \u03c6(wT1 xn + b1) \u00b7 \u00b7 \u00b7 \u03c6(wTk xn + bk)\n T\n(4) and \u03c1 is the largest canonical correlation between the nonlinear projections \u03b1TX and \u03b2TY of two random samples X \u2208 Rp\u00d7n and Y \u2208 Rq\u00d7n.\nWe used the Randomized Dependence Coefficient (RDC) as a measure of dependence between the student\u2019s comments and their corresponding scores. RDC is defined as the largest canonical correlation between random non-linear projections of the variables\u2019 copula transformations. Unlike the other non-linear dependence measures such as Kernel Canonical Correlation Analysis (KCCA) [1] and Copula Maximum Mean Discrepancy [2] which exhibit prohibitive running times on large scale data, RDC has low computational cost of O(n log n), where n is the number of samples. Moreover, RDC is easy to implement, invariant to monotonically increasing transformations, and performs well under the existence of additive noise."}, {"heading": "IV. PROPOSED APPROACH", "text": "In this work a combination of word vectors and feature reduction techniques are used. This section will discuss how these methods were applied."}, {"heading": "A. Word2Vec", "text": "Word2Vec is one method for training word vectors. The Google News data set contains word vectors which were trained on around 100 billion words from Google News articles. Some other common models are trained on data sets such as Wikipedia, but the Google News set is more widely used.\nFigure 1 below shows the neural network model in which Word2Vec is trained. The network has a single hidden layer. Both the output and output layers consist of the number of neurons equal to the number of unique words in the training vocabulary. The number of neurons in the hidden layer is equal to the dimentionality of the word vector [15].\nFigure 2 shows an example of how Word2Vec can model particular words once reduced to 2 dimensional space (in this case using PCA). You can see relationships between certain words represented in a vector form. As an equation, we can express the relationship between these vectors in the following equation,\nV (king)\u2212 V (man) + V (woman) \u2248 V (queen) (5)\nThis equation describes a gender relationship, which has been learnt through embedded neural networks. As a further example, finding the angle between two vectors demonstrates the similarity between those two words.\nAnother example of a gender relationship demonstrated in Figure 2 is the equation below.\nV (uncle)\u2212 V (man) + V (woman) \u2248 V (aunt) (6)\nThese are just two examples of equations which demonstrate the implicit understanding of the Word2Vec model trained on many articles."}, {"heading": "B. Choosing Dimensionality", "text": "We validated the empirical performance of RDC and MMD on the student\u2019s evaluation data of university subjects. The random dimension for RDC was set to 20 as we could not observe any improvements for embedding dimensions d \u2265 100. Moreover, as noted in [16], the randomised embedding dimension should not be too much smaller than the original dimension D to prevent a point in the set from being mapped to the origin. To ensure this, we refer to the Universality Law for the Embedding Dimension from [16], [17]:\nTheorem 1: A Universality Law for the Embedding Dimension. Given the n \u00d7 h random projector \u03c6 with the\nparameters p > 4, \u03bd \u2265 1, % \u2208 (0, 1), and \u03b5 \u2208 (0, 1), there is a number N := N(p, \u03bd, %, \u03b5) for which the following statement holds. Suppose that the ambient dimension n \u2265 N ; E is a nonempty, compact subset of Rn that does not contain the origin; the statistical dimension of E is proportional to the ambient dimension: %n \u2264 \u03b8(E) \u2264 n. Then h \u2265 (1 + \u03b5)\u03b8(E) implies P{0 /\u2208 \u03c6(E)} \u2265 1 \u2212 Cpn1\u2212 p 4 . Furthermore, if \u03b8(E) is spherically convex, then h \u2264 (1 \u2212 \u03b5)\u03b8(E) implies P{0 \u2208 \u03c6(E)} \u2265 1\u2212 Cpn1\u2212 p 4 .\nWe performed greedy word selection to construct the subset of words in the students comments that maximises the dependence between the word set and the associated score value. We reported the results of our algorithm as a set of words which has the maximum dependency to the target value.\nThe computational complexity of RDC is O((p+q)n log n+ kn log(pq) + k2n) where p and q are the dimensions of the random variables, n is the sample size, and k is the reduced dimension. The cost of RDC can be approximated by O(n log n) if applied on large scale data (very large n)."}, {"heading": "C. Feature Reduction", "text": "The proposed approach for feature reduction uses a greedy search method for calculating correlation with both RDC and MMD. This has also been compared to PCA for reference. It was decided that the features for each vector would be reduced from 300 to 20 using each method. For both RDC and MMD, we started with 1 feature for all words (N \u00d71), and found the maximum correlation between a feature, and the output label. After the optimum single feature was chosen, this was then repeated, adding another feature to the input, and maximising that correlation. This was repeated, until the 20 features with the combined highest correlation where selected. Figure 3 below shows an example of this graphically. In this figure, N is the number of text responses, C is the correlation, and i is the number of features being tested (between 1 and 20). The function being used is either the RDC or MMD feature correlation.\nThis was designed to maximise the correlation, adding one feature at a time."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. The Data Set", "text": "The data for this experiment was gathered from the Queensland University of Technology. QUT runs two student surveys titled \u2018Pulse\u2019 and \u2018Insight\u2019 as a part of the framework \u2018Reframe\u2019 for evaluating learning and teaching [18]. The first survey, the Pulse survey, solicits students\u2019 feedback in the early weeks of the semester. The second survey, titled Insight, surveys students at the end of semester. In each of these surveys, students are asked to rate their views on three statements, however we focus on the final question \u201cI am satisfied with this unit so far.\u201d\nEach statement was responded to on the Likert scale, 1 being disagree strongly, and 5 being agree strongly. After answering these questions, an open ended response was left optional, for students to respond to with feedback and suggestions asking them to \u201cPlease provide any further feedback you may have about this unit.\u201d. This feedback provides recommendations and suggestions for teaching staff to read and take into consideration.\nThis study and analysis used Pulse and Insight survey data from 19 units in the Science and Engineering Faculty, over a 4- year duration. The data consisted of more than 3000 responses which included the numerical and text feedback.\nExamples of some responses include, \u2022 Satisfaction rating: 4. Free response: \u201cExcellent work.\nVery helpful staff and lots of assistance given via video tutorials.\u201d \u2022 Satisfaction rating: 5. \u201cGreat structured unit. Very well organised and great learning environments\u201d \u2022 Satisfaction rating: 1. Free response: \u201cThe unit moved too quickly through concepts, and the content in the workshops has not helped with assessment items\u201d \u2022 Satisfaction rating: 2. Free response: \u201cThe assessment and the lectures didn\u2019t relate well - very confusing overall. Not enough practice problems.\u201d"}, {"heading": "B. Procedure", "text": "The procedure was broken down into four major steps: Data Preparation and Preprocessing, Removing excess data, Reducing dimensionality and applying various machine models.\n1) Data Preparation and Preprocessing: The text data for each student response was initially preprocessed. This involves: \u2022 Turning all text into lower case. \u2022 Remove punctuation from the text data. \u2022 Remove stop words (i.e. \u2018the\u2019, \u2018and\u2019, \u2018it\u2019) \u2022 Split data into individual words, instead of one sentence.\nAfter this, the pre-existing Word2Vec model, trained from news articles was applied to each individual student response. Each word in the response was transformed into a vector, and the average of all these vectors taken. This vector was then normalised before further analysis was undertaken.\nInitial results, as well as manual inspection determined that being able to tell the difference between strongly agree (5/5)\nand agree (4/5), even for the human eye can be difficult. The classes were then rounded from the original 5 categories into 3 smaller categories (Disagree, Neutral and Agree).\n1) Category 1 - 1 or 2 out of 5 (Disagree) 2) Category 2 - 3 out of 5 (Neutral) 3) Category 3 - 4 or 5 out of 5 (Agree)\n2) Remove Excess Data: Initial work showed higher accuracies, however it was noticed that much of the data was unbalanced - many more agree responses. This was unfortunately giving predictions which didn\u2019t match expectations. The data was therefore levelled out to have similar numbers in all three chosen classes\n3) Reduce Dimensionality: In several of the experiments, we used dimension reducing techniques (PCA, MMD and RDC) to reduce the complexity of the data. Following the procedure discussed previously in Section IV, the 300 dimension feature vectors were reduced down to be 20 dimensions.\n4) Apply Models: Several common machine learning models were selected to evaluate the accuracy of the methods being testes. These methods are listed below. Several of these methods also required additional parameters which were configured.\n\u2022 Extra Trees Classifier (Etrees) \u2022 Linear Discriminant Analysis (LDA) - Using an SVD\nsolver \u2022 Logistic Regression (Log) - Regularisation strength value\nof C = 1 \u2022 k-Nearest Neighbour (kNN) - Using the 5 nearest neigh-\nbours, k = 5, and a uniform weighting \u2022 Decision Tree Classifier (DT) \u2022 Gaussian Naive Bayes (G-NB) \u2022 Support Vector Machine - Linear (L-SVM) \u2022 Support Vector Machine - Gaussian (G-SVM) - RBF\nkernel used, with a Regularisation strength value of C = 1 and the variance \u03c3 of the Gaussian kernel is systematically chosen to be the median squared distance between all training samples. \u2022 Support Vector Regression - Linear (L-SVR) - since the output of this method is continuous, results were rounded to the nearest class. \u2022 Support Vector Regression - Gaussian (G-SVR) - since the output of this method is continuous, results were rounded to the nearest class. RBF kernel used, with a Regularisation strength value of C = 1 and the variance \u03c3 of the Gaussian kernel is systematically chosen to be the median squared distance between all training samples.\nAfter configuring each of these models, they were applied in a 5-fold validation, with an 80/20 training test data split."}, {"heading": "C. Quantitative Results", "text": "1) Bag of Words and TF-IDF Accuracies: Bag of words and TF-IDF are used first to get an initial accuracy. Several common machine learning algorithms were picked as initial benchmarks for testing.\nThese results here show initial promising accuracies, especially for the SVMs - the TF-IDF slightly outperforming the BOW method.\n2) Word2Vec Accuracies: Representing the words as vectors, allows more information to be uncovered. These results show Word2Vec with the same machine learning models that were tested above.\nSince the Word2Vec model has a high dimensionality space (300), different methods were explored to reduce the complexity of the data. These vectors were reduced to 20 dimensions, following the procedure mentioned previously. Table II shows some accuracies from both sets of methods.\nThe initial results from Table II show that the Gaussian Kernel - SVM produced the highest accuracy (51.67%) in predicting student satisfaction scores. The accuracy was also higher than the best accuracy from the bag of words and TFIDF methods demonstrated above.\nWe can also see that in terms of accuracy the reduced dimension vectors (20 dimensions) do not perform as well as the full 300 dimension vectors. They however, do have significantly less computational cost. PCA performs best from\nthe compared techniques, and the accuracy of MMD is significantly less. However, comparing the computational cost of RDC against PCA, RDC is smaller. This demonstrates the trade-off between accuracy and complexity. RDC and MMD were used as state-of-the-art methods, and PCA used as a comparison point."}, {"heading": "D. Qualitative Results", "text": "Previous results sections above have looked at the accuracies of individual methods, in this section we focus on several selected comments in further detail. In this section Pos refers to positive, Neu to negative and Neg to negative.\n1) Correctly Classified Responses: The examples in this first section show examples of comments that were classified correctly by the Gaussian SVM model, as well as all three tested feature reduction techniques.\nResponse: \u201cNot having the answers available from tutorials I felt was a disadvantage. This is an introductory course, it\u2019s not like we are 4th year electrical students. When struggling to learn the content and work out questions and not having a concrete answer to check was very demotivating. The people who would abuse having answers available and not do the work for them selves are the ones that wouldn\u2019t do the work in the first place. I felt like the tutorial class sizes were too big and it would of been on benefit to have smaller groups to encourage discussion. Assessment so far has been set out well.\u201d\nThis first example above was rated as a \u201cnegative\u201d result by a student, which is also clear to the reader when examining it as well. The Gaussian SVM, as well as the three different feature reduction methods (RDC, MMD and PCA) all predict this to be a positive result also. This demonstrates the ability for Word2Vec to operate on long responses, even though the average is computed for all the words in the response. The feature reduction methods also retain the important dimensional vectors for classifying the response.\nResponse: \u201cThis unit is presented well by (Lecturer Name). He is enthusiastic and his teacher/student interaction is good. The only very minor issue that I may have is that his examples are sometimes hard to read with his handwriting, especially if he is using a thick point pen, like a white board marker. All in all the unit is good and he explains it all well.\u201d\nThis example was rated \u201cpositive\u201d, once again showing correlation between the satisfaction score given, as well as the scores predicted by the algorithms. The only discrepancy here was the prediction from MMD. This however is expected, as the overall accuracy for MMD was much lower than the PCA or RDC methods.\nResponse: \u201dGood unit.\u201d This result, similar to the previous one was rated as \u201cpositive\u201d by the student. This was correctly classified by the Gaussian SVM, as well as all three feature reduction methods. This example was included to demonstrate that even short responses of only a few words can still be correctly classified.\n2) Incorrectly Labelled Responses: Included in this section are several responses that were \u2019classified incorrect\u2019, however upon further inspection, are perhaps underlying difficulties associated with the data set.\nResponse: \u201cLectures are not engaging. Feels like either way too much content to go through for 2 hours lecture pace is a bit fast tutorials are good\u201d\nReading this review here, it appears that the students comments are mostly negative, however this was marked as \u201cNeutral\u201d by the corresponding student. The G-SVM, PCA and MMD methods all classified this response as \u201cNeutral\u201d, but it was marked negative by the RDC reduction method. Although this response was technically classified incorrect by RDC, the we believe that the RDC chosen category is more suitable.\nResponse: \u201cLecturer has to explain things a bit better as most students are currently completing (Unit Name) at the same time.\u201d\nAgain, this is another example which the RDC reduced feature vector produces a better representation. The rating given by the student was \u201cpositive\u201d, however reading the comment, it appears to be most likely Neutral, or slightly negative. Both RDC and MMD scored this comment as \u201cNeutral\u201d, which we believe is more indicative of the comment."}, {"heading": "E. Summary", "text": "The selected comments above are a demonstration of some examples where the chosen algorithms have correctly identified the label category, and examples of where this is not the case. This demonstrates RDC being able to select relevant feature vectors from the many provided by Word2Vec.\nThe qualitative results also demonstrate the importance of not solely relying on accuracy scores for these comments, especially since the satisfaction score and students comments may not line up, or give all relevant information.\nBoth the qualitative and quantitative results highlight different points of importance. The quantitative results demonstrate the benefit of modelling the students responses as vectors, and show that the Gaussian SVM returns the best classified results. We also see however, that examining some of the results further in detail, feature reduction techniques, mainly demonstrate classification which perhaps follows more closely to"}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "In this work, we have compared several text analysis techniques in the context of predicting student satisfaction. We have demonstrated a strong correlation between student satisfaction scores for a particular unit. Word2Vec provided the best accuracy in predicting satisfaction when using a Gaussian SVM model. The Word2Vec model used contained a large number of dimensions, so we also explored several feature reduction techniques. PCA, as expected, provided the best numerical accuracy of the three tested techniques (PCA, RDC and MMD), however RDC was a close second. Looking at several responses qualitatively, RDC was shown to obtain results which were not necessarily classified correctly, but when reading the response manually, seemed to fit more into the category which RDC placed it in. These techniques allow those dimensions which have the highest influence to be found.\nThis work provides a comparison between various machine learning models and feature reduction techniques. A strong correlation was shown between the text written by a student, and the score that they give. The initial accuracy provides a promising result, but there are several ways that we believe the performance could be improved.\nDictionary learning could be an alternative approach to improving accuracy. Learning a specific dictionary for this task would allow important words to be selected, and means that ones which are less relevant could be filtered out. Words that do not impact the score, need not be considered.\nFinally, one aspect that is lacking from the model currently being used is word order. When the word vector is calculated the vectors are averaged across the sentence. This means that a sentence with the same words in a different order would have the same word vector. Using a Recurrent Neural Network as part of the process would allow word order within sentences to become a part of the process as well.\nThis work shows the application of these techniques on one set of textual data, and the corresponding scores, however the procedures shown can be applied on various other datasets."}, {"heading": "ACKNOWLEDGEMENT", "text": "The authors would like to thank QUT for providing the data, and QUT students for completing the survey."}], "references": [{"title": "Kernel Independent Component Analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1\u201348, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Copula-based Kernel Dependency Measures", "author": ["J. Schneider", "B. Poczos", "Z. Ghahramani"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), pp. 775\u2013782, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The randomized dependence coefficient", "author": ["D. Lopez-Paz", "P. Hennig", "B. Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems, 2013, pp. 1\u20139.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "author": ["P.D. Turney"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002, pp. 417\u2013424.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Multimodels of quality in education", "author": ["Y. Cheong Cheng", "W. Ming Tam"], "venue": "Quality Assurance in Education, vol. 5, no. 1, pp. 22\u201331, 1997. [Online]. Available: http://www.emeraldinsight.com/doi/10.1108/ 09684889710156558", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142\u2013150, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Movie Reviews and Revenues : An Experiment in Text Regression", "author": ["M. Joshi", "D. Das", "K. Gimpel", "N.A. Smith"], "venue": "no. June, pp. 293\u2013296, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Using TF-IDF to Determine Word Relevance in Document Queries", "author": ["J. Ramos", "J. Eden", "R. Edu"], "venue": "Proceedings of the first instructional conference on machine learning, vol. 242, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Nips, pp. 1\u20139, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "word2vec Explained: Deriving Mikolov et al.\u2019s Negative-Sampling Word-Embedding Method", "author": ["Y. Goldberg", "O. Levy"], "venue": "arXiv preprint arXiv:1402.3722, no. 2, pp. 1\u20135, 2014. [Online]. Available: http: //arxiv.org/abs/1402.3722", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pp. 1532\u20131543, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Text comparison using word vector representations and dimensionality reduction", "author": ["H. Heuer"], "venue": "no. Euroscipy, pp. 13\u201316, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing Data using t-SNE", "author": ["L. Van Der Maaten", "G. Hinton", "G.H. van der Maaten"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2579\u20132605, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A kernel method for the two-sample-problem", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Advances in Neural Information Processing Systems, vol. 19, no. 157, pp. 0\u201343, 2006. [Online]. Available: machinelearning.wustl.edu/mlpapers/paper{ }files/NIPS2006{ }583.pdf", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic Similarity", "author": ["K. Sugathadasa", "B. Ayesha", "N. de Silva", "A.S. Perera", "V. Jayawardana", "D. Lakmal", "M. Perera"], "venue": "arXiv preprint arXiv:1706.01967, 2017. [Online]. Available: http://arxiv.org/abs/1706. 01967", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1967}, {"title": "Universality laws for randomized dimension reduction, with applications", "author": ["S. Oymak", "J.A. Tropp"], "venue": "no. November 2015, 2015. [Online]. Available: http://arxiv.org/abs/1511.09433", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust Domain Generalisation by Enforcing Distribution Invariance", "author": ["S.M. Erfani", "M. Baktashmotlagh", "M. Moshtaghi", "V. Nguyen", "C. Leckie", "J. Bailey", "K. Ramamohanarao"], "venue": "IJCAI, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Reframe: Queensland university of technology\u2019s evaluation framework", "author": ["Queensland University of Technology"], "venue": "May 2013. [Online]. Available: https://cms.qut.edu.au/{ }data/assets/pdf{ }file/ 0007/261718/reframe-qut-evaluation-framework{ }2013.pdf", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Statistical dependence measures such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and the Randomized dependence coefficient (RDC) [3] have been extensively used in the areas of pattern recognition and computer vision to find correlation between the random variables.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Statistical dependence measures such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and the Randomized dependence coefficient (RDC) [3] have been extensively used in the areas of pattern recognition and computer vision to find correlation between the random variables.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Statistical dependence measures such as Canonical Correlation Analysis (CCA) [1], Maximum Mean Discrepancy (MMD) [2], and the Randomized dependence coefficient (RDC) [3] have been extensively used in the areas of pattern recognition and computer vision to find correlation between the random variables.", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "Movie reviews consisting of solely text data have also been used to predict ratings or scores [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Part of teaching is fulfilling students\u2019 expectations, and feedback and ratings from evaluation allows that to be done [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Maas et al [6] show a generic approach to using words as a vector and their application to sentiment analysis.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "A study, that provides valuable insights into text data analysis, looks at predicting movie revenue from ratings written by critics [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "TF-IDF calculates values for every word which is a proportion of the word frequency in one document with respect to the frequency percentage of all documents the word appears in [8].", "startOffset": 178, "endOffset": 181}, {"referenceID": 8, "context": "1) Word2Vec: Developed in 2013, Word2Vec is one method of modelling words as vectors [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Other versions developed using similar processes exist, however this model was developed using a neural network training approach, and a negative skip gram model [10].", "startOffset": 162, "endOffset": 166}, {"referenceID": 10, "context": "2) Glove: Another alternative to Word2Vec is the word vector representation Glove [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "3) t-SNE: Visualising high-dimensional data is an important problem and needs to be considered carefully [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Using t-Distributed Stochastic Neighbour Embedding (t-SNE), data which previously has many dimensions can be reduced to just two or three for visualisation purposes [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 13, "context": "Definition 1: [14] Let F be a class of functions f : X \u2192 R.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Theorem 1: [14] Let F be a unit ball in a Reproducing Kernel Hilbert Space (RKHS), defined on a compact metric space X with associated kernel k(\u00b7, \u00b7).", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "Unlike the other non-linear dependence measures such as Kernel Canonical Correlation Analysis (KCCA) [1] and Copula Maximum Mean Discrepancy [2] which exhibit prohibitive running times on large scale data, RDC has low computational cost of O(n log n), where n is the number of samples.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Unlike the other non-linear dependence measures such as Kernel Canonical Correlation Analysis (KCCA) [1] and Copula Maximum Mean Discrepancy [2] which exhibit prohibitive running times on large scale data, RDC has low computational cost of O(n log n), where n is the number of samples.", "startOffset": 141, "endOffset": 144}, {"referenceID": 14, "context": "The number of neurons in the hidden layer is equal to the dimentionality of the word vector [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "Word2Vec Model [15]", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Moreover, as noted in [16], the randomised embedding dimension should not be too much smaller than the original dimension D to prevent a point in the set from being mapped to the origin.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "To ensure this, we refer to the Universality Law for the Embedding Dimension from [16], [17]: Theorem 1: A Universality Law for the Embedding Dimension.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "To ensure this, we refer to the Universality Law for the Embedding Dimension from [16], [17]: Theorem 1: A Universality Law for the Embedding Dimension.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "QUT runs two student surveys titled \u2018Pulse\u2019 and \u2018Insight\u2019 as a part of the framework \u2018Reframe\u2019 for evaluating learning and teaching [18].", "startOffset": 132, "endOffset": 136}], "year": 2017, "abstractText": "Various text analysis techniques exist, which attempt to uncover unstructured information from text. In this work, we explore using statistical dependence measures for textual classification, representing text as word vectors. Student satisfaction scores on a 3-point scale and their free text comments written about university subjects are used as the dataset. We have compared two textual representations: a frequency word representation and term frequency relationship to word vectors, and found that word vectors provide a greater accuracy. However, these word vectors have a large number of features which aggravates the burden of computational complexity. Thus, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between the text reviews and corresponding scores. Our quantitative and qualitative analysis on a student satisfaction dataset shows that our approach achieves comparable accuracy to the full feature vector, while being an order of magnitude faster in testing. These text analysis and feature reduction techniques can be used for other textual data applications such as sentiment analysis.", "creator": "LaTeX with hyperref package"}}}