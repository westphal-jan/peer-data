{"id": "1709.01073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Predicting Remaining Useful Life using Time Series Embeddings based on Recurrent Neural Networks", "abstract": "We look at the problem of estimating the remaining service life (RUL) of a system or machine based on sensor data. Many approaches to estimating RUL based on sensor data assume the degradation of machines. In addition, sensor data from machines is noisy and often suffers from a lack of values in many practical environments. We propose Embed-RUL: a novel approach to RUL estimation of sensor data that is not based on assumptions about degradation trends, is noise-resistant and can handle missing values. Embed-RUL uses a sequence-to-sequence model that relies on recurring neural networks (RNNNs) to generate embedding for multivariate time series sub-sequences. Embedding for normal and degraded machines tend to differ, and thus prove useful for the RUL estimation to relate to the overall behavior of the two types of roughness filters, showing that the overall behavior of the two corresemble each other.", "histories": [["v1", "Mon, 4 Sep 2017 12:15:44 GMT  (920kb,D)", "http://arxiv.org/abs/1709.01073v1", "Presented at 2nd ML for PHM Workshop at SIGKDD 2017, Halifax, Canada"], ["v2", "Fri, 6 Oct 2017 10:42:52 GMT  (892kb,D)", "http://arxiv.org/abs/1709.01073v2", "Presented at 2nd ML for PHM Workshop at SIGKDD 2017, Halifax, Canada"]], "COMMENTS": "Presented at 2nd ML for PHM Workshop at SIGKDD 2017, Halifax, Canada", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["narendhar gugulothu", "vishnu tv", "pankaj malhotra", "lovekesh vig", "puneet agarwal", "gautam shroff"], "accepted": false, "id": "1709.01073"}, "pdf": {"name": "1709.01073.pdf", "metadata": {"source": "META", "title": "Predicting Remaining Useful Life using Time Series Embeddings based on Recurrent Neural NetworksCopyright \u00a9 2017 Tata Consultancy Services Ltd. ", "authors": ["Narendhar Gugulothu", "Pankaj Malhotra", "Lovekesh Vig", "Puneet Agarwal", "Gautam Shro"], "emails": ["narendhar.g@tcs.com", "vishnu.tv@tcs.com", "malhotra.pankaj@tcs.com", "lovekesh.vig@tcs.com", "puneet.a@tcs.com", "gautam.shro@tcs.com"], "sections": [{"heading": null, "text": "KEYWORDS Recurrent Neural Networks, Remaining Useful Life, Embeddings, Multivariate Time Series Representations, Machine Health Monitoring\nACM Reference format: Narendhar Gugulothu, Vishnu TV, Pankaj Malhotra, Lovekesh Vig, Puneet Agarwal, Gautam Shro . 2017. Predicting Remaining Useful Life using Time Series Embeddings based on Recurrent Neural Networks1 . In Proceedings of 2nd ML for PHM Workshop at Special Interest Group on Knowledge Discovery and Data Mining, Canada, Aug 2017 (SIGKDD), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn"}, {"heading": "1 INTRODUCTION", "text": "It is quite common in the current era of the \u2018Industrial Internet of ings\u2019 [9] for a large number of sensors to be installed for monitoring the operational behavior of machines. Consequently, there is considerable interest in exploiting data from such sensors for health monitoring tasks such as anomaly detection, fault detection, as well as prognostics, i.e., estimating remaining useful life (RUL) of machines in the eld.\n1Copyright \u00a9 2017 Tata Consultancy Services Ltd.\nWe highlight some of the practical challenges in using datadriven approaches for health monitoring and RUL estimation, and propose an approach that can handle these challenges: 1) Health degradation trend: In complex machines with several components, it is di cult to build physics based models for health degradation analysis. Many data-driven approaches assume a degradation trend, e.g., exponential degradation [5, 8, 34, 39, 45]. is is particularly useful in cases where there is no explicit measurable parameter of the health of a machine. Such an assumption may not hold in other scenarios, e.g., when a component in a machine is approaching failure, the symptoms in the sensor data may initially be intermi ent and then grow over time in a non-exponential manner. 2) Noisy sensor readings: Sensor readings o en su er from varying levels of environmental noise which entails the use of denoising techniques. e amount of noise may even vary across sensors. 3) Partial unavailability of sensor data: Sensor data may be partially unavailable due to several reasons such as network communication loss and damaged or faulty sensors. 4) Complex temporal dependencies between sensors: Multiple components interact with each other in a complex way leading to complex dependencies between sensor readings. For example, a change in one sensor may lead to a change in another sensor a er a delay of few seconds or even hours. It is desirable to have an approach that can capture the complex operational behavior of machine(s) from sensor readings while accounting for temporal dependencies.\nIn this paper, we propose Embed-RUL: an approach for RUL estimation using Recurrent Neural Networks (RNNs) to address the above challenges. An RNN is used as an encoder to obtain a xeddimensional representation that serves as an embedding for multisensor time series data. e health of a machine at any point of time can be estimated by comparing an embedding computed using recent sensor history with representative embeddings computed for periods of normal behavior. Our approach for RUL estimation does not rely on degradation trend assumptions, can handle noise and missing values, and can capture complex temporal dependencies among the sensors. e key contributions of this work are:\n\u2022 We show that time series embeddings or representations obtained using an RNN Encoder are useful for RUL estimation (refer Section 5.2). \u2022 We show that embeddings are robust and perform well for the RUL estimation task even under noisy conditions, i.e., when sensor readings are noisy (refer Section 5.3). \u2022 Our approach compares favorably to previous benchmarks\nfor RUL estimation [24] on the turbofan engine dataset [38] as well as on a real-world pump dataset (refer Section 5.2).\nar X\niv :1\n70 9.\n01 07\n3v 1\n[ cs\n.L G\n] 4\nS ep\n2 01\n7\ne rest of the paper is organized as follows: We provide a review of related work in Section 2. Section 3 motivates our approach and brie y introduces existing RNN-based approaches for machine health monitoring and RUL estimation using sensor data. In Section 4 we explain our proposed approach for RUL estimation, and provide experimental details and observations in Section 5, and conclude in Section 6."}, {"heading": "2 RELATEDWORK", "text": "Data-driven approaches for RUL estimation: Several approaches for RUL estimation based on sensor data have been proposed. A review of these approaches can be found in [40]. [11, 19] propose estimating RUL directly by calculating the similarity between the sensors without deriving any health estimates. Similarly, Support Vector Regression [18], RNNs [14], Deep Convolutional Neural Networks [3] have been proposed to estimate the RUL directly by modeling the relations among the sensors without estimating the health of the machines. However, unlike Embed-RUL, none of these approaches focus on robust RUL estimation, and in particular, on robustness to noise.\nRobust RUL Estimation: Wavelet lters have been proposed to handle noise for robust performance degradation assessment in [33]. In [16], ensemble of models is used to ensure that predictions are robust. Our proposed approach handles noise in sensor readings by learning robust representations from sensor data via RNN EncoderDecoder (RNN-ED) models.\nTime series representation learning: Unsupervised representation learning for sequences using RNNs has been proposed for applications in various domains including text, video, speech, and time series (e.g., sensor data). Long Short Term Memory (LSTM) [15] based encoders trained using encoder-decoder framework have been proposed to learn representations of video sequences [42]. Pre-trained LSTM Encoder based on autoencoders are used to initialize networks for classi cation tasks and are shown to achieve improved performance [10] for text applications. Gated Recurrent Units (GRUs) [7] based encoder named Timenet [25] has been recently proposed to obtain embeddings for time series from several domains. e embeddings are shown to be e ective for time series classi cation tasks. Stacked denoising autoencoders have been used to learn hierarchical features from sensor data in [46]. ese features are shown to be useful for anomaly detection. However, to the best of our knowledge, the proposed Embed-RUL is the rst a empt at using RNN-based embeddings of multivariate sensor data for machine health monitoring, and more speci cally, for RUL estimation.\nOther Deep learning models for Machine Health Monitoring: Various architectures based on Restricted Boltzmann Machines, RNNs (discussed in Section 3.2) and Convolutional Neural Networks have been proposed for machine health monitoring in di erent contexts. Many of these architectures and applications of deep learning to machine health monitoring have been surveyed in [48]. An end-to-end convolutional selective autoencoder for early detection and monitoring of combustion instabilites in high speed ame video frames was proposed in [2]. A combination of deep learning and survival analysis for asset health management has been proposed in [20] using sequential data by stacking a LSTM layer, a feed forward layer\nand a survival model layer to arrive at the asset failure probability. Deep belief networks and autoencoders have been used for health monitoring of aerospace and building systems in [36]. However, none of these approaches are proposed for RUL estimation. Predicting milling machine tool wear from sensor data has been proposed using deep LSTM networks in [47]. In [49], a convolutional bidirectional LSTM network along with fully connected layers at the top is shown to predict tool wear. e convolutional layer extracts robust local features while LSTM layer encodes temporal information. ese methods model the problem of degradation estimation in a supervised manner unlike our approach of estimating machine health using embeddings generated using seq2seq models."}, {"heading": "3 BACKGROUND", "text": "Many data-driven approaches a empt to estimate the health of a machine from sensor data in terms of a health index (HI) (e.g., [34, 45]). e trend of HI over time, referred to as HI curve, is then used to estimate the RUL by comparing it with the trends of failed instances. e HI curve for a test instance is compared with the HI curve of failed (train) instance to estimate the RUL of the test instance, as shown in Figure 1. In general, the HI curve of the test instance is compared with HI curves of several failed instances, and weighted average of the obtained RUL estimates from the failed instances is used to obtain the nal RUL estimate (refer Section 4.3 for details).\nIn Section 3.1, we introduce a simple approach for HI estimation that maps the current sensor readings to HI. Next, we introduce existing HI estimation techniques that leverage RNNs to capture the temporal pa erns in sensor readings, and provide a motivation for our approach in Section 3.2."}, {"heading": "3.1 Degradation trend assumption based HI estimation", "text": "Consider a HI curve H = [h1,h2, . . . ,hT ], where 0 \u2264ht \u2264 1, t = 1, 2, . . . ,T . When a machine is healthy, ht = 1 and when a machine is near failure or about to fail, ht = 0. e multi-sensor readings xt \u2208 Rn at time t can be used to obtain an estimate h\u2032t for the actual HI value ht . One way of obtaining this mapping is via a linear regression model: h\u2032t = f\u03b8 (xt ) = \u03b8T xt + \u03b80, where \u03b8 \u2208 Rn and \u03b80 \u2208 R. e parameters \u03b8 and \u03b80 are estimated by minimizing\u2211T t=1 (h\u2032t \u2212 ht )2, where the target HI curve can be assumed to follow an exponential degradation trend (e.g., [45]). Once the mapping is learned, the sensor readings at a time instant can be used to obtain HI. Such approaches have two shortcomings: i) they rely on an assumption about the degradation trend, ii) they do not take into account the temporal aspect of the sensor data. We show that the target HI curve for learning such a mapping (i.e., learning the parameters \u03b8 and \u03b80) can be obtained using RNN models instead of relying on the exponential assumption (refer Section 5 for details)."}, {"heading": "3.2 RNNs for Machine Health Monitoring", "text": "RNNs, especially those based on LSTM units or GRUs have been successfully used to achieve state-of-the-art results on sequence modeling tasks such as machine translation [7] and speech recognition [13]. Recently, deep RNNs have been shown to be useful for\nhealth monitoring from multi-sensor time series data [12, 23, 26]. e key idea behind using RNNs for health monitoring is to learn a temporal model of the system by capturing the complex temporal as well as instantaneous dependencies between sensor readings.\nAutoencoders have been used to discover interesting structures in the data by means of regularization such as by adding constraints on the number of hidden units of the autoencoder [29], or by adding noise to the input and training the network to reconstruct a denoised version of the input [44]. e key idea behind such autoencoders is that the hidden representation obtained for an input retains the underlying important pa ern(s) in the input and ignores the noise component.\nRNN autoencoders have been shown to be useful for RUL estimation [24] in which the RNN-based model learns to capture the behavior of a machine by learning to reconstruct multivariate time series corresponding to normal behavior in an unsupervised manner. Since the network is trained only on time series corresponding to normal behavior, it is expected to reconstruct the normal behavior well and perform poorly while reconstructing the abnormal behavior. is results in small reconstruction error for normal time series and large reconstruction error for abnormal time series. e reconstruction error is then used as a proxy to estimate the health or degree of degradation, and in turn estimate the RUL of the machine. We refer to this reconstruction error based approach for RUL estimation as Recon-RUL.\nWe propose to learn robust xed-dimensional representations for multi-sensor time series data via sequence-to-sequence [4, 43] autoencoders based on RNNs. Here we brie y introduce multilayered RNNs based on GRUs that serve as building blocks of sequence-tosequence autoencoders (refer Section 4 for details).\n3.2.1 Multilayered RNN with Dropout. We use Gated Recurrent Units [7] in the hidden layers of sequence-to-sequence autoencoder. Dropout is used for regularization [32, 41] and is applied only to the non-recurrent connections, ensuring information ow across timesteps. For a multilayered RNN with L hidden layers, the hidden state zlt at time t for lth hidden layer is obtained from zlt\u22121 and zl\u22121t as in Equation 1. e time series goes through the following transformations iteratively for t = 1 through T , where T is length of the time series:\nreset \u0434ate : rlt = \u03c3 (Wlr \u00b7 [D(zl\u22121t ), zlt\u22121]) update \u0434ate : ult = \u03c3 (Wlu \u00b7 [D(zl\u22121t ), zlt\u22121])\nproposed state : z\u0303lt = tanh(Wlp \u00b7 [D(zl\u22121t ), rt zlt\u22121])\nhidden state : zlt = (1 \u2212 ult ) zlt\u22121 + u l t z\u0303lt\n(1)\nwhere is Hadamard product, [a, b] is concatenation of vectors a and b, D(\u00b7) is dropout operator that randomly sets the dimensions of its argument to zero with probability equal to dropout rate, z0t equals the input at time t . Wr , Wu , and Wp are weight matrices of appropriate dimensions s.t. rlt , ult , z\u0303lt , and zlt are vectors in Rc\nl , where cl is the number of units in layer l . e sigmoid (\u03c3 ) and tanh activation functions are applied element-wise."}, {"heading": "4 RUL ESTIMATION USING EMBEDDINGS", "text": "We consider a scenario where sensor readings over the operational life of one or multiple instances of a machine or a component are available. We denote the set of instances byI. For an instance i \u2208 I, we consider a multi-sensor time series x(i) = {x(i)1 , x (i) 2 , \u00b7 \u00b7 \u00b7 , x (i) T (i ) }, where T (i) is the length of the time series, x(i)t \u2208 Rn is an ndimensional vector corresponding to the readings of the n sensors at time t . For a failed instance i , the length T (i) corresponds to the total operational life (from start to end of life) while for a currently operating instance the lengthT (i) corresponds to the elapsed operational life till the latest available sensor reading.\nTypically, if T (i) is large, we divide the time series into windows (subsequences) of xed length w . We denote a time series window from time t1 to t2 for instance i by x(i)(t1, t2). A xed-dimensional representation or embedding for each such window is obtained using an RNN Encoder that is trained in an unsupervised manner using RNN-ED. We train RNN-ED using time series subsequences from the entire operational life of machines (including normal as well as faulty operations)2. We use the embedding for a window to estimate the health of the instance at the end of that window. e RNN Encoder is likely to retain the important characterstics of machine behavior in the embeddings, and therefore discriminate between embeddings of windows corresponding to degraded behavior from those of normal behavior. We describe how these embeddings are obtained in Section 4.1, and then describe how health index curves and RUL estimates can be obtained using the embeddings in Sections 4.2 and 4.3, respectively. Figure 2 provides an overview of the steps involved in the proposed approach for RUL estimation."}, {"heading": "4.1 Obtaining Embeddings using RNN Encoder-Decoder", "text": "We brie y introduce RNN Encoder-Decoder (RNN-ED) networks based on sequence-to-sequence (seq2seq) learning framework. In general, a seq2seq model consists of a pair of multilayered RNNs trained together: an encoder RNN and a decoder RNN. Figure 3 shows the workings of encoder-decoder pair for a sample time series {x1, x2, x3}. Given an input time series x(i)(t \u2212 w + 1, t), the encoder RNN iterates through the points in the time series to compute the nal hidden state z(i)t , given by the concatenation of the hidden state vectors from all the layers in the encoder, s.t. z(i)t = [z (i) t,1, z (i) t,2, . . . , z (i) t,L], where z (i) t,l is the hidden state vector for the lth layer of encoder. e total number of recurrent units in the encoder is given by c = \u2211L l=1 c\nl , s.t. z(i)t \u2208 Rc (refer Section 3.2.1). 2Unlike the proposed approach, Recon-RUL [24] uses time series subsequences only from normal operation of the machine.\ne decoder RNN has the same network structure as the encoder, and uses the hidden state z(i)t as its initial hidden state, and iteratively (for w steps) goes through the transformations in Equation 1 (followed by a linear output layer) to reconstruct the input time series. e overall process can be thought of as a non-linear mapping of the input multivariate time series to a xed-dimensional vector representation (embedding) via an encoder function fenc , followed by another non-linear mapping of the xed-dimensional vector to a multivariate time series via a decoder function fdec :\nembeddin\u0434 z(i)t = fenc (x (i)(t \u2212w + 1, t))\nreconstructed time series x\u2032(i)(t \u2212w + 1, t) = fdec (z (i) t )\n(2)\ne reconstruction error at any point t \u2032 in (t \u2212 w + 1), . . . , t is e(i)t \u2032 = x (i) t \u2032 \u2212 x \u2032(i) t \u2032 . e overall reconstruction error for the input\ntime series window x(i)(t\u2212w+1, t) is given by e(i)t = \u2211t t \u2032=(t\u2212w+1) \u2016 e(i)t \u2032 \u2016 2 2 . e RNN-ED is trained to minimize the loss function given\nby the squared reconstruction error: L = \u2211i \u2208I \u2211T (i )t=w e(i)t . Typically, along with the nal hidden state, an additional input is given to the decoder RNN at each time step. is input is the output of the decoder RNN at the previous time step, as used in [24]. We, however, do not give any such additional inputs to the decoder along with the nal hidden state of encoder. is ensures that the nal hidden state of encoder retains all the information required to reconstruct the time series back via the decoder RNN. is approach of learning robust embeddings or representations for time series has been shown to be useful for time series classi cation in [25]. Figure 4 shows a typical example of input and output from\nRNN-ED, where the smoothed reconstruction suggests that the embeddings capture the necessary pa ern in the input and remove noise.\n4.1.1 Handling missing values. In real-world data, the sensor readings tend to be intermi ently missing. We include masking and delta vectors as additional inputs to the RNN-ED at each time instant, (as in [6]). e masking vector helps to identify the sensors that are missing at time t , and the delta vector indicates the time elapsed till t , from the most recent non-missing values for sensors in the past. We omit superscript (i) for denoting an instance of the machine from the notation of masking and delta vectors de ned below for simplicity.\n\u2022 Masking vector (mt ) denotes the missing sensors at time t and mt \u2208 {0, 1}n , where n is the number of sensors. e jth element of vector mt is given by:\nm j t = { 0, if x jt is missing 1, otherwise\n(3)\nwhere j = 1, . . . ,n, andx jt denotes the jth element of vector xt . When mjt = 0, we set x j t to 0 or to the average value\nfor jth sensor (we use 0 for the experiments in Section 5). \u2022 Delta vector (\u03b4t ) indicates the time elapsed till t , from the\nmost recent non-missing values for the sensors in the past\nand \u03b4t \u2208 Rn . e jth element of vector \u03b4t is given by:\n\u03b4 j t =  yt \u2212 yt\u22121 + \u03b4 jt\u22121, if t > 1,m j t\u22121 = 0 yt \u2212 yt\u22121, if t > 1,mjt\u22121 = 1 0, for t = 1\n(4)\nwhere j = 1, . . . ,n and yt \u2208 R is the time elapsed from start when t th reading is available and y1 = 0. It is to be noted that the sensor readings may not be available at regular time intervals. erefore, the sequence of readings is indexed by time t = 1, 2, . . . ,T , while the actual timestamps are denoted by y1,y2, . . . ,yT .\ne masking and delta vectors are given as additional inputs to the RNN-ED but are not reconstructed, s.t. only the actual sensors are reconstructed. erefore, the modi ed input time series x\u0302(i)t = [x (i) t ,m (i) t ,\u03b4 (i) t ], while the corresponding target to be reconstructed is x(i)t . e loss function (L) of the RNN-ED is also modi ed accordingly, so that the model is not penalized for reconstructing the missing sensors incorrectly. e contribution of a time series subsequence x(i)(t \u2212w + 1, t) to the loss function is thus given by e(i)t = \u2211t t \u2032=(t\u2212w+1) \u2016 e (i) t \u2032 \u00b7m (i) t \u2032 \u20162. In e ect, the network focuses on reconstructing the available sensor readings only."}, {"heading": "4.2 Obtaining HI Curves using Embeddings", "text": "Here we describe how the embeddings of time series subsequences are utilized to estimate the health of machines. Since the RNN Encoder captures the important pa erns in the input time series subsequences, the embeddings thus obtained can be used to differentiate between normal and degraded regions in the data. We maintain a set of embeddings, Znorm , corresponding to the time series subsequences from the normal behavior of all the instances in I. As a machine operates, its health degrades over time and the corresponding subsequence embeddings tend to be di erent from those in Znorm . So, we estimate the HI for a subsequence as follows:\nh (i) t =min(\u2016 z (i) t \u2212 z \u20162), \u2200 z \u2208 Znorm (5)\ne HI curve for an instance i obtained from the HI estimates at each time is denoted by h(i) = {h(i)w ,h (i) w+1, . . . ,h (i) T (i ) }. Like the set of normal embeddings Znorm , we also maintain a setH containing the HI curves of all instances in I."}, {"heading": "4.3 RUL Estimation using HI Curves", "text": "We use the same approach for estimating RUL from the HI curve as in [24]. We present it here for the sake of completeness. To estimate the RUL for a test instance i\u2217, its HI curve h(i\u2217) is compared with the HI curves inH . e initial health of a train instance and a test instance need not be same. We therefore allow for a time-lag tD in comparing the HI curve of test instance and train instance.\ne similarity between the HI curves of the test instance i\u2217 and a train instance i \u2208 I for a time-lag tD is given by:\ns(i\u2217, i, tD ) = exp(\u2212 1\nT (i\u2217)\nT (i \u2217)\u2211\nk=w\n(h(i \u2217) k \u2212 h (i) k+tD )2/\u03bb) (6)\n\u03bb > 0, tD \u2208 {1, 2, ...,\u03c4 }, tD + T (i \u2217) \u2264 T (i). Here, \u03c4 is maximum allowed time-lag, and \u03bb controls the notion of similarity: a small\nvalue of \u03bb would imply a large value for s even when the di erence in HI curves is small. e RUL estimate for i\u2217 based on the HI curve for i and time-lag tD is given by R\u2032(i \u2217)(i, tD ) = T (i) \u2212T (i \u2217) \u2212 tD .\nA weighted average of the RUL estimates obtained using all combinations of i and tD is used as the nal estimate R\u2032(i\n\u2217), and is given by:\nR\u2032(i \u2217) =\n\u2211 s(i\u2217, i, tD ) \u00b7 R\u2032(i\n\u2217)(i, tD )\u2211 s(i\u2217, i, tD )\n(7)\nwhere the summation is over only those combinations of i and tD which satisfy s(i\u2217, i, tD ) \u2265 \u03b1 .smax , where smax =maxi \u2208I,tD \u2208{1 , ..., \u03c4 } {s(i\u2217, i, tD )}, 0 \u2264 \u03b1 \u2264 1."}, {"heading": "5 EXPERIMENTAL EVALUATION", "text": "We evaluate our proposed approach for RUL estimation on two datasets: i) a publicly available C-MAPSS Turbofan Engine dataset [38], ii) a proprietary real-world pump dataset. We use Tensorow [1] library for implementing the various RNN models. We present the details of datasets in Section 5.1. In Section 5.2, we show that the results for embedding distance based approaches for RUL estimation compare favorably to the previously reported results using reconstruction error based approaches [24] on the engine dataset , as well as on the real-world pump dataset. Further, we evaluate the robustness of the embedding distances and reconstruction error based approaches by measuring the e ect of additive random Gaussian noise in the sensor readings on RUL estimation in Section 5.3."}, {"heading": "5.1 Datasets Description", "text": "5.1.1 Engine dataset. We use the rst dataset from the four simulated turbofan engine datasets from the NASA Ames Prognostics Data Repository [38]. is dataset contains time series of readings for 24 sensors for 100 train instances (train FD001.txt) of turbofan engine from the beginning of usage till end of life. ere are 100 test instances for which the time series are pruned some time prior to failure, s.t. the instances are currently operational and their RUL needs to be estimated (test FD001.txt). e actual RUL for the test instances are available in RUL FD001.txt. Noticeably, each engine instance has a di erent initial degree of wear such that the initial HI of each instance is likely to be di erent (implying potential usefulness of \u03c4 as introduced in Section 4.3).\nWe randomly select 80 train instances to train the models. Remaining 20 instances from the train set are used as validation set to select the parameters. e trajectories for these 20 engines are randomly truncated at ve di erent locations to obtain ve di erent instances from each instance for the RUL estimation task. We use Principal Components Analysis (PCA) [17] to reduce the dimensionality of data and select the number of principal components (p) to be used based on the validation set.\n5.1.2 Pump dataset. is dataset contains hourly sensor readings for 38 pumps that have reached end of life and 24 pumps that are currently operational. is dataset contains readings over a period of 2.5 years with each pump having 7 sensors installed on it. e 38 failed instances are randomly split into training, validation and test sets with 70%, 15%, and 15% instances in them, respectively. e 24 operational instances are added to training and validation set only for obtaining the RNN-ED model (they are not part of the\nsetH as their actual RUL is not known). e data is notably sparse with over 45% missing values across sensors. Also, for most pumps the sensor readings are not available from the date of installation but only few months (average 3.5 months) a er the installation date. Depending on the time elapsed, the health degradation level when sensor data is rst available for each pump varies signi cantly. e total operational life of the pumps varies from a minimum of 57 days to a maximum of 726 days.\nWe downsample the time series data from the original one reading per hour to one reading per day. To do this, we use following four statistics for each sensor over a day as derived sensors: minimum, maximum, average, and standard deviation, such that there are 28 (=7 \u00d7 4) derived sensors for each day. Further, using the derived sensors also helps take care of missing values which reduce from 45% for hourly sampling rate data to 33% for daily sampling rate data. We use masking and delta vectors as additional inputs in this case to train RNN-ED models as described in Section 4.1.1, s.t. the nal input dimension is 42 (28 for derived sensors, and 7 each for masking and delta vectors). Unlike the engine dataset where RUL is estimated only at the last available reading for each test instance, here we estimate RUL on every third day of operation for each test instance.\nA description of the performance metrics used for evaluation (taken from [37]) is provided in Appendix A. e hyper-parameters of our model, to be tuned are: number of principal components (p), number of hidden layers for RNN-ED (L), number of units in a hidden layer l (cl ) (we use same number of units in each hidden layer), dropout rate (d), window length (w), maximum allowed time-lag (\u03c4 ), similarity threshold (\u03b1 ), maximum predicted RUL (Rmax ), and parameter (\u03bb). e window length (w) can be tuned as a hyperparameter but in practice domain knowledge based selection of window length may be e ective."}, {"heading": "5.2 Embeddings for RUL Estimation", "text": "We follow similar evaluation protocol as used in [24]. To the best of our knowledge, the reconstruction error based model, LR-ED2, reported the best performance for RUL estimation on the engine dataset in terms of timeliness score (refer Appendix B). We compare variants of embedding distance based approach and reconstruction error based approach. We refer to HI curve obtained using the proposed embedding distance based approach as HIe (refer Section 4.2), and the HI curve obtained using the reconstrcution error based approach in [24] as HIr . Here, we refer the reconstruction error based LSTM-ED, LR-ED1 and LR-ED2 models reported in [24], as Recon-RUL, Recon-LR1, and Recon-LR2, respectively. We compare following models based on RNNs for RUL estimation task:\n\u2022 Embed-RUL Vs Recon-RUL: We compare RUL estimation performance of Embed-RUL that uses HIe curves and ReconRUL that uses HIr curves. \u2022 Linear Regression models: We learn a linear regression model (as described in Section 3.1) using normalized health index curves HIe as target and call it as Embed-LR1. EmbedLR2 is obtained using squared normalized HIe as target for the linear regression model. Similarly, Recon-LR1 and Recon-LR2 are obtained based on HIr .\n\u2022 RNN Regression model: RNN-based regression model (RNNReg.) is directly used to predict RUL (similar to [14])\n5.2.1 Performance on Engine dataset. We use \u03c41=13, \u03c42=10 as proposed in [39] for this dataset (refer Equations 8-11 in Appendix A). e parameters are obtained using grid search to minimize the timeliness score S (refer Equation 8) on the validation set. e parameters obtained for the best model (Embed-LR1) are p = 2, L = 1, cl = 55, d = 0.2, w = 30, \u03c4 = 30, \u03b1 = 0.95, Rmax = 120, and \u03bb = 0.005.\nTable 2 shows the performance in terms of various metrics on this dataset.We observe that each variant of embedding distance based approach perform be er than the corresponding variant of reconstruction error based approach in terms of timeliness score S . Figure 8(a) shows the distribution of errors for Embed-RUL and Recon-RUL models, and Figure 8(b) shows the distribution of errors for the best linear regression models of embedding distance (Embed-LR1) and reconstruction error (Recon-LR2) based approaches. e error ranges for reconstruction error based models are more spread-out (e.g., -70 to +50 for Recon-RUL) than the corresponding embedding distances based models (e.g., -60 to +30 for Embed-RUL), suggesting the robustness of the embedding distances based models. Figure 6 shows the actual RULs and the RUL estimates from Embed-LR1 and Recon-LR2.\n5.2.2 Performance on Pump dataset. e parameters are obtained using grid search to minimize the MSE for RUL estimation on the validation set. e parameters for the best model (EmbedRUL) are L = 1, cl = 390, d = 0.3, w = 30, \u03c4 = 70, \u03b1 = 0.8, Rmax = 150, and \u03bb = 10. e MSE and MAE performance metrics for the RUL estimation task are given in Table 3. e embedding distance based Embed-RUL model performs signi cantly be er than\n\u221220 \u221215 \u221210 \u22125 0 5 10 15 20 \u221220\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\n20\n(a) Engine dataset \u221225 \u221220 \u221215 \u221210 \u22125 0 5 10 15 20 \u221220\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\n20\n25\n(b) Pump dataset\nany of the other approaches. It is \u2248 35% be er than the second best model (Recon-RUL). e linear regression (LR) based approaches perform signi cantly worse than the raw embedding distance or reconstruction error based approaches for HI estimation indicating that the temporal aspect of the sensor readings is very important in this case. Figure 7 shows the actual and estimated RUL values for the pumps with best and worst performance in terms of MSE for the Embed-RUL model.\n5.2.3 alitative Analysis of Embeddings. We analyze the embeddings given by RNN Encoder for the Embed-RUL models. e original dimension of embeddings for Embed-RUL for engine and pump datasets are 55 and 390, respectively. We use t-SNE [21] to map the embeddings to 2-D space. Figure 5 shows the 2-D sca er plot for the embeddings at the rst 25% (normal behavior) and last 25% (degraded behavior) points in the life of all test instances. We observe that RNN Encoder tends to give di erent embeddings for windows corresponding to normal and degraded behaviors. e sca er\nplots indicate that normal windows are close to each other and far from degraded windows, and vice-versa. Note: Since t-SNE does non-linear dimensionality reduction, the actual distances between normal and degraded windows may not be re ected in these plots."}, {"heading": "5.3 Robustness of Embeddings to Noise", "text": "We evaluate the robustness of Embed-RUL and Recon-RUL for RUL estimation by adding Gaussian noise to the sensor readings. e sensor reading x(i\n\u2217) t for a test instance i\u2217 at any time t is corrupted\nwith additive Gaussian noise to obtain a noisy version x\u2032(i \u2217)\nt s.t. x\u2032(i\n\u2217) t |x (i\u2217) t \u223c N(x (i\u2217) t ,\u03c3\n2I ). Table 1 shows the e ect of noise on performance for both engine and pump datasets. For both datasets, the standard deviation of the MSE values over di erent noise levels is much lesser for Embed-RUL compared to Recon-RUL. is suggests that embedding distance based models are more robust to noise compared to reconstruction error based models. Also, for the engine dataset, we observe similar behavior in terms of timeliness score (S): 819\u00b141 for Embed-RUL and 1189\u00b1110 for Recon-RUL.\nFigure 9 depicts a sample scenario showing the health index generated from noisy sensor data. e vertical bar corresponds to 1-sigma deviation in estimate. e reconstruction error and embedding distance increase over time indicating gradual degradation. While reconstruction error based HI varies signi cantly with varying noise levels, embedding distance based HI is fairly robust to\nnoise. is suggests that reconstruction error varies signi cantly with change in noise levels impacting HI estimates while distance between embeddings does not change much leading to robust HI estimates."}, {"heading": "6 DISCUSSION", "text": "We have proposed an approach for health monitoring via health index estimation and remaining useful life (RUL) estimation. e proposed approach is capable of dealing with several of the practical challenges in data-driven RUL estimation including noisy sensor readings, missing data, and lack of prior knowledge about degradation trends. e RNN Encoder-Decoder (RNN-ED) is trained in an unsupervised manner to learn xed-dimensional representations or embeddings to capture machine behavior. e health of a machine is then estimated by comparing the recent embedding with the existing set of embeddings corresponding to normal behavior. We found that our approach using RNN-ED based embedding distances is be er compared to the previously known best approach using RNN-ED based reconstruction error on the engine dataset. e proposed approach also gives be er results on the real-world pump dataset. We have also shown that embedding distances based RUL estimates are robust to noise."}, {"heading": "A PERFORMANCE METRICS", "text": "ere are several metrics proposed to evaluate the performance of prognostics models [37]. We measure the performance of our models in terms of Timeliness Score (S), Accuracy (A), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), False Positive Rate (FPR) and False Negative Rate (FNR) as mentioned in Equations 8-11.\nFor a test instance i\u2217, the error \u2206(i\u2217) = R\u0302(i\u2217) \u2212 R(i\u2217) between the estimated RUL (R\u0302(i\u2217)) and actual RUL (R(i\u2217)). e timeliness score S used to measure the performance of a model is given by:\nS = N\u2211\ni\u2217=1 (exp(\u03b3 \u00b7 |\u2206(i\u2217) |) \u2212 1) (8)\nwhere \u03b3 = 1/\u03c41 if \u2206(i \u2217) < 0, else \u03b3 = 1/\u03c42, N is total test instances. Usually, \u03c41 > \u03c42 such that late predictions are penalized more\ncompared to early predictions. e lower the value of S , the be er is the performance.\nA = 100 N N\u2211 i\u2217=1 I (\u2206(i\u2217)) (9)\nwhere I (\u2206(u\u2217)) = 1 i f \u2206(u\u2217) \u2208 [\u2212\u03c41,\u03c42], else I (\u2206(u \u2217)) = 0, \u03c41 > 0,\u03c42 > 0.\nMAE = 1 N N\u2211 i\u2217=1 |\u2206(i\u2217) |, MSE = 1 N N\u2211 i\u2217=1 (\u2206(i\u2217))2 (10)\nMAPE = 100 N N\u2211 i\u2217=1 |\u2206(i\u2217) | R(i\u2217)\n(11)\nA prediction is false positive (FP) if \u2206(i\u2217) < \u2212\u03c41, and false negative (FN) if \u2206(i\u2217) > \u03c42."}, {"heading": "B BENCHMARKS ON TURBOFAN ENGINE DATASET", "text": "We provide a comparison of some approaches for RUL estimation on the engine dataset (test FD001.txt) below:\n3Referred to as MAPE1 in [24] 4Dataset simulated under similar se ings 5Unlike this method which tunes the parameters on the test set to obtain the maximum S , we learn the parameters of the model on a validation set and still get similar performance in terms of S ."}], "references": [{"title": "Early Detection of Combustion Instabilities using Deep Convolutional Selective Autoencoders on Hi-speed Flame Video", "author": ["Adedotun Akintayo", "Kin Gwn Lore", "Soumalya Sarkar", "Soumik Sarkar"], "venue": "CoRR abs/1603.07839", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Deep convolutional neural network based regression approach for estimation of remaining useful life", "author": ["Giduthuri Sateesh Babu", "Peilin Zhao", "Xiao-Li Li"], "venue": "In International Conference on Database Systems for Advanced Applications", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Comparison of sensors and methodologies for e\u0082ective prognostics on railway turnout systems", "author": ["Fatih Camci", "Omer Faruk Eker", "Saim Ba\u015fkan", "Savas Konur"], "venue": "Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit 230,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Recurrent neural networks for multivariate time series with missing values", "author": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "venue": "arXiv preprint arXiv:1606.01865", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "NIST/SEMATECH e-handbook of statistical methods", "author": ["Carroll Croarkin", "Paul Tobias"], "venue": "NIST/SEMATECH", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Internet of things in industries: A survey", "author": ["Li Da Xu", "Wu He", "Shancang Li"], "venue": "IEEE Transactions on industrial informatics 10,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "\u008boc V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A similarity-based prognostics approach for remaining useful life prediction", "author": ["\u00d6mer Faruk Eker", "Faith Camci", "Ian K Jennions"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Multivariate Industrial Time Series with Cyber-A\u008aack Simulation: Fault Detection Using an LSTM-based Predictive Data Model. NIPS Time Series Workshop 2016", "author": ["Pavel Filonov", "Andrey Lavrentyev", "Artem Vorontsov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geo\u0082rey Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recurrent neural networks for remaining useful life estimation", "author": ["Felix O Heimes"], "venue": "In Prognostics and Health Management,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Ensemble of data-driven prognostic algorithms for robust prediction of remaining useful life", "author": ["Chao Hu", "Byeng D Youn", "Pingfeng Wang", "Joung Taek Yoon"], "venue": "Reliability Engineering & System Safety", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Direct Remaining Useful Life Estimation Based on Support Vector Regression", "author": ["Racha Khelif", "Brigi\u008ae Chebel-Morello", "Simon Malinowski", "Emna Laajili", "Farhat Fnaiech", "Noureddine Zerhouni"], "venue": "IEEE Transactions on Industrial Electronics 64,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "A Framework of Comnbining Deep Learning and Survival Analysis for Asset Health Management", "author": ["Linxia Liao", "Hyung-il Ahn"], "venue": "1st ACM SIGKDDWorkshop on ML for PHM", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geo\u0082rey Hinton"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Performing Diagnostics and Prognostics On Simulated Engine Failures Using Neural Networks", "author": ["Owen B Macmann", "Timothy M Seitz", "Alireza R Behbahani", "Kelly Cohen"], "venue": "In 52nd AIAA/SAE/ASEE Joint Propulsion Conference", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Multi-Sensor Prognostics using an Unsupervised Health Index based on LSTM Encoder-Decoder", "author": ["Pankaj Malhotra", "Vishnu TV", "Anusha Ramakrishnan", "Gaurangi Anand", "Lovekesh Vig", "Puneet Agarwal", "Gautam Shro"], "venue": "1st ACM SIGKDD Workshop on ML for PHM. arXiv preprint arXiv:1608.06154", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "TimeNet: Pre-trained deep recurrent neural network for time series classi\u0080cation", "author": ["Pankaj Malhotra", "Vishnu TV", "Lovekesh Vig", "Puneet Agarwal", "Gautam Shro"], "venue": "In 25th European Symposium on Arti\u0080cial Neural Networks, Computational Intelligence and Machine Learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "Long Short Term Memory Networks for Anomaly Detection in Time Series", "author": ["Pankaj Malhotra", "Lovekesh Vig", "Gautam Shro", "Puneet Agarwal"], "venue": "In ESANN, 23rd European Symposium on Arti\u0080cial Neural Networks, Computational Intelligence and Machine Learning", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Data-driven prognostic method based on Bayesian approaches for direct remaining useful life prediction", "author": ["Ahmed Mosallam", "Kamal Medjaher", "Noureddine Zerhouni"], "venue": "Journal of Intelligent Manufacturing", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Component based data-driven prognostics for complex systems: Methodology and applications", "author": ["A Mosallam", "K Medjaher", "N Zerhouni"], "venue": "In Reliability Systems Engineering (ICRSE),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Sparse autoencoder", "author": ["Andrew Ng"], "venue": "CS294A Lecture notes", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "A naive Bayes model for robust remaining useful life prediction of lithium-ion ba\u008aery", "author": ["Selina SY Ng", "Yinjiao Xing", "Kwok L Tsui"], "venue": "Applied Energy", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "A modi\u0080ed echo state network based remaining useful life estimation approach", "author": ["Yu Peng", "Hong Wang", "Jianmin Wang", "Datong Liu", "Xiyuan Peng"], "venue": "In IEEE Conference on Prognostics and Health Management (PHM),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "\u008c\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Robust performance degradation assessment methods for enhanced rolling element bearing prognostics", "author": ["Hai Qiu", "Jay Lee", "Jing Lin", "Gang Yu"], "venue": "Advanced Engineering Informatics 17,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Investigating computational geometry for failure prognostics", "author": ["Emmanuel Ramasso"], "venue": "International Journal of Prognostics and Health Management 5,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Joint prediction of continuous and discrete states in time-series based on belief functions", "author": ["Emmanuel Ramasso", "Michele Rombaut", "Noureddine Zerhouni"], "venue": "Cybernetics, IEEE Transactions on 43,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Applying Deep Learning for Prognostic Health Monitoring of Aerospace and Building Systems", "author": ["Kishore K Reddy", "Vivek Venugopalan", "Michael J Giering"], "venue": "1st ACM SIGKDD Workshop on ML for PHM", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Metrics for evaluating performance of prognostic techniques", "author": ["Abhinav Saxena", "Jose Celaya", "Edward Balaban", "Kai Goebel", "Bhaskar Saha", "Sankalita Saha", "Mark Schwabacher"], "venue": "In Prognostics and health management,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Turbofan Engine Degradation Simulation Data Set", "author": ["A Saxena", "K Goebel"], "venue": "NASA Ames Prognostics Data Repository", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Damage propagation modeling for aircra\u0089 engine run-to-failure simulation", "author": ["Abhinav Saxena", "Kai Goebel", "Don Simon", "Neil Eklund"], "venue": "In Prognostics and Health Management,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Remaining useful life estimation\u2013A review on the statistical data driven approaches", "author": ["Xiao-Sheng Si", "Wenbin Wang", "Chang-Hua Hu", "Dong-Hua Zhou"], "venue": "European journal of operational research 213,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from over\u0080\u008aing", "author": ["Nitish Srivastava", "Geo\u0082rey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhudinov"], "venue": "In International Conference on Machine Learning", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "\u008boc V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "A similarity-based prognostics approach for remaining useful life estimation of engineered systems", "author": ["Tianyi Wang", "Jianbo Yu", "David Siegel", "Jay Lee"], "venue": "In Prognostics and Health Management,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2008}, {"title": "On accurate and reliable anomaly detection for gas turbine combustors: A deep learning approach", "author": ["Weizhong Yan", "Lijie Yu"], "venue": "In Proceedings of the Annual Conference of the Prognostics and Health Management Society", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Machine health monitoring with LSTM networks", "author": ["Rui Zhao", "Jinjiang Wang", "Ruqiang Yan", "Kezhi Mao"], "venue": "In Sensing Technology (ICST),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Deep Learning and Its Applications to Machine Health Monitoring: A Survey", "author": ["Rui Zhao", "Ruqiang Yan", "Zhenghua Chen", "Kezhi Mao", "Peng Wang", "Robert X Gao"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "We perform experiments on publicly available turbofan engine dataset and a proprietary real-world dataset, and demonstrate that Embed-RUL outperforms the previously reported [24] state-of-the-art on several metrics.", "startOffset": 174, "endOffset": 178}, {"referenceID": 7, "context": "It is quite common in the current era of the \u2018Industrial Internet of \u008cings\u2019 [9] for a large number of sensors to be installed for monitoring the operational behavior of machines.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": ", exponential degradation [5, 8, 34, 39, 45].", "startOffset": 26, "endOffset": 44}, {"referenceID": 6, "context": ", exponential degradation [5, 8, 34, 39, 45].", "startOffset": 26, "endOffset": 44}, {"referenceID": 29, "context": ", exponential degradation [5, 8, 34, 39, 45].", "startOffset": 26, "endOffset": 44}, {"referenceID": 34, "context": ", exponential degradation [5, 8, 34, 39, 45].", "startOffset": 26, "endOffset": 44}, {"referenceID": 40, "context": ", exponential degradation [5, 8, 34, 39, 45].", "startOffset": 26, "endOffset": 44}, {"referenceID": 19, "context": "\u2022 Our approach compares favorably to previous benchmarks for RUL estimation [24] on the turbofan engine dataset [38] as well as on a real-world pump dataset (refer Section 5.", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "\u2022 Our approach compares favorably to previous benchmarks for RUL estimation [24] on the turbofan engine dataset [38] as well as on a real-world pump dataset (refer Section 5.", "startOffset": 112, "endOffset": 116}, {"referenceID": 35, "context": "A review of these approaches can be found in [40].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "[11, 19] propose estimating RUL directly by calculating the similarity between the sensors without deriving any health estimates.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "Similarly, Support Vector Regression [18], RNNs [14], Deep Convolutional Neural Networks [3] have been proposed to estimate the RUL directly by modeling the relations among the sensors without estimating the health of the machines.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "Similarly, Support Vector Regression [18], RNNs [14], Deep Convolutional Neural Networks [3] have been proposed to estimate the RUL directly by modeling the relations among the sensors without estimating the health of the machines.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "Similarly, Support Vector Regression [18], RNNs [14], Deep Convolutional Neural Networks [3] have been proposed to estimate the RUL directly by modeling the relations among the sensors without estimating the health of the machines.", "startOffset": 89, "endOffset": 92}, {"referenceID": 28, "context": "Robust RUL Estimation: Wavelet \u0080lters have been proposed to handle noise for robust performance degradation assessment in [33].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "In [16], ensemble of models is used to ensure that predictions are robust.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Long Short Term Memory (LSTM) [15] based encoders trained using encoder-decoder framework have been proposed to learn representations of video sequences [42].", "startOffset": 30, "endOffset": 34}, {"referenceID": 37, "context": "Long Short Term Memory (LSTM) [15] based encoders trained using encoder-decoder framework have been proposed to learn representations of video sequences [42].", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "Pre-trained LSTM Encoder based on autoencoders are used to initialize networks for classi\u0080cation tasks and are shown to achieve improved performance [10] for text applications.", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "Gated Recurrent Units (GRUs) [7] based encoder named Timenet [25] has been recently proposed to obtain embeddings for time series from several domains.", "startOffset": 29, "endOffset": 32}, {"referenceID": 20, "context": "Gated Recurrent Units (GRUs) [7] based encoder named Timenet [25] has been recently proposed to obtain embeddings for time series from several domains.", "startOffset": 61, "endOffset": 65}, {"referenceID": 41, "context": "Stacked denoising autoencoders have been used to learn hierarchical features from sensor data in [46].", "startOffset": 97, "endOffset": 101}, {"referenceID": 43, "context": "Many of these architectures and applications of deep learning to machine health monitoring have been surveyed in [48].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "An end-to-end convolutional selective autoencoder for early detection and monitoring of combustion instabilites in high speed \u0083ame video frames was proposed in [2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 16, "context": "A combination of deep learning and survival analysis for asset health management has been proposed in [20] using sequential data by stacking a LSTM layer, a feed forward layer and a survival model layer to arrive at the asset failure probability.", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "Deep belief networks and autoencoders have been used for health monitoring of aerospace and building systems in [36].", "startOffset": 112, "endOffset": 116}, {"referenceID": 42, "context": "Predicting milling machine tool wear from sensor data has been proposed using deep LSTM networks in [47].", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": ", [34, 45]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 40, "context": ", [34, 45]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 40, "context": ", [45]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": "RNNs, especially those based on LSTM units or GRUs have been successfully used to achieve state-of-the-art results on sequence modeling tasks such as machine translation [7] and speech recognition [13].", "startOffset": 170, "endOffset": 173}, {"referenceID": 11, "context": "RNNs, especially those based on LSTM units or GRUs have been successfully used to achieve state-of-the-art results on sequence modeling tasks such as machine translation [7] and speech recognition [13].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "health monitoring from multi-sensor time series data [12, 23, 26].", "startOffset": 53, "endOffset": 65}, {"referenceID": 21, "context": "health monitoring from multi-sensor time series data [12, 23, 26].", "startOffset": 53, "endOffset": 65}, {"referenceID": 24, "context": "in the data by means of regularization such as by adding constraints on the number of hidden units of the autoencoder [29], or by adding noise to the input and training the network to reconstruct a denoised version of the input [44].", "startOffset": 118, "endOffset": 122}, {"referenceID": 39, "context": "in the data by means of regularization such as by adding constraints on the number of hidden units of the autoencoder [29], or by adding noise to the input and training the network to reconstruct a denoised version of the input [44].", "startOffset": 228, "endOffset": 232}, {"referenceID": 19, "context": "RNN autoencoders have been shown to be useful for RUL estimation [24] in which the RNN-based model learns to capture the behavior of a machine by learning to reconstruct multivariate time series corresponding to normal behavior in an unsupervised manner.", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "We propose to learn robust \u0080xed-dimensional representations for multi-sensor time series data via sequence-to-sequence [4, 43] autoencoders based on RNNs.", "startOffset": 119, "endOffset": 126}, {"referenceID": 38, "context": "We propose to learn robust \u0080xed-dimensional representations for multi-sensor time series data via sequence-to-sequence [4, 43] autoencoders based on RNNs.", "startOffset": 119, "endOffset": 126}, {"referenceID": 5, "context": "We use Gated Recurrent Units [7] in the hidden layers of sequence-to-sequence autoencoder.", "startOffset": 29, "endOffset": 32}, {"referenceID": 27, "context": "Dropout is used for regularization [32, 41] and is applied only to the non-recurrent connections, ensuring information \u0083ow across timesteps.", "startOffset": 35, "endOffset": 43}, {"referenceID": 36, "context": "Dropout is used for regularization [32, 41] and is applied only to the non-recurrent connections, ensuring information \u0083ow across timesteps.", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "2Unlike the proposed approach, Recon-RUL [24] uses time series subsequences only from normal operation of the machine.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "\u008cis input is the output of the decoder RNN at the previous time step, as used in [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "\u008cis approach of learning robust embeddings or representations for time series has been shown to be useful for time series classi\u0080cation in [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "We include masking and delta vectors as additional inputs to the RNN-ED at each time instant, (as in [6]).", "startOffset": 101, "endOffset": 104}, {"referenceID": 19, "context": "We use the same approach for estimating RUL from the HI curve as in [24].", "startOffset": 68, "endOffset": 72}, {"referenceID": 33, "context": "We evaluate our proposed approach for RUL estimation on two datasets: i) a publicly available C-MAPSS Turbofan Engine dataset [38], ii) a proprietary real-world pump dataset.", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "2, we show that the results for embedding distance based approaches for RUL estimation compare favorably to the previously reported results using reconstruction error based approaches [24] on the engine dataset , as well as on the real-world pump dataset.", "startOffset": 184, "endOffset": 188}, {"referenceID": 33, "context": "We use the \u0080rst dataset from the four simulated turbofan engine datasets from the NASA Ames Prognostics Data Repository [38].", "startOffset": 120, "endOffset": 124}, {"referenceID": 32, "context": "A description of the performance metrics used for evaluation (taken from [37]) is provided in Appendix A.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "We follow similar evaluation protocol as used in [24].", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "2), and the HI curve obtained using the reconstrcution error based approach in [24] as HIr .", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "Here, we refer the reconstruction error based LSTM-ED, LR-ED1 and LR-ED2 models reported in [24], as Recon-RUL, Recon-LR1, and Recon-LR2, respectively.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": ") is directly used to predict RUL (similar to [14])", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "We use \u03c41=13, \u03c42=10 as proposed in [39] for this dataset (refer Equations 8-11 in Appendix A).", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "We use t-SNE [21] to map the embeddings to 2-D space.", "startOffset": 13, "endOffset": 17}], "year": 2017, "abstractText": "We consider the problem of estimating the remaining useful life (RUL) of a system or a machine from sensor data. Many approaches for RUL estimation based on sensor data make assumptions about how machines degrade. Additionally, sensor data from machines is noisy and o\u0089en su\u0082ers from missing values in many practical se\u008aings. We propose Embed-RUL: a novel approach for RUL estimation from sensor data that does not rely on any degradation-trend assumptions, is robust to noise, and handles missing values. EmbedRUL utilizes a sequence-to-sequence model based on Recurrent Neural Networks (RNNs) to generate embeddings for multivariate time series subsequences. \u008ce embeddings for normal and degraded machines tend to be di\u0082erent, and are therefore found to be useful for RUL estimation. We show that the embeddings capture the overall pa\u008aern in the time series while \u0080ltering out the noise, so that the embeddings of two machines with similar operational behavior are close to each other, even when their sensor readings have signi\u0080cant and varying levels of noise content. We perform experiments on publicly available turbofan engine dataset and a proprietary real-world dataset, and demonstrate that Embed-RUL outperforms the previously reported [24] state-of-the-art on several metrics.", "creator": "LaTeX with hyperref package"}}}