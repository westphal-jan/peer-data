{"id": "1709.02279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2017", "title": "Cynical Selection of Language Model Training Data", "abstract": "The Moore-Lewis method of \"intelligent selection of training data for language models\" is very effective, cost-effective, efficient... and also has structural problems. (1) The method defines relevance by playing language models trained on the domain and the domain (or the data pool). This powerful idea - which we want to preserve - treats the two corporations as the opposite ends of a single spectrum. This lack of nuance does not allow the two corporations to be very similar. In extreme cases, if they come from the same distribution, all sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The sentences selected are not able to model the domain data, nor do they cover the domain data at all. They are simply well received by the domain model; this is necessary, but not sufficient. (3) There is no way to say what the optimal number of sentences to select is short of selecting the thresholds and the different systems.", "histories": [["v1", "Thu, 7 Sep 2017 14:30:50 GMT  (16kb)", "http://arxiv.org/abs/1709.02279v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amittai axelrod"], "accepted": false, "id": "1709.02279"}, "pdf": {"name": "1709.02279.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["amittai@b0rked.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n02 27\n9v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\nand the out-of-domain (or data pool) corpora against each other. This powerful idea \u2013 which we set out to preserve \u2013 treats the two corpora as the opposing ends of a single spectrum. This lack of nuance does not allow for the two corpora to be very similar. In the extreme case where the come from the same distribution, all of the sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The selected sentences are not guaranteed to be able to model the in-domain data, nor to even cover the in-domain data. They are simply well-liked by the in-domain model; this is necessary, but not sufficient. (3) There is no way to tell what is the optimal number of sentences to select, short of picking various thresholds and building the systems. We present \u201ccynical selection of training data\u201d: a greedy, lazy, approximate, and generally efficient method of accomplishing the same goal. It has the following properties: (1) Is responsive to the extent to which two corpora differ. (2) Quickly reaches near-optimal vocabulary coverage. (3) Takes into account what has already been selected. (4) Does not involve defining any kind of domain, nor any kind of classifier. (5) Has real units. (6) Knows approximately when to stop."}, {"heading": "1. Scenario", "text": "We have a number of translation tasks, each defined as \u201ca flavor of data we want to be able to translate well\u201d. Perhaps it\u2019s a specific client\u2019s data, or a kind of data such as \u201ccustomer support chat logs\u201d, or just a system for a language arc (\u201cMexican Spanish to American English\u2019). Machine Translation (MT) tasks often have one of these two problems:\n(1) Web-scale data, or too much data than can be used readily to train or run an MT system. We want to know what data we can exclude from training without sacrificing performance.\n(2) Not enough parallel data to train an MT system. We want to know what data we can add to improve performance, and how much improvement we might expect.\nWork unaffiliated with amazon.com.\n1\nWe can add data from known parallel data, or by paying to have monolingual data translated.\nWe present a single method for handling both:\n(1) Given a too-large parallel corpus, we identify the best subset to use for training a system that is at least as good as the full system. This is the traditional data selection scenario, and we improve upon the standard Moore-Lewis cross-entropy difference method (Moore and Lewis, 2010).\n(2) Given a representative monolingual corpus for a translation task, and an optional parallel corpus in the language pair, identify the monolingual sentences that should be manually translated and added to the parallel corpus in order to improve translation for the specific task.\nTranslating monolingual data is expensive, and training on bilingual data is expensive. By \u201cexpense\u201d we mean effort, computation, time, and dollars. As a rule, we want to spend as little as possible to get as much out of the available data as we can."}, {"heading": "2. Context", "text": "2.1. Language Model Size Seymore and Rosenfeld (1996) decided that pruning a model is better than incrementally growing it:\nUsing more training data, up to at least 25 - 30 million words initially, and then pruning it down is a better approach than just starting with a small amount of training data [...] Beyond 25 million words, the amount of training data does not have a noticeable effect.\nStolcke (1998) showed how to produce an efficiently-sized language model (LM) by building an LM and then pruning entries in the model that do not help model the training set. This is determined by whether removing the entries changes the entropy of the training set, or not. They \u201cassume that the n-grams affect the relative entropy roughly independently, and compute [the change in entropy] due to each individual n-gram [...] then rank the n-grams by their effect on the model entropy\u201d and focus on those that affect the model entropy the least. The relative entropy score of the sentence actually can be decomposed into the sum of the word scores (see derivation by Sethy et al. (2006b)), so there is no need to assume.\nSiivola and Pellom (2005) approach from the other direction: incrementally growing an n-gram LM, adding entries that decrease the number of bits required to model the training data. They compute gains on the data coding length, along the lines of the Minimum Description Length principle which minimizes the size of the model plus the training data as encoded by the model (Rissanen, 1983). Sethy et al. (2006b) also take the growing\napproach, using an information-gain -based score. However, due to computational cost, they did not update the estimate after every new selection.\n2.2. Corpus Similarity Kilgarriff (2001) posited:\nCorpus similarity is complex, and there is no absolute answer to \u201cis Corpus 1 more like Corpus 2 than Corpus 3?\u201d. All there are are possible measures which serve particular purposes more or less well.\nPerhaps similarity cannot be defined absolutely, but the units of measurement can. Bits of information are ideal for this.\n2.3. Data Selection Data selection \u2013 the deliberate reduction in size of the training corpus \u2013 is a roundabout way of achieving domain adaptation. Each of the training data sentences is scored by its relevance to the data to be modeled, and only the best-scoring sentences are kept. The standard approach for data selection is the same for both language modeling (Moore and Lewis, 2010) and machine translation (Axelrod et al., 2011). Called \u201ccross-entropy difference\u201d, it preferentially selects data that is like the translation task and unlike the rest of the data pool.1\n2.4. Cynically Quantifying Relevance One major advantage to the above-mentioned model-growing methods is that there is no need to train a huge language model in order to figure out how to build a smaller one. An advantage of information-theoretic similarity measurements is that the focus is on quantifying the relationship between sentences and corpora, so the output can be used for a variety of scenarios and not just language modeling (or, by extension, machine translation).\nHere we propose a method to incrementally construct an efficiently-sized downstream model by incrementally constructing an efficiently-sized training corpus. We score data by how useful it would be to add it to a corpus that is used to model a particular translation task. In particular, we score sentences by how much we\u2019d learn if we added it to our existing data, right now. Sethy et al. (2006b) describe the general idea as \u201can incremental greedy selection scheme based on relative entropy, which selects a sentence if adding it to the already selected set of sentences reduces the relative entropy with respect to the in-domain data distribution\u201d.\n1 More complete explanations can be found in previous work: Axelrod et al. (2011) and Axelrod (2014)."}, {"heading": "3. Inputs", "text": "The algorithm requires four inputs to be specified, twice as many as the Moore-Lewis crossentropy difference method. which only had two: the in-domain and the general-pool data files. We separate the idea of the vocabulary distributions of the kinds of language that we have (or want) from the actual corpora that we have (or want). These additional inputs allow the cynical selection method to be used in a wider range of applications.\n3.1. REPRESENTATIVE corpus distribution A corpus that is representative of what we want to translate. For example: all the data we have for a task, or a sample of a data stream from the domain, or whatever. We will evaluate the selected data by its ability to model the Representative corpus (specifically: by computing the perplexity of the Representative corpus using a language model trained on the selected data). We only use the Representative file to compute some vocabulary statistics (described later). This means a client with confidential data they can\u2019t share could just share the derived statistics and not the text itself.\n3.2. UNADAPTED corpus distribution A corpus that reflects our currently-available data. This was called the \u201cdata pool\u201d in prior work, but we\u2019ve changed the name to make some distinctions clearer. We only need this corpus to compute some corpus statistics, and then compare the statistics of this corpus with that of the Representative corpus. Perhaps the Unadapted is generic data, perhaps it is just all the data that we have in the language of interest. By default the Unadapted corpus distribution would come from the set of sentences to select from (i.e. the data pool is the unadapted corpus is the set of sentences to be scored).\nThe Representative corpus gives us the vocabulary distribution of the data we want, and Unadapted gives us the vocabulary distribution of the data we have. We need these two distributions in order to quantify how these two sets of language differ.\n3.3. SEED text file Contains any headway that has been made towards assembling a corpus that meets the goal of being able to train a system to translate Representative-type data. It might be that the client has already sent off 1000 hand-picked sentences to be translated, or we have 100k sentences we are certain come from Spanish-speaking customers in Mexico or whatever the unifying theme of the Representative corpus is. The Seed is assumed to be empty by default.\n3.4. AVAILABLE text file The set of candidate sentences we can choose from. We will score and rank these sentences according to their usefulness in modeling the Representative distribution. In most cases, the Available corpus is the same one that is used to compute the Unadapted corpus distribution.\nHowever, if the goal is to pick monolingual sentences to send off for manual translation, then the Available corpus is probably the Representative corpus! In this case, the Representative corpus would be monolingual client data: things we want to know how to translate, but have no translations for. The Unadapted is probably our existing parallel data, and we want to know how much parallel Representative data we need to purchase and add to the Unadapted in order to build a system to translate the rest of the Representative data.\n3.5. JADED text file This is the output file. It is a re-ranked subset of the Available corpus with a few special (cynical) properties. Sentences in the file are ordered (and scored) by the amount of useful information they contain about the Representative corpus. Specifically, we measure how much we can improve (decrease) the perplexity of the Representative corpus evaluated with a language model trained on the Seed+Jaded data. Other practical properties: if we read the file line by line, everything already read is more useful than everything we have not yet seen. Furthermore, the next line in the file is always the most informative sentence to add to what we have already seen. Every sentence in the file is ranked strictly according to its utility."}, {"heading": "4. Notation", "text": "\u2022 The lowercase w variable is for word tokens. Any capital W is some set of tokens in a corpus.\n\u2022 The lowercase v variable is for word types (lexical / vocabulary items). Any capital V is a set of vocabulary items in a corpus.\n\u2022 The lowercase c variable is for the count of a single word type. Any capital C variable is the count of some word type in a corpus.\n\u2022 wn is the set of tokens in the nth selected line. vn is the set of word types in the n\nth selected line. cn(v) is the count of word type v in the n th line.\n\u2022 Wn and Vn are the total number of word tokens and word types, respectively, in the first n selected lines. Cn(v) is the count of word v in the first n selected lines.\n\u2022 Wrepr and Vrepr are the total number of word tokens and word types, respectively, in the representative corpus. Guess what Wunadapt and Vunadapt are?"}, {"heading": "5. Math", "text": "5.1. Cross-Entropy: What It Is Let Q be the probability distribution from a language model LM . The general definition\nof the cross-entropy H of the Repr corpus using Q is:\nHLM (repr) = \u2212 \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr logQ(v)(1)\nH represents how well the language model\u2019s training corpus can be used to model the Representative corpus. Perplexity is normally used to evaluate language models. The general definition is:\npplLM (repr) = 2 H(repr)\nEverything that follows could be rewritten in terms of perplexity, but we won\u2019t.\nThe stupidest kind of language model we can build is an unsmoothed unigram LM (the maximum likelihood estimate LM). That kind of LM makes Q be the empirical probability of each word in the corpus:\nH(repr) = \u2212 \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nC(v)\n|W |(2)\nH is always positive, and smaller (lower entropy) is better.\nWe will be selecting sentences to incrementally grow a corpus. After selecting the nth line, the cross-entropy between the n-sentence corpus and the Representative one is:\nHn(repr) = \u2212 \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nWn (3)\nCall that Hn for short. We will be computingH1,H2, . . . ,Hn\u22121,Hn as we select lines.\n5.2. Cross-Entropy: What It Is Not Entropy, cross-entropy, and relative entropy are easily confused in the literature. They\nare three different things. Compounding the situation, there are also alternate names: \u2018Entropy\u2019 specifically means \u2018Shannon entropy\u2019, and \u2018Relative Entropy\u2019 means the KullbackLeibler (KL) divergence. The relationship between the kinds of (Shannon) entropy regarding two probability distributions P and Q is:\nEntropy = Cross Entropy \u2212 Relative Entropy = H(P,Q) \u2212DKL(P ||Q)\n= \u2212 \u2211\nx\np(x) log q(x) \u2212 \u2211\nx\np(x) log p(x)\nq(x)\n= \u2212 \u2211\nx\np(x) log q(x) \u2212 \u2211\nx\np(x)(log p(x)\u2212 log q(x))\n= \u2212 \u2211\nx\np(x) log q(x) \u2212 \u2211\nx\np(x) log p(x) + \u2211\nx\np(x) log q(x)\nH(P ) = \u2212 \u2211\nx\np(x) log p(x)\nAn intuitive explanation2: The Shannon entropy of P is the minimum number of bits required to perfectly encode all events drawn from distribution P . Cross entropy is the number of bits required to encode all events drawn from P , but using an imperfect code based on distribution Q. Relative entropy is the number of bits that are wasted by using Q instead of the true distribution P . Then: Number of bits used - wasted bits = optimal number of bits.\n5.3. Defining Cross-Entropy Greedily Suppose we have selected n sentences, and have computed the current subset\u2019s score Hn. We want to know what is the next sentence we should select to achieve our goal of finding the subset of sentences that minimize H. After the n+ 1th step we wish to have the best (lowest) possible Hn+1 score. We cannot un-select a sentence, so Hn+1 can be defined as:\nHn+1 = Hn + \u2206H n\u2192n+1 (4)\nIt is sufficient to find the n+1th sentence that minimizes3 the negative quantity \u2206H n\u2192n+1 .\n2 Expanded from anonymous Stanford NLP notes on KL Divergence: https://nlp.stanford.edu/fsnlp/mathfound/fsnlp-slides-kl.pdf\n3 Sign-switching is hard: H ideally decreases over time, making Hn+1 < Hn. Then [Hn+1 \u2212 Hn] is negative, and ideally as negative as possible. It could be less complicated to define Hn+1 = Hn \u2212 | \u2206H\nn\u2192n+1 |\nand then maximize the positive quantity |\u2206H |.\nWe will shortly derive the following:\n\u2206H n\u2192n+1 = Penalty n\u2192n+1 + Gain n\u2192n+1 (5)\nso of course:\nHn+1 = Hn + Penalty n\u2192n+1 + Gain n\u2192n+1 (6)\nThe penalty term is positive, and the gain term is a negative number. Adding any new line to the corpus incurs an entropy-increasing penalty. If the candidate line is a good one to add to the corpus, then it adds useful information to the model and will lower the cross entropy. A sufficiently good candidate sentence will provide a gain that outweighs the penalty. We would like to minimize the (positive) penalty term, and also minimize the gain term (as negative as possible).\n5.4. Deriving the Greedy Cross-Entropy Delta Expanding the definition of \u2206H (Equation 4) using Equation 3:\n\u2206H n\u2192n+1\n= Hn+1 \u2212Hn\n=\n(\n\u2212 \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn+1(v)\nWn+1\n) \u2212 ( \u2212 \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nWn\n)\n=\n( \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nWn\n) \u2212 ( \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn+1(v)\nWn+1\n)\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr\n(\nlog Cn(v) Wn \u2212 log Cn+1(v) Wn+1\n)\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr\n\nlog\nCn(v) Wn\nCn+1(v) Wn+1\n\n\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\n( Cn(v)\nWn \u00b7 Wn+1 Cn+1(v)\n)\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log ( Wn+1 Wn \u00b7 Cn(v) Cn+1(v) )\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr\n(\nlog Wn+1 Wn + log Cn(v) Cn+1(v)\n)\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log Wn+1 Wn + \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nCn+1(v)\n= log Wn+1 Wn\n( \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr\n)\n+ \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nCn+1(v)\n= log Wn+1 Wn + \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nCn+1(v)\n\u2206H n\u2192n+1\n= log Wn + wn+1\nWn \ufe38 \ufe37\ufe37 \ufe38\nPenalty\n+ \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + cn+1(v) \ufe38 \ufe37\ufe37 \ufe38\nGain\nWe have derived Equation 5, the greedy cross-entropy delta. Sethy et al. (2006a) and (2006b) derived instead the greedy relative-entropy delta. Interestingly, both kinds of entropy have the same deltas because the extra p(x) terms cancel out. Our Penalty term is the same as their T1 term, and our Gain term is their T2 term.4\n4 Actually, our Gain = \u2212(T2), because we gather the negative sign inside the term and they do not.\nThe penalty term biases the delta score towards selecting short sentences. Adding any new line to the corpus incurs an entropy-increasing penalty proportional to the number of tokens in the new line, regardless of what the words are. The penalty term is always positive, and decreases asymptotically to zero: adding one more sentence makes less and less of a difference as the selected corpus grows larger. A specific sentence\u2019s penalty decreases over time, too, for the same reason.\nThe gain term biases the delta score towards longer sentences that contain many words which are high in probability in the Repr distribution but do not appear many times in the sentences selected so far. There is an entropy-lowering benefit to adding information, so the gain of adding any sentence is always a negative number5 that gets subtracted from Hn. Adding more data can be net good (the magnitude of the gain term is larger than the penalty, so H decreases) or net bad (the magnitude of the gain is less than the penalty, increasing H). A sufficiently good candidate sentence will provide a gain that outweighs the penalty: it is adding useful information to the model, and will lower the cross-entropy.\nThe length biases of the penalty and the gain terms counteract each other, guarding the algorithm from the Moore-Lewis method\u2019s fixation on one-word sentences with a very common token, or long sentences full of OOVs.\nWe can plug Equation 5 into Equation 6 to get an iterative method for computing crossentropy:\nHn+1 = Hn + Penalty n\u2192n+1\n+ Gain n\u2192n+1\n= Hn + log Wn + wn+1\nWn \ufe38 \ufe37\ufe37 \ufe38\nPenalty\n+ \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + cn+1(v) \ufe38 \ufe37\ufe37 \ufe38\nGain\nAgain: the log terms force the penalty to be positive and the gain to be negative.\n5.5. Algorithmic Complexity of Greedy Cross-Entropy Delta Method Suppose we have already selected the n most useful sentences from Available, removing them from their original corpus and placing them into Jaded. The theoretically optimal algorithm picks sentence n+ 1 as follows:\n(1) Compute the \u2206H n\u2192n+1 score for each sentence remaining in Available. This requires\ngoing word by word through the sentences in Available to compute the Penalty and Gain terms based on the current count of each word in the sentence.\n5 Negative or zero, so... non-positive?\n(2) Sort the sentences in Available by \u2206H n\u2192n+1 score, and choose the sentence with the\nbest (lowest) score to be sentence n+1. Remove it from Available and add it to Jaded.\nThe steps are repeated for each subsequent sentence to be chosen.\nThe cost appears brutally prohibitive: O(N) iterations, each requiring an update of O(Wavail) words and then a sort of O(N logN) lines. The number of words in the corpus is roughly linear with respect to the number of lines (N = 15-ish W ), so the cost is O(N3 logN).\nWe will not quite do that.\n6. Hulk Smash Math\n6.1. Good-Enough Sentences The Gain term for a particular sentence after selecting n other ones is always:\nGain n\u2192n+1\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + cn+1(v)\nThe total lexicon Vrepr can be divided into two parts: vocabulary items vn+1 that do appear in the (n + 1)st sentence, and those that do not (v /\u2208 vn+1 = Vrepr \\ vn+1) :\nGain n\u2192n+1\n= \u2211\nv\u2208Vrepr\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + cn+1(v)\n= \u2211\nv\u2208vn+1\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + cn+1(v) +\n\u2211\nv/\u2208vn+1\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + 0\n= \u2211\nv\u2208vn+1\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + cn+1(v) +\n\u2211\nv/\u2208vn+1\nCrepr(v)\nWrepr \u00b7 0\n= \u2211\nv\u2208vn+1\nCrepr(v)\nWrepr log\nCn(v)\nCn(v) + cn+1(v)\n= \u2211\nv\u2208vn+1\nGain n\u2192n+1 (v)\nThe words that do not appear in the sentence do not affect the \u2206H score. The gain for the sentence is the sum of the gains for each word (type) in the sentence. The empirical probability Crepr(v)Wrepr has the usual long-tail distribution, so we hypothesize that the sentence gain will often be dominated by only one or a few of the word (type) gain terms. Those dominant words will be ones that the Jaded corpus needs to see more times in order to accurately model the repr distribution.\nIn that case, it would always be useful to select a sentence that contains the word with the best word gain. The best-gain sentence that contains the best-gain word may not be the\noverall best-gain sentence, but it will still be pretty good. It does contain the single most useful word, so even if would ideally not choose to pick this sentence right now, we would probably want to pick it before too long.\nThe exact gain for a sentence from containing a specific word type depends on cn+1(v), the number of times the word type appears in the sentence. We hypothesize that most words will appear once per sentence. The obvious exception are the closed-class words, and they are few. As such, we can estimate the word gain as:\nGain n\u2192n+1\n(v) = Crepr(v)\nWrepr log\nCn(v) Cn(v) + cn+1(v) \u2248 Crepr(v) Wrepr log Cn(v) Cn(v) + 1\nThis estimate is an upper bound on the true value of the word gain, because the gain term is negative and log AA+1 > log A A+2 . This means we are selecting the best word based on a lower bound estimate of the magnitude of its goodness, which is reasonable. The true word gain for any word is at least as good (more negative) as its estimate, so the best word is at least as good as we think it is. We now select sentence n+ 1 as follows:\n(1) Estimate the Gain n\u2192n+1 (v) \u2248 Crepr(v)Wrepr log Cn(v) Cn(v)+1 for all word types v \u2208 V .\n(2) Sort the word types in V by their estimated gain, and note the best word v\u2032\n(3) Let Avail(v\u2032) be the set of lines still in Available which contain v\u2032. Compute the \u2206H\nn\u2192n+1 score for each sentence in Avail(v\u2032).\nThis requires going word by word through each of those sentences to compute the Penalty and Gain terms based on the current count of each word in the sentence.\n(4) Sort Avail(v\u2032) by the \u2206H n\u2192n+1 score, and choose the sentence with the best (lowest)\nscore to be sentence n+ 1. Remove it from Available and add it to Jaded.\nThis algorithm takes O(N) iterations, each requiring: an update of V word gains and a sort of O(V log V ) words to get the best word v\u2032, and then an update of O(WAvail(v\u2032)) words and a sort of O(|Avail(v\u2032)| \u00b7 log |Avail(v\u2032))| lines to get the best line containing v\u2032. The number of words in those lines WAvail(v\u2032) is again linear w.r.t. the number of lines, or O(|Avail(v\u2032)|). The size of |Avail(v\u2032)| can be estimated by the average number of lines that a specific word appears in. Empirically, it\u2019s a little less than 3 \u221a N = N 1 3 , as well as smaller than both log2 N and log2 V . We\u2019ll use the N 1 3 term for clarity. This reduces the complexity to O(N ( V 2 log V +N 1 3 +N 1 3 log(N 1 3 ) ) ), which is mostly O(NV 2 log V )\nbecause large-set vocabularies have over a million word types.\n6.2. Good-Enough Words We have reduced the time complexity of the optimal algorithm to be heavily dependent on\nthe size of the vocabulary rather than the number of lines in the training data. We now focus on reducing the size of the vocabulary itself, with the following intuition (Axelrod et al., 2015):\n\u201cWhere the frequencies of words differ, the corpora differ. Where the frequencies do not differ, neither do the corpora. [...] We use the ratio of the word\u2019s probabilities in the corpora to determine how much the two specific corpora differ with respect to a word. [...]. This can also be readily computed using unigram LMs trained on each of the corpora.\u201d\nConsider the unigram frequency ratio Prepr(v)Punadapt(v) of an arbitrary word v. There are four cases of interest to us:\n(1) Crepr(v) < 3 and Cunadapt(v) < 3: Barely-seen words do not have reliable empirical statistics. A word appearing twice in one corpus and once in another is not necessarily twice as likely, nor is a singleton that does not appear in the other corpus infinitely more likely. The unigram ratio is therefore also not reliable. The threshold of 3 is not important; it was picked because LM smoothing is often applied to terms appearing < 3 times. Choose a threshold x based on the question: \u201cIf I saw a word x \u2212 1 times, would I trust its probability?\u201d. It does not need to be the same threshold for both corpora, either.\n(2) Prepr(v)Punadapt(v) << 1\nWhen the ratio is considerably less than6 1, the word is strongly indicative of the Unadapted distribution. These words (by definition) appear relatively rarely in the Repr corpus, so cannot comprise a significant chunk of the cross-entropy H.\n(3) Prepr(v)Punadapt(v) \u2248 1 These words appear roughly as often (within an order of magnitude) in one distribution as in the other. A sample of the Avail corpus can be expected to contain these words in a reasonable proportion.\n(4) Prepr(v)Punadapt(v) >> 1\nWhen the ratio is considerably greater than7 1, the word is strongly indicative of the Representative distribution. These words are the most important ones: They comprise much of the probability mass of the Repr distribution (according to how much Repr and Unadapt diverge), and they are relatively rare in the Avail corpus.\nWe assert that the words in the fourth category are most important, and that it suffices to use only them for selecting sentences. We collapse each of the first three categories\n6 An order of magnitude less or so: < 1 e , < 1 2 , or < 1 10 depending on base. 7 An order of magnitude more or so: > e, > 2, or > 10 depending on base.\nto a single token, again following Axelrod et al. (2015) and Axelrod (2014). Category 1 words become \u2018dubious\u2019, category 2 are \u2018bad\u2019, and category 3 are \u2018meh\u2019. The words in category 4 remain unchanged. We further differentiate words in Repr but not Avail as \u2018impossible\u2019, and words in Avail+Unadapt but not in Repr as \u2018useless\u2019. This replacement strategy consolidates probability mass for negative and neutral events, letting the distribution focus on words whose presence is meaningful and relevant.\nEmpirically, the reduces the number of words in the vocabulary to around 10,000 word types: words V+REPR with heavy bias towards the repr distribution. The complexity calculation in Section 6.1 becomes\n(1) Estimate the Gain n\u2192n+1 (v) \u2248 Crepr(v)Wrepr log Cn(v) Cn(v)+1 for the\u224810,000 word types in V+REPR.\n(2) Sort the word types in V+REPR by their estimated gain, and note the best word v \u2032\n(3) Let Avail(v\u2032) be the set of lines still in Available that contain v\u2032. Compute the \u2206H\nn\u2192n+1 score for each sentence in Avail(v\u2032).\n(4) Sort Avail(v\u2032) by \u2206H n\u2192n+1 score, and choose the sentence with the best (lowest) score\nto be sentence n+ 1. Remove it from Available and add it to Jaded.\nThis algorithm takes O(N) iterations, each requiring: an update and then sort of V+REPR word gains, but O(10, 000) is now constant: O(C). An update of O(WAvail(v\u2032)) words and a sort of O(|Avail(v\u2032)| \u00b7 log |Avail(v\u2032))| lines to get the best line containing v\u2032. The number of words in those lines WAvail(v\u2032) is again < N 1 3 . The complexity is\nO(N ( C +N 1 3 +N 1 3 logN 1 3 ) ) \u2248 O(NN 13 logN 13 ) = O(N 43 \u00b7 1 3 logN) = O(N 4 3 logN)\nThis value compares very favorably with the original O(N3 logN).\n6.3. Good-Enough Scoring The bulk of the processing time in the algorithmic block is in the rescoring and re-sorting of the sentences that contain the best word for that particular timestep. We assert we do not need to score all the sentences every time.\nThe sentence \u2206H score estimate term is the sum of the penalty and gain terms. While both the penalty and the gain decrease over time, their sum is not monotonic. The sentence gain estimate ( \u2211\nv\u2208Vrepr Crepr(v) Wrepr\n)\nfor a particular sentence always increases over time (the\nterm becomes less negative, so the sentence has less of an effect on the entropy score). A sentence\u2019s current gain estimate is thus a lower bound on the sentence\u2019s gain estimate at some future iteration.\nThe sentence penalty (\nlog Wn+wn+1Wn\n)\nfor a particular sentence always decreases over time.\nHowever, if we compare the magnitude of the derivatives, the penalty term ddnPenalty appears to be smaller than ddnGain as n \u2192 \u221e.8 The penalty term changes mostly as a function of time: each selected sentence increases the size of the selected (cynical) corpus by roughly the average number of words per sentence. By contrast, the sentence gain estimate changes as a function of which particular sentences have been chosen. There is no equivalent notion of \u201caverage vocabulary content of a sentence\u201d, so the gain terms continue to vary significantly according to the sentence.\nThe converging penalty terms mean that we can get away with sorting and storing sentences by their estimated gain rather than their estimated \u2206H score, and compute the penalty on the fly. There may be a few cases where ordering by \u2206H score would not match the ordering by estimated gain, but on the whole it is good enough. A sentence with a pleasing gain estimate is probably useful, even if it is long enough (or the current n+ 1th iteration is early enough) to have a large penalty term.\n(1) Estimate the Gain n\u2192n+1 (v) \u2248 Crepr(v)Wrepr log Cn(v) Cn(v)+1 for the\u224810,000 word types in V+REPR.\n(2) Sort the word types in V+REPR by their estimated gain, and note the best word v \u2032\n(3) Let Avail(v\u2032) be the set of lines still in Available that contain v\u2032. The previous iteration left this list sorted by estimated sentence gain. We can update the sentence gain score for only the first sentence on the list, and then note where in the list it would be resorted to. Let the formerly-best sentence\u2019s new index in the list be i. Thanks to the monotonicity of the sentence gain estimate, only sentences with index < i need to have their scores updated. Compute the \u2206H\nn\u2192n+1 score for each such sentence in Avail(v\u2032).\n(4) Sort Avail(v\u2032) by \u2206H n\u2192n+1 score, and choose the sentence with the best (lowest) score\nto be sentence n+ 1. Remove it from Available and add it to Jaded.\nIt does not seem like the complexity cost changes appreciably.\n6.4. Good-Enough Code The process of removing the chosen sentence from Available is not efficient. Recall that we are only looking at lines that contain our best word v\u2032. For each of the 10,000 or so words in V+REPR, we maintain a list of all sentences that contain it. These lists enable\n8 The following argument might hold, but we are not sure:\nUsing standard identities, d dx\n(\nlog x+k x\n)\n= \u2212k x(x+k) and d dy\n(\nlog y y+i\n)\n= i y(y+i) , so d dn Penalty \u2248 \u2212k Wn(Wn+k)\nand d dn Gain \u2248 \u2211 v i Cn(v)(Cn(v)+i) In general, k (avg number of words per sentence) > i (avg number of occurrences of a word within a sentence), and Wn = \u2211\nv Cn(v), so the magnitude of the derivative of the\npenalty should be less than the magnitude of the derivative of the gain, at least in the limit.\nthe significant complexity reduction in Section 6.1. Every sentence thus appears as many times in the full mapping as there are words types in the sentence.\nWhen we remove the best sentence from Available at each iteration, we are simply using shift to remove the first element from the list of sentences for the single best word. The other mentions of that sentence, in the sentence-lists for the other words in the sentence, are not removed. Each sentence-list is kept sorted by the estimated gain of the sentences in it. Therefore there is not a cheap way to find and remove a sentence without checking every sentence in the list.\nInstead, we maintain a small hash table in which the sentence IDs are the keys, and the value is \u201ctrue\u201d. When we remove the best sentence from Available, we delete its key from the hash. The entries for that sentence in the other lists are unchanged, but they are now ghosts of sentences already-selected. We eliminate these ghosts only when we happen to see them, namely when we (try to) update the sentence\u2019s score or select the same sentence again.\nWhen updating a sentence\u2019s score, it is simple enough to check whether the sentence ID is still in the hash table. This operation is O(1), and we do it as many times as there are sentences to check. If we checked every sentence, we would not save any time. In Section 6.3 we mentioned using the change in index after sorting as a way to reduce the number of sentences to check\u2013 here is why this reduction matters. The worst case scenario is that the sentences always get resorted to the end of the list, so the complexity cost does not change even though in practice it is faster."}, {"heading": "7. Batch Mode = Fast Mode", "text": "Finding the single next best sentence to add is the most efficient strategy for the case where we really want to pick the absolute best sentences for manual processing (transcription, translation, etc), because the external cost for using each sentence is relatively high.\nHowever, it is common to just want a relevant subcorpus for automatically training a language model, an MT system, or some other downstream system. Here the cost for using each sentence is relatively low, and it only matters whether a particular sentence is selected (or not). It does not matter much whether the sentence is chosen on the thirtieth or three millionth step, and certainly not if it was number 235,442 versus number 235,443. As such, there is little point to worrying about getting the best sentence at the optimal iteration. Any iteration will do, much like any port in a storm.\nWe enable Batch Mode to hurry the process along a bit. Let A(v\u2032) = |Avail(v\u2032)| as the number of available sentences still containing the best word v\u2032. At every iteration, we now always update the scores of the top \u221a A(v\u2032) lines (not counting ghosts that we prune). We then always select (and remove) the top 12 \u221a A(v\u2032) lines.\nThis reduces the complexity even further. We previously estimated A(v\u2032) \u2248 N 13 , so:\nO( 1\n1 2\n\u221a A(v\u2032) N\n4 3 logN) \u2248 O(2 \u00b7 N\n4 3\nN 1 3\nlogN) \u2248 O(N logN)\nThis is about as good as one can hope for.\nAs string-identical sentences have identical estimated gains and penalties, Batch Mode will often select multiple copies of the same sentence in a single batch (the one-at-a-time version will never do this). Picking a sentence twice (or twenty times) is not a bad thing, as long as the sentence contains at least one word that needs to be seen many more times in order to better match the Representative distribution. As the sentences are selected based on the presence of such a word, the condition is satisfied by construction.\nHowever, it greatly bothers human users to see The Algorithm make such an obvious \u2018mistake\u2019, and it leads to bikeshedding. We are tired of explaining how statistical methods work to people who think that a second copy of a sentence invalidates the entire selection method. Therefore, we unique all sentences within a batch, and un-select any copies. The un-selected identical sentences go back onto the word\u2019s list of un-selected sentences (e.g. A(v\u2032)). Thus it is still both possible and probable for 500 occurrences of something as banal as \u2018\u2018Thank you .\u2019\u2019 to be selected. Mercifully, these copies will be sprinkled across 500 batches in the final selected corpus, so fewer humans will notice."}, {"heading": "8. Wrapping Up", "text": "All experimental results will be published separately, for entirely amiable legal reasons. The code for cynical data selection is going through proper corporate/legal channels for release. It will be on github with a permissive license as soon as that is completed (expected September 2017). In a pinch, this work contains enough detail to implement the algorithm.\nThe method presented in the preceding sections has the following properties:\n\u2022 Is responsive to the extent to which two corpora differ. It retains the core concept from cross-entropy difference of playing the two distributions against each other, but only for the parts that actually differ between the two distributions.\n\u2022 Quickly reaches near-optimal vocabulary coverage. The cynical selection method selects sentences by first looking for word whose probability estimate can be improves the most. It happens that the greatest possible improvement is to go from zero \u2013 unseen \u2013 to a bad estimate (seen once). Because of this, it winds up selecting almost every word in the Representative corpus\u2013 exactly once! \u2013 very early in the process. This minimizes the number of unknown words in the training corpus, meaning the cynical subset preserves the breadth of useful vocabulary found in the much larger Available corpus. This is also an\nimprovement over Moore-Lewis: empirically, the cynical subsets have about 80% fewer OOV (out of vocabulary) tokens than the Moore-Lewis subsets.\n\u2022 Takes into account what has already been selected. This is the key benefit of the submodular data selection methods of Bilmes and Kirchhoff (2014) and Wei et al. (2014). Our cynical method seems to satisfy the definition of submodularity9 However, we are able to avoid much of their heavy lifting by always using the same feature function (entropy gain) and not needing to specify a budget. We can automatically pick a good cutoff point by select sentences until the estimated gain becomes (and stays) positive.\n\u2022 Does not involve defining any kind of domain. More importantly, it also does not define what it means to not belong to the domain. A sentence could be moved from one corpus to another, or deleted, or cloned into both, without changing the definition of the Representative and Unadapted distributions. Those distributions represent the two corpora, regardless of what they may contain. Compare with the Moore-Lewis method, where the Pool data had to differ significantly from the Task corpus, and subsequent classifier-based extensions where \u201cnot-in-domain\u201d data had to be defined. Similarly, our method does not involve any kind of classifier. We can quantify relevance without needing to label anything.\n\u2022 Has real units. A sentence\u2019s relevance score is the change it has, in bits of entropy, on how well a model can represent the Representative data. As they are real units, they can be compared across systems, methods, etc. Cross-entropy difference scores are also in bits, but they are relative: \u201cby how many more bits does the in-domain LM like the sentence than the Pool LM does?\u201d. Changing either of the models renders the scores meaningless. Other methods use abstract measures like cosine similarity in higher-dimensional spaces, or other normalized scores that lack units entirely. Three may be the number of the count10, but it makes little sense to say \u201cthis sentence is 3 close to the in-domain model\u201d.\n\u2022 Knows approximately when to stop. Data selection methods often require gridsearch to determine the best amount of data to select. First try 1%, then 5%, then 10% and so on, build systems on each of them, and take the one that works the best. The optimal value does not carry over between systems nor language arcs. The cynical method knows it has selected enough data for the most good enough (if not best) subset: when \u2206H\nn\u2192n+1 becomes\n(and stays) positive. No tuning nor searching is needed.\n9 from Wei et al. (2014): \u201c...an equivalent definition of submodularity is f(j|S) \u2265 f(j|T ), \u2200S?T . That is, the incre- mental gain of adding item j to the set decreases when the set in which j is considered grows from S to T .\u201d In our case, the gain of adding a sentence decreases as the already-selected set grows.\n10 \u201c...and the number of the counting shall be three.\u201d"}], "references": [{"title": "Data Selection for Statistical Machine Translation", "author": ["A. Axelrod"], "venue": "PhD thesis, University of Washington.", "citeRegEx": "Axelrod,? 2014", "shortCiteRegEx": "Axelrod", "year": 2014}, {"title": "Domain Adaptation Via Pseudo In-Domain Data Selection", "author": ["A. Axelrod", "X. He", "J. Gao"], "venue": "EMNLP (Empirical Methods in Natural Language Processing), pages 355\u2013362.", "citeRegEx": "Axelrod et al\\.,? 2011", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Class-Based N-gram Language Difference Models for Data Selection", "author": ["A. Axelrod", "Y. Vyas", "M. Martindale", "M. Carpuat"], "venue": "IWSLT (International Workshop on Spoken Language Translation).", "citeRegEx": "Axelrod et al\\.,? 2015", "shortCiteRegEx": "Axelrod et al\\.", "year": 2015}, {"title": "Submodularity for Data Selection in Statistical Machine Translation", "author": ["J. Bilmes", "K. Kirchhoff"], "venue": "EMNLP (Empirical Methods in Natural Language Processing).", "citeRegEx": "Bilmes and Kirchhoff,? 2014", "shortCiteRegEx": "Bilmes and Kirchhoff", "year": 2014}, {"title": "Comparing Corpora", "author": ["A. Kilgarriff"], "venue": "International Journal of Corpus Linguistics.", "citeRegEx": "Kilgarriff,? 2001", "shortCiteRegEx": "Kilgarriff", "year": 2001}, {"title": "Intelligent Selection of Language Model Training Data", "author": ["R.C. Moore", "W.D. Lewis"], "venue": "ACL (Association for Computational Linguistics).", "citeRegEx": "Moore and Lewis,? 2010", "shortCiteRegEx": "Moore and Lewis", "year": 2010}, {"title": "A Universal Prior for Integers and Estimation by Minimum Description Length", "author": ["J. Rissanen"], "venue": "The Annals of Statistics, 11(2):416\u2013431.", "citeRegEx": "Rissanen,? 1983", "shortCiteRegEx": "Rissanen", "year": 1983}, {"title": "Selecting Relevant Text Subsets from Web-Data for Building Topic Specific Language Models", "author": ["A. Sethy", "P.G. Georgiou", "S. Narayanan"], "venue": "NAACL (North American Association for Computational Linguistics).", "citeRegEx": "Sethy et al\\.,? 2006a", "shortCiteRegEx": "Sethy et al\\.", "year": 2006}, {"title": "Text Data Acquisition for DomainSpecific Language Models", "author": ["A. Sethy", "P.G. Georgiou", "S. Narayanan"], "venue": "EMNLP (Empirical Methods in Natural Language Processing), (July):382\u2013389.", "citeRegEx": "Sethy et al\\.,? 2006b", "shortCiteRegEx": "Sethy et al\\.", "year": 2006}, {"title": "Scalable Backoff Language Models", "author": ["K. Seymore", "R. Rosenfeld"], "venue": "ICLSP (International Conference on Spoken Language Processing).", "citeRegEx": "Seymore and Rosenfeld,? 1996", "shortCiteRegEx": "Seymore and Rosenfeld", "year": 1996}, {"title": "Growing an N-gram Language Model", "author": ["V. Siivola", "B.L. Pellom"], "venue": "INTERSPEECH.", "citeRegEx": "Siivola and Pellom,? 2005", "shortCiteRegEx": "Siivola and Pellom", "year": 2005}, {"title": "Entropy-Based Pruning of Backoff Language Models", "author": ["A. Stolcke"], "venue": "DARPA Broadcast News Transcription and Understanding Workshop.", "citeRegEx": "Stolcke,? 1998", "shortCiteRegEx": "Stolcke", "year": 1998}, {"title": "Submodular Subset Selection for Large-Scale Speech Training Data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "C. Bartels", "J. Bilmes"], "venue": "ICASSP (International Conference on Acoustics, Speech, and Signal Processing).", "citeRegEx": "Wei et al\\.,? 2014", "shortCiteRegEx": "Wei et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "This is the traditional data selection scenario, and we improve upon the standard Moore-Lewis cross-entropy difference method (Moore and Lewis, 2010).", "startOffset": 126, "endOffset": 149}, {"referenceID": 9, "context": "Language Model Size Seymore and Rosenfeld (1996) decided that pruning a model is better than incrementally growing it:", "startOffset": 20, "endOffset": 49}, {"referenceID": 7, "context": "The relative entropy score of the sentence actually can be decomposed into the sum of the word scores (see derivation by Sethy et al. (2006b)), so there is no need to assume.", "startOffset": 121, "endOffset": 142}, {"referenceID": 6, "context": "They compute gains on the data coding length, along the lines of the Minimum Description Length principle which minimizes the size of the model plus the training data as encoded by the model (Rissanen, 1983).", "startOffset": 191, "endOffset": 207}, {"referenceID": 6, "context": "They compute gains on the data coding length, along the lines of the Minimum Description Length principle which minimizes the size of the model plus the training data as encoded by the model (Rissanen, 1983). Sethy et al. (2006b) also take the growing", "startOffset": 192, "endOffset": 230}, {"referenceID": 4, "context": "Corpus Similarity Kilgarriff (2001) posited:", "startOffset": 18, "endOffset": 36}, {"referenceID": 5, "context": "The standard approach for data selection is the same for both language modeling (Moore and Lewis, 2010) and machine translation (Axelrod et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 1, "context": "The standard approach for data selection is the same for both language modeling (Moore and Lewis, 2010) and machine translation (Axelrod et al., 2011).", "startOffset": 128, "endOffset": 150}, {"referenceID": 7, "context": "Sethy et al. (2006b) describe the general idea as \u201can incremental greedy selection scheme based on relative entropy, which selects a sentence if adding it to the already selected set of sentences reduces the relative entropy with respect to the in-domain data distribution\u201d.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "1 More complete explanations can be found in previous work: Axelrod et al. (2011) and Axelrod (2014).", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "1 More complete explanations can be found in previous work: Axelrod et al. (2011) and Axelrod (2014).", "startOffset": 60, "endOffset": 101}, {"referenceID": 7, "context": "Sethy et al. (2006a) and (2006b) derived instead the greedy relative-entropy delta.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "Sethy et al. (2006a) and (2006b) derived instead the greedy relative-entropy delta.", "startOffset": 0, "endOffset": 33}, {"referenceID": 2, "context": "We now focus on reducing the size of the vocabulary itself, with the following intuition (Axelrod et al., 2015): \u201cWhere the frequencies of words differ, the corpora differ.", "startOffset": 89, "endOffset": 111}, {"referenceID": 0, "context": "to a single token, again following Axelrod et al. (2015) and Axelrod (2014).", "startOffset": 35, "endOffset": 57}, {"referenceID": 0, "context": "to a single token, again following Axelrod et al. (2015) and Axelrod (2014). Category 1 words become \u2018dubious\u2019, category 2 are \u2018bad\u2019, and category 3 are \u2018meh\u2019.", "startOffset": 35, "endOffset": 76}, {"referenceID": 3, "context": "This is the key benefit of the submodular data selection methods of Bilmes and Kirchhoff (2014) and Wei et al.", "startOffset": 68, "endOffset": 96}, {"referenceID": 3, "context": "This is the key benefit of the submodular data selection methods of Bilmes and Kirchhoff (2014) and Wei et al. (2014). Our cynical method seems to satisfy the definition of submodularity However, we are able to avoid much of their heavy lifting by always using the same feature function (entropy gain) and not needing to specify a budget.", "startOffset": 68, "endOffset": 118}, {"referenceID": 12, "context": "9 from Wei et al. (2014): \u201c.", "startOffset": 7, "endOffset": 25}], "year": 2017, "abstractText": "The Moore-Lewis method of \u201cintelligent selection of language model training data\u201d is very effective, cheap, efficient... and also has structural problems. (1) The method defines relevance by playing language models trained on the in-domain and the out-of-domain (or data pool) corpora against each other. This powerful idea \u2013 which we set out to preserve \u2013 treats the two corpora as the opposing ends of a single spectrum. This lack of nuance does not allow for the two corpora to be very similar. In the extreme case where the come from the same distribution, all of the sentences have a Moore-Lewis score of zero, so there is no resulting ranking. (2) The selected sentences are not guaranteed to be able to model the in-domain data, nor to even cover the in-domain data. They are simply well-liked by the in-domain model; this is necessary, but not sufficient. (3) There is no way to tell what is the optimal number of sentences to select, short of picking various thresholds and building the systems. We present \u201ccynical selection of training data\u201d: a greedy, lazy, approximate, and generally efficient method of accomplishing the same goal. It has the following properties: (1) Is responsive to the extent to which two corpora differ. (2) Quickly reaches near-optimal vocabulary coverage. (3) Takes into account what has already been selected. (4) Does not involve defining any kind of domain, nor any kind of classifier. (5) Has real units. (6) Knows approximately when to stop. 1. Scenario We have a number of translation tasks, each defined as \u201ca flavor of data we want to be able to translate well\u201d. Perhaps it\u2019s a specific client\u2019s data, or a kind of data such as \u201ccustomer support chat logs\u201d, or just a system for a language arc (\u201cMexican Spanish to American English\u2019). Machine Translation (MT) tasks often have one of these two problems: (1) Web-scale data, or too much data than can be used readily to train or run an MT system. We want to know what data we can exclude from training without sacrificing performance. (2) Not enough parallel data to train an MT system. We want to know what data we can add to improve performance, and how much improvement we might expect. Work unaffiliated with amazon.com. 1 2 CYNICAL SELECTION OF LANGUAGE MODEL TRAINING DATA We can add data from known parallel data, or by paying to have monolingual data translated. We present a single method for handling both: (1) Given a too-large parallel corpus, we identify the best subset to use for training a system that is at least as good as the full system. This is the traditional data selection scenario, and we improve upon the standard Moore-Lewis cross-entropy difference method (Moore and Lewis, 2010). (2) Given a representative monolingual corpus for a translation task, and an optional parallel corpus in the language pair, identify the monolingual sentences that should be manually translated and added to the parallel corpus in order to improve translation for the specific task. Translating monolingual data is expensive, and training on bilingual data is expensive. By \u201cexpense\u201d we mean effort, computation, time, and dollars. As a rule, we want to spend as little as possible to get as much out of the available data as we can.", "creator": "LaTeX with hyperref package"}}}