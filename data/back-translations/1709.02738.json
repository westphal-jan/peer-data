{"id": "1709.02738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Cycles in adversarial regularized learning", "abstract": "Regularized learning is a basic technique in online optimization, machine learning and many other areas of computer science. A natural question that arises in this environment is how regulated learning algorithms behave toward each other. We study a natural formulation of this problem by coupling regulated learning dynamics in zero-sum games. We show that the behavior of the system Poincar\\'e is recurrent, which implies that almost every trajectory checks each (arbitrarily small) neighborhood of its starting point infinitely often. This cycle behavior is robust to the choice of the regulatory mechanism by the agents (each agent could use a different regulator), to positive-affinity transformations of agents \"tools, and it also exists in the case of networked competition, i.e. zero-sum games in the polymatrix.", "histories": [["v1", "Fri, 8 Sep 2017 15:16:54 GMT  (1106kb,D)", "http://arxiv.org/abs/1709.02738v1", "22 pages, 4 figures"]], "COMMENTS": "22 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["panayotis mertikopoulos", "christos papadimitriou", "georgios piliouras"], "accepted": false, "id": "1709.02738"}, "pdf": {"name": "1709.02738.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["panayotis.mertikopoulos@imag.fr,", "christos@berkeley.edu,", "georgios@sutd.edu.sg."], "sections": [{"heading": "1. Introduction", "text": "Regularization is a fundamental and incisive method in optimization, its present zeitgeist and its entry into machine learning. Through the introduction of a new component in the objective, regularization techniques overcome ill-conditioning and overfitting, and they yield algorithms that achieve sparsity and parsimony without sacrificing efficiency [2, 5, 8].\nIn the context of online optimization, these features are exemplified in the family of learning algorithms known as \u201cFollow the Regularized Leader\u201d (FoReL) [41]. FoReL represents an important archetype of adaptive behavior for several reasons: it provides optimal min-max regret guarantees (O(t\u22121/2) in an adversarial setting), it offers significant flexibility with respect to the geometry of the problem at hand, and it captures numerous other dynamics as special cases (hedge, multiplicative weights, gradient descent, etc.) [2, 8, 15]. As such, given that these regret guarantees hold without any further assumptions about how payoffs/costs are determined at each stage, the dynamics of FoReL have been the object of intense scrutiny and study in algorithmic game theory.\nThe standard way of analyzing such no-regret dynamics in games involves a two-step approach. The first step exploits the fact that the empirical frequency of\n\u2217 Univ. Grenoble Alpes, CNRS, Inria, LIG, F-38000 Grenoble, France. \u00a7 UC Berkeley. \u2021 Singapore University of Technology and Design. E-mail addresses: panayotis.mertikopoulos@imag.fr, christos@berkeley.edu,\ngeorgios@sutd.edu.sg. Key words and phrases. Regret; multiplicative weights update; zero-sum games; dueling algorithms. Panayotis Mertikopoulos was partially supported by the French National Research Agency (ANR) project ORACLESS (ANR\u2013GAGA\u201313\u2013JS01\u20130004\u201301) and the Huawei Innovation Research Program ULTRON. Georgios Piliouras would like to acknowledge SUTD grant SRG ESD 2015 097 and MOE AcRF Tier 2 Grant 2016-T2-1-170.\n1\nar X\niv :1\n70 9.\n02 73\n8v 1\n[ cs\n.G T\n] 8\nS ep\n2 01\n7\nplay under a no-regret algorithm converges to the game\u2019s set of coarse correlated equilibria (CCE). The second involves proving some useful property of the game\u2019s CCE: For instance, leveraging (\u03bb, \u00b5)-robustness [33] implies that the social welfare at a CCE lies within a small constant of the optimum social welfare; as another example, the product of the marginal distributions of CCE in zero-sum games is Nash. In this way, the no-regret properties of FoReL can be turned into convergence guarantees for the players\u2019 empirical frequency of play (that is, in a time-averaged, correlated sense).\nRecently, several papers have moved beyond this \u201cblack-box\u201d framework and focused instead on obtaining stronger regret/convergence guarantees for systems of learning algorithms coupled together in games with a specific structure. Along these lines, Daskalakis et al. [9] and Rakhlin and Sridharan [31] developed classes of dynamics that enjoy a O(log t/t) regret minimization rate in two-player zerosum games. Syrgkanis et al. [43] further analyzed a recency biased variant of FoReL in more general multi-player games and showed that it is possible to achieve an O(t\u22123/4) regret minimization rate. The social welfare converges at a rate of O(t\u22121), a result which was extended to standard versions of FoReL dynamics in [11].\nWhilst a regret-based analysis provides significant insights about these systems, it does not answer a fundamental behavioral question:\nDoes the system converge to a Nash equilibrium? Does it even stabilize?\nThe dichotomy between a self-stabilizing, convergent system and a system with recurrent cycles is of obvious significance, but a regret-based analysis cannot distinguish between the two. Indeed, convergent, recurrent, and even chaotic [26] systems may exhibit equally strong regret minimization properties in general games, so the question remains: What does the long-run behavior of FoReL look like, really?\nThis question becomes particularly interesting and important under perfect competition (such as zero-sum games and variants thereof). Especially in practice, zero-sum games can capture optimization \u201cduels\u201d [18]: for example, two Internet search engines competing to maximize their market share can be modeled as players in a zero-sum game with a convex strategy space. In [18] it was shown that the time-average of a regret-minimizing class of dynamics converges to an approximate equilibrium of the game. Finally, zero-sum games have also been used quite recently as a model for deep learning optimization techniques in image generation and discrimination [14, 39].\nIn each of the above cases, min-max strategies are typically thought of as the axiomatically correct prediction. The fact that the time average of the marginals of a FoReL procedure converges to such states is considered as further evidence of the correctness of this prediction. However, the long-run behavior of the actual sequence of play (as opposed to its time-averages) seems to be trickier, and a number of natural questions arise:\n- Does optimization-driven learning converge under perfect competition? - Does fast regret minimization necessarily imply (fast) equilibration in this case?\nOur results. We settle these questions with a resounding \u201cno\u201d. Specifically, we show that the behavior of FoReL in zero-sum games with an interior equilibrium\n(e.g. Matching Pennies) is Poincar\u00e9 recurrent, implying that almost every trajectory revisits any (arbitrarily small) neighborhood of its starting point infinitely often. Importantly, the observed cycling behavior is robust to the agents\u2019 choice of regularization mechanism (each agent could be using a different regularizer), and it applies to any positive affine transformation of zero-sum games (and hence all strictly competitive games [1]) even though these transformations lead to different trajectories of play. Finally, this cycling behavior also persists in the case of networked competition, i.e. for constant-sum polymatrix games [6, 7, 10].\nGiven that the no-regret guarantees of FoReL require a decreasing step-size (or learning rate),1 we focus on a smooth version of FoReL described by a dynamical system in continuous time. The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48]. In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.\nFrom a technical point of view, our analysis touches on several issues. Our first insight is to focus not on the simplex of the players\u2019 mixed strategies, but on a dual space of payoff differences. The reason for this is that the vector of cumulative payoff differences between two strategies fully determines a player\u2019s mixed strategy under FoReL, and it is precisely these differences that ultimately drive the players\u2019 learning process. Under this transformation, FoReL exhibits a striking property,\n1A standard trick is to decrease step-sizes by a constant factor after a window of \u201cdoubling\u201d length [40].\nincompressibility : the flow of the dynamics is volume-preserving, so a ball of initial conditions in this dual space can never collapse to a point.\nThat being said, the evolution of such a ball in the space of payoffs could be transient, implying in particular that the players\u2019 mixed strategies could converge (because the choice map that links payoff differences to strategies is nonlinear). To rule out such behaviors, we show that FoReL in zero-sum games with an interior Nash equilibrium has a further important property: it admits a constant of motion. Specifically, if x\u2217 = (x\u2217i )i\u2208N is an interior equilibrium of the game and yi is an arbitrary point in the payoff space of player i, this constant is given by the coupling function\nG(y) = \u2211 i\u2208N [h\u2217i (yi)\u2212 \u3008yi, x\u2217i \u3009],\nwhere h\u2217i (yi) = maxxi{\u3008yi, xi\u3009 \u2212 hi(xi)} is the convex conjugate of the regularizer hi that generates the learning process of player i (for the details, see Sections 3 and 4). Coupled with the dynamics\u2019 incompressibility, this invariance can be used to show that FoReL is recurrent : after some finite time, almost every trajectory returns arbitrarily close to its initial state.\nOn the other hand, if the game does not admit an interior equilibrium, the coupling above is no longer a constant of motion. In this case, G decreases over time until the support of the players\u2019 mixed strategies matches that of a Nash equilibrium with maximal support: as this point in time is approached, G essentially becomes a constant. Thus, in general zero-sum games, FoReL wanders perpetually in the smallest face of the game\u2019s strategy space containing all of the game\u2019s equilibria; indeed, the only possibility that FoReL converges is if the game admits a unique Nash equilibrium in pure strategies \u2013 a fairly restrictive requirement."}, {"heading": "2. Definitions from game theory", "text": "2.1. Games in normal form. We begin with some basic definitions from game theory. A finite game in normal form consists of a finite set of players N = {1, . . . , N}, each with a finite set of actions (or strategies) Ai. The preferences of player i for one action over another are determined by an associated payoff function ui : A \u2261 \u220f iAi \u2192 R which assigns a reward ui(\u03b1i;\u03b1\u2212i) to player i \u2208 N under the strategy profile (\u03b1i;\u03b1\u2212i) of all players\u2019 actions.2 Putting all this together, a game in normal form will be written as a tuple \u0393 \u2261 \u0393(N ,A, u) with players, actions and payoffs defined as above.\nPlayers can also use mixed strategies, i.e. mixed probability distributions xi = (xi\u03b1i)\u03b1i\u2208Ai \u2208 \u2206(Ai) over their action sets Ai. The resulting probability vector xi is called a mixed strategy and we write Xi = \u2206(Ai) for the mixed strategy space of player i. Aggregating over players, we also write X = \u220f i Xi for the game\u2019s strategy space, i.e. the space of all strategy profiles x = (xi)i\u2208N . In this context (and in a slight abuse of notation), the expected payoff of the i-th player in the profile x = (x1, . . . , xN ) is\nui(x) = \u2211 \u03b11\u2208A1 \u00b7 \u00b7 \u00b7 \u2211 \u03b1N\u2208AN ui(\u03b11, . . . , \u03b1N )x1\u03b11 \u00b7 \u00b7 \u00b7xN\u03b1N . (2.1)\nTo keep track of the payoff of each pure strategy, we also write vi\u03b1i(x) = ui(\u03b1i;x\u2212i) for the payoff of strategy \u03b1i \u2208 Ai under the profile x \u2208 X and vi(x) = (vi\u03b1i(x))\u03b1i\u2208Ai\n2In the above, we use the standard shorthand (\u03b2i;\u03b1\u2212i) for the profile (\u03b11, . . . , \u03b2i, . . . , \u03b1N ).\nfor the resulting payoff vector of player i. We then have ui(x) = \u3008vi(x), xi\u3009 = \u2211 \u03b1i\u2208Ai xi\u03b1ivi\u03b1i(x), (2.2)\nwhere \u3008v, x\u3009 \u2261 v>x denotes the ordinary pairing between v and x. The most widely used solution concept in game theory is that of a Nash equilibrium (NE), defined here as a mixed strategy profile x\u2217 \u2208 X such that\nui(x \u2217 i ;x \u2217 \u2212i) \u2265 ui(xi;x\u2217\u2212i) (NE)\nfor every deviation xi \u2208 Xi of player i and all i \u2208 N . Writing supp(x\u2217i ) = {\u03b1i \u2208 Ai : x\u2217i > 0} for the support of x\u2217i \u2208 Xi, a Nash equilibrium x\u2217 \u2208 X is called pure if supp(x\u2217i ) = {\u03b1\u2217i } for some \u03b1\u2217i \u2208 Ai and all i \u2208 N . At the other end of the spectrum, x\u2217 is said to be interior (or fully mixed) if supp(x\u2217i ) = Ai for all i \u2208 N . Finally, a coarse correlated equilibrium (CCE) is a distribution \u03c0 over the set of action profiles A \u2261 \u220f iAi such that, for every player i \u2208 N and every\naction \u03b2i \u2208 Ai, we have \u2211 \u03b1\u2208A vi(\u03b1)\u03c0(\u03b1) \u2265 \u2211 \u03b1\u2212i\u2208A\u2212i vi(\u03b2i, \u03b1\u2212i)\u03c0i(\u03b1\u2212i), where\n\u03c0i(\u03b1\u2212i) = \u2211 \u03b1i\u2208\u03b1i \u03c0(\u03b1i, \u03b1\u2212i) is the marginal distribution of \u03c0 with respect to i.\n2.2. Zero-sum games and zero-sum polymatrix games. Perhaps the most widely studied class of finite games (and certainly the first to be considered) is that of 2- player zero-sum games, i.e. when N = {1, 2} and u1 = \u2212u2. Letting u \u2261 u1 = \u2212u2, the value of a 2-player zero-sum game \u0393 is defined as\nu\u0393 = max x1\u2208X1 min x2\u2208X2 u(x1, x2) = min x2\u2208X2 max x1\u2208X1 u(x1, x2), (2.3)\nwith equality following from von Neumann\u2019s celebrated min-max theorem [47]. As is well known, the solutions of this saddle-point problem form a closed, convex set consisting precisely of the game\u2019s Nash equilibria; moreover, the players\u2019 equilibrium payoffs are simply u\u0393 and \u2212u\u0393 respectively. As a result, Nash equilibrium is the standard game-theoretic prediction in such games.\nAn important question that arises here is whether the straightforward equilibrium structure of zero-sum games extends to the case of a network of competitors. Following [6, 7, 10], an N -player pairwise zero-/constant-sum polymatrix game consists of an (undirected) interaction graph G \u2261 G(N , E) whose set of nodes N represents the competing players, with two nodes i, j \u2208 N connected by an edge e = (i, j) in E if and only if the corresponding players compete with each other in a two-player zero-/constant-sum game.\nTo formalize this, we assume that a) every player has a finite set of actions Ai (as before); and b) to each edge e = {i, j} \u2208 E is associated a two-player game zero-/constant-sum \u0393e with player set Ne = {i, j}, action sets Ai and Aj , and payoff functions uij = \u03b3{i,j} \u2212 uji : Ai \u00d7Aj \u2192 R respectively.3 The space of mixed strategies of player i is again Xi = \u2206(Ai), but the player\u2019s payoff is now determined by aggregating over all games involving player i, i.e.\nui(x) = \u2211 j\u2208Ni uij(xi, xj), (2.4)\n3In a zero-sum game, we have \u03b3{i,j} = 0 by default. Since the underlying interaction graph is assumed undirected, we also assume that the labeling of the players\u2019 payoff functions is symmetric. At the expense of concision, our analysis extends to directed graphs, but we stick with the undirected case for clarity.\nwhere Ni = {j \u2208 N : {i, j} \u2208 E} denotes the set of \u201cneighbors\u201d of player i. In other words, the payoff to player i is simply the the sum of all payoffs in the zero/constant-sum games that player i plays with their neighbors.\nIn what follows, we will also consider games which are payoff-equivalent to positive-affine transformations of pairwise constant-sum polymatrix games. Formally, we will allow for games \u0393 such that there exists a pairwise constant-sum polymatrix game \u0393\u2032 and constants ai > 0 and bi \u2208 R for each player i such that u\u0393i (\u03b1) = aiu \u0393\u2032 i (\u03b1) + bi for each outcome \u03b1 \u2208 A."}, {"heading": "3. No-regret learning via regularization", "text": "Throughout this paper, our focus will be on repeated decision making in lowinformation environments where the players don\u2019t know the rules of the game (perhaps not even that they are playing a game). In this case, even if the game admits a unique Nash equilibrium, it is not reasonable to assume that players are able to pre-compute their component of an equilibrium strategy \u2013 let alone assume that all players are fully rational, that there is common knowledge of rationality, etc.\nWith this in mind, we only make the bare-bones assumption that every player seeks to at least minimize their \u201cregret\u201d, i.e. the average payoff difference between a player\u2019s mixed strategy at time t \u2265 0 and the player\u2019s best possible strategy in hindsight. Formally, assuming that play evolves in continuous time, the regret of player i along the sequence of play x(t) is defined as\nRegi(t) = max pi\u2208Xi\n1\nt \u222b t 0 [ui(pi;x\u2212i(s))\u2212 ui(x(s))] ds, (3.1)\nand we say that player i has no regret under x(t) if lim supt\u2192\u221eRegi(t) \u2264 0. The most widely used scheme to achieve this worst-case guarantee is known as \u201cFollow the Regularized Leader\u201d (FoReL), an exploitation-exploration class of policies that consists of playing a mixed strategy that maximizes the player\u2019s expected cumulative payoff (the exploitation part) minus a regularization term (exploration). In our continuous-time framework, this is described by the learning dynamics\nyi(t) = yi(0) + \u222b t 0 vi(x(s)) ds,\nxi(t) = Qi(yi(t)),\n(FoReL)\nwhere the so-called choice map Qi : RAi \u2192 Xi is defined as Qi(yi) = arg max\nxi\u2208Xi {\u3008yi, xi\u3009 \u2212 hi(xi)}. (3.2)\nIn the above, the regularizer function hi : Xi \u2192 R is a convex penalty term which smoothens the \u201chard\u201d arg max correspondence yi 7\u2192 arg maxxi\u2208Xi\u3008yi, xi\u3009 that maximizes the player\u2019s cumulative payoff over [0, t]. As a result, the \u201cregularized leader\u201d Qi(yi) = arg maxxi\u2208Xi{\u3008yi, xi\u3009 \u2212 hi(xi)} is biased towards the prox-center pi = arg minxi\u2208Xi hi(xi) of Xi. For most common regularizers, the prox-center is interior (and usually coincides with the barycenter of X ), so the regularization in (3.2) encourages exploration by favoring mixed strategies with full support.\nIn Appendix A, we present in detail two of the prototypical examples of (FoReL): i) the multiplicative weights (MW) dynamics induced by the entropic regularizer function hi(x) = \u2211 \u03b1i\u2208Ai xi\u03b1i log xi\u03b1i (which lead to the replicator dynamics of evolutionary game theory); and ii) the projection dynamics induced by the Euclidean\nregularizer hi(x) = 12\u2016xi\u2016 2. For concreteness, we will assume in what follows that the regularizer of every player i \u2208 N satisfies the following minimal requirements: (1) hi is continuous and strictly convex on Xi. (2) hi is smooth on the relative interior of every face of Xi (including Xi itself).\nUnder these basic assumptions, the \u201cregularized leader\u201d Qi(yi) is well-defined in the sense that (3.2) admits a unique solution. More importantly, we have the following no-regret guarantee:\nTheorem 3.1. A player following (FoReL) enjoys an O(1/t) regret bound, no matter what other players do. Specifically, if player i \u2208 N follows (FoReL), then, for every continuous trajectory of play x\u2212i(t) of the opponents of player i, we have\nRegi(t) \u2264 \u2126i t , (3.3)\nwhere \u2126i = maxhi \u2212minhi is a positive constant.\nTo streamline our discussion, we relegate the proof of Theorem 3.1 to Appendix C; we also refer to [20] for a similar regret bound for (FoReL) in the context of online convex optimization. Instead of discussing the proof, we close this section by noting that (3.3) represents a striking improvement over the \u0398(t\u22121/2) worst-case bound for FoReL in discrete time [40]. In view of this, the continuous-time framework we consider here can be seen as particularly amenable to learning because it allows players seek to minimize their regret (and thus converge to coarse correlated equilibria) at the fastest possible rate."}, {"heading": "4. Recurrence in adversarial regularized learning", "text": "In this section, our aim is to take a closer look at the ramifications of fast regret minimization under (FoReL) beyond convergence to the set of coarse correlated equilibria. Indeed, as is well known, this set is fairly large and may contain thoroughly non-rationalizable strategies: for instance, Viossat and Zapechelnyuk [46] recently showed that a coarse correlated equilibrium could assign positive selection probability only to strictly dominated strategies. Moreover, the time-averaging that is inherent in the definition of the players\u2019 regret leaves open the possibility of complex day-to-day behavior e.g. periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37]. Motivated by this, we examine the long-run behavior of the (FoReL) in the popular setting of zero-sum games (with or without interior equilibria) and several extensions thereof.\nA key notion in our analysis is that of (Poincar\u00e9) recurrence. Intuitively, a dynamical system is recurrent if, after a sufficiently long (but finite) time, almost every state returns arbitrarily close to the system\u2019s initial state.4 More formally, given a dynamical system on X that is defined by means of a semiflow \u03a6: X \u00d7 [0,\u221e)\u2192 X , we have:5\nDefinition 4.1. A point x \u2208 X is said to be recurrent under \u03a6 if, for every neighborhood U of x in X , there exists an increasing sequence of times tn \u2191 \u221e such that\n4Here, \u201calmost\u201d means that the set of such states has full Lebesgue measure. 5Recall that a continuous map \u03a6: X\u00d7[0,\u221e)\u2192 X is a semiflow if \u03a6(x, 0) = x and \u03a6(x, t+s) = \u03a6(\u03a6(x, t), s) for all x \u2208 X and all s, t \u2265 0. Heuristically, \u03a6t(x) \u2261 \u03a6(x, t) describes the trajectory of the dynamical system starting at x.\n\u03a6(x, tn) \u2208 U for all n. Moreover, the flow \u03a6 is called (Poincar\u00e9) recurrent if, for every measurable subset A of X , the set of recurrent points in A has full measure.\nAn immediate consequence of Definition 4.1 is that, if a point is recurrent, there exists an increasing sequence of times tn \u2191 \u221e such that \u03a6(x, tn) \u2192 x. On that account, recurrence can be seen as the flip side of convergence: under the latter, (almost) every initial state of the dynamics eventually reaches some well-defined end-state; instead, under the former, the system\u2019s orbits fill the entire state space and return arbitarily close to their starting points infinitely often (so there is no possibility of convergence beyond trivial cases).\n4.1. Zero-sum games with an interior equilibrium. Our first result is that (FoReL) is recurrent (and hence, non-convergent) in zero-sum games with an interior Nash equilibrium:\nTheorem 4.2. Let \u0393 be a 2-player zero-sum game that admits an interior Nash equilibrium. Then, almost every solution trajectory of (FoReL) is recurrent; specifically, for (Lebesgue) almost every initial condition x(0) = Q(y(0)) \u2208 X , there exists an increasing sequence of times tn \u2191 \u221e such that x(tn)\u2192 x(0).\nThe proof of Theorem 4.2 is fairly complicated, so we outline the basic steps below:\n(1) We first show that the dynamics of the score sequence y(t) are incompressible, i.e. the volume of a set of initial conditions remains invariant as the dynamics evolve over time. By Poincar\u00e9\u2019s recurrence theorem (cf. Appendix B), if every solution orbit y(t) of (FoReL) remains in a compact set for all t \u2265 0, incompressibility implies recurrence. (2) To counter the possibility of solutions escaping to infinity, we introduce a transformed system based on the differences between scores (as opposed to the scores themselves). To establish boundedness in these dynamics, we consider the \u201cprimal-dual\u201d coupling\nG(y) = \u2211 i\u2208N [h\u2217i (yi)\u2212 \u3008yi, x\u2217i \u3009], (4.1)\nwhere x\u2217 is an interior Nash equilibrium and h\u2217i (yi) = maxxi\u2208Xi{\u3008yi, xi\u3009 \u2212 hi(xi)} denotes the convex conjugate of hi.6 The key property of this coupling is that it remains invariant under (FoReL); however, its level sets are not bounded so, again, precompactness of solutions is not guaranteed. (3) Nevertheless, under the score transformation described above, the level sets of G are compact. Since the transformed dynamics are invariant under said transformation, Poincar\u00e9\u2019s theorem finally implies recurrence.\nProof of Theorem 4.2. To make the above plan precise, fix some \u201cbenchmark\u201d strategy \u03b1\u0302i \u2208 Ai for every player i \u2208 N and, for all \u03b1i \u2208 Ai\\{\u03b1\u0302i}, consider the corresponding score differences zi\u03b1i = yi\u03b1i \u2212 yi,\u03b1\u0302i . (4.2) Obviously, zi\u03b1\u0302i = yi\u03b1\u0302i \u2212 yi\u03b1\u0302i is identically zero so we can ignore it in the above definition. In so doing, we obtain a linear map \u03a0i : RAi \u2192 RAi\\{\u03b1\u0302i} sending yi 7\u2192 zi; aggregating over all players, we also write \u03a0 for the product map \u03a0 = (\u03a01, . . . ,\u03a0N )\n6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].\nsending y 7\u2192 z. For posterity, note that this map is surjective but not injective,7 so it does not allow us to recover the score vector y from the score difference vector z.\nNow, under (FoReL), the score differences (4.2) evolve as\nz\u0307i\u03b1i = vi\u03b1i(x(t))\u2212 vi\u03b1\u0302i(x(t)). (4.3)\nHowever, since the right-hand side (RHS) of (4.3) depends on x = Q(y) and the mapping y 7\u2192 z is not invertible (so y cannot be expressed as a function of z), the above does not a priori constitute an autonomous dynamical system (as required to apply Poincar\u00e9\u2019s recurrence theorem). Our first step below is to show that (4.3) does in fact constitute a well-defined dynamical system on z.\nTo do so, consider the reduced choice map Q\u0302i : RAi\\{\u03b1\u0302i} \u2192 Xi defined as\nQ\u0302i(zi) = Qi(yi) (4.4)\nfor some yi \u2208 RAi such that \u03a0i(yi) = zi. That such a yi exists is a consequence of \u03a0i being surjective; furthemore, that Q\u0302i(zi) is well-defined is a consequence of the fact that Qi is invariant on the fibers of \u03a0i. Indeed, by construction, we have \u03a0i(yi) = \u03a0i(y \u2032 i) if and only if y\u2032i\u03b1i = yi\u03b1i + c for some c \u2208 R and all \u03b1i \u2208 Ai. Hence, by the definition of Qi, we get\nQi(y \u2032 i) = arg max\nxi\u2208Xi\n{ \u3008yi, xi\u3009+ c \u2211 \u03b1i\u2208Ai xi\u03b1i \u2212 hi(xi) } = arg max\nxi\u2208Xi {\u3008yi, xi\u3009 \u2212 hi(xi)} = Qi(yi), (4.5)\nwhere we used the fact that \u2211 \u03b1i\u2208Ai xi\u03b1i = 1. The above shows thatQi(y \u2032 i) = Qi(yi) if and only if \u03a0i(yi) = \u03a0i(y\u2032i), so Q\u0302i is well-defined. Letting Q\u0302 \u2261 (Q\u03021, . . . , Q\u0302N ) denote the aggregation of the players\u2019 individual choice maps Q\u0302i, it follows immediately that Q(y) = Q\u0302(\u03a0(y)) = Q\u0302(z) by construction. Hence, the dynamics (4.3) may be written as\nz\u0307 = V (z), (4.6)\nwhere Vi\u03b1i(z) = vi\u03b1i(Q\u0302i(z))\u2212 vi\u03b1\u0302i(Q\u0302i(z)). (4.7)\nThese dynamics obviously constitute an autonomous system, so our goal will be to use Liouville\u2019s formula and Poincar\u00e9\u2019s theorem in order to establish recurrence and then conclude that the induced trajectory of play x(t) is recurrent by leveraging the properties of Q\u0302.\nAs a first step towards applying Liouville\u2019s formula, we note that the dynamics (4.6) are incompressible. Indeed, we have\n\u2202Vi\u03b1i \u2202zi\u03b1i = \u2211 \u03b2i\u2208Ai \u2202Vi\u03b1i \u2202xi\u03b2i \u2202xi\u03b2i \u2202zi\u03b1i = 0, (4.8)\nbecause vi does not depend on xi. We thus obtain divz V (z) = 0, i.e. the dynamics (4.6) are incompressible.\nWe now show that every solution orbit z(t) of (4.6) is precompact, that is, supt\u22650\u2016z(t)\u2016 < \u221e. To that end, note that the coupling G(y) = \u2211 i\u2208N [h \u2217 i (yi) \u2212\n7Specifically, \u03a0i(yi) = \u03a0i(y\u2032i) if and only if y \u2032 i\u03b1i = yi\u03b1i + c for some c \u2208 R and all \u03b1i \u2208 Ai.\n\u3008yi, x\u2217i \u3009] defined in (4.1) remains invariant under (FoReL) when \u0393 is a 2-player zero-sum game. Indeed, by Lemma C.1, we have dG\ndt = \u2211 i\u2208N \u3008vi(x), xi \u2212 x\u2217i \u3009 = \u3008v1(x), x1 \u2212 x\u22171\u3009+ \u3008v2(x), x2 \u2212 x\u22172\u3009\n= u1(x1, x2)\u2212 u1(x\u22171, x2) + u2(x1, x2)\u2212 u2(x1, x\u22172) = 0, (4.9)\nwhere we used the fact that Qi = \u2207h\u2217i in the first line (cf. (C.2) above), and the assumption that x\u2217 is an interior Nash equilibrium of a 2-player zero-sum game in the last one. We conclude that G(y(t)) remains constant under (FoReL), as claimed.\nBy Lemma D.2 in Appendix D, the invariance of G(y(t)) under (FoReL) implies that the score differences zi\u03b1i(t) = yi\u03b1i(t)\u2212yi\u03b1\u0302i(t) also remain bounded for all t \u2265 0. Hence, by Liouville\u2019s formula and Poincar\u00e9\u2019s recurrence theorem, the dynamics (4.6) are recurrent, i.e. for (Lebesgue) almost every initial condition z0 and every neighborhood U of z0, there exists some \u03c4U such that z(\u03c4U ) \u2208 U (cf. Definition 4.1). Thus, taking a shrinking net of balls Bn(z0) = {z : \u2016z \u2212 z0\u2016 \u2264 1/n} and iterating the above, it follows that there exists an increasing sequence of times tn \u2191 \u221e such that z(tn) \u2192 z0. Therefore, to prove the corresponding claim for the induced trajectories of play x(t) = Q(y(t)) = Q\u0302(z(t)) of (FoReL), fix an initial condition x0 \u2208 X \u25e6 and take some z0 such that x0 = Q\u0302(z0). By taking tn as above, we have z(tn) \u2192 z0 so, by continuity, x(tn) = Q\u0302(zn) \u2192 Q\u0302(z0) = x0. This shows that any solution orbit x(t) of (FoReL) is recurrent and our proof is complete.\nRemark. We close this section by noting that the invariance of (4.1) under (FoReL) induces a foliation of X , with each individual trajectory of (FoReL) living on a \u201cleaf\u201d of the foliation (a level set of G). Fig. 1 provides a schematic illustration of this foliation/cycling structure.\n4.2. Zero-sum games with no interior equilibria. At first sight, Theorem 4.2 suggests that cycling is ubiquitous in zero-sum games; however, if the game does not admit an interior equilibrium, the behavior of (FoReL) turns out to be qualitatively different. To state our result for such games, it will be convenient to assume that the players\u2019 regularizer functions are strongly convex, i.e. each hi can be bounded from below by a quadratic minorant:\nhi(txi + (1\u2212 t)x\u2032i) \u2264 thi(xi) + (1\u2212 t)hi(x\u2032i)\u2212 12Kit(1\u2212 t)\u2016xi \u2212 x \u2032 i\u2016 2 , (4.10)\nfor all xi, x\u2032i \u2208 Xi and for all t \u2208 [0, 1]. Under this technical assumption, we have:\nTheorem 4.3. Let \u0393 be a 2-player zero-sum game that does not admit an interior Nash equilibrium. Then, for every initial condition of (FoReL), the induced trajectory of play x(t) converges to the boundary of X . Specifically, if x\u2217 is a Nash equilibrium of \u0393 with maximal support, x(t) converges to the relative interior of the face of X spanned by supp(x\u2217).\nTheorem 4.3 is our most comprehensive result for the behavior of (FoReL) in zero-sum games, so several remarks are in order. First, we note that Theorem 4.3 complements Theorem 4.2 in a very natural way: specifically, if \u0393 admits an interior Nash equilibrium, Theorem 4.3 suggests that the solutions of (FoReL) will stay within the relative interior X \u25e6 of X (since an interior equilibrium is supported on\nall actions). Of course, Theorem 4.2 provides a stronger result because it states that, within X \u25e6, (FoReL) is recurrent. Hence, applying both results in tandem, we obtain the following heuristic for the behavior of (FoReL) in zero-sum games:\nIn the long run, (FoReL) wanders in perpetuity in the smallest face of X containing the equilibrium set of \u0393.\nThis leads to two extremes: On the one hand, if \u0393 admits an interior equilibrium, (FoReL) is recurrent and cycles in the level sets of the coupling function (4.1). At the other end of the spectrum, if \u0393 admits only a single, pure equilibrium, then (FoReL) converges to it (since it has to wander in a singleton set). In all other \u201cin-between\u201d cases, (FoReL) exhibits a hybrid behavior, converging to the face of X that is spanned by the maximal support equilibrium of \u0393, and then cycling in that face in perpetuity.\nThe reason for this behavior is that the coupling (4.1) is no longer a constant of motion of (FoReL) if the game does not admit an interior equilibrium. As we show in Appendix C, the coupling (4.1) is strictly decreasing when the support of x(t) is strictly greater than that of a Nash equilibrium x\u2217 with maximal support. When the two match, the rate of change of (4.1) drops to zero, and we fall back to a \u201cconstrained\u201d version of Theorem 4.2. We make this argument precise in Appendix C (where we present the proof of Theorem 4.3).\n4.3. Zero-sum polymatrix games & positive affine payoff transformations. We close this section by showing that the recurrence properties of (FoReL) are not unique to \u201cvanilla\u201d zero-sum games, but also occur when there is a network of competitors \u2013 i.e. in N -player zero-sum polymatrix games. In fact, the recurrence results carry over to any N -player game which is isomorphic to a constant-sum polymatrix game with an interior equilibrium up to a positive-affine payoff transformation (possibly different transformation for each agent). For example, this class of games contains all strictly competitive games [1]. Such transformations do not affect the equilibrium structure of the game, but can affect the geometry of the trajectories; nevertheless, the recurrent behavior persists as shown by the following result:\nTheorem 4.4. Let \u0393 = (\u0393e)e\u2208E be a constant-sum polymatrix game (or a positive affine payoff transformation thereof ). If \u0393 admits an interior Nash equilibrium, almost every solution trajectory of (FoReL) is recurrent; specifically, for (Lebesgue) almost every initial condition x(0) = Q(y(0)) \u2208 X , there exists an increasing sequence of times tn \u2191 \u221e such that x(tn)\u2192 x(0).\nWe leave the case of zero-sum polymatrix games with no interior equilibria to future work."}, {"heading": "5. Conclusions", "text": "Our results show that the behavior of regularized learning in adversarial environments is considerably more intricate than the strong no-regret properties of FoReL might at first suggest. Even though the empirical frequency of play under FoReL converges to the set of coarse correlated equilibria (possibly at an increased rate, depending on the game\u2019s structure), the actual trajectory of play under FoReL is recurrent and exhibits cycles in zero-sum games. We find this property particularly interesting as it suggests that \u201cblack box\u201d guarantees are not the be-all/end-all\nof learning in games: the theory of dynamical systems is rife with complex phenomena and notions that arise naturally when examining the behavior of learning algorithms in finer detail."}, {"heading": "Appendix A. Examples of FoReL dynamics", "text": "Example A.1 (Multiplicative weights and the replicator dynamics). Perhaps the most widely known example of a regularized choice map is the so-called logit choice map\n\u039bi(y) = (exp(yi\u03b1i))\u03b1i\u2208Ai\u2211 \u03b2i\u2208Ai exp(yi\u03b2i) . (A.1)\nThis choice model was first studied in the context of discrete choice theory by McFadden [22] and it leads to the multiplicative weights (MW) dynamics:8\ny\u0307i = vi(x), xi = \u039bi(yi). (MW)\nAs is well known, the logit map above is obtained by the model (3.2) by considering the entropic regularizer hi(x) = \u2211 \u03b1i\u2208Ai xi\u03b1i log xi\u03b1i , (A.2) i.e. the (negative) Gibbs\u2013Shannon entropy function. A simple differentiation of (MW) then shows that the players\u2019 mixed strategies evolve according to the dynamics\nx\u0307i\u03b1i = xi\u03b1i vi\u03b1i(x)\u2212 \u2211 \u03b2i\u2208Ai xi\u03b2ivi\u03b2i(x) , (RD) This equation describes the replicator dynamics of [45], the most widely studied model for evolution under natural selection in population biology and evolutionary game theory. The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others. Example A.2 (Euclidean regularization and the projection dynamics). Another widely used example of regularization is given by the quadratic penalty\nhi(xi) = 1\n2 \u2211 \u03b1i\u2208Ai x2i\u03b1i . (A.3)\nThe induced choice map (3.2) is the (Euclidean) projection map \u03a0i(yi) = arg maxxi\u2208Xi { \u3008yi, xi\u3009 \u2212 12\u2016xi\u2016 2 2 } = arg minxi\u2208Xi\u2016yi \u2212 xi\u2016 2 2, (A.4)\nleading to the projected reinforcement learning process y\u0307i = vi(x),\nxi = \u03a0i(yi). (PL)\n8The terminology \u201cmultiplicative weights\u201d refers to the fact that (MW) is the continuous version of the discrete-time multiplicative weights update rule:\nxi\u03b1i (t+ 1) = xi\u03b1i (t)e \u03b7ivi\u03b1i (x(t))\u2211 \u03b2i\u2208Ai xi\u03b2i (t)e \u03b7ivi\u03b2i (x(t)) , (MWU)\nwhere \u03b7i > 0 is the scheme\u2019s \u201clearning rate\u201d. For more details about (MWU), we refer the reader to [2].\nThe players\u2019 mixed strategies are then known to follow the projection dynamics\nx\u0307i\u03b1i =\n{ vi\u03b1i(x)\u2212 |supp(xi)| \u22121\u2211 \u03b2i\u2208supp(xi) vi\u03b2i(x) if \u03b1i \u2208 supp(xi),\n0 if \u03b1i /\u2208 supp(xi), (PD)\nover all intervals for which the support of x(t) remains constant [24]. The dynamics (PD) were introduced in game theory by [12] as a geometric model of the evolution of play in population games; for a closely related approach, see also [21, 25] and references therein."}, {"heading": "Appendix B. Liouville\u2019s formula and Poincar\u00e9 recurrence", "text": "Below we present for completeness some basic results from the theory of dynamical systems.\nLiouville\u2019s Formula. Liouville\u2019s formula can be applied to any system of autonomous differential equations with a continuously differentiable vector field \u03be on an open domain of S \u2282k. The divergence of \u03be at x \u2208 S is defined as the trace of the corresponding Jacobian at x, i.e., div[\u03be(x)] = \u2211k i=1 \u2202\u03bei \u2202xi\n(x). Since divergence is a continuous function we can compute its integral over measurable sets A \u2282 S. Given any such set A, let A(t) = {\u03a6(x0, t) : x0 \u2208 A} be the image of A under map \u03a6 at time t. A(t) is measurable and is volume is vol[A(t)] = \u222b A(t)\ndx. Liouville\u2019s formula states that the time derivative of the volume A(t) exists and is equal to the integral of the divergence over A(t):\nd dt [A(t)] = \u222b A(t) div[\u03be(x)]dx.\nA vector field is called divergence free if its divergence is zero everywhere. Liouville\u2019s formula trivially implies that volume is preserved in such flows.\nPoincar\u00e9\u2019s recurrence theorem. The notion of recurrence that we will be using in this paper goes back to Poincar\u00e9 and specifically to his study of the threebody problem. In 1890, in his celebrated work [30], he proved that whenever a dynamical system preserves volume almost all trajectories return arbitrarily close to their initial position, and they do so an infinite number of times. More precisely, Poincar\u00e9 established the following:\nPoincar\u00e9 Recurrence: [4, 30] If a flow preserves volume and has only bounded orbits then for each open set there exist orbits that intersect the set infinitely often."}, {"heading": "Appendix C. Technical proofs", "text": "The first result that we prove in this appendix is a key technical lemma concerning the evolution of the coupling function (4.1):\nLemma C.1. Let pi \u2208 Xi and let Gi(yi) = h\u2217i (yi)\u2212\u3008yi, pi\u3009 denote the coupling (4.1) for player i \u2208 N . If player i \u2208 N follows (FoReL), we have\nd dt Gi(yi(t)) = \u3008vi(x(t)), xi(t)\u2212 pi\u3009, (C.1)\nfor every trajectory of play x\u2212i(t) of all players other than i.\nProof. We begin by recalling the \u201cmaximizing argument\u201d identity\nQi(yi) = \u2207h\u2217i (yi) (C.2)\nwhich expresses the choice map Qi as a function of the convex conjugate of hi [40, p. 149]. With this at hand, a simple differentiation gives\nd dt Gi(yi(t)) = d dt h\u2217i (yi(t))\u2212 \u3008y\u0307i(t), pi\u3009\n= \u3008y\u0307i(t),\u2207h\u2217i (yi(t))\u2212 pi\u3009 = \u3008vi(x(t)), xi(t)\u2212 pi\u3009, (C.3)\nwhere the last step follows from the fact that xi(t) = Qi(yi(t)) = \u2207h\u2217i (yi(t)).\nArmed with this lemma, we proceed to prove the no-regret guarantees of (FoReL):\nProof of Theorem 3.1. Fix some base point pi \u2208 Xi and let Li(t) = Gi(yi(t)) = hi(yi(t))\u2212 \u3008yi(t), pi\u3009. Then, by Lemma C.1, we have\nL\u2032i(t) = \u3008vi(x(t)), xi(t)\u2212 pi\u3009 (C.4)\nand hence, after integrating and rearranging, we get\u222b t 0 [ui(pi;x\u2212i(s))\u2212ui(x(s))]ds = \u222b t 0 \u3008vi(x(s)), pi\u2212xi(s)\u3009ds = Li(0)\u2212Li(t), (C.5)\nwhere we used the fact that ui(pi;x\u2212i) = \u3008vi(x), pi\u3009 \u2013 cf. Eq. (2.2) in Section 2. However, expanding the RHS of (C.5), we get\nLi(0)\u2212 Li(t) = h\u2217i (yi(0))\u2212 \u3008yi(0), pi\u3009 \u2212 h\u2217i (yi(t)) + \u3008yi(t), pi\u3009 \u2264 h\u2217i (yi(0))\u2212 \u3008yi(0), pi\u3009+ hi(pi) = hi(pi)\u2212 hi(Qi(yi(0))) \u2264 maxhi \u2212minhi \u2261 \u2126i, (C.6)\nwhere we used the defining property of convex conjugation in the second and third lines above \u2013 i.e. that h\u2217i (yi) \u2265 \u3008yi, xi\u3009 \u2212 hi(xi) for all xi \u2208 Xi, with equality if and only if xi = Qi(yi). Thus, maximizing (C.5) over pi \u2208 Xi, we finally obtain\nRegi(t) = max pi\u2208Xi\n1\nt \u222b t 0 [ui(pi;x\u2212i(s))\u2212 ui(x(s))] ds \u2264 \u2126i t , (C.7)\nas claimed.\nWe now turn to two-player zero-sum games that do not admit interior equilibria. To describe such equilibria in more detail, we consider below the notion of essential and non-essential strategies:\nDefinition C.2. A strategy \u03b1i of agent i \u2208 {1, 2} in a zero sum game is called essential if there exists a Nash equilibrium in which player i plays \u03b1i with positive probability. A strategy that is not essential is called non-essential.\nAs it turns out, the Nash equilibria of a zero-sum game admit a very useful characterization in terms of essential and non-essential strategies:\nLemma C.3. Let \u0393 be a 2-player zero-sum game that does not admit an interior Nash equilibrium. Then, there exists a mixed Nash equilibrium (x1, x2) such that a) each agent plays each of their essential strategies with positive probability; and\nb) for each agent deviating to a non-essential strategy results to a strictly worse performance than the value of the game.\nThe key step in proving this characterization is Farkas\u2019 lemma; the version we employ here is due to Gale, Kuhn and Tucker [13]):\nLemma C.4 (Farkas\u2019 lemma). Let P \u2208 Rm\u00d7n and b \u2208 Rm. Then exactly one of the following two statements is true:\n\u2022 There exists a x \u2208 Rm such that P>x \u2265 0 and b>x < 0. \u2022 There exists a y \u2208 Rn such that P \u00b7 y = b and y \u2265 0.\nWith this lemma at hand, we have:\nProof of Lemma C.3. Assume without loss of generality that the value of the zerosum game is zero. and that the first agent is a maximizing agent. Let A be the payoff matrix of the first agent and hence AT = A the payoff matrix of the second/minimizing agent. We will show first that for any non-essential strategy \u03b1i of each agent there exists a Nash equilibrium strategy of his opponent such that the expected performance of \u03b1i is strictly worse than the value of the game (i.e. zero).\nIt suffices to argue this for the first agent. Let \u03b1i be one of his non-essential strategies then by definition there does not exist any Nash equilibrium strategy of that agent that chooses \u03b1i with positive probability. This is equivalent to the negation of the following statement:\nThere exists a x \u2208 Rm such that P>x \u2265 0 and b>x < 0 where\nP> =\n( A>\nIm\u00d7m\n) =  a11 a21 . . . am1 ... ... . . . ... a1n a2n . . . amn 1 0 . . . 0 0 1 . . . 0 ... ... . . . ...\n0 0 . . . 1\n , (C.8)\nand b = \u2212ei = (0, . . . , 0,\u22121, 0, . . . , 0)T , the standard basis vector of dimension m that \u201cchooses\" the i-th strategy. By Farkas\u2019 lemma, there exists a y \u2208 Rm+n such that Py = b and y \u2265 0. It is convenient to express y = (z;w) where z \u2208 Rn and w \u2208 Rm. Hence, for all j 6= i \u2208 {1, 2, . . . ,m} : (Py)j = (Az)j + wj = 0 and thus (Az)j \u2264 0. Finally, for j = i : (Py)i = (Az)i + wi = \u22121 and thus (Az)i < 0. Hence z is a Nash equilibrium strategy for the second player such that when the first agent chooses the non-essential strategy \u03b1i he receives payoff which is strictly worse than his value (zero).\nTo complete the proof, for each essential strategy of the first agent there exists one equilibrium strategy of his that chooses it with positive probability (by definition). Similarly, for each non-essential strategy of the second agent there exists one equilibrium strategy of the first agent such that makes the expected payoff of that non-essential strategy strictly worse than the value of the game. The barycenter of all the above equilibrium strategies is still an equilibrium strategy (by convexity) and has all the desired properties.\nWith all this at hand, we are finally in a position to prove Theorem 4.3: Proof of Theorem 4.3. We first show that the coupling G(y) = \u2211 i\u2208N [h \u2217 i (yi) \u2212 \u3008yi, x\u2217i \u3009] defined in (4.1) given any fully mixed initial condition strictly increases under (FoReL) when \u0393 is a 2-player zero-sum game that does not have an equilibrium with full support.\nIndeed, by (C.3) there exists a mixed Nash equilibrium (x\u22171, x\u22172) such that i) both players employ each of their essential strategies with positive probability over time; and ii) every player deviating to a non-essential strategy obtains a payoff lower than the value of the game. As a result, any player playing an interior (fully mixed) strategy against such an equilibrium strategy must receive less utility than their value. In more detail, we have\ndG dt = \u2211 i\u2208N \u3008vi(x), xi \u2212 x\u2217i \u3009 = \u3008v1(x), x1 \u2212 x\u22171\u3009+ \u3008v2(x), x2 \u2212 x\u22172\u3009\n= u1(x1, x2)\u2212 u1(x\u22171, x2) + u2(x1, x2)\u2212 u2(x1, x\u22172) = \u2212u1(x\u22171, x2)\u2212 u2(x1, x\u22172) < \u2212u1(x\u22171, x\u22172)\u2212 u2(x\u22171, x\u22172) = 0, (C.9)\nwhere we used the fact that Qi = \u2207h\u2217i in the first line (cf. Appendix D), and the assumption that x\u2217 is a Nash equilibrium of a 2-player zero-sum game such that any agent playing an interior (fully mixed) strategy against such an equilibrium strategy must receive less utility than their value (and hence the agent himself receives more utility than the value of the game). We thus conclude that G(y(t)) strictly increases under (FoReL), as claimed.\nLet x\u2217 = (x\u22171, x\u22172) be the Nash equilibrium identified in (C.9) and let L(t) = G(y(t)) = \u2211 i\u2208N [h \u2217 i (yi(t)) \u2212 \u3008yi(t), x\u2217i \u3009] denote the primal-dual coupling (4.1) between y(t) and x\u2217. From (C.9), we have that starting from any fully mixed strategy profile x(0) \u2208 \u220f i int(Xi) and for all t \u2265 0, L\u2032(t) = \u3008y\u0307(t),\u2207G(y(t))\u3009 < 0. However,\nG is bounded from below by \u2212 \u2211 i maxxi\u2208Xi hi(xi), and since G(y(t)) is strictly decreasing, it must exhibit a finite limit. We begin by noting that x(t) = Q(y(t)) is Lipschitz continuous in t. Indeed, v is Lipschitz continuous on X by linearity; furthermore, since the regularizer functions hi are assumed Ki-strongly convex, it follows that Qi is (1/Ki)-continuous by standard convex analysis arguments [32, Theorem 12.60]. In turn, this implies that the field of motion V (y) \u2261 v(Q(y)) of (FoReL) is Lipschitz continuous, so the dynamics are well-posed and y(t) is differentiable. Since y\u0307 = v and, in addition, v is bounded on X , we conclude that y\u0307 is bounded so, in particular, y(t) is Lipschitz continuous on [0,\u221e). We thus conclude that x(t) = Q(y(t)) is Lipschitz continuous as the composition of Lipschitz continuous functions.\nWe now further claim that L\u2032(t) is also Lipschitz continuous in t. Indeed, by (C.1), we have L\u2032(t) = \u2211 i\u2208N \u3008vi(x(t)), xi(t) \u2212 x\u2217i \u3009; since vi is Lipschitz continuous in x and x(t) is Lipschitz continuous in t, our claim follows trivially. Hence, by Lemma D.3, we conclude that limt\u2192\u221e L\u2032(t) = lim\u2192\u221e \u2211 i\u2208N \u3008vi(x(t), xi(t)\u2212x\u2217i \u3009 = 0.\nBy (C.9), we know that L\u2032(t) < 0 as long as x(t) is interior. Hence, any \u03c9-limit x\u0302 of x(t) cannot be interior (given that the embedded game does not have any interior Nash equilibria). Moreover, we can repeat this argument for any subspace such that the restriction of the game on that subspace (when ignoring the strategies that are played with probability zero) does not have a fully mixed NE. We thus\nconclude that the support of x\u0302 must be a subset of the support of x\u2217. Since \u0393 does not admit an interior equilibrium, x\u2217 does not have full support, so every \u03c9-limit of x(t) lies on the boundary of X , as claimed.\nWe close this appendix with the proof of our result on constant-sum polymatrix games (and positive affine transformations thereof):\nProof of Theorem 4.4. Our proof follows closely that of Theorem 4.2; to streamline our presentation, we only highlight here the points that differ due to working with (an positive-affine transformations of) a network of constant-sum games (as opposed to a single 2-player zero-sum game).\nThe first such point is the incompressibility of the \u201creduced\u201d dynamics (4.6). By definition, we have ui(x) = \u2211 j\u2208Ni uij(xi, xj), so we also have\nvi\u03b1i(x) = \u2211 j\u2208Ni uij(\u03b1i, xj). (C.10)\nSince uij(\u03b1i, xj) does not depend on xi, we readily obtain \u2202\u03b1ivi\u03b1i(x) = 0 and incompressibility follows as before.\nLet the network game in question be isomorphic to a network of constantsum games after the following positive-affine transformation of utilities, ui(x) \u2190 aiui(x) + bi where ai > 0. The second point of interest is the use of the coupling G(y) = \u2211 i\u2208N ai[h \u2217 i (yi) \u2212 \u3008yi, x\u2217i \u3009] as a constant of motion for (FoReL). Indeed, adapting the derivation of (C.1), we now get\ndG\ndt = \u3008y\u0307,\u2207G(y)\u3009 = \u2211 i\u2208N \u3008vi(x), ai(\u2207h\u2217i (yi)\u2212 x\u2217i )\u3009 = \u2211 i\u2208N \u3008aivi(x), xi \u2212 x\u2217i \u3009\n= \u2211 i\u2208N \u2211 j\u2208Ni \u3008aivij(x), xi \u2212 x\u2217i \u3009\n= \u2211 {i,j}\u2208E [aiuij(xi, xj) + bi \u2212 aiuij(x\u2217i , xj)\u2212 bi + ajuji(xi, xj) + bj \u2212 ajuji(xi, x\u2217j )\u2212 bj ]\n= 0, (C.11)\nwhere the third line follows by regrouping the summands in the second line by edge, and the last line follows as in the case of (C.1). This implies that G(y(t)) remains constant along any solution of (FoReL), so the rest of the proof follows as in the case of Theorem 4.2."}, {"heading": "Appendix D. Auxiliary results", "text": "In this appendix, we provide two auxiliary results that are used in the proof of Theorem 4.2. The first one shows that if the score difference between two strategies grows large, the strategy with the lower score becomes extinct:\nLemma D.1. Let A be a finite set and let h be a regularizer on X \u2261 \u2206(A). If the sequence yn \u2208 RA is such that y\u03b2,n \u2212 y\u03b1,n \u2192 \u221e for some \u03b1, \u03b2 \u2208 A, then limn\u2192\u221eQ\u03b1(yn) = 0.\nProof. Set xn = Q(yn) and, by descending to a subsequence if necessary, assume there exists some \u03b5 > 0 such that x\u03b1,n \u2265 \u03b5 > 0 for all n. Then, by the defining relation Q(y) = arg max{\u3008y, x\u3009 \u2212 h(x)} of Q, we have:\n\u3008yn, xn\u3009 \u2212 h(xn) \u2265 \u3008yn, x\u2032\u3009 \u2212 h(x\u2032) (D.1)\nfor all x\u2032 \u2208 \u2206. Therefore, taking x\u2032n = xn + \u03b5(e\u03b2 \u2212 e\u03b1), we readily obtain\n\u03b5(y\u03b1,n \u2212 y\u03b2,n) \u2265 h(xn)\u2212 h(x\u2032n) \u2265 minh\u2212maxh (D.2)\nwhich contradicts our original assumption that y\u03b1,n \u2212 y\u03b2,n \u2192 \u2212\u221e. With \u2206 compact, the above shows that x\u2217\u03b1 = 0 for any limit point x\u2217 of xn, i.e. Q\u03b1(yn)\u2192 0.\nA key step of the proof of Theorem 4.2 consists of showing that the level sets of the Fenchel coupling G(p, y) become bounded under the coordinate reduction transformation y 7\u2192 \u03a0(y) = z, so every solution orbit z(t) of (4.6) also remains bounded. We encode this in the following lemma:\nLemma D.2. Let A be a finite set, let h be a regularizer on X \u2261 \u2206(A), and fix some interior p \u2208 X . If the sequence yn \u2208 RA is such that supn|h\u2217(yn) \u2212 \u3008yn, p\u3009| < \u221e, the differences y\u03b2,n \u2212 y\u03b1,n also remain bounded for all \u03b1, \u03b2 \u2208 A.\nProof. We argue by contradiction. Indeed, assume that the sequence Gn \u2261 h\u2217(yn)\u2212 \u3008yn, p\u3009 is bounded but lim supn\u2192\u221e|y\u03b1,n \u2212 y\u03b2,n| = \u221e for some \u03b1, \u03b2 \u2208 A. Letting y+n = max\u03b1 y\u03b1,n and y\u2212n = min\u03b1\u2208A y\u03b1,n, this implies that lim supn\u2192\u221e(y+n \u2212 y\u2212n ) = \u221e. Hence, by descending to a subsequence if necessary, there exist \u03b1+, \u03b1\u2212 \u2208 A such that a) y\u00b1n = y\u03b1\u00b1,n for all n; and b) y\u03b1+,n \u2212 y\u03b1\u2212,n \u2192\u221e as n\u2192\u221e.\nBy construction, we have y\u03b1\u2212,n = y\u2212n \u2264 y\u03b1,n \u2264 y+n = y\u03b1+,n for all \u03b1 \u2208 A. Thus, by descending to a further subsequence if necessary, we may assume that the index set A can be partitioned into two nonempty sets A+ and A\u2212 such that\n(1) y+n \u2212 y\u03b1,n is bounded for all \u03b1 \u2208 A+. (2) y+n \u2212 y\u03b1,n \u2192\u221e for all \u03b1 \u2208 A\u2212.\nIn more detail, consider the quantity\n\u03b4\u03b1 = lim inf n\u2192\u221e\n(y+n \u2212 y\u03b1,n), (D.3)\nand construct the required partition {A+,A\u2212} according to the following procedure:\n0: Set A+ \u2190 {\u03b1+}, A\u2212 = A \\ A+ 1: while \u03b4\u03b1 <\u221e for some \u03b1 \u2208 A\u2212 do 2: pick \u03b1\u2217 such that \u03b4\u03b1\u2217 <\u221e; 3: set A+ \u2190 A+ \u222a {\u03b1\u2217}, A\u2212 \u2190 A\u2212\\{\u03b1\u2217}; 4: descend to a subsequence of yn that realizes \u03b4\u03b1\u2217 ; 5: redefine \u03b4\u03b1 for all \u03b1 \u2208 A based on chosen subsequence; 6: end while 7: return A+,A\u2212\nThus, if we let xn = Q(yn) we readily obtain: \u3008yn, p\u2212 xn\u3009 = \u2211 \u03b1\u2208A y\u03b1,n(p\u03b1 \u2212 x\u03b1,n) = \u2211 \u03b1\u2208A (y\u03b1,n \u2212 y+n )(p\u03b1 \u2212 x\u03b1,n)\n= \u2211 \u03b1\u2208A+ (y\u03b1,n \u2212 y+n )(p\u03b1 \u2212 x\u03b1,n) + \u2211 \u03b1\u2208A\u2212 (y\u03b1,n \u2212 y+n )(p\u03b1 \u2212 x\u03b1,n), (D.4)\nwhere we used the fact that \u2211 \u03b1\u2208A p\u03b1 = \u2211 \u03b1\u2208A x\u03b1,n = 1 in the first line. The first sum above is bounded by assumption. As for the second one, the fact that y\u03b1+,n\u2212y\u03b1,n = y+n \u2212y\u03b1,n \u2192\u221e implies that x\u03b1,n \u2192 0 for all \u03b1 \u2208 A\u2212 (by Lemma D.1 above). We thus get lim infn(p\u03b1 \u2212 x\u03b1,n) > 0 (recall that p \u2208 \u2206\u25e6), and hence,\u2211 \u03b1\u2208A\u2212(y\u03b1,n \u2212 y+n )(p\u03b1 \u2212 x\u03b1,n)\u2192 \u2212\u221e. From the above, we conclude that \u3008yn, p \u2212 xn\u3009 \u2192 \u2212\u221e as n \u2192 \u221e. However, by construction, we also have\nGn = h \u2217(yn)\u2212\u3008yn, x\u2217\u3009 = \u3008yn, xn\u3009\u2212h(xn)\u2212\u3008yn, x\u2217\u3009 = \u3008yn, p\u2212xn\u3009\u2212h(xn). (D.5)\nSince h is finite on x, it follows that Gn \u2192 \u2212\u221e, contradicting our assumption that Gn is bounded. Retracing our steps, this implies that supn|y\u03b1,n \u2212 y\u03b2,n| < \u221e, as claimed.\nThe final result we state here is a technical result regarding the asymptotic behavior of the derivative of functions with a finite limit at infinity:\nLemma D.3. Suppose that L : [0,\u221e)\u2192 R is differentiable with Lipschitz continuous derivative. If limt\u2192\u221e L(t) exists and is finite, we have limt\u2192\u221e L\u2032(t) = 0.\nProof. Assume ad absurdum that limt\u2192\u221e L\u2032(t) 6= 0. Then, without loss of generality, we may assume there exists some \u03b5 > 0 and an increasing sequence tn \u2191 \u221e such that L\u2032(tn) \u2265 \u03b5 for all n \u2208 N. Thus, if M denotes the Lipschitz constant of L\u2032 and t \u2208 [tn, tn + \u03b5/(2M)], we readily obtain\n|L\u2032(t)\u2212 L\u2032(tn)| \u2264M |t\u2212 tn| \u2264M \u00b7 \u03b5 2M = \u03b5 2 (D.6)\nby the Lipschitz continuity of L\u2032. Since L\u2032(tn) \u2265 \u03b5 by assumption, we conclude that L\u2032(t) \u2265 \u03b5/2 for all t \u2208 [tn, tn + \u03b5/(2M)]. Hence, by integrating, we get L(tn + \u03b5/(2M)) \u2265 L(tn) + \u03b5/(2M) \u00b7 (\u03b5/2) = L(tn) + \u03b52/(4M) for all n \u2208 N. Taking n \u2192 \u221e and recalling that L\u221e \u2261 limt\u2192\u221e L(t) exists and is finite, we get L\u221e = L\u221e + \u03b5 2/(4M) > L\u221e, a contradiction."}], "references": [{"title": "A note on strictly competitive games", "author": ["I. Adler", "C. Daskalakis", "C.H. Papadimitriou"], "venue": "in WINE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The multiplicative weights update method: a metaalgorithm and applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Singular Riemannian barrier methods and gradient-projection dynamical systems for constrained optimization, Optimization", "author": ["H. Attouch", "J. Bolte", "P. Redont", "M. Teboulle"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Poincare recurrence: old and new, in XIVth", "author": ["L. Barreira"], "venue": "International Congress on Mathematical Physics. World Scientific.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Lectures on modern convex optimization: analysis, algorithms, and engineering", "author": ["A. Ben-Tal", "A. Nemirovski"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Zero-sum polymatrix games: A generalization of minmax", "author": ["Y. Cai", "O. Candogan", "C. Daskalakis", "C. Papadimitriou"], "venue": "Mathematics of Operations Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "On minmax theorems for multiplayer games", "author": ["Y. Cai", "C. Daskalakis"], "venue": "in ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Lugoisi, Prediction, Learning, and Games", "author": ["G.N. Cesa-Bianchi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Near-optimal no-regret algorithms for zero-sum games, in Proceedings of the Twenty-second", "author": ["C. Daskalakis", "A. Deckelbaum", "A. Kim"], "venue": "Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "On a network generalization of the minmax theorem, in ICALP 2009", "author": ["C. Daskalakis", "C.H. Papadimitriou"], "venue": "Proceedings of the 2009 International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning in games: Robustness of fast convergence, in Advances in Neural Information", "author": ["D.J. Foster", "T. Lykouris", "K. Sridharan", "E. Tardos"], "venue": "Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Linear Programming and the Theory of Games - Chapter XII) in Koopmans, Activity Analysis of Production and Allocation", "author": ["D. Gale", "H. Kuhn", "A.W. Tucker"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1951}, {"title": "Generative adversarial nets, in Advances in neural information processing", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Introduction to online convex optimization, Foundations and Trends\u00ae", "author": ["E. Hazan"], "venue": "in Optimization,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Evolutionary Games and Population Dynamics", "author": ["J. Hofbauer", "K. Sigmund"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Kiwiel, Free-steering relaxation methods for problems with strictly convex costs and linear constraints", "author": ["C. K"], "venue": "Mathematics of Operations Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "A continuous-time approach to online optimization", "author": ["J. Kwon", "P. Mertikopoulos"], "venue": "Journal of Dynamics and Games,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "The projection dynamic and the geometry of population", "author": ["R. Lahkar", "W.H. Sandholm"], "venue": "games, Games and Economic Behavior,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Conditional logit analysis of qualitative choice behavior, in Frontiers in Econometrics, P. Zarembka, ed", "author": ["D.L. McFadden"], "venue": "CYCLES IN ADVERSARIAL REGULARIZED", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1974}, {"title": "The emergence of rational behavior in the presence of stochastic perturbations", "author": ["P. Mertikopoulos", "A.L. Moustakas"], "venue": "The Annals of Applied Probability,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Learning in games via reinforcement and regularization", "author": ["P. Mertikopoulos", "W.H. Sandholm"], "venue": "Mathematics of Operations Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Projected dynamical systems in the formulation, stability analysis, and computation of fixed demand traffic network equilibria", "author": ["A. Nagurney", "D. Zhang"], "venue": "Transportation Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Multiplicative Weights Update with Constant Step-Size in Congestion Games: Convergence, Limit Cycles and Chaos, ArXiv e-prints, (2017)", "author": ["G. Palaiopanos", "I. Panageas", "G. Piliouras"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "From nash equilibria to chain recurrent sets: Solution concepts and topology, in ITCS, 2016", "author": ["C. Papadimitriou", "G. Piliouras"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Persistent patterns: Multi-agent learning beyond equilibrium and utility, in AAMAS", "author": ["G. Piliouras", "C. Nieto-Granda", "H.I. Christensen", "J.S. Shamma"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Optimization despite chaos: Convex relaxations to complex limit sets via poincar\u00e9 recurrence", "author": ["G. Piliouras", "J.S. Shamma"], "venue": "Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Optimization, learning, and games with predictable sequences, in Advances in Neural Information", "author": ["S. Rakhlin", "K. Sridharan"], "venue": "Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Variational Analysis, vol. 317 of A Series of Comprehensive Studies", "author": ["R.T. Rockafellar", "R.J.B. Wets"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Intrinsic robustness of the price of anarchy", "author": ["T. Roughgarden"], "venue": "in Proc. of STOC,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Optimal properties of stimulus-response learning models", "author": ["A. Rustichini"], "venue": "Games and Economic Behavior,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Population Games and Evolutionary Dynamics", "author": ["W.H. Sandholm"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Chaos in learning a simple two-person game", "author": ["Y. Sato", "E. Akiyama", "J.D. Farmer"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "Deep learning games, in Advances in Neural Information", "author": ["D. Schuurmans", "M.A. Zinkevich"], "venue": "Processing Systems,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Convex repeated games and Fenchel duality, in Advances in Neural Information", "author": ["S. Shalev-Shwartz", "Y. Singer"], "venue": "Processing Systems", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Exponential weight algorithm in continuous time", "author": ["S. Sorin"], "venue": "Mathematical Programming,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Fast convergence of regularized learning in games", "author": ["V. Syrgkanis", "A. Agarwal", "H. Luo", "R.E. Schapire"], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Evolutionarily stable strategies with two types of player", "author": ["P.D. Taylor"], "venue": "Journal of Applied Probability,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1979}, {"title": "Evolutionary stable strategies and game dynamics", "author": ["P.D. Taylor", "L.B. Jonker"], "venue": "Mathematical Biosciences,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1978}, {"title": "Evolutionary Game Theory, MIT Press; Cambridge, MA: Cambridge University Press", "author": ["J.W. Weibull"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "Through the introduction of a new component in the objective, regularization techniques overcome ill-conditioning and overfitting, and they yield algorithms that achieve sparsity and parsimony without sacrificing efficiency [2, 5, 8].", "startOffset": 224, "endOffset": 233}, {"referenceID": 4, "context": "Through the introduction of a new component in the objective, regularization techniques overcome ill-conditioning and overfitting, and they yield algorithms that achieve sparsity and parsimony without sacrificing efficiency [2, 5, 8].", "startOffset": 224, "endOffset": 233}, {"referenceID": 7, "context": "Through the introduction of a new component in the objective, regularization techniques overcome ill-conditioning and overfitting, and they yield algorithms that achieve sparsity and parsimony without sacrificing efficiency [2, 5, 8].", "startOffset": 224, "endOffset": 233}, {"referenceID": 34, "context": "In the context of online optimization, these features are exemplified in the family of learning algorithms known as \u201cFollow the Regularized Leader\u201d (FoReL) [41].", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": ") [2, 8, 15].", "startOffset": 2, "endOffset": 12}, {"referenceID": 7, "context": ") [2, 8, 15].", "startOffset": 2, "endOffset": 12}, {"referenceID": 13, "context": ") [2, 8, 15].", "startOffset": 2, "endOffset": 12}, {"referenceID": 28, "context": "The second involves proving some useful property of the game\u2019s CCE: For instance, leveraging (\u03bb, \u03bc)-robustness [33] implies that the social welfare at a CCE lies within a small constant of the optimum social welfare; as another example, the product of the marginal distributions of CCE in zero-sum games is Nash.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "[9] and Rakhlin and Sridharan [31] developed classes of dynamics that enjoy a O(log t/t) regret minimization rate in two-player zerosum games.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[9] and Rakhlin and Sridharan [31] developed classes of dynamics that enjoy a O(log t/t) regret minimization rate in two-player zerosum games.", "startOffset": 30, "endOffset": 34}, {"referenceID": 36, "context": "[43] further analyzed a recency biased variant of FoReL in more general multi-player games and showed that it is possible to achieve an O(t\u22123/4) regret minimization rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The social welfare converges at a rate of O(t\u22121), a result which was extended to standard versions of FoReL dynamics in [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 22, "context": "Indeed, convergent, recurrent, and even chaotic [26] systems may exhibit equally strong regret minimization properties in general games, so the question remains: What does the long-run behavior of FoReL look like, really? This question becomes particularly interesting and important under perfect competition (such as zero-sum games and variants thereof).", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "Finally, zero-sum games have also been used quite recently as a model for deep learning optimization techniques in image generation and discrimination [14, 39].", "startOffset": 151, "endOffset": 159}, {"referenceID": 32, "context": "Finally, zero-sum games have also been used quite recently as a model for deep learning optimization techniques in image generation and discrimination [14, 39].", "startOffset": 151, "endOffset": 159}, {"referenceID": 0, "context": "Importantly, the observed cycling behavior is robust to the agents\u2019 choice of regularization mechanism (each agent could be using a different regularizer), and it applies to any positive affine transformation of zero-sum games (and hence all strictly competitive games [1]) even though these transformations lead to different trajectories of play.", "startOffset": 269, "endOffset": 272}, {"referenceID": 5, "context": "for constant-sum polymatrix games [6, 7, 10].", "startOffset": 34, "endOffset": 44}, {"referenceID": 6, "context": "for constant-sum polymatrix games [6, 7, 10].", "startOffset": 34, "endOffset": 44}, {"referenceID": 9, "context": "for constant-sum polymatrix games [6, 7, 10].", "startOffset": 34, "endOffset": 44}, {"referenceID": 37, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 148, "endOffset": 160}, {"referenceID": 38, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 148, "endOffset": 160}, {"referenceID": 20, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 189, "endOffset": 201}, {"referenceID": 14, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 314, "endOffset": 326}, {"referenceID": 30, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 314, "endOffset": 326}, {"referenceID": 39, "context": "The resulting FoReL dynamics enjoy a particularly strong O(t\u22121) regret minimization rate and they capture as a special case the replicator dynamics [38, 44, 45] and the projection dynamics [12, 24, 36], arguably the most widely studied game dynamics in biology, evolutionary game theory and transportation science [16, 35, 48].", "startOffset": 314, "endOffset": 326}, {"referenceID": 14, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 24, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 25, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 31, "context": "In this way, our analysis unifies and generalizes many prior results on the cycling behavior of evolutionary dynamics [16, 28, 29, 37] and it provides a new interpretation of these results through the lens of optimization and machine learning.", "startOffset": 118, "endOffset": 134}, {"referenceID": 33, "context": "1A standard trick is to decrease step-sizes by a constant factor after a window of \u201cdoubling\u201d length [40].", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "Following [6, 7, 10], an N -player pairwise zero-/constant-sum polymatrix game consists of an (undirected) interaction graph G \u2261 G(N , E) whose set of nodes N represents the competing players, with two nodes i, j \u2208 N connected by an edge e = (i, j) in E if and only if the corresponding players compete with each other in a two-player zero-/constant-sum game.", "startOffset": 10, "endOffset": 20}, {"referenceID": 6, "context": "Following [6, 7, 10], an N -player pairwise zero-/constant-sum polymatrix game consists of an (undirected) interaction graph G \u2261 G(N , E) whose set of nodes N represents the competing players, with two nodes i, j \u2208 N connected by an edge e = (i, j) in E if and only if the corresponding players compete with each other in a two-player zero-/constant-sum game.", "startOffset": 10, "endOffset": 20}, {"referenceID": 9, "context": "Following [6, 7, 10], an N -player pairwise zero-/constant-sum polymatrix game consists of an (undirected) interaction graph G \u2261 G(N , E) whose set of nodes N represents the competing players, with two nodes i, j \u2208 N connected by an edge e = (i, j) in E if and only if the corresponding players compete with each other in a two-player zero-/constant-sum game.", "startOffset": 10, "endOffset": 20}, {"referenceID": 16, "context": "1 to Appendix C; we also refer to [20] for a similar regret bound for (FoReL) in the context of online convex optimization.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "3) represents a striking improvement over the \u0398(t\u22121/2) worst-case bound for FoReL in discrete time [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 23, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 25, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 31, "context": "periodicity, recurrence, limit cycles or chaos [26, 27, 29, 37].", "startOffset": 47, "endOffset": 63}, {"referenceID": 2, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 15, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 20, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 33, "context": "6This coupling is closely related to the so-called Bregman divergence \u2013 for the details, see [3, 19, 24, 40].", "startOffset": 93, "endOffset": 108}, {"referenceID": 0, "context": "for all xi, xi \u2208 Xi and for all t \u2208 [0, 1].", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "For example, this class of games contains all strictly competitive games [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 18, "context": "This choice model was first studied in the context of discrete choice theory by McFadden [22] and it leads to the multiplicative weights (MW) dynamics:8 \u1e8fi = vi(x), xi = \u039bi(yi).", "startOffset": 89, "endOffset": 93}, {"referenceID": 38, "context": "This equation describes the replicator dynamics of [45], the most widely studied model for evolution under natural selection in population biology and evolutionary game theory.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 138, "endOffset": 154}, {"referenceID": 20, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 138, "endOffset": 154}, {"referenceID": 35, "context": "The basic relation between (MW) and (RD) was first noted in a single-agent environment by [34] and was explored further in game theory by [17, 23, 24, 42] and many others.", "startOffset": 138, "endOffset": 154}, {"referenceID": 1, "context": "For more details about (MWU), we refer the reader to [2].", "startOffset": 53, "endOffset": 56}, {"referenceID": 20, "context": "over all intervals for which the support of x(t) remains constant [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "The dynamics (PD) were introduced in game theory by [12] as a geometric model of the evolution of play in population games; for a closely related approach, see also [21, 25] and references therein.", "startOffset": 165, "endOffset": 173}, {"referenceID": 21, "context": "The dynamics (PD) were introduced in game theory by [12] as a geometric model of the evolution of play in population games; for a closely related approach, see also [21, 25] and references therein.", "startOffset": 165, "endOffset": 173}, {"referenceID": 3, "context": "Poincar\u00e9 Recurrence: [4, 30] If a flow preserves volume and has only bounded orbits then for each open set there exist orbits that intersect the set infinitely often.", "startOffset": 21, "endOffset": 28}, {"referenceID": 11, "context": "The key step in proving this characterization is Farkas\u2019 lemma; the version we employ here is due to Gale, Kuhn and Tucker [13]):", "startOffset": 123, "endOffset": 127}], "year": 2017, "abstractText": "Regularized learning is a fundamental technique in online optimization, machine learning and many other fields of computer science. A natural question that arises in these settings is how regularized learning algorithms behave when faced against each other. We study a natural formulation of this problem by coupling regularized learning dynamics in zero-sum games. We show that the system\u2019s behavior is Poincar\u00e9 recurrent, implying that almost every trajectory revisits any (arbitrarily small) neighborhood of its starting point infinitely often. This cycling behavior is robust to the agents\u2019 choice of regularization mechanism (each agent could be using a different regularizer), to positive-affine transformations of the agents\u2019 utilities, and it also persists in the case of networked competition, i.e., for zero-sum polymatrix games.", "creator": "pdfLaTeX"}}}