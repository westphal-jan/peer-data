{"id": "1709.03363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2017", "title": "A Planning Approach to Monitoring Behavior of Computer Programs", "abstract": "We describe a novel approach to monitoring high-level behavior using concepts from AI planning. Our goal is to understand what a program does based on its system call tracking, which is particularly important for detecting malware. We approach this problem by building an abstract model of the operating system using the STRIPS planning language, which outputs system calls as planning operators. In the face of a system call tracking, we simulate the corresponding operators on our model, and by observing the characteristics of the state achieved, we learn the nature of the original program and its behavior. Unlike most statistical detection methods, which focus on syntactic features, our approach is therefore semantic in nature. Therefore, it is more robust against malware disguising techniques that alter the appearance of the track, but not its effect. We demonstrate the effectiveness of our approach by evaluating it based on actual system call tracking.", "histories": [["v1", "Mon, 11 Sep 2017 13:19:08 GMT  (170kb,D)", "http://arxiv.org/abs/1709.03363v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CR", "authors": ["alexandre cukier", "ronen i brafman", "yotam perkal", "david tolpin"], "accepted": false, "id": "1709.03363"}, "pdf": {"name": "1709.03363.pdf", "metadata": {"source": "CRF", "title": "A Planning Approach to Monitoring Computer Programs\u2019 Behavior", "authors": ["Alexandre Cukier", "Ronen I. Brafman", "Yotam Perkal", "David Tolpin"], "emails": ["alexandre.cukier@gmail.com", "brafman@cs.bgu.ac.il", "yperkal@paypal.com", "dvd@offtopia.net"], "sections": [{"heading": "Introduction", "text": "Malware is a serious threat for computer and Internet security for both individuals and entities. 430 million new unique pieces of malware were detected by Symantec in 2015, and 94.1 millions of malware variants during only the month of February 2017. Not surprisingly, to counter this threat, many techniques for malware detection have been proposed. In this paper we are interested in the more general problem of understanding the behaviors taking place in the system. Given this information, one can determine whether they are malicious or not, and if malicious, provide an informed response.\nThe standard approach to this problem is to use pattern-recognition methods, which are syntactic in nature. Roughly speaking, they view the input, whether code or events, as a long string of symbols, and seek properties of these strings that help classify them. To fool these methods, malware attempts to obfuscate its behavior by changing the sequence\u2019s properties (You and Yim 2010). Semantic methods, instead, try to model the underlying system, seeking to understand the input\u2019s meaning, where in this paper, the input used is\nCopyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nthe system-call trace.1 Therefore, they have the potential to be more robust to obfuscation attempts.\nThe most extreme and most accurate semantic approach is a faithful simulation of every trace followed by careful analysis of the resulting system state. This is impractical: the analysis of the state of a computer following each trace is a non-trivial time consuming task that requires deducing high-level insights from the low level state and can only be conducted by experts.\nInstead, we propose a methodology that uses an abstract system model based on AI-planning languages and models. It requires a one-time, off-line effort by an expert, and can be used automatically to analyze each trace: An expert that understands the semantics of system calls generates a planning operator for every system call. Each operator describes how the state of the system changes in response to the application of some system call. Each operator is an abstraction that attempts to capture the system call\u2019s relevant effects. The abstraction process also involves the generation of a set of propositions describing the system state. Now, given a system call trace, instead of simulating it on the real system, we simulate the corresponding planning operators on the abstract state. The propositions true in the resulting state give us the needed information about what behaviors were carried out by this code. This approach is fast and difficult to fool: obfuscation techniques that do not impact the actual behavior will not impact relevant aspects of the state.\nIn what follows we describe this methodology using examples, and demonstrate its advantages by comparing it to statistical methods on actual system calls related to a mail application."}, {"heading": "Related work", "text": "(Forrest et al. 1996) is considered the seminal work which pushed forward research on methods and representations of operating system process monitoring based on system calls. (Warrender, Forrest, and Pearlmutter 1999) provides an early comparison of machine\n1A system call is a mechanism used by a program to request from the operating system services it cannot perform directly, such access to hardware, files, network or memory.\nar X\niv :1\n70 9.\n03 36\n3v 1\n[ cs\n.A I]\n1 1\nSe p\n20 17\nlearning methods for modeling process behavior. (Gao, Reiter, and Song 2004) introduces the model of execution graph, and behavior similarity-measure based on the execution graph. (Mutz et al. 2006) combines multiple models into an ensemble to improve anomaly detection. (Xu and Shelton 2010) applies continuous time Bayesian network (CTBN) to system call processes to account for time-dependent features and address high variability of system call streams over time. (Kim et al. 2016) applies a deep LSTM-based architecture to sequences of individual system calls, treating system calls as a language model.\nInitially, only system call indices were used as features (Forrest et al. 1996; Warrender, Forrest, and Pearlmutter 1999). (Liu et al. 2005) compares three different representations of system calls: n-grams of system call names, histograms of system call names, and individual system calls with associated parameters. (Poulose Jacob and Surekha 2007) proposes the use of system call sequences of varying length as features. (Liu et al. 2005; Tandon and Chan 2006) investigate extracting features for machine learning from arguments of system calls. (Wressnegger et al. 2013) studies novel techniques of anomaly detection and classification using n-grams of system calls. (Canzanese, Mancoridis, and Kam 2015) conducts a case study of n-gram based feature selection for system-call based monitoring, and analyses the influence of the size of the n-gram set and the maximum n-gram length on detection accuracy.\nOther work attempted to detect behaviors in a semantic way, using abstract representations of behaviors based on low level events and various techniques for detection. They all carry the notion of state, keeping track of effects of previous events. (Christodorescu et al. 2005) is the first to introduce semantics to characterize malicious behaviors. It builds behavior templates from binaries using formal semantics, which is used through a semantics-aware algorithm for detection. (Martignoni et al. 2008) builds multi-layered behavior graphs from low level events used through a behavior matcher. (Jacob, Debar, and Filiol 2009) uses attribute grammars for abstraction and specification, using parallel automata for parsing and detection. (Tokhtabayev, Skormin, and Dolgikh 2010) specifies behaviors through UML activity diagrams from which one generates colored Petri Nets for detection. (Beaucamps, Gnaedig, and Marion 2012) uses first-order linear temporal logic to specify behaviors and model checking techniques for detection. (Ezzati-Jivan and Dagenais 2012) offers an advanced state-full approach where behaviors are specified as finite state machines. Our approach is more fine-grained and general. We model the actual operators, not the target behaviors, although the model is informed by the behaviors. We illustrate this using the reverse shell example in the next section.\nBehavior recognition is closely related to plan and goal recognition (Sukthankar et al. 2014). Given a sequence of observed actions, the goal is to try to infer the actor\u2019s intentions. Typically, the output is a ranked list\nof hypothesized goals. Most work assumes a library of possible behavior instances, i.e., plans, an approach limited in its ability to go beyond known instances. Probabilistic techniques, such as (Baker, Tenenbaum, and Saxe 2005) use Bayesian methods to assess the probability of various goals based on the actions involved. An influential recent approach is plan-recognition as planning (Ram\u0131\u0301rez and Geffner 2009), where the authors do away with the assumption of an explicit plan library. The plan library is replaced by a model of the domain (which implicitly defines the set of possible plans), and the goal is to compute a good plan that is closest to the observed behavior. This line of work is appropriate when the observations are a subset of the actual actions taken, or when we attempt to recognize the goal before plan completion. We attempt to recognize malicious behavior off-line given a complete trace, although extensions for the online setting are natural."}, {"heading": "Background", "text": ""}, {"heading": "AI Planning", "text": "AI Planning is a decision making technique used to find sequences of actions that can transform a system from some initial state into a goal state. Formally, a classical planning problem is a tuple: \u03c0 = \u3008P,A, I,G\u3009. where: P is a set of primitive propositions describing properties of interest of the system; A is the action set. Each action transforms the state of the system in some way; I is the start state; and G is the goal condition \u2014 usually a conjunction of primitive propositions. A state of the world, s, assigns truth values to all p \u2208 P . Recall that a literal is simply a primitive proposition or its negation.\nAn action a \u2208 A is a pair, {pre(a), effects(a)}, where pre(a) is a conjunction of literals, and effects(a) is a set of pairs (c, e) denoting conditional effects. We use a(s) to denote the state that is obtained when a is executed in state s. If s does not satisfy all literals in pre(a), then a(s) is undefined. Otherwise, a(s) assigns to each proposition p the same value as s, unless there exists a pair (c, e) \u2208 effects(a) such that s |= c and e assigns p a different value than s. We assume that a is well defined, that is, if (c, e) \u2208 effects(a) then c\u2227pre(a) is consistent, and that if both (c, e), (c\u2032, e\u2032) \u2208 effects(a) and s |= c\u2227 c\u2032 for some state s, then e \u2227 e\u2032 is consistent.\nThe classical planning problem is defined as follows: given a planning problem \u03c0, find a sequence of actions {a1, . . . , ak} (a.k.a. a plan) such that ak(\u00b7 \u00b7 \u00b7 (a1(I)) \u00b7 \u00b7 \u00b7) |= G.\nTo illustrate this model, consider a simplified domain with three action types: socket, listen, and accept. These actions model the effect of system calls that create a socket, listen for an incoming connection, and accept a connection. For the sake of this example, we ignore various parameters of these system calls, and assume that system calls do not fail.\nThe set P contains: {(opened socket-descriptor), (listening socket-descriptor), (connected socketdescriptor)}, where socket-descriptor is a parameter\nthat we abbreviate as sd. The set of actions is:\n\u2022 socket(returned-sd) with precondition: \u00ac(opened returned-sd), and the effect: (opened returned-sd)2\n\u2022 listen(sd) has no precondition and the conditional effect: (listening sd) when (opened sd) \u2227\u00ac(listening sd)\n\u2022 accept(sd, returned-sd) has the preconditions: (listening sd) and the effects: (opened returned-sd) and (connected returned-sd)\nThe plan socket(sd1), listen(sd1), accept(sd1, sd2) is a legal plan. Initially, all propositions are false. Because (opened sd1) is false, we can apply socket(sd1). Once applied, (opened sd1), the precondition of listen, becomes true. This results in (listening sd1) becoming true. Finally, accept needs a socket descriptor in the state listening (sd1) and another having opened false (sd2). It now sets sd2 to opened and connected. Given the resulting state, we recognize that a host connected itself to our local server.\nOn the other hand, the plan: socket(sd1), accept(sd1, sd2) is invalid because the preconditions of accept are not all sastified: (listening sd2) is not set to true.\nTypically, planning models are used for generating plans. Thus, in the above example, a planning algorithm could find the (abstracted) sequence of system calls required to achieve various goals. Our focus in this paper is on the planning model itself \u2014 the propositions and the operators, as an abstraction of the operating system. The acting agent is a running process, the OS is the environment in which it is acting, and its system-call trace defines the plan, via our mapping. To determine what the process is doing, we simply observe the abstract state of the OS. For the purpose of this paper, we consider that the OS abstraction has a unique running and single thread process."}, {"heading": "Our approach", "text": "We propose to build an abstract system model and simulate an abstraction of the system call trace on it.\nThe manual part of our approach is the construction of the abstraction. We associate an action with each system call, with preconditions (typically empty) and effects (typically conditional). The set of propositions that we use to describe the system is informed by the type of behaviors we want to capture. For example, whether channels were opened, files accessed, information transmitted over a channel, etc. An action describes what new facts will become true following the execution of the system call it models, possibly conditional on other facts being true prior to its execution.\nWe illustrate this using the example of a remote shell: a command line interface controlled by a remote host often used by attackers to execute system commands. We focus on the reverse shell, where a host connects itself to a remote server. Starting a reverse shell requires\n2A more faithful model will use conditional effects instead, and will also consider their return value.\na few steps: (1) Create a socket. (2) Independently connect the socket to an endpoint and duplicate the socket descriptor to the standard input and output (so that the input and output streams go through the socket). (3) Execute a shell.\nWe use system calls socket, connect, dup, fcntl, close and execve that, respectively, create a socket, connect a socket to a remote host, duplicate a socket, set properties to a socket, close a socket, and execute a program.\nPropositions. The propositions are: (opened fd), (issocket fd), (equal-fds fd1 fd2), (close-on-exec fd), (connected sd), (is-shell path), (remote-shell-started)\nInitial state. The initial state initiates the resources used by a process when it starts, and taints the ones that have targeted properties:\n\u2022 Propositions (opened fd0), (opened fd1), (opened fd2) are set to true, as fd0/1/2 denote standard input/output/error, respectively, and these files are open.\n\u2022 Shell executable paths are tainted. We assume that we know all of those presents on the operating system. We handle two of them in this example: /bin/sh and /bin/bash that we name respectively sh and bash. Thus, (is-shell sh) and (is-shell bash) are set to true.\nActions. Planning operators are a simplified abstraction of the system calls. Since system calls called with wrong arguments do not make programs crash, and have no effect, the corresponding actions use conditional effects only \u2013 i.e., they are always executable but change the state only if their conditions are met.\n\u2022 socket(returned-sd, cloexec) has the effects: The flag FD CLOEXEC is represented by the boolean cloexec.\n\u2013 (opened returned-sd)\u2227(is-socket returned-sd) if \u00ac(opened returned-sd) \u2013 (close-on-exec returned-sd) if \u00ac(opened returnedsd)\u2227(= cloexec True)\n\u2022 connect(sd) has the effects: \u2013 (connected sd) if (opened sd)\u2227(is-socket\nsd)\u2227\u00ac(connected sd) \u2013 \u2200fd, (connected fd) if (equal-fds sd fd)\u2227(opened\nsd)\u2227(is-socket sd)\u2227\u00ac(connected sd) \u2022 dup(sd, returned-sd) has the effects:\n\u2013 (opened returned-sd)\u2227(equal-fds sd returnedsd)\u2227(equal-fds returned-sd sd) if (opened sd)\u2227\u00ac(opened returned-sd) \u2013 (is-socket returned-sd) if (is-socket sd)\u2227(opened sd)\u2227\u00ac(opened returned-sd) \u2013 (connected returned-sd) if (connected sd)\u2227(opened sd)\u2227\u00ac(opened returned-sd) \u2013 \u2200fd, (equal-fds fd returned-sd)\u2227(equal-fds returned-sd fd) if (equal-fds fd sd)\u2227\u00ac(opened returned-sd)\n\u2022 fcntl(sd, command, returned-sd, cloexec) has the effects:\nreturned-sd is the argument of the command F DUPFD and cloexec is the argument of the command F SETFD. F DUPFD CLOEXEC uses both. The flag FD CLOEXEC is represented by the boolean cloexec.\n\u2013 same effects as dup(sd, returned-sd) if (= command F DUPFD)\u2228(= command F DUPFD CLOEXEC)\n\u2013 (close-on-exec sd) if [[(= command F SETFD)\u2227(= cloexec True)]\u2228(= command F DUPFD CLOEXEC)]\u2227(opened sd)\u2227\u00ac(opened returned-sd) \u2013 \u00ac(close-on-exec sd) if (= command F SETFD)\u2227(= cloexec False)\u2227(opened sd)\u2227\u00ac(opened returned-sd)\n\u2022 close(sd) has the effects: \u2013 \u00ac(opened sd)\u2227\u00ac(is-socket sd)\u2227 \u00ac(connected\nsd)\u2227\u00ac(close-on-exec sd) \u2013 \u2200fd, \u00ac(equal-fds sd fd)\u2227\u00ac(equal-fds fd sd) \u2022 execve(path) has the effect:\n\u2013 (remote-shell-started) if (is-shell path) \u2227\u2203fd, (connected fd)\u2227\u00ac(close-on-exec fd)\u2227[(= fd fd0)\u2228(equalfds fd fd0)]\u2227[(= fd fd1)\u2228(equal-fds fd fd1)]\nValid plans The five different valid plans shown in Figure 1 show how diverse the plans are even for such a simple example. Plan 1 is the standard sequence performed to establish a reverse shell, which appears in most shellcode databases. Plan 2 uses the fact that we know that system call socket allocates the lowest file descriptor available. Calling close(fd0) before socket avoids the duplication of the socket on the file descriptor 0. Plan 3 replaces one system call by an equivalent one: dup is replaced by fcntl called with the command F DUPFD. Plan 4 demonstrates that planning captures and updates correctly properties set by flags and through different system calls. The flag FD CLOEXEC is first set through system call socket, and reset later by fcntl called on F SETFD. Plan 5 shows that planning is able to follow complex flow of operation on file descriptors. The key point is that, despite major differences in appearance, which are likely to fool syntactic methods (certainly, if some of the plans were not available previously), our semantic approach recognizes the behavior they implement.\nThe main effort required by our approach is building an appropriate model for each system call. This model is informed by the basic set of low-level behaviors one would like to model. Once completed, we can simulate any sequence of system calls by applying them to an initial state of the abstract system using any planning simulator/validator. By examining the final state of the system, we can recognize which behaviors took place. Thus, the off-line modeling task is done once, and the resulting model can be used repeatedly, automatically, and very cheaply, to analyze programs.\nThe (manual) abstraction process is flexible. We can use it to identify simple behaviors, such as create a socket, connect to a remote host, read data from socket, open file for writing, write into file, etc. And we can also recognize complex behaviors by detecting combinations of simple behaviors. For example, downloading a file requires reading data from a connected socket and writing it to an opened file. Thus, once we have the low-level behaviors, it is easy to capture the higher level ones. We can do this by either modifying the action model or by adding axioms, which are a method of adding a simple form of inference to planning. With such a layered approach, basic behaviors can be reused to identify multiple high level behaviors.\nAs this model is an abstraction, some information is lost in this model, and the method cannot be 100% correct and capture every nuance. Much can be captured by building a more elaborate model, but some aspects, such as accurate modeling of system resources, are not likely to be practical."}, {"heading": "Empirical evaluation", "text": "In the previous section we demonstrated the capabilities of our approach to recognize behaviors on the reverse shell domain, where our planning model is able to recognize the same behavior generated in different ways. We now want to highlight our ability to recognize complex, higher level behaviors that are built from lower level behaviors, compared to statistical methods that are quite popular in this area. To do this, we consider the behavior of real processes involved in a mail service. Given the system call trace logs of several processes, we attempt to recognize which behavior is realized by each of the processes, such as sending an email via SMTP, collecting an email from a remote server via IMAP, and so on. The code and data set used for the empirical evaluation can be obtained from a Git repository at https://github.com/alexEnsimag/ planning-for-syscall-monitoring."}, {"heading": "Data Collection", "text": "We generate system call traces of processes running in a mail service (Figure 2). The setup consists of two hosts: the client and the server, and involves a number of processes, denoted in what follows in italic. The hosts collect emails from an external server. In order to provide sufficient volume and diversity of the data\nprocessed, we opened a dedicated email account with a web-based email service, and subscribed to multiple promotion and notification mailing lists. On the client, fetchmail is used to retrieve emails from a web-based email provider via the IMAP protocol. Then, procmail dispatches received emails, which are then sent by postfix to the server via SMTP protocol. The server\u2019s postfix process receives the emails, passes them through the amavis antivirus and stores in the local filesystem. The dovecot process serves emails via the IMAP protocol. The emails are retrieved by the client\u2019s fetchmail, and stored in the filesystem. We use Docker (Hykes 2013 2017) to run containers encapsulating the mail server and mail client hosts, and sysdig (Draios, Inc. 2012 2016) to record the system calls.\nWe analyze system call traces of the following processes: smtpd, fetchmail on the client, fetchmail on the server, imap-login, all other processes.\nThese processes realize the following behaviors:\n\u2022 receiving an email over the SMTP protocol; \u2022 receiving an email over IMAP protocol; \u2022 forwarding an email from the client to the server; \u2022 IMAP connection setup and authentication; \u2022 other behaviours not tracked by the system. We use 440 samples for each process (including other). Data is split into training and test sets as 66%/33%.\nStatistical Classification. We compare our goaltracking approach to a baseline, commonly used statistical classification of processes based on system call sequences. We train a statistical classifier (random forest) on the collected system call traces. This classification approach is similar to approaches used in system call monitoring literature (Canali et al. 2012), (Firdausi et al. 2010). We use bi-gram vectorization of the system call names. Our vocabulary contains 100 most recurrent bi-grams in the corpus. Thus, each sample is represented by a vector of 100 elements, where each element in the vector represents one of the bi-grams in the vocabulary and the values represent the number of times a specific bi-gram appeared in that sample. For\nexample, if the first 4 elements in the vector correspond to bi-grams (open, read), (read, write), (write, read), and (write, close), a system call trace\nopen, read, write, read, write, close\nwill produce a bi-gram vector: 1, 2, 1, 1, . . . We proceed with the empirical evaluation as follows:\n1. We train a classifier that classifies each of the behaviors based on non-obfuscated system call sequences. The classifier achieves 97% accuracy.\n2. We create obfuscated samples in a way that \u2018breaks\u2019 the bi-grams by inserting a system call that has no effect on the process behavior (for example, sleep with a sufficiently small argument) in between each couple of system calls in the sequence. This method is called adding semantic no-ops and is the focus of (Rosenberg and Gudes 2017). When testing the statistical model on the obfuscated data we get 0% accuracy (all samples in the test set are classified as \u2018other\u2019).\n3. We retrain our model on both obfuscated and nonobfuscated data. It now achieves \u2248 66% accuracy.\nPlanning-based Classification Information contained in the system logs and manual inspection of system call traces in the training set are used to specify the planning domain and the goal for each of the behaviors. Then, the VAL plan validation system (Long 2014) is used to classify system call traces in the test set. The planning-based classifier based on the domain built for the original, non-obfuscated system call logs is applied to both non-obfuscated and obfuscated system call traces. In both cases, the planning based classifier reaches over 98% accuracy. Manual inspection of the misclassified samples suggests that the samples correspond to failed communication between components of the mail service.\nTable 1 summarizes the results. Our goal-tracking approach successfully classifies processes. Statistical methods are able to distinguish between different processes, but are prone to obfuscation. The obfuscation technique we used in this case study is particularly challenging for statistical classification, and with other obfuscation techniques the difference in accuracy might not be as sharp. However, the planning-based approach is inherently more robust in the face of obfuscation, as it captures semantic behavior, which must remain intact, rather than just statistical manifestations, which can be easily altered."}, {"heading": "Discussion and future work", "text": "We presented an approach for monitoring computer programs using an abstract model of the system state\nand the basic \u201dactions\u201d that operate on this state \u2014 system calls, in our case. The method is semantic in nature, and hence not prone to the weaknesses of syntactic methods that consider the command sequences form rather than their meaning. Unlike statistical methods that, in principle, can be fully automated, our approach has a non-trivial, one-time manual modeling step. But once the model is constructed, it can be used automatically and with little cost.\nWe demonstrated the effectiveness of our method by first showing how we capture a simple low level behavior that has diverse implementations using a simple model. Syntactically, each implementation is quite different, yet the common semantics can be captured by modeling just a few system calls. Then, we showed how we detect more complex, higher level behavior with almost perfect accuracy, without being affected by obfuscation techniques that easily fool state-of-the-art statistical methods. The approach used here can be used for other applications beyond system-call logs, such as analysis of transactions, HTTP logs, and more. Moreover, we believe that it could complement statistical methods by allowing us to run statistical analysis on the higher level features generated by our abstract state."}], "references": [{"title": "J", "author": ["Baker, C.L.", "Tenenbaum"], "venue": "B.; and Saxe, R.", "citeRegEx": "Baker. Tenenbaum. and Saxe 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "AbstractionBased Malware Analysis Using Rewriting and Model Checking", "author": ["Gnaedig Beaucamps", "P. Marion 2012] Beaucamps", "I. Gnaedig", "J.-Y. Marion"], "venue": null, "citeRegEx": "Beaucamps et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Beaucamps et al\\.", "year": 2012}, {"title": "A quantitative study of accuracy in system call-based malware detection", "author": ["Canali"], "venue": "ISSTA", "citeRegEx": "Canali,? \\Q2012\\E", "shortCiteRegEx": "Canali", "year": 2012}, {"title": "System call-based detection of malicious processes", "author": ["Mancoridis Canzanese", "R. Kam 2015] Canzanese", "S. Mancoridis", "M. Kam"], "venue": "In Int. Conf. on Software Quality, Reliability and Security,", "citeRegEx": "Canzanese et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Canzanese et al\\.", "year": 2015}, {"title": "R", "author": ["M. Christodorescu", "S. Jha", "S.A. Seshia", "D. Song", "Bryant"], "venue": "E.", "citeRegEx": "Christodorescu et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "M", "author": ["N. Ezzati-Jivan", "Dagenais"], "venue": "R.", "citeRegEx": "Ezzati.Jivan and Dagenais 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of machine learning techniques used in behavior-based malware detection", "author": ["Firdausi"], "venue": "In ACT", "citeRegEx": "Firdausi,? \\Q2010\\E", "shortCiteRegEx": "Firdausi", "year": 2010}, {"title": "T", "author": ["S. Forrest", "S.A. Hofmeyr", "A. Somayaji", "Longstaff"], "venue": "A.", "citeRegEx": "Forrest et al. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "M", "author": ["Gao, D.", "Reiter"], "venue": "K.; and Song, D.", "citeRegEx": "Gao. Reiter. and Song 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Malware behavioral detection by attribute-automata using abstraction from platform and language", "author": ["Debar Jacob", "G. Filiol 2009] Jacob", "H. Debar", "E. Filiol"], "venue": "In RAID", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Lstm-based system-call language modeling and robust ensemble method for designing host-based intrusion detection systems. arXiv preprint arXiv:1611.01726", "author": ["Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2016\\E", "shortCiteRegEx": "Kim", "year": 2016}, {"title": "A comparison of system call feature representations for insider threat detection", "author": ["Liu"], "venue": "In Information Assurance Workshop", "citeRegEx": "Liu,? \\Q2005\\E", "shortCiteRegEx": "Liu", "year": 2005}, {"title": "J", "author": ["L. Martignoni", "E. Stinson", "M. Fredrikson", "S. Jha", "Mitchell"], "venue": "C.", "citeRegEx": "Martignoni et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Anomalous system call detection", "author": ["Mutz"], "venue": "ACM Trans. Inf. Syst. Secur. 9(1):61\u201393", "citeRegEx": "Mutz,? \\Q2006\\E", "shortCiteRegEx": "Mutz", "year": 2006}, {"title": "M", "author": ["K. Poulose Jacob", "Surekha"], "venue": "V.", "citeRegEx": "Poulose Jacob and Surekha 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Gudes", "author": ["I. Rosenberg"], "venue": "E.", "citeRegEx": "Rosenberg and Gudes 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "R", "author": ["G. Sukthankar", "C. Geib", "H. Bui", "D. Pynadath", "Goldman"], "venue": "P., eds.", "citeRegEx": "Sukthankar et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "P", "author": ["G. Tandon", "Chan"], "venue": "K.", "citeRegEx": "Tandon and Chan 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic, resilient detection of complex malicious functionalities in the system call domain", "author": ["Skormin Tokhtabayev", "A. Dolgikh 2010] Tokhtabayev", "V. Skormin", "A. Dolgikh"], "venue": "In MILCOM\u201910,", "citeRegEx": "Tokhtabayev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tokhtabayev et al\\.", "year": 2010}, {"title": "Detecting intrusions using system calls: Alternative data models", "author": ["Forrest Warrender", "C. Pearlmutter 1999] Warrender", "S. Forrest", "B. Pearlmutter"], "venue": "In IEEE Sym. on Security and Privacy,", "citeRegEx": "Warrender et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Warrender et al\\.", "year": 1999}, {"title": "A close look on n-grams in intrusion detection: Anomaly detection vs. classification", "author": ["Wressnegger"], "venue": "In ACM WS on AI and Security,", "citeRegEx": "Wressnegger,? \\Q2013\\E", "shortCiteRegEx": "Wressnegger", "year": 2013}, {"title": "C", "author": ["J. Xu", "Shelton"], "venue": "R.", "citeRegEx": "Xu and Shelton 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Yim", "author": ["I. You"], "venue": "K.", "citeRegEx": "You and Yim 2010", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [], "year": 2017, "abstractText": "We describe a novel approach to monitoring high level behaviors using concepts from AI planning. Our goal is to understand what a program is doing based on its system call trace. This ability is particularly important for detecting malware. We approach this problem by building an abstract model of the operating system using the STRIPS planning language, casting system calls as planning operators. Given a system call trace, we simulate the corresponding operators on our model and by observing the properties of the state reached, we learn about the nature of the original program and its behavior. Thus, unlike most statistical detection methods that focus on syntactic features, our approach is semantic in nature. Therefore, it is more robust against obfuscation techniques used by malware that change the outward appearance of the trace but not its effect. We demonstrate the efficacy of our approach by evaluating it on actual system call traces.", "creator": "LaTeX with hyperref package"}}}