{"id": "0801.4716", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2008", "title": "Methods to Integrate a Language Model with Semantic Information for a Word Prediction Component", "abstract": "most current descriptive word prediction design systems make their use of n - gram language knowledge models ( aka lm ) to estimate the probability of the syllables following unique word structure in a preceding phrase. nowhere in the past years \u00ab there have been many attempts'to truly enrich quantitative such language models with finding further syntactic or semantic information. we want holmes to explore the quantitative predictive powers of latent semantic retrieval analysis ( lsa ), what a method demonstrate that he has been shown to naturally provide reliable information on long - distance semantic dependencies identified between words grouped in a context. we present and evaluate here two several methods combined that can integrate practical lsa - based information methods with a robust standard language representation model : a redundant semantic cache, partial reranking, and different forms thereof of meaningful interpolation. we found conclusions that though all methods lacked show any significant improvements, also compared to perhaps the usual 4 - gram coding baseline, and do most others of them allow to share a simple cache reading model as remarkably well.", "histories": [["v1", "Wed, 30 Jan 2008 17:10:24 GMT  (198kb)", "http://arxiv.org/abs/0801.4716v1", "10 pages ; EMNLP'2007 Conference (Prague)"]], "COMMENTS": "10 pages ; EMNLP'2007 Conference (Prague)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tonio wandmacher", "jean-yves antoine"], "accepted": true, "id": "0801.4716"}, "pdf": {"name": "0801.4716.pdf", "metadata": {"source": "CRF", "title": "Methods to integrate a language model with semantic information for a word prediction component", "authors": ["Tonio Wandmacher"], "emails": [], "sections": [{"heading": "1 Introduction: NLP for AAC systems", "text": "Augmented and Alternative Communication (AAC) is a field of research which concerns natural language processing as well as human-machine interaction, and which aims at restoring the communicative abilities of disabled people with severe speech and motion impairments. These people can be for instance cerebrally and physically handicapped persons or they suffer from a locked-in syndrome due to a cerebral apoplexy. Whatever the disease or impairment considered, oral communication is impossible for these persons who have in addition serious difficulties to control physically\ntheir environment. In particular, they are not able to use standard input devices of a computer. Most of the time, they can only handle a single switch device. As a result, communicating with an AAC system consists of typing messages by means of a virtual table of symbols (words, letters or icons) where the user successively selects the desired items.\nBasically, an AAC system, such as FASTY (Trost et al. 2005) or SIBYLLE (Schadle et al, 2004), consists of four components. At first, one finds a physical input interface connected to the computer. This device is adapted to the motion capacities of the user. When the latter must be restricted to a single switch (eye glimpse or breath detector, for instance), the control of the environment is reduced to a mere Yes/No command.\nSecondly, a virtual keyboard is displayed on screen. It allows the user to select successively the symbols that compose the intended message. In SIBYLLE, key selection is achieved by pointing letters through a linear scan procedure: a cursor successively highlights each key of the keyboard.\nThe last two components are a text editor (to write e-mails or other documents) and a speech synthesis module, which is used in case of spoken communication. The latest version of SIBYLLE works for French and German, and it is usable with any Windows\u2122 application (text editor, web browser, mailer...), which means that the use of a specific editor is no longer necessary. The main weakness of AAC systems results from the slowness of message composition. On average, disabled people cannot type more than 1 to 5 words per minute; moreover, this task is very tiring. The use of NLP techniques to improve AAC systems is therefore of first importance.\nTwo complementary approaches are possible to speed up communication. The first one aims at minimizing the duration of each item selection. Considering a linear scan procedure, one could for instance dynamically reorganize the keyboard in order to present the most probable symbols at first. The second strategy tries to minimize the number of keystrokes to be made. Here, the system tries to predict the words which are likely to occur just after those already typed. The predicted word is then either directly displayed after the end of the inserted text (a method referred to as \u201cword completion\u201d, cf. Boissi\u00e8re and Dours, 1996), or a list of Nbest (typically 3 to 7) predictions is provided on the virtual keyboard. When one of these predictions corresponds to the intended word, it can be selected by the user. As can be seen in figure 1, the interface of the SIBYLLE system presents such a list of most probable words to the user.\nSeveral approaches can be used to carry out word prediction. Most of the commercial AAC systems make only use of a simple lexicon: in this approach, the context is not considered.\nOn the other hand, stochastic language models can provide a list of word suggestions, depending on the n-1 (typically n = 3 or 4) last inserted words. It is obvious that such a model cannot take into account long-distance dependencies. There have been\nattempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989). Trnka et al. (2005) dynamically interpolate a high number of topic-oriented models in order to adapt their predictions to the current topic of the text or conversation.\nClassically, word predictors are evaluated by an objective metric called Keystroke Saving Rate (ksr):\n1001 \u22c5      \u2212= a p n k k ksr (1)\nwith kp, ka being the number of keystrokes needed on the input device when typing a message with (kp) and without prediction (ka = number of characters in the text that has been entered, n = length of the prediction list, usually n = 5). As\nTrost et al. (2005) and Trnka et al. (2005), we assume that one additional keystroke is required for the selection of a word from the list and that a space is automatically inserted afterwards. Note also that words, which have already occurred in the list, will not reappear after the next character has been inserted.\nThe perplexity measure, which is frequently used to assess statistical language models, proved to be less accurate in this context. We still present perplexities as well in order to provide comparative results."}, {"heading": "2 Language modeling and semantics", "text": ""}, {"heading": "2.1 Statistical Language Models", "text": "For about 10 to 15 years statistical language modeling has had a remarkable success in various NLP domains, for instance in speech recognition, machine translation, Part-of-Speech tagging, but also in word prediction systems. N-gram based language models (LM) estimate the probability of occurrence for a word, given a string of n-1 preceding words. However, computers have only recently become powerful enough to estimate probabilities on a reasonable amount of training data. Moreover, the larger n gets, the more important the problem of combinatorial explosion for the probability estimation becomes. A reasonable trade-off between performance and number of estimated events seems therefore to be an n of 3 to 5, including sophisticated techniques in order to estimate the probability of unseen events (smoothing methods).\nWhereas n-gram-like language models are already performing rather well in many applications, their capacities are also very limited in that they cannot exploit any deeper linguistic structure. Long-distance syntactic relationships are neglected as well as semantic or thematic constraints.\nIn the past 15 years many attempts have been made to enrich language models with more complex syntactic and semantic models, with varying success (cf. (Rosenfeld, 1996), (Goodman, 2002) or in a word prediction task: (Fazly and Hirst, 2003), (Schadle, 2004), (Li and Hirst, 2005)). We want to explore here an approach based on Latent Semantic Analysis (Deerwester et al, 1990)."}, {"heading": "2.2 Latent Semantic Analysis", "text": "Several works have suggested the use of Latent Semantic Analysis (LSA) in order to integrate se-\nmantic similarity to a language model (cf. Bellegarda, 1997; Coccaro and Jurafsky, 1998). LSA models semantic similarity based on co-occurrence distributions of words, and it has shown to be helpful in a variety of NLP tasks, but also in the domain of cognitive modeling (Landauer et al, 1997).\nLSA is able to relate coherent contexts to specific content words, and it is good at predicting the occurrence of a content word in the presence of other thematically related terms. However, since it does not take word order into account (\u201cbag-ofwords\u201d model) it is very poor at predicting their actual position within the sentence, and it is completely useless for the prediction of function words. Therefore, some attempts have been made to integrate the information coming from an LSA-based model with standard language models of the ngram type.\nIn the LSA model (Deerwester et al, 1990) a word wi is represented as a high-dimensional vector, derived by Singular Value Decomposition (SVD) from a term \u00d7 document (or a term \u00d7 term) co-occurrence matrix of a training corpus. In this framework, a context or history h (= w1, ... , wm) can be represented by the sum of the (already normalized) vectors corresponding to the words it contains (Landauer et al. 1997):\n\u2211 =\n= m\ni iwh 1\nrr (2)\nThis vector reflects the meaning of the preceding (already typed) section, and it has the same dimensionality as the term vectors. It can thus be compared to the term vectors by well-known similarity measures (scalar product, cosine)."}, {"heading": "2.3 Transforming LSA similarities into probabilities", "text": "We make the assumption that an utterance or a text to be entered is usually semantically cohesive. We then expect all word vectors to be close to the current context vector, whose corresponding words belong to the semantic field of the context. This forms the basis for a simple probabilistic model of LSA: After calculating the cosine similarity for\neach word vector i\nw r with the vector h r of the cur-\nrent context, we could use the normalized similarities as probability values. This probability distribution however is usually rather flat (i.e. the dynamic\nrange is low). For this reason a contrasting (or temperature) factor \u03b3 is normally applied (cf. Coccaro and Jurafsky, 1998), which raises the cosine to some power (\u03b3 is normally between 3 and 8). After normalization we obtain a probability distribution which can be used for prediction purposes. It is calculated as follows:\n( ) ( )\u2211 \u2212 \u2212= k \u03b3 k \u03b3 i iLSA hhw hhw hwP )(cos),cos( )(cos),cos( )( min min rrr rrr (3)\nwi is a word in the vocabulary, h is the current con-\ntext (history) i\nw r and h r are their corresponding vec-\ntors in the LSA space; cosmin( h r ) returns the lowest cosine value measured for h r\n). The denominator then normalizes each similarity value to ensure that \u2211 = n k kLSA hwP 1),( .\nLet us illustrate the capacities of this model by giving a short example from the French version of our own LSA predictor:\nContext: \u201cMon p\u00e8re \u00e9tait professeur en math\u00e9matiques\net je pense que \u201d (\u201cMy dad has been a professor in mathematics and I think that \u201d)\nRank Word P 1. professeur (\u2018professor\u2019) 0.0117 2. math\u00e9matiques (\u201cmathematics\u201d) 0.0109 3. enseign\u00e9 (participle of \u2018taught\u2019) 0.0083 4. enseignait (\u2018taught\u2019) 0.0053 5. mathematicien (\u2018mathematician\u2019) 0.0049 6. p\u00e8re (\u2018father\u2019) 0.0046 7. math\u00e9matique (\u2018mathematics\u2019) 0.0045 8. grand-p\u00e8re (\u2018grand-father\u2019) 0.0043 9. sciences (\u2018sciences\u2019) 0.0036\n10. enseignant (\u2018teacher\u2019) 0.0032\nExample 1: Most probable words returned by the LSA model for the given context. As can be seen in example 1, all ten predicted words are semantically related to the context, they should therefore be given a high probability of occurrence. However, this example also shows the drawbacks of the LSA model: it totally neglects the presence of function words as well as the syntactic structure of the current phrase. We therefore need to find an appropriate way to integrate the information coming from a standard n-gram model and the LSA approach."}, {"heading": "2.4 Density as a confidence measure", "text": "Measuring relation quality in an LSA space, Wandmacher (2005) pointed out that the reliability of LSA relations varies strongly between terms. He also showed that the entropy of a term does not correlate with relation quality (i.e. number of semantically related terms in an LSA-generated term cluster), but he found a medium correlation (Pearson coeff. = 0.56) between the number of semantically related terms and the average cosine similarity of the m nearest neighbors (density). The closer the nearest neighbors of a term vector are, the more probable it is to find semantically related terms for the given word. In turn, terms having a high density are more likely to be semantically related to a given context (i.e. their specificity is higher).\nWe define the density of a term wi as follows:\n\u2211 =\n\u22c5= m\nj ijiim wNNw m wD 1\n))(,cos( 1 )( rr\n(4)\nIn the following we will use this measure (with m=100) as a confidence metric to estimate the reliability of a word being predicted by the LSA component, since it showed to give slightly better results in our experiments than the entropy measure."}, {"heading": "3 Integrating semantic information", "text": "In the following we present several different methods to integrate semantic information as it is provided by an LSA model into a standard LM."}, {"heading": "3.1 Semantic cache model", "text": "Cache (or recency promotion) models have shown to bring slight but constant gains in language modeling (Kuhn and De Mori, 1990). The underlying idea is that words that have already occurred in a text are more likely to occur another time. Therefore their probability is raised by a constant or exponentially decaying factor, depending on the position of the element in the cache. The idea of a decaying cache function is that the probability of reoccurrence depends on the cosine similarity of the word in the cache and the word to be predicted. The highest probability of reoccurrence is usually after 15 to 20 words. Similar to Clarkson and Robinson (1997), we implemented an exponentially decaying cache of length l (usually between 100 and 1000), using the\nfollowing decay function for a word wi and its position p in the cache.\n2)(5,0\n),(  \n  \n \u2212\u2212 = \u03c3 \u00b5p\nid epwf (5)\n\u03c3 = \u00b5/3 if p < \u00b5 and \u03c3 = l/3 if p \u2265 \u00b5. The function returns 0 if wi is not in the cache, and it is 1 if p = \u00b5. A typical graph for (5) can be seen in figure (2).\nWe extend this model by calculating for each element having occurred in the context its m nearest LSA neighbors ( ),( \u03b8wNN\noccm\nr , using cosine simi-\nlarity), if their cosine lies above a threshold \u03b8, and add them to the cache as well, right after the word that has occurred in the text (\u201cBring your friends\u201dstrategy). The size of the cache is adapted accordingly (for \u00b5, \u03c3 and l), depending on the number of neighbors added. This results in the following cache function:\n),(),()( 1 cos pwfwwf\u03b2wP id\nl\ni\ni occicache \u2211 \u22c5\u22c5= (6) with l = size of the cache. \u03b2 is a constant controlling the influence of the component (usually \u03b2 \u2248 0.1/l); wiocc is a word that has already recently occurred in the context and is therefore added as a standard cache element, whereas wi is a nearest neighbor to wiocc. fcos(w i occ, wi) returns the cosine similarity between i occ w r and i w r , with cos( i occ w r , i w r ) > \u03b8 (Rem: wi with cos( i occ w r , i w r ) \u2264 \u03b8 have not been\nadded to the cache). Since cos( i\nw r\n, i\nw r )=1, terms\nhaving actually occurred before will be given full weight, whereas all wi being only nearest LSA neighbors to wiocc will receive a weight correspond-\ning to their cosine similarity with wiocc , which is less than 1 (but larger than \u03b8).\nfd(wi,p) is the decay factor for the current position p of wi in the cache, calculated as shown in equation (5)."}, {"heading": "3.2 Partial reranking", "text": "The underlying idea of partial reranking is to regard only the best n candidates from the basic language model for the semantic model in order to prevent the LSA model from making totally implausible (i.e. improbable) predictions. Words being improbable for a given context will be disregarded as well as words that do not occur in the semantic model (e.g. function words), because LSA is not able to give correct estimates for this group of words (here the base probability remains unchanged). For the best n candidates their semantic probability is calculated and each of these words is assigned an additional value, after a fraction of its base probability has been subtracted (jackpot strategy). For a given context h we calculate the ordered set\nBESTn(h) = <w1, \u2026 , wn>, so that P(w1|h) \u2265 P(w2|h) \u2265\u2026\u2265P(wn|h)\nFor each wi in BESTn(h) we then calculate its reranking probability as follows:\n)),(()(),cos()( iniiiRR\nwhBestIwDhw\u03b2wP \u22c5\u22c5\u22c5= rr (7)\n\u03b2 is a weighting constant controlling the overall influence of the reranking process, cos( i w r , i w r ) returns the cosine of the word\u2019s vector and the current context vector, D(wi) gives the confidence measure of wi and I is an indicator function being 1, iff wi \u2208BEST(h), and 0 otherwise."}, {"heading": "3.3 Standard interpolation", "text": "Interpolation is the standard way to integrate information from heterogeneous resources. While for a linear combination we simply add the weighted probabilities of two (or more) models, geometric interpolation multiplies the probabilities, which are weighted by an exponential coefficient (0\u2264\u03bb1\u22641): Linear Interpolation (LI):\n)()1()()(' 11 isibi wP\u03bbwP\u03bbwP \u22c5\u2212+\u22c5= (8)\nGeometric Interpolation (GI):\n\u2211 = \u2212\n\u2212\n\u22c5 \u22c5= n\nj\n\u03bb\njs\n\u03bb\njb\n\u03bb\nis\n\u03bb\nib\ni\nwPwP\nwPwP wP\n1\n)11(1\n)11(1\n)()(\n)()( )(' (9)\nThe main difference between the two methods is that the latter takes the agreement of two models into account. Only if each of the single models assigns a high probability to a given event will the combined probability be assigned a high value. If one of the models assigns a high probability and the other does not the resulting probability will be lower."}, {"heading": "3.4 Confidence-weighted interpolation", "text": "Whereas in standard settings the coefficients are stable for all probabilities, some approaches use confidence-weighted coefficients that are adapted for each probability. In order to integrate n-gram and LSA probabilities, Coccaro and Jurafsky (1998) proposed an entropy-related confidence measure for the LSA component, based on the observation that words that occur in many different contexts (i.e. have a high entropy), cannot well be predicted by LSA. We use here a density-based measure (cf. section 2.2), because we found it more reliable than entropy in preliminary tests. For interpolation purposes we calculate the coefficient of the LSA component as follows:\n)( ii wD\u03b2\u03bb \u22c5= , iff D(wi) > 0; 0 otherwise (10)\nwith \u03b2 being a weighting constant to control the influence of the LSA predictor. For all experiments, we set \u03b2 to 0.4 (i.e. 0 \u2264 \u03bbi \u2264 0.4), which proved to be optimal in pre-tests."}, {"heading": "4 Results", "text": "We calculated our baseline n-gram model on a 44 million word corpus from the French daily Le Monde (1998-1999). Using the SRI toolkit (Stolcke, 2002)1 we computed a 4-gram LM over a controlled 141,000 word vocabulary, using modified Kneser-Ney discounting (Goodman, 2001), and we applied Stolcke pruning (Stolcke, 1998) to reduce the model to a manageable size (\u03b8 = 10-7). 1 SRI Toolkit: www.speech.sri.com.\nThe LSA space was calculated on a 100 million word corpus from Le Monde (1996 \u2013 2002). Using the Infomap toolkit2, we generated a term \u00d7 term co-occurrence matrix for an 80,000 word vocabulary (matrix size = 80,000 \u00d7 3,000), stopwords were excluded. After several pre-tests, we set the size of the co-occurrence window to \u00b1100. The matrix was then reduced by singular value decomposition to 150 columns, so that each word in the vocabulary was represented by a vector of 150 dimensions, which was normalized to speed up similarity calculations (the scalar product of two normalized vectors equals the cosine of their angle).\nOur test corpus consisted of 8 sections from the French newspaper Humanit\u00e9, (January 1999, from 5,378 to 8,750 words each), summing up to 58,457 words. We then calculated for each test set the keystroke saving rate based on a 5-word list (ksr5) and perplexity for the following settings3:\n1. 4-gram LM only (baseline)\n2. 4-gram + decaying cache (l = 400)\n3. 4-gram + LSA using linear interpolation with \u03bbLSA = 0.11 (LI).\n4. 4-gram + LSA using geometric interpolation, with \u03bbLSA = 0.07 (GI).\n5. 4-gram + LSA using linear interpolation and (density-based) confidence weighting (CWLI).\n6. 4-gram + LSA using geometric interpolation and (density-based) confidence weighting (CWGI).\n7. 4-gram + partial reranking (n = 1000, \u03b2 = 0.001)\n8. 4-gram + decaying semantic cache (l = 4000; m = 10; \u03b8 = 0.4, \u03b2 = 0.0001)\nFigures 3 and 4 display the overall results in terms of ksr and perplexity.\n2 Infomap Project: http://infomap-nlp.sourceforge.net/ 3 All parameter settings presented here are based on results of extended empirical pre-tests. We used held-out development data sets that have randomly been chosen from the Humanit\u00e9 corpus.(8k to 10k words each). The parameters being presented here were optimal for our test sets. For reasons of simplicity we did not use automatic optimization techniques such as the EM algorithm (cf. Jelinek, 1990).\nUsing the results of our 8 samples, we performed paired t tests for every method with the baseline as well as with the cache model. All gains for ksr turned out to be highly significant (sig. level < 0.001), and apart from the results for CWLI, all perplexity reductions were significant as well (sig. level < 0.007), with respect to the cache results. We can therefore conclude that, with exception of CWLI, all methods tested have a beneficial effect, even when compared to a simple cache model. The highest gain in ksr (with respect to the baseline) was obtained for the confidence-weighted geometric interpolation method (CWGI; +1.05%), the highest perplexity reduction was measured for GI as well as for CWGI (-9.3% for both). All other methods (apart from IWLI) gave rather similar results (+0.6 to +0.8% in ksr, and -6.8% to -7.7% in perplexity).\nWe also calculated for all samples the correlation between ksr and perplexity. We measured a Pearson coefficient of -0.683 (Sig. level < 0.0001).\nAt first glance, these results may not seem overwhelming, but we have to take into account that our ksr baseline of 57.9% is already rather high,\nand at such a level, additional gains become hard to achieve (cf. Lesher et al, 2002).\nThe fact that CWLI performed worse than even simple LI was not expected, but it can be explained by an inherent property of linear interpolation: If one of the models to be interpolated overestimates the probability for a word, the other cannot compensate for it (even if it gives correct estimates), and the resulting probability will be too high. In our case, this happens when a word receives a high confidence value; its probability will then be overestimated by the LSA component."}, {"heading": "5 Conclusion and further work", "text": "Adapting a statistical language model with semantic information, stemming from a distributional analysis like LSA, has shown to be a non-trivial problem. Considering the task of word prediction in an AAC system, we tested different methods to integrate an n-gram LM with LSA: A semantic cache model, a partial reranking approach, and some variants of interpolation.\nWe evaluated the methods using two different measures, the keystroke saving rate (ksr) and perplexity, and we found significant gains for all methods incorporating LSA information, compared to the baseline. In terms of ksr the most successful method was confidence-weighted geometric interpolation (CWGI; +1.05% in ksr); for perplexity, the greatest reduction was obtained for standard as well as for confidence-weighted geometric interpolation (-9.3% for both). Partial reranking and the semantic cache gave very similar results, despite their rather different underlying approach.\nWe could not provide here a comparison with other models that make use of distributional information, like the trigger approach by Rosenfeld (1996), Matiasek and Baroni (2003) or the model presented by Li and Hirst (2005), based on Pointwise Mutual Information (PMI). A comparison of these similarities with LSA remains to be done.\nFinally, an AAC system has not only the function of simple text entering but also of providing cognitive support to its user, whose communicative abilities might be totally depending on it. Therefore, she or he might feel a strong improvement of the system, if it can provide semantically plausible predictions, even though the actual gain in ksr might be modest or even slightly decreasing. For this reason we will perform an extended qualitative\nanalysis of the presented methods with persons who use our AAC system SIBYLLE. This is one of the main aims of the recently started ESAC_IMC project. It is conducted at the Functional Reeducation and Rehabilitation Centre of Kerpape, Brittany, where SIBYLLE is already used by 20 children suffering from traumatisms of the motor cortex. They appreciate the system not only for communication but also for language learning purposes.\nMoreover, we intend to make the word predictor of SIBYLLE publicly available (AFM Voltaire project) in the not-too-distant future."}, {"heading": "Acknowledgements", "text": "This research is partially founded by the UFA (Universit\u00e9 Franco-Allemande) and the French foundations APRETREIMC (ESAC_IMC project) and AFM (VOLTAIRE project). We also want to thank the developers of the SRI and the Infomap toolkits for making their programs available."}], "references": [{"title": "A Latent Semantic Analysis Framework for Large-Span Language Modeling", "author": ["J. Bellegarda"], "venue": "Proceedings of the Eurospeech 97,", "citeRegEx": "Bellegarda,? \\Q1997\\E", "shortCiteRegEx": "Bellegarda", "year": 1997}, {"title": "VITIPI : Versatile interpretation of text input by persons with impairments", "author": ["Boissi\u00e8re Ph", "Dours D"], "venue": "Proceedings ICCHP'1996", "citeRegEx": "Ph. and D.,? \\Q1996\\E", "shortCiteRegEx": "Ph. and D.", "year": 1996}, {"title": "Word association norms, mutual information and lexicography", "author": ["K. Church", "P. Hanks"], "venue": "Proceedings of ACL,", "citeRegEx": "Church and Hanks,? \\Q1989\\E", "shortCiteRegEx": "Church and Hanks", "year": 1989}, {"title": "Language Model Adaptation using Mixtures and an Exponentially Decaying Cache", "author": ["P.R. Clarkson", "A.J. Robinson"], "venue": "in Proc. of the IEEE ICASSP-97,", "citeRegEx": "Clarkson and Robinson,? \\Q1997\\E", "shortCiteRegEx": "Clarkson and Robinson", "year": 1997}, {"title": "Towards better integration of semantic predictors in statistical language modeling", "author": ["N. Coccaro", "D. Jurafsky"], "venue": "Proc. of the ICSLP-98,", "citeRegEx": "Coccaro and Jurafsky,? \\Q1998\\E", "shortCiteRegEx": "Coccaro and Jurafsky", "year": 1998}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S.C. Deerwester", "S. Dumais", "T. Landauer", "G. Furnas", "R. Harshman"], "venue": "JASIS", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Testing the efficacy of part-of-speech information in word completion", "author": ["A. Fazly", "G. Hirst"], "venue": "Proceedings of the Workshop on Language Modeling for Text Entry Methods on EACL, Budapest", "citeRegEx": "Fazly and Hirst,? \\Q2003\\E", "shortCiteRegEx": "Fazly and Hirst", "year": 2003}, {"title": "A Bit of Progress in Language Modeling\u201d, Extended Version Microsoft Research Technical Report MSR-TR-2001-72", "author": ["J. Goodman"], "venue": null, "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Self-organized Language Models for Speech Recognition", "author": ["F. Jelinek"], "venue": "Readings in Speech Recognition,", "citeRegEx": "Jelinek,? \\Q1990\\E", "shortCiteRegEx": "Jelinek", "year": 1990}, {"title": "A Cache-Based Natural Language Model for Speech Reproduction", "author": ["R. Kuhn", "R. De Mori"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kuhn and Mori,? \\Q1990\\E", "shortCiteRegEx": "Kuhn and Mori", "year": 1990}, {"title": "How well can passage meaning be derived without using word order? A comparison of LSA and humans", "author": ["T.K. Landauer", "D. Laham", "B. Rehder", "M.E. Schreiner"], "venue": "Proceedings of the 19th annual meeting of the Cognitive Science Society,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Limits of human word prediction performance", "author": ["G.W. Lesher", "Moulton", "B. J", "D.J. Higginbotham", "B. Alsofrom"], "venue": "Proceedings of the CSUN", "citeRegEx": "Lesher et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lesher et al\\.", "year": 2002}, {"title": "Semantic knowledge in a word completion task", "author": ["J. Li", "G. Hirst"], "venue": "Proc. of the 7 Int. ACM Conference on Computers and Accessibility,", "citeRegEx": "Li and Hirst,? \\Q2005\\E", "shortCiteRegEx": "Li and Hirst", "year": 2005}, {"title": "Exploiting long distance collocational relations in predictive typing", "author": ["H. Matiasek", "M. Baroni"], "venue": "Proceedings of the EACL-03 Workshop on Language Modeling for Text Entry Methods,", "citeRegEx": "Matiasek and Baroni,? \\Q2003\\E", "shortCiteRegEx": "Matiasek and Baroni", "year": 2003}, {"title": "A maximum entropy approach to adaptive statistical language modelling", "author": ["R. Rosenfeld"], "venue": "Computer Speech and Language,", "citeRegEx": "Rosenfeld,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld", "year": 1996}, {"title": "Sibyl - AAC system using NLP techniques", "author": ["I. Schadle", "J.-Y. Antoine", "B. Le P\u00e9v\u00e9dic", "F. Poirier"], "venue": "Proc. ICCHP\u20192004,", "citeRegEx": "Schadle et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schadle et al\\.", "year": 2004}, {"title": "Entropy-based pruning of backoff language models", "author": ["A. Stolcke"], "venue": "Proc.s of the DARPA Broadcast News Transcription and Understanding Workshop", "citeRegEx": "Stolcke,? \\Q1998\\E", "shortCiteRegEx": "Stolcke", "year": 1998}, {"title": "SRILM - An Extensible Language Modeling Toolkit", "author": ["A. Stolcke"], "venue": "Proc. of the Intl. Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Topic Modeling in Fringe Word Prediction for AAC", "author": ["K. Trnka", "D. Yarrington", "K.F. McCoy", "C. Pennington"], "venue": "In Proceedings of the 2006 International Conference on Intelligent User Interfaces,", "citeRegEx": "Trnka et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Trnka et al\\.", "year": 2006}, {"title": "The Language Component of the FASTY Text Prediction System", "author": ["H. Trost", "J. Matiasek", "M. Baroni"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "Trost et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Trost et al\\.", "year": 2005}, {"title": "How semantic is Latent Semantic Analysis?", "author": ["T. Wandmacher"], "venue": "Proceedings of TALN/RECITAL", "citeRegEx": "Wandmacher,? \\Q2005\\E", "shortCiteRegEx": "Wandmacher", "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "Basically, an AAC system, such as FASTY (Trost et al. 2005) or SIBYLLE (Schadle et al, 2004), consists of four components.", "startOffset": 40, "endOffset": 59}, {"referenceID": 6, "context": "There have been attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction.", "startOffset": 65, "endOffset": 88}, {"referenceID": 2, "context": "Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989).", "startOffset": 123, "endOffset": 152}, {"referenceID": 5, "context": "There have been attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989).", "startOffset": 66, "endOffset": 520}, {"referenceID": 5, "context": "There have been attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989).", "startOffset": 66, "endOffset": 551}, {"referenceID": 2, "context": "Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989). Trnka et al. (2005) dynamically interpolate a high number of topic-oriented models in order to adapt their predictions to the current topic of the text or conversation.", "startOffset": 129, "endOffset": 174}, {"referenceID": 18, "context": "(2005) and Trnka et al. (2005), we assume that one additional keystroke is required for the selection of a word from the list and that a space is automatically inserted afterwards.", "startOffset": 11, "endOffset": 31}, {"referenceID": 14, "context": "(Rosenfeld, 1996), (Goodman, 2002) or in a word prediction task: (Fazly and Hirst, 2003), (Schadle, 2004), (Li and Hirst, 2005)).", "startOffset": 0, "endOffset": 17}, {"referenceID": 6, "context": "(Rosenfeld, 1996), (Goodman, 2002) or in a word prediction task: (Fazly and Hirst, 2003), (Schadle, 2004), (Li and Hirst, 2005)).", "startOffset": 65, "endOffset": 88}, {"referenceID": 12, "context": "(Rosenfeld, 1996), (Goodman, 2002) or in a word prediction task: (Fazly and Hirst, 2003), (Schadle, 2004), (Li and Hirst, 2005)).", "startOffset": 107, "endOffset": 127}, {"referenceID": 4, "context": "Several works have suggested the use of Latent Semantic Analysis (LSA) in order to integrate semantic similarity to a language model (cf. Bellegarda, 1997; Coccaro and Jurafsky, 1998).", "startOffset": 133, "endOffset": 183}, {"referenceID": 10, "context": ", wm) can be represented by the sum of the (already normalized) vectors corresponding to the words it contains (Landauer et al. 1997):", "startOffset": 111, "endOffset": 133}, {"referenceID": 20, "context": "Measuring relation quality in an LSA space, Wandmacher (2005) pointed out that the reliability of LSA relations varies strongly between terms.", "startOffset": 44, "endOffset": 62}, {"referenceID": 3, "context": "Similar to Clarkson and Robinson (1997), we implemented an exponentially decaying cache of length l (usually between 100 and 1000), using the", "startOffset": 11, "endOffset": 40}, {"referenceID": 4, "context": "In order to integrate n-gram and LSA probabilities, Coccaro and Jurafsky (1998) proposed an entropy-related confidence measure for the LSA component, based on the observation that words that occur in many different contexts (i.", "startOffset": 52, "endOffset": 80}, {"referenceID": 17, "context": "Using the SRI toolkit (Stolcke, 2002) we computed a 4-gram LM over a controlled 141,000 word vocabulary, using modified Kneser-Ney discounting (Goodman, 2001), and we applied Stolcke pruning (Stolcke, 1998) to reduce the model to a manageable size (\u03b8 = 10).", "startOffset": 22, "endOffset": 37}, {"referenceID": 7, "context": "Using the SRI toolkit (Stolcke, 2002) we computed a 4-gram LM over a controlled 141,000 word vocabulary, using modified Kneser-Ney discounting (Goodman, 2001), and we applied Stolcke pruning (Stolcke, 1998) to reduce the model to a manageable size (\u03b8 = 10).", "startOffset": 143, "endOffset": 158}, {"referenceID": 16, "context": "Using the SRI toolkit (Stolcke, 2002) we computed a 4-gram LM over a controlled 141,000 word vocabulary, using modified Kneser-Ney discounting (Goodman, 2001), and we applied Stolcke pruning (Stolcke, 1998) to reduce the model to a manageable size (\u03b8 = 10).", "startOffset": 191, "endOffset": 206}, {"referenceID": 12, "context": "We could not provide here a comparison with other models that make use of distributional information, like the trigger approach by Rosenfeld (1996), Matiasek and Baroni (2003) or the model presented by Li and Hirst (2005), based on Pointwise Mutual Information (PMI).", "startOffset": 131, "endOffset": 148}, {"referenceID": 12, "context": "We could not provide here a comparison with other models that make use of distributional information, like the trigger approach by Rosenfeld (1996), Matiasek and Baroni (2003) or the model presented by Li and Hirst (2005), based on Pointwise Mutual Information (PMI).", "startOffset": 149, "endOffset": 176}, {"referenceID": 12, "context": "We could not provide here a comparison with other models that make use of distributional information, like the trigger approach by Rosenfeld (1996), Matiasek and Baroni (2003) or the model presented by Li and Hirst (2005), based on Pointwise Mutual Information (PMI).", "startOffset": 202, "endOffset": 222}], "year": 2007, "abstractText": "Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4gram baseline, and most of them to a simple cache model as well.", "creator": "PDFCreator Version 0.9.0"}}}