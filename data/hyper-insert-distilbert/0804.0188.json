{"id": "0804.0188", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2008", "title": "Support Vector Machine Classification with Indefinite Kernels", "abstract": "separately we propose a complexity method for integrating support vector machine classification using indefinite kernels. instead of basically directly minimizing or systematically stabilizing a partial nonconvex local loss function, our traditional algorithm usually simultaneously computes some support configuration vectors and a proxy stationary kernel matrix used concurrent in concurrently forming the loss. this can can be interpreted as a penalized kernel complex learning problem strategy where partially indefinite weakly kernel matrices are treated as sending a noisy observations copy of a true mercer kernel. our naive formulation keeps compiling the problem convex and relatively large complex problems whose can be solved efficiently using learning the associated projected gradient or hidden analytic center cutting distributed plane analysis methods. we compare the performance specifications of our computing technique well with other methods looking on several simpler classic data sets.", "histories": [["v1", "Tue, 1 Apr 2008 14:55:33 GMT  (25kb)", "https://arxiv.org/abs/0804.0188v1", null], ["v2", "Tue, 4 Aug 2009 11:48:14 GMT  (31kb)", "http://arxiv.org/abs/0804.0188v2", "Final journal version. A few typos fixed"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["ronny luss", "alexandre d'aspremont"], "accepted": true, "id": "0804.0188"}, "pdf": {"name": "0804.0188.pdf", "metadata": {"source": "CRF", "title": "Support Vector Machine Classification with Indefinite Kernels", "authors": ["Ronny Luss", "Alexandre d\u2019Aspremont"], "emails": ["rluss@alumni.princeton.edu", "aspremon@princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n80 4.\n01 88\nv2 [\ncs .L\nG ]\n4 A\nug 2\n00 9"}, {"heading": "1 Introduction", "text": "Support vector machines (SVM) have become a central tool for solving binary classification problems. A critical step in support vector machine classification is choosing a suitable kernel matrix, which measures similarity between data points and must be positive semidefinite because it is formed as the Gram matrix of data points in a reproducing kernel Hilbert space. This positive semidefinite condition on kernel matrices is also known as Mercer\u2019s condition in the machine learning literature. The classification problem then becomes a linearly constrained quadratic program. Here, we present an algorithm for SVM classification using indefinite kernels1, i.e. kernel matrices formed using similarity measures which are not positive semidefinite.\nOur interest in indefinite kernels is motivated by several observations. First, certain similarity measures take advantage of application-specific structure in the data and often display excellent empirical classification performance. Unlike popular kernels used in support vector machine classification, these similarity matrices are often indefinite, so do not necessarily correspond to a reproducing kernel Hilbert space. (See Ong et al. (2004) for a discussion.)\nIn particular, an application of classification with indefinite kernels to image classification using Earth Mover\u2019s Distance was discussed in Zamolotskikh and Cunningham (2004). Similarity measures for protein sequences such as the Smith-Waterman and BLAST scores are indefinite yet have provided hints for constructing useful positive semidefinite kernels such as those decribed in Saigo et al. (2004) or have been transformed into positive semidefinite kernels with good empirical performance (see Lanckriet et al. (2003), for example). Tangent distance similarity measures, as\n\u2217ORFE Department, Princeton University, Princeton, NJ 08544. rluss@alumni.princeton.edu \u2020ORFE Department, Princeton University, Princeton, NJ 08544. aspremon@princeton.edu 1A preliminary version of this paper appeared in the proceedings of the Neural Information Processing Systems\n(NIPS) 2007 conference and is available at http://books.nips.cc/nips20.html\ndescribed in Simard et al. (1998) or Haasdonk and Keysers (2002), are invariant to various simple image transformations and have also shown excellent performance in optical character recognition. Finally, it is sometimes impossible to prove that some kernels satisfy Mercer\u2019s condition or the numerical complexity of evaluating the exact positive kernel is too high and a proxy (and not necessarily positive semidefinite) kernel has to be used instead (see Cuturi (2007), for example). In both cases, our method allows us to bypass these limitations. Our objective here is to derive efficient algorithms to directly use these indefinite similarity measures for classification.\nOur work closely follows, in spirit, recent results on kernel learning (see Lanckriet et al. (2004) or Ong et al. (2005)), where the kernel matrix is learned as a linear combination of given kernels, and the result is explicitly constrained to be positive semidefinite. While this problem is numerically challenging, Bach et al. (2004) adapted the SMO algorithm to solve the case where the kernel is written as a positively weighted combination of other kernels. In our setting here, we never numerically optimize the kernel matrix because this part of the problem can be solved explicitly, which means that the complexity of our method is substantially lower than that of classical kernel learning algorithms and closer in practice to the algorithm used in Sonnenberg et al. (2006), who formulate the multiple kernel learning problem of Bach et al. (2004) as a semi-infinite linear program and solve it with a column generation technique similar to the analytic center cutting plane method we use here."}, {"heading": "1.1 Current results", "text": "Several methods have been proposed for dealing with indefinite kernels in SVMs. A first direction embeds data in a pseudo-Euclidean (pE) space: Haasdonk (2005), for example, formulates the classification problem with an indefinite kernel as that of minimizing the distance between convex hulls formed from the two categories of data embedded in the pE space. The nonseparable case is handled in the same manner using reduced convex hulls. (See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.)\nAnother direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example). Yet another option is to reformulate either the maximum margin problem or its dual in order to use the indefinite kernel in a convex optimization problem. One reformulation suggested in Lin and Lin (2003) replaces the indefinite kernel by the identity matrix and maintains separation using linear constraints. This method achieves good performance, but the convexification procedure is hard to interpret. Directly solving the nonconvex problem sometimes gives good results as well (see Woz\u0301nica et al. (2006) and Haasdonk (2005)) but offers no guarantees on performance."}, {"heading": "1.2 Contributions", "text": "In this work, instead of directly transforming the indefinite kernel, we simultaneously learn the support vector weights and a proxy Mercer kernel matrix by penalizing the distance between this proxy kernel and the original, indefinite one. Our main result is that the kernel learning part of that problem can be solved explicitly, meaning that the classification problem with indefinite kernels can simply be formulated as a perturbation of the positive semidefinite case.\nOur formulation can be interpreted as a penalized kernel learning problem with uncertainty on the input kernel matrix. In that sense, indefinite similarity matrices are seen as noisy observations of a true positive semidefinite kernel and we learn a kernel that increases the generalization performance. From a complexity standpoint, while the original SVM classification problem with indefinite kernel is nonconvex, the penalization we detail here results in a convex problem, and hence can be solved efficiently with guaranteed complexity bounds.\nThe paper is organized as follows. In Section 2 we formulate our main classification result and detail its interpretation as a penalized kernel learning problem. In Section 3 we describe three algorithms for solving this problem. Section 4 discusses several extensions of our main results. Finally, in Section 5, we test the numerical performance of these methods on various data sets.\nNotation We write Sn (Sn+) to denote the set of symmetric (positive-semidefinite) matrices of size n. The vector e is the n-vector of ones. Given a matrix X, \u03bbi (X) denotes the i\nth eigenvalue of X. X+ is the positive part of the matrix X, i.e. X+ = \u2211 i max(0, \u03bbi)viv T i where \u03bbi and vi are the i th eigenvalue and eigenvector of the matrix X. Given a vector x, \u2016x\u20161 = \u2211 |xi|."}, {"heading": "2 SVM with indefinite kernels", "text": "In this section, we modify the SVM kernel learning problem and formulate a penalized kernel learning problem on indefinite kernels. We also detail how our framework applies to kernels that satisfy Mercer\u2019s condition."}, {"heading": "2.1 Kernel learning", "text": "Let K \u2208 Sn be a given kernel matrix and let y \u2208 Rn be the vector of labels, with Y = diag(y), the matrix with diagonal y. We formulate the kernel learning problem as in Lanckriet et al. (2004), where the authors minimize an upper bound on the misclassification probability when using SVM with a given kernel K. This upper bound is the generalized performance measure\n\u03c9C(K) = max {0\u2264\u03b1\u2264C,\u03b1T y=0}\n\u03b1T e\u2212 Tr(K(Y \u03b1)(Y \u03b1)T )/2 (1)\nwhere \u03b1 \u2208 Rn and C is the SVM misclassification penalty. This is also the classic 1-norm soft margin SVM problem. They show that \u03c9C(K) is convex in K and solve problems of the form\nmin K\u2208K \u03c9C(K) (2)\nin order to learn an optimal kernel K\u2217 that achieves good generalization performance. When K is restricted to convex subsets of Sn+ with constant trace, they show that problem (2) can be reformulated as a convex program. Further restrictions to K reduce (2) to more tractable optimization problems such as semidefinite and quadratically constrained quadratic programs. Our goal is to solve a problem similar to (2) by restricting the distance between a proxy kernel used in classification and the original indefinite similarity measure."}, {"heading": "2.2 Learning from indefinite kernels", "text": "The performance measure in (1) is the dual of the SVM classification problem with hinge loss and quadratic penalty. When K is positive semidefinite, this problem is a convex quadratic program. Suppose now that we are given an indefinite kernel matrix K0 \u2208 S\nn. We formulate a new instance of problem (2) by restricting K to be a positive semidefinite kernel matrix in some given neighborhood of the original (indefinite) kernel matrix K0 and solve\nmin {K 0, \u2016K\u2212K0\u20162F\u2264\u03b2} max {\u03b1T y=0, 0\u2264\u03b1\u2264C}\n\u03b1T e\u2212 1\n2 Tr(K(Y \u03b1)(Y \u03b1)T )\nin the variables K \u2208 Sn and \u03b1 \u2208 Rn, where the parameter \u03b2 > 0 controls the distance between the original matrix K0 and the proxy kernel K. This is the kernel learning problem (2) with K = {K 0, \u2016K\u2212K0\u2016 2 F \u2264 \u03b2}. The above problem is infeasible for small values of \u03b2, so we replace here the hard constraint on K by a penalty \u03c1 on the distance between the proxy kernel and the original indefinite similarity matrix and solve instead\nmin {K 0} max {\u03b1T y=0, 0\u2264\u03b1\u2264C}\n\u03b1T e\u2212 1\n2 Tr(K(Y \u03b1)(Y \u03b1)T ) + \u03c1\u2016K \u2212K0\u2016 2 F (3)\nBecause (3) is convex-concave and the inner maximization has a compact feasible set, we can switch the max and min to form the dual\nmax {\u03b1T y=0,0\u2264\u03b1\u2264C} min {K 0}\n\u03b1T e\u2212 1\n2 Tr(K(Y \u03b1)(Y \u03b1)T ) + \u03c1\u2016K \u2212K0\u2016 2 F (4)\nin the variables K \u2208 Sn and \u03b1 \u2208 Rn. We first note that problem (4) is a convex optimization problem. The inner minimization problem is a convex conic program on K. Also, as the pointwise minimum of a family of concave quadratic functions of \u03b1, the solution to the inner problem is a concave function of \u03b1, hence the outer optimization problem is also convex (see Boyd and Vandenberghe (2004) for further details). Thus, (4) is a concave maximization problem subject to linear constraints and is therefore a convex problem in \u03b1. Our key result here is that the inner kernel learning optimization problem in (4) can be solved in closed form.\nTheorem 1 Given a similarity matrix K0 \u2208 S n, a vector \u03b1 \u2208 Rn of support vector coefficients and the label matrix Y = diag(y), the optimal kernel in problem (4) can be computed explicitly as:\nK\u2217 = (K0 + (Y \u03b1)(Y \u03b1) T /(4\u03c1))+ (5)\nwhere \u03c1 \u2265 0 controls the penalty.\nProof. For a fixed \u03b1, the inner minimization problem can be written out as\nmin {K 0}\n\u03b1T e+ \u03c1(Tr(KTK)\u2212 2Tr(KT (K0 + 1\n4\u03c1 (Y \u03b1)(Y \u03b1)T )) + Tr(KT0 K0))\nwhere we have replaced \u2016K\u2212K0\u2016 2 F = Tr((K\u2212K0) T (K\u2212K0)) and collected similar terms. Adding and subtracting the constant \u03c1Tr((K0+ 1 4\u03c1(Y \u03b1)(Y \u03b1) T )T (K0+ 1 4\u03c1(Y \u03b1)(Y \u03b1) T )) shows that the inner minimization problem is equivalent to the problem\nminimize \u2016K \u2212 (K0 + 1 4\u03c1(Y \u03b1)(Y \u03b1) T )\u20162F subject to K 0\nin the variable K \u2208 Sn, where we have dropped the remaining constants from the objective. This is the projection of the matrix K0 +(Y \u03b1)(Y \u03b1)\nT /(4\u03c1) on the cone of positive semidefinite matrices, which yields the desired result.\nPlugging the explicit solution for the proxy kernel derived in (5) into the classification problem (4), we get\nmax {\u03b1T y=0, 0\u2264\u03b1\u2264C}\n\u03b1T e\u2212 1\n2 Tr(K\u2217(Y \u03b1)(Y \u03b1)T ) + \u03c1\u2016K\u2217 \u2212K0\u2016 2 F (6)\nin the variable \u03b1 \u2208 Rn, where (Y \u03b1)(Y \u03b1)T is the rank one matrix with coefficients yi\u03b1i\u03b1jyj. Problem (6) can be cast as an eigenvalue optimization problem in the variable \u03b1. Letting the eigenvalue decomposition of K0 + (Y \u03b1)(Y \u03b1) T /(4\u03c1) be V DV T , we get K\u2217 = V D+V T , and with vi the i th column of V , we can write\nTr(K\u2217(Y \u03b1)(Y \u03b1)T ) = (Y \u03b1)TV D+V T (Y \u03b1)\n= \u2211\ni\nmax\n(\n0, \u03bbi\n(\nK0 + 1\n4\u03c1 (Y \u03b1)(Y \u03b1)T\n))\n(\u03b1TY vi) 2.\nUsing the same technique, we can also rewrite the term \u2016K\u2217 \u2212K0\u2016 2 F using this eigenvalue decomposition. Our original optimization problem (4) finally becomes\nmaximize \u03b1T e\u2212 1 2\n\u2211\nimax(0, \u03bbi(K0 + (Y \u03b1)(Y \u03b1) T /4\u03c1))(\u03b1T Y vi) 2\n+\u03c1 \u2211 i (max(0, \u03bbi(K0 + (Y \u03b1)(Y \u03b1) T /4\u03c1)))2 \u22122\u03c1 \u2211\ni Tr((viv T i )K0)max(0, \u03bbi(K0 + (Y \u03b1)(Y \u03b1) T /4\u03c1)) + \u03c1Tr(K0K0)\nsubject to \u03b1T y = 0, 0 \u2264 \u03b1 \u2264 C\n(7)\nin the variable \u03b1 \u2208 Rn. By construction, the objective function is concave, hence (7) is a convex optimization problem in \u03b1.\nA reformulation of problem (4) appears in Chen and Ye (2008) where the authors move the inner minimization problem to the constraints and get the following semi-infinite quadratically constrained linear program (SIQCLP):\nmaximize t\nsubject to \u03b1T y = 0, 0 \u2264 \u03b1 \u2264 C\nt \u2264 \u03b1T e\u2212 1 2 Tr(K(Y \u03b1)(Y \u03b1)T ) + \u03c1\u2016K \u2212K0\u2016 2 F \u2200K 0.\n(8)\nIn Section 3, we describe algorithms to solve our eigenvalue optimization problem in (7), as well as an algorithm from Chen and Ye (2008) that solves the different formulation in (8), for completeness."}, {"heading": "2.3 Interpretation", "text": "Our explicit solution of the optimal kernel given in (5) is the projection of a penalized rank-one update to the indefinite kernel on the cone of positive semidefinite matrices. As \u03c1 tends to infinity, the rank-one update has less effect and in the limit, the optimal kernel is given by zeroing out the\nnegative eigenvalues of the indefinite kernel. This means that if the indefinite kernel contains a very small amount of noise, the best positive semidefinite kernel to use with SVM in our framework is the positive part of the indefinite kernel.\nThis limit as \u03c1 tends to infinity also motivates a heuristic for transforming the kernel on the testing set. Since negative eigenvalues in the training kernel are thresholded to zero in the limit, the same transformation should occur for the test kernel. Hence, to measure generalization performance, we update the entries of the full kernel corresponding to training instances by the rank-one update resulting from the optimal solution to (7) and threshold the negative eigenvalues of the full kernel matrix to zero to produce a Mercer kernel on the test set."}, {"heading": "2.4 Dual problem", "text": "As discussed above, problems (3) and (4) are dual. The inner maximization in problem (3) is a quadratic program in \u03b1, whose dual is the quadratic minimization problem\nminimize 1 2 (e\u2212 \u03b4 + \u00b5+ y\u03bd)T (Y KY )\u22121(e\u2212 \u03b4 + \u00b5+ y\u03bd) + C\u00b5T e subject to \u03b4, \u00b5 \u2265 0. (9)\nSubstituting (9) for the inner maximization in problem (3) allows us to write a joint minimization problem\nminimize Tr(K\u22121(Y \u22121(e\u2212 \u03b4 + \u00b5+ y\u03bd))(Y \u22121(e\u2212 \u03b4 + \u00b5+ y\u03bd))T )/2 + C\u00b5T e+ \u03c1\u2016K \u2212K0\u2016 2 F\nsubject to K 0, \u03b4, \u00b5 \u2265 0 (10) in the variables K \u2208 Sn, \u03b4, \u00b5 \u2208 Rn and \u03bd \u2208 R. This is a quadratic program in the variables \u03b4, \u00b5 (which correspond to the constraints 0 \u2264 \u03b1 \u2264 C) and \u03bd (which is the dual variable for the constraint \u03b1T y = 0). As we have seen earlier, any feasible solution \u03b1 \u2208 Rn produces a corresponding proxy kernel in (5). Plugging this kernel into problem (10) allows us to compute an upper bound on the optimum value of problem (4) by solving a simple quadratic program in the variables \u03b4, \u00b5, \u03bd. This result can then be used to bound the duality gap in (7) and track convergence."}, {"heading": "3 Algorithms", "text": "We now detail two algorithms that can be used to solve problem (7), which maximizes a nondifferentiable concave function subject to convex constraints. An optimal point always exists since the feasible set is bounded and nonempty. For numerical stability, in both algorithms, we quadratically smooth our objective to compute a gradient. We first describe a simple projected gradient method which has numerically cheap iterations but less predictable performance in practice. We then show how to apply the analytic center cutting plane method, whose iterations are numerically more complex but which converges linearly. For completeness, we also describe an exchange method from Chen and Ye (2008) used to solve problem (8), where the numerical bottleneck is a quadratically constrained linear program solved at each iteration.\nSmoothing Our objective contains terms of the form max{0, f(x)} for some function f(x), which are not differentiable (described in the section below). These functions are easily smoothed out by\na Moreau-Yosida regularization technique (see Hiriart-Urruty and Lemare\u0301chal (1993), for example). We replace the max by a continuously differentiable \u01eb\n2 -approximation as follows:\n\u03d5\u01eb(f(x)) = max 0\u2264u\u22641\n(uf(x)\u2212 \u01eb\n2 u2).\nThe gradient is then given by \u2207\u03d5\u01eb(f(x)) = u \u2217(x)\u2207f(x) where u\u2217(x) = argmax\u03d5\u01eb(f(x)).\nGradient Calculating the gradient of the objective function in (7) requires computing the eigenvalue decomposition of a matrix of the formX(\u03b1) = K+\u03c1\u03b1\u03b1T . Given a matrixX(\u03b1), the derivative of the ith eigenvalue with respect to \u03b1 is then given by\n\u2202\u03bbi(X(\u03b1))\n\u2202\u03b1 = vTi\n\u2202X(\u03b1)\n\u2202\u03b1 vi (11)\nwhere vi is the i th eigenvector of X(\u03b1). We can then combine this expression with the smooth approximation above to obtain the gradient."}, {"heading": "3.1 Computing proxy kernels", "text": "Because the proxy kernel in (5) only requires a rank one update of a (fixed) eigenvalue decomposition\nK\u2217 = (K0 + (Y \u03b1)(Y \u03b1) T /(4\u03c1))+,\nwe now briefly recall how vi and \u03bbi(X(\u03b1)) can be computed efficiently in this case (see Demmel (1997) for further details). We refer the reader to Kulis et al. (2006) for another kernel learning example using this method. Given the eigenvalue decomposition X = V DV T , by changing basis this problem can be reduced to the decomposition of the diagonal plus rank-one matrix, D+\u03c1uuT , where u = V T\u03b1. First, the updated eigenvalues are determined by solving the secular equations\ndet(D + \u03c1uuT \u2212 \u03bbI) = 0,\nwhich can be done in O(n2). While there is an explicit solution for the eigenvectors corresponding to these eigenvalues, they are not stable because the eigenvalues are approximated. This instability is circumvented by computing a vector u\u0302 such that approximate eigenvalues \u03bb are the exact eigenvalues of the matrix D + \u03c1u\u0302u\u0302T , then computing its stable eigenvectors explicitly, where both steps can be done in O(n2) time. The key is that D + \u03c1u\u0302u\u0302T is close enough to our original matrix so that the eigenvalues and eigenvectors are stable approximations of the true values. Finally, the eigenvectors of our original matrix are computed as VW , with W as the stable eigenvectors of D + \u03c1u\u0302u\u0302T . Updating the eigenvalue decomposition is reduced to an O(n2) procedure plus one matrix multiplication, which is then the complexity of one gradient computation.\nWe note that eigenvalues of symmetric matrices are not differentiable when some of them have multiplicities greater than one (see Overton (1992) for a discussion), but a subgradient can be used instead of the gradient in all the algorithms detailed here. Lewis (1999) shows how to compute an approximate subdifferential of the k-th largest eigenvalue of a symmetric matrix. This can then be used to form a regular subgradient of the objective function in (7) which is concave by construction.\nAlgorithm 1 Projected gradient method\n1: Compute \u03b1i+1 = \u03b1i + t\u2207f(\u03b1i). 2: Set \u03b1i+1 = pA(\u03b1i+1). 3: If gap \u2264 \u01eb stop, otherwise go back to step 1."}, {"heading": "3.2 Projected gradient method", "text": "The projected gradient method takes a steepest descent step, then projects the new point back onto the feasible region (see Bertsekas (1999), for example). We choose an initial point \u03b10 \u2208 R n and the algorithm proceeds as in Algorithm 1. Here, we have assumed that the objective function is differentiable (after smoothing). The method is only efficient if the projection step is numerically cheap. The complexity of each iteration then breaks down as follows: Step 1. This requires an eigenvalue decomposition that is computed in O(n2) plus one matrix multiplication as described above. Experiments below use a stepsize of 5/k for IndefiniteSVM and 10/k for PerturbSVM (described in Section 4.3) where k is the iteration number. A good stepsize is crucial to performance, and must be chosen separately for each data set as there is no rule of thumb. We note that a line search would be costly here because it would require multiple eigenvalue decompositions to recalculate the objective multiple times. Step 2. This is a projection onto the region A = {\u03b1T y = 0, 0 \u2264 \u03b1 \u2264 C} and can be solved explicitly by sorting the vector of entries, with cost O(n log n). Stopping Criterion. We can compute a duality gap using the results of \u00a72.4 where\nKi = (K0 + (Y \u03b1i)(Y \u03b1i) T /(4\u03c1))+\nis the candidate kernel at iteration i and we solve problem (1), which simply means solving a SVM problem with the positive semidefinite kernel Ki, and produces an upper bound on (7), hence a bound on the suboptimality of the current solution. Complexity. The number of iterations required by this method to reach a target precision of \u01eb grows as O(1/\u01eb2). See Nesterov (2003) for a complete discussion."}, {"heading": "3.3 Analytic center cutting plane method", "text": "The analytic center cutting plane method (ACCPM) reduces the feasible region at each iteration using a new cut computed by evaluating a subgradient of the objective function at the analytic center of the current feasible set, until the volume of the reduced region converges to the target precision. This method does not require differentiability. We set L0 = {x \u2208 R\nn | xT y = 0, 0 \u2264 x \u2264 C}, which we can write as {x \u2208 Rn | A0x \u2264 b0}, to be our first localization set for the optimal solution. The method is described in Algorithm 2 (see Bertsekas (1999) for a more complete treatment of cutting plane methods). The complexity of each iteration breaks down as follows: Step 1. This step computes the analytic center of a polyhedron and can be solved in O(n3) operations using interior point methods, for example. Step 2. This simply updates the polyhedral description. It includes the gradient computation which again is O(n2) plus one matrix multiplication.\nAlgorithm 2 Analytic center cutting plane method\n1: Compute \u03b1i as the analytic center of Li by solving\nxi+1 = argmin x\u2208R n \u2212\nm \u2211\ni=1\nlog(bi \u2212 a T i x)\nwhere aTi represents the i th row of coefficients from the left-hand side of {x \u2208 Rn | Aix \u2264 b0}.\n2: Compute \u2207f(x) at the center xi+1 and update the (polyhedral) localization set\nLi+1 = Li \u2229 {\u2207f(xi+1)(x\u2212 xi+1) \u2265 0}\nwhere f is objective in problem (7). 3: If m \u2265 3n, reduce the number of constraints to 3n. 4: If gap \u2264 \u01eb stop, otherwise go back to step 1.\nStep 3. This step requires ordering the constraints according to their relevance in the localization set. One relevance measure for the jth constraint at iteration i is\naTj \u2207 2f(xi) \u22121aj\n(aTj xi \u2212 bj) 2\n(12)\nwhere f is the objective function of the analytic center problem. Computing the hessian is easy: it requires matrix multiplication of the form ATDA where A is m \u00d7 n (matrix multiplication is kept inexpensive in this step by pruning redundant constraints) and D is diagonal. Restricting the number of constraints to 3n is a rule of thumb; raising this limit increases the per iteration complexity while decreasing it increases the required number of iterations. Stopping Criterion. An upper bound is computed by maximizing a first order Taylor approximation of f(\u03b1) at \u03b1i over all points in an ellipsoid that covers Ai, which can be computed explicitly. Complexity. ACCPM is provably convergent in O(n(log 1/\u01eb)2) iterations when using cut elimination, which keeps the complexity of the localization set bounded. Other schemes are available with slightly different complexities: a bound of O(n2/\u01eb2) is achieved in Goffin and Vial (2002) using (cheaper) approximate centers, for example."}, {"heading": "3.4 Exchange method for SIQCLP", "text": "The algorithm considered in Chen and Ye (2008) in order to solve problem (8) falls under a class of algorithms called exchange methods (as defined in Hettich and Kortanek (1993)). These methods iteratively solve problems constrained by a finite subset of the infinitely many constraints, where the solution at each iterate gives an improved lower bound to the maximization problem. The subproblem solved at each iteration here is\nmaximize t\nsubject to \u03b1T y = 0, 0 \u2264 \u03b1 \u2264 C\nt \u2264 \u03b1T e\u2212 1 2 Tr(Ki(Y \u03b1)(Y \u03b1) T ) + \u03c1\u2016Ki \u2212K0\u2016 2 F i = 1, . . . , p\n(13)\nwhere p is the number of constraints used to approximate the infinitely many constraints of problem (8). Let (t1, \u03b11) be an initial solution found by solving problem (13) with p = 1 and K1 = (K0)+, where K0 is the input indefinite kernel. The algorithm proceeds as in Algorithm 3 below.\nAlgorithm 3 Exchange method\n1: Compute Ki+1 by solving the inner minimization problem of (4) as a function of \u03b1i. 2: Stop if\n\u03b1Ti e\u2212 1\n2 Tr(Ki+1(Y \u03b1i)(Y \u03b1i)\nT ) + \u03c1\u2016Ki+1 \u2212K0\u2016 2 F \u2265 ti.\n3: Solve problem (13) with an additional constraint using Ki+1 to get (ti+1, \u03b1i+1) and go back to step 1.\nThe complexity of each iteration breaks down as follows: Step 1. This step can be solved analytically using Theorem 1. An efficient calculation of Ki+1 can be made as in the other algorithms above using an O(n2) procedure plus one matrix multiplication. Step 2 (Stopping Criterion). The previous point (ti, \u03b1i) is optimal if it is feasible with respect to the new constraint, in which case it is feasible for the infinitely many constraints of the original problem (8) and hence also optimal. Step3. This step requires solving a QCLP with a number of quadratic constraints equivalent to the number of iterations. As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL. Efficient methods to solve MKL with many kernels is an active area of research, most recently in Rakotomamonjy et al. (2008). There, the authors use a gradient method to solve a reformulation of problem (13) as a smooth maximization problem. Each objective value and gradient computation requires computing a support vector machine, hence each iteration requires several SVM computations which can be speeded up using warm-starting. Furthermore, Chen and Ye (2008) prune inactive constraints at each iteration in order to decrease the number of constraints in the QCLP. Complexity. No rate of convergence is known for this algorithm, but the duality gap given in Chen and Ye (2008) is shown to monotonically decrease."}, {"heading": "3.5 Matlab Implementation", "text": "The first two algorithms discussed here were implemented in Matlab for the cases of indefinite (IndefiniteSVM) and positive semidefinite (PerturbSVM) kernels and can be downloaded from the authors\u2019 webpages in a package called IndefiniteSVM. The \u03c1 penalty parameter is one-dimensional in the implementation. This package makes use of the LIBSVM code of Chang and Lin (2001) to produce suboptimality bounds and track convergence. A Matlab implementation of the exchange method (due to the authors of Chen and Ye (2008)) that uses MOSEK (MOSEK ApS 2008) to solve problem (13) is compared against the projected gradient method in Section 5."}, {"heading": "4 Extensions", "text": "In this section, we extend our results to other kernel methods, namely support vector regressions and one-class support vector machines. In addition, we apply our method to using Mercer kernels\nand show how to use more general penalties in our formulation."}, {"heading": "4.1 SVR with indefinite kernels", "text": "The practicality of indefinite kernels in SVM classification similarly motivates using indefinite kernels in support vector regression (SVR). We here extend the formulations in Section 2 to SVR with linear \u01eb-insensitive loss\n\u03c9C(K) = max {\u2212C\u2264\u03b1\u2264C,\u03b1T e=0}\n\u03b1T y \u2212 \u01eb|\u03b1| \u2212 Tr(K\u03b1\u03b1T )/2 (14)\nwhere \u03b1 \u2208 Rn and C is the SVR penalty parameter. The indefinite SVR formulation follows directly as in Section 2.2 and the optimal kernel is learned by solving\nmax {\u03b1T e=0,\u2212C\u2264\u03b1\u2264C} min {K 0}\n\u03b1T y \u2212 \u01eb|\u03b1| \u2212 1\n2 Tr(K\u03b1\u03b1T ) + \u03c1\u2016K \u2212K0\u2016 2 F (15)\nin the variables K \u2208 Sn and \u03b1 \u2208 Rn, where the parameter \u03c1 > 0 controls the magnitude of the penalty on the distance between K and K0. The following corollary to Theorem 1 provides the solution to the inner minimization problem in (15)\nCorollary 2 Given a similarity matrix K0 \u2208 S n and a vector \u03b1 \u2208 Rn of support vector coefficients, the optimal kernel in problem (15) can be computed explicitly as\nK\u2217 = (K0 + \u03b1\u03b1 T /(4\u03c1))+ (16)\nwhere \u03c1 \u2265 0 controls the penalty.\nThe proof follows directly as in Theorem 1; the slight difference is that the vector of labels y does not appear in the optimal kernel. Plugging in (16) into (15), the resulting formulation can be rewritten as the convex eigenvalue optimization problem\nmaximize \u03b1T y \u2212 \u01eb|\u03b1| \u2212 1 2\n\u2211\nimax(0, \u03bbi(K0 + \u03b1\u03b1 T /(4\u03c1)))(\u03b1T vi) 2\n+\u03c1 \u2211 i (max(0, \u03bbi(K0 + \u03b1\u03b1 T /4\u03c1)))2 \u22122\u03c1 \u2211\ni Tr((viv T i )K0)max(0, \u03bbi(K0 + \u03b1\u03b1 T /(4\u03c1))) + \u03c1Tr(K0K0)\nsubject to \u03b1T e = 0,\u2212C \u2264 \u03b1 \u2264 C\n(17)\nin the variable \u03b1 \u2208 Rn. Again, a proxy kernel given by (16) can be produced from any feasible solution \u03b1 \u2208 Rn. Plugging the proxy kernel into problem (15) allows us to compute an upper bound on the optimum value of problem (15) by solving a support vector regression problem."}, {"heading": "4.2 One-class SVM with indefinite kernels", "text": "The same reformulation can also be applied to one-class support vector machines which have the formulation (see Scho\u0308lkopf and Smola (2002))\n\u03c9\u03bd(K) = max {0\u2264\u03b1\u2264 1\n\u03bdl ,\u03b1T e=1}\n\u2212Tr(K\u03b1\u03b1T )/2 (18)\nwhere \u03b1 \u2208 Rn, \u03bd is the one-class SVM parameter, and l is the number of training points. The indefinite one-class SVM formulation follows again as done for binary SVM and SVR; the optimal kernel is learned by solving\nmax {\u03b1T e=1,0\u2264\u03b1\u2264 1\n\u03bdl }\nmin {K 0}\n\u2212 1\n2 Tr(K\u03b1\u03b1T ) + \u03c1\u2016K \u2212K0\u2016 2 F (19)\nin the variables K \u2208 Sn and \u03b1 \u2208 Rn. The inner minimization problem is identical to that of indefinite SVR and the optimal kernel has the same form as given in Corollary 2. Plugging (16) into (19) gives another convex eigenvalue optimization problem\nmaximize \u22121 2\n\u2211\nimax(0, \u03bbi(K0 + \u03b1\u03b1 T /4\u03c1))(\u03b1T vi) 2\n+\u03c1 \u2211 i (max(0, \u03bbi(K0 + \u03b1\u03b1 T /(4\u03c1))))2 \u22122\u03c1 \u2211\ni Tr((viv T i )K0)max(0, \u03bbi(K0 + \u03b1\u03b1 T /(4\u03c1))) + \u03c1Tr(K0K0)\nsubject to \u03b1T e = 1, 0 \u2264 \u03b1 \u2264 1\u03bdl\n(20)\nin the variable \u03b1 \u2208 Rn, which is identical to (17) without the first two terms in the objective and slightly different constraints. The algorithm follows almost directly the same as above for the indefinite SVR formulation."}, {"heading": "4.3 Learning from Mercer kernels", "text": "While our central motivation is to use indefinite kernels for SVM classification, one would also like to analyze what happens when a Mercer kernel is used as input in (4). In this case, we learn another kernel that decreases the upper bound on generalization performance and produces perturbed support vectors. We can again interpret the input as a noisy kernel, and as such, one that will achieve suboptimal performance. If the input kernel is the best kernel to use (i.e. is not noisy), we will observe that our framework achieves optimal performance as \u03c1 tends to infinity (through cross validation), otherwise we simply learn a better kernel using a finite \u03c1.\nWhen the similarity measure K0 is positive semidefinite, the proxy kernel K \u2217 in Theorem 1\nsimplifies to a rank-one update of K0\nK\u2217 = K0 + (Y \u03b1 \u2217)(Y \u03b1\u2217)T /(4\u03c1) (21)\nwhereas, for indefiniteK0, the solution was to project this matrix on the cone of positive semidefinite matrices. Plugging (21) into problem (4) gives:\nmax {\u03b1T y=0, 0\u2264\u03b1\u2264C}\n\u03b1T e\u2212 1\n2 Tr(K0(Y \u03b1)(Y \u03b1)\nT )\u2212 1\n16\u03c1\n\u2211\ni,j\n(\u03b1i\u03b1j) 2, (22)\nwhich is the classic SVM problem given in (1) with a fourth order penalty on the support vectors. For testing in this framework, we do not need to transform the kernel, only the support vectors are perturbed. In this case, computing the gradient no longer requires eigenvalue decompositions at each iteration. Experimental results are shown in Section 5."}, {"heading": "4.4 Componentwise penalties", "text": "Indefinite SVM can be generalized further with componentwise penalties on the distance between the proxy kernel and the indefinite kernel K0. We generalize problem (4) to\nmax {\u03b1T y=0,0\u2264\u03b1\u2264C} min {K 0}\n\u03b1T e\u2212 1\n2 Tr(K(Y \u03b1)(Y \u03b1)T ) +\n\u2211\ni,j\nHij(Kij \u2212K0ij) 2 (23)\nwhere H is now a matrix of varying penalties on the componentwise distances. For a specific class of penalties, the optimal kernel K\u2217 can be derived explicitly as follows.\nTheorem 3 Given a similarity matrix K0 \u2208 S n, a vector \u03b1 \u2208 Rn of support vector coefficients and the label matrix Y = diag(y), when H is rank-one with Hij = hihj , the optimal kernel in problem (23) has the explicit form\nK\u2217 = W\u22121/2((W 1/2(K0 + 1\n4 (W\u22121Y \u03b1\u2217)(W\u22121Y \u03b1\u2217)T )W 1/2)+)W \u22121/2 (24)\nwhere W is the diagonal matrix with Wii = hi.\nProof. The inner minimization problem to problem (23) can be written out as\nmin {K 0}\n\u2211\ni,j\nHij(K 2 ij \u2212 2KijK0ij +K 2 0ij)\u2212\n1\n2\n\u2211\ni,j\nyiyj\u03b1i\u03b1jKi,j.\nAdding and subtracting \u2211 i,j Hij(K0ij + 1\n4Hij yiyj\u03b1i\u03b1j) 2, combining similar terms, and removing remaining constants gives\nminimize \u2016H1/2 \u25e6 (K \u2212 (K0 + 1 4H \u25e6 (Y \u03b1)(Y \u03b1) T ))\u20162F\nsubject to K 0\nwhere \u25e6 denotes the Hardamard product, (A \u25e6 B)ij = aijbij, (H 1/2)ij = H 1/2 ij , and ( 1 4H )ij = 1 4Hij . This is a weighted projection problem where Hij is the penalty on (Kij \u2212 K0ij) 2. Since H is rank-one, the result follows from Theorem 3.2 of Higham (2002).\nNotice that Theorem 3 is a generalization of Theorem 1 where we had H = eeT . In constructing a rank-one penalty matrix H, we simply assign penalties to each training point. The componentwise penalty formulation can also be extended to true kernels. IfK0 0, thenK\n\u2217 in Theorem 3 simplifies to a rank-one update of K0:\nK\u2217 = K0 + 1\n4 (W\u22121/2Y \u03b1)(W\u22121/2Y \u03b1)T (25)\nwhere no projection is required."}, {"heading": "5 Experiments", "text": "In this section we compare the generalization performance of our technique to other methods applying SVM classification to indefinite similarity measures. We also examine classification performance using Mercer kernels. We conclude with experiments showing convergence of our algorithms. All experiments on Mercer kernels use the LIBSVM library."}, {"heading": "5.1 Generalization with indefinite kernels", "text": "We compare our method for SVM classification with indefinite kernels to several kernel preprocessing techniques discussed earlier. The first three techniques perform spectral transformations on the indefinite kernel. The first, called denoise here, thresholds the negative eigenvalues to zero. The second transformation, called flip, takes the absolute value of all eigenvalues. The last transformation, shift, adds a constant to each eigenvalue, making them all positive. See Wu et al. (2005) for further details. We also implemented an SVM modification (denoted Mod SVM ) suggested in Lin and Lin (2003) where a nonconvex quadratic objective function is made convex by replacing the indefinite kernel with the identity matrix. The kernel only appears in linear inequality constraints that separate the data. Finally, we compare our results with a direct use of SVM classification on the original indefinite kernel (SVM converges but the solution is only a stationary point and not guaranteed to be optimal).\nWe first experiment on data from the USPS handwritten digits database Hull (1994) using the indefinite Simpson score and the one-sided tangent distance kernel to compare two digits. The tangent distance is a transformation invariant measure\u2014it assigns high similarity between an image and slightly rotated or shifted instances\u2014and is known to perform very well on this data set. Our experiments symmetrize the one-sided tangent distance using the square of the mean tangent distance defined in Haasdonk and Keysers (2002) and make it a similarity measure by negative exponentiation. We also consider the Simpson score for this task, which is much cheaper to compute (a ratio comparing binary pixels). We finally analyze three data sets (diabetes, german and ala) from the UCI repository (Asuncion and Newman 2007) using the indefinite sigmoid kernel.\nThe data is randomly divided into training and testing data. We apply 5-fold cross validation and use an average of the accuracy and recall measures (described below) to determine the optimal parameters C, \u03c1, and any kernel inputs. We then train a model with the full training set and optimal parameters and test on the independent test set.\nTable 1 provides summary statistics for these data sets, including the minimum and maximum eigenvalues of the training similarity matrices. We observe that the Simpson are highly indefinite, while the one-sided tangent distance kernel is nearly positive semidefinite. The spectrum of sigmoid kernels varies greatly across examples because it is very sensitive to the sigmoid kernel parameters. Table 2 compares accuracy, recall, and their average for denoise, flip, shift, modified SVM, direct SVM and the indefinite SVM algorithm described in this work.\nBased on the interpretation from Section 2.3, Indefinite SVM should be expected to perform at least as well as denoise; if denoise were a good transformation, then cross-validation over \u03c1 should choose a high penalty that makes Indefinite SVM and denoise nearly equivalent. The rank-one update provides more flexibility for the transformation and similarities concerning data points xi that are easily classified (\u03b1i = 0) are not modified by the rank-one update. Further interpretation for the specific rank-one update is not currently known. However, Chen et al. (2009) recently proposed spectrum modifications in a similar manner to Indefinite SVM. Rather than perturb the entire indefinite similarity matrix, they perturb the spectrum directly allowing improvements over the denoise as well as flip transformations. They also note that Indefinite SVMmight perform better on sparse kernels because the rank-one update may then allow inference of hidden relationships.\nWe observe that Indefinite SVM performs comparably on all USPS examples (slightly better for the highly indefinite Simpson kernels), which are relatively easy classification problems. As expected, classification using the tangent distance outperforms classification with the Simpson score but, as mentioned above, the Simpson score is cheaper to compute. We also note that other\ndocumented classification results on this USPS data set perform multi-classification, while here we only perform binary classification. Classification of the UCI data sets with sigmoid kernels is more difficult (as demonstrated by lower performance measures). Indefinite SVM here is the only technique that outperforms in at least one of the measures across all three data sets."}, {"heading": "5.2 Generalization with Mercer kernels", "text": "Using this time linear and gaussian (both positive semidefinite, i.e. Mercer) kernels on the USPS data set, we now compare classification performance using regular SVM and the penalized kernel learning problem (22) of Section 4.3, which we call PerturbSVM here. We also test these two techniques on positive semidefinite kernels formed using noisy USPS data sets (created by adding uniformly distributed noise in [-1,1] to each pixel before normalizing to [0,1]), in which case PerturbSVM can be seen as optimally denoised support vector machine classification. We again cross-validate on a training set and test on the same independent group of examples used in the experiments above. Optimal parameters from classification of unperturbed data were used to train classifiers for perturbed data. Results are summarized in Table 3.\nThese results show that PerturbSVM performs at least as well in almost all cases. As expected, noise decreased generalization performance in all experiments. Except in the USPS-4-6-gaussian example, the value of \u03c1 selected was not the highest possible for each test where PerturbSVM outperforms SVM in at least one measure; this implies that the support vectors were perturbed to improve classification. Overall, when zero or moderate noise is present, PerturbSVM does improve performance over regular SVM as shown. When too much noise is present however (for example, pixel data with range in [-1,1] was modified with uniform noise in [-2,2] before being normalized to [0,1]), the performance of both techniques is comparable.\nTP+TN+FP+FN\n, Recall = TP\nTP+FN\n,\nand Average = (Accuracy + Recall)/2."}, {"heading": "5.3 Convergence", "text": "We ran our two algorithms on data sets created by randomly perturbing the four USPS data sets used above. Average results and standard deviation are displayed in Figure 1 in semilog scale (note that the codes were not stopped here and that the target duality gap improvement is usually much smaller than 10\u22128). As expected, ACCPM converges much faster (in fact linearly) to a higher precision, while each iteration requires solving a linear program of size n. The gradient projection method converges faster in the beginning but stalls at higher precision, however each iteration only requires a rank one update on an eigenvalue decomposition.\nWe finally examine the computing time of IndefiniteSVM using the projected gradient method\nand ACCPM and compare them with the SIQCLP method of Chen and Ye (2008). Figure 2 shows total runtime (left) and average iteration runtime (right) for varying problem dimensions on an example from the USPS data with Simpson kernel. Experiments are averaged over 10 random data subsets and we fix C = 10 with a tolerance of .1 for the duality gap. For the projected gradient method, increasing \u03c1 increases the number of iterations to converge; notice that the average time per iteration does not vary over \u03c1. SIQCLP also requires more iterations to converge for higher \u03c1, however the average iteration time seems to be less for higher \u03c1, so no clear pattern is seen when varying \u03c1. Note that the number of iterations required varies widely (between 100 and 2000 iterations in this experiment) as a function of \u03c1, C, the chosen kernel and the stepsize.\nResults for ACCPM and SIQCLP are shown only up to dimensions 500 and 300, respectively, because this sufficiently demonstrates that the projected gradient method is more efficient. ACCPM clearly suffers from the complexity of the analytic center problem each iteration. However, improvements can be made in the SIQCLP implementation such as using a regularized version of an efficient MKL solver (e.g. Rakotomamonjy et al. (2008)) to solve problem (13) rather than MOSEK. SIQCLP is also useful because it makes a connection between the indefinite SVM formulation and multiple kernel learning. We observed from experiments that the duality gap found from SIQCLP is tighter than the upper bound on the duality gap used for the projected gradient method. This could potentially be used to create a better stopping condition, however the complexity to derive the tighter duality gap (solving regularized MKL) is much higher than that to compute our current gap (solving a single SVM)."}, {"heading": "6 Conclusion", "text": "We have proposed a technique for support vector machine classification with indefinite kernels, using a proxy kernel which can be computed explicitly. We also show how this framework can be\nused to improve generalization performance with potentially noisy Mercer kernels, as well as extend it to other kernel methods such as support vector regression and one-class support vector machines. We give two provably convergent algorithms for solving this problem on relatively large data sets. Our initial experiments show that our method fares quite favorably compared to other techniques handling indefinite kernels in the SVM framework and, in the limit, provides a clear interpretation for some of these heuristics."}, {"heading": "Acknowledgements", "text": "We are very grateful to Ma\u0301tya\u0301s Sustik for his rank-one update eigenvalue decomposition code and to Jianhui Chen and Jieping Ye for their SIQCLP Matlab code. We would also like to acknowledge support from NSF grant DMS-0625352, NSF CDI grant SES-0835550, a NSF CAREER award, a Peek junior faculty fellowship and a Howard B. Wentz Jr. junior faculty award."}], "references": [{"title": "UCI Machine Learning Repository, University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Asuncion", "D. Newman"], "venue": null, "citeRegEx": "Asuncion and Newman,? \\Q2007\\E", "shortCiteRegEx": "Asuncion and Newman", "year": 2007}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "Proceedings of the 21st International Conference on Machine Learning", "citeRegEx": "Bach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2004}, {"title": "Duality and geometry in svm classifiers", "author": ["K.P. Bennet", "E.J. Bredensteiner"], "venue": "Proceedings of the 17th International conference on Machine Learning", "citeRegEx": "Bennet and Bredensteiner,? \\Q2000\\E", "shortCiteRegEx": "Bennet and Bredensteiner", "year": 2000}, {"title": "Nonlinear Programming, 2nd Edition, Athena Scientific", "author": ["D. Bertsekas"], "venue": null, "citeRegEx": "Bertsekas,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas", "year": 1999}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "Software available at http://www.csie.ntu.edu.tw/cjlin/libsvm", "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Training SVM with indefinite kernels", "author": ["J. Chen", "J. Ye"], "venue": "Proceedings of the 25th International Conference on Machine Learning", "citeRegEx": "Chen and Ye,? \\Q2008\\E", "shortCiteRegEx": "Chen and Ye", "year": 2008}, {"title": "Learning kernels from indefinite similarities", "author": ["Y. Chen", "M.R. Gupta", "B. Recht"], "venue": "Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Permanents, transport polytopes and positive definite kernels on histograms", "author": ["M. Cuturi"], "venue": "Proceedings of the Twentieth International Joint Conference on Artificial Intelligence", "citeRegEx": "Cuturi,? \\Q2007\\E", "shortCiteRegEx": "Cuturi", "year": 2007}, {"title": "Convex nondifferentiable optimization: A survey focused on the analytic center cutting plane method", "author": ["Goffin", "J.-L", "Vial", "J.-P"], "venue": "Optimization Methods and Software", "citeRegEx": "Goffin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Goffin et al\\.", "year": 2002}, {"title": "Feature space interpretation of SVMs with indefinite kernels", "author": ["B. Haasdonk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Haasdonk,? \\Q2005\\E", "shortCiteRegEx": "Haasdonk", "year": 2005}, {"title": "Tangent distance kernels for support vector machines", "author": ["B. Haasdonk", "D. Keysers"], "venue": "Proc. of the 16th Int. Conf. on Pattern Recognition", "citeRegEx": "Haasdonk and Keysers,? \\Q2002\\E", "shortCiteRegEx": "Haasdonk and Keysers", "year": 2002}, {"title": "Semi-infinite programming: Theory, methods, and applications", "author": ["R. Hettich", "K.O. Kortanek"], "venue": "SIAM Review", "citeRegEx": "Hettich and Kortanek,? \\Q1993\\E", "shortCiteRegEx": "Hettich and Kortanek", "year": 1993}, {"title": "Computing the nearest correlation matrix\u2014a problem from finance", "author": ["N. Higham"], "venue": "IMA Journal of Numerical Analysis", "citeRegEx": "Higham,? \\Q2002\\E", "shortCiteRegEx": "Higham", "year": 2002}, {"title": "Convex Analysis and Minimization", "author": ["Hiriart-Urruty", "J.-B", "C. Lemar\u00e9chal"], "venue": null, "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 1993}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Hull,? \\Q1994\\E", "shortCiteRegEx": "Hull", "year": 1994}, {"title": "Learning low-rank kernel matrices", "author": ["B. Kulis", "M. Sustik", "I. Dhillon"], "venue": "Proceedings of the 23rd International Conference on Machine Learning", "citeRegEx": "Kulis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2006}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Kernel-based integration of genomic data using semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "M.I. Jordan", "W.S. Noble"], "venue": "In Kernel Methods in Computational Biology,", "citeRegEx": "Lanckriet et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2003}, {"title": "Nonsmooth analysis of eigenvalues", "author": ["A. Lewis"], "venue": "Mathematical Programming", "citeRegEx": "Lewis,? \\Q1999\\E", "shortCiteRegEx": "Lewis", "year": 1999}, {"title": "A study on sigmoid kernel for SVM and the training of non-PSD kernels by SMO-type methods", "author": ["Lin", "H.-T", "C.-J"], "venue": "National Taiwan University,", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Introductory Lectures on Convex Optimization, Springer", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov,? \\Q2003\\E", "shortCiteRegEx": "Nesterov", "year": 2003}, {"title": "Learning with non-positive kernels", "author": ["C.S. Ong", "X. Mary", "S. Canu", "A.J. Smola"], "venue": "Proceedings of the 21st International Conference on Machine Learning", "citeRegEx": "Ong et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2004}, {"title": "Learning the kernel with hyperkernels", "author": ["C.S. Ong", "A.J. Smola", "R.C. Williamson"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2005}, {"title": "Large-scale optimization of eigenvalues", "author": ["M. Overton"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Overton,? \\Q1992\\E", "shortCiteRegEx": "Overton", "year": 1992}, {"title": "Protein homology detection using string alignment kernels", "author": ["H. Saigo", "J.P. Vert", "N. Ueda", "T. Akutsu"], "venue": "Bioinformatics", "citeRegEx": "Saigo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Saigo et al\\.", "year": 2004}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Transformation invariance in pattern recognition-tangent distance and tangent propogation", "author": ["P.Y. Simard", "Y.A.L. Cun", "J.S. Denker", "B. Victorri"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "Simard et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1998}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenberg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Sonnenberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenberg et al\\.", "year": 2006}, {"title": "Distances and (indefinite) kernels for set of objects", "author": ["A. Wo\u017anica", "A. Kalousis", "M. Hilario"], "venue": "Proceedings of the 6th International Conference on Data Mining pp", "citeRegEx": "Wo\u017anica et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wo\u017anica et al\\.", "year": 2006}, {"title": "An analysis of transformation on non-positive semidefinite similarity matrix for kernel machines", "author": ["G. Wu", "E.Y. Chang", "Z. Zhang"], "venue": "Proceedings of the 22nd International Conference on Machine Learning", "citeRegEx": "Wu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2005}, {"title": "An assessment of alternative strategies for constructing EMDbased kernel functions for use in an SVM for image classification", "author": ["A. Zamolotskikh", "P. Cunningham"], "venue": "Technical Report UCD-CSI-2007-3 ", "citeRegEx": "Zamolotskikh and Cunningham,? \\Q2004\\E", "shortCiteRegEx": "Zamolotskikh and Cunningham", "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "(See Ong et al. (2004) for a discussion.", "startOffset": 5, "endOffset": 23}, {"referenceID": 19, "context": "(See Ong et al. (2004) for a discussion.) In particular, an application of classification with indefinite kernels to image classification using Earth Mover\u2019s Distance was discussed in Zamolotskikh and Cunningham (2004). Similarity measures for protein sequences such as the Smith-Waterman and BLAST scores are indefinite yet have provided hints for constructing useful positive semidefinite kernels such as those decribed in Saigo et al.", "startOffset": 5, "endOffset": 219}, {"referenceID": 19, "context": "(See Ong et al. (2004) for a discussion.) In particular, an application of classification with indefinite kernels to image classification using Earth Mover\u2019s Distance was discussed in Zamolotskikh and Cunningham (2004). Similarity measures for protein sequences such as the Smith-Waterman and BLAST scores are indefinite yet have provided hints for constructing useful positive semidefinite kernels such as those decribed in Saigo et al. (2004) or have been transformed into positive semidefinite kernels with good empirical performance (see Lanckriet et al.", "startOffset": 5, "endOffset": 445}, {"referenceID": 16, "context": "(2004) or have been transformed into positive semidefinite kernels with good empirical performance (see Lanckriet et al. (2003), for example).", "startOffset": 104, "endOffset": 128}, {"referenceID": 18, "context": "described in Simard et al. (1998) or Haasdonk and Keysers (2002), are invariant to various simple image transformations and have also shown excellent performance in optical character recognition.", "startOffset": 13, "endOffset": 34}, {"referenceID": 7, "context": "(1998) or Haasdonk and Keysers (2002), are invariant to various simple image transformations and have also shown excellent performance in optical character recognition.", "startOffset": 10, "endOffset": 38}, {"referenceID": 6, "context": "Finally, it is sometimes impossible to prove that some kernels satisfy Mercer\u2019s condition or the numerical complexity of evaluating the exact positive kernel is too high and a proxy (and not necessarily positive semidefinite) kernel has to be used instead (see Cuturi (2007), for example).", "startOffset": 261, "endOffset": 275}, {"referenceID": 6, "context": "Finally, it is sometimes impossible to prove that some kernels satisfy Mercer\u2019s condition or the numerical complexity of evaluating the exact positive kernel is too high and a proxy (and not necessarily positive semidefinite) kernel has to be used instead (see Cuturi (2007), for example). In both cases, our method allows us to bypass these limitations. Our objective here is to derive efficient algorithms to directly use these indefinite similarity measures for classification. Our work closely follows, in spirit, recent results on kernel learning (see Lanckriet et al. (2004) or Ong et al.", "startOffset": 261, "endOffset": 581}, {"referenceID": 6, "context": "Finally, it is sometimes impossible to prove that some kernels satisfy Mercer\u2019s condition or the numerical complexity of evaluating the exact positive kernel is too high and a proxy (and not necessarily positive semidefinite) kernel has to be used instead (see Cuturi (2007), for example). In both cases, our method allows us to bypass these limitations. Our objective here is to derive efficient algorithms to directly use these indefinite similarity measures for classification. Our work closely follows, in spirit, recent results on kernel learning (see Lanckriet et al. (2004) or Ong et al. (2005)), where the kernel matrix is learned as a linear combination of given kernels, and the result is explicitly constrained to be positive semidefinite.", "startOffset": 261, "endOffset": 602}, {"referenceID": 1, "context": "While this problem is numerically challenging, Bach et al. (2004) adapted the SMO algorithm to solve the case where the kernel is written as a positively weighted combination of other kernels.", "startOffset": 47, "endOffset": 66}, {"referenceID": 1, "context": "While this problem is numerically challenging, Bach et al. (2004) adapted the SMO algorithm to solve the case where the kernel is written as a positively weighted combination of other kernels. In our setting here, we never numerically optimize the kernel matrix because this part of the problem can be solved explicitly, which means that the complexity of our method is substantially lower than that of classical kernel learning algorithms and closer in practice to the algorithm used in Sonnenberg et al. (2006), who formulate the multiple kernel learning problem of Bach et al.", "startOffset": 47, "endOffset": 513}, {"referenceID": 1, "context": "While this problem is numerically challenging, Bach et al. (2004) adapted the SMO algorithm to solve the case where the kernel is written as a positively weighted combination of other kernels. In our setting here, we never numerically optimize the kernel matrix because this part of the problem can be solved explicitly, which means that the complexity of our method is substantially lower than that of classical kernel learning algorithms and closer in practice to the algorithm used in Sonnenberg et al. (2006), who formulate the multiple kernel learning problem of Bach et al. (2004) as a semi-infinite linear program and solve it with a column generation technique similar to the analytic center cutting plane method we use here.", "startOffset": 47, "endOffset": 587}, {"referenceID": 8, "context": "A first direction embeds data in a pseudo-Euclidean (pE) space: Haasdonk (2005), for example, formulates the classification problem with an indefinite kernel as that of minimizing the distance between convex hulls formed from the two categories of data embedded in the pE space.", "startOffset": 64, "endOffset": 80}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.", "startOffset": 5, "endOffset": 37}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example).", "startOffset": 5, "endOffset": 369}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example).", "startOffset": 5, "endOffset": 408}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example). Yet another option is to reformulate either the maximum margin problem or its dual in order to use the indefinite kernel in a convex optimization problem. One reformulation suggested in Lin and Lin (2003) replaces the indefinite kernel by the identity matrix and maintains separation using linear constraints.", "startOffset": 5, "endOffset": 628}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example). Yet another option is to reformulate either the maximum margin problem or its dual in order to use the indefinite kernel in a convex optimization problem. One reformulation suggested in Lin and Lin (2003) replaces the indefinite kernel by the identity matrix and maintains separation using linear constraints. This method achieves good performance, but the convexification procedure is hard to interpret. Directly solving the nonconvex problem sometimes gives good results as well (see Wo\u017anica et al. (2006) and Haasdonk (2005)) but offers no guarantees on performance.", "startOffset": 5, "endOffset": 931}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example). Yet another option is to reformulate either the maximum margin problem or its dual in order to use the indefinite kernel in a convex optimization problem. One reformulation suggested in Lin and Lin (2003) replaces the indefinite kernel by the identity matrix and maintains separation using linear constraints. This method achieves good performance, but the convexification procedure is hard to interpret. Directly solving the nonconvex problem sometimes gives good results as well (see Wo\u017anica et al. (2006) and Haasdonk (2005)) but offers no guarantees on performance.", "startOffset": 5, "endOffset": 951}, {"referenceID": 16, "context": "We formulate the kernel learning problem as in Lanckriet et al. (2004), where the authors minimize an upper bound on the misclassification probability when using SVM with a given kernel K.", "startOffset": 47, "endOffset": 71}, {"referenceID": 5, "context": "A reformulation of problem (4) appears in Chen and Ye (2008) where the authors move the inner minimization problem to the constraints and get the following semi-infinite quadratically constrained linear program (SIQCLP):", "startOffset": 42, "endOffset": 61}, {"referenceID": 5, "context": "In Section 3, we describe algorithms to solve our eigenvalue optimization problem in (7), as well as an algorithm from Chen and Ye (2008) that solves the different formulation in (8), for completeness.", "startOffset": 119, "endOffset": 138}, {"referenceID": 5, "context": "For completeness, we also describe an exchange method from Chen and Ye (2008) used to solve problem (8), where the numerical bottleneck is a quadratically constrained linear program solved at each iteration.", "startOffset": 59, "endOffset": 78}, {"referenceID": 15, "context": "We refer the reader to Kulis et al. (2006) for another kernel learning example using this method.", "startOffset": 23, "endOffset": 43}, {"referenceID": 15, "context": "We refer the reader to Kulis et al. (2006) for another kernel learning example using this method. Given the eigenvalue decomposition X = V DV T , by changing basis this problem can be reduced to the decomposition of the diagonal plus rank-one matrix, D+\u03c1uuT , where u = V T\u03b1. First, the updated eigenvalues are determined by solving the secular equations det(D + \u03c1uu \u2212 \u03bbI) = 0, which can be done in O(n). While there is an explicit solution for the eigenvectors corresponding to these eigenvalues, they are not stable because the eigenvalues are approximated. This instability is circumvented by computing a vector \u00fb such that approximate eigenvalues \u03bb are the exact eigenvalues of the matrix D + \u03c1\u00fb\u00fbT , then computing its stable eigenvectors explicitly, where both steps can be done in O(n) time. The key is that D + \u03c1\u00fb\u00fbT is close enough to our original matrix so that the eigenvalues and eigenvectors are stable approximations of the true values. Finally, the eigenvectors of our original matrix are computed as VW , with W as the stable eigenvectors of D + \u03c1\u00fb\u00fbT . Updating the eigenvalue decomposition is reduced to an O(n) procedure plus one matrix multiplication, which is then the complexity of one gradient computation. We note that eigenvalues of symmetric matrices are not differentiable when some of them have multiplicities greater than one (see Overton (1992) for a discussion), but a subgradient can be used instead of the gradient in all the algorithms detailed here.", "startOffset": 23, "endOffset": 1372}, {"referenceID": 15, "context": "We refer the reader to Kulis et al. (2006) for another kernel learning example using this method. Given the eigenvalue decomposition X = V DV T , by changing basis this problem can be reduced to the decomposition of the diagonal plus rank-one matrix, D+\u03c1uuT , where u = V T\u03b1. First, the updated eigenvalues are determined by solving the secular equations det(D + \u03c1uu \u2212 \u03bbI) = 0, which can be done in O(n). While there is an explicit solution for the eigenvectors corresponding to these eigenvalues, they are not stable because the eigenvalues are approximated. This instability is circumvented by computing a vector \u00fb such that approximate eigenvalues \u03bb are the exact eigenvalues of the matrix D + \u03c1\u00fb\u00fbT , then computing its stable eigenvectors explicitly, where both steps can be done in O(n) time. The key is that D + \u03c1\u00fb\u00fbT is close enough to our original matrix so that the eigenvalues and eigenvectors are stable approximations of the true values. Finally, the eigenvectors of our original matrix are computed as VW , with W as the stable eigenvectors of D + \u03c1\u00fb\u00fbT . Updating the eigenvalue decomposition is reduced to an O(n) procedure plus one matrix multiplication, which is then the complexity of one gradient computation. We note that eigenvalues of symmetric matrices are not differentiable when some of them have multiplicities greater than one (see Overton (1992) for a discussion), but a subgradient can be used instead of the gradient in all the algorithms detailed here. Lewis (1999) shows how to compute an approximate subdifferential of the k-th largest eigenvalue of a symmetric matrix.", "startOffset": 23, "endOffset": 1495}, {"referenceID": 3, "context": "2 Projected gradient method The projected gradient method takes a steepest descent step, then projects the new point back onto the feasible region (see Bertsekas (1999), for example).", "startOffset": 152, "endOffset": 169}, {"referenceID": 20, "context": "See Nesterov (2003) for a complete discussion.", "startOffset": 4, "endOffset": 20}, {"referenceID": 3, "context": "The method is described in Algorithm 2 (see Bertsekas (1999) for a more complete treatment of cutting plane methods).", "startOffset": 44, "endOffset": 61}, {"referenceID": 5, "context": "The algorithm considered in Chen and Ye (2008) in order to solve problem (8) falls under a class of algorithms called exchange methods (as defined in Hettich and Kortanek (1993)).", "startOffset": 28, "endOffset": 47}, {"referenceID": 5, "context": "The algorithm considered in Chen and Ye (2008) in order to solve problem (8) falls under a class of algorithms called exchange methods (as defined in Hettich and Kortanek (1993)).", "startOffset": 28, "endOffset": 178}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL.", "startOffset": 12, "endOffset": 156}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL. Efficient methods to solve MKL with many kernels is an active area of research, most recently in Rakotomamonjy et al. (2008). There, the authors use a gradient method to solve a reformulation of problem (13) as a smooth maximization problem.", "startOffset": 12, "endOffset": 366}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL. Efficient methods to solve MKL with many kernels is an active area of research, most recently in Rakotomamonjy et al. (2008). There, the authors use a gradient method to solve a reformulation of problem (13) as a smooth maximization problem. Each objective value and gradient computation requires computing a support vector machine, hence each iteration requires several SVM computations which can be speeded up using warm-starting. Furthermore, Chen and Ye (2008) prune inactive constraints at each iteration in order to decrease the number of constraints in the QCLP.", "startOffset": 12, "endOffset": 706}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL. Efficient methods to solve MKL with many kernels is an active area of research, most recently in Rakotomamonjy et al. (2008). There, the authors use a gradient method to solve a reformulation of problem (13) as a smooth maximization problem. Each objective value and gradient computation requires computing a support vector machine, hence each iteration requires several SVM computations which can be speeded up using warm-starting. Furthermore, Chen and Ye (2008) prune inactive constraints at each iteration in order to decrease the number of constraints in the QCLP. Complexity. No rate of convergence is known for this algorithm, but the duality gap given in Chen and Ye (2008) is shown to monotonically decrease.", "startOffset": 12, "endOffset": 923}, {"referenceID": 5, "context": "A Matlab implementation of the exchange method (due to the authors of Chen and Ye (2008)) that uses MOSEK (MOSEK ApS 2008) to solve problem (13) is compared against the projected gradient method in Section 5.", "startOffset": 70, "endOffset": 89}, {"referenceID": 25, "context": "2 One-class SVM with indefinite kernels The same reformulation can also be applied to one-class support vector machines which have the formulation (see Sch\u00f6lkopf and Smola (2002))", "startOffset": 152, "endOffset": 179}, {"referenceID": 12, "context": "2 of Higham (2002).", "startOffset": 5, "endOffset": 19}, {"referenceID": 0, "context": "We finally analyze three data sets (diabetes, german and ala) from the UCI repository (Asuncion and Newman 2007) using the indefinite sigmoid kernel.", "startOffset": 86, "endOffset": 112}, {"referenceID": 24, "context": "See Wu et al. (2005) for further details.", "startOffset": 4, "endOffset": 21}, {"referenceID": 24, "context": "See Wu et al. (2005) for further details. We also implemented an SVM modification (denoted Mod SVM ) suggested in Lin and Lin (2003) where a nonconvex quadratic objective function is made convex by replacing the indefinite kernel with the identity matrix.", "startOffset": 4, "endOffset": 133}, {"referenceID": 10, "context": "We first experiment on data from the USPS handwritten digits database Hull (1994) using the indefinite Simpson score and the one-sided tangent distance kernel to compare two digits.", "startOffset": 70, "endOffset": 82}, {"referenceID": 7, "context": "Our experiments symmetrize the one-sided tangent distance using the square of the mean tangent distance defined in Haasdonk and Keysers (2002) and make it a similarity measure by negative exponentiation.", "startOffset": 115, "endOffset": 143}, {"referenceID": 0, "context": "We finally analyze three data sets (diabetes, german and ala) from the UCI repository (Asuncion and Newman 2007) using the indefinite sigmoid kernel. The data is randomly divided into training and testing data. We apply 5-fold cross validation and use an average of the accuracy and recall measures (described below) to determine the optimal parameters C, \u03c1, and any kernel inputs. We then train a model with the full training set and optimal parameters and test on the independent test set. Table 1 provides summary statistics for these data sets, including the minimum and maximum eigenvalues of the training similarity matrices. We observe that the Simpson are highly indefinite, while the one-sided tangent distance kernel is nearly positive semidefinite. The spectrum of sigmoid kernels varies greatly across examples because it is very sensitive to the sigmoid kernel parameters. Table 2 compares accuracy, recall, and their average for denoise, flip, shift, modified SVM, direct SVM and the indefinite SVM algorithm described in this work. Based on the interpretation from Section 2.3, Indefinite SVM should be expected to perform at least as well as denoise; if denoise were a good transformation, then cross-validation over \u03c1 should choose a high penalty that makes Indefinite SVM and denoise nearly equivalent. The rank-one update provides more flexibility for the transformation and similarities concerning data points xi that are easily classified (\u03b1i = 0) are not modified by the rank-one update. Further interpretation for the specific rank-one update is not currently known. However, Chen et al. (2009) recently proposed spectrum modifications in a similar manner to Indefinite SVM.", "startOffset": 87, "endOffset": 1618}, {"referenceID": 5, "context": "and ACCPM and compare them with the SIQCLP method of Chen and Ye (2008). Figure 2 shows total runtime (left) and average iteration runtime (right) for varying problem dimensions on an example from the USPS data with Simpson kernel.", "startOffset": 53, "endOffset": 72}, {"referenceID": 5, "context": "and ACCPM and compare them with the SIQCLP method of Chen and Ye (2008). Figure 2 shows total runtime (left) and average iteration runtime (right) for varying problem dimensions on an example from the USPS data with Simpson kernel. Experiments are averaged over 10 random data subsets and we fix C = 10 with a tolerance of .1 for the duality gap. For the projected gradient method, increasing \u03c1 increases the number of iterations to converge; notice that the average time per iteration does not vary over \u03c1. SIQCLP also requires more iterations to converge for higher \u03c1, however the average iteration time seems to be less for higher \u03c1, so no clear pattern is seen when varying \u03c1. Note that the number of iterations required varies widely (between 100 and 2000 iterations in this experiment) as a function of \u03c1, C, the chosen kernel and the stepsize. Results for ACCPM and SIQCLP are shown only up to dimensions 500 and 300, respectively, because this sufficiently demonstrates that the projected gradient method is more efficient. ACCPM clearly suffers from the complexity of the analytic center problem each iteration. However, improvements can be made in the SIQCLP implementation such as using a regularized version of an efficient MKL solver (e.g. Rakotomamonjy et al. (2008)) to solve problem (13) rather than MOSEK.", "startOffset": 53, "endOffset": 1281}], "year": 2009, "abstractText": "We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several standard data sets.", "creator": "LaTeX with hyperref package"}}}