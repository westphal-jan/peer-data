{"id": "0911.0460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2009", "title": "Feature-Weighted Linear Stacking", "abstract": "ensemble method methods, such as stacking, are designed effectively to routinely boost predictive precision accuracy by neatly blending the predictions of successively multiple sequential machine learning geometric models. recent computational work has even shown suggest that performing the use of meta - features, excluding additional inputs \u2014 describing each example in a dataset, can boost the true performance of generic ensemble methods, but the greatest predicted reported gains also have come from nonlinear filtering procedures requiring not significant mathematical tuning selection and training time. especially here, we present solely a linear technique, feature - matching weighted standard linear stacking ( unlike fwls ), calling that merely incorporates meta - dynamic features for its improved accuracy \u2026 while retaining also the well - known virtues of linear regression relying regarding reconstruction speed, stability, and interpretability. fwls combines interactive model predictions constructed linearly using transformation coefficients that are themselves objective linear functions sequences of meta - features. once this technique was twice a revolutionary key facet finding of the solution of the second place team in the recently concluded digital netflix golden prize competition. significant increases in accuracy over typical standard linear program stacking is again demonstrated on filming the netflix best prize category collaborative model filtering program dataset.", "histories": [["v1", "Tue, 3 Nov 2009 08:17:05 GMT  (18kb)", "https://arxiv.org/abs/0911.0460v1", "17 pages, 1 figure, 2 tables"], ["v2", "Wed, 4 Nov 2009 08:55:28 GMT  (18kb)", "http://arxiv.org/abs/0911.0460v2", "17 pages, 1 figure, 2 tables"]], "COMMENTS": "17 pages, 1 figure, 2 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["joseph sill", "gabor takacs", "lester mackey", "david lin"], "accepted": false, "id": "0911.0460"}, "pdf": {"name": "0911.0460.pdf", "metadata": {"source": "CRF", "title": "Feature-Weighted Linear Stacking", "authors": ["Joseph Sill", "Gabor Takacs", "Lester Mackey", "David Lin"], "emails": ["sill@yahoo.com)", "(gtakacs@sze.hu)", "(lmackey@cs.berkeley.edu)", "david.yang.lin@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :0\n91 1.\n04 60\nv2 [\ncs .L\nG ]\n4 N"}, {"heading": "1 Introduction", "text": "\u201cStacking\u201d is a technique in which the predictions of a collection of models are given as inputs to a second-level learning algorithm. This second-level algorithm is trained to combine the model predictions optimally to form a final set of predictions. Many machine learning practitioners have had success using stacking and related techniques to boost prediction accuracy beyond the level obtained by any of the individual models. In some contexts, stacking is also referred to as blending, and we will use the terms interchangeably here. Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7]. One prominent recent example of the\npower of model blending was the Netflix Prize1 collaborative filtering competition. The team BellKor\u2019s Pragmatic Chaos won the $1 million prize using a blend of hundreds of different models [22, 11, 14]. Indeed, the winning solution was a blend at multiple levels, i.e., a blend of blends.\nIntuition suggests that the reliability of a model may vary as a function of the conditions in which it is used. For instance, in a collaborative filtering context where we wish to predict the preferences of customers for various products, the amount of data collected may vary significantly depending on which customer or which product is under consideration. Model A may be more reliable than model B for users who have rated many products, but model B may outperform model A for users who have only rated a few products. In an attempt to capitalize on this intuition, many researchers have developed approaches that attempt to improve the accuracy of stacked regression by adapting the blending on the basis of side information. Such an additional source of information, like the number of products rated by a user or the number of days since a product was released, is often referred to as a \u201cmeta-feature,\u201d and we will use that terminology here.\nUnsurprisingly, linear regression is the most common learning algorithm used in stacked regression. The many virtues of linear models are well known to modellers. The computational cost involved in fitting such models (via the solution of a linear system) is usually modest and always predictable. They typically require a minimum of tuning. The transparency of the functional form lends itself naturally to interpretation. At a minimum, linear models are often an obvious initial attempt against which the performance of more complex models is benchmarked. Unfortunately, linear models do not (at first glance) appear to be well suited to capitalize on meta-features. If we simply merge a list of meta-features with a list of models to form one overall list of independent variables to be linearly combined by a blending algorithm, then the resulting functional form does not appear to capture the intuition that the relative emphasis given the predictions of various models should depend on the meta-features, since the coefficient associated with each model is constant and unaffected by the values of the meta-features.\nPrevious work has indeed suggested that nonlinear, iteratively trained models are needed to make good use of meta-features for blending. The winning Netflix Prize submission of BellKor\u2019s Pragmatic Chaos is a complex blend of many sub-blends, and many of the sub-blends use blending techniques which incorporate meta-features. The number of user and movie ratings, the number of items the user rated on a particular day, the date to be predicted, and various internal parameters extracted from some of the recommendation models were all used within the overall blend. In almost all cases, the algorithms used for the sub-blends incorporating meta-features were nonlinear and iterative, i.e., either a neural network or a gradient-\n1http://www.netflixprize.com\nboosted decision tree. In [2], a system called STREAM (Stacking Recommendation Engines with Additional Meta-Features) which blends recommendation models is presented. Eight meta-features are tested, but the results showed that most of the benefit came from using the number of user ratings and the number of item ratings, which were also two of the most commonly used meta-features by BellKor\u2019s Pragmatic Chaos. Linear regression, model trees, and bagged model trees are used as blending algorithms with bagged model trees yielding the best results. Linear regression was the least successful of the approaches.\nCollaborative filtering is not the only application area where the use of meta-features or other dynamic approaches to model blending has been attempted. In a classification problem context [7], Dzeroski and Zenko attempt to augment a linear regression stacking algorithm by meta-features such as the entropy of the predicted class probabilities, although they found that it yielded limited benefit on a suite of tasks from the UC Irvine machine learning repository. An approach which does not use meta-features per se but which does employ an adaptive approach to blending is described by Puuronen, Terziyan, and Tsymbal [15]. They present a blending algorithm based on weighted nearest neighbors which changes the weightings assigned to the models depending on estimates of the accuracies of the models within particular subareas of the input space.\nThus, a survey of the pre-existing literature suggests that nonparametric or iterative nonlinear approaches are usually required in order to make good use of meta-features when blending. The method presented in this paper, however, can capitalize on meta-features while being fit via linear regression techniques. The method does not simply add meta-features as additional inputs to be regressed against. It parametrizes the coefficients associated with the models as linear functions of the meta-features. Thus, the technique has all the familiar speed, stability, and interpretability advantages associated with linear regression while still yielding a significant accuracy boost. The blending approach was an important part of the solution submitted by The Ensemble, the team which finished in second place in the Netflix Prize competition."}, {"heading": "2 Feature-Weighted Linear Stacking", "text": ""}, {"heading": "2.1 Algorithm", "text": "Let X represent the space of inputs and g1, g2, \u00b7 \u00b7 \u00b7 , gL denote the learned prediction functions of L machine learning models with gi : X \u2192 R,\u2200i. In addition, let f1, f2, \u00b7 \u00b7 \u00b7 , fM represent a collection of M meta-feature functions to be used in blending. Each meta-feature function fi maps each datapoint x \u2208 X to its corresponding meta-feature fi(x) \u2208 R. Standard linear regression stacking [6] seeks a blended prediction function b of the\nform\nb(x) = \u2211\ni\nwigi(x),\u2200x \u2208 X (1)\nwhere each learned model weight, wi, is a constant in R. Feature-weighted linear stacking (FWLS) instead models the blending weights wi as linear functions of the meta-features, i.e.\nwi(x) = \u2211\nj\nvijfj(x),\u2200x \u2208 X (2)\nfor learned weights vij \u2208 R. Under this assumption, Eq. 1 can be rewritten as\nb(x) = \u2211\ni,j\nvijfj(x)gi(x),\u2200x \u2208 X (3)\nyielding the following FWLS optimization problem:\nmin v\n\u2211\nx\u2208X\u0303\n\u2211\ni,j\n(vijfj(x)gi(x)\u2212 y(x)) 2. (4)\nwhere y(x) is the target prediction for datapoint x and X\u0303 is the subset of X used to train the stacking parameters.\nWe thereby obtain an expression for b which is linear in the free parameters vij , and we can use a single linear regression to estimate those parameters. The independent variables of the regression (i.e., the \u201cinputs\u201d, in machine learning parlance) are the ML products fj(x)gi(x) of meta-feature function and model predictor evaluated at each x \u2208 X\u0303 .\nFigure 1 shows a graphical interpretation of FWLS. The outputs of individual models gi are represented as SVD, K-NN, and RBM in the figure. These acronyms represent common collaborative filtering algorithms which will be described in the next section. While it is helpful conceptually to think of a linear combination of models where the coefficients of the combination vary as a function of meta-features, the figure portrays the alternative but equivalent interpretation corresponding to equation 3 and corresponding to a concrete software implementation, i.e., a regression against all possible two-way products of models and meta-features.\nAn alternate interpretation of FWLS is to view it as a kind of bipartite quadratic regression against a set of independent variables consisting of the models and the meta-features. We obtain the FWLS form by starting with a full quadratic regression and dropping the terms resulting from interacting the models with themselves and each other, as well as the terms resulting from interacting the meta-features with themselves and each other.\nIt is important that the dataset collected for the stacked regression consists of out-of-sample model predictions. In other words, to obtain the prediction of a model on a particular data point, the model parameters should have been fitted on a training set which does not include that data point. This is normally achieved by via K-fold cross-validation. The training data is split into K subsets and K versions of the model are trained, each on a version of the data with a different subset removed. Model predictions for the kth subset are generated from the version of the model whose training set did not include that subset. Occasionally, however, the data distribution the models are to be tested on is not the same as the distribution from which the training data was drawn, in which case the blending procedure may differ 1.\nIt is reasonable to assume that there should be a constant component to the wi as well as a component which varies with the fj. In the experiments shown in the next section of the paper, we do indeed allow for a constant component of the weights. We can represent this within the above notation by defining f0 to be a special meta-feature which always takes the value 1. Similarly, one might expect the fj to add some modest amount of value when included as inputs to the regression on their own (i.e., without interacting them with the gi). The above notation can be understood to cover this case by including a special, constant model g0 which always takes the value 1.\nThe number of estimated parameters, ML, can be substantial when blending a large collection of models using a long-list of meta-features. Ridge regression (a.k.a. Tikhonov regularization) can be used to combat overfitting in these cases, such as the experimental results presented later in this paper.\nIt should be noted that there was prior work in which the FWLS functional form was employed on a small scale, but with important differences. In their 2008 Netflix Progress Prize paper [4], Bell, Koren and Volinsky make use of two meta-features (number of user and number of movie ratings) within a linear model in a construction which is similar to the formulation we present here, although their approach also includes coefficients which are specific to each movie. Perhaps more importantly, their approach employs stochastic gradient descent in order to fit the blending parameters rather than the classic linear-system-solution approach to regression which we advocate here. The results of their specific blending effort appear to play a minor role in their overall blend.\n1In this situation, if there is a small subset S of the training data drawn from the same distribution as the test data, then that subset can be removed from the training data and used instead to fit the stacking linear regression model, since the blend should be optimized with respect to the test distribution. In order to maximize accuracy to the fullest, the models can then be retrained on the full training set, including S, and the blending function obtained from the stacking linear regression can be used to blend the predictions of the retrained models. Although this procedure may be difficult to justify with full rigor, it can work well in practice. This was the approach commonly taken during the Netflix Prize competition, which will be described in more detail in section 3."}, {"heading": "2.2 Implementation Details", "text": "Let N be the number of data points used in the stacking regression, and let A be the N \u00d7ML matrix with elements An,(i+L(j\u22121)) = fj(xn)gi(xn), where xn is the input vector for the nth data point in X\u0303 . Performing a linear regression with Tikhonov regularization amounts to solving the system\n(ATA+ \u03bbI)v = AT y (5)\nwhere y represents the vector of target outputs for the N data points, and \u03bb is a given regularization parameter.\nThe time complexity of FWLS is O(NM2L2 + M3L3), where the first term corresponds to the cost of computing ATA and the second term corresponds to the solution of the linear system. In practice, N is normally much larger than ML and almost all of the computational cost comes from computing ATA. For many realistic scenarios, this computation can be completed quickly. For instance, for the parameters N = 162, 731, M = 26,and L = 10, the entire regression finishes in 1 minutes and 35 seconds on a single core of a 1.8Ghz Intel T7100 processor. For very large problems in which hundreds of models are blended using dozens of meta-features, however, the computational cost can be significant. Fortunately, the computation of ATA naturally lends itself to parallelization (e.g. by multithreading) so multiple cores can easily be capitalized on. In such large-problem scenarios, a naive implementation in which the entire A matrix is represented simultaneously in memory could run into memory constraint difficulties. It is straightforward, however, to implement an approach which calculates the entries of ATA directly without ever forming the entirety of A in memory at the same time. This approach requires O(M2L2) memory and can be executed by iterating only once over the training data.\nIn the dynamic setting, where models or meta-features are gradually added to a blend over time, significant computation is saved by serializing previously computed matrices ATA and AT y to disk. When a new model or meta-feature arrives, the previous results can be reloaded, and only the new entries (i.e., those involving the new model or meta-feature) of ATA and AT y need to be computed. This reduces the computational cost of adding a new model to O(NM2L+M3L3) and the cost of adding a new meta-feature to O(NML2 +M3L3), assuming that linear system is solved from scratch. A faster approach is to use the Sherman-Morrison formula [18] for updating the inverse of a matrix, in which case the second terms in the two preceding expressions can be improved to M3L2 and M2L3, respectively. Similarly, adding a single new data point to an existing, saved blend is O(M3L3) if the linear system is solved from the beginning every time and only O(M2L2) if the Sherman-Morrison formula for the inverse is employed."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Netflix Prize Overview", "text": "The Netflix Prize dataset is a collection of ratings (1 through 5 stars) submitted by customers of the DVD rental company Netflix. Each rating indicates how much a customer liked a particular movie seen in the past. There are 480, 189 users, 17, 770 movies, and 100, 480, 507 movie/user pairs for which the rating is supplied. The date on which the rating was made is also included. A \u201cqualifying set\u201d of 2,817,131 movie/user pairs was constructed where the rating the user made was not supplied to competitors2. Competitors were asked to submit rating predictions for the qualifying set. The qualifying set was derived from a larger set of 4, 225, 526 data points formed by collecting the 9 most recent ratings from each user. This larger set was randomly split into 3 subsets of equal size: the probe set, the quiz set, and the test set. The probe set (including the ratings) was included in the training data. The quiz set and the test set together formed the qualifying set, although competitors were not told which of the 2.8 million data points were in one set or the other. The quiz set had virtually no bearing on the official outcome of the competition, but the accuracy of teams\u2019 predictions on the quiz set was reported on a publicly viewable leaderboard during the competition. The prediction accuracy teams achieved on the test set determined who won the prize.\nIn order to qualify for the prize, a team had to improve upon the accuracy of Netflix\u2019s pre-existing algorithm, Cinematch, by at least 10% on the test set in terms of root mean squared error (RMSE). Since Cinematch\u2019s test RMSE was 0.9525, an improvement of 0.0001 in terms of raw RMSE closely corresponded to a 1 basis point (0.01%) percentage improvement. Test set scores were unknown to anyone other than Netflix during the competition to ensure that the test set served as a truly out-of-sample evaluation of the submitted solutions. Prior to the awarding of the $1 million grand prize, there were also two $50,000 \u201cProgress Prizes\u201d awarded in the fall of 2007 and 2008 to the teams with the best scores at that point in the competition.\nAn overview of the techniques used to win the prizes is presented in the papers written by the prize winners [22, 11, 14]. We briefly summarize a few of the main techniques here in order to provide background for the meta-features selected. Perhaps the most important class of algorithms proved to be matrix factorization techniques, sometimes referred to as by SVD (singular value decomposition) techniques. See [20] for an overview of these techniques. This simplest version of this approach represents each user and each movie as a vector of F real numbers. The predicted rating is given by the dot product of the user vector with the movie vector. The user\n2Now that the contest is over, those ratings are available, along with the rest of the dataset, at the UC Irvine Machine Learning Repository[1].\nand movie vector parameters are minimized on the training data, although regularization is typically employed as well. This is called a matrix factorization approach because the U by M rating matrix of all possible (user,movie) pairs is approximated by a low-rank matrix which is the product of a U by F matrix of user parameters and the transpose of an M by F matrix of movie parameters. More sophisticated versions add various additional parameters, such as means for each user and movie and parameters which model time effects, including some which model single-day effects. NSVD1 is an important variation on SVD first proposed by Paterek [13]. This model represents a user as the sum of a set of vectors corresponding to the movies the user has seen, where these vectors are distinct from the vectors comprising the aforementioned M by F movie matrix.\nPerhaps the second most prominent class of algorithms used in the prizewinning solution are the nearest neighbor models, which have a longer and more widespread academic and commerical collaborative filtering history. Nearest neighbor (K-NN) algorithms use a measure of similarity between the movie to be predicted and the movies the user has already rated in order to generate a prediction. The most common similarity measure involves computing the correlation between the ratings two movies received from the same set of users [9], although other similarity measures were also employed. Standard approaches take a weighted average of the user\u2019s ratings on the K most similar movies, where the weighting is a function of the similarity level. Many variations of this approach were also implemented (e.g. [5, 10]). There is also a user-based version of nearest neighbors where the ratings which correlated users gave the movie to be predicted are used to generate a prediction, but this proved to be much less useful on the Netflix Prize dataset. Restricted Boltzmann machines (RBMs) [17], a kind of stochastic recurrent neural network, are a third major class of algorithms.\nMany algorithms involved a kind of preprocessing known amongst the Netflix Prize community as the removal of global effects, which was largely pioneered by Bell, Koren, and Volinsky [3] . Global effects are predictions which can be made without simultaneous knowledge of the specific identities of the both the user and the movie. The two simplest examples of global effects are the average rating of the user and the average rating of the movie, although many others have also been found. Global effects are estimated in succession, so the average rating of the movie, for instance, might be estimated on the residual of the rating after subtracting out the average rating of the user."}, {"heading": "3.2 Results", "text": "We present the results of FWLS on 119 of the models of one of the leading teams in the Netflix Prize competition, Grand Prize Team. It should be noted that the team name was only aspirational in nature, as the team\ndid not ultimately win the grand prize. However, it did form half of a larger coalition known as The Ensemble, which tied BellKor\u2019s Pragmatic Chaos in terms of test RMSE and finished in second only because its best submission was made 20 minutes after the best submission of BellKor\u2019s Pragmatic Chaos.\nSince the probe set was statistically representative of the test set, it was standard practice among Netflix competitors to use the probe data for the sake of fitting a blend, and we followed this procedure here. Two versions of each of the 119 models were trained, with the first version being fitted a training set with the probe set removed and the second version being fitted on a training set including the probe set. The first version of the models was used to generate probe set predictions, and the FWLS regressions were performed using the probe set.The final blending parameters (i.e., the vij) used to generate the qualifying set predictions were obtained by fitting on the entire probe set and then using those parameters to blend the second version of the models, those that were fitted on the training set with the probe set included.\nNote that when parameters are chosen to minimize squared error on the probe set, reductions in probe set RMSE will not be entirely reflective of reductions in test set RMSE. In evaluating our methods, we addressed this issue by computing out-of-sample (OOS) probe set RMSE based on 10-fold cross validation. Ten blends were fit, each on a version of the probe set with a different 10% removed, and out-of-sample predictions were generated from each blend on the portion of the data on which it was not fit.\nBased on 00S probe set RMSE, we found a list of 24 meta-features which proved to be helpful. A description of these meta-features is shown in Table 1. A blend which uses only meta-feature 1 (the constant meta-feature which always takes the value 1) is equivalent to standard linear regression stacking (this trivial meta-feature is not included when arriving at the count of 24 useful meta-features). The creation of useful meta-features is an art which is guided by a detailed understanding of the characteristics of the models to be blended and an intuition about conditions under which certain models might merit greater emphasis. For the sake of brevity, we will only discuss the reasoning behind a handful of the 24 meta-features.\nMeta-features 3 and 6 (the log of the number of user and movie ratings, respectively) are the most commonly used meta-features in the previously existing literature. The intuition behind their usage is fairly clear, i.e., the relative accuracy of models may depend on how much information we have (i.e., how many ratings have been collected) regarding particular users and movies. The reasoning behind meta-features 12 and 24 is similar. The information we have about a particular movie depends not only on the number of users who rated it but also on whether or not those users have rated many movies. An analogous statement can be made if we switch users and movies in the previous statement.\nAs mentioned in the previous section, many nearest-neighbor-based techniques rely on the estimated correlations between pairs of movies. Metafeatures 10 and 14 attempt to characterize the correlation information available regarding the particular (user,movie) pair to be predicted. If metafeature 10 is large, then the user has rated many movies which have a significant correlation with the movie to be predicted, which should bode well for neighborhood-based techniques. Meta-feature 14 measures whether the total represented in meta-feature 10 is concentrated in just a few very highly correlated movies or whether it is distributed across a larger set of movies.\nMany models attempt to capture effects which vary over time, and there are several meta-features designed to reflect this. Some models associate a mean rating (a.k.a. a bias) with each distinct date on which the user rated movies [12]. If the date-specific means for certain users vary a great deal from day to day, then one might guess that the models which capture these effects should be given more emphasis in those cases. Meta-feature 15 is motivated by this observation. Meta-feature 4 and 13 are also designed to suggest the importance of time effects by measuring, in different ways, how dispersed in time the ratings are.\nIt is possible to implement a version of an SVD algorithm which produces a probability distribution over the 5 possible ratings and hence a standard deviation indicating the uncertainty around the predicted rating. The specific approach used to derive meta-feature 11 is described in [19], although an alternate technique which builds 4 separate models for the probability that the rating is less than or equal to r, 1 \u2264 r \u2264 4 is described in [14]. A high standard deviation may reasonably be interpreted as low confidence in the model\u2019s prediction, so it is not surprising that this meta-feature is useful for blending SVD algorithms with other algorithms.\nTable 2 shows the cross-validated probe set RMSE results(based on version 1 of the models) and also the RMSE on the test set (based on version 2 of the models). Row m corresponds to the results using meta-features 1 through m, so the difference between RMSEs in rows m \u2212 1 and m represents the incremental contribution of adding the mth meta-feature to the set of meta-features. As was previously mentioned, the test set RMSE was unknown to competitors during the competition, so the set of meta-features was developed without knowing those results. Since the contribution of some meta-features to accuracy on the probe set is modest, it is unsurprising to learn that a few of the meta-features were in fact mildly harmful to the test set RMSE, but in general the cross-validated probe set RMSE is a reasonably reliable indicator of the test set RMSE. The meta-features contribute 23.88 basis points of accuracy on the probe set and 19.72 basis points of accuracy on the test set. It is, of course, debatable whether all meta-features would be used in a commercial context, given that the computational cost of fitting the blend grows quadratically with the number of meta-features. In the context of the Netflix Prize competition, however, every basis point\nof improvement was precious. We argued in the introduction that a simple linear regression approach which merely includes the meta-features as additional independent variables to be regressed against is ill-suited as an approach to stacking with metafeatures. Our experiments on the Netflix Prize data confirm this suspicion. Including the same 24 meta-features as additional inputs to the regression yields a cross-validated probe set RMSE of 0.868641, i.e., only 1 basis point better than the RMSE obtained without using the meta-features at all.\nIt is important to note that meta-features were added to the collection one by one after demonstrating an ability to decrease the RMSE obtained with the previous set of meta-features. For this reason, the set of metafeatures arrived at is \u201cpath dependent\u201d in the sense that it depended on the order in which potential new meta-features occurred to the authors and were tested. There were many other meta-features evaluated which are not listed in Table 1 which were highly valuable when used in isolation, in the sense that the RMSE with the meta-feature alone significantly improved upon the RMSE obtained by standard linear regression with no meta-features. However, such meta-features did not improve the RMSE of the blend when using the entire set of meta-features already employed. It is possible that by removing some of the meta-features already in the set and adding in new meta-features under consideration that a superior set of meta-features could have been obtained, but this line of experimentation was generally not pursued."}, {"heading": "4 Discussion", "text": "There are several potential extensions of this work, many of which we plan to pursue and present in a longer paper. In one of the classic papers on stacking [6], Breiman strongly advocates the use of non-negative weights when using a linear blending model. The results we have presented do not constrain the vij in any way, but work is underway to evaluate the value of using nonnegativity constraints for FWLS.\nAnother line of research to pursue will involve pruning the expanded space of ML model/meta-feature pairs in order to reduce the number of parameters estimated and thereby perhaps improve upon both out-of-sample accuracy and the speed with which the blend can be fit. There are a variety of linear model feature selection and pruning algorithms which may be applicable here.\nIt is also likely the case that meta-features discovered in the course of using FWLS would be useful as inputs to nonlinear blenders such as neural networks and trees. Indeed, initial experiments involving neural networks confirm this suspicion and suggest that a second-level blending of a neural network blend and an FWLS blend, both using the same set of meta-features,\nyields accuracy superior to either individual blend. We plan to present detailed results on this topic in the future. The speed of FWLS, when used for blending a moderately-sized model collection, allows for quick discovery of useful meta-features which can then be passed on for use with neural networks, trees, and other nonlinear techniques. Thus, even if another blending approach is strongly preferred, FWLS may have value as a mechanism for discovering meta-features.\nThe interpretability of linear models is not to be forgotten as one of the additional merits of the approach. FWLS affords the opportunity to examine the effective coefficients associated with each model under various conditions, i.e., various values of the meta-features. Scrutiny of such coefficients may lead to insights regarding the conditions under which the various models are most successful.\nFinally, the authors wish to emphasize that FWLS should in principle be applicable to a wide variety of situations in which stacking is employed, so applications to domains other than collaborative filtering will be explored in the future."}], "references": [{"title": "Applying machine learning for prediction, recommmendation, and integration", "author": ["Xinlong Bao"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The bellkor solution to the netflix prize", "author": ["Robert Bell", "Yehuda Koren", "Chris Volinsky"], "venue": "http://www.netflixprize.com/", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "The bellkor 2008 solution to the netflix prize", "author": ["Robert Bell", "Yehuda Koren", "Chris Volinsky"], "venue": "http://www.netflixprize", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["Robert M. Bell", "Yehuda Koren"], "venue": "In ICDM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Stacked regressions", "author": ["Leo Breiman"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Is combining classifiers with stacking better than selecting the best one", "author": ["Saso D\u1e91eroski", "Bernard \u1e90enko"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Mccv stacked regression for model combination and fast spectral interval selection in multivariate calibration", "author": ["Lu Xu et. al"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "The bellkor solution to the netflix grand prize", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Improving regularized singular value decomposition for collaborative filtering", "author": ["A. Paterek"], "venue": "KDD-Cup and Workshop, ACM press,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The pragmatic theory solution to the netflix grand prize", "author": ["Martin Piotte", "Martin Chabbert"], "venue": "http://www.netflixprize", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A dynamic integration algorithm for an ensemble of classifiers", "author": ["Seppo Puuronen", "Vagan Terziyan", "Alexey Tsymbal"], "venue": "Foundations of Intelligent Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Stacking classifiers for anti-spam filtering of E-mail", "author": ["Georgios Sakkis", "Ion Androutsopoulos", "Georgios Paliouras", "Vangelis Karkaletsis", "Constantine D. Spyropoulos", "Panagiotis Stamatopoulos"], "venue": "Proceedings of EMNLP-01,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Zoubin Ghahramani, editor, ICML, volume 227 of ACM International Conference Proceeding Series,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Adjustment of an inverse matrix corresponding to changes in the elements of a given column or 16  a given row of the original matrix", "author": ["Jack Sherman", "Winifred J. Morrison"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1949}, {"title": "Ordinal matrix factorization, September 2009. http:// www.netflixprize.com/community/viewtopic.php?id=1541", "author": ["Joseph Sill"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Scalable collaborative filtering approaches for large recommender", "author": ["G\u00e1bor Tak\u00e1cs", "Istv\u00e1n Pil\u00e1szy", "Botty\u00e1n N\u00e9meth", "Domonkos Tikk"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Issues in stacked generalization", "author": ["Kai Ming Ting", "Ian H. Witten"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "The bigchaos solution to the netflix grand prize", "author": ["Andreas Toscher", "Michael Jahrer", "Robert Bell"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Stacked generalization", "author": ["David H. Wolpert"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}], "referenceMentions": [{"referenceID": 20, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 13, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 235, "endOffset": 242}, {"referenceID": 5, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 235, "endOffset": 242}, {"referenceID": 19, "context": "The team BellKor\u2019s Pragmatic Chaos won the $1 million prize using a blend of hundreds of different models [22, 11, 14].", "startOffset": 106, "endOffset": 118}, {"referenceID": 8, "context": "The team BellKor\u2019s Pragmatic Chaos won the $1 million prize using a blend of hundreds of different models [22, 11, 14].", "startOffset": 106, "endOffset": 118}, {"referenceID": 11, "context": "The team BellKor\u2019s Pragmatic Chaos won the $1 million prize using a blend of hundreds of different models [22, 11, 14].", "startOffset": 106, "endOffset": 118}, {"referenceID": 0, "context": "In [2], a system called STREAM (Stacking Recommendation Engines with Additional Meta-Features) which blends recommendation models is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In a classification problem context [7], Dzeroski and Zenko attempt to augment a linear regression stacking algorithm by meta-features such as the entropy of the predicted class probabilities, although they found that it yielded limited benefit on a suite of tasks from the UC Irvine machine learning repository.", "startOffset": 36, "endOffset": 39}, {"referenceID": 12, "context": "An approach which does not use meta-features per se but which does employ an adaptive approach to blending is described by Puuronen, Terziyan, and Tsymbal [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "Standard linear regression stacking [6] seeks a blended prediction function b of the", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "In their 2008 Netflix Progress Prize paper [4], Bell, Koren and Volinsky make use of two meta-features (number of user and number of movie ratings) within a linear model in a construction which is similar to the formulation", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "A faster approach is to use the Sherman-Morrison formula [18] for updating the inverse of a matrix, in which case the second terms in the two preceding expressions can be improved to M3L2 and M2L3, respectively.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "An overview of the techniques used to win the prizes is presented in the papers written by the prize winners [22, 11, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 8, "context": "An overview of the techniques used to win the prizes is presented in the papers written by the prize winners [22, 11, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 11, "context": "An overview of the techniques used to win the prizes is presented in the papers written by the prize winners [22, 11, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 17, "context": "See [20] for an overview of these techniques.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "NSVD1 is an important variation on SVD first proposed by Paterek [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "[5, 10]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[5, 10]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "Restricted Boltzmann machines (RBMs) [17], a kind of stochastic recurrent neural network, are a third major class of algorithms.", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "pioneered by Bell, Koren, and Volinsky [3] .", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "a bias) with each distinct date on which the user rated movies [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "The specific approach used to derive meta-feature 11 is described in [19], although an alternate technique which builds 4 separate models for the probability that the rating is less than or equal to r, 1 \u2264 r \u2264 4 is described in [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "The specific approach used to derive meta-feature 11 is described in [19], although an alternate technique which builds 4 separate models for the probability that the rating is less than or equal to r, 1 \u2264 r \u2264 4 is described in [14].", "startOffset": 228, "endOffset": 232}, {"referenceID": 4, "context": "In one of the classic papers on stacking [6], Breiman strongly advocates the use of non-negative weights", "startOffset": 41, "endOffset": 44}], "year": 2009, "abstractText": "Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset.", "creator": "LaTeX with hyperref package"}}}