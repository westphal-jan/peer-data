{"id": "0912.3995", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2009", "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "abstract": "we clearly consider the problem goal of optimizing implementing an unknown, noisy function that is heavily expensive to evaluate. we cast above this problem indirectly as a multiarmed bandit management problem where the payoff function is sampled from a gaussian process. collectively we resolve an important globally open problem setting on significantly deriving regret bounds \u03b5 for attaining this setting. in accounting particular, we analyze requires an upper level confidence problem algorithm satisfying and bound its arbitrary cumulative average regret in terms of the maximal information gain hurdle due effectively to sampling, thus connecting gaussian process consumption bandits independently and determining optimal experimental design. moreover, we bound beyond the possible maximal information measurement gain burden by drastically exploiting known log spectral performance properties instead of the popular output classes of kernels and conversely obtain sub - weighted linear regret sampling bounds effectively for our algorithm. somewhat in particular, yet we show that, just perhaps surprisingly, the regret bounds obtained for the squared exponential kernel depend are only immensely very mildly weakly on changing the dimensionality of the problem.", "histories": [["v1", "Mon, 21 Dec 2009 00:08:19 GMT  (476kb,D)", "http://arxiv.org/abs/0912.3995v1", "17 pages, 5 figures"], ["v2", "Thu, 4 Feb 2010 06:15:15 GMT  (175kb,D)", "http://arxiv.org/abs/0912.3995v2", null], ["v3", "Sat, 13 Feb 2010 18:24:43 GMT  (175kb,D)", "http://arxiv.org/abs/0912.3995v3", null], ["v4", "Wed, 9 Jun 2010 23:24:13 GMT  (292kb,DS)", "http://arxiv.org/abs/0912.3995v4", null]], "COMMENTS": "17 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["niranjan srinivas", "andreas krause 0001", "sham kakade", "matthias w seeger"], "accepted": true, "id": "0912.3995"}, "pdf": {"name": "0912.3995.pdf", "metadata": {"source": "CRF", "title": "Gaussian Process Bandits without Regret: An Experimental Design Approach", "authors": ["Niranjan Srinivas", "Andreas Krause", "Matthias Seeger"], "emails": ["niranjan@caltech.edu", "krausea@caltech.edu", "sham@tti-c.org", "mseeger@mmci.uni-saarland.de"], "sections": [{"heading": "1 Introduction", "text": "In many real-world problems, one needs to optimize a noisy function which is expensive to evaluate. Recent examples of interest include choosing the advertisements in sponsored search to maximize profit in a click-through model [2] and learning optimal control strategies for robots [3]. A common approach is to use a probabilistic model to estimate the function response on the inputs [4, 5, 6, 7]. One natural choice that has proven effective [3] is to use Gaussian process models for the function response. However, so far, analysis of algorithms for Gaussian process optimization has proven very challenging, and there are no results on convergence rates. A key challenge is that any algorithm for GP optimization must trade off exploration \u2013 sampling to determine the shape of the function \u2013 and exploitation \u2013 sampling where the function is expected to achieve high values. We tackle this challenge by casting GP optimization as a multi-armed bandit problem [8] with a large (possibly infinite) number of arms \u2013 one for each possible input. There has been some recent success on analyzing bandit problems with many arms. For example, regret bounds are known in the case of linear payoff functions [9] and Lipschitz-continuous payoff functions [10, 11]. However, linearity is often a strong assumption, since many real-world problems (such as ad revenue and robot control) exhibit non-linear behavior. In contrast, Lipschitz-continuity is a rather weak assumption, and captures many phenomena. However, the regret bounds for Lipschitz-continuous payoff functions degrade quickly with the dimensionality of the problem (\u2126(T d+1 d+2 )). By varying the covariance function, the framework of GP optimization allows us to very naturally encode various levels of smoothness or other properties of the payoff function. In fact, there is a large amount of work on kernels for objects other than Euclidean vectors, such as graphs, sets, lists, etc. [12].\n1An earlier version of this paper appeared in the NIPS 2009 workshop on Adaptive Sensing, Active Learning and Experimental Design, Whistler, B.C., Canada. See [1].\nar X\niv :0\n91 2.\n39 95\nv1 [\ncs .L\nG ]\n2 1\nD ec"}, {"heading": "1.1 Related Work", "text": "Brochu et al. [13] provide a comprehensive review of and motivation for Bayesian optimization. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions with the goal of minimizing the number of function evaluations is proposed in [6]; and extended for noisy black-box functions using Gaussian processes in [14]. Convergence of EGO using multivariate Gaussian processes (without an analysis of rates) is established in [15]. In fact, Brochu et al. [13] clearly point out one of the major problems with Bayesian optimization - It is also often unclear how to handle the trade-off between exploration and exploitation in the utility function. Too much exploration, and many iterations can go by without improvement. Too much exploitation leads to local maximization. Gaussian Processes are used for estimating the value function in Reinforcement Learning problems with continuous state spaces in [16, 17].\nIn the classic bandit optimization literature with independent arms, [18, 19] achieve a regret bound of O( \u221a KT ) for the K-armed bandit case. This bound is clearly vacuous forK =\u221e orK = \u2126(T ). In the infinitely many arms case, [9] and [20] assume a linear payoff function and derive regret bounds. However, the linearity assumption is very strong and not useful in many practical applications with nonlinear response functions. [10] and [11] assume the payoff function to be Lipschitz, which captures a large class of applications. However, this weak assumption comes at the price that the regret bounds scale poorly with dimension. Also, [10] analyzes the bandit problem in a metric space, while [21] allows the arms to be from a generic topological space.\nOur observation that linearity assumptions are too strong and that Lipschitz assumptions seem too weak motivates our choice to model the dependencies in the infinite-armed bandit using a Gaussian Process, since the choice of Kernel function allows one to model several levels of intermediate smoothness. Now we proceed to describe our main contributions toward GP bandit optimization."}, {"heading": "1.2 Main Contributions", "text": "We consider the problem of adaptively choosing inputs x1, x2, ..., xT to an unknown function f sampled from a Gaussian Process, such that we maximize \u2211 f(xt). We cast the problem as a bandit problem with infinitely many arms, where the dependencies between the arms are modeled by the Gaussian Process. We analyze a simple and intuitive Upper-Confidence based algorithm and prove that this algorithm achieves sublinear no-regret for many popular classes of kernel functions.\nOur proof demonstrates a novel general bound for the cumulative regret in terms of maximal information gain, thus connecting GP optimization and optimal experimental design. This result does not assume any structure on the decision space D (apart from the existence of a suitable kernel function). We bound this maximal information gain for compact decision sets in Rd by exploiting known spectral properties of popular classes of kernels. Moreover, we show that the regret bounds for the squared exponential kernel depend only very weakly on the dimensionality d of the decision set. Lastly, we experimentally demonstrate that our information-gain based regret bounds accurately capture the growth of cumulative regret on several important classes of kernels."}, {"heading": "2 Gaussian Processes", "text": "A Gaussian process (c.f., [22]) is a collection of random variables such that every finite subset is distributed according to a multivariate Gaussian distribution. A GP F (x) \u223c GP (m(x), k(x, x\u2032)) is completely specified by its mean function\nm(x) = E[F (x)]\nand its covariance function\nk(x, x\u2032) = E[(F (x)\u2212m(x))(F (x\u2032)\u2212m(x\u2032))] A GP is therefore a distribution over functions. Moreover, the choice of covariance function is crucial since it controls the smoothness properties of the sample functions drawn from the GP.\nOne advantage of using Gaussian processes is that well-known closed-form formulae [22] exist for the mean and variance of the posterior distribution, which enables exact inference in an efficient manner. Suppose we have n noisy samples from F i.e., data D = {(xs, ys)|s = 1, ..., n}, where\nA = {1, 2, ..., n} is the set of indices of the sampling points XA = {xs|s = 1, ..., n}. To be precise, ys = f(xs) + s where s \u223c N(0, \u03c32) is iid Gaussian noise and f is a particular realization of F . Let XV be the set of test points and y be the vector of noisy function values ys. Define K(C,D) to be the |C| \u00d7 |D| matrix of covariances between the points in set C and the points in set D, where C and D could each be either XA or XV .\nThen, the posterior predictive distribution of FV = F (XV ) is also Gaussian, with mean.\nF\u0304V = K(XV , XA)(K(XA, XA) + \u03c32In)\u22121y\nand variance\nV(FV ) = K(XV , XV )\u2212K(XV , XA)(K(XA, XA) + \u03c32In)\u22121K(XA, XV )\nSome popular covariance functions include: finite-dimensional Bayesian Linear Regression (BLR), the squared exponential kernel and the Mate\u0301rn class of covariance functions [22]. The simplest of these is the BLR case, where the function F is assumed to arise as a linear combination of basis functions; i.e., F (x) = \u2211 \u03b1i\u03c6i(x) where \u03c61(x), \u03c62(x), ..., \u03c6q(x) are the basis functions, and \u03b1 \u223c N(0, I). Then F \u223c GP (0, k), and the kernel funcion k is given by k(x, x\u2032) = \u03c6(x)\u2020\u03c6(x\u2032), where \u03c6(x)\u2020 = (\u03c61(x), \u03c62(x), ..., \u03c6q(x)).(Throughout this paper, Object\u2020 refers to the transpose of Object).\nAlso, the Gram covariance matrix over the sampling points x1, x2, ..., xn becomes\nG =  \u03c6\u2020(x1) \u03c6\u2020(x2)\n... \u03c6\u2020(xn)\n (\u03c6(x1), \u03c6(x2), ..., \u03c6(xn))\nThe squared exponential kernel is k(x, x\u2032) = exp(\u2212 |x\u2212x \u2032|2\n2l2 ), where l is the lengthscale parameter. Samples from this kernel are analytic (infinitely often differentiable) with probability 1 [22].\nThe Mate\u0301rn class of covariance functions is given by k(x, x\u2032) = 2 1\u2212\u03bd \u0393(\u03bd) ( \u221a 2\u03bd|x\u2212x\u2032| l ) \u03bdK\u03bd( \u221a 2\u03bd|x\u2212x\u2032| l ), where \u03bd is a parameter that controls smoothness and K\u03bd is a modified Bessel function. A function arising from the Mate\u0301rn class is r times mean-square differentiable if and only if r < \u03bd. Therefore, the higher the \u03bd, the stronger the smoothness assumptions and as \u03bd \u2192 \u221e the Mate\u0301rn covariance function reduces to the squared exponential kernel. An example plot of functions sampled from each of the three kernels is provided in Figure 1."}, {"heading": "3 GP Optimization and the UCB algorithm", "text": "Our objective while sampling is to maximize the sum of the \u2018rewards\u2019 we get, i.e., to choose our sampling points in order to maximize the sum of the function values obtained. This problem is analogous to the classical multi-armed bandit problem, where we have one arm for each possible decision. In bandit problems, we have the exploration-exploitation trade-off between the need to\nexplore various possible options and the desire to constantly choose the empirically best option. At each play t, the Upper Confidence Bound (UCB) [23, 24] algorithm chooses the arm xt by maximizing an upper confidence index; for each arm, this index is calculated by adding the current estimate of the mean of that arm and the weighted standard deviation of that arm:\nxt = argmax x\u2208D\n\u00b5\u0302t(x) + \u221a \u03b2t\u03c3\u0302(x, x) (1)\nSo, if an arm lies in an unexplored region, its index is likely to be high even if its initial mean estimate is low. The algorithm promises to converge to the optimal choice by ensuring that it never misses out on a \u2018good\u2019 arm due to an initial \u2018bad\u2019 return obtained for that arm. In this way, the UCB algorithm implicitly negotiates the exploration-exploitation trade-off. Classically, the UCB algorithm would keep independent statistics about mean and standard deviation for all arms. It is naturally extended to the GP optimization setting, by performing Bayesian inference, conditioning previous observations, and thereby exploiting correlation between the arms. The Gaussian Bandit UCB algorithm is provided in Figure 2. Figure 3 illustrates the performance of the UCB algorithm in two subsequent iterations.\nIf the decision set D is infinite, solving for xt in (1) is non-trivial; therefore, we discretize D into a set V = VT in our algorithm. For compact decision spaces in Rd and many kernels (with a fixed lengthscale i.e., independent of the discretization), a polynomial increase of n = |VT | = O(T \u03c4 ) suffices to ensure that the discretization error vanishes \u201cquickly enough\u201d. For each fixed d, \u03c4 is set appropriately. We discuss discretization details in Section 5.3.\nAnalyzing the UCB algorithm: Why does it work? UCB algorithms have been used as a heuristic in several applications [24, 2, 3]; but our current theoretical understanding of its success in GP optimization is unsatisfactory. To the best of our knowledge, we now provide the first analysis of the UCB algorithm in this context. Algorithms for bandit problems are usually analyzed in terms of their \u2018cumulative regret\u2019. For a particular choice of arm xt, and a particular realization f of F , the instantaneous regret is\nrt = f(x\u2217)\u2212 f(xt)\nwhere f(x) is maximized at x\u2217. Essentially, rt it is the mean (w.r.t noise) opportunity cost or loss of reward we incur due to our lack of knowledge of the best arm to play. The cumulative regret RT at time T is the sum of instantaneous regrets:\nRT = T\u2211 t=1 rt\nIn a multi-armed bandit problem, the quest is to find a no-regret algorithm for choosing the arms to play; an algorithm A is said to be no-regret if\nlim T\u2192\u221e RA(T ) T = 0\nFor the classicalK-armed bandit problem, the UCB algorithm is known to be no-regret; however the best bound available isO( \u221a KT ) [18, 19] and therefore vacuous for infinitely many arms (or ifK = \u2126(T )). In the sequel, we will show that we can replace K in the bound by the maximum possible information gain due to sampling, thus connecting GP bandit optimization and optimal experimental design. Coupled with a proof of diminishing returns for the information gain, this will yield a noregret bound for GP bandit optimization. The key insight is the following: in the classical setting, each arm is assumed to be independent; i.e, playing one arm gives us no information at all about any other arm. Thus, the regret grows unboundedly with the number of available arms. However, the GP setting imposes smoothness through the covariance structure, and therefore playing one \u2018arm\u2019 does give us information about other \u2018arms\u2019 in its neighbourhood \u2013 therefore the information gain grows sublinearly during the sampling process."}, {"heading": "4 Regret bounds", "text": "We will now establish a bound on the cumulative regret for GP bandit optimization. First, in Section 4.1, we will show that the growth of the cumulative regret is bounded by the information due to sampling. In Section 4.2, we will then bound the information gain for important examples of kernel functions."}, {"heading": "4.1 A general bound", "text": "Throughout the discussion, wherever appropriate, upper case notation (F , Y ) refers to random variables, whereas lower case notation (f ,y) refers to their corresponding realization. Let V represent the discretization of the decision set. We observe points ys = u\u2020sfV + s, where fV is the realization of the random unknown reward function F at the points in V , i is a Gaussian white noise process with variance \u03c32 and us is an indicator vector which refers to the particular member s of V we choose to observe. Further, fV is the vector of f -values at the discretization points V . We can think of picking the set of vectors ui in terms of choosing the matrix A with ui as the columns. Then, we observe YA = A\u2020FV + , where \u223c N(0, \u03c32I). We define the information gain to be\n\u03b3T = max A I(FV ;YA) = max A (H(YA)\u2212H(YA|FV )) (2)\nwhere I(\u00b7, \u00b7) stands for mutual information [25]. Fortunately, for multivariate Gaussians, the mutual information can be readily computed in closed form, since for an n-dimensional multivariate Gaussian random variable with covariance matrix \u03a3, the entropy is simply 12 log((2\u03c0e) n det \u03a3).\nOur main result is the following regret bound:\nTheorem 1 Let 0 < \u03b4 < 1. If we run the Gaussian Bandit UCB algorithm with discretization V , |V | = T \u03c4 and parameter \u03b4, the cumulative regret RT after T plays is bounded as:\nProb(\u2200T,RT \u2264 \u221a 8T\u03b2T \u03b3T ) \u2265 1\u2212 \u03b4\nwhere \u03b2T = 4(log( 2T \u03c4+1 \u03b4 )) 2.\nThe proof of all our theoretical results are given in the Appendix. Thus, with probability arbitrarily close to 1, we can bound the cumulative regret in terms of the maximum possible information gain due to sampling. This connects the GP bandit problem with the problem of experimental design, where points of measurement need to be chosen in order to maximize the information gain."}, {"heading": "4.2 Bounding the maximum possible information gain", "text": "Since the bound in Theorem 1 depends on the information gain, the key remaining question is how to bound the quantity maxA I(A;FV ) \u2261 I(A) for practical classes of kernels. Intuitively, due to correlation between the arms, observations for one input imply observations of \u201cnearby\u201d arms, and thus we expect diminishing returns in the information obtained through sampling. Thus, in order to bound \u03b3T , we exploit the fact that information gain is submodular, a natural formalization of the notion of diminishing returns. Submodularity guarantees that the information gain on sampling from one point is larger when our number of earlier sample points is low than when it is high [26]. That is, I(A \u222a {s})\u2212 I(A) \u2265 I(A\u2032 \u222a {s})\u2212 I(A\u2032) whenever A \u2286 A\u2032 and s /\u2208 A. Submodularity implies a performance guarantee on the \u2018greedy\u2019 algorithm for maximizing the information gain: A fundamental result by Nemhauser et al. [27] states that the simple greedy algorithm which always picks the input smaximizing the increase in information gain I(A\u222a{s})\u2212I(A) over all possible choices of inputs s obtains at least a constant fraction of the maximum information gain: For the matrix AT associated with the set of T points chosen by the greedy algorithm it holds that\nI(AT ) \u2265 (1\u2212 1/e) max A I(A).\nThus, in order to bound the worst-case information gain incurred from T samples, it suffices to analyze the choices that the greedy algorithm makes.\nSince F is sampled from the GP, FV \u223c N(0,\u03a3), we have YA \u223c N(0, A\u2020\u03a3A+\u03c32IT ) and YA|FV \u223c N(A\u2020FV , \u03c32IT ). Therefore the the last term in (2) is independent of A and depends only on \u03c3.\nSo, if we allow selecting arbitrary vectors of unit norm (instead of just point evaluations), maximizing the RHS of (2) reduces to\nmax A H(YA) = max A\nlog(detA\u2020\u03a3A) + const.\nWhen A = v is just a vector, i.e., we sample just one point at a time, we have\nargmax A H(YA) = argmax \u2016v\u20162\u22641\nv\u2020\u03a3v (3)\nWe know that (3) is maximized by selecting the eigenvector v of \u03a3 with maximal absolute eigenvalue. This insight shows that the worst case bound occurs when the UCB algorithm is allowed to sample eigenvectors of \u03a3. Further, the submodularity of information gain allows us to choose the sampling points greedily \u2013 picking eigenvectors with maximal eigenvalues maximizes information gain. The following result bounds the information gain by how frequently each eigenvector has been sampled:\nTheorem 2 The maximal information gain \u03b3T is bounded as follows:\n\u03b3T \u2264 (1\u2212 1 e )\u22121 max m1,...,mn n\u2211 t=1 log(1 + mt\u03bbt \u03c32 )\nwhere mi is the number of times the eigenvector with eigenvalue \u03bbi is picked. That is, m1,m2, ...,mn with \u2211 imi = T is an allocation of the T plays.\nThus, in order to bound the maximal information gain, we need to understand which allocation m1, . . . ,mn maximizes the bound in Theorem 2. This optimal allocation will depend on the spectral properties of the kernel matrices involved. If we know the decay of the eigenvalues \u03bbi, we can bound the optimal allocation using a fractional relaxation, i.e., by allowing mi be arbitrary nonnegative real numbers. Therefore, maximizing the RHS of the bound in Theorem 2 subject to the constraints\u2211n t=1mt = T and mt \u2265 0 using the Lagrange multiplier method, we arrive at the following optimality conditions: whenever mi,mj 6= 0 \u03bbi\n\u03c32 + \u03bbimi = \u03bbj \u03c32 + \u03bbjmj\n(4)\nExploiting the spectral decay properties of the kernel in question, we use (4) to get a recursive relation between the allocations mi and mj . From [28, 29] we know the asymptotic relationship between the eigenvalues \u00b5i of the operator defined by a kernel K and the eigenvalues \u03bbi of the Gram matrix of dimension n corresponding to the same kernel; we have \u03bbi \u2248 n\u00b5i. For the squared exponential and Mate\u0301rn kernels, the spectral decay results for the d dimensional case are provided in [30]. Using this general strategy, in the next section, we will derive regret bounds for three popular classes of kernels."}, {"heading": "5 Bounds for common kernel functions", "text": "We now show that if we choose to discretize such that n = \u0398(T \u03c4 ), where \u03c4 is fixed in accordance with the dimension d of the decision set D, the average regret vanishes asymptotically with T . We justify this choice and discuss the details at the end of the section. Also, note that \u03b2T = O(log2(T ))."}, {"heading": "5.1 Finite dimensional Bayesian linear regression", "text": "Consider finite-dimensional Bayesian linear regression, with piecewise Lipschitz-continuous basis functions \u03c6(x)\u2020 = (\u03c61(x), \u03c62(x), ..., \u03c6q(x)). As we saw in Section 2, the Gram covariance matrix for sample points x1, x2, ..., xn is\nG =  \u03c6\u2020(x1) \u03c6\u2020(x2)\n... \u03c6\u2020(xn)  (\u03c6(x1), \u03c6(x2), ..., \u03c6(xn)) The maximal number of non-zero eigenvalues that G can have is q. Therefore, we have\n\u03b3T \u2264 (1\u2212 1 e )\u22121 q\u2211 t=1 log(1 + \u03bbtmt \u03c32 )\n\u2264 (1\u2212 1 e )\u22121q log(1 + \u03bbmaxT \u03c32 )\nwhere \u03bbmax is the maximal eigenvalue of G. It is easy to see that \u03bbmax = O(n). Therefore, we have\n\u03b3T = O(log(nT ))\nFrom Theorem 1, we have the upper bound RT = O( \u221a T\u03b2T \u03b3T )\nFor Bayesian linear regression, since \u03b3T = O(log(nT )) and n = \u0398(T \u03c4 ) , we have \u03b3T = O(log(T )), which yields\nRT = O( \u221a T log3(T ))\nwhich is the no-regret bound for the Bayesian linear regression case. Note that the bound is independent of the dimension of the decision set."}, {"heading": "5.2 Squared exponential and Mate\u0301rn kernels", "text": "For the squared exponential kernel, we know \u00b5s \u2264 b d 2 c\u2212s 1/d\u2200 s \u2265 0, where b < 1 and c > 1 are constants and d is the dimension of the decision space. Therefore, for the eigenvalues of the Gram matrix, we have\n\u03bbi \u2264 nb d 2 c\u2212s 1/d .\nTherefore, equation (4) implies\nmt = m1 \u2212 \u03c32c\nn b d 2 (ct 1/d\u22121 \u2212 1)\nAs is intuitive, the allocations decrease as the eigenvalues decay. Let N0 be the number of non-zero allocations. We want to enforce 0 \u2264 mt for t \u2264 N0. We have, since m1 \u2264 T ,\n0 \u2264 mt \u2264 T \u2212 \u03c32c\nn b d 2 (ct 1/d\u22121 \u2212 1) (5)\nSolving (5) for t gives us\nt \u2264 (1 + log(1 + nT\u03c32cb d 2 )\nlog(c) )d\nwhich yields N0 = O((log(nT ))d)\nTherefore, we have\n\u03b3T \u2264 (1\u2212 1 e )\u22121 n\u2211 t=1 log(1 + mt\u03bbt \u03c32 )\n= (1\u2212 1 e\n)\u22121 (log(nT ))d\u2211\nt=1\nlog(1 + mt\u03bbt \u03c32 )\n\u2264 (1\u2212 1 e )\u22121(log(nT ))d log(1 + nT \u03c32c b d 2 ) \u2248 (log(nT ))d+1\nUsing Theorem 1 and \u03b3T = O((log(nT ))d+1),\nRT = O( \u221a T (log(T )) d+3 2 )\nThis forces the average regret to go to zero asymptotically wth T . Moreover, note that the regret bound has very weak dependence on the dimension; d is important only as an index for a term polylog in T .\nFor the Mate\u0301rn kernels, we have \u00b5s = O(s\u2212 2\u03bd+d d ) \u2200 s \u2265 0, where \u03bd is the Mate\u0301rn smoothness parameter and d, the dimension of the decision space. The analysis is similar to the squared exponential case:\nCorollary. For the Mate\u0301rn kernel with smoothness parameter \u03bd, we have\nRT = O(T 1 2 + (\u03c4+1)d 2(2\u03bd+d) log(T )). (6)\nEquation (6) suggests that the average regret vanishes asymptotically wth T provided we have \u03bd > \u03c4d 2 . This is intuitive \u2013 the higher the dimensionality of the problem, the stronger our smoothness assumptions need to be to guarantee zero asymptotic average regret. However, we actually need to ensure \u03bd > min( \u03c4d2 , 2) since the lower bound of 2 ensures Lipschitz continuity, as demonstrated below."}, {"heading": "5.3 Discretization of the decision set", "text": "We need to ensure that the discretization error vanishes asympotically \u2013 that is, that the optimal point over the discretization converges to the optimum over the decision set. First, we show that if f is Lipschitz continuous, this requirement is satisfied.\nLet x\u2217 = argmaxD f and xV = argmaxV f . Then, we have |F (x\u2217) \u2212 F (xV )| \u2264 \u03ba\u2016x\u2217 \u2212 xV \u2016 for some \u03ba > 0. Moreover, if we choose n = \u0398(T \u03c4 ), we have \u2016x\u2217 \u2212 xV \u2016 \u2264 \u221a d\nT \u03c4/d . Therefore, an\nappropriate choice of \u03c4 (for example, d2 ) is sufficient to ensure that the discretization error vanishes asymptotically sufficiently quicky (i.e. faster than 1\u221a\nT ) since d is fixed and known.\nTherefore, we just need to prove Lipschitz continuity of our payoff function f . For the BLR and squared exponential cases the sample functions are infinitely smooth and so the Lipschitz argument follows trivially. For all Mate\u0301rn kernels, \u03bd > 2 implies continuity of the derivative of the sample function f - which is sufficient for Lipschitz continuity through the mean value theorem, since D is compact. Therefore, we conclude that appropriate choice of discretization parameter \u03c4 guarantees zero asymptotic average regret."}, {"heading": "6 Experiments", "text": "We perform some preliminary experiments to demonstrate that our bounds accurately capture the growth of the cumulative regret. We discretize the unit interval D = [0, 1] uniformly into V \u2286 D where |V | = n = 1000. We ran the UCB algorithm for T = 1000 (results averaged over 30 random runs) for the Squared Exponential and Mate\u0301rn (with \u03bd = 2.5) kernels, each using lengthscale param-\neter 0.01. The sampling noise variance \u03c32 was set to .001 and the precision/confidence parameter delta to 0.01.\nWe computed the actual cumulative regret incurred and information gained by the UCB algorithm. We also ran the greedy algorithm in order to compute the regret bound from Theorem 1 and the bound on the information gain from Theorem 2. Figure 4 shows the results for the squared exponential kernel, and Figure 5 shows the results for the Mate\u0300rn kernel. While there is a gap between the actual incurred regret and our bound (Figures 4(a) and 5(a)), we find that the actual cumulative regret correlates strongly with the regret bound (Figures 4(b) and 5(b)), indicating that our constants may be loose, but the rates correspond accurately with the actual growth of the regret. Similarly, there is strong correlation between the actual information gain incurred and the bound on information gain (Figures 4(d) and 5(d))."}, {"heading": "7 Conclusions", "text": "In this paper, we prove the first sublinear regret bounds for the UCB algorithm for GP optimization for several popular kernel functions. Our regret bounds are independent of the dimension of the decision set for Bayesian Linear Regression and have very weak dependence for the squared exponential kernel. For the Mate\u0300rn Kernel, we demand higher smoothness for ensuring zero asymptotic average regret as the dimensionality of the decision space increases, which is very intuitive.\nMoreover, our proof proceeds by bounding the cumulative regret in terms of maximal information gain without assuming any structure on the decision space D, thus yielding a natural connection between Gaussian process bandit optimization and optimal experimental design. Our preliminary\nexperiments indicate that our bounds accurately capture the growth of the cumulative regret for the UCB algorithm."}, {"heading": "A Proofs", "text": "First, we prove the corollary stated for the Mate\u0301rn class of kernels in Section 5.2.\nProof [Regret bounds for Mate\u0301rn class of kernels] For the Mate\u0301rn kernels, we have \u00b5s = O(s\u2212 2\u03bd+dd ) \u2200 s \u2265 0, where \u03bd is the Mate\u0301rn smoothness parameter and d, the dimension of the decision space. Therefore, for the eigenvalues of the Gram matrix, we have\n\u03bbs = O(ns\u2212 2\u03bd+d d )\n(4) implies\nmt = m1 \u2212 \u03c32\nn (t\n2\u03bd+d d \u2212 1) (7)\nCalculations directly analogous to the squared exponential kernel case yield\nN0 \u2264 (1 + nT\n\u03c32 )\nd 2\u03bd+d\nwhich implies N0 = O((nT ) d 2\u03bd+d )\nTherefore, bounding \u03b3T similarly, we have\n\u03b3T = O((nT ) d 2\u03bd+d log(nT )). (8)\nUsing Theorem 1 and (8),\nRT = O(T 1 2 + (\u03c4+1)d 2(2\u03bd+d) log(T )). (9)\nNow, we state a theorem that is crucial for proving our main result Theorem 1.\nTheorem 3 (Sum of Squares Regret Bound) Let \u03b4 > 0. Then, if rt = f(x\u2217)\u2212 f(xt) is the instantaneous regret for Gaussian Bandit UCB on round t, we have, with probability 1\u2212 \u03b4\nT\u2211 t=1 r2t \u2264 8\u03b2T \u03b3T\nwhere \u03b2t = 4(log( 2T \u03c4+1 \u03b4 )) 2.\nNow, we prove that Theorem 1 follows from Theorem 3. Proof [Proof of Theorem 1] By Theorem 3, we know that with probability at least 1\u2212\u03b4, \u2211T t=1 r 2 t \u2264 8\u03b2T \u03b3T . Applying the Cauchy-Schwarz inequality, we have, with probability at least 1\u2212 \u03b4\nRT = T\u2211 t=1 rt\n\u2264 ( T\nT\u2211 t=1 r2t )1/2 which completes the proof.\nBefore we prove Theorem 3, we prove a series of useful lemmas. First, we prove a lemma that helps us understand how the covariance/precision matrices are updated after each round.\nLemma 1 Suppose FV , Y , ys, us and s are defined as before. We know FV |Y = ys is normally distributed with mean 0; suppose it has the covariance matrix \u03a3FV |Y . Then, we have\n\u03a3F |Y = (P1 + \u03c3\u22122usu\u2020s) \u22121\nwhere P1 = \u03a3\u22121.\nProof For simplicity, we drop the indexing subscripts. We know( F\nY\n) = (\n1 0 u\u2020 1\n)( F ) and therefore (\nF\nY\n) \u223c N(0, V )\nwhere\nV = (\n1 0 u\u2020 1 )( \u03a3 0 0 \u03c32 )( 1 u 0 1 ) = (\n\u03a3 \u03a3u u\u2020\u03a3 u\u2020\u03a3u+ \u03c32 ) Also, by using the standard expression for posterior variance on Gaussian updation using Bayes\u2019 rule,\n\u03a3FV |Y = \u03a3\u2212 (\u03a3u)(u \u2020\u03a3u+ \u03c32)\u22121(u\u2020\u03a3) (10)\nUsing the Matrix-Inversion lemma, the RHS of (10) simplifies directly to (\u03a3\u22121 + \u03c3\u22122uu\u2020)\u22121. Therefore, we have \u03a3F |Y = (P1 + \u03c3\u22122uu\u2020)\u22121 (11) The result follows for more than one observation by induction.\nCorollary. From the Bayesian update rule we have just proved in Lemma 1, it follows that\n\u03b3T = max u1,u2,...uT log\n( detP1 + \u03c3\u22122 \u2211T t=1 utu \u2020 t\ndetP1\n)\nOur next key insight is that on any round t, with probability 1\u2212\u03b4, the instantaneous regret is at most the \u201cwidth\u201d of the confidence bound in the direction of the chosen decision. We now formalize this.\nDefine wt := \u03c3\u22121 \u221a x\u2020tP \u22121 t xt\nwhich we interpret as the \u201cnormalized width\u201d at time t in the direction of the chosen decision. The true width, 2 \u221a \u03b2twt, turns out to be an upper bound for the instantaneous regret.\nLemma 2 For any 0 < \u03b4 < 1, choosing \u03b2t = 4(log( 2nT\u03b4 )) 2 yields P ( \u2200 s \u2208 V, t \u2264 T, |fs \u2212 f\u0302s,t| \u2264 \u221a \u03b2t\u03c32s,t ) \u2265 1\u2212 \u03b4 \u2200 T.\nwhere fs is the true mean marginal payoff function at s, f\u0302s,t is our estimate of fs at step t of the algorithm, and \u03c32s,t is the marginal posterior variance at s at step t.\nProof By assumption, f is sampled from a GP with known prior mean and covariance function. Suppose that f\u0302s,t is our posterior distribution along the marginal s at step t after t observations. Then, because of the posterior mean is Gaussian, we have\nP (|fs \u2212 f\u0302s,t| > c\u03c3s,t) \u2264 2e\u2212 c 2\nUsing the union bound, we have\nP (\u2203 s, t | |fs \u2212 f\u0302s,t| > c\u03c3s,t) \u2264 2nTe\u2212 c 2\nTherefore, P (\u2200 s, t, |fs \u2212 f\u0302s,t| \u2264 c\u03c3s,t) \u2265 1\u2212 2nTe\u2212 c 2 Choosing c = \u221a \u03b2t, we have\nP (\u2200 s, t, |fs \u2212 f\u0302s,t| \u2264 \u221a \u03b2t\u03c3s,t) \u2265 1\u2212 2nTe\u2212 \u221a \u03b2t 2\nNow choose \u03b2t = 4(log( 2nT\u03b4 )) 2, which yields\nP (\u2200 s, t, |fs \u2212 f\u0302s,t| \u2264 \u221a \u03b2t\u03c3s,t) \u2265 1\u2212 2nT \u03b4\n2nT \u2265 1\u2212 \u03b4\nIn the next lemma we provide a probabilistic bound for the instantaneous regret incurred by the algorithm, hence formalizing our insight that with probability 1 \u2212 \u03b4, the instantaneous regret is at most the \u201cwidth\u201d of the confidence bound in the direction of the chosen decision.\nLemma 3 Let wt be the standard deviation in the direction of the input s chosen by our algorithm at step t. Then, we show that the instantaneous regret rt is bounded by 2 min( \u221a \u03b2twt, 1), provided the high probability event (\u2200 s\u2032 \u2208 V, t \u2264 T, |fs\u2032 \u2212 f\u0302s\u2032,t| \u2264 \u221a \u03b2t\u03c3s\u2032,t) is true.\nProof Clearly, rt = maxs fs \u2212 fs. Therefore we have\nrt \u2264 |max s fs \u2212 fs|\n= |max s fs \u2212 f\u0302s,t + f\u0302s,t \u2212 fs|\n\u2264 |max s fs \u2212 f\u0302s,t|+ |f\u0302s,t \u2212 fs| \u2264 \u221a \u03b2twt + \u221a \u03b2twt\n= 2 \u221a \u03b2twt\nFor the last step, use Lemma 2 and observe that the choice of s by the algorithm implies f\u0302s,t + \u221a \u03b2Twt > maxs fs. Also, notice that without loss of generality we can assume that the loss at any step is bounded by 1, which yields the other part of the bound.\nThe following fact is useful.\nLemma 4 For every t \u2264 T , detPt+1 detP1 = t\u220f\n\u03c4=1\n(1 + w2t ).\nProof By the way the precision matrices are updated i.e. from Lemma 1, we have\ndetPt+1 = det(Pt + \u03c3\u22122xtx \u2020 t)\n= det(P 1/2t (I + P \u22121/2 t \u03c3 \u22122xtx \u2020 tP \u22121/2)P 1/2) = detPt det(I + P \u22121/2 t \u03c3 \u22121xt(P \u22121/2 t \u03c3 \u22121xt)\u2020) = detPt det(I + vtv \u2020 t ),\nwhere vt := \u03c3\u22121P \u22121/2 t xt. Now observe that v \u2020 t vt = w2t and\n(I + vtv \u2020 t )vt = vt + vt(v \u2020 t vt) = (1 + w 2 t )vt\nHence (1+w2t ) is an eigenvalue of I+vtv \u2020 t . Since vtv \u2020 t is a rank one matrix, all the other eigenvalues of I + vtv \u2020 t equal 1. It follows that det(I + vtv \u2020 t ) is (1 + w2t ), and so\ndetPt+1 = (1 + w2t ) detPt.\nThe result follows now by induction.\nFinally, we are ready to prove Theorem 3.\nProof [Proof of Theorem 3] First, by using the fact that for any 0 \u2264 z \u2264 1, ln(1 + z) \u2265 z/2, we have min(w2\u03c4 , 1) \u2264 2 ln(1 + w2\u03c4 ) Also, we have the following probabilistic bounds:\nT\u2211 t=1 r2t \u2264 T\u2211 t=1 4\u03b2t min(w2t , 1) by Lemma 3\n\u2264 4\u03b2T T\u2211 t=1 min(w2t , 1) since 1 < \u03b21 < \u00b7 \u00b7 \u00b7 < \u03b2T\n\u2264 8\u03b2T T\u2211 t=1 ln(1 + w2\u03c4 ) since 1 < \u03b21 < \u00b7 \u00b7 \u00b7 < \u03b2T\n\u2264 8\u03b2T ln (\ndetPt+1 detP1\n)\nThe proof follows from the \u201cmaximal\u201d nature of the definition of \u03b3.\nOur next lemma shows that the information gain is submodular in our setting. This allows us to bound the information gain by using a greedy algorithm.\nLemma 5 Information gain in our setting is submodular.\nProof We need to show that the information gain satisfies the weak assumptions in [26]. Let A be the index set of points we observe. Then, our information measure is G(A) = I(F ;YA). We have\nYs = u\u2020sFV + s and therfore Yi and Yj are clearly conditionally independent given FV . Hence, information gain is submodular.\nNext, we compute the information gain of the greedy algorithm when it samples an eigenvector.\nLemma 6 Suppose the ith decision point is chosen and \u03a3i is updated to \u03a3i+1. Then, if \u03bb1 is the eigenvalue of \u03a3i corresponding to the eigenvector v1 picked by Gaussian Bandit UCB, then \u03a3i+1 has the same eigenvalues and eigenvectors as \u03a3i with the exception of \u03bb1, which is reduced.\nProof Without loss of generality, assume i = 1. Then\n\u03a31 = n\u2211 i=1 \u03bbiviv \u2020 i\nand \u03a32 = (\u03a3\u221211 + \u03c3 \u22122v1v \u2020 1) \u22121 and since\n\u03a3\u221211 = N\u2211 i=1 1 \u03bbi viv \u2020 i\n\u03a32 = n\u2211 i=2 \u03bbiviv \u2020 i + ( 1 \u03bb1 + 1 \u03c32 )\u22121v1v \u2020 1\nHence, all eigenvectors are exactly the same, and all eigenvalues with the exception of \u03bb1 which has been reduced to \u03bb1( \u03c3 2\n\u03c32+\u03bb1 ).\nLemma 7 The Information gain after sampling mt times at each of the n eigenvectors vt corresponding to eigenvalues \u03bbt is \u2211n t=1 log(1 + mt\u03bbt \u03c32 ).\nProof We shall show that the information gain on sampling once from eigenvector v1 corresponding to eigenvalue \u03bb1 is log(1 + \u03bb1\u03c32 ), the rest follows by induction.\nSuppose we are sampling at Fs \u223c N(0, \u03bb1). However, since our sampling is noisy, we actually observe from Y = Fs + s, where s \u223c N(0, \u03c32). We know that F |Y = ys is normally distributed with variance \u03bb1\u03c3 2\n\u03bb1+\u03c32 .\nTherefore, the information gain obtained is log \u03bb1 \u2212 log( \u03bb1\u03c3 2\n\u03bb1+\u03c32 ) = log(1 + \u03bb1\u03c32 ).\nProof [Proof of Theorem 2] Follows from Lemma 6 and Lemma 7. Lemma 6 implies that the eigenvectors of the covariance matrix contain independent information; sampling an eigenvector gives Gaussian Bandit UCB information only about that eigenvector and not about any others. This demonstrates that the information gained by Gaussian Bandit UCB just adds after each round. Lemma 7 computes the information gained by Gaussian Bandit UCB at each round, and therefore Theorem 2 follows.\nNow, we derive the recurrence relation \u03bbi\u03c32+\u03bbimi = \u03bbj \u03c32+\u03bbjmj by maximizing the RHS of (2) subject to the constraints \u2211n t=1mt = T and mt \u2265 0.\nLemma 8 mi,mj 6= 0 =\u21d2 \u03bbi\u03c32+\u03bbimi = \u03bbj \u03c32+\u03bbjmj\nProof Writng the Lagrangian, we need to maximize\nL = n\u2211 t=1 log(1 + mt\u03bbt \u03c32 )\u2212 k0( n\u2211 t=1 mt \u2212 T )\u2212 n\u2211 t=1 ktmt\nsubject to kt \u2264 0 \u2200 i, and the complimentary slackness conditions ktmt = 0 \u2200 t, and k0( \u2211n t=1mt\u2212 T ) = 0.\nDifferentiating L with respect to mt and setting it to 0, we have\nkt + k0 = \u03bbt\n\u03c32 +mt\u03bbt \u2200 t (12)\nMultiplying (12) by mt and using the complimentary slackness conditions, we have\nmtk0 = mt\u03bbt\nmt\u03bbt + \u03c32\nwhich implies either mt = 0 or k0 = \u03bbt\u03c32+\u03bbtmt . Therfore, whenever mi,mj 6= 0 we have\n\u03bbi \u03c32 + \u03bbimi = \u03bbj \u03c32 + \u03bbjmj"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We consider the problem of optimizing an unknown, noisy function that is ex-<lb>pensive to evaluate. We cast this problem as a multiarmed bandit problem where<lb>the payoff function is sampled from a Gaussian Process. We resolve an important<lb>open problem on deriving regret bounds for this setting. In particular, we ana-<lb>lyze an upper confidence algorithm and bound its cumulative regret in terms of<lb>the maximal information gain due to sampling, thus connecting Gaussian Process<lb>bandits and optimal experimental design. Moreover, we bound the maximal infor-<lb>mation gain by exploiting known spectral properties of popular classes of kernels<lb>and obtain sub-linear regret bounds for our algorithm. In particular, we show that,<lb>perhaps surprisingly, the regret bounds for the squared exponential kernel depend<lb>only very weakly on the dimensionality of the problem.", "creator": "LaTeX with hyperref package"}}}