{"id": "1001.0921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2010", "title": "Graph Quantization", "abstract": "vector model quantization ( vq ) function is a symmetric lossy data compression technique primarily from signal cluster processing, which almost is restricted to orthogonal feature vectors and therefore inapplicable for constrained combinatorial structures. this architectural contribution presents a theoretical technical foundation of spatial graph quantization ( mesh gq ) that also extends 2d vq mapping to the domain of internally attributed graphs. we here present precisely the necessary lloyd - kaufman max conditions principle for robust optimality of computing a graph domain quantizer and consistency constraint results for optimal target gq optimal design implementation based purely on empirical distortion measures and stochastic optimization. these results help statistically justify existing clustering algorithms in the domain classes of constraint graphs. the similarly proposed approach architecture provides a template of examining how fastest to consciously link structural comb pattern recognition methods to other than than designing gq to improve statistical pattern recognition.", "histories": [["v1", "Wed, 6 Jan 2010 15:46:03 GMT  (30kb,D)", "http://arxiv.org/abs/1001.0921v1", "24 pages; submitted to CVIU"]], "COMMENTS": "24 pages; submitted to CVIU", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["brijnesh j jain", "klaus obermayer"], "accepted": false, "id": "1001.0921"}, "pdf": {"name": "1001.0921.pdf", "metadata": {"source": "CRF", "title": "Graph Quantization", "authors": ["Brijnesh J. Jain", "Klaus Obermayer"], "emails": ["jbj@cs.tu-berlin.de", "oby@cs.tu-berlin.de"], "sections": [{"heading": "1 Introduction", "text": "Vector quantization is a classical technique from signal processing suitable for lossy data compression, density estimation, and prototype-based clustering [7, 14, 30]. The problem of optimal vector quantizer design is to find a codebook consisting of a finite set of prototypes such that an expected distortion with respect to some (differentiable) distortion measure is minimized.\nSince the probability distribution of the input patterns is usually unknown, vector quantizer design techniques use empirical data. Extensively studied design techniques are, for example, k-means and simple competitive learning. The kmeans algorithm is also commonly referred to as the Linde-Buzo-Gray (LBG) algorithm [24] the generalized Lloyd algorithm [25]. This algorithm is a local optimizer of the empirical sum-of-squared-error distortion without any global optimal or consistency guarantees. In contrast to k-means, competitive learning directly minimizes the expected distortion and is a consistent learner under very general conditions in the sense that it almost surely converges to a local optimal solution of the expected distortion.\nOne limitation of VQ is its restriction to patterns that are represented by vectors. For patterns that are more naturally represented by finite combinatorial structures, the theoretical framework of VQ as well its design techniques are no longer applicable. Examples of such structures include, for example, point patterns, strings, trees, and graphs arising from diverse application areas like proteomics, chemoinformatics, and computer vision.\nTo overcome this limitation, we generalize vector quantization to quantization of graphs. A number of graph quantizer design techniques for the purpose of\nar X\niv :1\n00 1.\n09 21\nv1 [\ncs .A\nI] 6\nJ an\n2 01\n0\nprototype-based clustering have already been proposed. Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29]. Related clustering method are presented in [3, 26, 31]. Due to a lack of an appropriate theoretical framework, all these graph quantizer design techniques (or clustering methods) have been developed in order to minimize an empirical distortion function without justifying whether the solutions found are statistically consistent estimators of the true but unknown solutions. In addition, it is unclear whether the nearest neighbor and centroid condition, which are also referred to as the Lloyd-Max conditions, are necessary conditions for optimality.\nIn this contribution, we propose graph quantization in a mathematically principled way as an extension of vector quantization, where we consider the graph edit distance as an underlying graph distortion measure. The key results of this contribution are consistency statements for estimators based on empirical distortion measures and estimators based on stochastic optimization. Furthermore, we prove that the Llyod-Max conditions are also necessary condition for optimal graph quantizers. In order to achieve the consistency results and the Lloyd-Max conditions, we isometrically embed \u2013 without loss of structural information \u2013 graphs as points into some Riemannian orbifold. An orbifold is the quotient of a manifold by a finite group action and therefore generalizes the notion of manifold. Using orbifolds we can define geometric and analytic concept such as length, angle, derivative, gradient, and integral locally to a Euclidean space. This construction forms the basis for extending consistency results from Euclidean vector spaces to the domain of graphs.\nThe proposed approach has the following properties: First, it can be applied to finite combinatorial structures other than graphs like, for example, point patterns, sequences, trees, and hypergraphs. For the sake of concreteness, we restrict our attention exclusively to the domain of graphs. Second, for graphs consisting of a single vertex with feature vectors as attributes, graph quantization coincides with vector quantization. Third, the proposed consistency results justify some of the above referenced graph clustering methods as statistically consistent learners. Fourth, the underlying mathematical framework can be applied in order to link other structural pattern recognition methods that directly operate in the domain of graphs to methods from statistical pattern recognition.\nThe paper is organizes as follows. Section 2 describes the problem of graph quantizer design. Section 3 introduces Riemannian orbifolds. In Section 4, we extend VQ to GQ and present consistency result for GQ design techniques. Section 5 briefly discusses the case of general graph edit distance functions. Finally, Section 6 concludes."}, {"heading": "2 The Problem of Graph Quantizer Design", "text": "This section aims at outlining the problem of extending VQ to the quantization of graphs."}, {"heading": "2.1 Attributed Graphs", "text": "To begin with, we first describe the structures we want to quantize.\nLet A be a set of attributes and let \u03b5 \u2208 A be a distinguished element denoting the null or void element. An attributed graph is a tuple X = (V, \u03b1) consisting of a finite nonempty set V of vertices and an attribute function \u03b1 : V \u00d7 V \u2192 A. Elements of the set\nE = {(i, j) \u2208 V \u00d7 V : i 6= j and \u03b1(i, j) 6= \u03b5}\nare the edges of X. By GA we denote the set of all attributed graphs with attributes from A. The vertex set of an attributed graph X is often referred to as VX and its attribute function as \u03b1X .\nAn alignment of a graph X is a graph X \u2032 with VX \u2286 VX\u2032 and\n\u03b1X\u2032(i, j) =\n{ \u03b1X(i, j) : (i, j) \u2208 VX \u00d7 VX\n\u03b5 : otherwise\nfor all i, j \u2208 VX\u2032 . Thus, we obtain an alignment of X by adding isolated vertices with null-attribute. The set VX\u2032 \\ VX is the set of aligned vertices. By A(X) we denote the (infinite) set of all alignments of X.\nA pairwise alignment of graphs X and Y is a triple (\u03c6,X \u2032, Y \u2032) consisting of alignments X \u2032 \u2208 A(X) and Y \u2032 \u2208 A(Y ) together with a bijective mapping\n\u03c6 : VX\u2032 \u2192 VY \u2032 , i 7\u2192 i\u03c6.\nBy A(X,Y ) we denote the set of all pairwise alignments between X and Y . Sometimes we briefly write \u03c6 instead of (\u03c6,X \u2032, Y \u2032)."}, {"heading": "2.2 The Graph Edit Distance", "text": "Fundamental for quantizing data is the notion of distortion. This section briefly introduces the graph edit distance functions as our choice of distortion measure. For a more detailed definition of the graph edit distance, we refer to [2]. In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33]. For sake of convenience, we assume that all distances are metrics.\nEach pairwise alignment (\u03c6,X \u2032, Y \u2032) \u2208 A(X,Y ) can be regarded as an edit path with cost\nd\u03c6 (X,Y ) = \u2211\ni,j\u2208VX\u2032\ndA ( \u03b1X\u2032(i, j), \u03b1Y \u2032(i \u03c6, j\u03c6) ) ,\nwhere dA : A\u00d7A \u2192 R+ is a distance function defined on the set A of attributes. Observe that deletion (insertion) of vertices also deletes (inserts) all edges the respective vertices are incident to.\nThe graph edit distance of X and Y is then defined as the edit path with minimal cost\nd(X,Y ) = min {d\u03c6 (X,Y ) : \u03c6 \u2208 A(X,Y )} .\nNote that the set A(X,Y ) of pairwise alignments is of infinite cardinality. But since dA(\u03b5, \u03b5) = 0, we actually take the minimum over a finite subset by ignoring all pairwise alignments that map aligned vertices with null-attributes onto each other.\nNext, we consider an important example of the graph edit distance based on a generalization of the concept of maximum common subgraph. We derive this graph metric from a similarity measure in the same way the Euclidean distance is derived from an inner product.\nSuppose that kA : A\u00d7A \u2192 R with kA(\u00b7, \u03b5) = 0 is a positive definite kernel. We measure the quality of a pairwise alignment \u03c6 \u2208 A(X,Y ) by\nk\u03c6(X,Y ) = \u2211\ni,j\u2208VX\nkA ( \u03b1X(i, j), \u03b1Y (i \u03c6, j\u03c6) ) .\nAn optimal alignment kernel is a graph similarity measure of the form\nk(X,Y ) = max {k\u03c6(X,Y ) : \u03c6 \u2208 A(X,Y )} . (1)\nNote that k (\u00b7|\u00b7) is symmetric but indefinite as a pointwise maximizer of a set of positive definite kernels.\nThe distance metric on GA induced by an optimal alignment kernel k (\u00b7|\u00b7) is defined by\nd(X,Y ) = \u221a l(X)2 \u2212 2k(X,Y ) + l(Y )2, (2)\nwhere l(X) = \u221a k(X,X) denotes the length of an attributed graph X. As shown in [23], d is indeed a metric and can be expressed as a graph edit distance."}, {"heading": "2.3 The Problem of Graph Quantizer Design", "text": "Let (GA, d) be a graph distance space, where d (\u00b7|\u00b7) is a graph edit distance. Optimal graph quantization design aims at minimizing the expected distortion\nD(C) = \u222b GA d(X,Q(X)) dP (X),\nwhere Q : GA \u2192 C is a graph quantizer, C = {Y1, . . . , Yk} a codebook consisting of k code graphs, and P = PGA is a probability measure defined on some appropriate measurable space (GA, \u03a3GA).\nAs opposed to vector quantization, the following factors complicate designing an optimal graph quantizer in a statistically consistent way:\n1. The graph distance d(X,Y ) is in general non-convex and non-differentiable. 2. Neither a well-defined addition on graphs nor the notion of derivative for\nfunctions on graphs is known.\nTo overcome these difficulties, we isometrically embed graphs as points into a Riemannian orbifold in order to apply methods that generalize gradient descent techniques and methods from stochastic optimization for non-convex and nondifferentiable distortion functions."}, {"heading": "3 Riemannian Orbifolds", "text": "Orbifolds generalize the notion of manifold as locally being a quotient of Rn by finite group actions. Consequently, learning on orbifolds generalizes learning on Euclidean spaces and Riemannian manifolds. This section introduces Riemannian orbifolds and their intrinsic metric structure. Proofs for new results are delegated to Section B.1. For all other proofs we refer to [4, 21]."}, {"heading": "3.1 Riemannian Orbifolds", "text": "To keep the treatment simple, we assume that X = Rn is the n-dimensional Euclidean vector space, and \u0393 is a permutation group acting on X . In a more general setting, however, we can assume that X is a Riemannian manifold, and \u0393 is a finite group of isometries acting effectively on X .\nThe binary operation\n\u00b7 : \u0393 \u00d7X \u2192 X , (\u03b3,x) 7\u2192 \u03b3(x)\nis a group action of \u0393 on X . For x \u2208 X , the orbit of x is the set defined by\n[x] = {\u03b3(x) : \u03b3 \u2208 \u0393} .\nThe quotient set X\u0393 = X/\u0393 = {[x] : x \u2208 X}\nconsisting of all all orbits carries the structure of a Riemannian orbifold. Its orbifold chart is the surjective continuous mapping\n\u03c0 : X \u2192 X\u0393 , x 7\u2192 [x]\nthat projects each point x to its orbit [x]. In the following, an orbifold is a triple Q = (X , \u0393, \u03c0) consisting of an Euclidean space X , a permutation group \u0393 acting on X and its orbifold chart \u03c0. With \u0393 = {id} being the trivial permutation group consisting of the identity only, a manifold X is also an orbifold. In general, however, the underlying space X\u0393 of an orbifold is not a manifold. Thus, orbifolds generalize the notion of manifold. The points at which an orbifold X\u0393 is locally not homeomorphic to a manifold are its singular points. We call the elements of X\u0393 structures, since they represent combinatorial structures like attributed graphs. We use capital letters X,Y, Z, . . . to denote structures from X\u0393 and write, by abuse of notation, x \u2208 X if \u03c0(x) = X. Each vector x \u2208 X is a vector representation of structure X and the set X of all vector representation is the representation space of X\u0393 .\nExample 1. Let X = R2 and let \u0393 be the group generated by reflections across the main-diagonal of the x-y-plane. Then Q = (X\u0393 , \u0393, \u03c0) is a Riemannian orbifold with\n\u03c0 : X \u2192 X\u0393 , x = (x1, x2) 7\u2192 [x] = {(x1, x2), (x2, x1)} .\nThe singular points of X\u0393 are all structuresX represented by vectors x = (x1, x2) with x1 = x2."}, {"heading": "3.2 The Riemannian Orbifold of Attributed Graphs", "text": "In this section, we show that attributes graphs can be identified with points in some Riemannian orbifold.\nRiemannian orbifolds of attributed graphs arise by considering equivalence classes of matrices representing the same graph. To identify graphs with points in a Riemannian orbifold without loss of structural information, some technical assumptions and restrictions to simplify the mathematical treatment are necessary. For this, let (GA, d) be a graph distance space with graph edit distance d(\u00b7|\u00b7). Then we make the following assumptions:\nP1 There is a feature map \u03a6 : A \u2192 H of the attributes into some finite dimensional Euclidean feature space H and a distance function dH : H\u00d7H \u2192 R+ such that \u03a6(\u03b5) = 0 \u2208 H and\ndA(a, a \u2032) = dH(\u03a6(a), \u03a6(a \u2032))\nfor all attributes a, a\u2032 \u2208 A. P2 All graphs are finite of bounded order n, where n is a sufficiently large\nnumber. Graphs X of order less than n, say m < n, are aligned to graphs X \u2032 of order n by inserting p = n\u2212m isolated vertices with null attribute \u03b5.\nBefore discussing the impact of both assumptions for practical application, we first restate our first assumptions for graph metrics induced by optimal alignment kernels. By definition kA : A\u00d7A \u2192 R is a positive definite kernel corresponding to an inner product kA(x, y) = \u3008\u03a6(x), \u03a6(y)\u3009 in some feature space H. Our first assumption requires thatH is a finite dimensional Euclidean space and \u03a6(\u03b5) = 0.\nNow let us consider the above assumptions in more detail. Both conditions do not effect the graph edit distance, provided an appropriate feature map for the attributes can be found. Restricting to finite dimensional Euclidean feature spaces H is necessary for deriving consistency results and for applying methods from stochastic optimization. Limiting the maximum size of the graphs to some arbitrarily large number n and aligning smaller graphs to graphs of oder n are purely technical assumptions to simplify mathematics. For machine learning problems, this limitation should have no practical impact, because neither the bound n needs to be specified explicitly nor an extension of all graphs to an identical order needs to be performed. When applying the theory, all we actually require is that the order of the graphs is bounded.\nWith both assumptions in mind, we construct the Riemannian orbifold of attributed graphs. Let X = Hn\u00d7n be the set of all (n\u00d7n)-matrices with elements from feature space H. A graph X is completely specified by a representation matrix X = (xij) from X with elements\nxij =  \u03c6 (\u00b5X(i)) : i = j\u03c6 (\u03bdX(i, j)) : (i, j) \u2208 E 0 : otherwise\nfor all i, j \u2208 VX . The form of a representation matrix X of X is generally not unique and depends on how the vertices are arranged in the diagonal of X.\nNow suppose that \u03a0n be the set of all (n \u00d7 n)-permutation matrices. For each P \u2208 \u03a0n we define a mapping\n\u03b3P : X \u2192 X , X 7\u2192 P TXP .\nThen \u0393 = {\u03b3P : P \u2208 \u03a0n} is a permutation group acting on X . Regarding an arbitrary matrix X as a representation of some graph X, then the orbit [X] consists of all possible matrices that can represent X. By identifying the orbits of X\u0393 with attributed graphs, the set GA of attributed graphs of bounded order n is a Riemannian orbifold."}, {"heading": "3.3 Metric Structures", "text": "Let Q = (X , \u0393, \u03c0) be an orbifold. We derive an intrinsic metric that enables us to do Riemannian geometry. In the case of a Riemannian orbifold of attributed graphs the intrinsic metric coincides with the graph metric of (2) induced by an optimal alignment kernel.\nAny inner product \u3008\u00b7, \u00b7\u3009 on X gives rise to a maximizer of the form\nk : X\u0393 \u00d7X\u0393 \u2192 R, (X,Y ) 7\u2192 max {\u3008x,y\u3009 : x \u2208 X,y \u2208 Y } .\nWe call the kernel function k(\u00b7|\u00b7) optimal alignment kernel, induced by the inner product \u3008\u00b7, \u00b7\u3009. Note that the maximizer of a set of positive definite kernels is an indefinite kernel in general. Since \u0393 is a group, we find that\nk(X,Y ) = max {\u3008x,y\u3009 : x \u2208 X} .\nwhere y is an arbitrary but fixed vector representation of Y . In general, we have\nk(X,Y ) \u2265 \u3008x,y\u3009\nfor all x \u2208 X and y \u2208 Y .\nExample 2. Consider the Riemannian orbifold (X , \u0393, \u03c0) of Example 1, where X = R2 and \u0393 = {id, \u03b3} is the group generated by reflections across the x-yplane. Suppose that x = (1, 2) is a vector representation of X and y = (3, 2) is a\nvector representation of Y . Then the optimal alignment kernel k (X,Y ) induced by the standard inner product of X is given by\nk(X,Y ) = max {\u3008x,y\u3009, \u3008\u03b3(x),y\u3009, \u3008x, \u03b3(y\u3009), \u3008\u03b3(x), \u03b3(y\u3009)}\nEvaluating the inner products yields\n\u3008x,y\u3009 = \u3008(1, 2), (3, 2)\u3009 = 7 \u3008\u03b3(x),y\u3009 = \u3008(2, 1), (3, 2)\u3009 = 8 \u3008x, \u03b3(y)\u3009 = \u3008(1, 2), (2, 3)\u3009 = 8\n\u3008\u03b3(x), \u03b3(y)\u3009 = \u3008(2, 1), (2, 3)\u3009 = 7.\nThus, we have k(X,Y ) = 8.\nExample 3. Suppose that X and Y are attributed graphs where edges have attribute 1 and vertices have attribute 0. The optimal alignment kernel k (X,Y ) induced by the standard inner product of X is the number of edges of a maximum common subgraph of X and Y .\nExample 4. More generally, if property P1 is satisfied, then any optimal alignment kernel on a bounded set of attributed graphs as defined in (1) is also an optimal assignment kernel of some Riemannian orbifold.\nSuppose that X \u2208 X\u0393 . Since k(X,X) = \u3008x,x\u3009 for all x \u2208 X, we can define the length of X by l(X) = \u221a k(X,X).\nThe optimal alignment kernel together with the length satisfies the CauchySchwarz inequality\n|k(X,Y )| \u2264 l(X) \u00b7 l(Y ).\nSince the Cauchy-Schwarz inequality is valid, the geometric interpretation of k(\u00b7|\u00b7) is that it computes the cosine of a well-defined angle between X and X \u2032 provided they are normalized to length 1.\nLikewise, k(\u00b7|\u00b7) gives rise to a distance function defined by d(X,Y ) = \u221a l(X)2 \u2212 2k(X,Y ) + l(Y ).\nFrom the definition of k(\u00b7|\u00b7) follows that d is a metric. In addition, we have\nd(X,Y ) = min {\u2016x\u2212 y\u2016 : x \u2208 X,y \u2208 Y }, (3)\nwhere \u2016\u00b7\u2016 denotes the Euclidean norm induced by the inner product \u3008\u00b7, \u00b7\u3009 of the Euclidean space X .\nExample 5. Consider the Riemannian orbifold (X , \u0393, \u03c0) of Example 1 and 2. Suppose that x = (1, 2) is a vector representation of X and y = (3, 2) is a vector representation of Y . Then the squared lengths of X and Y are l(X)2 = 5 and l(Y )2 = 13. Since k(X,Y ) = 8 according to Example 2, the distance is d(X,Y ) = \u221a 5\u2212 16 + 13 = \u221a 2.\nExample 6. If properties P1 and P2 are satisfied, then the graph metric (2) coincides with the intrinsic orbifold metric (3).\nEquation (3) states that d (\u00b7|\u00b7) is the length of a minimizing geodesic of X and Y and therefore an intrinsic metric, because it coincides with the infimum of the length of all admissible curves from X to Y . In addition, we find that the topology of X\u0393 induced by the metric d coincides with the quotient topology induced by the topology of the Euclidean space X ."}, {"heading": "3.4 Orbifold Functions", "text": "Suppose that Q = (X , \u0393, \u03c0) is an orbifold. An orbifold function is a mapping\nf : X\u0393 \u2192 R.\nThe lift of f is a function f\u0303 : X \u2192 R\nsatisfying f\u0303 = f \u25e6 \u03c0. The lift f\u0303 is invariant under group actions of \u0393 , that is f\u0303(x) = f\u0303 (\u03b3(x)) for all \u03b3 \u2208 \u0393 .\nWe say, an orbifold function f : X\u0393 \u2192 R is continuous (locally Lipschitz, differentiable, generalized differentiable) at X \u2208 X\u0393 if its lift f\u0303 is continuous (locally Lipschitz, differentiable, generalized differentiable) at some vector representation x \u2208 X. The definition is independent of the choice of the vector representation that projects to X (see Section B.1, Prop. 1 \u2013 Prop. 4). For a definition of generalized differentiable functions and their basic properties we refer to Section A.\nExample 7. Consider the Riemannian orbifold (X , \u0393, \u03c0) of Example 1-5. The function\nfY : X\u0393 \u2192 R, X 7\u2192 k(X,Y )\nfor some Y \u2208 X\u0393 is an orbifold function with lift\nf\u0303Y : X \u2192 R, x 7\u2192 max {\u3008x,y\u3009, \u3008x, \u03b3(y)\u3009},\nwhere y \u2208 Y . Analytical properties of f such as continuity and differentiability can be investigated using the lift f\u0303 of f . For example, if f\u0303 is differentiable at x \u2208 X then it is also differentiable at \u03b3(x) according to Prop. 3. Hence, differentiability of the orbifold function f is well-defined at X."}, {"heading": "3.5 Gradients and Generalized Gradients of Orbifold Functions", "text": "We extend the notion of gradient and generalized gradient to differentiable and generalized differentiable orbifold functions.\nGradient of Differentiable Orbifold Functions. Suppose that f : X\u0393 \u2192 R is differentiable at X \u2208 X\u0393 . Then its lift f\u0303 : X \u2192 R is differentiable at all vector representations that project to X. The gradient \u2207f(X) of f at X is defined by the projection\n\u2207f(X) = \u03c0 ( \u2207f\u0303(x) ) of the gradient \u2207f\u0303(x) of f\u0303 at a vector representation x \u2208 X. This definition is independent of the choice of the vector representation. We have\n\u2207f\u0303(\u03b3(x)) = \u03b3 ( \u2207f\u0303(x) ) for all \u03b3 \u2208 \u0393 . This implies that the gradients of f\u0303 at x and \u03b3(x) are vector representations of the same structure, namely the gradient\u2207f(X) of the orbifold function f at X. Thus, the gradient of f at X is a well-defined structure pointing to the direction of steepest ascent (see Section B.1, Prop. 3).\nSubdifferential of Generalized Differentiable Orbifold Functions. Suppose that f : X\u0393 \u2192 R is generalized differentiable at X \u2208 X\u0393 . Then its lift f\u0303 : X \u2192 R is generalized differentiable at all vector representations that project to X. The subdifferential \u2202f(X) of f at X is defined by the projection\n\u2202f(X) = \u03c0 ( \u2202f\u0303(x) ) of the subdifferential \u2202f\u0303(x) of f\u0303 at a vector representation x \u2208 X. This definition is independent of the choice of the vector representation. We have\n\u2202f\u0303(\u03b3(x)) = \u03b3 ( \u2202f\u0303(x) ) for all \u03b3 \u2208 \u0393 . This implies that the subdifferentials \u2202f\u0303(x) \u2286 X and \u2202f\u0303(\u03b3(x)) \u2286 X are subsets that project to the same subset of X\u0393 , namely the subdifferential \u2202f(X) (see Section B.1, Prop. 4).\nThe properties of generalized differentiable function as listed in Section A carry over to generalized differentiable orbifold functions via their lifts. For example, a generalized differentiable orbifold function is locally Lipschitz and therefore differentiable almost everywhere.\nExample 8. Let (GA, d) be a graph space, where\nd(X,Y ) = min \u03c6\u2208A(X,Y ) d\u03c6(X,Y )\nis a graph edit distance. We can identify GA with a Riemannian orbifold Q = (X , \u0393, \u03c0) and the graph edit distance d (\u00b7|\u00b7) with a distance function defined on X\u0393 . Suppose that the cost functions d\u03c6 (\u00b7|\u00b7) of the edit paths are continuously differentiable (generalized differentiable). Then the distance d (\u00b7|\u00b7) is generalized differentiable.\nExample 9. Let Q be a Riemannian orbifold of attributed graphs. Then (i) an optimal assignment kernel k (\u00b7|\u00b7), (ii) the intrinsic metric d (\u00b7|\u00b7) induced by k (\u00b7|\u00b7), and (iii) the squared metric d (\u00b7|\u00b7)2 are generalized differentiable."}, {"heading": "3.6 Integration on Orbifolds", "text": "Suppose that Q = (X , \u0393, \u03c0) is a Riemannian orbifold with singular set SQ. In order to integrate orbifold functions f : X\u0393 \u2192 R by the Lebesgue integral, we need to construct an appropriate measurable space together with an orbifold measure. The measurable space is defined by the Borel set B(X\u0393 ) generated by the open sets of X\u0393 . From the orbifold measure we expect that it is compatible with the local Riemannian measures. In addition, we demand that the singular set SQ has measure 0. This is motivated by the following fact: The singular set is covered locally by the finite union of totally geodesic submanifolds, which has measure 0 relative to the local canonical Riemannian measure. Since the projection to the orbifold is distance decreasing, it is reasonable to ask for an orbifold measure that assigns measure 0 to the singular set SQ.\nLet B (X\u0393 \\ SQ) denote the Borel set generated by the open sets of X\u0393 \\ SQ. Then there exists a complete canonical measure \u00b5 on the the Borel set B (X\u0393 \\ SQ) given by a unique volume form on X\u0393 \\ SQ. The measure \u00b5 can be extended to a complete measure \u03bd on the Borel set B(X\u0393 ) such that\n\u03bd (A) = \u00b5 (A \\ SQ) = \u222b A\\SQ d\u00b5.\nIn particular, we have \u03bd(A) = 0 for any subset A \u2286 SQ. For proofs we refer to [4].\nIn the following we write\u222b U\u0393 f(X)dX = \u222b U\u0393 fd\u03bd\nfor the integral of an orbifold function f : U\u0393 \u2192 R defined on a measurable subset U\u0393 \u2286 X\u0393 . We tacitly assume that all integrals occurring in the following sections exist."}, {"heading": "4 Graph Quantization", "text": "This section extends vector quantization to quantization of graphs."}, {"heading": "4.1 The Basics", "text": "Suppose that Q = (X , \u0393, \u03c0) is a Riemannian orbifold. A graph quantizer of size k is a mapping of the form\nQ : X\u0393 \u2192 C\nwhere C = {Y1, . . . , Yk} \u2286 X\u0393 is a finite set, called codebook. The elements Yj \u2208 C are the code graphs. The graph quantizer Q partitions the input space X\u0393 into k disjoint regions\nRj = {X \u2208 X\u0393 : Q(X) = Yj}\nsuch that their union covers X\u0393 . By PQ we denote the partition of Q consisting of all k regions Rj .\nSuppose that J = {1, . . . , k}. The basic operation of a vector quantizer Q can be written as a composition Q = dQ \u25e6 eQ of an encoder eQ : X\u0393 \u2192 J and a decoder dQ : J \u2192 C. The encoder assigns each input graph to a region via the index set J . The decoder maps indices of J referring to regions to code graphs."}, {"heading": "4.2 Graph Quantizer Performance", "text": "We measure the performance of a graph quantizer Q by the expected distortion D(Q) = EX [d (X,Q(X))] = \u222b X\u0393 d(X,Q(X))dP (X),\nwhere X \u2208 X\u0393 is a random variable with probability measure P = PX\u0393 representing the observable graphs to be quantized. The expectation EX is taken with respect to some probability space (X\u0393 , \u03a3X\u0393 , PX\u0393 ). The quantity d(X,Y ) measures the distortion of the random input graph X and code graph Y . Here we consider graph distortion measures that are graph edit distances. An example is the squared metric induced by an optimal alignment kernel\nd (X,Y ) = min x\u2208X,y\u2208Y\n\u2016x\u2212 y\u20162\nUsing the codebook and partition for the given quantizer Q, we can rewrite the expected distortion by\nD(C) = k\u2211 j=1 \u222b Rj d(X,Y )dP (X)."}, {"heading": "4.3 The Problem of Optimal Graph Quantizer Design", "text": "The problem of optimal graph quantizer design is stated as follows: Find a codebook C specifying the decoder dQ and a partition PQ specifying the encoder eQ such that the expected distortionD(Q) is minimized. The composite mapping Q = dQ \u25e6 eQ of the resulting encoder and decoder is then an optimal graph quantizer.\nAn optimal graph quantizer satisfies the following necessary conditions, also known as the Lloyd-Max conditions:\n1. Nearest Neighbor Condition. Given a fixed codebook C, a graph quantizer Q is optimal, if the code vector Q(X) of an input pattern X satisfies the nearest neighbor rule\nQ(X) = argmin Y \u2208C d (X,Y )\nfor all X \u2208 X\u0393 , where ties are resolved according to some rule. A proof is given in Section B.2, Theorem 3.\n2. Centroid Condition. Given a fixed partition PQ, a vector quantizer Q is optimal, if each code vector Yj is the centroid of region Rj , that is\nYj = arg min Y \u2208X\u0393\nE [d (X,Y ) |X \u2208 Rj ]\nfor all Y \u2208 X\u0393 and all j \u2208 J . A proof is given in Section B.2, Theorem 4.\nNote that Yj with\nYj = arg min Y \u2208X\u0393\nE [d (X,Y ) |X \u2208 Rj ]\nis called a centroid of region Rj . The centroids may not be unique. This also holds for squared metrics induced by some optimal assignment kernel, which are the counterparts of squared Euclidean distances."}, {"heading": "4.4 Graph Quantizer Design", "text": "Since the distribution P = PX\u0393 of the observable graphs is usually unknown, the expected distortion D(C) can neither be computed nor be minimized directly. Instead, we design (estimate) an optimal quantizer from empirical data. For vectors, prominent methods for designing an optimal quantizer are k-means and simple competitive learning. Both methods, k-means and simple competitive learning have been extended for designing graph quantizers in the context of prototype based clustering. To derive consistency results for k-means and simple competitive learning in the domain of graphs, we consider estimators based on empirical distortions and on stochastic approximation.\nEstimators based on Empirical Distortion Measures. In order to derive consistency results, we restrict the set of feasible codebooks to a compact subspace\nW \u2282 X k\u0393 = X\u0393 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X\u0393\ufe38 \ufe37\ufe37 \ufe38 k-times\nof the topological space X k\u0393 . The problem of designing an optimal quantizer for graphs is then of the form\nmin C\u2208W D(C) = k\u2211 j=1 \u222b Rj d(X,Y )dP (X).\nwhere the minimum is taken over the compact set W rather than X k\u0393 . Let\n1. D\u2217 be the set of minimal values of the expected distortion D(C), 2. W\u2217 = {C \u2208 W : D(C) = D\u2217} be the set of true (optimal) codebooks, and 3. W\u2217\u03b5 = {C \u2208 W : D(C) \u2264 D\u2217 + \u03b5} be the set of approximate solutions.\nTo design an optimal graph quantizer, we minimize the empirical distortion\nD\u0302N (C) = 1\nN N\u2211 i=1 min j\u2208J d (Xi, Yj) ,\nwhere C \u2208 W and S = {X1, . . . , XN} is a training set consisting of N independent graphs Xi drawn from X\u0393 . Let\n1. D\u0302\u2217N be the set of minimal values of the empirical distortion D\u0302N (C), 2. W\u2217N = {C \u2208 W : D\u0302N (C) = D\u0302\u2217N} be the set of empirical codebooks, and 3. W\u2217N\u03b5 = {C \u2208 W : D\u0302N (C) \u2264 D\u0302\u2217N + \u03b5} be the set of approximate solutions.\nThe next result shows that estimators based on empirical distortions are consistent estimators.\nTheorem 1. Suppose that Q = (X , \u0393, \u03c0) is a Riemannian orbifold, d(X,Y ) is a locally Lipschitz metric on X\u0393 with integrable Lipschitz constant, and W \u2286 X k\u0393 is compact. Then we have\nlim N\u2192\u221e\nD\u0302\u2217N (\u03c9) = D \u2217\nlim N\u2192\u221e\nW\u2217N (\u03c9) =W\u2217\nlim N\u2192\u221e\nW\u2217 N (\u03c9) =W\u2217\nalmost surely.\nThe proof follows from [8] applied to the lift d\u0303 of distortion d. Examples of locally Lipschitz distance metrics on X\u0393 with integrable Lipschitz constants are metrics induced by an optimal alignment kernel\nd(X,Y ) = min x\u2208X,y\u2208Y\n\u2016x\u2212 y\u2016\nas well as d(X,Y )2.\nK-Means. In order to extend the standard k-means method to graphs for constructing an empirical codebook, we use the following update rule\nyt+1j = 1\nN tj N\u2211 i=1 qtijxi,\nwhere t > 0 is the iteration, xi \u2208 Xi and ytj \u2208 Y tj are vector representations that are optimally aligned,1 and Qt = ( qtij ) is the matrix representation of the nearest neighbor quantizer Qt restricted to the training set S. The elements of Qt are of the form\nqtij =\n{ 1 : Qt(Xi) = Y t j\n0 : otherwise .\n1 Recall that two vector representations x \u2208 X and y \u2208 Y are optimally aligned if \u2016x\u2212 y\u2016 = d(X,Y )\nThe quantity N tj denotes the number of elements from the training sets that are quantized by code graph Y tj .\nAs for vectors, a drawback of k-means for graphs is that it is a local optimization technique for which existing consistency theorems are inapplicable, because Theorem 1 assumes global instead of local minimizers of the empirical distortion as estimators.\nEstimators based on Stochastic Optimization. Suppose that W = X k\u0393 . Stochastic optimization methods directly minimize the expected distortion\nD (C) = k\u2211 j=1 \u222b Rj d (X,Yj) dP (X)\n= k\u2211 j=1 \u222b X\u0393 min 1\u2264j\u2264k d (X,Yj) dP (X),\nusing a training set S = {X1, . . . , XN} of N independent graphs Xi drawn from X\u0393 . We assume that the loss function\nL(X, C) = min 1\u2264j\u2264k d (X,Yj)\nis generalized-differentiable, hence L(X, C) is differentiable almost everywhere.\nExample 10. If he graph distortion d(\u00b7|\u00b7) is generalized differentiable, then the loss function L(X, C) is also generalized differentiable by calculus of generalized differentiable functions. This holds for graph distortions of Example 8 and 9.\nSince the interchange of integral and generalized gradient remains valid for generalized differentiable loss functions, that is\n\u2202D(C) = EX [\u2202L(X, C)]\nunder mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method:\nyt+1 = yt + \u03b7t (xt \u2212 yt), (4)\nwhere xt is a vector representation of input pattern Xt \u2208 S, which is optimally aligned to vector representation yt of a code graph Yt closest to Xt. The random elements st = xt \u2212 yt \u2208 St are vector representations of stochastic generalized gradients St, i.e. random variables defined on the probability space (X\u0393 , \u03a3X\u0393 , PX\u0393 ) \u221e such that\nE [St | C0, . . . , Ct] \u2208 \u2202D (C) . (5)\nWe consider the following conditions for almost sure convergence of stochastic optimization:\nA1 The sequence (\u03b7t)t\u22650 of step sizes satisfies\n\u03b7t > 0, lim t\u2192\u221e \u03b7t = 0, \u221e\u2211 t=1 \u03b7t =\u221e, \u221e\u2211 t=1 \u03b72t <\u221e.\nA2 The stochastic generalized gradients (St)t\u22650 satisfy (5). A3 The expected squared norm of stochastic generalized gradients (St)t\u22650 is\nbounded by E [ \u2016St\u20162 ] < +\u221e.\nThe next result shows that the SGG method is a consistent estimator. Theorem 2. Let Q = (X , \u0393, \u03c0) be a Riemannian orbifold and let d(X,Y ) be a generalized differentiable metric on X\u0393 . Suppose that assumptions (A1) \u2212 (A3) hold. Then the sequence (Ct)t\u22650 generated by the SGG method converges almost surely to graphs satisfying necessary extremum conditions\nW\u2217 = {C \u2208 W : 0 \u2208 \u2202D(C)} .\nBesides the sequence (D(Ct))t\u22650 converges almost surely and we have\nlim t\u2192\u221e\nD(Ct) \u2208 D(W\u2217).\nThe proof is a direct consequence of Ermoliev and Norkin\u2019s Theorem [11] applied on the lift d\u0303 (\u00b7|\u00b7) of d (\u00b7|\u00b7)."}, {"heading": "5 Remarks to GQ using the Graph Edit Distance", "text": "In many applications, the graph edit distance is discontinuous. Examples include edit distances with constant non-zero deletion and/or insertion cost. A necessary (but not sufficient) condition for the consistency results stated in Theorem 1 and 2 is that the underlying graph distortion is locally Lipschitz. Hence, both consistency results are inapplicable for discontinuous graph distortions. Let us consider both cases separately.\nEstimators based on Empirical Distortion Measures. Estimators based on empirical distortion measures aim at approximating the expected distortion D(C) by its empirical mean\nmin C\u2208W\nD\u0302N (C) = 1\nN N\u2211 i=1 min j\u2208J d (Xi, Yj) .\nAs shown in [10], minimizing the empirical distortion is often meaningless, if the underlying graph edit distance function d (\u00b7|\u00b7) and thus D\u0302N (C) is discontinuous, even if the expectation D(C) may be continuously differentiable. Since the local solutions of D\u0302N (C) may have nothing in common with the local solutions of the original problem, estimators based on the empirical distortion D\u0302N (C) can be statistically inconsistent. Hence, minimizing D\u0302N (C) with underlying discontinuous graph edit distance using global or local optimization techniques like, for example, k-means lacks theoretical support.\nEstimators based on Stochastic Optimization. The situation is better for estimators based on methods from stochastic optimization. For discontinuous graph edit distances d (\u00b7|\u00b7) the expected distortion can be minimized in a statistically consistent way, for example, by methods based on approximations of d (\u00b7|\u00b7) via averaged functions obtained by convolution with so-called mollifiers. For details, we refer to [9]."}, {"heading": "6 Conclusion", "text": "This contribution proposes a theoretical sound foundation of graph quantization generalizing the ideas of vector quantizations to the domain of attributed graph. We presented consistency results for graph quantizer design, where the underlying graph edit distances is generalized differentiable. As for vectors, estimators based on empirical distortion and stochastic optimization are statistically consistent. If the underlying distortion measure is a discontinuous graph edit distance, estimators based on empirical distortion measures lack theoretical justification. Thus, the proposed consistency results justify existing research on prototype-based clustering in the domain of graphs. In addition, we showed that the Lloyd-Max conditions are necessary conditions for optimality of GQ.\nThe mathematical framework that enables us to derive consistency results are Riemannian orbifolds. Identifying graphs with points in a Riemannian orbifold provides us locally access to a Euclidean space. This in turn allows us to introduce geometrical and analytical concepts for extending vector quantization to the domain of graphs. The implication of this approach is that it provides us a template for consistently linking methods from structural pattern recognition other than GQ to statistical pattern recognition methods.\nAcknowledgments. The first author is very grateful to Vladimir Norkin for his kind support and valuable comments."}, {"heading": "A Generalized Differentiable Functions", "text": "Let X = Rn be a finite-dimensional Euclidean space. A function f : X \u2192 R is generalized differentiable at x \u2208 X in the sense of Norkin [27] if there is a multi-valued map \u2202f : X \u2192 2X in a neighborhood of x such that\n1. \u2202f(x) is a convex and compact set; 2. \u2202f(x) is upper semicontinuous at x, that is, if yi \u2192 x and gi \u2208 \u2202f(yi) for\neach i \u2208 N, then each accumulation point g of (gi) is in \u2202f(x); 3. for each y \u2208 X there is a g \u2208 \u2202f(y) with f(y) = f(x)+\u3008g,y \u2212 x\u3009+o (x,y, g),\nwhere lim i\u2192\u221e |o (x,yi, gi)| \u2016yi \u2212 x\u2016 = 0\nfor all sequences yi \u2192 y and gi \u2192 g with gi \u2208 \u2202f (yi).\nWe call f generalized differentiable if it is generalized differentiable at each point x \u2208 X . The set \u2202f(x) is the subdifferential of f at x and its elements are called generalized gradients.\nGeneralized differentiable functions have the following properties [27]:\n(GD1) Generalized differentiable functions are locally Lipschitz and therefore continuous and differentiable almost everywhere. (GD2) Continuously differentiable, convex, and concave functions are generalized differentiable. (GD3) Suppose that f1, . . . , fn : X \u2192 R are generalized differentiable at x \u2208 X . Then\nf\u2217(x) = min(f1(x), . . . , fm(x)) f\u2217(x) = max(f1(x), . . . , fm(x))\nare generalized differentiable at x \u2208 X . (GD4) Suppose that f1, . . . , fm : X \u2192 R are generalized differentiable at x \u2208 X\nand f0 : Rm \u2192 R is generalized differentiable at y = (f1(x), . . . , fm(x)) \u2208 Rm. Then f(x) = f0(f1(x), . . . , fm(x)) is generalized differentiable at x \u2208 X . The subdifferential of f at x is of the form\n\u2202f(x) = con { g \u2208 X : g = [ g1g2 . . . gm ] g0,\ng0 \u2208 \u2202f0(y), gi \u2208 \u2202fi(x), 1 \u2264 i \u2264 m } .\nwhere [g1g2 . . . gm] is a (N \u00d7m)-matrix. (GD5) Suppose that F (x) = Ez [f(x, z)], where f(\u00b7, z) is generalized differen-\ntiable. Then F is generalized differentiable and its subdifferential at x \u2208 X is of the form \u2202F (x) = Ez [\u2202f(x, z)]."}, {"heading": "B Proofs", "text": "Suppose thatQ = (X , \u0393, \u03c0) is a Riemannian orbifold. By U\u03b4(x) = {x\u2032 : \u2016x\u2032\u2016 < \u03b4} we denote the open ball with center x and radius \u03b4 > 0. Note that U\u03b4(\u03b3(x)) = \u03b3 (U\u03b4(x)) for all \u03b3 \u2208 \u0393 .\nB.1 Orbifold Functions\nContinuous Orbifold Functions\nProposition 1. Let f : X\u0393 \u2192 R be an orbifold function. Suppose that its lift f\u0303 : X \u2192 R is continuous at a vector representation x that projects to X \u2208 X\u0393 . Then f\u0303 is continuous at \u03b3(x) for all \u03b3 \u2208 \u0393 .\nProof. Let \u03b3 \u2208 \u0393 be a permutation and x\u2032 = \u03b3(x). Suppose that (y\u2032i)i\u2208N is a sequence with y\u2032i \u2192 x\u2032. Then there is a sequence (yi)i\u2208N with \u03b3(yi) = y\u2032i for each i \u2208 N. Since permutations are homeomorphisms, we find that\nlim i\u2192\u221e yi = lim i\u2192\u221e\n\u03b3\u22121(y\u2032i) = \u03b3 \u22121(x\u2032) = x.\nFrom continuity of f\u0303 at x follows that f\u0303(yi)\u2192 f\u0303(x). Since f\u0303 is invariant under group actions from \u0393 , we have f\u0303(x) = f\u0303(x\u2032) and f\u0303(yi) = f\u0303(y\u2032i) for each i \u2208 N. We obtain\nlim i\u2192\u221e f\u0303 (y\u2032i) = lim i\u2192\u221e f\u0303 (yi) = f\u0303(x) = f\u0303(x \u2032).\nThis proves that f\u0303 is continuous at each vector representation that projects to X. ut\nLocally Lipschitz Orbifold Functions\nProposition 2. Let f : X\u0393 \u2192 R be an orbifold function. Suppose that its lift f\u0303 : X \u2192 R is locally Lipschitz at a vector representation x that projects to X \u2208 X\u0393 . Then f\u0303 is locally Lipschitz at \u03b3(x) for all \u03b3 \u2208 \u0393 .\nProof. Since f\u0303 is locally Lipschitz at x there is a L \u2265 0 and \u03b4 > 0 such that\u2223\u2223\u2223f\u0303(y)\u2212 f\u0303(z)\u2223\u2223\u2223 \u2264 L \u2016y \u2212 z\u2016 for all y, z \u2208 U\u03b4(x). Let \u03b3 \u2208 \u0393 be a permutation and x\u2032 = \u03b3(x). Since \u03b3 is an isometric homeomorphism, we have U\u03b4(x\u2032) = \u03b3 (U\u03b4(x)). From \u0393 -invariance of f\u0303 and the isometric property of \u03b3 follows\u2223\u2223\u2223f\u0303(y\u2032)\u2212 f\u0303(z\u2032)\u2223\u2223\u2223 = \u2223\u2223\u2223f\u0303(y)\u2212 f\u0303(z)\u2223\u2223\u2223 \u2264 L \u2016y \u2212 z\u2016 = L \u2016y\u2032 \u2212 z\u2032\u2016 for all y\u2032, z\u2032 \u2208 U\u03b4(x\u2032), where y = \u03b3\u22121(y\u2032) \u2208 U\u03b4(x) and z = \u03b3\u22121(z) \u2208 U\u03b4(x). This proves that f\u0303 is locally Lipschitz at each vector representation that projects to X. ut\nDifferentiable Orbifold Functions\nProposition 3. Let f : X\u0393 \u2192 R be an orbifold function. Suppose that its lift f\u0303 : X \u2192 R is differentiable at a vector representation x that projects to X \u2208 X\u0393 . Then f\u0303 is differentiable at \u03b3(x) for all \u03b3 \u2208 \u0393 . The gradient of f\u0303 at \u03b3(x) is of the form\n\u2207f\u0303(\u03b3(x)) = \u03b3 ( \u2207f\u0303(x) ) .\nProof. Since the lift f\u0303 of f is differentiable at x, there is a \u03b4 > 0 such that f\u0303(x+ h) = f\u0303(x) + \u2329 \u2207f\u0303 (x),h \u232a + o(h)\nfor all h \u2208 U\u03b4(0). Let x\u2032 be an arbitrary vector representation that projects to X. Then there is a \u03b3 \u2208 \u0393 with x\u2032 = \u03b3(x). Since f\u0303 is invariant under the group actions of \u0393 , we have f\u0303(x\u2032) = f\u0303(x). Then for each h\u2032 \u2208 U\u03b4(0), we find that\nf\u0303(x\u2032 + h\u2032)\u2212 f\u0303(x\u2032) = f\u0303(x+ h)\u2212 f\u0303(x) = \u2329 \u2207f\u0303 (x),h \u232a + o(h),\nwhere h \u2208 X with \u03b3(h) = h\u2032. Since the elements of \u0393 are isometries, we have \u2016h\u2016 = \u2016h\u2032\u2016 giving h \u2208 U\u03b4(0). In addition, from isometry of \u03b3 follows\n\u3008fx,h\u3009 = \u2329 \u03b3 ( \u2207f\u0303 (x) ) , \u03b3(h) \u232a = \u2329 \u03b3 ( \u2207f\u0303 (x) ) ,h\u2032 \u232a .\nWe obtain f\u0303(x\u2032 + h\u2032)\u2212 f\u0303(x\u2032) = \u2329 \u03b3 ( \u2207f\u0303 (x) ) ,h\u2032 \u232a + o\u2032(h\u2032),\nwhere o\u2032(h\u2032) = o \u25e6 \u03b3\u22121(h\u2032) satisfies\nlim h\u2032\u21920\no\u2032(h\u2032)\n\u2016h\u2032\u2016 = lim h\u2032\u21920\no(\u03b3\u22121(h\u2032))\n\u2016h\u2032\u2016 = lim h\u2032\u21920\no(\u03b3\u22121(h\u2032)) \u2016\u03b3\u22121(h\u2032)\u2016 = 0.\nThis proves that f\u0303 is differentiable at each vector representation that projects to X. In addition, from the proof follows that the gradient of f\u0303 at x\u2032 = \u03b3(x) is of the form\n\u2207f\u0303 (x\u2032) = \u03b3 ( \u2207f\u0303 (x) ) .\nut\nGeneralized Differentiable Orbifold Functions\nProposition 4. Let f : X\u0393 \u2192 R be an orbifold function. Suppose that its lift f\u0303 : X \u2192 R is generalized differentiable at a vector representation x that projects to X \u2208 X\u0393 . Then f\u0303 is generalized differentiable at \u03b3(x) for all \u03b3 \u2208 \u0393 and\n\u2202f\u0303(\u03b3(x)) = \u03b3 ( \u2202f\u0303(x) ) .\nis a subdifferential of f\u0303 at \u03b3(x) for all \u03b3 \u2208 \u0393 .\nProof. Since f\u0303 is generalized differentiable at x, there is a multi-valued mapping \u2202f\u0303 : U\u03b4(x)\u2192 2X defined on some neighborhood U\u03b4(x). Let \u03b3 \u2208 \u0393 be an arbitrary permutation and x\u2032 = \u03b3(x). Then\n\u2202f\u0303 : U\u03b4(x\u2032)\u2192 2X , y\u2032 = \u03b3(y) 7\u2192 \u03b3 ( \u2202f\u0303(y) ) is a multi-valued mapping in a neighborhood of x\u2032.\nSince \u03b3 is a homeomorphic linear map, we find that \u03b3(\u2202f\u0303(x)) = \u2202f\u0303(x\u2032) is a convex and compact set.\nNext we show that f\u0303 is upper semicontinuous at x\u2032. Suppose that y\u2032i \u2192 x\u2032, g\u2032i \u2208 f\u0303c(y\u2032i) for each i \u2208 N, and g\u2032 is an accumulation point of (g\u2032i)i\u2208N. Then there is a i0 \u2208 N such that y\u2032i \u2208 U\u03b4(x\u2032) for all i \u2265 i0. From\nU\u03b4(x\u2032) = U\u03b4(\u03b3(x)) = \u03b3 (U\u03b4(x))\nfollows that there are vector representations yi \u2208 U\u03b4(x) with \u03b3(yi) = y\u2032i for each i \u2265 i0. From continuity of \u03b3\u22121 follows that yi \u2192 x. By construction of \u2202f\u0303 follows that\ng\u2032i \u2208 \u2202f\u0303 (y\u2032i) = \u2202f\u0303 (\u03b3 (yi)) = \u03b3 ( \u2202f\u0303 (yi) ) for each i \u2265 i0. Hence, there are vector representations gi \u2208 \u2202f\u0303(yi) with \u03b3(gi) = g\u2032i for each i \u2265 i0. Since f\u0303 is upper semicontinuous at x, we find that g \u2208 \u2202f\u0303(x). Again by construction of \u2202f\u0303 follows that\ng\u2032 = \u03b3(g) \u2208 \u03b3 ( \u2202f\u0303(x) ) = \u2202f\u0303 (\u03b3(x)) = \u2202f\u0303(x\u2032).\nThis proves upper semicontinuity of \u2202f\u0303 at all vector representations projecting to X = \u03c0(x).\nFinally, we prove that f\u0303 satisfies the subderivative property at x\u2032. Suppose that y\u2032,y \u2208 X with y\u2032 = \u03b3(y). By \u0393 -invariance of f\u0303 , we have f\u0303(y\u2032) = f\u0303(y). Since f\u0303 is generalized differentiable at x, we find a g \u2208 \u2202f\u0303(y) such that\nf\u0303(y\u2032) = f\u0303(y) = f\u0303(x) + \u3008g,y \u2212 x\u3009+ o(x,y, g)\nwith o(x,y, g) tending faster to zero than \u2016y \u2212 x\u2016. Let g\u2032 = \u03b3(g). Exploiting \u0393 -invariance of f\u0303 as well as isometry and linearity of \u03b3 yields\nf\u0303(y\u2032) = f\u0303(\u03b3(x)) + \u3008\u03b3(g), \u03b3(y \u2212 x)\u3009+ o(x,y, g) = f\u0303(x\u2032) + \u3008g\u2032,y\u2032 \u2212 x\u2032\u3009+ o(x,y, g).\nWe define o\u2032(x\u2032,y\u2032, g\u2032) = o \u25e6 \u03b3\u22121(x\u2032,y\u2032, g\u2032) = o(x,y, g) showing that o\u2032 tends faster to zero than normy\u2032 \u2212 x. This proves the subderivative property of f\u0303 at all vector representations projecting to X = \u03c0(x).\nPutting all results together yields that f\u0303 is generalized differentiable at \u03b3(x) for all \u03b3 \u2208 \u0393 . ut\nB.2 Lloyd-Max Necessary Conditions for Optimality\nDue to the comparable nice analytical properties of Riemannian orbifolds, the proofs for the nearest neighbor and centroid condition of optimal graph quantizers are similar to their respective counterparts in vector quantization.\nTheorem 3 (Nearest Neighbor Condition). Suppose that C is a fixed codebook. Any graph quantizer Q : X\u0393 \u2192 C with\nQ(X) = argmin Y \u2208C d (X,Y )\nfor all X \u2208 X\u0393 , where ties are resolved according to some rule, has minimal expected distortion.\nProof. Suppose that Q\u2032 : X\u0393 \u2192 C is a graph quantizer with arbitrary regions. Then we have\nd(X,Q\u2032(X)) \u2265 min Y \u2208Y d(X,Y ) = d(X,Q(X))\nfor all X \u2208 X\u0393 . This implies\nD(Q\u2032) = EX [d (X,Q\u2032(X))] \u2265 EX [d (X,Q(X))] = D(Q).\nut\nTheorem 4 (Nearest Neighbor Condition). Suppose that PQ is a fixed partition and Q : X\u0393 \u2192 C a graph quantizer with codebook C satisfying\nYj = arg min Y \u2208X\u0393\nE [d (X,Y ) |X \u2208 Rj ]\nfor all Y \u2208 X\u0393 and all j \u2208 J . Then Q has minimal expected distortion.\nProof. Let Pj = P (X \u2208 Rj). Suppose that Q\u2032 is a quantizer with partition {R1, . . . ,Rk} and arbitrary codebook C = {Y \u20321 , . . . , Y \u2032k}. Then we have\nE [d(X,Q\u2032(X))] = k\u2211 j=1 PjE [d(X,Q\u2032(X)) |X \u2208 Rj ]\n= k\u2211 j=1 PjE [ d(X,Y \u2032j ) |X \u2208 Rj ] \u2265\nk\u2211 j=1 Pj min Y \u2208X\u0393 E [d(X,Y ) |X \u2208 Rj ]\n= k\u2211 j=1 PjE [d(X,Yj) |X \u2208 Rj ] = E [d(X,Q(X))]\nut"}, {"heading": "1. H. Almohamad and S. Duffuaa, \"A linear programming approach for the weighted", "text": "graph matching problem\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(5)522\u2013525, 1993. 2. H. Bunke and B.T. Messmer, \"Similarity measures for structured representations\", Lecture Notes in Computer Science, 837.106\u2013118, 1994. 3. H. Bunke, P. Foggia, C. Guidobaldi, and M. Vento, \"Graph clustering using the weighted minimum common supergraph\" Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science, 2726:235\u2013246, 2003 4. J.E. Borzellino, Riemannian geometry of orbifolds, PhD thesis, University of California, Los Angelos, 1992. 5. T.S. Caetano, L. Cheng, Q.V. Le, and A.J. Smola, \"Learning graph matching\" International Conference on Computer Vision, p. 1\u20138, 2007.\n6. T. Cour, P. Srinivasan, and J. Shi, \"Balanced graph matching\", NIPS 2006 Conference Proceedings, 2006. 7. R.O. Duda, P.E. Hart, and D.G. Stork Pattern Classification, Wiley & Sons, 2000. 8. Y.M. Ermoliev and V.I. Norkin, \"Normalized convergence in stochastic optimiza-\ntion\", Annals of Operations Research, 30:187\u2013198, 1991, 9. Y.M. Ermoliev, V.I. Norkin, and R. Wets, \"The minimization of discontinuous func-\ntions: mollifier subgradients\", SIAM Journal on Control and Optimization, 33:149\u2013 167, 1995. 10. Y.M. Ermoliev and V.I. Norkin, \"On nonsmooth and discontinuous problems of stochastic systems optimization\", European Journal of Operational Research, 101:230\u2013244, 1997. 11. Y. M. Ermoliev and V.I. Norkin, \"Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization\", Cybernetics and Systems Analysis, 34(2), 196\u2013215, 1998. 12. M. Ferrer, Theory and algorithms on the median graph. application to graph-based classification and clustering, PhD Thesis, Univ. Aut\u2018onoma de Barcelona, 2007. 13. M. Ferrer, E. Valveny, F. Serratosa, I. Bardaj\u00ed, and H. Bunke, \"Graph-Based kMeans Clustering: A Comparison of the Set Median versus the Generalized Median Graph\" CAIP 2009 Conference Proceedings, 2009. 14. A. Gersho and R.M. Gray, Vector Quantization and Signal Compression, Kluwer Academic Publishers, 1992. 15. S. Gold and A. Rangarajan, \"Graduated Assignment Algorithm for Graph Matching\", IEEE Trans. Pattern Analysis and Machine Intelligence, 18:377\u2013388, 1996. 16. S. Gold, A. Rangarajan, and E. Mjolsness, \"Learning with preknowledge: clustering with point and graph matching distance measures\" Neural Computation, 8(4):787\u2013 804, 1996. 17. S. G\u00fcnter and H. Bunke, \"Self-organizing map for clustering in the graph domain\", Pattern Recognition Letters, 23(4):405\u2013417, 2002. 18. M. Hagenbuchner, A. Sperduti, and A.C. Tsoi, \u00d2A Self-Organizing Map for Adaptive Processing of Structured Data,\u00d3 IEEE Transaction on Neural Networks, 14:491\u2013505, 2003. 19. B. Jain and F. Wysotzki, \"Central Clustering of Attributed Graphs\", Machine Learning, 56, 169\u2013207, 2004. 20. B. Jain and K. Obermayer, \"On the sample mean of graphs\", IJCNN 2008 Conference Proceedings, p. 993\u20131000, 2008. 21. B. Jain and K. Obermayer, \"Structure Spaces\", Journal of Machine Learning Research, 10:2667\u20132714, 2009. 22. B. Jain and K. Obermayer, \"Accelerating Competitive Learning Graph Quantization\", Computer Vision and Image Understanding, 2009 (submitted). 23. B. Jain and K. Obermayer, \"Elkan\u2019s k-Means for Graphs\", arXiv:0912.4598v1 [cs.AI], 2009. 24. Y. Linde, A. Buzo, and R. M. Gray, \u00d2An algorithm for vector quantizer design,\u00d3 IEEE Transactions on Communications, 28:84\u201395, 1980. 25. S.P. Lloyd, \u00d2Least squares quantization in PCM\u00d3, IEEE Transactions on Information Theory, 28:129\u2013137, 1982, reprint of 1957. 26. M.A. Lozano and F. Escolano, \"ACM attributed graph clustering for learning classes of images\", Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science, 2726:247\u2013258, 2003 27. V.I. Norkin, \"Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization\", Cybernetics, 22(6), 804\u2013809, 1986."}, {"heading": "28. A. Schenker, M. Last, H. Bunke, and A. Kandel, \"Clustering of web documents", "text": "using a graph model\", Web Document Analysis: Challenges and Opportunities, p. 1\u201316, 2003. 29. A. Schenker, M. Last, H. Bunke, and A. Kandel, Graph-Theoretic Techniques for Web Content Mining, World Scientific Publishing, 2005. 30. S. Theodoridis and K. Koutroumbas, Pattern Recognition, Elsevier, 2009. 31. A. Torsello and E.R. Hancock, \"Learning shape-classes using a mixture of\ntree-unions\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(6):954-967, 2006. 32. S. Umeyama, \"An eigendecomposition approach to weighted graph matching problems\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(5):695\u2013 703, 1988. 33. M. Van Wyk, M. Durrani, and B. Van Wyk, \"A RKHS interpolator-based graph matching algorithm\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7):988\u2013995, 2002."}], "references": [{"title": "Duffuaa, \"A linear programming approach for the weighted graph matching problem", "author": ["S.H. Almohamad"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Messmer, \"Similarity measures for structured representations", "author": ["B.T.H. Bunke"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Graph clustering using the weighted minimum common supergraph\" Graph Based Representations in Pattern Recognition, Lecture", "author": ["H. Bunke", "P. Foggia", "C. Guidobaldi", "M. Vento"], "venue": "Notes in Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Riemannian geometry of orbifolds", "author": ["J.E. Borzellino"], "venue": "PhD thesis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Learning graph matching", "author": ["T.S. Caetano", "L. Cheng", "Q.V. Le", "A.J. Smola"], "venue": "International Conference on Computer Vision, p", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Balanced graph matching", "author": ["T. Cour", "P. Srinivasan", "J. Shi"], "venue": "NIPS 2006 Conference Proceedings,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Norkin, \"Normalized convergence in stochastic optimization", "author": ["V.I.Y.M. Ermoliev"], "venue": "Annals of Operations Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "The minimization of discontinuous functions: mollifier subgradients", "author": ["Y.M. Ermoliev", "V.I. Norkin", "R. Wets"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Norkin, \"On nonsmooth and discontinuous problems of stochastic systems optimization", "author": ["V.I.Y.M. Ermoliev"], "venue": "European Journal of Operational Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Norkin, \"Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization", "author": ["V.I.Y.M. Ermoliev"], "venue": "Cybernetics and Systems Analysis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Theory and algorithms on the median graph. application to graph-based classification and clustering", "author": ["M. Ferrer"], "venue": "PhD Thesis, Univ. Aut\u2018onoma de Barcelona,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Graph-Based kMeans Clustering: A Comparison of the Set Median versus the Generalized Median Graph", "author": ["M. Ferrer", "E. Valveny", "F. Serratosa", "I. Bardaj\u00ed", "H. Bunke"], "venue": "CAIP 2009 Conference Proceedings,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Vector Quantization and Signal Compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "Graduated Assignment Algorithm for Graph Matching", "author": ["S. Gold", "A. Rangarajan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Learning with preknowledge: clustering with point and graph matching distance measures", "author": ["S. Gold", "A. Rangarajan", "E. Mjolsness"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Self-organizing map for clustering in the graph domain", "author": ["S. G\u00fcnter", "H. Bunke"], "venue": "Pattern Recognition Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "\u00d2A Self-Organizing Map for Adaptive Processing of Structured Data,\u00d3", "author": ["M. Hagenbuchner", "A. Sperduti", "A.C. Tsoi"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Central Clustering of Attributed Graphs", "author": ["B. Jain", "F. Wysotzki"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "On the sample mean of graphs", "author": ["B. Jain", "K. Obermayer"], "venue": "IJCNN 2008 Conference Proceedings,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Structure Spaces", "author": ["B. Jain", "K. Obermayer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Accelerating Competitive Learning Graph Quantization", "author": ["B. Jain", "K. Obermayer"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Elkan\u2019s k-Means for Graphs", "author": ["B. Jain", "K. Obermayer"], "venue": "[cs.AI],", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "\u00d2Least squares quantization in PCM\u00d3", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1982}, {"title": "ACM attributed graph clustering for learning classes of images", "author": ["M.A. Lozano", "F. Escolano"], "venue": "Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization", "author": ["V.I. Norkin"], "venue": "Cybernetics, 22(6),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1986}, {"title": "Clustering of web documents using a graph model\", Web Document Analysis: Challenges and Opportunities", "author": ["A. Schenker", "M. Last", "H. Bunke", "A. Kandel"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Graph-Theoretic Techniques for Web Content Mining, World", "author": ["A. Schenker", "M. Last", "H. Bunke", "A. Kandel"], "venue": "Scientific Publishing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Learning shape-classes using a mixture of tree-unions", "author": ["A. Torsello", "E.R. Hancock"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "An eigendecomposition approach to weighted graph matching problems", "author": ["S. Umeyama"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1988}, {"title": "A RKHS interpolator-based graph matching algorithm", "author": ["M. Van Wyk", "M. Durrani", "B. Van Wyk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2002}], "referenceMentions": [{"referenceID": 12, "context": "Vector quantization is a classical technique from signal processing suitable for lossy data compression, density estimation, and prototype-based clustering [7, 14, 30].", "startOffset": 156, "endOffset": 167}, {"referenceID": 22, "context": "The kmeans algorithm is also commonly referred to as the Linde-Buzo-Gray (LBG) algorithm [24] the generalized Lloyd algorithm [25].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 15, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 16, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 17, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 18, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 20, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 10, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 11, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 17, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 18, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 21, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 25, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 26, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 2, "context": "Related clustering method are presented in [3, 26, 31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 23, "context": "Related clustering method are presented in [3, 26, 31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 27, "context": "Related clustering method are presented in [3, 26, 31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 1, "context": "For a more detailed definition of the graph edit distance, we refer to [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 4, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 5, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 13, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 28, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 29, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 21, "context": "As shown in [23], d is indeed a metric and can be expressed as a graph edit distance.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "For all other proofs we refer to [4, 21].", "startOffset": 33, "endOffset": 40}, {"referenceID": 19, "context": "For all other proofs we refer to [4, 21].", "startOffset": 33, "endOffset": 40}, {"referenceID": 3, "context": "For proofs we refer to [4].", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "The proof follows from [8] applied to the lift d\u0303 of distortion d.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "under mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method: yt+1 = yt + \u03b7t (xt \u2212 yt), (4)", "startOffset": 28, "endOffset": 36}, {"referenceID": 24, "context": "under mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method: yt+1 = yt + \u03b7t (xt \u2212 yt), (4)", "startOffset": 28, "endOffset": 36}, {"referenceID": 9, "context": "The proof is a direct consequence of Ermoliev and Norkin\u2019s Theorem [11] applied on the lift d\u0303 (\u00b7|\u00b7) of d (\u00b7|\u00b7).", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "As shown in [10], minimizing the empirical distortion is often meaningless, if the underlying graph edit distance function d (\u00b7|\u00b7) and thus D\u0302N (C) is discontinuous, even if the expectation D(C) may be continuously differentiable.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "For details, we refer to [9].", "startOffset": 25, "endOffset": 28}, {"referenceID": 24, "context": "A function f : X \u2192 R is generalized differentiable at x \u2208 X in the sense of Norkin [27] if there is a multi-valued map \u2202f : X \u2192 2X in a neighborhood of x such that", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Generalized differentiable functions have the following properties [27]:", "startOffset": 67, "endOffset": 71}], "year": 2010, "abstractText": "Vector quantization(VQ) is a lossy data compression technique from signal processing, which is restricted to feature vectors and therefore inapplicable for combinatorial structures. This contribution presents a theoretical foundation of graph quantization (GQ) that extends VQ to the domain of attributed graphs. We present the necessary Lloyd-Max conditions for optimality of a graph quantizer and consistency results for optimal GQ design based on empirical distortion measures and stochastic optimization. These results statistically justify existing clustering algorithms in the domain of graphs. The proposed approach provides a template of how to link structural pattern recognition methods other than GQ to statistical pattern recognition.", "creator": "TeX"}}}