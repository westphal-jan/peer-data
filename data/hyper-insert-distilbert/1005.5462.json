{"id": "1005.5462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2010", "title": "On the clustering aspect of nonnegative matrix factorization", "abstract": "while this paper repeatedly provides a theoretical explanation on the clustering aspect of nonnegative matrix random factorization ( extended nmf ). we alternately prove alternatively that even vectors without formally imposing orthogonality or additive sparsity or constraint on the basis components and / imaginary or variance coefficient matrix, continuous nmf still coefficients can give clustering results, thus providing a direct theoretical textual support for the works concluded of yun xu qing et al. [ round 1 ]... and kim jin et al. [ 2 ], provided where the authors showed the superiority capabilities of since the standard used nmf as a stationary clustering method.", "histories": [["v1", "Sat, 29 May 2010 15:27:16 GMT  (58kb)", "https://arxiv.org/abs/1005.5462v1", "4 pages, no figure"], ["v2", "Sat, 12 Jun 2010 10:40:53 GMT  (59kb)", "http://arxiv.org/abs/1005.5462v2", "4 pages, no figure, to appear in ICEIE 2010"]], "COMMENTS": "4 pages, no figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andri mirzal", "masashi furukawa"], "accepted": false, "id": "1005.5462"}, "pdf": {"name": "1005.5462.pdf", "metadata": {"source": "CRF", "title": "On the clustering aspect of nonnegative matrix factorization", "authors": ["Andri Mirzal", "Masashi Furukawa"], "emails": ["andri@complex.eng.hokudai.ac.jp", "mack@complex.eng.hokudai.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 5.\n54 62\nv2 [\ncs .L\nG ]\n1 2\nJu n\n20 10\nKeywords\u2014bound-constrained optimization, clustering method, non-convex optimization, nonnegative matrix factorization\nI. INTRODUCTION\nNMF is a matrix approximation technique that factorizes a nonnegative matrix into a pair of other nonnegative matrices of much lower rank:\nA \u2248 BC, (1)\nwhere A \u2208 RM\u00d7N+ = [a1, . . . , aN ] denotes the featureby-item data matrix, B \u2208 RM\u00d7K+ = [b1, . . . ,bK ] denotes the basis matrix, C \u2208 RK\u00d7N+ = [c1, . . . , cN ] denotes the coefficient matrix, and K denotes the number of factors which usually chosen so that K \u226a min(M,N). There are also other variants of NMF like semi-NMF, convex NMF, and symmetric NMF. Detailed discussions can be found in, e.g., [3] and [4].\nThe nonnegativity constraints and the reduced dimensionality define the uniqueness and power of NMF. The nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors bk to construct the data vectors an, thus providing the parts-based interpretations as shown in [5], [6], [7]. And the reduced dimensionality provides NMF with the clustering aspect and data compression capabilities.\nThe most important NMF\u2019s application is in the data clustering, as some works have shown that it is a superior method compared to the standard clustering methods like spectral methods and K-means algorithm. In particular, Xu et al. [1] showed that NMF outperforms standard spectral methods in finding the document clustering in two text corpora, TDT2 and Reuters. And Kim et al. [2] showed that NMF and sparse NMF are much more superior methods compared to the K-means algorithm in both a synthetic dataset (which is well separated) and a real dataset (TDT2).\nIf sparsity constraints are imposed to columns of C, the clustering aspect of NMF is intuitive since in the extreme case where there is only one nonzero entry per column, NMF will be equivalent to the K-means algorithm employed to the data vectors an [8], and the sparsity constraints can be thought as the relaxation to the strict orthogonality constraints on rows of\nC (an equivalent explanation can also be stated for imposing sparsity on rows of B).\nHowever, as reported by Xu et al. [1] and Kim et al. [2], even without imposing sparsity constraints, NMF still can give very promising clustering results. But the authors didn\u2019t give any theoretical analysis on why the standard NMF\u2014NMF without sparsity nor orthogonality constraint\u2014can give such good results. So far the best explanation for this remarkable fact is only qualitative: the standard NMF produces nonorthogonal latent semantic directions (the basis vectors) that are more likely to correspond to each of the clusters than those produced by the spectral methods, thus the clustering induced from the latent semantic directions of the standard NMF are better than clustering by the spectral methods [1]. Therefore, this work attempts to provide a theoretical support for the clustering aspect of the standard NMF."}, {"heading": "II. CLUSTERING ASPECT OF NMF", "text": "To compute B and C, usually eq. 1 is rewritten into a minimization problem in the Frobenius norm criterion.\nmin B,C\nJ (B,C) = 1\n2 \u2016A\u2212BC\u20162F s.t. B \u2265 0,C \u2265 0. (2)\nIn addition to the usual Frobenius norm criterion, the family of Bregman divergences\u2014which Frobenius norm and KullbackLeibler divergence are part of it\u2014can also be used as the affinity measures. Detailed discussion on the Bregman divergences for NMF can be found in [9].\nSometimes it is more practical and intuitive to decompose J (B,C) into a series of smaller objectives.\nmin B,C J (B,C) \u2261\n(\nmin B,c1 J1(B, c1), . . . , min B,cN JN (B, cN )\n)\n,\n(3)\nwhere\nmin B,cn\nJn (B, cn) = 1\n2 \u2016an \u2212Bcn\u2016 2 2, n \u2208 [1, N ]. (4)\nMinimizing Jn is known to be the nonnegative least square (NLS) problem, and some fast NMF algorithms are developed based on solving the NLS subproblems, e.g., alternating NLS with block principal pivoting algorithm [10], active set method [11], and projected quasi-Newton algorithm [12]. Decomposing NMF problem into NLS subproblems also transforms the non-convex optimization in eq. 3 to the convex optimization subproblems in eq. 4. Even though eq. 4 is not strictly convex,\nfor two-block case, any limit point of the sequence {Bt,Ct}, where t is the updating step, is a stationary point [13].\nThe objective in eq. 4 aims to simultaneously find the suitable basis vectors such that the latent factors are revealed, and the coefficient vector cn such that a linear combination of the basis vectors (Bcn) is close to an. In clustering term this can be rephrased as: to simultaneously find the cluster centers and the cluster assignments.\nTo investigate the clustering aspect of NMF, four possibilities of NMF settings are discussed: (1) imposing orthogonality constraints on both rows of C and columns of B, (2) imposing orthogonality constraints on rows of C, (3) imposing orthogonality constraints on columns of B, and (4) no orthogonality constraint is imposed. The last case is the standard NMF which its clustering aspect is the focus of this paper as many works reported that it is a very effective clustering method."}, {"heading": "A. Orthogonality constraints on both B and C", "text": "The following theorems proves that imposing columnorthogonality constraints on B and row-orthogonality constraints on C lead to the simultaneous clustering of similar items and related features.\nTheorem 1. Minimizing the following objective\nmin B,C\nJa (B,C) = 1\n2 \u2016A\u2212BC\u20162F (5)\ns.t. B \u2265 0,C \u2265 0,BTB = I,CCT = I\nis equivalent to applying ratio association to G(ATA) and G(AAT ), where ATA and AAT are the item affinity matrix and the feature affinity matrix respectively, thus leads to simultaneous clustering of similar items and related features.\nProof:\n\u2016A\u2212BC\u20162F = tr ( (A\u2212BC) T (A\u2212BC) )\n= tr ( A T A\u2212 2CTBTA+ I ) . (6)\nThe Lagrangian function:\nLa (B,C) = Ja (B,C)\u2212 tr ( \u0393BB T ) \u2212 tr (\u0393CC)+\ntr ( \u039bB ( B T B\u2212 I )) + tr ( \u039bC ( CC T \u2212 I )) ,\n(7)\nwhere \u0393B \u2208 R M\u00d7K + , \u0393C \u2208 R N\u00d7K + , \u039bB \u2208 R K\u00d7K + , and \u039bC \u2208 R K\u00d7K + are the Lagrange multipliers. By the Karush-KuhnTucker (KKT) optimality conditions we get:\n\u2207BLa = B\u2212AC T \u2212 \u0393B + 2B\u039bB = 0, (8) \u2207CLa = C\u2212B T A\u2212 \u0393T C + 2\u039bCC = 0, (9)\nwith complementary slackness:\n\u0393B \u2297B = 0, \u0393 T C \u2297C = 0, (10)\nwhere \u2297 denotes component-wise multiplications. Assume \u0393B = 0, \u039bB = 0, \u0393C = 0, and \u039bB = 0 (at the stationary point these assumptions are reasonable since the\ncomplementary slackness conditions hold and the Lagrange multipliers can be assigned to zeros), we get:\nB = ACT and (11)\nC = BTA. (12)\nSubstituting eq. 11 into eq. 6, we get:\nmin C Ja (C) \u2261 max C\ntr ( CA T AC T ) . (13)\nSimilarly, substituting eq. 12 into eq. 6, we get:\nmin B Ja (B) \u2261 max B\ntr ( B T AA T B ) . (14)\nTherefore, minimizing Ja is equivalent to simultaneously optimizing:\nmax C\ntr ( CA T AC T ) s.t. CCT = I, and (15)\nmax B\ntr ( B T AA T B ) s.t. BTB = I. (16)\nEq. 15 and eq.16 are the ratio association objectives (see [14] for details on various graph cuts objectives) applied to G(ATA) and G(AA)T respectively. Thus minimizing Ja leads to the simultaneous clustering of similar items and related features."}, {"heading": "B. Orthogonality constraints on C", "text": "When the orthogonality constraints are imposed only on rows of C, it is no longer clear whether columns of B will lead to the feature clustering. The following theorem shows that without imposing the orthogonality constraints on bk, the resulting B can still lead to the feature clustering.\nTheorem 2. Minimizing the following objective\nmin B,C\nJb (B,C) = 1\n2 \u2016A\u2212BC\u20162F (17)\ns.t. B \u2265 0,C \u2265 0,CCT = I\nis equivalent to applying ratio association to G(ATA), and also leads to the feature clustering indicator matrix B which is approximately column-orthogonal.\nProof:\n\u2016A\u2212BC\u20162F = tr ( (A\u2212BC)T (A\u2212BC) )\n= tr ( A T A\u2212 2BTACT +CTBTBC ) .\n(18)\nThe Lagrangian function:\nLb (B,C) = Jb (B,C)\u2212 tr ( \u0393BB T ) \u2212 tr (\u0393CC)+\ntr ( \u039bC ( CC T \u2212 I )) . (19)\nBy applying the KKT conditions, we get:\nB = ACT and (20)\nC = BTA. (21)\nBy substituting eq. 20 and eq. 21 into eq. 18, minimizing Jb is equivalent to simultaneously optimizing:\nmax C\ntr ( CA T AC T ) s.t. CCT = I, (22)\nmax B\ntr ( B T AA T B ) , and (23)\nmin B\ntr ( A T BB T BB T A ) \u2261 min\nB\ntr ( B T BB T B ) . (24)\nNote that the step in eq. 24 is justifiable since A is a constant matrix. By using the fact tr(XTX) = \u2016X\u20162F , eq. 24 can be rewritten as:\nmin B\n\u2225 \u2225B T B \u2225 \u2225 2\nF = min\nB\n(\u2211\ni\n( b T i bi )2 + \u2211\ni6=j\n( b T i bj\n)2 )\n.\n(25) The objective in eq. 22 is equivalent to eq. 15 and eventually leads to the clustering of similar items. So the remaining problem is how to prove that optimizing eq. 23 and 25 simultaneously will lead to the feature clustering indicator matrix B which is approximately column-orthogonal.\nEq. 23 resembles eq. 14, but without orthogonality nor upper bound constraint, so one can easily optimizing eq. 23 by setting B to an infinity matrix. However, this violates eq. 25 which favors small B. Conversely, one can optimizing eq. 25 by setting B to a null matrix, but again this violates eq. 23. Therefore, these two objectives create implicit lower and upper bound constraints on B, and eq. 23 and eq. 25 can be rewritten into:\nmax B\ntr ( B T A\u0302B ) , and (26)\nmin B\n(\u2211\ni\n( b T i bi )2\n\ufe38 \ufe37\ufe37 \ufe38\njb1\n+ \u2211\ni6=j\n( b T i bj )2\n\ufe38 \ufe37\ufe37 \ufe38\njb2\n)\n(27)\ns.t. 0 \u2264 B \u2264 \u03a5B,\nwhere A\u0302 denotes the feature affinity matrix and \u03a5B denotes the upperbound constraints on B. Now we have box-constraint objectives which are known to behave well and are guaranteed to converge to the stationary point [15].\nEven though the objectives are now transformed into boxconstraint optimization problems, since there is no columnorthogonality constraint, maximizing eq. 26 can be easily done by setting each entry of B to the corresponding largest possible value (in graph term this means to only create one partition on G(A\u0302)). But this scenario results in the maximum value of eq. 27, which violates the objective. Conversely, minimizing eq. 27 to the smallest possible value (minimizing jb1 implies minimizing jb2, but not vice versa) violates eq. 26.\nThus, the most reasonable scenario is: setting jb2 as small as possible and balancing jb1 with eq. 26. This scenario is the relaxed ratio association applied to G(A\u0302), and as long as vertices of G(A\u0302) are clustered, simultaneous optimizing eq. 26 and eq. 27 leads to the clustering of related features. Moreover, as jb2 is minimum, B is approximately column-orthogonal."}, {"heading": "C. Orthogonality constraints on B", "text": "Theorem 3. Minimizing the following objective\nmin B,C\nJc (B,C) = 1\n2 \u2016A\u2212BC\u20162F (28)\ns.t. B \u2265 0,C \u2265 0,BTB = I\nis equivalent to applying ratio association to G(AAT ), and also leads to the item clustering indicator matrix C which is approximately row-orthogonal.\nProof: By following the proof of theorem 2, minimizing Jc is equivalent to simultaneously optimizing:\nmax B\ntr ( B T AA T B ) s.t. BTB = I, (29)\nmax C\ntr ( 2CTATAC ) , and (30)\nmin C\ntr ( C T CA T AC T C ) \u2261 min\nC\ntr ( CC T CC T ) . (31)\nEq. 29 is equivalent to eq. 16 and leads to the clustering of related features. And optimizing eq. 30 and Eq. 31 simultaneously is equivalent to:\nmax C\ntr ( CA\u0303C T ) , and (32)\nmin C\n(\u2211\ni\n( c\u030cic\u030c T i )2\n\ufe38 \ufe37\ufe37 \ufe38\njc1\n+ \u2211\ni6=j\n( c\u030cic\u030c T j )2\n\ufe38 \ufe37\ufe37 \ufe38\njc2\n)\n(33)\ns.t. 0 \u2264 C \u2264 \u03a5C,\nwhere A\u0303 denotes the item affinity matrix, c\u030ci denotes the i-th row of C, and \u03a5C denotes the upperbound constraints on C.\nAs in the proof of theorem 2, the most reasonable scenario in simultaneously optimizing eq. 32 and eq. 33 is by setting jc2 as small as possible and balancing jc1 with eq. 32. This leads to the clustering of similar items, and as jc2 is minimum, C is approximately row-orthogonal."}, {"heading": "D. No orthogonality constraint on both B and C", "text": "In this section we prove that applying the standard NMF to the feature-by-item data matrix eventually leads to the simultaneous feature and item clustering.\nTheorem 4. Minimizing the following objective\nmin B,C\nJd (B,C) = 1\n2 \u2016A\u2212BC\u20162F (34)\ns.t. B \u2265 0,C \u2265 0,\nleads to the feature clustering indicator matrix B and the item clustering indicator matrix C which are approximately column- and row-orthogonal respectively.\nProof: By following the proof of theorem 2, minimizing Jd is equivalent to simultaneously optimizing:\nmax B,C\ntr ( B T AC T ) , and (35)\nmin B,C\ntr ( B T BCC T ) . (36)\nBy substituting B = ACT and C = BTA into the above equations, we get:\nmax B\ntr ( B T A\u0302B ) , and (37)\nmin B\ntr ( B T BB T AA T B ) \u2261 min\nB\ntr ( B T BB T B )\n(38)\nfor feature clustering, and:\nmax C\ntr ( CA\u0303C T ) , and (39)\nmin C\ntr ( CA T AC T CC T ) \u2261 min\nC\ntr ( CC T CC T )\n(40)\nfor item clustering. Therefore, minimizing Jd is equivalent to simultaneously optimizing:\nmax B\ntr ( B T A\u0302B ) , (41)\nmin B\n(\u2211\ni\n( b T i bi )2 + \u2211\ni6=j\n( b T i bj\n)2 )\n, (42)\nmax C\ntr ( CA\u0303C T ) , and (43)\nmin C\n(\u2211\ni\n( c\u030cic\u030c T i )2 + \u2211\ni6=j\n( c\u030cic\u030c T j\n)2 )\n, (44)\ns.t. 0 \u2264 B \u2264 \u03a5B, and 0 \u2264 C \u2264 \u03a5C,\nwhich will lead to the feature clustering indicator matrix B and the item clustering indicator matrix C that are approximately column- and row-orthogonal respectively."}, {"heading": "III. UNIPARTITE AND DIRECTED GRAPH CASES", "text": "The affinity matrix W induced from a unipartite (undirected) graph is a symmetric matrix, which is a special case of the rectangular affinity matrix A. Therefore, by following the discussion in section II, it can be shown that the standard NMF applied to W leads to the clustering indicator matrix which is almost orthogonal.\nThe affinity matrix V induced from a directed graph is an asymmetric square matrix. Since columns and rows of V correspond to the same set of vertices with the same order, as the clustering problem is concerned, V can be replaced by V+VT which is a symmetric matrix. Then the standard NMF can be applied to this matrix to get the clustering indicator matrix which is almost orthogonal."}, {"heading": "IV. RELATED WORKS", "text": "Ding et al. [8] provides the theoretical analysis on the equivalences between orthogonal NMF to K-means clustering for both rectangular data matrices and symmetric matrices. However as their proofs utilize the zero gradient conditions, the hidden assumptions (setting the Lagrange multipliers to zeros) are not revealed there. Actually it can be easily shown that their approach is the KKT conditions applied to the unconstrained version of eq. 2. Thus there is no guarantee that minimizing eq. 2 by using the zero gradient conditions leads to the stationary point located on the nonnegative orthant as required by the objective.\nApplying the standard NMF to the symmetric matrix leads to almost orthogonal matrix was previously proven by Ding\net al. [16]. But due to the used approach, the theorem cannot be extended to the rectangular matrices which so far are the usual form of the data (practical applications of NMF seemed exclusively for rectangular matrices). Therefore, their results cannot be used to explain the abundant experimental results that show the power of the standard NMF in clustering, latent factors identification, learning the parts of objects, and producing sparse matrices even without explicit sparsity constraint [5]."}, {"heading": "V. CONCLUSION", "text": "By using the strict KKT optimality conditions, we showed that even without explicitly imposing orthogonality nor sparsity constraint NMF produces approximately columnorthogonal basis matrix and row-orthogonal coefficient matrix which lead to the simultaneous feature and item clustering. This result, therefore, gives the theoretical explanation on some experimental results that show the power of the standard NMF as a clustering tool which are reported to be better than the spectral methods [1] and K-means algorithm [2]."}], "references": [{"title": "Document clustering based on non-negative matrix factorization,", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "Proc. ACM SIGIR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Sparse nonnegative matrix factorization for clustering,", "author": ["J. Kim", "H. Park"], "venue": "CSE Technical Reports, Georgia Institute of Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "The relationships among various nonnegative matrix factorization methods for clustering,", "author": ["T. Li", "C. Ding"], "venue": "Proc. ACM 6th Int\u2019l Conf. on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Convex and Semi-Nonnegative Matrix Factorizations,", "author": ["C. Ding", "T. Li", "M.I. Jordan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Learning the parts of objects by non-negative matrix factorization,", "author": ["D. Lee", "H. Seung"], "venue": "Nature,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Learning spatially localized, parts-based representation,", "author": ["S.Z. Li", "X.W. Hou", "H.J. Zhang", "Q.S. Cheng"], "venue": "Proc. IEEE Comp. Soc. Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Non-negative Matrix Factorization with Sparseness Constraints,", "author": ["P.O. Hoyer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering,", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "Proc. 12th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Generalized nonnegative matrix approximation with Bregman divergences,", "author": ["I.S. Dhillon", "S. Sra"], "venue": "UTCS Technical Reports, The University of Texas at Austin,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Toward faster nonnegative matrix factorization: A new algorithm and comparisons,", "author": ["J. Kim", "H. Park"], "venue": "Proc. 8th IEEE International Conference on Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method,", "author": ["H. Kim", "H. Park"], "venue": "SIAM. J. Matrix Anal. & Appl.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Fast projection-based methods for the least squares nonnegative matrix approximation problem,", "author": ["D. Kim", "S. Sra", "I.S. Dhillon"], "venue": "Stat. Anal. Data Min.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "On the convergence of the block nonlinear Gauss-Seidel method under convex constraints,", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Operation Research Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Weighted Graph Cuts without eigenvectors: A multilevel approach,", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "More, \u201cProjected gradient methods for linearly constrained problems,", "author": ["J.J.P.H. Calamai"], "venue": "Mathematical Programming,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1987}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering,", "author": ["C. Ding", "X. He", "H.D. Simon"], "venue": "Proc. SIAM Data Mining Conference,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "[1] and Kim et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], that show the superiority of the standard NMF as a clustering method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": ", [3] and [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [3] and [4].", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "The nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors bk to construct the data vectors an, thus providing the parts-based interpretations as shown in [5], [6], [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "The nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors bk to construct the data vectors an, thus providing the parts-based interpretations as shown in [5], [6], [7].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "The nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors bk to construct the data vectors an, thus providing the parts-based interpretations as shown in [5], [6], [7].", "startOffset": 204, "endOffset": 207}, {"referenceID": 0, "context": "[1] showed that NMF outperforms standard spectral methods in finding the document clustering in two text corpora, TDT2 and Reuters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] showed that NMF and sparse NMF are much more superior methods compared to the K-means algorithm in both a synthetic dataset (which is well separated) and a real dataset (TDT2).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "If sparsity constraints are imposed to columns of C, the clustering aspect of NMF is intuitive since in the extreme case where there is only one nonzero entry per column, NMF will be equivalent to the K-means algorithm employed to the data vectors an [8], and the sparsity constraints can be thought as the relaxation to the strict orthogonality constraints on rows of C (an equivalent explanation can also be stated for imposing sparsity on rows of B).", "startOffset": 251, "endOffset": 254}, {"referenceID": 0, "context": "[1] and Kim et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], even without imposing sparsity constraints, NMF still can give very promising clustering results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "So far the best explanation for this remarkable fact is only qualitative: the standard NMF produces nonorthogonal latent semantic directions (the basis vectors) that are more likely to correspond to each of the clusters than those produced by the spectral methods, thus the clustering induced from the latent semantic directions of the standard NMF are better than clustering by the spectral methods [1].", "startOffset": 400, "endOffset": 403}, {"referenceID": 8, "context": "Detailed discussion on the Bregman divergences for NMF can be found in [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": ", alternating NLS with block principal pivoting algorithm [10], active set method [11], and projected quasi-Newton algorithm [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": ", alternating NLS with block principal pivoting algorithm [10], active set method [11], and projected quasi-Newton algorithm [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": ", alternating NLS with block principal pivoting algorithm [10], active set method [11], and projected quasi-Newton algorithm [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "for two-block case, any limit point of the sequence {B,C}, where t is the updating step, is a stationary point [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "16 are the ratio association objectives (see [14] for details on various graph cuts objectives) applied to G(AA) and G(AA) respectively.", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "Now we have box-constraint objectives which are known to behave well and are guaranteed to converge to the stationary point [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "[8] provides the theoretical analysis on the equivalences between orthogonal NMF to K-means clustering for both rectangular data matrices and symmetric matrices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Therefore, their results cannot be used to explain the abundant experimental results that show the power of the standard NMF in clustering, latent factors identification, learning the parts of objects, and producing sparse matrices even without explicit sparsity constraint [5].", "startOffset": 274, "endOffset": 277}, {"referenceID": 0, "context": "This result, therefore, gives the theoretical explanation on some experimental results that show the power of the standard NMF as a clustering tool which are reported to be better than the spectral methods [1] and K-means algorithm [2].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "This result, therefore, gives the theoretical explanation on some experimental results that show the power of the standard NMF as a clustering tool which are reported to be better than the spectral methods [1] and K-means algorithm [2].", "startOffset": 232, "endOffset": 235}], "year": 2010, "abstractText": "This paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization (NMF). We prove that even without imposing orthogonality nor sparsity constraint on the basis and/or coefficient matrix, NMF still can give clustering results, thus providing a theoretical support for many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority of the standard NMF as a clustering method. Keywords\u2014bound-constrained optimization, clustering method, non-convex optimization, nonnegative matrix factorization", "creator": "LaTeX with hyperref package"}}}