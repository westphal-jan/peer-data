{"id": "1005.5581", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2010", "title": "Multi-View Active Learning in the Non-Realizable Case", "abstract": "many classical smoothing bounds defined on the higher sample complexity of active temporal learning based uniformly on the usual realizability above assumption there have been derived, yet which show that active learning factors can have exponentially improve keeping the sample complexity over sustained passive learning. ; however, challenging this realizability assumption always could not be met in practice therefore and includes few sufficient results on the generalized exponential maximum improvement present in the sample compartment complexity conditions in whether the non - realizable case has neither been obtained. merely in this paper, may we still theoretically sufficiently characterize enhancing the polynomial sample segment complexity of active principal learning in preserving the non - realizable case with only tsybakov noise condition under capturing the multi - view optimal setting. perhaps we furthermore prove that approaching the sample complexity of dynamic active secondary learning with unbounded tsybakov noise can uniquely be $ \\ widetilde { o } ( \\ _ log \\ e frac { * 1 } { \\ epsilon } ) $, contrasting to that such polynomial gap improvement is the best intrinsic possible forecast achievement dealing with discovering the same noise constraint condition in any single - view setting. we also prove that, contrasting clearly to that weakness in analyzing previous robust polynomial graph bounds wherein the order of $ 1 / \\ epsilon $ q is related below to the pure tsybakov jump noise minimal condition, in general being multi - view estimation setting the best sample complexity of'active learning with low unbounded tsybakov'noise limit is $ \\ phi widetilde { f o } ( \\ frac { number 1 } { \\ & epsilon } ) $, where perhaps the order volume of $ 1 / \\ epsilon $ is independent of containing the greater tsybakov noise condition.", "histories": [["v1", "Mon, 31 May 2010 03:59:35 GMT  (36kb)", "https://arxiv.org/abs/1005.5581v1", "24 pages, 1 figure"], ["v2", "Fri, 29 Oct 2010 09:44:44 GMT  (35kb)", "http://arxiv.org/abs/1005.5581v2", "22 pages, 1 figure"]], "COMMENTS": "24 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wei wang 0028", "zhi-hua zhou"], "accepted": true, "id": "1005.5581"}, "pdf": {"name": "1005.5581.pdf", "metadata": {"source": "CRF", "title": "Multi-View Active Learning in the Non-Realizable Case", "authors": ["Wei Wang", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 5.\n55 81\nv2 [\ncs .L\nG ]\n2 9\nO ct\n2 01\nThe sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multiview setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multiview active learning can be O\u0303(log 1\u01eb ), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is O\u0303(1\u01eb ), where the order of 1/\u01eb is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/\u01eb is related to the parameter in Tsybakov noise. Key words: active learning, non-realizable case"}, {"heading": "1. Introduction", "text": "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle. In this way, the active learner can achieve good performance with much fewer labels than passive learning. The number of these queried labels, which is necessary and sufficient for obtaining a good leaner, is well-known as the sample complexity of active learning.\nMany theoretical bounds on the sample complexity of active learning have been derived based on the realizability assumption (i.e., there exists a hypothesis perfectly separating the data in\n\u2217Corresponding author. Email: zhouzh@nju.edu.cn\nPreprint submitted for review November 1, 2010\nthe hypothesis class) [4, 5, 11, 12, 14, 16]. The realizability assumption, however, rarely holds in practice. Recently, the sample complexity of active learning in the non-realizable case (i.e., the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17]. It is worth noting that these bounds obtained in the non-realizable case match the lower bound \u2126(\u03b7 2\n\u01eb2 ) [19], in the same order as the upper bound O( 1 \u01eb2 ) of passive\nlearning (\u03b7 denotes the generalization error rate of the optimal classifier in the hypothesis class and \u01eb bounds how close to the optimal classifier in the hypothesis class the active learner has to get). This suggests that perhaps active learning in the non-realizable case is not as efficient as that in the realizable case. To improve the sample complexity of active learning in the non-realizable case remarkably, the model of the noise or some assumptions on the hypothesis class and the data distribution must be considered. Tsybakov noise model [21] is more and more popular in theoretical analysis on the sample complexity of active learning. However, existing result [8] shows that obtaining exponential improvement in the sample complexity of active learning with unbounded Tsybakov noise is hard.\nInspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case. In this paper, we present the first analysis on the sample complexity of active learning in the non-realizable case under multi-view setting, where the non-realizability is caused by Tsybakov noise. Specifically:\n-We define \u03b1-expansion, which extends the definition in [3] and [23] to the non-realizable case,\nand \u03b2-condition for multi-view setting.\n-We prove that the sample complexity of active learning with Tsybakov noise under multi-view setting can be improved to O\u0303(log 1\u01eb ) when the learner satisfies non-degradation condition. 1 This exponential improvement holds no matter whether Tsybakov noise is bounded or not, contrasting to single-view setting where the polynomial improvement is the best possible achievement for active learning with unbounded Tsybakov noise.\n-We also prove that, when non-degradation condition does not hold, the sample complexity of active learning with unbounded Tsybakov noise under multi-view setting is O\u0303(1\u01eb ), where the order\n1The O\u0303 notation is used to hide the factor log log( 1 \u01eb ).\nof 1/\u01eb is independent of the parameter in Tsybakov noise, i.e., the sample complexity is always O\u0303(1\u01eb ) no matter how large the unbounded Tsybakov noise is. While in previous polynomial bounds, the order of 1/\u01eb is related to the parameter in Tsybakov noise and is larger than 1 when unbounded Tsybakov noise is larger than some degree (see Section 2). This discloses that, when non-degradation condition does not hold, multi-view setting is still able to lead to a faster convergence rate and our polynomial improvement in the sample complexity is better than previous polynomial bounds when unbounded Tsybakov noise is large.\nThe rest of this paper is organized as follows. After introducing related work in Section 2 and preliminaries in Section 3, we define \u03b1-expansion in the non-realizable case in Section 4. Then we analyze the sample complexity of active learning with Tsybakov noise under multi-view setting with and without the non-degradation condition in Section 5 and Section 6, respectively, and verify the improvement in the sample complexity empirically in Section 7. Finally we conclude the paper in Section 8."}, {"heading": "2. Related Work", "text": "Generally, the non-realizability of learning task is caused by the presence of noise. For learning the task with arbitrary forms of noise, Balcan et al. [2] proposed the agnostic active learning algorithm A2 and proved that its sample complexity is O\u0302(\u03b7 2\n\u01eb2 ). 2 Hoping to get tighter bound on\nthe sample complexity of the algorithm A2, Hanneke [17] defined the disagreement coefficient \u03b8, which depends on the hypothesis class and the data distribution, and proved that the sample complexity of the algorithm A2 is O\u0302(\u03b82 \u03b7 2\n\u01eb2 ). Later, Dasgupta et al. [13] developed a general\nagnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is O\u0302(\u03b8 \u03b7 2\n\u01eb2 ).\nRecently, the popular Tsybakov noise model [21] was considered in theoretical analysis on active learning and there have been some bounds on the sample complexity. For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18]. As for the situation where Tsybakov noise is unbounded,\n2The O\u0302 notation is used to hide the factor polylog( 1 \u01eb ).\nonly polynomial improvement in the sample complexity has been obtained. Balcan et al. [4] assumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the sample complexity of active learning with unbounded Tsybakov noise is O ( \u01eb\u2212 2 1+\u03bb ) (\u03bb > 0 depends on Tsybakov noise). This uniform distribution assumption, however, rarely holds in practice. Castro and Nowak [8] showed that the sample complexity of active learning with unbounded Tsybakov noise is O\u0302 ( \u01eb \u2212 2\u00b5\u03c9+d\u22122\u03c9\u22121 \u00b5\u03c9 ) (\u00b5 > 1 depends on another form of Tsybakov noise, \u03c9 \u2265 1 depends on the Ho\u0308lder smoothness and d is the dimension of the data). This result is also based on the strong uniform distribution assumption. Cavallanti et al. [9] assumed that the labels of examples are generated according to a simple linear noise model and indicated that the sample complexity of active learning with unbounded Tsybakov noise is O ( \u01eb \u2212 2(3+\u03bb) (1+\u03bb)(2+\u03bb) ) . Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity O\u0302 ( \u01eb\u2212 2 1+\u03bb ) for active learning with unbounded Tsybakov noise. For active learning with unbounded Tsybakov noise, Castro and Nowak [8] also proved that at least \u2126(\u01eb\u2212\u03c1) labels are requested to learn an \u01eb-approximation of the optimal classifier (\u03c1 \u2208 (0, 2) depends on Tsybakov noise). This result shows that the polynomial improvement is the best possible achievement for active learning with unbounded Tsybakov noise in single-view setting. Wang [22] introduced smooth assumption to active learning with approximate Tsybakov noise and proved that if the classification boundary and the underlying distribution are smooth to \u03be-th order and \u03be > d, the sample complexity of active learning is O\u0302 ( \u01eb\u2212 2d \u03be+d ) ; if the boundary and the distribution are infinitely smooth, the sample complexity of active learning is O ( polylog(1\u01eb ) ) . Nevertheless, this result is for approximate Tsybakov noise and the assumption on large smoothness order (or infinite smoothness order) rarely holds for data with high dimension d in practice."}, {"heading": "3. Preliminaries", "text": "In multi-view setting, the instances are described with several different disjoint sets of features. For the sake of simplicity, we only consider two-view setting in this paper. Suppose that X = X1 \u00d7X2 is the instance space, X1 and X2 are the two views, Y = {0, 1} is the label space and D is the distribution over X \u00d7 Y . Suppose that c = (c1, c2) is the optimal Bayes classifier, where c1 and c2 are the optimal Bayes classifiers in the two views, respectively. Let H1 and H2 be the hypothesis class in each view and suppose that c1 \u2208 H1 and c2 \u2208 H2. For any instance\nx = (x1, x2), the hypothesis hv \u2208 Hv (v = 1, 2) makes that hv(xv) = 1 if xv \u2208 Sv and hv(xv) = 0 otherwise, where Sv is a subset ofXv . In this way, any hypothesis hv \u2208 Hv corresponds to a subset Sv of Xv (as for how to combine the hypotheses in the two views, see Section 5). Considering that x1 and x2 denote the same instance x in different views, we overload Sv to denote the instance set {x = (x1, x2) : xv \u2208 Sv} without confusion. Let S \u2217 v correspond to the optimal Bayes classifier cv. It is well-known [15] that S \u2217 v = {xv : \u03d5v(xv) \u2265 1 2}, where \u03d5v(xv) = P (y = 1|xv). Here, we also overload S\u2217v to denote the instances set {x = (x1, x2) : xv \u2208 S \u2217 v}. The error rate of a hypothesis Sv under the distribution D is R(hv) = R(Sv) = Pr(x1,x2,y)\u2208D ( y 6= I(xv \u2208 Sv) ) . In general, R(S\u2217v ) 6= 0 and the excess error of Sv can be denoted as follows, where Sv\u2206S \u2217 v = (Sv \u2212S \u2217 v )\u222a (S \u2217 v \u2212Sv) and d(Sv , S \u2217 v) is a pseudo-distance between the sets Sv and S \u2217 v .\nR(Sv)\u2212R(S \u2217 v ) =\n\u222b\nSv\u2206S\u2217v\n|2\u03d5v(xv)\u2212 1|pxvdxv , d(Sv, S \u2217 v ) (1)\nLet \u03b7v denote the error rate of the optimal Bayes classifier cv which is also called as the noise rate in the non-realizable case. In general, \u03b7v is less than 1 2 . In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv\u2208Xv (|\u03d5v(xv) \u2212 1/2| \u2264 t) \u2264 C0t \u03bb for some finite C0 > 0, \u03bb > 0 and all 0 < t \u2264 1/2, where \u03bb = \u221e corresponds to the best learning situation and the noise is called bounded [8]; while \u03bb = 0 corresponds to the worst situation. When \u03bb < \u221e, the noise is called unbounded [8]. According to Proposition 1 in [21], it is easy to know that (2) holds.\nd(Sv, S \u2217 v ) \u2265 C1d k \u2206(Sv, S \u2217 v ) (2)\nHere k = 1+\u03bb\u03bb , C1 = 2C \u22121/\u03bb 0 \u03bb(\u03bb + 1) \u22121\u22121/\u03bb, d\u2206(Sv, S \u2217 v) = Pr(Sv \u2212 S \u2217 v) + Pr(S \u2217 v \u2212 Sv) is also a pseudo-distance between the sets Sv and S \u2217 v , and d(Sv , S \u2217 v) \u2264 d\u2206(Sv, S \u2217 v ) \u2264 1. We will use the following lamma [1] which gives the standard sample complexity for non-realizable learning task.\nLemma 1 Suppose that H is a set of functions from X to Y = {0, 1} with finite VC-dimension V \u2265 1 and D is the fixed but unknown distribution over X \u00d7 Y . For any \u01eb, \u03b4 > 0, there is a positive constant C, such that if the size of sample {(x1, y1), . . . , (xN , yN )} from D is N(\u01eb, \u03b4) = C \u01eb2 ( V + log(1\u03b4 ) ) , then with probability at least 1\u2212 \u03b4, for all h \u2208 H, the following holds.\n| 1\nN \u2211N i=1 I ( h(xi) 6= yi ) \u2212E(x,y)\u2208DI ( h(x) 6= y ) | \u2264 \u01eb"}, {"heading": "4. \u03b1-Expansion in the Non-realizable Case", "text": "Multi-view active learning first described in [20] focuses on the contention points (i.e., unlabeled instances on which different views predict different labels) and queries some labels of them. It is motivated by that querying the labels of contention points may help at least one of the two views to learn the optimal classifier. Let S1 \u2295 S2 = (S1 \u2212 S2) \u222a (S2 \u2212 S1) denote the contention points between S1 and S2, then Pr(S1 \u2295 S2) denotes the probability mass on the contentions points. \u201c\u2206\u201d and \u201c\u2295\u201d mean the same operation rule. In this paper, we use \u201c\u2206\u201d when referring the excess error between Sv and S \u2217 v and use \u201c\u2295\u201d when referring the difference between the two views S1 and S2. In order to study multi-view active learning, the properties of contention points should be considered. One basic property is that Pr(S1 \u2295 S2) should not be too small, otherwise the two views could be exactly the same and two-view setting would degenerate into single-view setting.\nIn multi-view learning, the two views represent the same learning task and generally are consistent with each other, i.e., for any instance x = (x1, x2) the labels of x in the two views are the same. Hence we first assume that S\u22171 = S \u2217 2 = S \u2217. As for the situation where S\u22171 6= S \u2217 2 , we will discuss on it further in Section 5.2. The instances agreed by the two views can be denoted as (S1 \u2229 S2) \u222a (S1 \u2229 S2). However, some of these agreed instances may be predicted different label by the optimal classifier S\u2217, i.e., the instances in (S1 \u2229 S2 \u2212 S \u2217) \u222a (S1 \u2229 S2 \u2212 S\u2217). Intuitively, if the contention points can convey some information about (S1 \u2229 S2 \u2212 S \u2217) \u222a (S1 \u2229 S2 \u2212 S\u2217), then querying the labels of contention points could help to improve S1 and S2. Based on this intuition and that Pr(S1 \u2295S2) should not be too small, we give our definition on \u03b1-expansion in the non-realizable case.\nDefinition 1 D is \u03b1-expanding if for some \u03b1 > 0 and any S1 \u2286 X1, S2 \u2286 X2, (3) holds.\nPr ( S1 \u2295 S2 ) \u2265 \u03b1 ( Pr ( S1 \u2229 S2 \u2212 S \u2217 ) + Pr ( S1 \u2229 S2 \u2212 S\u2217 )) (3)\nWe say that D is \u03b1-expanding with respect to hypothesis class H1 \u00d7H2 if the above holds for all S1 \u2208 H1\u2229X1, S2 \u2208 H2 \u2229X2 (here we denote by Hv \u2229Xv the set {h\u2229Xv : h \u2208 Hv} for v = 1, 2).\nBalcan et al. [3] also gave a definition of expansion, Pr(T1\u2295T2) \u2265 \u03b1min [ Pr(T1\u2229T2), P r(T1\u2229T2) ] , for realizable learning task under the assumptions that the learner in each view is never \u201cconfident but wrong\u201d and the learning algorithm is able to learn from positive data only. Here Tv denotes\nthe instances which are classified as positive confidently in each view. Generally, in realizable learning tasks, we aim at studying the asymptotic performance and assume that the performance of initial classifier is better than guessing randomly, i.e., Pr(Tv) > 1/2. This ensures that Pr(T1 \u2229 T2) is larger than Pr(T1 \u2229 T2). In addition, in [3] the instances which are agreed by the two views but are predicted different label by the optimal classifier can be denoted as T1 \u2229 T2. So, it can be found that Definition 1 and the definition of expansion in [3] are based on the same intuition that the amount of contention points is no less than a fraction of the amount of instances which are agreed by the two views but are predicted different label by the optimal classifiers."}, {"heading": "5. Multi-view Active Learning with Non-degradation Condition", "text": "In this section, we first consider the multi-view learning in Table 1 and analyze whether multiview setting can help improve the sample complexity of active learning in the non-realizable case remarkably. In multi-view setting, the classifiers are often combined to make predictions and many strategies can be used to combine them. In this paper, we consider the following two combination schemes, h+ and h\u2212, for binary classification:\nhi+(x) =    1 if hi1(x1) = h i 2(x2) = 1\n0 otherwise hi\u2212(x) =\n   0 if hi1(x1) = h i 2(x2) = 0\n1 otherwise (4)\n5.1. The Situation Where S\u22171 = S \u2217 2\nWith (4), the error rate of the combined classifiers hi+ and h i \u2212 satisfy (5) and (6), respectively.\nR(hi+)\u2212R(S \u2217) = R(Si1 \u2229 S i 2)\u2212R(S \u2217) \u2264 d\u2206(S i 1 \u2229 S i 2, S \u2217) (5) R(hi\u2212)\u2212R(S \u2217) = R(Si1 \u222a S i 2)\u2212R(S \u2217) \u2264 d\u2206(S i 1 \u222a S i 2, S \u2217) (6)\nHere Siv \u2282 Xv (v = 1, 2) corresponds to the classifier h i v \u2208 Hv in the i-th round. In each round of multi-view active learning, labels of some contention points are queried to augment the training data set L and the classifier in each view is then refined. As discussed in [23], we also assume that the learner in Table 1 satisfies the non-degradation condition as the amount of labeled training examples increases, i.e., (7) holds, which implies that the excess error of Si+1v is no larger than that of Siv in the region of S i 1 \u2295 S i 2.\nPr ( Si+1v \u2206S \u2217 \u2223\u2223Si1 \u2295 Si2 ) \u2264 Pr(Siv\u2206S \u2217 \u2223\u2223Si1 \u2295 Si2) (7)\nTo illustrate the non-degradation condition, we give the following example: Suppose the data in Xv (v = 1, 2) fall into n different clusters, denoted by \u03c0 v 1 , . . . , \u03c0 v n, and every cluster has the same probability mass for simplicity. The positive class is the union of some clusters while the negative class is the union of the others. Each positive (negative) cluster \u03c0v\u03be in Xv is associated with only 3 positive (negative) clusters \u03c03\u2212v\u03c2 (\u03be, \u03c2 \u2208 {1, . . . , n}) in X3\u2212v (i.e., given an instance xv in \u03c0 v \u03be , x3\u2212v will only be in one of these \u03c03\u2212v\u03c2 ). Suppose the learning algorithm will predict all instances in each cluster with the same label, i.e., the hypothesis class Hv consists of the hypotheses which do not split any cluster. Thus, the cluster \u03c0v\u03be can be classified according to the posterior probability P (y = 1|\u03c0v\u03be ) and querying the labels of instances in cluster \u03c0 v \u03be will not influence the estimation of the posterior probability for cluster \u03c0v\u03c2 (\u03c2 6= \u03be). It is evident that the non-degradation condition holds in this task. Note that the non-degradation assumption may not always hold, and we will discuss on this in Section 6. Now we give Theorem 1.\nTheorem 1 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to Definition 1, when the non-degradation condition holds, if s = \u2308 2 log 1 8\u01eb\nlog 1 C2\n\u2309 and mi =\n256kC C21 ( V +log(16(s+1)\u03b4 ) ) , the multi-view active learning in Table 1 will generate two classifiers hs+ and hs\u2212, at least one of which is with error rate no larger than R(S \u2217) + \u01eb with probability at least 1\u2212 \u03b4. Here, V = max[V C(H1), V C(H2)] where V C(H) denotes the VC-dimension of the hypothesis class H, k = 1+\u03bb\u03bb , C1 = 2C \u22121/\u03bb 0 \u03bb(\u03bb+ 1) \u22121\u22121/\u03bb and C2 = 5\u03b1+8 6\u03b1+8 .\nProof: Let Qi = S i 1 \u2295 S i 2. First we prove that if each view Xv (v = 1, 2) satisfies Tsybakov noise condition, i.e., Prxv\u2208Xv(|\u03d5v(xv)\u2212 1/2| \u2264 t) \u2264 C3t \u03bb3 for some finite C3 > 0, \u03bb3 > 0 and all 0 < t \u2264 1/2, Tsybakov noise condition can also be met in Qi, i.e., Prxv\u2208Qi(|\u03d5v(xv)\u22121/2|\u2264t)\nPr(Qi) \u2264 C4t\n\u03bb4\nfor some finite C4 > 0, \u03bb4 > 0 and all 0 < t \u2264 1/2. Suppose Tsybakov noise condition cannot be met in Qi, then for C\u2217 = C3\nPr(Qi) and \u03bb\u2217 = \u03bb3, there exists some 0 < t\u2217 \u2264 1/2 to satisfy that\nPrxv\u2208Qi (|\u03d5v(xv)\u22121/2|\u2264t)\nPr(Qi) > C\u2217t\n\u03bb\u2217 \u2217 . So we get\nPrxv\u2208Xv(|\u03d5v(xv)\u2212 1/2| \u2264 t) \u2265 Prxv\u2208Qi(|\u03d5v(xv)\u2212 1/2| \u2264 t) > C3t \u03bb3 \u2217 .\nIt is in contradiction with that Xv satisfies Tsybakov noise condition. Thus, we get that Tsybakov noise condition can also be met in Qi. Without loss of generality, suppose that Tsybakov noise condition in all Qi and Xv can be met for the same finite C0 and \u03bb.\nSince m0 = 256kC C21 ( V + log(16(s+1)\u03b4 ) ) , according to Lemma 1 we know that d(S0v , S \u2217) \u2264 C1 16k with probability at least 1 \u2212 \u03b416(s+1) . With d(Sv, S \u2217 v ) \u2265 C1d k \u2206(Sv, S \u2217 v ), we get d\u2206(S 0 v , S \u2217) \u2264 116 . It is easy to find that d\u2206(S 0 1 \u2229S 0 2 , S \u2217) \u2264 d\u2206(S 0 1 , S \u2217)+d\u2206(S 0 2 , S \u2217) \u2264 1/8 holds with probability at least 1\u2212 \u03b48(s+1) .\nFor i \u2265 0, mi+1 number of labels are queried randomly from Qi. Thus, similarly according to Lemma 1 we have d\u2206(S i+1 1 \u2229 S i+1 2 | Qi, S \u2217 | Qi) \u2264 1/8 with probability at least 1 \u2212 \u03b4 8(s+1) . Let T i+1v = S i+1 v \u2229Qi and \u03c4i+1 = Pr(T i+11 \u2295T i+1 2 \u2212S \u2217)\nPr(T i+11 \u2295T i+1 2 )\n\u2212 12 , it is easy to get\nPr ( S\u2217 \u2229 (Si+11 \u2295 S i+1 2 )|Qi ) \u2212 Pr ( S\u2217 \u2229 (Si+11 \u2295 S i+1 2 )|Qi ) = \u22122\u03c4i+1Pr(S i+1 1 \u2295 S i+1 2 |Qi).\nConsidering the non-degradation condition and d\u2206(S i 1 \u2229 S i 2|Qi, S \u2217|Qi) = d\u2206(S i v|Qi, S \u2217|Qi), we calculate that\nd\u2206(S i+1 1 \u2229 S i+1 2 |Qi, S \u2217|Qi)\n= 1\n2\n( d\u2206(S i+1 1 |Qi, S \u2217|Qi) + d\u2206(S i+1 2 |Qi, S \u2217|Qi) ) + 1\n2 Pr\n( S\u2217 \u2229 (Si+11 \u2295 S i+1 2 )|Qi )\n\u2212 1\n2 Pr\n( S\u2217 \u2229 (Si+11 \u2295 S i+1 2 )|Qi )\n\u2264 1\n2\n( d\u2206(S i 1|Qi, S \u2217|Qi) + d\u2206(S i 2|Qi, S \u2217|Qi) ) \u2212 \u03c4i+1Pr(S i+1 1 \u2295 S i+1 2 |Qi)\n= d\u2206(S i 1 \u2229 S i 2|Qi, S \u2217|Qi)\u2212 \u03c4i+1Pr(S i+1 1 \u2295 S i+1 2 |Qi).\nSo we have\nd\u2206(S i+1 1 \u2229 S i+1 2 , S \u2217)\n= d\u2206(S i+1 1 \u2229 S i+1 2 |Qi, S \u2217|Qi)Pr(Qi) + d\u2206(S i+1 1 \u2229 S i+1 2 |Qi, S \u2217|Qi)Pr(Qi) \u2264 1\n8 Pr(Qi) + d\u2206(S\ni 1 \u2229 S i 2|Qi, S \u2217|Qi)Pr(Qi)\u2212 \u03c4i+1Pr ( (Si+11 \u2295 S i+1 2 ) \u2229Qi ) .\nConsidering d\u2206(S i 1 \u2229 S i 2|Qi, S \u2217|Qi)Pr(Qi) = Pr(S i 1 \u2229 S i 2 \u2212 S \u2217) + Pr(Si1 \u2229 S i 2 \u2212 S \u2217), we have\nd\u2206(S i+1 1 \u2229 S i+1 2 , S \u2217)\n\u2264 Pr(Si1 \u2229 S i 2 \u2212 S \u2217) + Pr(Si1 \u2229 S i 2 \u2212 S\n\u2217) + 1\n8 Pr(Si1 \u2295 S i 2)\u2212 \u03c4i+1Pr\n( (Si+11 \u2295 S i+1 2 ) \u2229Qi ) .\nSimilarly, we get\nd\u2206(S i+1 1 \u222a S i+1 2 , S \u2217)\n\u2264 Pr(Si1 \u2229 S i 2 \u2212 S \u2217) + Pr(Si1 \u2229 S i 2 \u2212 S\n\u2217) + 1\n8 Pr(Si1 \u2295 S i 2) + \u03c4i+1Pr\n( (Si+11 \u2295 S i+1 2 ) \u2229Qi ) .\nLet \u03b3i = Pr(Si1\u2295S i 2\u2212S \u2217)\nPr(Si1\u2295S i 2)\n\u2212 12 , we have\nd\u2206(S i 1 \u2229 S i 2, S \u2217) = d\u2206(S i 1 \u2229 S i 2|Qi, S \u2217|Qi)Pr(Qi) + d\u2206(S i 1 \u2229 S i 2|Qi, S \u2217|Qi)Pr(Qi)\n= (1/2 \u2212 \u03b3i)Pr(S i 1 \u2295 S i 2) + Pr(S i 1 \u2229 S i 2 \u2212 S \u2217) + Pr(Si1 \u2229 S i 2 \u2212 S \u2217)\nand d\u2206(S i 1 \u222a S i 2, S \u2217) = (1/2 + \u03b3i)Pr(S i 1 \u2295 S i 2) + Pr(S i 1 \u2229 S i 2 \u2212 S \u2217) + Pr(Si1 \u2229 S i 2 \u2212 S \u2217).\nAs in each round of the multi-view active learning some contention points of the two views are queried and added into the training set, the difference between the two views is decreasing, i.e., Pr(Si+11 \u2295 S i+1 2 ) is no larger than Pr(S i 1 \u2295 S i 2).\nCase 1: If |\u03c4i+1| \u2264 \u03b3i, with respect to Definition 1, we have\nd\u2206(S i+1 1 \u222a S i+1 2 , S \u2217)\nd\u2206(S i 1 \u222a S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + |\u03c4i+1|Pr(S i+1 1 \u2295 S i+1 2 ) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + \u03b3i)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 (18 + \u03b3i)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + \u03b3i)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 5\u03b1+ 8\n8\u03b1+ 8 ;\nCase 2: If \u2212|\u03c4i+1| > \u03b3i, with respect to Definition 1, we have\nd\u2206(S i+1 1 \u2229 S i+1 2 , S \u2217)\nd\u2206(Si1 \u2229 S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + |\u03c4i+1|Pr(S i+1 1 \u2295 S i+1 2 ) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + |\u03b3i|)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 5\u03b1+ 8\n8\u03b1+ 8 ;\nCase 3: If \u03c4i+1 \u2265 \u03b3i and 0 \u2264 \u03b3i \u2264 1 4 , with respect to Definition 1, we have\nd\u2206(S i+1 1 \u2229 S i+1 2 , S \u2217)\nd\u2206(Si1 \u2229 S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 \u2212 \u03b3i)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 \u03b1+ 8\n2\u03b1+ 8 ;\nCase 4: If \u03c4i+1 \u2265 \u03b3i and 1 4 < \u03b3i \u2264 1 2 , with respect to Definition 1, we have\nd\u2206(S i+1 1 \u222a S i+1 2 , S \u2217)\nd\u2206(Si1 \u222a S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + \u03c4i+1Pr(S i+1 1 \u2295 S i+1 2 ) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + \u03b3i)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 5\u03b1+ 8\n6\u03b1+ 8 ;\nCase 5: If \u03c4i+1 < \u03b3i and \u2212 1 4 \u2264 \u03b3i \u2264 0, with respect to Definition 1, we have\nd\u2206(S i+1 1 \u222a S i+1 2 , S \u2217)\nd\u2206(Si1 \u222a S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + \u03b3i)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 \u03b1+ 8\n2\u03b1+ 8 ;\nCase 6: If \u03c4i+1 < \u03b3i and \u2212 1 2 \u2264 \u03b3i < \u2212 1 4 , with respect to Definition 1, we have\nd\u2206(S i+1 1 \u2229 S i+1 2 , S \u2217)\nd\u2206(Si1 \u2229 S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + |\u03c4i+1|Pr(S i+1 1 \u2295 S i+1 2 ) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + |\u03b3i|)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 5\u03b1+ 8\n6\u03b1+ 8 ;\nCase 7: If \u03c4i+1 \u2264 \u2212\u03b3i and 0 \u2264 \u03b3i \u2264 1 2 , with respect to Definition 1, we have\nd\u2206(S i+1 1 \u222a S i+1 2 , S \u2217)\nd\u2206(S i 1 \u222a S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + \u03b3i)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 \u03b1+ 8\n4\u03b1+ 8 ;\nCase 8: If \u03c4i+1 > \u2212\u03b3i and \u2212 1 2 \u2264 \u03b3i \u2264 0, with respect to Definition 1, we have\nd\u2206(S i+1 1 \u2229 S i+1 2 , S \u2217)\nd\u2206(Si1 \u2229 S i 2, S\n\u2217) \u2264\n1 8Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n(12 + |\u03b3i|)Pr(S i 1 \u2295 S i 2) + 1 \u03b1Pr(S i 1 \u2295 S i 2)\n\u2264 \u03b1+ 8\n4\u03b1+ 8 .\nThus, after the (i+1)-th round, either d\u2206(S\ni+1 1 \u2229S i+1 2 ,S \u2217)\nd\u2206(Si1\u2229S i 2,S\n\u2217) \u2264 5\u03b1+86\u03b1+8 or\nd\u2206(S i+1 1 \u222aS i+1 2 ,S \u2217)\nd\u2206(Si1\u222aS i 2,S\n\u2217) \u2264 5\u03b1+86\u03b1+8 holds.\nHence, we have d\u2206(S s 1 \u2229S s 2, S \u2217) \u2264 18 ( 5\u03b1+8 6\u03b1+8 )s/2 or d\u2206(S s 1 \u222aS s 2, S \u2217) \u2264 18 ( 5\u03b1+8 6\u03b1+8 )s/2 with probability\nat least 1 \u2212 \u03b4. When s = \u2308 2 log 1 8\u01eb\nlog 1 C2\n\u2309, where C2 = 5\u03b1+8 6\u03b1+8 is a constant less than 1, we have either\nd\u2206(S s 1 \u2229 S s 2, S \u2217) \u2264 \u01eb or d\u2206(S s 1 \u222a S s 2, S \u2217) \u2264 \u01eb with probability at least 1 \u2212 \u03b4. Thus, considering R(hi+)\u2212R(S \u2217) = R(Si1\u2229S i 2)\u2212R(S \u2217) \u2264 d\u2206(S i 1\u2229S i 2, S \u2217) and R(hi\u2212)\u2212R(S \u2217) = R(Si1\u222aS i 2)\u2212R(S \u2217) \u2264 d\u2206(S i 1 \u222a S i 2, S \u2217), we have either R(hs+) \u2264 R(S \u2217) + \u01eb or R(hs\u2212) \u2264 R(S \u2217) + \u01eb.\nFrom Theorem 1 we know that we only need to request \u2211s\ni=0mi = O\u0303(log 1 \u01eb ) labels to learn h s +\nand hs\u2212, at least one of which is with error rate no larger than R(S \u2217) + \u01eb with probability at least 1\u2212 \u03b4. If we choose hs+ and it happens to satisfy R(h s +) \u2264 R(S \u2217) + \u01eb, we can get a classifier whose error rate is no larger than R(S\u2217) + \u01eb. Fortunately, there are only two classifiers and the probability of getting the right classifier is no less than 12 . To study how to choose between h s + and hs\u2212, we give Definition 2 at first.\nDefinition 2 The multi-view classifiers S1 and S2 satisfy \u03b2-condition if (8) holds for some \u03b2 > 0.\n\u2223\u2223\u2223 Pr\n( {x : x \u2208 S1 \u2295 S2 \u2227 y(x) = 1} )\nPr(S1 \u2295 S2) \u2212\nPr ( {x : x \u2208 S1 \u2295 S2 \u2227 y(x) = 0} )\nPr(S1 \u2295 S2)\n\u2223\u2223\u2223 \u2265 \u03b2 (8)\n(8) implies the difference between the examples belonging to positive class and that belonging to negative class in the contention region of S1 \u2295 S2. Based on Definition 2, we give Lemma 2 which provides information for deciding how to choose between h+ and h\u2212. This helps to get Theorem 2.\nLemma 2 If the multi-view classifiers Ss1 and S s 2 satisfy \u03b2-condition, with the number of\n2 log( 4 \u03b4 )\n\u03b22\nlabels we can decide correctly whether Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) or Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) ) is smaller with probability at least 1\u2212 \u03b4.\nProof: We apply Ss1 and S s 2 to the unlabeled instances set and identify the contention point set. Then we query for labels of 2 log( 4 \u03b4 )\n\u03b22 instances drawn randomly from the contention points set.\nWith these labels we estimate the empirical value P\u03021 of Pr({x:x\u2208Ss1\u2295S s 2\u2227y(x)=1})\nPr(Ss1\u2295S s 2)\nand the empirical\nvalue P\u03022 of Pr({x:x\u2208Ss1\u2295S s 2\u2227y(x)=0})\nPr(Ss1\u2295S s 2)\n. By Chernoff bound, with number of 2 log( 4 \u03b4 )\n\u03b22 labels we have the\nfollowing two equations with probability at least 1\u2212 \u03b4.\nP\u03021 \u2208 [Pr\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} )\nPr(Ss1 \u2295 S s 2)\n\u2212 \u03b2 2 , P r\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} )\nPr(Ss1 \u2295 S s 2)\n+ \u03b2\n2\n]\nP\u03022 \u2208 [Pr\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} )\nPr(Ss1 \u2295 S s 2)\n\u2212 \u03b2 2 , P r\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} )\nPr(Ss1 \u2295 S s 2)\n+ \u03b2\n2\n]\nIf P\u03021 \u2264 P\u03022, we get Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) \u2264 Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) with probability at least 1 \u2212 \u03b4; otherwise, we get Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) > Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) with probability at least 1\u2212 \u03b4.\nTheorem 2 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to Definition 1, when the non-degradation condition holds, if the multi-view classifiers satisfy \u03b2-condition, by requesting O\u0303(log 1\u01eb ) labels the multi-view active learning in Table 1 will generate a classifier whose error rate is no larger than R(S\u2217) + \u01eb with probability at least 1\u2212 \u03b4.\nProof: According to Theorem 1, by requesting O\u0303(log 1\u01eb ) labels the multi-view active learning in Table 1 can get either R(hs+) \u2264 R(S \u2217) + \u01eb or R(hs\u2212) \u2264 R(S \u2217) + \u01eb with probability at least 1 \u2212 \u03b42 . According to Lemma 2, by requesting 2 log( 8 \u03b4 ) \u03b22 labels we can decide correctly whether Pr ( {x : x \u2208 Ss1 \u2295S s 2 \u2227 y(x) = 1} ) or Pr ( {x : x \u2208 Ss1 \u2295S s 2 \u2227 y(x) = 0} ) is smaller with probability at least 1\u2212 \u03b42 . Case 1: If Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) \u2264 Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) , we have R(hs\u2212) \u2264 R(h s +). Thus, we get R(h s \u2212) \u2264 R(S \u2217) + \u01eb with probability at least 1\u2212 \u03b4. Case 2: If Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) > Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) , we have R(hs+) < R(h s \u2212). Thus, we get R(h s +) \u2264 R(S \u2217) + \u01eb with probability at least 1\u2212 \u03b4.\nThe total number of labels to be requested is O\u0303(log 1\u01eb ) + 2 log( 8 \u03b4 ) \u03b22 = O\u0303(log 1\u01eb ).\nFrom Theorem 2 we know that we only need to request O\u0303(log 1\u01eb ) labels to learn a classifier with error rate no larger than R(S\u2217)+\u01eb with probability at least 1\u2212\u03b4. Thus, we achieve an exponential improvement in sample complexity of active learning in the non-realizable case under multi-view setting. Sometimes, the difference between the examples belonging to positive class and that belonging to negative class in Ss1 \u2295 S s 2 may be very small, i.e., (9) holds.\n\u2223\u2223\u2223 Pr\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} )\nPr(Ss1 \u2295 S s 2)\n\u2212 Pr\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} )\nPr(Ss1 \u2295 S s 2)\n\u2223\u2223\u2223 = O(\u01eb) (9)\nIf so, we need not to estimate whether R(hs+) or R(h s \u2212) is smaller and Theorem 3 indicates that both hs+ and h s \u2212 are good approximations of the optimal classifier.\nTheorem 3 For data distribution D \u03b1-expanding with respect to hypothesis class H1\u00d7H2 according to Definition 1, when the non-degradation condition holds, if (9) is satisfied, by requesting O\u0303(log 1\u01eb ) labels the multi-view active learning in Table 1 will generate two classifiers h s + and hs\u2212 which satisfy either (a) or (b) with probability at least 1 \u2212 \u03b4. (a) R(h s +) \u2264 R(S \u2217) + \u01eb and R(hs\u2212) \u2264 R(S \u2217) +O(\u01eb); (b) R(hs+) \u2264 R(S \u2217) +O(\u01eb) and R(hs\u2212) \u2264 R(S \u2217) + \u01eb.\nProof: Since Pr(Ss1 \u2295 S s 2) \u2264 1, with the following equation\n\u2223\u2223\u2223 Pr\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} )\nPr(Ss1 \u2295 S s 2)\n\u2212 Pr\n( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} )\nPr(Ss1 \u2295 S s 2)\n\u2223\u2223\u2223 = O(\u01eb)\nwe have |Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) \u2212 Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) | = O(\u01eb). So it is easy to get |R(hs+)\u2212R(h s \u2212)| = O(\u01eb). According to Theorem 1, by requesting O\u0303(log 1 \u01eb ) labels we can get either R(hs+) \u2264 R(S \u2217) + \u01eb or R(hs\u2212) \u2264 R(S \u2217) + \u01eb with probability at least 1 \u2212 \u03b4. Thus, we get that hs+ and h s \u2212 satisfy either (a) or (b) with probability at least 1\u2212 \u03b4.\n5.2. The Situation Where S\u22171 6= S \u2217 2\nAlthough the two views represent the same learning task and generally are consistent with each other, sometimes S\u22171 may be not equal to S \u2217 2 . Therefore, the \u03b1-expansion assumption in Definition 1 should be adjusted to the situation where S\u22171 6= S \u2217 2 . To analyze this theoretically, we replace S\u2217 by S\u22171 \u2229 S \u2217 2 in Definition 1 and get (10). Similarly to Theorem 1, we get Theorem 4.\nPr ( S1 \u2295 S2 ) \u2265 \u03b1 ( Pr ( S1 \u2229 S2 \u2212 S \u2217 1 \u2229 S \u2217 2 ) + Pr ( S1 \u2229 S2 \u2212 S \u2217 1 \u2229 S \u2217 2 )) (10)\nTheorem 4 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to (10), when the non-degradation condition holds, if s = \u2308 2 log 1 8\u01eb\nlog 1 C2\n\u2309 and mi = 256kC C21\n( V +\nlog(16(s+1)\u03b4 ) ) , the multi-view active learning in Table 1 will generate two classifiers hs+ and h s \u2212, at least one of which is with error rate no larger than R(S\u22171 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b4. (V , k, C1 and C2 are given in Theorem 1.)\nProof: Since S\u2217v is the optimal Bayes classifier in the v-th view, obviously, R(S \u2217 1 \u2229S \u2217 2) is no less than R(S\u2217v ), (v = 1, 2). So, learning a classifier with error rate no larger than R(S \u2217 1 \u2229 S \u2217 2) + \u01eb is not harder than learning a classifier with error rate no larger than R(S\u2217v) + \u01eb. Now we aim at\nlearning a classifier with error rate no larger than R(S\u22171 \u2229 S \u2217 2) + \u01eb. Without loss of generality, we assume R(Siv) > R(S \u2217 1 \u2229 S \u2217 2) for i = 0, 1, . . . , s. If R(S i v) \u2264 R(S \u2217 1 \u2229 S \u2217 2), we get a classifier with error rate no larger than R(S\u22171 \u2229 S \u2217 2) + \u01eb. Thus, we can neglect the probability mass on the hypothesis whose error rate is less than R(S\u22171 \u2229S \u2217 2) and regard S \u2217 1 \u2229S \u2217 2 as the optimal. Replacing S\u2217 by S\u22171 \u2229 S \u2217 2 in the discussion of Section 5.1, with the proof of Theorem 1 we get Theorem 4 proved.\nTheorem 4 shows that for the situation where S\u22171 6= S \u2217 2 , by requesting O\u0303(log 1 \u01eb ) labels we can learn two classifiers hs+ and h s \u2212, at least one of which is with error rate no larger than R(S \u2217 1 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b4. With Lemma 2, we get Theorem 5 from Theorem 4.\nTheorem 5 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy \u03b2-condition, by requesting O\u0303(log 1\u01eb ) labels the multi-view active learning in Table 1 will generate a classifier whose error rate is no larger than R(S\u22171 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b4.\nProof: According to Theorem 4, by requesting O\u0303(log 1\u01eb ) labels the multi-view active learning in Table 1 can get either R(hs+) \u2264 R(S \u2217 1 \u2229 S \u2217 2) + \u01eb or R(h s \u2212) \u2264 R(S \u2217 1 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b42 . According to Lemma 2, by requesting 2 log( 8 \u03b4 ) \u03b22 labels we can decide correctly whether Pr ( {x : x \u2208 Ss1 \u2295S s 2 \u2227 y(x) = 1} ) or Pr ( {x : x \u2208 Ss1 \u2295S s 2 \u2227 y(x) = 0} ) is smaller with probability at least 1\u2212 \u03b42 . Case 1: If Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) \u2264 Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) , we have R(hs\u2212) \u2264 R(h s +). Thus, we get R(h s \u2212) \u2264 R(S \u2217 1 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b4. Case 2: If Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 1} ) > Pr ( {x : x \u2208 Ss1 \u2295 S s 2 \u2227 y(x) = 0} ) , we have R(hs+) < R(h s \u2212). Thus, we get R(h s +) \u2264 R(S \u2217 1 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b4. The total number of labels to be requested is O\u0303(log 1\u01eb ) + 2 log( 8 \u03b4 ) \u03b22 = O\u0303(log 1\u01eb ).\nGenerally, R(S\u22171\u2229S \u2217 2) is larger than R(S \u2217 1) and R(S \u2217 2). When S \u2217 1 is not too much different from S \u2217 2 , i.e., Pr(S\u22171 \u2295 S \u2217 2) \u2264 \u01eb/2, we have Corollary 1 which indicates that the exponential improvement in the sample complexity of active learning with Tsybakov noise is still possible.\nCorollary 1 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7H2 according to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy \u03b2-condition and Pr(S\u22171 \u2295 S \u2217 2) \u2264 \u01eb/2, by requesting O\u0303(log 1 \u01eb ) labels the multi-view active learning in Table 1 will generate a classifier with error rate no larger than R(S\u2217v) + \u01eb (v = 1, 2) with probability at least 1\u2212 \u03b4.\nProof: According to Theorem 5 we know that by requesting O\u0303(log 1\u01eb ) labels the multi-view active learning in Table 1 will generate a classifier whose error rate is no larger than R(S\u22171 \u2229 S \u2217 2) + \u01eb 2 with probability at least 1\u2212 \u03b4. Considering that\nR(S\u22171 \u2229 S \u2217 2)\u2212R(S \u2217 v) =\n\u222b\n(S\u22171\u2229S \u2217 2 )\u2206S \u2217 v\n|2\u03d5v(xv)\u2212 1|pxvdxv \u2264 Pr(S \u2217 1 \u2295 S \u2217 2),\nwe have R(S\u22171 \u2229 S \u2217 2) \u2264 R(S \u2217 v) + \u01eb 2 . Thus, we get that R(S \u2217 1 \u2229 S \u2217 2) + \u01eb 2 is no larger than R(S \u2217 v ) + \u01eb."}, {"heading": "6. Multi-view Active Learning without Non-degradation Condition", "text": "Section 5 considers situations when the non-degradation condition holds, there are cases, however, the non-degradation condition (7) does not hold. In this section we focus on the multi-view active learning in Table 2 and give an analysis with the non-degradation condition waived. Firstly, we give Theorem 6 for the sample complexity of multi-view active learning in Table 2 when S\u22171 = S \u2217 2 = S \u2217.\nTheorem 6 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to Definition 1, if s = \u2308 2 log 1 8\u01eb\nlog 1 C2\n\u2309 and mi = 256kC C21 ( V + log(16(s+1)\u03b4 ) ) , the multi-view active\nlearning in Table 2 will generate two classifiers hs+ and h s \u2212, at least one of which is with error rate no larger than R(S\u2217) + \u01eb with probability at least 1 \u2212 \u03b4. (V , k, C1 and C2 are given in Theorem 1.)\nProof: After the i-th round in Table 2, the number of training examples in L is \u2211i\nb=0 2 bmi =\n(2i+1 \u2212 1)mi. While in the (i + 1)-th round, we randomly query (2 i+1 \u2212 1)mi labels from the region of Qi and add them into L. So in the (i + 1)-th round, the number of training examples\nfor Si+1v (v = 1, 2) drawn randomly from region of Qi is larger than the number of whole training examples for Siv. Since the optimal Bayes classifier cv belongs to Hv, according to the standard PAC-model, it is easy to know that d(Si+1v |Qi, S \u2217|Qi) \u2264 d(S i v|Qi, S \u2217|Qi) can be met for any \u03d5v, where d(Sv|Qi, S \u2217|Qi) is defined as\nd(Sv|Qi, S \u2217|Qi) , R(Sv|Qi)\u2212R(S \u2217|Qi) =\n\u222b\n(Sv\u2229Qi)\u2206(S\u2217\u2229Qi) |2\u03d5v(xv)\u2212 1|pxvdxv\n/ Pr(Qi).\nSo, by setting \u03d5v \u2208 {0, 1}, we get d\u2206(S i+1 v |Qi, S \u2217|Qi) \u2264 d\u2206(S i v|Qi, S \u2217|Qi), which implies the non-degradation condition. Thus, with the proof of Theorem 1, we get Theorem6 proved.\nTheorem 6 shows that we can request \u2211s\ni=0 2 imi = O\u0303( 1 \u01eb ) labels to learn two classifiers h s + and\nhs\u2212, at least one of which is with error rate no larger than R(S \u2217) + \u01eb with probability at least 1 \u2212 \u03b4. To guarantee the non-degradation condition (7), we only need to query (2i \u2212 1)mi more labels in the i-th round. With Lemma 2, we get Theorem 7.\nTheorem 7 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to Definition 1, if the multi-view classifiers satisfy \u03b2-condition, by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 will generate a classifier whose error rate is no larger than R(S\u2217) + \u01eb with probability at least 1\u2212 \u03b4.\nProof: According to Theorem 6, by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 will generate two classifiers hs+ and h s \u2212, at least one of which is with error rate no larger\nthan R(S\u2217) + \u01eb with probability at least 1 \u2212 \u03b4. Similarly to the proof of Theorem 2, we get Theorem 7 proved.\nTheorem 7 shows that, without the non-degradation condition, we need to request O\u0303(1\u01eb ) labels to learn a classifier with error rate no larger than R(S\u2217) + \u01eb with probability at least 1\u2212 \u03b4. The order of 1/\u01eb is independent of the parameter in Tsybakov noise. Similarly to Theorem 3, we get Theorem 8 which indicates that both hs+ and h s \u2212 are good approximations of the optimal classifier.\nTheorem 8 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to Definition 1, if (9) holds, by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 will generate two classifiers hs+ and h s \u2212 which satisfy either (a) or (b) with probability at least 1\u2212 \u03b4. (a) R(hs+) \u2264 R(S \u2217) + \u01eb and R(hs\u2212) \u2264 R(S \u2217) +O(\u01eb); (b) R(hs+) \u2264 R(S \u2217) +O(\u01eb) and R(hs\u2212) \u2264 R(S \u2217) + \u01eb.\nProof: According to Theorem 6, by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 will generate two classifiers hs+ and h s \u2212, at least one of which is with error rate no larger than R(S\u2217) + \u01eb with probability at least 1 \u2212 \u03b4. Similarly to the proof of Theorem 3, we get Theorem 8 proved.\nAs for the situation where S\u22171 6= S \u2217 2 , similarly to Theorem 5 and Corollary 1, we have Theorem 9 and Corollary 2.\nTheorem 9 For data distribution D \u03b1-expanding with respect to hypothesis class H1 \u00d7 H2 according to (10), if the multi-view classifiers satisfy \u03b2-condition, by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 will generate a classifier whose error rate is no larger than R(S\u22171 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b4.\nProof: Similarly to the proof of Theorem 4 and Theorem 6, we know that by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 can get either R(h s +) \u2264 R(S \u2217 1 \u2229 S \u2217 2) + \u01eb or R(hs\u2212) \u2264 R(S \u2217 1 \u2229 S \u2217 2) + \u01eb with probability at least 1 \u2212 \u03b4 2 . According to Lemma 2, by requesting 2 log( 8 \u03b4 )\n\u03b22 labels we can decide correctly whether R(hs+) or R(h s \u2212) is smaller with probability at\nleast 1\u2212 \u03b42 . Thus, we can get a classifiers whose error rate is no larger than R(S \u2217 1 \u2229 S \u2217 2) + \u01eb with probability at least 1\u2212 \u03b4. The total number of labels to be requested is O\u0303(1\u01eb ) + 2 log( 8 \u03b4 ) \u03b22 = O\u0303(1\u01eb ).\nCorollary 2 For data distribution D \u03b1-expanding with respect to hypothesis class H1\u00d7H2 according to (10), if the multi-view classifiers satisfy \u03b2-condition and Pr(S\u22171 \u2295S \u2217 2) \u2264 \u01eb/2, by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 will generate a classifier with error rate no larger than R(S\u2217v) + \u01eb (v = 1, 2) with probability at least 1\u2212 \u03b4.\nProof: According to Theorem 9 we know that by requesting O\u0303(1\u01eb ) labels the multi-view active learning in Table 2 will generate a classifier whose error rate is no larger than R(S\u22171 \u2229 S \u2217 2) + \u01eb 2 with probability at least 1\u2212 \u03b4. With the proof of Corollary 1, we get that R(S\u22171 \u2229 S \u2217 2) + \u01eb 2 is no larger than R(S\u2217v ) + \u01eb."}, {"heading": "7. Empirical Verification", "text": "In this section we empirically verify that whether multi-view setting can improve the sample complexity of active learning in the non-realizable case remarkably.\nIn the experiment we use the semi-artificial data set [20] and the course data set [6]. The semiartificial data set has two artificial views which are created by randomly pairing two examples from the same class and contains 800 examples. In order to control the correlation between the two views, the number of clusters per class can be set as a parameter. We use 1 cluster, 2 clusters and 4 clusters in the experiments, respectively. The course data set has two natural views: pages view (i.e., the text appearing on the page) and links view (i.e., the anchor text attached to hyper-links pointing to the page) and contains 1,051 examples. We randomly use 25% data as the test set and use the remaining 75% data to generate the unlabeled data set U . We use Random Sampling as the baseline. In each round, we fix the number of examples to be queried in Multi-View Active Learning and that in Random Sampling. Thus, we can study their performances under the same number of queried examples. In the experiments, we query two\nexamples in each round of the two methods and implement the classifiers with NaiveBayes in WEKA. The experiments are repeated for 20 runs and Figure 1 plots the average error rates of the two methods against the number of examples that have been queried. From Figure 1 it can be found that the performance of Multi-View Active Learning is far better than the performance of Random Sampling with the same number of queried examples. In other words, multi-view setting can help improve the sample complexity of active learning in the non-realizable case remarkably."}, {"heading": "8. Conclusion", "text": "We present the first study on active learning in the non-realizable case under multi-view setting in this paper. We prove that the sample complexity of multi-view active learning with unbounded Tsybakov noise can be improved to O\u0303(log 1\u01eb ), contrasting to single-view setting where only polynomial improvement is proved possible with the same noise condition. In general multi-view\nsetting, we prove that the sample complexity of active learning with unbounded Tsybakov noise is O\u0303(1\u01eb ), where the order of 1/\u01eb is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/\u01eb is related to the parameter in Tsybakov noise. Generally, the non-realizability of learning task can be caused by many kinds of noise, e.g., misclassification noise and malicious noise. It would be interesting to extend our work to more general noise model."}], "references": [{"title": "editors", "author": ["M. Anthony", "P.L. Bartlett"], "venue": "Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "ICML, pages 65\u201372", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Co-training and expansion: Towards bridging theory and practice", "author": ["M.-F. Balcan", "A. Blum", "K. Yang"], "venue": "In NIPS", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A.Z. Broder", "T. Zhang"], "venue": "COLT, pages 35\u201350", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "The true sample complexity of active learning", "author": ["M.-F. Balcan", "S. Hanneke", "J. Wortman"], "venue": "COLT, pages 45\u201356", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "COLT, pages 92\u2013100", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Upper and lower error bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": "Allerton Conference, pages 225\u2013234", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Minimax bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": "IEEE Transactions on Information Theory, 54(5):2339\u20132353", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Linear classification and selective sampling under low noise conditions", "author": ["G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile"], "venue": "In NIPS", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Improving generalization with active learning", "author": ["D.A. Cohn", "L.E. Atlas", "R.E. Ladner"], "venue": "Machine Learning, 15(2):201\u2013221", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In NIPS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A.T. Kalai", "C. Monteleoni"], "venue": "COLT, pages 249\u2013263", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "editors", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "A Probabilistic Theory of Pattern Recognition. Springer, New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, 28(2-3):133\u2013168", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "ICML, pages 353\u2013360", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "COLT", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "ACL, pages 63\u201377", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Active + semi-supervised learning = robust multi-view learning", "author": ["I. Muslea", "S. Minton", "C.A. Knoblock"], "venue": "ICML, pages 435\u2013442", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A. Tsybakov"], "venue": "The Annals of Statistics, 32(1):135\u2013166", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Sufficient conditions for agnostic active learnable", "author": ["L. Wang"], "venue": "In NIPS 22,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "On multi-view active learning and the combination with semisupervised learning", "author": ["W. Wang", "Z.-H. Zhou"], "venue": "ICML, pages 1152\u20131159", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.", "startOffset": 19, "endOffset": 31}, {"referenceID": 12, "context": "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.", "startOffset": 19, "endOffset": 31}, {"referenceID": 15, "context": "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.", "startOffset": 19, "endOffset": 31}, {"referenceID": 3, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 10, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 11, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 13, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 15, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].", "startOffset": 122, "endOffset": 133}, {"referenceID": 12, "context": ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].", "startOffset": 122, "endOffset": 133}, {"referenceID": 16, "context": ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].", "startOffset": 122, "endOffset": 133}, {"referenceID": 18, "context": "It is worth noting that these bounds obtained in the non-realizable case match the lower bound \u03a9( 2 \u01eb2 ) [19], in the same order as the upper bound O( 1 \u01eb2 ) of passive learning (\u03b7 denotes the generalization error rate of the optimal classifier in the hypothesis class and \u01eb bounds how close to the optimal classifier in the hypothesis class the active learner has to get).", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "Tsybakov noise model [21] is more and more popular in theoretical analysis on the sample complexity of active learning.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "However, existing result [8] shows that obtaining exponential improvement in the sample complexity of active learning with unbounded Tsybakov noise is hard.", "startOffset": 25, "endOffset": 28}, {"referenceID": 22, "context": "Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case.", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "-We define \u03b1-expansion, which extends the definition in [3] and [23] to the non-realizable case, and \u03b2-condition for multi-view setting.", "startOffset": 56, "endOffset": 59}, {"referenceID": 22, "context": "-We define \u03b1-expansion, which extends the definition in [3] and [23] to the non-realizable case, and \u03b2-condition for multi-view setting.", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "[2] proposed the agnostic active learning algorithm A2 and proved that its sample complexity is \u00d4( 2 \u01eb2 ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "2 Hoping to get tighter bound on the sample complexity of the algorithm A2, Hanneke [17] defined the disagreement coefficient \u03b8, which depends on the hypothesis class and the data distribution, and proved that the sample complexity of the algorithm A2 is \u00d4(\u03b82 \u03b7 2 \u01eb2 ).", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "[13] developed a general agnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is \u00d4(\u03b8 \u03b7 2 \u01eb2 ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[13] developed a general agnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is \u00d4(\u03b8 \u03b7 2 \u01eb2 ).", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "Recently, the popular Tsybakov noise model [21] was considered in theoretical analysis on active learning and there have been some bounds on the sample complexity.", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].", "startOffset": 145, "endOffset": 155}, {"referenceID": 6, "context": "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].", "startOffset": 145, "endOffset": 155}, {"referenceID": 17, "context": "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].", "startOffset": 145, "endOffset": 155}, {"referenceID": 3, "context": "[4] assumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the sample complexity of active learning with unbounded Tsybakov noise is O ( \u01eb 2 1+\u03bb ) (\u03bb > 0 depends on Tsybakov noise).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Castro and Nowak [8] showed that the sample complexity of active learning with unbounded Tsybakov noise is \u00d4 ( \u01eb \u2212 2\u03bc\u03c9+d\u22122\u03c9\u22121 \u03bc\u03c9 ) (\u03bc > 1 depends on another form of Tsybakov noise, \u03c9 \u2265 1 depends on the H\u00f6lder smoothness and d is the dimension of the data).", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "[9] assumed that the labels of examples are generated according to a simple linear noise model and indicated that the sample complexity of active learning with unbounded Tsybakov noise is O ( \u01eb \u2212 2(3+\u03bb) (1+\u03bb)(2+\u03bb) ) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity \u00d4 ( \u01eb 2 1+\u03bb ) for active learning with unbounded Tsybakov noise.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity \u00d4 ( \u01eb 2 1+\u03bb ) for active learning with unbounded Tsybakov noise.", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity \u00d4 ( \u01eb 2 1+\u03bb ) for active learning with unbounded Tsybakov noise.", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "For active learning with unbounded Tsybakov noise, Castro and Nowak [8] also proved that at least \u03a9(\u01eb\u2212\u03c1) labels are requested to learn an \u01eb-approximation of the optimal classifier (\u03c1 \u2208 (0, 2) depends on Tsybakov noise).", "startOffset": 68, "endOffset": 71}, {"referenceID": 21, "context": "Wang [22] introduced smooth assumption to active learning with approximate Tsybakov noise and proved that if the classification boundary and the underlying distribution are smooth to \u03be-th order and \u03be > d, the sample complexity of active learning is \u00d4 ( \u01eb 2d \u03be+d ) ; if the boundary and the distribution are infinitely smooth, the sample complexity of active learning is O ( polylog(1\u01eb ) ) .", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "It is well-known [15] that S \u2217 v = {xv : \u03c6v(xv) \u2265 1 2}, where \u03c6v(xv) = P (y = 1|xv).", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv\u2208Xv (|\u03c6v(xv) \u2212 1/2| \u2264 t) \u2264 C0t \u03bb for some finite C0 > 0, \u03bb > 0 and all 0 < t \u2264 1/2, where \u03bb = \u221e corresponds to the best learning situation and the noise is called bounded [8]; while \u03bb = 0 corresponds to the worst situation.", "startOffset": 158, "endOffset": 162}, {"referenceID": 7, "context": "In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv\u2208Xv (|\u03c6v(xv) \u2212 1/2| \u2264 t) \u2264 C0t \u03bb for some finite C0 > 0, \u03bb > 0 and all 0 < t \u2264 1/2, where \u03bb = \u221e corresponds to the best learning situation and the noise is called bounded [8]; while \u03bb = 0 corresponds to the worst situation.", "startOffset": 343, "endOffset": 346}, {"referenceID": 7, "context": "When \u03bb < \u221e, the noise is called unbounded [8].", "startOffset": 42, "endOffset": 45}, {"referenceID": 20, "context": "According to Proposition 1 in [21], it is easy to know that (2) holds.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "We will use the following lamma [1] which gives the standard sample complexity for non-realizable learning task.", "startOffset": 32, "endOffset": 35}, {"referenceID": 19, "context": "Multi-view active learning first described in [20] focuses on the contention points (i.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "[3] also gave a definition of expansion, Pr(T1\u2295T2) \u2265 \u03b1min [ Pr(T1\u2229T2), P r(T1\u2229T2) ] , for realizable learning task under the assumptions that the learner in each view is never \u201cconfident but wrong\u201d and the learning algorithm is able to learn from positive data only.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In addition, in [3] the instances which are agreed by the two views but are predicted different label by the optimal classifier can be denoted as T1 \u2229 T2.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "So, it can be found that Definition 1 and the definition of expansion in [3] are based on the same intuition that the amount of contention points is no less than a fraction of the amount of instances which are agreed by the two views but are predicted different label by the optimal classifiers.", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "As discussed in [23], we also assume that the learner in Table 1 satisfies the non-degradation condition as the amount of labeled training examples increases, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "In the experiment we use the semi-artificial data set [20] and the course data set [6].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "In the experiment we use the semi-artificial data set [20] and the course data set [6].", "startOffset": 83, "endOffset": 86}], "year": 2010, "abstractText": "The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multiview setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multiview active learning can be \u00d5(log 1\u01eb ), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is \u00d5(1\u01eb ), where the order of 1/\u01eb is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/\u01eb is related to the parameter in Tsybakov noise.", "creator": "LaTeX with hyperref package"}}}