{"id": "1006.2899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2010", "title": "Approximated Structured Prediction for Learning Large Scale Graphical Models", "abstract": "in this paper here we must propose an approximated structured prediction framework for understanding large scale graphical models specifically and already derive message - passing network algorithms for learning 3d their parameters very efficiently. we first first relate crfs and constrained structured svms computation and show that culminating in crfs a variant of solving the empirical log - partition function, is known as linear soft - noise max, whereby smoothly approx approximates the hinge loss function of random structured fuzzy svms. above we here then propose an intuitive improved approximation for the structured channel prediction solution problem, using duality, effectively based immediately on successive local polynomial entropy efficiency approximations performed and derive an efficient wireless message - passing algorithm noting that accuracy is guaranteed theoretically to converge to the optimum for strongly concave network entropy quality approximations. unlike future existing complexity approaches, this application allows us to learn efficiently graphical temporal models with linear cycles and solve very narrow large number of independent parameters. we demonstrate the effectiveness of our approach in an image denoising that task. now this task was previously already solved by sharing parameters processes across cliques. in contrast, our algorithm is able to more efficiently learn large particular number of parameters tasks resulting in orders 2 of magnitude better graphical prediction.", "histories": [["v1", "Tue, 15 Jun 2010 06:55:03 GMT  (968kb,D)", "https://arxiv.org/abs/1006.2899v1", null], ["v2", "Mon, 9 Jul 2012 18:22:27 GMT  (14kb)", "http://arxiv.org/abs/1006.2899v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["tamir hazan", "raquel urtasun"], "accepted": false, "id": "1006.2899"}, "pdf": {"name": "1006.2899.pdf", "metadata": {"source": "CRF", "title": "Approximated Structured Prediction for Learning Large Scale Graphical Models", "authors": ["Tamir Hazan", "Raquel Urtasun"], "emails": ["hazan@ttic.edu", "rurtasun@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 6.\n28 99\nv2 [\ncs .L\nG ]\n9 J\nul 2\n01 2"}, {"heading": "Approximated Structured Prediction for Learning Large Scale Graphical Models", "text": "Tamir Hazan TTI Chicago\nhazan@ttic.edu\nRaquel Urtasun TTI Chicago\nrurtasun@ttic.edu\nThis manuscript contains the proofs for \u201dA Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction\u201d\nClaim 1 The dual program of the structured prediction program in (3) takes the form\nmax px,y(y\u0302)\u2208\u2206Y\n\u2211\n(x,y)\u2208S\n(\n\u01ebH(px,y) + p \u22a4 x,yey\n) \u2212 C1\u2212q\nq\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\n(x,y)\u2208S\n\u2211\ny\u0302\u2208Y\npx,y(y\u0302)\u03a6(x, y\u0302)\u2212 d\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 q\nq\n,\nwhere \u2206Y is the probability simplex over Y and H(px,y) = \u2212 \u2211 y\u0302 px,y(y\u0302) ln px,y(y\u0302) is the entropy.\nProof: We first describe an equivalent program to the one in (3) by adding variables \u00b5(x, y\u0302) instead of \u03b8\u22a4\u03a6(x, y\u0302) to decouple the soft-max from the regularization.\nmin \u03b8, \u00b5(x, y\u0302)\n\u00b5(x, y\u0302) = \u03b8\u22a4\u03a6(x, y\u0302)\n\n\n\n\u2211\n(x,y)\u2208S\n\u01eb ln \u2211\ny\u0302\nexp ey(y\u0302) + \u00b5(x, y\u0302)\n\u01eb \u2212 d\u22a4\u03b8 +\nC p \u2016\u03b8\u2016pp\n\n\n\n,\nTo maintain consistency, we add the constraints \u00b5(x, y\u0302) = \u03b8\u22a4\u03a6(x, y\u0302), for every (x, y) \u2208 S and every y\u0302 \u2208 Y . We compute the Lagrangian by adding the Lagrange multipliers px,y(y\u0302)\nL() = \u2211\n(x,y)\u2208S\n\u01eb ln \u2211\ny\u0302\u2208Y\nexp ey(y\u0302) + \u00b5(x, y\u0302)\n\u01eb \u2212d\u22a4\u03b8+\nC p \u2016\u03b8\u2016pp\u2212\n\u2211\n(x,y)\u2208S,y\u0302\u2208Y\npx,y(y\u0302) ( \u00b5(x, y\u0302)\u2212 \u03b8\u22a4\u03a6(x, y\u0302) ) .\nThe dual function is a function of the Lagrange multipliers, and it is computed by minimizing the Lagrangian, namely q(px,y) = min\u00b5,\u03b8 L(\u00b5, \u03b8,px,y). In particular the dual function can be written as \u2211\n(x,y)\nmin \u00b5(x,y\u0302)\n\n\n\n\u01eb ln \u2211\ny\u0302\nexp ey(y\u0302) + \u00b5(x, y\u0302)\n\u01eb \u2212\n\u2211\ny\u0302\n\u00b5(x, y\u0302)px,y(y\u0302)\n\n\n\n+min \u03b8\n\n\n\nC p \u2016\u03b8\u2016pp \u2212 \u03b8 \u22a4( \u2211\n(x,y),y\u0302\npx,y(y\u0302)\u03a6(x, y\u0302)\u2212 d)\n\n\n\nand composed from the conjugate dual of the soft-max and the conjugate dual of the \u2113p norm. Recall that the conjugate dual for the soft-max is the entropy barrier \u01ebH(px,y) over the set of probability distributions \u2206Y (cf. [4] Theorem 8.1), and that the linear shift of the soft-max argument by ey(y\u0302) result in the linear shift of the conjugate dual, thus we get the first part of the dual function \u2211\n(x,y)(\u01ebH(px,y) + e \u22a4 y px,y). Similarly, the conjugate dual of 1 p \u2016\u03b8\u2016pp is 1 q \u2016z\u2016qq for the dual norm\n1/p+ 1/q = 1 (cf. [2]), where in our case z = \u2211\n(x,y),y\u0302 px,y(y\u0302)\u03a6(x, y\u0302)\u2212 d.\nTheorem 1 The approximation of the structured prediction program in (3) takes the form\nmin \u03bbx,y,v\u2192\u03b1,\u03b8\n\u2211\n(x,y)\u2208S,v\n\u01ebcv ln \u2211\ny\u0302v\nexp\n(\ney(y\u0302v) + \u2211\nr:v\u2208Vr,x \u03b8r\u03c6r,v(x, y\u0302v)\u2212\n\u2211\n\u03b1\u2208N(v) \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebcv\n)\n+ \u2211\n(x,y)\u2208S,\u03b1\n\u01ebc\u03b1 ln \u2211\ny\u0302\u03b1\nexp\n( \u2211\nr:\u03b1\u2208Er \u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) +\n\u2211\nv\u2208N(\u03b1) \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebc\u03b1\n)\n\u2212 d\u22a4\u03b8 \u2212 C\np \u2016\u03b8\u2016pp\nProof: We add auxiliary variables z and constrain them such that\nzr = \u2211\n(x,y)\u2208S,v\u2208Vr,x,y\u0302v\nbx,y,v(y\u0302v)\u03c6r,v(x, y\u0302v) + \u2211\n(x,y)\u2208S,\u03b1\u2208Er,x,y\u0302\u03b1\nbx,y,\u03b1(y\u0302\u03b1)\u03c6r,\u03b1(x, y\u0302\u03b1).\nWe derive the Lagrangian by introducing the Lagrange multipliers \u03bbx,y,v\u2192\u03b1(y\u0302v) for every marginalization constraint \u2211\ny\u0302\u03b1\\y\u0302v bx,y,\u03b1(y\u0302\u03b1) = bx,y,v(y\u0302v), and Lagrange multipliers \u03b8r for every equality\nconstraint involving zr. In particular, the Lagrangian has the form:\nL() = \u2211\n(x,y)\u2208S\n\n\n\u2211\n\u03b1\u2208E\n\u01ebc\u03b1H(bx,y,\u03b1) + \u2211\nv\u2208V\n\u01ebcvH(bx,y,v) + \u2211\nv\u2208V,y\u0302v\nbx,y,v(y\u0302v)ey,v(y\u0302v)\n\n\u2212 C1\u2212q\nq \u2016z\u2212 d\u2016qq\n+ \u2211\nr\n\u03b8r\n\n\n\u2211\n(x,y)\u2208S,v\u2208Vr,y\u0302v\nbx,y,v(y\u0302v)\u03c6r,v(x, y\u0302v) + \u2211\n(x,y)\u2208S,\u03b1\u2208Er,y\u0302\u03b1\nbx,y,\u03b1(y\u0302\u03b1)\u03c6r,\u03b1(x, y\u0302\u03b1)\u2212 zr\n\n\n+ \u2211\nv,\u03b1\u2208N(v),y\u0302v\n\u03bbx,y,v\u2192\u03b1(y\u0302v)\n\n\n\u2211\ny\u0302\u03b1\\y\u0302v\nbx,y,\u03b1(y\u0302\u03b1)\u2212 bx,y,v(y\u0302v)\n\n\nWe obtain the dual function by minimizing the beliefs over their compact domain, i.e.\nq(\u03bbx,y,v\u2192\u03b1, \u03b8) = max bx,y,v(y\u0302v)\u2208\u2206Yv , bx,y,\u03b1(y\u0302\u03b1)\u2208\u2206Y\u03b1 L(bx,y,v,bx,y,\u03b1,\u03bbx,y,v\u2192\u03b1, \u03b8),\nDeriving the dual by minimizing over the compact set of beliefs enables us to obtain an unconstrained dual, which corresponds to the approximated structured prediction program. The dual function is described by the conjugate dual function:\n\u2211\n(x,y)\u2208S,v\nmax bx,y,v\u2208\u2206Yv\n\n\n\n\u01ebcvH(bx,y,v) + \u2211\ny\u0302v\nbx,y,v(y\u0302v)\n\ney(y\u0302v) + \u2211\nr:v\u2208Vr\n\u03b8r\u03c6r,v(x, y\u0302v)\u2212 \u2211\n\u03b1\u2208N(v)\n\u03bbx,y,v\u2192\u03b1(y\u0302v)\n\n\n\n\n\n+ \u2211\n(x,y)\u2208S,\u03b1\nmax bx,y,\u03b1\u2208\u2206Y\u03b1\n\n\n\n\u01ebc\u03b1H(bx,y,\u03b1) + \u2211\ny\u0302\u03b1\nbx,y,\u03b1(y\u0302\u03b1)\n\n\n\u2211\nr:\u03b1\u2208Er\n\u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) + \u2211\nv\u2208N(\u03b1)\n\u03bbx,y,v\u2192\u03b1(y\u0302v)\n\n\n\n\n\n+max z\n{\n\u2212 C1\u2212q\nq \u2016z\u2212 d\u2016qq \u2212 z \u22a4\u03b8\n}\nIts final form is derived similarly to Claim 1, where we show that the conjugate dual of the entropy barrier is the soft-max function and the conjugate dual of the \u2113qq is the \u2113 p p.\nLemma 1 Given a vertex v in the graphical model, the optimal \u03bbx,y,v\u2192\u03b1(y\u0302v) for every \u03b1 \u2208 N(v), y\u0302v \u2208 Yv, (x, y) \u2208 S in the approximated program of Theorem 1 satisfies\n\u00b5x,y,\u03b1\u2192v(y\u0302v) = \u01ebc\u03b1 ln\n\n\n\u2211\ny\u0302\u03b1\\y\u0302v\nexp\n( \u2211\nr:\u03b1\u2208Er,x \u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) +\n\u2211\nu\u2208N(\u03b1)\\v \u03bbx,y,u\u2192\u03b1(y\u0302u)\n\u01ebc\u03b1\n)\n\n\n\u03bbx,y,v\u2192\u03b1(y\u0302v) = c\u03b1 c\u0302v\n\ney,v(y\u0302v) + \u2211\nr:v\u2208Vr,x\n\u03b8r\u03c6r,v(x, y\u0302r) + \u2211\n\u03b2\u2208N(v)\n\u00b5x,y,\u03b2\u2192v(y\u0302v)\n\n \u2212 \u00b5x,y,\u03b1\u2192v(y\u0302v) + cx,y,v\u2192\u03b1\nfor every constant cx,y,v\u2192\u03b11, where c\u0302v = cv + \u2211\n\u03b1\u2208N(v) c\u03b1. In particular, if either \u01eb and/or c\u03b1 are zero then \u00b5x,y,\u03b1\u2192v corresponds to the \u2113\u221e norm and can be computed by the max-function. Moreover, if either \u01eb and/or c\u03b1 are zero in the objective, then the optimal \u03bbx,y,v\u2192\u03b1 can be computed for any arbitrary c\u03b1 > 0, similarly for cv > 0.\n1For numerical stability in our algorithm we set cx,y,v\u2192\u03b1 such that \u2211\ny\u0302v \u03bbx,y,v\u2192\u03b1(y\u0302v) = 0\nProof: For a given x, y and v, optimizing \u03bbx,y,v\u2192\u03b1(y\u0302v) for every \u03b1 \u2208 N(v) and y\u0302v \u2208 Yv while holding the rest of the variables fixed, reduces the problem to\nmin \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebcv ln \u2211\ny\u0302v\nexp\n(\ney(y\u0302v) + \u2211\nr:v\u2208Vr,x \u03b8r\u03c6r,v(x, y\u0302v)\u2212\n\u2211\n\u03b1\u2208N(v) \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebcv\n)\n+ \u2211\n\u03b1\u2208N(v)\n\u01ebc\u03b1 ln \u2211\ny\u0302\u03b1\nexp\n( \u2211\nr:\u03b1\u2208Er \u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) +\n\u2211\nv\u2208N(\u03b1) \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebc\u03b1\n)\nLet\n\u00b5x,y,\u03b1\u2192v(y\u0302v) = c\u03b1 ln \u2211\ny\u0302\u03b1\\y\u0302v\nexp\n( \u2211\nr:\u03b1\u2208Er \u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) +\n\u2211\nu\u2208N(\u03b1)\\v \u03bbx,y,u\u2192\u03b1(y\u0302u)\n\u01ebc\u03b1\n)\n,\nand also \u03c6x,y,v(y\u0302v) = ey(y\u0302v) + \u2211\nr:v\u2208Vr,x \u03b8r\u03c6r,v(x, y\u0302v). We find the optimal \u03bbx,y,v\u2192\u03b1(y\u0302v) when-\never the gradient vanishes, i.e.\n0 = \u2207\n\n\n\n\u01ebc\u03b1 ln \u2211\ny\u0302v\nexp\n(\n\u00b5x,y,\u03b1\u2192v(y\u0302v) + \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebc\u03b1\n)\n+ \u01ebcv ln \u2211\ny\u0302v\nexp\n(\n\u03c6x,y,v(y\u0302v)\u2212 \u2211 \u03b1\u2208N(v) \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebcv\n)\n\n\n\nTaking the vanishing point of the gradient we derive two probabilities over y\u0302v that need to be the same, namely\nexp (\n\u00b5x,y,\u03b1\u2192v(y\u0302v)+\u03bbx,y,v\u2192\u03b1(y\u0302v) \u01ebc\u03b1\n)\n\u2211\ny\u0303v exp\n(\n\u00b5x,y,\u03b1\u2192v(y\u0303v)+\u03bbx,y,v\u2192\u03b1(y\u0303v) \u01ebc\u03b1\n) = exp\n( \u03c6x,y,v(y\u0302v)\u2212 \u2211\n\u03b2\u2208N(v) \u03bbx,y,v\u2192\u03b2(y\u0302v)\n\u01ebcv\n)\n\u2211\ny\u0303v exp\n( \u03c6x,y,v(y\u0303v)\u2212 \u2211\n\u03b2\u2208N(v) \u03bbx,y,v\u2192\u03b2(y\u0303v)\n\u01ebcv\n) .\nFor simplicity we need to consider only the numerator, while taking one degree of freedom in the normalization. Taking log of the numerator we get that the gradient vanishes if the following holds\nc\u0302x,y,v\u2192\u03b1 + \u00b5x,y,\u03b1\u2192v(y\u0302v) + \u03bbx,y,v\u2192\u03b1(y\u0302v)\nc\u03b1 =\n\u03c6x,y,v(y\u0302v)\u2212 \u2211 \u03b2\u2208N(v) \u03bbx,y,v\u2192\u03b2(y\u0302v)\ncv . (1)\nMultiplying both sides of the equation by cvc\u03b1, and summing both sides with respect to \u03b2 \u2208 N(v) gives\nc\u0303x,y,v\u2192\u03b1+cv \u2211\n\u03b2\u2208N(v)\n(\u00b5x,y,\u03b2\u2192v(y\u0302v) + \u03bbx,y,v\u2192\u03b2(y\u0302v)) =\n\n\n\u2211\n\u03b2\u2208N(v)\nc\u03b2\n\n\n\n\u03c6x,y,v(y\u0302v)\u2212 \u2211\n\u03b2\u2208N(v)\n\u03bbx,y,v\u2192\u03b2(y\u0302v)\n\n .\n(2) We wish to find the optimal value of \u03bbx,y,v\u2192\u03b1(y\u0302v), namely the value that satisfies Eq. (1). For that purpose we recover the value of \u2211\nb\u2208N(v) \u03bbx,y,v\u2192\u03b2(y\u0302v) from (2):\nc\u0303x,y,v\u2192\u03b1+\n\ncv + \u2211\n\u03b2\u2208N(v)\nc\u03b2\n\n\n\n\n\u2211\n\u03b2\u2208N(v)\n\u03bbx,y,v\u2192\u03b2(y\u0302v)\n\n =\n\n\n\u2211\n\u03b2\u2208N(v)\nc\u03b2\n\n\u03c6x,y,v(y\u0302v)\u2212cv \u2211\n\u03b2\u2208N(v)\n\u00b5x,y,\u03b2\u2192v(y\u0302v).\nPlugging this into 1 gives\n\u00b5x,y,\u03b1\u2192v(y\u0302v)+\u03bbx,y,v\u2192\u03b1(y\u0302v) = c\u03b1\ncv + \u2211 \u03b2\u2208N(v) c\u03b2\n\n\u03c6x,y,v(y\u0302v) + \u2211\n\u03b2\u2208N(v)\n\u00b5x,y,\u03b2\u2192v(y\u0302v)\n\n+cx,y,v\u2192\u03b1\nwhich concludes the proof for \u01eb, c\u03b1, cv > 0. Whenever any of these quantitates is zero, Danskin\u2019s theorem (cf. [1], Theorem 4.5.1) states that its corresponding subgradient is described by a probability distribution over its maximal assignments. Therefore if c\u03b1 = 0 in the objective function, then equality (1) holds for every c\u03b1, and similarly whenever cv = 0 in the objective, equality holds for every cv.\nLemma 2 The gradient of the approximated structured prediction program in Theorem 1 with respect to \u03b8r equals to\n\u2211\n(x,y)\u2208S,v\u2208Vr,x,y\u0302v\nbx,y,v(y\u0302v)\u03c6r,v(x, y\u0302v) + \u2211\n(x,y)\u2208S,\u03b1\u2208Er,x,y\u0302\u03b1\nbx,y,\u03b1(y\u0302\u03b1)\u03c6r,\u03b1(x, y\u0302\u03b1)\u2212 dr +C \u00b7 |\u03b8r| p\u22121 \u00b7 sign(\u03b8r),\nwhere\nbx,y,v(y\u0302v) \u221d exp\n(\ney(y\u0302v) + \u2211\nr:v\u2208Vr,x \u03b8r\u03c6r,v(x, y\u0302v)\u2212\n\u2211\n\u03b1\u2208N(v) \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebcv\n)\nbx,y,\u03b1(y\u0302\u03b1) \u221d exp\n( \u2211\nr:\u03b1\u2208Er,x \u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) +\n\u2211\nv\u2208N(\u03b1) \u03bbx,y,v\u2192\u03b1(y\u0302\u03b1)\n\u01ebc\u03b1\n)\nHowever, if either \u01eb and/or c\u03b1 equal zero, then the beliefs bx,y,\u03b1(y\u0302\u03b1) can be taken from the set of probability distributions over support of the max-beliefs, namely bx,y,\u03b1(y\u0302\u2217\u03b1) > 0 only if y\u0302\u2217\u03b1 \u2208 argmaxy\u0302\u03b1 { \u2211 r:\u03b1\u2208Er,x \u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) + \u2211 v\u2208N(\u03b1) \u03bbx,y,v\u2192\u03b1(y\u0302\u03b1) } . Similarly for bx,y,v(y\u0302\u2217v) whenever \u01eb and/or cv equal zero.\nProof: This is a direct computation of the gradient. In the special case of \u01eb, c\u03b1 = 0 then bx,y,\u03b1(y\u0302\u03b1) corresponds to the subgradient and similarly when \u01eb, cv = 0, cf. Danskin\u2019s theorem (cf. [1], Theorem 4.5.1).\nClaim 2 The block coordinate descent algorithm in lemmas 1 and 2 monotonically reduces the approximated structured prediction objective in Theorem 1, therefore the value of its objective is guaranteed to converge. Moreover, if \u01eb, c\u03b1, cv > 0, the objective is guaranteed to converge to the global minimum, and its sequence of beliefs are guaranteed to converge to the unique solution of the approximated structured prediction dual.\nProof: The approximated structured prediction dual is strictly concave in the dual variables bx,y,v(y\u0302v), bx,y,\u03b1(y\u0302\u03b1), z subject to linear constraints. The claim properties are a direct consequence of [3] for this type of programs.\nClaim 3 Whenever the approximated structured prediction is non convex, i.e., \u01eb, c\u03b1 > 0 and cv < 0, the algorithm in lemmas 1 and 2 is not guaranteed to converge, but whenever it converges it reaches a stationary point of the primal and dual approximated structured prediction programs.\nProof: The approximated structured prediction in Theorem 1 is unconstrained. The update rules defined in Lemmas 1 and 2 are directly related to vanishing points of the gradient of this function, even when it is non-convex. Therefore a stationary point of the algorithm corresponds to an assignment \u03bbx,y,v\u2192\u03b1(y\u0302v), \u03b8r for which the gradient equals zero, or equivalently a stationary point of the approximated structured prediction.\nThe dual approximated structured prediction in (??) is a constrained optimization and its stationary points are saddle points of the Lagrangian, defined in Theorem 1, with respect to the probability simplex bx,y,v(y\u0302v) \u2208 \u2206Yv and bx,y,\u03b1(y\u0302\u03b1) \u2208 \u2206Y\u03b1 . Note that since \u01eb, c\u03b1, cv 6= 0 the entropy functions act as barrier functions on the nonnegative cone, therefore we need not consider the nonnegative constraints over the beliefs. In the following we show that at stationary points the inferred beliefs of the Lagrangian satisfy the marginalization constraints, therefore are saddle points of the Lagrangian.\nWhen \u01eb, c\u03b1 > 0 the stationary beliefs bx,y,\u03b1(y\u0302\u03b1) are achieved by maximizing over \u2206Y\u03b1 , resulting in\nbx,y,\u03b1(y\u0302\u03b1) \u221d exp\n( \u2211\nr:\u03b1\u2208Er,x \u03b8r\u03c6r,\u03b1(x, y\u0302\u03b1) +\n\u2211\nv\u2208N(\u03b1) \u03bbx,y,v\u2192\u03b1(y\u0302\u03b1)\n\u01ebc\u03b1\n)\n.\nHowever, since cv < 0 the stationary beliefs bx,y,v(y\u0302v) are achieved by minimizing over \u2206Yv resulting in\nbx,y,v(y\u0302v) \u221d exp\n(\ney(y\u0302v) + \u2211\nr:v\u2208Vr,x \u03b8r\u03c6r,v(x, y\u0302v)\u2212\n\u2211\n\u03b1\u2208N(v) \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebcv\n)\n.\nTo prove these beliefs correspond to a stationary point we show that they satisfy the marginalization constraints. This fact is a direct consequence of the update rule in Lemma 1, where by direct computation one can verify that\n\u2211\ny\u0302\u03b1\\y\u0302v\nbx,y,\u03b1(y\u0302\u03b1) \u221d exp\n(\n\u00b5x,y,\u03b1\u2192v(y\u0302v) + \u03bbx,y,v\u2192\u03b1(y\u0302v)\n\u01ebc\u03b1\n)\n.\nFollowing the definition of bx,y,v(y\u0302v) one can see that the update rule in Lemma 1 enforces the marginalization constraints. This implies that the gradient of the approximated structured prediction program measures the disagreements between \u2211\ny\u0302\u03b1\\y\u0302v bx,y,\u03b1(y\u0302\u03b1) and bx,y,v(y\u0302v), and the\ngradient vanishes only when they agree. Therefore these beliefs correspond to a saddle point of the Lagrangian."}], "references": [{"title": "Convex Analysis and Optimization", "author": ["D.P. Bertsekas", "A. Nedi\u0107", "A.E. Ozdaglar"], "venue": "Athena Scientific,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton university press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1970}, {"title": "Relaxation methods for problems with strictly convex separable costs and linear constraints", "author": ["P. Tseng", "D.P. Bertsekas"], "venue": "Mathematical Programming,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends R \u00a9 in Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "[4] Theorem 8.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2]), where in our case z = \u2211 (x,y),\u0177 px,y(\u0177)\u03a6(x, \u0177)\u2212 d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], Theorem 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], Theorem 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The claim properties are a direct consequence of [3] for this type of programs.", "startOffset": 49, "endOffset": 52}], "year": 2012, "abstractText": "and composed from the conjugate dual of the soft-max and the conjugate dual of the lp norm. Recall that the conjugate dual for the soft-max is the entropy barrier \u01ebH(px,y) over the set of probability distributions \u2206Y (cf. [4] Theorem 8.1), and that the linear shift of the soft-max argument by ey(\u0177) result in the linear shift of the conjugate dual, thus we get the first part of the dual function \u2211 (x,y)(\u01ebH(px,y) + e \u22a4 y px,y). Similarly, the conjugate dual of 1 p \u2016\u03b8\u2016p is 1 q \u2016z\u2016q for the dual norm 1/p+ 1/q = 1 (cf. [2]), where in our case z = \u2211 (x,y),\u0177 px,y(\u0177)\u03a6(x, \u0177)\u2212 d. Theorem 1 The approximation of the structured prediction program in (3) takes the form min \u03bbx,y,v\u2192\u03b1,\u03b8 \u2211", "creator": "LaTeX with hyperref package"}}}