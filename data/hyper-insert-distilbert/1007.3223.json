{"id": "1007.3223", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2010", "title": "Testing and Debugging Techniques for Answer Set Solver Development", "abstract": "typically this mathematical paper successfully develops fast automated testing and other debugging techniques for answer questions set solver puzzle development. below we describe a flexible grammar - based black - box type asp computational fuzz application testing tool which is remarkably able to reveal various defects such like as underlying unsound knowledge and incomplete behavior, i. e. combining invalid answer response sets inability and inability matching to find existing solutions, resulted in state - of - the - art answer set solver learning implementations. moreover, meanwhile we develop generic delta debugging capability techniques for permanently shrinking any failure - inducing inputs on neurons which solvers exhibit defective behavior. in particular, additionally we typically develop exactly a delta search debugging algorithm in the technical context of answer set solving, and better evaluate successfully two different distinct elimination mapping strategies for using the aforementioned algorithm.", "histories": [["v1", "Mon, 19 Jul 2010 17:51:54 GMT  (46kb,S)", "http://arxiv.org/abs/1007.3223v1", "18 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.AI cs.SE", "authors": ["robert brummayer", "matti j\\\"arvisalo"], "accepted": false, "id": "1007.3223"}, "pdf": {"name": "1007.3223.pdf", "metadata": {"source": "CRF", "title": "Testing and Debugging Techniques for Answer Set Solver Development", "authors": ["ROBERT BRUMMAYER"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n00 7.\n32 23\nv1 [\ncs .A\nI] 1\nKEYWORDS: answer set programming, answer set solvers, testing, debugging"}, {"heading": "1 Introduction", "text": "Answer set programming (ASP) (Gelfond and Lifschitz 1988; Niemela\u0308 1999) is a rule-based declarative programming paradigm that has proven to be an effective approach to knowledge representation and reasoning in various hard combinatorial problem domains. This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemela\u0308 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).\nImplementing robust, sound and complete answer set solvers is a demanding task. For achieving high solver performance, one needs to implement error-prone and complex inference rules, specialized data structures, and other complex optimizations. On the other hand, robustness and correctness are two essential criteria for answer set solvers. The users of answer set solvers heavily depend on correct results and, in particular, correct answer sets. The lack of systematized testing tools for answer set solver development may leave intricate implementation bugs unnoticed. Indeed, in practice, small sets of problem instances that are typically used during regression and unit testing are not enough for testing correctness during solver development. Moreover, while the availability of standard benchmark instances is\nof high importance for benchmarking solver implementations, testing during solver development should not solely rely on these instances. In support of these claims, by examining the detailed results of the first and second ASP programming competitions (Gebser et al. 2007; Denecker et al. 2009) one notices that, on the sets of (typical) benchmarks used in these competition, only very few solvers on very few benchmarks were judged as providing incorrect results. In other words, almost no defective behavior seems to have been detected. In contrast, we will show that by using the testing and debugging techniques developed in this work, various kinds of incorrect and erroneous behavior can be automatically detected and debugged for various state-of-the-art answer set solvers; furthermore, this is achieved without the need for the user to construct hand-crafted benchmarks. The testing techniques developed here provide complementary means for developing highly correct solvers, which can be applied in addition to domain-specific benchmarks. Additionally, our delta debugging techniques can be naturally applied also when defective solver behavior is detected on domain-specific benchmarks.\nIn more detail, this paper develops domain-specific grammar-based black-box fuzz testing and delta debugging techniques that enable more systematic testing and debugging solutions for answer set solver development. Fuzz testing, also called fuzzing (Sutton et al. 2007; Takanen et al. 2008), has its origin in software security and quality assurance. The main idea of fuzzing is to test software against random inputs in order to find failure-inducing inputs that trigger defective behavior. In order to find as many defects as possible, a \u201cgood\u201d fuzzer (the input generator) should generate a wide variety of different inputs. In the grammar-based approach, the generated input is guaranteed to be syntactically valid, i.e. the input respects the expected input format. In black-box fuzzing, testing is performed against the software interface without access to the implementation details of the software. We develop a black-box grammar-based fuzz testing tool that is able to generate random ASP instances from various different classes.\nIn many cases, randomly generated failure-inducing inputs may contain large parts that are irrelevant for triggering defective behavior, and can hence be too large to enable efficient debugging. In the context of ASP, a failure-inducing answer set program can simply have too many rules and atoms for a developer to manually pinpoint the subset of program rules that triggers the defective behavior. In order to isolate the failure-inducing parts of such failure-inducing inputs, an automatic technique called delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) has been proposed. Minimization of the size of failure-inducing input is intractable in general, and hence delta debuggers are based on greedy heuristics. We develop an efficient and novel delta debugger which is very effective in producing small failure-inducing inputs (answer set programs), hence relieving the solver developer from the troublesome task of locating the typically small failure-inducing parts of a large failure-inducing answer set program.\nMain contributions We develop a grammar-based black-box fuzz testing tool for answer set solvers that covers various different classes of grounded answer set\nprograms. Our experimental analysis shows that our fuzzing approach is very effective in revealing various sources of incorrectness, including both unsound and incomplete behavior, in a wide range of state-of-the-art answer set solvers. Additionally, of independent interest is that since our fuzzer is highly configurable, it can also be used as a flexible ASP benchmark generator. Moreover, we develop a novel delta debugging algorithm for answer set solver development. Our algorithm uses the hierarchical (Misherghi and Su 2006) structure of answer set programs to shrink failure-inducing instances effectively. Furthermore, we evaluate two different elimination strategies we have implemented for our delta debugging algorithm: a simple and easy-to-implement \u201cone-by-one\u201d elimination strategy, and another one based on the more intricate DDMin algorithm originally proposed in different context (Zeller and Hildebrandt 2002).\nAll tools developed in this work are publicly available and can be downloaded at http://fmv.jku.at/fuzzddtools/. Since the tools treat answer set solvers to be tested as black-box entities, no modifications to the actual solvers are needed in order to apply these tools in the development process of any answer set solver that accepts input in the standard lparse format.\nThe rest of this paper is organized as follows. First we review necessary concepts related to answer set programs (Section 2). We then present our fuzzing approach for testing answer set solvers (Section 3), with experimental results of the effectiveness of the fuzzer (Section 4). Finally, we develop delta debugging strategies in the context of answer set solving (Section 5) and present an experimental evaluation of our approach (Section 6), followed by pointers to related work (Section 7)."}, {"heading": "2 Answer Set Programs", "text": "This section reviews the stable model semantics and the classes of answer set programs considered in this work.\nNormal Logic Programs and Answer Sets A normal logic program (NLP) consists of a finite set of normal rules of the form\nr : h \u2190 a1, . . . , an ,\u223cb1, . . . ,\u223cbm ,\nwhere each ai and bj is a propositional (or normal) atom, and h is either a propositional atom or the symbol \u22a5 that stands for falsity. A rule r consists of a head, head(r) = h, and a body, body(r) = {a1, . . . , an ,\u223cb1, . . . ,\u223cbm}. A rule r is a fact if body(r) = \u2205, and an integrity constraint if head(r) = \u22a5. The symbol \u201c\u223c\u201d denotes default negation. A default literal is an atom a or its default negation \u223ca.\nFor a rule r , let body(r)+ = {a1, . . . , an} and body(r)\u2212 = {b1, . . . , bm} denote\nthe sets of positive and negative (default negated) atoms in body(r), respectively.\nIn ASP, we are interested in stable models (Gelfond and Lifschitz 1988) (or answer sets) of a program \u03a0. An interpretation M \u2286 atom(\u03a0) defines which atoms of \u03a0 are true (a \u2208 M ) and which are false (a 6\u2208 M ). An interpretation M \u2286 atom(\u03a0) satisfies a normal rule r if and only if body(r)+ \u2286 M and body(r)\u2212 \u2229M = \u2205 imply head(r) \u2208 M , and hence M is a (classical) model of \u03a0 if M satisfies all rules in \u03a0.\nA model M of a program \u03a0 is an answer set of \u03a0 if and only if there is no model M \u2032 \u2282 M of \u03a0M , where \u03a0M = {head(r) \u2190 body(r)+ | r \u2208 \u03a0 and body(r)\u2212\u2229M = \u2205} is called the Gelfond-Lifschitz reduct of \u03a0 with respect to M . The problem of deciding whether a NLP has an answer set is NP-complete.\nWeight Constraint Programs In order to enable more convenient modeling in ASP, extensions of normal programs have been proposed. Examples of such extensions are what we refer here to as weight constraint programs (WCPs). A weight atom is of the form\nl [a1 = wa1 , . . . , an = wan ,\u223cb1 = wb1 , . . . ,\u223cbm = wbm ]u,\nwhere each li \u2208 {ai}1\u2264i\u2264n \u222a {\u223cbj}1\u2264j\u2264m is a default literal, each wi an integer (the weight of li), and u, l are integers with l \u2264 u (the lower and upper bound, respectively). If one of the bounds u and l is omitted, this bound is implicitly \u221e (for u) or \u2212\u221e (for l). A cardinality atom is the special case of a weight atom in which each literal has weight one. The variant of cardinality atoms in which both of u, l are omitted is called a choice atom, that is an expression of the form {a1, . . . , an}, where each ai is a normal atom.\nA weight constraint rule is of the form r : C0 \u2190 C1, . . . ,Cn , where the head C0 is a normal, weight, cardinality, choice atom, or \u22a5 and each Ci , i > 0 in the body (which can also be empty) is a normal, weight, or cardinality literal. We use the term weight constraint programs for the set of programs that consist of weight constraint rules. Hence NLPs are special cases of weight constraint programs.\nGiven an interpretation M , the stable model semantics extends to weight constraint programs by defining that a weight atom is satisfied by M if and only if l \u2264 \u2211\nai\u2208M wi + \u2211 bj 6\u2208M wj \u2264 u. A choice atom is always satisfied by M . The\nproblem of deciding whether a given weight constraint program has an answer set remains in NP.\nDisjunctive Logic Programs Another extension of normal rules are disjunctive rules, in which the head can, instead of a normal atom, be a disjunction \u2228n\ni=1 ai of normal\natoms. Disjunctive logic programs (DLPs) can contain normal and disjunctive rules. The stable model semantics extends to the disjunctive case naturally by defining that a disjunctive rule r with head(r) = \u2228n\ni=1 ai is satisfied by an interpretation\nM if and only if body(r)+ \u2286 M and body(r)\u2212 \u2229 M = \u2205 imply ai \u2208 M for some i . The problem of deciding whether a DLP has an answer set is \u03a3p2 -complete and thus presumably harder than the case of NLPs."}, {"heading": "3 Fuzz Testing Answer Set Solvers", "text": "In this section we develop a native grammar-based black-box fuzzing approach for testing answer set solvers."}, {"heading": "3.1 Grammar-Based ASP Fuzzing", "text": "In order to apply grammar-based fuzz testing to answer set solvers, methods for generating wide varieties of different answer set programs need to be developed. There\nare only a few studies that consider the problem of generating random logic programs in the context of ASP (Zhao and Lin 2003; Namasivayam and Truszczyn\u0301ski 2009). These studies consider rather restricted subclasses of NLPs and focus on theoretical aspects such as the study of phase transition behavior. In contrast, our aim here is to generate a wide variety of different random answer set programs in order to test answer set solvers."}, {"heading": "3.1.1 Ineffectiveness of CNF-Based ASP Fuzzing", "text": "A simple approach to generating random answer set programs consists of first using generators for random conjunctive normal form (CNF) instances of the Boolean satisfiability (SAT) problem and translating the generated CNFs into answer set programs afterwards. However, this approach appears to be ineffective, as revealed by the following evaluation.\nWe obtained CNF instances by generating random propositional formulas as\nBoolean circuits and translating them to CNF via a standard encoding (Tseitin 1983). The NLPs were obtained from the CNF instances using the following standard translation: given a CNF F , introduce (i) for each Boolean variable x in F the rules x \u2190 \u223cx\u0302 and x\u0302 \u2190 \u223cx (forcing classical interpretations); and (ii) for each clause c in F , the rule \u22a5 \u2190 \u223cc and for each Boolean variable in c the rule c \u2190 x (c \u2190 \u223cx , resp.) if x occurs positively (negatively, resp.) in c (stating the clause should be satisfied). Notice that this translation always results in tight NLPs, a subclass of NLPs.\nNotably, using a 1-hour time limit and the same hardware settings as in our latter experiments, we tested all of the answer set solvers that are shown in Table 1 on 8850 CNF instances, but did not find any defects. We conjecture that the CNFbased fuzz testing approach for ASP is unsuccessful as it lacks domain knowledge and considers only tight NLPs. This gives motivation to develop domain-aware fuzz testing approaches for answer set solver development, which take the specific features of different ASP classes into account."}, {"heading": "3.2 FuzzASP: A Native ASP Fuzzer", "text": "In order to generate a wide variety of different answer set programs, we developed FuzzASP, which is a native fuzzer for ground answer set programs generated in the syntax of lparse. In addition to normal logic programs, it supports combinations of disjunctive and extended rules with choice, cardinality and weight atoms, and classical negation. FuzzASP is able to generate varying types of random program instances from large classes of programs in order to provide high variety for different combinations of rule constructs.\nFuzzASP generates programs as follows. Let A be a set of n normal atoms.\n1. A set of f facts (normal rules with empty bodies) is generated by picking each\nhead uniformly at random (u.a.r.) from A.\n2. Normal rules with non-empty bodies and varying lengths are generated until\neach atom in A occurs in at least rb and rh bodies and heads, respectively. Each normal rule is generated by picking the head and each body atom u.a.r.\nfrom A. Moreover, each body atom is default negated with probability pdn . The body length of each normal rule is chosen u.a.r. from a predefined range. 3. A set of i integrity constraints is generated, picking each body atom u.a.r.\nfrom A and default negating each atom with probability pdn .\nWhen generating a WCP:\n4. A set of W weight constraint rules is generated. The head of each rule is\nrandomly chosen to be a normal atom from A, or a weight, cardinality, or choice atom. A weight atom l [a1 = wa1 , . . . , an = wan ,\u223cb1 = wb1 , . . . ,\u223cbm = wbm ]u is generated by picking atoms u.a.r. from A, and negating each atom with probability pdn . The weights for the literals and the bounds l and u, where l \u2264 u, are chosen randomly. The number of normal atoms to appear in each weight atom is chosen u.a.r. from a predefined range. Additionally, one of the bounds u and l , and weights of individual literals are left out with certain probabilities. Cardinality and choice atoms are generated analogously. Each body literal is similarly chosen to be a normal, weight or cardinality atom.\nWhen generating a DLP:\n4. A set of d disjunctive rules are generated. A disjunctive head is generated by\npicking da atoms u.a.r. from A (similarly for the normal atoms in the bodies, default negating with probability pdn). The head length of each disjunctive rule is chosen u.a.r. from a predefined range.\nThe fuzzer has been designed to be highly configurable. Nearly every detail can be configured. However, this is optional as the fuzzer already comes with reasonable default values. Due to page limitations, for details on the actual default values provided in the implementation of FuzzASP, please refer to the help provided in the actual FuzzASP implementation.\nWe have configured the default values through experimentation so that the generated logic programs are not trivial but also not too hard to solve. One key success factor of fuzz testing is high test throughput, which means that generating hard instances solely is counterproductive. On the other hand, trivial instances are unlikely to be critical failure-inducing inputs, as they can often be solved in early phases of the answer set solver, e.g. in a pre-processing phase. Therefore, in order to generate various different programs of varying difficulty, the fuzzer randomizes its parameters. For each parameter a minimum and a maximum value is considered. The fuzzer respectively picks one value within the particular range."}, {"heading": "3.3 Solver Defect Categories", "text": "We divide defects of answer set solvers into three categories:\nErrors contains instances on which the solver terminates in an unexpected way\nwithout providing a result, e.g. segmentation faults and assertion failures;\nInvalid models contains instances on which the solver reports a solution that\nis not an answer set (either not minimal or not even a classical model) of the instance; and\nIncorrect where, if the solution provided by at least one solver is a correct answer\nset, all solvers that report that no answer sets exist are treated as incorrect.\nNotice that these categories are disjoint in the sense that, for a given instance, each solver can only exhibit a defect that belongs to exactly one of the categories. Notice also that in this paper we concentrate on the problem of answer set existence. Hence, especially considering the defect categories invalid models and incorrect, for each solver a single answer set for each instance is checked for correctness. However, the testing and debugging techniques developed in this work can also easily be adapted for the problem of answer set enumeration, that is, for checking the validity of all answer sets reported by a solver.\nConsidering the defect category invalid models, for a given instance \u03a0, we employ the following method for checking if solutions reported by solvers are valid answer sets of \u03a0. For given a model candidate M reported by a solver, we construct the set IM = \u22c3 a\u2208M{\u22a5 \u2190 \u223ca}\u222a \u22c3 a 6\u2208M {\u22a5 \u2190 a} of integrity constraints. Then M is a valid answer set of \u03a0 if and only if \u03a0 \u222a IM has an answer set. Notice that it is trivial to check whether \u03a0 \u222a IM has an answer set. Using this method, also incorrect solver behavior is captured in the case a solver claims that there are no answer sets for a given instance \u03a0, if some other solver reports an answer set that is determined as a valid one using the checking method.\nFurthermore, we also crosscheck all occurrences in the categories invalid models and incorrect using voting. In more detail, assume that, based on the abovedescribed checking method, a specific solver S reports an invalid model (or claims incorrectly that no answer sets exist) for a given instance \u03a0. Then, by running a set of solvers on \u03a0, we crosscheck invalid models and incorrectness by checking that a majority of the solvers report that there are no answer sets for \u03a0 (or report an answer set), that is, a majority of the solvers vote against the output of the solver S on \u03a0. In the ideal case, all other solvers vote against the output of S . In our experiments in this paper, all crosschecks turned out to be ideal in this sense."}, {"heading": "4 Fuzzing Experiments", "text": "We performed fuzz testing experiments using FuzzASP for the following classes of logic programs: NLP (normal programs), WCP (weight constraint programs), and DLP (disjunctive programs). We ran our experiments under Ubuntu Linux on an Intel Core 2 Quad 2.66 GHz machine with 8 GB of RAM. Our fuzzing framework used all the four cores for parallel testing. Using default settings, we tested a wide selection1 of answer set solvers that participated in the first (Gebser et al. 2007) or second (Denecker et al. 2009) ASP Competition in 2007/2009. The grounder lparse\n1 Solvers: Clasp 1.2.1, ClaspD 1.1, Cmodels 3.79, DLV precompiled build BEN/Oct 11 2007, GnT2 precompiled v. 2.1 using Smodels 2.33 as backend, lp2diff precompiled 1.19 with lp2normal 1.7 using Z3 2.0 SMT solver (de Moura and Bj\u00f8rner 2008) as backend, lp2sat precompiled 1.11 with lp2atomic 1.12 using Picosat 913 SAT solver (Biere 2008) as backend, noMoRe++ 1.5., PBmodels 0.2 using Minisat+ 1.0 pseudo-boolean solver as backend, Smodels 2.33, Smodels-ie standalone 1.0.0, Smodels cc 1.08, SUP 0.4 using Minisat 1.12b SAT solver (Ee\u0301n and So\u0308rensson 2004).\n(version 1.1.1) was used as a front-end for the solvers. The only exception was the solver DLV, which does not require an external front-end. For each class, we restricted the total fuzz testing time to one hour.\nWe want to emphasize that our goal is not to present results for all available solvers, but rather to demonstrate the wide applicability of our testing and debugging techniques on different types of answer set solvers.\nThe experimental results for NLP, WCP, and DLP are presented in Tables 1, 2, and 3, respectively. In short, our fuzz testing approach is very effective in finding solver defects in state-of-the-art answer set solvers, due to the impressive number of critical defects found in the experiments. Next, we will discuss the results for each of the considered program classes (NLP, WCP, and DLP) in more detail. Full results including failure-inducing inputs can be found in the archive http://fmv.jku.at/brummayer/fuzz-dd-asp.tar.7z."}, {"heading": "4.1 Defects Found on NLP", "text": "For the class NLP, the 1-hour time limit resulted in testing the solvers listed in Table 1 on 10190 instances generated by FuzzASP. For generating NLPs, we used the default options of FuzzASP (with weight and cardinality literals disabled). As shown in Table 1, the effectiveness of detecting solver defects in NLP is rather modest. Mostly, errors such as crashes were detected, most notably in high numbers for lp2sat, Smodels-ie, and SUP. A few invalid models were reported by Cmodels and lp2diff. Moreover, we found two instances on which Clasp incorrectly reports that there are no answer sets.\nThis is already in contrast to the ineffective CNF based ASP fuzzing experiment, confirming our conjecture that domain-unaware fuzz testing approaches are in general not effective in finding solver defects, and therefore domain-aware fuzzing techniques have to be developed individually.\nIn order to verify the validity of a model M reported by a solver for a test instance \u03a0, we used Smodels as trusted solver for checking whether the program \u03a0 \u222a IM\nhas an answer set (the original test instance enhanced with the model candidate as integrity constraints IM ). This check is trivial as it requires only deterministic propagation and no search. Smodels was chosen for NLP and additionally for WCP, since it exhibited neither errors nor incorrect results.\nWe want to further stress that although we do not have the precise running time data for individual solvers available, the total time for this experiment was one hour wall clock time using four processor cores, and hence the testing time used on each of the 11 solvers was around 20 minutes on average, and in this time over 10000 test cases were tried, which totals in over 110000 solver calls. Furthermore, we would like to point out that the 1-hour testing time limit used in the fuzzing experiments was only enforced for obtaining a representative snapshot to the effectiveness of the testing technique. In practice, the testing loop works by generating one test case at a time, and running all solvers on this test case. For testing a specific solver, one can stop the testing loop as soon as a single failure (error, incorrect result, or the like) is detected for the specific solver."}, {"heading": "4.2 Defects Found on WCP", "text": "In particular, the effectiveness of FuzzASP is very impressive on WCP and DLP. The fuzz testing results on 19840 inputs for WCP are shown in Table 2. The input logic programs were generated using the FuzzASP option of introducing additional choice, cardinality and weight rules, each up to 5% of generated normal rules. Here, we tested those solvers that accept the class WCP (supported by lparse) as input.\nThe results on NLP andWCP suggest that many defects are due to the techniques implemented for inference on weight constraint rules. Based on the results, Smodels appears to be the most stable solver for this class of programs, being the only solver for which no defects were found. As an example of the difficulty of maintaining correctness while optimizing solver performance, we observed a high number of defects in each category for Smodels-ie, which is a re-implementation of Smodels with improved data structures aimed at better memory locality.\nFor answer set solvers that apply different back-end solvers such as SAT and SMT solvers, the back-end solver may be to blame for incorrectness. Taking lp2diff as an example, we did a cross-check in order to pinpoint the source of incorrectness on both NLP and WCP. The same incorrect behavior occurred when two other SMT\nsolvers, CVC3 (Barrett and Tinelli 2007) and Yices (Dutertre and de Moura 2006) were used as back-end solvers. Hence, the source of incorrectness is highly likely to be in lp2diff itself.\nAgain, notice that the total time for this experiment was one hour wall clock time (including all setup and test instance generation times) using four processor cores, and hence the testing time used on each of the six solvers was around 40 minutes on average, and in this time close to 20000 test cases were tried."}, {"heading": "4.3 Defects Found on DLP", "text": "The fuzz testing results on DLP are shown in Table 2. Here we used the FuzzASP option of introducing disjunctive rules up to 5% of generated normal rules.\nWhile no defects were found for DLV and GnT2, a vast number of defects were found for ClaspD and Cmodels. Due to its robustness, DLV was used as trusted solver for checking validity of reported models in DLP. Based on these results, we conclude that many defects are due to the techniques particularly implemented for handling disjunctive rules."}, {"heading": "5 Delta Debugging for Answer Set Solvers", "text": "Now, we focus on developing delta debugging algorithms for answer set solvers. The overall goal of delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) is to minimize the size of failure-inducing inputs while maintaining the same observable behavior. In this way, large irrelevant parts of the inputs are pruned away, resulting in small program instances that consist of isolated failure-inducing parts."}, {"heading": "5.1 The Delta Debugging Algorithm", "text": "As an overview, our delta debugging algorithm DeltaASP works as follows. The delta debugger runs the solver on the original failure-inducing input in order to observe the defective behavior, e.g. the solver crashes with a segmentation fault, or outputs a solution which is not a valid answer set. Then, the delta debugger iteratively tries to eliminate parts of the current input. After each elimination, the delta debugger runs the solver on the current (reduced) input. If the solver shows the same observable behavior, the delta debugger continues with this reduced input. Otherwise, the delta debugger undoes the last elimination, and continues with other eliminations. Finally, after a given time limit or after reaching a fix-point, the delta\ndebugger terminates and outputs a smaller program that is guaranteed to trigger the same observable behavior as the original program instance.2\nNotice that the goal of delta debugging is to obtain a small failure-inducing input within a reasonable time limit, e.g. a few seconds or minutes. In other words, in practice, engineers are hardly interested in minimal failure-inducing inputs if they have to wait a long time. Therefore, delta debuggers typically apply (greedy) elimination heuristics for reducing failure-inducing inputs within a small time limit.\nGiven a failure-inducing answer set program \u03a0 as input, the eliminations attempted by the DeltaASP delta debugging algorithm can be divided into the following phases:\n1. Remove rules from \u03a0 until fix-point (heuristically).\n2. For each rule r \u2208 \u03a0: if r is neither a fact nor a constraint, then try to replace\nhead(r) with \u22a5 and resp. body with \u2205.\n3. If at least one rule could be reduced in phase 2, goto 1.\n4. For each rule r \u2208 \u03a0: try to remove individual literals from head(r) resp. body(r)\nwhile |head(r)| > 1 resp. |body(r)| > 1.\n5. For each rule r \u2208 \u03a0: try to remove individual elements of each weight, cardi-\nnality, and choice literal in r while elements are left.\n6. For each rule r \u2208 \u03a0: try to remove the negation from individual negative\nliterals in r .\n7. If at least one rule could be reduced in 4\u20136, goto 1. Otherwise, output the\ncurrent program and terminate.\nDeltaASP can be seen as a variation of hierarchical delta debugging (Misherghi and Su 2006),\nsince our method proceeds from the top-most elements of the hierarchy (rules) to lower-level elements: first rules, then individual heads and bodies of rules, then individual literals, and, at last, negations. As a greedy heuristic, our primary objective is to minimize the number of rules as soon as possible. This may drastically prune large irrelevant parts of the inputs up front. As removing and reducing individual rules may enable removing rules that could not be removed before, we perform rule removal until fix-point in phase 1, and perform a \u201crestart\u201d in phase 3 if at least one rule could be reduced in phase 2. We perform this restart in order to minimize the number of rules, heads and bodies up front before we try more fine-grained reductions in phases 4\u20136. Typically, as soon as we reach phase 4, the input has already been reduced significantly.\nOur secondary objective is to reduce individual rules after no more rules can be removed. These reductions are performed in phases 2 and 4\u20136. Again, reducing individual rules may enable the removal and reduction of rules that could not be removed resp. reduced before. As restarting (\u201cgoto 1\u201d) after each individual phase of 4\u20136 can be too costly, we postpone restarts until phase 7.\n2 For the defect categories invalid models and incorrect, same observable behavior is checked against the result reported by the trusted solver on the same instance. Another possibility would be to employ majority voting by running multiple solvers on the same instance."}, {"heading": "5.2 Removal strategies", "text": "In phases 1, 4, and 5 we consider a set from which we want to eliminate as many elements as possible, e.g. the set of rules in phase 1. Next, we discuss and evaluate the differences between a simple one-by-one approach (OBO) and a more intricate strategy based on the DDMin algorithm (Zeller and Hildebrandt 2002).\nDDMin The original DDMin algorithm (Zeller and Hildebrandt 2002) attempts to divide the current set into k subsets, where k (the granularity) is initialized to 2. If at least one of the subsets is enough to reproduce the same observable behavior, the current set is reduced to this subset, granularity is reset to 2, and the algorithm continues. Otherwise, it tries the complement sets of each of the subsets. If using the set complements does not succeed either, the granularity k is doubled, i.e. in the next iteration the current set is divided into smaller subsets. In the last iteration, the granularity is equal to the size of the current set, which means that each element is in its own subset. Notice that in order to avoid recomputations on already considered subsets, intermediate results need to be cached.\nIf the failure-inducing input part is rather local and does not depend much on other input parts, the DDMin algorithm tends to simulate a binary search strategy during the first iterations, since the current set can often be reduced to one of its considered two subsets. However, if the failure-inducing part strongly depends on other parts of the input, trying k subsets up front will only seldom lead to success. Then, DDMin has to consider the set complements and to iteratively increase the granularity, which may be rather ineffective.\nOBO As an alternative to DDMin, we also consider a simple strategy based on a one-by-one (OBO) principle. We iterate over all elements in the set and try to remove them one by one. After each iteration, we repeat the process if at least one element could be removed. In principle, we could immediately restart the algorithm as soon as we have been able to remove one element. However, this may be too costly and therefore the restart is postponed until the end of the iteration. The benefit of the OBO strategy is that, in contrast to the DDMin strategy, it is easy to implement and does not need any caching techniques."}, {"heading": "6 Delta Debugging Experiments", "text": "For the delta debugging experiments, we used the same hardware (this time using a single processor core) and settings as for the fuzzing experiments. However, the delta debugging experiments were not run simultaneously. For the experiments, we used the failure-inducing inputs found in the fuzzing experiments reported in Section 4. Depending on the classes of errors and error messages, we semi-automatically divided the failure-inducing inputs into different bug classes.\nThe actual implementation of DeltaASP compares exit codes in order to determine whether the observable behavior has changed or not. Instead of passing the name of the answer set solver executable directly to DeltaASP, we pass the name of a wrapper script that calls the answer set solver and returns a specific exit code if\nthe defective behavior occurs, e.g. grep for a specific error message was successful. This approach makes the delta debugger highly flexible. In this way, the concept of observable behavior is not limited to one solver, but can be extended to multiple solvers. For example, considering the classes invalid models and incorrect, if we observe that two solvers report different results on the same instance, we can use a simple shell script to execute both solvers on the instance passed as argument. If the solvers agree on the result, we return exit code 1 and 0 otherwise. With this technique we delta debugged incorrect results as already proposed in (Brummayer and Biere 2009), but with exactly one trusted solver. Alternatively, one could apply majority voting using multiple solvers.\nNotice that, in principle, DeltaASP could reorder rules and rule elements before delta debugging. For example, the rules could be sorted with respect to rule size such that OBO tries to eliminate larger rules first. However, we found out that changing the order of rules and individual rule elements may make the considered failure disappear. Therefore, DeltaASP does not change the original relative order of rules and individual rule elements.\nFor the experiments, we used a limit of 100 inputs for each class. The results are shown in Tables 4, 5 and 6. Due to page limitations, examples of failure-inducing inputs, and the instances resulting from delta debugging these inputs, can be found in the archive http://fmv.jku.at/brummayer/fuzz-dd-asp.tar.7z that contains the full delta debugging results. In Tables 4, 5 and 6, ins is the resulting total number of instances delta debugged for each solver, c the number of different bug classes, time the average delta debugging time in seconds for OBO (obo) and DDMin (ddm), and size (resp. red) the average size of the resulting instance in bytes (the average reduction in percentages). Notice that, reflecting the reduction in the size of the failure-inducing answer set program, we measure the success of delta debugging in file size. Alternatively, the number of lines or number of rules could be used. However, we found out that these strongly correlate with the file size.\nFor both of the elimination strategies DDMin and OBO, the average delta debugging times in all categories are less than one minute, and led to an impressively high reduction of at least 98.6%. This clearly shows the effectiveness and overall success of our DeltaASP delta debugging algorithm.\nThe size reduction achieved by the OBO and DDMin strategies is almost identical for the three program classes NLP, WCP, and DLP. The running times using OBO and DDMin vary more noticeably. Moreover, the more effective strategy depends on the program class. For NLP, DDMin results in making over 50% more calls to the solvers than OBO on average (916 and 572 calls, respectively). This is also reflected in the running times for the strategies on NLP. Thus it seems that a simple one-byone elimination strategy is the preferred one for NLPs, as the more intricate DDMin makes many ineffective elimination checks. On the other hand, the two elimination strategies give almost identical results on WCPs also time-wise. For DLPs, however, the situation is the opposite to the NLP case: while the difference in the number of solver calls is relatively small, DDMin is over twice as fast as OBO. Comparing this with the NLP case, we believe that the result for DLP is due to the fact that,\nsince disjunctive programs are fundamentally harder to solve than NLPs, it pays off to apply an eager elimination strategy such as DDMin, which results in calling a solver with relatively smaller programs during delta debugging. In particular, if the granularity is rather low, DDMin may eliminate large subsets.\nFinally, we note that even non-deterministic solver behavior can be handled by our delta debugging framework. For example, we observed non-deterministic behavior for Smodels-ie which, when run on the same instance multiple times, either crashed with a segmentation fault, terminated with a result, or did not terminate at all. In order to handle such cases in which a solver may not always terminate\nduring delta debugging, time limits can be used. (Here we had to use a time limit (5 seconds) for Smodels-ie.) In more detail, recall that the delta debugger calls the solver (the wrapper script) after each elimination. Each call to a solver is run with a fixed time limit. If the solver does not return a result within the time limit, the shell script returns a specific exit code different from the exit code on the original instance. Then, the delta debugger treats this case as if the elimination has failed, undoes the last elimination, and continues."}, {"heading": "7 Related work", "text": "The most closely related work is the fuzz testing and delta debugging approach developed for SMT (satisfiability modulo theories) solvers in (Brummayer and Biere 2009). Our work differs in developing ASP specific fuzzing techniques and, especially, novel delta debugging techniques and strategies in the context of answer set solving. In contrast to our generic black-box approach, solver-specific white-box testing solutions are used in the development process of the DLV solver (Calimeri et al. 2009). In the context of inductive logic programming for data mining, a DDMin-based white-box trace-based delta debugger was developed (Tronc\u0327on and Janssens 2006).\nAs a final note, we want to stress that this work develops debugging techniques for answer set solvers, with the aim of developing and providing automated techniques for developing correct solvers. While this work focuses on solver testing and debugging, we note that, when considering applications of ASP, another possible source of errors is the modeling phase in which errors may be introduced by either on a conceptual level or through bugs in software which generate answer set programs encoding instances of the application domain. Incorrect modeling can result in answer set programs the answer sets of which do not precisely capture the set of solutions to the original problem instance. Various solutions have been recently proposed for debugging answer set programs (Brain and de Vos 2005; Syrja\u0308nen 2006; Brain et al. 2007; Gebser et al. 2008) where the aim is to find explanations on why a set of program rules does not describe a correct set of answer sets."}, {"heading": "8 Conclusions", "text": "We developed novel fuzz testing and delta debugging techniques for answer set solver development. The tools provide black-box solutions for more rigorous testing of a wide range of answer set solvers. ASP applications heavily depend on the robustness and correctness of answer set solvers. However, our experimental analysis clearly showed that our fuzz testing tool is able to reveal a variety of different critical defects such as segmentation faults, aborts, infinite loops, incorrect results and invalid answer sets in various state-of-the-art answer set solvers. Moreover, we showed that our delta debugging techniques are very effective in shrinking failureinducing inputs, which enables efficient debugging of answer set solvers.\nAs an extension of this work, we are particularly interested in testing and debugging solutions for the non-ground case. As many of the current state-of-the-art solvers heavily depend on the robustness and correctness of grounders, we find this an interesting and important aspect of future work."}], "references": [{"title": "The nomore++ approach to answer set solving", "author": ["C. Anger", "M. Gebser", "T. Linke", "A. Neumann", "T. Schaub"], "venue": "Proceedings of the 12th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning (LPAR 2005), G. Sutcliffe and A. Voronkov, Eds. Lecture Notes in Computer Science, vol. 3835. Springer, 95\u2013109.", "citeRegEx": "Anger et al\\.,? 2005", "shortCiteRegEx": "Anger et al\\.", "year": 2005}, {"title": "CVC3", "author": ["C. Barrett", "C. Tinelli"], "venue": "Proceedings of the 19th International Conference on Computer Aided Verification (CAV 2007), W. Damm and H. Hermanns, Eds. Lecture Notes in Computer Science, vol. 4590. Springer, 298\u2013302.", "citeRegEx": "Barrett and Tinelli,? 2007", "shortCiteRegEx": "Barrett and Tinelli", "year": 2007}, {"title": "PicoSAT essentials", "author": ["A. Biere"], "venue": "Journal on Satisfiability, Boolean Modeling and Computation 4, 75\u201397.", "citeRegEx": "Biere,? 2008", "shortCiteRegEx": "Biere", "year": 2008}, {"title": "Debugging ASP programs by means of ASP", "author": ["B. Brain", "M. Gebser", "J. P\u00fchrer", "T. Schaub", "H. Tompits", "S. Woltran"], "venue": "Proceedings of the 9th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2007), C. Baral, G. Brewka, and J. S. Schlipf, Eds. Lecture Notes in Computer Science, vol. 4483. Springer, 31\u201343.", "citeRegEx": "Brain et al\\.,? 2007", "shortCiteRegEx": "Brain et al\\.", "year": 2007}, {"title": "Debugging logic programs under the answer set semantics", "author": ["M. Brain", "M. de Vos"], "venue": "Proceedings of the 3rd Workshop on Answer Set Programming: Advances in Theory and Implementation (ASP 2005).", "citeRegEx": "Brain and Vos,? 2005", "shortCiteRegEx": "Brain and Vos", "year": 2005}, {"title": "The significance of memory costs in answer set solver implementation", "author": ["M. Brain", "M. De Vos"], "venue": "Journal of Logic and Computation 19, 4, 615\u2013641.", "citeRegEx": "Brain and Vos,? 2009", "shortCiteRegEx": "Brain and Vos", "year": 2009}, {"title": "Fuzzing and delta-debugging SMT solvers", "author": ["R. Brummayer", "A. Biere"], "venue": "Proceedings of the 7th International Workshop on Satisfiability Modulo Theories (SMT 2009). ACM International Conference Proceedings Series, vol. 375. ACM, 1\u20135.", "citeRegEx": "Brummayer and Biere,? 2009", "shortCiteRegEx": "Brummayer and Biere", "year": 2009}, {"title": "A visual tracer for DLV", "author": ["F. Calimeri", "N. Leone", "F. Ricca", "P. Veltri"], "venue": "Proceedings of the 2nd International Workshop on Software Engineering for Answer Set Programming (SEA 2009), M. De Vos and T. Schaub, Eds. 79\u201393.", "citeRegEx": "Calimeri et al\\.,? 2009", "shortCiteRegEx": "Calimeri et al\\.", "year": 2009}, {"title": "QuickCheck: a lightweight tool for random testing of Haskell programs", "author": ["K. Claessen", "J. Hughes"], "venue": "Proceedings of the 5th ACM SIGPLAN International Conference on Functional Programming (ICFP 2000). SIGPLAN Notices 35(9). ACM, 268\u2013279.", "citeRegEx": "Claessen and Hughes,? 2000", "shortCiteRegEx": "Claessen and Hughes", "year": 2000}, {"title": "Z3: An efficient SMT solver", "author": ["L.M. de Moura", "N. Bj\u00f8rner"], "venue": "In Proceedings of the 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS", "citeRegEx": "Moura and Bj\u00f8rner,? \\Q2008\\E", "shortCiteRegEx": "Moura and Bj\u00f8rner", "year": 2008}, {"title": "The second answer set programming competition", "author": ["M. Denecker", "J. Vennekens", "S. Bond", "M. Gebser", "M. Truszczynski"], "venue": "Proceedings of the 10th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2009), E. Erdem, F. Lin, and T. Schaub, Eds. Lecture Notes in Computer Science, vol. 5753. Springer, 637\u2013654. See also competition results at", "citeRegEx": "Denecker et al\\.,? 2009", "shortCiteRegEx": "Denecker et al\\.", "year": 2009}, {"title": "Conflict-driven disjunctive answer set solving", "author": ["M. Gebser", "T. Grote", "B. Kaufmann", "A. K\u00f6nig", "M. Ostrowski", "T. Schaub"], "venue": "In Proceedings of the 11th International Conference on Principles of Knowledge Representation and Reasoning (KR", "citeRegEx": "Drescher et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drescher et al\\.", "year": 2008}, {"title": "The Yices SMT solver", "author": ["B. Dutertre", "L. de Moura"], "venue": "http://yices.csl.sri.com/tool-paper.pdf.", "citeRegEx": "Dutertre and Moura,? 2006", "shortCiteRegEx": "Dutertre and Moura", "year": 2006}, {"title": "An extensible SAT-solver", "author": ["N. E\u00e9n", "N. S\u00f6rensson"], "venue": "6th International Conference on Theory and Applications of Satisfiability Testing (SAT 2003), Selected Revised Papers, E. Giunchiglia and A. Tacchella, Eds. Lecture Notes in Computer Science, vol. 2919. Springer, 502\u2013518.", "citeRegEx": "E\u00e9n and S\u00f6rensson,? 2004", "shortCiteRegEx": "E\u00e9n and S\u00f6rensson", "year": 2004}, {"title": "Conflict-driven answer set solving", "author": ["M. Gebser", "B. Kaufmann", "A. Neumann", "T. Schaub"], "venue": "Proceedings of the 20th International Joint Conference on Articifial Intelligence (IJCAI 2007), M. M. Veloso, Ed. Morgan Kaufmann, 286\u2013392.", "citeRegEx": "Gebser et al\\.,? 2007", "shortCiteRegEx": "Gebser et al\\.", "year": 2007}, {"title": "The first answer set programming system competition", "author": ["M. Gebser", "L. Liu", "G. Namasivayam", "A. Neumann", "T. Schaub", "M. Truszczynski"], "venue": "Proceedings of the 9th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2007), C. Baral, G. Brewka, and J. S. Schlipf, Eds. Lecture Notes in Computer Science, vol. 4483. Springer, 3\u201317. See also competition results", "citeRegEx": "Gebser et al\\.,? 2007", "shortCiteRegEx": "Gebser et al\\.", "year": 2007}, {"title": "A meta-programming technique for debugging answer-set programs", "author": ["M. Gebser", "J. P\u00fchrer", "T. Schaub", "H. Tompits"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI 2008), D. Fox and C. P. Gomes, Eds. AAAI Press, 448\u2013453.", "citeRegEx": "Gebser et al\\.,? 2008", "shortCiteRegEx": "Gebser et al\\.", "year": 2008}, {"title": "The stable model semantics for logic programming", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "Proceedings of the 5th International Conference and Symposium on Logic Programming (ICLP/SLP 1988), R. A. Kowalski and K. A. Bowen, Eds. MIT Press, 1070\u20131080.", "citeRegEx": "Gelfond and Lifschitz,? 1988", "shortCiteRegEx": "Gelfond and Lifschitz", "year": 1988}, {"title": "Answer set programming based on propositional satisfiability", "author": ["E. Giunchiglia", "Y. Lierler", "M. Maratea"], "venue": "Journal of Automated Reasoning 36, 4, 345\u2013377.", "citeRegEx": "Giunchiglia et al\\.,? 2006", "shortCiteRegEx": "Giunchiglia et al\\.", "year": 2006}, {"title": "Some (in)translatability results for normal logic programs and propositional theories", "author": ["T. Janhunen"], "venue": "Journal of Applied Non-Classical Logics 16, 1\u20131, 35\u201386.", "citeRegEx": "Janhunen,? 2006", "shortCiteRegEx": "Janhunen", "year": 2006}, {"title": "GnT \u2013 a solver for disjunctive logic programs", "author": ["T. Janhunen", "I. Niemel\u00e4"], "venue": "Proceedings of the 7th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2004), V. Lifschitz and I. Niemel\u00e4, Eds. Lecture Notes in Computer Science, vol. 2923. Springer, 331\u2013335.", "citeRegEx": "Janhunen and Niemel\u00e4,? 2004", "shortCiteRegEx": "Janhunen and Niemel\u00e4", "year": 2004}, {"title": "Computing stable models via reductions to difference logic", "author": ["T. Janhunen", "I. Niemel\u00e4", "M. Sevalnev"], "venue": "10th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2009), E. Erdem, F. Lin, and T. Schaub, Eds. Lecture Notes in Computer Science, vol. 5753. Springer, 142\u2013154.", "citeRegEx": "Janhunen et al\\.,? 2009", "shortCiteRegEx": "Janhunen et al\\.", "year": 2009}, {"title": "The DLV system for knowledge representation and reasoning", "author": ["N. Leone", "G. Pfeifer", "W. Faber", "T. Eiter", "G. Gottlob", "S. Perri", "F. Scarcello"], "venue": "ACM Transactions on Computational Logic 7, 3, 499\u2013562.", "citeRegEx": "Leone et al\\.,? 2006", "shortCiteRegEx": "Leone et al\\.", "year": 2006}, {"title": "ASSAT: Computing answer sets of a logic program by SAT solvers", "author": ["F. Lin", "Y. Zhao"], "venue": "Artificial Intelligence 157, 1\u20132, 115\u2013137.", "citeRegEx": "Lin and Zhao,? 2004", "shortCiteRegEx": "Lin and Zhao", "year": 2004}, {"title": "Pbmodels - software to compute stable models by pseudoboolean solvers", "author": ["L. Liu", "M. Truszczynski"], "venue": "Proceedings of the 8th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2005), C. Baral, G. Greco, N. Leone, and G. Terracina, Eds. Lecture Notes in Computer Science, vol. 3662. Springer, 410\u2013415.", "citeRegEx": "Liu and Truszczynski,? 2005", "shortCiteRegEx": "Liu and Truszczynski", "year": 2005}, {"title": "HDD: hierarchical delta debugging", "author": ["G. Misherghi", "Z. Su"], "venue": "Proceedings of the 28th International Conference on Software Engineering (ICSE 2006), L. J. Osterweil, H. D. Rombach, and M. L. Soffa, Eds. ACM, 142\u2013151.", "citeRegEx": "Misherghi and Su,? 2006", "shortCiteRegEx": "Misherghi and Su", "year": 2006}, {"title": "Simple random logic programs", "author": ["G. Namasivayam", "M. Truszczy\u0144ski"], "venue": "Proceedings of the 10th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2009), E. Erdem, F. Lin, and T. Schaub, Eds. Lecture Notes in Computer Science, vol. 5753. Springer, 223\u2013235.", "citeRegEx": "Namasivayam and Truszczy\u0144ski,? 2009", "shortCiteRegEx": "Namasivayam and Truszczy\u0144ski", "year": 2009}, {"title": "Logic programs with stable model semantics as a constraint programming paradigm", "author": ["I. Niemel\u00e4"], "venue": "Annals of Mathematics and Artificial Intelligence 25, 3\u20134, 241\u2013273.", "citeRegEx": "Niemel\u00e4,? 1999", "shortCiteRegEx": "Niemel\u00e4", "year": 1999}, {"title": "Extending and implementing the stable model semantics", "author": ["P. Simons", "I. Niemel\u00e4", "T. Soininen"], "venue": "Artificial Intelligence 138, 1\u20132, 181\u2013234.", "citeRegEx": "Simons et al\\.,? 2002", "shortCiteRegEx": "Simons et al\\.", "year": 2002}, {"title": "Fuzzing - Brute Force Vulnerability Discovery", "author": ["M. Sutton", "A. Greene", "P. Amini"], "venue": "Pearson Education.", "citeRegEx": "Sutton et al\\.,? 2007", "shortCiteRegEx": "Sutton et al\\.", "year": 2007}, {"title": "Debugging inconsistent answer set programs", "author": ["T. Syrj\u00e4nen"], "venue": "Proceedings of the 11th International Workshop on Nonmonotonic Reasoning (NMR 2006), J. Dix and A. Hunter, Eds. IfI Technical Report Series, vol. IfI-06-04. TU Clausthal, 77\u201383.", "citeRegEx": "Syrj\u00e4nen,? 2006", "shortCiteRegEx": "Syrj\u00e4nen", "year": 2006}, {"title": "Fuzzing for Software Security Testing and Quality Assurance", "author": ["A. Takanen", "J. Demott", "C. Miller"], "venue": null, "citeRegEx": "Takanen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Takanen et al\\.", "year": 2008}, {"title": "A delta debugger for ILP query execution", "author": ["R. Tron\u00e7on", "G. Janssens"], "venue": "Proceedings of the 16th Workshop on Logic-Based Methods in Programming Environments (WLPE 2006).", "citeRegEx": "Tron\u00e7on and Janssens,? 2006", "shortCiteRegEx": "Tron\u00e7on and Janssens", "year": 2006}, {"title": "On the complexity of derivation in propositional calculus", "author": ["G.S. Tseitin"], "venue": "Automation of Reasoning 2: Classical Papers on Computational Logic 1967\u20131970, J. Siekmann and G. Wrightson, Eds. Springer, 466\u2013483.", "citeRegEx": "Tseitin,? 1983", "shortCiteRegEx": "Tseitin", "year": 1983}, {"title": "Answer set programming with clause learning", "author": ["J. Ward", "J.S. Schlipf"], "venue": "Proceedings of the 7th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR 2004), V. Lifschitz and I. Niemel\u00e4, Eds. Lecture Notes in Computer Science, vol. 2923. Springer, 302\u2013313.", "citeRegEx": "Ward and Schlipf,? 2004", "shortCiteRegEx": "Ward and Schlipf", "year": 2004}, {"title": "Why Programs Fail", "author": ["A. Zeller"], "venue": "A Guide to Systematic Debugging. Morgan Kaufmann.", "citeRegEx": "Zeller,? 2005", "shortCiteRegEx": "Zeller", "year": 2005}, {"title": "Simplifying and isolating failure-inducing input", "author": ["A. Zeller", "R. Hildebrandt"], "venue": "IEEE Transactions on Software Engineering 28, 2, 183\u2013200.", "citeRegEx": "Zeller and Hildebrandt,? 2002", "shortCiteRegEx": "Zeller and Hildebrandt", "year": 2002}, {"title": "Answer set programming phase transition: A study on randomly generated programs", "author": ["Y. Zhao", "F. Lin"], "venue": "Proceedings of the 19th International Conference on Logic Programming (ICLP 2003), C. Palamidessi, Ed. Lecture Notes in Computer Science, vol. 2916. Springer, 239\u2013253.", "citeRegEx": "Zhao and Lin,? 2003", "shortCiteRegEx": "Zhao and Lin", "year": 2003}], "referenceMentions": [{"referenceID": 17, "context": "Answer set programming (ASP) (Gelfond and Lifschitz 1988; Niemel\u00e4 1999) is a rule-based declarative programming paradigm that has proven to be an effective approach to knowledge representation and reasoning in various hard combinatorial problem domains.", "startOffset": 29, "endOffset": 71}, {"referenceID": 27, "context": "Answer set programming (ASP) (Gelfond and Lifschitz 1988; Niemel\u00e4 1999) is a rule-based declarative programming paradigm that has proven to be an effective approach to knowledge representation and reasoning in various hard combinatorial problem domains.", "startOffset": 29, "endOffset": 71}, {"referenceID": 28, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 34, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 23, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 20, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 24, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 0, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 22, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 18, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 19, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 14, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 11, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 21, "context": "This success has been enabled by the development of efficient answer set solvers (Simons et al. 2002; Ward and Schlipf 2004; Lin and Zhao 2004; Janhunen and Niemel\u00e4 2004; Liu and Truszczynski 2005; Anger et al. 2005; Leone et al. 2006; Giunchiglia et al. 2006; Janhunen 2006; Gebser et al. 2007; Drescher et al. 2008; Janhunen et al. 2009; Brain and De Vos 2009).", "startOffset": 81, "endOffset": 362}, {"referenceID": 14, "context": "In support of these claims, by examining the detailed results of the first and second ASP programming competitions (Gebser et al. 2007; Denecker et al. 2009) one notices that, on the sets of (typical) benchmarks used in these competition, only very few solvers on very few benchmarks were judged as providing incorrect results.", "startOffset": 115, "endOffset": 157}, {"referenceID": 10, "context": "In support of these claims, by examining the detailed results of the first and second ASP programming competitions (Gebser et al. 2007; Denecker et al. 2009) one notices that, on the sets of (typical) benchmarks used in these competition, only very few solvers on very few benchmarks were judged as providing incorrect results.", "startOffset": 115, "endOffset": 157}, {"referenceID": 29, "context": "Fuzz testing, also called fuzzing (Sutton et al. 2007; Takanen et al. 2008), has its origin in software security and quality assurance.", "startOffset": 34, "endOffset": 75}, {"referenceID": 31, "context": "Fuzz testing, also called fuzzing (Sutton et al. 2007; Takanen et al. 2008), has its origin in software security and quality assurance.", "startOffset": 34, "endOffset": 75}, {"referenceID": 36, "context": "In order to isolate the failure-inducing parts of such failure-inducing inputs, an automatic technique called delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) has been proposed.", "startOffset": 126, "endOffset": 217}, {"referenceID": 35, "context": "In order to isolate the failure-inducing parts of such failure-inducing inputs, an automatic technique called delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) has been proposed.", "startOffset": 126, "endOffset": 217}, {"referenceID": 8, "context": "In order to isolate the failure-inducing parts of such failure-inducing inputs, an automatic technique called delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) has been proposed.", "startOffset": 126, "endOffset": 217}, {"referenceID": 25, "context": "In order to isolate the failure-inducing parts of such failure-inducing inputs, an automatic technique called delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) has been proposed.", "startOffset": 126, "endOffset": 217}, {"referenceID": 25, "context": "Our algorithm uses the hierarchical (Misherghi and Su 2006) structure of answer set programs to shrink failure-inducing instances effectively.", "startOffset": 36, "endOffset": 59}, {"referenceID": 36, "context": "Furthermore, we evaluate two different elimination strategies we have implemented for our delta debugging algorithm: a simple and easy-to-implement \u201cone-by-one\u201d elimination strategy, and another one based on the more intricate DDMin algorithm originally proposed in different context (Zeller and Hildebrandt 2002).", "startOffset": 284, "endOffset": 313}, {"referenceID": 17, "context": "In ASP, we are interested in stable models (Gelfond and Lifschitz 1988) (or answer sets) of a program \u03a0.", "startOffset": 43, "endOffset": 71}, {"referenceID": 37, "context": "are only a few studies that consider the problem of generating random logic programs in the context of ASP (Zhao and Lin 2003; Namasivayam and Truszczy\u0144ski 2009).", "startOffset": 107, "endOffset": 161}, {"referenceID": 26, "context": "are only a few studies that consider the problem of generating random logic programs in the context of ASP (Zhao and Lin 2003; Namasivayam and Truszczy\u0144ski 2009).", "startOffset": 107, "endOffset": 161}, {"referenceID": 33, "context": "We obtained CNF instances by generating random propositional formulas as Boolean circuits and translating them to CNF via a standard encoding (Tseitin 1983).", "startOffset": 142, "endOffset": 156}, {"referenceID": 14, "context": "Using default settings, we tested a wide selection of answer set solvers that participated in the first (Gebser et al. 2007) or second (Denecker et al.", "startOffset": 104, "endOffset": 124}, {"referenceID": 10, "context": "2007) or second (Denecker et al. 2009) ASP Competition in 2007/2009.", "startOffset": 16, "endOffset": 38}, {"referenceID": 2, "context": "12 using Picosat 913 SAT solver (Biere 2008) as backend, noMoRe++ 1.", "startOffset": 32, "endOffset": 44}, {"referenceID": 13, "context": "12b SAT solver (E\u00e9n and S\u00f6rensson 2004).", "startOffset": 15, "endOffset": 39}, {"referenceID": 1, "context": "solvers, CVC3 (Barrett and Tinelli 2007) and Yices (Dutertre and de Moura 2006) were used as back-end solvers.", "startOffset": 14, "endOffset": 40}, {"referenceID": 36, "context": "The overall goal of delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) is to minimize the size of failure-inducing inputs while maintaining the same observable behavior.", "startOffset": 36, "endOffset": 127}, {"referenceID": 35, "context": "The overall goal of delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) is to minimize the size of failure-inducing inputs while maintaining the same observable behavior.", "startOffset": 36, "endOffset": 127}, {"referenceID": 8, "context": "The overall goal of delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) is to minimize the size of failure-inducing inputs while maintaining the same observable behavior.", "startOffset": 36, "endOffset": 127}, {"referenceID": 25, "context": "The overall goal of delta debugging (Zeller and Hildebrandt 2002; Zeller 2005; Claessen and Hughes 2000; Misherghi and Su 2006) is to minimize the size of failure-inducing inputs while maintaining the same observable behavior.", "startOffset": 36, "endOffset": 127}, {"referenceID": 25, "context": "DeltaASP can be seen as a variation of hierarchical delta debugging (Misherghi and Su 2006), since our method proceeds from the top-most elements of the hierarchy (rules) to lower-level elements: first rules, then individual heads and bodies of rules, then individual literals, and, at last, negations.", "startOffset": 68, "endOffset": 91}, {"referenceID": 36, "context": "Next, we discuss and evaluate the differences between a simple one-by-one approach (OBO) and a more intricate strategy based on the DDMin algorithm (Zeller and Hildebrandt 2002).", "startOffset": 148, "endOffset": 177}, {"referenceID": 36, "context": "DDMin The original DDMin algorithm (Zeller and Hildebrandt 2002) attempts to divide the current set into k subsets, where k (the granularity) is initialized to 2.", "startOffset": 35, "endOffset": 64}, {"referenceID": 6, "context": "With this technique we delta debugged incorrect results as already proposed in (Brummayer and Biere 2009), but with exactly one trusted solver.", "startOffset": 79, "endOffset": 105}, {"referenceID": 6, "context": "The most closely related work is the fuzz testing and delta debugging approach developed for SMT (satisfiability modulo theories) solvers in (Brummayer and Biere 2009).", "startOffset": 141, "endOffset": 167}, {"referenceID": 7, "context": "In contrast to our generic black-box approach, solver-specific white-box testing solutions are used in the development process of the DLV solver (Calimeri et al. 2009).", "startOffset": 145, "endOffset": 167}, {"referenceID": 32, "context": "In the context of inductive logic programming for data mining, a DDMin-based white-box trace-based delta debugger was developed (Tron\u00e7on and Janssens 2006).", "startOffset": 128, "endOffset": 155}, {"referenceID": 30, "context": "Various solutions have been recently proposed for debugging answer set programs (Brain and de Vos 2005; Syrj\u00e4nen 2006; Brain et al. 2007; Gebser et al. 2008) where the aim is to find explanations on why a set of program rules does not describe a correct set of answer sets.", "startOffset": 80, "endOffset": 157}, {"referenceID": 3, "context": "Various solutions have been recently proposed for debugging answer set programs (Brain and de Vos 2005; Syrj\u00e4nen 2006; Brain et al. 2007; Gebser et al. 2008) where the aim is to find explanations on why a set of program rules does not describe a correct set of answer sets.", "startOffset": 80, "endOffset": 157}, {"referenceID": 16, "context": "Various solutions have been recently proposed for debugging answer set programs (Brain and de Vos 2005; Syrj\u00e4nen 2006; Brain et al. 2007; Gebser et al. 2008) where the aim is to find explanations on why a set of program rules does not describe a correct set of answer sets.", "startOffset": 80, "endOffset": 157}], "year": 2010, "abstractText": "This paper develops automated testing and debugging techniques for answer set solver development. We describe a flexible grammar-based black-box ASP fuzz testing tool which is able to reveal various defects such as unsound and incomplete behavior, i.e. invalid answer sets and inability to find existing solutions, in state-of-the-art answer set solver implementations. Moreover, we develop delta debugging techniques for shrinking failureinducing inputs on which solvers exhibit defective behavior. In particular, we develop a delta debugging algorithm in the context of answer set solving, and evaluate two different elimination strategies for the algorithm.", "creator": "LaTeX with hyperref package"}}}