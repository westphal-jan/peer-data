{"id": "1101.2301", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2011", "title": "A Factorial Experiment on Scalability of Search Based Software Testing", "abstract": "digital software testing processing is particularly an undoubtedly expensive process, which this is vital to in deciding the industry. construction of the test - data format in software personality testing requires the major cost and needs to decide which method to use in order to precisely generate meaning the test data is always important. this section paper successfully discusses the efficiency dynamics of binary search - based sequential algorithms ( unlike preferably genetic algorithm ) versus sophisticated random memory testing, differing in its soft - run ware test - data cache generation. this textbook study differs from all previous studies due to sample programs ( suts ) systems which are popularly used. further since we want to independently in - crease the complexity of capturing suts somewhat gradually, production and the program generation is largely automatic as was well, binary grammatical error evolution is used partially to guide the program generation. suts are generated uniformly according systematically to the random grammar rules we provide, always with different optimal levels amount of complexity. suts therefore will only first undergo routine genetic al - gorithm and then random testing. based on weighing the test results, this separate paper also recommends such one method to use simulation for automation of software testing.", "histories": [["v1", "Wed, 12 Jan 2011 09:28:22 GMT  (376kb,D)", "http://arxiv.org/abs/1101.2301v1", "3d Artificial Intelligence Techniques in Software Engineering Workshop, 7 October, 2010, Larnaca, Cyprus"]], "COMMENTS": "3d Artificial Intelligence Techniques in Software Engineering Workshop, 7 October, 2010, Larnaca, Cyprus", "reviews": [], "SUBJECTS": "cs.SE cs.AI", "authors": ["arash mehrmand", "robert feldt"], "accepted": false, "id": "1101.2301"}, "pdf": {"name": "1101.2301.pdf", "metadata": {"source": "CRF", "title": "A Factorial Experiment on Scalability of Search Based Software Testing", "authors": ["Arash Mehrmand", "Robert Feldt"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Automated Software Testing, Search-based Software Testing, Genetic Algorithms, Random Testing, Grammatical Evolution"}, {"heading": "1 Introduction", "text": "Software testing is one of the most critical and at the same time most time and cost consuming activity in software development process [1]. In order to cut this costs as much as possible we need to employ effective techniques to automate this process. What actually needs to be automated is generating the test data which is a demanding process [2]. Automation of generating test data helps to have fast, cheap and error free process [2].\nIf we can optimize the test cases generation process, then we save a lot of time and money. Among optimization techniques, search based optimization is the one which has been applied to different areas of Software Engineering applications [2].\nSoftware testing was not an exception in Software Engineering area and the term \u201cSearch Based Software Testing (SBST)\u201c proves it. The recent years show a dramatic rise of interest in Search Based Testing (SBT) [2]. SBT has been applied to different testing areas such as structural, functional, non-functional, mutation, regression and so on [2].\nThe term \u201dSearch Based Testing\u201d in general means using some sort of search alogorithms to generate test cases for testing a software.\nar X\niv :1\n10 1.\n23 01\nv1 [\ncs .S\nE ]\n1 2\nJa n\n20 11\n2 There are many Search Based techniques and each of them uses a different algorithm to optimize the search and the results.\nThe three popular ones which are in use for testing purpose are Neighborhood Search, Simulated annealing and Genetic algorithms (GA). And of course there are simple techniques as well such as random testing which does not use any optimization and search technique. [3]. According to [3] flexibility and strength of these heuristic optimization techniques help a lot in finding the optimal solutions in large and complex search spaces.\nThis study focuses on using the SBT techniques or as we said heuristic optimization techniques for software testing, when we have different programs regarding to complexity. We will apply the technique which is called Genetic algorithm to wide variety of programs to see the effectiveness of Genetic algorithm compared to random testing. The final results of this paper could help Software engineers and especially Software testers to compare the efficiency of search based techniques versus random testing when it comes to get the maximum percentage of coverage on each code, as they deal with programs with different levels of complexity.\nThe structure of this paper is as follows: Next section, presents the existing research in this area. Section 3 explains the experimental approach of this study and study design. Section 4 talks about the design of experiment in this study. Sections 5 and 6 include the results and discussion about them. And finally conclusion and further work will come under section 7."}, {"heading": "2 Previous Research and Related Work", "text": "There exist many studies in software testing automation area and SBST. Some of them deal with software complexity as well but we could have not found any of them similar to this study as we deal with increasing of software complexity along our study. For example, authors in [4] discuss the experiments with test case generation with large and complex programs and they conclude that, there is a wide gap between the techniques based on GA and those based on random testing. They have chosen some programs which are written in C language and used them as SUT.\nCompared to what the authors tried to explain in [4], our study has a major difference and that is the use of many Java programs that vary in size and complexity.\nMany other studies tried to experiment software testing using genetic algorithms and compare the results with other techniques. Authors in [5] performed experiments on some small and a few complex test objects and they came up with this results: Particle Swarm Optimization overcomes GA in coverage of many code elements so not always GA is the best solution. Another paper also talks about GA based software testing and the main idea is to find problematic situations while testing programs with GA [6].\nAlba and Chicano in their paper [7] describe how canonical genetic algorithms and evolutionary strategies can help in software testing. They used a benchmark\n3 of twelve test programs and applied both Evolutionary Strategies and GA and compared the results at the end.\nHowever, many existing research and studies (e.g. [8],[9],[10]) have used SBT specially GA in order to test the software and compare the results with other techniques."}, {"heading": "3 Experiment Approach", "text": "The chosen approach for this study is factorial experiment which compares GA and random testing in efficiency. Factorial experiment is an experiment whose design consists of two or more factors and each factor has discrete possible values which we call them levels [1]. Factors, mentioned in table 1, are the test search techniques which has GA and random as values, complexity which has low, medium and high as levels , statement coverage and branch coverage with their defined levels. Since we have to increase the complexity of our generated programs gradually, there are some factors which we have to increase or decrease, in order to change the complexity the programs. Code complexity is actually an independent variable in our experiment. Statement and branch are independent of each other but dependent to the complexity level; the more complexity level we have, the more branch or statement we deal with. Below is a table which shows the factorial design of this experiment.\nFor both statement and branch coverage, we have source codes with low, medium and high complexity. For each coverage method, we test our codes with both GA and random testing and record the coverage percentages as you can see in plots under section 5.\nThese factors (variables) are explained in details in section 4 which is Experiment Design. After making all the experiments, we will have two tables and six graphs which show the efficiency of GA compared to random testing; these results will come in section 6 which is Experiment Results."}, {"heading": "4 Experiment Design", "text": "In order to design a fully automated process for our experiment, we need to follow a step by step structure. These steps are the following subsections:\n4"}, {"heading": "4.1 Generate Software Under Test (SUT) Using Grammatical Evolution (GE)", "text": "Grammatical Evolution (GE) is a kind of Genetic Programming based on grammar. GE combines basis and rules of molecular biology with representational power of formal grammars. GE has a unique flexibility due to its rich modularity, and makes it possible to use alternative search strategies, whether evolutionary, or other heuristic, be it stochastic or deterministic, and to radically change its behavior by only slight changes to the grammar supplied. One of the main advantages of GE, is the easiness of modifying the output structures by simply editing the plain text grammar. This advantage actually originates from the use of grammar to describe the structures that are generated by GE [12].\nTo use GE, for our SUTs generation, we used an open source software implementation of grammatical evolution in Java. Grammatical Evolution in Java (GEVA) is the name of the tool which provides the possibility to use GE with our supplied grammar, in order to generate evolved programs. However, for us, to use GEVA, there was a need to make some modifications and supply our own material to reach our goal which is generating Java programs with different levels of complexity.\nFirst of all we need to define our problem, which is generating different Java programs. For this purpose, we wrote a Java code which automatically generates the header and footer of the programs and makes the contents of the body of the program, according to the BNF which we will provide. So next step is to write a BNF. Using Backus-Naur form (BNF), it is possible to provide the specification of a programming language structure, which is Java in this case. Since we have to have a subset of grammars, we should define our BNF according to what subsets of Java programming language we need to generate. The genetic configuration of GEVA is also changed according to our need. Following is a part of our properties, used in GEVA:\n\u2013 Population size: 200 \u2013 Generation: 10000 \u2013 Initial chromosome-size: 200 \u2013 Selection type: Roulette wheel [17].\nProviding GEVA, with the problem definition and BNF file, it can generate our desired programs. Because we want to increase the complexity of our programs, we need to run the generation, three different times. When the fitness function is set to the number of statements, we run the generation, once with 75 statements, once with 150 and finally with 300 statements. And when the fitness function is set to the number of branches in the code, we run the generation once with 25 branches, once with 50 branches and finally with 100 branches. However, the complexity levels could be extended to more than three levels, e.g. very low, low, medium, high, very high or we can have the same three levels with different numbers; the results and the effectiveness of GA over random search will not change though. So three levels which are used in this experiment are sufficient enough for us to conclude which method is more efficient.\n5 It is worth mentioning that our sample programs are just regular codes written in Java and they involve different kinds of conditions and statements. By this we mean we did not follow any specific problem to generate the sample programs."}, {"heading": "4.2 GA", "text": "After generating the SUTs, we need to apply GA to our code so it will act genetically during the testing process. The library which is used for this purpose, in this study, is called JGAP. JGAP is actually a Genetic Algorithm and Genetic Programming component which is provided as Java framework. Using JGAP it is possible to apply evolutionary principles to problem solutions since it provides us, the basic genetic mechanisms [13].\nUsing JGAP, first step is to plan the chromosomes, so we have to decide on type and quantity of genes that we are going to use. Each chromosome is made of genes, and in this study each gene is actually a test data. And depending on the size and number of input variables, we decide on number of genes. Second step is to implement fitness function. Although JGAP is designed to do almost all the evolutionary steps in different problems but it does not know, by itself, whether or not a potential solution is better than the other ones. Our fitness function, depending on which part of experiment we are, is the coverage percentage of the generated test data. For example if our complexity metric is the number of statements, then the best chromosomes are those which provide better statement coverage on the code. Third step is to setup the configuration object of JGAP which means deciding on how big our population is, and all other GA configurations. In this experiment the population size is set as 200 and number of generations is set to 10000. Fourth and fifth steps are creating and evolving the population. And obviously after each generation, the best test data of each population are taken. However, the followings are the steps, GA basically, uses to test a software where P (t) is the population and t is the generation [17]:\nInitialize P(t)\nEvaluate P(t) While the termination condition is not reached, do\nselect P(t+1) from P(t) recombine P(t+1) evaluate P(t+1) t=t+1\nEvaluating the fitness function depends on which experiment we are running; so for statement coverage and branch coverage it differs. Please refer to 3.1.3, where defining the fitness of the experiment is explained in more details. This was the whole strategy of GA application to our SUTs.\nDefining The Fitness Function What genetic algorithm does is actually searching for the suitable test case, to kill a given mutant; for this purpose\n6 fitness function is used, which assigns a non-negative cost to each candidate input value. The more cost each input value has, the more appropriate it is, so we aim to maximize this value and therefore maximize the fitness function [21].\nWe used the term which is called function minimization [17]. Function minimization helps to find the desired input for each condition to be executed. Using this way, each condition in the code has its own function. Genetic search, then, tries to find the input which minimizes the value of that specific function. For example, imagine we have this condition on line 300 of the program:\nif (x >= z+10)\nand we aim to execute the true condition of this branch. Then the, the minimization function is defined as below:\nf(n) = { (z300 + 10) \u2212 x300 ifZ300 + 10 \u2264 x300 0 otherwise\nThe above function says, if z300 + 10 is less than or equal to x300 then try to set the value of z300 and x300 so that the value of (z300 + 10) \u2212 x300 is as minimum as possible. Please notice that 300 here is the line number. We have to define the line number since the value of input parameters (x and z), change over time, as the programs run.\nWhen the fitness function is statement coverage, we need to count the number of statements which are blocked between braces of correspondent branch. We use it as the weight and divide the value of [the correspondent] function with this weight. For branch coverage, we do not need this weight since we are looking for the covered branches not the statements.\nBecause our sample programs execute repeatedly over time, the input parameters change every time the program executes. The minimization function decides, how closed the input parameters are to the desired values which satisfy the conditions and we use GA in order to minimize the function; the fittest chromosomes mean the closer results to minimize the function."}, {"heading": "4.3 Random Testing", "text": "Random testing is the chosen approach in this study, to be compared with SBST. According to [16] random testing is actually the use of randomly generated of test data which has some advantages such as easy and simple implementation and speed of execution, which means less run time. When running the test using random testing, we have to define a parameter; this parameter defines how many times we should generate test data using random testing. For example if we set the parameter to 100, then it generates 100 set of test cases. Out of these [e.g.] 100, we take the best and compare it with the fittest result of GA. However, in this experiment, this number is set to 100000.\nUsing random testing we need to define our input domain, for test data, so the numbers will be randomly generated form this domain. In this study this set starts from \u22121000000 to +1000000.\n7"}, {"heading": "5 Experiment Results", "text": "After testing the generated SUTs, we came up with the results. As you can see in the following plots, GA outperforms the random testing in almost, all the experiments which we have done. As mentioned before, the whole experiment includes testing 60 programs; 30 (10 with low, 10 with medium, and 10 with high complexity) with statement coverage purpose and 30 (10 with low, 10 with medium, and 10 with high complexity) with branch coverage purpose. So the fitness function was different for each purpose. The generated programs are also different since they are generated with two different fitness function. The sample programs for statement coverage purpose, are generated with number of statements purpose and the ones which are used for branch coverage purpose, are generated with number of branches purpose. Please not that the following plots include 10 programs each, which are randomly chosen from multiple runs that we had.\n8\nAs mentioned above, these plots represent the result of 10 chosen programs, but to see how efficient GA is, please refer to the tables II and III, where they show the average percentage of coverage with both GA and random search.\nIn table II, for statement coverage, from low to high, t-test shows that means are not significantly different from the Selected Confidence Level. Although we might not be able to prove GA outperforms random search statistically because of the actual confidence level here, but still the overall mean of coverage in GA is more than random search in all cases.\nNote: desired confidence level is set to 90 And in Table III, for branch coverage, from low to high, t-test shows that\nmeans are significantly different from the Selected Confidence Level. Note: desired confidence level is set to 90"}, {"heading": "6 Discussion", "text": "the results from the plots in section 6, illustrate GA has outperformed random testing in most cases and in very few cases they have same coverage percentage.\n9\nThe design of this experiment is factorial so that we can assure in different situations, with different complexity levels and different source codes, random testing can never outperform GA from the coverage and efficiency point of view.\nHowever, there exist some issues which need to be discussed here, some of them could be serious problems though, if they are not handled properly.\nOur SUTs here are generated using GE. As mentioned in previous sections, for GE to generate programs, we have to provide it with a BNF. Unfortunately it is impossible to define semantics (logic) in BNF file which means there is a very low chance of controlling what you are generating. During these experiments, we had many programs which had same statements, or same loops or even same blocks due to GA behavior of generating fittest chromosome.\nBig numbers, which are made throughout the process of multiplication, addition and subtraction, are out of range of integer and double variable. These out of range variables can cause the code not to be tested completely because when the test reaches these numbers, stops running. We did not define divisions in our BNF since it can cause many problems e.g. division by zero.\nAnother important issue is the infinite loops in the code. And , we controlled them using exit-loop which means having maximum iteration and break it after the value of maximum iterations have been reached. However, having breaks, even if we assign maximum iteration to a large number, causes the code not to get tested completely [14,15].\nConditions in the loops are very important during testing, because if we can satisfy the condition and go through the block of that condition, we can execute more statements. There are two kind of conditions which can cause problems. First one is the condition which is not reachable at all and second kind is a very easy condition which both GA and random testing can cover it easily.\nThis problem is solved in our BNF which means we have the minimum number of easy and unreachable conditions so we do not face serious problems during our experiment.\nUsing GE and BNF to generate programs has some difficulties and limitations which make it very hard to generate a piece of code without infinite loops, hard but possible to cover conditions. And testing the programs which are generated using GE has its own limitations since we have limited the number of iterations of existing loops, and eliminate the nested loops because of exit-loop problem.\n10\nThe authors in [18] do not recommend using nested loops; instead they suggest another way which is called LoopN. In LoopN the use of nested loops is forbidden. They think loop does not do much harm if it is not nested. However limiting the code, and not having the nested loops can decrease the overall complexity, which was not our aim but there was no other way than eliminating them.\nRather than having the problem with BNF, instrumentation itself, is a big issue. This is mainly because, the more details we need to instrument, the less speed we have. For example in case of nested loops; instrumenting nested loops needs us to keep track of each loop (inner and outer), each branch, and each condition more precisely."}, {"heading": "7 Conclusion", "text": "In this paper, we have reported on results from a factorial experiment, where the effectiveness of GA is compared to random testing in automation of software testing. In this experiment, the process of generating SUTs were also automated, which means we used GE to generate our sample Java programs. To our knowledge, the presented results we had are not yet reported in the software test data generation area, because of having automatically generated programs, different levels of complexity and different kinds of coverage at the same time. However, it is worth mentioning, this study focuses on the efficiency of the techniques and does not consider the other attributes such as time, cost etc. The followings are our observations from the experiments and the taken results.\n\u2013 GA can outperform random testing and it did actually in this study with our automatically generated SUTs. We proved it using a factorial experiment. This factorial experiment with different levels of code complexity and two methods of code coverage, shows that in most of the situations GA has better efficiency than random testing. \u2013 Random testing could be, however, a faster solution but it is not recommended since in real life we look for a software with less bugs, not a software which is tested quickly. This means more bugs that we find in our programs, more cost we save; this is what happens in real world. \u2013 Number of statements, is not a good metric for software complexity because adding extra statement hardly makes the code more complex. What makes the code really complex, is the conditions.\nThis study can be extended by applying well known software complexities i.e. cyclomatic complexity."}], "references": [{"title": "A Novel Approach To Automated Testing To Increase Software Reliability", "author": ["M. Catelani", "L. Ciani", "V. Scarano", "A. Bacioccola"], "venue": "Instrumentation and Measurement Technology Conference Proceedings 1499\u20131502", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Automated Test Data Generation using Search Based Software Engineering", "author": ["Harman", "Mark"], "venue": "AST \u201907: Proceedings of the Second International Workshop on Automation of Software Test, 2", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "A Search-Based Automated Test-Data Generation Framework for Safety-Critical Software", "author": ["Nigel James Tracy"], "venue": "University of York", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Genetic algorithms for dynamic test data generation", "author": ["Christoph C. Michael", "Gary E. Mcgraw", "Michael A", "Curtis C. Walton"], "venue": "In Proc. ASE97, 307\u2013308", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Applying particle swarm soptimization to software testing", "author": ["Windisch", "Andreas", "Wappler", "Stefan", "Wegener", "Joachim"], "venue": "GECCO \u201907: Proceedings of the 9th annual conference on Genetic and evolutionary computation, 1121\u20131128", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Genetic algorithm based software testing", "author": ["Alander", "Jarmo T.", "Mantere", "Timo", "Turunen", "Pekka."], "venue": "Turku Centre for Computer Science, Springer-Verlag", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Observations in using parallel and sequential evolutionary algorithms for automatic software testing", "author": ["Alba", "Enrique", "Chicano", "Francisco."], "venue": "Comput. Oper. Res., 3161\u20133183", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Search-based test case generation for object-oriented java software using strongly-typed genetic programming", "author": ["Ribeiro", "Jos\u00e9 Carlos Bregieiro"], "venue": "GECCO \u201908: Proceedings of the 2008 GECCO conference companion on Genetic and evolutionary computation, 1819\u20131822", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "The Automatic Generation of Software Test Data Using Genetic Algorithms", "author": ["Harmen-Hinrich Sthamer"], "venue": "University of Glamorgan", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Effective Black-Box Testing with Genetic Algorithms", "author": ["Mark Last", "Shay Eyal", "Abraham Kandel"], "venue": "Hardware and Software, Verification and Testing, 134\u2013148", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Selection in factorial experiments", "author": ["Bechhofer", "Robert E."], "venue": "WSC \u201977: Proceedings of the 9th conference on Winter simulation 65\u201370", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1977}, {"title": "GEVA: grammatical evolution in Java", "author": ["O\u2019Neill", "Michael", "Hemberg", "Erik", "Gilligan", "Conor", "Bartley", "Eliott", "McDermott", "James", "Brabazon", "Anthony"], "venue": "SIGEVOlution,ACM, 17\u201322", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "A BNF-based automatic test program generator for compatible microprocessor verification ACM Trans", "author": ["Wu", "Lieh-Ming", "Wang", "Kuochen", "Chiu", "Chuang-Yi"], "venue": "Des. Autom. Electron. Syst., 105\u2013132", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Automatic test generation for functional verification of microprocessors", "author": ["J. BMiyake", "G. Brown"], "venue": "In Proceedings of the Third Asian Test Symposium , 292\u2013297", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Experimental assessment of random testing for object-oriented software", "author": ["Ciupa", "Ilinca", "Leitner", "Andreas", "Oriol", "Manuel", "Meyer", "Bertrand."], "venue": "ISSTA \u201907: Proceedings of the 2007 international symposium on Software testing and analysis, 84\u201394", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Generating Software Test Data by Evolution", "author": ["Gary Mcgraw", "Christoph Michael", "Michael Schatz"], "venue": "IEEE Transactions on Software Engineering, 1085\u20131110", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Genetic Programming with simple loops", "author": ["Yuesheng Qi", "Baozhong Wang", "Lishan Kang."], "venue": "Journal of Computer Science and Technology,429\u2013434", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Designing and Prototyping a Functor Language Using Denotational Semantics", "author": ["Wolfgang Stcher"], "venue": "RISC Report Series, University of Linz, Austria,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Towards a simpler method of operational semantics for language definition", "author": ["M.D. Derk"], "venue": "SIGPLAN Not., ACM, 39\u201344", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Genetic Algorithm Fitness Function for Mutation Testing", "author": ["Bottaci", "Leonardo."], "venue": "Department of Computer Science, The University of Hull, Hull HU6 7RX, U.K.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Software testing is one of the most critical and at the same time most time and cost consuming activity in software development process [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "What actually needs to be automated is generating the test data which is a demanding process [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "Automation of generating test data helps to have fast, cheap and error free process [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Among optimization techniques, search based optimization is the one which has been applied to different areas of Software Engineering applications [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "The recent years show a dramatic rise of interest in Search Based Testing (SBT) [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "SBT has been applied to different testing areas such as structural, functional, non-functional, mutation, regression and so on [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "According to [3] flexibility and strength of these heuristic optimization techniques help a lot in finding the optimal solutions in large and complex search spaces.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "For example, authors in [4] discuss the experiments with test case generation with large and complex programs and they conclude that, there is a wide gap between the techniques based on GA and those based on random testing.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "Compared to what the authors tried to explain in [4], our study has a major difference and that is the use of many Java programs that vary in size and complexity.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "Authors in [5] performed experiments on some small and a few complex test objects and they came up with this results: Particle Swarm Optimization overcomes GA in coverage of many code elements so not always GA is the best solution.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "Another paper also talks about GA based software testing and the main idea is to find problematic situations while testing programs with GA [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 6, "context": "Alba and Chicano in their paper [7] describe how canonical genetic algorithms and evolutionary strategies can help in software testing.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "[8],[9],[10]) have used SBT specially GA in order to test the software and compare the results with other techniques.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8],[9],[10]) have used SBT specially GA in order to test the software and compare the results with other techniques.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "[8],[9],[10]) have used SBT specially GA in order to test the software and compare the results with other techniques.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "Factorial experiment is an experiment whose design consists of two or more factors and each factor has discrete possible values which we call them levels [1].", "startOffset": 154, "endOffset": 157}, {"referenceID": 11, "context": "This advantage actually originates from the use of grammar to describe the structures that are generated by GE [12].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "\u2013 Population size: 200 \u2013 Generation: 10000 \u2013 Initial chromosome-size: 200 \u2013 Selection type: Roulette wheel [17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "However, the followings are the steps, GA basically, uses to test a software where P (t) is the population and t is the generation [17]:", "startOffset": 131, "endOffset": 135}, {"referenceID": 19, "context": "The more cost each input value has, the more appropriate it is, so we aim to maximize this value and therefore maximize the fitness function [21].", "startOffset": 141, "endOffset": 145}, {"referenceID": 15, "context": "We used the term which is called function minimization [17].", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "According to [16] random testing is actually the use of randomly generated of test data which has some advantages such as easy and simple implementation and speed of execution, which means less run time.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "However, having breaks, even if we assign maximum iteration to a large number, causes the code not to get tested completely [14,15].", "startOffset": 124, "endOffset": 131}, {"referenceID": 13, "context": "However, having breaks, even if we assign maximum iteration to a large number, causes the code not to get tested completely [14,15].", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "The authors in [18] do not recommend using nested loops; instead they suggest another way which is called LoopN.", "startOffset": 15, "endOffset": 19}], "year": 2016, "abstractText": "Software testing is an expensive process, which is vital in the industry. Construction of the test-data in software testing requires the major cost and to decide which method to use in order to generate the test data is important. This paper discusses the efficiency of searchbased algorithms (preferably genetic algorithm) versus random testing, in software test-data generation. This study differs from all previous studies due to sample programs ot software under test (SUT) which are used. Since we want to increase the complexity of SUTs gradually, and the program generation is automatic as well, Grammatical Evolution is used to guide the program generation. SUTs are generated according to the grammar we provide, with different levels of complexity. SUTs will first undergo genetic algorithm and then random testing. Based on the test results, this paper recommends one method to use for automation of software testing.", "creator": "LaTeX with hyperref package"}}}