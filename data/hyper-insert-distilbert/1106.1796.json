{"id": "1106.1796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "Accelerating Reinforcement Learning by Composing Solutions of Automatically Identified Subtasks", "abstract": "this paper discusses developing a system technique that accelerates reinforcement reinforcement learning performance by using transfer from similarly related scientific tasks. without creating such transfer, even if two tasks are relatively very similar task at some abstract level, an extensive conscious re - learning human effort now is required. the modeled system achieves little much of sustaining its power by physically transferring conflicting parts of previously delayed learned solutions rather than a single partial complete feedback solution. the system exploits fewer strong features in the multi - 3 dimensional function value produced subsequently by reinforcement induction learning in solving a potentially particular possible task. with these features are stable and easy to recognize early procedures in the learning measurement process. they generate a partitioning of the state space function and are thus the input function. the function partition is represented below as on a graph. this relation is used to index and compose biological functions stored in a case base to form thereby a simple close approximation to the solution of the initial new task. laboratory experiments later demonstrate weaknesses that function composition computation often wrongly produces conclusions more than either an alpha order instant of magnitude increase in learning rate variance compared to a basic reinforcement learning algorithm.", "histories": [["v1", "Thu, 9 Jun 2011 13:11:20 GMT  (290kb)", "http://arxiv.org/abs/1106.1796v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["c drummond"], "accepted": false, "id": "1106.1796"}, "pdf": {"name": "1106.1796.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Chris Drummond"], "emails": ["cdrummon@site.uottawa.ca"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 16 (2002) 59-104 Submitted 5/01; published 2/02 Accelerating Reinforcement Learning by ComposingSolutions of Automatically Identi ed SubtasksChris Drummond cdrummon@site.uottawa.caSchool of Information Technology and EngineeringUniversity of Ottawa, Ontario, Canada, K1N 6N5AbstractThis paper discusses a system that accelerates reinforcement learning by using transferfrom related tasks. Without such transfer, even if two tasks are very similar at someabstract level, an extensive re-learning e ort is required. The system achieves much of itspower by transferring parts of previously learned solutions rather than a single completesolution. The system exploits strong features in the multi-dimensional function producedby reinforcement learning in solving a particular task. These features are stable and easyto recognize early in the learning process. They generate a partitioning of the state spaceand thus the function. The partition is represented as a graph. This is used to index andcompose functions stored in a case base to form a close approximation to the solution ofthe new task. Experiments demonstrate that function composition often produces morethan an order of magnitude increase in learning rate compared to a basic reinforcementlearning algorithm.1. IntroductionA standard reinforcement learning algorithm, applied to a series of related tasks, could learneach new task independently. It only requires knowledge of its present state and infrequentnumerical rewards to learn the actions necessary to bring a system to some desired goalstate. But this very paucity of knowledge results in a slow learning rate. This paper showshow to exploit the results of prior learning to speed up the process while maintaining therobustness of the general learning method.The system proposed here achieves much of its power by transferring parts of previouslylearned solutions, rather than a single complete solution. The solution pieces representknowledge about how to solve certain subtasks. We might call them macro-actions (Precup,Sutton, & Singh, 1997), with the obvious allusion to macro-operators commonly found inArti cial Intelligence systems. The main contribution of this work is in providing a way ofautomatically identifying these macro-actions and mapping them to new tasks.This work uses syntactic methods of compositionmuch like in symbolic planning, but thenovelty arises in that the parts being composed are multi-dimensional real-valued functions.These functions are learned using reinforcement learning as part of more complex functionsassociated with compound tasks. The e cacy of this approach is due to the compositionoccurring at a su ciently abstract level, where much of the uncertainty has been removed.Each function acts much like a funnel operator (Christiansen, 1992), so although individualactions may be highly uncertain, the overall result is largely predictable.c 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nDrummondThe subtasks are identi ed on the basis of strong features in the multi-dimensionalfunction that arise during reinforcement learning. The features are not \\in the world\",but in the system's interaction with the world. Here, \\strong\" means that the features arestable (i.e. relatively insensitive to variations in the low level learning process) and easyto recognize and locate accurately early in the learning process. One important aspect ofthese features is that they largely dictate the shape of the function. If the features di er bya small amount, one would expect the function to di er by a small amount.The features generate a partitioning of the function. A popular technique in objectrecognition, the snake (Kass, Witkin, & Terzopoulus, 1987; Suetens, Fua, & Hanson, 1992),is used to produce this partition. In object recognition, the snake produces a closed curvethat lies along the boundary of an object, as de ned by edges in an image. In this appli-cation, the snake groups together sets of features to de ne a region of the function. Theboundary of the region is a low order polygon, demarcating an individual subtask. Thisis repeated until the whole function is covered. The polygons are converted into discretegraphs, a vertex of the polygon becoming a node of the graph. Merging these graphsproduces a composite graph representing the whole task.The composite graph is used to control the transfer by accessing a case base of previouslylearned functions. The case base is indexed by graphs. The relevant function is determinedby matching a subgraph of the composite graph with one acting as an index to a case. Theassociated functions are transformed and composed to form a solution to the new task. Thisis used to reinitialize the lower level learning process. It is not necessary for the transfer toproduce an exact solution for the new task. It is su cient that the solution is close enoughto the nal solution often enough to produce an average speed up. Reinforcement learningwill further re ne the function and quickly remove any error.This paper demonstrates the applicability of transfer in two di erent situations. In the rst, the system learns a task for a particular goal position and then the goal is moved.Although the function itself will change signi cantly, the partition generated on the initialtask can be used to compose the function for the new task. In the second situation consid-ered, the system is placed in a di erent environment within the same domain. Here, a newpartition has to be extracted to control the composition process.This paper uni es and signi cantly extends previous work by the author (Drummond,1997, 1998). Additional work has largely focussed on removing some of the limitationsinherent in the partitioning approach introduced in Drummond (1998). One limitation ofthe original approach was that the snake could only extract polygons that were rectangles.This paper relaxes this restriction, allowing it to be applied to a di erent environmentwithin the same domain and to a di erent task domain. Although lifting this restrictionremoves some desirable bias, the experiments demonstrate that none of the e cacy of theoriginal system is lost. Further, the results are more broadly obtained on the larger set ofrelated tasks and in a di erent domain. Overall, the function composition approach oftenproduces more than an order of magnitude increase in learning rate when compared to abasic reinforcement learning algorithm.The rest of the paper begins with Section 2 giving a very high level discussion of theapproach taken. Section 3 gives a more in depth discussion of the techniques used. Sec-tions 4 and 5 present and analyze the experimental results. Subsequent sections deal withlimitations and related research. 60\nAccelerating Reinforcement Learning2. An OverviewThe intent of this section is to appeal to the intuitions of the reader, leaving much of thedetail to later sections in the paper. The subsections that follow will demonstrate in turn:that there are features in the function produced by reinforcement learning; that graphsbased on these features can be used to control the composition of the function pieces; thatthese features are easy to detect early in the learning process; that these features exist inmultiple domains.2.1 Features in the Reinforcement Learning FunctionThis overview begins with a very high level introduction to reinforcement learning and thefunction it produces. It will show that there are features in this function which can beextracted and converted into a graphical representation.One of the experimental test beds used is this paper is a simulated robot environment ofdi erent con gurations of interconnected rooms. The robot must learn to navigate e cientlythrough these rooms to reach a speci ed goal from any start location. Figure 1 shows oneexample with 5 rooms and the goal in the top right corner. The robot's actions are smallsteps in any of eight directions, as indicated by the arrows. Here, the location, or state,is simply the robot's x and y coordinates. The thin lines of Figure 1 are the walls of therooms, the thick lines the boundary of the state space. Robot\n14\n11 13\n11 13 10 12 12\nGoal\nY\n-1\n+1\nX -1 +1Figure 1: Robot Navigating Through a Series of Rooms61\nDrummondIf each action is independent of preceding actions, the task becomes one of learningthe best action in any state. The best overall action would be one that takes the robotimmediately to the goal. But this is only possible in states close to the goal. Supposethe robot is in a particular state and that the number of steps to goal from each of itsneighboring states is known, indicated by the numbered squares surrounding the robot inFigure 1. Then a one step look ahead procedure would consider each step and select theone that reaches the neighboring state with the shortest distance to goal. In Figure 1 therobot would move to the state 10 steps from goal. If this process is repeated, the robot willtake the shortest path to goal. In practice we must, of course, learn such values. This canbe done using some type of reinforcement learning (Watkins & Dayan, 1992; Sutton, 1990)which progressively improves estimates of the distance to goal from each state until theyconverge to the correct values. O\nI\n(0.25,1.0)(-1.0,1.0)\n(0.25,0.25)(-1.0,0.25)\n(-0.9,0.25)\n(0.25,0.9)\nFigure 2: The Value Function Obtained Using Reinforcement LearningThe function shown in Figure 2 is called the value function. Subsequently, the termfunction will mean the value function unless otherwise indicated. The function is the resultof reinforcement learning on the problem of Figure 1, but instead of it representing theactual distance to goal, it represents essentially an exponential decay with distance to goal.The reasons for this will be made clear in Section 3.1. The shaded areas represent largegradients in the learned function. Comparing this to the environment shown in Figure 1, itis apparent that these correspond to the walls of the various rooms. These are the strongfeatures discussed in this paper. They exist because of the extra distance for the robotto travel around the wall to reach the inside of the next room on the path to the goal.These features are visually readily apparent to a human, so it seems natural to use visionprocessing techniques to locate them.An edge detection technique called a snake is used to locate these features. The snakeproduces a polygon, in this instance a rectangle, locating the boundary of each room. Thedoorways to the room occur where the di erential of the function, along the body of thesnake, is at a local minimum. The direction of the di erential with respect to edges of62\nAccelerating Reinforcement Learningthe polygon, associated with the walls of the room, determines if it is an entrance or anexit. A positive gradient into the room indicates an entrance; a positive gradient out ofthe room indicates an exit. From this information, a plane graph, labeled with an (x; y)coordinate for each node, is constructed. Figure 2 shows one such example, for the roomat the top left corner of the state space, subsequent graphs will not show the coordinates.Nodes corresponding to the doorways are labeled \\I\" or \\O\" for in and out respectively;their positions on the function are indicated by the dashed arrows.2.2 Composing Function PiecesThis overview continues by showing how the graphs, extracted from the features in thefunction learned by reinforcement learning, can be used to produce a good approximationto the solution for a new goal position. The left hand side of Figure 3 shows plane graphsfor all the rooms (ignore the dashed lines and circles for now). The node representing thegoal is labeled \\G\". A directed edge is added from \\I\" to \\O\" or \\I\" to \\G\", as appropriate.Associated with this edge is a number representing the distance between the nodes. This isdetermined from the value of the function at the points of the doorways. Each individualgraph is then merged with its neighbor to produce a graph for the whole problem, the righthand side of Figure 3. The doorway nodes have been relabeled to \\D\". The compositegraph represents the whole function. Each individual subgraph represents a particular partof the function. This information is stored in a case base. Each subgraph is an index andthe corresponding part of the function is the case. I O\nI\nI G\nO\nD\nD\nD\nD\nGG\nO\nI\nO\nExtract Graphs Merge GraphsG\nFigure 3: Graphical RepresentationNow suppose the goal is moved from the top right corner to the top left corner of thestate space. Reinforcement learning in its most basic form would be required to learn thenew function from scratch. In this work if the goal is moved, once the new goal position63\nDrummondis known, the node representing the goal can be relocated. The new goal position is shownas the dashed circle in Figure 3. The edges connecting the doorways and the goal arechanged to account for the new goal position. The dashed lines representing the new edgesreplace the arrows in the same subgraph. To produce a new function, the idea is to regressbackwards from the goal along these edges. For each edge, the small subgraph containingthe edge is extracted. The extracted subgraph is used to index the case base of functions.The retrieved function is transformed and added to the appropriate region of the state spaceto form the new function.\nGI\nI\nRotate\nStretch\nRotate\nStretch\nFigure 4: Function CompositionIn this example, some of the existing subgraphs match the new con guration. The twothat do not are the subgraph originally containing the goal and the subgraph now containingthe goal. It is certainly possible to exchange these two, using an appropriate transform.But other graphs in the case base may better match the new task. The best match forthe subgraph containing the new goal is, in fact, the subgraph for the goal in the originalproblem. To t this to the new task, the plane graph is rotated and stretched slightly inthe new x direction by changing the coordinates of its nodes, see Figure 4. Then this sametransformation is applied to the function. But for the room containing the original goal, acase obtained when solving another task is a better match. The other three rooms use thefunctions from the original problem, since changing the goal position has little e ect on theactions taken. In fact, only the height of the functions must be changed. This is simplya multiplication by a value representing the distance to goal from the \\O\" doorway (thisis discussed in detail at the end of Section 3.3). Because the matching of the subgraphsallows some error and asymmetric scaling may be used, the resulting function may not beexact. But as the experiments will demonstrate, the function is often very close and furtherreinforcement learning will quickly correct any error.64\nAccelerating Reinforcement LearningThe new position of the goal must be established before the graph can be modi ed andfunction composition can occur. The system is not told that the goal has moved, rather itdiscovers this by determining that it is no longer at the maximum of the existing function.There is some uncertainty in the exact boundary of the original goal. The robot may reacha state which it believes is part of the original goal region, but fail to detect it even ifthe goal has not moved. To be reasonably certain that the goal has in fact moved, this isrequired to occur ten times with no intervening occurrence of the goal being detected atthe maximum.The system then composes a search function, by assuming a particular room containsthe goal. Search functions are also produced by composing previously learned functions.However, for the room assumed to contain the goal the function is a constant. This doesnot bias the search to any particular part of the room and allows some limited learning toencourage exploration of the room. The search function drives the robot into the room fromanywhere else in the state space. If it fails to nd the goal after a xed number of steps,a new search function is composed with another room assumed to contain the goal. Thisprocess is repeated until the goal has been located ten times, this ensures a good estimateof the \\center of mass\" of the goal. The \\center of mass\" is used as the new position ofthe goal node in the composite graph. Requiring that the old goal or new goal positionsare sampled a xed number of times has proven to be e ective in the domains discussed inthis paper. Nevertheless, it is a somewhat ad hoc procedure and will be addressed in futurework, discussed in Section 6.2.2.3 Detecting Features EarlyIn the previous section, the existing task and the new task were strongly related, the wallsand doorways were xed and only the goal position was di erent. In this section, no suchrelationship is assumed. The robot is faced with a brand new task and must determinewhat, if any, relationship exists between the new task and any previous tasks.The experimental testbed is again a simulated robot environment, but this time theproblem is simpli ed to just an inner rectangular room and an outer L-shaped room. Figures5 and 6 show two possible room con gurations. Again, the thin lines are the walls of theroom, the thick lines the boundary of the state space. Suppose the robot had alreadylearned a function for the \\Old Task\" of Figure 5. We would hope that we could adapt theold solution to t the closely related \\New task\" of Figure 6.The steps, in this example, are essentially those in the previous one. But now as thelearning process is started afresh, there are no features and the system must wait until theyemerge through the normal reinforcement learning process. Then we can proceed much asbefore. First a graph for the inner room is extracted. The best matching graph in thecase base from the old task is rotated and stretched to t the new task. Next a matchinggraph for the outer L-shaped room is rotated and stretched around the larger inner room.The same transforms are then applied to the associated functions, any height adjustmentscarried out and the functions composed to form an approximate solution to the new task.In this example, the rst step in the process is to locate the goal. As there is nopartition to aid the search, the initial value function is set to a mid-range constant value(see Figure 7). This allows some limited learning which encourages the system to move65\nFigure 6: The New Taskaway from regions it has explored previously, to prevent a completely random walk throughstate space. Once the goal is located, the learning algorithm is reinitialized with a functionfor the same goal position but no walls (see Figure 8). If such a function does not exist inthe case base, any rough approximation could be used instead. The \\no walls\" function isnot used exactly as stored in the case base. The di erence between the goal and the rest ofthe state space is reduced by scaling the function then adding a constant. This reduces the\\bias\" of the function, allowing the learning algorithm to alter it relatively easily as newinformation becomes available.\nFigure 8: Intermediate FunctionFigure 9 shows the resultant function about 3000 exploratory steps from the beginningof the learning process. Again, the large gradients associated with the walls are readily66\nAccelerating Reinforcement Learningapparent. Figure 10 shows the function for the new task if it had been allowed to convergeto a good solution. Both functions have roughly the same form, the large gradients arein the same position, although learning the latter took some 200,000 steps. After the \\nowalls\" function is introduced the features take some time to clearly emerge. The snake willtypically lter out features that are too small and not well formed. Additional ltering atthe graphical level further constrains acceptable features. The total set of features mustproduce a consistent composite graph, the doorways from di erent subgraphs must alignand the graph must overlay the complete state space. There must also be a matching casein the case base for every subtask. Many of these checks and balances will be removed whenthe iterative updating technique of Section 6.2 is incorporated.\nFigure 10: New Task Function2.4 A Di erent Task DomainThe previous sections dealt with a simple robot navigation problem. This section demon-strates that these features also exist in a quite di erent domain, that of a two degrees offreedom robot arm, as shown in Figure 11. The shoulder joint can achieve any angle be-tween radians, the elbow joint any angle between =2 radians, zero is indicated by thearrows. If the arm is straight and the shoulder joint rotated, the elbow joint will describethe inner dotted circle, the hand the outer dotted circle. There are eight actions, smallrotations either clockwise or anti-clockwise for each joint separately or together. The aimis to learn to move the arm e ciently from any initial position until the hand reaches thegoal on the perimeter of the arm's work space.The state space, for the purposes of reinforcement learning, is the con guration spacefor the arm, sometimes called the joint space (see Figure 12). The x-axis is the angle of theshoulder joint, the y-axis the elbow joint. The eight actions when mapped to actions in thecon guration space become much like the actions in the robot navigation problem, as shownby the shaded diamond (labeled Arm) in Figure 12. To map an obstacle in the work spaceto the con guration space, one must nd all pairs of shoulder and elbow angles blocked bythe obstacle. The obstacles in this space become elongated to form barriers much like the67\nDrummond\nA L\nO\nG\n0\n0\nHand\nObstacle\nObstacle\nShoulder\nElbow\nL\nO A\nG\n+\u03c0/2\n\u2212\u03c0/2 \u2212\u03c0 +\u03c00\n0\nShoulder Angle\nObstacle\nArm\nE lb\now A\nng le\nObstacle\nFigure 12: Con guration Spacewalls in the experiments of the previous sections. If this is not clear, imagine straighteningthe arm in the work space and rotating it such that it intersects one of the obstacles, themiddle dotted line in Figure 11. The arm can then be rotated at the shoulder joint with aroughly linearly proportional rotation in the elbow joint, but in the opposite direction, suchas to keep it intersecting the obstacle. This produces the \\wall\" in the con guration space.This linearity holds only for small objects not too far from the perimeter of the work space.More complex, larger objects, would result in more complex shapes in the con gurationspace. At the moment the feature extraction method is limited to these simpler shapes,this will be discussed further in Section 6.The reinforcement learning function produced by this problem is shown in Figure 13. Asbefore the features are shaded for clarity. The large gradient associated with the obstacleon the left hand side of the con guration space can be clearly seen. There is a similar largegradient associated with the obstacle on the right hand side of the con guration space.Again, these features can be used to control the composition of functions if the goal ismoved or for a di erent task in the same domain.3. Details of the Techniques UsedThis section will discuss in more detail the techniques used. These include: reinforcementlearning to produce the initial function, snakes to extract the features producing the graph,and the transformation and composition of the subgraphs, and their corresponding func-tions, to t the new task.3.1 Reinforcement LearningReinforcement learning typically works by re ning its estimate of expected future reward.In goal-directed tasks, such as the ones investigated here, this is equivalent to progressively68\nAccelerating Reinforcement Learning\nFigure 13: The Robot Arm Functionimproving the estimate of the distance to goal from each state. This estimate is updatedby the best local action, i.e. the one moving the robot, or arm, to the new state with thesmallest estimated distance. Early in the learning process, only states close to the goal arelikely to have accurate estimates of true distance. Each time an action is taken, the estimateof distance at the new state is used to update the estimate at the old state. Eventually thisprocess will propagate back accurate estimates from the goal to all other states.Rather than directly estimating the distance to goal, the system uses the expecteddiscounted reward for each state E[P1t=1 trt]. The in uence of rewards, rt, are reducedprogressively the farther into the future they occur by using a less than one. In this work,the only reward is for reaching the goal. So the farther the state is from the goal the smallerthe value. The use of an expectation allows the actions to be stochastic, so when the robot,or arm, takes a particular action in a particular state, the next state is not always the same.To carry out reinforcement learning, this research uses the Q-learning algorithm (Watkins& Dayan, 1992). This algorithm assumes the world is a discrete Markov process, thus bothstates and actions are discrete. For each action a in each state s, Q-learning maintains arolling average of the immediate reward r plus the maximum value of any action a0 in thenext state s0 (see Equation 1). The action selected in each state is usually the one with thehighest score. But to encourage exploration of the state space, this paper uses an -greedypolicy (Sutton, 1996) which chooses a random action a fraction of the time. The onlye ect that function composition has on the Q-learning algorithm is that the initial valuefor each state-action pair is set to some value other than zero.Qt+1s;a = (1 )Qts;a + (r + maxa0Qts0;a0) (1)The Q-function over state and action is usually referred to as the action-value function.In this paper, it is the action-value function that is transformed and composed to form asolution to the new task. The value function, discussed in previous sections and shown in69\nDrummondthe gures, is the maximum value of the Q-function. It is used to generate the partitionand associated graphs needed to control the process.Watkins and Dayan (1992) proved that Q-learning will converge to the optimal valuewith certain constraints on the reward and the learning rate . The optimal solution is pro-duced by taking the action with the greatest value in any state. So, for goal-directed tasks,a greedy algorithm will take the shortest path to the goal, once learning is complete. Theextension to continuous spaces may be done using function approximation. The simplestmethod, and the one used here, is to divide the state dimensions into intervals. The result-ing action-value function has cells representing the average Q-value of taking each actionfrom somewhere within a region of the state space. In o -line learning, where any actionin any state can be executed, this representation has been proven to converge (Gordon,1995). In on-line learning, where the current state is determined by the environment, thisapproach is generally successful, but there exists no proof of its convergence.3.2 Feature ExtractionFeature extraction uses a vision processing technique that ts a deformable model calleda snake (Kass et al., 1987) to edges in an image. After initializing the snake, the processiterates until external forces, due to the edges, balance internal forces in the snake thatpromote a smooth shape. Here, the external forces are due to steep gradients in the valuefunction. As a piecewise constant function approximator is used, a smoothed cubic b-splineis tted to the value-function and used to generate the necessary derivatives. The left handside of Figure 14 is the gradient of the value function shown in Figure 9 when extractingfeatures early in the learning process. The system has added a gradient around the borderto represent the state space boundary.To locate the features, a curve is found that lies along the ridge of the hills, a localmaximum in the di erential. On the right hand side of Figure 14, the dashed lines arecontour lines for the small inner room as indicated. The bold lines, on the right hand sideof Figure 14, are the snake at di erent stages of the process. The snake is rst positionedapproximately in the center of the room, the innermost circle. It is then expanded untilit abuts on the base of the hills. Now to simplify the exposition, we can imagine that thesnake consists of a number of individual hill climbers spread out along the line representingthe snake, indicated by the small white circles. But instead of being allowed to climbindependently, their movement relative to each other is constrained to maintain a smoothshape. When the snake reaches the top of the ridge, it is further constrained to be polygon{ in this instance a quadrilateral { the outside dark line in Figure 14. At this point, itwill tend to oscillate around an equilibrium position. By limiting the step size the processcan be brought into a stationary state. A more detailed mathematical treatment of thisapproach is given in Appendix A.The polygon forms the \\skeleton\" for the graph, as shown at the top left of Figure 14.Nodes in a graph correspond to vertices of the polygon and to the doorways and the goal.Looking at the gradient plot, the doorways are regions with a small di erential betweenthe ridges. Their locations can be determined from the magnitude of the gradient alongthe boundary of the polygon. In this example, a node is added for the goal (labeled G)and this is connected to the \\in\" doorway (labeled I). The polygon delimits a region of70\nAccelerating Reinforcement Learning\nDoorway\nPolygon\nGraph G\nI\nFigure 14: The Gradient and Resultant Polygon (Left) Extracted by the Snake (Right)the state space, and therefore a region of the action-value function. This becomes a casein the case base, and the corresponding graph its index. Constraining the snake to be apolygon is done for two reasons. Firstly, the vertices are needed to produce nodes in theplane graphs, which are important part of the matching process. Secondly, the additionalconstraint results in a more accurate t to the boundaries of the subtask. This, in turn,results in a more accurate solution after function composition.3.2.1 Three Extensions to the Snake ApproachThis section introduces three extensions to the basic snake approach to facilitate the ex-traction of features.The rst extension a ects the direction the snake moves when hill climbing the gradient.In normal hill climbing, each step is taken in the direction of steepest ascent, the step sizebeing determined by the size of the di erential. Roughly, this translates into forces at pointsalong the body of the snake. Each force points in the direction of steepest ascent locally, butinteracts with other forces through the various shape constraints. Looking at the gradientfunction and contour lines of Figure 14, there is a steep slope leading to the top of eachridge. But there is also a signi cant slope along each ridge away from the doorway towardsthe boundary of the state space. Thus the force on a single point on the body of the snake71\nDrummondis not directly towards the top of the ridge but turned towards its apex, as indicated by thebold black arrow on the left hand side of Figure 15. Steepest Ascent Snake\nNormal\nTangent\nFigure 15: Controlling Forces on the SnakeThis force can be broken into two components with respect to the snake, a normal anda tangential force. The latter force acts along the body of the snake. Once the shape isconstrained to be a quadrilateral, this will cause the relevant side to shrink. This e ect willbe partially counteracted by the force towards the top of the ridge on the adjacent side ofthe quadrilateral. But the net result will be a shrinking of the two sides associated with theridges inwards until the forces are balanced. This will push the corner of the quadrilateralnear the doorway inwards, as indicated by the thin black arrow in Figure 15. In an extremecase, this might cause the snake to collapse into something close to a triangle. But the morelikely outcome will just be a degradation of the accuracy of registration of the ridges.Drummond (1998) prevented this degradation of the accuracy by restricting the snakesto rectangular shapes. But with the weakening of this constraint to more general polygons,this e ect again becomes a problem. This problem is addressed by removing the componentof the force tangential to the snake. Then hill climbing is always in the direction of thenormal. This does not signi cantly restrict the motion of the snake: all that is being removedis the component along the body of the snake. Thus it mainly prevents the stretching andshrinking of the snake due to the gradient.The second extension controls the way the snake is expanded to reach the base of thehills. Drummond (1998) used a ballooning force, as introduced by Cohen and Cohen (1993).But problems arose when extending the system to deal with more general shapes thanrectangles, such as the outer L-shaped room in Figure 6. The ballooning force expands thesnake in directions normal to its body. One deleterious e ect of this is if the snake contactsa sharp external corner, such as that of the inner room, the force tends to push the snakethrough the corner. This can be seen in Figure 16; the bold continuous lines are the snake;the bold dashed lines are the ridges. If we imagine starting o with a circular snake in the72\nAccelerating Reinforcement Learningmiddle of the L-shaped outer room, by the time it reaches the walls of the inner room thesides of the snake are roughly perpendicular to the ridges. Thus there is little to restrainthe expansion of the snake and it passes completely through the walls of the inner room.\nRidge\nBallooning\nRidge\nForce\nFigure 16: Using the Ballooning ForceThe approach adopted here is analogous to the ow of mercury. If we imagine startingsomewhere in the middle of the L-shaped room and progressively adding mercury, it wouldtend to ll up the lower regions of the valley rst and reach the bases of the hills roughly atthe same time. The analogy of mercury is used as it has a high surface tension preventingit from owing through small gaps in the edges associated with doorways. To increase thee ectiveness of this idea, the absolute value of the di erential of the gradient is thresholded,values above the threshold being set to one those below to zero. It is then smoothed witha truncated Gaussian, as shown in Figure 17. Smoothing and thresholding are commonlyused techniques in machine vision (Tanimoto, 1990). They are typically used to removenoise, but here the aim is to strongly blur the thresholded image. This produces bowlsassociated with each room. In this example, the smoothing has almost completely obscuredthe presence of the doorway, although this is generally not the case.The snake is initialized as a small circle at the minimum of one of these bowls. Thisis shown as the circle in the middle of Figure 18, where the dashed lines are the contourlines of this function. It then ows outwards, so as to follow the contour lines of the bowl;the largest component of the ow being in the direction of the arrows in Figure 18. Thisis achieved by varying the force normal to the body of the snake according to the heightdi erence with the average height of the snake. Thus points along the snake which arehigher than average tend to get pushed inwards, those lower pushed outwards. The surfacetension of the mercury is produced by various smoothing constraints on the rst and seconddi erentials of the snake (see Appendix A).The third extension limits changes in the shape of the snake as it expands from its initialposition to reach the base of the hills. The smoothness constraints on the snake, that givethe mercury-like properties, prevent the snake owing through the gaps associated with the73\nDrummond\nFigure 17: Smoothed Function Figure 18: Mercury Flowdoorways. But even this proved insu cient if the width of the rooms and the width ofdoorways were of similar sizes. In Figure 12, looking at the \\room\" on the left hand sideof the con guration space of the robot arm, the \\doorway\" and the \\room\" at the top areof similar width. Increasing the surface tension of the mercury su ciently to prevent owthrough the doorways also prevents the ow to the top of the room.The solution is to limit the amount the snake can change its shape as it grows. Thisis achieved by constraining how much the second di erential of the snake can change fromstep to step. In Figure 18, it is apparent that the snake takes up a good approximationto the shape of the room some time before it reaches the ridges. If the shape can belocked-in before reaching the ridges, the problem just described can be avoided. Whenthe snake is initialized, the only constraint is smoothness. As the snake is expanded, thissmoothness constraint is progressively weakened and the curvature constraint progressivelystrengthened. This progressively locks in the shape while still allowing the snake to makesmall local adjustments to better t the features.The extensions, discussed in this section, either modify the traditional forces that acton the snake or add new ones. There are also forces associated with knot spacing and drag.How the snake moves, with each iteration, depends on the vector addition of these forces.The sum acts to accelerate the body of the snake which has both mass and velocity, andtherefore momentum. A schematic representation of these forces is shown in Figure 19; amore detailed mathematical description is given in Appendix A. The dashed line representsthe body of the snake; the arrows are the forces applied to one point on the body. The snakeis a parameterized function, given by f\u0302(s) = (x(s); y(s)) where x(s) and y(s) are individualcubic b-splines giving the x and y coordinates associated with a variable s along the bodyof the snake. The circles represent points equi-distant in s but not necessarily in x and y.These points are kept roughly the same Euclidean distance apart in x and y due to the knotspacing force. The momentum, although not strictly a force, encourages the point to move74\nAccelerating Reinforcement Learningin constant direction; the drag opposes any motion. The sti ness encourages the snake tomaintain a smooth shape. The overall sti ness is reduced as the snake grows, to keep its exibility per unit length roughly constant, and is also controlled locally to maintain itsshape. Steepest Ascent\nMomentum\nStiffness\nMercuryFlow\nKnot Spacing\nDrag\nFigure 19: The Forces on the SnakeThe following is an algorithmic summary of the processing of the snake: Initialize the coe cients to produce a circular snake in the middle of a room. Iterate until the forces are roughly in equilibrium and the snake oscillates around astationary value. Modify the sti ness to enforce the polygonal constraints Iterate for a further 25 steps increasing the momentum and drag at each step to reducethe oscillation to a small value. Use the nal position of the snake to form the polygon that delimits the boundary ofthe room.3.3 TransformationThis section discusses the matching process { how a subgraph is used to locate and transforma function from the case base. The matching process rst nds all subgraphs in the case baseisomorphic to the extracted subgraph and all possible isomorphic mappings between theirnodes, using a labeling algorithm (MacDonald, 1992). The number of isomorphic mappings75\nDrummondis potentially exponential in the number of nodes. Here, the graphs typically have onlya few nodes and a few symmetries, so only a few isomorphic mappings. Associated witheach node of a subgraph is an (x; y) coordinate. An a ne transform, Equation 2, is foundthat minimizes the distances between the coordinates of the mapped nodes for each of theisomorphic subgraphs. The advantage of this transform is its relative exibility while havinga simple form. x0 = C0x+ C1y + C2 y0 = C3x+ C4y + C5 (2)Ideally the transformed nodes would be positioned exactly over the mapped nodes, butthis is not usually possible. Even with simple rectangular shapes, the case base may notcontain a graph with exactly the same doorway positions. Using a graph that is not anexact match will introduce some error in the composed function for the new task. Byweighting some nodes more than others where the error occurs can be controlled. One aimis to minimize the introduction of errors that a ect the overall path length. However, ofequal importance is that the errors introduced be easily correctable by normal reinforcementlearning.\n1\n1 42 2 1\n1Figure 20: Weighting Graph NodesThe left hand side of Figure 20 shows the composite graph for the new task. The righthand side shows the result of overlaying it with a graph from the case base. If the t at thedoorway of the outer L-shaped room is in error, the robot will tend to miss the doorwayand collide with the wall on one side. The farther the doorway is out of position, the longernormal reinforcement learning will take to correct the error. To encourage a good t atthe doorway, a weight of 4 is used. Nodes adjacent to the doorway are given a weight of 2,all other nodes have a weight of one. This is based on the intuition that more trajectories,from di erent parts of the state space, will be pass through the region close to the doorway.Any error here is likely to have a broader e ect, and take longer for normal reinforcement76\nAccelerating Reinforcement Learninglearning to correct, than in regions far from the doorway. So the t around the inner roomis improved by sacri cing t far from the doorway.The exact position of the doorway in the inner room is not critical and its weight isset to 0.5. Whatever the position of the doorway, the shape of the function will be correctinside the room as the goal is also in this room. However, the further the doorway is fromits correct position, the greater the error in the edge length. This will produce some errorin the composed function, but again the expectation is that this error will be small andreinforcement learning will quickly correct it.Not only should the t be good, but we would also prefer that the amount of trans-formation be small. All transforms produce some error and this is particularly true ofasymmetric scaling, as discussed later in this section. Generally the transform producestranslation, re ection, rotation, shearing and independent scaling in each dimension. Inthe robot navigation domain, the distance between points in the state space is just thenormal Euclidean distance. The reinforcement learning function is an exponential decayin the distance to goal. If the transformation does not change the Euclidean distance, thetransformed function should be directly applicable.Affine Similar Symmetric (3)The a ne transformation is just one family in a hierarchy of transformations. At thebottom of this hierarchy, shown in Equation 3, are the symmetric transformations. Thesesolid body transformations do not change the Euclidean distance. The next step up inthe hierarchy introduces scaling, equal in each dimension. This will a ect the Euclideandistance but only by a multiplicative factor. Thus the only change needed to the transformedfunction is to scale the height. The a ne transformations allow the addition of asymmetricscaling and shear, which will distort the Euclidean distance. To determine the amountof distortion, the transformation is applied to the unit circle. The symmetric, rigid body,transformations will not alter the circle, but the other transformations will. The symmetricscaling transform just changes the diameter of the circle. The asymmetric scaling and sheartransformations change the circle into the ellipse. The amount of distortion of the Euclideandistance introduced by the transform can be determined by the ratio of lengths of the majorand minor axes of the ellipse.error = sqrt(Piwi( x2i + y2i )) (node misalignment)+ log2 rmajrmin 2 (Euclidean Distortion)+ 0:05 log2 jrmaj+rminj2 2 (scaling factor) (4)The error of t of the transformed subgraph can be combined with the transformationerror using the lengths of the major and minor axes, rmaj and rmin respectively, of theellipse. There is a penalty for Euclidean Distortion from asymmetric scaling and shear.The log factor is added directly to the error of t as shown in Equation 4. Log factors areused, so that the penalty functions are symmetric. There is a small penalty for symmetricscaling. Once the best matching subgraph has been found, the same transformation can beapplied to the associated function. If no isomorphic graph is found with a total error lessthan 1.5, a constant function will be used as a default. Where the new graph overlays theold graph, values are assigned by using bilinear interpolation on the discrete values of the77\nDrummondfunction. Where it does not, bilinear extrapolation is used, based on the closest values. Inboth cases once the four values are selected, the value for the new point is calculated asshown in Equation 5. As the action-value function is indexed by action as well as state, thisprocess is carried out for each action in turn. Any rotation or re ection in the transformis also applied to a prede ned matrix of actions. This produces the necessary mapping ofactions from the original to the new action-value function.v = c1x+ c2y + c2xy + c3 (5)Finally, the height of the new action-value function must be adjusted to account for thechange in overall distance to goal. The height of the value function at an \\out\" doorway is dg where dg is the distance to goal and the discount factor. The value at some randompoint within the room is dg+dd where dd is the distance to the doorway. The action-valuefunction is rst normalized by dividing by dg, the height of the function at the doorwayin the original problem. It is then multiplied by dng, where dng is the distance to thenew goal; the value of the point becomes dng+dd. Scaling will also a ect the height of thefunction. Assuming the scaling is symmetric then the new value function for anywhere inthe room will be c dd where c is the scale factor. Thus raising the function to the power of ci.e. ( dd)c will account for scaling. When scaling is symmetric the result is exact, assumingdistance is based on the linear combination of the two dimensions. With asymmetric scaling,the result is not exact. But if the di erence between the two scale factors is relatively small,it is a useful approximation to use their maximum.The following is an algorithmic summary of the whole matching process: SG = subgraph extracted from the new task. For each subgraph G acting as an index to the case base{ For each isomorphic mapping of G to SG Find minimum weighted least squares t of G to SG using mapping A ne transform = coe cients of least squares t Penalized t = least squares error + transform penalty Keep graph and transform with lowest penalized t Retrieve function associated with best graph from case base (if none use default) Apply a ne transform to function Apply bilinear interpolation/extrapolation Adjust function height Add new function to existing function 78\nAccelerating Reinforcement Learning3.4 CompositionThis section describes function composition, how the transformation is applied successivelyto the series of subgraphs extracted from the composite graph. Function composition uses aslightly modi ed form of Dijkstra's algorithm (Dijkstra, 1959) to traverse the edges betweendoorway nodes. The left hand side of Figure 21 shows the composite graph after movingthe goal in the robot navigation example of Section 2.2. The right hand side shows thegraph traversed by Dijkstra's algorithm.\nd3\nGr1 Gr2\nGr5\nGr3 D\nD\nD\nD\nG d1\nd2\nGr4\nD\nD\nD\nD\nG\nFigure 21: Using Dijkstra's AlgorithmTo begin the process, the subgraph which contains the goal is extracted and the bestmatching isomorphic subgraph is found. The edge lengths in the composite graph arethen updated using the scaled length of the corresponding edge in the matching isomorphicsubgraph, d1 and d2 in Figure 21. As d2 is less than d1, the next subgraph extracted,Gr2, is the one sharing the doorway node with the edge of length d2. The best matchingisomorphic subgraph is found and the edge of length d3 updated. The shortest path isagain determined. As d1 is less than d2 + d3 subgraph, Gr3 is extracted. The process isrepeated until all subgraphs have been updated. At each stage when a subgraph is matched,the corresponding transformed function is retrieved and added to the new function in theappropriate region.In this example, there is only a single path to the goal from each room. Often there willbe multiple paths. Suppose room 5 had an additional doorway in the lower left corner ofthe room, labeled \\B\" on the left hand side of Figure 22, in addition to the original doorwaylabeled \\A\". The graph, shown on the right hand side of Figure 22, would result. There arenow two possible paths to the goal of lengths d4 and d5. If the length across room 5, d6, isgreater than the absolute di erence between d4 and d5, the choice of path from this roomwill be determined by a decision boundary inside the room. This is produced by taking the79\nDrummond\nRoom 1 Room 3 Room 2 Room 4 Room 5\nGr5\nd6d4\nd5\nn2\nn3\nn1\nA\nB\nA\nB Figure 22: Multiple Paths to Goalmaximum of two functions as shown in Figure 23: one for entering by doorway \\A\" andleaving by doorway \\B\"; one for entering by doorway \\B\" and leaving by doorway \\A\".This principle can be repeated if there are more than two paths to the goal from a givenroom.If the cross-room distance, d6, had been smaller than the di erence (jd4-d5j) the decisionboundary would have to be in another room. In general, we want to nd the room in whichthe cross-room distance is larger than the di erence between the incident paths. This isrepeated for every cycle in the path graph. A cycle is detected when a node is visited twice,indicating that it is reachable by two separate paths. Let us suppose this is node n3 in thegraph of Figure 22. As Dijkstra's algorithm is being used, we know that all previous nodes,on either path, such as n1 and n2 are already closed. This must be true for both pathsto have reached n3. All the rooms on paths up to these nodes cannot contain the decisionboundary, so it must be in either room 4 or 5. To decide which remaining room it is in, wecompare the two path lengths. If d4 is longer than d5 + d6 then the decision boundary willbe in room 4; otherwise it will be in room 5.Whichever room is selected, the decision boundary is produced from the maximumof twofunctions. The heights of the two functions, when adjusted for their path lengths, determinewhere the decision boundary occurs within the room. If the paths are of equal length, takingthe maximum will correctly put the decision boundary at the doorway. If there are no suchfunctions in the case base, functions that already include decision boundaries may be used.This technique produces a correct decision boundary if the di erence in the path lengthsentering the room is less than the di erence between the heights of the function at the \\out\"doorways. On the left hand side of Figure 24 there is a room with two doorways. As path1 is signi cantly longer than path 2, the decision boundary is far to the left. The shortestpath to the goal from most of the room is via the right hand doorway. If this function iscombined with a mirror image of itself, it will produce a decision boundary in the middle80\n81\nDrummondof the room, as shown on the right hand side of Figure 25. This could be used for the newproblem shown on the left hand side of Figure 25 where the two paths are the same length.Again the heights of the two functions can be changed to move the decision boundary. Butit cannot be moved to anywhere in the room. The decision boundary can be moved nocloser to a particular doorway than in the original function shown in Figure 24 Path 1 Path2\nRoom\nDecision Boundary\nFigure 25: Combining Decision Functions4. ExperimentsThis section compares learning curves for function composition and a simple baseline al-gorithm. Four sets of results are presented; one for each of the two types of related taskin each of the two domains. The learning curves represent the average distance to goal asa function of the number of actions taken during learning. The distance is averaged over64 di erent start positions, distributed uniformly throughout the state space, and over thedi erent experimental runs. To determine this distance, normal learning is stopped after a xed number of actions and a copy of the function learned so far is stored. One of the 64start positions is selected, learning is restarted and the number of actions needed to reachthe goal is recorded. If a trial takes 2000 actions and has not yet reached the goal, it isstopped and the distance to goal recorded as 2000. The function is then reinitialized withthe stored version and another start state selected. This is repeated 64 times. Then thefunction is reinitialized once more and normal learning resumed.The baseline algorithm and the underlying learning algorithm for the function compo-sition system is the basic Q-learning algorithm, using a discrete function approximator asdiscussed in Section 3.1. The learning rate is set to 0.1, the greedy policy uses an of 0.1(the best action is selected 90% of the time), the future discount is 0.8 and a reward of 1.0is received on reaching the goal. Although the state spaces for the di erent domains repre-sent two quite di erent things { the robot's hx; yi location and the angle of the arm's twojoints { the actual representation is the same. The state space ranges between 1 for eachdimension. A step is 0:25 in each dimension either separately or together, giving the eightpossible actions. The actions are stochastic, a uniformly distributed random value between 0:125 being added to each dimension of the action. In the robot navigation examples if82\nAccelerating Reinforcement Learningthe robot hits the wall, it is positioned a small distance from the wall along the direction ofits last action. This has not been implemented for the robot arm as it is a somewhat morecomplex calculation. Instead, if a collision with an obstacle occurs the arm is restored toits position before taking the action.Learning begins at a randomly selected start state and continues until the goal is reached.Then a new start state is selected randomly and the process repeated. This continues untilthe requisite total number of actions is achieved. Speed up is calculated by dividing thenumber of learning steps at one speci c point on the baseline learning curve by the numberof learning steps at an equivalent point on the function composition system's learning curve.The knee of the function composition system's curve is used. This occurs where the lowlevel learning algorithm is initialized with the composed function. This is compared to theapproximate position of the knee of the baseline curve.4.1 Robot Navigation, Goal RelocationThe rst experiment investigates the time taken to correct a learned function when the goalis relocated in the robot navigation domain. There are nine di erent room con gurations,as shown in Figure 26, the number of rooms varying from three to ve and there are fourdi erent goal positions. Each room has one or two doorways and one or two paths to thegoal. To initialize the case base, a function is learned for each of these con gurations withthe goal in the position shown by the black square. The rooms are generated randomly,with some constraints on the con guration of the rooms and doorways: a room can notbe too small or narrow, a doorway can not be too large. The case base also includesfunctions generated for the experiments discussed in Section 4.3. This was necessary togive a su cient variety of cases to cover most of the new tasks. Even with this addition,not all subgraphs are matched. Constant valued default functions are used when there isnot a match. This reduces speed up signi cantly, but does not eliminate it altogether. 1 2 3\n4 5 6 7 8 9Figure 26: The Di erent Suites of Rooms83\nDrummondOnce the case base is loaded, the basic Q-learning algorithm is rerun on each roomcon guration with the goal in the position shown. After 400,000 steps the goal is moved,this is denoted as time t on the x-axis of Figure 27. The goal is moved to one of thethree remaining corners of the state space, a task not included in the case base. Learningcontinues for a further 300,000 steps. At xed intervals, learning is stopped and the averagenumber of steps to reach the goal is recorded. The curves in Figure 27 are the average of27 experimental runs, three new goal positions for each of the nine room con gurations.\nt-400....t-100 t-50 t t+50 t+100 t+150 t+200 t+250 t+300 10\n1\n10 2\n10 3\nt + No. of Learning Steps X 1000\nA ve\nra ge\nN o.\no f\nS te\nps to\nG oa\nl\nFunction Composition Q-Learning (No Reinit) Q-Learning\nFigure 27: Learning Curves: Robot Navigation, Goal RelocationThe basic Q-learning algorithm, the top curve of Figure 27, performs poorly because,when the goal is moved, the existing function pushes the robot towards the old goal position.A variant of the basic algorithm reinitializes the function to zero everywhere on detectingthat the goal has moved. This reinitialized Q-learning, the middle curve, performed muchbetter, but it still has to learn the new task from scratch.The function composition system, the lowest curve, performed by far the best. Theprecise position of the knee of this curve is di cult to determine due to the e ect of usingdefault functions. If only those examples using case base functions are considered, the kneepoint is very sharp at about 3000 steps. The average number of steps to goal at 3000 steps,for all examples, is 40. The non-reinitialized Q-learning fails to reach this value within300,000 steps giving a speed of over 100. The reinitialized Q-learning reaches this valueat about 120,000 steps, giving a speed up of about 40. Function composition generallyproduces accurate solutions. Even if some error is introduced, further Q-learning quicklyre nes the function towards the asymptotic value of about 17. After about 150,000 steps,84\nAccelerating Reinforcement Learningnormal Q-learning reaches an average value of 24 steps and then slowly re nes the solutionto reach an average value of 21 after 300,000 steps.4.2 Robot Arm, Goal RelocationThe second experiment is essentially a repeat of the rst experiment but in the robot armdomain. The initial number of steps, before the goal was moved, was reduced to 300,000to speed up the experiments. As the arm has only two degrees of freedom, and with therestrictions discussed in Section 2.4, the number of variations is small. So only three obstaclecon gurations were used, constructed by hand, with two obstacles in each. To increase thenumber of experiments, to allow for greater statistical variation, each con guration wasrepeated with the goal in each of three possible positions, as shown in Figure 28. Theblack diamonds represent the obstacles, the black rectangles the goal. Solutions to all thesetasks were loaded into the case base. When composing a function, however, the system isprevented from selecting a case that comes from the same goal and obstacle con guration. 1 2 3\n4 5 6\n7 8 9Figure 28: The Robot Arm Obstacle and Goal PositionsThe curves in Figure 29 are the average of 18 experimental runs, two new goal positionsfor each of the three original goal positions in the three obstacle con gurations shown inFigure 28. There are only two learning curves, non-reinitialized Q-Learning being dropped.As in the rst experiment, the function composition system, the lower curve, performedmuch better than Q-learning. The knee of the function composition system occurs at 2000steps, the knee of Q-learning at 50,000 steps, giving a speed up of 25. In this experiment,the case base contained subgraphs that matched for all new tasks, so default functions werenot needed. The composed functions tend to be very accurate and little further re nementis necessary. 85\nDrummond\nt-300....t-100 t-50 t t+50 t+100 t+150 t+200 t+250 t+300 10\n1\n10 2\n10 3\nt + No. of Learning Steps X 1000\nA ve\nra ge\nN o.\no f\nS te\nps to\nG oa\nl\nQ-Learning\nFunction Composition\nFigure 29: Learning Curves: Robot Arm, Goal Relocation4.3 Robot Navigation, New EnvironmentThe third experiment investigates the time taken to learn in a new, but related, environmentin the robot navigation domain. Nine di erent inner rooms were generated randomly, againunder some constraints. All have a single doorway, but the size and position of the roomand the location of the doorway are varied as shown in Figure 30. To initialize the case base,a function is learned for each of these con gurations with the goal inside the small room asindicate by the dark square. Learning is then repeated on each of the room con gurationsin turn. However, when composing the new function the system is prevented from selectinga case learned from the same goal and room con guration. Experimental runs for the Q-learning algorithm and the function composition system are initialized with a at functionof zero and 0.75 everywhere respectively, denoted as zero on the x-axis. Learning continuesfor 100,000 steps. To improve the statistical variation, experiments for each con gurationwere repeated three times, each time with a new random seed. The curves in Figure 31 are,therefore, the average across 27 experimental runs.The top curve is the Q-learning algorithm, the bottom curve the function compositionsystem. For these experiments, locating the goal took typically between 400 and 1200 steps,although some took 2000 steps. The function composition system then introduces the \\nowalls\" function and typically a further 800 to 4000 steps are taken before usable features aregenerated. Again, certain experimental runs took longer, this will be discussed in Section5.2. Due to these runs, the knee of the function composition system's curve occurs at 12,000steps. The knee of the basic Q-learning curve occurs at approximately 54,000 steps giving86\nAccelerating Reinforcement Learning 1 2 3 4 5 6 7 8 9Figure 30: The Single Rooms\n0 10 20 30 40 50 60 70 80 90 100 10\n1\n10 2\n10 3\nNo. of Learning Steps X 1000\nA ve\nra ge\nN o.\no f\nS te\nps to\nG oa\nl\nQ-Learning\nFunction Composition\nFigure 31: Learning Curves: Robot Navigation, New Environment87\nDrummonda speed up of 4.5. As in previous experiments once initialized the function is very accurateand little further re nement is necessary. Basic Q-learning, on reaching the knee, takes along time to remove the residual error.4.4 Robot Arm, New EnvironmentThe fourth experiment is essentially the same as the third experiment except in the robotarm domain. Here, three, hand crafted, con gurations of a single obstacle with the goal ina xed position were used, as shown in Figure 32. To increase the statistical variation eachcon guration was run ve times with a di erent random seed. The curves in Figure 33 aretherefore the average across 15 experimental runs. 1 2 3\nFigure 32: The Di erent Obstacle Positions\n0 10 20 30 40 50 60 70 80 90 100 10\n1\n10 2\n10 3\nA ve\nra ge\nN o.\no f\nS te\nps to\nG oa\nl\nNo. of Learning Steps X 1000\nFunction Composition Q-Learning\nFigure 33: Learning Curves: Robot Arm, New Environment88\nAccelerating Reinforcement LearningThe top curve of Figure 31 is the Q-learning algorithm, the bottom curve the functioncomposition system. The knee of the function composition system's curve occurs at about4400 steps. The knee of the basic Q-learning algorithm at about 68,000 steps giving a speedup of about 15.5. Analysis of ResultsThe experiments of the previous section have shown that function composition producesa signi cant speed up across two di erent types of related task and across two domains.In addition, the composed solutions tend to be very accurate and little further re nementis required. This section begins by looking at possible concerns with the experimentalmethodology that might a ect the measurement of speed up. It then discusses variousproperties of the task being solved that a ect the speed up achieved by using functioncomposition.5.1 Possible Concerns with the Experimental MethodologyThe speed up obtained using function composition is su ciently large that small variationsin the experimental set up should be unlikely to a ect the overall result. Nevertheless, thereare a number of concerns that might be raised about the experimental methodology. Somewill be, at least partially, addressed in this section; others will be the subject of future work.The rst concern might be how the estimated value of speed up is measured. Thevalue represents the speed up of the average of a set of learning tasks, rather than theaverage of the speed up in each of the tasks. One of the di culties of estimation, withcurves for single tasks, is that the average distance to goal may oscillate up and down aslearning progresses, even though the general trend is downwards. This makes judging theposition of the knee of the curves di cult, and any estimate of speed up questionable. Evenexperimental runs using the same con guration, but with di erent random seeds, exhibit aconsiderable variation. In some instances, the speed up measured on individual curves maybene t the function composition system, in others, the baseline algorithm. Nevertheless,probably overall most of these e ects will cancel out.The second concern might be the e ect on speed up of the limit of 2000 steps whenmeasuring the distance to the goal. Comparing two averages of values limited in this wayis sometimes misleading (Gordon & Segre, 1996). But this limit primarily a ects only thebaseline algorithm, and was only signi cant when the goal was moved and the function notreinitialized. Estimation of speed up is principally concerned with comparing the positionof the knees of the di erent curves. Here, the average distance to goal is relatively small,so limiting the value is likely to have little e ect.The third concern might be that the value of speed up is dependent on the con gurationof the baseline algorithm. Certainly, it is the experience of this author that the way thefunction is initialized, and how actions are selected, can have an impact on the speed oflearning. In previous work (Drummond, 1998), the function was initialized to a constantvalue of 0.75, a technique termed \\optimistic initial values\" by Sutton and Barto (1998).Tie breaking between actions of the same value was achieved by adding a small amount ofnoise (circa 5 10 5). It was expected that this would increase exploration early on inthe learning process and speed up learning overall. However, using an initial value of zero89\nDrummondand a strict tie-breaker, randomly selecting amongst actions with the same value, turnedout to produce a signi cant speed up in the baseline learning algorithm. This con gurationwas used for the preceding experiments, but on one experimental run this caused seriousproblems for the baseline algorithm.\n0 50 100 150 200 250 300 10\n1\n10 2\n10 3\nNo. of Learning Steps X 1000\nA ve\nra ge\nN o.\no f\nS te\nps to\nG oa\nl\nFigure 34: Learning Curves in a Partially Observable DomainThe upper learning curve of Figure 34 is for the baseline algorithm, for one run whenthe goal was moved in the robot arm domain. As it had such a large impact on the averagelearning curve, it was replaced by the lower curve, produced by repeating the experimentwith a di erent random seed. This very slow learning rate arises from the interactionof the partial observability of the robot arm domain with the use of an initial value ofzero. Individual cells of the function approximator straddle the obstacles allowing a \\leak-through\" of value from one side of the obstacle to the other. Starting with a zero value,once an action receives some value it will remain the best action for some time. Continualupdate of this action will decrease the value, but it can only asymptotically approach zero.Until other actions for the same state are updated, it will always be selected as the greedyaction. This did not occur for higher initial values. It may be that in domains where there issome degree of partial observability, small initial values are better than zero or some meansof improving exploration for very small values might be necessary.Other variations in the parameters of the baseline algorithm have not been exploredin this paper. For instance, a constant learning rate of 0.1 was used. Alternatives, suchas starting with a higher rate and reducing it as learning progresses might also improvethe overall speed of the baseline algorithm. Some preliminary experiments were, however,90\nAccelerating Reinforcement Learningcarried out using undiscounted reinforcement learning, the discounting being strictly un-necessary in goal-directed tasks. Room con guration 1 of Figure 26, with the goal in thelower right hand corner, was used as the experimental task. The discounting, discussed inSection 3.1, is turned o by setting to 1. In addition, the value on reaching the goal stateis set to zero and a cost is associated with every action. This form of learning simpli esfunction composition, normalization procedures needed to compensate for the value func-tion's exponential form being no longer required. With normalization disabled, the snakesuccessfully partitioned the function, the most critical part of the process. However, thebaseline learner took considerably longer to learn the function than in the discounted case.With discounting, the learner reached an average distance to goal of about 72 steps after80,000 learning steps. Without discounting, the learner reached an average of 400 steps atthe same point in time and only an average of 80 steps after 300,000 learning steps. Theaction-value function was initialized to zero, which appears to be the standard practice inthe literature. However, the experience with initialization in the discounted case suggeststhis might be the part of problem and this will be investigated in future work.The baseline Q-learning algorithm used is the most basic and a more sophisticated onewould unquestionably reduce the speed up experimentally obtained. For instance, someform of reinforcement learning using eligibility traces (Singh & Sutton, 1996) might beused. For the experiments when the goal was moved, a baseline such Dyna-Q+ (Sutton,1990) which was speci cally designed to deal with changing worlds would probably be abetter reference point.The speed up obtained, by transferring pieces of an action-value function, has also notbeen compared to alternatives, such as transferring pieces of a policy or transferring piecesof a model. Transferring pieces of a policy would reduce memory requirements and notrequire the rescaling applied to pieces of an action-value function. It does, however, havetwo disadvantages. Firstly, a solution can not be directly composed, as the position ofdecision boundaries can not be determined. Further learning would be necessary to decidethe appropriate policy for each room. Secondly, the policy only indicates the best action.The action-value function orders the actions, indicating potentially useful small changesto the policy which might improve the accuracy on a new task. Transferring pieces of amodel, would require rst learning a model consisting of a probability distribution functionfor each action in each state. The memory requirement is considerably larger, unless thestates reachable by an action are limited beforehand. Nevertheless, a model would needless modi cation in a changing world, such as when the goal is moved. It also carriesmore information which might speed up learning. The action-value function seems a goodcompromise in terms of complexity versus information content, but this would need to beempirically validated and is the subject of future work.5.2 Performance Variation with Task Con gurationGenerally, function composition outperforms the baseline learning algorithm by an amountdependent on the complexity of the learning problem. In the robot navigation domainwhen the goal was moved, the amount of speed up increased with more rooms and fewerpaths to goal. A speed up of 60, against an average speed up of 40, was obtained on thecon gurations with ve rooms and a single path to goal. Con gurations with only three91\nDrummondrooms had the least speed up, but this was not only due to the relative simplicity of theproblem.The top of Figure 35 shows the average of four learning curves for the three roomcon gurations. The bottom of Figure 35 shows one of the con gurations that producedthese curves. Not only is it one of the easiest tasks (from the experimental set) for thebaseline algorithm, but also there are no solutions in the case base for the lowest room.There are no isomorphic subgraphs of this form. Rather than not composing a solution,the system introduces a constant value function for this room. This room represents almosthalf the state space, so much additional learning is required. As the top of Figure 35 shows,initially there is signi cant speed up. Further re nement reduces the advantage and fora short while the baseline algorithm is better. But later, function composition gains theupper hand and converges more quickly than the baseline algorithm towards the asymptoticvalue.\nAccelerating Reinforcement LearningIn the robot navigation domain when learning a new task, the amount of speed up variedwith the size of the inner room. This was primarily due to the number of actions neededbefore the features emerged with su cient clarity for the snake to locate them. Functioncomposition is most successful when the inner room is small. If a wall is long, the featuretakes more time to develop, more re nement by Q-learning is needed to make it apparent.Very short walls are also hard to identify. The likelihood of the robot colliding with themis small and it takes many exploratory actions for the features to emerge clearly.The features may be su ciently clear for the snake to form a partition, yet not be wellenough de ned to precisely locate the doorways. A doorway may appear to be a bit widerthan it actually is. More importantly, it may appear to be displaced from its true position.Typically, the error in the composed function is small and normal reinforcement learningquickly eliminates it. In one of the experimental runs, con guration 2 in Figure 30, thespeed up was reduced by a factor of 2 due to the doorway being incorrectly positioned.The feature representing the lower wall had not completely emerged when the partitionwas generated. This made the doorway appear to be almost exactly at the corner. Thealgorithm, in fact, positioned the doorway just on the wrong side of the corner. This resultedin the signi cantly reduced speed up. But it is unclear why reinforcement learning tookso long to correct what seems, on the surface at least, to be a local error. This will beinvestigated in future work.6. LimitationsLimitations come in , roughly, two kinds: those arising from the overall approach and thosearising from the way it was implemented. In the former case, ways to address these limita-tions may be highly speculative, or impossible without abandoning some of the fundamentalideas behind the approach. In the latter case, there is a reasonable expectation that futurework will address these limitations. The following sections will deal with these cases inturn.6.1 Limitations in the ApproachTo explore the possible limitations in the approach, this section reviews the fundamentalassumptions on which it is based.It is a fundamental assumption that features arise in the reinforcement learning functionthat qualitatively de ne its shape. The features used in this paper are the violation of asmoothness assumption, that neighboring states have very similar utility values. A wall, bypreventing transitions between neighboring states, typically causes such a violation. Otherthings, such as actions with a signi cant cost, would have a similar e ect. Smaller, andmuch more varied costs, will not generate the features required by this approach, so it o erslittle in the way of speed up in these cases. If there is a mixture of large and small costs,it is expected that the system will capture features generated by the former, initialize thefunction and normal reinforcement learning will address the latter.The smoothness assumption is less clear if the dimensions are not numeric. The neigh-borhood relation, used here, is a prede ned distance metric over a continuous space. Innominal, binary or mixed domains it is not obvious how such a metric would be de ned,although there is some work on such metrics for other applications (Osborne & Bridge,93\nDrummond1997). If the dimensions are mixed, feature location might be limited to the continuousones. If the dimensions are purely nominal or binary, a generalization of the snake may beappropriate. The snake is, at an abstract level, a constrained hill climber. But whether ornot this idea would usefully generalize in this way is at present somewhat speculative.It is a fundamental assumption that the features clearly delimit subtasks. In the do-mains discussed in this paper, the obstacles and walls subdivide the state space into regionsconnected by small \\doorways\". The subtask of reaching one doorway is not greatly a ectedby the subsequent subtask. In other domains this may not be the case. As the doorwaysbecome larger, the context sensitivity increases. As long as the composed solution is rea-sonably accurate, reinforcement learning can easily correct the error although speed up willbe reduced. At some point however, due to a very large amount of context sensitivity, theadvantage of dividing the task into subtasks will become questionable. It would be possibleto account for some of the context dependency in the graph matching stage, looking atlarger units than subgraphs. If two adjacent subgraphs match the new problem, they mightbe used as a pair, thereby including any contextual relationship between them. Even ifsingle subgraphs were used, the context in which they appear, i.e. the shape of neighboringsubgraphs, could be taken into account. In the limit, graph matching the whole task mightbe used. But, as was argued in the introduction, this would considerably limit when transferis applicable, and thus its overall e ectiveness.It is a fundamental assumption that the absolute position of the features is unimpor-tant, it is the shape of the delimited region that matters. To increase the likelihood oftransfer, solutions to subtasks have been subjected to a variety of transformations. Insome domains, many, if not all, of these transformations will be invalid. If actions cannotbe rotated or re ected, or if many small costs a ect di erent regions of the state space,the e ectiveness of transfer will be reduced. This would be, to some extent, addressed byadditional penalties for di erent transformations, but again this would limit the opportu-nities for transfer. Which transformations are appropriate, and whether or not this can bedetermined automatically from the domain, will be the subject of future research.It is a fundamental assumption that a vision processing technique can locate thesefeatures in a timely fashion, even in very high dimensional domains. Learning in very highdimensional domains is likely to be slow whatever technique is used. Normal reinforcementlearning will take time to navigate the much larger space, slowing down the emergence ofthe features. Although the time taken to partition the function will increase, the frequencywith which partitioning is applicable will decrease. Thus the amortized cost will rise moreslowly. Further, as high dimensional spaces are generally problematical, methods such asprincipal components analysis and projection pursuit (Nason, 1995) can be used to reducedimensionality. It may prove in practice that the dimensionality which is important, and isthe focus of feature extraction, is much smaller than the actual dimensionality of the space.6.2 Limitations in the ImplementationIf the assumptions of the previous section are met, it is expected that the remaining lim-itations are due to the present implementation. These limitations are likely to becomeapparent when the system is applied to other domains. Certainly other domains may di erfrom those presented in this paper in a number of ways.94\nAccelerating Reinforcement LearningA domain may di er in that the dimensionality of the space is higher than the twodimensions of the tasks investigated in this paper. The implementation of the snake hasbeen updated to work in higher dimensions. The bold lines at the top of Figure 36 areone of the simpler tasks from the robot navigation domain. The task has been extended inthe Z-dimension. The snake starts out as a sphere and then expands outwards until it llsthe room. In this example, the polygonal constraint has not been used, but everything elseremains the same. Figure 37 shows the complete partition of the task.\nFigure 36: Adding a Z-Dimension Figure 37: The Complete 3D PartitionThe mathematics behind the snake is not limited to three dimensions. There also seemsto be nothing in principle that would prevent other processes such as graph matching,planning or transformation from working in higher dimensions. Speed is the main problem.This is not a problem unique to this approach and there is a large body of research addressingthis issue. For instance, although graph matching is in general NP-complete, there ismuch active research in speeding up matching on the average or in special cases (Gold &Rangarajan, 1996; Galil, 1986). At present, the snake represents the principal restrictionon speed. This is an issue of great importance to the vision processing community. Currentresearch is investigating this problem, at least in two or three dimensions. One example ishierarchical methods (Schnabel, 1997; Leroy, Herlin, & Cohen, 1996) which nd solutionsfor the snake at progressively ner and ner resolution scales. The results of such researchwill undoubtedly be of importance here.A domain may di er in that the value function learned might not produce features lo-catable by the snake with the present parameter settings. The values of the parameterswere empirically determined, using hand crafted examples from the robot navigation andthe robot arm domains. The obvious danger is that the parameters might be tuned to theseexamples. To demonstrate that this is not the case, con gurations for the experiments inthe robot navigation domain were generated randomly. As con gurations for the robot armdomain are more tightly constrained, the hand crafted examples were used in the experi-ments. Nevertheless, the experiments have shown that the parameters worked successfullyfor random examples in the robot navigation domain. The same parameters also work suc-cessfully in the second domain, the robot arm. The following discussion demonstrates that95\nDrummondthey are also reasonably e ective in a quite di erent domain, the \\car on the hill\". It isanticipated that using the results of current research into snakes will automate the selectionof many parameters.In the \\car on the hill\"domain (Moore, 1992), the task, simply stated, is to get a carup a steep hill, Figure 38. If the car is stationary part way up the hill, in fact anywherewithin the dotted line, then it has insu cient acceleration to make it to the top. So thecar must reverse down the hill and then achieve su cient forward velocity, by acceleratingdown the other side, before accelerating up the hill. The state space, for the purposes ofreinforcement learning, is de ned by two dimensions. These are the position and velocityof the car, as shown in Figure 39. The goal is to reach the top of the hill with a smallpositive or negative velocity. In this domain there are two possible actions: accelerateforward, accelerate backwards. Unlike in previous domains, there is no clear mapping ofthe actions onto the state space. The state achieved on applying an action is determined byNewton's laws of motion. As the car has insu cient acceleration to make it up the hill fromeverywhere in the state space, a \\wall\" is e ectively introduced, the bold line in Figure 39.To reach the top of the hill, the car must follow a trajectory around this \\wall\", the dashedline in Figure 39.\nFigure 39: Car State SpaceFigure 40 shows the reinforcement learning function. It exhibits the same steep gradientas the other domains. The important point to note is that, unlike in the other domains,no physical object causes this gradient. It is implicit in the problem itself, yet the featuresstill exist. Figure 41 shows the partition produced when applying the snake to the \\car onthe hill\" domain. The main di erence from the previous examples is that the polygonalconstraint has not been used. When the snake initially comes to rest, the mercury forceis turned o and then the snake is allowed to nd the minimum energy state. It was alsonecessary to reduce the scaling of the edges, by about a factor of three quarters, to achievethe accuracy of t. The t around the top left corner of the second snake, the dashed line,96\nAccelerating Reinforcement Learningalso has some problems: the snake is growing very slowly downwards and is, at present, onlystopped because it has reached the maximum number of iterations allowed. One di cultyin this example is that there is not such clear delimitation of the upper and lower regionsat the end of the feature. Future work will investigate altering the stopping condition toeliminate this problem.\nFigure 40: The Steep Gradient Figure 41: The Regions ExtractedA domain may di er in that the shape of various regions in the partition is more com-plex than can be dealt with by the present snake. Fitting the snake to the task discussed inthe previous paragraphs goes some way towards mitigating that concern. Nevertheless, therandomly generated examples of Section 4.1 were subject to certain constraints. Con gu-rations with narrower rooms were tried informally, but the snake did not reliably locate thefeatures. The con gurations in Section 4 represent the limit of the complexity of partitionthe snake can produce at present. It is expected that using ideas from the large body ofalready published research into snakes will go a long way towards addressing this limitation.For complex regions, locating all the subtleties of the underlying shape may be unnecessary,or even undesirable. The aim is to speed up low level learning. As long as the solution isreasonably accurate, speed up should be obtained. Being too sensitive to minor variationsin shape may severely limit the opportunities for transfer and thus reduce speed up overall.A domain may di er in that changes in the environment are more complex than thoseinvestigated in this paper. At present, the system detects that the goal has moved bycounting how often a reward is received at the old goal position. Not only is this a ratherad hoc approach, but it also does not account for other possible changes, such as pathsbecoming blocked or short-cuts becoming available. At present, when learning a new taskthe system is restarted and is not required to determine that its present solution is no longerapplicable. In future work, the system should decide when its model of the world is no longercorrect. It should also decide what, if any, relationship there is to the existing task andhow it might be best exploited. This will allow a more complex interaction of the functioncomposition system with reinforcement learning. For instance, the learning of a new task for97\nDrummondthe robot navigation domain used the relatively simple situation of two rooms. The functioncomposition system initialized the low level algorithm once on detecting suitable features. Inthe future, to address more complex tasks, with many more rooms, an incremental approachwill be used. When a new task is being learned, the system will progressively build up asolution by function composition as di erent features become apparent.This approach also should handle any errors the system might make with feature ex-traction. In the experiments with these simple room con gurations, the ltering discussedin Section 2.3 proved su cient to prevent problems. But in more complex tasks, it is likelythat false \\doorways\" will be detected, simply because the system has not explored thatregion of the state space. A composed function including that extra doorway will drive thesystem into that region. It should then become quickly apparent that the doorway does notexist and a new function can be composed.7. Related WorkThe most strongly related work is that investigating macro actions in reinforcement learn-ing. Precup, Sutton and Singh (1997, 1998) propose a possible semantics for macro actionswithin the framework of normal reinforcement learning. Singh (1992) uses policies, learnedto solve low level problems, as primitives for reinforcement learning at a higher level. Ma-hadevan and Connell (1992) use reinforcement learning in behavior based robot control.To learn a solution to a new task, all these systems require a de nition for each subtaskand their interrelationships in solving the compound task. The work presented here givesone way that macro actions can be extracted directly from the system's interaction withits environment, without any such hand-crafted de nitions. It also shows how to determinethe interrelationships of these macro actions needed to solve the new task. Thrun's re-search (1994) does identify macro actions, by nding commonalities in multiple tasks. Butunlike the research presented here, no mapping of such actions to new tasks is proposed.Hauskrecht et al. (1998) discuss various methods of generating macro actions. Parr (1998)develops algorithms to control the caching of policies that can be used in multiple tasks.But in both cases, they need to be given a partitioning over the state space. It is theautomatic generation of just such a partition that has been the focus of much of the workpresented in this paper. It may well be that this approach to generating partitions and todetermining the interrelationships between partitions of related tasks will prove useful tothis other work.Another group of closely connected work is the various forms of instance based or casebased learning that have been used in conjunction with reinforcement learning. They havebeen used to address a number of issues: (1) the economical representation of the statespace, (2) prioritizing states for updating and (3) dealing with hidden state. The rst issueis addressed by Peng (1995) and by Tadepalli and Ok (1996) who use learned instancescombined with linear regression over a set of neighboring points. Sheppard and Salzberg(1997) also use learned instances, but they are carefully selected by a genetic algorithm. Thesecond issue is addressed by Moore and Atkeson (1993) who keep a queue of \\interesting\"instances, predecessors of those states where learning produces a large change in values.These are updated most frequently to improve the learning rate. The third issue is addressedby McCallum (1995b) who uses trees which expand the state representation to include prior98\nAccelerating Reinforcement Learningstates, removing ambiguity due to hidden states. In further work, McCallum (1995a) usesa single representation to address both the hidden state problem and the general problemof representing a large state space by using a case base of state sequences associated withvarious trajectories. Unlike this other research, in the work presented here the case is notan example of the value function during learning. Instead, it is the result of a completelearning episode, so the method should be complementary to these other approaches.This work is also related to case based planning (Hammond, 1990; Veloso & Carbonell,1993), rstly through the general connection of reinforcement learning and planning. Butit is analogous in other ways. When there is a small change to the world, such as thegoal being moved, a composite plan is modi ed by using sub-plans extracted from othercomposite plans.Last, but not least, is the connection with object recognition in vision research (Suetenset al., 1992; Chin & Dyer, 1986). In the work presented here, many of the methods { if notthe nal application { has come from that eld. The features in the reinforcement learningfunction are akin to edges in an image. These are located by nding the zero crossing pointof the Laplacian as introduced by Marr (1982). In the work presented here, it was proposedthat the features largely dictate the form of the function. Mallat and Zhong (1992) haveshown that a function can be accurately reconstructed from a record of its steep slopes.8. ConclusionsThis paper described a system that transfers the results of prior learning to signi cantlyspeed up reinforcement learning on related tasks. Vision processing techniques are utilizedto extract features from the learned function. The features are then used to index a casebase and control function composition to produce a close approximation to the solution ofa new task. The experiments demonstrated that function composition often produces morethan an order of magnitude increase in learning rate compared to a basic reinforcementlearning algorithm.AcknowledgementsThe author would like to thank Rob Holte for many useful discussions and help in preparingthis paper. This work was in part supported by scholarships from the Natural Sciences andEngineering Research Council of Canada and the Ontario Government.Appendix A. Spline RepresentationsThis appendix presents some of the underlying mathematics associated with spline repre-sentations and the snake. It is not meant to be an introduction to the subject. Rather itis added for completeness to discuss certain important aspects of the system not addressedelsewhere in this paper. Knowledge of these aspects is not necessary to understand the basicprinciples of the approach discussed in this paper, but would be necessary if one wantedto duplicate the system. More detailed explanation is given in Drummond (1999). Somespeci c papers that address these ideas in much greater detail are: for splines (Terzopoulos,1986) and for snakes (Cohen & Cohen, 1993; Leymarie & Levine, 1993).99\nDrummondSplines are piecewise polynomials where the degree of the polynomial determines thecontinuity and smoothness of the function approximation. Additional smoothing constraintscan be introduced by penalty terms which reduce the size of various di erentials. One waythen to view spline tting is in the form of an energy functional such as Equation 6.Espline(f\u0302) = ZR Efit(f\u0302) +Esmooth(f\u0302) ds (6)Here, there is an energy associated with the goodness of t, some measure of how closethe approximating function is to the input function. This is typically the least squaresdistance between the functions. There is an energy associated with the smoothness of thefunction. Two very commonly used smoothness controls produce the membrane and thinplate splines by restricting the rst and second di erentials of the function respectively. To t the spline to the function, the total energy must be minimized. A necessary conditionfor this is an Euler-Lagrange di erential equation such as Equation 7. Here !t controls thetension in the spline (the resistance to stretching) and !s the sti ness (the resistance tobending). Often the error function will be based on individual data points and the left handside of Equation 7 would include delta functions. @@s(!t(s)@f\u0302(s)@s ) + @@s2 (!s(s)@2f\u0302(s)@s2 ) = fin(s) f\u0302(s) (7)In this work, such splines have been used for a number of purposes. When tting thesnake, measures of the rst and second di erential are needed. A two dimensional quadraticspline is tted to the discrete representation of the maximum Q-values. An !t of 0.2 is used(!s is zero) to limit overshoot (Drummond, 1996) to prevent false edges. Values from anidentical spline except using an !t of 2.0 are squared and then divided into the di erentialvalues. This normalizes the di erentials, so that the size of edges is not dependent on wherethey occur in the function. The same type of spline is used to produce the bowls associatedwith the rooms as discussed in Section 3.2.1. Here !t is 1.0 and !s is 0.5 giving roughlyGaussian smoothing. The values used to produce this function are weighted. Values closeto one are given weights of 200, lower values a weight of 1. This prevents the sides of thebowls from collapsing under smoothing.A one dimensional cubic spline is used in locating the doorways. These are found bysteepest descent on the value of the di erential along the body of the snake. This di erentialcontains many local minima not associated with doorways. These arise either from theinherent noise in the process or from errors of t in the snake. The aim is to remove theones not associated with doorways by smoothing and thresholding. This is achieved by rst sampling the gradient at points along the snake. The values are then normalized to liebetween zero and one. The spline has an !t of 0.15 (!s of 0.0). Here a weighted least meansquares t is used. The weighting function is the inverse square of the values, preventingthe spline from being overwhelmed by large values. Starting points for steepest descent arechanges in the sign of the coe cients of the gradient of the spline. The initial step sizeis set to slightly larger than a knot spacing and then decreased over time. When a localminimum is found if the value exceeds a threshold (of 0.5), it is rejected.To represent the snake, the model of the spline must be changed somewhat. The snakeitself is a one dimensional cubic spline. But the energy minimum that is being sought is100\nAccelerating Reinforcement Learningin the di erential of the Qmax function, subject to other constraints. The dynamics of thesnake are de ned by the Euler-Langrange equation shown in Equation 8. @2f\u0302@t2 + @f\u0302@t + @@t @@s2 !c(s)@2f\u0302@s2 !!+ @@s2 !tp(s)@2f\u0302@s2 ! = F (f\u0302) (8)An !c of 512 minimizes changes to the snake's shape as it grows, by penalizing thedi erence in the second di erential to the previous time step scaled by the ratio of theirlengths. An !s of 8.0 is the initial sti ness of the snake. This is reduced proportionately tothe snake's length to give the spline more degrees of freedom. A of 96 and a of 96 controlthe momentum and the drag on the snake respectively. As in Cohen and Cohen (1993), afactor is added to the energy associated with the di erential that is in the direction normalto the body of the snake, as shown in Equation 9. But instead of it being a constant, avariable is used to produce the mercury model discussed in Section 3.2.1.F (f\u0302) =M(f\u0302) !n (s) +r( rQmax(f\u0302) 2) !n (s) (9)The energy minimization process is carried out iteratively interleaving steps for the xand y directions. The di erential of rjQmaxj2 for the x direction is given by Equation 10,a similar equation is used for the y direction. @ jrQmaxj2@x = 2 \"(@Qmax@x )(@2Qmax@x2 ) + (@Qmax@y )(@2Qmax@x@y )# (10)The snake grows under the forces of the mercury model until it reaches an approximatelystable position, subject only to small oscillations. It is then converted into a polygon by nding the corners (where the normal passes through (2n+1) 4 where n = 0 : : : 3). Thecoe cient !1 is set to zero everywhere. The coe cient !2 is set to zero at the corners and15 between them. This produces a polygon which is exible at its vertices.To detect the features as early as possible in the learning process, as discussed in Section2.4, the height of the gradient is scaled according to the signal to noise ratio. The noisearises from variations in the low level learning process and the stochastic nature of the task.Both the size of the features and the noise grow with time and are somewhat normalized bythis scaling process. The idea is to collect uniformly sampled values of the function shownin Equation 10 for both the x and y directions and nd the median of their absolute values.The median is not strongly a ected by extreme values and thus largely ignores the size ofthe features, measuring only the noise of the regions in between.ReferencesChin, C. H., & Dyer, C. R. (1986). Model-based recognition in robot vision. ComputingSurveys, 18 (1), 67{108.Christiansen, A. D. (1992). Learning to predict in uncertain continuous tasks. In Proceedingsof the Ninth International Workshop on Machine Learning, pp. 72{81.101\nDrummondCohen, L. D., & Cohen, I. (1993). Finite element methods for active contour models andballoons for 2-d and 3-d images. IEEE Transactions On Pattern Analysis And MachineIntelligence, 15 (11), 1131{1147.Dijkstra, E. W. (1959). A note on two problems in connexion with graphs. NumerischeMathematik, 1, 269{271.Drummond, C. (1996). Preventing overshoot of splines with application to reinforcementlearning. Computer science technical report TR-96-05, School of Information Tech-nology and Engineering, University of Ottawa, Ottawa, Ontario, Canada.Drummond, C. (1997). Using a case-base of surfaces to speed-up reinforcement learning.In Proceedings of the Second International Conference on Case-Based Reasoning, Vol.1266 of LNAI, pp. 435{444.Drummond, C. (1998). Composing functions to speed up reinforcement learning in a chang-ing world. In Proceedings of the Tenth European Conference on Machine Learning,Vol. 1398 of LNAI, pp. 370{381.Drummond, C. (1999). A Symbol's Role in Learning Low Level Control Functions. Ph.D.thesis, School of Information Technology and Engineering, University of Ottawa, Ot-tawa, Ontario, Canada.Galil, Z. (1986). E cient algorithms for nding maximum matching in graphs. ACMComputing Surveys, 18 (1), 23{38.Gold, S., & Rangarajan, A. (1996). A graduated assignment algorithm for graph matching.IEEE Transactions On Pattern Analysis And Machine Intelligence, 18 (4), 377{388.Gordon, G. J. (1995). Stable function approximation in dynamic programming. In Proceed-ings of the Twelfth International Conference of Machine Learning, pp. 261{268.Gordon, G. J., & Segre, A. M. (1996). Nonparametric statistical methods for experimen-tal evaluations of speedup learning. In Proceedings of the Thirteenth InternationalConference of Machine Learning, pp. 200{206.Hammond, K. J. (1990). Case-based planning: A framework for planning from experience.The Journal of Cognitive Science, 14 (3), 385{443.Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L. P., & Dean, T. (1998). Hierarchicalsolution for Markov decision processes using macro-actions. In Proceedings of theFourteenth Conference on Uncertainty In Arti cial Intelligence, pp. 220{229.Kass, M., Witkin, A., & Terzopoulus, D. (1987). Snakes: Active contour models. Interna-tional Journal of Computer Vision, 1, 321{331.Leroy, B., Herlin, I. L., & Cohen, L. D. (1996). Multi-resolution algorithms for activecontour models. In Proceedings of the Twelfth International Conference on Analysisand Optimization of Systems, pp. 58{65.102\nAccelerating Reinforcement LearningLeymarie, F., & Levine, M. D. (1993). Tracking deformable objects in the plane usingan active contour model. IEEE Transactions On Pattern Analysis And MachineIntelligence, 15 (6), 617{634.MacDonald, A. (1992). Graphs: Notes on symetries, imbeddings, decompositions. Tech.rep. Electrical Engineering Department TR-92-10-AJM, Brunel University, Uxbridge,Middlesex, United Kingdom.Mahadevan, S., & Connell, J. (1992). Automatic programming of behavior-based robotsusing reinforcement learning. Arti cial Intelligence, 55, 311{365.Mallat, S., & Zhong, S. (1992). Characterization of signals from multiscale edges. IEEETransactions On Pattern Analysis And Machine Intelligence, 14 (7), 710{732.Marr, D. (1982). Vision: a Computational Investigation into the Human Representationand Processing of Visual Information. W.H. Freeman.McCallum, R. A. (1995a). Instance-based state identi cation for reinforcement learning. InAdvances in Neural Information Processing Systems 7, pp. 377{384.McCallum, R. A. (1995b). Instance-based utile distinctions for reinforcement learning withhidden state. In Proceedings of the Twelfth International Conference on MachineLearning, pp. 387{395.Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning withless data and less real time. Machine Learning, 13, 103{130.Moore, A. W. (1992). Variable resolution dynamic programming: E ciently learning actionmaps in multivariate real-valued state spaces. In Proceedings of the Ninth InternationalWorkshop on Machine Learning.Nason, G. (1995). Three-dimensional projection pursuit. Tech. rep., Department of Math-ematics, University of Bristol, Bristol, United Kingdom.Osborne, H., & Bridge, D. (1997). Similarity metrics: A formal uni cation of cardinaland non-cardinal similarity measures. In Proceedings of the Second InternationalConference on Case-Based Reasoning, Vol. 1266 of LNAI, pp. 235{244.Parr, R. (1998). Flexible decomposition algorithms for weakly coupled Markov decisionproblems. In Proceedings of the Fourteenth Conference on Uncertainty In Arti cialIntelligence, pp. 422{430.Peng, J. (1995). E cient memory-based dynamic programming. In Proceedings of theTwelfth International Conference of Machine Learning, pp. 438{439.Precup, D., Sutton, R. S., & Singh, S. P. (1997). Planning with closed-loop macro actions.In Working notes of the 1997 AAAI Fall Symposium on Model-directed AutonomousSystems, pp. 70{76. 103\nDrummondPrecup, D., Sutton, R. S., & Singh, S. P. (1998). Theoretical results on reinforcementlearning with temporally abstract options. In Proceedings of the Tenth EuropeanConference on Machine Learning, Vol. 1398 of LNAI, pp. 382{393.Schnabel, J. A. (1997). Multi-Scale Active Shape Description in Medical Imaging. Ph.D.thesis, University of London, London, United Kingdom.Sheppard, J. W., & Salzberg, S. L. (1997). A teaching strategy for memory-based control.Arti cial Intelligence Review: Special Issue on Lazy Learning, 11, 343{370.Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.Machine Learning, 22, 123{158.Singh, S. P. (1992). Reinforcement learning with a hierarchy of abstract models. In Pro-ceedings of the Tenth National Conference on Arti cial Intelligence, pp. 202{207.Suetens, P., Fua, P., & Hanson, A. (1992). Computational strategies for object recognition.Computing Surveys, 24 (1), 5{61.Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting basedon approximating dynamic programming. In Proceedings of the Seventh InternationalConference on Machine Learning, pp. 216{224.Sutton, R. S. (1996). Generalization in reinforcement learning: Successful examples usingsparse coarse coding. In Advances in Neural Information Processing Systems 8, pp.1038{1044.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.Tadepalli, P., & Ok, D. (1996). Scaling up average reward reinforcement learning by approx-imating the domain models and the value function. In Proceedings of the ThirteenthInternational Conference of Machine Learning, pp. 471{479.Tanimoto, S. L. (1990). The Elements of Art cial Intelligence. W.H. Freeman.Terzopoulos, D. (1986). Regularization of inverse visual problems involving discontinuities.IEEE Transactions On Pattern Analysis And Machine Intelligence, 8 (4), 413{423.Thrun, S., & Schwartz, A. (1994). Finding structure in reinforcement learning. In Advancesin Neural Information Processing Systems 7, pp. 385{392.Veloso, M. M., & Carbonell, J. G. (1993). Derivational analogy in prodigy: Automatingcase acquisition, storage and utilization. Machine Learning, 10 (3), 249{278.Watkins, C. J., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning, 8 (3-4),279{292. 104"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": null, "creator": "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"}}}