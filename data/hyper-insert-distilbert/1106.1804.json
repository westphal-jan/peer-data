{"id": "1106.1804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "A Critical Assessment of Benchmark Comparison in Planning", "abstract": "recent trends in detailed planning research trends have led more to empirical comparison becoming often commonplace. as the field has started to quickly settle into a simplified methodology for performing such comparisons, distinguishing which for an obvious practical reasons requires running a highly subset number of planners on a subset of existing problems. shown in this paper, we characterize under the case methodology and also examine several eight implicit modeling assumptions general about the evaluation problems, planners and efficiency metrics used in many domains of these hypothetical comparisons. hence the problem assumptions assumed are : pr1 ) the performance attributes of a hierarchical general purpose planner should need not be actively penalized / falsely biased if executed on a sampling grid of problems and policy domains, pr2 ) minor syntactic differences unique in representation don't inadvertently affect performance, and pr3 ) problems but should be solvable by strips capable program planners unless they require adl. the planner assumptions made are : pl1 ) the latest version requiring of constructing a capability planner prediction is determining the best capability one to justify use, pl2 ) default simulated parameter settings requiring approximate good optimization performance, and pl3 ) recent time cut - offs don't modify unduly bias outcome. exactly the metrics assumptions adopted are : m1 ) performance degrades rather similarly so for each execution planner except when run on degraded client runtime environments ( g e. b g., machine platform ) and m2 ) reduce the number of plan steps distinguishes worse performance. we find that most points of course these institutional assumptions furthermore are not supported empirically ; most in that particular, that planners already are affected differently by following these assumptions. we might conclude itself with receiving a call to the expert community process to devote a research regarding resources contributing to improving the state theory of how the resulting practice and made especially contributions to enhancing assess the available possible benchmark problems.", "histories": [["v1", "Thu, 9 Jun 2011 13:20:39 GMT  (152kb)", "http://arxiv.org/abs/1106.1804v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["e dahlman", "a e howe"], "accepted": false, "id": "1106.1804"}, "pdf": {"name": "1106.1804.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Adele E. Howe", "Eric Dahlman"], "emails": ["howe@cs.colostate.edu", "dahlman@cs.colostate.edu"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 17 (2002) 1-33 Submitted 8/01; published 7/02 A Critical Assessment ofBenchmark Comparison in PlanningAdele E. Howe howe@cs.colostate.eduEric Dahlman dahlman@cs.colostate.eduComputer Science DepartmentColorado State University, Fort Collins, CO 80523AbstractRecent trends in planning research have led to empirical comparison becoming com-monplace. The eld has started to settle into a methodology for such comparisons, whichfor obvious practical reasons requires running a subset of planners on a subset of problems.In this paper, we characterize the methodology and examine eight implicit assumptionsabout the problems, planners and metrics used in many of these comparisons. The prob-lem assumptions are: PR1) the performance of a general purpose planner should not bepenalized/biased if executed on a sampling of problems and domains, PR2) minor syntacticdi erences in representation do not a ect performance, and PR3) problems should be solv-able by STRIPS capable planners unless they require ADL. The planner assumptions are:PL1) the latest version of a planner is the best one to use, PL2) default parameter settingsapproximate good performance, and PL3) time cut-o s do not unduly bias outcome. Themetrics assumptions are: M1) performance degrades similarly for each planner when runon degraded runtime environments (e.g., machine platform) and M2) the number of plansteps distinguishes performance. We nd that most of these assumptions are not supportedempirically; in particular, that planners are a ected di erently by these assumptions. Weconclude with a call to the community to devote research resources to improving the stateof the practice and especially to enhancing the available benchmark problems.1. IntroductionIn recent years, comparative evaluation has become increasingly common for demonstratingthe capabilities of new planners. Planners are now being directly compared on the sameproblems taken from a set of domains. As a result, recent advances in planning havetranslated to dramatic increases in the size of the problems that can be solved (Weld,1999), and empirical comparison has highlighted those improvements.Comparative evaluation in planning has been signi cantly in uenced and expedited bythe Arti cial Intelligence Planning and Scheduling (AIPS) conference competitions. Thesecompetitions have had the dual e ect of highlighting progress in the eld and providinga relatively unbiased comparison of state-of-the-art planners. When individual researcherscompare their planners to others, they include fewer other planners and fewer test problemsbecause of time constraints.To support the rst competition in 1998 (McDermott, 2000), Drew McDermott de ned,with contributions from the organizing committee, a shared problem/domain de nitionlanguage, PDDL (McDermott et al., 1998) (Planning Domain De nition Language). Usingc 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nHowe & Dahlmana common language means that planners' performance can be directly compared, withoutentailing hand translation or factoring in di erent representational capabilities.As a second bene t, the lack of translation (or at least human accomplished trans-lation) meant that performance could be compared on a large number of problems anddomains1. In fact, the ve competition planners were given a large number of problems(170 problems for the ADL track and 165 for the STRIPS track) within seven domains,including one domain that the planner developers had never seen prior to the competition.So the rst competition generated a large collection of benchmarks: seven domains used inthe competition plus 21 more that were considered for use. All 28 domains are availableat ftp://ftp.cs.yale.edu/pub/mcdermott/domains/. The second competition added threenovel domains to that set.A third major bene t of the competitions is that they appear to have motivated re-searchers to develop systems that others can use. The number of entrants went from ve inthe rst competition to 16 in the second. Additionally, all of the 1998 competitors and sixout of sixteen of the 2000 competitors made their code available on web sites. Thus, otherscan perform their own comparisons.In this paper, we describe the current practice of comparative evaluation as it has evolvedsince the AIPS competitions and critically examine some of the underlying assumptionsof that practice. We summarize existing evidence about the assumptions and describeexperimental tests of others that had not previously been considered. The assumptionsare organized into three groups concerning critical decisions in the experiment design: theproblems tested, the planners included and the performance metrics collected.Comparisons (as part of competitions or by speci c researchers) have proven to be enor-mously useful to motivating progress in the eld. Our goal is to understand the assumptionsso that readers know how far the comparative results can be generalized. In contrast to thecompetitions, the community cannot legislate fairness in individual researcher's compara-tive evaluations, but readers may be able to identify cases in which results should be viewedeither skeptically or with con dence. Thus, we conclude the paper with some observationsand a call for considerably more research into new problems, metrics and methodologies tosupport planner evaluation.Also in contrast to the competitions, our goal is not to declare a winner. Our goal isalso not to critique individual studies. Consequently, to draw attention away from such apossible interpretation, whenever possible, we report all results using letter designators thatwere assigned randomly to the planners.2. Planning Competitions and Other Direct ComparisonsRecently, the AIPS competitions have spurred considerable interest in comparative evalua-tion. The roots of comparative planner evaluation go back considerably further, however.Although few researchers were able to run side-by-side comparisons of their planners with1. To solve a particular planning problem (i.e., construct a sequence of actions to transform an initial state toa goal state), planners require a domain theory and a problem description. The domain theory representsthe abstract actions that can be executed in the environment; typically, the domain descriptions includevariables that can be instantiated to speci c objects or values. Multiple problems can be de ned foreach domain; problem descriptions require an initial state description, a goal state and an associationwith some domain. 2\nA Critical Assessment of Benchmark Comparison in Planningothers, they were able to demonstrate performance of their planner on well-known prob-lems, which could be viewed as de facto benchmarks. Sussman's anomaly (Sussman, 1973)in Blocksworld was the premier planning benchmark problem and domain for many years;every planner needed to \\cut its teeth\" on it.As researchers tired of Blocksworld, many called for additional benchmark problemsand environments. Mark Drummond, Leslie Kaelbling and Stanley Rosenschein organizeda workshop on benchmarks and metrics (Drummond, Kaelbling, & Rosenschein, 1990).Testbed environments, such as Martha Pollack's TileWorld (Pollack & Ringuette, 1990) orSteve Hanks's TruckWorld (Hanks, Nguyen, & Thomas, 1993), were used for comparingalgorithms within planners. By 1992, UCPOP (Penberthy & Weld, 1992) was distributedwith a large set of problems (117 problems in 21 domains) for demonstration purposes. In1995, Barry Fox and Mark Ringer set up a planning and scheduling benchmarks web page(http://www.newosoft.com/~benchmrx/) to collect problem de nitions, with an emphasison manufacturing applications. Recently, PLANET (a coordinating organization for Euro-pean planning and scheduling researchers) has proposed a planning benchmark collectioninitiative (http://planet.dfki.de).Clearly, benchmark problems have become well-established means for demonstratingplanner performance. However, the practice has known bene ts and pitfalls; Hanks, Pollackand Cohen (1994) discuss them in some detail in the context of agent architecture design.The bene ts include providing metrics for comparison and supporting experimental control.The pitfalls include a lack of generality in the results and a potential for the benchmarks tounduly in uence the next generation of solutions. In other words, researchers will constructsolutions to excel on the benchmarks, regardless of whether the benchmarks accuratelyrepresent desired real applications.To obtain the bene ts just listed for benchmarks, the problems often are idealized orsimpli ed versions of real problems. As Cohen (1991) points out , most research papers inAI, or at least at an AAAI conference, exploit benchmark problems; yet few of them relatethe benchmarks to target tasks. This may be a signi cant problem; for example, in a studyof owshop scheduling2 benchmarks, we found that performance on the standard bench-mark set did not generalize to performance on problems with realistic structure (Watson,Barbulescu, Howe, & Whitley, 1999). A study of just Blocksworld problems found that thebest known Blocksworld benchmark problems are atypical in that they require only shortplans for solution and optimal solutions are easy to nd (Slaney & Thiebaux, 2001).In spite of these di culties, benchmark problems and the AIPS competitions have con-siderably in uenced comparative planner evaluations. For example, in the AIPS 2000 con-ference proceedings (Chien, Kambhampati, & Knoblock, 2000), all of the papers on im-provements to classical planning (12 out of 44 papers at the conference) relied heavily oncomparative evaluation using benchmark problems; the other papers concerned scheduling,speci c applications, theoretical analyses or special extensions to the standard paradigm(e.g., POMDP, sensing). Of the 12 classical papers, six used problems from the AIPS98competition benchmark set, six used problems from Kautz and Selman's distribution ofproblems with blackbox (Kautz, 2002) and three added some of their own problems aswell. Each paper showed results on a subset of problems from the benchmark distributions2. Scheduling is an area related to planning in which the actions are already known, but their sequence stillneeds to be determined. Flowshop scheduling is a type of manufacturing scheduling problem.3\nHowe & Dahlman(e.g., Drew McDermott's from the rst competition) with logistics, blocksworld, rocket andgripper domains being most popular (used in 11, 7, 5 and 5 papers, respectively). The avail-ability of planners from the competition was also exploited; eight of the papers comparedtheir systems to other AIPS98 planners: blackbox, STAN, IPP and HSP (in 5, 3, 3 and 1papers, respectively).3. Assumptions of Direct ComparisonA canonical planner evaluation experiment follows the procedure in Table 1. The procedureis designed to compare performance of a new planner to the previous state of the art andhighlight superior performance in some set of cases for the new planner. The exact formof an experiment depends on its purpose, e.g., showing superiority on a class of problem orhighlighting the e ect of some design decision.1. Select and/or construct a subset of planner domains2. Construct problem set by: running large set of benchmark problems selecting problems with desirable features varying some facet of the problem to increase di culty (e.g., number of blocks)3. Select other planners that are: representative of the state of the art on the problems OR similar to or distinct from the new planner, depending on the point of the com-parison or advance of the new planner OR available and able to parse the problems4. Run all problems on all planners using default parameters and setting an upper limiton time allowed5. Record which problems were solved, how many plan steps/actions were in the solutionand how much CPU time was required to either solve the problem, fail or time outTable 1: Canonical comparative planner evaluation experiment.The protocol depends on three selections: problems, planners and evaluation metrics.It is simply not practical or even desirable to run all available planners on all availableproblems. Thus, one needs to make informed decisions about which to select. A purposeof this paper is to examine the assumptions underlying these decisions to help make themmore informed. Every planner comparison does not adopt every one of these assumptions,but the assumptions are ones commonly found in planner comparisons. For example, thosecomparisons designed for a speci c purpose (e.g., to show scale-up on certain problemsor suitability of the planner for logistics problems) will carefully select particular types ofproblems from the benchmark sets. 4\nA Critical Assessment of Benchmark Comparison in PlanningProblems Many planning systems were developed to solve a particular type of planningproblem or explore a speci c type of algorithmic variation. Consequently, one would expectthem to perform better on the problems on which and for which they were developed. Evenwere they not designed for a speci c purpose, the test set used during development may havesubtly biased the development. The community knows that planner performance dependson problem features, but not in general, how, when and why. Researchers tend to designplanners to be general purpose. Consequently, comparisons assume thatthe performance of a general-purpose planner should not be penalized/biased ifexecuted on a sampling of problems and domains (problem assumption 1).The community also knows that problem representation in uences planner performance.For example, benchmark problem sets include many versions of Blocksworld problems, de-signed by di erent planner developers. These versions vary in their problem representation,both minor apparently syntactic changes (e.g., how clauses are ordered within operators,initial conditions and goals, and whether any information is extraneous) and changes re- ecting addition of domain knowledge (e.g., what constraints are included and whethervariables are typed). Consequently, comparisons assume thatsyntactic representational modi cations either do not matter or a ect each plan-ner equally (problem assumption 2).PDDL includes a eld, :requirements, for the capabilities required of a planner to solvethe problem. PDDL1.0 de ned 21 values for the :requirements eld; the base/default re-quirement is :strips, meaning STRIPS derived add and delete sets for action e ects. :adl(from Pednault's Action Description Language) requires variable typing, disjunctive pre-conditions, equality as a built-in predicate, quanti ed preconditions and conditional e ectsin addition the :strips capability. Yet, many planners either ignore the :requirements eld or reject the problem only if it speci es :adl (ignoring many of the other requirementsthat could also cause trouble). Thus, comparisons assume thatproblems in the benchmark set should be solvable by a STRIPS planner unlessthey require :adl (problem assumption 3).Planners The wonderful trend of making planners publicly available has led to a dilemmain determining which to use and how to con gure them. The problem is compounded by thelongevity of some of these planner projects; some projects have produced multiple versions.Consequently, comparisons tend to assume thatthe latest version of the planner is the best (planner assumption 1).These planners may also include parameters. For example, the blackbox planner allows theuser to de ne a strategy for applying di erent solution methods. Researchers expect thatparameters a ect performance. Consequently, comparisons assume thatdefault parameter settings approximate good performance (planner assumption2). 5\nHowe & DahlmanExperiments invariably use time cut-o s for concluding planning that has not yet founda solution or declared failure. Many planners would need to exhaustively search a large spaceto declare failure. For practical reasons, a time out threshold is set to determine when tohalt a planner, with a failure declared when the time-out is reached. Thus, comparisonsassume thatif one picks a su ciently high time-out threshold, then it is highly unlikely thata solution would have been found had slightly more time been granted (plannerassumption 3).Metrics Ideally, performance would be measured based on how well the planner doesits job (i.e., constructing the `best' possible plan to solve the problem) and how e cientlyit does so. Because no planner has been shown to solve all possible problems, the basicmetric for performance is the number or percentage of problems actually solved within theallowed time. This metric is commonly reported in the competitions. However, researchpapers tend not to report it directly because they typically test a relatively small numberof problems.E ciency is clearly a function of memory and e ort. Memory size is limited by thehardware. E ort is measured as CPU time, preferably but not always on the same platformin the same language. The problems with CPU time are well known: programmer skillvaries; research code is designed more for fast prototyping than fast execution; numbers inthe literature cannot be compared to newer numbers due to processor speed improvements.However, if CPU times are regenerated in the experimenter's environment then one assumesthat performance degrades similarly with reductions in capabilities of the runtimeenvironment (e.g., CPU speed, memory size) (metric assumption 1).In other words, an experimenter or user of the system does not expect that code has beenoptimized for a particular compiler/operating system/hardware con guration, but it shouldperform similarly when moved to another compatible environment.The most commonly reported comparison metric is computation time. The second mostis number of steps or actions (for planners that allow parallel execution) in a plan. Althoughplanning seeks solutions to achieving goals, the goals are de ned in terms of states of theworld, which does not lend itself well to general measures of quality. In fact, quality is likelyto be problem dependent (e.g., resource cost, amount of time to execute, robustness), whichis why number of plan steps has been favored. Comparisons assume thatnumber of steps in a resulting plan varies between planner solutions and approx-imates quality (metric assumption 2).Any comparison, competitions especially, has the unenviable task of determining how totrade-o or combine the three metrics (number solved, time, and number of steps). Thus,if number of steps does not matter, then the comparison could be simpli ed.We converted each assumption into a testable question. We then either summarized theliterature on the question or ran an experiment to test it.6\nA Critical Assessment of Benchmark Comparison in Planning3.1 Our Experimental SetupSome of the key issues have been examined previously, directly or indirectly. For those,we simply summarize the results in the subsections that follow. However, some are openquestions. For those, we ran seven well known planners on a large set of 2057 benchmarkproblems. The planners all accept the PDDL representation, although some have built-intranslators for PDDL to their internal representation and others rely on translators that weadded. When several versions of a planner were available, we included them all (for a totalof 13 planners). The basic problem set comprises the UCPOP benchmarks, the AIPS98 and2000 competition test sets and an additional problem set developed for a speci c application.With the exception of the permuted problems (see the section on Problem Assumption2 for speci cs), the problems were run on 440 MHz Ultrasparc 10s with 256 Megabytesof memory running SunOS 2.8. Whenever possible, versions compiled by the developerswere used; when only source code was available, we compiled the systems according to thedevelopers' instructions. The planners written in Common Lisp were run under AllegroCommon Lisp version 5.0.1. The other planners were compiled with GCC (EGCS version2.91.66). Each planner was given a 30 minute limit of wall clock time3 to nd a solution;however, all times reported are run times returned by the operating system.3.1.1 PlannersThe planners are all what have been called primitive-action planners (Wilkins & desJardins,2001), planners that require relatively limited domain knowledge and construct plans fromsimple action descriptions. Because the AIPS98 competition required planners to acceptPDDL, the majority of planners used in this study were competition entrants or are laterversions thereof 4. The common language facilitated comparison between the planners with-out having to address the e ects of a translation step. The two exceptions were UCPOP andProdigy; however, their representations are similar to PDDL and were translated automat-ically. The planners represent ve di erent approaches to planning: plan graph analysis,planning as satis ability, planning as heuristic search, state-space planning with learningand partial order planning. When possible, we used multiple versions of a planner, and notnecessarily the most recent. Because we conducted this study over some period of time (al-most 1.5 years), we froze the set early on; we are not comparing the performance to declarea winner and so did not think that the lack of recent versions undermined the results oftesting our assumptions.IPP (Koehler, Nebel, Ho mann, & Dimopoulos, 1997) extends the Graphplan (Blum &Furst, 1997) algorithm to accept a richer plan description language. In its early versions,this language was a subset of ADL that extends the STRIPS formalism of Graphplanto allow for conditional and universally quanti ed e ects in operators. Until version 4.0,negation was handled via the introduction of new predicates for the negated preconditions3. We used actual time on lightly loaded machines because occasionally a system would thrash due toinadequate memory resulting in little progress over considerable time.4. We used the BUS system as the manager for running the planners (Howe, Dahlman, Hansen, Scheetz, &von Mayrhauser, 1999), which was implemented with the AIPS98 competition planners. This facilitatedthe running of so many di erent planners, but did somewhat bias what was included.7\nHowe & Dahlmanand corresponding mutual exclusion rules; subsequent versions handle it directly (Koehler,1999). We used the AIPS98 version of IPP as well as the later 4.0 version.SGP (Sensory Graph Plan) (Weld, Anderson, & Smith, 1998) also extends Graphplan toa richer domain description language, primarily focusing on uncertainty and sensing. Aswith IPP, some of this transformation is performed using expansion techniques to removequanti cation. SGP also directly supports negated preconditions and conditional e ects.SGP tends to be slower (it is implemented in Common Lisp instead of C) than some of theother Graphplan based planners. We used SGP version 1.0b.STAN (STate ANalysis) (Fox & Long, 1999) extends the Graphplan algorithm in part byadding a preprocessor (called TIM) to infer type information about the problem and domain.This information is then used within the planning algorithm to reduce the size of the searchspace that the Graphplan algorithm would search. STAN also incorporated optimized datastructures (bit vectors of the planning graph) that help avoid many of the redundant calcu-lations performed by Graphplan. Additionally, STAN maintains a wave front during graphconstruction to track remaining goals and so limit graph construction. Subsequent versionsincorporated further analyses (e.g., symmetry exploitation) and an additional simpler plan-ning engine. Four versions of STAN were tested: the AIPS98 competition version, version3.0, version 3.0s and a development snapshot of version 4.0.blackbox (Kautz & Selman, 1998) converts planning problems into Boolean satis abilityproblems, which are then solved using a variety of di erent techniques. The user indicateswhich techniques should be tried in what order. In constructing the satis ability problem,blackbox uses the planning graph constructed as in Graphplan. For blackbox, we usedversion 2.5 and version 3.6b.HSP (Heuristic Search Planner) (Bonet & Ge ner, 1999) is based on heuristic search. Theplanner uses a variation of hill-climbing with random restarts to solve planning problems.The heuristic is based on using the Graphplan algorithm to solve a relaxed form of theplanning problem. In this study, we used version 1.1, which is an algorithmic re nement ofthe version entered into the AIPS98 competition, and version 2.0.Prodigy 5 (The Prodigy Research Group, 1992) combines state-space planning with back-ward chaining from the goal state. A plan under construction consists of a head-plan oftotally ordered actions starting from the initial state and a tail-plan of partially orderedactions related to the goal state. Although not o cially entered into the competition, in-formal results presented at the AIPS98 competition suggested that Prodigy performed wellin comparison to the entrants. We used Prodigy version 4.0.UCPOP (Barrett, Golden, Penberthy, & Weld, 1993) is a Partial Order Causal Linkplanner. The decision to include UCPOP was based on several factors. First, it doesnot expand quanti ers and negated preconditions; for some domains, the expansion fromgrounding operators can be so great as to make the problem insolvable. Second, UCPOPis based on a signi cantly di erent algorithm in which interest has recently resurfaced. Weused UCPOP version 4.1.5. We thank Eugene Fink for code that translates PDDL to Prodigy.8\nA Critical Assessment of Benchmark Comparison in PlanningSource # of Domains # of ProblemsBenchmarks 50 293AIPS 1998 6 202AIPS 2000 5 892Developers 1 13Application 3 72Table 2: Summary of problems in our testing set: source of the problems, the number ofdomains and problems within those domains.3.1.2 Test ProblemsFollowing standard practice, our experiments require planners to solve commonly availablebenchmark problems and the AIPS competition problems. In addition, to test our assump-tions about the in uence of domains (assumption PR1) and representations of problems(assumption PR2), we will also include permuted benchmark problems and some other ap-plication problems. This section describes the set of problems and domains in our study,focusing on their source and composition.The problems require only STRIPS capabilities (i.e., add and delete lists). We chose thisleast common denominator for several reasons. First, more capable planners can still handleSTRIPS requirements; thus, this maximized the number of planners that could be includedin our experiment. Also, not surprisingly, more problems of this type are available. Second,we are examining assumptions of evaluation, including the e ect of required capabilities onperformance. We do not propose to duplicate the e ort of the competitions in singling outplanners for distinction, but rather, our purpose is to determine what factors di erentiallya ect planners.The bulk of the problems came from the AIPS98 and AIPS 2000 problem sets andthe set of problems distributed with the PDDL speci cation. The remaining problemswere solicited from several sources. The source and counts of problems and domains aresummarized in Table 2.Benchmark Problems The preponderance of problems in planning test sets are \\toyproblems\": well-known synthetic problems designed to test some attribute of planners.The Blocksworld domain has long been included in any evaluation because it is well known,can have subgoal interactions and supports constructing increasingly complex problems(e.g., towers of more blocks). A few benchmark problems are simpli ed versions of realisticplanning problems, e.g., the at tire, refrigerator repair or logistics domains. We used theset included with the UCPOP planner. These problems were contributed by a large numberof people and include multiple encodings of some problems/domains, especially Blocksworld.AIPS Competitions: 1998 and 2000 For the rst AIPS competition, Drew McDer-mott solicited problems from the competitors as well as constructing some of his own, suchas the mystery domain, which had semantically useless names for objects and operators.Problems were generated for each domain automatically. The competition included 155problems from six domains: robot movement in a grid, gripper in which balls had to be9\nHowe & Dahlmanmoved between rooms by a robot with two grippers, logistics of transporting packages, orga-nizing snacks for movie watching, and two mystery domains, which were disguised logisticsproblems.The format of the 1998 competition required entrants to execute 140 problems in the rst round. Of these problems, 52 could not be solved by any planner. For round two, theplanners executed 15 new problems in three domains, one of which had not been includedin the rst round.The 2000 competition attracted 15 competitors in three tracks: STRIPS, ADL anda hand-tailored track. It required performance on problems in ve domains: logistics,Blocksworld, parts machining, Freecell (a card game), and Miconic-10 elevator control.These domains were determined by the organizing committee, with Fahiem Bacchus as thechair, and represented a somewhat broader range. We chose problems from the UntypedSTRIPS track for our set.From a scienti c standpoint, one of the most interesting conclusions of both competi-tions was the observed trade-o s in performance. Planners appeared to excel on di erentproblems, either solving more from a set or nding a solution faster. In 1998, IPP solvedmore problems and found shorter plans in round two; STAN solved its problems the fastest;HSP solved the most problems in round one; and blackbox solved its problems the fastestin round one. In 2000, awards were given to two groups of distinguished planners acrossthe di erent categories of planners (STRIPS, ADL and hand tailored), because accordingto the judges, \\it was impossible to say that any one planner was the best\"(Bacchus, 2000);TalPlanner and FF were in the highest distinguished planner group. The graphs of perfor-mance do show di erences in computation time relative to other planners and to problemscale-up. However, each planner failed to solve some problems, which makes these trendsharder to interpret (the computation time graphs have gaps).The purpose of these competitions was to showcase planner technology at which theysucceeded admirably. The planners solved much harder problems than could have beenaccomplished in years past. Because of this trend in planners handling increasingly di cultproblems, the competition test sets may become of historical interest for tracking the eld'sprogress.Problems Solicited from Planner Developers We also asked planner developers whatproblems she had used during development. One developer, Maria Fox, sent us a domain(Sodor, which is a logistics application) and set of problems that they had used. We wouldhave included other domains and problems had we received any others.Other Applications The Miconic elevator domain from the AIPS2000 competition wasderived from an actual planning application. The domain and problems were extremelysimpli ed (e.g., removing the arithmetic).To add another realistic problem to the comparison, we included one other planning ap-plication to the set of test domains: generating cases to test a software interface. Becauseof the similarities between software interface test cases and plans, we developed a system,several years ago, for automatically generating interface test cases using an AI planner.The system was designed to generate test cases for the user interface to Storage Technol-ogy's robot tape library (Howe, von Mayrhauser, & Mraz, 1997). The interface (i.e., thecommands in the interface) was coded as the domain theory. For example, the mount com-10\nA Critical Assessment of Benchmark Comparison in Planningmand/action's description required that a drive be empty and had the e ect of changingthe position of the tape being mounted and changing the status of the tape drive. Problemsdescribed initial states of the tape library (e.g., where tapes were resident, what was thestatus of the devices and software controller) and goal states that a human operator mightwish to achieve.At the time, we found that only the simplest problems could be generated using theplanners available. We included this application in part because we knew it would be achallenge. As part of the test set, we include three domain theories (di erent ways ofcoding the application involving 8-11 operators) and twenty-four problems for each domain.We included only 24 because we wanted to include enough problems to see some e ect, butnot too many to overly bias the results. These problems were relatively simple, requiringthe movement of no more than one tape coupled with some status changes, but they werestill more di cult than could be solved in our original system.3.2 Problem AssumptionsGeneral-purpose planners exhibit di erential capabilities on domains and sometimes evenproblems within a domain. Thus, the selection of problem set would seem to be criticalto evaluation. For example, many problems in benchmark sets are variants of logisticsproblems; thus, a general-purpose planner that was actually tailored for logistics may appearto be better overall on current benchmarks. In this section, we will empirically examinesome possible problem set factors that may in uence performance results.Problem Assumption 1: To What Extent Is Performance of General PurposePlanners Biased Toward Particular Problems/Domains? Although most plannersare developed as general purpose, the competitions and previous studies have shown thatplanners excel on di erent domains/problems. Unfortunately, the community does not yethave a good understanding of why a planner does well on a particular domain. We studiedthe impact of problem selection on performance in two ways.First, we assessed whether performance might be positively biased toward problemstested during development. Each developer6 was asked to indicate which domains they usedduring development. We then compared each planner's performance on their developmentproblems (i.e., the development set) to the problems remaining in the complete test set(rest). We ran 2x2 2 tests comparing number of problems solved versus failed in thedevelopment and test sets. We included only the number solved and failed in the analysisas timed-out problems made no di erence to the results7.The results of this analysis are summarized in Table 3; Figure 1 graphically displays theratio of successes to failures for the development and other problems. All of the plannersexcept C performed signi cantly better on their development problems. This suggests thatthese planners have been tailored (intentionally or not) for particular types of problems andthat they will tend to do better on test sets biased accordingly. For example, one of the6. We decided against studying some of the planners in this way because the representations for theirdevelopment problems were not PDDL.7. One planner was the exception to this rule; in one case, the planner timed out far more frequently onnon-development problems. 11\nHowe & DahlmanDevelopment RestPlanner Sol. Fail Sol. Fail 2 PA 48 56 207 1026 51.70 0.001B 42 34 226 929 51.27 0.001C 30 0 549 16 0.13 0.722G 43 35 233 924 49.56 0.001H 52 9 234 655 91.41 0.001I 113 20 328 920 187.72 0.001J 114 24 388 949 157.62 0.001K 37 56 203 987 27.82 0.001L 63 32 358 846 52.13 0.001Table 3: 2 results comparing outcome on development versus other problems.planners in our set, STAN, was designed with an emphasis on logistics problems (Fox &Long, 1999).\nFigure 1: Histogram of ratios of success/failures for development and other problems foreach of the planners.The above analysis introduces a variety of biases. The developers tended to give us shortlists that probably were not really representative of what they actually used. The set usedis a moving target, rather than stationary as this suggests. The set of problems includedin experimentation for publication may be di erent still. Consequently, for the secondpart, we broadened the question to determine the e ect of di erent subsets of problems on12\nA Critical Assessment of Benchmark Comparison in PlanningRank Dominancen 0 1 2 3 4 5 6 7 8 9 10 Total Pairs5 0 1 2 0 5 7 10 4 10 18 21 7810 0 3 0 0 4 10 6 7 5 23 20 7820 0 0 0 0 1 3 8 7 11 8 40 7830 0 0 0 0 1 1 9 6 9 8 44 78Table 4: Rank dominance counts for 10 samples of domains with domain sizes (n) of vethrough 30.performance. For each of 10 trials, we randomly selected n domains (and their companionproblems) to form the problem set. We counted how many of these problems could besolved by each planner and then ranked the relative performance of each planner. Thus,for each value of n, we obtained 10 planner rankings. We focused on rankings of problemssolved for two reasons: First, each domain includes a di erent number of problems, makingthe count of problems variable across each of the trials. Second, relative ranking gets to theheart of whether one planner might be considered to be an improvement over another.We tested values of 5, 10, 20 and 30 for n (30 is half of the domains at our disposal).To give a sense of the variability in size, at n = 5, the most problems solved in a trialvaried from 11 to 64. To assess the changes in rankings across the trials, we computed rankdominance for all pairs of planners; rank dominance is de ned as the number of trials inwhich planner x's rank was lower than planner y's (note: ties would count toward neitherplanner). The 13 planners in our study resulted in 78 dominance pairings. If the relativeranking between two planners is stable, then one would expect one to always dominate theother, i.e., have rank dominance of 10.Table 4 shows the number of pairs having each value (0-10) of rank dominance for thefour values for n. For a given pair, we used the highest number as the rank dominance forthe pair, e.g., if one always has a lower rank, then the pair's rank dominance is 10 or ifboth have ve, then it is ve. Because of ties, the maximum can be less than ve. Thedata suggest that even when picking half of the domains, the rankings are not completelystable: in 56% of the pairings, one always dominates, but 22% have a 0.3 or greater chanceof switching relative ranking. The values degrade as n decreases with only 27% alwaysdominating for n = 5.Problem Assumption 2: How Do Syntactic Representation Di erences A ectPerformance? Although it is well known that some planners' performance depends onrepresentation (Joslin & Pollack, 1994; Srinivasan & Howe, 1995), two recent developmentsin planner research suggest that the e ect needs to be better understood. First, a commonrepresentation, i.e., PDDL, may bias performance. Some planners rely on a pre-processingstep to convert PDDL to their native representation, a step that usually requires makingarbitrary choices about ordering and coding. Second, an advantage of planners based onGraphplan is that they are supposed to be less vulnerable to minor changes in representa-13\nHowe & DahlmanPlanner All None SubsetA 65 315 30B 70 295 45C 318 74 18D 202 169 39E 111 132 167F 112 138 160G 70 295 45H 91 290 29I 109 134 167J 150 124 136K 60 305 45L 112 284 14M 212 148 50Table 5: The number of problems for which the planners were able to solve all, none oronly a subset of the permutations.tion. Although the reasoning for the claim is sound, the exigencies of implementation mayrequire re-introduction of representation sensitivity.To evaluate the sensitivity to representation, ten permutations of each problem in theAIPS2000 set were generated, resulting in 4510 permuted problems. The permutations wereconstructed by randomly reordering the preconditions in the operator de nitions and theorder of the de nitions of the operators within the domain de nition.We limited the number of problems in this study because ten permutations of all prob-lems would be prohibitive. We selected the AIPS2000 problems for attention because thiswas the most recently developed benchmark set. Even within that set, not all of the domainswere permuted because some would not result in di erent domains under the transforma-tion we used. For the purposes of this investigation, we limited the set of modi cations topermutations of preconditions and operators because these were known to a ect some plan-ners and because practical considerations limited the number of permutations that could beexecuted. Finally, for expediency, we ran the permutations on a smaller number of fasterplatforms because it expedited throughput and computation time was not a factor in thisstudy.To analyze the data, we divided the performance on the permutations of the problemsinto three groups based on whether the planner was able to solve all of the permutations,none of the permutations or only a subset of the permutations. If a planner is insensitive tothe minor representational changes, then the subset count should be zero. From the resultsin Table 5, we can see that all of the planners were a ected by the permutation operation.The susceptibility to permuting the problem was strongly planner dependent ( 2 = 1572:16,P < 0:0001), demonstrating that some planners are more vulnerable than others.By examining the number in the Subset column, one can assess the degree of suscepti-bility. All of the planners were sensitive to reorderings, even those that relied on Graphplan14\nA Critical Assessment of Benchmark Comparison in PlanningPlanner FeatureAxioms Cond.E . Dis.Pre. Equality 9Pre. Safety Strips Typing 8Pre.A 0 0 0 35 0 0 255 8 0B 0 0 0 8 0 0 268 0 0C 5 169 165 216 163 2 561 197 160D 3 164 166 196 139 0 279 180 139E 1 162 152 199 157 0 384 168 149F 0 157 145 185 150 0 376 165 145G 0 0 0 8 0 0 276 0 0H 0 0 0 46 0 0 285 17 0I 0 138 138 169 138 0 441 139 138J 0 130 130 160 130 0 502 130 130K 0 0 0 8 0 0 240 0 0L 0 19 13 24 16 0 421 13 13M 0 168 169 212 149 2 372 180 151Table 6: The number of problems claiming to require each PDDL feature solved by eachplanner.methodology. The most sensitive were E, F, I and J (which included some Graphplan basedplanners and in which 40% of the problems had mixed results on the permutations) with Cand L being least sensitive (3-4% were a ected).Problem Assumption 3: Does Performance Depend on PDDL RequirementsFeatures? The planners were all intended to handle STRIPS problems. Some of theproblems in the test set claim to require features other than STRIPS; one would expectthat some of the planners would not be able to handle those problems. In addition, thoseplanners that claim to be able to handle a given feature may not do as well as other planners.Table 6 shows the e ects of feature requirements on the ability to solve problems. The datain this table are based on the features speci ed with the :requirements list in the PDDLde nition of the domain.We did not verify that the requirements were accurate or necessary; thus, the problemmay be solvable by ignoring a part of the PDDL syntax that is not understood, or theproblem may have been mislabeled by its designer. This is evident in cases where a plannerthat does not support a given feature still appears to be able to solve the correspondingproblem. Some planners, e.g., older versions of STAN, will reject any problem that requiresmore than STRIPS without trying to solve it; an ADL problem that only makes use ofSTRIPS features would not be attempted.As guidance on which planner to use when, these results must be viewed with someskepticism. For example, it would appear based on these results that planner I might be15\nHowe & Dahlmana good choice for problems with conditional e ects as it was able to solve many of theseproblems. This would be a mistake, since that planner cannot actually handle these typesof problems. In these cases, the problems claim to require ADL, but in fact, they only makeuse of the STRIPS subset.Clearly, certain problems can only be solved by speci c planners. For instance, C andM are the only planners that are able to handle safety constraints, while based on the data,only C, D and E appear to handle domain axioms. About half the planners had troublewith the typed problems. Some of the gaps appear to be due to problems in the translationto native representation.3.3 PlannersPublicly available, general-purpose planners tend to be large programs developed over aperiod of years and enhanced to include additional features over time. Thus, several versionsare likely to be available, and those versions are likely to have features that can be turnedon/o via parameter settings.When authors release later versions of their planning systems, the general assumption isthat these newer versions will outperform their predecessors. However, this may not be thecase in practice. For instance, a planner could be better optimized toward a speci c classof problem which then in turn hurts its performance on other problems. Also, advancedcapabilities, even when unused, may incur overhead in the solution of all problems.So for comparison purposes, should one use the latest version? First, we tested thisquestion in a study comparing multiple versions of four of the planners. Second, eachplanner relies on parameter settings to tune its performance. Some, such as blackbox, havemany parameters. Others have none. Comparisons tend to use the default or publishedparameter settings because few people usually understand the e ects of the parametersand tuning can be extremely time consuming. So does this practice undermine a faircomparison?Planner Assumption 1: Is the Latest Version the Best? In this study, we comparedperformance of multiple versions of four planners (labeled for this section with W, X, Y andZ, with larger version numbers indicating subsequent versions). We considered two criteriafor improvement: outcome of planning and computation time for solved problems. Theoutcome of planning is one of: solved, failed or timed-out. On each criterion, we statisticallyanalyzed the data for superior performance of one of the versions. The outcome results forall the planners are summarized in Table 7. As the table shows, rarely does a new versionresult in more problems being solved. Only Z improved the number of our test problemssolved in subsequent versions.To check for whether the di erences in outcome are signi cant, we ran 2x3 2 tests withplanner version as independent variable and outcome as dependent. Table 8 summarizesthe results of the 2 analysis. For Z, we compared each version to its successor only. Thedi erences are signi cant except for Y and the transition from Z 2 to 3 (this was expectedbecause these two versions were extremely similar).Another planner performance metric, which we evaluated, was the speed of solution. Forthis analysis, we limited the comparison to just those problems that were solved by bothversions of the planner. We then classi ed each problem by whether the later version solved16\nA Critical Assessment of Benchmark Comparison in PlanningPlanner Version Solved Failed Timeout Solved?W 1 286 664 533W 2 255 1082 147 +X 1 502 973 3X 2 441 940 103 +Y 1 387 750 339Y 2 382 771 329 +Z 1 240 1043 201Z 2 276 959 248 *Z 3 268 963 252 +Z 4 421 878 184 *Table 7: Version performance: counts of outcome and change in number solved.old newPlanner Version Version 2 PW 1 2 320.96 .0001X 1 2 98.84 .0001Y 1 2 .46 .79Z 1 2 10.96 .004Z 2 3 .158 .924Z 3 4 48.50 .0001Table 8: 2 results comparing versions of the same planner.the problem faster, slower, or in the same time as the preceding version. From the resultsin Table 9, we see that all of the planners improved in the average speed of solution forsubsequent versions, with the exception of Z (transition from the 1 to 2 versions). However,Z did increase the number of problems solved between those versions.Planner Old New Faster Slower Same TotalW 1 2 161 61 30 252X 1 2 295 126 0 421Y 1 2 222 82 53 357Z 1 2 84 121 30 235Z 2 3 131 84 53 268Z 3 4 115 92 21 228Table 9: Improvements in execution speed across versions. The Faster column counts thenumber of cases in which the new version solved the problem faster; Slower speci esthose cases in which the new version took longer to solve a given problem.17\nHowe & DahlmanPlanner Assumption 2: Do Parameter Settings Matter to a Fair Comparison?In this planner set, only three have obvious, easily manipulable parameters: Blackbox, HSPand UCPOP. blackbox has an extensive set of parameters that control everything fromhow much trace information to print to the sequence of solver applications. HSP's functioncan be varied to include (or not) loop detection, change the search heuristic and vary thenumber paths to expand. For UCPOP, the user can change the strategies governing nodeorderings and aw selection.We did not run any experiments for this assumption because not all of the plannershave parameters and because it is clear from the literature that the parameters do matter.Blackbox relies heavily on random restarts and trying alternative SAT solvers. In Kautzand Selman (1999), the authors of blackbox carefully study aspects of blackbox's design anddemonstrate di erential performance using di erent SAT solvers; they propose hypothesesfor the performance di erences and are working on better models of performance variation.At the heart of HSP is heuristic search. Thus, its performance varies depending onthe heuristics. Experiments with both HSP and FF (a planner that builds on some ideasfrom HSP) have shown the importance of heuristic selection in search space expansion,computation time and problem scale up (Haslum & Ge ner, 2000; Ho mann & Nebel,2001).As with HSP, heuristic search is critical to UCPOP's performance. A set of studies haveexplored alternative settings to the aw selection heuristics employed by UCPOP (Joslin &Pollack, 1994; Srinivasan & Howe, 1995; Genevini & Schubert, 1996), producing dramaticimprovements on some domains with some heuristics. As Pollack et al. (1997) con rmed,a good default strategy could be derived, but its performance was not the best under somecircumstances.Thus, because parameters can control fundamental aspects of algorithms, such as theirsearch strategies, the role of parameters in comparisons cannot be easily dismissed.Planner Assumption 3: Are Time Cut-o s Unfair? Planners often do not admitto failure. Instead, the planner stops when it has used the allotted time and not founda solution. So setting a time threshold is a requirement of any planner execution. Ina comparison, one might always wonder whether enough time was allotted to be fair {perhaps the solution was almost found when execution was terminated.To determine whether our cut-o of 30 minutes was fair, we examined the distributionof times for declared successes and failures8. Across the planners and the problem set, wefound that the distributions were skewed (approximately log normal with long right tails)and that the planners were quick to declare success or failure, if they were going to do so.Table 10 shows the max, mean, median and standard deviation for success and failure timesfor each of the planners. The di erences between mean and median indicate the distributionskew, as do the low standard deviations relative to the observed max times. The max timeshows that on rare occasions the planners might make a decision within 2 minutes of ourcut-o .8. We separated the two because we usually observed a signi cant di erence in the distributions of time tosucceed and time to fail { about half the planners were quick to succeed and slow to fail, the other halfreversed the relationship. 18\nA Critical Assessment of Benchmark Comparison in PlanningSuccesses FailuresPlanner Max Mean Median Sd Max Mean Median SdA 667.9 34.0 1.3 98.7 1116.4 44.9 4.9 128.8B 1608.5 38.5 0.5 182.8 1692.0 45.6 17.8 96.8C 1455.4 89.9 1.6 244.6 1.4 0.4 0.13 0.4D 481.0 17.8 1.1 77.4 713.6 26.3 1.1 122.6E 1076 26.2 0.1 126.8 1622.8 286.9 260.6 189.1F 1282.4 44.4 0.1 126.8 1188.4 22.3 0.2 104.8G 1456.2 44.6 0.7 188.5 1196.5 43.8 16.7 78.5H 657.7 29.58 1.4 80.6 1080.6 93.8 1.4 162.1I 1713.8 115.4 0.2 303.1 50.6 5.1 4.9 6.3J 1596.5 43.6 4.3 127.4 1796 11.0 11.0 57.9K 1110.5 31.0 0.32 121.8 1298.8 27.7 12.1 65.2L 1611.9 54.4 2.0 180.9 847.1 124.1 68.4 164.8M 1675.3 53.4 1.45 196.5 1.6 0.9 0.8 0.4Table 10: Max, mean, median and standard deviations (Sd) for the computation times tosuccess and failure for each planner. What this table does not show, but the observed distributions do show, is that veryfew values are greater than half of the time until the cut-o . Figures 2 and 3 displaythe distributions for planner F, which had means in the middle of the set of planners andquite typical distributions. Consequently, at least for these problems, any cut-o above 15minutes (900 seconds) would not signi cantly change the results.\n0 104 208 312 416 520 624 728 832 936 1040 1144 1248 success.time\n0\n100\n200\n300\nFigure 2: Histogram of times, in seconds, for planner F to succeed.19\nHowe & Dahlman\n0 96 192 288 384 480 576 672 768 864 960 1056 1152 fail.time\n0\n200\n400\n600\nFigure 3: Histogram of times, in seconds, for planner F to fail.3.4 Performance MetricsMost comparisons emphasize the number of problems solved and the CPU time to comple-tion as metrics. Often, the problems are organized in increasing di culty to show scale-up.Comparing based on these metrics leaves a lot open to interpretation. For example, someplanners are designed to nd the optimal plan, as measured by number of steps in eithera parallel or sequential plan. Consequently, these planners may require more computation.Thus, by ignoring plan quality, these planners may be unfairly judged. We also hypothesizethat the hardware and software platform for the tests can vary the results. If a planner isdeveloped for a machine with 1GB of memory, then likely its performance will degrade withless. A key issue is whether the e ect is more or less uniform across the set of planners.In this section, we examine these two issues: execution platform and e ect of planquality.Metric Assumption 1: Does Performance Vary between Planners When Runon Di erent Hardware Platforms? Often when a planner is run at a competition orin someone else's lab, the hardware and software platforms di er from the platform usedduring development. Clearly, slowing down the processor speed should slow down planning,requiring higher cut-o s. Reduction in memory may well change the set of problems thatcan be solved or increase the processing time due to increased swapping. Changing thehardware con guration may change the way memory is cached and organized, favoringsome planners' internal representations over others. Changing compilers could also a ectthe amount and type of optimizations in the code. The exact e ects are probably unknown.The assumption is that such changes a ect all planners more or less equally.To test this, we ran the planners on a less powerful, lower memory machine and comparedthe results on the two platforms: the base Sun Ultrasparc 10/440 with 256mb of memoryand Ultrasparc 1/170 with 128mb of memory. The operating system and compilers werethe same versions for both machines. The same problems were run on both platforms. Wefollowed much the same methodology as in the comparison of planner versions: comparingon both number of problems solved and time to solution. Table 11 shows the results asmeasured by problems solved, failed or timed-out for each planner on the two platforms.20\nA Critical Assessment of Benchmark Comparison in Planning Planner Platform Solved Failed Timed-Out 2 p % ReductionA Ultra 1 94 383 27Ultra 10 95 389 20 1.09 .58 1B Ultra 1 121 346 37Ultra 10 121 353 30 0.80 .67 0C Ultra 1 354 7 143Ultra 10 367 7 130 0.85 .65 4D Ultra 1 218 59 227Ultra 10 217 59 228 0.01 .998 -.4E Ultra 1 280 145 79Ultra 10 284 150 70 0.66 .72 1F Ultra 1 277 155 72Ultra 10 284 154 66 0.35 .84 2G Ultra 1 120 347 37Ultra 10 121 352 31 0.57 .75 1H Ultra 1 116 350 38Ultra 10 122 338 44 0.80 .67 7I Ultra 1 265 201 38Ultra 10 274 201 29 1.36 .51 3J Ultra 1 280 220 4Ultra 10 285 217 2 0.73 .69 2K Ultra 1 108 370 26Ultra 10 108 368 28 0.08 .96 0L Ultra 1 149 339 16Ultra 10 150 341 13 0.32 .85 1M Ultra 1 250 65 189Ultra 10 258 66 180 0.35 .84 3Table 11: Number of problems solved, failed and timed-out for each planner on the twohardware platforms. Last column is the percentage reduction in the numbersolved from the faster to slower platforms.\n21\nHowe & DahlmanPlanner Faster Slower Same Total# Mean Sd # Mean Sd A 92 5.18 30.76 1 1 94B 120 4.02 10.01 0 1 121C 294 31.89 101.71 60 0.29 0.14 0 354D 177 11.02 82.82 39 0.23 0.14 1 217E 275 2.68 12.27 1 4 280F 271 14.86 72.44 0 6 277G 117 5.02 17.17 1 2 120H 115 6.86 25.24 0 1 116I 261 25.73 119.97 0 4 265J 280 42.24 138.16 0 0 280K 107 15.26 75.42 0 1 108L 148 16.81 98.54 1 0 149M 194 32.72 139.73 56 0.30 0.18 0 250Table 12: Improvements in execution speed moving from slower to faster platform. Countsonly problems that were solved on both platforms. For faster and slower, themean and standard deviation (Sd) of di erence is also provided.As before, we also looked at change in time to solution. Table 12 shows how the timeto solution changes for each planner. Not surprisingly, faster processor and more memorynearly always lead to better performance. Somewhat surprisingly, the di erence is far lessthan the doubling that might be expected; the mean di erences are much less than themean times on the faster processor (see Table 10 for the mean solution times).Also, the e ect seems to vary between the planners. Based on the counts, the Lisp-basedplanners appear to be less susceptible to this trend (the only ones that sometimes were fasteron the slower platform). However, the advantages are very small, a ecting primarily thesmaller problems. We think that this e ect is due to the need to load in a Lisp imageat startup from a centralized server; thus, computation time for small problems will bedominated by any network delay. Older versions of planners appear to be less sensitive tothe switch in platform.In this study, the platforms make little di erence to the results, despite a more thandoubling of processor speed and doubling of memory. However, the two platforms areunderpowered when compared to the development platforms for some of the planners. Wechose these platforms because they di ered in only a few characteristics (processor speedand memory amount) and because we had access to 20 identically con gured machines. Toreally observe a di erence, 1GB9 of memory or more may be needed.Recent trends in planning technology have exploited cheap memory: translations topropositional representations, compilation of the problems and built-in caching and memorymanagement techniques. Thus, some planners are designed to trade-o memory for time;9. We propose this gure because it is the amount requested by some of the participants in the AIPS 2000planning competition. 22\nA Critical Assessment of Benchmark Comparison in Planningthese planners will understandably be a ected by memory limitations for some problems.Given the results of this study, we considered performing a more careful study of memoryby arti cially limiting memory for the planners but did not do so because we did not haveaccess to enough su ciently large machines to likely make a di erence and because we couldnot devise a scheme for fairly doing so across all the planners (which are implemented indi erent languages and require di erent software run-time environments).Another important factor may be memory architecture/management. Some plannersinclude their own memory managers, which map better to some hardware platforms thanto others (e.g., HSP uses a linear organization that appears to t well with Intel's memoryarchitecture).Metric Assumption 2: Do the Number of Plan Steps Vary? Several researchershave examined the issue of measuring plan quality and directing planning based on it, e.g.,(Perez, 1995; Estlin & Mooney, 1997; Rabideau, Englehardt, & Chien, 2000). The numberof steps in a plan is a rather weak measure of plan quality, but so far, it is the only onethat has been widely used for primitive-action planning.We expect that some planners sacri ce quality (as measured by plan length) for speed.Thus, ignoring even this measure of plan quality may be unfair to some planners. Tocheck whether this appears to be a factor in our problem set, we counted the plan lengthin the plans returned in output and compared the lengths across the planners. Becausenot all of the planners construct parallel plans, we adopted the most general de nition:sequential plan length. We then compared the plan lengths returned by each planner onevery successfully solved problem.We found that 11% of the problems were solved by only one planner (not necessarily thesame one). The planners found equal length solutions for 62% of those that remained (493problems). We calculated the standard deviation (SD) of plan length for solutions to eachproblem and then analyzed the SDs. We found that the minimum observed SD was 0.30,the maximum was 63.30, the mean was 2.43 and the standard deviation was 5.45. Thirteencases showed SDs higher than 20. Obviously, these cases involved fairly long plans (up to165 steps); the cases were for problems from the logistics and gripper domains.To check whether some planners favored minimal lengths, we counted the number ofcases in which each planner found the shortest length plan (ties were attributed to allplanners) when there was some variance in plan length. Table 13 lists the results. Mostplanners nd the shortest length plans on about one third of these problems. Planner Fwas designed to optimize plan length, which shows in the results. With one exception, theolder planners rarely nd the shortest plans.4. Interpretation of Results and RecommendationsThe previous section presented our summarization and analysis of the planner runs. Inthis section, we re ect on what those results mean for empirical comparison of planners; wesummarize the results and recommend some partial solutions. It is not possible to guaranteefairness and we propose no magic formula for performing evaluations, but the state of thepractice in general can certainly be improved. We propose three general recommendationsand 12 recommendations targeted to speci c assumptions.23\nHowe & DahlmanPlanner CountA 178B 169C 0D 161E 5F 319G 171H 176I 222J 0K 159L 151M 283Table 13: Number of plans on which each planner found the shortest plan. The data onlyinclude problems for which di erent length plans were found.Many of the targeted recommendations amount to requesting problem and planner devel-opers to be more precise about the requirements for and expectations of their contributions.Because the planners are extremely complex and time consuming to build, the documenta-tion may be inadequate to determine how a subsequent version di ers from the previous orunder what conditions (e.g., parameter settings, problem types) the planner can be fairlycompared. With the current positive trend in making planners available, it behooves thedeveloper to include such information in the distribution of the system.The most sweeping recommendation is to shift the research focus away from developingthe best general-purpose planner. Even in the competitions, some of the planners identi edas superior have been ones designed for speci c classes of problems, e.g., FF and IPP. Thecompetitions have done a great job of exciting interest and encouraging the developmentand public availability of planners that incorporate the same representation.However, to advance the research, the most informative comparative evaluations arethose designed for a speci c purpose { to test some hypothesis or prediction about theperformance of a planner10. An experimental hypothesis focuses the analysis and oftenleads naturally to justi ed design decisions about the experiment itself. For example, Ho -mann and Nebel, the authors of the Fast-Forward (FF) system, state in the introduction totheir JAIR paper that FF's development was motivated by a speci c set of the benchmarkdomains; because the system is heuristic, they designed the heuristics to t the expec-tations/needs of those domains (Ho mann & Nebel, 2001). Additionally, in part of theirevaluation, they compare to a speci c system on which their own system had commonalitiesand point out the various advantages or disadvantages of their design decisions on speci c10. Paul Cohen has advocated such an experimental methodology for all of arti cial intelligence based onhypotheses, predictions and models in considerable detail; see Cohen (1991, 1995).24\nA Critical Assessment of Benchmark Comparison in Planningproblems. Follow-up work or researchers comparing their own systems to FF now have awell-de ned starting point for any comparison.Recommendation 1: Experiments should be driven by hypotheses. Re-searchers should precisely articulate in advance of the experiments their expecta-tions about how their new planner or augmentations to an existing planner addto the state of the art. These expectations should in turn justify the selectionof problems, other planners and metrics that form the core of the comparativeevaluation.A general issue is whether the results are accurate. We reported the results as they areoutput by the planners. If a planner stated in its output that it had been successful, wetook it at face value. However, by examining some of the output, we determined that someclaims of successful solution were erroneous { the proposed solution would not work. Theonly way to ensure that the output is correct is with a solution checker. Drew McDermottused a solution checker in the AIPS98 competition. However, the planners do not allprovide output in a compatible format with his checker. Thus, another concern with anycomparative evaluation is that the output needs to be cross-checked. Because we are notdeclaring a winner (i.e., that some planner exhibited superior performance), we do not thinkthat the lack of a solution checker casts serious doubt on our results. For the most part, wehave only been concerned with factors that cause the observed success rates to change.Recommendation 2: Just as input has been standardized with PDDL, outputshould be standardized, at least in the format of returned plans.Another general issue is whether the benchmark sets are representative of the space ofinteresting planning problems. We did not test this directly (in fact, we are not sure howone could do so), but the clustering of results and observations by others in the planningcommunity suggest that the set is biased toward logistics problems. Additionally, many ofthe problems are getting dated and no longer distinguish performance. Some researchershave begun to more formally analyze the problem set, either in service of building improvedplanners (e.g., Ho mann & Nebel, 2001) or to better understand planning problems. Forexample, in the related area of scheduling, our group has identi ed distinctive patterns inthe topology of search spaces for di erent types of classical scheduling problems and hasrelated the topology to performance of algorithms (Watson, Beck, Barbulescu, Whitley, &Howe, 2001). Within planning, Ho mann has examined the topology of local search spacesin some of the small problems in the benchmark collection and found a simple structurewith respect to some well-known relaxations (Ho mann, 2001). Additionally, he has workedout a partial taxonomy, based on three characteristics, for the analyzed domains. Helmerthas analyzed the computational complexity of a subclass of the benchmarks, transportationproblems, and has identi ed key features that a ect the di culty of such problems (Helmert,2001).Recommendation 3: The benchmark problem sets should themselves be eval-uated and over-hauled. Problems that can be easily solved should be removed.Researchers should study the benchmark problems/domains to classify them25\nHowe & Dahlmaninto problem types and key characteristics. Developers should contribute appli-cation problems and realistic versions of them to the evolving set.The remainder of this section describes other recommendations for improving the stateof the art in planner comparisons.Problem Assumption 1: Are General Purpose Planners Biased Toward Par-ticular Problems/Domains? The set of problems on which a planner was developedcan have a strong e ect on the performance of the planner. This can be either the e ectof unintentional over-specialization or the result of a concerted e ort on the part of thedevelopers to optimize their system to solve a speci c problem. With one exception, everyplanner fared better on the tailored subset of problems (training set). Consequently, wemust conclude that the choice of a subset of problems may well a ect the outcome of anycomparison.A fair planner comparison must account for likely biases in the problem set. Goodperformance on a certain class of problems does not imply good performance in general.A large performance di erential for planners with a targeted problem domain (i.e., do wellon their focus problems and poorly on others) may well indicate that the developers havesucceeded in optimizing the performance of their planner.Recommendation 4: Problem sets should be constructed to highlight thedesigners' expectations about superior performance for their planner, and theyshould be speci c about this selection criteria.On the other hand, if the goal is to demonstrate across the board performance, thenour results at randomly selecting domains suggests that biases can be mitigated.Recommendation 5: If highlighting performance on \\general\" problems isthe goal, then the problem set should be selected randomly from the benchmarkdomains.Problem Assumption 2: How Do Syntactic Representation Di erences A ectPerformance? Many studies, including this, have shown that planners may be sensitiveto representational features. Just because representations can be translated automaticallydoes not mean that performance will be una ected. Just because an algorithm shouldtheoretically be insensitive to a factor does not mean that in practice it is. All of theplanners showed some sensitivity to permuted problems, and the degree of sensitivity varied.This outcome suggests that translators and even minor variations on problem descriptionsimpact outcome and should be used with care, especially when the sensitivity is not thefocus of the study and some other planner is more vulnerable to the e ect.Recommendation 6: Representation translators should be avoided by usingnative versions of problems and testing multiple versions of problems if necessary.With many planner developers participating in the AIPS competitions, this should becomeless of an issue.More importantly, researchers should be explicitly testing the e ect of alternative phras-ings of planning problems to determine the sensitivity of performance and to separate thee ects of advice/tuning from the essence of the problem.26\nA Critical Assessment of Benchmark Comparison in PlanningRecommendation 7: Studies should consider the role of minor syntactic vari-ations in performance and include permuted problems (i.e., initial conditions,goals, preconditions and actions) in their problem sets because they can demon-strate robustness, provide an opportunity for learning and protect developersfrom accidentally over- tting their algorithm to the set of test problems.Problem Assumption 3: Does Performance Depend on PDDL RequirementsFeatures? The planners did not perform quite as advertised or expected given someproblem features. This discrepancy could have many possible causes: problems incorrectlyspeci ed, planners with less sensitivity than thought, solutions not being correct, etc. Forexample, many of the problems in the benchmark set were not designed for the competitionsor even intended to be widely used and so may not have been speci ed carefully enough.Recommendation 8: When problems are contributed to the benchmark set,developers should verify that the requirements stated in the description of eachproblem correctly re ect the subset of features needed. Planner evaluatorsshould then use only those problems that match a planner's capabilities.Depending on the cause, the results can be skewed, e.g., a planner may be unfairlymaligned for being unable to solve a problem that it was speci cally designed not to solve.The above recommendation addresses gaps in the speci cation of the problem set, but somemismatches between the capabilities speci able in PDDL and those that planners possessremain.Recommendation 9: Planner developers should develop a vocabulary fortheir planner's capabilities, as in the PDDL ags, and specify the expectedcapabilities in the planner's distribution.Planner Assumption 1: Is the Latest Version the Best? Our results suggest thatnew versions run faster, but often do not solve more problems. Thus, the newest version maynot represent the \\best\" (depending on your de nition) performance for the class of planner.Some competitions in other elds, e.g., the automatic theorem proving community, requirethe previous year's best performer to compete as well; this has the advantage of establishinga baseline of performance as well as allowing a comparison to how the focus may shift overtime. Recommendation 10: If the primary evaluation metric is speed, then a newerversion may be the best competition. If it is number of problems solved or if onewishes to establish what progress has been made, then it may be worth runningagainst an older version as well. If recommendation 9 has been followed, thenevaluators should select a version based on this guidance.Planner Assumption 2: What Are the E ect of Parameter Settings? Perfor-mance of some planners does vary with the parameter settings. Unfortunately, it often isdi cult to gure out how to set the parameters properly, and changing settings makes itdi cult to compare results across experiments. Generally, this is not an issue because the27\nHowe & Dahlmandevelopers and other users tend to rely on the default parameter settings. Unfortunately,sometimes the developers exploit alternative settings in their own experiments, complicatinglater comparison.Recommendation 11: If a planner includes parameters, the developer shouldguide users in their settings. If they do not, then the default settings should beused by both the developers and others in experiments to facilitate comparison.Planner Assumption 3: Are Time Cut-o s Unfair? We found little bene t fromincreasing time cut-o s beyond 15 minutes for our problems.Recommendation 12: If total computation time is a bottleneck, then run theproblems in separate batches, incrementally increasing the time cut-o betweenruns and including only unresolved problems in subsequent runs. When noadditional problems are solved in a run, stop.Metric Assumption 1: Do Alternative Platforms Lead to Di erent Perfor-mance? In our experiments, performance did not vary as much as we expected. Thisresult suggests that researchers in general are not developing for speci c hardware/softwarecon gurations, but recent trends suggest otherwise, at least with regards to memory. Again,because these systems are research prototypes, it behooves the developer to be clear abouthis/her expectations and anyone subsequently using the system to accommodate those re-quests in their studies.Recommendation 13: As with other factors in planner design, researchersmust clearly state the hardware/software requirements for their planners, if thedesign is based on platform assumptions. Additionally, a careful study of mem-ory versus time trade-o s should be undertaken, given the recent trends in mem-ory exploitation.Metric Assumption 2: Do the Number of Plan Steps Vary? They certainly can.If one neglects quality measures, then some planners are being penalized in e orts to declarea best planner.Recommendation 14: To expedite generalizing across studies, reports shoulddescribe performance in terms of what was solved (how many of what types),how much time was required and what were the quality of the solutions. Trade-o s should be reported, when possible, e.g., 12% increase in computation timefor 30% decrease in plan length. Additionally, if the design goal was to nd anoptimal solution, compare to other planners with that as their design goal.Good metrics of plan quality are sorely needed. The latest speci cation of the PDDLspeci cation supports the de nition of problem-speci c metrics (Fox & Long, 2002); thesemetrics indicate whether total-time (a new concept supported by speci cation of actiondurations) or speci ed functions should be minimized or maximized. This addition is anexcellent start, but general metrics other than just plan-length and total-time are alsoneeded to expedite comparisons across problems.28\nA Critical Assessment of Benchmark Comparison in PlanningRecommendation 15: Developing good metrics is a valuable research contri-bution. Researchers should consider it a worthwhile project, conference organiz-ers and reviewers should encourage papers on the topic, and planner developersshould implement their planners to be responsive to new quality metrics (i.e.,support tunable heuristics or evaluation criteria).5. ConclusionsFair evaluation and comparison of planners is hard. Many apparently benign factors exertsigni cant e ects on performance. Superior performance of one planner over another ona problem that neither was intentionally designed to solve may be explained by minorrepresentational features. However, comparative analysis on general problems is of practicalimportance as it is not practical to create a specialized solution to every problem.We have analyzed the e ects of experiment design decisions in empirical comparison ofplanners and made some recommendations for ameliorating the e ects of these decisions.Most of the recommendations are common sense suggestions for improving the currentmethodology.To expand beyond the current methodology will require at least two substantive changes.First, the eld needs to question whether we should be trying to show performance onplanning problems in general. A shift from general comparisons to focused comparisons (onproblem class or mechanism or on hypothesis testing) could produce signi cant advances inour understanding of planning.Second, the benchmark problem sets require attention. Many of the problems should bediscarded because they are too simple to show much. The domains are far removed fromreal applications. It may be time to revisit testbeds. For example, several researchers inrobotics have constructed an interactive testbed for comparing motion planning algorithms(Piccinocchi, Ceccarelli, Piloni, & Bicchi, 1997). The testbed consists of a user interface forde ning new problems, a collection of well-known algorithms and a simulator for testingalgorithms on speci c problems. Thus, the user can design his/her own problems and com-pare performance of various algorithms (including their own) on them via a web site. Such atestbed a ords several advantages over the current paradigm of static benchmark problemsand developer conducted comparisons, in particular, replicability and extendability of thetest set. Alternatively, challenging problem sets can be developed by modifying deployedapplications (Wilkins & desJardins, 2001; Engelhardt, Chien, Barrett, Willis, & Wilklow,2001).In recent years, the planning community has signi cantly improved the size of planningproblems that can be solved in reasonable time and has advanced the state of the art inempirical comparison of our systems. To interpret the results of empirical comparisonsand understand how they should motivate further development in planning, the communityneeds to understand the e ects of the empirical methodology itself. The purpose of thispaper is to further that understanding and initiate a dialogue about the methodology thatshould be used. 29\nHowe & DahlmanAcknowledgmentsThis research was partially supported by a Career award from the National ScienceFoundation IRI-9624058 and by a grant from Air Force O ce of Scienti c Research F49620-00-1-0144. The U.S. Government is authorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copyright notation thereon. We are mostgrateful to the reviewers for the careful reading of and well-considered comments on thesubmitted version; we hope we have done justice to your suggestions.ReferencesBacchus, F. (2000). AIPS-2000 planning competition.http://www.cs.toronto.edu/aips2000/SelfContainedAIPS-2000.ppt.Barrett, A., Golden, K., Penberthy, S., & Weld, D. (1993). UCPOP User's Manual. Dept.of Computer Science and Engineering, University of Washington, Seattle, WA. TR93-09-06.Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Arti cialIntelligence Journal, 90 (1-2), 225{279.Bonet, B., & Ge ner, H. (1999). Planning as heuristic search: New results. In Proceedingsof the Fifth European Conference on Planning (ECP-99) Durham, UK.Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.)(2000). Proceedings of the FifthInternational Conference on Arti cial Intelligence Planning and Scheduling (AIPS2000). AAAI Press, Breckenridge, CO.Cohen, P. R. (1991). A survey of the eighth national conference on arti cial intelligence:Pulling together or pulling apart? AI Magazine, 12 (1), 16{41.Cohen, P. R. (1995). Empirical Methods for Arti cial Intelligence. MIT Press.Drummond, M. E., Kaelbling, L. P., & Rosenschein, S. J. (1990). Collected notes from thebenchmarks and metrics workshop. Arti cial intelligence branch FIA-91-06, NASAAmes Research Center.Engelhardt, B., Chien, S., Barrett, T., Willis, J., & Wilklow, C. (2001). The data-chaserand citizen explorer benchmark problem sets. In Proceedings of the Sixth EuropeanConference on Planning (ECP 01) Toledo, Spain.Estlin, T. A., & Mooney, R. J. (1997). Learning to improve both e cicency and quality ofplanning. In Proceedings of the Fifteenth International Joint Conference on Arti cialIntelligence, pp. 1227{1233, Nagoya, Japan.Fox, M., & Long, D. (1999). The e cient implementation of the plan-graph in STAN.Journal of Arti cial Intelligence Research, 10, 87{115.Fox, M., & Long, D. (2002). PDDL2.1: An extension to PDDL for expressing temporalplanning domains. Available at http://www.dur.ac.uk/d.p.long/pddl2.ps.gz.30\nA Critical Assessment of Benchmark Comparison in PlanningGenevini, A., & Schubert, L. (1996). Accelerating partial-order planners: Some techniquesfor e ective search control and pruning. Journal of Arti cial Intelligence Research, 5,95{137.Hanks, S., Nguyen, D., & Thomas, C. (1993). A beginner's guide to the truckworld simu-lator. Dept. of Computer Science and Engineering UW-CSE-TR 93-06-09, Universityof Washington.Hanks, S., Pollack, M. E., & Cohen, P. R. (1994). Benchmarks, test beds, controlledexperimentation and the design of agent architectures. AI Magazine, 17{42.Haslum, P., & Ge ner, H. (2000). Admissible heuristics for optimal planning. In Pro-ceedings of the Fifth International Conference on Arti cial Intelligence Planning andScheduling (AIPS 2000), pp. 140{149, Breckenridge, CO. AAAI Press.Helmert, M. (2001). On the complexity of planning in transportation domains. In 6thEuropean Conference on Planning (ECP'01), Lecture Notes in Arti cial Intelligence,New York, Springer-Verlag.Ho mann, J. (2001). Local search topology in planning benchmarks: An empirical analysis.In Proceedings of the 17th International Joint Conference on Arti cial IntelligenceSeattle, WA, USA.Ho mann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation throughheuristic search. Journal of Arti cial Intelligence Research, 14, 253{302.Howe, A. E., Dahlman, E., Hansen, C., Scheetz, M., & von Mayrhauser, A. (1999). Exploit-ing competitive planner performance. In Proceedings of the Fifth European Conferenceon Planning, Durham, UK.Howe, A. E., von Mayrhauser, A., & Mraz, R. T. (1997). Test case generation as an AIplanning problem. Automated Software Engineering, 4 (1), 77{106.Joslin, D., & Pollack, M. (1994). Least-cost aw repair: A plan re nement strategy forpartial-order planning. In Proceedings of the Twelfth National Conference on Arti cialIntelligence, pp. 1004{1009, Seattle, WA.Kautz, H., & Selman, B. (1998). BLACKBOX: A new approach to the application oftheorem proving to problem solving. In Working notes of the AIPS98 Workshop onPlanning as Combinatorial Search, Pittsburgh, PA.Kautz, H. blackbox: a SAT technology planning system.http://www.cs.washington.edu/homes/kautz/blackbox/index.html.Kautz, H., & Selman, B. (1999). Unifying SAT-based and graph-based planning. In Proceed-ings of the Sixteenth International Joint Conference on Arti cial Intelligence, Stock-holm, Sweden.Koehler, J. (1999). Handling of conditional e ects and negative goals in IPP. Tech. rep.128, Institute for Computer Science, Albert Ludwigs University, Freiburg, Germany.31\nHowe & DahlmanKoehler, J., Nebel, B., Ho mann, J., & Dimopoulos, Y. (1997). Extending planning graphsto an ADL subset. In Proceedings of the Fourth European Conference in Planning.McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &Wilkins, D. (1998). The Planning Domain De nition Language.McDermott, D. (2000). The 1998 AI planning systems competition. AI Magazine, 21 (2),35{56.Penberthy, J. S., & Weld, D. S. (1992). UCPOP: a sound, complete, partial order plannerfor adl. In Proceedings of the Third International Conference on Knowledge Repre-sentation and Reasoning, pp. 103{114.Perez, M. A. (1995). Learning Search Control Knowledge to Improve Plan Quality. Ph.D.thesis, Carnegie-Mellon University.Piccinocchi, S., Ceccarelli, M., Piloni, F., & Bicchi, A. (1997). Interactive benchmark forplanning algorithms on the web. In Proceedings of IEEE International Conference onRobotics and Automation.Pollack, M. E., & Ringuette, M. (1990). Introducing the Tileworld: Experimentally evaluat-ing agent architectures. In Proceedings of the Eight National Conference on Arti cialIntelligence, pp. 183{189, Boston, MA.Pollack, M., Joslin, D., & Paolucci, M. (1997). Flaw selection strategies for partial-orderplanning. Journal of Arti cial Intelligence Research, 6, 223{262.Rabideau, G., Englehardt, B., & Chien, S. (2000). Using generic prferences to incrementallyimprove plan quality. In Proceedings of the Fifth International Conference on Arti cialIntelligence Planning and Scheduling (AIPS 2000), Breckenridge, CO.Slaney, J., & Thiebaux, S. (2001). Blocks world revisited. Arti cial Intelligence Journal,125 (1-2), 119{153.Srinivasan, R., & Howe, A. E. (1995). Comparison of methods for improving search e ciencyin a partial-order planner. In Proceedings of the 14th International Joint Conferenceon Arti cial Intelligence, pp. 1620{1626, Montreal, Canada.Sussman, G. A. (1973). A computational model of skill acquisition. Tech. rep. Memo no.AI-TR-297, MIT AI Lab.The Prodigy Research Group (1992). PRODIGY 4.0; the manual and tutorial. School ofComputer Science 92-150, Carnegie Mellon University.Watson, J., Barbulescu, L., Howe, A., & Whitley, L. D. (1999). Algorithm performance andproblem structure for ow-shop scheduling. In Proceedings of the Sixteenth NationalConference on Arti cial Intelligence (AAAI-99), Orlando, FL.Watson, J., Beck, J., Barbulescu, L., Whitley, L. D., & Howe, A. (2001). Toward a de-scriptive model of local search cost in job-shop scheduling. In Proceedings of SixthEuropean Conference on Planning (ECP'01), Toledo, Spain.32\nA Critical Assessment of Benchmark Comparison in PlanningWeld, D., Anderson, C., & Smith, D. (1998). Extending graphplan to handle uncertaintyand sensing actions. In Proceedings of the Fifteenth National Conference on Arti cialIntelligence Madison, WI.Weld, D. S. (1999). Recent advances in AI planning. AI Magazine, 20 (2), 93{122.Wilkins, D. E., & desJardins, M. (2001). A call for knowledge-based planning. AI Magazine,22 (1), 99{115.\n33"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": null, "creator": "dvips 5.76 Copyright 1997 Radical Eye Software (www.radicaleye.com)"}}}