{"id": "1109.3843", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2011", "title": "Fast approximation of matrix coherence and statistical leverage", "abstract": "often the \\ emph { compare statistical ` leverage scores } coefficients of a matrix $ a $ are additionally the coarse squared weighted row - correlation norms of the matrix containing its ( quad top ) total left singular utility vectors and the \\ emph { coherence } column is the largest leverage data score. together these differential quantities mainly have been of interest developed in more recently - publicized popular algorithms problems - such are as adaptive matrix completion surgery and nystr \\ \" { o } m - based cumulative low - rank matrix approximation ; in large - valued scale statistical data analysis applications more generally ; but and for since they define the key structural nonuniformity vector that evaluation must be dealt repeatedly with in recent developing current fast randomized matrix learning algorithms. therefore our main analytic result construction is a conditional randomized extraction algorithm that takes for as its input matrix an arbitrary $ n \\ times | d $ matrix $ an a $, with $ ~ n \\ d gg d $, and matching that returns twice as output reverse relative - resident error approximations parallel to \\ emph { for all } $ _ n $ p of the statistical leverage scores. the simple proposed algorithm runs ( under assumptions on combining the precise values of $ \u2032 n $ and $ d $ ) in $ * o ( n d \\ log / n ) $ time, as opposed to the $ an o ( nd ^ 2 ) $ time projection required estimated by the equation na \\ \" { i } rd ve algorithm calculation that involves computing an overall orthogonal likelihood basis model for shrinking the range of $ - a $. \u201c our analysis field may instead be alternately viewed in complementary terms of computing a relative - dependent error global approximation, to yields an \\ ch emph { an under } constraints constrained least - squares approximation problem, or, relatedly, more it may be viewed just as an application of johnson - lindenstrauss type ideas. several various practically - important extensions of to our conceptual basic result framework are well also subsequently described, including the approximation rule of so - speaking called fisher cross - function leverage scores, bringing the extension proof of specifically these general ideas to matrices communicating with $ n \\ approx d $, thereby and moreover the extension mathematics to streaming mapping environments.", "histories": [["v1", "Sun, 18 Sep 2011 04:38:12 GMT  (60kb)", "https://arxiv.org/abs/1109.3843v1", "28 pages"], ["v2", "Wed, 5 Dec 2012 00:13:53 GMT  (35kb)", "http://arxiv.org/abs/1109.3843v2", "29 pages; conference version is in ICML; journal version is in JMLR"]], "COMMENTS": "28 pages", "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["michael w mahoney", "petros drineas", "malik magdon-ismail", "david p woodruff"], "accepted": true, "id": "1109.3843"}, "pdf": {"name": "1109.3843.pdf", "metadata": {"source": "CRF", "title": "Fast approximation of matrix coherence and statistical leverage", "authors": ["Petros Drineas", "Michael W. Mahoney", "David P. Woodruff"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 9.\n38 43\nv2 [\ncs .D\nS] 5\nD ec\n2 01\n2"}, {"heading": "1 Introduction", "text": "The concept of statistical leverage measures the extent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21]. A related notion is that of matrix coherence, which has been of interest in recently popular problems such as matrix completion and Nystro\u0308m-based low-rank matrix approximation [13, 46]. Defined more precisely below, the statistical leverage scores may be computed as the squared Euclidean norms of the rows of the matrix containing the top left singular vectors and the coherence of the matrix is the largest statistical leverage score. Statistical leverage scores have a long history in statistical data analysis, where they have been used for outlier detection in regression diagnostics [29, 14]. Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion. The na\u0308\u0131ve and best previously existing algorithm to compute these scores would compute an orthogonal basis for the dominant part of the spectrum of A, e.g., the basis provided by the Singular Value Decomposition (SVD) or a basis provided by a QR decomposition [26], and then use that basis to compute diagonal elements of the projection matrix onto the span of that basis.\n\u2217Dept. of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180. Email: drinep@cs.rpi.edu \u2020Dept. of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180. Email: magdon@cs.rpi.edu \u2021Dept. of Mathematics, Stanford University, Stanford, CA 94305. Email: mmahoney@cs.stanford.edu \u00a7IBM Almaden Research Center, 650 Harry Road, San Jose, CA 95120 USA. Email: dpwoodru@us.ibm.com\nWe present a randomized algorithm to compute relative-error approximations to every statistical leverage score in time qualitatively faster than the time required to compute an orthogonal basis. For the case of an arbitrary n \u00d7 d matrix A, with n \u226b d, our main algorithm runs (under assumptions on the precise values of n and d, see Theorem 1 for an exact statement) in O(nd log n/\u01eb2) time, as opposed to the \u0398(nd2) time required by the na\u0308\u0131ve algorithm. As a corollary, our algorithm provides a relative-error approximation to the coherence of an arbitrary matrix in the same time. In addition, several practically-important extensions of the basic idea underlying our main algorithm are also described in this paper."}, {"heading": "1.1 Overview and definitions", "text": "We start with the following definition of the statistical leverage scores of a matrix.\nDefinition 1. Given an arbitrary n \u00d7 d matrix A, with n > d, let U denote the n \u00d7 d matrix consisting of the d left singular vectors of A, and let U(i) denote the i-th row of the matrix U as a row vector. Then, the statistical leverage scores of the rows of A are given by\n\u2113i = \u2225 \u2225U(i) \u2225 \u2225 2\n2 , (1)\nfor i \u2208 {1, . . . , n}; the coherence \u03b3 of the rows of A is\n\u03b3 = max i\u2208{1,...,n} \u2113i, (2)\ni.e., it is the largest statistical leverage score of A; and the (i, j)-cross-leverage scores cij are\ncij = \u2329 U(i), U(j) \u232a , (3)\ni.e., they are the dot products between the ith row and the jth row of U .\nAlthough we have defined these quantities in terms of a particular basis, they clearly do not depend on that particular basis, but only on the space spanned by that basis. To see this, let PA denote the projection matrix onto the span of the columns of A. Then,\n\u2113i = \u2225 \u2225U(i) \u2225 \u2225 2\n2 =\n( UUT )\nii = (PA)ii . (4)\nThat is, the statistical leverage scores of a matrix A are equal to the diagonal elements of the projection matrix onto the span of its columns.1 Similarly, the (i, j)-cross-leverage scores are equal to the off-diagonal elements of this projection matrix, i.e.,\ncij = (PA)ij = \u2329 U(i), U(j) \u232a . (5)\nClearly, O(nd2) time suffices to compute all the statistical leverage scores exactly: simply perform the SVD or compute a QR decomposition of A in order to obtain any orthogonal basis for the range of A and then compute the Euclidean norm of the rows of the resulting matrix. Thus, in this paper, we are interested in algorithms that run in o(nd2) time.\nSeveral additional comments are worth making regarding this definition. First, since \u2211n\ni=1 \u2113i = \u2016U\u20162F = d, we can define a probability distribution over the rows of A as pi = \u2113i/d. As discussed\n1In this paper, for simplicity of exposition, we consider the case that the matrix A has rank equal to d, i.e., has full column rank. Theoretically, the extension to rank-deficient matrices A is straightforward\u2014simply modify Definition 1 and thus Eqns. (4) and (5) to let U be any orthogonal matrix (clearly, with fewer than d columns) spanning the column space of A. From a numerical perspective, things are substantially more subtle, and we leave this for future work that considers numerical implementations of our algorithms.\nbelow, these probabilities have played an important role in recent work on randomized matrix algorithms and an important algorithmic question is the degree to which they are uniform or nonuniform.2 Second, one could also define leverage scores for the columns of a \u201ctall\u201d matrix A, but clearly those are all equal to one unless n < d or A is rank-deficient. Third, and more generally, given a rank parameter k, one can define the statistical leverage scores relative to the best rank-k approximation to A to be the n diagonal elements of the projection matrix onto the span of Ak, the best rank-k approximation to A."}, {"heading": "1.2 Our main result", "text": "Our main result is a randomized algorithm for computing relative-error approximations to every statistical leverage score, as well as an additive-error approximation to all of the large crossleverage scores, of an arbitrary n \u00d7 d matrix, with n \u226b d, in time qualitatively faster than the time required to compute an orthogonal basis for the range of that matrix. Our main algorithm for computing approximations to the statistical leverage scores (see Algorithm 1 in Section 3) will amount to constructing a \u201crandomized sketch\u201d of the input matrix and then computing the Euclidean norms of the rows of that sketch. This sketch can also be used to compute approximations to the large cross-leverage scores (see Algorithm 2 of Section 3).\nThe following theorem provides our main quality-of-approximation and running time result for Algorithm 1.\nTheorem 1. Let A be a full-rank n\u00d7d matrix, with n \u226b d; let \u01eb \u2208 (0, 1/2] be an error parameter; and recall the definition of the statistical leverage scores \u2113i from Definition 1. Then, there exists a randomized algorithm (Algorithm 1 of Section 3 below) that returns values \u2113\u0303i, for all i \u2208 {1, . . . , n}, such that with probability at least 0.8,\n\u2223 \u2223 \u2223 \u2113i \u2212 \u2113\u0303i \u2223 \u2223 \u2223 \u2264 \u01eb\u2113i (6)\nholds for all i \u2208 {1, . . . , n}. Assuming d \u2264 n \u2264 ed, the running time of the algorithm is\nO ( nd ln ( d\u01eb\u22121 ) + nd\u01eb\u22122 lnn+ d3\u01eb\u22122 (lnn) ( ln ( d\u01eb\u22121 ))) .\nAlgorithm 1 provides a relative-error approximation to all of the statistical leverage scores \u2113i of A and, assuming d ln d = o (\nn lnn\n)\n, lnn = o (d), and treating \u01eb as a constant, its running time is o(nd2), as desired. As a corollary, the largest leverage score (and thus the coherence) is approximated to relative-error in o(nd2) time.\nThe following theorem provides our main quality-of-approximation and running time result for Algorithm 2.\nTheorem 2. Let A be a full-rank n\u00d7d matrix, with n \u226b d; let \u01eb \u2208 (0, 1/2] be an error parameter; let \u03ba be a parameter; and recall the definition of the cross-leverage scores cij from Definition 1. Then, there exists a randomized algorithm (Algorithm 2 of Section 3 below) that returns the pairs {(i, j)} together with estimates {c\u0303ij} such that, with probability at least 0.8,\ni. If c2ij \u2265 d\n\u03ba + 12\u01eb\u2113i\u2113j, then (i, j) is returned; if (i, j) is returned, then c\n2 ij \u2265\nd \u03ba \u2212 30\u01eb\u2113i\u2113j.\nii. For all pairs (i, j) that are returned, c\u03032ij \u2212 30\u01eb\u2113i\u2113j \u2264 c2ij \u2264 c\u03032ij + 12\u01eb\u2113i\u2113j. 2Observe that if U consists of d columns from the identity, then the leverage scores are extremely nonuniform: d of them are equal to one and the remainder are equal to zero. On the other hand, if U consists of d columns from a normalized Hadamard transform (see Section 2.3 for a definition), then the leverage scores are very uniform: all n of them are equal to d/n.\nThis algorithm runs in O(\u01eb\u22122n lnn+ \u01eb\u22123\u03bad ln2 n) time.\nNote that by setting \u03ba = n lnn, we can compute all the \u201clarge\u201d cross-leverage scores, i.e., those satisfying c2ij \u2265 dn lnn , to within additive-error in O ( nd ln3 n ) time (treating \u01eb as a constant). If ln3 n = o (d) the overall running time is o(nd2), as desired."}, {"heading": "1.3 Significance and related work", "text": "Our results are important for their applications to fast randomized matrix algorithms, as well as their applications in numerical linear algebra and large-scale data analysis more generally.\nSignificance in theoretical computer science. The statistical leverage scores define the key structural nonuniformity that must be dealt with (i.e., either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12]. Roughly, the best random sampling algorithms use these scores (or the generalized leverage scores relative to the best rank-k approximation to A) as an importance sampling distribution to sample with respect to. On the other hand, the best random projection algorithms rotate to a basis where these scores are approximately uniform and thus in which uniform sampling is appropriate. See [32] for a detailed discussion.\nAs an example, the CUR decomposition of [21, 33] essentially computes pi = \u2113i/k, for all i \u2208 {1, . . . , n} and for a rank parameter k, and it uses these as an importance sampling distribution. The computational bottleneck for these and related random sampling algorithms is the computation of the importance sampling probabilities. On the other hand, the computational bottleneck for random projection algorithms is the application of the random projection, which is sped up by using variants of the Fast Johnson-Lindenstrauss Transform [2, 3]. By our main result, the leverage scores (and thus these probabilities) can be approximated in time that depends on an application of a Fast Johnson-Lindenstrauss Transform. In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].\nApplications to numerical linear algebra. Recently, high-quality numerical implementations of variants of the basic randomized matrix algorithms have proven superior to traditional deterministic algorithms [44, 43, 6]. An important question raised by our main results is how these will compare with an implementation of our main algorithm. More generally, density functional theory [8] and uncertainty quantification [7] are two scientific computing areas where computing the diagonal elements of functions (such as a projection or inverse) of very large input matrices is common. For example, in the former case, \u201cheuristic\u201d methods based on using Chebychev polynomials have been used in numerical linear algebra to compute the diagonal elements of the projector [8]. Our main algorithm should have implications in both of these areas.\nApplications in large-scale data analysis. The statistical leverage scores and the scores relative to the best rank-k approximation to A are equal to the diagonal elements of the socalled \u201chat matrix\u201d [29, 15]. As such, they have a natural statistical interpretation in terms of the \u201cleverage\u201d or \u201cinfluence\u201d associated with each of the data points [29, 14, 15]. In the context of regression problems, the ith leverage score quantifies the leverage or influence of the ith constraint/row of A on the solution of the overconstrained least squares optimization problem minx \u2016Ax\u2212 b\u20162 and the (i, j)-th cross leverage score quantifies how much influence or leverage the ith data point has on the jth least-squares fit (see [29, 14, 15] for details). When applied to lowrank matrix approximation problems, the leverage score \u2113j quantifies the amount of leverage or influence exerted by the jth column of A on its optimal low-rank approximation. Historically, these quantities have been widely-used for outlier identification in diagnostic regression analysis [47, 16].\nMore recently, these scores (usually the largest scores) often have an interpretation in terms of the data and processes generating the data that can be exploited. For example, depending on the setting, they can have an interpretation in terms of high-degree nodes in data graphs, very small clusters in noisy data, coherence of information, articulation points between clusters, the value of a customer in a network, space localization in sensor networks, etc. [9, 42, 38, 30, 32]. In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40]. Our main result will permit the computation of these scores and related quantities for significantly larger SNP data sets than has been possible previously [41, 19, 40, 24].\nRemark. Lest there be any confusion, we should emphasize our main contributions. First, note that statistical leverage and matrix coherence are important concepts in statistics and machine learning. Second, recall that several random sampling algorithms for ubiquitous matrix problems such as least-squares approximation and low-rank matrix approximation use leverage scores in a crucial manner; but until now these algorithms were \u2126(TSVD), where TSVD is the time required to compute a QR decomposition or a partial SVD of the input matrix. Third, note that, in some cases, o(TSV D) algorithms exist for these problems based on fast random projections. But recall that the existence of those projection algorithms in no way implies that it is easy or obvious how to compute the statistical leverage scores efficiently. Fourth, one implication of our main result is that those random sampling algorithms can now be performed just as efficiently as those random projection algorithms; thus, the solution for those matrix problems can now be obtained while preserving the identity of the rows. That is, these problems can now be solved just as efficiently by using actual rows, rather than the arbitrary linear combinations of rows that are returned by random projections. Fifth, we provide a generalization to \u201cfat\u201d matrices and to obtaining the cross-leverage scores. Sixth, we develop algorithms that can compute leverage scores and related statistics even in streaming environments."}, {"heading": "1.4 Empirical discussion of our algorithms", "text": "Although the main contribution of our paper is to provide a rigorous theoretical understanding of fast leverage score approximation, our paper does analyze the theoretical performance of what is meant to be a practical algorithm. Thus, one might wonder about the empirical performance of our algorithms\u2014for example, whether hidden constants render the algorithms useless for data of realistic size. Not surprisingly, this depends heavily on the quality of the numerical implementation, whether one is interested in \u201ctall\u201d or more general matrices, etc. We will consider empirical and numerical aspects of these algorithms in forthcoming papers, e.g., [25]. We will, however, provide here a brief summary of several numerical issues for the reader interested in these issues.\nEmpirically, the running time bottleneck for our main algorithm (Algorithm 1 of Section 3) applied to \u201ctall\u201d matrices is the application of the random projection \u03a01. Thus, empirically the running time is similar to the running time of random projection based methods for computing approximations to the least-squares problem, which is also dominated by the application of the random projection. The state of the art here is the Blendenpik algorithm of [6] and the LSRN algorithm of [34]. In their Blendenpik paper, Avron, Maymounkov, and Toledo showed that their high-quality numerical implementation of a Hadamard-based random projection (and associated least-squares computation) \u201cbeats Lapack\u2019s3 direct dense least-squares solver by a large margin on essentially any dense tall matrix,\u201d and they concluded that their empirical results \u201csuggest\n3 Lapack (short for Linear Algebra PACKage) is a high-quality and widely-used software library of numerical\nroutines for solving a wide range of numerical linear algebra problems.\nthat random projection algorithms should be incorporated into future versions of Lapack\u201d [6]. The LSRN algorithm of Meng, Saunders, and Mahoney improves Blendenpik in several respects, e.g., providing better handling of sparsity and rank deficiency, but most notably the random projection underlying LSRN is particularly appropriate for solving large problems on clusters with high communication cost, e.g., it has been shown to scale well on Amazon Elastic Cloud Compute clusters. Thus, our main algorithm should extend easily to these environments with the use of the random projection underlying LSRN. Moreover, for both Blendenpik and LSRN (when implemented with a Hadamard-based random projection), the hidden constants in the Hadamard-based random projection are so small that the random projection algorithm (and thus the empirical running time of our main algorithm for approximating leverage scores) beats the traditional O(nd2) time algorithm for dense matrices as small as thousands of rows by hundreds of columns."}, {"heading": "1.5 Outline", "text": "In Section 2, we will provide a brief review of relevant notation and concepts from linear algebra. Then, in Sections 3 and 4, we will present our main results: Section 3 will contain our main algorithm and Section 4 will contain the proof of our main theorem. Section 5 will then describe extensions of our main result to general \u201cfat\u201d matrices, i.e., those with n \u2248 d. Section 6 will conclude by describing the relationship of our main result with another related estimator for the statistical leverage scores, an application of our main algorithm to the under-constrained least-squares approximation problem, and extensions of our main algorithm to streaming environments."}, {"heading": "2 Preliminaries on linear algebra and fast random projections", "text": ""}, {"heading": "2.1 Basic linear algebra and notation", "text": "Let [n] denote the set of integers {1, 2, . . . , n}. For any matrix A \u2208 Rn\u00d7d, let A(i), i \u2208 [n], denote the i-th row of A as a row vector, and let A(j), j \u2208 [d] denote the j-th column of A as a column vector. Let \u2016A\u20162F = \u2211n i=1 \u2211d j=1A 2 ij denote the square of the Frobenius norm of A, and let \u2016A\u20162 = sup \u2016x\u20162=1 \u2016Ax\u20162 denote the spectral norm of A. Relatedly, for any vector x \u2208 R n, its Euclidean norm (or \u21132-norm) is the square root of the sum of the squares of its elements. The dot product between two vectors x, y \u2208 Rn will be denoted \u3008x, y\u3009, or alternatively as xT y. Finally, let ei \u2208 Rn, for all i \u2208 [n], denote the standard basis vectors for Rn and let In denote the n \u00d7 n identity matrix.\nLet the rank of A be \u03c1 \u2264 min{n, d}, in which case the \u201ccompact\u201d or \u201cthin\u201d SVD of A is denoted by A = U\u03a3V T , where U \u2208 Rn\u00d7\u03c1, \u03a3 \u2208 R\u03c1\u00d7\u03c1, and V \u2208 Rd\u00d7\u03c1. (For a general matrix X, we will write X = UX\u03a3XV T X .) Let \u03c3i(A), i \u2208 [\u03c1] denote the i-th singular value of A, and let \u03c3max(A) and \u03c3min(A) denote the maximum and minimum singular values of A, respectively. The Moore-Penrose pseudoinverse of A is the d\u00d7n matrix defined by A\u2020 = V \u03a3\u22121UT [37]. Finally, for any orthogonal matrix U \u2208 Rn\u00d7\u2113, let U\u22a5 \u2208 Rn\u00d7(n\u2212\u2113) denote an orthogonal matrix whose columns are an orthonormal basis spanning the subspace of Rn that is orthogonal to the subspace spanned by the columns of U (i.e., the range of U). It is always possible to extend an orthogonal matrix U to a full orthonormal basis of Rn as [U U\u22a5].\nThe SVD is important for a number of reasons [26]. For example, the projection of the columns of A onto the k left singular vectors associated with the top k singular values gives the best rank-k approximation to A in the spectral and Frobenius norms. Relatedly, the solution to the least-squares (LS) approximation problem is provided by the SVD: given an n \u00d7 d matrix\nA and an n-vector b, the LS problem is to compute the minimum \u21132-norm vector x such that \u2016Ax\u2212 b\u20162 is minimized over all vectors x \u2208 Rd. This optimal vector is given by xopt = A\u2020b. We call a LS problem overconstrained (or overdetermined) if n > d and underconstrained (or underdetermined) if n < d."}, {"heading": "2.2 The Fast Johnson-Lindenstrauss Transform (FJLT)", "text": "Given \u01eb > 0 and a set of points x1, . . . , xn with xi \u2208 Rd, a \u01eb-Johnson-Lindenstrauss Transform (\u01eb-JLT), denoted \u03a0 \u2208 Rr\u00d7d, is a projection of the points into Rr such that\n(1\u2212 \u01eb)\u2016xi\u201622 \u2264 \u2016\u03a0xi\u201622 \u2264 (1 + \u01eb)\u2016xi\u201622. (7)\nTo construct an \u01eb-JLT with high probability, simply choose every entry of \u03a0 independently, equal to \u00b1 \u221a\n3/r with probability 1/6 each and zero otherwise (with probability 2/3) [1]. Let \u03a0JLT be a matrix drawn from such a distribution over r \u00d7 d matrices.4 Then, the following lemma holds.\nLemma 1 (Theorem 1.1 of [1]). Let x1, . . . , xn be an arbitrary (but fixed) set of points, where xi \u2208 Rd and let 0 < \u01eb \u2264 1/2 be an accuracy parameter. If\nr \u2265 1 \u01eb2\n(\n12 ln n+ 6 ln 1\n\u03b4\n)\nthen, with probability at least 1\u2212 \u03b4, \u03a0JLT \u2208 Rr\u00d7d is an \u01eb-JLT .\nFor our main results, we will also need a stronger requirement than the simple \u01eb-JLT and so we will use a version of the Fast Johnson-Lindenstrauss Transform (FJLT), which was originally introduced in [2, 3]. Consider an orthogonal matrix U \u2208 Rn\u00d7d, viewed as d vectors in Rn. A FJLT projects the vectors from Rn to Rr, while preserving the orthogonality of U ; moreover, it does so very quickly. Specifically, given \u01eb > 0, \u03a0 \u2208 Rr\u00d7n is an \u01eb-FJLT for U if\n\u2022 \u2225 \u2225Id \u2212 UT\u03a0T\u03a0U \u2225 \u2225 2 \u2264 \u01eb, and\n\u2022 given any X \u2208 Rn\u00d7d, the matrix product \u03a0X can be computed in O(nd ln r) time.\nThe next lemma follows from the definition of an \u01eb-FJLT, and its proof can be found in [20, 22].\nLemma 2. Let A be any matrix in Rn\u00d7d with n \u226b d and rank(A) = d. Let the SVD of A be A = U\u03a3V T , let \u03a0 be an \u01eb-FJLT for U (with 0 < \u01eb \u2264 1/2) and let \u03a8 = \u03a0U = U\u03a8\u03a3\u03a8V T\u03a8 . Then, all the following hold:\nrank(\u03a0A) = rank(\u03a0U) = rank(U) = rank(A) = d, (8) \u2225 \u2225I \u2212 \u03a3\u22122\u03a8 \u2225 \u2225 2 \u2264 \u01eb/(1\u2212 \u01eb), and (9)\n(\u03a0A)\u2020 = V \u03a3\u22121(\u03a0U)\u2020. (10)"}, {"heading": "2.3 The Subsampled Randomized Hadamard Transform (SRHT)", "text": "One can use a Randomized Hadamard Transform (RHT) to construct, with high probability, an \u01eb-FJLT. Our main algorithm will use this efficient construction in a crucial way.5 Recall that the\n4When no confusion can arise, we will use \u03a0JLT to refer to this distribution over matrices as well as to a specific matrix drawn from this distribution.\n5Note that the RHT has also been crucial in the development of o(nd2) randomized algorithms for the general overconstrained LS problem [22] and its variants have been used to provide high-quality numerical implementations of such randomized algorithms [44, 6].\n(unnormalized) n\u00d7 n matrix of the Hadamard transform H\u0302n is defined recursively by\nH\u03022n =\n[\nH\u0302n H\u0302n H\u0302n \u2212H\u0302n\n]\n,\nwith H\u03021 = 1. The n\u00d7 n normalized matrix of the Hadamard transform is equal to\nHn = H\u0302n/ \u221a n.\nFrom now on, for simplicity and without loss of generality, we assume that n is a power of 2 and we will suppress n and just write H. (Variants of this basic construction that relax this assumption and that are more appropriate for numerical implementation have been described and evaluated in [6].) Let D \u2208 Rn\u00d7n be a random diagonal matrix with independent diagonal entries Dii = +1 with probability 1/2 and Dii = \u22121 with probability 1/2. The product HD is a RHT and it has three useful properties. First, when applied to a vector, it \u201cspreads out\u201d its energy. Second, computing the product HDx for any vector x \u2208 Rn takes O(n log2 n) time. Third, if we only need to access r elements in the transformed vector, then those r elements can be computed in O(n log2 r) time [4]. The Subsampled Randomized Hadamard Transform (SRHT) randomly samples (according to the uniform distribution) a set of r rows of a RHT.\nUsing the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator ST . Let the matrix \u03a0FJLT = S\nTHD be generated using the SRHT.6 The most important property about the distribution \u03a0FJLT is that if r is large enough, then, with high probability, \u03a0FJLT generates an \u01eb-FJLT. We summarize this discussion in the following lemma (which is essentially a combination of Lemmas 3 and 4 from [22], restated to fit our notation).\nLemma 3. Let \u03a0FJLT \u2208 Rr\u00d7n be generated using the SRHT as described above and let U \u2208 Rn\u00d7d (n \u226b d) be an (arbitrary but fixed) orthogonal matrix. If\nr \u2265 14 2d ln(40nd)\n\u01eb2 ln\n(\n302d ln(40nd)\n\u01eb2\n)\n,\nthen, with probability at least 0.9, \u03a0FJLT is an \u01eb-FJLT for U ."}, {"heading": "3 Our main algorithmic results", "text": "In this section, we will describe our main results for computing relative-error approximations to every statistical leverage score (see Algorithm 1) as well as additive-error approximations to all of the large cross-leverage scores (see Algorithm 2) of an arbitrary matrix A \u2208 Rn\u00d7d, with n \u226b d. Both algorithms make use of a \u201crandomized sketch\u201d of A of the form A(\u03a01A)\n\u2020\u03a02, where \u03a01 is an \u01eb-FJLT and \u03a02 is an \u01eb-JLT. We start with a high-level description of the basic ideas."}, {"heading": "3.1 Outline of our basic approach", "text": "Recall that our first goal is to approximate, for all i \u2208 [n], the quantities\n\u2113i = \u2225 \u2225U(i) \u2225 \u2225 2\n2 =\n\u2225 \u2225eTi U \u2225 \u2225\n2 2 , (11)\n6Again, when no confusion can arise, we will use \u03a0FJLT to denote a specific SRHT or the distribution on matrices implied by the randomized process for constructing an SRHT.\nwhere ei is a standard basis vector. The hard part of computing the scores \u2113i according to Eqn. (11) is computing an orthogonal matrix U spanning the range of A, which takes O(nd2) time. Since UUT = AA\u2020, it follows that\n\u2113i = \u2225 \u2225eTi UU T \u2225 \u2225\n2 2 =\n\u2225 \u2225 \u2225 eTi AA \u2020 \u2225 \u2225 \u2225 2\n2 =\n\u2225 \u2225\n\u2225 (AA\u2020)(i)\n\u2225 \u2225 \u2225 2\n2 , (12)\nwhere the first equality follows from the orthogonality of (the columns of) U . The hard part of computing the scores \u2113i according to Eqn. (12) is two-fold: first, computing the pseudoinverse; and second, performing the matrix-matrix multiplication of A and A\u2020. Both of these procedures take O(nd2) time. As we will see, we can get around both of these bottlenecks by the judicious application of random projections to Eqn. (12).\nTo get around the bottleneck of O(nd2) time due to computing A\u2020 in Eqn. (12), we will compute the pseudoinverse of a \u201csmaller\u201d matrix that approximates A. A necessary condition for such a smaller matrix is that it preserves rank. So, na\u0308\u0131ve ideas such as uniformly sampling r1 \u226a n rows from A and computing the pseudoinverse of this sampled matrix will not work well for an arbitrary A. For example, this idea will fail (with high probability) to return a meaningful approximation for matrices consisting of n \u2212 1 identical rows and a single row with a nonzero component in the direction perpendicular to that the identical rows; finding that \u201coutlying\u201d row is crucial to obtaining a relative-error approximation. This is where the SRHT enters, since it preserves important structures of A, in particular its rank, by first rotating A to a random basis and then uniformly sampling rows from the rotated matrix (see [22] for more details). More formally, recall that the SVD of A is U\u03a3V T and let \u03a01 \u2208 Rr1\u00d7n be an \u01eb-FJLT for U (using, for example, the SRHT of Lemma 3 with the appropriate choice for r1). Then, one could approximate the \u2113i\u2019s of Eqn. (12) by\n\u2113\u0302i = \u2225 \u2225 \u2225 eTi A (\u03a01A) \u2020 \u2225 \u2225 \u2225 2\n2 , (13)\nwhere we approximated the n \u00d7 d matrix A by the r1 \u00d7 d matrix \u03a01A. Computing A (\u03a01A)\u2020 in this way takes O (ndr1) time, which is not efficient because r1 > d (from Lemma 3).\nTo get around this bottleneck, recall that we only need the Euclidean norms of the rows of the matrix A (\u03a01A)\n\u2020 \u2208 Rn\u00d7r1 . Thus, we can further reduce the dimensionality of this matrix by using an \u01eb-JLT to reduce the dimension r1 = \u2126(d) to r2 = O(lnn). Specifically, let \u03a0 T 2 \u2208 Rr2\u00d7r1 be an \u01eb-JLT for the rows of A (\u03a01A) \u2020 (viewed as n vectors in Rr1) and consider the matrix \u2126 = A (\u03a01A) \u2020\u03a02. This n\u00d7 r2 matrix \u2126 may be viewed as our \u201crandomized sketch\u201d of the rows of AA\u2020. Then, we can compute and return\n\u2113\u0303i = \u2225 \u2225 \u2225 eTi A (\u03a01A) \u2020\u03a02 \u2225 \u2225 \u2225 2\n2 , (14)\nfor each i \u2208 [n], which is essentially what Algorithm 1 does. Not surprisingly, the sketch A (\u03a01A)\n\u2020\u03a02 can be used in other ways: for example, by considering the dot product between two different rows of this randomized sketching matrix (and some additional manipulations) Algorithm 2 approximates the large cross-leverage scores of A."}, {"heading": "3.2 Approximating all the statistical leverage scores", "text": "Our first main result is Algorithm 1, which takes as input an n \u00d7 d matrix A and an error parameter \u01eb \u2208 (0, 1/2], and returns as output numbers \u2113\u0303i, i \u2208 [n]. Although the basic idea to approximate \u2225 \u2225(AA\u2020)(i) \u2225 \u2225 2 was described in the previous section, we can improve the efficiency of our approach by avoiding the full sketch of the pseudoinverse. In particular, let A\u0302 = \u03a01A and let its SVD be A\u0302 = U\nA\u0302 \u03a3 A\u0302 V T A\u0302 . Let R\u22121 = V A\u0302 \u03a3\u22121 A\u0302 and note that R\u22121 \u2208 Rd\u00d7d is an orthogonalizer\nInput: A \u2208 Rn\u00d7d (with SVD A = U\u03a3V T ), error parameter \u01eb \u2208 (0, 1/2]. Output: \u2113\u0303i, i \u2208 [n].\n1. Let \u03a01 \u2208 Rr1\u00d7n be an \u01eb-FJLT for U , using Lemma 3 with\nr1 = \u2126\n(\nd lnn\n\u01eb2 ln\n(\nd lnn\n\u01eb2\n))\n.\n2. Compute \u03a01A \u2208 Rr1\u00d7d and its SVD, \u03a01A = U\u03a01A\u03a3\u03a01AV T\u03a01A. Let R\u22121 = V\u03a01A\u03a3 \u22121 \u03a01A\n\u2208 Rd\u00d7d. (Alternatively, R could be computed by a QR factorization of \u03a01A.)\n3. View the normalized rows of AR\u22121 \u2208 Rn\u00d7d as n vectors in Rd, and construct \u03a02 \u2208 Rd\u00d7r2 to be an \u01eb-JLT for n2 vectors (the aforementioned n vectors and their n2 \u2212 n pairwise sums), using Lemma 1 with\nr2 = O ( \u01eb\u22122 lnn ) .\n4. Construct the matrix product \u2126 = AR\u22121\u03a02.\n5. For all i \u2208 [n] compute and return \u2113\u0303i = \u2225 \u2225\u2126(i) \u2225 \u2225 2\n2 .\nAlgorithm 1: Approximating the (diagonal) statistical leverage scores \u2113i.\nfor A\u0302 since U A\u0302 = A\u0302R\u22121 is an orthogonal matrix.7 In addition, note that AR\u22121 is approximately orthogonal. Thus, we can compute AR\u22121 and use it as an approximate orthogonal basis for A and then compute \u2113\u0302i as the squared row-norms of AR\n\u22121. The next lemma states that this is exactly what our main algorithm does; even more, we could get the same estimates by using any \u201corthogonalizer\u201d of \u03a01A.\nLemma 4. Let R\u22121 be such that Q = \u03a01AR\u22121 is an orthogonal matrix with rank(Q) = rank(\u03a01A). Then, \u2225 \u2225(AR\u22121)(i) \u2225 \u2225 2\n2 = \u2113\u0302i.\nProof. Since A\u0302 = \u03a01A has rank d (by Lemma 2) and R \u22121 preserves this rank, R\u22121 is a d \u00d7 d invertible matrix. Using A\u0302 = QR and properties of the pseudoinverse, we get ( A\u0302 )\u2020 = R\u22121QT .\nThus,\n\u2113\u0302i = \u2225 \u2225 \u2225 (A (\u03a01A) \u2020)(i) \u2225 \u2225 \u2225 2\n2 =\n\u2225 \u2225 \u2225 ( AR\u22121QT )\n(i)\n\u2225 \u2225 \u2225 2\n2 =\n\u2225 \u2225 \u2225 ( AR\u22121 )\n(i) QT\n\u2225 \u2225 \u2225 2\n2 =\n\u2225 \u2225 \u2225 ( AR\u22121 )\n(i)\n\u2225 \u2225 \u2225 2\n2 .\nThis lemma says that the \u2113\u0302i of Eqn. (13) can be computed with any QR decomposition, rather than with the SVD; but note that one would still have to post-multiply by \u03a02, as in Algorithm 1, in order to compute \u201cquickly\u201d the approximations of the leverage scores.\n7This preprocessing is reminiscent of how [44, 6] preprocessed the input to provide numerical implementations of the fast relative-error algorithm [22] for approximate LS approximation. From this perspective, Algorithm 1 can be viewed as specifying a particular basis Q, i.e., as choosing Q to be the left singular vectors of \u03a01A."}, {"heading": "3.3 Approximating the large cross-leverage scores", "text": "By combining Lemmas 6 and 7 (in Section 4.2 below) with the triangle inequality, one immediately obtains the following lemma.\nLemma 5. Let \u2126 be either the sketching matrix constructed by Algorithm 1, i.e., \u2126 = AR\u22121\u03a02, or \u2126 = A (\u03a01A)\n\u2020\u03a02 as described in Section 3.1. Then, the pairwise dot-products of the rows of \u2126 are additive-error approximations to the leverage scores and cross-leverage scores:\n\u2223 \u2223\u3008U(i), U(j)\u3009 \u2212 \u3008\u2126(i),\u2126(j)\u3009 \u2223 \u2223 \u2264 3\u01eb 1\u2212 \u01eb \u2225 \u2225U(i) \u2225 \u2225 2 \u2225 \u2225U(j) \u2225 \u2225 2 .\nThat is, if one were interested in obtaining an approximation to all the cross-leverage scores to within additive error (and thus the diagonal statistical leverage scores to relative-error), then the algorithm which first computes \u2126 followed by all the pairwise inner products achieves this in time T (\u2126) + O ( n2r2 ) , where T (\u2126) is the time to compute \u2126 from Section 3.2 and r2 = O(\u01eb \u22122 lnn).8 The challenge is to avoid the n2 computational complexity and this can be done if one is interested only in the large cross-leverage scores.\nOur second main result is provided by Algorithms 2 and 3. Algorithm 2 takes as input an n\u00d7d matrix A, a parameter \u03ba > 1, and an error parameter \u01eb \u2208 (0, 1/2], and returns as output a subset of [n]\u00d7 [n] and estimates c\u0303ij satisfying Theorem 2. The first step of the algorithm is to compute the matrix \u2126 = AR\u22121\u03a02 constructed by Algorithm 1. Then, Algorithm 2 uses Algorithm 3 as a subroutine to compute \u201cheavy hitter\u201d pairs of rows from a matrix.\nInput: A \u2208 Rn\u00d7d and parameters \u03ba > 1, \u01eb \u2208 (0, 1/2]. Output: The set H consisting of pairs (i, j) together with estimates c\u0303ij satisfying Theorem 2.\n1. Compute the n\u00d7 r2 matrix \u2126 = AR\u22121\u03a02 from Algorithm 1.\n2. Use Algorithm 3 with inputs \u2126 and \u03ba\u2032 = \u03ba(1 + 30d\u01eb) to obtain the set H containing all the \u03ba\u2032-heavy pairs of \u2126.\n3. Return the pairs in H as the \u03ba-heavy pairs of A.\nAlgorithm 2: Approximating the large (off-diagonal) cross-leverage scores cij ."}, {"heading": "4 Proofs of our main theorems", "text": ""}, {"heading": "4.1 Sketch of the proof of Theorems 1 and 2", "text": "We will start by providing a sketch of the proof of Theorems 1 and 2. A detailed proof is provided in the next two subsections. In our analysis, we will condition on the events that \u03a01 \u2208 Rr1\u00d7n is an \u01eb-FJLT for U and \u03a02 \u2208 Rr1\u00d7r2 is an \u01eb-JLT for n2 points in Rr1 . Note that by setting \u03b4 = 0.1 in Lemma 1, both events hold with probability at least 0.8, which is equal to the success probability of Theorems 1 and 2. The algorithm estimates \u2113\u0303i = \u2016u\u0303i\u201622, where u\u0303i = eTi A(\u03a01A)\u2020\u03a02. First,\n8The exact algorithm which computes a basis first and then the pairwise inner products requires O(nd2 + n2d) time. Thus, by using the sketch, we can already improve on this running time by a factor of d/ lnn.\nInput: X \u2208 Rn\u00d7r with rows x1, . . . , xn and a parameter \u03ba > 1. Output: H = {(i, j), c\u0303ij} containing all heavy (unordered) pairs. The pair (i, j), c\u0303ij \u2208 H if and only if c\u03032ij = \u3008xi, xj\u30092 \u2265 \u2225 \u2225XTX \u2225 \u2225 2 F /\u03ba.\n1: Compute the norms \u2016xi\u20162 and sort the rows according to norm, so that \u2016x1\u20162 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u2016xn\u20162. 2: H \u2190 {}; z1 \u2190 n; z2 \u2190 1. 3: while z2 \u2264 z1 do 4: while \u2016xz1\u201622\u2016xz2\u201622 < \u2225 \u2225XTX \u2225 \u2225 2 F /\u03ba do 5: z2 \u2190 z2 + 1. 6: if z2 > z1 then 7: return H. 8: end if\n9: end while\n10: for each pair (i, j) where i = z1 and j \u2208 {z2, z2 + 1, . . . , z1} do 11: c\u03032ij = \u3008xi, xj\u30092. 12: if c\u03032ij \u2265 \u2225 \u2225XTX \u2225 \u2225 2 F /\u03ba then 13: add (i, j) and c\u0303ij to H. 14: end if 15: z1 \u2190 z1 \u2212 1. 16: end for 17: end while 18: return H.\nAlgorithm 3: Computing heavy pairs of a matrix.\nobserve that the sole purpose of \u03a02 is to improve the running time while preserving pairwise inner products; this is achieved because \u03a02 is an \u01eb-JLT for n 2 points. So, the results will follow if\neTi A(\u03a01A) \u2020((\u03a01A) \u2020)TAT ej \u2248 eTi UUT ej and (\u03a01A) \u2020 can be computed efficiently. Since \u03a01 is an \u01eb-FJLT for U , where A = U\u03a3V T , (\u03a01A)\u2020 can be computed in O(nd ln r1 + r1d 2) time. By Lemma 2, (\u03a01A) \u2020 = V \u03a3\u22121(\u03a01U)\u2020, and so\neTi A(\u03a01A) \u2020((\u03a01A) \u2020)TAT ej = e T i U(\u03a01U) \u2020(\u03a01U) \u2020TUT ej .\nSince \u03a01 is an \u01eb-FJLT for U , it follows that (\u03a01U) \u2020(\u03a01U)\u2020 T \u2248 Id, i.e., that \u03a01U is approximately orthogonal. Theorem 1 follows from this basic idea. However, in order to prove Theorem 2, having a sketch which preserves inner products alone is not sufficient. We also need a fast algorithm to identify the large inner products and to relate these to the actual cross-leverage scores. Indeed, it is possible to efficiently find pairs of rows in a general matrix with large inner products. Combining this with the fact that the inner products are preserved, we obtain Theorem 2."}, {"heading": "4.2 Proof of Theorem 1", "text": "We condition all our analysis on the events that \u03a01 \u2208 Rr1\u00d7n is an \u01eb-FJLT for U and \u03a02 \u2208 Rr1\u00d7r2 is an \u01eb-JLT for n2 points in Rr1 . Define\nu\u0302i = e T i A(\u03a01A)\n\u2020, and\nu\u0303i = e T i A(\u03a01A) \u2020\u03a02.\nThen, \u2113\u0302i = \u2016u\u0302i\u201622 and \u2113\u0303i = \u2016u\u0303i\u201622. The proof will follow from the following two lemmas. Lemma 6. For i, j \u2208 [n],\n\u2223 \u2223\u3008U(i), U(j)\u3009 \u2212 \u3008u\u0302i, u\u0302j\u3009 \u2223 \u2223 \u2264 \u01eb 1\u2212 \u01eb \u2225 \u2225U(i) \u2225 \u2225 2 \u2225 \u2225U(j) \u2225 \u2225 2 . (15)\nLemma 7. For i, j \u2208 [n], |\u3008u\u0302i, u\u0302j\u3009 \u2212 \u3008u\u0303i, u\u0303j\u3009| \u2264 2\u01eb\u2016u\u0302i\u20162\u2016u\u0302j\u20162. (16)\nLemma 6 states that \u3008u\u0302i, u\u0302j\u3009 is an additive error approximation to all the cross-leverage scores (i 6= j) and a relative error approximation for the diagonals (i = j). Similarly, Lemma 7 shows that these cross-leverage scores are preserved by \u03a02. Indeed, with i = j, from Lemma 6 we have |\u2113\u0302i \u2212 \u2113i| \u2264 \u01eb1\u2212\u01eb\u2113i, and from Lemma 7 we have |\u2113\u0302i \u2212 \u2113\u0303i| \u2264 2\u01eb\u2113\u0302i. Using the triangle inequality and \u01eb \u2264 1/2:\n\u2223 \u2223 \u2223 \u2113i \u2212 \u2113\u0303i \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 \u2113i \u2212 \u2113\u0302i + \u2113\u0302i \u2212 \u2113\u0303i \u2223 \u2223 \u2223 \u2264 \u2223 \u2223 \u2223 \u2113i \u2212 \u2113\u0302i \u2223 \u2223 \u2223 + \u2223 \u2223 \u2223 \u2113\u0302i \u2212 \u2113\u0303i \u2223 \u2223 \u2223 \u2264\n(\n\u01eb 1\u2212 \u01eb + 2\u01eb ) \u2113i \u2264 4\u01eb\u2113i.\nThe theorem follows after rescaling \u01eb.\nProof of Lemma 6. Let A = U\u03a3V T . Using this SVD of A and Eqn. (10) in Lemma 2,\n\u3008u\u0302i, u\u0302j\u3009 = eTi U\u03a3V TV \u03a3\u22121 (\u03a01U)\u2020 (\u03a01U)\u2020 T \u03a3\u22121V TV \u03a3UT ej = e T i U (\u03a01U) \u2020 (\u03a01U) \u2020TUT ej.\nBy performing standard manipulations, we can now bound \u2223 \u2223\u3008U(i), U(j)\u3009 \u2212 \u3008u\u0302i, u\u0302j\u3009 \u2223 \u2223:\n\u2223 \u2223\u3008U(i), U(j)\u3009 \u2212 \u3008u\u0302i, u\u0302j\u3009 \u2223 \u2223 = eTi UU T ej \u2212 eTi U (\u03a01U)\u2020 (\u03a01U)\u2020T UT ej\n= eTi U ( Id \u2212 (\u03a01U)\u2020 (\u03a01U)\u2020T ) UT ej \u2264 \u2225 \u2225 \u2225 Id \u2212 (\u03a01U)\u2020 (\u03a01U)\u2020T \u2225 \u2225 \u2225\n2\n\u2225 \u2225U(i) \u2225 \u2225\n2\n\u2225 \u2225U(j) \u2225 \u2225\n2 .\nLet the SVD of \u03a8 = \u03a01U be \u03a8 = U\u03a8\u03a3\u03a8V T \u03a8 , where V\u03a8 is a full rotation in d dimensions (because rank(A) = rank(\u03a01U)). Then, \u03a8 \u2020\u03a8\u2020 T = V\u03a8\u03a3 \u22122 \u03a8 V T \u03a8 . Thus,\n\u2223 \u2223\u3008U(i), U(j)\u3009 \u2212 \u3008u\u0302i, u\u0302j\u3009 \u2223 \u2223 \u2264 \u2225 \u2225Id \u2212 V\u03a8\u03a3\u22122\u03a8 V T\u03a8 \u2225 \u2225\n2\n\u2225 \u2225U(i) \u2225 \u2225\n2\n\u2225 \u2225U(j) \u2225 \u2225\n2\n= \u2225 \u2225V\u03a8V T \u03a8 \u2212 V\u03a8\u03a3\u22122\u03a8 V T\u03a8 \u2225 \u2225\n2\n\u2225 \u2225U(i) \u2225 \u2225\n2\n\u2225 \u2225U(j) \u2225 \u2225\n2\n= \u2225 \u2225Id \u2212 \u03a3\u22122\u03a8 \u2225 \u2225\n2\n\u2225 \u2225U(i) \u2225 \u2225\n2\n\u2225 \u2225U(j) \u2225 \u2225\n2 ,\nwhere we used the fact that V\u03a8V T \u03a8 = V T \u03a8 V\u03a8 = Id and the unitary invariance of the spectral norm. Finally, using Eqn. (9) of Lemma 2 the result follows.\nProof of Lemma 7. Since \u03a02 is an \u01eb-JLT for n 2 vectors, it preserves the norms of an arbitrary (but fixed) collection of n2 vectors. Let xi = u\u0302i/\u2016u\u0302i\u20162. Consider the following n2 vectors:\nxi for i \u2208 [n], and xi + xj for i, j \u2208 [n], i 6= j.\nBy the \u01eb-JLT property of \u03a02 and the fact that \u2016xi\u20162 = 1,\n1\u2212 \u01eb \u2264 \u2016xi\u03a02\u201622 \u2264 1 + \u01eb for i \u2208 [n], and (17) (1\u2212 \u01eb)\u2016xi + xj\u201622 \u2264 \u2016xi\u03a02 + xj\u03a02\u2016 2 2 \u2264 (1 + \u01eb)\u2016xi + xj\u2016 2 2 for i, j \u2208 [n], i 6= j. (18)\nCombining Eqns. (17) and (18) after expanding the squares using the identity \u2016a+ b\u20162 = \u2016a\u20162 + \u2016b\u20162 + 2\u3008a, b\u3009, substituting \u2016xi\u2016 = 1, and after some algebra, we obtain\n\u3008xi, xj\u3009 \u2212 2\u01eb \u2264 \u3008xi\u03a02, xj\u03a02\u3009 \u2264 \u3008xi, xj\u3009+ 2\u01eb.\nTo conclude the proof, multiply throughout by \u2016u\u0302i\u2016\u2016u\u0302j\u2016 and use the homogeneity of the inner product, together with the linearity of \u03a02, to obtain:\n\u3008u\u0302i, u\u0302j\u3009 \u2212 2\u01eb\u2016u\u0302i\u2016\u2016u\u0302j\u2016 \u2264 \u3008u\u0302i\u03a02, u\u0302j\u03a02\u3009 \u2264 \u3008u\u0302i, u\u0302j\u3009+ 2\u01eb\u2016u\u0302i\u2016\u2016u\u0302j\u2016.\nRunning Times. By Lemma 4, we can use V\u03a01A\u03a3 \u22121 \u03a01A instead of (\u03a01A) \u2020 and obtain the same estimates. Since \u03a01 is an \u01eb-FJLT, the product \u03a01A can be computed in O(nd ln r1) while its SVD takes an additional O(r1d 2) time to return V\u03a01A\u03a3 \u22121 \u03a01A\n\u2208 Rd\u00d7d. Since \u03a02 \u2208 Rd\u00d7r2 , we obtain V\u03a01A\u03a3 \u22121 \u03a01A\n\u03a02 \u2208 Rd\u00d7r2 in an additional O(r2d2) time. Finally, premultiplying by A takes O(ndr2) time, and computing and returning the squared row-norms of \u2126 = AV\u03a01A\u03a3 \u22121 \u03a01A\n\u03a02 \u2208 Rn\u00d7r2 takes O (nr2) time. So, the total running time is the sum of all these operations, which is\nO(nd ln r1 + ndr2 + r1d 2 + r2d 2).\nFor our implementations of the \u01eb-JLTs and \u01eb-FJLTs (\u03b4 = 0.1), r1 = O ( \u01eb\u22122d (lnn) ( ln ( \u01eb\u22122d ln n ))) and r2 = O(\u01eb \u22122 lnn). It follows that the asymptotic running time is\nO ( nd ln ( d\u01eb\u22121 ) + nd\u01eb\u22122 lnn+ d3\u01eb\u22122 (lnn) ( ln ( d\u01eb\u22121 ))) .\nTo simplify, suppose that d \u2264 n \u2264 ed and treat \u01eb as a constant. Then, the asymptotic running time is\nO ( nd lnn+ d3 (lnn) (ln d) ) ."}, {"heading": "4.3 Proof of Theorem 2", "text": "We first construct an algorithm to estimate the large inner products among the rows of an arbitrary matrix X \u2208 Rn\u00d7r with n > r. This general algorithm will be applied to the matrix \u2126 = AV\u03a01A\u03a3 \u22121 \u03a01A\n\u03a02. Let x1, . . . , xn denote the rows of X; for a given \u03ba > 1, the pair (i, j) is heavy if\n\u3008xi, xj\u30092 \u2265 1\n\u03ba\n\u2225 \u2225XTX \u2225 \u2225\n2 F .\nBy the Cauchy-Schwarz inequality, this implies that\n\u2016xi\u201622\u2016xj\u201622 \u2265 1\n\u03ba\n\u2225 \u2225XTX \u2225 \u2225\n2 F , (19)\nso it suffices to find all the pairs (i, j) for which Eqn. (19) holds. We will call such pairs normheavy. Let s be the number of norm-heavy pairs satisfying Eqn. (19). We first bound the number of such pairs.\nLemma 8. Using the above notation, s \u2264 \u03bar.\nProof. Observe that\nn \u2211\ni,j=1\n\u2016xi\u201622\u2016xj\u201622 = ( n \u2211\ni=1\n\u2016xi\u201622\n)2 = \u2016X\u20164F = ( r \u2211\ni=1\n\u03c32i\n)2\n,\nwhere \u03c31, . . . , \u03c3r are the singular values of X. To conclude, by the definition of a heavy pair,\n\u2211\ni,j\n\u2016xi\u201622\u2016xj\u201622 \u2265 s\n\u03ba\n\u2225 \u2225XTX \u2225 \u2225\n2 F =\ns\n\u03ba\nr \u2211\ni=1\n\u03c34i \u2265 s\n\u03bar\n(\nr \u2211\ni=1\n\u03c32i\n)2\n,\nwhere the last inequality follows by Cauchy-Schwarz.\nAlgorithm 3 starts by computing the norms \u2016xi\u201622 for all i \u2208 [n] and sorting them (in O (nr + n lnn) time) so that we can assume that \u2016x1\u20162 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u2016xn\u20162. Then, we initialize the set of norm-heavy pairs to H = {} and we also initialize two pointers z1 = n and z2 = 1. The basic loop in the algorithm checks if z2 > z1 and stops if that is the case. Otherwise, we increment z2 to the first pair (z1, z2) that is norm-heavy. If none of pairs are norm heavy (i.e., z2 > z1 occurs), then we stop and output H; otherwise, we add (z1, z2), (z1, z2 + 1), . . . , (z1, z1) to H. This basic loop computes all pairs (z1, i) with i \u2264 z1 that are norm-heavy. Next, we decrease z1 by one and if z1 < z2 we stop and output H; otherwise, we repeat the basic loop. Note that in the basic loop z2 is always incremented. This occurs whenever the pair (z1, z2) is not norm-heavy. Since z2 can be incremented at most n times, the number of times we check whether a pair is norm-heavy and fail is at most n. Every successful check results in the addition of at least one norm-heavy pair into H and thus the number of times we check if a pair is norm heavy (a constant-time operation) is at most n + s. The number of pair additions into H is exactly s and thus the total running time is O(nr+ n lnn+ s). Finally, we must check each norm-heavy pair to verify whether or not it is actually heavy by computing s inner products vectors in Rr; this can be done in O(sr) time. Using s \u2264 \u03bar we get the following lemma.\nLemma 9. Algorithm 3 returns H including all the heavy pairs of X in O(nr + \u03bar2 + n lnn) time.\nTo complete the proof, we apply Algorithm 3 with \u2126 = AV\u03a01A\u03a3 \u22121 \u03a01A \u03a02 \u2208 Rn\u00d7r2 , where r2 = O(\u01eb\u22122 lnn). Let u\u03031, . . . , u\u0303n denote the rows of \u2126 and recall that A = U\u03a3V T . Let u1, . . . , un denote the rows of U ; then, from Lemma 5,\n\u3008ui, uj\u3009 \u2212 3\u01eb 1\u2212 \u01eb\u2016ui\u2016\u2016uj\u2016 \u2264 \u3008u\u0303i, u\u0303j\u3009 \u2264 \u3008ui, uj\u3009+ 3\u01eb 1\u2212 \u01eb\u2016ui\u2016\u2016uj\u2016. (20)\nGiven \u01eb, \u03ba, assume that for the pair of vectors ui and uj\n\u3008ui, uj\u30092 \u2265 1\n\u03ba\n\u2225 \u2225UTU \u2225 \u2225\n2 F + 12\u01eb\u2016ui\u20162\u2016uj\u20162 =\nd \u03ba + 12\u01eb\u2016ui\u20162\u2016uj\u20162,\nwhere the last equality follows from \u2225 \u2225UTU \u2225 \u2225 2 F = \u2016Id\u20162F = d. By Eqn. (20), after squaring and using \u01eb < 0.5,\n\u3008ui, uj\u30092 \u2212 12\u01eb\u2016ui\u20162\u01eb\u2016uj\u20162 \u2264 \u3008u\u0303i, u\u0303j\u30092 \u2264 \u3008ui, uj\u30092 + 30\u01eb\u2016ui\u20162\u2016uj\u20162. (21)\nThus, \u3008u\u0303i, u\u0303j\u30092 \u2265 d/\u03ba and summing Eqn. (21) over all i, j we get \u2225 \u2225\u2126T\u2126 \u2225 \u2225 2 F \u2264 d + 30\u01ebd2, or, equivalently,\nd \u2265 \u2225 \u2225\u2126T\u2126 \u2225 \u2225 2 F\n1 + 30d\u01eb .\nWe conclude that\n\u3008ui, uj\u30092 \u2265 d\n\u03ba + 12\u01eb\u2016ui\u20162\u2016uj\u20162 =\u21d2 \u3008u\u0303i, u\u0303j\u30092 \u2265\nd \u03ba \u2265\n\u2225 \u2225\u2126T\u2126 \u2225 \u2225 2\nF\n\u03ba(1 + 30d\u01eb) . (22)\nBy construction, Algorithm 3 is invoked with \u03ba\u2032 = \u03ba \u2225 \u2225\u2126T\u2126 \u2225 \u2225 2\nF /d and thus it finds all pairs with\n\u3008u\u0303i, u\u0303j\u30092 \u2265 \u2225 \u2225\u2126T\u2126 \u2225 \u2225 2 F /\u03ba\u2032 = d/\u03ba. This set contains all pairs for which\n\u3008ui, uj\u30092 \u2265 d\n\u03ba + 12\u01eb\u2016ui\u20162\u2016uj\u20162.\nFurther, since every pair returned satisfies \u3008u\u0303i, u\u0303j\u30092 \u2265 d/\u03ba, by Eqn. (21), cij \u2265 d/\u03ba\u221230\u01eb\u2113i\u2113j . This proves the first claim of the Theorem; the second claim follows analogously from Eqn. (21).\nUsing Lemma 9, the running time of our approach is O ( nr2 + \u03ba \u2032r22 + n lnn ) . Since r2 =\nO ( \u01eb\u22122 lnn ) , and, by Eqn. (22), \u03ba\u2032 = \u03ba \u2225 \u2225\u2126T\u2126 \u2225 \u2225 2 F /d \u2264 \u03ba(1 + 30d\u01eb), the overall running time is O ( \u01eb\u22122n lnn+ \u01eb\u22123\u03bad ln2 n ) ."}, {"heading": "5 Extending our algorithm to general matrices", "text": "In this section, we will describe an important extension of our main result, namely the computation of the statistical leverage scores relative to the best rank-k approximation to a general matrix A. More specifically, we consider the estimation of leverage scores for the case of general \u201cfat\u201d matrices, namely input matrices A \u2208 Rn\u00d7d, where both n and d are large, e.g., when d = n or d = \u0398(n). Clearly, the leverage scores of any full rank n\u00d7 n matrix are exactly uniform. The problem becomes interesting if one specifies a rank parameter k \u226a min{n, d}. This may arise when the numerical rank of A is small (e.g., in some scientific computing applications, more than 99% of the spectral norm of A may be captured by some k \u226a min{n, d} directions), or, more generally, when one is interested in some low rank approximation to A (e.g., in some data analysis applications, a reasonable fraction or even the majority of the Frobenius norm of A may be captured by some k \u226a min{n, d} directions, where k is determined by some exogenously-specified model selection criterion). Thus, assume that in addition to a general n \u00d7 d matrix A, a rank parameter k < min{n, d} is specified. In this case, we wish to obtain the statistical leverage scores \u2113i = \u2225 \u2225(Uk)(i) \u2225 \u2225 2\n2 for Ak = Uk\u03a3kV T k , the best rank-k approximation to A. Equivalently, we\nseek the normalized leverage scores\npi = \u2113i k . (23)\nNote that \u2211n i=1 pi = 1 since \u2211n i=1 \u2113i = \u2016Uk\u20162F = k. Unfortunately, as stated, this is an ill-posed problem. Indeed, consider the degenerate case when A = In (i.e., the n \u00d7 n identity matrix). In this case, Uk is not unique and the leverage scores are not well-defined. Moreover, for the obvious (\nn k\n)\nequivalent choices for Uk, the leverage scores defined according to any one of these choices do not provide a relative error approximation to the leverage scores defined according to any other choices. More generally, removing this trivial degeneracy does not help. Consider the matrix\nA =\n(\nIk 0 0 (1\u2212 \u03b3)In\u2212k\n)\n\u2208 Rn\u00d7n.\nIn this example, the leverage scores for Ak are well defined. However, as \u03b3 \u2192 0, it is not possible to distinguish between the top-k singular space and its complement. This example suggests that it should be possible to obtain some result conditioning on the spectral gap at the kth singular value. For example, one might assume that \u03c32k\u2212\u03c32k+1 \u2265 \u03b3 > 0, in which case the parameter \u03b3 would play an important role in the ability to solve this problem. Any algorithm which cannot distinguish the singular values with an error less than \u03b3 will confuse the k-th and (k+1)-th singular vectors and consequently will fail to get an accurate approximation to the leverage scores for Ak.\nIn the following, we take a more natural approach which leads to a clean problem formulation. To do so, recall that the leverage scores and the related normalized leverage scores of Eqn. (23) are used to approximate the matrix in some way, e.g., we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc. In all these cases, we only care that the estimated leverage scores are a good approximation to the leverage scores of some \u201cgood\u201d low-rank approximation to A. The following definition captures the notion of a set of rank-k matrices that are good approximations to A.\nDefinition 2. Given A \u2208 Rn\u00d7d and a rank parameter k \u226a min {n, d}, let Ak be the best rank-k approximation to A. Define the set S\u01eb of rank-k matrices that are good approximations to A as follows (for \u03be = 2, F ):\nS\u01eb = { X \u2208 Rn\u00d7d : rank(X) = k and \u2016A\u2212X\u2016\u03be \u2264 (1 + \u01eb)\u2016A\u2212Ak\u2016\u03be } . (24)\nWe are now ready to define our approximations to the normalized leverage scores of any matrix A \u2208 Rn\u00d7d given a rank parameter k \u226a min {n, d}. Instead of seeking to approximate the pi of Eqn. (23) (a problem that is ill-posed as discussed above), we will be satisfied if we can approximate the normalized leverage scores of some matrix X \u2208 S\u01eb. This is an interesting relaxation of the task at hand: all matrices X that are sufficiently close to Ak are essentially equivalent, since they can be used instead of Ak in applications.\nDefinition 3. Given A \u2208 Rn\u00d7d and a rank parameter k \u226a min {n, d}, let S\u01eb be the set of matrices of Definition 2. We call the numbers p\u0302i (for all i \u2208 [n]) \u03b2-approximations to the normalized leverage scores of Ak (the best rank-k approximation to A) if, for some matrix X \u2208 S\u01eb,\np\u0302i \u2265 \u03b2 \u2225 \u2225(UX)(i) \u2225 \u2225 2 2\nk and\nn \u2211\ni=1\np\u0302i = 1.\nHere UX \u2208 Rn\u00d7k is the matrix of the left singular vectors of X.\nThus, we will seek algorithms whose output is a set of numbers, with the requirement that those numbers are good approximations to the normalized leverage scores of some matrix X \u2208 S\u01eb (instead of Ak). This removes the ill-posedness of the original problem. Next, we will give two examples of algorithms that compute such \u03b2-approximations to the normalized leverage scores of a general matrix A with a rank parameter k for two popular norms, the spectral norm and the Frobenius norm.9"}, {"heading": "5.1 Leverage Scores for Spectral Norm Approximators", "text": "Algorithm 4 approximates the statistical leverage scores of a general matrix A with rank parameter k in the spectral norm case. It takes as inputs a matrix A \u2208Rn\u00d7d with rank(A) = \u03c1 and a rank parameter k \u226a \u03c1, and outputs a set of numbers p\u0302i for all i \u2208 [n], namely our approximations to the normalized leverage scores of A with rank parameter k.\nThe next lemma argues that there exists a matrix X \u2208 Rn\u00d7d of rank k that is sufficiently close to A (in particular, it is a member of S\u01eb with constant probability) and, additionally, can be written as X = BY, where Y \u2208 R2k\u00d7d is a matrix of rank k. A version of this lemma was\n9Note that we will not compute S\u01eb, but our algorithms will compute a matrix in that set. Moreover, that matrix can be used for high-quality low-rank matrix approximation. See the comments in Section 1.4 for more details.\nInput: A \u2208 Rn\u00d7d with rank(A) = \u03c1 and a rank parameter k \u226a \u03c1 Output: p\u0302i, i \u2208 [n]\n1. Construct \u03a0 \u2208 Rd\u00d72k with entries drawn in i.i.d. trials from the normal distribution N (0, 1).\n2. Compute B = ( AAT )q A\u03a0 \u2208 Rn\u00d72k, with q as in Eqn. (26).\n3. Approximately compute the statistical leverage scores of the \u201ctall\u201d matrix B by calling Algorithm 1 with inputs B and \u01eb; let \u2113\u0302i (for all i \u2208 [n]) be the outputs of Algorithm 1.\n4. Return\np\u0302i = \u2113\u0302i\n\u2211n j=1 \u2113\u0302j\n(25)\nfor all i \u2208 [n].\nAlgorithm 4: Approximating the statistical leverage scores of a general matrix A (spectral norm case).\nessentially proven in [27], but see also [43] for computational details; we will use the version of the lemma that appeared in [10]. (See also the conference version [11], but in the remainder we refer to the technical report version [10] for consistency of numbering.) Note that for our purposes in this section, the computation of Y is not relevant and we defer the reader to [27, 10] for details.\nLemma 10 (Spectral Sketch). Given A \u2208 Rn\u00d7d of rank \u03c1, a rank parameter k such that 2 \u2264 k < \u03c1, and an error parameter \u01eb such that 0 < \u01eb < 1, let \u03a0 \u2208 Rd\u00d72k be a standard Gaussian matrix (with entries selected in i.i.d. trials from N (0, 1)). If B = ( AAT )q A\u03a0, where\nq \u2265\n\n   \nln ( 1 + \u221a\nk k\u22121 + e\n\u221a\n2 k\n\u221a min {n, d} \u2212 k )\n2 ln (1 + \u01eb/10) \u2212 1/2\n\n    , (26)\nthen there exists a matrix X \u2208 Rn\u00d7d of rank k satisfying X = BY (with Y \u2208 R2k\u00d7d) such that\nE [\u2016A\u2212X\u20162] \u2264 ( 1 + \u01eb\n10\n)\n\u2016A\u2212Ak\u20162.\nThe matrix B can be computed in O (ndkq) time.\nThis version of the above lemma is proven in [10].10 Now, since X has rank k, it follows that \u2016A\u2212X\u20162 \u2265 \u2016A\u2212Ak\u20162 and thus we can consider the non-negative random variable \u2016A\u2212X\u20162 \u2212 \u2016A\u2212Ak\u20162 and apply Markov\u2019s inequality to get that\n\u2016A\u2212X\u20162 \u2212 \u2016A\u2212Ak\u20162 \u2264 \u01eb\u2016A\u2212Ak\u20162 holds with probability at least 0.9. Thus, X \u2208 S\u01eb with probability at least 0.9.\n10More specifically, the proof may be found in Lemma 32 and in particular in Eqn. (14) in Section A.2; note that for our purposes here we replaced \u01eb/ \u221a 2 by \u01eb/10 after adjusting q accordingly.\nThe next step of the proposed algorithm is to approximately compute the leverage scores of B \u2208 Rn\u00d72k via Algorithm 1. Under the assumptions of Theorem 1, this step runs in O ( nk\u01eb\u22122 lnn ) time. Let UX \u2208 Rn\u00d7k be the matrix containing the left singular vectors of the matrix X of Lemma 10. Then, since X = BY by Lemma 10, it follows that\nUB = [UX UR]\nis a basis for the subspace spanned by the columns of B. Here UR \u2208 Rn\u00d7k is an orthogonal matrix whose columns are perpendicular to the columns of UX . Now consider the approximate leverage scores \u2113\u0302i computed by Algorithm 1 and note that (by Theorem 1),\n\u2223 \u2223 \u2223 \u2223 \u2113\u0302i \u2212 \u2225 \u2225 \u2225 (UB)(i) \u2225 \u2225 \u2225 2\n2\n\u2223 \u2223 \u2223 \u2223 \u2264 \u01eb \u2225 \u2225 \u2225 (UB)(i) \u2225 \u2225 \u2225 2\n2\nholds with probability at least 0.8 for all i \u2208 [n]. It follows that n \u2211\nj=1\n\u2113\u0302j \u2264 (1 + \u01eb) n \u2211\nj=1\n\u2225 \u2225 \u2225 (UB)(j) \u2225 \u2225 \u2225 2\n2 = (1 + \u01eb)\nn \u2211\nj=1\n\u2016UB\u20162F = 2 (1 + \u01eb) k.\nFinally,\np\u0302i = \u2113\u0302i\n\u2211n j=1 \u2113\u0302j\n\u2265 (1\u2212 \u01eb)\n\u2225 \u2225 \u2225 (UB)(i) \u2225 \u2225 \u2225 2\n2 \u2211n\nj=1 \u2113\u0302j\n\u2265 (1\u2212 \u01eb)\n\u2225 \u2225 \u2225 (UX)(i) \u2225 \u2225 \u2225 2\n2 +\n\u2225 \u2225 \u2225 (UR)(i) \u2225 \u2225 \u2225 2\n2 \u2211n\nj=1 \u2113\u0302j\n\u2265 1\u2212 \u01eb 2\n\u2225 \u2225 \u2225 (UX)(i) \u2225 \u2225 \u2225 2\n2 \u2211n\nj=1 \u2113\u0302j\n\u2265 1\u2212 \u01eb 2 (1 + \u01eb)\n\u2225 \u2225 \u2225 (UX)(i) \u2225 \u2225 \u2225 2\n2\nk .\nClearly, \u2225 \u2225\n\u2225 (UX)(i)\n\u2225 \u2225 \u2225 2\n2 /k are the normalized leverage scores of the matrix X. Recall that X \u2208 S\u01eb\nwith probability at least 0.9 and use Definition 3 to conclude that the scores p\u0302i of Eqn. (25) are (\n1\u2212\u01eb 2(1+\u01eb)\n)\n-approximations to the normalized leverage scores of A with rank parameter k. The\nfollowing Theorem summarizes the above discussion:\nTheorem 3. Given A \u2208 Rn\u00d7d, a rank parameter k, and an accuracy parameter \u01eb, Algorithm 4 computes a set of normalized leverage scores p\u0302i that are ( 1\u2212\u01eb 2(1+\u01eb) ) -approximations to the normalized leverage scores of A with rank parameter k with probability at least 0.7. The proposed algorithm runs in\nO\n( ndk ln (min{n, d})\nln (1 + \u01eb) + nk\u01eb\u22122 lnn\n)\ntime.\nInput: A \u2208 Rn\u00d7d with rank(A) = \u03c1 and a rank parameter k \u226a \u03c1 Output: p\u0302i, i \u2208 [n]\n1. Let r be as in Eqn. (28) and construct \u03a0 \u2208 Rd\u00d7r whose entries are drawn in i.i.d. trials from the normal distribution N (0, 1).\n2. Compute B = A\u03a0 \u2208 Rn\u00d7r.\n3. Compute a matrix Q \u2208 Rn\u00d7r whose columns form an orthonormal basis for the column space of B.\n4. Compute the matrix QTA \u2208 Rr\u00d7d and its left singular vectors UQTA \u2208 Rr\u00d7d.\n5. Let UQTA,k \u2208 Rr\u00d7k denote the top k left singular vectors of the matrix QTA (the first k columns of UQTA) and compute, for all i \u2208 [n],\n\u2113\u0302i = \u2225 \u2225 \u2225 ( QUQTA,k )\n(i)\n\u2225 \u2225 \u2225 2\n2 . (27)\n6. Return p\u0302i = \u2113\u0302i/k for all i \u2208 [n].\nAlgorithm 5: Approximating the statistical leverage scores of a general matrix A (Frobenius norm case)."}, {"heading": "5.2 Leverage Scores for Frobenius Norm Approximators.", "text": "Algorithm 5 approximates the statistical leverage scores of a general matrix A with rank parameter k in the Frobenius norm case. It takes as inputs a matrix A \u2208Rn\u00d7d with rank(A) = \u03c1 and a rank parameter k \u226a \u03c1, and outputs a set of numbers p\u0302i for all i \u2208 [n], namely our approximations to the normalized leverage scores of A with rank parameter k. It is worth noting that\n\u2211n i=1 \u2113\u0302i =\n\u2225 \u2225QUQTA,k \u2225 \u2225 2\nF =\n\u2225 \u2225UQTA,k \u2225 \u2225 2\nF = k and thus the p\u0302i sum up to one. The next lemma argues that\nthere exists a matrix X \u2208 Rn\u00d7d of rank k that is sufficiently close to A (in particular, it is a member of S\u01eb with constant probability). Unlike the previous section (the spectral norm case), we will now be able to provide a closed-form formula for this matrix X and, more importantly, the normalized leverage scores of X will be exactly equal to the p\u0302i returned by our algorithm. Thus, in the parlance of Definition 3, we will get a 1-approximation to the normalized leverage scores of A with rank parameter k.\nLemma 11 (Frobenius Sketch). Given A \u2208 Rn\u00d7d of rank \u03c1, a rank parameter k such that 2 \u2264 k < \u03c1, and an error parameter \u01eb such that 0 < \u01eb < 1, let \u03a0 \u2208 Rd\u00d7r be a standard Gaussian matrix (with entries selected in i.i.d. trials from N (0, 1)) with\nr \u2265 k + \u2308 10k\n\u01eb + 1\n\u2309\n. (28)\nLet B = A\u03a0 and let X be as in Eqn. (29). Then,\nE [ \u2016A\u2212X\u20162F ] \u2264 ( 1 + \u01eb\n10\n)\n\u2016A\u2212Ak\u20162F .\nThe matrix B can be computed in O ( ndk\u01eb\u22121 ) time.\nLet X = Q ( QTA )\nk \u2208 Rn\u00d7d, (29)\nwhere ( QTA ) k is the best rank-k approximation to the matrix QTA; from standard linear algebra, (\nQTA )\nk = UQTA,kU T QTA,k QTA. Then, the above lemma is proven in [10].11 Now, since X has\nrank k, it follows that \u2016A\u2212X\u20162F \u2265 \u2016A\u2212Ak\u20162F and thus we can consider the non-negative random variable \u2016A\u2212X\u20162F \u2212 \u2016A\u2212Ak\u20162F and apply Markov\u2019s inequality to get that\n\u2016A\u2212X\u20162F \u2212 \u2016A\u2212Ak\u20162F \u2264 \u01eb\u2016A\u2212Ak\u20162F\nholds with probability at least 0.9. Rearranging terms and taking square roots of both sides implies that\n\u2016A\u2212X\u2016F \u2264 \u221a 1 + \u01eb\u2016A\u2212Ak\u2016F \u2264 (1 + \u01eb) \u2016A\u2212Ak\u2016F .\nThus, X \u2208 S\u01eb with probability at least 0.9. To conclude our proof, recall that Q is an orthonormal basis for the columns of B. From Eqn. (29),\nX = Q ( QTA )\nk = QUQTA,kU T QTA,kQ TA = QUQTA,k\u03a3QTA,kV T QTA,k.\nIn the above, \u03a3QTA,k \u2208 Rk\u00d7k is the diagonal matrix containing the top k singular values of QTA and V T\nQTA,k \u2208 Rk\u00d7d is the matrix whose rows are the top k right singular vectors of QTA. Thus,\nthe left singular vectors of the matrix X are exactly equal to the columns of the orthogonal matrix QUQTA,k; it now follows that the \u2113\u0302i of Eqn. (27) are the leverage scores of the matrix X and, finally, that the p\u0302i returned by the proposed algorithm are the normalized leverage scores of the matrix X.\nWe briefly discuss the running time of the proposed algorithm. First, we can compute B in O(ndr) time. Then, the computation of Q takes O(nr2) time. The computation of QTA takes O(ndr) time and the computation of UQTA takes O(dr\n2) time. Thus, the total time is equal to O ( ndr + (n+ d)r2 ) . The following Theorem summarizes the above discussion.\nTheorem 4. Given A \u2208 Rn\u00d7d, a rank parameter k, and an accuracy parameter \u01eb, Algorithm 5 computes a set of normalized leverage scores p\u0302i that are 1-approximations to the normalized leverage scores of A with rank parameter k with probability at least 0.7. The proposed algorithm runs in O ( ndk\u01eb\u22121 + (n+ d)k2\u01eb\u22122 ) time."}, {"heading": "6 Discussion", "text": "We will conclude with a discussion of our main results in a broader context: understanding the relationship between our main algorithm and a related estimator for the statistical leverage scores; applying our main algorithm to solve under-constrained least squares problems; and implementing variants of the basic algorithm in streaming environments."}, {"heading": "6.1 A related estimator for the leverage scores", "text": "Magdon-Ismail in [31] presented the following algorithm to estimate the statistical leverage scores: given as input an n\u00d7 d matrix A, with n \u226b d, the algorithm proceeds as follows.\n\u2022 Compute \u03a0A, where the O (\nn lnd ln2 n\n)\n\u00d7 n matrix \u03a0 is a SRHT or another FJLT. 11More specifically, the proof may be found in Lemma 33 in Section A.3; note that for our purposes here we set\np = \u2308 10k \u01eb + 1 \u2309 .\n\u2022 Compute X = (\u03a0A)\u2020\u03a0.\n\u2022 For t = 1, . . . , n, compute the estimate w\u0303t = AT(t)X(t) and set wt = max { d ln2 n 4n , w\u0303t } .\n\u2022 Return the quantities p\u0303i = wi/ \u2211n i\u2032=1wi\u2032 , for i \u2208 [n].\n[31] argued that the output p\u0303i achieves an O(ln 2 n) approximation to all of the (normalized) statistical leverage scores of A in roughly O(nd2/ ln n) time. (To our knowledge, prior to our work here, this is the only known estimator that obtains any nontrivial provable approximation to the leverage scores of a matrix in o(nd2) time.) To see the relationship between this estimator and our main result, recall that\n\u2113i = e T i UU T ei = e T i AA \u2020ei = x T i yi,\nwhere the vector xTi = e T i A is cheap to compute and the vector yi = A \u2020ei is expensive to compute. The above algorithm effectively approximates yi = A\n\u2020ei via a random projection as y\u0303i = (\u03a0A)\n\u2020\u03a0ei, where \u03a0 is a SRHT or another FJLT. Since the estimates xTi y\u0303i are not necessarily positive, a truncation at the negative tail, followed by a renormalization step, must be performed in order to arrive at the final estimator returned by the algorithm. This truncationrenormalization step has the effect of inflating the estimates of the small leverage scores by an O(ln2 n) factor. By way of comparison, Algorithm 1 essentially computes a sketch of AA\u2020 of the form A(\u03a0A)\u2020\u03a0T that maintains positivity for each of the row norm estimates.\nAlthough both Algorithm 1 and the algorithm of this subsection estimate AA\u2020 by a matrix of the form A(\u03a0A)\u2020\u03a0T , there are notable differences. The algorithm of this subsection does not actually compute or approximate AAT directly; instead, it separates the matrix into two parts and computes the dot product between eTi A and (\u03a0A)\n\u2020\u03a0ei. Positivity is sacrificed and this leads to some complications in the estimator; however, the truncation step is interesting, since, despite the fact that the estimates are \u201cbiased\u201d (in a manner somewhat akin to what is obtained with \u201cthresholding\u201d or \u201cregularization\u201d procedures), we still obtain provable approximation guarantees. The algorithm of this subsection is simpler (since it uses an application of only one random projection), albeit at the cost of weaker theoretical guarantees and a worse running time than our main algorithm. A direction of considerable practical interest is to evaluate empirically the performance of these two estimators, either for estimating all the leverage scores or (more interestingly) for estimating the largest leverage scores for data matrices for which the leverage scores are quite nonuniform."}, {"heading": "6.2 An application to under-constrained least-squares problems", "text": "Consider the following under-constrained least-squares problem:\nmin x\u2208Rd\n\u2016Ax\u2212 b\u20162 , (30)\nwhere A \u2208 Rn\u00d7d has much fewer rows than columns, i.e., n \u226a d. It is well-known that we can solve this problem exactly in O(n2d) time and that the minimal \u21132-norm solution is given by xopt = A\n\u2020b. For simplicity, let\u2019s assume that the input matrix A has full rank (i.e., rank(A) = n) and thus \u2016Axopt \u2212 b\u20162 = 0.\nIn this section, we will argue that Algorithm 6 computes a simple, accurate estimator x\u0303opt for xopt. In words, Algorithm 6 samples a small number of columns from A (note that the columns of A correspond to variables in our under-constrained problem) and uses the sampled columns to compute x\u0303opt. However, in order to determine which columns will be included in the sample,\nthe algorithm will make use of the statistical leverage scores of the matrix AT ; more specifically, columns (and thus variables) will be chosen with probability proportional to the corresponding statistical leverage score. We will state Algorithm 6 assuming that these probabilities are parts of the input; the following theorem is our main quality-of-approximation result for Algorithm 6.\nTheorem 5. Let A \u2208 Rn\u00d7d be a full-rank matrix with n \u226a d; let \u01eb \u2208 (0, 0.5] be an accuracy parameter; let \u03b4 \u2208 (0, 1) be a failure probability; and let xopt = A\u2020b be the minimal \u21132-norm solution to the least-squares problem of Eqn. (30). Let pi \u2265 0, i \u2208 [d], be a set of probabilities satisfying\n\u2211d i=1 pi = 1 and\npi \u2265 \u03b2 \u2225 \u2225V(i) \u2225 \u2225 2 2\nn (31)\nfor some constant \u03b2 \u2208 (0, 1]. (Here V \u2208 Rd\u00d7n is the matrix of the right singular vectors of A.) If x\u0303opt is computed via Algorithm 6 then, with probability at least 1\u2212 \u03b4,\n\u2016xopt \u2212 x\u0303opt\u20162 \u2264 2\u01eb \u2016xopt\u20162 .\nAlgorithm 6 runs in O ( n3\u01eb\u22122\u03b2\u22121 ln (n/\u01eb\u03b2\u03b4) + nd ) time.\nProof: Let the singular value decomposition of the full-rank matrix A be A = U\u03a3V T , with U \u2208 Rn\u00d7n, \u03a3 \u2208 Rn\u00d7n, and V \u2208 Rd\u00d7n; note that all the diagonal entries of \u03a3 are strictly positive since A has full rank. We can now apply Theorem 4 of Section 6.1 of [22] to get12 that\n\u2225 \u2225In \u2212 V TSSTV \u2225 \u2225\n2 =\n\u2225 \u2225V TV \u2212 V TSSTV \u2225 \u2225 2 \u2264 \u01eb (32)\nfor our choice of r with probability at least 1 \u2212 \u03b4. Note that V TS \u2208 Rn\u00d7r (with r \u2265 n) and let \u03c3i ( V TS )\ndenote the singular values of V TS for all i \u2208 [n]; the above inequality implies that for all i \u2208 [n]\n\u2223 \u22231\u2212 \u03c32i ( V TS ) \u2223 \u2223 \u2264 \u2225 \u2225In \u2212 V TSSTV \u2225 \u2225 2 \u2264 \u01eb \u2264 0.5.\nThus, all the singular values of V TS are strictly positive and hence V TS has full rank equal to n. Also, using \u01eb \u2264 0.5,\n\u2223 \u22231\u2212 \u03c3\u22122i ( V TS )\u2223 \u2223 \u2264 \u01eb 1\u2212 \u01eb \u2264 2\u01eb. (33)\nWe are now ready to prove our theorem:\n\u2016xopt \u2212 x\u0303opt\u20162 = \u2225 \u2225 \u2225 AT (AS)\u2020T (AS)\u2020 b\u2212A\u2020b \u2225 \u2225 \u2225\n2\n= \u2225 \u2225 \u2225 V \u03a3UT ( U\u03a3V TS )\u2020T ( U\u03a3V TS )\u2020 b\u2212 V \u03a3\u22121UT b \u2225 \u2225 \u2225\n2\n= \u2225 \u2225 \u2225 \u03a3UTU\u03a3\u22121 ( V TS )\u2020T ( V TS )\u2020 \u03a3\u22121UT b\u2212 \u03a3\u22121UT b \u2225 \u2225 \u2225\n2\n= \u2225 \u2225\n\u2225\n( V TS )\u2020T ( V TS )\u2020 \u03a3\u22121UT b\u2212 \u03a3\u22121UT b \u2225 \u2225\n\u2225 2 .\nIn the above derivations we substituted the SVD of A, dropped terms that do not change unitarily invariant norms, and used the fact that V TS and \u03a3 have full rank in order to simplify the pseudoinverse. Now let ( V TS )\u2020T ( V TS )\u2020\n= In + E and note that Eqn. (33) and the fact that V TS has full rank imply\n\u2016E\u20162 = \u2225 \u2225 \u2225 In \u2212 ( V TS )\u2020T ( V TS )\u2020\u2225 \u2225 \u2225\n2 = max i\u2208[n]\n\u2223 \u22231\u2212 \u03c3\u22122i ( V TS ) \u2223 \u2223 \u2264 2\u01eb.\n12We apply Theorem 4 of Section 6.1 of [22] with A = V T and note that \u2225 \u2225V T \u2225 \u2225 2\nF = n \u2265 1,\n\u2225 \u2225V T \u2225 \u2225\n2 = 1, and\n( V T )(i)\n= V(i).\nThus, we conclude our proof by observing that\n\u2016xopt \u2212 x\u0303opt\u20162 = \u2225 \u2225(In + E)\u03a3 \u22121UT b\u2212 \u03a3\u22121UT b \u2225 \u2225 2\n= \u2225 \u2225E\u03a3\u22121UT b \u2225 \u2225\n2\n\u2264 \u2016E\u20162 \u2225 \u2225\u03a3\u22121UT b \u2225 \u2225\n2\n\u2264 2\u01eb \u2016xopt\u20162 .\nIn the above we used the fact that \u2016xopt\u20162 = \u2225 \u2225A\u2020b \u2225 \u2225 2 = \u2225 \u2225V \u03a3\u22121UT b \u2225 \u2225 2 = \u2225 \u2225\u03a3\u22121UT b \u2225 \u2225 2 . The running time of the algorithm follows by observing that AS is an n\u00d7r matrix and thus computing its pseudoinverse takes O(n2r) time; computing xopt takes an additional O(nr + dn) time.\n\u22c4\nInput: A \u2208 Rn\u00d7d, b \u2208 Rn, error parameter \u01eb \u2208 (0, .5], failure probability \u03b4, and a set of probabilities pi (for all i \u2208 [d]) summing up to one and satisfying Eqn. (31). Output: x\u0303opt \u2208 Rd.\n1. Let r = 96n \u03b2\u01eb2\nln (\n96n \u03b2\u01eb2 \u221a \u03b4\n)\n.\n2. Let S \u2208 Rd\u00d7r be an all-zeros matrix.\n3. For t = 1, . . . , r do\n\u2022 Pick it \u2208 [d] such that Pr (it = i) = pi. \u2022 Sitt = 1/ \u221a rpit.\n4. Return x\u0303opt = A T (AS)\u2020T (AS)\u2020 b.\nAlgorithm 6: Approximately solving under-constrained least squares problems.\nWe conclude the section with a few remarks. First, assuming that \u01eb, \u03b2, and \u03b4 are constants and n lnn = o(d), it immediately follows that Algorithm 6 runs in o(n2d) time. It should be clear that we can use Theorem 1 and the related Algorithm 1 to approximate the statistical leverage scores, thus bypassing the need to exactly compute them. Second, instead of approximating the statistical leverage scores needed in Algorithm 6, we could use the randomized Hadamard transform (essentially post-multiply A by a randomized Hadamard transform to make all statistical leverage scores uniform). The resulting algorithm could be theoretically analyzed following the lines of [22]. It would be interesting to evaluate experimentally the performance of the two approaches in real data."}, {"heading": "6.3 Extension to streaming environments", "text": "In this section, we consider the estimation of the leverage scores and of related statistics when the input data set is so large that an appropriate way to view the data is as a data stream [36]. In this context, one is interested in computing statistics of the data stream while making one pass (or occasionally a few additional passes) over the data from external storage and using only a small amount of additional space. For an n \u00d7 d matrix A, with n \u226b d, small additional space means\nthat the space complexity only depends logarithmically on the high dimension n and polynomially on the low dimension d. When we discuss bits of space, we assume that the entries of A can be discretized to O(log n) bit integers, though all of our results can be generalized to arbitrary word sizes. The general strategy behind our algorithms is as follows.\n\u2022 As the data streams by, compute TA, for an appropriate problem-dependent linear sketching matrix T , and also compute \u03a0A, for a random projection matrix \u03a0.13\n\u2022 After the first pass over the data, compute the matrix R\u22121, as described in Algorithm 1, corresponding to \u03a0A (or compute the pseudoinverse of \u03a0A or the R matrix from any other QR decomposition of A).\n\u2022 Compute TAR\u22121\u03a02, for a random projection matrix \u03a02, such as the one used by Algorithm 1.\nWith the procedure outlined above, the matrix T is effectively applied to the rows of AR\u22121\u03a02, i.e., to the sketch of A that has rows with Euclidean norms approximately equal to the row norms of U , and pairwise inner products approximately equal to those in U . Thus statistics related to U can be extracted.\nLarge Leverage Scores. Given any n \u00d7 d matrix A in a streaming setting, it is known how to find the indices of all rows A(i) of A for which \u2016A(i)\u201622 \u2265 \u03c4\u2016A\u20162F , for a parameter \u03c4 , and in addition it is known how to compute a (1+ \u01eb)-approximation to \u2016A(i)\u201622 for these large rows. The basic idea is to use the notion of \u21132-sampling on matrix A, namely, to sample random entries Aij with probability A 2 ij/\u2016A\u20162F . A single entry can be sampled from this distribution in a single pass using O(\u01eb\u22122 log3(nd)) bits of space [35, 5]. More precisely, these references demonstrate that there is a distribution over O(d\u01eb\u22122 log3(nd)) \u00d7 n matrices T for which for any fixed matrix A \u2208 Rn\u00d7d, there is a procedure which given TA, outputs a sample (i, j) \u2208 [n]\u00d7[d] with probability (1 \u00b1 \u01eb) A 2 i,j\n\u2016A\u20162 F \u00b1 n\u2212O(1). Technically, these references concern sampling from vectors rather than matrices, so T (A) is a linear operator which treats A as a length-nd vector and applies the algorithm of [35, 5]. However, by simply increasing the number of rows in T by a factor of the small dimension d, we can assume T is left matrix multiplication. By considering the marginal along [n], the probability that i = a, for any a \u2208 [n], is\n(1\u00b1 \u01eb) \u2016U(a)\u201622 \u2016U\u20162F \u00b1 (nd)\u2212O(1).\nBy the coupon collector problem, running O(\u03c4\u22121 log \u03c4\u22121) independent copies is enough to find a set containing all rows A(i) for which \u2016A(i)\u201622 \u2265 \u03c4\u2016A\u20162F , and no rows A(i) for which \u2016A(i)\u201622 < \u03c4 2\u2016A\u20162F with probability at least 0.99.\nWhen applied to our setting, we can apply a random projection matrix \u03a0 and a linear sketching matrix T which has O(d\u03c4\u22121\u01eb\u22122 log3(n) log \u03c4\u22121) rows in the following manner. First, TA and \u03a0A are computed in the first pass over the data; then, at the end of the first pass, we compute R\u22121; and finally, we compute TAR\u22121\u03a02, for a random projection matrix \u03a02. This procedure effectively applies the matrix T to the rows of AR\u22121\u03a02, which have norms equal to the row norms of U , up to a factor of 1 + \u01eb. The multiplication at the end by \u03a02 serves only to speed up the time for\n13In the offline setting, one would use an SRHT or another FJLT, while in the streaming setting one could use either of the following. If the stream is such that one sees each entire column of A at once, then one could do an FJLT on the column. Alternatively, if one see updates to the individual entries of A in an arbitrary order, then one could apply any sketching matrix, such as those of [1] or of [17].\nprocessing TAR\u22121. Thus, by the results of [35, 5], we can find all the leverage scores \u2016U(i)\u201622 that are of magnitude at least \u03c4\u2016U\u20162F in small space and a single pass over the data. By increasing the space by a factor of O(\u01eb\u22122 log n), we can also use the \u21132-samples to estimate the norms \u2016U(i)\u201622 for the row indices i that we find.\nEntropy. Given a distribution \u03c1, a statistic of \u03c1 of interest is the entropy of this distribution, where the entropy is defined as H(\u03c1) = \u2211\ni \u03c1(i) log2(1/\u03c1(i)). This statistic can be approximated in a streaming setting. Indeed, it is known that estimating H(\u03c1) up to an additive \u01eb can be reduced to (1 + \u01eb\u0303)-approximation of the \u2113p-norm of the vector (\u03c1(1), . . . , \u03c1(n)), for O(log 1/\u01eb) different p \u2208 (0, 1) [28]. Here \u01eb\u0303 = \u01eb/(log3 1/\u01eb \u00b7 log n). When applied to our setting, the distribution of interest is \u03c1(i) = 1\nd \u2016U(i)\u201622. To compute the entropy of this distribution, there exist sketching\nmatrices T for providing (1 + \u01eb)-approximations to the quantity Fp(F2) of an n \u00d7 d matrix A, where Fp(F2) is defined as \u2211n i=1 \u2016A(i)\u2016 2p 2 , using O(\u01eb\n\u22124 log2 n log 1/\u01eb) bits of space (see Theorem 1 of [23]). Thus, to compute the entropy of the leverage score distribution, we can do the following. First, maintain TA and \u03a0A in the first pass over the data, where T is a sketching matrix for Fp(F2), p \u2208 (0, 1). At the end of the first pass, compute R\u22121; and finally, compute TAR\u22121\u03a02, which effectively applies the Fp(F2)-estimation matrix T to the rows of the matrix AR\n\u22121\u03a02. Therefore, by the results of [28, 23], we can compute an estimate \u03c6 which is within an additive \u01eb of H(\u03c1) using O(d\u01eb\u22124 log6 n log14 1/\u01eb) bits of space and a single pass. We note that it is also possible to estimate H(\u03c1) up to a multiplicative 1 + \u01eb factor using small, but more, space; see, e.g., [28].\nSampling Row Identities. Another natural problem is that of obtaining samples of rows of A proportional to their leverage score importance sampling probabilities. To do so, we use \u21132sampling [35, 5] as used above for finding the large leverage scores. First, compute TA and \u03a0A in the first pass over the data stream; then, compute R\u22121; and finally, compute TAR\u22121. Thus, by applying the procedures of [5] a total of s times independently, we obtain s samples i1, . . . , is, with replacement, of rows of A proportional to \u2016U(i1)\u201622, . . . , \u2016U(is)\u201622, i.e., to their leverage score. The algorithm requires O(sd\u01eb\u22122 log4 n) bits of space and runs in a single pass. To obtain more than just the row identities i1, . . . , is, e.g., to obtain the actual samples, one can read off these rows from A in a second pass over the matrix."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences, 66(4):671\u2013687", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "Proceedings of the 38th Annual ACM Symposium on Theory of Computing, pages 557\u2013563", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "The fast Johnson-Lindenstrauss transform and approximate nearest neighbors", "author": ["N. Ailon", "B. Chazelle"], "venue": "SIAM Journal on Computing, 39(1):302\u2013322", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast dimension reduction using Rademacher series on dual BCH codes", "author": ["N. Ailon", "E. Liberty"], "venue": "Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1\u20139", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Streaming algorithms from precision sampling", "author": ["A. Andoni", "R. Krauthgamer", "K. Onak"], "venue": "Technical report. Preprint: arXiv:1011.1263 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Blendenpik: Supercharging LAPACK\u2019s leastsquares solver", "author": ["H. Avron", "P. Maymounkov", "S. Toledo"], "venue": "SIAM Journal on Scientific Computing, 32:1217\u20131236", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Low cost high performance uncertainty quantification", "author": ["C. Bekas", "A. Curioni", "I. Fedulova"], "venue": "Proceedings of the 2nd Workshop on High Performance Computational Finance, page Article No.: 8", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "An estimator for the diagonal of a matrix", "author": ["C. Bekas", "E. Kokiopoulou", "Y. Saad"], "venue": "Applied Numerical Mathematics, 57:1214\u20131229", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Power and centrality: A family of measures", "author": ["P. Bonacich"], "venue": "The American Journal of Sociology, 92(5):1170\u20131182", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["C. Boutsidis", "P. Drineas", "M. Magdon-Ismail"], "venue": "Technical report. Preprint: arXiv:1103.0995 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["C. Boutsidis", "P. Drineas", "M. Magdon-Ismail"], "venue": "Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science, pages 305\u2013314", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 968\u2013977", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Technical report. Preprint: arXiv:0805.4471 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Influential observations", "author": ["S. Chatterjee", "A.S. Hadi"], "venue": "high leverage points, and outliers in linear regression. Statistical Science, 1(3):379\u2013393", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1986}, {"title": "Sensitivity Analysis in Linear Regression", "author": ["S. Chatterjee", "A.S. Hadi"], "venue": "John Wiley & Sons, New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Regression Analysis by Example", "author": ["S. Chatterjee", "A.S. Hadi", "B. Price"], "venue": "John Wiley & Sons, New York", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "A sparse Johnson-Lindenstrauss transform", "author": ["A. Dasgupta", "R. Kumar", "T. Sarl\u00f3s"], "venue": "Proceedings of the 42nd Annual ACM Symposium on Theory of Computing, pages 341\u2013350", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM Journal on Computing, 36:132\u2013157", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Inferring geographic coordinates of origin for Europeans using small panels of ancestry informative markers", "author": ["P. Drineas", "J. Lewis", "P. Paschou"], "venue": "PLoS ONE, 5(8):e11892", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Sampling algorithms for l2 regression and applications", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1127\u20131136", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications, 30:844\u2013881", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Faster least squares approximation", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan", "T. Sarl\u00f3s"], "venue": "Numerische Mathematik, 117(2):219\u2013249", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimating hybrid frequency moments of data streams", "author": ["S. Ganguly", "M. Bansal", "S. Dube"], "venue": "Proceedings of the 2nd Annual International Workshop on Frontiers in Algorithmics, pages 55\u201366", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "In preparation", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press, Baltimore", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J.A. Tropp"], "venue": "SIAM Review, 53(2):217\u2013288", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Sketching and streaming entropy via approximation theory", "author": ["N.J.A. Harvey", "J. Nelson", "K. Onak"], "venue": "Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science, pages 489\u2013498", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "The hat matrix in regression and ANOVA", "author": ["D.C. Hoaglin", "R.E. Welsch"], "venue": "The American Statistician, 32(1):17\u201322", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1978}, {"title": "Effective resistance of Gromovhyperbolic graphs: Application to asymptotic sensor network problems", "author": ["E.A. Jonckheere", "M. Lou", "J. Hespanha", "P. Barooah"], "venue": "Proceedings of the 46th IEEE Conference on Decision and Control, pages 1453\u20131458", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Row sampling for matrix algorithms via a non-commutative Bernstein bound", "author": ["M. Magdon-Ismail"], "venue": "Technical report. Preprint: arXiv:1008.0587 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Randomized algorithms for matrices and data", "author": ["M.W. Mahoney"], "venue": "Foundations and Trends in Machine Learning. NOW Publishers, Boston", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proc. Natl. Acad. Sci. USA, 106:697\u2013702", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "LSRN: A parallel iterative solver for strongly over- or under-determined systems", "author": ["X. Meng", "M.A. Saunders", "M.W. Mahoney"], "venue": "Technical report. Preprint: arXiv:1109.5981 ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "1-pass relative-error lp-sampling with applications", "author": ["M. Monemizadeh", "D.P. Woodruff"], "venue": "Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1143\u20131160", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Data Streams: Algorithms and Applications", "author": ["S. Muthukrishnan"], "venue": "Foundations and Trends in Theoretical Computer Science. Now Publishers Inc, Boston", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "editor", "author": ["M.Z. Nashed"], "venue": "Generalized Inverses and Applications. Academic Press, New York", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1976}, {"title": "A measure of betweenness centrality based on random walks", "author": ["M.E.J. Newman"], "venue": "Social Networks, 27:39\u201354", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Latent semantic indexing: a probabilistic analysis", "author": ["C.H. Papadimitriou", "P. Raghavan", "H. Tamaki", "S. Vempala"], "venue": "Journal of Computer and System Sciences, 61(2):217\u2013235", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "Ancestry informative markers for finescale individual assignment to worldwide populations", "author": ["P. Paschou", "J. Lewis", "A. Javed", "P. Drineas"], "venue": "Journal of Medical Genetics, page doi:10.1136/jmg.2010.078212", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "PCA-correlated SNPs for structure identification in worldwide human populations", "author": ["P. Paschou", "E. Ziv", "E.G. Burchard", "S. Choudhry", "W. Rodriguez-Cintron", "M.W. Mahoney", "P. Drineas"], "venue": "PLoS Genetics, 3:1672\u20131686", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Mining knowledge-sharing sites for viral marketing", "author": ["M. Richardson", "P. Domingos"], "venue": "Proceedings of the 8th Annual ACM SIGKDD Conference, pages 61\u201370", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "A randomized algorithm for principal component analysis", "author": ["V. Rokhlin", "A. Szlam", "M. Tygert"], "venue": "SIAM Journal on Matrix Analysis and Applications, 31(3):1100\u20131124", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast randomized algorithm for overdetermined linear leastsquares regression", "author": ["V. Rokhlin", "M. Tygert"], "venue": "Proc. Natl. Acad. Sci. USA, 105(36):13212\u201313217", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T. Sarl\u00f3s"], "venue": "Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, pages 143\u2013152", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Matrix coherence and the Nystr\u00f6m method", "author": ["A. Talwalkar", "A. Rostamizadeh"], "venue": "Proceedings of the 26th Conference in Uncertainty in Artificial Intelligence", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient computing of regression diagnostics", "author": ["P.F. Velleman", "R.E. Welsch"], "venue": "The American Statistician, 35(4):234\u2013242", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1981}], "referenceMentions": [{"referenceID": 45, "context": "1 Introduction The concept of statistical leverage measures the extent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21].", "startOffset": 278, "endOffset": 290}, {"referenceID": 31, "context": "1 Introduction The concept of statistical leverage measures the extent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21].", "startOffset": 278, "endOffset": 290}, {"referenceID": 20, "context": "1 Introduction The concept of statistical leverage measures the extent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21].", "startOffset": 278, "endOffset": 290}, {"referenceID": 12, "context": "A related notion is that of matrix coherence, which has been of interest in recently popular problems such as matrix completion and Nystr\u00f6m-based low-rank matrix approximation [13, 46].", "startOffset": 176, "endOffset": 184}, {"referenceID": 44, "context": "A related notion is that of matrix coherence, which has been of interest in recently popular problems such as matrix completion and Nystr\u00f6m-based low-rank matrix approximation [13, 46].", "startOffset": 176, "endOffset": 184}, {"referenceID": 27, "context": "Statistical leverage scores have a long history in statistical data analysis, where they have been used for outlier detection in regression diagnostics [29, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 13, "context": "Statistical leverage scores have a long history in statistical data analysis, where they have been used for outlier detection in regression diagnostics [29, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 20, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 31, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 11, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 19, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 43, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 21, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 30, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 268, "endOffset": 272}, {"referenceID": 24, "context": ", the basis provided by the Singular Value Decomposition (SVD) or a basis provided by a QR decomposition [26], and then use that basis to compute diagonal elements of the projection matrix onto the span of that basis.", "startOffset": 105, "endOffset": 109}, {"referenceID": 43, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 174, "endOffset": 182}, {"referenceID": 21, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 174, "endOffset": 182}, {"referenceID": 37, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 43, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 20, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 31, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 11, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 30, "context": "See [32] for a detailed discussion.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "As an example, the CUR decomposition of [21, 33] essentially computes pi = li/k, for all i \u2208 {1, .", "startOffset": 40, "endOffset": 48}, {"referenceID": 31, "context": "As an example, the CUR decomposition of [21, 33] essentially computes pi = li/k, for all i \u2208 {1, .", "startOffset": 40, "endOffset": 48}, {"referenceID": 1, "context": "On the other hand, the computational bottleneck for random projection algorithms is the application of the random projection, which is sped up by using variants of the Fast Johnson-Lindenstrauss Transform [2, 3].", "startOffset": 205, "endOffset": 211}, {"referenceID": 2, "context": "On the other hand, the computational bottleneck for random projection algorithms is the application of the random projection, which is sped up by using variants of the Fast Johnson-Lindenstrauss Transform [2, 3].", "startOffset": 205, "endOffset": 211}, {"referenceID": 19, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 49, "endOffset": 61}, {"referenceID": 20, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 49, "endOffset": 61}, {"referenceID": 31, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 49, "endOffset": 61}, {"referenceID": 43, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 246, "endOffset": 250}, {"referenceID": 42, "context": "Recently, high-quality numerical implementations of variants of the basic randomized matrix algorithms have proven superior to traditional deterministic algorithms [44, 43, 6].", "startOffset": 164, "endOffset": 175}, {"referenceID": 41, "context": "Recently, high-quality numerical implementations of variants of the basic randomized matrix algorithms have proven superior to traditional deterministic algorithms [44, 43, 6].", "startOffset": 164, "endOffset": 175}, {"referenceID": 5, "context": "Recently, high-quality numerical implementations of variants of the basic randomized matrix algorithms have proven superior to traditional deterministic algorithms [44, 43, 6].", "startOffset": 164, "endOffset": 175}, {"referenceID": 7, "context": "More generally, density functional theory [8] and uncertainty quantification [7] are two scientific computing areas where computing the diagonal elements of functions (such as a projection or inverse) of very large input matrices is common.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "More generally, density functional theory [8] and uncertainty quantification [7] are two scientific computing areas where computing the diagonal elements of functions (such as a projection or inverse) of very large input matrices is common.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "For example, in the former case, \u201cheuristic\u201d methods based on using Chebychev polynomials have been used in numerical linear algebra to compute the diagonal elements of the projector [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 27, "context": "The statistical leverage scores and the scores relative to the best rank-k approximation to A are equal to the diagonal elements of the socalled \u201chat matrix\u201d [29, 15].", "startOffset": 158, "endOffset": 166}, {"referenceID": 14, "context": "The statistical leverage scores and the scores relative to the best rank-k approximation to A are equal to the diagonal elements of the socalled \u201chat matrix\u201d [29, 15].", "startOffset": 158, "endOffset": 166}, {"referenceID": 27, "context": "As such, they have a natural statistical interpretation in terms of the \u201cleverage\u201d or \u201cinfluence\u201d associated with each of the data points [29, 14, 15].", "startOffset": 138, "endOffset": 150}, {"referenceID": 13, "context": "As such, they have a natural statistical interpretation in terms of the \u201cleverage\u201d or \u201cinfluence\u201d associated with each of the data points [29, 14, 15].", "startOffset": 138, "endOffset": 150}, {"referenceID": 14, "context": "As such, they have a natural statistical interpretation in terms of the \u201cleverage\u201d or \u201cinfluence\u201d associated with each of the data points [29, 14, 15].", "startOffset": 138, "endOffset": 150}, {"referenceID": 27, "context": "In the context of regression problems, the i leverage score quantifies the leverage or influence of the i constraint/row of A on the solution of the overconstrained least squares optimization problem minx \u2016Ax\u2212 b\u20162 and the (i, j)-th cross leverage score quantifies how much influence or leverage the i data point has on the j least-squares fit (see [29, 14, 15] for details).", "startOffset": 348, "endOffset": 360}, {"referenceID": 13, "context": "In the context of regression problems, the i leverage score quantifies the leverage or influence of the i constraint/row of A on the solution of the overconstrained least squares optimization problem minx \u2016Ax\u2212 b\u20162 and the (i, j)-th cross leverage score quantifies how much influence or leverage the i data point has on the j least-squares fit (see [29, 14, 15] for details).", "startOffset": 348, "endOffset": 360}, {"referenceID": 14, "context": "In the context of regression problems, the i leverage score quantifies the leverage or influence of the i constraint/row of A on the solution of the overconstrained least squares optimization problem minx \u2016Ax\u2212 b\u20162 and the (i, j)-th cross leverage score quantifies how much influence or leverage the i data point has on the j least-squares fit (see [29, 14, 15] for details).", "startOffset": 348, "endOffset": 360}, {"referenceID": 45, "context": "Historically, these quantities have been widely-used for outlier identification in diagnostic regression analysis [47, 16].", "startOffset": 114, "endOffset": 122}, {"referenceID": 15, "context": "Historically, these quantities have been widely-used for outlier identification in diagnostic regression analysis [47, 16].", "startOffset": 114, "endOffset": 122}, {"referenceID": 8, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 40, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 36, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 39, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 31, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 18, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 38, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 39, "context": "Our main result will permit the computation of these scores and related quantities for significantly larger SNP data sets than has been possible previously [41, 19, 40, 24].", "startOffset": 156, "endOffset": 172}, {"referenceID": 18, "context": "Our main result will permit the computation of these scores and related quantities for significantly larger SNP data sets than has been possible previously [41, 19, 40, 24].", "startOffset": 156, "endOffset": 172}, {"referenceID": 38, "context": "Our main result will permit the computation of these scores and related quantities for significantly larger SNP data sets than has been possible previously [41, 19, 40, 24].", "startOffset": 156, "endOffset": 172}, {"referenceID": 23, "context": ", [25].", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": "The state of the art here is the Blendenpik algorithm of [6] and the LSRN algorithm of [34].", "startOffset": 57, "endOffset": 60}, {"referenceID": 32, "context": "The state of the art here is the Blendenpik algorithm of [6] and the LSRN algorithm of [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "that random projection algorithms should be incorporated into future versions of Lapack\u201d [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 35, "context": "The Moore-Penrose pseudoinverse of A is the d\u00d7n matrix defined by A\u2020 = V \u03a3\u22121UT [37].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "The SVD is important for a number of reasons [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "3/r with probability 1/6 each and zero otherwise (with probability 2/3) [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "1 of [1]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "For our main results, we will also need a stronger requirement than the simple \u01eb-JLT and so we will use a version of the Fast Johnson-Lindenstrauss Transform (FJLT), which was originally introduced in [2, 3].", "startOffset": 201, "endOffset": 207}, {"referenceID": 2, "context": "For our main results, we will also need a stronger requirement than the simple \u01eb-JLT and so we will use a version of the Fast Johnson-Lindenstrauss Transform (FJLT), which was originally introduced in [2, 3].", "startOffset": 201, "endOffset": 207}, {"referenceID": 19, "context": "The next lemma follows from the definition of an \u01eb-FJLT, and its proof can be found in [20, 22].", "startOffset": 87, "endOffset": 95}, {"referenceID": 21, "context": "The next lemma follows from the definition of an \u01eb-FJLT, and its proof can be found in [20, 22].", "startOffset": 87, "endOffset": 95}, {"referenceID": 21, "context": "Note that the RHT has also been crucial in the development of o(nd) randomized algorithms for the general overconstrained LS problem [22] and its variants have been used to provide high-quality numerical implementations of such randomized algorithms [44, 6].", "startOffset": 133, "endOffset": 137}, {"referenceID": 42, "context": "Note that the RHT has also been crucial in the development of o(nd) randomized algorithms for the general overconstrained LS problem [22] and its variants have been used to provide high-quality numerical implementations of such randomized algorithms [44, 6].", "startOffset": 250, "endOffset": 257}, {"referenceID": 5, "context": "Note that the RHT has also been crucial in the development of o(nd) randomized algorithms for the general overconstrained LS problem [22] and its variants have been used to provide high-quality numerical implementations of such randomized algorithms [44, 6].", "startOffset": 250, "endOffset": 257}, {"referenceID": 5, "context": "(Variants of this basic construction that relax this assumption and that are more appropriate for numerical implementation have been described and evaluated in [6].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "Third, if we only need to access r elements in the transformed vector, then those r elements can be computed in O(n log2 r) time [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 17, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 19, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 20, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 21, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 21, "context": "We summarize this discussion in the following lemma (which is essentially a combination of Lemmas 3 and 4 from [22], restated to fit our notation).", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "This is where the SRHT enters, since it preserves important structures of A, in particular its rank, by first rotating A to a random basis and then uniformly sampling rows from the rotated matrix (see [22] for more details).", "startOffset": 201, "endOffset": 205}, {"referenceID": 42, "context": "This preprocessing is reminiscent of how [44, 6] preprocessed the input to provide numerical implementations of the fast relative-error algorithm [22] for approximate LS approximation.", "startOffset": 41, "endOffset": 48}, {"referenceID": 5, "context": "This preprocessing is reminiscent of how [44, 6] preprocessed the input to provide numerical implementations of the fast relative-error algorithm [22] for approximate LS approximation.", "startOffset": 41, "endOffset": 48}, {"referenceID": 21, "context": "This preprocessing is reminiscent of how [44, 6] preprocessed the input to provide numerical implementations of the fast relative-error algorithm [22] for approximate LS approximation.", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 112, "endOffset": 116}, {"referenceID": 39, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 218, "endOffset": 226}, {"referenceID": 31, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 218, "endOffset": 226}, {"referenceID": 25, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 345, "endOffset": 349}, {"referenceID": 25, "context": "essentially proven in [27], but see also [43] for computational details; we will use the version of the lemma that appeared in [10].", "startOffset": 22, "endOffset": 26}, {"referenceID": 41, "context": "essentially proven in [27], but see also [43] for computational details; we will use the version of the lemma that appeared in [10].", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "essentially proven in [27], but see also [43] for computational details; we will use the version of the lemma that appeared in [10].", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "(See also the conference version [11], but in the remainder we refer to the technical report version [10] for consistency of numbering.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "(See also the conference version [11], but in the remainder we refer to the technical report version [10] for consistency of numbering.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": ") Note that for our purposes in this section, the computation of Y is not relevant and we defer the reader to [27, 10] for details.", "startOffset": 110, "endOffset": 118}, {"referenceID": 9, "context": ") Note that for our purposes in this section, the computation of Y is not relevant and we defer the reader to [27, 10] for details.", "startOffset": 110, "endOffset": 118}, {"referenceID": 9, "context": "This version of the above lemma is proven in [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "Then, the above lemma is proven in [10].", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": "1 A related estimator for the leverage scores Magdon-Ismail in [31] presented the following algorithm to estimate the statistical leverage scores: given as input an n\u00d7 d matrix A, with n \u226b d, the algorithm proceeds as follows.", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "[31] argued that the output p\u0303i achieves an O(ln 2 n) approximation to all of the (normalized) statistical leverage scores of A in roughly O(nd2/ ln n) time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "1 of [22] to get12 that", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "1 of [22] with A = V T and note that \u2225 V T \u2225", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "The resulting algorithm could be theoretically analyzed following the lines of [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 34, "context": "3 Extension to streaming environments In this section, we consider the estimation of the leverage scores and of related statistics when the input data set is so large that an appropriate way to view the data is as a data stream [36].", "startOffset": 228, "endOffset": 232}], "year": 2012, "abstractText": "The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr\u00f6mbased low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n\u00d7 d matrix A, with n \u226b d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O(nd) time required by the n\u00e4\u0131ve algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n \u2248 d, and the extension to streaming environments.", "creator": "LaTeX with hyperref package"}}}