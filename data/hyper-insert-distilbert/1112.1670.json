{"id": "1112.1670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2011", "title": "Data Mining Session-Based Patient Reported Outcomes (PROs) in a Mental Health Setting: Toward Data-Driven Clinical Decision Support and Personalized Treatment", "abstract": ", the cdoi outcome expectation measure - a robust patient - reported medical outcome ( pro ) perception instrument utilizing direct client feedback - technology was implemented in a critically large, real - world behavioral healthcare setting audience in order to evaluate significant previous findings from smaller controlled studies. pros provide an alternative window into treatment partner effectiveness based on relative client perception and assist facilitate presenting detection factors of significant problems / symptoms for topics which there is probably no discernible measure ( include e. g. pain ). continuing the principal focus of the study was continued to evaluate the effectiveness utility of interviewing the cdoi participants for presenting predictive modeling profiles of outcomes in comparing a reported live clinical setting. implementation planning factors potentially were also properly addressed within the framework of the theory of directed planned partnership behavior theory by linking individual adoption rates to implementation practices predictions and clinician perceptions. all the results showed that the cdoi effectiveness does that contain significant capacity to predict outcome delta over variable time improvement based model on baseline and early product change scores in observing a large, hypothetical real - world clinical measurement setting, as been suggested in previous research. the subsequent implementation analysis revealed a number of associated critical factors while affecting planned successful implementation design and adoption of with the cdoi outcome estimation measure, though there was shown a less notable uncertainty disconnect between clinician intentions and projected actual risky behavior. most importantly, the predictive capacity gained of performing the methodology cdoi underscores the intended utility functions of direct with client citizen feedback measures such as past pros and their potential use variables as potentially the basis guide for making next : generation quantitative clinical method decision support research tools and implementing personalized clinical treatment approaches.", "histories": [["v1", "Wed, 7 Dec 2011 19:44:48 GMT  (382kb)", "http://arxiv.org/abs/1112.1670v1", "Keywords- Data Mining; Patient-Reported Outcomes; CDOI; Implementation; Electronic Health Records; Decision Support Systems, Clinical; Theory of Planned Behavior"]], "COMMENTS": "Keywords- Data Mining; Patient-Reported Outcomes; CDOI; Implementation; Electronic Health Records; Decision Support Systems, Clinical; Theory of Planned Behavior", "reviews": [], "SUBJECTS": "cs.AI cs.GL", "authors": ["casey bennett", "thomas doub", "april bragg", "jason luellen", "christina van regenmorter", "jennifer lockman", "randall reiserer"], "accepted": false, "id": "1112.1670"}, "pdf": {"name": "1112.1670.pdf", "metadata": {"source": "CRF", "title": "Data Mining Session-Based Patient Reported Outcomes (PROs) in a Mental Health Setting: Toward Data-Driven Clinical Decision Support and Personalized Treatment", "authors": ["Casey Bennett", "Jennifer Lockman"], "emails": ["Casey.Bennett@CenterstoneResearch.org"], "sections": [{"heading": null, "text": "outcome (PRO) instrument utilizing direct client feedback \u2013 was implemented in a large, real-world behavioral healthcare setting in order to evaluate previous findings from smaller controlled studies. PROs provide an alternative window into treatment effectiveness based on client perception and facilitate detection of problems/symptoms for which there is no discernible measure (e.g. pain). The principal focus of the study was to evaluate the utility of the CDOI for predictive modeling of outcomes in a live clinical setting. Implementation factors were also addressed within the framework of the Theory of Planned Behavior by linking adoption rates to implementation practices and clinician perceptions. The results showed that the CDOI does contain significant capacity to predict outcome delta over time based on baseline and early change scores in a large, real-world clinical setting, as suggested in previous research. The implementation analysis revealed a number of critical factors affecting successful implementation and adoption of the CDOI outcome measure, though there was a notable disconnect between clinician intentions and actual behavior. Most importantly, the predictive capacity of the CDOI underscores the utility of direct client feedback measures such as PROs and their potential use as the basis for next generation clinical decision support tools and personalized treatment approaches.\nKeywords- Data Mining; Patient-Reported Outcomes; CDOI; Implementation; Electronic Health Records; Decision Support Systems, Clinical; Theory of Planned Behavior\nI. INTRODUCTION\nRecent years have seen the proliferation of the concepts of patient-reported outcomes (PROs) and client feedback throughout the healthcare and behavioral healthcare spheres. Research has begun to illuminate their utility on the effectiveness of clinical treatment across multiple disorders and diseases, including depression, schizophrenia, diabetes, cancer, and speech problems, among others [1-5]. The basic concept is that utilizing outcomes and/or feedback directly from the patient, rather than filtered through the perspective of the clinician, can provide meaningful information around a patient\u2019s current status, progress, and prognosis.\nMore specifically, PROs have been suggested to benefit clinical practice via 1) facilitating the detection of overlooked problems, including some symptoms for which there is no discernible measure (e.g. pain), 2) providing an alternative window into treatment effectiveness that still utilizes standardized measures, 3) potentially improving patient-clinician communication, and 4) being unaffected by inter-rater reliability [1,6]. On the other hand, PROs have been criticized for 1) being too lengthy and burdensome on clinical workflow, and 2) the skepticism surrounding their clinical meaning and relevance [6]. Beyond this, even if PROs have no direct negative effects, their use may preclude the use of other, more meaningful measures given the limited clinical time available for outcomes collection [7].\nCDOI (Client-Directed Outcome Informed) is a PRO developed by Miller et al. [8]. It was developed with the specific purpose of providing a valid yet brief outcome measurement system that could fit practically into real-world clinical practice. The CDOI is a session-based measure, collected electronically at every visit. It is comprised of two ultra-brief scales comprising 4 questions each \u2013 ORS (a measure of clinical symptomology and functioning) and SRS (a measure of therapeutic alliance). Previous research has shown ORS to compare favorably with other, longer outcome measures such as OQ45 [9,10], as well as the SRS comparing favorably to other, longer measures of therapeutic alliance such as the HAQ-II [11]. In concert with findings from other PRO studies, evaluation of the CDOI has shown that it can improve clinical outcomes [8,12].\nThe principal focus of the study was on evaluating the utility of the CDOI for predictive modeling of outcomes in a live clinical setting. The question exists what utility such an approach may hold with respect to clinical decision support (CDSS) recommendations, particularly a data-driven approach to CDSS that relies on data from live clinical systems and artificial intelligence algorithms that \u201clearn\u201d over time. Electronic health records themselves are only a first step. Techniques such as predictive modeling and data mining can detect patterns in the data that can be applied to new clients when they walk in the door, for example providing recommendations to clinicians about the treatment\noptions most likely to be effective for a specific individual. This is, in essence, an individualized form of practice-based evidence. Another term for this is \u201cpersonalized medicine\u201d [13,14]. PROs and client feedback can play an important role in this sort of individualized approach [5].\nPrior work in this area has primarily addressed the utility of genetic data to inform individualized care. However, it is likely that the next decade will see the integration of multiple sources of data \u2013 genetic, clinical, socio-demographic \u2013 to build a more complete profile of the individual, their inherited risks, and the environmental/behavioral factors associated with disorder and the effective treatment thereof [15]. Indeed, we already see the trend of combining clinical and genetic indicators in prediction of cancer prognosis as a way of developing cheaper, more effective prognostic tools [16-18].\nPROs and client feedback fit into this larger paradigm around personalized medicine and clinical decision support technology (CDSS). However, evidence shows only a small minority of clinicians report collecting outcomes in any form [19]. Bickman [20] addressed these issues, noting that \u201ccurrently everyone but the client appears to benefit from not having a measurement feedback system\u201d (pp.1115). States and funders can claim they are providing for the needs of their citizens without worrying about the effectiveness (or ineffectiveness) of the resources spent. Provider organizations can justify their use of funds by claiming they use \u201cevidence-based\u201d practices. Clinicians and supervisors can evaluate their performance based on their own perceptions rather than actual data [20]. Establishing the utility of PROs and client feedback for making actual treatment decisions is critical to addressing such realities.\nA secondary focus of the study was to examine the role of implementation factors within the framework of the Theory of Planned Behavior (TPB) [21] by linking adoption rates to implementation practices and clinician perceptions. This is a critical topic in understanding utilization of technology, research innovations, outcomes assessments (including PROs such as CDOI), and the like in real world settings [22-25]. Good technology that goes unused is of questionable success. There is still a need for better understanding of multi-pronged, flexible yet replicable implementation strategies and barriers thereof [26,27].\nII. METHODS\nA. Setting and Data\nThe CDOI was implemented at Centerstone, the largest community-based (outpatient clinic) mental healthcare provider in the United States seeing over 75,000 distinct individuals a year across 130 clinic locations in Indiana and Tennessee. Centerstone Research Institute is an arm of Centerstone devoted to integrating evidence and practice, conducting clinical research, developing clinical decision support tools, and building new healthcare informatics technologies, among other goals. Centerstone has a fully functional electronic health record (EHR) that maintains virtually all relevant patient records. Its clinical services operate under a mixture of both fee-for-service and case rate\npayment methodologies, including Medicare, Safety Net (Tennessee-sponsored health insurance for the population of seriously mentally ill who are ineligible for Medicaid), three distinct Medicaid payers (due to state subcontracting) with different sets of rules, a variety of Commercial payers, as well as an assortment of \u201cother\u201d payers such as county subsidies, DCS (Tennessee Dept. of Children Services), federal probation funds, and grants.\nApproximately 150 clinicians were selected for inclusion in a 6-month pilot study from March 2010 thru August 2010. These included clinicians both in adult outpatient therapy as well as intensive in-home case management for children. For purposes of this analysis, data was limited to that collected for adults and adolescents over the age of 13 who received at least some therapy. The CDOI was integrated into the EHR and collected on a per session basis. As often occurs in real clinical practice, the CDOI was not always collected with fidelity at every single time point, and patients had the option to skip the ORS or SRS at any point if they so chose. The ORS was collected at the beginning of each session via a slider bar (scale 0-10) directly on the screen that the client could manipulate, in relation to each of the four questions that comprise the ORS scale [8]. The SRS was similarly collected, but at the end of each session via a slider bar (scale 0-10) directly on the screen that the client could manipulate, in relation to the four questions that comprise the SRS scale [8]. Furthermore, clients were identified as either \u201cnew\u201d or \u201cold\u201d based on whether they had been seen within 90 days prior of the baseline CDOI at any Centerstone treatment facility or program.\nFor purposes of the data mining analysis, data was limited to those patients who received both a baseline CDOI and 3rd visit CDOI (based on previous \u201cearly change\u201d findings, see the Results section), as well as having an additional CDOI at some point between the 5th visit and 10th visit (defined as the \u201cfinal\u201d CDOI). As such, all patients included in the sample were essentially examined based on their first 10 visits, which generally equates to a 3-4 month time period. The final sample was n=714 (n=253 for new patients only). On average, they received 7.2 visits during the 6 month pilot, with an average length of stay of 115 days (defined as baseline to final; patients may have continued to receive services outside the scope of the CDOI analysis).\nB. Data Analysis\nData was pulled nightly from the electronic health record and loaded into a data warehouse (DW) specifically for answering research and analytical questions. The DW also served as the basis for reporting and clinical feedback as specified in the implementation section below [14]. Subsequently, data was loaded into KNIME (Version 2.3.1) [28], an advanced data mining, modeling, and statistical software package. Data mining typically follows a standard process flow that can be broken into a number of main steps: data preparation, feature selection, model construction, and model evaluation. It should be noted that not all of these steps are performed all the time \u2013 for instance one may build\nmodels without any feature selection in order to evaluate the effect of feature selection on a particular dataset. Below, these steps are briefly outlined in the context of the current study; a more comprehensive overview of specific data mining strategies and methodologies can be found in any of a number of resources on the subject [29,30].\nThe primary analysis focused on clinical outcomes as measured by the change in ORS scores over time (research questions #1 and #2). The primary question was whether clients would obtain average or better outcome delta (change over time) from baseline to final visit (or vice versa, worse outcome delta). As such, the target variable (ORS delta final) was discretized into a binary variable of plus/minus the mean (equivalent to an equal bins classification approach). The consequences and assumptions of reduction to a binary classification problem are addressed in Boulesteix et al. [18], noting that the issues of making such assumptions are roughly equivalent to making such assumptions around normal distributions. All predictor variables (shown in Table I) were z-score normalized. Subsequently, all predictor variables were either 1) not discretized (labeled \u201cBin Target\u201d), or 2) discretized via CAIM (Class-Attribute Interdependence Maximization). CAIM is a form of entropy-based discretization that attempts to maximize the available \u201cinformation\u201d in the dataset by delineating categories in the predictor variables that relate to classes of the target variable. By identifying and using patterns in the data itself, CAIM has been shown to improve classifier performance [31]. It should be noted that not all models are capable of handling both discretized and continuous variables, and thus both methods were not applied to all modeling methods. Additionally, some methods, such as certain kinds of neural networks or decision trees, may dynamically convert numeric variables into binary or categorical variables as part of their modeling process. As such, even when no pre-discretization was performed, it may have occurred within the modeling process itself. It should also be noted that practically all clients during the pilot received therapy services, which effectively excised all the q_therapy variables, including specific individual and group ones, from the analysis due to lack of variance.\nMultiple models were constructed on the dataset to determine optimal performance using both native, built-in KNIME models as well as models incorporated from WEKA (Waikato Environment for Knowledge Analysis; Version 3.5.6) [30]. Models were generally run using default parameters, though some experimentation was performed. Of note, decay was set to true for MP Neural Networks, max_parents was set to 3 for Bayesian Network-K2, and number of nearest neighbors was set to 3 for K-Nearest Neighbors. Models tested included Na\u00efve Bayes [30], HNB (Hidden Na\u00efve Bayes) [32], AODE (Aggregating OneDependence Estimators) [33], Bayesian Networks[30], Multi-layer Perceptron neural networks [30], Random Forests [34], J48 Decision Trees (a variant of the classic C4.5 algorithm) [35], Log Regression, and K-Nearest Neighbors [36]. Additionally, ensembles were built using a combination of Na\u00efve Bayes, Multi-layer Perceptron neural network, Random Forests, K-nearest neighbors, and logistic regression, employing forward selection optimized by AUC (area under the curve) [37]. Voting by committee was also performed with those same five methods, based on maximum probability [38]. Voting by committee is a \u201cmetamodeling\u201d technique (like ensemble) that \u201ccombines\u201d multiple models by allowing them to \u201cvote\u201d for the winning classification. It seeks to take advantage of the strengths of different modeling approaches while minimizing their drawbacks. Due to the number of models used, detailed explanations of individual methods are not provided here for brevity, but can be found elsewhere [29,30].\nThe last step was to evaluate model performance to rule out the possibility that statistical findings may be an artifact of capitalization on chance, which was performed using 10- fold cross-validation [30]. All models were evaluated using multiple performance metrics, including raw predictive accuracy; variables related to standard ROC (receiver operating characteristic) analysis such as AUC (area under the curve) and true/false positive rates [39]; and Hand\u2019s H [40]. The data mining methodology and reporting is in keeping with recommended guidelines [41,42], such as the proper construction of cross-validation, incorporation of feature selection within cross-validation folds, testing of multiple methods, and reporting of multiple metrics of performance, among others.\nAdditionally, some of the better performing models were evaluated using feature selection prior to modeling (but within each cross-validation fold). Feature selection is a key component in filtering out noisy and/or redundant variables from datasets and building parsimonious, explanatory models that retain generalizability. Particularly of interest was how clinical and demographic variables would compare in utility versus baseline outcome measures and \u201cearly change\u201d delta (baseline to third visit) for predicting final outcome delta. The feature selection methods used include univariate filter methods (Chi-squared, Relief-F), multivariate subset methods (Consistency-Based \u2013Best First Search, Symmetrical Uncertainty Correlation-Based Subset Evaluator) and wrapper-based (Rank Search employing Chisquared and Gain Ratio). The advantages and disadvantages\nof these different types of feature selection are welladdressed elsewhere [43]. For purposes of this study, feature selection was only performed using a Na\u00efve Bayes wrapper (where variables to be included in the final model were chosen by first building preliminary Na\u00efve Bayes models using different selections of variables and selecting the feature set with the highest cross-validated performance) [44].\nC. Implementation Analysis\nA clinician survey based on TPB was sent to all clinicians, with about half (n=66) responding. The survey included questions to assess the three domains of TPB as well as questions on intention to use the CDOI and years of experience as a practicing clinician (see Table II), each scored as a 4-point Likert scale. These data were then combined with other EHR data such as clinician age and gender. Adoption rates for each clinician (indicating the percent of the time the CDOI was completed when it should have been, not counting client opt-out skips) were calculated for November, 3 months after the end of the pilot and implementation efforts. Additionally, average outcome scores (including baseline ORS and SRS and ORS delta from baseline to final) were collected for each clinician, based on the same n=714 sample described for data mining above (as such only about half of the clinician respondents, n=33, had qualifying outcomes data). The principle questions were 1) how the survey results associated with the three components of TPB (Questions #1-9) would affect intentions (as indicated by Question #10), 2) if those effects would be mediated by other factors such as clinician experience or age, and 3) whether intentions were related to actual behavior (measured by adoption rates and outcomes). Survey and associated data were analyzed via correlation analysis, ttests, ANOVAs, and regression analysis.\nIII. RESULTS\nA. Data Analysis\nDescriptive statistics for ORS are presented in Table III, including baseline ORS, final ORS, and ORS final delta, broken out by state and by new vs. old clients. Additionally, to account for a regression to the mean effect, reliable change (where for ORS delta, <-4 equates to \u201cdeteriorate\u201d, between -4 and 4 equates to \u201cno change\u201d, and >4 equates to \u201cimprove\u201d, as defined by Miller et al. (2006) was tested via Chi-Square for all clients (not excluding clients with baseline ORS>25) and found significant deviations from expectation (\u03c7 2 =128.6, p<.000, n=714) given equal categorical expectations. It should be noted that approximately 52.8% (377/714) of clients achieved reliable improvement, which compares closely to Miller et al. [8] final values of 47%, although the reliable deterioration of 19.6% is much higher than their reported final 8%. Table IV shows the crosstabulation breakout of reliable change and clinical significance (defined as moving from below clinical cutoff to above, for ORS cutoff=25, as per Miller et al. [8]) for clients who started in the clinical range (baseline ORS<=25), broken out by old and new clients. For the 184 new clients who fit this profile, nearly 46%, or approximately half, of those achieved both reliable improvement (reliable change = 3) and clinically significant change (clinical significance=1). Success was lower for old, existing clients, yet still 35.2% achieved both reliable improvement and clinically significant change.\nModeling results to predict ORS delta final can be seen based on all clients (Table V) and on new clients only (Table VI) with and without pre-discretization (CAIM) of the predictor variables, sorted by AUC. Both of these sets of results are based on models produced using a Na\u00efve Bayes wrapper for feature selection (for reference, models of comparable performance were built without feature selection for all clients, while models consisting of new clients only did improve performance via feature selection). For both all clients and new only, the best performing models were generally either Ensemble methods or Bayesian methods (Na\u00efve Bayes, AODE, LBR). For all clients, the best performing models produced around 70-71% accuracy and .76-.77 AUC. For new clients only, the best performing models produced around 75-77% accuracy and .82-.83 AUC. These results indicate that the CDOI has the ability to serve as the basis for predictive models that may anticipate the effect of clinical treatment in terms of change over time with reasonable accuracy."}, {"heading": "10X Cross-Val (partitioned)", "text": ""}, {"heading": "10X Cross-Val (partitioned)", "text": "An additional question was whether early ORS/SRS ratings (including baseline scores) were predictive of ORS delta final, including relative to other potential clinical/demographic predictors such as age, gender, diagnosis, and treatment modality variation. The results can be seen in Table VII. Consistently, bl_ors and third_delta_ors appeared as the most significant variables \u2013 which given the lack of a \u201cregression to the mean\u201d effect, is notable. This finding agrees with the theory of Miller [8], who argues that early change from treatment (first 3 visits) predicts long term change (e.g. change after 3 months). Equivalent findings have been reported in other diseases such as lung cancer [45]. Additionally, bl_srs was identified as an important predictor whenever pre-discretization (CAIM) was applied. Payor_grp also appeared numerous times, with age, gender, diag_cat, and county occasionally appearing. Quantifying the actual importance of each variable relative to odds ratios, where possible, is particularly\ninformative. In this regard, the third_delta_ors was by far the most critical variable, with an odds ratio of 11.37 \u00b1 6.03 (higher values more likely to show greater final ORS delta). Bl_ors followed, with an odds ratio of 7.86 \u00b1 3.58, lower values being more likely to show greater final ORS delta. Both bl_srs and gender had above-1 odds ratios, though the lower confidence interval fell near or below 1 (the significance threshold). In terms of payor_grp, it appears that commercial clients were more likely to obtain better final ORS delta values. It is possible that this result is related to socio-economic status, but this interpretation warrants caution and further study. There were also some differences in diag_cat, but they were very slight and of questionable importance. It should be noted that because all clients received the same treatment modality during the pilot (outpatient therapy), we were not able to truly evaluate the influence of treatment modality variation, and its effect remains to be seen.\nTable VIII shows the correlations between various components of TPB, clinician age, and behavior (adoption rate), as well as clinical outcomes. Both normative beliefs (spearman\u2019s rho=.45, p=.000, n=66) and perceived utility (spearman\u2019s rho=.25, p=.042, n=66) significantly correlated with intentions. However, there was not a significant relationship between intentions and eventual behavior (spearman\u2019s rho= -.178). This result represents a previously identified problem in which beliefs/attitudes may correlate with intentions, but intentions do not necessarily correlate with behavior [46,47]. The most likely explanation is that organizational requirements to collect the CDOI by management affected behavior in ways that mitigated the impact of intent. However, one specific question (#4) that sought to address that issue found that older, more experienced clinicians were less likely to agree that they were expected to use the tool. Additional regression analysis found that TPB components explained 30.3% of the variance in intent (F=8.983, p=.000, df=62) but an insignificant amount of actual behavior (adoption).\nThe CDOI, a patient-reported outcome measure, was implemented in a large, real-world behavioral healthcare setting in order to evaluate previous findings from smaller controlled studies, such as Miller [8]. PROs provide an alternative window into treatment effectiveness based on client perception and facilitate detection of problems/symptoms for which there is no discernible measure (e.g. pain) [6]. The principle focus of this paper is on evaluating the utility of PROs for predictive modeling of outcomes in a live clinical setting. The CDOI was found to be a relatively good predictor of change over time, in terms of predicting final outcome delta based on baseline and early change scores, as predicted by Miller [8] and other domains such as cancer [45]. Both measures of symptomology/functioning (ORS) and therapeutic alliance (SRS) contributed to this predictive capacity. Other variables, such as gender, also potentially affect this predictive capacity. Using new clients only, algorithms were constructed that could successfully predict above or below mean change approximately 75-77% of the time, well in excess of the 50% random chance. Analysis also showed that the outcome deltas over time were not simply a matter of regression to the mean.\nMost importantly, the predictive capacity of the CDOI underscores the utility of direct client feedback measures such as PROs and their potential use as the basis for next generation clinical decision support (CDSS) tools and personalized treatment approaches. Although many CDSS\ntools have been constructed previously using traditional outcome measures, PROs open a new and uncharted window into understanding and predicting clinical outcomes and treatment effectiveness.\nImplementation efforts were evaluated within the framework of the Theory of Planned Behavior (TPB) [21]. Significant relationships were found between normative beliefs, perceived utility, and clinician intent to use the CDOI. However, intention to use the CDOI did not necessarily translate into actual adoption behavior, a phenomenon previously reported elsewhere [46,47]. Adoption behavior appeared to be impacted by clinician experience and age, with older, more experienced clinicians more likely to use the CDOI despite lower levels of perceived utility and intent. Neither TPB components nor intent had any relation to actual clinical outcomes using the CDOI. This study provides an example of a framework for an integrated implementation evaluation in the context of a larger technology or outcome implementation. This sort of approach is a necessity for understanding technologies such as clinical decision support and outcomes measures such as PROs in real-world settings and the process of adoption [24,25]\nThe study presented here was a pilot of using PROs such as CDOI in a large, real-world mental health setting. Future directions include a larger evaluation of the CDOI in the context of clinical decision support for treatment recommendations in a real-world setting, and a comparison of the utility of the CDOI to more traditional measures of clinical outcome such PHQ9 in such settings [48]. Such an approach can produce individualized treatment predictions, as shown the examples in Figures 1 and 2 based on similar outcome measures, providing a clinician the probability of average or better treatment response across a number of modality combinations [13].\nUtilizing PROs to enhance clinical decision-making\nthrough such data-driven CDSS approaches remains the\nlong-term focus."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was funded by the Ayers Foundation and the Joe C. Davis Foundation. The opinions expressed herein do not necessarily reflect the views of Centerstone or its affiliates. The authors have no conflict of interest with the subjects described herein."}], "references": [{"title": "Patient-reported outcome measures: Use in evaluation of treatment for aphasia", "author": ["K.B. Ross"], "venue": "Journal of Medical Speech, vol. 14, no. 3, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Patient-reported outcomes in schizophrenia", "author": ["R. McCabe", "M. Saidi", "S. Priebe"], "venue": "The British Journal of Psychiatry. Supplement, vol. 50, pp. s21-28, Aug. 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Validation of two generic patient-reported outcome measures in patients with type 2 diabetes", "author": ["L.S. Matza", "K.S. Boye", "N. Yurgin"], "venue": "Health and Quality of Life Outcomes, vol. 5, pp. 47-47, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Systematic use of patient-rated depression severity monitoring: is it helpful and feasible in clinical psychiatry", "author": ["F.F. Duffy", "H. Chung", "M. Trivedi", "D.S. Rae", "D.A. Regier", "D.J. Katzelnick"], "venue": "Psychiatric Services, vol. 59, no. 10, pp. 1148-1154, Oct. 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "The prognostic significance of patient-reported outcomes in cancer clinical trials", "author": ["C.C. Gotay", "C.T. Kawamoto", "A. Bottomley", "F. Efficace"], "venue": "Journal of Clinical Oncology, vol. 26, no. 8, pp. 1355- 1363, Mar. 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "The impact of measuring patient-reported outcomes in clinical practice: a systematic review of the literature", "author": ["J.M. Valderas"], "venue": "Quality of Life Research, vol. 17, no. 2, pp. 179-193, Mar. 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Measuring patientreported outcomes: moving from clinical trials into clinical practice", "author": ["J.M. Valderas", "J. Alonso", "G.H. Guyatt"], "venue": "The Medical Journal of Australia, vol. 189, no. 2, pp. 93-94, Jul. 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Using formal client feedback to improve retention and outcome: Making ongoing, real-time assessment feasible", "author": ["S.D. Miller", "B.L. Duncan", "J. Brown", "R. Sorrell", "M.B. Chalk"], "venue": "Journal of Brief Therapy, vol. 5, no. 1, pp. 5\u201322, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "The reliability and validity of the Outcome Rating Scale: A replication study of a brief clinical measure", "author": ["D.L. Bringhurst"], "venue": "Journal of Brief Therapy, vol. 5, no. 1, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Outcome Rating Scale and Session Rating Scale in psychological practice: Clinical utility of ultra-brief measures", "author": ["A. Campbell", "S. Hemsley"], "venue": "Clinical Psychologist, vol. 13, no. 1, pp. 1\u20139, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "The Session Rating Scale: Preliminary Psychometric Properties of a \u2018Working\u2019 Alliance Measure", "author": ["B.L. Duncan", "S.D. Miller", "J.A. Sparks", "D.A. Claud", "L.R. Reynolds", "J. Brown", "L.D. Johnson"], "venue": "Journal of Brief Therapy, vol. 3, no. 1, pp. 3-12, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Using client feedback to improve couple therapy outcomes: a randomized clinical trial in a naturalistic setting", "author": ["M.G. Anker", "B.L. Duncan", "J.A. Sparks"], "venue": "Journal of Consulting and Clinical Psychology, vol. 77, no. 4, pp. 693-704, Aug. 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Data mining and electronic health records: Selecting optimal clinical treatments in practice", "author": ["C. Bennett", "T. Doub"], "venue": "Proceedings of The 6th International Conference on Data Mining, pp. 313-318, 2010. http://www.openminds.com/library/110410dmehr.htm", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Clinical productivity system: A decision support model.", "author": ["C.C. Bennett"], "venue": "International Journal of Productivity and Performance Management,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "The twin questions of personalized medicine: who are you and whom do you most resemble", "author": ["I.S. Kohane"], "venue": "Genome Medicine, vol. 1, no. 1, pp. 4-4, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting the prognosis of breast cancer by integrating clinical and microarray data with Bayesian networks", "author": ["O. Gevaert", "F. De Smet", "D. Timmerman", "Y. Moreau", "B. De Moor"], "venue": "Bioinformatics, vol. 22, no. 14, pp. e184-190, Jul. 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Improved breast cancer prognosis through the combination of clinical and genetic markers", "author": ["Y. Sun", "S. Goodison", "J. Li", "L. Liu", "W. Farmerie"], "venue": "Bioinformatics, vol. 23, no. 1, p. 30, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Microarray-based classification and clinical predictors: on combined classifiers and additional predictive value", "author": ["A. Boulesteix", "C. Porzelius", "M. Daumer"], "venue": "Bioinformatics, vol. 24, no. 15, pp. 1698-1706, Aug. 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "The use of outcome measures by psychologists in clinical practice", "author": ["D.R. Hatfield", "B.M. Ogles"], "venue": "Professional Psychology: Research and Practice, vol. 35, no. 5, p. 485, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "A measurement feedback system (MFS) is necessary to improve mental health outcomes", "author": ["L. Bickman"], "venue": "Journal of the American Academy of Child and Adolescent Psychiatry, vol. 47, no. 10, pp. 1114-1119, Oct. 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "The theory of planned behavior", "author": ["I. Ajzen"], "venue": "Organizational Behavior and Human Decision Processes, vol. 50, no. 2, pp. 179-211, 1991.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1991}, {"title": "Improving clinical practice using clinical decision support systems: a systematic review of trials to identify features critical to success", "author": ["K. Kawamoto", "C.A. Houlihan", "E.A. Balas", "D.F. Lobach"], "venue": "British Medical Journal, vol. 330, no. 7494, pp. 765-765, 2005.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Implementation matters: a review of research on the influence of implementation on program outcomes and the factors affecting implementation", "author": ["J.A. Durlak", "E.P. DuPre"], "venue": "American Journal of Community Psychology, vol. 41, no. 3, pp. 327-350, Jun. 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Grand challenges in clinical decision support", "author": ["D.F. Sittig"], "venue": "Journal of Biomedical Informatics, vol. 41, no. 2, pp. 387-392, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "The dissemination and implementation of evidence-based psychological treatments. A review of current efforts", "author": ["R.K. McHugh", "D.H. Barlow"], "venue": "The American Psychologist, vol. 65, no. 2, pp. 73-84, Mar. 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Factors influencing the implementation of clinical guidelines for health care professionals: A systematic meta-review", "author": ["A.L. Francke", "M.C. Smit", "A.J. de Veer", "P. Mistiaen"], "venue": "BMC Medical Informatics and Decision Making, vol. 8, pp. 38-38.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 0}, {"title": "Why don't physicians adhere to guideline recommendations  in practice? An analysis of barriers among Dutch general practitioners", "author": ["M. Lugtenberg", "J.M. Zegers-van Schaick", "G.P. Westert", "J.S. Burgers"], "venue": "Implementation Science, vol. 4, pp. 54-54.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 0}, {"title": "KNIME: The Konstanz Information Miner", "author": ["M.R. Berthold"], "venue": "Data Analysis, Machine Learning and Applications, 2008, pp. 319- 326.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Principles of data mining", "author": ["D.J. Hand", "H. Mannila", "P. Smyth"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2001}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques, Second Edition, 2nd ed", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "CAIM Discretization Algorithm", "author": ["L.A. Kurgan", "K.J. Cios"], "venue": "IEEE Trans. on Knowl. and Data Eng., vol. 16, no. 2, pp. 145-153, 2004.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Hidden naive Bayes", "author": ["H. Zhang", "L. Jiang", "J. Su"], "venue": "Proceedings of the 20th national conference on Artificial intelligence - Volume 2, pp. 919-924, 2005.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Not so naive bayes: Aggregating one-dependence estimators", "author": ["G.I. Webb", "J.R. Boughton", "Z. Wang"], "venue": "Machine Learning, vol. 58, no. 1, pp. 5\u201324, 2005.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, pp. 5\u201332, Oct. 2001.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "5: programs for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Instance-Based Learning Algorithms", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": "Machine Learning, vol. 6, pp. 37\u201366, Jan. 1991.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1991}, {"title": "Ensemble selection from libraries of models", "author": ["R. Caruana", "A. Niculescu-Mizil", "G. Crew", "A. Ksikes"], "venue": "Proceedings of the twenty-first international conference on Machine learning, pp. 18, 2004.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "ROC graphs: Notes and practical considerations for researchers", "author": ["T. Fawcett"], "venue": "Machine Learning, vol. 31, 2004.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Measuring classifier performance: a coherent alternative to the area under the ROC curve", "author": ["D. Hand"], "venue": "Machine Learning, vol. 77, no. 1, pp. 103-123, Oct. 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Classifier Technology and the Illusion of Progress", "author": ["D.J. Hand"], "venue": "Statistical Science, vol. 21, no. 1, pp. 1-14, Feb. 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Critical review of published microarray studies for cancer outcome and guidelines on statistical analysis and reporting", "author": ["A. Dupuy", "R.M. Simon"], "venue": "Journal of the National Cancer Institute, vol. 99, no. 2, pp. 147-157, Jan. 2007.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "A review of feature selection techniques in bioinformatics", "author": ["Y. Saeys", "I. Inza", "P. Larra\u00f1aga"], "venue": "Bioinformatics (Oxford, England), vol. 23, no. 19, pp. 2507-2517, Oct. 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial Intelligence, vol. 97, pp. 273\u2013324, 1997.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1997}, {"title": "Early change in patient-reported health during lung cancer chemotherapy predicts clinical outcomes beyond those predicted by baseline report: results from Eastern Cooperative Oncology Group Study 5592", "author": ["D.T. Eton", "D.L. Fairclough", "D. Cella", "S.E. Yount", "P. Bonomi", "D.H. Johnson"], "venue": "Journal of Clinical Oncology, vol. 21, no. 8, pp. 1536-1543, Apr. 2003.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2003}, {"title": "Applying theory-driven approaches to understanding and modifying clinicians' behavior: what do we know", "author": ["M.B. Perkins"], "venue": "Psychiatric Services (Washington, D.C.), vol. 58, no. 3, pp. 342-348, Mar. 2007.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "The technology acceptance model: its past and its future in health care", "author": ["R.J. Holden", "B. Karsh"], "venue": "Journal of Biomedical Informatics, vol. 43, no. 1, pp. 159-172, 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimized Antidepressant Therapy and Pain Self- Management in Primary Care Patients with Depression and Musculoskeletal Pain: A Randomized Controlled Trial", "author": ["K. Kroenke"], "venue": "JAMA : the journal of the American Medical Association, vol. 301, no. 20, pp. 2099-2110, May. 2009.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Research has begun to illuminate their utility on the effectiveness of clinical treatment across multiple disorders and diseases, including depression, schizophrenia, diabetes, cancer, and speech problems, among others [1-5].", "startOffset": 219, "endOffset": 224}, {"referenceID": 1, "context": "Research has begun to illuminate their utility on the effectiveness of clinical treatment across multiple disorders and diseases, including depression, schizophrenia, diabetes, cancer, and speech problems, among others [1-5].", "startOffset": 219, "endOffset": 224}, {"referenceID": 2, "context": "Research has begun to illuminate their utility on the effectiveness of clinical treatment across multiple disorders and diseases, including depression, schizophrenia, diabetes, cancer, and speech problems, among others [1-5].", "startOffset": 219, "endOffset": 224}, {"referenceID": 3, "context": "Research has begun to illuminate their utility on the effectiveness of clinical treatment across multiple disorders and diseases, including depression, schizophrenia, diabetes, cancer, and speech problems, among others [1-5].", "startOffset": 219, "endOffset": 224}, {"referenceID": 4, "context": "Research has begun to illuminate their utility on the effectiveness of clinical treatment across multiple disorders and diseases, including depression, schizophrenia, diabetes, cancer, and speech problems, among others [1-5].", "startOffset": 219, "endOffset": 224}, {"referenceID": 0, "context": "pain), 2) providing an alternative window into treatment effectiveness that still utilizes standardized measures, 3) potentially improving patient-clinician communication, and 4) being unaffected by inter-rater reliability [1,6].", "startOffset": 223, "endOffset": 228}, {"referenceID": 5, "context": "pain), 2) providing an alternative window into treatment effectiveness that still utilizes standardized measures, 3) potentially improving patient-clinician communication, and 4) being unaffected by inter-rater reliability [1,6].", "startOffset": 223, "endOffset": 228}, {"referenceID": 5, "context": "On the other hand, PROs have been criticized for 1) being too lengthy and burdensome on clinical workflow, and 2) the skepticism surrounding their clinical meaning and relevance [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 6, "context": "Beyond this, even if PROs have no direct negative effects, their use may preclude the use of other, more meaningful measures given the limited clinical time available for outcomes collection [7].", "startOffset": 191, "endOffset": 194}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Previous research has shown ORS to compare favorably with other, longer outcome measures such as OQ45 [9,10], as well as the SRS comparing favorably to other, longer measures of therapeutic alliance such as the HAQ-II [11].", "startOffset": 102, "endOffset": 108}, {"referenceID": 9, "context": "Previous research has shown ORS to compare favorably with other, longer outcome measures such as OQ45 [9,10], as well as the SRS comparing favorably to other, longer measures of therapeutic alliance such as the HAQ-II [11].", "startOffset": 102, "endOffset": 108}, {"referenceID": 10, "context": "Previous research has shown ORS to compare favorably with other, longer outcome measures such as OQ45 [9,10], as well as the SRS comparing favorably to other, longer measures of therapeutic alliance such as the HAQ-II [11].", "startOffset": 218, "endOffset": 222}, {"referenceID": 7, "context": "In concert with findings from other PRO studies, evaluation of the CDOI has shown that it can improve clinical outcomes [8,12].", "startOffset": 120, "endOffset": 126}, {"referenceID": 11, "context": "In concert with findings from other PRO studies, evaluation of the CDOI has shown that it can improve clinical outcomes [8,12].", "startOffset": 120, "endOffset": 126}, {"referenceID": 12, "context": "Another term for this is \u201cpersonalized medicine\u201d [13,14].", "startOffset": 49, "endOffset": 56}, {"referenceID": 13, "context": "Another term for this is \u201cpersonalized medicine\u201d [13,14].", "startOffset": 49, "endOffset": 56}, {"referenceID": 4, "context": "PROs and client feedback can play an important role in this sort of individualized approach [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 14, "context": "However, it is likely that the next decade will see the integration of multiple sources of data \u2013 genetic, clinical, socio-demographic \u2013 to build a more complete profile of the individual, their inherited risks, and the environmental/behavioral factors associated with disorder and the effective treatment thereof [15].", "startOffset": 314, "endOffset": 318}, {"referenceID": 15, "context": "Indeed, we already see the trend of combining clinical and genetic indicators in prediction of cancer prognosis as a way of developing cheaper, more effective prognostic tools [16-18].", "startOffset": 176, "endOffset": 183}, {"referenceID": 16, "context": "Indeed, we already see the trend of combining clinical and genetic indicators in prediction of cancer prognosis as a way of developing cheaper, more effective prognostic tools [16-18].", "startOffset": 176, "endOffset": 183}, {"referenceID": 17, "context": "Indeed, we already see the trend of combining clinical and genetic indicators in prediction of cancer prognosis as a way of developing cheaper, more effective prognostic tools [16-18].", "startOffset": 176, "endOffset": 183}, {"referenceID": 18, "context": "However, evidence shows only a small minority of clinicians report collecting outcomes in any form [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "Bickman [20] addressed these issues, noting that \u201ccurrently everyone but the client appears to benefit from not having a measurement feedback system\u201d (pp.", "startOffset": 8, "endOffset": 12}, {"referenceID": 19, "context": "Clinicians and supervisors can evaluate their performance based on their own perceptions rather than actual data [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": "A secondary focus of the study was to examine the role of implementation factors within the framework of the Theory of Planned Behavior (TPB) [21] by linking adoption rates to implementation practices and clinician perceptions.", "startOffset": 142, "endOffset": 146}, {"referenceID": 21, "context": "This is a critical topic in understanding utilization of technology, research innovations, outcomes assessments (including PROs such as CDOI), and the like in real world settings [22-25].", "startOffset": 179, "endOffset": 186}, {"referenceID": 22, "context": "This is a critical topic in understanding utilization of technology, research innovations, outcomes assessments (including PROs such as CDOI), and the like in real world settings [22-25].", "startOffset": 179, "endOffset": 186}, {"referenceID": 23, "context": "This is a critical topic in understanding utilization of technology, research innovations, outcomes assessments (including PROs such as CDOI), and the like in real world settings [22-25].", "startOffset": 179, "endOffset": 186}, {"referenceID": 24, "context": "This is a critical topic in understanding utilization of technology, research innovations, outcomes assessments (including PROs such as CDOI), and the like in real world settings [22-25].", "startOffset": 179, "endOffset": 186}, {"referenceID": 25, "context": "There is still a need for better understanding of multi-pronged, flexible yet replicable implementation strategies and barriers thereof [26,27].", "startOffset": 136, "endOffset": 143}, {"referenceID": 26, "context": "There is still a need for better understanding of multi-pronged, flexible yet replicable implementation strategies and barriers thereof [26,27].", "startOffset": 136, "endOffset": 143}, {"referenceID": 7, "context": "The ORS was collected at the beginning of each session via a slider bar (scale 0-10) directly on the screen that the client could manipulate, in relation to each of the four questions that comprise the ORS scale [8].", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "The SRS was similarly collected, but at the end of each session via a slider bar (scale 0-10) directly on the screen that the client could manipulate, in relation to the four questions that comprise the SRS scale [8].", "startOffset": 213, "endOffset": 216}, {"referenceID": 13, "context": "The DW also served as the basis for reporting and clinical feedback as specified in the implementation section below [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "1) [28], an advanced data mining, modeling, and statistical software package.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Below, these steps are briefly outlined in the context of the current study; a more comprehensive overview of specific data mining strategies and methodologies can be found in any of a number of resources on the subject [29,30].", "startOffset": 220, "endOffset": 227}, {"referenceID": 29, "context": "Below, these steps are briefly outlined in the context of the current study; a more comprehensive overview of specific data mining strategies and methodologies can be found in any of a number of resources on the subject [29,30].", "startOffset": 220, "endOffset": 227}, {"referenceID": 17, "context": "[18], noting that the issues of making such assumptions are roughly equivalent to making such assumptions around normal distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "By identifying and using patterns in the data itself, CAIM has been shown to improve classifier performance [31].", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "6) [30].", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Models tested included Na\u00efve Bayes [30], HNB (Hidden Na\u00efve Bayes) [32], AODE (Aggregating OneDependence Estimators) [33], Bayesian Networks[30], Multi-layer Perceptron neural networks [30], Random Forests [34], J48 Decision Trees (a variant of the classic C4.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "Models tested included Na\u00efve Bayes [30], HNB (Hidden Na\u00efve Bayes) [32], AODE (Aggregating OneDependence Estimators) [33], Bayesian Networks[30], Multi-layer Perceptron neural networks [30], Random Forests [34], J48 Decision Trees (a variant of the classic C4.", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "Models tested included Na\u00efve Bayes [30], HNB (Hidden Na\u00efve Bayes) [32], AODE (Aggregating OneDependence Estimators) [33], Bayesian Networks[30], Multi-layer Perceptron neural networks [30], Random Forests [34], J48 Decision Trees (a variant of the classic C4.", "startOffset": 116, "endOffset": 120}, {"referenceID": 29, "context": "Models tested included Na\u00efve Bayes [30], HNB (Hidden Na\u00efve Bayes) [32], AODE (Aggregating OneDependence Estimators) [33], Bayesian Networks[30], Multi-layer Perceptron neural networks [30], Random Forests [34], J48 Decision Trees (a variant of the classic C4.", "startOffset": 139, "endOffset": 143}, {"referenceID": 29, "context": "Models tested included Na\u00efve Bayes [30], HNB (Hidden Na\u00efve Bayes) [32], AODE (Aggregating OneDependence Estimators) [33], Bayesian Networks[30], Multi-layer Perceptron neural networks [30], Random Forests [34], J48 Decision Trees (a variant of the classic C4.", "startOffset": 184, "endOffset": 188}, {"referenceID": 33, "context": "Models tested included Na\u00efve Bayes [30], HNB (Hidden Na\u00efve Bayes) [32], AODE (Aggregating OneDependence Estimators) [33], Bayesian Networks[30], Multi-layer Perceptron neural networks [30], Random Forests [34], J48 Decision Trees (a variant of the classic C4.", "startOffset": 205, "endOffset": 209}, {"referenceID": 34, "context": "5 algorithm) [35], Log Regression, and K-Nearest Neighbors [36].", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "5 algorithm) [35], Log Regression, and K-Nearest Neighbors [36].", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "Additionally, ensembles were built using a combination of Na\u00efve Bayes, Multi-layer Perceptron neural network, Random Forests, K-nearest neighbors, and logistic regression, employing forward selection optimized by AUC (area under the curve) [37].", "startOffset": 240, "endOffset": 244}, {"referenceID": 28, "context": "Due to the number of models used, detailed explanations of individual methods are not provided here for brevity, but can be found elsewhere [29,30].", "startOffset": 140, "endOffset": 147}, {"referenceID": 29, "context": "Due to the number of models used, detailed explanations of individual methods are not provided here for brevity, but can be found elsewhere [29,30].", "startOffset": 140, "endOffset": 147}, {"referenceID": 29, "context": "The last step was to evaluate model performance to rule out the possibility that statistical findings may be an artifact of capitalization on chance, which was performed using 10fold cross-validation [30].", "startOffset": 200, "endOffset": 204}, {"referenceID": 37, "context": "All models were evaluated using multiple performance metrics, including raw predictive accuracy; variables related to standard ROC (receiver operating characteristic) analysis such as AUC (area under the curve) and true/false positive rates [39]; and Hand\u2019s H [40].", "startOffset": 241, "endOffset": 245}, {"referenceID": 38, "context": "All models were evaluated using multiple performance metrics, including raw predictive accuracy; variables related to standard ROC (receiver operating characteristic) analysis such as AUC (area under the curve) and true/false positive rates [39]; and Hand\u2019s H [40].", "startOffset": 260, "endOffset": 264}, {"referenceID": 39, "context": "The data mining methodology and reporting is in keeping with recommended guidelines [41,42], such as the proper construction of cross-validation, incorporation of feature selection within cross-validation folds, testing of multiple methods, and reporting of multiple metrics of performance, among others.", "startOffset": 84, "endOffset": 91}, {"referenceID": 40, "context": "The data mining methodology and reporting is in keeping with recommended guidelines [41,42], such as the proper construction of cross-validation, incorporation of feature selection within cross-validation folds, testing of multiple methods, and reporting of multiple metrics of performance, among others.", "startOffset": 84, "endOffset": 91}, {"referenceID": 41, "context": "of these different types of feature selection are welladdressed elsewhere [43].", "startOffset": 74, "endOffset": 78}, {"referenceID": 42, "context": "For purposes of this study, feature selection was only performed using a Na\u00efve Bayes wrapper (where variables to be included in the final model were chosen by first building preliminary Na\u00efve Bayes models using different selections of variables and selecting the feature set with the highest cross-validated performance) [44].", "startOffset": 321, "endOffset": 325}, {"referenceID": 7, "context": "[8] final values of 47%, although the reliable deterioration of 19.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8]) for clients who started in the clinical range (baseline ORS<=25), broken out by old and new clients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "This finding agrees with the theory of Miller [8], who argues that early change from treatment (first 3 visits) predicts long term change (e.", "startOffset": 46, "endOffset": 49}, {"referenceID": 43, "context": "Equivalent findings have been reported in other diseases such as lung cancer [45].", "startOffset": 77, "endOffset": 81}, {"referenceID": 44, "context": "This result represents a previously identified problem in which beliefs/attitudes may correlate with intentions, but intentions do not necessarily correlate with behavior [46,47].", "startOffset": 171, "endOffset": 178}, {"referenceID": 45, "context": "This result represents a previously identified problem in which beliefs/attitudes may correlate with intentions, but intentions do not necessarily correlate with behavior [46,47].", "startOffset": 171, "endOffset": 178}, {"referenceID": 7, "context": "The CDOI, a patient-reported outcome measure, was implemented in a large, real-world behavioral healthcare setting in order to evaluate previous findings from smaller controlled studies, such as Miller [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 5, "context": "pain) [6].", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "The CDOI was found to be a relatively good predictor of change over time, in terms of predicting final outcome delta based on baseline and early change scores, as predicted by Miller [8] and other domains such as cancer [45].", "startOffset": 183, "endOffset": 186}, {"referenceID": 43, "context": "The CDOI was found to be a relatively good predictor of change over time, in terms of predicting final outcome delta based on baseline and early change scores, as predicted by Miller [8] and other domains such as cancer [45].", "startOffset": 220, "endOffset": 224}, {"referenceID": 20, "context": "Implementation efforts were evaluated within the framework of the Theory of Planned Behavior (TPB) [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 44, "context": "However, intention to use the CDOI did not necessarily translate into actual adoption behavior, a phenomenon previously reported elsewhere [46,47].", "startOffset": 139, "endOffset": 146}, {"referenceID": 45, "context": "However, intention to use the CDOI did not necessarily translate into actual adoption behavior, a phenomenon previously reported elsewhere [46,47].", "startOffset": 139, "endOffset": 146}, {"referenceID": 23, "context": "This sort of approach is a necessity for understanding technologies such as clinical decision support and outcomes measures such as PROs in real-world settings and the process of adoption [24,25] The study presented here was a pilot of using PROs such as CDOI in a large, real-world mental health setting.", "startOffset": 188, "endOffset": 195}, {"referenceID": 24, "context": "This sort of approach is a necessity for understanding technologies such as clinical decision support and outcomes measures such as PROs in real-world settings and the process of adoption [24,25] The study presented here was a pilot of using PROs such as CDOI in a large, real-world mental health setting.", "startOffset": 188, "endOffset": 195}, {"referenceID": 46, "context": "Future directions include a larger evaluation of the CDOI in the context of clinical decision support for treatment recommendations in a real-world setting, and a comparison of the utility of the CDOI to more traditional measures of clinical outcome such PHQ9 in such settings [48].", "startOffset": 277, "endOffset": 281}, {"referenceID": 12, "context": "Such an approach can produce individualized treatment predictions, as shown the examples in Figures 1 and 2 based on similar outcome measures, providing a clinician the probability of average or better treatment response across a number of modality combinations [13].", "startOffset": 262, "endOffset": 266}, {"referenceID": 11, "context": "Example 1 of treatment recommendations using pre-set \u201cservice packages\u201d (from [12]) Fig.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Example 2 of treatment recommendations using pre-set \u201cservice packages\u201d (from [12])", "startOffset": 78, "endOffset": 82}], "year": 2011, "abstractText": "The CDOI outcome measure \u2013 a patient-reported outcome (PRO) instrument utilizing direct client feedback \u2013 was implemented in a large, real-world behavioral healthcare setting in order to evaluate previous findings from smaller controlled studies. PROs provide an alternative window into treatment effectiveness based on client perception and facilitate detection of problems/symptoms for which there is no discernible measure (e.g. pain). The principal focus of the study was to evaluate the utility of the CDOI for predictive modeling of outcomes in a live clinical setting. Implementation factors were also addressed within the framework of the Theory of Planned Behavior by linking adoption rates to implementation practices and clinician perceptions. The results showed that the CDOI does contain significant capacity to predict outcome delta over time based on baseline and early change scores in a large, real-world clinical setting, as suggested in previous research. The implementation analysis revealed a number of critical factors affecting successful implementation and adoption of the CDOI outcome measure, though there was a notable disconnect between clinician intentions and actual behavior. Most importantly, the predictive capacity of the CDOI underscores the utility of direct client feedback measures such as PROs and their potential use as the basis for next generation clinical decision support tools and personalized treatment approaches. KeywordsData Mining; Patient-Reported Outcomes; CDOI; Implementation; Electronic Health Records; Decision Support Systems, Clinical; Theory of Planned Behavior", "creator": "Microsoft\u00ae Word 2010"}}}