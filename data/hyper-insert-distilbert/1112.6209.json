{"id": "1112.6209", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2011", "title": "Building high-level features using large scale unsupervised learning", "abstract": "surely we can consider solution the common problem of building detectors for high - level concepts using usually only loosely unsupervised computer feature learning. for taking example, we would ask like testing to understand knowing if it occurs is possible to learn a face detector using only small unlabeled computer images downloaded from wandering the internet. to automatically answer this incorrect question, we get trained is a simple feature learning concept algorithm on encoding a complete large scaled dataset of captured images ( actually 10 million images, except each image is 200x200 ). generally the full simulation implementation is naturally performed here on a combined cluster of several 1000 nm machines with fast network delivery hardware for one week. despite extensive experimental results nonetheless reveal surprising indirect evidence that such high - level study concepts can indeed be learned using only unlabeled data and a simple learning algorithm.", "histories": [["v1", "Thu, 29 Dec 2011 00:26:54 GMT  (2624kb,D)", "http://arxiv.org/abs/1112.6209v1", null], ["v2", "Tue, 22 May 2012 08:12:49 GMT  (2978kb,D)", "http://arxiv.org/abs/1112.6209v2", null], ["v3", "Tue, 12 Jun 2012 05:12:56 GMT  (2978kb,D)", "http://arxiv.org/abs/1112.6209v3", null], ["v4", "Wed, 11 Jul 2012 04:40:33 GMT  (2978kb,D)", "http://arxiv.org/abs/1112.6209v4", null], ["v5", "Thu, 12 Jul 2012 04:32:50 GMT  (2978kb,D)", "http://arxiv.org/abs/1112.6209v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["quoc v le", "marc'aurelio ranzato", "rajat monga", "matthieu devin", "greg corrado", "kai chen 0010", "jeffrey dean", "andrew y ng"], "accepted": true, "id": "1112.6209"}, "pdf": {"name": "1112.6209.pdf", "metadata": {"source": "CRF", "title": "Building high-level features using large scale unsupervised learning", "authors": ["Quoc V. Le", "Rajat Monga", "Matthieu Devin", "Greg Corrado", "Kai Chen", "Marc\u2019Aurelio Ranzato", "Jeff Dean", "Andrew Y. Ng"], "emails": ["qvl@google.com", "rajatmonga@google.com", "mdevin@google.com", "gcorrado@google.com", "kaichen@google.com", "ranzato@google.com", "jeff@google.com", "ayng@google.com"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised feature learning is the current methodology in machine learning to build features automatically from data, especially data without human labelling. Successful feature learning algorithms and their applications can be found in recent literature, e.g., RBMs [9], Autoencoders [1], Sparse Coding [18], Kmeans [2], ISA [16]. So far, most of these algorithms have succeeded in learning low-level features such as \u201cedge\u201d or \u201cblob\u201d detectors.\nThe focus of this work is to build features that detect high-level concepts in images, thereby attempting to simulate a simpler version of important neuroscientific results known as \u201cgrandmother cell.\u201d This class of cells have been argued to exist in the brain with the role of detecting faces and hands [5] or specific people [21] (e.g., famous actress Jennifer Aniston).\nCurrent computer vision practices typically advocate the use of labeled data to achieve these high-level concepts. For example, to build a face detector, one needs a large collection of images with labeled faces and their locations. In terms of practical applications, the demand of large labeled sets renders this approach difficult to apply to problems where labeled data are rare or the number of classes is large.\nar X\niv :1\n11 2.\n62 09\nv1 [\ncs .L\nG ]\n2 9\nIt has been strongly hypothesized, however, that the brain approaches this problem by making an intelligent use of unlabeled data. This hypothesis is based on the following observation. A human brain typically has approximately 1014 synapses (connections) and we live around 109 seconds. Thus, even if each synapse encodes one bit of information, it would require about 1014 / 109 = 105 bits per second to figure out all connections. This amount of information surpasses the amount of labelled data we have access to via human supervisors.1 Hence, it is more likely that we learn using unsupervised principles rather than supervised principles.\nThe goal of this work is to assess the feasibility of building such high-level features from unlabeled data. A positive answer to this question can lead to two further important outcomes. First and practically, this gives us a way to develop features automatically from inexpensive unlabeled data in contrast to the conventional approach of using more expensive labeled data. More importantly, it answers a more intriguing question if certain properties of the grandmother cell can indeed be learned from a collection of unlabeled images. Informally, this means whether a baby can learn to group faces into one class because it has seen many of them and not because it is guided by supervision or rewards.\nWe conjecture that the lack of high-level features in previous work stems from the fact that it is difficult to learn high-level concepts from downsampled images. We removed this difficulty by using a large dataset consisting of random images downloaded from the internet.2 Feature representations are learned by a sparse deep autoencoder. To scale the model to large images, we use the idea of local receptive fields [23, 15] and many levels of parallelism: model parallelism (parameters are distributed across machines) and data parallelism (data are distributed across machines).\nExperimental results reveal that it is indeed possible to build high-level features with unlabeled data. In particular, using a hold-out test set consisting of faces and random distractors, we discovered a feature that is highly selective for faces. This further confirms the power of deep networks in learning invariant feature representations [7, 8]."}, {"heading": "2 Dataset constructions", "text": "Our dataset is constructed by sampling frames from 10 million YouTube videos. To avoid duplicates, each video contributes only one image to the dataset. Each image has 200x200 pixels and three RGB channels. We then whiten each image by convolving it with fixed on-center off-surround kernels on each channel (similar to local contrast normalization) [13].\nThis dataset is assembled with the goal of representing the experience of a one and a half year old baby without any supervision. Specifically, if each frame represents 5 seconds then our dataset with 10 million images can be as long as 1.5 years.\nTo check the percentage of faces in the dataset, we also ran a face detector in openCV.3\n1This argument, often credited to G. Hinton and cited as motivations in [22], is somewhat widely held in neuroscience.\n2This is different from the work of [19] who trained their model on images from one class. 3http://opencv.willowgarage.com/wiki/\nThis experiment shows that the percentage of faces is approximately 17% in the dataset. However, the positions, orientations, scales of these faces are quite diverse. In section 9, we also show a control experiment on dataset where images, that have faces, are intentionally removed."}, {"heading": "3 Algorithm", "text": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning [20, 9, 1, 24, 18]. In particular, in their seminal work, Olshausen and Field [20] discovered sparse coding, an unsupervised learning principle, that yields receptive fields having properties like those of V1 simple cells. These properties in turn were characterized by [11, 12].\nA shortcoming of early approaches such as sparse coding [20] is that their architectures are shallow and typically shown to capture simple variations (or learned features such as edge detectors). This issue is tackled by recent work of [9, 1, 19] who extend sparse coding to build hierarchies of feature representations. In particular, Lee et al [19] have shown that a convolutional DBNs, trained on images of faces, can learn a decent face detector. This result is interesting but not striking given the fact that the training images (i.e., Caltech 101 images) are homogenous and belong to one selected category.\nOur algorithm is built upon these ideas, especially [15]. In particular, our feature learning algorithm can be viewed as a deep autoencoder with sparsity. To scale these autoencoders to large images, we use a simple idea known as local receptive fields [17, 23, 19, 15]. Specifically, each feature in the autoencoder can connect only to a small region of the lower layer. Fur-\nthermore, to achieve spatial invariances, we employ a technique known as pooling [17, 19].4\nOur deep autoencoder with pooling has three layers. The structure of one layer and its parameters are shown in Figure 1. As can be seen, this layer consists of two sublayers.\nNeurons in each sublayer only connect to a small area of the lower sublayer. In our experiments, the first sublayer has receptive fields of 18x18 and the second sublayer has receptive fields of 5x5 (also known as pooling size). The neurons in the first sublayer connect to pixels in all input channels (or maps) whereas the neurons in the second sublayer connect to pixels of only one channel (or map). The neurons on the second sublayer are also known as pooling neurons. The activation function of the first sublayer is square whereas the activation of the second sublayer is squareroot.\nThe overall network is constructed by replicating this architecture three times, resulting in 6 sublayers. This style of stacking a series of uniform structures is reminiscent of HMAX [25] and Neocognition [6] approaches.\nDuring parameter learning, the parameters of the second sublayers (H) are fixed. There are two sets of parameters associated with the first sublayer, encoding weights W1 and decoding weights W2. These two sets of weights are adjusted using the following optimization problem\nminimize W1,W2 m\u2211 i=1 (\u2225\u2225W2W T1 x(i) \u2212 x(i)\u2225\u222522 + k\u2211 j=1 \u221a +Hj(W1x(i))2 ) . (1)\nThis optimization problem is also known as reconstruction Topographic Independent Component Analysis [14].5 The first term in the objective requires the features to be able to encode important information about the data, i.e., they can reconstruct input data. The second term encourages pooling features to group similar features together to achieve invariances.\nUnlike previous convolutional methods [17, 19], local receptive fields in our model are untied: the weights are different for different locations in the image. In addition to being more biologically feasible, this lets us learn more invariant features other than translational invariances [15].\nSince the weights W1,W2 and H are local, we distribute them to different machines (model parallelism). We also employ data parallelism and distribute the computation over data examples to many machines. The algorithm is implemented on a cluster with fast network hardware. We used stochastic gradient descent with a minibatch of 100 and trained the network on a cluster with 1000 machines for one week.\n4There are many types of pooling such as mean pooling, max pooling [17] and square-squareroot pooling [15] but in this work we will use the last type of pooling because it allows the learning of invariant features [13, 15]. For learned invariances when the same algorithm is trained on natural images, see http://cs.stanford.edu/\u223cquocle/TCNNweb/index.html\n5In [14], the encoding weights and the decoding weights are tied: W1 = W2. However, for better parallelism and better features, we used the version without tied weights."}, {"heading": "4 Test set", "text": "The test set consists of 37000 images sampled from two datasets: Labeled Faces In the Wild [10] and ImageNet [4]. There are 13026 faces sampled from non-aligned Labeled Faces in The Wild.6 The rest are distractor objects randomly sampled from ImageNet. These images are resized to 80x80, padded by zeros and placed in the center area of the 200x200 image. The placement is carried out such that i) all images are aligned and have the same size and ii) the central pixel of the 80x80 image is at (100,80) in the larger image (see Figure 2). In later sections, we will assess how learned features change with respect to changes in position and scale of the object within the larger image.\n6http://vis-www.cs.umass.edu/lfw/lfw.tgz\nIn Figure 4, we show the areas of the test image faces with respect to the covering areas of the receptive fields on each layer."}, {"heading": "5 Experiment protocols", "text": "After training, we used this test set to measure the performance of each neuron in classifying faces against random distractors. For each neuron, we found its maximum activation value and minimum activation value. We then picked 20 equally spaced thresholds from maximum to minimum. For each neuron, we select the best classification accuracy among 20 thresholds."}, {"heading": "6 Classification results", "text": "The classification accuracy of the best neuron in a three layered network is 82.5% on a test set with 37000 images. There are 13026 faces in the test set, so guessing all negative only achieves 64.8%. The best neuron in a one-layered network only achieves 71% accuracy. Best linear filter, among 30000 filters sampled randomly from the training set, only achieves 74%.\nWe visualize histograms of activation values for face images and random images in Figure 5. It can be seen that, even with totally unlabeled data, the neuron learns to differentiate between faces and random distractors. Specifically, when we give a face as an input image, the neuron tends to fire with activation value larger than the threshold, 10. In contrast, if we give a random image as an input image, the neuron tends to fire with activation value less than 10."}, {"heading": "7 Visualization", "text": "In the following, we show the most responsive stimuli of the best neuron. Using reverse correlation and averaging, we also show the averaged image of the most responsive stimuli.7\n7This visualization technique is also used to visualize the receptive fields of simple cells (see [3] for an example)."}, {"heading": "8 Invariance properties", "text": "In the following, we choose a set of 10 face images and perform simple transformations to them, scaling and translating. Next we want to check how sensitive the neuron is with respect to the changes. In the following, we will plot the averaged response over ten images of the best neuron with respect to the changes in scale (Figure 7) and translation (Figure 8). The results show that the neuron is quite robust to those changes (especially scale)."}, {"heading": "9 Control experiment", "text": "In the previous section, we reported that our neuron achieves 82.5% accuracy in classify faces against random distractors. What if we remove all images that have faces?\nWe performed the control experiment by running a face detector in openCV and removing those training images that contain faces. The best neuron using the same architecture drops\nto 73% classification accuracy. This performance is as low as random filters sampled from training set as reported in section 6."}, {"heading": "10 Conclusion", "text": "In this work, we focus on simulating a simple version of a \u201cgrandmother cell\u201d feature using unlabeled data. We achieved this by combining ideas from recently developed algorithms. To train on large and realistic images, we implemented the algorithm using MapReduce with both styles of parallelism: data parallelism and model parallelism. The algorithm was trained on a cluster of 1000 machines. The results show that it is possible to achieve certain properties of these highly selective neurons with only unlabeled data.\nAcknowledgements: We thank Adam Coates, Tom Dean, Andrew Saxe, Jon Shlens for insightful discussions and help with the project."}], "references": [{"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "AISTATS 14", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems", "author": ["P. Dayan", "L.F. Abbott"], "venue": "MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Stimulus-selective properties of inferior temporal neurons in the macaque", "author": ["R. Desimone", "T. Albright", "C. Gross", "C. Bruce"], "venue": "The Journal of Neuroscience", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1984}, {"title": "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position", "author": ["K. Fukushima", "S. Miyake"], "venue": "Pattern Recognition, pages 455\u2013469", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1982}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q.V. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Layer-wise analysis of deep networks with gaussian kernels", "author": ["M.B. Gregoire Montavon", "K.-R. Muller"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical Report", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Receptive fields of single neurons in the the cat\u2019s visual cortex", "author": ["D.H. Hubel", "T. Wiesel"], "venue": "Journal of Physiology", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1959}, {"title": "Receptive fields", "author": ["D.H. Hubel", "T. Wiesel"], "venue": "binocular interaction and functional architecture in the cat\u2019s visual cortex. Journal of Physiology", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1962}, {"title": "Natural Image Statistics", "author": ["A. Hyv\u00e4rinen", "J. Hurri", "P.O. Hoyer"], "venue": "Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Tiled convolutional neural networks", "author": ["Q.V. Le", "J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "A.Y. Ng"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceeding of the IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A. Ng"], "venue": "International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B. Olshausen", "D. Field"], "venue": "Nature", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Invariant visual representation by single neurons in the human brain", "author": ["R. Quiroga", "L. Reddy", "G. Kreiman", "C. Koch", "I. Fried"], "venue": "Nature", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Self-taught learning: Transfer learning from unlabelled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A. Ng"], "venue": "International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["R. Raina", "A. Madhavan", "A.Y. Ng"], "venue": "International Conference on Machine Learning", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Y. Boureau", "Y. LeCun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nature Neuroscience", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 8, "context": ", RBMs [9], Autoencoders [1], Sparse Coding [18], Kmeans [2], ISA [16].", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": ", RBMs [9], Autoencoders [1], Sparse Coding [18], Kmeans [2], ISA [16].", "startOffset": 25, "endOffset": 28}, {"referenceID": 17, "context": ", RBMs [9], Autoencoders [1], Sparse Coding [18], Kmeans [2], ISA [16].", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": ", RBMs [9], Autoencoders [1], Sparse Coding [18], Kmeans [2], ISA [16].", "startOffset": 57, "endOffset": 60}, {"referenceID": 15, "context": ", RBMs [9], Autoencoders [1], Sparse Coding [18], Kmeans [2], ISA [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "\u201d This class of cells have been argued to exist in the brain with the role of detecting faces and hands [5] or specific people [21] (e.", "startOffset": 104, "endOffset": 107}, {"referenceID": 20, "context": "\u201d This class of cells have been argued to exist in the brain with the role of detecting faces and hands [5] or specific people [21] (e.", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "To scale the model to large images, we use the idea of local receptive fields [23, 15] and many levels of parallelism: model parallelism (parameters are distributed across machines) and data parallelism (data are distributed across machines).", "startOffset": 78, "endOffset": 86}, {"referenceID": 14, "context": "To scale the model to large images, we use the idea of local receptive fields [23, 15] and many levels of parallelism: model parallelism (parameters are distributed across machines) and data parallelism (data are distributed across machines).", "startOffset": 78, "endOffset": 86}, {"referenceID": 6, "context": "This further confirms the power of deep networks in learning invariant feature representations [7, 8].", "startOffset": 95, "endOffset": 101}, {"referenceID": 7, "context": "This further confirms the power of deep networks in learning invariant feature representations [7, 8].", "startOffset": 95, "endOffset": 101}, {"referenceID": 12, "context": "We then whiten each image by convolving it with fixed on-center off-surround kernels on each channel (similar to local contrast normalization) [13].", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "Hinton and cited as motivations in [22], is somewhat widely held in neuroscience.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "This is different from the work of [19] who trained their model on images from one class.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning [20, 9, 1, 24, 18].", "startOffset": 104, "endOffset": 122}, {"referenceID": 8, "context": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning [20, 9, 1, 24, 18].", "startOffset": 104, "endOffset": 122}, {"referenceID": 0, "context": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning [20, 9, 1, 24, 18].", "startOffset": 104, "endOffset": 122}, {"referenceID": 23, "context": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning [20, 9, 1, 24, 18].", "startOffset": 104, "endOffset": 122}, {"referenceID": 17, "context": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning [20, 9, 1, 24, 18].", "startOffset": 104, "endOffset": 122}, {"referenceID": 19, "context": "In particular, in their seminal work, Olshausen and Field [20] discovered sparse coding, an unsupervised learning principle, that yields receptive fields having properties like those of V1 simple cells.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "These properties in turn were characterized by [11, 12].", "startOffset": 47, "endOffset": 55}, {"referenceID": 11, "context": "These properties in turn were characterized by [11, 12].", "startOffset": 47, "endOffset": 55}, {"referenceID": 19, "context": "A shortcoming of early approaches such as sparse coding [20] is that their architectures are shallow and typically shown to capture simple variations (or learned features such as edge detectors).", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "This issue is tackled by recent work of [9, 1, 19] who extend sparse coding to build hierarchies of feature representations.", "startOffset": 40, "endOffset": 50}, {"referenceID": 0, "context": "This issue is tackled by recent work of [9, 1, 19] who extend sparse coding to build hierarchies of feature representations.", "startOffset": 40, "endOffset": 50}, {"referenceID": 18, "context": "This issue is tackled by recent work of [9, 1, 19] who extend sparse coding to build hierarchies of feature representations.", "startOffset": 40, "endOffset": 50}, {"referenceID": 18, "context": "In particular, Lee et al [19] have shown that a convolutional DBNs, trained on images of faces, can learn a decent face detector.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Our algorithm is built upon these ideas, especially [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "To scale these autoencoders to large images, we use a simple idea known as local receptive fields [17, 23, 19, 15].", "startOffset": 98, "endOffset": 114}, {"referenceID": 22, "context": "To scale these autoencoders to large images, we use a simple idea known as local receptive fields [17, 23, 19, 15].", "startOffset": 98, "endOffset": 114}, {"referenceID": 18, "context": "To scale these autoencoders to large images, we use a simple idea known as local receptive fields [17, 23, 19, 15].", "startOffset": 98, "endOffset": 114}, {"referenceID": 14, "context": "To scale these autoencoders to large images, we use a simple idea known as local receptive fields [17, 23, 19, 15].", "startOffset": 98, "endOffset": 114}, {"referenceID": 16, "context": "thermore, to achieve spatial invariances, we employ a technique known as pooling [17, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 18, "context": "thermore, to achieve spatial invariances, we employ a technique known as pooling [17, 19].", "startOffset": 81, "endOffset": 89}, {"referenceID": 24, "context": "This style of stacking a series of uniform structures is reminiscent of HMAX [25] and Neocognition [6] approaches.", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "This style of stacking a series of uniform structures is reminiscent of HMAX [25] and Neocognition [6] approaches.", "startOffset": 99, "endOffset": 102}, {"referenceID": 13, "context": "This optimization problem is also known as reconstruction Topographic Independent Component Analysis [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 16, "context": "Unlike previous convolutional methods [17, 19], local receptive fields in our model are untied: the weights are different for different locations in the image.", "startOffset": 38, "endOffset": 46}, {"referenceID": 18, "context": "Unlike previous convolutional methods [17, 19], local receptive fields in our model are untied: the weights are different for different locations in the image.", "startOffset": 38, "endOffset": 46}, {"referenceID": 14, "context": "In addition to being more biologically feasible, this lets us learn more invariant features other than translational invariances [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "There are many types of pooling such as mean pooling, max pooling [17] and square-squareroot pooling [15] but in this work we will use the last type of pooling because it allows the learning of invariant features [13, 15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "There are many types of pooling such as mean pooling, max pooling [17] and square-squareroot pooling [15] but in this work we will use the last type of pooling because it allows the learning of invariant features [13, 15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "There are many types of pooling such as mean pooling, max pooling [17] and square-squareroot pooling [15] but in this work we will use the last type of pooling because it allows the learning of invariant features [13, 15].", "startOffset": 213, "endOffset": 221}, {"referenceID": 14, "context": "There are many types of pooling such as mean pooling, max pooling [17] and square-squareroot pooling [15] but in this work we will use the last type of pooling because it allows the learning of invariant features [13, 15].", "startOffset": 213, "endOffset": 221}, {"referenceID": 13, "context": "html In [14], the encoding weights and the decoding weights are tied: W1 = W2.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "The test set consists of 37000 images sampled from two datasets: Labeled Faces In the Wild [10] and ImageNet [4].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "The test set consists of 37000 images sampled from two datasets: Labeled Faces In the Wild [10] and ImageNet [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "This visualization technique is also used to visualize the receptive fields of simple cells (see [3] for an example).", "startOffset": 97, "endOffset": 100}], "year": 2013, "abstractText": "We consider the problem of building detectors for high-level concepts using only unsupervised feature learning. For example, we would like to understand if it is possible to learn a face detector using only unlabeled images downloaded from the internet. To answer this question, we trained a simple feature learning algorithm on a large dataset of images (10 million images, each image is 200x200). The simulation is performed on a cluster of 1000 machines with fast network hardware for one week. Extensive experimental results reveal surprising evidence that such high-level concepts can indeed be learned using only unlabeled data and a simple learning algorithm.", "creator": "LaTeX with hyperref package"}}}