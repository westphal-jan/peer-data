{"id": "1201.2416", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2012", "title": "Stochastic Low-Rank Kernel Learning for Regression", "abstract": "we present a novel improved approach to learn a kernel - representation based regression function. if it normally is based also on the simplest useof generalized conical combinations of unique data - structure based parameterized kernels and on understanding a new stochastic convex analysis optimization scoring procedure of scaling which we establish computational convergence guarantees. the overall learning procedure clearly has the nice graphical properties that a ) thus the naive learned conical combination is automatically designed again to perform against the regression integration task at hand and b ) the updates prior implicated implemented by the optimization measurement procedure are quite rarely inexpensive. in here order to completely shed light on the new appositeness mechanisms of presenting our learning strategy, though we occasionally present empirical stability results from certain experiments conducted on our various benchmark datasets.", "histories": [["v1", "Wed, 11 Jan 2012 21:03:55 GMT  (306kb,D)", "http://arxiv.org/abs/1201.2416v1", "International Conference on Machine Learning (ICML'11), Bellevue (Washington) : United States (2011)"]], "COMMENTS": "International Conference on Machine Learning (ICML'11), Bellevue (Washington) : United States (2011)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pierre machart", "thomas peel", "sandrine anthoine", "liva ralaivola", "herv\u00e9 glotin"], "accepted": true, "id": "1201.2416"}, "pdf": {"name": "1201.2416.pdf", "metadata": {"source": "META", "title": "Stochastic Low-Rank Kernel Learning for Regression", "authors": ["Pierre Machart", "Thomas Peel", "Sandrine Anthoine", "Liva Ralaivola", "Herv\u00e9 Glotin"], "emails": ["PIERRE.MACHART@LIF.UNIV-MRS.FR", "THOMAS.PEEL@LIF.UNIV-MRS.FR", "ANTHOINE@CMI.UNIV-MRS.FR", "LIVA.RALAIVOLA@LIF.UNIV-MRS.FR", "GLOTIN@UNIV-TLN.FR"], "sections": [{"heading": "1. Introduction", "text": "Our goal is to learn a kernel-based regression function, tackling at once two problems that commonly arise with kernel methods: working with a kernel tailored to the task at hand and efficiently handling problems whose size prevents the Gram matrix from being stored in memory. Though the present work focuses on regression, the material presented here might as well apply to classification.\nCompared with similar methods, we introduce two novelties. Firstly, we build conical combinations of rank-1\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nNystro\u0308m approximations, whose weights are chosen so as to serve the regression task \u2013 this makes our approach different from (Kumar et al., 2009) and (Suykens et al., 2002), which focus on approximating the full Gram matrix with no concern for any specific learning task. Secondly, to solve the convex optimization problem entailed by our modeling choice, we provide an original stochastic optimization procedure based on (Nesterov, 2010). It has the following characteristics: i) the computations of the updates are inexpensive (thanks to the designing choice of using rank1 approximations) and ii) the convergence is guaranteed. To assess the practicality and effectiveness of our learning procedure, we conduct a few experiments on benchmark datasets, which allow us to draw positive conclusions on the relevance of our approach.\nThe paper is organized as follows. Section 2 introduces some notation and our learning setting; in particular the optimization problem we are interested in and the rank1 parametrization of the kernel our approach builds upon. Section 3 describes our new stochastic optimization procedure, establishes guarantees of convergence and details the computations to be implemented. Section 4 discusses the hyperparameters inherent to our modeling as well as the complexity of the proposed algorithm. Section 5 reports results from numerical simulations on benchmark datasets."}, {"heading": "2. Proposed Model", "text": "Notation X is the input space, k : X\u00d7X \u2192 R denotes the (positive) kernel function we have at hand and \u03c6 : X \u2192 H refers to the mapping \u03c6(x) := k(x, \u00b7) from X to the reproducing kernel Hilbert space H associated with k. Hence,\nar X\niv :1\n20 1.\n24 16\nv1 [\ncs .L\nG ]\nk(x,x\u2032)=\u3008\u03c6(x), \u03c6(x\u2032)\u3009, with \u3008\u00b7, \u00b7\u3009 the inner product ofH.\nThe training set is L := {(xi, yi)}ni=1 \u2208 (X \u00d7 R)n, where yi is the target value associated to xi. K = (k(xi,xj))1\u2264i,j\u2264n \u2208 Rn\u00d7n is the Gram matrix of k with respect to L. For m = 1, . . . , n, cm \u2208 Rn is defined as:\ncm := 1\u221a\nk(xm,xm) [k(x1,xm), . . . , k(xn,xm)]\n>."}, {"heading": "2.1. Data-parameterized Kernels", "text": "For m = 1, . . . , n, \u03c6\u0303m : X \u2192 H\u0303m is the mapping:\n\u03c6\u0303m(x) := \u3008\u03c6(x), \u03c6(xm)\u3009 k(xm,xm) \u03c6(xm). (1)\nIt directly follows that k\u0303m defined as, \u2200x,x\u2032 \u2208 X ,\nk\u0303m(x,x \u2032) := \u3008\u03c6\u0303m(x), \u03c6\u0303m(x\u2032)\u3009 =\nk(x,xm)k(x \u2032,xm)\nk(xm,xm) ,\nis indeed a positive kernel. Therefore, these parameterized kernels k\u0303m give rise to a family (K\u0303m)1\u2264m\u2264n of Gram matrices of the following form:\nK\u0303m = (k\u0303m(xi,xj))1\u2264i,j\u2264n = cmc T m, (2)\nwhich can be seen as rank-1 Nystro\u0308m approximations of the full Gram matrix K (Drineas & Mahoney, 2005; Williams & Seeger, 2001).\nAs studied in (Kumar et al., 2009), it is sensible to consider convex combinations of the K\u0303m if they are of very low rank. Building on this idea, we will investigate the use of a parameterized Gram matrix of the form:\nK\u0303(\u00b5) = \u2211 m\u2208S \u00b5mK\u0303m with \u00b5m \u2265 0, (3)\nwhere S is a set of indices corresponding to the specific rank-one approximations used. Note that since we consider conical combinations of the K\u0303m, which are all positive semi-definite, K\u0303(\u00b5) is positive semi-definite as well.\nUsing (1), one can show that the kernel k\u0303\u00b5, associated to our parametrized Gram matrix K\u0303(\u00b5), is such that:\nk\u0303\u00b5(x,x \u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009A = \u03c6(x)>A\u03c6(x), (4)\nwith A : = \u2211 m\u2208S \u00b5m \u03c6(xm)\u03c6(xm) > k(xm,xm) . (5)\nIn other words, our parametrization induces a modified metric in the feature space H associated to k. On a side note, remark that when S = {1 . . . , n} (i.e. all the columns are picked) and we have uniform weights \u00b5, then K\u0303(\u00b5) = KK>, which is a matrix encountered when working with the so-called empirical kernel map (Scho\u0308lkopf et al., 1999).\nFrom now on, M denotes the size of S and m0 refers to the number of non-zero components of \u00b5 (i.e. it is the 0- pseudo-norm of \u00b5)."}, {"heading": "2.2. Kernel Ridge Regression", "text": "Kernel Ridge regression (KRR) is the kernelized version of the popular ridge regression (Hoerl & Kennard, 1970) method. The associated optimization problem reads:\nmin w\n{ \u03bb\u2016w\u20162 +\nn\u2211 i=1\n(yi \u2212 \u3008w, \u03c6(xi)\u3009)2 } , (6)\nwhere \u03bb > 0 is a regularization parameter.\nUsing I for the identity matrix, the following dual formulation may be considered:\nmax \u03b1\u2208Rn\n{ FKRR(\u03b1) := y\nT\u03b1\u2212 1 4\u03bb \u03b1T (\u03bbI +K)\u03b1\n} . (7)\nThe solution\u03b1\u2217 of the concave problem (7) and the optimal solution w\u2217 of (6) are connected through the equality\nw\u2217 = 1\n2\u03bb n\u2211 i=1 \u03b1\u2217i \u03c6(xi),\nand \u03b1\u2217 can be found by setting the gradient of FKRR to zero, to give\n\u03b1\u2217 = 2(I + 1\u03bbK) \u22121y. (8)\nThe value of the objective function at \u03b1\u2217 is then:\nFKRR(\u03b1 \u2217) = yT (I + 1\u03bbK) \u22121y, (9)\nand the resulting regression function is given by:\nf(x) = 1\n2\u03bb n\u2211 i=1 \u03b1\u2217i k(xi,x). (10)"}, {"heading": "2.3. A Convex Optimization Problem", "text": "KRR may be solved by solving the linear system (I + K \u03bb )\u03b1 = 2y, at a cost of O(n\n3) operations. This might be prohibitive for large n, even more so if the matrix I+ K\u03bb does not fit into memory. To cope with this possible problem, we work with K\u0303(\u00b5) (3) instead of the Gram matrix K. As we shall see, this not only makes it possible to avoid memory issues but it also allows us to set up a learning problem where both \u00b5 and a regression function are sought for at once. This is very similar to the Multiple Kernel Learning paradigm (Rakotomamonjy et al., 2008) where one learns an optimal kernel along with the target function.\nTo set up the optimization problem we are interested in, we proceed in a way similar to (Rakotomamonjy et al., 2008). For m = 1, . . . , n, define the Hilbert space H\u0303\u2032m as:\nH\u0303\u2032m := { f \u2208 H\u0303m \u2223\u2223\u2223\u2223\u2016f\u2016H\u0303m\u00b5m <\u221e } . (11)\nOne can prove (Aronszajn, 1950) that H\u0303 = \u2295 H\u0303\u2032m is the\nRKHS associated to k\u0303 = \u2211 \u00b5mk\u0303m. Mimicking the reasoning of (Rakotomamonjy et al., 2008), our primal optimization problem reads:\nmin {fm},\u00b5 { \u03bb \u2211 m\u2208S 1 \u00b5m \u2016fm\u20162H\u0303\u2032m + n\u2211 i=1 (yi \u2212 \u2211 m\u2208S fm(xi)) 2 } ,\ns.t. \u2211 m\u2208S \u00b5m \u2264 n1 , \u00b5m \u2265 0, (12)\nwhere n1 is a parameter controlling the 1-norm of \u00b5. As this problem is also convex in \u00b5, using the earlier results on the KRR problem, (12) is equivalent to:\nmin \u00b5\u22650 { max \u03b1 yT\u03b1\u2212 1 4\u03bb \u03b1T (\u03bbI + K\u0303(\u00b5))\u03b1 } = min \u00b5\u22650 { yT (I + 1\u03bbK\u0303(\u00b5)) \u22121y } s.t. \u2211 m\u2208S \u00b5m \u2264 n1. (13)\nFinally, using the equivalence between Tikhonov and Ivanov regularization methods (Vasin, 1970), we obtain the convex and smooth optimization problem we focus on:\nmin \u00b5\u22650\n{ F (\u00b5) := yT (I+ 1\u03bbK\u0303(\u00b5)) \u22121y + \u03bd \u2211 m \u00b5m } . (14)\nThe regression function f\u0303 is derived using (1), a minimizer \u00b5\u2217 of the latter problem and the accompanying weight vector \u03b1\u2217 such that\n\u03b1\u2217 = 2 ( I + 1\u03bbK\u0303(\u00b5 \u2217) )\u22121 y, (15)\n(obtained adapting (8) to the case K = K(\u00b5\u2217)). We have:\nf\u0303(x) = 1\n2\u03bb n\u2211 i=1 \u03b1\u2217i k\u0303(xi,x) = 1 2\u03bb \u2211 m\u2208S \u00b5\u2217m n\u2211 i=1 \u03b1\u2217i k\u0303m(xi,x)\n= 1\n2\u03bb \u2211 m\u2208S \u03b1\u0303\u2217mk(xm,x), (16)\nwhere \u03b1\u0303\u2217m := \u00b5 \u2217 m\nc>m\u03b1 \u2217\u221a\nk(xm,xm) . (17)"}, {"heading": "3. Solving the problem", "text": "We now introduce a new stochastic optimization procedure to solve (14). It implements a coordinate descent strategy with step sizes that use second-order information."}, {"heading": "3.1. A Second-Order Stochastic Coordinate Descent", "text": "Problem (14) is a constrained minimization based on the differentiable and convex objective function F . Usual convex optimization methods (such as projected gradient descent, proximal methods) may be employed to solve this\nproblem, but they may be too computationally expensive if n is very large, which is essentially due to a suboptimal exploitation of the parametrization of the problem. Instead, the optimization strategy we propose is specifically tailored to take advantage of the parametrization of K\u0303(\u00b5).\nAlgorithm 1 depicts our stochastic descent method, inspired by (Nesterov, 2010). At each iteration, a randomly chosen coordinate of \u00b5 is updated via a Newton step. This method has two essential features: i) using coordinate-wise updates of \u00b5 involves only partial derivatives which can be easily computed and ii) the stochastic approach ensures a reduced memory cost while still guaranteeing convergence.\nAlgorithm 1 Stochastic Coordinate Newton Descent Input: \u00b50 random. repeat\nChoose coordinate mk uniformly at random in S. Update : \u00b5k+1m = \u00b5 k m if m 6= mk and\n\u00b5k+1mk =argmin v\u22650 \u2202F (\u00b5k) \u2202\u00b5mk (v\u2212\u00b5kmk)+ 1 2 \u22022F (\u00b5k) \u2202\u00b52mk (v\u2212\u00b5kmk) 2,\n(18) until F (\u00b5k)\u2212 F (\u00b5k\u2212M ) < F (\u00b5k\u2212M )\nNotice that the Stochastic Coordinate Newton Descent (SCND) is similar to the algorithm proposed in (Nesterov, 2010), except that we replace the Lipschitz constants by the second-order partial derivatives \u2202\n2F (\u00b5k) \u2202\u00b52mk\n. Thus, we replace a constant step-size gradient descent by a the Newton-step in (18), which allows us to make larger steps.\nWe show that for the function F in (14), SCND does provably converge to a minimizer of Problem (14). First, we rewrite (18) as a Newton step and compute the partial derivatives: Proposition 1. Eq. (18) is equivalent to\n\u00b5k+1mk =\n{ ( \u00b5kmk \u2212 \u2202F (\u00b5k) \u2202\u00b5mk /\u2202 2F (\u00b5k) \u2202\u00b52mk ) + if \u2202 2F (\u00b5k) \u2202\u00b52mk\n6=0 0 otherwise.\n(19)\nProof. (19) gives the optimality conditions for (18).\nProposition 2. The partial derivatives \u2202 pF (\u00b5) \u2202\u00b5pm are:\n\u2202F (\u00b5) \u2202\u00b5m = \u2212\u03bb(y>K\u0303\u22121\u03bb,\u00b5cm) 2 + \u03bd, (20)\n\u2202pF (\u00b5) \u2202\u00b5pm = (\u22121)pp!\u03bb(y>K\u0303\u22121\u03bb,\u00b5cm) 2(c>mK\u0303 \u22121 \u03bb,\u00b5cm) p\u22121,\nwith p \u2265 2 and K\u0303\u22121\u03bb,\u00b5 := (\u03bbI + K\u0303(\u00b5)) \u22121. (21)\nProof. Easy but tedious calculations give the results.\nTheorem 1 (Convergence). For any sequence {mk}k, the sequence {F (\u00b5k)}k verifies:\n(a) \u2200k, F (\u00b5k+1) \u2264 F (\u00b5k). (b) limk\u2192\u221e F (\u00b5k) = min\u00b5\u22650 F (\u00b5).\nMoreover, if there exists a minimizer \u00b5\u2217 of F such that the Hessian\u22072F (\u00b5\u2217) is positive definite then:\n(c) \u00b5\u2217 is the unique minimizer of F . The sequence {\u00b5k} converges to \u00b5\u2217: ||\u00b5k\u2212\u00b5\u2217||\u21920.\nSketch of proof. (a) Using that \u2202 3F (\u00b5) \u2202\u00b53m\n\u2264 0 (see (20)), one shows that the Taylor series truncated to the second order: v \u2192 F (\u00b5) + \u2202F (\u00b5)\u2202\u00b5m (vm\u2212\u00b5m) + 1 2 \u22022F (\u00b5) \u2202\u00b52m\n(vm\u2212 \u00b5m)\n2, is a quadratic upper-bound of F that matches F and\u2207F at point \u00b5 (for any fixed m and \u00b5). From this, the update formula (18) yields F (\u00b5k+1) \u2264 F (\u00b5k).\n(b) First note that ||\u00b5k|| \u2264 F (\u00b50) and extract a converging subsequence {\u00b5\u03c6(k)}. Denote the limit by \u00b5\u0302. Separating the cases where \u2202\n2F (\u00b5\u0302) \u2202\u00b52m\nis zero or not, one shows that \u00b5\u0302 satisfies the optimality conditions: \u3008\u2207F (\u00b5\u0302),v \u2212 \u00b5\u0302\u3009 \u2265 0, \u2200v \u2265 0. Thus \u00b5\u0302 is a minimizer of F and we have limF (\u00b5k) = limF (\u00b5\u03c6(k)) = F (\u00b5\u0302) = min\u00b5\u22650 F (\u00b5).\n(c) is standard in convex optimization."}, {"heading": "3.2. Iterative Updates", "text": "One may notice that the computations of the derivatives (20), as well as the computation of \u03b1\u2217, depend on K\u0303\u22121\u03bb,\u00b5. Moreover, the dependency in \u00b5, for all those quantities, only lies in K\u0303\u22121\u03bb,\u00b5. Thus, a special care need be taken on how K\u0303\u22121\u03bb,\u00b5 is stored and updated throughout.\nLet S+\u00b5 = {m \u2208 S|\u00b5m > 0} and m0 = \u2016\u00b5\u20160 = |S+\u00b5 |. Let C = [ci1 \u00b7 \u00b7 \u00b7 cim0 ] be the concatenation of the cij \u2019s, for ij \u2208 S+\u00b5 andD the diagonal matrix with diagonal elements \u00b5ij , for ij \u2208 S+\u00b5 . Remark that throughout the iterations the sizes of C and D may vary. Given (21) and using Woodbury formula (Theorem 2, Appendix), we have:\nK\u0303\u22121\u03bb,\u00b5 = ( \u03bbI + CDC> )\u22121 = 1\n\u03bb I \u2212 1 \u03bb2 CGC> (22)\nwith G := ( D\u22121 + 1\n\u03bb C>C\n)\u22121 . (23)\nNote that G is a square matrix of order m0 and that an update on \u00b5 will require an update on G. Even though updating G\u22121, i.e. D\u22121 + 1\u03bbC\n>C, is trivial, it is more efficient to directly store and update G. This is what we describe now.\nAt each iteration, only one coordinate of \u00b5 is updated. Let p be the index of the updated coordinate, \u00b5old, Cold, Dold\nand Gold, the vectors and matrices before the update and \u00b5new, Cnew, Dnew and Gnew the updated matrices/vectors. Let also ep bethe vector whose pth coordinate is 1 while other coordinates are 0. We encounter four different cases.\nCase 1: \u00b5oldp = 0 and \u00b5newp = 0. No update needed:\nGnew = Gold. (24)\nCase 2: \u00b5oldp 6= 0 and \u00b5newp 6= 0. Here, Cold = Cnew and\nD\u22121new = D \u22121 old + \u2206pepe > p , where \u2206p :=\n1 \u00b5newp \u2212 1 \u00b5oldp .\nThen, using Woodbury formula, we have: Gnew = ( G\u22121old + \u2206pepe > p )\u22121 = Gold\u2212\n\u2206p 1 + \u2206pgpp gpg > p ,\n(25) with gpp the (p, p)th entry of Gold and gp its pth column.\nCase 3: \u00b5oldp 6= 0 and \u00b5newp = 0. Here, S+\u00b5new = S+\u00b5old \\ {p}. It follows that we have to remove cp from Cold to have Cnew. To get Gnew, we may consider the previous update formula when \u00b5newp \u2192 0 (that is, when \u2206p \u2192 +\u221e). Note that we can use the previous formula because \u00b5p 7\u2192 K\u0303\u22121\u03bb,\u00b5 is well-defined and continuous at 0. Thus, as lim\u00b5newp \u21920 \u2206p 1+\u2206pgpp = 1gpp , we have:\nGnew = ( Gold \u2212 1\ngpp gpg\n> p ) \\{p} , (26)\nwhere A\\{p} denotes the matrix A from which the pth column and pth row have been removed.\nCase 4: \u00b5oldp = 0 and \u00b5newp 6= 0. We have Cnew = [Cold cp ] . Using (23), it follows that\nGnew =\n( D\u22121old + 1 \u03bbC > oldCold\n1 \u03bbC > oldcp\n1 \u03bbc > p Cold\n1 \u00b5newp + 1\u03bbc > p cp\n)\u22121\n=\n( G\u22121old\n1 \u03bbC > oldcp\n1 \u03bbc > p Cold 1 \u00b5newp + 1\u03bbc > p cp )\u22121 = ( A v v> s ) ,\nwhere, using the block-matrix inversion formula of Theorem 3 (Appendix), we have:\ns =\n( 1\n\u00b5newp +\n1 \u03bb c>p cp \u2212 1 \u03bb2 c>p ColdGoldC > oldcp )\u22121 v = \u2212 s\n\u03bb GoldC\n> oldcp (27)\nA = Gold + 1\ns vv>.\nAlgorithm 2 SLKL: Stochastic Low-Rank Kernel Learning inputs: L := {(xi, yi)}ni=1, \u03bd > 0, M > 0, > 0. outputs: \u00b5, G and C (yield (\u03bbI +K(\u00b5))\u22121 from (22)).\ninitialization: \u00b5(0) = 0. repeat\nChoose coordinate mk uniformly at random in S . Update \u00b5(k) according to (19), by changing only the mk-th coordinate \u00b5kmk of \u00b5 (k):\n\u2022 compute the second order derivative\nh = \u03bb(y>K\u0303\u22121\u03bb,\u00b5cmk) 2(c>mkK\u0303 \u22121 \u03bb,\u00b5cmk) ;\n\u2022 if h > 0 then\n\u00b5(k+1)mk = max ( 0, \u00b5(k)mk + \u03bb(y>K\u0303\u22121\u03bb,\u00b5cmk ) 2 \u2212 \u03bd\nh\n) ;\nelse \u00b5(k+1)mk = 0.\nUpdate G(k) and C(k) according to (24)-(27). until F (\u00b5k)\u2212 F (\u00b5k\u2212M ) < F (\u00b5k\u2212M )\nComplete learning algorithm. Algorithm 2 depicts the full Stochastic Low-Rank Kernel Learning algorithm (SLKL), which recollects all the pieces just described."}, {"heading": "4. Analysis", "text": "Here, we discuss the relation between \u03bb and \u03bd and we argue that there is no need to keep both hyperparameters. In addition, we provide a short analysis on the runtime complexity of our learning procedure."}, {"heading": "4.1. Pivotal Hyperparameter \u03bb\u03bd", "text": "First recall that we are interested in the minimizer \u00b5\u2217\u03bb,\u03bd of constrained optimization problem (14), i.e.:\n\u00b5\u2217\u03bb,\u03bd = argmin \u00b5\u22650 F\u03bb,\u03bd(\u00b5), (28)\nwhere, for the sake of clarity, we purposely show the dependence on \u03bb and \u03bd of the objective function F\u03bb,\u03bd\nF\u03bb,\u03bd(\u00b5) = y > ( I + K\u0303 ( \u00b5 \u03bb ))\u22121 y + \u03bb\u03bd \u2211 m \u00b5m \u03bb , (29) We may name \u03b1\u2217\u03bb,\u03bd , \u03b1\u0303 \u2217 \u03bb,\u03bd the weight vectors associated with \u00b5\u2217\u03bb,\u03bd (see (15) and (17)). We have the following: Proposition 3. Let \u03bb, \u03bd, \u03bb\u2032, \u03bd\u2032 be strictly positive real numbers. If \u03bb\u03bd = \u03bb\u2032\u03bd\u2032 then\n\u00b5\u2217\u03bb\u2032,\u03bd\u2032 = \u03bb\u2032 \u03bb \u00b5 \u2217 \u03bb,\u03bd , and f\u0303\u03bb,\u03bd = f\u0303\u03bb\u2032,\u03bd\u2032 ."}, {"heading": "As a direct consequence:", "text": "\u2200\u03bb, \u03bd \u2265 0, f\u0303\u03bb,\u03bd = f\u03031,\u03bb\u03bd .\nProof. Suppose that we know \u00b5\u2217\u03bb,\u03bd . Given the definition (29) of F\u03bb,\u03bd and using \u03bb\u03bd = \u03bb\u2032\u03bd\u2032, we have\nF\u03bb,\u03bd(\u00b5) = F\u03bb\u2032,\u03bd\u2032 ( \u03bb\u2032 \u03bb \u00b5 )\nSince the only constraint of problem (28) is the nonnegativity of the components of \u00b5, it directly follows that \u03bb\u2032\u00b5\u2217\u03bb,\u03bd/\u03bb is a minimizer of F\u03bb\u2032,\u03bd\u2032 (under these constraints), hence \u00b5\u2217\u03bb\u2032,\u03bd\u2032 = \u03bb \u2032\u00b5\u2217\u03bb,\u03bd/\u03bb.\nTo show f\u0303\u03bb,\u03bd = f\u0303\u03bb\u2032,\u03bd\u2032 , it suffices to observe that, according to the way \u03b1\u2217\u03bb,\u03bd is defined (cf. (15)),\n\u03b1\u2217\u03bb\u2032,\u03bd\u2032 = 2 ( I +K ( \u00b5\u2217 \u03bb\u2032,\u03bd\u2032 \u03bb\u2032 ))\u22121 y\n= 2 ( I +K ( \u03bb\u2032\n\u03bb \u00b5\u2217\u03bb,\u03bd \u03bb\u2032 ))\u22121 y = \u03b1\u2217\u03bb,\u03bd ,\nand, thus, \u03b1\u0303\u2217\u03bb\u2032,\u03bd\u2032 = \u03bb \u2032\u03b1\u0303\u2217\u03bb,\u03bd/\u03bb. The definition (16) of f\u0303\u03bb,\u03bd then gives f\u0303\u03bb,\u03bd = f\u0303\u03bb\u2032,\u03bd\u2032 , which entails f\u0303\u03bb,\u03bd = f\u03031,\u03bb\u03bd .\nThis proposition has two nice consequences. First, it says that the pivotal hyperparameter is actually the product \u03bb\u03bd: this is the quantity that parametrizes the learning problem (not \u03bb or \u03bd, seen independently). Thus, the set of regression functions, defined by the \u03bb and \u03bd hyperparameter space, can be described by exploring the set of vectors (\u00b5\u22171,\u03bd)\u03bd>0, which only depends on a single parameter. Second, considering (\u00b5\u22171,\u03bd)\u03bd>0 allows us to work with the family of objective functions (F1,\u03bd)\u03bd>0, which are well-conditioned numerically as the hyperparameter \u03bb is set to 1."}, {"heading": "4.2. Runtime Complexity and Memory Usage", "text": "For the present analysis, let us assume that we pre-compute the M (randomly) selected columns c1, . . . , cM . If a is the cost of computing a column cm, the pre-computation has a cost of O(Ma) and has a memory usage of O(nM).\nAt each iteration, we have to compute the first and secondorder derivatives of the objective function, as well as its value and the weight vector \u03b1. Using (22), (20), (14) and (15), one can show that those operations have a complexity of O(nm0) if m0 is the zero-norm of \u00b5.\nBesides, in addition to C, we need to store G for a memory cost of O(m20). Overall, if we denote the number of iterations by k, the algorithm has a memory cost of O(nM +m20) and a complexity of O(knm0 +Ma).\nIf memory is a critical issue, one may prefer to compute the columns cm on-the-fly and m0 columns need to be stored instead of M (this might be a substantial saving in terms of memory as can be seen in the next section). This improvement in term of memory usage implies an additive cost in the runtime complexity. In the worst case, we have\nto compute a new column c at each iteration. The resulting memory requirement scales as O(nm0 + m20) and the runtime complexity varies as O(k(nm0 + a))."}, {"heading": "5. Numerical Simulations", "text": "We now present results from various numerical experiments, for which we describe the datasets and the protocol used. We study the influence of the different parameters of our learning approach on the results and compare the performance of our algorithm to that of related methods."}, {"heading": "5.1. Setup", "text": "First, we use a toy dataset (denoted by sinc) to better understand the role and influence of the parameters. It consists in regressing the cardinal sine of the two-norm (i.e. x 7\u2192 sin(\u2016x\u2016)/\u2016x\u2016) of random two-dimensional points, each drawn uniformly between\u22125 and +5. In order to have a better idea on how the solutions may or may not over-fit the training data, we add some white Gaussian noise on the target variable of the randomly picked 1000 training points (with a 10 dB signal-to-noise ratio). The test set is made of 1000 non-noisy independent instance/target pairs.\nWe then assess our method on two UCI datasets: Abalone (abalone) and Boston Housing (boston), using the same normalizations, Gaussian kernel parameters (\u03c3 denotes the kernel width) and data partition as in (Smola & Scho\u0308lkopf, 2000). The United States Postal Service (USPS) dataset is used with the same setting as in (Williams & Seeger, 2001). Finally, the Modified National Institute of Standards and Technology (MNIST) dataset is used with the same preprocessing as in (Maji & Malik, 2009). Table 1 summarizes the characteristics of all the datasets we used.\nAs displayed in Algorithm 1, at each iteration k > M , we check if F (\u00b5k)\u2212F (\u00b5k\u2212M ) < F (\u00b5k\u2212M ) holds. If so, we stop the optimization process. thus controls our stopping criterion. In the experiments, we set = 10\u22124 unless otherwise stated and we set \u03bb to 1 for all the experiments and we run simulations for various values of \u03bd and M . In order to assess the variability incurred by the stochastic nature of our learning algorithm, we run each experiment 20 times."}, {"heading": "5.2. Influence of the parameters", "text": ""}, {"heading": "5.2.1. EVOLUTION OF THE OBJECTIVE", "text": "We have established (Section 3) the convergence of our optimization procedure, under mild conditions. A question that we have not tackled yet is to evaluate its convergence rate. Figure 1 plots the evolution of the objective function on the sinc dataset. We observe that the evolutions of the objective function are impressively similar among the different runs. This empirically tends to assert that it is relevant to look for theoretical results on the convergence rate.\nA question left for future work is the impact of the random selection of the set of columns S on the reached solution.\n5.2.2. ZERO-NORM OF \u00b5\nAs shown in Section 4.2, both memory usage and the complexity of the algorithm depend on m0. Thus, it is interesting to take a closer look at how this quantity evolves. Figure 2 and 3 experimentally point out two things. On the one hand, the number of active componentsm0 = \u2016\u00b5\u20160 remains significantly smaller thanM . In other words, as long as the regularization parameter is well-chosen, we never have to store all of the cm at the same time. On the other hand, the solution \u00b5\u2217 is sparse and \u2016\u00b5\u2217\u20160 grows with M and diminishes with \u03bd. A theoretical study on the dependence of\u00b5\u2217 andm0 inM and \u03bd, left for future work, would\nbe all the more interesting since sparsity is the cornerstone on which the scalability of our algorithm depends."}, {"heading": "5.3. Comparison to other methods", "text": "This section aims at giving a hint on how our method performs on regression tasks. To do so, we compare the Mean Square Error (over the test set). In addition to our Stochastic Low-Rank Kernel Learning method (SLKL), we solve the problem with the standard Kernel Ridge Regression method, using the n training data (KRRn) and using onlyM training data (KRRM). We also evaluate the performance of the KRR method, using the kernel obtained with uniform weights on the M rank-1 approximations selected for SLKL (Unif ). The results are displayed in Table 2, where the bold font indicates the best low-rank method (KRRM, Unif or SLKL) for each experiment.\nTable 2 confirms that optimizing the weight vector \u00b5 is decisive as our results dramatically outperform those of Unif. As long as M < n, our method also outperforms KRRM. The explanation probably lies in the fact that our approximations keep information about similarities between the M selected points and the n \u2212 M others. Furthermore, our method SLKL achieves comparable performances (or even better on abalone) than KRRn, while finding sparse solutions. Compared to the approach from (Smola & Scho\u0308lkopf, 2000), we seem to achieve lower test error on the boston dataset even for M = 128. On the abalone dataset, this method outperforms ours for every M we tried.\nFinally, we also compare the results we obtain on the USPS dataset with the ones obtained in (Williams & Seeger, 2001) (Nyst). As it consists in a classification task, we actually perform a regression on the labels to adapt our method, which is known to be equivalent to solving Fisher Discriminant Analysis (Duda & Hart, 1973). The performance achieved by Nyst outperforms ours. However, one may argue that the performance have a same order of magnitude and note that the Nyst approach focuses on the classification task, while ours was designed for regression."}, {"heading": "5.4. Large-scale dataset", "text": "To assess the scalability of our method, we ran experiments on the larger handwritten digits MNIST dataset, whose training set is made of 60000 examples. We used a Gaussian kernel computed over histograms of oriented gradients as in (Maji & Malik, 2009), in a \u201cone versus all\u201d setting. ForM=1000, we obtained classification error rates around 2% over the test set, which do not compete with state-ofthe-art results but achieve reasonable performance, considering that we use only a small part of the data (cf. the size of M ) and that our method was designed for regression.\nAlthough our method overcomes memory usage issues for such large-scale problems, it still is computationally intensive. In fact, a large number of iterations is spent picking coordinates whose associated weight remains at 0. Though those iterations do not induce any update, they do require computing the associated Gram matrix column (which is not stored as it does not weigh in the conic combination) as well as the derivatives of the objective function. The main focus of our future work is to avoid those computations, using e.g. techniques such as shrinkage (Hsieh et al., 2008)."}, {"heading": "6. Conclusion", "text": "We have presented an original kernel-based learning procedure for regression. The main features of our contribution are the use of a conical combination of data-based kernels and the derivation of a stochastic convex optimization procedure, that acts coordinate-wise and makes use of second-order information. We provide theoretical convergence guarantees for this optimization procedure, we depict the behavior of our learning procedure and illustrate its effectiveness through a number of numerical experiments carried out on several benchmark datasets.\nThe present work naturally raises several questions. Among them, we may pinpoint that of being able to establish precise rate of convergence for the stochastic optimization procedure and that of generalizing our approach to the use of several kernels. Establishing data-dependent generalization bounds taking advantage of either the one-norm constraint on \u00b5 or the size M of the kernel combination is of primary importance to us. The connection established between the one-norm hyperparameter \u03bd and the ridge pa-\nrameter \u03bb, in section 4, seems interesting and may be witnessed in (Rakotomamonjy et al., 2008). Although not been mentioned so far, there might be connections between our modeling strategy and boosting/leveraging-based optimization procedures. Finally, we plan on generalizing our approach to other kernel methods, noting that rank-1 update formulas as those proposed here can possibly be exhibited even for problems with no closed-form solution."}, {"heading": "Acknowledgments", "text": "This work is partially supported by the IST Program of the European Community, under the FP7 Pascal 2 Network of Excellence (ICT-216886-NOE) and by the ANR project LAMPADA (ANR-09-EMER-007)."}, {"heading": "A. Matrix Inversion Formulas", "text": "Theorem 2. (Woodbury matrix inversion formula (Woodbury, 1950)) Let n and m be positive integers, A \u2208 Rn\u00d7n and C \u2208 Rm\u00d7m be non-singular matrices and let U \u2208 Rn\u00d7m and V \u2208 Rm\u00d7n be two matrices. If C\u22121+V A\u22121U is non-singular then so is A+UCV and:\n(A+UCV )\u22121 = A\u22121\u2212A\u22121U(C\u22121+V A\u22121U)\u22121V A\u22121.\nTheorem 3. (Matrix inversion with added column) Given m, integer and M \u2208 R(n+1)\u00d7(n+1) partitioned as:\nM =\n( A b\nb> c\n) , where A \u2208 Rn\u00d7n, b \u2208 Rn and c \u2208 R."}, {"heading": "If A is non-singular and c\u2212 b>A\u22121b 6= 0, then M is nonsingular and the inverse of M is given by", "text": "M\u22121 =\n( A\u22121 + 1kA \u22121bb>A\u22121 \u2212 1kA \u22121b\n\u2212 1kb >A\u22121 1k\n) , (30)\nwhere k = c\u2212 b>A\u22121b."}], "references": [{"title": "On the nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney", "year": 2005}, {"title": "Pattern Classification and Scene Analysis", "author": ["Duda", "Richard O", "Hart", "Peter E"], "venue": null, "citeRegEx": "Duda et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Duda et al\\.", "year": 1973}, {"title": "A dual coordinate descent method for largescale linear svm", "author": ["Hsieh", "C.-J", "Chang", "K.-W", "Lin", "Keerthi", "S. Sathiya", "S. Sundararajan"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Ensemble nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Fast and accurate digit classification", "author": ["S. Maji", "J. Malik"], "venue": "Technical report, EECS Department, UC Berkeley,", "citeRegEx": "Maji and Malik,? \\Q2009\\E", "shortCiteRegEx": "Maji and Malik", "year": 2009}, {"title": "Efficiency of coordinate descent methods on hugescale optimization problems", "author": ["Y. Nesterov"], "venue": "Core discussion papers,", "citeRegEx": "Nesterov,? \\Q2010\\E", "shortCiteRegEx": "Nesterov", "year": 2010}, {"title": "Input space versus feature space in kernel-based methods", "author": ["B. Sch\u00f6lkopf", "S. Mika", "C.J.C. Burges", "P. Knirsch", "K.R. M\u00fcller", "G. R\u00e4tsch", "A.J. Smola"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Sparse greedy matrix approximation for machine learning", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Smola and Sch\u00f6lkopf,? \\Q2000\\E", "shortCiteRegEx": "Smola and Sch\u00f6lkopf", "year": 2000}, {"title": "Relationship of several variational methods for the approximate solution of ill-posed problems", "author": ["V.V. Vasin"], "venue": "Mathematical Notes,", "citeRegEx": "Vasin,? \\Q1970\\E", "shortCiteRegEx": "Vasin", "year": 1970}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Seeger,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "Inverting modified matrices", "author": ["M.A. Woodbury"], "venue": "Technical report,", "citeRegEx": "Woodbury,? \\Q1950\\E", "shortCiteRegEx": "Woodbury", "year": 1950}], "referenceMentions": [{"referenceID": 3, "context": "Nystr\u00f6m approximations, whose weights are chosen so as to serve the regression task \u2013 this makes our approach different from (Kumar et al., 2009) and (Suykens et al.", "startOffset": 125, "endOffset": 145}, {"referenceID": 5, "context": "Secondly, to solve the convex optimization problem entailed by our modeling choice, we provide an original stochastic optimization procedure based on (Nesterov, 2010).", "startOffset": 150, "endOffset": 166}, {"referenceID": 3, "context": "As studied in (Kumar et al., 2009), it is sensible to consider convex combinations of the K\u0303m if they are of very low rank.", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "all the columns are picked) and we have uniform weights \u03bc, then K\u0303(\u03bc) = KK>, which is a matrix encountered when working with the so-called empirical kernel map (Sch\u00f6lkopf et al., 1999).", "startOffset": 160, "endOffset": 184}, {"referenceID": 8, "context": "Finally, using the equivalence between Tikhonov and Ivanov regularization methods (Vasin, 1970), we obtain the convex and smooth optimization problem we focus on:", "startOffset": 82, "endOffset": 95}, {"referenceID": 5, "context": "Algorithm 1 depicts our stochastic descent method, inspired by (Nesterov, 2010).", "startOffset": 63, "endOffset": 79}, {"referenceID": 5, "context": "Notice that the Stochastic Coordinate Newton Descent (SCND) is similar to the algorithm proposed in (Nesterov, 2010), except that we replace the Lipschitz constants by the second-order partial derivatives \u2202 F (\u03bc) \u2202\u03bc2mk .", "startOffset": 100, "endOffset": 116}, {"referenceID": 2, "context": "techniques such as shrinkage (Hsieh et al., 2008).", "startOffset": 29, "endOffset": 49}, {"referenceID": 10, "context": "(Woodbury matrix inversion formula (Woodbury, 1950)) Let n and m be positive integers, A \u2208 Rn\u00d7n and C \u2208 Rm\u00d7m be non-singular matrices and let U \u2208 Rn\u00d7m and V \u2208 Rm\u00d7n be two matrices.", "startOffset": 35, "endOffset": 51}], "year": 2012, "abstractText": "We present a novel approach to learn a kernelbased regression function. It is based on the use of conical combinations of data-based parameterized kernels and on a new stochastic convex optimization procedure of which we establish convergence guarantees. The overall learning procedure has the nice properties that a) the learned conical combination is automatically designed to perform the regression task at hand and b) the updates implicated by the optimization procedure are quite inexpensive. In order to shed light on the appositeness of our learning strategy, we present empirical results from experiments conducted on various benchmark datasets.", "creator": "LaTeX with hyperref package"}}}