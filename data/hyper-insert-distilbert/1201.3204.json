{"id": "1201.3204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2012", "title": "Evaluation of a Simple, Scalable, Parallel Best-First Search Strategy", "abstract": "utilizing large - capacity scale, nearly parallel clusters uniquely composed collectively of commodity concurrent processors algorithms are increasingly available, enabling the use of vast integrated processing capabilities and integrated distributed application ram processing to solve hard component search capability problems. likewise we investigate hash - matching distributed databases a * ( notably hda * ), a computational simple modeling approach fundamental to parallel best - tailed first search that fundamentally asynchronously distributes query and schedules work among futures processors based on a hash aggregation function of attaining the search node state. we use this approach systematically to parallelize solving the array a * constraint algorithm in an inherently optimal sequential processor version of the fast downward planner, as to well subsequently as to a 24 - bar puzzle stack solver. the scaling behavior of hda * is evaluated whether experimentally on being a shared memory, continuously multicore threaded machine with 8 cores, a cluster of commodity machines systematically us - ing each up 420 to 64 cores, simultaneously and conversely a large - scale sequential high - performing performance cluster alignment using arrays up to 1024 processors. indeed we show observations that running this approach adequately scales well, allowing the effective utilization of large amount of carefully distributed online memory to optimally not solve problems which therefore require more than a short terabyte of ram. we also compare hda * to transposition - table driven convergence scheduling ( tds ), implying a distributed hash - map based mutual parallelization of ida *, xml and lastly show instances that, in planning, hda * theoretically significantly outperforms tds. a simple backward hybrid proposal which significantly combines sophisticated hda * hardware and tds to exploit both characteristics of their main strengths is soon proposed.", "histories": [["v1", "Mon, 16 Jan 2012 10:31:47 GMT  (262kb,S)", "https://arxiv.org/abs/1201.3204v1", null], ["v2", "Thu, 25 Oct 2012 03:39:16 GMT  (119kb)", "http://arxiv.org/abs/1201.3204v2", "in press, to appear in Artificial Intelligence"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["akihiro kishimoto", "alex fukunaga", "adi botea"], "accepted": false, "id": "1201.3204"}, "pdf": {"name": "1201.3204.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Akihiro Kishimoto", "Alex Fukunaga", "Adi Botea"], "emails": ["kishimoto@is.titech.ac.jp", "fukunaga@idea.c.u-tokyo.ac.jp", "adibotea@ie.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 1.\n32 04\nv2 [\ncs .A\nI] 2\n5 O\nLarge-scale, parallel clusters composed of commodity processors are increasingly available, enabling the use of vast processing capabilities and distributed RAM to solve hard search problems. We investigate Hash-Distributed A* (HDA*), a simple approach to parallel best-first search that asynchronously distributes and schedules work among processors based on a hash function of the search state. We use this approach to parallelize the A* algorithm in an optimal sequential version of the Fast Downward planner, as well as a 24-puzzle solver. The scaling behavior of HDA* is evaluated experimentally on a shared memory, multicore machine with 8 cores, a cluster of commodity machines using up to 64 cores, and large-scale high-performance clusters, using up to 2400 processors. We show that this approach scales well, allowing the effective utilization of large amounts of distributed memory to optimally solve problems which require terabytes of RAM. We also compare HDA* to Transposition-table Driven Scheduling (TDS), a hash-based parallelization of IDA*, and show that, in planning, HDA* significantly outperforms TDS. A simple hybrid which combines HDA* and TDS to exploit strengths of both algorithms is proposed and evaluated.\nKeywords: planning; A* search; parallel search\nEmail addresses: kishimoto@is.titech.ac.jp (Akihiro Kishimoto), fukunaga@idea.c.u-tokyo.ac.jp (Alex Fukunaga), adibotea@ie.ibm.com (Adi Botea)\nPart of this research was performed while Adi Botea was affiliated with NICTA and The Australian National University.\nPreprint submitted to Artificial Intelligence October 26, 2012"}, {"heading": "1. Introduction", "text": "Parallel search is an important research area for two reasons. First, many search problems, including planning instances, continue to be difficult for sequential algorithms. Parallel search on state-of-the-art, parallel clusters has the potential to provide both the memory and the CPU resources required to solve challenging problem instances. Second, while multiprocessors were previously expensive and rare, multicore machines are now ubiquitous. Future generations of hardware are likely to continue to have an increasing number of processors, where the speed of each individual CPU core does not increase as rapidly as in past decades. Thus, exploiting parallelism is necessary to extract significant speedups from the hardware.\nOur work is primarily motivated by domain-independent planning. In classical planning, many problem instances continue to pose a challenge for state-of-the-art planning systems. Both the memory and the CPU requirements are main causes of performance bottlenecks. The problem is especially pressing in sequential optimal planning. Despite significant progress in recent years in developing domain-independent admissible heuristics [1, 2, 3], scaling up optimal planning remains a challenge. Multi-processor, parallel planning3 has the potential to provide both the memory and the CPU resources required to solve challenging problem instances.\nWe introduce and evaluate Hash Distributed A* (HDA*), a parallelization of A* [4]. HDA* runs A* on every processor, where each processor has its own open and closed lists. A hash function assigns each state to a unique processor, so that every state has an \u201cowner\u201d. Whenever a state is generated, its owner processor is computed according to this hash function, and the state is sent to its owner. This simple mechanism simultaneously accomplishes load balancing as well as duplicate pruning. While the key idea of hashbased assignment of states to processors was initially proposed as part of the PRA* algorithm by Evett et al. [5], and later extended by Mahapatra and Dutt [6], the scalability and limitations of hash-based work distribution and duplicate pruning have not been previously evaluated in depth. In addition, hash-based work distribution has never been applied to domain-independent planning.\nHDA* has two key attributes which make it worth examining in detail.\n3In this paper, parallel planning refers to computing sequential plans with multiprocessor planning, as opposed to computing parallel plans with a serial algorithm.\nFirst, it is inherently scalable on large-scale parallel systems, because it is a distributed algorithm with no central bottlenecks. While there has been some recent work in parallel search [7, 8, 9], these approaches are multi-threaded, and limited to a single, shared-memory machine. HDA*, on the other hand, scales naturally from a single, multicore desktop machine to a large scale, distributed memory cluster of multicore machines. When implemented using the standard MPI message passing library [10], the exact same code can be executed on a wide array of parallel environments, ranging from a standard desktop multicore to a massive cluster with thousands of cores, effectively using all of the aggregate CPU and memory resources available on the system.\nSecond, HDA* is a very simple algorithm \u2013 conceptually, the only difference between HDA* and A* is that when a node is generated, we compute its hash value, and send the node to the closed list of the processor that \u201cowns\u201d the hash value. Everything runs asynchronously, and there is no tricky synchronization. This simplicity is extremely valuable in parallel algorithms, as parallel programming is notoriously difficult. Thus, the goal of our work is to evaluate the performance and the scalability of HDA*.\nHDA* is implemented as an extension of two state-of-the-art solvers. The first solver is the Fast Downward domain-independent planner. We use the cost-optimal version with the explicit (merge-and-shrink) abstraction heuristic reported by Helmert, Haslum, and Hoffman [3]. The second solver is an application-specific 24-puzzle solver, which uses the pattern database heuristic code provided by Korf and Felner [11]. A key difference between these is the relative speed of processing an individual state. The domainspecific 24-puzzle solver processes states significantly faster than the domainindependent Fast Downward planner, expanding 2-5 times more states per second.4 The speed of processing a state can significantly impact the efficiency of a parallel search algorithm, which is the speedup relative to a serial implementation divided by the number of CPU cores. A larger processing cost per state tends to diminish the impact of parallel-specific overheads, such as the communication and the synchronization overhead (introduced in Section 2). Therefore, using both types of solvers allows us to assess the efficiency of HDA* across a range of application problems.\nThe scaling behavior of the algorithm is evaluated on a wide range of par-\n4Korf and Felner\u2019s code uses IDA*, which generates states much faster; we only incorporate their pattern database heuristic code in our parallel A* framework.\nallel machines. First, we show that on a standard, 8-core workstation, HDA* achieves speedups up to 6.6. We then evaluate the exact same HDA* code on a commodity cluster with an Ethernet network, and two high-performance computing (HPC) clusters with up to 2400 processors and 10.5TB of RAM. The scale of our experiments goes well beyond previous studies of hash-based work distribution. We show that HDA* scales well, allowing effective utilization of a large amount of distributed memory to optimally solve problems that require terabytes of RAM.\nThe experiments include a comparison with TDS [12, 13], a successful parallelization of IDA* with a distributed transposition table. Using up to 64 processors on a commodity cluster, we show that HDA* is 2-65 times faster and solves more instances than TDS. We propose a simple, hybrid approach that combines strengths of both HDA* and TDS: Run HDA* first, and if HDA* succeeds, return the solution. Otherwise, start a TDS search where the initial threshold for the depth-first exploration is provided by the failed HDA* search. Clearly, when HDA* succeeds, this hybrid algorithm runs as fast as HDA*. We show that, when HDA* fails, the total runtime of the hybrid approach is comparable to the running time of TDS.\nThe rest of the paper is organized as follows. Section 2 presents background information. The HDA* algorithm is described in Section 3. We then present an empirical evaluation and analysis of the scalability HDA* in Section 4. Tuning the performance of HDA* is addressed in Section 5. Hash-based distribution is compared with a simpler, randomized work distribution strategy in Section 6. In Section 7, we compare HDA* with TDS, and propose a hybrid strategy which combines the strengths of both algorithms. We review other approaches to parallel search in Section 8. This is followed by a summary and discussion of the results, and directions for future work."}, {"heading": "2. Background", "text": "Efficient implementation of parallel search algorithms is challenging due to several types of overhead. Search overhead occurs when a parallel implementation of a search algorithm expands (or generates) more states than a serial implementation. The main cause of search overhead is partitioning of the search space among processors, which has the side effect that the access to non-local information is restricted. For example, sequential A* can terminate immediately after a solution is found, because it is guaranteed to be optimal. In contrast, when a parallel A* algorithm finds a (first) solution at\nsome processor, it is not necessarily a globally optimal solution. Better solutions might exist in non-local portions of the search space. A more detailed discussion of the search overhead is available in Section 4.2.2.\nSynchronization overhead is the idle time wasted at synchronization points, where some processors have to wait for the others to reach the synchronization point. For example, in a shared-memory environment, the idle time can be caused by mutual exclusion locks on shared data. Finally, communication overhead refers to the cost of inter-process information exchange in a distributed-memory environment (i.e., the cost of sending a message from one processor to another over a network).\nThe key to achieving a good speedup in parallel search is to minimize such overheads. This is often a difficult task, in part because the overheads are interdependent. For example, reducing search overhead usually increases synchronization and communication overhead.\nThere are several, broad approaches to parallelizing search algorithms. This paper focuses on parallelization by partitioning the search space, so this section reviews this approach. Other approaches, including parallelization of the computation performed on each state, running a different search algorithm on each processor, and parallel window search, are reviewed in Section 8. In addition, this paper focuses on parallel best-first search. The use of hash-based work distribution techniques similar to HDA* for breadth-first search in model checking is reviewed in Section 8.\nOne general framework for partitioning the search space among parallel processes first starts with a root process which initially generates some seed nodes using sequential search, and assigns these seed nodes among the available processors. Then, at each processor, a search algorithm begins to explore the descendants of its seed node. These seed nodes become the local root nodes for a local, sequential search algorithm. This basic strategy can be applied to depth-first search algorithms, including simple depth-first search, branch-and-bound, and IDA*, as well as breadth-first and best-first search algorithms such as A*.\nWork stealing is a standard approach for partitioned, parallel search, and is used in many applications, particularly in shared-memory environments. In work-stealing, each processor maintains a local work queue. When a processor P generates new work (i.e., new states to be expanded) w, it places w in P \u2019s own local queue. When P has no work in its queue, it \u201csteals\u201d work from the queue of a busy processor.\nTwo key considerations in a particular work-stealing strategy are how to\ndecide which processor to steal work from, as well as how to decide which and how much work to steal. Various work-stealing strategies for depthfirst, linear-space algorithms such as depth-first branch-and-bound IDA*, and minimax search have been studied (e.g. [14, 15, 16]). While most work on work-stealing has been on MIMD systems, parallelization of IDA* on SIMD machines using an alternating, two-phase mechanism, with a search phase and a load balancing phase, has also been investigated [17, 18].5\nAnother approach to search space partitioning (particularly in sharedmemory search) is derived from a line of work on addressing memory capacity limitations by using a large amount of slower, external memory (such as disks), to store states in search [19, 20, 21] (external memory was also used specifically for planning [22]). An issue with using external memory is the overhead of expensive I/O operations, so techniques for structuring the search to minimize these overheads have been the focus of work in this area. Korf has implemented a multithreaded, breadth-first search using a shared work queue which uses external memory [7, 20]. Interestingly, some approaches to reducing the I/O overhead in external memory search can be adapted to handle the inter-process communication overhead in parallel search. Zhou and Hansen [8] introduce a parallel, breadth-first search algorithm. Parallel structured duplicate detection seeks to reduce synchronization overhead. The original state space is partitioned into collections of states called blocks. The duplicate detection scope of a state contains the blocks that correspond to the successors of that state. States whose duplicate detection scopes are disjoint can be expanded with no need for synchronization. Burns et al. [9] have investigated best-first search algorithms that include enhancements such as structured duplicate detection and speculative search. These techniques were shown to be effective in a shared memory machine with up to 8 cores.\nWe now review the line of work directly related to HDA*. Algorithms such as breadth-first or best-first search (including A*) use an open list which stores the set of states that have been generated but not yet expanded. In an early study, Kumar, Ramesh, and Rao [23] identified two broad approaches to parallelizing best-first search, based on how the usage and maintenance of the open list was parallelized. In a centralized approach, a single open\n5In MIMD (Multiple-Instruction, Multiple-Data stream) systems, each processor executes code independently; in SIMD (Single-Instruction, Multiple-Data stream) systems, all processors execute the same instruction (on different data).\nlist is shared among all processes. Each process expands one of the current best nodes from the globally shared open list, and generates and evaluates its children. This centralized approach introduces very little or no search overhead, and no load balancing among processors is necessary. Furthermore, this method is especially simple to implement in a shared-memory architecture by using a shared data structure for the open list. However, concurrent access to the shared open list becomes a bottleneck and inherently limits the scalability of the centralized approach, except in cases where the cost of processing each node (e.g., evaluating the node with a heuristic function) is extremely expensive, in which case overheads associated with shared open list access become insignificant.\nIn contrast, in a decentralized approach to parallel best-first search, each process has its own open list. Initially, the root processor generates and distributes some search nodes among the available processes. Then, each process starts to locally run best-first search using its local open list (as well as a closed list, in case of algorithms such as A*). Decentralizing the open list eliminates the concurrency overheads associated with a shared, centralized open list, but load balancing becomes necessary.\nKumar, Ramesh and Rao [23], as well as Karp and Zhang [24, 25] proposed a random work allocation strategy, where newly generated states were sent to random processors. In parallel architectures with non-uniform communication costs, a straightforward variant of this randomized strategy is to send states to a random neighboring processor (with low communication cost) to avoid the cost of sending to an arbitrary processor (c.f., [26]).\nIn addition to load balancing, another issue that a parallel search algorithm must address is duplicate detection. In many search applications, including domain-independent planning, the search space is a graph rather than a tree, and there are multiple paths to the same state. In sequential search, duplicates can be detected and pruned by using a closed list (e.g., hash table) or other duplicate detection techniques (e.g. [27, 28]). Efficient duplicate detection is critical for performance, both in serial and parallel search algorithms, and can potentially eliminate vast amounts of redundant work.\nIn parallel search, duplicate state detection incurs several overheads, depending on the algorithm and the machine environment. For instance, in a shared-memory environment, many approaches, including work-stealing, need to carefully manage locks on the shared open and closed lists.\nParallel Retracting A* (PRA*) [5] simultaneously addresses the problem\nof work distribution and duplicate state detection. In PRA*, each processor maintains its own open and closed lists. A hash function maps each state to exactly one processor which \u201cowns\u201d the state. When generating a state, PRA* distributes it to the corresponding owner. If the hash keys are distributed uniformly across the processors, load balancing is achieved. After receiving states, PRA* has the advantage that duplicate detection can be performed efficiently and locally at the destination processor.\nWhile PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of RA* [5], a limited memory search algorithm closely related to MA* [29] and SMA* [30]. When a processor\u2019s memory became full, Parallel Retracting A* retracts states from the search frontier, and their f -values are stored in their parents, which frees up memory. Thus, unlike parallel A*, PRA* does not store all expanded nodes in memory, and will not terminate due to running out of memory in some process. On the other hand, the implementation of this retraction mechanism in [5] incurs a significant synchronization overhead: when a processor P generates a new state s and sends it to the destination processor Q, P blocks and waits for Q to confirm that s has successfully been received and stored (or whether the send operation failed due to memory exhaustion at the target process). Unlike PRA*, HDA* does not incorporate a node retraction mechanism. Also, unlike the original implementation of PRA*, HDA* is a fully asynchronous algorithm, where all messages are sent/received asynchronously.\nThe idea of hash-based work distribution was investigated further by Mahapatra and Dutt [6], who studied parallel A* on a Hypercube architecture, where CPUs are connected by a hypercube network, while in current standard architectures machines are connected by either a mesh or torus network. As a baseline, they implemented Global Hashing (GOHA), which is similar to PRA*, except that GOHA is a parallelization of SEQ A*, a variant of A* which performs partial expansion of states, while HDA* is a parallelization of standard A*. They proposed two alternatives to the simple hash-based work distribution in PRA*. The first approach by Mahapatra and Dutt, called Global Hashing of Nodes and Quality Equalizing (GOHA&QE), decouples load balancing and duplicate checking. States are assigned an owner (based on hash key) for duplicate checking, and a newly generated state is first sent to its owner process for duplicate checking. If the state is in the open list of the owner process, then it is a duplicate and discarded. Otherwise, it is added to the open list of the owner, and the state is possibly reassigned to another\nprocess using Dutt and Mahapatra\u2019s Quality Equalizing (QE) strategy [26]. In both PRA* and GOHA&QE, the hash function is global \u2013 a state can be hashed to any of the processors in the system. In parallel architectures where communication costs vary among pairs of processors, such a global hashing may be suboptimal. Therefore, Mahapatra and Dutt also proposed Local Hashing of Nodes and QE (LOHA&QE), which incorporates a state space partitioning strategy and allocates disjoint partitions to disjoint processor groups in order to minimize communication costs [6]. Mahapatra and Dutt showed that GOHA&QA and LOHA&QE outperformed the simpler, global hash-based work distribution method used in PRA* on the Travelling Salesperson Problem (TSP).\nMahapatra and Dutt\u2019s local hashing is based on a number of restrictive assumptions that do not allow applying this strategy to planning. Specifically, local hashing works in problems with so-called levelized search graphs. In a levelized graph, a given state will always have the same depth (distance from root node), regardless of the path that connects the root and the state. The notion of the levelized graph can sometimes be extended to exploit local hashing if the search space has some regularities on depths such as multiple sequence alignment [31]. However, planning and the sliding-tile puzzle do not belong to this class of problems. The reason is that the same state could be reached via different-length paths. For example, if two cities A and B are connected by two routes of different lengths, then driving a truck from A to B via each route will result in the same state but the paths from the starting state will have different lengths.\nTransposition-table driven work scheduling (TDS) [12, 13] is a distributed memory, parallel IDA* algorithm. Similarly to PRA*, TDS distributes work using a state hash function. The transposition table is partitioned over processors to be used for detecting and pruning duplicate states that arrive at the processor. Thus, TDS distributes a transposition table for IDA* among the processing nodes, similarly to how PRA* distributes the open and the closed lists for A*. This distributed transposition table allows TDS to exhibit a very low (sometimes negative) search overhead, compared to a sequential IDA* that runs on a single computational node with limited RAM capacity. TDS achieved impressive speedups in applications such as the 15-puzzle, the double-blank puzzle, and the Rubik\u2019s cube, on a distributed-memory machine. The ideas behind TDS have also been successfully integrated in adversarial two-player search [32, 33, 34].\nThus, the idea of hash-based distribution of work is not new, but there\nare several reasons to revisit the idea and perform an in-depth evaluation at this point. First, the primary motivation for this work was to advance the state of the art of domain-independent planning by parallelizing search. While there has been some previous, smaller-scale work on parallel planning, large-scale parallel planning has not been previously attempted, and hashbased work distribution is a natural approach for scaling parallel planning to large-scale parallel clusters.\nSecond, parallel systems have become much more common today than when the earlier work by Evett et al. [5] and Mahapatra and Dutt [6] was done, and the parallel systems which are prevalent today have very different architectures. The most common parallel architectures today are commodity, multicore, shared memory machines, as well as distributed memory clusters which are composed of shared memory multicore nodes. Hash-based work distribution is a simple approach that can potentially scale naturally from single, multicore nodes to large clusters of multicore nodes, and it is important to evaluate its performance on current, standard parallel architectures. In addition, previous algorithms based on this idea, such as PRA* and Mahapatra and Dutt\u2019s method, make some assumptions that are specific to the hardware architecture in use. In contrast, we aim at obtaining an algorithm as general as possible, avoiding hardware-specific assumptions. In fact, as mentioned earlier, our MPI-based implementation can be run on a variety of platforms, including shared-memory and distributed-memory systems.\nThird, there are some important issues that were not fully explored in the earlier work. For example, factors which can potentially limit the scaling of hash-based work distribution, such as search overhead and communication overhead, have not been analyzed in detail. Also, the performance impact of asynchronous vs synchronous communication, as well as the impact of using a hash function for work distribution, as opposed to a randomized strategy, have not been previously investigated.\nFourth, while the work on TDS showed the utility of asynchronous, hashbased work distribution for IDA*, this previous work was done on 15-puzzle variants and the Rubik\u2019s cube, which are two domains where the overhead incurred by re-exploration of states in IDA* is known to be relatively small. In some other domains, this overhead can be quite significant, which can result in significant costs on a cluster environment. Thus, an investigation of the scalability of hash-based, parallel A* and a comparison with TDS is worthwhile.\nThus, while previous work has considered global hash-based distribution\neither as a component of a more complex algorithm [5] or as a straw man against which local hash-based distribution was considered [6], this is the first paper which analyzes the scalability and limitations of global hashing in depth. An early version of this work has been previously presented in a conference paper [35]. However, that initial work was limited to up to 128 CPU cores on an older system, contained no results on the 24-puzzle, no comparison to TDS, and included a less detailed evaluation and analysis."}, {"heading": "3. Hash Distributed A*", "text": "We now describe Hash Distributed A* (HDA*), a simple parallelization of A* which uses the hash-based work distribution strategy originally proposed in PRA* [5]. In HDA* the closed and open lists are implemented as a distributed data structure, where each processor \u201cowns\u201d a partition of the entire search space. The local open and closed list for processor P is denoted OpenP and ClosedP . The partitioning is done via a hash function on the state, as described later.\nHDA* starts by expanding the initial state at the root processor. Then, each processor P executes the following loop until an optimal solution is found:\n1. First, P checks if one or more new states have been received in its message queue. If so, P checks for each new state s in ClosedP , in order to determine whether s is a duplicate, or whether it should be inserted in OpenP\n6. 2. If the message queue is empty, then P selects a highest priority state\nfrom OpenP and expands it, resulting in newly generated states. For each newly generated state s, a hash key K(s) is computed based on the state representation, and s is sent to the processor which owns K(s). This send is asynchronous and non-blocking. P continues its computation without waiting for a reply from the destination.\nIn a straightforward implementation of hash-based work distribution on a shared memory machine, each thread owns a local open/closed list implemented in shared memory, and when a state s is assigned to some thread,\n6Even if the heuristic function [3] is consistent, parallel A* search may sometimes have to re-open a state saved in the closed list. For example, P may receive many identical states with various priorities from different processors and these states may reach P in any order.\nthe writer thread obtains a lock on the target shared memory, writes s, then releases the lock. Note that whenever a thread P \u201csends\u201d a state s to a destination dest(s), then P must wait until the lock for shared open list (or message queue) for dest(s) is available and not locked by any other thread. This results in significant synchronization overhead \u2013 for example, it was observed in [9] that a straightforward implementation of PRA* exhibited extremely poor performance on the Grid search problem, and multicore performance for up to 8 cores was consistently slower than sequential A*. While it is possible to speed up locking operations by using, for example, highly optimized lock operations implementations in inline assembly language, the performance degradation due to synchronization remains a considerable problem.\nIn contrast, the open/closed lists in HDA* are not explicitly shared among the processors. Thus, even in a multicore environment where it is possible to share memory, all communications are done between separate MPI processes using non-blocking send/receive operations. Our program implements this by using MPI Bsend and MPI Iprobe, and relies on highly optimized message buffers implemented in MPI.\nEvery state must be sent from the processor where it is generated to its \u201cowner\u201d processor. In their work with transposition-table driven scheduling for parallel IDA*, Romein et al. [12] showed that this communication overhead could be overcome by packing multiple states with the same destination into a single message. HDA* uses this state packing strategy to reduce the number of messages. The relationship between performance and message sizes depends on several factors such as network configurations, the number of CPU cores, and CPU speed. In our experiments, 100 states are packed into each message on a commodity cluster using more than 16 CPU cores and a HPC cluster, while 10 states are packed on the commodity cluster using less than 16 cores.\nIn a decentralized parallel A* (including HDA*), when a solution is discovered, there is no guarantee at that time that the solution is optimal [23]. When a processor discovers a locally optimal solution, the processor broadcasts its cost. The search cannot terminate until all processors have proved that there is no solution with a better cost. In order to correctly terminate a decentralized parallel A*, it is not sufficient to check the local open list at every processor. We must also ensure that there is no message en route to some processor that could lead to a better solution. Various algorithms to handle termination exist. In our implementation of HDA*, we used the time algorithm of Mattern [36], which was also used in TDS.\nMattern\u2019s method is based on counting sent messages and received messages. If all processors were able to count simultaneously, it would be trivial to detect whether a message is still en route. However, in reality, different processors Pi will report their sent and received counters, S(ti) and R(ti), at different times ti. To handle this, Mattern introduces a basic method where the counters are reported in two different waves. Let R\u2217 = \u2211 i R(ti) be the accumulated received counter at the end of the first wave, and S \u2032\u2217 = \u2211\ni S(t \u2032 i) be the accumulated sent counter at the end of the second wave. Mattern proved that if S \u2032\u2217 = R\u2217, then the termination condition holds (i.e., there are no messages en route that can lead to a better solution).\nMattern\u2019s time algorithm is a variation of this basic method which allows checking the termination condition in only one wave. Each work message (i.e., containing search states to be processed) has a time stamp, which can be implemented as a clock counter maintained locally by each processor. Every time a new termination check is started, the initiating processor increments its clock counter and sends a control message to another processor, starting a chain of control messages that will visit all processors and return to the first one. When receiving a control message, a processor updates its clock counter C to max(C, T ), where T is the maximum clock value among processors visited so far. If a processor contains a received message m with a time stamp tm \u2265 T , then the termination check fails. Obviously, if, at the end of the chain of messages, the accumulated sent and received counters differ, then the termination check fails as well.\nIn hash based work distribution, the choice of the hash function is essential for achieving uniform distribution of the keys, which results in effective load balancing. Our implementation of HDA* uses the Zobrist function [37] to map a SAS+ state representation [38] to a hash key. The Zobrist function is commonly used in the game tree search community to detect duplicate states. The Zobrist hash value is computed by XOR\u2019ing predefined random numbers associated with the components of a state. When a new state is generated, the hash value of the new state can be computed incrementally from the hash value of the parent state by incrementally XOR\u2019ing the state components that differ. The Zobrist function was previously used in domainindependent planning in MacroFF [39]. It is possible for two different states to have the same hash key, although the probability of such a collision is extremely low with 64-bit keys. In MacroFF, as well as an earlier version of HDA* [35], duplicate checking in the open/list was performed by checking if the hash key of a state was present in the open/closed list, so there was a\nnon-zero (albeit tiny) probability of a false positive duplicate check result. In this paper, our HDA* implementations perform duplicate checks by comparing the actual states. Although this is slightly slower than comparing only the hash key, duplicate checks are guaranteed to be correct."}, {"heading": "4. Scalability of HDA*", "text": "We experimentally evaluated HDA* on top of a domain-independent planner and an application-specific, 24-puzzle solver. Our hardware environments, including a single, multicore machine (Multicore), a commodity cluster (Commodity), and two high-performance clusters (HPC clusters) with up to 2400 processors (HPC1, HPC2), are shown in Table 1. In all of our experiments, HDA* is implemented in C++, compiled with g++ and parallelized using the MPI message passing library.\nWe first describe experimental results for domain-independent planning. We parallelized the sequential optimal version of the Fast Downward planner, enhanced with the so-called LFPA heuristic, which is based on explicit (merge-and-shrink) state abstraction [3]. All the reported results are obtained with the abstraction size set to 1,000. Preliminary experiments with the abstraction size set to 5,000 did not change the results qualitatively. As benchmark problems, we use classical planning instances from past planning competitions. We selected instances that are hard or unsolvable for the sequential optimal version of Fast Downward.\nHDA*, like other asynchronous parallel search algorithms, behaves nondeterministically, resulting in some differences in search behavior between identical invocations of the algorithms. However, on the runs where we collected multiple data points, we did not observe significant differences between runs of HDA*. Therefore, due to the enormous resource requirements of a large-scale experimental study,7 the results shown are for single runs."}, {"heading": "4.1. Asynchronous vs Synchronous Communications: Experiments on a Single, Multicore Machine", "text": "First, we evaluate HDA* on a single, multicore machine (presented in Table 1) in order to investigate the impact of asynchronous vs synchronous communications in parallel A*. We compare HDA* with sequential A* and a shared-memory implementation of Parallel Retracting A* (PRA*) [5] on a single, multicore machine. As described in Section 2, PRA* uses the same hash-based work distribution strategy as HDA*, but uses synchronous communications. As in Burns et al.\u2019s experiments [9], our PRA* implementation does not include the node retraction scheme because the main goal of our experiments is to show the impact of eliminating synchronization overhead from PRA*.\nOur HDA* implementation is the same MPI-based implementation used in our larger-scale, distributed memory experiments described below. It executes a separate OS process for each thread of execution, and we rely on the message passing functions in MPI for asynchronous communications. While it may be possible to develop a more efficient implementation of HDA* specifically for shared memory machines (e.g., using kernel threads and shared memory constructs), we wanted to investigate the scalability of the same HDA* implementation on both shared and distributed memory machines.\nLow level optimizations were implemented to make both the HDA* and PRA* as fast as possible. In addition to locks available in the Boost C++ library, we also incorporated spin locks based on the \u201cxchgl\u201d assembly operation in order to speed up PRA*.\nEach algorithm used the full 32 gigabytes of RAM available on the machine at hand. That is, n-core HDA* spawns n processes, each using 32/n gigabytes of RAM, sequential A* used the full 32GB available, and the multithreaded PRA* algorithm shared 32GB of RAM among all of the threads.\n7In addition to usage charges for the clusters, there are issues of resource contention because the clusters are shared among hundreds of users.\nTable 2 shows the speedup of HDA* and PRA* for 4 cores and 8 cores. We show only instances that can be solved by the serial planner with 32GB RAM available. In addition to runtimes for the sequential A* algorithm, the speedup and the parallel efficiency are shown for HDA* and PRA*. Efficiency is defined as S/P , where S is the speedup over a serial run and P is the number of cores. As shown in Table 2, HDA* clearly outperforms PRA*. With 4 cores, the speedup of HDA* ranges from 2.11 to 3.91, and the efficiency ranges from 0.53 to 0.98. With 8 cores, the speedup of HDA* ranges from 3.76 to 6.62, and the efficiency ranges from 0.47 to 0.83. These results\ndemonstrate the benefit of asynchronous message passing over a synchronous implementation of hash-based work distribution."}, {"heading": "4.2. Planning Experiments on a HPC Cluster", "text": "Next, we investigate the scaling behavior of HDA* on the HPC2 cluster, (see machine specs in Table 1). We used 1-200 nodes in our experiments (i.e., 1-2400 cores).\nAs in Table 2, the times shown in Table 3 include the time for the search algorithm execution, and exclude the time required to compute the abstraction table for the LFPA heuristic, since this phase of Fast Downward+LFPA has not been parallelized yet and therefore requires the same amount of time to run regardless of the number of cores. For example, the IPC-6 Pegsol-30 instance, which requires 168.18 seconds with 60 cores, was solved in 20.54 seconds with 1200 cores, plus 1.30 seconds for the abstraction table generation. A value of \u201c-\u201d for the runtimes in Table 3 indicates a failure, i.e., the planner terminated because one of the nodes ran out of memory. For example, the Freecell-12 instance was first solved using 600 cores.\nLet tk be the runtime to solve a problem using k processors. Standard metrics for evaluating parallel performance on n processors include speedup and efficiency, defined as as Sn = t1/tn, and En = Sn/n, respectively. These standard metrics have limited applicability for parallel A* because sequential runtimes cannot be measured for hard problems. On hard problems that truly require large-scale parallel A*, sequential A* exhausts RAM and terminates before finding a solution. Since sequential runtime t1 cannot be measured, Sn and En cannot be computed. While we could use only benchmarks which can be solved using a single processor (as we did for our multicore experiment in Section 4.1), this would restrict the benchmark set to problems that can be solved by A* using only the RAM available on 1 processing node. Suppose there is 4.5GB RAM per core, as is the case with our HPC2 cluster. A single-threaded A* algorithm that generates 50,000 new states per second, using 50 bytes per state, will exhaust 4.5GB within 30 minutes. In fact, because the state size for difficult planning instances is usually much larger than 50 bytes, serial A* exhausts 4.5GB memory much more quickly. Problems that require only a few minutes to solve using sequential search are poor benchmarks for large-scale parallel search, because such problems can be solved in a few seconds using 1000 processors.\nMahapatra and Dutt also noted that sequential runtimes were not available for their scalability experiments [6]. They evaluated their algorithms by\ncomputing approximate speedups, defined as follows. Let pmin be the minimum number of processors that solved the problem. They make the assump-\ntion that the speedup with pmin processors is pmin (i.e., they assume that linear speedup can be obtained up to pmin processors). Based on this, they estimate that sequential runtime is pmin \u00d7 tpmin , and speedups for p > pmin processors can be computed by computing the ratio of parallel runtime to this estimate of sequential runtime.\nMahapatra and Dutt argue that this is a conservative estimate of speedup because overheads increase as the number of processors increases, so the assumption that Sp = pmin is a lower bound on the the actual speedup for p \u2265 pmin. However, there are some issues with this approach: (1) such estimates of \u201cspeedup\u201d are not actual speedup values, (2) pmin can be different for different problem instances, and (3) when pmin is large, e.g., if a problem is first solved using 600 cores, including these estimated speedups significantly biases computations of average speedup to seem artificially high \u2013 in other words, assuming that linear speedups are obtainable up to pmin processors is unrealistic when pmin is large.\nThus, we take a different approach to measuring the scalability of parallel search, based on the performance of n processors relative to the performance on pmin. Let pmin be the smallest number of cores that solves a problem, and tmin be the wall-clock runtime for pmin cores. The relative speedup is defined as Sn = tmin/tn, and the relative efficiency is defined as En = Sn/(n/pmin). Relative efficiency has been used by other researchers, c.f., Niewiadomski et al. [40] (who called it \u201cspeedup efficiency\u201d).\nA second issue related to measuring performance on a cluster of multicore machines is the baseline configuration of a single processing node. As shown later in Section 4.2.5, the number of cores used per processing node has a significant effect on performance. Our main goal is to investigate the scalability of HDA* while fully utilizing all available processors, so in our scalability experiments, we use all cores on a processor, e.g., on the HPC2 cluster, we use the 12 cores per core, allocating 4.5GB RAM per core.\nTable 3 shows the wall-clock runtimes of HDA* on a set of 45 IPC benchmark problems. The results are grouped according to the smallest number of processors that solved the problem. As mentioned earlier, a \u201c-\u201d indicates a failure, i.e., the planner terminated because one of the cores ran out of memory. For example, the Depot16 instance was first solved using 144 cores. Sokoban.p24-Sokoban.p27 failed with 2400 cores, even though they could be solved with fewer cores.\nFigure 1 shows that the relative efficiency (see above) of HDA* on planning generally scales well for 2pmin and 4pmin cores, even when pmin = 600\ncores. Even for 8pmin cores, the relative efficiency is over 0.5 for pmin \u2208 {12, 24, 60, 144, 300}. However, as p/pmin increases, the relative efficiency degrades. For pmin = 12, when p = 1200 (p/pmin = 100), relative efficiency is 9.25%, and for p = 2400 cores, the relative efficiency is a mere 2.2%.\nIn other words, HDA* scales reasonably efficiently for up to 4-8 times pmin, the minimum number of processors that can solve the problem. However, as the number of processors is further increased, there is a point of diminishing returns, and eventually, adding more processors can result in longer runtimes. An extreme case of this scaling limitation can be seen on the Sokoban benchmark problems (Sokoban.p24-p.28), which were solvable with up to 1200 cores (with diminishing returns), but terminated due to memory exhaustion at some processor on 2400 cores. On the other hand, it is important to note that even for very large values of pmin (e.g., 600 cores), HDA* continues to scale very efficiently for 4pmin (2400) cores."}, {"heading": "4.2.1. Load Balance", "text": "A common metric for measuring how evenly the work is distributed among the cores is the load balance, defined as the ratio of the maximal number of states expanded by a core and the average number of states expanded by each core. As shown in Figure 2, HDA* achieves good load balance when p/pmin \u2264 8. While load balance tends to degrade as the number of processors increases, it is important to note that load imbalance does not seem to be simply caused by using a large, absolute number of processors. For instance,\non all 6 problems where pmin \u2265 600, the load balance for 2400 cores is less than 1.10.\nOne possible reason for load imbalance may be \u201chotspots\u201d \u2013 frequently generated duplicate nodes mapped to a small number of cores by the hash function. This is caused by transpositions in the search space, which are states that can be reached through different paths. In HDA*, if a processor receives a state s which is already in the closed list but the g-value of s is smaller than that in the closed list, s must be enqueued in the open list. However, the heuristic value of s is not recomputed in saving s to the open list, because the value is already saved in the closed list. For example, in solving PipesNoTk24 with 2400 cores, more than 70% of generated states are duplicates. A processor involved in a hotspot receives about 377% more duplicate states than the processor receiving duplicate states least frequently, although we observe that they receive similar amounts of work. As a result, the numbers of calls for the heuristic function are different (about 144%) between these processors. Thus, a processor which executes fewer heuristic evaluations (relative to other processors receiving a comparable number of states) has a higher state expansion rate than the other processors, resulting in load imbalance."}, {"heading": "4.2.2. Search Overhead", "text": "The search overhead, which indicates the extra states explored by parallel search, is defined as:\nSO = 100 \u00d7 ( number of states expanded by parallel search\nnumber of states expanded by baseline search \u2212 1).\nThis is the percentage of extra node expansions performed compared to a given baseline algorithm or hardware configuration. For a direct measurement of the search overhead for some given instance our baseline algorithm (configuration) will be HDA* on pmin \u2208 {12, 24, 60, 144, 300, 600, 1200, 2400} cores, the minimal number of cores that solves that instance. This has the advantage that, by definition, the baseline configuration will always succeed, allowing a direct measurement of the search overhead. As we show in this section, it is also possible to analyze the search overhead compared to serial A* even in cases where serial A* fails. Such comparisons can show how effective HDA* is in terms of wasted search effort.\nWe start by identifying possible causes of the search overhead in the case when the baseline algorithm is serial A*. Let c\u2217 be the cost of an optimal solution. Serial A* with a consistent heuristic in use expands all states with f < c\u2217, and some of the states with f = c\u2217. No states with f > c\u2217 get expanded. No state is expanded more than once. In contrast, parallel variants of A*, including HDA*, can re-expand states with f \u2264 c\u2217 which are not re-expanded by A*, can expand states with f > c\u2217, and can expand a different number of states with f = c\u2217 than serial A*. Possible causes include incomplete local knowledge of open lists at other processors, and nondeterministic travel time and arrival order of messages containing states.\nParallel A* usually expands more nodes than serial A*. If we consider only nodes with f < c\u2217 or f > c\u2217, the best that parallel A* can possibly achieve is matching the number of expansions performed by serial A* with a consistent heuristic. Thus, while it is possible, in principle, for parallel A* to expand fewer nodes than serial A* with a consistent heuristic, this is only possible if parallel A* expands fewer nodes with f = c\u2217 than serial A*.\nDefine R< as the fraction of expanded nodes with f < c \u2217. R= and R> are defined similarly. Let Rr be the ratio of node re-expansions, which is the total number of re-expansions divided by the total number of expansions. Figure 3 summarizes R<, R= and R> for domain-independent planning, according to pmin. Rr, the ratio of re-expansions, is shown in Figure 4.\nWhen serial A* fails to solve an instance, a direct measurement of the search overhead relative to serial A* cannot be performed. Fortunately, in such cases, our \u201cR\u2217 metrics\u201d (R<, R=, R> and Rr) can allow us to make inferences about the search overhead. The key observation is that serial A* expands all nodes with f < c\u2217. Therefore, if HDA* exhibits low values for R=, R> and Rr, on some run, we can conclude that the search overhead is low\nin that run. In other words, in such cases, there is little wasted search effort introduced by distributing the search. This appears to be the case for almost all instances, when using pmin cores. A noticeable exception is Mprime15 (pmin = 144), where R< = 6% and R= = 94%.\n8 As p, the number of cores in use for a given instance, grows increasingly larger than pmin, the R\u2217 metrics maintain very good values for a while. Then a degradation can eventually be observed, for those instances where pmin \u2264 144 (Figure 3).\nTo explain the good R> values of HDA* run on pmin cores and other core configurations as well (cf. Figure 3), we start by recalling that serial A* with a consistent heuristic will expand all nodes with f = v before expanding any node with f > v. Such a strict monotonicity cannot be guaranteed for HDA*. However, we believe that HDA* can expand states almost monotonically, especially when the number of nodes with f < c\u2217 is large enough to make the instance challenging to a pmin-core configuration. More specifically, consider an instance where the number of nodes with f < c\u2217 is large, and the number of nodes with f = c\u2217 that end up being expanded by serial A* is very small in comparison. Serial A* will expand all nodes with f < c\u2217, after which it will start expanding nodes with f = c\u2217. Likewise, we hypothesize that HDA* could expand a majority of the nodes with f < c\u2217, before starting expanding nodes with f \u2265 c\u2217. If an optimal solution is found soon after\n8This is because the optimal solution cost for Mprime15 is only 6 and there are many states with tied f values.\nstarting expanding nodes with f \u2265 c\u2217, the extra work caused by expanding nodes with f > c\u2217 would be a small fraction of the total search effort of HDA*. This informal explanation is consistent with the cases of low R> values observed among our data.\nAll benchmark problems in this paper have unit costs for all transitions between states. We believe that this contributes to the good (low) re-expansion rate Rr, when using pmin cores and p > pmin cores as well, unless p gets much larger than pmin (see Figure 4). Kobayashi et al. [31] have observed that the re-expansion rate increases in domains with non-unit transition costs.\nNext we focus on the search overhead when the baseline algorithm is HDA* using pmin cores (as defined above, pmin depends on the instance). Figure 5 summarizes search overhead data for domain-independent planning, showing average values over all instances with the same pmin.\nOnce again, as a general tendency, as we go further away from the baseline pmin, the search overhead stays stable for a while, after which it grows (Figure 5). Thus, some of the largest search overheads are seen when the difference p vs pmin is the largest (2400 vs 12). However, search overhead does not appear to be simply caused by using a large number of processors. For example, on 13 out of 45 problems, the search overhead on 2400 cores is less than 10%. This is despite the fact that pmin is in these cases significantly lower than 2400, varying from 24 cores (PipesTank10) to 600 cores (PipeNoTank25, PipeNoTank27).\nThere seem to be some problem domains which are highly prone to search\noverhead. Very large search overheads compared to other domains are seen in Sokoban problems (Sokoban.p24, Sokoban.p25, Sokoban.p26, Sokoban.p27, Sokoban.p28). The R\u2217 data available indicates that re-expansions are a major cause of this behavior. In addition, on 1200 cores, the R< metric also degrades for a few Sokoban instances. As the number of processors increased from 600 to 1200, the search overhead on Sokoban.p24, Sokoban.p26, and Sokoban.p27, more than doubles. The growth in search overhead appears to be accelerating as p/pmin increases. This large, rapidly growing search overhead explains the failure to solve the Sokoban problems with 2400 cores. As we increase from 1200 to 2400 cores, the amount of search overhead added is greater than the additional aggregate RAM capacity, so HDA* fails because some processor runs out of RAM.\nNote that there are some instances where search overhead is negative relative to pmin. For example, Mprime30 has a negative overhead for p \u2208 {60, 144, 300}, relative to pmin = 12. This suggests some wasted search effort in the baseline configuration, which then gets corrected for a larger p. Indeed, the R< value is 62.2% for Mprime30 solved on pmin = 12 cores, whereas most other instances, in all considered domains, have much better R< values (i.e., close to 100%) on their corresponding pmin configuration. Since Mprime has much shorter solutions than the other instances, HDA* tends to expand many more states with the f -value identical to the optimal length.\nIn summary, our analysis shows that HDA* is scalable, in the sense that it can use a large number of CPUs to solve difficult instances (which fail on a small number of CPU cores) quite efficiently, with a reasonably low search overhead. At the same time, for easy instances that can be solved with fewer CPUs, the tendency is the following: As the number of processors increases from pmin, the search overhead stays low for a while, after which a degradation is observed. One possible approach to alleviate this is using a solving strategy able to dynamically change the hardware resources (e.g., number of CPUs) [41]. Mechanisms for keeping the search overhead low when using many more processors than pmin is an interesting topic for future work."}, {"heading": "4.2.3. Node Expansion Rate", "text": "The node expansion rate helps evaluate other parallel overhead, besides the search overhead. As the number of processors increases, the communication overhead can reduce node expansion rate. Part of the communication overhead is alleviated by the fact that searching and travelling overlap to a great extent. Said in simple words, some states are being expanded while\nsome other states are travelling to their owner processor. Since each core sends generated successors to their home processors in HDA*, the total number of messages exchanged among processors increases with a larger number of cores. This is exacerbated by the fact that more cores can end up generating more states (search overhead). Therefore, another cause of slower node expansion rates may be message processing overhead \u2013 even with asynchronous communications, HDA* must deal with a larger number of messages as the number of cores increases.\nFigure 6 plots the expansion rate, measured for planning instances on the HPC2 cluster. Each data point in the figure is computed as follows. Let N \u03b9p be the total number of expansions for an instance \u03b9 solved on p cores. Let T \u03b9p, the total search time to solve instance \u03b9, be the (parallel) wall-clock time multiplied by p. Then, the node expansion rate is N \u03b9p/T \u03b9 p. Let A(p, pmin) be the expansion rate, when using p cores, averaged over all instances \u03b9 with a given pmin value. Finally, R(p, pmin) = A(p, pmin)/A(pmin, pmin) normalizes these average values relatively to the baseline configuration with pmin cores. Figure 6 plots R(p, pmin). If R(p, pmin) is close to 1, this indicates that the communication overhead is low. Values smaller than 1 indicate a degradation of the expansion rate, typically resulting in a degradation of the parallel search efficiency, unless the increase in time spent per node is compensated by a corresponding reduction in the total number of node expansions.\nFigure 6 shows that the expansion rate remains almost constant relative to pmin cores, unless the difference p\u2212pmin grows too large. Note that due to the logarithmic scale of the X-axis (# of cores), the expansion rate is more stable than it might initially appear in Figure 6. Despite the degradation of node\nexpansion rate at each processor, the aggregate node expansion rate across all processors continues to increase as the number of processors increases. For example, when p = 2400 and pmin = 12, the expansion rate degrades by a factor of about 22, according to Figure 6. On the other hand, the number of available CPUs increases by a factor or 2400/12 = 200, so the net gain in aggregate node expansion rate is 200/22 = 9.1 times. Similarly, there are substantial overall gains in the aggregate expansion rate, for all p and pmin values considered."}, {"heading": "4.2.4. Termination Detection", "text": "Termination detection is not a performance bottleneck, as it succeeds quickly after proving solution optimality. When an optimal (but not proven optimal yet) solution is found, its cost c\u2217 is broadcast, such that nodes with f \u2265 c\u2217 will not be expanded from now on. Thus, the only nodes expanded after the broadcast (if any) are nodes with f < c\u2217, which must be expanded anyway to prove optimality (serial A* expands them as well). Therefore, by the time c\u2217 has been broadcast and all nodes with f < c\u2217 have been expanded, state expansion and generation will stop, messages with states will not be sent around any longer, and the termination test will succeed."}, {"heading": "4.2.5. Scaling Behavior and the Number of Nodes (Machines) on a HPC Cluster", "text": "So far, we have considered the scaling of HDA* as the number of cores was increased. However, the scaling behavior of HDA* on a cluster is affected by factors other than the number of cores. Another factor is the cost of communication between nodes. Communications between cores in the same node are done via a shared memory bus, while communications between cores residing on different nodes are performed over a local area network (e.g., Infiniband or Ethernet).\nTo evaluate the impact of the communication delay, we ran the planner on the HPC1 cluster, whose specs are shown in Table 1. We used 64 cores, where the cores were distributed evenly on 4-64 nodes (i.e., 1-16 cores per node). The results are shown in the upper half of Table 4. If the communication delay was a significant factor, we would expect that, as the number of nodes increased, the runtime would increase. Interestingly, Table 4 (upper half) shows that runtimes decreased on almost all of the problem instances as the number of nodes increased.\nThe explanation for this counterintuitive result is memory contention within each single node. The limited bandwidth of the memory bus is saturated when many cores in the node simultaneously access memory. We further investigated this hypothesis as follows. First, cache contention was ruled out as a possible explanation because the processors in the cluster are based on the AMD Opteron architecture, where each core has a private L1 and L2 cache. Then, we performed an experiment where, again, we used 4-64 nodes, but this time, on each node, on each core that was not used by HDA*, we executed a dummy process, which makes no direct contribution towards solving the instance at hand. Each dummy process is an invocation of HDA* which is forced to run sequentially on the instance, and therefore behaves similarly to the \u201creal\u201d HDA* process with respect to memory access patterns (and therefore contends for memory with the \u201creal\u201d HDA* process).\nTable 4 indicates that the overhead of local memory access congestion within a node is much larger than the overhead due to communication between nodes. In the normal configurations without dummy processes (Table 4, upper half), using fewer cores per node results in less local memory con-\ntention and more inter-node communication. Overall, the performance improves as the 64 cores are distributed among an increasing number of nodes. On the other hand, in the configurations with dummy processes (Table 4, bottom half), local memory access within a node becomes equally congested regardless of the number of nodes, so there is no benefit to distributing the cores. Once the local memory access overhead is equalized for all configurations (4-64 nodes), variations in performance (if any present) among configurations could be explained by the overhead of inter-node communication. We notice, however, that the differences are small in row \u201cAvg time\u201d in the bottom half of Table 4. Even on a per-instance basis, the performance degradation as the number of nodes increases from 4 to 64 is within 20%. This shows that communications among nodes is not a critical bottleneck for HDA* on the HPC1 cluster."}, {"heading": "4.3. Results on the 24-Puzzle on a HPC Cluster", "text": "We evaluated HDA* on the 24-puzzle, using an application-specific solver based on IDA* code provided by Rich Korf, which uses a disjoint pattern database heuristic [11]. We replaced the IDA* search strategy with A* and HDA*. In the original code, each state has two redundant halves, trading memory for a faster state processing. While this makes sense for IDA*, it is not the best trade-off for HDA*. Thus, we reduce the state size to one half and compute the missing half on demand with a loop of 25 iterations per state. Other parts of the program, including the disjoint pattern database heuristic, were unchanged. As benchmark instances, we used the 50-instance set of 24-puzzles reported in [11], Table 2. We excluded the time required to read pattern databases from the hard-disk.\nFirst, we investigated the scalability of HDA* by running it on the fifty 24-puzzle instances, using 1-1200 cores on 1-100 processing nodes. As with domain-independent planning, we computed the efficiencies relative to the smallest number of cores pmin \u2208 {12, 24, 60, 144, 300, 600, 1200} which solved the problem. These average relative efficiencies for each pmin are shown in Figure 7. Compared to domain-independent planning (Figure 1), the relative efficiency of HDA* degrades much more rapidly on the 24-puzzle. As previously mentioned in Section 1, a key difference between domain-independent planning and the 24-puzzle is that individual states are processed much faster in the application specific 24-puzzle solver, and therefore parallel overheads have a greater weight in the total running time.\nTable 5 provides summary data obtained on the HPC2 cluster (see Table 1 for machine specs). Instances are partitioned according to pmin. For each partition, we report the runtime, the speedup, the load balance, the search overhead relative to pmin cores, and the R\u2217 metrics (i.e., R<, R=, R>, Rr) related to the search overhead, introduced in Section 4.2.2. All shown values are averages over instances with the same pmin. The load balance is quite good, and is no worse than 1.17 (300 and 600 cores for pmin = 12). For pmin processors, the search overhead is small. Rr and R> are within 4%, even for pmin = 1200. R= varies from 8% (pmin \u2208 {144, 300}) to 24% (pmin = 24). As with our planning results, search overhead increases, as more processors are used and the difference between p and pmin increases. Not surprisingly, the largest increase is seen when pmin = 12 and p = 1200 cores. When an instance is easy enough to be solved with 12 cores, 1200 cores will do much redundant work, resulting in R> as high as 90%. The search overhead has a significant impact on the overall speedup reported in Table 5. As p grows larger and larger than pmin, the speedup increases for a while, after which the search overhead dominates and the speedup degrades.\nThe trends in search overhead for the 24-puzzle are similar to those observed for domain-independent planning. On difficult instances (large pmin), HDA* can make an effective use of a large number of cores, solving such instances with a reasonably low search overhead. For instances that are sufficiently easy to be solved with relatively few cores (small pmin), the performance (e.g., speedup) increases for a while, after which a degradation is observed. A remarkable difference from the planning data is that the search overhead is significantly higher in the 24-puzzle, and it has a sharper degradation rate as well."}, {"heading": "4.4. Scaling Behavior on Planning on a Commodity Cluster", "text": "While the previous set of large-scale experiments were performed on a campus high-performance computing cluster, we also evaluated the scalability of HDA* on the Commodity cluster (see Table 1 for machine specs). Table 6 shows the relative efficiency of 16, 32, and 64 cores compared to a baseline of 8 cores (1 processing node). The results are organized according to pmin \u2208 {8, 16, 32}, the minimum number of tested cores that solved the instance. For each pmin, the average relative efficiency and relative speedups are shown. Several trends can be seen. First, when pmin = 8, and the number of cores used is increased to 16, the relative efficiency (0.55) and speedup (1.10) are very poor. On the other hand, after this initial threshold (the\njump from 1 processing node to 2 processing nodes) is crossed, the relative efficiency and relative speedups are near-linear, with low search overheads,\nsimilar to our results above on the HPC1 and HPC2 clusters when the number of cores used is within a factor of 8 of pmin. Inter-node communication plays a role in this behavior. Moving from one processing node to 2 nodes introduces inter-node communication, which is relatively slow in a commodity cluster. From two to more nodes the relative performance grows more steadily, since inter-node communication is present in all multi-node configurations.\nIn addition, as shown below in Section 7, HDA* significantly outperforms TDS, the previous state of the art algorithm, when the two are compared on this commodity cluster."}, {"heading": "5. Tuning HDA* Performance", "text": "The previous section investigated the scaling behavior of HDA* on various parallel environments as the amount of available resources varied. In this section, we consider how the behavior of HDA* can be tuned by adjusting two parameters: the number of cores to utilize per processor node, and the number of states to pack in each message between processes."}, {"heading": "5.1. Adjusting the Number of Cores to Utilize Per Processing Node", "text": "Next, we investigate the effect of scaling the number of processing nodes (machines) for the 24-puzzle. We ran the solver on a set of 100 nodes, which have 1200 cores in total, and varied the number of cores used per node between 1-12, so that 100-1200 cores were used. The runtimes are shown in\nTable 7. IDA* solves all 50 instances [11] whereas with our HDA* using 12 cores only 10 instances can be solved with 54GB of memory.\nAs shown in Table 7, using 1200 cores (4.5GB/core), 40 out of the 50 problems were solved. With 600 cores (9GB/core), 41 problems were solved. With 300 cores (18GB/core), 44 problems were solved, and with 100 cores (54GB/core), 45 problems were solved. That is, reducing the number of processes down to one process per node increases the number of solved instances.\nA closer look at the trade-offs involved explains this behavior. First there is the reduction in execution time when performing a given amount of work with more cores. Indeed, Table 7 indicates that, if an instance is\nsolved by a larger number of cores, the time tends to decrease as more cores were used. The notable exception is the configuration with 1200 cores where the large search overhead actually degrades the time performance. Other exceptions are instances that are easy for 100 cores, such as p22, p25, p28, p38, p40 and p44, which were solved in a few seconds and left little room for further improvement in runtime, resulting in consistently increasing runtimes as the number of cores increased. On the other hand, using more cores per processing node can lead to solving fewer instances. Suppose that k unique states need to be stored in open/closed in order to solve a problem. A sequential search with enough memory capacity to hold k states can solve this problem. If HDA* allocated work perfectly equally among the processors, and there was no search overhead, then the partitioning of memory among n cores would have no negative impact. In practice, load balancing is imperfect, and search overhead is non-zero, so increasing the number of cores (for a fixed amount of aggregate RAM) increases the chance of failure on a hard problem.\nWhen analyzing the relative impact of search overhead vs. RAM partitioning, we found that the former plays a significantly greater role in the reduction of the number of solved instances. Table 5 shows that the search overhead eventually increases significantly as the number of cores increases, meaning that more nodes must be stored in the open/closed lists. The rate of increase accelerates as the number of cores increases further beyond the minimal number of cores needed to solve the problem. On the other hand, our use of static RAM partitioning (as opposed to a more flexible, dynamic partitioning) is not a significant bottleneck. We measured the size of the open and closed lists for all processors, and observed that when the first processor runs out of memory, most other processors have almost exhausted their memory as well. This means that there is little opportunity for improvements to be made by using a more flexible, dynamic memory partitioning method.\nIn Section 4.2.5, we observed slowdowns in domain-independent planning as the ratio of utilized cores per node increased, and ascribed the slowdown to local memory bus contention. In the 24-puzzle, the node expansion rate decreases by 35% as the number of cores per processing node increased from 1 to 12. We attribute this increase to both local memory bus contention, and factors discussed in Section 4.2.3."}, {"heading": "5.2. The Effect of the Number of States Packed into Each Message", "text": "In HDA*, each state generation necessitates sending the state from processor where a state is generated to the processor which \u201cowns\u201d the state.\nSending a message from processor P to Q each time a state owned by Q is generated at P may result in excessive communication overhead, as well as overhead for creating/manipulating MPI message structures.\nIn order to amortize these overheads, Romein et al. [12], in their work on TDS, proposed packing multiple states with the same destination.\nOn the other hand, packing too many states into a message from processor P to Q might result in degraded performance for two reasons. First, the destination Q might be starved for work and be idle. Second, too much packing can result in search overhead, as follows. Consider a state S on an optimal path, which is \u201cdelayed\u201d from being sent to its owner because the processor which generated S is waiting to pack more states into the message containing S. In the meantime, the owner of S is expanding states which are worse than S and possibly sending those successors to fill up the open list of their owner processors, and so on.\nWe compared packing 10, 100, and 1000 states per message on the Commodity cluster using 64 cores, using the same benchmark instances in Table 6, except that the Airport17 instance was excluded because it could not be solved using 10 states per message. Using 10 states per message, the average runtime was 31.6% slower than when using 100 states per message, and 1.0% fewer nodes were expanded. Using 1000 states per message, the average runtime was 37.4% slower than when using 100 states per message, and 11.6% more nodes were expanded. Thus, while packing fewer states per message reduces search overhead, packing more states per message reduces communication overhead, and in this case, packing 100 states per message performs well by striking a balance between these two factors."}, {"heading": "6. Hash-Based Work Distribution vs. Random Work Distribution on a HPC Cluster", "text": "Kumar et al. [23] and Karp and Zhang [24] proposed a simple, random work distribution strategy for best-first search where generated nodes are sent to a random processor. While this is similar to HDA* in that a randomization mechanism is used to distribute work, the difference is that duplicate states are not necessarily sent to the same processor, since a state has no \u201cowner\u201d. Although duplicates are pruned locally at each processor, there is no global duplicate detection, so in the worst case, a state can be in the local open/closed lists of every single processor.\nWe evaluated this \u201cRandom\u201d work distribution strategy on our FastDownward based domain-independent planner on the HPC1 cluster. First, we attempted to compare HDA* and Random using 16 cores, 2GB per core. However, the Random strategy failed to solve any of the test problems in this configuration \u2013 all of the runs failed due to memory exhaustion. This indicated that, due to the lack of (global) duplicate detection, the random work distribution strategy requires much more RAM to solve our benchmarks. Hence we performed a comparison using 16 cores, 32GB per core (as with the experiments in Section 4.2.5, this configuration left 15 of the 16 cores on each processing node idle, in order to maximize memory available per core). The results are shown in Table 8. The execution time and number of nodes expanded by HDA* is more than an order of magnitude less than those of the random work distribution strategy. In fact, the random work distribution strategy is slower than the sequential A* algorithm because of large search overhead. The load balance is similar for both HDA* and random work distribution, indicating that hash-based work distribution is successfully distributing the work as evenly as a pure, randomized strategy. These results clearly demonstrate the benefit of using hash-based work distribution in order to perform global duplicate detection as well as load balancing."}, {"heading": "7. Comparison of HDA* vs. TDS on a Commodity Cluster", "text": "Transposition-Driven Scheduling (TDS) is a parallelization of IDA* with a distributed transposition table, where hash-based work distribution is used to map states to processors for both scheduling and transposition table checks [13]. TDS has been applied successfully to sliding tiles puzzles and Rubik\u2019s Cube [13], and has also been adapted for adversarial search [32, 33]. While TDS has not been applied to planning, recent work has shown that IDA* with a transposition table (IDA*+TT) is a successful search strategy for optimal, domain-independent planning [42] \u2013 on problems which can be solved by A*, the runtime of IDA*+TT is usually within a factor of 4 of A*, and IDA*+TT can eventually solve problems where A* exhausts memory. Therefore, we compared HDA* and TDS [13] for planning.\nThe experiments were performed on the Commodity cluster (see Table 1 for machine specs) using 64 cores, with a 20 minute time limit per instance. Both HDA* and TDS were run on 35 planning instances (the same instances as for the experiments in Section 4.4, Table 6).\nOur implementation of TDS uses a transposition table implementation based on [42], where the table entry replacement policy is a batch replacement policy which sorts entries according to access frequency and periodically frees 30% of the entries (preferring to keep most frequently accessed entries).9 We incorporated techniques to overcome higher latency in a lower-bandwidth network described in [13], such as their modification to the termination detection algorithm.\nOut of 35 instances, HDA* failed to solve one case (Blocks12-1) and TDS failed to solve 8 cases (Logistics00-8-1, Sokoban22, Sokoban26, Truck6, Gripper9, Freecell6, Rover6, Satellite7). Figure 8 directly compares HDA* and TDS on 26 instances solved by both algorithms, for which it is possible to compute the ratios of performance measures such as time, expanded states and evaluated states.10 HDA* is consistently faster, with a maximum speedup of about 65.\nThere are several differences between HDA* and TDS that could cause\n9Although replacement based on subtree size performed best in sequential search [42], we did not implement this policy because subtree size computation would require extensive message passing in parallel search.\n10Evaluated states are states which were not found in the Open/Closed set (in the case of HDA*), or not found in the transposition table (in the case of TDS).\nthis significant performance difference. Below, we first enumerate these differences, and then consider how they apply to our results.\nFirst, TDS is an iterative deepening strategy, so TDS will incur state reexpansion overhead, since many states will be reexpanded as the iteration f -bound increases.\nSecond, HDA* opens states in a (processor-local) best-first order, while TDS opens states in a last-in-first-out order in its local stack, subject to an iteration bound for the f -value. This difference in state expansion policy results in different search overheads for the two algorithms.\nThird, each processor in TDS needs memory not only for a transposition table and the merge-and-shrink abstraction (for planning), but also for a work stack. In the tested configuration, each processor is allocated 2GB of memory. 1.2GB is allocated to the transposition table and the abstraction. The remaining memory is reserved for the work stack.11 Therefore, there may be cases where there is sufficient aggregate memory in HDA* to solve an instance, but TDS (on the same system) exhausts the space allocated for its distributed transposition table. In such a situation, TDS applies a replacement policy to replace some of its transposition table entries with newly expanded states. While this seeks to maintain the most useful working set of states, it is possible that valuable entries (states) in the table are replaced, resulting in wasted work later when these states are revisited.\nFourth, TDS incurs synchronization overhead while all processors wait for the current iteration to end.\nStandard IDA* often generates states faster than A*, because successor generation can be implemented as an in-place incremental update of a data structure representing the current state. Thus, sequential IDA* can outperform A* even if IDA* searches less efficiently than A*. When a transposition table is added to IDA*, state generation incurs a significant overhead because the hash value of each generated state must be computed, and if there is a transposition table miss, some hashed representation of the state must be stored in the table. Furthermore, in TDS, every distributed transposition\n11While Romein et al.\u2019s stack does not exceed 1MB in their applications [13], our stack often used hundreds of megabytes of memory. Due to the much larger branching factors of our planning domains, compared to their domains (15-puzzle, Rubik\u2019s cube), TDS sends away more states when generating successors, causing a larger work queue. A possible future improvement is to combine Dutt and Mahapatra\u2019s technique in SEQ A* [6] with TDS and restrict initiating parallelism.\ntable access requires a hashed representation of the state to be generated and sent to the owner process. Thus, parallel IDA* no longer has an inherently faster state generation rate than parallel A*.\nWe now consider the contribution of each of the above differences. In Figure 8, the ratio of expanded states closely follows the runtime ratio.12 On the other hand, the ratio of evaluated states is close to 1 in most cases. This suggests that both algorithms explore a similar number of unique states. Furthermore, the similarity in the number of evaluated states shows that the transposition table used by TDS is large enough to fit most (unique) states generated during search, so the size of the transposition table did not limit the performance of TDS.\nThe difference in performance is mostly due to the re-expansion overhead incurred by TDS during iterative deepening. This conclusion is supported by the close correspondence between the runtime ratio and the state expansion ratio. Also, it is consistent with the behavior of sequential A* and IDA*. For example, on the Airport17 instance, both A* and IDA* evaluate 10,397,245 states. While A* expands only 7,126,967 states, IDA* expands 239,705,187\n12 The ratio of generated states, not shown to reduce clutter, is almost identical to the ratio of expanded states.\nstates, indicating that most of the states are reexpanded.13\nClearly, the number of re-expansions depends on the number of iterations performed by TDS. The exact number of iterations is unavailable when TDS runs out of time. An upper bound can be computed as u = c\u2217 \u2212 h(s0) + 1, where c\u2217 is the optimal cost (available due to HDA*), and h(s0) is the heuristic evaluation of the initial state. We observed that this was exactly the number of iterations in the cases where TDS succeeded within the time limit. More generally, in all cases that we observed, the cost threshold increase from one iteration to the next was 1. Thus, we hypothesize that the upper bound u is in fact quite an accurate estimate of the actual number of iterations. On the 35 instances considered in this experiment, the (estimated) number of iterations varies from 4 to 159, with an average value of 32.48. Data for all instances are available in Figure 9.\nThe synchronization overhead in TDS between iterations, as all processors wait for the iteration to end, does not appear to be significant in most cases, which is consistent with Romein et al.\u2019s results (otherwise, the runtime ratio would be consistently higher than the state expansion ratio).\n13 Extending TDS to backpropagate search results might reduce the very high reexpansion overhead in planning, which did not occur in Romein et al.\u2019s applications.\nNext we consider the speedups obtained by HDA* and TDS over their sequential counterparts, A* and IDA* with transposition table (IDA*+TT). The speedup of TDS vs IDA*+TT depends greatly on the amount of RAM memory available for the transposition table. The larger the (aggregate) transposition table, the faster TDS and IDA* are. While it is possible to achieve drastically large (sometimes super linear) speedup over IDA* [12, 13], the speedup diminishes as IDA* has access to larger and larger transposition tables. Figure 10 shows that, depending on the amount of RAM available to IDA*+TT, the speedup of TDS vs IDA*+TT can be either smaller or larger than speedup of HDA* vs A*. In this experiment, serial IDA*+TT was tested using 1.5GB of RAM and 26GB of RAM. TDS uses 1.2GB \u00d7 64 cores, or 76.8GB of aggregate RAM. The time limit of serial IDA*+TT was set to 1280 minutes (i.e., about 21.3 hours) per instance."}, {"heading": "7.1. A Simple, Hybrid Strategy Combining HDA* and TDS", "text": "The results above indicate that, for planning, HDA* significantly outperforms TDS on instances that can be solved within the memory available to HDA*. However, while HDA* will terminate and fail when memory is exhausted at any processor, TDS will not terminate if the local transposition table at any processor becomes full. Mechanisms which allow A*-based search to continue after memory is exhausted have been proposed. For ex-\nample, PRA* [5], has a state retraction mechanism which frees memory by retracting some states at the search frontier (however, as explained in Section 2), the PRA* retraction policy results in synchronization overhead). Determining an effective state retraction policy is a non-trivial extension to HDA* and an interesting direction for future work.\nConsider instances which are solvable by TDS (within a reasonable amount of time) but not by HDA*. Such problems are not very common. Given that HDA* and TDS explore a very similar set of unique states (as shown above), if a problem cannot be solved by HDA*, it indicates that the instance is quite difficult, and it is likely that TDS cannot solve the problem either within a reasonable time limit.\nThe instances that HDA* failed to solve, as well as the amount of time HDA* executed before exhausting RAM) are listed below for 16, 32, and 64 cores (all with 2GB per core):\n\u2022 Failed with 16 cores: Freecell6 (269 sec), Rover6 (453 sec), Satellite7 (573 sec), Sokoban26 (176 sec), Blocks11-1 (165 sec), Blocks12-1 (148 sec), Logistics00-8-1 (416 sec);\n\u2022 Failed with 32 cores: Freecell6 (303 sec), Satellite7 (722 sec), Sokoban26 (190 sec), Blocks11-1 (190 sec), Blocks12-1 (156 sec), Logistics00-8-1 (399 sec);\n\u2022 Failed with 64 cores: Blocks12-1 (189 sec).\nNext, we attempted to solve these instances using TDS with 16, 32, and 64 cores (all with 2GB/core), with an extended time limit of 1.5 hours for 32 and 64 cores and 3 hours for 16 cores per instance. The runtimes on the instances which were solved by TDS, but not by HDA* are as follows (TDS failed to find a solution within the time limit on the other instances):\n\u2022 16 cores: Blocks11-1 (2928 sec), Blocks12-1 (4139 sec);\n\u2022 32 cores: Freecell6 (4350 sec), Blocks11-1 (974 sec), Blocks12-1 (1874 sec);\n\u2022 64 cores: Blocks12-1 (838 sec).\nThus, even with a 1.5 or 3 hours per instance, many instances that cannot be solved by HDA* cannot be solved by TDS. In addition, on the instances\nwhere HDA* fails and TDS succeeds, HDA* fails relatively quickly compared to the time required by TDS to solve the problem. For example, with 32 cores HDA* exhausts memory in 156 seconds in solving Blocks12-1, and although TDS can solve this problem with 32 cores, it required 1874 seconds.\nThis suggests a very simple, hybrid approach which combines the speed of HDA* and the ability of TDS to eventually solve difficult problems without running out of memory. First, a slightly modified version of HDA* is applied, which keeps track of the lowest f -cost in the OPEN list is recorded at every processor. If HDA* finds a solution, then it is returned. However, if HDA* exhausts memory at any processor, then it terminates with failure, and also returns the minimum frontier value among all of the processors, fmin. Then, we apply TDS, except that instead of setting the initial iteration bound to the heuristic value at the initial state, we start with fmin as the lower bound.\nAccording to the data above, if the HDA* phase succeeds, this hybrid procedure will succeed significantly faster than TDS would, and if it fails, it will fail relatively quickly. Upon failure, the hybrid starts TDS with a iteration bound fmin (skipping some wasteful iterations).\nOn instances solvable by HDA*, the hybrid runtime will be the same as for HDA*. On instances where HDA* fails but TDS would eventually succeed, the hybrid runtime would be similar to the runtime of TDS. The time spent in the failed run of HDA* should only be a small fraction of the time required to eventually solve the instance; furthermore, using the fmin initial bound for TDS is expected to offset some of the time spent in the failed HDA* run by eliminating some of wasted expansions in TDS.\nTable 9 shows the results of applying this hybrid to the instances listed above which caused HDA* to fail. In most cases, the runtimes for the hybrid are comparable to the runtimes for TDS.This shows that the hybrid successfully combines the strengths of both HDA* and TDS while avoiding their disadvantages, and does so while avoiding excessive hybridization overhead."}, {"heading": "8. Related Work", "text": "We have discussed the related work that parallelizes search by partitioning the search space in Section 2. In this section, we review related work on parallel search in model checking. In the second half, we also review other approaches to parallelizing search algorithms.\nWhile this paper focuses on standard AI search domains including domain independent planning and the sliding tile puzzle, distributed search,\nincluding hash-based work distribution, has also been studied extensively by the parallel model checking community. Parallel Mur\u03d5 [43, 44] addresses verification tasks that involve exhaustively enumerating all reachable states in a state space. Similarly to HDA* and other work described in Section 2 [5, 6], Parallel Mur\u03d5 implements a hash-based work distribution schema where each state is assigned to a unique owner processor. Kumar and Mercer [45] present a load balancing technique as an alternative to the hash-based work distribution implemented in Mur\u03d5. The Eddy Murphi model checker [46] specializes processors\u2019 tasks, defining two threads for each processing node. The worker thread performs state processing (e.g., state expansion), whereas the other thread handles communication (e.g., sending and receiving states).\nLerda and Sisto parallelized the SPIN model checker to increase the availability of memory resources [47]. Similar to hash-based distribution, states are assigned to an owner processing node, and get expanded at their owner node. However, instead of using a hash function to determine the owner processor, only one state variable is taken into account. This is done to increase the likelihood that the processor where a state is generated is identical to the owner processor. Holzmann and Bos\u0302nac\u0302ki [48] introduce an extension to SPIN to multicore, shared memory machines. Garavel et al. [49] use hashbased work distribution to convert an implicitly defined model-checking state space into an explicit file representation. Symbolic parallel model checking\nhas been addressed in [50]. Thus, hash-based work distribution and related techniques for distributed search have been widely studied for parallel model checking. There are several important differences between previous work in model checking and this paper. First, this paper focuses on parallel A*. In model checking, there is usually no heuristic evaluation function, so depth-first search and breadthfirst search is used instead of best-first strategies such as A*.\nSecond, reachability analysis in model checking (e.g., [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality. Search overhead is not an issue because both serial and parallel solvers will expand all reachable states exactly once. In contrast, we specifically address the problem of finding an optimal path, a significant constraint which introduces the issue of search efficiency because distributed A* (including HDA*) searches many nodes with the f -cost greater than or equal to the optimal cost, as detailed in Section 4.2.2; furthermore, node re-expansions in parallel A* can introduce search overhead. This is the first paper to analyze search overhead.\nFinally, while previous work in model checking has used up to 256 processors [51], our work presents the largest scale experiments with hash-based work distribution to date, showing that hash-based work distribution can scale efficiently relative to pmin even for up to 2400 processors.\nKobayashi et al. [31] have applied HDA* to multiple sequence alignment (MSA). The non-unit transition costs lead to a higher rate of re-expansions, increasing the search overhead. To address this, the authors introduced a work distribution strategy that exploits the structure that MSA and potentially other problems exhibit, replacing the Zobrist-based, global hashing scheme. In contrast, in this work, our focus is to analyze HDA* at length, hoping to establish HDA* as a simple and scalable baseline algorithm for parallel optimal search.\nOne alternative to partitioning the search space among processors is to parallelize the computation done during the processing of a single search node (c.f., [52, 53]). The Operator Distribution Method for parallel Planning (ODMP) [54] parallelizes the computation at each node. In ODMP, there is a single, controlling thread, and several planning threads. The controlling thread is responsible for initializing and maintaining the current search state. At each step of the controlling thread main loop, it generates the applicable operators, inserts them in an operator pool, and activates the planning threads. Each planning thread independently takes an operator from this\nshared operator pool, computes the grounded actions, generates the resulting states, evaluates the states with the heuristic function, and stores the new state and its heuristic value in a global agenda data structure. After the operator pool is empty, the controlling thread extracts the best new state from the global agenda, assigning it to the new, current state.\nAnother approach is to run a set of different search algorithms in parallel. Each process executes mostly independently, with periodic communication of information between processors. This approach, which is a parallel version of an algorithm portfolio [55], seeks to exploit the long-tailed runtime distribution behavior encountered in search algorithms [56] by using different versions of search algorithms to search different (potentially overlapping) portions of the search space. An example of this is the ManySAT solver [57], which executes a different version of a DPLL-based backtracking SAT solver on each processor and periodically shares lemmas among the processes.\nA third approach is parallel-window search for IDA* [58], where each processor searches from the same root node, but is assigned a different bound \u2013 that is, each processor is assigned a different, independent iteration of IDA*. When a processor finishes an iteration, it is assigned the next highest bound which has not yet been assigned to a processor.\nVidal et al. [59] propose a multicore version of the KBFS algorithm [60]. In this approach, each thread expands one node from the Open list at a time. As each expansion step requires operations on the Open and Closed list, synchronization is needed to ensure that only one thread at a time can perform such operations. Experiments are reported for satisficing planning, as opposed to our work, which is focused on optimal planning.\nThe best parallelization strategy for a search algorithm depends on properties of the search space, as well as the parallel architecture on which the search algorithm is executed. The EUREKA system [61] used machine learning to automatically configure parallel IDA* for various problems (including nonlinear planning) and machine architectures.\nNiewiadomski et al. [40] propose PFA*-DDD, a parallel version of Frontier A* with Delayed Duplicate Detection. While achieving very good speedup efficiency (often superlinear), PFA*-DDD is limited to returning only the cost of a path from start to target, not an actual path. While divideand-conquer (DC) can be used to reconstruct a path (as in sequential frontier search), parallel DC poses non-trivial design issues that need to be addressed in future work. See the original paper for a discussion."}, {"heading": "9. Discussion and Conclusion", "text": "This paper investigated the use of hash-based work distribution to parallelize A* for hard graph search problems such as domain-independent planning. We implemented Hash-Distributed A*, a simple, scalable parallelization of A*. The key idea, first used in Parallel Retracting A* [5], is to distribute work according to the hash value for generated states. HDA* is a simple implementation of this idea, which, to our knowledge, has not been previously evaluated in depth. Unlike PRA*, which was a synchronous algorithm due to its retraction mechanism, HDA* operates completely asynchronously. Also, unlike previous work such as PRA* and GOHA [6], which implemented hash-based work distribution on variants of A*, HDA* is a straightforward implementation of hash-based work distribution for standard A*. We evaluated HDA* as a replacement for the sequential A* search engine for a state-of-the-art, optimal sequential planner, Fast Downward [3]. We also evaluated HDA* on the 24-puzzle domain by implementing a parallel solver with a disjoint pattern database heuristic [11].\nOur experimental evaluation shows that HDA* scales well in several parallel hardware configurations, including a single shared memory machine, high-performance computing clusters using Infiniband interconnects, and a local cluster with 1Gb(x2) Ethernet network.\nWhile HDA* is naturally suited for distributed memory parallel search on a distributed memory cluster of machines, we have shown that HDA* also achieves reasonable speedup on a single, shared memory machine with up to 8 cores, yielding speedups of 3.8-6.6 on 8 cores. Burns et al. have recently proposed PBNF, a shared memory, parallel best-first search algorithm, and showed that PBNF outperforms HDA* on planning in shared memory environments [62]. On the other hand, while HDA* is not necessarily the fastest algorithm on a shared memory environment, HDA* is simpler than PBNF (which is based on structured duplicate detection techniques). Furthermore, HDA* is suited for larger, distributed memory clusters, while PBNF is designed for shared memory machines.\nEvaluation of HDA* on a large high-performance cluster using up to 2400 cores showed that HDA* scales well relative to pmin, the minimum number of processors (+memory) that can solve the problem. In our benchmarks pmin ranged up to 600 cores. When up to 8 times pmin processors are used, HDA* scales up relatively efficiently. However, as additional processors are used, the search overhead (wasteful node expansions) increases, resulting in degraded\nefficiency as the number of processors used greatly exceeds pmin. Comparison with a randomized work distribution strategy which performs load balancing but no duplicate detection [23, 24] showed the simple hash-based duplicate detection mechanism is essential to the performance of HDA*. The scaling behavior of HDA* on a small commodity cluster with 1Gb(x2) Ethernet shows similar trends as on the HPC cluster, except that going from 8 cores (1 processor node) to 16 cores (2 processor nodes) is inefficient. Increasing the number of processing nodes beyond 2 scaled smoothly. Qualitatively similar scaling results were obtained for the 24-puzzle. However, because of the faster node processing time for the 24-puzzle compared to domainindependent planning, scaling degraded faster for the 24-puzzle.\nMuch of the previous literature on parallel search has focused on maximizing the usage of all available CPU cores. Our results indicate that this approach can be suboptimal. In fact, the best performance can be obtained by keeping many of the available processors idle. This is a result of the prevalent architecture in current clusters, which are composed of commodity, multicore shared-memory processors connected via a high-speed network. There are two distinct factors which can lead to suboptimal performance when CPU core usage is maximized. First, as we showed in Section 4.2.5, contention for local memory on each machine becomes a bottleneck, so using all of the available cores on a machine results in performance degradation. Automatically determining the optimal number of cores to use per processing node in order to obtain the best tradeoff between computation and memory bandwidth is an area for future work. Second, in parallel best-first search, there is a tradeoff between the number of cores used per machine, and the fragmentation of the local open list. Fragmentation of the open list results in search overhead, because the local open lists are not aware of the current global best f -values. Developing mechanisms which seek to reduce this search overhead is an area for future work.\nOur comparison of HDA* with TDS showed that when HDA* had sufficient memory to solve a problem, it significantly outperformed TDS on our benchmark planning instances. We showed that this performance gap was due to the reexpansion of states by TDS. We observed that on problem instances where HDA* fails due to memory exhaustion, TDS either fails due to exceeding the time limit, or TDS eventually solves the instance but requires a long time. Thus, we investigated a simple, hybrid strategy which first executes HDA* until either the problem is solved or until memory is exhausted. If HDA* fails, execute TDS starting with a more informed initial\nthreshold, provided by the failed HDA* run. We showed that this hybrid strategy effectively combines the advantages of both HDA* and TDS.\nOne particularly attractive feature of HDA* is its simplicity. Work distribution and duplicate detection is done by a simple hash function, and there is no complex load balancing mechanism. All communications are asynchronous. As far as we know, HDA* is the simplest parallelization for A* which achieves both load balancing and duplicate detection. Simplicity is very important for parallel algorithms, particularly for an algorithm that runs on multiple machines, as debugging a multi-machine, multicore algorithm is extremely challenging. In some preliminary efforts to implement a distributed memory, work-stealing algorithm, we have found that it is significantly more difficult to implement it correctly and efficiently compared to HDA*. Furthermore, using the standard MPI message passing library, the same HDA* code can be recompiled and executed on a wide range of shared memory and distributed clusters, making it simple to port HDA*. This is in contrast to more complex algorithms which depend on specific architectural features such as shared memory.\nWhile our investigation of HDA* scaling reveals that there is clearly room for improvement when the amount of parallel resources used greatly exceeds the minimal required resources (i.e., p > 8\u00d7pmin), we have shown that HDA* scales reasonably well across a wide range of parallel platforms, including a single multicore machine, a commodity cluster, and two high-performance computing clusters. Combined with its simplicity and portability, this suggests that HDA* should be considered a default, baseline algorithm for parallel best-first search.\nThis paper focused on the search phase of problem solving. Parallelization of the heuristic construction phase (e.g., computation of the abstraction heuristic table [3] and pattern database generation [11]) is another area for future work. A related avenue for future work is a more effective use of memory for heuristic tables. Our current implementation of HDA* uses a single process per core. When executed on one or more (shared memory) multicore machines, our implementation of HDA* executes as a set of independent processes without sharing any memory resources among cores that are on the same machine. This means that the memory used for an abstraction heuristic in planning or for a pattern database in 24-puzzle is unnecessarily replicated n times on an n-core machine. We are currently investigating a hybrid, distributed/shared memory implementation of HDA* which eliminates this inefficiency. One possible approach is to distribute work among\nmachines using hash-based distribution, but within a single machine incorporate techniques such as speculative expansion that have been shown to scale well on a shared memory environment [9].\nFinally, although efficiency (with respect to runtime) is an important characteristic for parallel search, the ability to solve difficult problems that cannot be solved using fewer resources (because A* exhausts RAM, and other methods such as IDA* on a single machine take too long) may be the most compelling reason to consider large scale parallel search on distributed memory clusters. Using up to 2400 cores and 10.5 terabytes of aggregate RAM, HDA* was able to compute optimal solutions to larger planning benchmark problems than was previously possible on a single machine. The increasing pervasiveness of massive \u201cutility computing\u201d resources (such as cloud computing services) means that further work is needed to develop and evaluate algorithms that can scale even further, to tens of thousands of cores and petabytes of RAM. Furthermore, a focus on solving problems using utility computing services which incur monetary usage costs requires the development of cost-effective utilization of resources. We have recently analyzed an iterative resource allocation policy to address this [41]."}, {"heading": "Acknowledgments", "text": "This research is supported by the JSPS Compview GCOE, the JST PRESTO program, and JSPS grants-in-aid for research. Thanks to Malte Helmert for providing the Fast Downward code, and to Rich Korf for providing his IDA* 24-puzzle solver, pattern database code, and puzzle instances. We thank the anonymous reviewers for their feedback.\n[1] P. Haslum, H. Geffner, Admissible heuristics for optimal planning, in: Proceedings of the Fifth International Conference on AI Planning and Scheduling, 2000, pp. 140\u2013149.\n[2] S. Edelkamp, Planning with pattern databases, in: Proceedings of the European Conference on Planning ECP-01, 2001, pp. 13\u201334.\n[3] M. Helmert, P. Haslum, J. Hoffmann, Flexible abstraction heuristics for optimal sequential planning, in: Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling ICAPS-07, 2007, pp. 176\u2013183.\n[4] P. Hart, N. Nilsson, B. Raphael, A formal basis for the heuristic determination of minimum cost paths, IEEE Transactions on Systems Science and Cybernetics 4 (2) (1968) 100\u2013107.\n[5] M. Evett, J. Hendler, A. Mahanti, D. Nau, PRA\u2217: Massively parallel heuristic search, Journal of Parallel and Distributed Computing 25 (2) (1995) 133\u2013143.\n[6] N. Mahapatra, S. Dutt, Scalable global and local hashing strategies for duplicate pruning in parallel A* graph search, IEEE Transactions on Parallel and Distributed Systems 8 (7) (1997) 738\u2013756.\n[7] R. Korf, P. Schultze, Large-scale parallel breadth-first search, in: Proceedings of the National Conference on Artificial Intelligence (AAAI), 2005, pp. 1380\u20131386.\n[8] R. Zhou, E. Hansen, Parallel structured duplicate detection, in: Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI), 2007, pp. 1217\u20131223.\n[9] E. Burns, S. Lemons, R. Zhou, W. Ruml, Best-first heuristic search for multi-core machines, in: Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence IJCAI-09, 2009, pp. 449\u2013455.\n[10] M. Snir, W. Gropp, MPI: the complete reference, MIT Press, 1998.\n[11] R. E. Korf, A. Felner, Disjoint pattern database heuristics, Artificial Intelligence 134 (1-2) (2002) 9\u201322.\n[12] J. W. Romein, A. Plaat, H. E. Bal, J. Schaeffer, Transposition table driven work scheduling in distributed search, in: Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI), 1999, pp. 725\u2013731.\n[13] J. W. Romein, H. E. Bal, J. Schaeffer, A. Plaat, A performance analysis of transposition-table-driven work scheduling in distributed search, IEEE Transactions on Parallel and Distributed Systems 13 (5) (2002) 447\u2013459.\n[14] V. N. Rao, V. Kumar, Parallel depth-first search on multiprocessors part I: Implementation, International Journal of Parallel Programming 16 (6) (1987) 479\u2013499.\n[15] R. Feldmann, Spielbaumsuche auf massiv parallelen systemen, Ph.D. thesis, University of Paderborn (1993).\n[16] M. Frigo, C. E. Leiserson, K. H. Randall, The implementation of the Cilk-5 multithreaded language, in: Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 1998, pp. 212\u2013223.\n[17] C. Powley, C. Ferguson, R. Korf, Depth-first heuristic search on a SIMD machine, Artificial Intelligence 60 (1993) 199\u2013242.\n[18] A. Mahanti, C. Daniels, A SIMD approach to parallel heuristic search, Artificial Intelligence 60 (1993) 243\u2013282.\n[19] R. Korf, Best-first frontier search with delayed duplicate detection, in: Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI), 2004, pp. 650\u2013657.\n[20] R. Korf, Linear-time disk-based implicit graph search, Journal of the Association for Computing Machinery 55 (6).\n[21] R. Zhou, E. Hansen, Structured duplicate detection in external-memory graph search, in: Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI), 2004, pp. 683\u2013689.\n[22] S. Edelkamp, S. Jabbar, Cost-optimal external planning, in: Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), 2006, pp. 821\u2013826.\n[23] V. Kumar, K. Ramesh, V. N. Rao, Parallel best-first search of statespace graphs: A summary of results, in: Proceedings of the 7th National Conference on Artificial Intelligence AAAI-88, 1988, pp. 122\u2013127.\n[24] R. Karp, Y. Zhang, A randomized parallel branch-and-bound procedure, in: Proceedings of the 20th ACM Symposium on Theory of Computing (STOC), 1988, pp. 290\u2013300.\n[25] R. Karp, Y. Zhang, Randomized parallel algorithms for backtrack search and branch-and-bound computation, Journal of the Association for Computing Machinery 40 (3) (1993) 765\u2013789.\n[26] S. Dutt, N. Mahapatra, Scalable load balancing strategies for parallel A* algorithms, Journal of Parallel and Distributed Computing 22 (1994) 488\u2013505.\n[27] R. E. Korf, W. Zhang, Divide-and-conquer frontier search applied to optimal sequence alignment, in: Proceedings of the 17th National Conference on Artificial Intelligence (AAAI), 2000, pp. 910\u2013916.\n[28] R. Zhou, E. Hansen, Domain-independent structured duplicate detection, in: Proceedings of the 21st National Conference on Artificial Intelligence (AAAI), 2006, pp. 683\u2013688.\n[29] P. Chakrabarti, S. Ghose, A. Acharya, S. de Sarkar, Heuristic search in restricted memory, Artificial Intelligence 41 (2) (1989) 197\u2013221.\n[30] S. Russell, Efficient memory-bounded search methods, in: Proceedings of the 10th European Conference on Artificial Intelligence (ECAI-92), 1992, pp. 1\u20135.\n[31] Y. Kobayashi, A. Kishimoto, O. Watanabe, Evaluations of Hash Distributed A* in optimal sequence alignment, in: Proceedings of the 22nd International Joint Conference on Artificial Intelligence, 2011, pp. 584\u2013 590.\n[32] A. Kishimoto, J. Schaeffer, Distributed game-tree search using transposition table driven work scheduling, in: Proceedings of the 31st International Conference on Parallel Processing, 2002, pp. 323\u2013330.\n[33] J. W. Romein, H. E. Bal, Solving Awari with Parallel Retrograde Analysis, IEEE Computer 36 (10) (2003) 26\u201333.\n[34] K. Yoshizoe, A. Kishimoto, T. Kaneko, H. Yoshimoto, Y. Ishikawa, Scalable distributed Monte-Carlo Tree Search, in: Proceedings of the 4th Symposium on Combinatorial Search SOCS-11, 2011, pp. 180\u2013187.\n[35] A. Kishimoto, A. Fukunaga, A. Botea, Scalable, parallel best-first search for optimal sequential planning, in: Proceedings of the International Conference on Automated Planning and Scheduling, 2009, pp. 201\u2013208.\n[36] F. Mattern, Algorithms for distributed termination detection, Distributed Computing 2 (3) (1987) 161\u2013175.\n[37] A. L. Zobrist, A new hashing method with applications for game playing, Tech. rep., Department of Computer Science, University of Wisconsin, Madison, reprinted in International Computer Chess Association Journal, 13(2):169-173, 1990 (1970).\n[38] C. Ba\u0308ckstro\u0308m, B. Nebel, Complexity results for SAS+ planning, Computational Intelligence 11 (4) (1995) 625\u2013655.\n[39] A. Botea, M. Enzenberger, M. Mu\u0308ller, J. Schaeffer, Macro-FF: Improving AI planning with automatically learned Macro-operators, Journal of Artificial Intelligence Research 24 (2005) 581\u2013621.\n[40] R. Niewiadomski, J. N. Amaral, R. C. Holte, Sequential and parallel algorithms for frontier A* with delayed duplicate detection, in: Proceedings of the 21st National Conference on Artificial Intelligence (AAAI), 2006, pp. 1039\u20131044.\n[41] A. Fukunaga, A. Kishimoto, A. Botea, Iterative resource allocation for memory intensive parallel search algorithms on clouds, grids, and shared clusters, in: Proceedings of the 26th AAAI Conference on Artificial Intelligence, 2012, pp. 478\u2013485.\n[42] Y. Akagi, A. Kishimoto, A. Fukunaga, On transposition tables for singleagent search and planning: Summary of results, in: Proceedings of the 3rd Symposium on Combinatorial Search (SOCS), 2010, pp. 1\u20138.\n[43] U. Stern, D. L. Dill, Parallelizing the Murphi verifier, in: Proceedings of the 9th International Conference on Computed Aided Verification, 1997, pp. 256\u2013278.\n[44] U. Stern, D. L. Dill, Parallelizing the Murphi verifier, Formal Methods in System Design 18 (2) (2001) 117\u2013129.\n[45] R. Kumar, E. G. Mercer, Load balancing parallel explicit state model checking, Electronic Notes in Theoretical Computer Science 128.\n[46] I. Melatti, R. Palmer, G. Sawaya, Y. Yang, R. M. Kirby, G. Gopalakrishnan, Parallel and distributed model checking in Eddy, International Journal on Software Tools for Technology Transfer 11 (1) (2009) 13\u201325.\n[47] F. Lerda, R. Sisto, Distributed-memory model checking with SPIN, in: Theoretical and Practical Aspects of SPIN Model Checking, 5th and 6th International SPIN Workshops, Vol. 1680 of Lecture Notes in Computer Science, 1999, pp. 22\u201339.\n[48] G. J. Holzmann, D. Bos\u0302nac\u0302ki, The design of a multicore extension of the SPIN model checker, IEEE Transactions on Software Engineering 33 (10) (2007) 659\u2013674.\n[49] H. Garavel, R. Mateescu, I. M. Smarandache, Parallel state space construction for model-checking, in: Proceedings of the 8th International SPIN Workshop, 2001, pp. 217\u2013234.\n[50] T. Heyman, D. Geist, O. Grumberg, A. Schuster, Achieving scalability in parallel reachability analysis of very large circuits, in: Proceedings 12th International Conference on Computer Aided Verification, 2000, pp. 20\u201335.\n[51] K. Verstoep, H. Bal, J. Barnat, L. Brim, Efficient large-scale model checking, in: 23rd IEEE International Parallel and Distributed Processing Symposium (IPDPS 2009), IEEE, 2009.\n[52] M. Campbell, J. Hoane, F. Hsu, Deep Blue, Artificial Intelligence 134 (1- 2) (2002) 57\u201383.\n[53] T. Cazenave, N. Jouandeau, On the parallelization of UCT, in: H. van den Herik et al. (Ed.), Proceedings of Computers and Games CG-08, Vol. 5131 of Lecture Notes in Computer Science, Springer, 2008, pp. 72\u201380.\n[54] D. Vrakas, I. Refanidis, I. Vlahavas, Parallel planning via the distribution of operators, Journal of Experimental and Theoretical Artificial Intelligence 13 (3) (2001) 211\u2013226.\n[55] B. Huberman, R. Lukose, T. Hogg, An economics approach to hard computational problems, Science 275 (5296) (1997) 51\u201354.\n[56] C. Gomes, B. Selman, N. Crato, H. Kautz, Heavy-tailed phenomena in satisfiability and constraint satisfaction problems, Journal of Automated Reasoning 24 (1-2) (2000) 67\u2013100.\n[57] Y. Hamadi, S. Jabbour, L. Sais, ManySAT: a parallel SAT solver, Journal on Satisfiability, Boolean Modeling and Computation 6 (2009) 245\u2013 262.\n[58] C. Powley, R. Korf, Single-agent parallel window search, IEEE Transactions on Pattern Analysis and Machine Intelligence 13 (5) (1991) 466\u2013 477.\n[59] V. Vidal, L. Bordeaux, Y. Hamadi, Adaptive k-parallel best-first search: A simple but efficient algorithm for multi-core domain-independent planning, in: Proceedings of the 3rd Symposium on Combinatorial Search (SOCS\u201910), 2010.\n[60] A. Felner, S. Kraus, R. E. Korf, Kbfs: K-best-first search, Annals of Mathematics and Artificial Intelligence 39 (2003) 19\u201339.\n[61] D. Cook, R. Varnell, Adaptive parallel iterative deepening search, Journal of Artificial Intelligence Research 9 (1998) 139\u2013166.\n[62] E. Burns, S. Lemons, W. Ruml, R. Zhou, Best-first heuristic search for multicore machines, Journal of Artificial Intelligence Research (JAIR) 39 (2010) 689\u2013743."}], "references": [{"title": "Admissible heuristics for optimal planning", "author": ["P. Haslum", "H. Geffner"], "venue": "in: Proceedings of the Fifth International Conference on AI Planning and Scheduling", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Planning with pattern databases", "author": ["S. Edelkamp"], "venue": "in: Proceedings of the European Conference on Planning ECP-01", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "in: Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling ICAPS-07", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P. Hart", "N. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics 4 (2) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1968}, {"title": "PRA\u2217: Massively parallel heuristic search", "author": ["M. Evett", "J. Hendler", "A. Mahanti", "D. Nau"], "venue": "Journal of Parallel and Distributed Computing 25 (2) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Scalable global and local hashing strategies for duplicate pruning in parallel A* graph search", "author": ["N. Mahapatra", "S. Dutt"], "venue": "IEEE Transactions on Parallel and Distributed Systems 8 (7) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Large-scale parallel breadth-first search", "author": ["R. Korf", "P. Schultze"], "venue": "in: Proceedings of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Parallel structured duplicate detection", "author": ["R. Zhou", "E. Hansen"], "venue": "in: Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Best-first heuristic search for multi-core machines", "author": ["E. Burns", "S. Lemons", "R. Zhou", "W. Ruml"], "venue": "in: Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence IJCAI-09", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "MPI: the complete reference", "author": ["M. Snir", "W. Gropp"], "venue": "MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Disjoint pattern database heuristics", "author": ["R.E. Korf", "A. Felner"], "venue": "Artificial Intelligence 134 (1-2) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Transposition table driven work scheduling in distributed search", "author": ["J.W. Romein", "A. Plaat", "H.E. Bal", "J. Schaeffer"], "venue": "in: Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "A performance analysis of transposition-table-driven work scheduling in distributed search", "author": ["J.W. Romein", "H.E. Bal", "J. Schaeffer", "A. Plaat"], "venue": "IEEE Transactions on Parallel and Distributed Systems 13 (5) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Parallel depth-first search on multiprocessors part I: Implementation", "author": ["V.N. Rao", "V. Kumar"], "venue": "International Journal of Parallel Programming 16 (6) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1987}, {"title": "Spielbaumsuche auf massiv parallelen systemen", "author": ["R. Feldmann"], "venue": "Ph.D. thesis, University of Paderborn ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "The implementation of the Cilk-5 multithreaded language", "author": ["M. Frigo", "C.E. Leiserson", "K.H. Randall"], "venue": "in: Proceedings of the ACM SIG- PLAN Conference on Programming Language Design and Implementation (PLDI)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Depth-first heuristic search on a SIMD machine", "author": ["C. Powley", "C. Ferguson", "R. Korf"], "venue": "Artificial Intelligence 60 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "A SIMD approach to parallel heuristic search", "author": ["A. Mahanti", "C. Daniels"], "venue": "Artificial Intelligence 60 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Best-first frontier search with delayed duplicate detection", "author": ["R. Korf"], "venue": "in: Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Structured duplicate detection in external-memory graph search", "author": ["R. Zhou", "E. Hansen"], "venue": "in: Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Cost-optimal external planning", "author": ["S. Edelkamp", "S. Jabbar"], "venue": "in: Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel best-first search of statespace graphs: A summary of results", "author": ["V. Kumar", "K. Ramesh", "V.N. Rao"], "venue": "in: Proceedings of the 7th National Conference on Artificial Intelligence AAAI-88", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "A randomized parallel branch-and-bound procedure", "author": ["R. Karp", "Y. Zhang"], "venue": "in: Proceedings of the 20th ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1988}, {"title": "Randomized parallel algorithms for backtrack search and branch-and-bound computation", "author": ["R. Karp", "Y. Zhang"], "venue": "Journal of the Association for Computing Machinery 40 (3) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Scalable load balancing strategies for parallel A* algorithms", "author": ["S. Dutt", "N. Mahapatra"], "venue": "Journal of Parallel and Distributed Computing 22 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Divide-and-conquer frontier search applied to optimal sequence alignment", "author": ["R.E. Korf", "W. Zhang"], "venue": "in: Proceedings of the 17th National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Domain-independent structured duplicate detection", "author": ["R. Zhou", "E. Hansen"], "venue": "in: Proceedings of the 21st National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "S", "author": ["P. Chakrabarti", "S. Ghose", "A. Acharya"], "venue": "de Sarkar, Heuristic search in restricted memory, Artificial Intelligence 41 (2) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1989}, {"title": "Efficient memory-bounded search methods", "author": ["S. Russell"], "venue": "in: Proceedings of the 10th European Conference on Artificial Intelligence (ECAI-92)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Evaluations of Hash Distributed A* in optimal sequence alignment", "author": ["Y. Kobayashi", "A. Kishimoto", "O. Watanabe"], "venue": "in: Proceedings of the 22nd International Joint Conference on Artificial Intelligence", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed game-tree search using transposition table driven work scheduling", "author": ["A. Kishimoto", "J. Schaeffer"], "venue": "in: Proceedings of the 31st International Conference on Parallel Processing", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Solving Awari with Parallel Retrograde Analysis", "author": ["J.W. Romein", "H.E. Bal"], "venue": "IEEE Computer 36 (10) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Scalable distributed Monte-Carlo Tree Search", "author": ["K. Yoshizoe", "A. Kishimoto", "T. Kaneko", "H. Yoshimoto", "Y. Ishikawa"], "venue": "in: Proceedings of the 4th Symposium on Combinatorial Search SOCS-11", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable", "author": ["A. Kishimoto", "A. Fukunaga", "A. Botea"], "venue": "parallel best-first search for optimal sequential planning, in: Proceedings of the International Conference on Automated Planning and Scheduling", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for distributed termination detection", "author": ["F. Mattern"], "venue": "Distributed Computing 2 (3) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1987}, {"title": "A new hashing method with applications for game playing", "author": ["A.L. Zobrist"], "venue": "Tech. rep., Department of Computer Science, University of Wisconsin, Madison, reprinted in International Computer Chess Association Journal, 13(2):169-173, 1990 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1970}, {"title": "Complexity results for SAS planning", "author": ["C. B\u00e4ckstr\u00f6m", "B. Nebel"], "venue": "Computational Intelligence 11 (4) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1995}, {"title": "Macro-FF: Improving AI planning with automatically learned Macro-operators", "author": ["A. Botea", "M. Enzenberger", "M. M\u00fcller", "J. Schaeffer"], "venue": "Journal of Artificial Intelligence Research 24 ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Sequential and parallel algorithms for frontier A* with delayed duplicate detection", "author": ["R. Niewiadomski", "J.N. Amaral", "R.C. Holte"], "venue": "in: Proceedings of the 21st National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Iterative resource allocation for memory intensive parallel search algorithms on clouds", "author": ["A. Fukunaga", "A. Kishimoto", "A. Botea"], "venue": "grids, and shared clusters, in: Proceedings of the 26th AAAI Conference on Artificial Intelligence", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "On transposition tables for singleagent search and planning: Summary of results", "author": ["Y. Akagi", "A. Kishimoto", "A. Fukunaga"], "venue": "in: Proceedings of the 3rd Symposium on Combinatorial Search (SOCS)", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelizing the Murphi verifier", "author": ["U. Stern", "D.L. Dill"], "venue": "in: Proceedings of the 9th International Conference on Computed Aided Verification", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1997}, {"title": "Parallelizing the Murphi verifier", "author": ["U. Stern", "D.L. Dill"], "venue": "Formal Methods in System Design 18 (2) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Parallel and distributed model checking in Eddy", "author": ["I. Melatti", "R. Palmer", "G. Sawaya", "Y. Yang", "R.M. Kirby", "G. Gopalakrishnan"], "venue": "International Journal on Software Tools for Technology Transfer 11 (1) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed-memory model checking with SPIN", "author": ["F. Lerda", "R. Sisto"], "venue": "in: Theoretical and Practical Aspects of SPIN Model Checking, 5th and 6th International SPIN Workshops, Vol. 1680 of Lecture Notes in Computer Science", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1999}, {"title": "The design of a multicore extension of the SPIN model checker", "author": ["G.J. Holzmann", "D. Bo\u015dna\u0109ki"], "venue": "IEEE Transactions on Software Engineering 33 (10) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel state space construction for model-checking", "author": ["H. Garavel", "R. Mateescu", "I.M. Smarandache"], "venue": "in: Proceedings of the 8th International SPIN Workshop", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2001}, {"title": "Achieving scalability in parallel reachability analysis of very large circuits", "author": ["T. Heyman", "D. Geist", "O. Grumberg", "A. Schuster"], "venue": "in: Proceedings 12th International Conference on Computer Aided Verification", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient large-scale model checking", "author": ["K. Verstoep", "H. Bal", "J. Barnat", "L. Brim"], "venue": "in: 23rd IEEE International Parallel and Distributed Processing Symposium ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep Blue", "author": ["M. Campbell", "J. Hoane", "F. Hsu"], "venue": "Artificial Intelligence 134 (1- 2) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}, {"title": "On the parallelization of UCT", "author": ["T. Cazenave", "N. Jouandeau"], "venue": "in: H. van den Herik et al. (Ed.), Proceedings of Computers and Games CG-08, Vol. 5131 of Lecture Notes in Computer Science, Springer", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "Parallel planning via the distribution of operators", "author": ["D. Vrakas", "I. Refanidis", "I. Vlahavas"], "venue": "Journal of Experimental and Theoretical Artificial Intelligence 13 (3) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2001}, {"title": "An economics approach to hard computational problems", "author": ["B. Huberman", "R. Lukose", "T. Hogg"], "venue": "Science 275 (5296) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1997}, {"title": "Heavy-tailed phenomena in satisfiability and constraint satisfaction problems", "author": ["C. Gomes", "B. Selman", "N. Crato", "H. Kautz"], "venue": "Journal of Automated Reasoning 24 (1-2) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2000}, {"title": "ManySAT: a parallel SAT solver", "author": ["Y. Hamadi", "S. Jabbour", "L. Sais"], "venue": "Journal on Satisfiability, Boolean Modeling and Computation 6 ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "Single-agent parallel window search", "author": ["C. Powley", "R. Korf"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 13 (5) ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1991}, {"title": "Adaptive k-parallel best-first search: A simple but efficient algorithm for multi-core domain-independent planning", "author": ["V. Vidal", "L. Bordeaux", "Y. Hamadi"], "venue": "in: Proceedings of the 3rd Symposium on Combinatorial Search (SOCS\u201910)", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "Kbfs: K-best-first search", "author": ["A. Felner", "S. Kraus", "R.E. Korf"], "venue": "Annals of Mathematics and Artificial Intelligence 39 ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive parallel iterative deepening search", "author": ["D. Cook", "R. Varnell"], "venue": "Journal of Artificial Intelligence Research 9 ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1998}, {"title": "Best-first heuristic search for multicore machines", "author": ["E. Burns", "S. Lemons", "W. Ruml", "R. Zhou"], "venue": "Journal of Artificial Intelligence Research (JAIR) 39 ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Despite significant progress in recent years in developing domain-independent admissible heuristics [1, 2, 3], scaling up optimal planning remains a challenge.", "startOffset": 100, "endOffset": 109}, {"referenceID": 1, "context": "Despite significant progress in recent years in developing domain-independent admissible heuristics [1, 2, 3], scaling up optimal planning remains a challenge.", "startOffset": 100, "endOffset": 109}, {"referenceID": 2, "context": "Despite significant progress in recent years in developing domain-independent admissible heuristics [1, 2, 3], scaling up optimal planning remains a challenge.", "startOffset": 100, "endOffset": 109}, {"referenceID": 3, "context": "We introduce and evaluate Hash Distributed A* (HDA*), a parallelization of A* [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "[5], and later extended by Mahapatra and Dutt [6], the scalability and limitations of hash-based work distribution and duplicate pruning have not been previously evaluated in depth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], and later extended by Mahapatra and Dutt [6], the scalability and limitations of hash-based work distribution and duplicate pruning have not been previously evaluated in depth.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "While there has been some recent work in parallel search [7, 8, 9], these approaches are multi-threaded, and limited to a single, shared-memory machine.", "startOffset": 57, "endOffset": 66}, {"referenceID": 7, "context": "While there has been some recent work in parallel search [7, 8, 9], these approaches are multi-threaded, and limited to a single, shared-memory machine.", "startOffset": 57, "endOffset": 66}, {"referenceID": 8, "context": "While there has been some recent work in parallel search [7, 8, 9], these approaches are multi-threaded, and limited to a single, shared-memory machine.", "startOffset": 57, "endOffset": 66}, {"referenceID": 9, "context": "When implemented using the standard MPI message passing library [10], the exact same code can be executed on a wide array of parallel environments, ranging from a standard desktop multicore to a massive cluster with thousands of cores, effectively using all of the aggregate CPU and memory resources available on the system.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "We use the cost-optimal version with the explicit (merge-and-shrink) abstraction heuristic reported by Helmert, Haslum, and Hoffman [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "The second solver is an application-specific 24-puzzle solver, which uses the pattern database heuristic code provided by Korf and Felner [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "The experiments include a comparison with TDS [12, 13], a successful parallelization of IDA* with a distributed transposition table.", "startOffset": 46, "endOffset": 54}, {"referenceID": 12, "context": "The experiments include a comparison with TDS [12, 13], a successful parallelization of IDA* with a distributed transposition table.", "startOffset": 46, "endOffset": 54}, {"referenceID": 13, "context": "[14, 15, 16]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 14, "context": "[14, 15, 16]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "[14, 15, 16]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "While most work on work-stealing has been on MIMD systems, parallelization of IDA* on SIMD machines using an alternating, two-phase mechanism, with a search phase and a load balancing phase, has also been investigated [17, 18].", "startOffset": 218, "endOffset": 226}, {"referenceID": 17, "context": "While most work on work-stealing has been on MIMD systems, parallelization of IDA* on SIMD machines using an alternating, two-phase mechanism, with a search phase and a load balancing phase, has also been investigated [17, 18].", "startOffset": 218, "endOffset": 226}, {"referenceID": 18, "context": "Another approach to search space partitioning (particularly in sharedmemory search) is derived from a line of work on addressing memory capacity limitations by using a large amount of slower, external memory (such as disks), to store states in search [19, 20, 21] (external memory was also used specifically for planning [22]).", "startOffset": 251, "endOffset": 263}, {"referenceID": 19, "context": "Another approach to search space partitioning (particularly in sharedmemory search) is derived from a line of work on addressing memory capacity limitations by using a large amount of slower, external memory (such as disks), to store states in search [19, 20, 21] (external memory was also used specifically for planning [22]).", "startOffset": 251, "endOffset": 263}, {"referenceID": 20, "context": "Another approach to search space partitioning (particularly in sharedmemory search) is derived from a line of work on addressing memory capacity limitations by using a large amount of slower, external memory (such as disks), to store states in search [19, 20, 21] (external memory was also used specifically for planning [22]).", "startOffset": 321, "endOffset": 325}, {"referenceID": 6, "context": "Korf has implemented a multithreaded, breadth-first search using a shared work queue which uses external memory [7, 20].", "startOffset": 112, "endOffset": 119}, {"referenceID": 7, "context": "Zhou and Hansen [8] introduce a parallel, breadth-first search algorithm.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "[9] have investigated best-first search algorithms that include enhancements such as structured duplicate detection and speculative search.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "In an early study, Kumar, Ramesh, and Rao [23] identified two broad approaches to parallelizing best-first search, based on how the usage and maintenance of the open list was parallelized.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "Kumar, Ramesh and Rao [23], as well as Karp and Zhang [24, 25] proposed a random work allocation strategy, where newly generated states were sent to random processors.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "Kumar, Ramesh and Rao [23], as well as Karp and Zhang [24, 25] proposed a random work allocation strategy, where newly generated states were sent to random processors.", "startOffset": 54, "endOffset": 62}, {"referenceID": 23, "context": "Kumar, Ramesh and Rao [23], as well as Karp and Zhang [24, 25] proposed a random work allocation strategy, where newly generated states were sent to random processors.", "startOffset": 54, "endOffset": 62}, {"referenceID": 24, "context": ", [26]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": "[27, 28]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[27, 28]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "Parallel Retracting A* (PRA*) [5] simultaneously addresses the problem", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of RA* [5], a limited memory search algorithm closely related to MA* [29] and SMA* [30].", "startOffset": 152, "endOffset": 155}, {"referenceID": 27, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of RA* [5], a limited memory search algorithm closely related to MA* [29] and SMA* [30].", "startOffset": 214, "endOffset": 218}, {"referenceID": 28, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of RA* [5], a limited memory search algorithm closely related to MA* [29] and SMA* [30].", "startOffset": 228, "endOffset": 232}, {"referenceID": 4, "context": "On the other hand, the implementation of this retraction mechanism in [5] incurs a significant synchronization overhead: when a processor P generates a new state s and sends it to the destination processor Q, P blocks and waits for Q to confirm that s has successfully been received and stored (or whether the send operation failed due to memory exhaustion at the target process).", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "The idea of hash-based work distribution was investigated further by Mahapatra and Dutt [6], who studied parallel A* on a Hypercube architecture, where CPUs are connected by a hypercube network, while in current standard architectures machines are connected by either a mesh or torus network.", "startOffset": 88, "endOffset": 91}, {"referenceID": 24, "context": "process using Dutt and Mahapatra\u2019s Quality Equalizing (QE) strategy [26].", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Therefore, Mahapatra and Dutt also proposed Local Hashing of Nodes and QE (LOHA&QE), which incorporates a state space partitioning strategy and allocates disjoint partitions to disjoint processor groups in order to minimize communication costs [6].", "startOffset": 244, "endOffset": 247}, {"referenceID": 29, "context": "The notion of the levelized graph can sometimes be extended to exploit local hashing if the search space has some regularities on depths such as multiple sequence alignment [31].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "Transposition-table driven work scheduling (TDS) [12, 13] is a distributed memory, parallel IDA* algorithm.", "startOffset": 49, "endOffset": 57}, {"referenceID": 12, "context": "Transposition-table driven work scheduling (TDS) [12, 13] is a distributed memory, parallel IDA* algorithm.", "startOffset": 49, "endOffset": 57}, {"referenceID": 30, "context": "The ideas behind TDS have also been successfully integrated in adversarial two-player search [32, 33, 34].", "startOffset": 93, "endOffset": 105}, {"referenceID": 31, "context": "The ideas behind TDS have also been successfully integrated in adversarial two-player search [32, 33, 34].", "startOffset": 93, "endOffset": 105}, {"referenceID": 32, "context": "The ideas behind TDS have also been successfully integrated in adversarial two-player search [32, 33, 34].", "startOffset": 93, "endOffset": 105}, {"referenceID": 4, "context": "[5] and Mahapatra and Dutt [6] was done, and the parallel systems which are prevalent today have very different architectures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5] and Mahapatra and Dutt [6] was done, and the parallel systems which are prevalent today have very different architectures.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "either as a component of a more complex algorithm [5] or as a straw man against which local hash-based distribution was considered [6], this is the first paper which analyzes the scalability and limitations of global hashing in depth.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "either as a component of a more complex algorithm [5] or as a straw man against which local hash-based distribution was considered [6], this is the first paper which analyzes the scalability and limitations of global hashing in depth.", "startOffset": 131, "endOffset": 134}, {"referenceID": 33, "context": "An early version of this work has been previously presented in a conference paper [35].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "We now describe Hash Distributed A* (HDA*), a simple parallelization of A* which uses the hash-based work distribution strategy originally proposed in PRA* [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "Even if the heuristic function [3] is consistent, parallel A* search may sometimes have to re-open a state saved in the closed list.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "This results in significant synchronization overhead \u2013 for example, it was observed in [9] that a straightforward implementation of PRA* exhibited extremely poor performance on the Grid search problem, and multicore performance for up to 8 cores was consistently slower than sequential A*.", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "[12] showed that this communication overhead could be overcome by packing multiple states with the same destination into a single message.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In a decentralized parallel A* (including HDA*), when a solution is discovered, there is no guarantee at that time that the solution is optimal [23].", "startOffset": 144, "endOffset": 148}, {"referenceID": 34, "context": "In our implementation of HDA*, we used the time algorithm of Mattern [36], which was also used in TDS.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "Our implementation of HDA* uses the Zobrist function [37] to map a SAS+ state representation [38] to a hash key.", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "Our implementation of HDA* uses the Zobrist function [37] to map a SAS+ state representation [38] to a hash key.", "startOffset": 93, "endOffset": 97}, {"referenceID": 37, "context": "The Zobrist function was previously used in domainindependent planning in MacroFF [39].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "In MacroFF, as well as an earlier version of HDA* [35], duplicate checking in the open/list was performed by checking if the hash key of a state was present in the open/closed list, so there was a", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "We parallelized the sequential optimal version of the Fast Downward planner, enhanced with the so-called LFPA heuristic, which is based on explicit (merge-and-shrink) state abstraction [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "We compare HDA* with sequential A* and a shared-memory implementation of Parallel Retracting A* (PRA*) [5] on a single, multicore machine.", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "\u2019s experiments [9], our PRA* implementation does not include the node retraction scheme because the main goal of our experiments is to show the impact of eliminating synchronization overhead from PRA*.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "Mahapatra and Dutt also noted that sequential runtimes were not available for their scalability experiments [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 38, "context": "[40] (who called it \u201cspeedup efficiency\u201d).", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] have observed that the re-expansion rate increases in domains with non-unit transition costs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": ", number of CPUs) [41].", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "We evaluated HDA* on the 24-puzzle, using an application-specific solver based on IDA* code provided by Rich Korf, which uses a disjoint pattern database heuristic [11].", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "As benchmark instances, we used the 50-instance set of 24-puzzles reported in [11], Table 2.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "IDA* solves all 50 instances [11] whereas with our HDA* using 12 cores only 10 instances can be solved with 54GB of memory.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "[12], in their work on TDS, proposed packing multiple states with the same destination.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] and Karp and Zhang [24] proposed a simple, random work distribution strategy for best-first search where generated nodes are sent to a random processor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] and Karp and Zhang [24] proposed a simple, random work distribution strategy for best-first search where generated nodes are sent to a random processor.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "Transposition-Driven Scheduling (TDS) is a parallelization of IDA* with a distributed transposition table, where hash-based work distribution is used to map states to processors for both scheduling and transposition table checks [13].", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "TDS has been applied successfully to sliding tiles puzzles and Rubik\u2019s Cube [13], and has also been adapted for adversarial search [32, 33].", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "TDS has been applied successfully to sliding tiles puzzles and Rubik\u2019s Cube [13], and has also been adapted for adversarial search [32, 33].", "startOffset": 131, "endOffset": 139}, {"referenceID": 31, "context": "TDS has been applied successfully to sliding tiles puzzles and Rubik\u2019s Cube [13], and has also been adapted for adversarial search [32, 33].", "startOffset": 131, "endOffset": 139}, {"referenceID": 40, "context": "While TDS has not been applied to planning, recent work has shown that IDA* with a transposition table (IDA*+TT) is a successful search strategy for optimal, domain-independent planning [42] \u2013 on problems which can be solved by A*, the runtime of IDA*+TT is usually within a factor of 4 of A*, and IDA*+TT can eventually solve problems where A* exhausts memory.", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Therefore, we compared HDA* and TDS [13] for planning.", "startOffset": 36, "endOffset": 40}, {"referenceID": 40, "context": "Our implementation of TDS uses a transposition table implementation based on [42], where the table entry replacement policy is a batch replacement policy which sorts entries according to access frequency and periodically frees 30% of the entries (preferring to keep most frequently accessed entries).", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "We incorporated techniques to overcome higher latency in a lower-bandwidth network described in [13], such as their modification to the termination detection algorithm.", "startOffset": 96, "endOffset": 100}, {"referenceID": 40, "context": "Although replacement based on subtree size performed best in sequential search [42], we did not implement this policy because subtree size computation would require extensive message passing in parallel search.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "\u2019s stack does not exceed 1MB in their applications [13], our stack often used hundreds of megabytes of memory.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "A possible future improvement is to combine Dutt and Mahapatra\u2019s technique in SEQ A* [6] with TDS and restrict initiating parallelism.", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "While it is possible to achieve drastically large (sometimes super linear) speedup over IDA* [12, 13], the speedup diminishes as IDA* has access to larger and larger transposition tables.", "startOffset": 93, "endOffset": 101}, {"referenceID": 12, "context": "While it is possible to achieve drastically large (sometimes super linear) speedup over IDA* [12, 13], the speedup diminishes as IDA* has access to larger and larger transposition tables.", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": "ample, PRA* [5], has a state retraction mechanism which frees memory by retracting some states at the search frontier (however, as explained in Section 2), the PRA* retraction policy results in synchronization overhead).", "startOffset": 12, "endOffset": 15}, {"referenceID": 41, "context": "Parallel Mur\u03c6 [43, 44] addresses verification tasks that involve exhaustively enumerating all reachable states in a state space.", "startOffset": 14, "endOffset": 22}, {"referenceID": 42, "context": "Parallel Mur\u03c6 [43, 44] addresses verification tasks that involve exhaustively enumerating all reachable states in a state space.", "startOffset": 14, "endOffset": 22}, {"referenceID": 4, "context": "Similarly to HDA* and other work described in Section 2 [5, 6], Parallel Mur\u03c6 implements a hash-based work distribution schema where each state is assigned to a unique owner processor.", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "Similarly to HDA* and other work described in Section 2 [5, 6], Parallel Mur\u03c6 implements a hash-based work distribution schema where each state is assigned to a unique owner processor.", "startOffset": 56, "endOffset": 62}, {"referenceID": 43, "context": "The Eddy Murphi model checker [46] specializes processors\u2019 tasks, defining two threads for each processing node.", "startOffset": 30, "endOffset": 34}, {"referenceID": 44, "context": "Lerda and Sisto parallelized the SPIN model checker to increase the availability of memory resources [47].", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "Holzmann and Bo\u015dna\u0109ki [48] introduce an extension to SPIN to multicore, shared memory machines.", "startOffset": 22, "endOffset": 26}, {"referenceID": 46, "context": "[49] use hashbased work distribution to convert an implicitly defined model-checking state space into an explicit file representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "has been addressed in [50].", "startOffset": 22, "endOffset": 26}, {"referenceID": 41, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 42, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 44, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 46, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 48, "context": "Finally, while previous work in model checking has used up to 256 processors [51], our work presents the largest scale experiments with hash-based work distribution to date, showing that hash-based work distribution can scale efficiently relative to pmin even for up to 2400 processors.", "startOffset": 77, "endOffset": 81}, {"referenceID": 29, "context": "[31] have applied HDA* to multiple sequence alignment (MSA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": ", [52, 53]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 50, "context": ", [52, 53]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 51, "context": "The Operator Distribution Method for parallel Planning (ODMP) [54] parallelizes the computation at each node.", "startOffset": 62, "endOffset": 66}, {"referenceID": 52, "context": "This approach, which is a parallel version of an algorithm portfolio [55], seeks to exploit the long-tailed runtime distribution behavior encountered in search algorithms [56] by using different versions of search algorithms to search different (potentially overlapping) portions of the search space.", "startOffset": 69, "endOffset": 73}, {"referenceID": 53, "context": "This approach, which is a parallel version of an algorithm portfolio [55], seeks to exploit the long-tailed runtime distribution behavior encountered in search algorithms [56] by using different versions of search algorithms to search different (potentially overlapping) portions of the search space.", "startOffset": 171, "endOffset": 175}, {"referenceID": 54, "context": "An example of this is the ManySAT solver [57], which executes a different version of a DPLL-based backtracking SAT solver on each processor and periodically shares lemmas among the processes.", "startOffset": 41, "endOffset": 45}, {"referenceID": 55, "context": "A third approach is parallel-window search for IDA* [58], where each processor searches from the same root node, but is assigned a different bound \u2013 that is, each processor is assigned a different, independent iteration of IDA*.", "startOffset": 52, "endOffset": 56}, {"referenceID": 56, "context": "[59] propose a multicore version of the KBFS algorithm [60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[59] propose a multicore version of the KBFS algorithm [60].", "startOffset": 55, "endOffset": 59}, {"referenceID": 58, "context": "The EUREKA system [61] used machine learning to automatically configure parallel IDA* for various problems (including nonlinear planning) and machine architectures.", "startOffset": 18, "endOffset": 22}, {"referenceID": 38, "context": "[40] propose PFA*-DDD, a parallel version of Frontier A* with Delayed Duplicate Detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The key idea, first used in Parallel Retracting A* [5], is to distribute work according to the hash value for generated states.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Also, unlike previous work such as PRA* and GOHA [6], which implemented hash-based work distribution on variants of A*, HDA* is a straightforward implementation of hash-based work distribution for standard A*.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "We evaluated HDA* as a replacement for the sequential A* search engine for a state-of-the-art, optimal sequential planner, Fast Downward [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 10, "context": "We also evaluated HDA* on the 24-puzzle domain by implementing a parallel solver with a disjoint pattern database heuristic [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 59, "context": "have recently proposed PBNF, a shared memory, parallel best-first search algorithm, and showed that PBNF outperforms HDA* on planning in shared memory environments [62].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "Comparison with a randomized work distribution strategy which performs load balancing but no duplicate detection [23, 24] showed the simple hash-based duplicate detection mechanism is essential to the performance of HDA*.", "startOffset": 113, "endOffset": 121}, {"referenceID": 22, "context": "Comparison with a randomized work distribution strategy which performs load balancing but no duplicate detection [23, 24] showed the simple hash-based duplicate detection mechanism is essential to the performance of HDA*.", "startOffset": 113, "endOffset": 121}, {"referenceID": 2, "context": ", computation of the abstraction heuristic table [3] and pattern database generation [11]) is another area for future work.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": ", computation of the abstraction heuristic table [3] and pattern database generation [11]) is another area for future work.", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "machines using hash-based distribution, but within a single machine incorporate techniques such as speculative expansion that have been shown to scale well on a shared memory environment [9].", "startOffset": 187, "endOffset": 190}, {"referenceID": 39, "context": "We have recently analyzed an iterative resource allocation policy to address this [41].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "[1] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[38] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[41] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[43] U.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[44] U.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[46] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[47] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[48] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[49] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[50] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[51] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[52] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[54] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[55] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[56] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[57] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[58] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[59] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[60] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "[61] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[62] E.", "startOffset": 0, "endOffset": 4}], "year": 2012, "abstractText": "Large-scale, parallel clusters composed of commodity processors are increasingly available, enabling the use of vast processing capabilities and distributed RAM to solve hard search problems. We investigate Hash-Distributed A* (HDA*), a simple approach to parallel best-first search that asynchronously distributes and schedules work among processors based on a hash function of the search state. We use this approach to parallelize the A* algorithm in an optimal sequential version of the Fast Downward planner, as well as a 24-puzzle solver. The scaling behavior of HDA* is evaluated experimentally on a shared memory, multicore machine with 8 cores, a cluster of commodity machines using up to 64 cores, and large-scale high-performance clusters, using up to 2400 processors. We show that this approach scales well, allowing the effective utilization of large amounts of distributed memory to optimally solve problems which require terabytes of RAM. We also compare HDA* to Transposition-table Driven Scheduling (TDS), a hash-based parallelization of IDA*, and show that, in planning, HDA* significantly outperforms TDS. A simple hybrid which combines HDA* and TDS to exploit strengths of both algorithms is proposed and evaluated.", "creator": "gnuplot 4.4 patchlevel 3"}}}