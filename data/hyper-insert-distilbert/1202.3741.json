{"id": "1202.3741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Noisy Search with Comparative Feedback", "abstract": "we would present theoretical results in descriptive terms neither of lower and upper sample bounds or on the whole query complexity of noisy continuous search with comparative feedback. in this search model, the noise in the second feedback step depends mostly on the distance between only query parameter points and the search target. consequently, the exponential error correlation probability distribution in defining the feedback is therefore not uniquely fixed but varies for excluding the queries posed by the whole search algorithm. consider our typical results again show that discovering a possible target out of column n items can be found in big o ( minus log n ) indexed queries. we also show the surprising simple result saying that means for k or possible answers per query, the speedup curve is so not mostly log k ( as for k - ary search ) but only log log o k in some suitable cases.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (224kb)", "http://arxiv.org/abs/1202.3741v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shiau hong lim", "peter auer"], "accepted": false, "id": "1202.3741"}, "pdf": {"name": "1202.3741.pdf", "metadata": {"source": "CRF", "title": "Noisy Search with Comparative Feedback", "authors": ["Shiau Hong Lim", "Peter Auer"], "emails": [], "sections": [{"heading": null, "text": "We present theoretical results in terms of lower and upper bounds on the query complexity of noisy search with comparative feedback. In this search model, the noise in the feedback depends on the distance between query points and the search target. Consequently, the error probability in the feedback is not fixed but varies for the queries posed by the search algorithm. Our results show that a target out of n items can be found in O(log n) queries. We also show the surprising result that for k possible answers per query, the speedup is not log k (as for k-ary search) but only log log k in some cases."}, {"heading": "1 Introduction", "text": "We investigate a form of noisy search that arises in information retrieval systems, e.g. content-based image retrieval [Cox et al., 2000]. Consider a system where a user can search for a target item (unknown to the system) by answering a sequence of \u201cqueries\u201d generated by the system. As an example, consider a content-based image retrieval system. The system first presents k images to the user. The user then selects one that is the most similar to the target image. Based on the image chosen by the user, the system presents a new set of k images and the user answers by selecting one of these k images. This continues until the target is found by the system. We assume that the user response in each query is probabilistic and depends on the similarities between the presented images and the target image according to a comparative feedback model as proposed in [Cox et al., 2000] and [Auer and Leung, 2009]. The search process is inherently noisy due to the probabilistic nature of the user feedback. We are interested in the performance of such systems in terms of query complexity, i.e the expected\nnumber of queries needed before a target is found."}, {"heading": "1.1 The comparative feedback model", "text": "We consider a finite set of n data points X = {x1, . . . , xn} and a search target T \u2208 X . In each query, a set of k (k \u2265 2) distinct data points Q = {q1, . . . , qk} \u2282 X is presented to the user. The search terminates if T \u2208 Q, otherwise the user responds by selecting one of the query points qj , j \u2208 {1, . . . , k}.\nLet R \u2208 {1, . . . , k} be the random user response (note that we use the indices instead of the query points themselves). The comparative feedback model specifies the probability of choosing a particular response R = r as follows:\nPr(R = r|Q, T ) = S(T, qr)\u2211k j=1 S(T, qj)\nwhere S(\u00b7, \u00b7) measures the similarity between data points. This implies that query points close to the target are chosen with larger probability than query points far from the target. Given a distance function d(\u00b7, \u00b7) between data points, we consider two types of similarity measures. The first similarity measure decreases polynomially with increasing distance,\nS(x, y) = d(x, y)\u2212\u03b8, (1)\nwhile the second similarity measure decreases exponentially,\nS(x, y) = exp{\u2212\u03b8d(x, y)}. (2)\nIn both cases, \u03b8 > 0 is a parameter indicating the user \u201csharpness\u201d. Larger \u03b8 implies that the user favors items closer to the target more intensely.1\n1The main difference between the two similarity measures is that for the polynomial similarity measure, the user response depends on the relative differences of the distances to the target, while for the exponential similarity measure the response depends on the absolute differences of the distances. To see this let k = 2. Then Pr(R = 1|q1, q2, T ) ="}, {"heading": "1.2 Overview of results", "text": "The results of our analysis provide both lower bounds and upper bounds for the query complexity. The lower bounds are shown to hold for any algorithm, and the upper bounds provide performance guarantees for actual algorithms that are presented in this paper.\nMany formulations of noisy search have been proposed in the literature, each with different assumptions regarding the type of noise/uncertainty present in the user feedback. In the simplest, noise-free case, the search is reduced to standard k-ary search, and the query complexity can be as low as lognlog k where n is the total number of items and k is the number of possible answers to the queries. In the case of binary search where the user makes a mistake with a fixed probability p in each query, a lower bound of logn+o(logn)1\u2212H(p) [Renyi, 1961] and an upper bound of logn+O(log logn \u03b4 ) 1\u2212H(p) [Ben-Or and Hassidim, 2008] are known, where H(p) denotes Shannon\u2019s entropy. Similar results for k-ary queries with fixed error probability are also available (see [Ben-Or and Hassidim, 2008]). For a survey of a wide range of noisy search problems, see [Pelc, 2002].\nThe type of uncertainty in the comparative feedback model, however, is unique in the sense that the noise in the user feedback is sensitive to distances among data points. For example, the uncertainty is higher when the query points have relatively similar distances to the target.\nIn the following section we present our theoretical results, while the proofs are given in a separate section. Most of our results are stated for 1-dimensional data. This might not be realistic for certain data (e.g. images) but highlights the properties of the search problem induced by the comparative feedback model. The main result is that with k = 2, O(log n) query complexity is possible for both the polynomial (Eq. 1) and the exponential (Eq. 2) similarity measures. This result carries over also to data of arbitrary dimension, but with a possible cost that is exponential in the dimension. Surprisingly, we can show that larger k helps more for the exponential similarity measure (by a factor log k as expected from k-ary search), while the improvement for the polynomial similarity measure (Eq. 1) is shown to be at most by a factor log log k in some cases.\n1/(1+[ d(q1,T ) d(q2,T ) ]\u03b8) for the polynomial similarity measure and Pr(R = 1|q1, q2, T ) = 1/(1+exp{\u03b8[d(q1, T )\u2212d(q2, T )]}) for the exponential similarity measure."}, {"heading": "2 Results", "text": "We assume that the user model and its parameters are known to the search algorithm.\nExcept for the results in Section 2.2 we also assume that X \u2282 R and that the distance between data points is measured as d(xi, xj) = |xi \u2212 xj |.\nSince the search algorithm knows the user model, it can maintain the posterior target distribution in respect to the queries and user responses so far. We denote the posterior probability of data point xi being the target by ai and we use a to denote the vector (a1, . . . , an). After receiving a user response R = r to a query Q, this posterior is updated as\na\u2032i \u2190 bi,r := Pr(T = xi|Q, R = r,a)\n= Pr(T = xi|a)Pr(R = r|Q, T = xi)\nPr(R = r|Q,a)\n= aipi,r Ar\nwhere\npi,r = Pr(R = r|Q, T = xi) = S(xi, qr)\u2211k j=1 S(xi, qj)\nand\nAr = Pr(R = r|Q,a) = n\u2211 i=1 aipi,r.\nWhen X \u2282 R, we assume x1 < \u00b7 \u00b7 \u00b7 < xn and denote by ci the cumulative probabilities\nci = i\u2211\ni\u2032=1\nai\u2032 .\nThe quantiles of the cumulative probability are denoted by I(p) where I(p) is the index such that\ncI(p)\u22121 < p \u2264 cI(p).\nWe will use A for (A1, . . . , Ak) and pi for (pi,1, . . . , pi,k). Finally, we denote the entropy function by\nH(a) = \u2212 n\u2211 i=1 ai log ai ,\nand the KL-divergence by\nD(a||a\u2032) = n\u2211 i=1 ai log ai a\u2032i .\nWe will use base-2 logarithms throughout. Also, for scalar p and p\u2032, we define H(p) = H(p, 1 \u2212 p), and similarly D(p||p\u2032) = D ( (p, 1\u2212 p)||(p\u2032, 1\u2212 p\u2032) ) ."}, {"heading": "2.1 Efficient search for the polynomial similarity measure", "text": "In this section we show, for X \u2282 R, how query complexity O(log n) can be achieved in the comparative feedback model with polynomial similarity measure for k = 2. The initial uncertainty about the target is log n when measured by entropy. To guarantee at most O(log n) query iterations, we need to select query points such that a constant information gain is guaranteed. This can be achieved in the following way:\n\u2022 Calculate the quantiles is = I(s/4) for s = 0, . . . , 4 (i0 = 1, i4 = n), and consider the resulting 4 intervals (see Fig. 1 for an example, where d1, . . . , d4 mark the intervals).\n\u2022 From these four intervals find the one with the smallest length.\n\u2022 Query the endpoints of that interval adjacent to the smallest interval, which does not contain x1 or xn.\nFigure 1 shows an example for the selection of the query points. The rationale for this algorithm can be seen by this example: for any data point x in interval d2 we have |x \u2212 q1|/|x \u2212 q2| \u2264 1/2, while for any data point x in interval d4 we have |x\u2212q1|/|x\u2212q2| \u2265 1. Thus a user response R = 1 significantly indicates that the target is rather in d2 than in d4, while a user response R = 2 mildly indicates that the target is rather in d4 than in d2. This is sufficient to prove the following theorem, the proof is given in Section 3.2.\nTheorem 1. For the comparative feedback model with polynomial similarity measure, the expected number of queries of the above algorithm is at most\n4 log2 n G\u03c1 + 4,\nwhere G\u03c1 = 14 ( D(\u03c1||\u03c6) +D( 12 ||\u03c6) ) > 0\nwith \u03c1 = 2 \u03b8\n1+2\u03b8 and \u03c6 = 14 + 1 2\u03c1. The expectation is\ntaken over targets selected uniformly at random and the random responses of the user.\n2.2 Search in a D-dimensional space\nThe next theorem generalizes Theorem 1 to Ddimensional data points. The drawback is that the constant in the upper bound depends exponentially on the dimension. It can indeed be shown that this is necessary if distances between data points are measured by the \u221e-norm.\nWe assume that distances are measured by some pnorm. To mimic the construction of Theorem 1, we need a strategy for choosing query points such that there are two \u201cwell-separated\u201d groups of data points with significant probability mass. This can be achieved as follows:\n\u2022 Let B be the smallest || \u00b7 ||p-ball with probability mass \u2211 i:xi\u2208B ai \u2265 c for some positive constant\nc (to be determined later). In case of ties, the || \u00b7 ||p-ball with the largest total probability mass is chosen.\n\u2022 Choose an arbitrary point in B as the first query point q1.\n\u2022 Let \u03bb be the radius of B. Let B\u2032 be the || \u00b7 ||p-ball centered on B with radius 7\u03bb.\n\u2022 Choose the second query point q2 as the data point not in B\u2032 that is closest to q1.\nBy the above construction we have for all x \u2208 B, ||x\u2212 q1||p \u2264 2\u03bb and ||x \u2212 q2||p \u2265 ||q2||p \u2212 ||x||p \u2265 6\u03bb, such that\n||x\u2212 q1||p \u2264 ||x\u2212 q2||p/3,\nand for all x 6\u2208 B\u2032 we have ||x \u2212 q2||p \u2264 ||x \u2212 q1||p + ||q1 \u2212 q2||p \u2264 2||x\u2212 q1||p such that\n||x\u2212 q1||p \u2265 ||x\u2212 q2||p/2.\nSince (14D)D || \u00b7 ||p-balls of radius \u03bb/2 are sufficient to cover B\u2032 (2), the probability mass of B\u2032 is at most (14D)Dc (otherwise there would be a || \u00b7 ||p-ball of radius \u03bb/2 with probability mass at least c). Thus for c = (14D)\u2212D/2 the probability mass of the data points not in B\u2032 is at least 1/2. This is sufficient to prove, similarly to Theorem 1, the following theorem. The detailed proof is omitted.\nTheorem 2. For the comparative feedback model with polynomial similarity measure and D-dimensional data, the expected number of queries of the above algorithm is at most O ( (14D)D log n ) .\n2This can be seen by covering B\u2032 with || \u00b7 ||\u221e-balls of radius \u03bb\n2D , and then covering each of these || \u00b7 ||\u221e-balls by\na || \u00b7 ||p-ball of radius \u03bb/2."}, {"heading": "2.3 For the polynomial similarity measure large k may not help much", "text": "Using k query points, one could expect that the number of query iterations \u2014 similarly as for k-ary search \u2014 can be reduced by a factor of log k. Surprisingly, we can show that for some search problems this is not the case in the comparative feedback model with polynomial similarity measure.\nTheorem 3. For the comparative feedback model with polynomial similarity measure, there are data points x1 < \u00b7 \u00b7 \u00b7 < xn \u2208 R such that the expected number of queries for any search algorithm is at least\nlog n\u2212 log(2k) 2 log log k + 4\n= \u2126 (\nlog n log log k ) when the target is selected uniformly at random from {x1, . . . , xn}.\nWe will choose increasingly distant data points, xi+1\u2212 xi xi \u2212 xi\u22121, such that the similarity of xi to data points with smaller index is not much smaller than the similarity to data points with larger index, S(xi, x1) \u2265 S(xi, xi+1)/2. The idea behind this is that then for any query q1 < \u00b7 \u00b7 \u00b7 < qk and any target T = xi, the probability of the user response can be bounded by Pr{R = j|T = xi} \u2264 2/j. Thus query responses with large j are rather unlikely for any target point, which ensures that the expected information gain per query response is only log log k, in contrast to information gain log k for k-ary search with deterministic responses. The full calculation leading to Theorem 3 is given in Section 3.3.\n2.4 The exponential similarity measure allows k-ary search\nIn contrast to the previous section, we show in this section that for the exponential similarity measure it is always possible to find the target within O ( logn log k ) queries. This is achieved by the following selection algorithm: the algorithm subsequently chooses k disjoint intervals I1, . . . , Ik of minimal length such that3\n\u2022 each interval has probability mass at least\nc = 1\n14k \u2212 12 ,\ni.e. \u2211\ni:xi\u2208Ij ai \u2265 c,\n\u2022 and the distance between two intervals Ij = [xj , yj ] and Ij\u2032 = [xj\u2032 , yj\u2032 ] is at least the length\n3In case the construction fails, it is easy to show that there must already exist data points with total probability mass at least 1/2. This will be covered in the proof.\nof the smaller interval, i.e. xj\u2032 \u2212 yj \u2265 min{yj \u2212 xj , yj\u2032 \u2212 xj\u2032} for yj < xj\u2032 .\nThe algorithm selects one endpoint from each interval as a query point. For each interval Ij = [xj , yj ], if the first half of the interval [xj , xj+yj 2 ] contains more probability mass than the second half [xj+yj2 , yj ] then xj is selected as the query point, otherwise yj is selected.\nLet denote by X \u2032 all data points in these intervals that are closer to the query point of the respective interval than to any other query point. By construction, the probability mass of these points is at least c/2 in each interval. For each xi \u2208 X \u2032 let ri be the index of the corresponding query point (i.e. the query point is qri). Furthermore, let \u03b40 be the minimal distance between any two data points. Then it can be shown that there is a constant \u03b2 > 0 depending on \u03b40 and the user sharpness \u03b8 (but not on k), such that for any xi \u2208 X \u2032,\npi,ri \u2265 \u03b2.\nTogether with the observation that the probability mass of X \u2032 is at least ck/2 \u2265 1/28, this is sufficient to show an information gain of \u2126(log k) in each query. This gives the following result.\nTheorem 4. For the comparative feedback model with exponential similarity measure and data points xi \u2208 R, the expected number of queries for the above selection algorithm is at most O ( logn log k ) .\nThe proofs are provided in Section 3.4."}, {"heading": "3 Proofs", "text": ""}, {"heading": "3.1 Preliminaries", "text": "Lemma 1. The expected information gain in each query is given by:\nE [ H(a)\u2212H(a\u2032) ] = n\u2211 i=1 aiD(pi||A)\nProof. After each query, the expected entropy of the posterior is given by:\nE [ H(a\u2032) ] = k\u2211 j=1 Pr(R = j)H(b1,j , . . . , bn,j)\n= \u2212 k\u2211\nj=1\nAj n\u2211 i=1 bi,j log bi,j\n= \u2212 k\u2211\nj=1\nAj n\u2211 i=1 aipi,j Aj log aipi,j Aj\n= \u2212 k\u2211\nj=1 n\u2211 i=1 aipi,j ( log ai + log pi,j Aj ) = H(a)\u2212\nn\u2211 i=1 aiD(pi||A)\nRearranging the terms completes the proof.\nLemma 2. Let \u03c61, . . . ,\u03c6l be any set of l probability distributions on the k query points and \u03b11, . . . , \u03b1l be any set of l positive weights. Then, for any distribution r on the k query points,\nl\u2211 i=1 \u03b1iD(\u03c6i||r) \u2265 l\u2211 i=1 \u03b1iD(\u03c6i||\u03c6\u0304)\nwhere\n\u03c6\u0304 = \u2211l\ni=1 \u03b1i\u03c6i\u2211l i=1 \u03b1i\n.\nProof.\nl\u2211 i=1 \u03b1iD(\u03c6i||r)\n= l\u2211\ni=1\n\u03b1i k\u2211 j=1 \u03c6i,j log ( \u03c6\u0304j rj )(\u03c6i,j \u03c6\u0304j )\n= k\u2211\nj=1 ( l\u2211 i=1 \u03b1i\u03c6i,j ) log \u03c6\u0304j rj + l\u2211 i=1 \u03b1i k\u2211 j=1 \u03c6i,j log \u03c6i,j \u03c6\u0304j\n= ( l\u2211\ni=1\n\u03b1i ) k\u2211 j=1 \u03c6\u0304j log \u03c6\u0304j rj + l\u2211 i=1 \u03b1i k\u2211 j=1 \u03c6i,j log \u03c6i,j \u03c6\u0304j\n= ( l\u2211\ni=l\n\u03b1i\n) D(\u03c6\u0304||r) + l\u2211 i=1 \u03b1iD(\u03c6i||\u03c6\u0304)\n\u2265 l\u2211\ni=1\n\u03b1iD(\u03c6i||\u03c6\u0304)\nwhere the last inequality is due to non-negativity of relative entropy.\nLemma 3. Let \u03a6 \u2282 {1, . . . , n} be any subset of indices for the data points. The expected information gain in any query is at least\u2211\ni\u2208\u03a6 aiD(pi||\u03c6)\nwhere \u03c6 = P\ni\u2208\u03a6 aipiP i\u2208\u03a6 ai .\nProof. Apply Lemma 2 to Lemma 1."}, {"heading": "3.2 Proof of Theorem 1", "text": "Lemma 4. Let \u03a6,\u03a8 \u2282 {1, . . . , n} be two disjoint sets of indices such that \u2211 i\u2208\u03a6 ai \u2265 \u03b1, \u2211 i\u2208\u03a8 ai \u2265 \u03b2, and\n(min i\u2208\u03a6 pi) = p > q = (max i\u2208\u03a8 pi) .\nLet p\u0304 = \u2211\ni\u2208\u03a6\u222a\u03a8 aipi\u2211 i\u2208\u03a6\u222a\u03a8 ai and \u00b5 = \u03b1p+ \u03b2q \u03b1+ \u03b2 .\nThen \u2211 i\u2208\u03a6\u222a\u03a8 aiD(pi||p\u0304) \u2265 \u03b1D(p||\u00b5) + \u03b2D(q||\u00b5) . Proof. Let a\u03a6 = \u2211 i\u2208\u03a6 ai, a\u03a8 = \u2211 i\u2208\u03a8 ai,\np\u03a6 = \u2211 i\u2208\u03a6 ai a\u03a6 pi and p\u03a8 = \u2211 i\u2208\u03a8 ai a\u03a8 pi .\nNote that\np\u0304 = a\u03a6p\u03a6 + a\u03a8p\u03a8\na\u03a6 + a\u03a8 .\nWe have \u2211 i\u2208\u03a6\u222a\u03a8 aiD(pi||p\u0304)\n= \u2211 i\u2208\u03a6 aiD(pi||p\u0304) + \u2211 i\u2208\u03a8 aiD(pi||p\u0304)\n= a\u03a6 \u2211 i\u2208\u03a6 ai a\u03a6 D(pi||p\u0304) + a\u03a8 \u2211 i\u2208\u03a8 ai a\u03a8 D(pi||p\u0304) \u2265 a\u03a6D(p\u03a6||p\u0304) + a\u03a8D(p\u03a8||p\u0304)\nwhere the inequality is due to convexity of D.\nLet f(a\u03a6, a\u03a8) = a\u03a6D(p\u03a6||p\u0304) + a\u03a8D(p\u03a8||p\u0304) be a function of a\u03a6 and a\u03a8. It can be shown, by taking derivatives, that f(a\u03a6, a\u03a8) \u2265 f(\u03b1, \u03b2).\nAssume now that a\u03a6 = \u03b1 and a\u03a8 = \u03b2. Let g(p\u03a6, p\u03a8) = \u03b1D(p\u03a6||p\u0304) + \u03b2D(p\u03a8||p\u0304) be a function of p\u03a6 and p\u03a8. Taking derivatives with respect to p\u03a6 and p\u03a8, and using the fact that p > q, it is straightforward to show that the minimum is at g(p, q).\nLemma 5. In each query iteration the algorithm of Section 2.1 achieves at least\nG\u03c1 = 14 ( D(\u03c1||\u03c6) +D( 12 ||\u03c6) ) expected information gain where \u03c1 = 2 \u03b8\n1+2\u03b8 and \u03c6 =\n1 4+ 1 2\u03c1, if all four intervals considered by the algorithm have non-zero length. Furthermore, G\u03c1 > 0.\nProof. Lemma 3 allows us to ignore the contribution of subsets of data points in analyzing the information gain. In particular, we will focus attention onto only two of the four intervals of data points, the smallest interval and another interval such that the two query points are between these two intervals. Without loss of generality we assume the configuration of Figure 1, such that these intervals are d2 and d4.\nThe key property of all data points in interval d2 is that the distance to q1 is at most half the distance to q2. Therefore, for any xi \u2208 d2,\npi,1 = S(xi, q1)\nS(xi, q1) + S(xi, q2) =\n|xi \u2212 q2|\u03b8\n|xi \u2212 q1|\u03b8 + |xi \u2212 q2|\u03b8\n\u2265 2 \u03b8\n1 + 2\u03b8 = \u03c1 > 1 2 .\nOn the other hand, for any xi \u2208 d4, pi,1 < 12 . Since\u2211 xi\u2208d2 ai \u2265 1/4 and \u2211 xi\u2208d4 ai \u2265 1/4, applying Lemma 3 and Lemma 4 (setting \u03b1 = \u03b2 = 1/4, p = \u03c1 and q = 1/2), the expected information gain is at least\u2211 i\u2208I\u03c31\u222aI\u03c32 aiD(pi||p\u0304) \u2265 1 4 D(\u03c1||\u03c6) + 1 4 D( 1 2 ||\u03c6) .\nFinally, since \u03b8 > 0, 12 < \u03c6 < \u03c1, and hence D(\u03c1||\u03c6) > 0 and D( 12 ||\u03c6) > 0.\nProof of Theorem 1. From Lemma 5 we get that the expected number of query iterations until one of the considered intervals is of zero length, is at most (log n)/G\u03c1. If there is a zero length interval, then there is a data point xi with ai \u2265 1/4 that is selected as one of the query points q1 or q2. Thus in this case the search terminates with probability at least 1/4.\nNow let \u03c4 be an upper bound on the expected total number of query iterations. Then the above reasoning gives the following recursion,\n\u03c4 \u2264 1 4 [ log n G\u03c1 + 1 ] + 3 4 [ log n G\u03c1 + 1 + \u03c4 ] ,\nsince the search essentially might restart when the query point from a zero length interval is not the target. Solving for \u03c4 gives\n\u03c4 \u2264 4 log n G\u03c1 + 4."}, {"heading": "3.3 Proof of Theorem 3", "text": "The following is a suitable choice of the data points: for all i > 1,\nxi+1 = xi \u2212 x1 \u00b7 2\u22121/\u03b8\n1\u2212 2\u22121/\u03b8\nsuch that ( xi+1 \u2212 xi xi+1 \u2212 x1 )\u03b8 = 1 2 ,\nwhere \u03b8 is the parameter in the polynomial similarity measure, S(x, y) = |x\u2212 y|\u2212\u03b8. Lemma 6. With the above choice of data points,\nS(x, xi) \u2264 2S(x, x1)\nfor any data points x 6= xi.\nProof. By construction we have S(xi+1, xi) = 2S(xi+1, x1). Since\nS(x,xi) S(x,x1) is decreasing in x, the statement holds for any x \u2265 xi+1. Furthermore, for x \u2264 xi\u22121, S(x, xi) = S(xi, x) \u2264 2S(xi, x1) \u2264 2S(x, x1).\nLemma 7. With the above choice of data points, for any query q1 < \u00b7 \u00b7 \u00b7 < qk and any data point x 6\u2208 {q1, . . . , qk}, the probability of choosing the j-th query point is bounded by\nPr(R = j|T = x) \u2264 2 j\n.\nProof. Let ` be the index of the query points such that q` < x < q`+1 and assume that ` \u2264 j. Then\nPr(R = j|T = x) = S(x, qj)Pk j\u2032=1 S(x, qj\u2032)\n= S(x, qj)P` j\u2032=1 S(x, qj\u2032) + Pk j\u2032=`+1 S(x, qj\u2032) \u2264 S(x, qj)P` j\u2032=1 S(x, qj\u2032) + Pj j\u2032=`+1 S(x, qj\u2032)\n\u2264 S(x, qj) ` \u00b7 S(x, x1) + (j \u2212 `) \u00b7 S(x, qj)\n\u2264 S(x, qj) j \u00b7min{S(x, x1), S(x, qj)}\n\u2264 2 j\nby Lemma 6. If ` > j then\nS(x, qj)\u2211k j\u2032=1 S(x, qj\u2032) \u2264 S(x, qj)\u2211j j\u2032=1 S(x, qj\u2032) \u2264 S(x, qj) j \u00b7 S(x, x1) \u2264 2 j\nagain by Lemma 6.\nLet Rt \u2208 {1, . . . , k} be the random user response to the t-th query and let \u03c9 denote a possible randomization of the search algorithm. Then, given a particular sequence of user responses R1 = j1, R2 = j2, . . . , Rt = jt, the set of query points chosen by the algorithm for the (t + 1)-th query is given by a function Q(j1, j2, . . . , jt|\u03c9). Let Qt be the random variable Qt = Q(R1, . . . , Rt\u22121|\u03c9). We assume that the target T 6\u2208 Qt if T \u2208 Q\u03c4 for some \u03c4 < t.\nLemma 8. If the target T is chosen uniformly at random from the choice of data points above, then for k \u2265 3,\nPr(T \u2208 Qt) \u2264 k\nn (4 log k)t\u22121 .\nProof.\nPr(T \u2208 Qt)\n= 1\nn X \u03c9 X x Pr(T \u2208 Qt|T = x, \u03c9) Pr(\u03c9)\n= 1\nn X \u03c9 X x X j1,...,jt\u22121 Pr(R1 = j1, . . .\n, Rt\u22121 = jt\u22121, T \u2208 Qt|T = x, \u03c9) Pr(\u03c9)\n= 1\nn X \u03c9 X x X j1,...,jt\u22121 Pr(R1 = j1, . . .\n, Rt\u22121 = jt\u22121, x \u2208 Q(j1, . . . , jt\u22121|\u03c9)|T = x, \u03c9) Pr(\u03c9)\n= 1\nn X \u03c9 X j1,...,jt\u22121 X x\u2208Q(j1,...,jt\u22121|\u03c9)\nPr(R1 = j1, . . . , Rt\u22121 = jt\u22121|T = x, \u03c9) Pr(\u03c9)\n\u2264 1 n X \u03c9 X j1,...,jt\u22121 X x\u2208Q(j1,...,jt\u22121|\u03c9)\u201c 2\nj1 \u201d \u00b7 \u00b7 \u00b7 \u201c 2 jt\u22121 \u201d Pr(\u03c9) (3)\n= k\nn X j1,...,jt\u22121 \u201c 2 j1 \u201d \u00b7 \u00b7 \u00b7 \u201c 2 jt\u22121 \u201d \u2264 k\nn (4 log k)t\u22121\nfor k \u2265 3. Inequality (3) follows from Lemma 7 and the fact that the user responses are independent given the target.\nProof of Theorem 3. Using Lemma 8, we have\n\u03c4\u2211 t=1 Pr(T \u2208 Qt) \u2264 k n (4 log k)\u03c4 \u2212 1 4 log k \u2212 1 \u2264 k n (4 log k)\u03c4 \u2264 1 2\nfor\n\u03c4 \u2264 log n2k\nlog log k + 2 ,\nand the theorem follows from Markov\u2019s inequality."}, {"heading": "3.4 Proof of Theorem 4", "text": "The query construction of the selection algorithm fails only if there are k data points with total probability mass \u2265 1/2. We will deal with this case later. For now we assume that the construction succeeds.\nLemma 9. For any xi \u2208 X \u2032,\npi,ri \u2265 \u03b2\nwhere\n\u03b2 = 1\n1 + 2e\u2212\u03b8\u03b40(1 + 1\u03b8\u03b40 ) .\nProof. For xi \u2208 X \u2032,\npi,ri = Pr(R = ri|T = xi)\n= S(xi, qri)\u2211k j=1 S(xi, qj) = 1\n1 + \u2211\nj 6=ri e \u2212\u03b8(|xi\u2212qj |\u2212|xi\u2212qri |)\n\u2265 1 1 + \u2211 j 6=ri e \u2212\u03b8(|j\u2212ri|\u03b40) (4) \u2265 1 1 + 2 \u2211(k\u22121)/2 j\u2032=1 e \u2212j\u2032\u03b8\u03b40 (5) \u2265 1 1 + 2e\u2212\u03b8\u03b40(1 + 1\u03b8\u03b40 ) (6)\nFor inequality (4), by construction, the distance between any two adjacent query points is at least \u03b40 and the distance between any two query points j\u2032 intervals apart is j\u2032\u03b40. In inequality (5), we rewrite the indices and use the worst case where ri is the query point in the center-most interval, with (k \u2212 1)/2 query points on each side of qri . The inequality (6) is obtained using the integral bound.\nLemma 10. For any query constructed as described in Section 2.4, the expected information gain is \u2126(log k).\nProof. Using Lemma 3 we find that the expected information gain is at least\u2211\ni:xi\u2208X \u2032 aiD(pi||\u03c6) (7)\nwhere\n\u03c6 = \u2211 i:xi\u2208X \u2032 aipi\u2211 i:xi\u2208X \u2032 ai .\nGiven the constraint of Lemma 9, (7) is minimized for pi,ri = \u03b2, pi,r = (1 \u2212 \u03b2)/(k \u2212 1) for r 6= ri, and \u03c6r = 1/k.4 Thus\nD(pi||\u03c6) = \u03b2 log(\u03b2k) + (1\u2212 \u03b2) log (1\u2212 \u03b2)k k \u2212 1\n= \u03b2 log k + (1\u2212 \u03b2) log \u201e 1 + 1\nk \u2212 1\n\u00ab \u2212H(\u03b2)\n= \u03b2 log k +O(1).\nSince \u2211\ni:xi\u2208X \u2032 ai \u2265 1/28, this completes the proof.\n4The minimum can be found using the method of Lagrange multipliers.\nProof of Theorem 4. Using Lemma 10, the proof proceeds similarly to the proof of Theorem 1, observing that if the construction of the selection algorithm fails, then the search terminates with probability \u2265 1/2."}, {"heading": "4 Empirical testing of the feedback model", "text": "A question regarding the comparative feedback model is whether it adequately models the uncertainty of user response in real-world applications. We performed statistical goodness-of-fit tests on actual user feedback in image search tasks where distances are measured by Euclidean distance in the image feature space [Auer et al., 2011]. In most of the tasks, the model with polynomial similarity measure fits the data very well while in contrast, the model with exponential similarity measure can be rejected with high confidence. We believe that the distance metric plays a crucial role here and more conclusive results require further investigations.\nThe results in this paper assume that the model parameter \u03b8 is known to the algorithm, which is unlikely in practice. However, empirical evidence suggests that search performance degrades rather gracefully under parameter mismatch. To illustrate this, Figure 2 shows the query performance with respect to varying \u03b8 assumed by the algorithm while the true \u03b8 is indicated by a cross."}, {"heading": "5 Open problems", "text": "There are some open questions concerning the necessary number of queries, that we would like to address in future work:\n\u2022 Can the exponential dependency on the dimension be avoided for D-dimensional data? Initial progress seems to indicate that for distances measured by p-norms with p <\u221e, the dependency on the dimension is only polynomial, while for the \u221e-norm the dependency is exponential.\n\u2022 Is there an algorithm that \u2014 for 1-dimensional data \u2014 guarantees O\n( logn\nlog log k\n) queries for any\nset of data points? We already can show that for data points as chosen in Section 2.3, indeed only O (\nlogn log log k\n) queries are necessary.\n\u2022 Can our analysis \u2014 which assumes a uniform (or known) prior distribution on the possible targets and known parameters of the feedback model \u2014 be extended to an unknown prior distribution and unknown parameters of the feedback model?"}, {"heading": "Acknowledgements", "text": "The research leading to these results has received funding from the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 231495 (CompLACS), n\u25e6 216886 (PASCAL2), n\u25e6 216529, Personal Information Navigator Adapting Through Viewing, PinView, and the Austrian Federal Ministry of Science and Research."}], "references": [{"title": "Exploration-exploitation trade-offs with delayed feedback", "author": ["Auer et al", "P. 2011] Auer", "D. Glowacka", "A. Leung", "S.H. Lim", "A. Medlar", "J. Shawe-Taylor"], "venue": "PinView Deliverable D4.3,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Relevance feedback models for contentbased image retrieval. Multimedia Analysis, Processing and Communications", "author": ["Auer", "Leung", "P. 2009] Auer", "A. Leung"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well)", "author": ["Ben-Or", "Hassidim", "M. 2008] Ben-Or", "A. Hassidim"], "venue": "In Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Sci-", "citeRegEx": "Ben.Or et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ben.Or et al\\.", "year": 2008}, {"title": "The Bayesian image retrieval system, PicHunter: theory, implementation, and psychophysical experiments", "author": ["Cox et al", "I. 2000] Cox", "M. Miller", "T. Minka", "T. Papathomas", "P. Yianilos"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "al. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "al. et al\\.", "year": 2000}], "referenceMentions": [], "year": 2011, "abstractText": "We present theoretical results in terms of lower and upper bounds on the query complexity of noisy search with comparative feedback. In this search model, the noise in the feedback depends on the distance between query points and the search target. Consequently, the error probability in the feedback is not fixed but varies for the queries posed by the search algorithm. Our results show that a target out of n items can be found in O(log n) queries. We also show the surprising result that for k possible answers per query, the speedup is not log k (as for k-ary search) but only log log k in some cases.", "creator": "TeX"}}}