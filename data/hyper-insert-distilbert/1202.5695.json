{"id": "1202.5695", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2012", "title": "Training Restricted Boltzmann Machines on Word Observations", "abstract": "now the data restricted boltzmann partition machine ( rbm ) is a flexible simulations tool targeted for modeling macro complex data, using however there have been significant computational difficulties in using discrete rbms to visually model high - dimensional multinomial observations. in general natural language processing applications, code words listed are essentially naturally modeled by complex k - tailed ary discrete distributions, where k complexity is determined by the word vocabulary size and can less easily correctly be stored in less the hundred thousands. the correct conventional approach to training constructing rbms systems on word level observations completely is limited because it desperately requires not sampling the states alone of k - each way repeating softmax visible a units successively during block gibbs updates, an operation that takes time linear distribution in k. in this work, we simply address this scaling issue appropriately by additionally employing a more general spectral class of markov tale chain monte. carlo constraint operators on sampling the visible dir units, without yielding sequence updates consistent with computational complexity dimensions independent of choosing k. we thus demonstrate the ongoing success of learning our approach by applying training rbms on hundreds of countless millions of short word n - grams using larger vocabularies than previously feasible cases with rbms counterparts and namely using retaining the learned features to improve performance on chunking and sentiment pattern classification tasks, theoretically achieving state - of - just the - counter art results on the latter.", "histories": [["v1", "Sat, 25 Feb 2012 20:23:37 GMT  (116kb,D)", "https://arxiv.org/abs/1202.5695v1", null], ["v2", "Thu, 5 Jul 2012 12:15:40 GMT  (132kb,D)", "http://arxiv.org/abs/1202.5695v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["george e dahl", "ryan prescott adams", "hugo larochelle"], "accepted": true, "id": "1202.5695"}, "pdf": {"name": "1202.5695.pdf", "metadata": {"source": "CRF", "title": "Training Restricted Boltzmann Machines on Word Observations", "authors": ["George E. Dahl", "Ryan P. Adams"], "emails": ["gdahl@cs.toronto.edu", "rpa@seas.harvard.edu", "hugo.larochelle@usherbrooke.ca"], "sections": [{"heading": "1. Introduction", "text": "The breadth of applications for the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund and Haussler, 1991) has expanded rapidly in recent years.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nFor example, RBMs have been used to model image patches (Ranzato et al., 2010), text documents as bags of words (Salakhutdinov and Hinton, 2009), and movie ratings (Salakhutdinov et al., 2007), among other data. Although RBMs were originally developed for binary observations, they have been generalized to several other types of data, including integer- and real-valued observations (Welling et al., 2005).\nHowever, one type of data that is not well supported by the RBM is word observations from a large vocabulary (e.g., 100,000 words). The issue is not one of representing such observations in the RBM framework: socalled softmax units (Salakhutdinov and Hinton, 2009) are the natural choice for modeling words. The issue is that manipulating distributions over the states of such units is expensive even for intermediate vocabulary sizes and becomes impractical for vocabulary sizes in the hundred thousands \u2014 a typical situation for NLP problems. For example, with a vocabulary of 100,000 words, modeling n-gram windows of size n = 5 is comparable in scale to training an RBM on binary vector observations of dimension 500,000 (i.e., more dimensions than a 700\u00d7 700 pixel image). This scalability issue has been a primary obstacle to using the RBM for natural language processing.\nIn this work, we directly address the scalability issues associated with large softmax visible units in RBMs. We describe a learning rule with a computational complexity independent of the number of visible units. We obtain this rule by replacing the Gibbs sampling transition kernel over the visible units with carefully implemented Metropolis\u2013Hastings transitions. By training RBMs in this way on hundreds of millions of word windows, they learn representations capturing meaningful syntactic and semantic properties of words. Our learned word representations provide benefits on a chunking task competitive with other methods of in-\nar X\niv :1\n20 2.\n56 95\nv2 [\ncs .L\nG ]\n5 J\nul 2\nducing word representations and our learned n-gram features yield even larger performance gains. Finally, we also show how similarly extracted n-gram representations can be used to obtain state-of-the-art performance on a sentiment classification benchmark."}, {"heading": "2. Restricted Boltzmann Machines", "text": "We first describe the restricted Boltzmann machine for binary observations, which provides the basis for other data types. An RBM defines a distribution over a binary visible vector v of dimensionality V and a layer h of H binary hidden units through an energy\nE(v,h) = \u2212b>v \u2212 c>h\u2212 h>Wv. (1)\nThis energy is parameterized by bias vectors b \u2208 RV and c \u2208 RH and weight matrix W \u2208 RH\u00d7V , and is converted into a probability distribution via\np(v,h) = exp (\u2212E(v,h)) /Z (2) Z = \u2211 v\u2032,h\u2032 exp(\u2212E(v\u2032,h\u2032)) . (3)\nThis yields simple conditional distributions: p(h|v) = \u220f j p(hj |v) p(v|h) = \u220f i p(vi|h) (4)\np(hj = 1|v) = sigm(cj + \u2211 i Wjivi) (5)\np(vi = 1|h) = sigm(bi + \u2211 j Wjihj), (6)\nwhere sigm(z) = 1/(1 + e\u2212z), which allow for efficient Gibbs sampling of each layer given the other layer.\nWe train an RBM from T visible data vectors {vt}Tt=1 by minimizing the scaled negative (in practice, penalized) log likelihood of the parameters \u03b8 = (b, c,W):\n\u03b8MLE = argmin \u03b8 L(\u03b8) L(\u03b8) = 1 T \u2211 t `(vt; \u03b8) (7)\n`(vt; \u03b8) = \u2212 log p(vt) = \u2212 log \u2211 h p(vt,h). (8)\nThe gradient of the objective with respect to \u03b8\n\u2202L(\u03b8) \u2202\u03b8 = 1 T \u2211 t Eh|vt [ \u2202E(vt,h) \u2202\u03b8 ] \u2212 Ev,h [ \u2202E(v,h) \u2202\u03b8 ] is intractable to compute because of the exponentially many terms in the sum over joint configurations of the visible and hidden units in the second expectation.\nFortunately, for a given \u03b8, we can approximate this gradient by replacing the second expectation with a\njeudi 23 f\u00e9vrier 12\nMonte Carlo estimate based on a set of M samples N = {v\u0303m} from the RBM\u2019s distribution:\nEv,h [ \u2202E(v,h)\n\u2202\u03b8\n] \u2248 1 M \u2211 v\u0303m\u2208N Eh|v\u0303m [ \u2202E(v\u0303m,h) \u2202\u03b8 ] . (9)\nThe samples {v\u0303m} are often referred to as \u201cnegative samples\u201d as they counterbalance the gradient due to the observed, or \u201cpositive\u201d data. To obtain these samples, we maintain M parallel Markov chains throughout learning and update them using Gibbs sampling between parameter updates.\nLearning with the Monte Carlo estimator alternates between two steps: 1) Using the current parameters \u03b8, simulate a fews steps of the Markov chain on the M negative samples using Eqs. (4)-(6); and 2) Using the negative samples, along with a mini-batch (subset) of the positive data, compute the gradient in Eq. (9) and update the parameters. This procedure belongs to the general class of Robbins-Monro stochastic approximation algorithms (Younes, 1989). Under mild conditions, which include the requirement that the Markov chain operators leave p(v,h | \u03b8) invariant, this procedure will converge to a stable point of L(\u03b8). For K-ary observations \u2014 observations belonging to a finite set of K discrete outcomes \u2014 we can use the same energy function as in Eq. (1) for the binary RBM by encoding each observation in a \u201cone-hot\u201d representation and concatenating the representations of all observations to construct v. In other words, for n separate K-ary observations, the visible units v will be partitioned into n groups of K binary units, with the constraint that each partition can only contain a single non-zero entry. Using the notation va:b to refer to the subvector of elements from index a to index b, the ith observation will then be encoded by the group of visible units v(i\u22121)K+1:iK . The one-hot encoding is enforced by constraining each group of units to contain only a single 1-valued unit, the others being set to 0. The difference between RBMs with binary and K-ary observations is illustrated in Figure 1.\nTo simplify notation, we refer to the ith group of visible units as v(i) =v(i\u22121)K+1:iK . Similarly, we will refer to the biases and weights associated with those units as b(i) = b(i\u22121)K+1:iK and W\n(i) = W\u00b7,(i\u22121)K+1:iK . We will also denote with ek the one-hot vector with its kth component set to 1.\nThe conditional distribution over the visible layer is\np(v|h) = n\u220f i=1 p(v(i)|h) (10)\np(v(i) =ek|h) = exp(b(i)\n> ek + h >W(i)ek)\u2211 k\u2032 exp(b (i)i>ek\u2032+h>W(i)ek\u2032) .\nEach group v(i) has a multinomial distribution given the hidden layer. Because the multinomial probabilities are given by a softmax nonlinearity, the group of units v(i) are referred to as softmax units (Salakhutdinov and Hinton, 2009)."}, {"heading": "3. Difficulties with Word Observations", "text": "While in the binary case the size of the visible layer is equal to data dimenionality, in the K-ary case the size of the visible layer is K times the dimensionality. For language processing applications, where K is the vocabulary size and can run into the hundred thousands, the visible layer can become unmanageably large.\nThe difficulty with large K is that the Gibbs operator on the visible units becomes expensive to simulate, making it difficult to perform updates of the negative samples. That is, generating a sample from the conditional distribution in Eq. (10) dominates the stochastic learning procedure as K increases. The reason for this expense is that it is necessary to compute the activity associated with each of the K possible outcomes, even though only a single one will actually be selected.\nOn the other hand, given a mini-batch {vt} and negative samples {v\u0303m}, the gradient computations in Eq. (9) are able to take advantage of the sparsity of the visible activity. Since each vt and v\u0303m only contain n non-zero entries, the cost of the gradient estimator has no dependence on K and can be rapidly computed. Thus the only barrier to efficient learning of high-dimensional multinomial RBMs is the complexity of the Gibbs update for the visible units.\nDealing with large multinomial distributions is an issue that has come up previously in work on neural network language models (Bengio et al., 2001). For example, Morin and Bengio (2005) addressed this problem by introducing a fixed factorization of the (conditional) multinomial using a binary tree in which each leaf is associated with a single word. The tree was de-\ntermined using an external knowledge base, although Mnih and Hinton (2009) investigated ways of extending this approach by learning the word tree from data.\nUnfortunately, tree-structured solutions are not applicable to the problem of modeling the joint distribution of n consecutive words, as we wish to do here. Introducing a directed tree breaks the undirected, symmetric nature of the interaction between the visible and hidden units of the RBM. While one strategy might be to use a conditional RBM to model the tree-based factorizations, similar to Mnih and Hinton (2009), the end result would not be an RBM model of n-gram word windows, nor would it even be a conditional RBM over the next word given the n\u2212 1 previous ones. In summary, dealing with K-ary observations in the Boltzmann machine framework for large K is a crucial open problem that has inhibited the development of deep learning solutions NLP problems."}, {"heading": "4. Metropolis\u2013Hastings for Softmax Units", "text": "Having identified the Gibbs update of the visible units as the limiting factor in efficient learning of largeK multinomial observations, it is natural to examine whether other operators might be used for the Monte Carlo estimate in Eq. (9). In particular, we desire a transition operator that can take advantage of the same sparse operations that enable the gradient to be efficiently computed from the positive and negative samples, while still leaving p(v,h) invariant and thus still satisfying the convergence conditions of the stochastic approximation learning procedure.\nTo achieve this, instead of sampling exactly from the conditionals p(v(i)|h) within the Markov chain, we use a small number of iterations of Metropolis\u2013Hastings (M\u2013H) sampling. Let q(v\u0302(i) \u2190 v(i)) be a proposal distribution for group i. The following stochastic operator leaves p(v,h) invariant:\n1. Given the current visible state v, sample a proposal v\u0302 for group i, such that v\u0302(i) \u223c q(v\u0302(i) \u2190 v(i)) and v\u0302(j) = v(j) for i 6= j (i.e. sample a proposed new word for position i).\n2. Replace the ith part of the current state v(i) with v\u0302(i) with probability:\nmin { 1, q(v(i) \u2190 v\u0302(i)) exp(b(i)> v\u0302(i) + h>W(i)v\u0302(i)) q(v\u0302(i) \u2190 v(i)) exp(b(i)>v(i) + h>W(i)v(i)) } .\nAssuming it is possible to efficiently sample from the proposal distribution q(v\u0302(i) \u2190 v(i)), this M\u2013H operator is fast to compute as it does not require normalizing\nover all possible values of the visible units in group i and, in fact, only requires the unnormalized probability of one of them. Moreover, as the n visible groups are conditionally independent given the hiddens, each group can be simulated in parallel (i.e., words are sampled at every position separately). The efficiency of these operations make it possible to apply this transition operator many times before moving on to other parts of the learning and still obtain a large speedup over exact sampling from the conditional."}, {"heading": "4.1. Efficient Sampling of Proposed Words", "text": "The utility of M\u2013H sampling for an RBM with word observations relies on the fact that sampling from the proposal q(v\u0302(i) \u2190 v(i)) is much more efficient than sampling from the correct softmax multinomial. Although there are many possibilities for designing such proposals, here we will explore the most basic variant: independence chain Metropolis\u2013Hastings in which the proposal distribution is fixed to be the marginal distribution over words in the corpus.\nNa\u0308\u0131ve procedures for sampling from discrete distributions typically have linear time complexity in the number of outcomes. However, the alias method (pseudocode at www.cs.toronto.edu/~gdahl) of Kronmal and Perterson (1979) can be used to generate samples in constant time with linear setup time. While the alias method would not help us construct a Gibbs sampler for the visibles, it does make it possible to generate proposals extremely efficiently, which we can then use to simulate the Metropolis\u2013Hastings operator, regardless of the current target distribution.\nThe alias method leverages the fact that any K-valued discrete distribution can be written as a uniform mixture of K Bernoulli distributions. Having constructed this mixture distribution at setup time (with linear time and space cost), new samples can be generated in constant time by sampling uniformly from the K mixture components, followed by sampling from that component\u2019s Bernoulli distribution."}, {"heading": "4.2. Mixing of Metropolis\u2013Hastings", "text": "Although this procedure eliminates dependence of the learning algorithm on K, it is important to examine the mixing of Metropolis\u2013Hastings and how sensitive it is to K in practice. Although there is evidence (Hinton, 2002) that poorly-mixing Markov chains can yield good learning signals, when this will occur is not as well understood. We examined the mixing issue using the model described in Section 6.1 with the parameters learned from the Gigaword corpus with a 100,000-word vocabulary as described in Section 6.2.\nWe analytically computed the distributions implied by iterations of the M\u2013H operator, assuming the initial state was drawn according to \u220f i q(v\n(i)). As this computation requires the instantiation of n 100k\u00d7 100k matrices, it cannot be done at training time, but was done offline for analysis purposes. Each application of Metropolis\u2013Hastings results in a new distribution converging to the target (true) conditional.\nFigures 2(a) and 2(b) show this convergence for the \u201creconstruction\u201d distributions of six randomly-chosen 5-grams from the corpus, using two metrics: symmetric Kullback\u2013Leibler (KL) divergence and total variation (TV) distance, which is the standard measure for analysis of MCMC mixing. The TV distance shown is the mean across the five group distributions. Figures 2(c) and 2(d) show these metrics broken down by grouping, for the slowest curves (dark green) of the top two figures. These curves highlight that the state of the hidden units has a strong impact on the mixing and that most groups mix very quickly while a few converge slowly. We feel that these curves, along with the results of Section 6, indicate that the mixing is effective, but could benefit from further study."}, {"heading": "5. Related Work", "text": "Using M\u2013H sampling for a multinomial distribution with softmax probabilities has been explored in the context of a neural network language model by Bengio\nand Se\u0301ne\u0301cal (2003). They used M\u2013H to estimate the training gradient at the output of the neural network. However, their work did not address or investigate its use in the context of Boltzmann machines in general.\nSalakhutdinov and Hinton (2009) describe an alternative to directed topic models called the replicated softmax RBM that uses softmax units over the entire vocabulary with tied weights to model an unordered collection of words (a.k.a. bag of words). Since their RBM ties the weights to all the words in a single document, there is only one vocabulary-sized multinomial distribution to compute per document, instead of the n required when modeling a window of consecutive words. Therefore sampling a document conditioned on the hidden variables of the replicated softmax still incurs a computational cost linear in K, although the problem is not amplified by a factor of n as it is here. Notably, Salakhutdinov and Hinton (2009) limited their vocabulary to K < 14, 000.\nNo known previous work has attempted to address the computational burden associated with K-ary observations with large K in RBMs. The M\u2013H-based approach used here is not specific to a particular Boltzmann machine and could be used for any model with large softmax units, although the applications that motivate us come from NLP. Dealing with the large softmax problem is essential if Boltzmann machines are to be practical for natural language data.\nIn Section 6, we present results on the task of learning word representations. This task has been investigated previously by others. Turian et al. (2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). We have already mentioned the work of Mnih and Hinton (2009), who model the conditional distribution of the last word in n-gram windows. Collobert and Weston (2008) follows a similar approach, by training a neural network to fill-in the middle word of an n-gram window, using a marginbased learning objective. In contrast, we model the joint distribution of the whole n-gram window, which implies that the RBM could be used to fill-in any word within a window. Moreover, inference with an RBM yields a hidden representation of the whole window and not simply of a single word."}, {"heading": "6. Experiments", "text": "We evaluated our M\u2013H approach to training RBMs on two NLP tasks: chunking and sentiment classification. Both applications will be based on the same RBM model over n-gram windows of words, hence we\nfirst describe the parameterization of this RBM and later present how it was used for chunking and sentiment classification. Both applications also take advantage of the model\u2019s ability to learn useful feature vectors for entire n-grams, not just individual words.\n6.1. RBM Model of n-gram Windows\nIn the standard parameterization presented in Section 2, the RBM uses separate weights (i.e., different columns of W) to model observations at different positions. When training an RBM on word n-gram windows, we would prefer to share parameters across identical words in different positions in the window and factor the weights into position-dependent weights and position-independent weights (word representations).\nTherefore, we use an RBM parameterization very similar to that of Mnih and Hinton (2007), which itself is inspired by previous work on neural language models (Bengio et al., 2001). The idea is to learn, for each possible word w, a lower-dimensional linear projection of its one-hot encoding by incorporating the projection directly in the energy function of the RBM. Moreover, we share this projection across positions within the n-gram window. Let D be the matrix of this linear projection and let ew be the one-hot representation of w (where we treat w as an integer index in the vocabulary), performing this projection Dew is equivalent to selecting the appropriate column D\u00b7,w of this matrix. This column vector can then be seen as a real-valued vector representation of that word. The real-valued vector representations of all words within the n-gram are then concatenated and connected to the hidden layer with a single weight matrix.\nMore specifically, let D be the D \u00d7K matrix of word representations. These word representations are introduced by reparameterizing W(i) = U(i)D, where U(i) is a position-dependent H \u00d7D matrix. The biases across positions are also shared, i.e., we learn a single bias vector b\u2217 that is used at all positions (b(i) = b\u2217 \u2200i). The energy function becomes\nE(v,h) = \u2212c>h + n\u2211 i=1 \u2212b\u2217>v(i) \u2212 h> U(i) Dv(i)\nwith conditional distributions p(h|v) = \u220f j p(hj |v) p(v|h) = n\u220f i=1 p(v(i)|h)\np(hj = 1|v) = sigm ( cj +\nn\u2211 i=1 U (i) j\u00b7 Dv (i)\n)\np(v(i) = ek|h) = exp(b\u2217 > ek + h >U(i) Dek)\u2211K k\u2032=1 exp(b \u2217>ek\u2032 + h>U(i) Dek\u2032)\nwhere U (i) j\u00b7 refers to the j th row vector of U(i). The gradients with respect to this parameterization are easily derived from Eq. (9). We refer to this construction as a word representation RBM (WRRBM).\nIn contrast to Mnih and Hinton (2007), rather than training the WRRBM conditionally to model p(wn+t\u22121|wt, . . . , wn+t\u22122), we train it using Metropolis\u2013Hastings to model the full joint distribution p(wt, . . . , wn+t\u22121). That is, we train the WRRBM based on the objective L(\u03b8) =\u2212 \u2211 t log p(v(1) =ewt ,v (2) =ewt+1 , . . . ,v (n) wn+t\u22121)\nusing stochastic approximation from M\u2013H sampling of the word observations. For models with n > 2, we also found it helpful to incorporate `2 regularization of the weights, and to use momentum when updating U(i)."}, {"heading": "6.2. Chunking Task", "text": "As described by Turian et al. (2010), learning realvalued word representations can be used as a simple way of performing semi-supervised learning for a given method, by first learning word representations on unlabeled text and then feeding these representations as additional features to a supervised learning model.\nWe trained the WRRBM on windows of text derived from the English Gigaword corpus1. The dataset is a corpus of newswire text from a variety of sources. We extracted each news story and trained only on windows of n words that did not cross the boundary between two different stories. We used NLTK (Bird et al., 2009) to tokenize the words and sentences, and also corrected a few common punctuation-related tokenization errors. As in Collobert et al. (2011), we lowercased all words and delexicalized numbers (replacing consecutive occurrences of one or more digits inside a word with just a single # character). Unlike Collobert et al. (2011), we did not include additional capitalization features, but discarded all capitalization information. We used a vocabulary consisting of the 100,000 most frequent words plus a special \u201cunknown word\u201d token to which all remaining words were mapped.\nWe evaluated the learned WRRBM word representations on a chunking task, following the setup described in Turian et al. (2010) and using the associated publicly-available code, as well as CRFSuite2. As in Turian et al. (2010), we used data from the CoNLL2000 shared task. We used a scale of 0.1 for the word\n1http://www.ldc.upenn.edu/Catalog/catalogEntry. jsp?catalogId=LDC2005T12\n2http://www.chokkan.org/software/crfsuite/\nrepresentation features (as Turian et al. (2010) recommend) and for each WRRBM model, tried `2 penalties \u03bb \u2208 {0.0001, 1.2, 2.4, 3.2} for CRF training. We selected the single model with the best validation F1 score over all runs and evaluated it on the test set. The model with the best validation F1 score used 3-gram word windows, \u03bb = 1.2, 250 hidden units, a learning rate of 0.01, and used 100 steps of M\u2013H sampling to update each word observation in the negative data.\nThe results are reported in Table 1, where we observe that word representations learned by our model achieved higher validation and test scores than the baseline of not using word representation features, and are comparable to the best of the three word representations tried in Turian et al. (2010)3.\nAlthough the word representations learned by our model are highly effective features for chunking, an important advantage of our model over many other ways of inducing word representations is that it also naturally produces a feature vector for the entire ngram. For the trigram model mentioned above, we also tried adding the hidden unit activation probability vector as a feature for chunking. For each word wi in the input sentence, we generated features using the hidden unit activation probabilities for the trigram wi\u22121wiwi+1. No features were generated for the first and last word of the sentence. The hidden unit activation probability features improved validation set F1 to 95.01 and test set F1 to 94.44, a test set result superior to all word embedding results on chunking reported in Turian et al. (2010).\nAs can be seen in Table 3, the learned word representations capture meaningful information about words. However, the model primarily learns word represen-\n3Better results have been reported by others for this dataset: the spectral approach of Dhillon et al. (2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al. (2010). Unfortunately, the vocabulary and preprocessing differences mean that neither our result nor the one in Turian et al. (2010) are directly comparable to Dhillon et al. (2011).\ntations that capture syntactic information (as do the representations studied in Turian et al. (2010)), as it only models short windows of text and must enforce local agreement. Nevertheless, word representations capture some semantic information, but only after similar syntactic roles have been enforced. Although not shown in Table 3, the model consistently embeds the following natural groups of words together (maintaining small intra-group distances): days of the week, words for single digit numbers, months of the year, and abbreviations for months of the year. A 2D visualization of the word representations generated by t-SNE (van der Maaten and Hinton, 2008) is provided at http://i.imgur.com/ZbrzO.png."}, {"heading": "6.3. Sentiment Classification Task", "text": "Maas et al. (2011) describe a model designed to learn word representations specifically for sentiment analysis. They train a probabilistic model of documents that is capable of learning word representations and leveraging sentiment labels in a semi-supervised framework. Even without using the sentiment labels, by treating each document as a single bag of words, their model tends to learn distributed representations for words that capture mostly semantic information since the co-occurrence of words in documents encodes very little syntactic information. To get the best results on sentiment classification, they combined features learned by their model with bag-of-words feature vectors (normalized to unit length) using binary term frequency weights (referred to as \u201cbnc\u201d).\nWe applied the WRRBM to the problem of sentiment classification by treating a document as a \u201cbag of ngrams\u201d, as this maps well onto the fixed-window model for text. At first glance, a word representation RBM might not seem to be a suitable model for learning features to improve sentiment classification. A WRRBM trained on the phrases \u201cthis movie is wonderful\u201d and \u201cthis movie is atrocious\u201d will learn that the word \u201cwonderful\u201d and the word \u201catrocious\u201d can appear in similar contexts and thus should have vectors near each other, even though they should be treated very differently for sentiment analysis. However, a classconditional model that trains separate WRRBMs on n-grams from documents expressing positive and negative sentiment avoids this problem.\nWe trained class-specific, 5-gram WRRBMs on the labeled documents of the Large Movie Review dataset introduced by Maas et al. (2011), independently parameterizing words that occurred at least 235 times in the training set (giving us approximately the same vocabulary size as the model used in Maas et al. (2011)).\nTo label a test document using the class-specific WRRBM, we fit a threshold to the difference between the average free energies assigned to n-grams in the document by the positive-sentiment and negative sentiment models. We explored a variety of different hyperparameters (number of hidden units, training parameters, and n) for the pairs of WRRBMs and selected the WRRBM pair giving best training set classification performance. This WRRBM pair yielded 87.42% accuracy on the test set.\nWe additionally examined the performance gain by appending to the bag-of-words features the average ngram free energies under both class-specific WRRBMs. The bag-of-words feature vector was weighted and normalized as in Maas et al. (2011) and the average free energies were scaled to lie on [0, 1]. We then trained a linear SVM to classify documents based on the resulting document feature vectors, giving us 89.23% accuracy on the test set. This result is the best known result on this benchmark and, notably, our method did not make use of the unlabeled data."}, {"heading": "7. Conclusion", "text": "We have described a method for training RBMs with large K-ary softmax units that results in weight updates with a computational cost independent of K, allowing for efficient learning even when K is large. Using our method, we were able to train RBMs that learn meaningful representations of words and n-grams. Our results demonstrated the benefits of these features for chunking and sentiment classification and, given these successes, we are eager to try RBM-based models on other NLP tasks. Although the simple proposal distribution we used for M-H updates in this work is effective, exploring more sophisticated proposal distributions is an exciting prospect for future work."}], "references": [{"title": "Information Processing in Dynamical Systems: Foundations of Harmony Theory. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, pages 194\u2013281", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Unsupervised learning of distributions of binary vectors using 2-layer networks", "author": ["Y. Freund", "D. Haussler"], "venue": "In NIPS", "citeRegEx": "Freund and Haussler.,? \\Q1991\\E", "shortCiteRegEx": "Freund and Haussler.", "year": 1991}, {"title": "Factored 3-way restricted Boltzmann machines for modeling natural images", "author": ["M. Ranzato", "A. Krizhevsky", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Replicated Softmax: an Undirected Topic Model", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In NIPS", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Restricted Boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G.E. Hinton"], "venue": "In NIPS", "citeRegEx": "Welling et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2005}, {"title": "Parameter inference for imperfectly observed Gibbsian fields", "author": ["L. Younes"], "venue": "Probability Theory Related Fields,", "citeRegEx": "Younes.,? \\Q1989\\E", "shortCiteRegEx": "Younes.", "year": 1989}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "In NIPS", "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Morin and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "In NIPS", "citeRegEx": "Mnih and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "On the alias method for generating random variables from a discrete distribution", "author": ["R.A. Kronmal", "A.V. Perterson"], "venue": "The American Statistician,", "citeRegEx": "Kronmal and Perterson.,? \\Q1979\\E", "shortCiteRegEx": "Kronmal and Perterson.", "year": 1979}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "S\u00e9n\u00e9cal. Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J.-S"], "venue": "In AISTATS,", "citeRegEx": "Bengio and J..S.,? \\Q2003\\E", "shortCiteRegEx": "Bengio and J..S.", "year": 2003}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In ICML,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Mnih and Hinton.,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["P. Dhillon", "D.P. Foster", "L. Ungar"], "venue": "In NIPS", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In ACL,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The breadth of applications for the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund and Haussler, 1991) has expanded rapidly in recent years.", "startOffset": 71, "endOffset": 115}, {"referenceID": 1, "context": "The breadth of applications for the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund and Haussler, 1991) has expanded rapidly in recent years.", "startOffset": 71, "endOffset": 115}, {"referenceID": 2, "context": "For example, RBMs have been used to model image patches (Ranzato et al., 2010), text documents as bags of words (Salakhutdinov and Hinton, 2009), and movie ratings (Salakhutdinov et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 3, "context": ", 2010), text documents as bags of words (Salakhutdinov and Hinton, 2009), and movie ratings (Salakhutdinov et al.", "startOffset": 41, "endOffset": 73}, {"referenceID": 4, "context": ", 2010), text documents as bags of words (Salakhutdinov and Hinton, 2009), and movie ratings (Salakhutdinov et al., 2007), among other data.", "startOffset": 93, "endOffset": 121}, {"referenceID": 5, "context": "Although RBMs were originally developed for binary observations, they have been generalized to several other types of data, including integer- and real-valued observations (Welling et al., 2005).", "startOffset": 172, "endOffset": 194}, {"referenceID": 3, "context": "The issue is not one of representing such observations in the RBM framework: socalled softmax units (Salakhutdinov and Hinton, 2009) are the natural choice for modeling words.", "startOffset": 100, "endOffset": 132}, {"referenceID": 6, "context": "This procedure belongs to the general class of Robbins-Monro stochastic approximation algorithms (Younes, 1989).", "startOffset": 97, "endOffset": 111}, {"referenceID": 3, "context": "Because the multinomial probabilities are given by a softmax nonlinearity, the group of units v are referred to as softmax units (Salakhutdinov and Hinton, 2009).", "startOffset": 129, "endOffset": 161}, {"referenceID": 7, "context": "Dealing with large multinomial distributions is an issue that has come up previously in work on neural network language models (Bengio et al., 2001).", "startOffset": 127, "endOffset": 148}, {"referenceID": 7, "context": "Dealing with large multinomial distributions is an issue that has come up previously in work on neural network language models (Bengio et al., 2001). For example, Morin and Bengio (2005) addressed this problem by introducing a fixed factorization of the (conditional) multinomial using a binary tree in which each leaf is associated with a single word.", "startOffset": 128, "endOffset": 187}, {"referenceID": 7, "context": "Dealing with large multinomial distributions is an issue that has come up previously in work on neural network language models (Bengio et al., 2001). For example, Morin and Bengio (2005) addressed this problem by introducing a fixed factorization of the (conditional) multinomial using a binary tree in which each leaf is associated with a single word. The tree was determined using an external knowledge base, although Mnih and Hinton (2009) investigated ways of extending this approach by learning the word tree from data.", "startOffset": 128, "endOffset": 443}, {"referenceID": 9, "context": "While one strategy might be to use a conditional RBM to model the tree-based factorizations, similar to Mnih and Hinton (2009), the end result would not be an RBM model of n-gram word windows, nor would it even be a conditional RBM over the next word given the n\u2212 1 previous ones.", "startOffset": 104, "endOffset": 127}, {"referenceID": 10, "context": "edu/~gdahl) of Kronmal and Perterson (1979) can be used to generate samples in constant time with linear setup time.", "startOffset": 15, "endOffset": 44}, {"referenceID": 11, "context": "Although there is evidence (Hinton, 2002) that poorly-mixing Markov chains can yield good learning signals, when this will occur is not as well understood.", "startOffset": 27, "endOffset": 41}, {"referenceID": 11, "context": "Turian et al. (2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008).", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "(2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008).", "startOffset": 89, "endOffset": 112}, {"referenceID": 9, "context": "(2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). We have already mentioned the work of Mnih and Hinton (2009), who model the conditional distribution of the last word in n-gram windows.", "startOffset": 89, "endOffset": 147}, {"referenceID": 9, "context": "(2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). We have already mentioned the work of Mnih and Hinton (2009), who model the conditional distribution of the last word in n-gram windows.", "startOffset": 89, "endOffset": 209}, {"referenceID": 9, "context": "(2010) provide an overview and evaluation of these different methods, including those of Mnih and Hinton (2009) and of Collobert and Weston (2008). We have already mentioned the work of Mnih and Hinton (2009), who model the conditional distribution of the last word in n-gram windows. Collobert and Weston (2008) follows a similar approach, by training a neural network to fill-in the middle word of an n-gram window, using a marginbased learning objective.", "startOffset": 89, "endOffset": 313}, {"referenceID": 7, "context": "Therefore, we use an RBM parameterization very similar to that of Mnih and Hinton (2007), which itself is inspired by previous work on neural language models (Bengio et al., 2001).", "startOffset": 158, "endOffset": 179}, {"referenceID": 8, "context": "Therefore, we use an RBM parameterization very similar to that of Mnih and Hinton (2007), which itself is inspired by previous work on neural language models (Bengio et al.", "startOffset": 66, "endOffset": 89}, {"referenceID": 9, "context": "In contrast to Mnih and Hinton (2007), rather than training the WRRBM conditionally to model p(wn+t\u22121|wt, .", "startOffset": 15, "endOffset": 38}, {"referenceID": 13, "context": "As described by Turian et al. (2010), learning realvalued word representations can be used as a simple way of performing semi-supervised learning for a given method, by first learning word representations on unlabeled text and then feeding these representations as additional features to a supervised learning model.", "startOffset": 16, "endOffset": 37}, {"referenceID": 16, "context": "We used NLTK (Bird et al., 2009) to tokenize the words and sentences, and also corrected a few common punctuation-related tokenization errors.", "startOffset": 13, "endOffset": 32}, {"referenceID": 16, "context": "We used NLTK (Bird et al., 2009) to tokenize the words and sentences, and also corrected a few common punctuation-related tokenization errors. As in Collobert et al. (2011), we lowercased all words and delexicalized numbers (replacing consecutive occurrences of one or more digits inside a word with just a single # character).", "startOffset": 14, "endOffset": 173}, {"referenceID": 16, "context": "We used NLTK (Bird et al., 2009) to tokenize the words and sentences, and also corrected a few common punctuation-related tokenization errors. As in Collobert et al. (2011), we lowercased all words and delexicalized numbers (replacing consecutive occurrences of one or more digits inside a word with just a single # character). Unlike Collobert et al. (2011), we did not include additional capitalization features, but discarded all capitalization information.", "startOffset": 14, "endOffset": 359}, {"referenceID": 13, "context": "We evaluated the learned WRRBM word representations on a chunking task, following the setup described in Turian et al. (2010) and using the associated publicly-available code, as well as CRFSuite.", "startOffset": 105, "endOffset": 126}, {"referenceID": 13, "context": "We evaluated the learned WRRBM word representations on a chunking task, following the setup described in Turian et al. (2010) and using the associated publicly-available code, as well as CRFSuite. As in Turian et al. (2010), we used data from the CoNLL2000 shared task.", "startOffset": 105, "endOffset": 224}, {"referenceID": 13, "context": "The baseline results were taken from Turian et al. (2010). The performance measure is F1.", "startOffset": 37, "endOffset": 58}, {"referenceID": 9, "context": "79 HLBL (Mnih and Hinton, 2009) 94.", "startOffset": 8, "endOffset": 31}, {"referenceID": 14, "context": "00 C&W (Collobert and Weston, 2008) 94.", "startOffset": 7, "endOffset": 35}, {"referenceID": 13, "context": "representation features (as Turian et al. (2010) recommend) and for each WRRBM model, tried `2 penalties \u03bb \u2208 {0.", "startOffset": 28, "endOffset": 49}, {"referenceID": 13, "context": "The results are reported in Table 1, where we observe that word representations learned by our model achieved higher validation and test scores than the baseline of not using word representation features, and are comparable to the best of the three word representations tried in Turian et al. (2010).", "startOffset": 279, "endOffset": 300}, {"referenceID": 13, "context": "44, a test set result superior to all word embedding results on chunking reported in Turian et al. (2010).", "startOffset": 85, "endOffset": 106}, {"referenceID": 17, "context": "Better results have been reported by others for this dataset: the spectral approach of Dhillon et al. (2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al.", "startOffset": 87, "endOffset": 109}, {"referenceID": 13, "context": "(2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al. (2010). Unfortunately, the vocabulary and preprocessing differences mean that neither our result nor the one in Turian et al.", "startOffset": 147, "endOffset": 168}, {"referenceID": 13, "context": "(2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al. (2010). Unfortunately, the vocabulary and preprocessing differences mean that neither our result nor the one in Turian et al. (2010) are directly comparable to Dhillon et al.", "startOffset": 147, "endOffset": 294}, {"referenceID": 13, "context": "(2011) used different (less stringent) preprocessing and a vocabulary of 300,000 words and obtained higher F1 scores than the methods evaluated in Turian et al. (2010). Unfortunately, the vocabulary and preprocessing differences mean that neither our result nor the one in Turian et al. (2010) are directly comparable to Dhillon et al. (2011).", "startOffset": 147, "endOffset": 343}, {"referenceID": 12, "context": "tations that capture syntactic information (as do the representations studied in Turian et al. (2010)), as it only models short windows of text and must enforce local agreement.", "startOffset": 81, "endOffset": 102}, {"referenceID": 20, "context": "We trained class-specific, 5-gram WRRBMs on the labeled documents of the Large Movie Review dataset introduced by Maas et al. (2011), independently parameterizing words that occurred at least 235 times in the training set (giving us approximately the same vocabulary size as the model used in Maas et al.", "startOffset": 114, "endOffset": 133}, {"referenceID": 20, "context": "We trained class-specific, 5-gram WRRBMs on the labeled documents of the Large Movie Review dataset introduced by Maas et al. (2011), independently parameterizing words that occurred at least 235 times in the training set (giving us approximately the same vocabulary size as the model used in Maas et al. (2011)).", "startOffset": 114, "endOffset": 312}, {"referenceID": 20, "context": "We trained class-specific, 5-gram WRRBMs on the labeled documents of the Large Movie Review dataset introduced by Maas et al. (2011), independently parameterizing words that occurred at least 235 times in the training set (giving us approximately the same vocabulary size as the model used in Maas et al. (2011)). Table 2. Experimental results on the sentiment classification task. The baseline results were taken from Maas et al. (2011). The performance measure is accuracy (%).", "startOffset": 114, "endOffset": 438}, {"referenceID": 20, "context": "96 Maas et al. (2011)\u2019s \u201cfull\u201d method 87.", "startOffset": 3, "endOffset": 22}, {"referenceID": 20, "context": "96 Maas et al. (2011)\u2019s \u201cfull\u201d method 87.44 Bag of words \u201cbnc\u201d 87.80 Maas et al. (2011)\u2019s \u201cfull\u201d method 88.", "startOffset": 3, "endOffset": 88}, {"referenceID": 20, "context": "96 Maas et al. (2011)\u2019s \u201cfull\u201d method 87.44 Bag of words \u201cbnc\u201d 87.80 Maas et al. (2011)\u2019s \u201cfull\u201d method 88.33 + bag of words \u201cbnc\u201d Maas et al. (2011)\u2019s \u201cfull\u201d method 88.", "startOffset": 3, "endOffset": 150}, {"referenceID": 20, "context": "The bag-of-words feature vector was weighted and normalized as in Maas et al. (2011) and the average free energies were scaled to lie on [0, 1].", "startOffset": 66, "endOffset": 85}], "year": 2012, "abstractText": "The restricted Boltzmann machine (RBM) is a flexible model for complex data. However, using RBMs for high-dimensional multinomial observations poses significant computational difficulties. In natural language processing applications, words are naturally modeled by K-ary discrete distributions, where K is determined by the vocabulary size and can easily be in the hundred thousands. The conventional approach to training RBMs on word observations is limited because it requires sampling the states of K-way softmax visible units during block Gibbs updates, an operation that takes time linear in K. In this work, we address this issue with a more general class of Markov chain Monte Carlo operators on the visible units, yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n-grams using larger vocabularies than previously feasible with RBMs and by using the learned features to improve performance on chunking and sentiment classification tasks, achieving state-of-the-art results on the latter.", "creator": "TeX"}}}