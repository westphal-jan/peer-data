{"id": "1202.6221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2012", "title": "Confusion Matrix Stability Bounds for Multiclass Classification", "abstract": "in this paper, we provide new theoretical mathematical results on the various generalization properties of learning algorithms for multiclass data classification reliability problems. the originality concept of our rigorous work below is saying that again we first propose to commercially use just the natural confusion correction matrix of creating a classifier as a measure of its quality ; essentially our contribution is in the line of engineering work nowadays which attempts rather to later set follows up standards and study the broad statistical response properties feature of new evaluation code measures such as, e. g. roc curves. in the geometric confusion - based learning framework we propose, we claim that a targetted objective is assumed to maximize minimize beyond the size, of selecting the confusion matrix c, positively measured n through its operator valued norm | | in c | |. but we formally derive generalization function bounds strongly on exploring the ( size of the ) confusion matrix contained in an extended framework of uniform stability, closely adapted broadly to the case of matrix asset valued loss. pivotal to our previous study is noting a very recent matrix identity concentration inequality that generalizes mcdiarmid't s thompson inequality. as provided an illustration of the numerical relevance of our improved theoretical results, we both show how essentially two svm learning procedures can uniquely be proved harder to yet be naturally confusion - friendly. to verify the desired best of our knowledge, thus the 2016 present paper is reportedly the first experiment that closely focuses on the confusion oriented matrix hypothesis from a theoretical point of view.", "histories": [["v1", "Tue, 28 Feb 2012 14:03:11 GMT  (38kb)", "http://arxiv.org/abs/1202.6221v1", null], ["v2", "Thu, 24 May 2012 19:27:24 GMT  (50kb)", "http://arxiv.org/abs/1202.6221v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pierre machart", "liva ralaivola"], "accepted": false, "id": "1202.6221"}, "pdf": {"name": "1202.6221.pdf", "metadata": {"source": "CRF", "title": "Confusion Matrix Stability Bounds for Multiclass Classification", "authors": ["Pierre Machart", "Liva Ralaivola"], "emails": ["firstname.name@lif.univ-mrs.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 2.\n62 21\nv1 [\ncs .L\nG ]\n2 8\nFe b\n20 12\nIn this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm \u2016C\u2016. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid\u2019s inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.\nKeywords: Machine Learning, Stability Generalization Bounds, Confusion Matrix, Non-Commutative Concentration Inequality, Multi-Class"}, {"heading": "1 Introduction", "text": "Multiclass classification is an important problem of machine learning. The issue of having at hand statistically relevant procedures to learn reliable predictors is of particular interest nowadays, given the widespread need of such predictors in information retrieval, web mining, bioinformatics or neuroscience.\nYet, the literature on multiclass learning is not as voluminous than that of binary classification, whereas this problem raises questions from the algorithmic, theoretical and practical points of view. One of the prominent questions is that of the measure to use in order to assess the quality of a multiclass predictor. Here, we develop our results with the idea that the confusion matrix is a performance measure that deserves to be studied as it provides a finer information on the properties of a classifier than the mere misclassification rate. More precisely, building on very recent matrix-based concentration inequalities provided by Tropp (2011) \u2014sometimes referred to as noncommutative concentration inequalities\u2014 we establish a stability based framework for confusion-aware learning algorithm. In particular, we prove a generalization bound for confusion stable learning algorithms and show that there exist such algorithms in the literature. In a sense, our framework and our results extend those of Bousquet and Elisseeff (2002), which are designed for scalar loss functions. To the best of our knowledge, this is the first work that establishes generalization bounds for confusion matrices.\nThe paper is organized as follows. Section 2 describes the setting we are interested in and motivates the use of the confusion matrix as a performance measure. Section 3 introduces the new notion of stability that will prove essential to our study; the main theorem of this paper, together with its proof, are provided. Section 4 is devoted to the analysis of two SVM procdures in the light of our new framework. A discussion on the merits and possible extensions of our approach concludes the paper (Section 5).\n1"}, {"heading": "2 Confusion Matrix Awareness", "text": ""}, {"heading": "2.1 Notation", "text": "As said earlier, we focus on the problem of multiclass classification The input space is denoted by X and the target space is Y = {1, . . . , Q}. The training sequence Z = {Zi = (Xi, Yi)}mi=1 is made of m identically and independently random pairs Zi = (Xi, Yi) distributed according to some unknown (but fixed) distribution D over X \u00d7 Y. The sequence of input data will be referred to as X = {Xi}mi=1 and the sequence of corresponding labels Y = {Yi}mi=1, we may write Z = {X,Y }. The realization of Zi = (Xi, Yi) is zi = (xi, yi) and z, x and y refer to the realizations of the corresponding sequences of random variables. For a sequence y = {y1, \u00b7 \u00b7 \u00b7 , ym} of m labels, mq(y), or simply mq when clear from context, denotes the number of labels from y that are equal to q; s(y) it the binary sequence {s1(y), . . . , sQ(y)} of size Q such that sq(y) = 1 if q \u2208 y and sq(y) = 0 otherwise.\nWe will use DX|y for the conditional distribution onX given that Y = y; therefore, for a given sequence y = {y1, . . . , ym} \u2208 Ym, DX|y = \u2297mi=1DX|yi is the distribution of the random sample X = {X1, . . . , Xm} over X Tm such that Xi is distributed according to DX|yi ; for q \u2208 Y, andX distributed according to DX|y, Xq = {Xi1 , . . . , Ximq } denotes the random sequence of variables such that Xik is distributed according to DX|q. E[\u00b7] and EX|y[\u00b7] denote the expectations with respect to D and DX|y, respectively.\nFor a training sequence Z, Zi denotes the sequence {Z1, . . . Zi\u22121, Z \u2032i, Zi+1, . . . , Zm} where Z \u2032i is distributed as Zi; Z\n\\i is the sequence {Z1, . . . Zi\u22121, Zi+1, . . . , Zm} \u2014 these definitions carry directly over when conditioned on a sequence of labels y (with, henceforth, y\u2032i = yi).\nWe will consider a family H of predictors such that H \u2286 {h : h(x) \u2208 RQ, \u2200x \u2208 X}. For h \u2208 H, hq \u2208 RX denotes its qth coordinate. Also, \u2113 = (\u2113q)1\u2264q\u2264Q is a set of loss functions such that: \u2113q : H\u00d7X \u00d7Y \u2192 R+."}, {"heading": "2.2 Confusion Matrix", "text": "We here provide a discussion as to why minding the confusion matrix or confusion loss (terms that we will use interchangeably) is crucial in multiclass classification. We also introduce the reason why we may see the confusion matrix as an operator and, therefore, motivate the recourse to the operator norm to measure the \u2018size\u2019 of the confusion matrix. As the definition of the confusion loss in the online learning framework is less usual and a bit more intricate than its definition in the batch scenario, we develop the discussion here within the batch setting \u2014obviously, the argument carries over to the online learning framework.\nIn many situations, e.g. class-imbalanced datasets, it is important not to measure the quality of a predictor H on its classification error PXY (h(X) 6= Y ) only; this may lead to erroneous conclusions regarding the quality of h. Indeed, if, for instance, some class q is predominantly present in the data at hand, say P(Y = q) = 1 \u2212 \u03b5, for some small \u03b5 > 0, then the predictor hmaj that always outputs hmaj(x) = q regardless of x has a classification error lower thant \u03b5. Yet, it might be important not to classify an instance of some class p in class q: in the context of classifying mushrooms according to the categories {hallucinogen, poisonous, innocuous}, the consequence of predicting innocuous (the majority class) instead of hallucinogen or poisonous might be disastrous.\nAs a consequence, we claim that a more relevant object to consider is the confusion matrix which, given a binary sequence s = {s1 \u00b7 \u00b7 \u00b7 sQ} \u2208 {0, 1}Q, is defined as\nCs(h) := \u2211\nq:sq=1\nEX|qL(h,X, q),\nwhere, given an hypothesis h \u2208 H, x \u2208 X , y \u2208 Y, L(h, x, y) = (lij)1\u2264i,j\u2264Q \u2208 RQ\u00d7Q is the loss matrix such that:\nlij := { \u2113j(h, x, y) if i = yand i 6= j 0 otherwise.\nNote that this matrix has at most one nonzero row, namely its ith row.\nTechnical Report V 1.0 2\nFor a sequence y \u2208 Ym of m labels and a random sequence X distributed according to DX|y, the conditional empirical confusion matrix C\u0302y(h,X) is given by\nC\u0302y(h,X) := m\u2211\ni=1\n1\nmyi L(h,Xi, yi) =\n\u2211 q\u2208y Lq(h,X,y),\nwhere\nLq(h,X,y) := 1\nmq\n\u2211\ni:yi=q\nL(h,Xi, q).\nFor a random sequence Z = {X,Y } distributed according to Dm, the (unconditional) empirical confusion matrix is given by EX|Y C\u0302Y (h,X) = Cs(Y )(h), which is a random variable, as it depends on the random sequence Y . For exposition purposes it will often be more convenient to consider a fixed sequence y of labels and state results on C\u0302y(h,X), noting that\nEX|yC\u0302y(h,X) = Cs(y)(h). The slight differences between our definitions of (conditional) confusion matrices and the usual definition of a confusion matrix is that the diagonal elements are all zero and that they can accomodate any family of loss functions (and not just the 0\u2212 1 loss).\nA natural objective that may be pursued in multiclass classification is to learn a classifier h with \u2018small\u2019 confusion matrix, where \u2018small\u2019 might be defined with respect to (some) matrix norm of Cs(h). The norm that we retain is the operator norm that we denote \u2016 \u00b7 \u2016 from now on: for a matrix M , \u2016M\u2016 is defined as\n\u2016M\u2016 = max v 6=0 \u2016Mv\u20162 \u2016v\u20162 ,\nwhere \u2016\u00b7\u20162 is the Euclidean norm; \u2016M\u2016 is merely the largest singular value ofM \u2014note that \u2016M\u22a4\u2016 = \u2016M\u2016. A nice reason for focusing on the operator norm is that Cs(h) is often precisely used as an operator that acts on the vector of prior distributions\n\u03c0 = [P(Y = 1) \u00b7 \u00b7 \u00b7P(Y = Q)].\nIndeed, a quantity of interest is for instance the risk R\u2113(h) of h, with\nR\u2113(h) := \u2016\u03c0\u22a4C1(h)\u20161 = Q\u2211\np,q=1\nEX|p\u2113q(h,X, p)\u03c0p\n= EY\n{ Q\u2211\nq=1\nEX|Y \u2113q(h,X, Y )\n}\n= EXY\n{ Q\u2211\nq=1\n\u2113q(h,X, Y )\n} .\nIt is interesting to observe that, \u2200h, \u2200\u03c0:\n0 \u2264 R\u2113(h) = \u2016\u03c0C1(h)\u20161 = \u03c0\u22a4C1(h)1 \u2264 \u221a Q \u2225\u2225\u03c0\u22a4C1(h) \u2225\u2225 2 = \u221a Q \u2225\u2225C\u22a41 (h)\u03c0 \u2225\u2225 2\n\u2264 \u221a Q \u2225\u2225C\u22a4\n1 (h) \u2225\u2225 \u2016\u03c0\u20162 \u2264 \u221a Q \u2225\u2225C\u22a41 (h) \u2225\u2225 = \u221a Q \u2016C1(h)\u2016 ,\nwhere we have used Cauchy-Schwarz inequalty in the second line, the definition of the operator norm on the third line and the fact that \u2016\u03c0\u20162 \u2264 1 for any \u03c0 in {\u03bb \u2208 RQ : \u03bbq \u2265 0, \u2211 q \u03bbq = 1}.\nIn addition to support the use of the operator norm, this also says that bounding the norm of the confusion loss is a good way to control the risk of h as well (independently of the prior distribution, see discussion below).\nTechnical Report V 1.0 3"}, {"heading": "3 Deriving Stability Bounds on the Confusion Matrix", "text": "One of the most prominent issues in learning theory is to estimate the real performance of a learning system. The usual approach consists in studying how empirical measures converge to their expectation. In the traditional settings, it often boils down to providing bounds describing how the empirical risk relates to the expected one. In this work, we show that one can use similar techniques to provide bounds on the operator norm of the confusion matrix."}, {"heading": "3.1 Stability", "text": "Following the early work of Vapnik (1982), the risk has traditionally been estimated through its empirical measure and a measure of the complexity of the hypothesis class such as Vapnik-Chervonenkis (VC-dim), fat-shattering dimension or Rademacher complexity. During the last decade, a new and successful approach based on algorithmic stability to provide some new bounds has emerged. One of the highlights of this approach is the focus on some properties of the learning algorithm at hand, instead of the characterization of the hypothesis class. Roughly, this makes it possible to take advantage from the knowledge on how a given algorithm actually explores the hypothesis space, often leading to tighter bounds.\nThe main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability.\nDefinition 1 (Uniform stability Bousquet and Elisseeff (2002)). An algorithm A has uniform stability \u03b2 with respect to the loss function l if the following holds:\n\u2200S \u2208 Zm, \u2200i \u2208 {1, . . . ,m}, \u2016l(AS , .)\u2212 l(AS\\i , .)\u2016\u221e \u2264 \u03b2.\nFollowing the same approach, we now focus on the generalization of such results for confusion matrices. We introduce a new definition of confusion stability.\nDefinition 2 (Confusion stability). An algorithm A is confusion stable with respect to the loss matrix L if \u2200q \u2208 Y, there exist a \u03b2q decreasing as 1mq such that A has \u03b2q uniform stability with respect to the loss \u2113q"}, {"heading": "3.2 Non-Commutative McDiarmid", "text": "In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma\u2019s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function.\nSome more recent work Tropp (2011) extends McDiarmid\u2019s inequality to the matrix setting. For the sake of self-containedness, we recall this non-commutative bound.\nTheorem 1 (Matrix bounded difference (Tropp (2011), corollary 7.5)). Let S and Si be defined as above, and let H be a function that maps m variables to a self-adjoint matrix of dimension Q. Consider a sequence {Ai} of fixed self-adjoint matrices that satisfy\n( H(S)\u2212H(Si) )2 4 A2k, (1)\nwhere zi and zi range over all possible values of the space Zi it belongs to, for each index i. Then, for all t \u2265 0,\nP{\u03bbmax ( H(z)\u2212 EH(z) ) \u2265 t} \u2264 Qe\u2212t2/8\u03c32 ,\nwhere z = (Z1, . . . , Zm) and \u03c3 2 := \u2016\u2211i A2i \u2016.\nOne may notice thatH has to be applied on a function mapping to self-adjoint matrices. Unfortunately, our confusion matrices are real-valued but are not symmetric. However, we can make use of a dilation technique to overcome this.\nNamely, instead of working directly on a non-self-adjoint matrix A, we will build a self-adjoint dilated matrix D(A) :\nD(A) = ( 0 A A\u2217 0 ) ,\nTechnical Report V 1.0 4\nwhere A\u2217 denotes the adjoint of A. The choice of such a dilation is directly motivated by the following property, recalled in Tropp (2011).\nLemma 1. If \u03bb is the largest eigenvalue of D(A), then \u03bb is the largest singular value of A.\nAs a direct consequence, any result on the largest eigenvalue of a dilated matrix D(A) directly applies to the operator norm \u2016A\u2016 of A."}, {"heading": "3.3 Stability Bound", "text": "Theorem 2 (Confusion bound). Let A be a learning algorithm. Assume that all the loss functions under consideration take values in the range [0;M ]. Let y \u2208 Ym be a fixed sequence of labels.\nIf A has confusion stability \u03b2 with respect to all loss matrices Lq, for q \u2208 Y, then, \u2200m \u2265 1, \u2200\u03b4 \u2208 (0, 1), the following holds, with probability 1\u2212 \u03b4 over the random draw of X \u223c DX|y,\n\u2225\u2225\u2225C\u0302y(A,X)\u2212 Cs(y)(A) \u2225\u2225\u2225 \u2264 2 \u2211\nq\n\u03b2q +Q\n\u221a 8 ln ( Q2\n\u03b4\n)( 4 \u221a mmin\u03b2min +M \u221a Q\nmmin\n) .\nAs a consequence, with probability 1\u2212 \u03b4 over the random draw of Z \u223c Dm,\n\u2225\u2225\u2225C\u0302Y (A,X)\u2212 Cs(Y )(A) \u2225\u2225\u2225 \u2264 2 \u2211\nq\n\u03b2q +Q\n\u221a 8 ln ( Q2\n\u03b4\n)( 4 \u221a mmin\u03b2min +M \u221a Q\nmmin\n) .\nSketch of proof. The complete proof can be found in the following subsection. We proceed in three steps to proof the first bound. To start with, we know that by the triangle inequality\n\u2016C\u0302(A,X)\u2212 Cs(y)(A)\u2016 = \u2225\u2225\u2225\u2225\u2225 \u2211\nq\u2208y (Lq(AZ ,Z)\u2212 EXLq(AZ ,Z))\n\u2225\u2225\u2225\u2225\u2225\n\u2264 \u2211\nq\u2208y \u2016Lq(AZ ,Z)\u2212 EXLq(AZ ,Z)\u2016 . (2)\nUsing standard uniform stability techniques, we bound each summand with probability 1\u2212 \u03b4/Q. Then, using the union bound we have a bound on \u2016C\u0302(A,X) \u2212 Cs(y)(A)\u2016 that holds with probability at least 1\u2212 \u03b4. Finally, recoursing to a simple argument, we express the obtained bound solely with respect to mmin. In order to get the bound with the uncondional confusion matrix Cs(Y )(A) it suffices to observe that for any event E(X,Y ) that depends on X and Y , such that for all sequences y, PX|y{E(X,y)} \u2264 \u03b4, the following holds:\nPXY (E(X ,Y )) = EXY { I{E(X,Y )} }\n= EY { EX|Y I{E(X,Y )} } = \u2211\ny\nEX|Y I{E(X,Y )}PY (Y = y)\n= \u2211\ny\nPX|y{E(X,y)}PY (Y = y)\n\u2264 \u2211\ny\n\u03b4PY (Y = y) = \u03b4,\nwhich gives the desired result.\nRemark 1. It is straightforward to directly obtain a bound on \u2016Cs(y)(A)\u2016 and \u2016Cs(Y )(A)\u2016 by using the triangle inequality |\u2016A\u2016 \u2212 \u2016B\u2016| \u2264 \u2016A\u2212B\u2016 on the bounds given in the theorem.\nTechnical Report V 1.0 5"}, {"heading": "3.4 Proof of Theorem 2", "text": "Proof. To ease the readability, we introduce some additional notation: Z,Zi,Z\\i will refer respectively to {X,y}, {Xi,yi}, {X\\i,y\\i}, and\nLq = EX|qL(AZ, X, q), L\u0302q = Lq(AZ,X,y), Liq = EX|qL(AZi , X, q), L\u0302iq = Lq(AZi ,Xi,yi), L\\iq = EX|qL(AZ\\i , X, q), L\u0302\\iq = Lq(AZ\\i ,X\\i,y\\i).\nAfter using the triangle inequality in (2), we need to provide a bound on each summand. To get the result, we will, for each q, fix the Xk such that yk 6= q and work with functions of mq variables. Then, we will apply Theorem 1 for each Hq(Xq,yq) := D(Lq)\u2212D(L\u0302q). To do so, we will bound the differences\n\u2016Hq(Xq,yq)\u2212Hq(Xiq,yiq)\u2016.\nNote that\n\u2016Hq(Xq,yq)\u2212Hq(X iq,yiq)\u2016 = \u2016D(Lq)\u2212D(L\u0302q)\u2212D(Liq) +D(L\u0302iq)\u2016 = \u2016Lq \u2212 L\u0302q \u2212 Liq + L\u0302iq\u2016 \u2264 \u2016Lq \u2212 Liq\u2016+ \u2016L\u0302q \u2212 L\u0302iq\u2016\nStep 1: bounding \u2016Lq \u2212 Liq\u2016. We can trivially write:\n\u2016Lq \u2212 Liq\u2016 \u2264 \u2016Lq \u2212 L\\iq \u2016+ \u2016Liq \u2212 L\\iq \u2016\nUsing the \u03b2q-stability of A:\n\u2016Lq \u2212 L\\iq \u2016 = \u2225\u2225EX|q [L(AZ , X, q)\u2212 L(AZ\\i , X, q)] \u2225\u2225\n\u2264 EX|q \u2016L(AZ , X, q)\u2212 L(AZ\\i , X, q)\u2016 \u2264 \u03b2q,\nand the same holds for \u2016Liq \u2212 L \\i q \u2016, i.e. \u2016Liq \u2212 L \\i q \u2016 \u2264 \u03b2q. Thus, we have:\n\u2016Lq \u2212 Liq\u2016 \u2264 2\u03b2q. (3)\nStep 2: bounding \u2016L\u0302q \u2212 L\u0302iq\u2016. This is a little trickier than the first step.\n\u2016L\u0302q \u2212 L\u0302iq\u2016 = \u2225\u2225Lq(AZ,Z)\u2212 Lq(AZi ,Zi) \u2225\u2225\n= 1\nmq\n\u2225\u2225\u2225 \u2211\nk:k 6=i,yk=q\n( L(AZ, Xk, q)\u2212 L(AZi , Xk, q) ) + L(AZ, Xi, q)\u2212 L(AZi , X \u2032i, q) \u2225\u2225\u2225\n\u2264 1 mq\n\u2225\u2225\u2225 \u2211\nk:k 6=i,yk=q\n( L(AZi , Xk, q)\u2212 L(AZi , Xk, q) )\u2225\u2225\u2225+ 1 mq \u2225\u2225\u2225L(AZ, Xi, q)\u2212 L(AZi , X \u2032i, q) \u2225\u2225\u2225\nUsing the \u03b2q-stability argument as before, we have:\n\u2225\u2225\u2225 \u2211\nk:k 6=i,yk=q\n( L(AZ, Xk, q)\u2212 L(AZi , Xk, q) )\u2225\u2225\u2225 \u2264 \u2211\nk:k 6=i,yk=q \u2016L(AZ, Xk, q)\u2212 L(AZi , Xk, q)\u2016\n\u2264 \u2211\nk:k 6=i,yk=q 2\u03b2q \u2264 2mq\u03b2q.\nOn the other hand, we observe that\n\u2225\u2225\u2225L(AZ, Xi, q)\u2212 L(AZi , X \u2032i, q) \u2225\u2225\u2225 \u2264 \u221a QM.\nTechnical Report V 1.0 6\nIndeed, the matrix \u2206 := L(AZ, Xi, q)\u2212L(AZi , X \u2032i, q) is a matrix that is zero except for (possibly) its qth row, that we may call \u03b4q. Thus:\n\u2016\u2206\u2016 = sup v:\u2016v\u20162\u22641 \u2016\u2206v\u20162 = sup v:\u2016v\u20162\u22641 \u2016\u03b4q \u00b7 v\u2016 = \u2016\u03b4q\u20162,\nwhere v is a vector of dimension Q. Since each of the Q elements of \u03b4q is in the range [\u2212M ;M ], we get that \u2016\u03b4q\u20162 \u2264 \u221a QM.\nThis allows us to conclude that\n\u2016L\u0302q \u2212 L\u0302iq\u2016 \u2264 2\u03b2q + \u221a QM\nmq (4)\nStep 3: Applying Matrix McDiarmid Combining (3) and (4) that we just proved, we have that, for all i such that yi = q\n(Hq(Zq)\u2212Hq(Ziq))2 4 ( 4\u03b2q + \u221a QM\nmq\n)2 I.\nTherefore, Theorem 1 may be applied on Hq(Xq, yq) = D(Lq \u2212 L\u0302q) with\n\u03c32q = mq\u03b2q = ( 4 \u221a mq\u03b2q + \u221a QM\n\u221a mq\n)2\nto give, for t > 0:\nPX|y { \u2016Lq \u2212 L\u0302q \u2212 E[Lq \u2212 L\u0302q]\u2016 \u2265 t } \u2264 2Q exp    \u2212 t 2\n8 ( 4 \u221a mq\u03b2q + \u221a QM\u221a mq )2\n   ,\nwhich, using the triangle inequality |\u2016A\u2016 \u2212 \u2016B\u2016| \u2264 \u2016A\u2212B\u2016,\ngives\nPX|y { \u2016Lq \u2212 L\u0302q\u2016 \u2265 t+ \u2016EX|y[Lq \u2212 L\u0302q]\u2016 } \u2264 2Q exp    \u2212 t 2\n8 ( 4 \u221a mq\u03b2q + \u221a QM\u221a mq )2\n   .\nWe may want to bound \u2016EX|y[Lq \u2212 L\u0302q]\u2016. To do so, we note that for any i such that yi = q, and for X \u2032i distributed according to DX|q:\nEX|yL\u0302q = EX|yLq(AZ,X,y)\n= 1\nmq\n\u2211\nj:yj=q\nEX|yL(AZ, Xj, q)\n= 1\nmq\n\u2211\nj:yj=q\nEX,X\u2032 i |yL(AZi , X \u2032i, q)\n= EX,X\u2032 i |yL(AZi , X \u2032i, q).\nHence, using the \u03b2q stability\n\u2016E[Lq \u2212 L\u0302q]\u2016 = \u2225\u2225EX,X\u2032 i |y [L(AZ, X \u2032i, q)\u2212 L(AZi , X \u2032i, q)] \u2225\u2225 , \u2264 EX,X\u2032 i |y \u2016L(AZ, X \u2032i, q)\u2212 L(AZi , X \u2032i, q)\u2016\n\u2264 EX,X\u2032 i |y \u2016L(AZ, X \u2032i, q)\u2212 L(AZ\\i , X \u2032i, q)\u2016+ EX,X\u2032i|y \u2016L(AZi , X \u2032 i, q)\u2212 L(AZ\\i , X \u2032i, q)\u2016 \u2264 2\u03b2q.\nThis leads to\nPX|y { \u2016Lq \u2212 L\u0302q\u2016 \u2265 t+ 2\u03b2q } \u2264 2Q exp    \u2212 t 2\n8 ( 4 \u221a mq\u03b2q + \u221a QM\u221a mq )2\n   .\nTechnical Report V 1.0 7\nStep 4. Union Bound Now, using the union bound, we have\nP { \u2203q : \u2016Lq \u2212 L\u0302q\u2016 \u2265 t+ 2\u03b2q } \u2264 \u2211\nq\u2208Y P\n{ \u2203q : \u2016Lq \u2212 L\u0302q\u2016 \u2265 t+ 2\u03b2q }\n\u2264 2Q \u2211\nq\nexp    \u2212 t 2\n8 ( 4 \u221a mq\u03b2q + \u221a QM\u221a mq )2\n  \n\u2264 2Q2max q exp    \u2212 t 2\n8 ( 4 \u221a mq\u03b2q + \u221a QM\u221a mq )2\n  \nAccording to our definition of confusion stability (cf. Definition 2), \u03b2q decreases as 1\nmq . Therefore\nP { \u2203q : \u2016Lq \u2212 L\u0302q\u2016 \u2265 t+ 2\u03b2q } \u2264 2Q2 max\nq exp    \u2212 t 2\n8 ( 4 \u221a mmin\u03b2min + \u221a QM\u221a mmin )2\n  \nwhere mmin = minq mq and \u03b2min is the associated \u03b2q. Setting the right hand side to \u03b4, we can get, with probability 1\u2212 \u03b4,\n\u2211\nq\n\u2016Cq \u2212 Cqemp\u2016 \u2264 2 \u2211\nq\n\u03b2q +Q\n\u221a 8 ln ( 2Q2\n\u03b4\n)( 4 \u221a mmin\u03b2min +M \u221a Q\nmmin\n)\nHence the result."}, {"heading": "4 Analysis of existing algorithms", "text": "Now that the main result on stability bound has been established, we will investigate how some already existing multi-class algorithms display some stability properties and thus fall in the scope of our analysis. More precisely, we will analyse two well-known models for multiclass support vector machines and we will show that they to promote small confusion error. But first, we will study the more general stability of multi-class algorithms using regularization in Reproducing Kernel Hilbert Spaces (RKHS)."}, {"heading": "4.1 Hilbert Space Regularized Algorithms", "text": "Many well-known and widely-used algorithms feature a minimization of a regularized objective functionTikhonov and Arsenin (1977). In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer \u2126(h) may take the following form:\n\u2126(h) = \u2211\nq\n\u2016hq\u20162k.\nwhere k : X \u00d7 X \u2192 R denotes the kernel associated to the RKHS H. In order to study the stability properties of algorithms, minimizing a data-fitting term, penalized by such regularizers, in our multi-class setting, we need to introduce a minor definition that is an addition to definition 19 of Bousquet and Elisseeff (2002).\nDefinition 3. A loss function \u2113 defined on HQ \u00d7Y is \u03c3-multi-admissible if \u2113 is \u03c3-admissible with respect to any of his Q first arguments.\nThis allows us to come up with the following theorem.\nTheorem 3. Let H be a reproducing kernel Hilbert space (with kernel k) such that \u2200X \u2208 X , k(X,X) \u2264 \u03ba2 < +\u221e. Let L be a loss matrix, such that \u2200q \u2208 Y, \u2113q is \u03c3q-multi-admissible. And let A be an algorithm such that\nAS = argmin h\u2208HQ\n\u2211\nq\n\u2211\nn:yn=q\n1\nmq \u2113q(h, xn, q) + \u03bb\n\u2211\nq\n\u2016hq\u20162k. := argmin h\u2208HQ J(h).\nTechnical Report V 1.0 8\nThen A is confusion stable with respect to the loss matrix L. Moreover, \u2200q \u2208 Y, we have\n\u03b2q \u2264 \u03c32qQ\u03ba 2\n2\u03bbmq\nSketch of proof. Roughly, the idea is to exploit definition 3 in order to apply the theorem 22 of Bousquet and Elisseeff (2002) for each loss \u2113q. Moreover our regularizer is a sum (over q) of RKHS norms, hence the additional Q in the the bound on \u03b2q.\nFrom now on, we will always suppose that we are working with kernels such that k(X,X) \u2264 \u03ba2 < +\u221e."}, {"heading": "4.2 Lee, Lin and Wahba model", "text": "One of the most well-known and well-studied model for multi-class classification, in the context of SVM, was proposed by Lee et al. (2004). In this work, the authors suggest the use of the following loss function.\n\u2113(h, z) = \u2211\nq 6=y\n( hq(x) + 1\nQ\u2212 1\n)\n+\nTheir algorithm, denoted ALLW, then consists in minimizing the following (penalized) functional,\nJ(h) = 1\nm\nm\u2211\nk=1\n\u2211\nq 6=yk\n( hq(xk) + 1\nQ\u2212 1\n)\n+\n+ \u03bb\nQ\u2211\nq=1\n\u2016hq\u20162,\nwith the constraint \u2211\nq hq = 0. We can trivially rewrite J(h) as\nJ(h) = \u2211\nq\n\u2211\nn:yn=q\n1\nmq \u2113q(h, xn, q) + \u03bb\nQ\u2211\nq=1\n\u2016hq\u20162,\nwith\n\u2113q(h, xn, q) = \u2211\np6=q\n( hp(xk) + 1\nQ\u2212 1\n)\n+\n.\nIt is straightforward that for any q, \u2113q is 1-multi-admissible. We thus can apply theorem 3 and get\n\u03b2q \u2264 Q\u03ba 2\n2\u03bbmq .\nLemma 2. Let h\u2217 denote the solution found by ALLW. \u2200x \u2208 X , \u2200y \u2208 Y, \u2200q, we have \u2113q(h\u2217, x, y) \u2264 Q\u03ba\u221a\u03bb +1. Proof. As h\u2217 is a minimizer of J , we have\nJ(h\u2217) \u2264 J(0) = \u2211\nq\n\u2211\nn:yn=q\n1\nmq \u2113q(0, xn, q)\n= \u2211\nq\n\u2211\nn:yn=q\n1\n(Q\u2212 1)mq = 1.\nAs the data fitting term is non-negative, we also have\nJ(h\u2217) \u2265 \u03bb \u2211\nq\n\u2016h\u2217q\u20162k.\nGiven that h\u2217 \u2208 H, Cauchy-Schwarz inequality gives\n\u2200x \u2208 X , \u2016h\u2217q\u2016k \u2265 |h\u2217q(x)|\n\u03ba .\nCollecting things, we have\n\u2200x \u2208 X , |h\u2217q(x)| \u2264 \u03ba\u221a \u03bb .\nGoing back to the definition of lq, we get the result.\nTechnical Report V 1.0 9\nUsing theorem 2, it follows that, with probability 1\u2212 \u03b4,\n\u2225\u2225\u2225C\u0302Y (ALLW,X)\u2212 Cs(Y )(ALLW) \u2225\u2225\u2225 \u2264 \u2211\nq\nQ\u03ba2 \u03bbmq +\n\u221a 8 ln ( Q2\n\u03b4\n)( 2Q2\u03ba2 \u03bb + ( Q\u03ba\u221a \u03bb + 1 ) Q \u221a Q )\n\u221a mmin\n.\nWith regards to the \u03b2q we obtained, one can conclude that ALLW is confusion stable."}, {"heading": "4.3 Weston and Watkins model", "text": "One of the oldest models, when it comes to multi-class SVM is due to Weston and Watkins (1998). They consider the following loss functions.\n\u2113(h, x, y) = \u2211\nq 6=y (1\u2212 hy(x) + hq(x))+\nThe algorithm AWW minimizes the following functional\nJ(h) = 1\nm\nm\u2211\nk=1\n\u2211\nq 6=yk\n(1\u2212 hy(x) + hq(x))+ + \u03bb Q\u2211\nq<p=1\n\u2016hq \u2212 hp\u20162,\nThis time, for 1 \u2264 p, q \u2264 Q, we will introduce the functions hpq = hp \u2212 hq. We can then rewrite J(h) as\nJ(h) = \u2211\nq\n\u2211\nn:yn=q\n1\nmq \u2113q(h, xn, q) + \u03bb\nQ\u2211\np=1\np\u22121\u2211\nq=1\n\u2016hpq\u20162,\nwith\n\u2113q(h, xn, q) = \u2211\np6=q (1\u2212 hpq(xn))+ .\nIt still is straightforward that for any q, \u2113q is 1-multi-admissible. However, this time, our regularizer\nconsists in the sum of Q(Q\u22121)2 < Q2 2 norms. Applying theorem 3 therefore gives us \u03b2q \u2264 Q2\u03ba2\n4\u03bbmq\nLemma 3. Let h\u2217 denote the solution found by AWW. \u2200x \u2208 X , \u2200y \u2208 Y, \u2200q, we have \u2113q(h\u2217, x, y) \u2264 Q ( 1 + \u03ba \u221a Q \u03bb ) .\nThis lemma can be proven following exactly the same techniques and reasoning as Lemma 2. Using theorem 2, it follows that, with probability 1\u2212 \u03b4,\n\u2225\u2225\u2225C\u0302Y (AWW,X)\u2212 Cs(Y )(AWW) \u2225\u2225\u2225 \u2264 \u2211\nq\nQ2\u03ba2 2\u03bbmq +\n\u221a 8 ln ( Q2\n\u03b4\n)( Q3\u03ba2 \u03bb +Q 2 (\u221a Q+ \u03ba Q\u221a \u03bb ))\n\u221a mmin\n.\nWith regards to the \u03b2q we obtained, one can conclude that AWW is confusion stable."}, {"heading": "5 Discussion and Conclusion", "text": "In this paper, we have proposed a new framework, namely the algorithmic confusion stability, together with new bounds to characterize the generalization properties of multiclass learning algorithms. The crux of our study is to envision the confusion matrix as a performance measure, which differs from commonly encountered approaches that investigate generalization properties of scalar-valued performances (such as, e.g., the accuracy).\nA few questions that are raised by the present work are the following. Is it possible to derive confusion stable algorithms that precisely aim at controlling the norm of their confusion matrix? Are there other algorithms than those analyzed here that may be studied in our new framework? In particular, is it the case for k-nearest-neighbors, the generalization analysis of which is amenable thanks to classical stability arguments? On a broader perspective: how can noncommutative concentration inequalities be of some use in machine learning?\nTechnical Report V 1.0 10"}], "references": [{"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff", "year": 2002}, {"title": "An Introduction to Support Vector Machines: and Other Kernel-Based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor,? \\Q2000\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor", "year": 2000}, {"title": "Multicategory support vector machines", "author": ["Y. Lee", "Y. Lin", "G. Wahba"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Lee et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2004}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "In Surveys in Combinatorics,", "citeRegEx": "McDiarmid,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid", "year": 1989}, {"title": "Solutions of Ill-Posed Problems. Winston", "author": ["A.N. Tikhonov", "V.Y. Arsenin"], "venue": null, "citeRegEx": "Tikhonov and Arsenin,? \\Q1977\\E", "shortCiteRegEx": "Tikhonov and Arsenin", "year": 1977}, {"title": "User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics", "author": ["J.A. Tropp"], "venue": null, "citeRegEx": "Tropp,? \\Q2011\\E", "shortCiteRegEx": "Tropp", "year": 2011}, {"title": "Estimation of Dependences Based on Empirical Data", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1982\\E", "shortCiteRegEx": "Vapnik", "year": 1982}, {"title": "Multi-class support vector machines", "author": ["J. Weston", "C. Watkins"], "venue": null, "citeRegEx": "Weston and Watkins,? \\Q1998\\E", "shortCiteRegEx": "Weston and Watkins", "year": 1998}], "referenceMentions": [{"referenceID": 4, "context": "More precisely, building on very recent matrix-based concentration inequalities provided by Tropp (2011) \u2014sometimes referred to as noncommutative concentration inequalities\u2014 we establish a stability based framework for confusion-aware learning algorithm.", "startOffset": 92, "endOffset": 105}, {"referenceID": 0, "context": "In a sense, our framework and our results extend those of Bousquet and Elisseeff (2002), which are designed for scalar loss functions.", "startOffset": 58, "endOffset": 88}, {"referenceID": 5, "context": "1 Stability Following the early work of Vapnik (1982), the risk has traditionally been estimated through its empirical measure and a measure of the complexity of the hypothesis class such as Vapnik-Chervonenkis (VC-dim), fat-shattering dimension or Rademacher complexity.", "startOffset": 40, "endOffset": 54}, {"referenceID": 0, "context": "The main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability.", "startOffset": 20, "endOffset": 50}, {"referenceID": 0, "context": "The main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability. Definition 1 (Uniform stability Bousquet and Elisseeff (2002)).", "startOffset": 20, "endOffset": 179}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds.", "startOffset": 31, "endOffset": 61}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma\u2019s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function.", "startOffset": 31, "endOffset": 244}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma\u2019s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function. Some more recent work Tropp (2011) extends McDiarmid\u2019s inequality to the matrix setting.", "startOffset": 31, "endOffset": 524}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma\u2019s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function. Some more recent work Tropp (2011) extends McDiarmid\u2019s inequality to the matrix setting. For the sake of self-containedness, we recall this non-commutative bound. Theorem 1 (Matrix bounded difference (Tropp (2011), corollary 7.", "startOffset": 31, "endOffset": 703}, {"referenceID": 5, "context": "The choice of such a dilation is directly motivated by the following property, recalled in Tropp (2011). Lemma 1.", "startOffset": 91, "endOffset": 104}, {"referenceID": 3, "context": "1 Hilbert Space Regularized Algorithms Many well-known and widely-used algorithms feature a minimization of a regularized objective functionTikhonov and Arsenin (1977). In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer \u03a9(h) may take the following form:", "startOffset": 140, "endOffset": 168}, {"referenceID": 1, "context": "In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer \u03a9(h) may take the following form:", "startOffset": 33, "endOffset": 69}, {"referenceID": 0, "context": "In order to study the stability properties of algorithms, minimizing a data-fitting term, penalized by such regularizers, in our multi-class setting, we need to introduce a minor definition that is an addition to definition 19 of Bousquet and Elisseeff (2002). Definition 3.", "startOffset": 230, "endOffset": 260}, {"referenceID": 0, "context": "Roughly, the idea is to exploit definition 3 in order to apply the theorem 22 of Bousquet and Elisseeff (2002) for each loss lq.", "startOffset": 81, "endOffset": 111}, {"referenceID": 2, "context": "2 Lee, Lin and Wahba model One of the most well-known and well-studied model for multi-class classification, in the context of SVM, was proposed by Lee et al. (2004). In this work, the authors suggest the use of the following loss function.", "startOffset": 148, "endOffset": 166}, {"referenceID": 7, "context": "3 Weston and Watkins model One of the oldest models, when it comes to multi-class SVM is due to Weston and Watkins (1998). They consider the following loss functions.", "startOffset": 2, "endOffset": 122}], "year": 2017, "abstractText": "In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm \u2016C\u2016. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid\u2019s inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.", "creator": "LaTeX with hyperref package"}}}