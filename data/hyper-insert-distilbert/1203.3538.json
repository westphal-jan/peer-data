{"id": "1203.3538", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains", "abstract": "\u00ab despite offering the intractability potential of completely generic optimal partially observable open markov decision process planning, there exist important solution problems emerging that routinely have relatively highly structured models. previous researchers have aggressively used applying this insight to construct some more detail efficient prediction algorithms for heavily factored domains, and especially for analytic domains built with detailed topological structural structure in the lazy flat state correspondence dynamics model. in our case work, sometimes motivated by findings evolving from the education community relevant to automated tutoring, we basically consider problems that exhibit a form of strict topological structure in the efficient factored dynamics model. therefore our easy reachable free anytime data planner for imprecisely - sensed domains ( rapid ) precisely leverages that this structure to succeed efficiently immediately compute at a good initial envelope of previously reachable quantum states under the optimal naive mdp policy in time linear in testing the number sequences of potential state reference variables. rapid performs partially - objective observable planning over both the limited envelope \u03c9 of candidate states, periodically and hence slowly expands the intermediate state space than considered as time allows. rapid performs well on a 2d large tutoring - inspired problem simulation with class 122 input state variables, corresponding essentially to a smooth flat state space of over magnitude 10 ^ 30 states.", "histories": [["v1", "Thu, 15 Mar 2012 11:25:52 GMT  (297kb)", "http://arxiv.org/abs/1203.3538v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["emma brunskill", "stuart russell"], "accepted": false, "id": "1203.3538"}, "pdf": {"name": "1203.3538.pdf", "metadata": {"source": "CRF", "title": "RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains", "authors": ["Emma Brunskill"], "emails": [], "sections": [{"heading": null, "text": "Despite the intractability of generic optimal partially observable Markov decision process planning, there exist important problems that have highly structured models. Previous researchers have used this insight to construct more efficient algorithms for factored domains, and for domains with topological structure in the flat state dynamics model. In our work, motivated by findings from the education community relevant to automated tutoring, we consider problems that exhibit a form of topological structure in the factored dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains (RAPID) leverages this structure to efficiently compute a good initial envelope of reachable states under the optimal MDP policy in time linear in the number of state variables. RAPID performs partially-observable planning over the limited envelope of states, and slowly expands the state space considered as time allows. RAPID performs well on a large tutoring-inspired problem simulation with 122 state variables, corresponding to a flat state space of over 1030 states."}, {"heading": "1 INTRODUCTION", "text": "One of the key questions in artificial intelligence research is how to make good decisions in large, stochastic, partially observable environments. Though generic optimal planning for finite-horizon partially observable Markov decision processes (POMDPs) is known to be PSPACEcomplete (Papadimitriou & Tsitsiklis, 1987), fortunately, some important POMDP domains have highly structured models. This insight has been used by previous researchers to design more efficient POMDP algorithms that leverage different types of structure. Focussing on domains that exhibit factored structure has led to POMDP planners that solve some of the largest POMDP problems in the litera-\nture, including a hand washing assistance program (Boger et al., 2005) and a RoboCup rescue task (Paquet et al., 2005). Other recent work (Dai & Goldsmith, 2007; Dibangoye et al., 2009) has focused on domains where the flat state dynamics model limits the possible backtracking to earlier states, and showed that planning can be performed more efficiently when this topological structure is present.\nIn this paper we focus on problems exhibiting both factored structure and a form of topological structure, and demonstrate that we can leverage these properties to scale to very large domains. Such properties are common in a number of important applications ranging from tutoring to dialogue systems. For example, some prior education studies coarsely approximate a student\u2019s knowledge as a factored set of binary variables, one for each skill, and infers a precondition graph structure among skills (known as a \u201clearning hierarchy\u201d) from student data: see for example Gagnee\u0301\u2019s and Briggs (1974) and Close and Murtagh (1986). Despite this structure, automated tutor action selection remains challenging as the factored state space may consist of hundreds of skills. In addition, the student state is not directly observable, but can be probed through the use of drill exercises and other student responses. Modelling a fairly small curriculum of 100 skills using an atomic-state POMDP framework could require planning over a state space of size 2100 \u2248 1030 which is far outside the range of generic, flat POMDP solvers.\nSpecifically we consider constructing policies for POMDPs that exhibit the following three properties: they are\n1. factored, 2. have positive-only effects, and 3. have unique preconditions for each variable.\nFor compactness, in the rest of the paper we will refer to Positive-Only effects, Factored, with Unique Preconditions (POFUP) POMDPs as POFUPP processes. Factored representations are those in which the world state is represented by a vector of variables. Positive-only effects, commonly leveraged in classical planning, imply that once a binary variable becomes true, it will not later become false. Before we describe the third property, recall that in a factored\nrepresentation, a given state variable sk\u2019s value on a subsequent time step depends on the action chosen, and the values of a set of the other state variables (which could include sk on the previous time slice): in a dynamic Bayes net (DBN), these would be called the parents of sk. The unique preconditions assumption implies that there is a single set of values of sk\u2019s precondition variables that allow sk to become true. In all the education learning hierarchies we examined, there was always a unique set of preconditions for each variable. It is important to note that while there is a unique set of preconditions for each state variable, there are still numerous (potentially exponential in the number of variables) paths to reach each state. We assume that the planning objective is to reach a goal state.\nOur Reachable Anytime Planner for Imprecisely-sensed Domains (RAPID) leverages these three structural properties to construct an initial policy with a computational cost that scales polynomially with the number of domain variables, instead of exponentially. RAPID first computes a solution to the fully observable MDP starting at an initial state sampled from the initial POMDP belief state. This process is very fast, taking only time linear in the number of state variables. RAPID then performs partially-observable planning over the limited envelope of states reached under this MDP policy, and then slowly expands the state space considered as time allows. At most the state space envelope will expand to become the reachable state space given the initial potential starting states, which is typically much smaller than the exponential potential state space.\nWe present promising experimental results on two large tutoring-inspired simulations. The second problem consists of 122 variables, or a potential flat state space of over 1030. RAPID manages to achieve good performance quickly in both problems, though several comparison planners, including a factored approach, fail to find a good policy."}, {"heading": "2 RELATEDWORK", "text": "There has been significant recent progress on planning in partially observable, stochastic domains. Two of the fastest generic POMDP planners are HSVI by Smith and Simmons (2005) and SARSOP by Kurniawati, Hsu and Lee (2008). Neither approach takes advantage of factored structure.\nA number of prior fully-observable MDP approaches do leverage factored structure (such as Boutlier, Dearden and Goldsmidt (2000)). Symbolic Perseus (Boger et al., 2005) and Symbolic HSVI (Sim et al., 2008) are two offline POMDP algorithms for factored state spaces which scale to large problems. In practice both perform fairly similarly to each other. Online, forward search POMDP planners can also leverage factored structure, and Paquet, Tobin and Chaib-draa (2005) used forward search to handle an extremely large, factored RoboCup rescue problem. How-\never, their approach and other forward search techniques typically scale as O((|A||Z|)H) where H is the search horizon,and |A| and |Z| are, respectively, the action and observation branching factors. Such approaches will typically struggle in long horizon problems with a large number of observations or actions unless value heuristics can be used to shape the search. Unlike our algorithm, these factored approaches do not leverage any further structure in the domain dynamics.\nSeveral recent approaches do seek to leverage topological structure in the dynamics model similar to the structure implied by our second and third assumptions. Dai and Goldsmith (2007) leveraged the presence of layered positiveeffect state structure (certain clusters of states cannot be returned to) in their Topological Value Iteration (TVI) MDP algorithm. Dibangoye et al. (2009) assumed a similar structure, and used this to create a heuristic Topological Order Planner (TOP) for POMDPs. These and related approaches consider structure in the ground state space: in contrast, our approach considers structure in the factored space. Focussing on structure in the factored space helps our approach to scale to large domains as we can often avoid even enumerating the flat state space.\nFinally, our approach is inspired by work in the fully observable planning community. To scale to very large, fully observable MDPs, Dean et al. (1995) proposed an anytime approach which initially restricts MDP planning to a smaller envelope of reachable states. Gardiol and Kaelbling (2004) extended this approach to be applicable in relational MDPs using action-based equivalence. To our knowledge our RAPID algorithm is the first approach that performs envelope-based planning in partially observable environments."}, {"heading": "3 PROBLEM DESCRIPTION", "text": "We are interested in decision making in POFUP partially observable, stochastic environments that may be specified by the tuple \u3008S,L,A,Z, b0, E, p((s i)\u2032|si, a), . . . p(z|(si)\u2032, a), r(s, a), sG, sT \u3009 where\n\u2022 S is a set of states. The domain consists of L binaryvalued variables s1, s2, . . . , sL, and each state is an\nassignment of values (true or false) to all the domain variables: s = \u3008s1, s2, . . . , sL\u3009.\n\u2022 A is a set of actions. Each action aij is associated with a particular state variable si and has the potential to\nmake only that variable true.1 There will generally be multiple actions associated with the same state variable si. For example, there could be a drill exercise\n1Actions or operators which have a single effect have been previously described as unary operators (Brafman & Domshlak, 2003).\naction and a lesson action to help a student understand two-digit addition.\n\u2022 Z is a set of observations.\n\u2022 b0 is the initial belief state which is a sparse representation of the possible initial states and associated prob-\nabilities. The sum of the probabilities over all possible initial states is constrained to equal 1.\n\u2022 E is a precondition graph which specifies for each state variable si the set of state variables\nsip1, sip2, . . . sipM (equivalent to parents of si in a DBN) that must be true before state variable si can become true. We assume there is a unique conjunction of precondition variables for each variable (for example, s1\u2227s2 can be a precondition, but not s1\u2228s2). As a concrete example, the precondition graph for a student to master the multiplication skill would include the addition skill as a prerequisite.\n\u2022 p((si)\u2032 = false|si = false, aij) specifies the probability of a state variable si remaining false even when\nall si\u2019s preconditions are satisfied and a relevant action ai\u2217 is taken. If a state variable s i\u2019s preconditions are not satisfied, and action ai\u2217 is applied, s i always remains false. Continuing the prior example, let aij be a multiplication exercise, and si be the multiplication skill. Then p((si)\u2032 = false|si = false, aij) is the probability that after trying a multiplication exercise, a student still may not yet understand multiplication, even if she has all the necessary preconditions skills (addition, etc.) as specified in E.\n\u2022 p(z|(si)\u2032, aij) specifies the probability of receiving a particular observation given that a particular action aij is taken, and the resulting value of the action\u2019s asso-\nciated state variable (si)\u2032. Note that since an action is only associated with a single variable, only a single p(z|(si)\u2032, aij) will be applicable at each time step.\n\u2022 r(s, aij) is the reward for taking action aij in state s. The reward is negative, and depends only on the action\n(aka independent of the state) for all states except the goal state sG and terminal state sT .\n\u2022 sG is the goal state. r(sG, a) is positive or zero.\n\u2022 sT is the terminal state. sG deterministically transitions to sT . sT is a sink state where the reward is 0\nand the observation probabilities are identical to the observation probabilities of sG.\nFigure 1 shows the relation between the number of variables as expressed in a precondition graph, and the potential state space and state transition graph.\nAs the states are partially observable, we maintain a distribution over states, known as the belief state, which is a sufficient statistic of the history of actions taken and observations received. The planning objective is to maximize the expected sum of rewards given the initial belief state b0. Due to the reward formulation, this is similar to a partially observable, stochastic shortest path problem."}, {"heading": "4 ALGORITHM", "text": "Prior flat and factored POMDP approaches typically fail to scale to domains with a large number of variables. This often continues to hold true even when, for particular initial belief states, the reachable state space is significantly smaller than the full state space.\nInstead we draw inspiration from envelope-based planning algorithms for large fully observable MDPs and extend these ideas to our POFUPP domains. Dean et al. (1995) presented the idea of computing a policy for fully observable, flat MDPs by planning only over a smaller envelope of states. As time allowed, the state envelope was expanded to include more of the reachable state space.\nAlgorithm 1 RAPID: REACHABLE ANYTIME PLANNING FOR IMPRECISELY-SENSED DOMAINS\n1: Sample an initial state from the initial belief 2: Construct an initial envelope using a deterministic\nMDP relaxation that can be solved efficiently.\n3: while remaining time do 4: Define & solve a POMDP over the envelope 5: Expand the envelope 6: end while\nTo our knowledge RAPID is the first algorithm to take a similar approach in the context of partially-observable planning. There are several key technical challenges that need to be overcome to apply envelope-based planning in partially observable domains that can be characterized as POFUPP problems. First, we require an algorithm for efficiently computing a good initial envelope over the large, factored, partially observable state space. Second, we need a method for converting this envelope into a fully defined POMDP and solving the resulting model. We present solutions for both these challenges, and RAPID\u2019s empirical efficiency allows us to scale to very large problem sizes. The RAPID algorithm is summarized in Algorithm 1."}, {"heading": "4.1 INITIAL ENVELOPE CONSTRUCTION", "text": "Given a POFUPP process M , we first need to construct an initial envelope of states. Ideally the envelope would include states that have a reasonable probability of being visited given a good policy for the partially observable domain. The states visited along the optimal MDP solution starting with one of the possible initial states would seem intuitively to be reasonable, as the MDP solution forms an upper bound on the POMDP performance. However, standard MDP value iteration will be intractable since it scales as a function of the state space, which in our process is an exponential function of the number of variables. Even alternate factored solvers will typically be too slow.\nInstead we propose an approach which leverages the particular properties of our structured process by first relaxing the process to its deterministic, fully observable equivalent, and use this to very quickly compute a good trajectory between a start state s0 and the goal sG.\nWe first sample a state s0 from the initial belief state b0. Given s0, and the variable precondition graph E, RAPID identifies the state variables whose value is false in s0 and true in the goal state sG. RAPID then computes a topological order of these state variables given the precondition graph E. A topological order of these variables is any linear ordering such that each state variable comes before all other state variables to which it has outbound arrows in the precondition graph. For example, in Figure 1a, state variable L1 must appear before all other variables, and L2 must appear before L3. As the precondition graph E is a\ndirected acyclic graph (DAG)2, the topological order can be computed in time linear in the number of state variables and precondition conditions (Cormen et al., 1999).\nThe computed topological state variable ordering (such as \u3008s2, s68, . . . s16\u3009) is converted into a state trajectory between the start s0 and goal state sG by simply adding in order each state variable to the original s0. Therefore the cost of generating an initial envelope is simply a linear function of the number of state variables. In Section 4.5 we will show that this state trajectory consists of the state variables visited by following an optimal MDP policy for M starting at the sampled state s0."}, {"heading": "4.2 ENVELOPE POMDP POLICY GENERATION", "text": "RAPID proceeds by defining a POMDP P \u2032 over the current state envelope. We supplement the envelope state space defined by the state trajectory sequence by two additional states: a terminal out state stout, and a terminal goal state stg . The definition of an out state follows prior work in the fully observable envelope literature (Dean et al., 1995). The dynamics of the states within the envelope are the same as in the original process M , except if a state transition lead to a state outside the envelope, then that transition, and associated probability, are set to go to the sout state. The out state itself transitions with probability one to the terminal out sink state stout which has self-loop dynamics. The separation of stout and sout is done in order to specify separate reward functions.\nTo discourage leaving the envelope, the reward for the out state is set to a large negative value. stout has reward zero. Separating sout from stout allows there to be a single shot cost for exiting the envelope.3\nThe observation model for all states within the envelope is the same as in the POFUPP M . In contrast to envelope planners for fully observable MDPs where all states, including the out state, is fully observed, in POMDP domains the out states are most naturally modeled as partially observable, since they represent the remaining partially observable states that are not in the envelope. This raises the interesting side problem of how to represent the observation probabilities for the out states, which represent the potential observation probabilities of all states outside the envelope. In general there will be an exponential (in the number of variables) states outside of the envelope, and so for now we take the simple approach of approximating the observation probability of sout by averaging the observation models of a sampled set of states lying outside the envelope. The observation model of stout is identical to sout.\n2Since actions have positive-only effects, there are also no cycles in the corresponding state dynamics.\n3An alternate strategy would be to define rewards over state, action, next state tuples.\nIf there is any initial probability over states outside of the envelope, then a new belief state is defined over only the envelope state space, with all remaining probability mass in the out state sout.\nPOMDP P \u2032 can be solved using any generic POMDP planner with optimality bounds and in our experiments we used the publicly-available HSVI (Smith & Simmons, 2005). POMDP planning proceeds until the error bound over the initial belief state drops below a chosen \u01eb-threshold, or a specified time limit is reached.\nNote that the computed policy for POMDP P \u2032 can be used to act in the original POFUPP M ."}, {"heading": "4.3 ENVELOPE EXTENSION", "text": "If additional planning time is available after the initial policy is computed, then the state envelope can be expanded. There are numerous potential strategies for envelope expansion and in this initial work we used a simple, but empirically effective approach. We consider three possible methods, in order, for identifying a new state to add to the envelope; in other words, we try the first method and see if it identifies a new state to be added, if it does, we stop, else we run the second method, etc.\nThe first method samples any potential initial state s0i which has non-zero probability in the initial belief state b0, but is not yet part of the envelope of states. If all potential initial states are in the envelope, the second method tries to find a new non-envelope state by expanding the envelope fringe. This expansion is performed by starting at a possible initial state and simulating a trajectory using an \u01ebrgreedy policy4 until either a non-envelope state is reached, or a goal state is reached. This process is repeated until a non-envelope state is reached or a set number of iterations pass. If no non-envelope states are found, in the third method, we iterates through each state and tries all applicable actions (given the preconditions the state represents) to see if a new non-envelope state is reachable. This ensures that, given enough time, the envelope will grow to reach the full reachable state space, given the possible initial states defined by the initial belief state.\nOnce a non-envelope state is identified, it must be added to the envelope. In many cases these newly-added states will be multiple state transitions from the existing envelope of states. For example, consider a mathematics tutor domain where to start a student either knows algebra, or algebra and calculus. If the initial envelope is constructed starting from the state where the student knows calculus, and then the state representing the student only knows algebra is added, there are many missing steps between algebra and calculus that need to be added in order to compute a reasonable pol-\n4The POMDP policy is followed (1\u2212\u01ebr) fraction of time time, and a random action is taken \u01ebr fraction of the time.\nicy for the newly added initial state. To address this, when a new potential state is added, RAPID re-performs the initial envelope construction of creating a complete state trajectory to the goal, starting from the newly added state. This process is very fast, and the main limitation of this approach is that it can add O(L) states to the envelope per state, which slows down the POMDP planning process. However, the benefit of increasing the probability that the new states will immediately improve the computed policy, was thought to outweigh this slight shortcoming."}, {"heading": "4.4 PERFORMANCE AND COMPUTATIONAL COMPLEXITY", "text": "First, for completeness, we note that RAPID is guaranteed to converge to an \u01eb-optimal policy, as long as an \u01eb-optimal POMDP planner is used, since RAPID is guaranteed to eventually expand the envelope to include all states reachable from the initial belief state.\nComputing a state trajectory from an initial to goal state, and associated value computations, takes time linear in the number of variables. The initial envelope will have at most O(L) states, which means that the initial POMDP planning will be performed over a state space which is a linear function of the number of variables. The maximum number of states in the envelope is the reachable state space, which is typically much smaller than the potential 2L state space. The complexity of solving a POMDP depends on the particular technique. HSVI performs a depth-first roll out, and updates an explicit representation of an upper and lower bounds on the POMDP value function along the roll out. Each lower bound backup and belief update is a quadratic function of the number of states, so both operations will be impacted positively by a smaller input state space."}, {"heading": "4.5 UPPER BOUNDS FOR POFUPP PROBLEMS", "text": "We will shortly prove that the trajectory of states between a start and the goal state, as computed during envelope initialization and expansion, consists of states visited by following an optimal policy for the fully observable MDP of the POFUPP process. We leverage this property to efficiently compute the fully-observable optimal MDP value of the states within the envelope, which can then be used to calculate an upper bound on the initial belief state b0. Such bounds can be useful for at least two reasons. First, many POMDP solvers (including HSVI and SARSOP) use upper bounds during planning. Typically these bounds are computed by solving the MDP, which is known to be an upper bound to the POMDP values. However, solving the flat MDP typically requires multiple backup operations, each of which requires time polynomial in the number of states. Second, upper bounds provide useful benchmarks for evaluating RAPID\u2019s performance. However, solving the MDP upper bound over the complete factored space of hundreds\nor more variables is computationally infeasible. In contrast, our approach scales as O(LNb0) where Nb0 is the number of initial states with non-zero probabilities.\nWe now illustrate how we compute the value of the the states along a trajectory between a start and goal state, as returned during envelope initialization and expansion. We first modify the original rewards. Let r\u0303(s\u2212i, aij) be the new reward for taking action aij in a state s\u2212i where state variable si is false but all its preconditions are true. We define the value of this new reward as:\nr\u0303(s\u2212i, aij) = r(s\u2212i, aij)\n1\u2212 p((s\u2212i)\u2032 = false|s\u2212i = false, aij) . (1)\nIntuitively, r\u0303(s\u2212i, aij) represents the expected reward/cost of making state variable si true using action aij , given the stochasticity of action aij . To compute the state trajectory values, we start with the goal state, and traverse the trajectory backwards, at each step selecting the action aij with the minimum expected cost r\u0303 required to make the subsequent variable si in the consecutive state true. The values are computed simultaneously, by summing up the rewards during the traversal:\na\u2217(s\u2212i) = argmax j r\u0303(s\u2212i, ai,j) (2) V (s\u2212i) = r\u0303(s\u2212i, a \u2217(s\u2212i)) + V (s+i) (3)\nwhere state s+i is the same as state s\u2212i except now state variable si is also true. This value computation requires time linear in the number of variables. This process can be done at the same time as when the state trajectory is constructed from the topological order.\nTheorem 1. Given a POFUPP M , let Mf be the fullyobservable MDP version ofM , s0 be a state sampled from b0, {s0, straj1, . . . , sG} be the state trajectory computed by the initial envelope method, \u03c0Mf be the associated policy, and V (s0), . . . , V (sG) be the calculated state trajectory values. These values and policy represent an optimal policy and the optimal values of these states in the MDP Mf .\nProof. (Sketch) The initial topological order constructed is an optimal plan to the goal from the start state s0 for the deterministic, uniform action-cost, fully observable process Mduf version of the POFUPP M . This is true due to the particular POFUPP structure assumed. Briefly, the positive-only effects and the presence of unique preconditions to make a single variable true, imply that all permutations (that respect the precondition structure) of the same set of state variables will result in the same final state. As in Mduf all rewards are constant except at the goal, all paths of the same length between the same start state and the goal state will have the same cost. Therefore we can arbitrarily select any ordering that respects the preconditions, and its value is guaranteed to be optimal (and equal to all other topological orderings between the same start state and goal).\nTo determine the optimal value (and policy) of each state along the corresponding state trajectory in the deterministic (but with the original action costs/rewards) MDP Mdf version of M requires considering the state-action values of each state. From Bellman the state-action value can be expressed as the immediate reward of taking an action in a state, plus the future expected reward. As we currently assume each action is deterministic, the state action value of a state s\u2212k which is a state where state variable s k is false but all its preconditions are true, can be expressed as\nQ(s\u2212k, akj) = r\u0303(s\u2212k, akj) + V (s+k) (4)\nwhere s+k is the state identical to s\u2212k except state variable sk is also true. In the deterministic MDP M , Q(s, akj) represents the expected cost of making the state variables in sG true which are false in the current state s. However, since in a POFUPP process each variable requires a unique set of precondition variables to be true, the order in which these state variables are acquired is irrelevant: any order that satisfies the precondition structure E is equivalent. The only difference in rewards/costs comes from which action akj out of a set of actions ak\u2217 is chosen to achieve a state variable sk; note here that all ak\u2217 have the same preconditions, but they may have different costs, different selftransition probabilities, and different observation probabilities. Therefore, the state trajectory obtained from the topological order is equal to any other trajectory of states between s0 and the goal G. Given this, action selection for each state along the trajectory can be restricted without loss of optimality to only those actions which pertain to the next variable to be acquired along the topological order (as specified by Equation 2). This means that the next state s+k in Equation 4 will be identical for all considered actions ak\u2217, and to find the optimal action it suffices to only consider the immediate expected reward r\u0303. Therefore the policy and values in Equation 2 and 3, respectively, represent an optimal policy and the optimal value for the deterministic MDP.\nFinally, the MDP Mf falls into the class of stochastic shortest path problems. Therefore the computed value function and policy for the deterministic MDP Mdf which has noself loops (as just specified) using the modified rewards defined in Equation 1 has an identical policy and value function to the original MDP Mf (pg.25, Bertsekas and Tsitsiklis, (1996)). Therefore, the values and policy computed using Equation 2 and 3 for all states along the trajectory are guaranteed to be optimal for MDP Mf .\nTheorem 1 shows that we can efficiently compute the optimal MDP values for states inside the constructed envelope.\nOnce the envelope includes all possible initial states, we will have a value on each initial state computed using Equation 3. We can then compute an upper bound for initial belief state value (V\u0304 (b0)) by taking the weighted sum of the\nstate values:\nV\u0304 (b0) = \u2211\nssi\nb0(ssi)V (ssi). (5)\nNote that this provides an upper bound to the original POFUPP process: in contrast, any bounds computed by the POMDP solvers over the envelope only apply to the restricted envelope POMDP P \u2032. We later calculate V\u0304 (b0) for our two experimental domains. This bound can be computed in time linear in the product of the number of state variables and initial possible states."}, {"heading": "5 EXPERIMENTS", "text": "Due to our interest in tutoring applications, we performed simulation experiments in two tutoring-inspired domains."}, {"heading": "5.1 DOMAINS", "text": "In both cases, the variable precondition graph construction was informed by literature from the education communities: the transition probabilities, observation probabilities and reward values were chosen by hand.\nThe first domain, SmallMath, consisted of 19 elementary math skills, yielding a potential state space of 219 \u223c 500, 000 states. The precondition graph for the skills is displayed in Figure 2. There are two possible observations, and 38 actions, 2 for each skill. The first action for a skill, a \u201cteaching\u201d action, has a high probability of causing the skill to transition to being true (p = 0.8) if it is not already and the preconditions for that skill are fulfilled; however, it does not provide any feedback about whether the student has successfully acquired the skill. In our experiments we set the probability of each observation is 0.5 for actions 1,3,5,. . .,37. The second action for each skill (actions 2,4,. . .,38) loosely corresponds to a practice exercise, and only causes skill acquisition with probability 0.5. However, practice exercises provide more useful feedback about whether that skill was acquired: the observation is true with probability 0.9 if the hidden skill is true, and true\nwith probability 0.2 if the skill is false. The reward for reaching the state where all skills are true was set to 10000, and there was a reward of -1 for all other states and actions. The initial belief state had three non-zero initial start states.\nIn the past there have been a number of papers on \u201clearning hierarchies\u201d in the education literature. Learning hierarchies consist of ordered hierarchies, or graphs, of skills, which are very similar to our variable precondition graphs. Numerous classroom studies have been done to construct these learning hierarchies from student data, though the analysis historically treats the data as fully observable rather than modeling student knowledge as a hidden state.\nGiven this work, for the second domain, BigMath, we constructed a larger tutoring-inspired problem consisting of addition, subtraction, multiplication, and addition and subtraction of fractions skills. The fraction precondition graph was derived from Miller and Phillips (1974) and Uprichard and Phillips (1977). We combined the fraction precondition structure with the subtraction hierarchy from Gagne\u0301 (1974), and the addition, subtraction, multiplication and division hierarchies from Close and Murtagh (1986). The full precondition graph is displayed in Figure 3 and consisted of 122 skills.5 The flat state space is 2122 which is over 1030 states. Similar to the first domain, we created an action space with two potential actions for each skill, one lesson-like action, and one drill-like action. The observation and transition probabilities, given the precondition variables are satisfied, were defined the same way as in the SmallMath domain. The reward for reaching the state where all skills are true was set at 100000, and there was a reward of -1 for all other states and actions. The original belief state had four non-zero probability initial start states, consisting of plausible variable subgroups.\nNote that the horizon of both problems is quite long. Even in the deterministic versions of both problems, if the world state starts with no variables true, the number of steps to reach the goal is 19 in SmallMath and 122 in BigMath.\n5A file displaying this precondition structure is available at http://www.cs.berkeley.edu/\u223cemma/bigmathpreconditions.pdf\nAs both problems are stochastic, the expected number of steps can be significantly longer, depending on the initial belief state distribution. Therefore, both domains exhibit what are typically known in the POMDP community as the curse of history, due to the long horizon, and the curse of dimensionality, due to the problem size."}, {"heading": "5.2 SOLUTION PARAMETERS", "text": "As stated earlier, we used HSVI to solve the envelope POMDPs. The maximum horizon for SmallMath was set at a conservative 450 steps, and for BigMath at 1000 steps. Identical horizon limits were used when evaluating the empirical reward of the computed policy. The reward for the out state was set to be -1000 for SmallMath and -100 for BigMath. As there will typically be some probability that the state will transition into an out-of-envelope state, and both problems can require a long horizon of acting to reach the goal, the out state reward was loosely chosen to discourage transitioning to the out state without so severely penalizing the transition that the computed policy conservatively avoids adding any more skills. We did not optimize performance by varying this parameter, and other values might lead to further performance benefits.\nHSVI terminates when a terminal time limit is reached or a minimum distance (\u01eb) between the upper and lower bounds on value of the initial belief state is achieved. In SmallMath we set the maximum time limit to 1200 seconds and \u01eb = 200. In BigMath we set the maximum time limit to 8000 seconds and \u01eb = 1000."}, {"heading": "5.3 EVALUATION METRICS", "text": "After each envelope expansion, we evaluated the envelope policy reward empirically over multiple episodes of the rel-\nevant problem\u2019s max horizon length. For SmallMath we evaluated the empirical reward for 20 episodes after each expansion, and for BigMath we evaluated the empirical reward for 5 episodes after each expansion: BigMath is significantly more computationally intensive to evaluate due to the larger state space, and longer problem horizon. We present results averaged over 5 runs with different initial seeds for SmallMath and 8 runs for BigMath."}, {"heading": "5.4 BASELINES", "text": "Even the smaller of the two problems, SmallMath, still requires over 500,000 states to enumerate the exhaustive set of state variable combinations, which limited the potential alternate algorithms to compare against.\nSARSOP (Kurniawati et al., 2008) is a non-factored stateof-the-art generic POMDP solver which accepts factored input files.\nSymbolic Perseus (Boger et al., 2005) is a factored-statespace POMDP solver. Symbolic Perseus was used to compute a good approximate solution to a factored handwashing assistance problem with 13 variables, and over 50 \u2217 106 states.\nIn some cases the reachable state space may be quite small, and so we also explored first enumerating the reachable space, and then using HSVI to compute a POMDP policy over the reachable states.\nWe also implemented a simple, very fast, heuristic Fixed Threshold, No-Forgetting (FTNF) policy similar to policies used in prior intelligent tutoring systems (Corbett & Anderson, 1995; Koedinger et al., 1997). At each step, FTNF identifies the variable with the highest probability of being true below an input threshold probability, whose preconditions have exceeded this threshold probability. FTNF\nexecutes the action most likely to make that variable true, and updates the belief probability over that variable. Once a variable exceeds the input probability threshold, its value is assumed to be true for the rest of the episode.\nFinally, we also computed an upper bound on the value of the initial belief state using Equation 5."}, {"heading": "5.5 RESULTS", "text": ""}, {"heading": "5.5.1 SmallMath", "text": "We display the performance of RAPID on SmallMath in Figure 4a. RAPID could generally quickly find a good solution, and its consistency in doing so increased as the computation time increased, as should be expected.\nWe represented SmallMath in the SARSOP POMDPX format but found that SARSOP problem initialization consistently tried to exceed our limit of available memory (2 gigabytes). We believe this is because the current implementation still uses a non-factored dynamics representation, and a full sized-representation of SmallMath would require 219 \u00d7 219 \u00d7 38 entries.\nSymbolic Perseus requires specifying the number of sampled belief states to use for planning. When we specified a small number of beliefs (N=20), the algorithm computed a solution in 5150s, but the resulting policy could never find a trajectory to the goal. Using a larger number of beliefs (N=120), Symbolic Perseus was still generating belief points and had yet started computing a policy after 8 hours; as this well exceeded the time necessary to achieve good performance in the SmallMath domain using RAPID, we did not run Symbolic Perseus further.\nGiven the initial belief selected, the reachable state space of SmallMath is significantly smaller than the potential\nstate space size, at only 109 states. It is computationally tractable to simply enumerate this reachable state space and run HSVI over the resulting states. This approach yielded the best performance, with an average reward of 9962 on 200 trials (each consisting of at most 200 steps). This corresponded to an average of 39 steps to reach the goal state. The heuristic FTNF policy performed worse than the RAPID policy over a number of thresholds, and was significantly worse (t-test, p\u00a10.001) than the POMDP solution over the reachable state space at even the best threshold (0.925) examined (FTNF average reward=9947, mean number of steps to goal=54). These results highlight the advantage of a POMDP planning approach, which may both infer the value of earlier variables based on later variable values, and revisit an earlier variable if later evidence suggests its value is not yet true."}, {"heading": "5.5.2 BigMath", "text": "RAPID again was able to fairly quickly achieve good performance in this domain. Figure 4b & c display the average performance after each envelope expansion for different runs versus cumulative running time, and after each envelope expansion, respectively.\nDue to our experience with SARSOP and Symbolic Perseus on SmallMath, we did not explore their use on BigMath, which is a substantially larger problem.\nIn BigMath, given the chosen initial belief, even the reachable space is over millions of states and the potential state space exceeds 1030 states. It was therefore not feasible to perform standard planning over the reachable space.\nWe compared FTNF to the performance of RAPID after 4 envelope expansions. Though FTNF is very fast, it generally performed much worse than RAPID over a wide range of thresholds (from 0.8 to 0.9999). FTNF with the best\nfound threshold (0.9999) performed slightly better than RAPID over an 80 episode simulation, but the difference was not significant (t-test, p=0.18). Our experience suggests that it may be hard to identify a good threshold for FTNF in advance, and choosing a too-high value can lead to overly conservative policies."}, {"heading": "6 CONCLUSION & FUTUREWORK", "text": "There exist a number of important stochastic, partially observable problems that exhibit a large amount of structure that can be used to perform efficient planning. In this paper we focused on problems exhibiting a form of topological structure in the factored state space: domains which possess such structure include student tutoring, dialogue and potentially assembly tasks. Our RAPID algorithm leverages this structure to compute an initial state envelope based on the optimal MDP policy in time linear in the number of variables. RAPID then performs standard POMDP planning over this restricted envelope, before expanding the envelope and re-solving in an anytime fashion. Our experimental results demonstrate RAPID can quickly produce a good policy for an extremely large factored problem where the problem structure is constructed using prior precondition graphs from the education community.\nThere is ample scope for future work. We intend to explore additional envelope expansion techniques, such as trying to bias the new trajectories to the goal to lie within existing parts of the envelope. In addition, we currently re-solve the POMDP without considering the previously computed solution. We believe it should be possible to achieve further computational gains by re-using the value function (\u03b1vectors) computed using the prior envelope, by setting the value of the additional states to a lower bound on their potential value.6 In this paper we have assumed the POMDP model parameters are provided, but to integrate this in a real ITS will necessitate learning the model parameters. We plan to learn model parameters across multiple students\u2019 performances, motivated by the success of prior ITSs (see Koedinger et al. 1997) that use population-level model parameters."}, {"heading": "Acknowledgements", "text": "The authors wish to thank Sarah Finney, Jason Wolfe, Luke Zettlemoyer and the anonymous reviewers for their helpful comments. E.Brunskill was supported by a NSF Mathematical Sciences Postdoctoral Fellowship."}], "references": [{"title": "A decision-theoretic approach to task assistance for persons with dementia", "author": ["J. Boger", "P. Poupart", "J. Hoey", "C. Boutilier", "G. Fernie", "A. Mihailidis"], "venue": null, "citeRegEx": "Boger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boger et al\\.", "year": 2005}, {"title": "Stochastic dynamic programming with factored representations", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Artificial Intelligence Journal", "citeRegEx": "Boutilier et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Structure and complexity in planning with unary operators", "author": ["R. Brafman", "C. Domshlak"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Brafman and Domshlak,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Domshlak", "year": 2003}, {"title": "An analysis of the relationships among computation-related skills using a hierarchicalclustering technique", "author": ["J. Close", "F. Murtagh"], "venue": "Journal for Research in Mathematics Education,", "citeRegEx": "Close and Murtagh,? \\Q1986\\E", "shortCiteRegEx": "Close and Murtagh", "year": 1986}, {"title": "Knowledge tracing: Modeling the acquisition of procedural knowledge", "author": ["A. Corbett", "J. Anderson"], "venue": "User Modeling and User-Adapted Interaction,", "citeRegEx": "Corbett and Anderson,? \\Q1995\\E", "shortCiteRegEx": "Corbett and Anderson", "year": 1995}, {"title": "Introduction to algorithms", "author": ["T. Cormen", "C. Leiserson", "R. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1999}, {"title": "Topological value iteration algorithm for Markov decision processes", "author": ["P. Dai", "J. Goldsmith"], "venue": null, "citeRegEx": "Dai and Goldsmith,? \\Q2007\\E", "shortCiteRegEx": "Dai and Goldsmith", "year": 2007}, {"title": "Planning under time constraints in stochastic domains", "author": ["T. Dean", "L.P. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": "Artificial Intelligence,", "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Topological order planner for POMDPs", "author": ["J. Dibangoye", "G. Shani", "B. Chaib-draa", "A. Mouaddib"], "venue": null, "citeRegEx": "Dibangoye et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dibangoye et al\\.", "year": 2009}, {"title": "Principles of instructional design", "author": ["R. Gagn\u00e9", "L. Briggs"], "venue": null, "citeRegEx": "Gagn\u00e9 and Briggs,? \\Q1974\\E", "shortCiteRegEx": "Gagn\u00e9 and Briggs", "year": 1974}, {"title": "Envelope-based planning in relational MDPs", "author": ["N.H. Gardiol", "L.P. Kaelbling"], "venue": null, "citeRegEx": "Gardiol and Kaelbling,? \\Q2004\\E", "shortCiteRegEx": "Gardiol and Kaelbling", "year": 2004}, {"title": "Intelligent tutoring goes to school in the big city", "author": ["K.R. Koedinger", "J. Anderson", "W. Hadley", "M. Mark"], "venue": "International Journal of Artificial Intelligence in Education,", "citeRegEx": "Koedinger et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Koedinger et al\\.", "year": 1997}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S"], "venue": null, "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "Developed of a learning hierarchy for the computational skills of fractional number subtraction", "author": ["P. Miller", "E.R. Phillips"], "venue": "American Educational Research Association Meeting", "citeRegEx": "Miller and Phillips,? \\Q1974\\E", "shortCiteRegEx": "Miller and Phillips", "year": 1974}, {"title": "The complexity of Markov decision processes", "author": ["C. Papadimitriou", "J. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "An online POMDP algorithm for complex multiagent environments. AAMAS", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": null, "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Symbolic heuristic search value iteration for factored pomdps", "author": ["H. Sim", "K. Kim", "J. Kim", "D. Chang", "M. Koo"], "venue": null, "citeRegEx": "Sim et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sim et al\\.", "year": 2008}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation. UAI", "author": ["T. Smith", "R. Simmons"], "venue": null, "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "An intraconcept analysis of rational number addition: A validation study", "author": ["A.E. Uprichard", "E. Phillips"], "venue": "Journal for Research in Mathematics Education,", "citeRegEx": "Uprichard and Phillips,? \\Q1977\\E", "shortCiteRegEx": "Uprichard and Phillips", "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "Focussing on domains that exhibit factored structure has led to POMDP planners that solve some of the largest POMDP problems in the literature, including a hand washing assistance program (Boger et al., 2005) and a RoboCup rescue task (Paquet et al.", "startOffset": 188, "endOffset": 208}, {"referenceID": 15, "context": ", 2005) and a RoboCup rescue task (Paquet et al., 2005).", "startOffset": 34, "endOffset": 55}, {"referenceID": 8, "context": "Other recent work (Dai & Goldsmith, 2007; Dibangoye et al., 2009) has focused on domains where the flat state dynamics model limits the possible backtracking to earlier states, and showed that planning can be performed more efficiently when this topological structure is present.", "startOffset": 18, "endOffset": 65}, {"referenceID": 3, "context": "For example, some prior education studies coarsely approximate a student\u2019s knowledge as a factored set of binary variables, one for each skill, and infers a precondition graph structure among skills (known as a \u201clearning hierarchy\u201d) from student data: see for example Gagne\u00e9\u2019s and Briggs (1974) and Close and Murtagh (1986). Despite this structure, automated tutor action selection remains challenging as the factored state space may consist of hundreds of skills.", "startOffset": 299, "endOffset": 324}, {"referenceID": 17, "context": "Two of the fastest generic POMDP planners are HSVI by Smith and Simmons (2005) and SARSOP by Kurniawati, Hsu and Lee (2008).", "startOffset": 54, "endOffset": 79}, {"referenceID": 17, "context": "Two of the fastest generic POMDP planners are HSVI by Smith and Simmons (2005) and SARSOP by Kurniawati, Hsu and Lee (2008). Neither approach takes advantage of factored structure.", "startOffset": 54, "endOffset": 124}, {"referenceID": 0, "context": "Symbolic Perseus (Boger et al., 2005) and Symbolic HSVI (Sim et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 16, "context": ", 2005) and Symbolic HSVI (Sim et al., 2008) are two offline POMDP algorithms for factored state spaces which scale to large problems.", "startOffset": 26, "endOffset": 44}, {"referenceID": 0, "context": "Symbolic Perseus (Boger et al., 2005) and Symbolic HSVI (Sim et al., 2008) are two offline POMDP algorithms for factored state spaces which scale to large problems. In practice both perform fairly similarly to each other. Online, forward search POMDP planners can also leverage factored structure, and Paquet, Tobin and Chaib-draa (2005) used forward search to handle an extremely large, factored RoboCup rescue problem.", "startOffset": 18, "endOffset": 338}, {"referenceID": 6, "context": "Dai and Goldsmith (2007) leveraged the presence of layered positiveeffect state structure (certain clusters of states cannot be returned to) in their Topological Value Iteration (TVI) MDP algorithm.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "Dai and Goldsmith (2007) leveraged the presence of layered positiveeffect state structure (certain clusters of states cannot be returned to) in their Topological Value Iteration (TVI) MDP algorithm. Dibangoye et al. (2009) assumed a similar structure, and used this to create a heuristic Topological Order Planner (TOP) for POMDPs.", "startOffset": 0, "endOffset": 223}, {"referenceID": 7, "context": "To scale to very large, fully observable MDPs, Dean et al. (1995) proposed an anytime approach which initially restricts MDP planning to a smaller envelope of reachable states.", "startOffset": 47, "endOffset": 66}, {"referenceID": 7, "context": "To scale to very large, fully observable MDPs, Dean et al. (1995) proposed an anytime approach which initially restricts MDP planning to a smaller envelope of reachable states. Gardiol and Kaelbling (2004) extended this approach to be applicable in relational MDPs using action-based equivalence.", "startOffset": 47, "endOffset": 206}, {"referenceID": 7, "context": "Dean et al. (1995) presented the idea of computing a policy for fully observable, flat MDPs by planning only over a smaller envelope of states.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "As the precondition graph E is a directed acyclic graph (DAG), the topological order can be computed in time linear in the number of state variables and precondition conditions (Cormen et al., 1999).", "startOffset": 177, "endOffset": 198}, {"referenceID": 7, "context": "The definition of an out state follows prior work in the fully observable envelope literature (Dean et al., 1995).", "startOffset": 94, "endOffset": 113}, {"referenceID": 12, "context": "The fraction precondition graph was derived from Miller and Phillips (1974) and Uprichard and Phillips (1977).", "startOffset": 49, "endOffset": 76}, {"referenceID": 12, "context": "The fraction precondition graph was derived from Miller and Phillips (1974) and Uprichard and Phillips (1977). We combined the fraction precondition structure with the subtraction hierarchy from Gagn\u00e9 (1974), and the addition, subtraction, multiplication and division hierarchies from Close and Murtagh (1986).", "startOffset": 49, "endOffset": 110}, {"referenceID": 12, "context": "The fraction precondition graph was derived from Miller and Phillips (1974) and Uprichard and Phillips (1977). We combined the fraction precondition structure with the subtraction hierarchy from Gagn\u00e9 (1974), and the addition, subtraction, multiplication and division hierarchies from Close and Murtagh (1986).", "startOffset": 49, "endOffset": 208}, {"referenceID": 3, "context": "We combined the fraction precondition structure with the subtraction hierarchy from Gagn\u00e9 (1974), and the addition, subtraction, multiplication and division hierarchies from Close and Murtagh (1986). The full precondition graph is displayed in Figure 3 and consisted of 122 skills.", "startOffset": 174, "endOffset": 199}, {"referenceID": 12, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 44, "endOffset": 71}, {"referenceID": 12, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 44, "endOffset": 105}, {"referenceID": 12, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 44, "endOffset": 172}, {"referenceID": 3, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 198, "endOffset": 225}, {"referenceID": 12, "context": "SARSOP (Kurniawati et al., 2008) is a non-factored stateof-the-art generic POMDP solver which accepts factored input files.", "startOffset": 7, "endOffset": 32}, {"referenceID": 0, "context": "Symbolic Perseus (Boger et al., 2005) is a factored-statespace POMDP solver.", "startOffset": 17, "endOffset": 37}, {"referenceID": 11, "context": "We also implemented a simple, very fast, heuristic Fixed Threshold, No-Forgetting (FTNF) policy similar to policies used in prior intelligent tutoring systems (Corbett & Anderson, 1995; Koedinger et al., 1997).", "startOffset": 159, "endOffset": 209}], "year": 2010, "abstractText": "Despite the intractability of generic optimal partially observable Markov decision process planning, there exist important problems that have highly structured models. Previous researchers have used this insight to construct more efficient algorithms for factored domains, and for domains with topological structure in the flat state dynamics model. In our work, motivated by findings from the education community relevant to automated tutoring, we consider problems that exhibit a form of topological structure in the factored dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains (RAPID) leverages this structure to efficiently compute a good initial envelope of reachable states under the optimal MDP policy in time linear in the number of state variables. RAPID performs partially-observable planning over the limited envelope of states, and slowly expands the state space considered as time allows. RAPID performs well on a large tutoring-inspired problem simulation with 122 state variables, corresponding to a flat state space of over 10 states.", "creator": null}}}