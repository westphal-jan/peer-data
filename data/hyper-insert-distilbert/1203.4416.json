{"id": "1203.4416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2012", "title": "On Training Deep Boltzmann Machines", "abstract": "the deep core boltzmann composite machine ( dbm ) has been an important development thread in succeeding the quest for powerful \" deep \" digital probabilistic performance models. to remain date, implementing simultaneous or joint training of all layers each of the entire dbm has subsequently been largely avoiding unsuccessful competition with existing training methods. we helped introduce together a simple regularization scheme structure that encourages altering the computational weight determination vectors associated internally with each explicitly hidden unit to have similar norms. we demonstrate positively that this regularization can be easily combined with all standard stochastic maximum extinction likelihood to yield quite an excellent effective structured training matrix strategy practical for reducing the robust simultaneous training of essentially all layers techniques of the deep boltzmann machine.", "histories": [["v1", "Tue, 20 Mar 2012 12:59:15 GMT  (269kb,D)", "http://arxiv.org/abs/1203.4416v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["guillaume desjardins", "aaron courville", "yoshua bengio"], "accepted": false, "id": "1203.4416"}, "pdf": {"name": "1203.4416.pdf", "metadata": {"source": "CRF", "title": "On Training Deep Boltzmann Machines", "authors": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio"], "emails": ["desjagui@iro.umontreal.ca", "courvila@iro.umontreal.ca", "bengioy@iro.umontreal.ca"], "sections": [{"heading": null, "text": "The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \u201cdeep\u201d probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine."}, {"heading": "1 Introduction", "text": "Since its introduction by Salakhutdinov and Hinton [7], the deep Boltzmann machine (DBM) has been one of the most ambitious attempts to build a probabilistic model with many layers of latent or hidden variables. The DBM shares some characteristics with the earlier deep belief network (DBN) [3]. Both models may be viewed as multi-layer or \u201cdeep\u201d extensions of the popular restricted Boltzmann machine (RBM). However, unlike the DBN, commonly used DBM approximate inference schemes implement true feedback mechanisms where the inferred activations of high-level units can influence the activations of lower-level units. Thus alternative interpretations of an input are able to compete at all levels of the model simultaneously. This sort of sophistication in inference has the potential to lead to more robust and globally coherent inferences, which are likely to translate to better performance when the model is employed in tasks such as classification.\nDespite its success, the DBM has not yet entirely fulfilled this potential and the DBN remains the more popular model paradigm for deep probabilistic models. One potential explanation of why this is so could be tied to the difficulties one encounters when attempting to estimate the model parameters from data. Straightforward applications of gradient-based methods such as stochastic maximum likelihood (SML) [10] (also known as persistent contrastive divergence [9]) appear to fall in poor local minima, and fail to adequately explore the space of model parameters.\nOne solution, as presented in [7], is based on a greedy layer-wise pretraining strategy which appears to overcome these poor local minima . Each layer is trained as an RBM with the latent activations of the layer below as its input. These layers are then recombined by rescaling the weights to account for the doubling of inputs into each intermediate layer. While this procedure seems to work well, it involves many steps and is more complicated than would be ideal. Also, more importantly, the necessity of layer-wise pretraining makes it very difficult for the organization of upper layers to influence the topological organization of lower layers. For example, in cases where we would like to learn a DBM that is not fully connected between each layer, it would potentially be very desirable for the global connectivity pattern (eg. local receptive fields) to influence the pattern of learned filters at all layers. Layer-wise pretraining precludes this possibility. If, on the other hand, one could jointly train all layers of a deep Boltzmann machine simultaneously, then the pattern of activation of the upper layer units would have an opportunity to influence the weights trained at the lower layers via their effect on lower-layer units activations.\nar X\niv :1\n20 3.\n44 16\nv1 [\ncs .N\nE ]\n2 0\nM ar\n2 01\nIn this paper, we describe a simple scheme for joint training of all layers of a deep Boltzmann machine. Our strategy is based on the observation that the poor local minima in which SML falls are characterized by high variance in the norms of the weight vectors, particularly those between the data and the first layer of hidden units. Our solution is to simply add a regularization term to the standard maximum likelihood training criterion that penalizes large differences in the norms of the weight vectors, both within a layer and across neighbouring layers. Our regularization is based on the intuition that successful models are those where all units contribute roughly equally to the representation of the data. This is a common principle that has previously been applied in the form of sparsity penalties for RBM and DBN training [5], where all hidden units are encouraged to be active over an equal proportion of the data. Our application of this principal is somewhat different, we do not explicitly control the activation of the hidden units, instead we encourage equal influence of each active hidden unit. We demonstrate, with experiments, both the failure of standard SML training for DBMs and the effectiveness of our regularized-SML DBM training strategy."}, {"heading": "2 Boltzmann Machines", "text": "A Boltzmann machine is defined as a network of symmetrically-coupled binary stochastic units (random variables). These stochastic units can be divided into two groups: (1) the visible units v \u2208 {0, 1}D that represent the data, and (2) the hidden units h \u2208 {0, 1}N that mediate dependencies between the visible units through their mutual interactions. The pattern of interaction is specified through the energy function:\nEBM(v, h; \u03b8) = \u2212 1 2 vTUv \u2212 1 2 hTV h\u2212 vTWh\u2212 bT v \u2212 dTh, (1)\nwhere \u03b8 = {U, V,W, b, d} are the model parameters which respectively encode the visible-to-visible interactions, the hidden-to-hidden interactions, the visible-to-hidden interactions, the visible selfconnections, and the hidden self-connections (also known as biases). To avoid over-parametrization, the diagonals of U and V are set to zero.\nThe energy function specifies the probability distribution over the joint space [v, h], via the Boltzmann distribution:\nP (v, h) = 1\nZ(\u03b8) exp (\u2212EBM(v, h; \u03b8)) , (2)\nwhere the partition function Z(\u03b8) ensures that the density normalizes:\nZ(\u03b8) = v1=1\u2211 v1=0 \u00b7 \u00b7 \u00b7 vD=1\u2211 vD=0 h1=1\u2211 h1=0 \u00b7 \u00b7 \u00b7 hN=1\u2211 hN=0 exp (\u2212EBM(v, h; \u03b8)) . (3)\nThis joint probability distribution gives rise to the set of conditional distributions of the form:\nP (hi | v, h\\i) = sigmoid \u2211 j Wjivj + \u2211 i\u2032\\i Vii\u2032hi\u2032 + di  (4) P (vj | h, v\\j) = sigmoid \u2211 i Wjivj + \u2211 j\u2032\\j Ujj\u2032vj\u2032 + bj\n . (5) In general, inference in the Boltzmann machine is intractable. For example, computing the conditional probability of hi given the visibles, P (hi | v), requires marginalizing over the rest of the hiddens which implies evaluating a sum with 2N\u22121 terms:\nP (hi | v) = h1=1\u2211 h1=0 \u00b7 \u00b7 \u00b7 hi\u22121=1\u2211 hi\u22121=0 hi+1=1\u2211 hi+1=0 \u00b7 \u00b7 \u00b7 hN=1\u2211 hN=0 P (h | v) (6)\nHowever with some judicious choices in the pattern of interactions between the visible and hidden units, more tractable subsets of the model family are possible."}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "The restricted Boltzmann machine (RBM) is likely the most popular subset of Boltzmann machines. They are defined by restricting the interactions in the Boltzmann energy function, in Eq. 10, to only those between h and v, i.e. for ERBM, U = 0 and V = 0. As such, the RBM can be said to form a bipartite graph with the visibles and the hiddens forming two layers of vertices in the graph. With this restriction, the RBM possesses the useful property that the conditional distribution over the hidden units factorizes given the visibles:\nP (h | v) = \u220f i P (hi | v) = \u220f i sigmoid \u2211 j Wjivj + di  (7) Likewise, the conditional distribution over the visible units given the hiddens also factorizes:\nP (v | h) = \u220f j P (vj | h) = \u220f j sigmoid (\u2211 i Wjihi + bj ) (8)\nThis conditional factorization property of the RBM immediately implies that most inferences we would like make are readily tractable. For example, the conditional independence of the hiddens implies that posterior marginals of the form P (hi | v) are immediately available. Importantly, the tractability of the RBM does not extend to its partition function, which still involves sums with exponential number of terms. It does imply however that we can limit the number of terms to min{2D, 2N}. Usually this is still an unmanageable number of terms and therefore we must resort to approximate methods to deal with its computation.\nLearning in the RBM is also rendered much more tractable in comparison to the general Boltzmann machine. Typically, learning involves finding a set of model parameters that approximately maximizes the log likelihood of a training dataset:\n\u2211 t logP (v:,t; \u03b8) = \u2211 t log h1,t=1\u2211 h1,t=0 \u00b7 \u00b7 \u00b7 hN,t=1\u2211 hN,t=0 P (v:,t, h:,t; \u03b8). (9)\nThis can be accomplished via gradient ascent. The gradient of the log likelihood of the data for the RBM is given by:\n\u2202\n\u2202\u03b8i ( T\u2211 t=1 log p(v:,t) ) = \u2212 T\u2211 t=1 \u2329 \u2202 \u2202\u03b8i ERBM(v:,t, h) \u232a p(h:,t|v:,t) + T\u2211 t=1 \u2329 \u2202 \u2202\u03b8i ERBM(v, h) \u232a p(v,h) ,\nwhere we have the expectations with respect to p(h:,t | v:,t) in the \u201cclamped\u201d condition, and over the full joint p(v:,t, h:,t) in the \u201cunclamped\u201d condition. In training, we follow the stochastic maximum likelihood (SML) algorithm (also know as persistent contrastive divergence or PCD) [10, 9], i.e., performing only one or a few updates of an MCMC chain between each parameter update."}, {"heading": "3 Deep Boltzmann Machines", "text": "The Deep Boltzmann Machine (DBM) is another particular subset of the Boltzmann machine family of models where the units are again arranged in layers. However unlike the RBM, the DBM possesses multiple layers of hidden units as illustrated in Figure 1. With respect to the Boltzmann energy function of Eq. 10, the DBM corresponds to setting U = 0 and a sparse connectivity structure in both V andW . We can make the structure of the DBM more explicit by specifying its energy function. For the 2-layer model it is given as:\nEDBM(v, h (1), h(2); \u03b8) = \u2212vTWh(1) \u2212 h(1) T V h(2) \u2212 d(1) T h(1) \u2212 d(2) T h(2) \u2212 bT v, (10)\nwith \u03b8 = {W,V, d(1), d(1), b}. The DBM can also be characterized as a bipartite graph between two sets of vertices, formed by the units in odd and even-numbered layers (with v := h(0))."}, {"heading": "3.1 Mean-field approximate inference", "text": "A key point of departure from the RBM is that the posterior distribution over the hidden units (given the visibles) is no longer tractable, due to the interactions between the hidden units. Salakhutdinov and Hinton [7] resort to a mean-field approximation to the posterior. Specifically, in the case of the 2-layer model, we wish to approximate P ( h(1), h(2) | v ) with the factored\ndistribution Qv(h(1), h(2)) = \u220fN1 j=1Qv ( h (1) j ) \u220fN2 i=1Qv ( h (2) i ) . such that the KL divergence\nKL ( P ( h(1), h(2) | v ) \u2016Qv(h1, h2) ) is minimized or equivalently, that a lower bound to the log likelihood is maximized:\nlogP (v) > L(Qv) \u2261 \u2211 h(1) \u2211 h(2) Qv(h (1), h(2)) log ( P (v, h(1), h(2)) Qv(h(1), h(2)) ) (11)\nMaximizing this lower-bound with respect to the mean-field distribution Qv(h1, h2) yields the following mean field update equations:\nh\u0302 (1) i \u2190 sigmoid \u2211 j Wjivj + \u2211 k Vikh\u0302 (2) k + d (1) i  (12) h\u0302 (2) k \u2190 sigmoid (\u2211 i Vikh\u0302 (1) i + d (2) k ) (13)\nIterating Eq. (12-13) until convergence yields the parameters used to estimate the \u201cvariational positive phase\u201d of Eq. 14:\nL(Qv) = EQv [ logP (v, h(1), h(2))\u2212 logQv(h(1), h(2)) ] = EQv [ \u2212EDBM(v, h(1), h(2))\u2212 logQv(h(1), h(2)) ] \u2212 logZ(\u03b8)\n\u2202L(Qv) \u2202\u03b8\n= \u2212EQv [ \u2202EDBM(v, h (1), h(2))\n\u2202\u03b8\n] + EP [ \u2202EDBM(v, h (1), h(2))\n\u2202\u03b8\n] (14)\nNote that this variational learning procedure leaves the \u201cnegative phase\u201d untouched. It can thus be estimated through SML or Contrastive Divergence [2] as in the RBM case."}, {"heading": "3.2 Training Deep Boltzmann Machines", "text": "Despite the intractability of inference in the DBM, its training should not, in theory, be much more complicated than that of the RBM. The major difference being that instead of maximizing the likelihood directly, we instead choose parameters to maximize the lower-bound on the likelihood given in Eq. 11. The SML-based algorithm for maximizing this lower-bound is as follows:\n1. Clamp the visible units to a training example.\n2. Iterate over Eq. (12-13) until convergence.\n3. Generate negative phase samples v\u2212, h(1)\u2212 and h(2)\u2212 through SML. 4. Compute \u2202L(Qv) /\u2202\u03b8 using the values obtained in steps 2-3. 5. Finally, update the model parameters with a step of gradient ascent.\nWhile the above procedure appears to be a simple extention of the highly effective SML scheme for training RBMs, as we demonstrate in Sec. 5, this procedure seems vulnerable to falling in poor local minima which leave many filters effectively dead (not significantly different from its random initialization with small norm).\nThe failure of the SML joint training strategy was noted by Salakhutdinov and Hinton [7]. As a far more successful alternative, they proposed a greedy layer-wise training strategy. This procedure consists in pre-training the layers of the DBM, in much the same way as the Deep Belief Network: i.e. by stacking RBMs and training each layer to independently model the output of the previous layer. A final joint \u201cfine-tuning\u201d is done following the above SML-based procedure."}, {"heading": "4 Joint Training of a DBM", "text": "While greedy layer-wise training has been shown to be reasonably successful, a means of joint training a DBM would be highly desireable. Not only would it be simpler, but it would open the door to local receptive field learning and more general architectures where we would like the topdown pattern of connectivity to influence the learning of lower-level features. In this section we detail our simple proposal for a means to jointly train all layers of a DBM.\nOur strategy is based on the observation that standard SML-based joint training tends produce high variance in the weight vectors associated with each hidden unit \u2013 particularly across the first-layer weight vectors that connect the visible units to the first hidden layer units.\nOur proposal is thus to regularize the maximum likelihood objective, in order to encourage units to have similar norms (i) within a given layer and (ii) to a lesser extent, across neighbouring layers. We thus introduce an additional parameter \u00b5(l) for each layer of the DBM, representing the average norm of weight vectors of the l-th layer. We then add a regularization term to our objective L(Qv), which penalizes deviations from this mean-value using a squared-error penalty. \u00b5(l)\u2019s belonging to adjacent layers are further constrained to be close (again in the l2 sense). This gives rise to the following regularized objective:\nmax \u03b8 L(Qv) + \u03b1(1) N1\u2211 j=1 ( \u2016W\u00b7i\u20162 \u2212 \u00b5(1) )2 + \u03b1(2) N2\u2211 i=1 ( \u2016V\u00b7j\u20162 \u2212 \u00b5(2) )2 + \u03b3 ( \u00b5(1) \u2212 \u00b5(2) )2 .\n(15)\nOne can think of this regularization term as a spring, which ensures that the system evolves jointly from an initial random weight configuration (where the columns ofW (l) have small norm) to a good model of the input distribution P (v), which undoubtedly requires weight vectors with larger norms."}, {"heading": "5 Experiments", "text": "To evaluate the effect of our regularization term, we trained deep Boltzmann machines on the pervasive MNIST dataset [4]. All our models were trained for 106 updates, using minibatches of size 50, varying the learning rate in {10\u22122, 10\u22123, 10\u22124}.\nDirect Approach Our first model was a 3-layer DBM with [500,500,1000] hidden units in the first, second and third layers (respectively). The resulting filters are shown in Figure 2.\nAs we can see, many of the first-layer filters are not significantly different from their random initialization, with very small norm. Based on the difference between these results and the successful training of an RBM, we speculate that the top-down interactions prevent the first layer from learning useful filters. We have confirmed that the high-frequency \u201cnoise\u201d filters are actually the ones with\nlowest norm. It seems as though early top-down input is influencing the activations of these hidden units, perhaps directly through some form of suppression, or indirectly, by reinforcing the activation of the subset of filters which become useful early on. While we plot filters for 3-layer networks, the same results hold for networks of depth two.\nJoint Training Using our regularized objective, we train a 2-layer DBM with 500 hidden units in each layer. We set the hyper-parameters of Eq.15 as follows: \u03b1(1) = 0.1, \u03b1(2) = 0.1 and \u03b3 = 1. Furthermore, we dampen the learning rate on the parameters \u00b5(1) and \u00b5(2) by a factor of a thousand, in order for the layer norms to evolve more slowly. A random subset of filters is shown in Figure 3.\nThe effect of our regularization term is drastic: the vast majority of first layer filters seem to train successfully and resemble the pen-stroke detectors characteristic of features learned on MNIST. While more difficult to interpret, the second layer filters are also clearly superior to the ones of Figure 2, combining lower-level features (pen strokes) into more global digit-like objects."}, {"heading": "6 Discussion", "text": "We have introduced a simple regularization scheme that appears to prevent SML from falling into a poor local minimum of our objective function. The regularizer encourages units to learn filters which have similar norms, both within and across adjacent layers. We have empirically demonstrated the failure of joint training of all layers of a deep Boltzmann machine which leaves many filters in the lower layer of a 2-layer DBM with very small norm and, consequently, little contribution to modeling the data. We have also empirically demonstrated the success of our regularization scheme in simultaneously training both layers of a 2-layer DBM. While we have not shown it here, our scheme also appears to work well for DBMs with more than 2 layers. In future work, we would like to determine how many layers can be learnt simultaneously using similar basic norm-regularizaiton schemes."}, {"heading": "Acknowledgments", "text": "The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec and CIFAR. We would also like to thank the developers of Theano [1]."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Technical Report GCNU TR 2000-004,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "In NIPS\u201907,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Annealed importance sampling", "author": ["R.M. Neal"], "venue": "Statistics and Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "ICML 2008,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates", "author": ["L. Younes"], "venue": "Stochastics and Stochastic Reports,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction Since its introduction by Salakhutdinov and Hinton [7], the deep Boltzmann machine (DBM) has been one of the most ambitious attempts to build a probabilistic model with many layers of latent or hidden variables.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "The DBM shares some characteristics with the earlier deep belief network (DBN) [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "Straightforward applications of gradient-based methods such as stochastic maximum likelihood (SML) [10] (also known as persistent contrastive divergence [9]) appear to fall in poor local minima, and fail to adequately explore the space of model parameters.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Straightforward applications of gradient-based methods such as stochastic maximum likelihood (SML) [10] (also known as persistent contrastive divergence [9]) appear to fall in poor local minima, and fail to adequately explore the space of model parameters.", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "One solution, as presented in [7], is based on a greedy layer-wise pretraining strategy which appears to overcome these poor local minima .", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "This is a common principle that has previously been applied in the form of sparsity penalties for RBM and DBN training [5], where all hidden units are encouraged to be active over an equal proportion of the data.", "startOffset": 119, "endOffset": 122}, {"referenceID": 9, "context": "In training, we follow the stochastic maximum likelihood (SML) algorithm (also know as persistent contrastive divergence or PCD) [10, 9], i.", "startOffset": 129, "endOffset": 136}, {"referenceID": 8, "context": "In training, we follow the stochastic maximum likelihood (SML) algorithm (also know as persistent contrastive divergence or PCD) [10, 9], i.", "startOffset": 129, "endOffset": 136}, {"referenceID": 6, "context": "Salakhutdinov and Hinton [7] resort to a mean-field approximation to the posterior.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "It can thus be estimated through SML or Contrastive Divergence [2] as in the RBM case.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "The failure of the SML joint training strategy was noted by Salakhutdinov and Hinton [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "5 Experiments To evaluate the effect of our regularization term, we trained deep Boltzmann machines on the pervasive MNIST dataset [4].", "startOffset": 131, "endOffset": 134}], "year": 2012, "abstractText": "The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \u201cdeep\u201d probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine.", "creator": "LaTeX with hyperref package"}}}