{"id": "1204.4329", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2012", "title": "Supervised feature evaluation by consistency analysis: application to measure sets used to characterise geographic objects", "abstract": "essentially nowadays, supervised domain learning training is commonly used in many communication domains. essentially indeed, now many the works propose to learn new knowledge from examples that translate automatically the expected behaviour of using the considered whole system. a key new issue of supervised learning has concerns the description and language used to represent using the standard examples. in this paper, we propose a semantic method described to evaluate the feature descriptive set used to describe them. our method is based on the computation of the consistency of using the example of base. since we carried out a case study in the natural domain of pure geomatic anthropology in establishing order better to evaluate, the sets of measures initially used to automatically characterise fundamental geographic objects. the case nature study strongly shows that adopting our method allows instructors to additionally give them relevant evaluations characteristics of measure sets.", "histories": [["v1", "Thu, 19 Apr 2012 12:03:20 GMT  (297kb)", "http://arxiv.org/abs/1204.4329v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["patrick taillandier", "alexis drogoul"], "accepted": false, "id": "1204.4329"}, "pdf": {"name": "1204.4329.pdf", "metadata": {"source": "CRF", "title": "Supervised feature evaluation by consistency analysis: application to measure sets used to characterise geographic objects", "authors": ["Patrick Taillandier", "Alexis Drogoul"], "emails": ["patrick.taillandier@gmail.com", "alexis.drogoul@gmail.com"], "sections": [{"heading": null, "text": "Supervised feature evaluation by consistency analysis: application to measure sets used to characterise geographic objects\nPatrick Taillandier1,2\n1IRD, UMI UMMISCO 209 Bondy, France\npatrick.taillandier@gmail.com\nAlexis Drogoul1,2,3 2IFI, MSI, UMI 209, Ha Noi, Viet Nam\nalexis.drogoul@gmail.com\n3UPMC, UMI 209,\nParis, France\nAbstract\u2014Nowadays, supervised learning is commonly used in many domains. Indeed, many works propose to learn new knowledge from examples that translate the expected behaviour of the considered system. A key issue of supervised learning concerns the description language used to represent the examples. In this paper, we propose a method to evaluate the feature set used to describe them. Our method is based on the computation of the consistency of the example base. We carried out a case study in the domain of geomatic in order to evaluate the sets of measures used to characterise geographic objects. The case study shows that our method allows to give relevant evaluations of measure sets.\nSupervised feature evaluation; consistency computation; geomatic\nI. INTRODUCTION (HEADING 1) Supervised learning is more and more used in numerous domains. Most of these works use an \u201cattributevalue\u201d formalism to represent examples. Thus, examples are represented by a set of feature values. The choice of this feature set is a key issue in supervised learning. Indeed, if the notion that we search to learn is not \u201ccontained\u201d in the feature set, it will be not possible to learn it. For example, if we search to learn the concept of \u201csquare object\u201d but if the feature set used to describe the objects contains no information about the shape of the objects, but only information concerning their colour, it is then not possible to learn this concept. Higher the number of features, more the learning is complex, and more examples are needed to learn. Thus, it is important to have enough feature to well represent the examples, but at the same time, to limit the number of features to ease the learning. In this context, be able to evaluate feature sets is particularly interesting. In this paper, we propose a supervised method to evaluate a feature set. Our method is based on the computation of the example base consistency.\nIn Section II, we describe the context of this work. Section III is dedicated to the presentation of our feature set evaluation method. Section IV presents a case-study carried out in the domain of geomatic. At last, Section V concludes and gives perspectives.\nII. CONTEXT"}, {"heading": "A. Formalisation of the problem", "text": "We are interested in the evaluation of a feature set considering a given task. Indeed, a feature set can be relevant for a given task, but not at all for another one.\nIn this work, we assume that an example base describing the expected behaviour of the considered system is defined. Examples are described by a vector of\nfeature values and by a label. The vector of feature values describes the example characteristics, and the label, the concept that we search to learn. In this work, we assume that the set of possible labels is finite. Figure 1 gives an example of an example base that concerns the learning of procedural knowledge for a geomatic process: the goal is to learn knowledge relative to the choice of the best operation to apply on building groups according to their geometric characteristics to produce a 50k map. The goal of the supervised feature set evaluation is then to compute, from the example base, a quality value for the feature set."}, {"heading": "B. Evaluation of feature sets", "text": "In the literature, many methods were proposed to\nevaluate a set of features from an example base. These methods can be divided in two major groups: the ones that work on individual features, and the ones that take into account all the features at the same time. Concerning the first groups of methods, many of them are based on classic measures such as the chi-squared statistic or the entropy gain. Other methods such as [1] propose to analysis the linked between the variations of a feature value and the label associated. For the second group of method, a classic strategy consists in evaluating the predictive model built from the example base (often by cross-validation [2]). Other methods imply to compute the\ndegree of redundancy between features [3]. Most of these methods search to estimate the worth of subsets of features in order to be able to remove useless features, and thus to ease the learning. In this paper, we propose a method that will focus on evaluating the necessity to add new features. Indeed, our goal will be more to determine if more features are needed rather than removing useless ones.\nIII. METHOD PROPOSED The method we propose for the supervised evaluation of feature sets is based on the computation of the inconsistency of the example base. Inconsistency means, here, cases where, for a same state (according to the feature set), different labels are assigned to the examples. For example, if an example base contains two examples representing two building groups that are similar in terms of feature values, and if the operation that has to be applied on the first building group is a building removing operation, and on the second one, a building displacement operation, the example base is inconsistent.\nOur feature set evaluation method is based on three steps:\n\u2022 Selection of the pertinent features. \u2022 Discretisation of the selected features. \u2022 Computation of the inconsistency from the resulting\nexample base."}, {"heading": "A. Selection of the pertinent features", "text": "As states in Section II.B, the goal of our method is to determine is a feature set characterises well enough the examples or if more features are needed.\nIn order to achieve this goal, we propose as a first step, to remove irrelevant features that only bring noise to the learning process. Indeed, some features can be totally unrelated to the concept we try to learn.\nFor example, colour information is useless to learn the concept of square object. In the literature, many supervised filtering algorithms were proposed (see [4]). These methods allow to select a subset of pertinent features from a example base. Most of them are based on the measures presented in Section II.B. The application of such algorithms on the example base allows to obtain a new feature set composed of features that are relevant to the concept we try to learn. Figure 2 gives an example of feature set filtering."}, {"heading": "B. Discretisation of the selected features", "text": "As a second step, we propose to discretise the numeric features. Similarly to the feature filtering problem, numerous works deals with the problem of supervised\nfeature discretisation (e.g. [5] or [6]). These algorithms propose, by example base analysis, to pass from continuous feature to nominal ones composed of sets of disjoint intervals. The goal is to decrease the size of the feature set space and to make finite the number of possible example descriptions. Indeed, some features can allow slight variation of their values that does not justify a different treatment of the examples. Figure 3 gives an example of feature set discretisation."}, {"heading": "C. Computation of the inconsistency from the resulting", "text": "example base At the end of the two previous steps, we obtain a new example base that we use to evaluate the feature set. We propose for that to evaluate the inconsistency rate of the resulting example base. An inconsistent example base is a base in which examples, with the same feature values, have different labels. An example base strongly inconsistent means that additional features are needed to characterise the examples.\nLet SB be the set of different feature descriptions (according to the feature values) contains in the examples base B. Thus, SB contains a unique specimen, in terms of feature values, of each example contained in B. We define the function nb_examplesB(s,l) that returns the number of examples in the example base B that have s for feature description and l for label. The inconsistency rate of an example base B for which the set of possible labels is L is computed by the following formulae:\n[ ] )( )),(_(max 1)( Bcard lsexamplesnb B BSs BLl\u2211\u2208 \u2208\u2212=ncyInconsiste\nFigure 4 gives an example of inconsistency computation for two examples bases.\nThe inconsistency rate is bounded by: )( 11)(0 Lcard B \u2212\u2264\u2264 ncyInconsiste\nIndeed, at best, for each different feature description of the example base, only one label is proposed. Thus, we have:\n[ ] )()),(_(max Bcardslexamplesnb BSs BLl =\u2211 \u2208 \u2208\nThe inconsistency rate is thus equals to 0, which means that the example base is totally consistent.\nAt worst, for each different feature descriptions of the example base, each possible label is represented with the same proportion in B. Thus, for each different feature description s, we can compute:\n)( 1)),(_(max Lcard lsexamplesnb BLl =\u2208\nThus, the inconsistency rate is equals to (1 \u2013 1/card(L)), which means that the example base is totally inconsistent.\nHigher the inconsistency rate of an example base, more this one suffers from a description noise, and thus, more it is important to add new relevant features to characterise the examples.\nIV. 4. CASE STUDY"}, {"heading": "A. General case-study context", "text": "1) Supervised learning in geomatic\nThe introduction of Artificial Intelligence techniques in cartography has allowed to automate many processes. In particular, Artificial Intelligence has brought new techniques to acquire knowledge from examples. Supervised learning techniques are now widely used in Geomatics: for acquisition of procedural knowledge (e.g. [7] or [8]), for data enrichment (e.g. [9] or [10]), system calibration (e.g. [11]), etc. Most of these works use an \u201cattribute-value\u201d formalism to represent examples. Thus, examples are represented by a set of measures characterising the geographic object states.\n2) Automated cartographic generalisation In order to evaluate our method, we present a case study we carried out in the domain of cartographic generalisation. Cartographic generalisation is the process that aims at simplifying geographic data to suit the scale and purpose of a map. Its automation is a complex problem. One approach to solve it consists in using a local, step-by-step and knowledge-based method ([7] and [12]). A classic generalisation model based on such approach is the AGENT model ([13] and [14]). It proposes to model geographic objects (or groups of geographic objects) by agents, i.e. autonomous entities that manage their own generalisation. In order to generalise itself, the geographic agent chooses actions (i.e. generalisation operations) to\napply according to procedural knowledge and to its state. The agent states are characterised by a set of measures that depends of the nature of the considered knowledge. The agents can also evaluate their satisfaction, which characterises the respect of cartographic constraints (map specifications) by its state. For example, a cartographic constraint can be for a building agent to be big enough to be readable. Each constraint has a satisfaction level that is ranged between 1 and 10 (10 means that the constraint is perfectly satisfied). The agent satisfaction consists in a linear combination of its constraint satisfaction weighted by their importance.\n3) Procedural knowledge revision In [15], we proposed an approach to revise the procedural knowledge of systems based on the AGENT model. This approach allows to build example bases (one per element of knowledge) from experience. These example bases are then used to search the knowledge that maximises the performance of the generalisation system.\nThree types of knowledge are concerned by the knowledge revision:\n\u2022 Knowledge relative to action application: for each action, a set of rules (that depends of a set of measures), defines if the action has to be applied or not, and with which priority.\n\u2022 Stopping criterion: a set of rules defines, according to a given state and to the initial state, if the agent has to try to apply more actions or not. If no specific stopping criterion is defined, the generalisation process ends when all actions have been tested for all states or when a perfect state (i.e. satisfaction = 10) has been found.\n\u2022 Optimality criterion: a set of rules defines, according to a given state, if this state can be improved or not, i.e. if the application of other actions can allow to obtain a better state or not. This criterion is different to the stopping criterion because it allows to backtrack to previous states and to test other actions. This criterion marks a local optimal while the stopping criterion marks a global optimal."}, {"heading": "B. Revision of knowledge used by building group agent", "text": "1) Building group generalisation\nThe case study we propose focuses on the generalisation of building groups: a building group is a space composed of a set of \u201cclose\u201d buildings belonging to the same building block (space surrounded by a minimum cycle of roads). The initial data are stemming from BD TOPO\u00ae, the 1m resolution database of the French NMA (reference scale approximately 1:15 000). The target scale is 1:50 000.\nWe defined six constraints for the building group: \u2022 Proximity constraint: states that the buildings should\nbe neither too close to each other, nor too close to the roads.\n\u2022 Building satisfaction constraint: states that the buildings composing the building group should individually satisfy their internal constraints.\n\u2022 Density constraint: states that the building density is not too high in comparison to the initial building density.\n\u2022 Spatial distribution constraint: states that the current building spatial distribution should be close to the initial building spatial distribution.\n\u2022 Big building preservation constraint states that the buildings, of which the area is bigger than a threshold, should not be removed.\n\u2022 Corner building preservation constraint: states that the buildings located at the corner of roads should not be removed.\nIn order to satisfy its constraints, the building group agent can apply five actions:\n\u2022 Building generalisation action: triggers the individual generalisation of the building agents composing the building group.\n\u2022 Building displacement action: displaces buildings that have proximity problems.\n\u2022 Local building removal action: removes buildings according to a local context.\n\u2022 Building removal/displacement action: selects the building that has the most serious proximity problems and removes it.\n\u2022 Global building removal action: removes buildings according to the global context.\nThe initial knowledge used for the experiment is presented in Table I. It tests all the possible actions for each state and does not define optimality nor stopping criterion. Thus, it ensures to find for each generalisation the best possible state considering the available constraints and actions. Nevertheless, it requires exploring many states per generalisation and is thus not efficient at all.\n2) Feature sets defined We defined 22 measures for the building group agent:\n\u2022 random: return a random number ranged between 0 and 1\n\u2022 id: building group identifier in the database \u2022 sat_proxy: satisfaction of the proximity constraint \u2022 sat_building_sat: satisfaction of the building\nsatisfaction constraint \u2022 sat_density: satisfaction of the density constraint \u2022 sat_spatial: satisfaction of the spatial distribution\nconstraint \u2022 sat_large: satisfaction of the large building\npreservation constraint \u2022 sat_corner: satisfaction of the corner building\npreservation constraint \u2022 nb_bldg: number of buildings \u2022 nb_large_bldg: Number of large buildings (larger than\na threshold)\n\u2022 nb_corner_bldg: Number of buildings located in the corner of roads\n\u2022 area: Building group area \u2022 density: Building density \u2022 min_sat: Minimum of the individual building\nsatisfaction \u2022 median_sat: Median of the individual building\nsatisfaction \u2022 first_quartile_sat: First quartile value of the individual\nbuilding satisfaction \u2022 min_dist: Minimum distance between buildings \u2022 median_dist: Median distance between buildings \u2022 first_quartile_dist: First quartile distance value\nbetween buildings \u2022 mean_dist: Mean distance between buildings \u2022 nb_overlaps: Number of buildings overlapping other\nbuildings or roads \u2022 ratio_overlaps: Ratio of building surface overlapping\nother buildings or roads.\nIn order to test our evaluation method, we defined three groups of measure (feature) sets:\n\u2022 Irrelevant measure sets (I): composed of measures that give no information about the knowledge. The interest to test these measures set is to check that our evaluation method allow to detect irrelevant measure sets.\n\u2022 Basic measure sets (B): composed of the constraint satisfaction. The interest to test these measure sets comes from the fact that these measures are often used to defined procedural knowledge (e.g. [16] and [17])\n\u2022 Complete measure set (C): composed of the constraint satisfaction and of additional measures.\nTable II gives the detailed composition of each measure set.\n\u2022 mean_dist \u2022 nb_overlaps \u2022 ratio_overlaps\nBuilding removal/di splacemen\nt action\n\u2022 random \u2022 id\n\u2022 sat_building_sat \u2022 sat_proxy \u2022 min_dist \u2022 median_dist \u2022 first_quartile_dist \u2022 mean_dist \u2022 nb_overlaps \u2022 ratio_overlaps\nGlobal building removal action\n\u2022 random \u2022 id\n\u2022 sat_density \u2022 sat_density \u2022 nb_bldg \u2022 area \u2022 density\n3) Results\nTable III presents the results of the irrelevant measure sets, Table IV of the basic measure sets and Table V of the complete measure sets. For each group of measure sets, we present for each element of knowledge:\n\u2022 Measures: The measures used by the rules obtained after revision.\n\u2022 Performance: The performance of the generalisation system while using the revised knowledge. This score was computed in a different area than the one used to revise the knowledge (and to compute the inconsistency rate of the measure sets). The test area is composed of 200 building groups and the revision was carried out from the generalisation of 50 building groups. The function uses to compute the system performance favours the effectiveness of the system (the cartographic quality of the results) over its efficiency (speed to carry out the generalisation process). Concerning the action application knowledge, only one score is presented for all actions because we revised all these elements of knowledge at the same time.\n\u2022 Minority: The proportion of examples belonging to the minority label. Concerning the elements of knowledge relative to action application, two labels are defined: \u201capply this action\u201d and \u201cdo not apply this action\u201d. For the stopping criterions, the two labels are: \u201cstop the generalisation process\u201d and \u201ccontinue the generalisation process\u201d. At last, for the optimality criterion, the two labels are: \u201coptimal state\u201d and \u201cnonoptimal state\u201d. The interest of this minority label proportion value comes from the fact that it gives an indication for the maximum value of the inconsistency rate. Indeed, in the case where no measure is relevant, no measure is kept after the measure selection step. Thus, card(SB)=1, and as only two different labels are defined for our elements of knowledge, the inconsistency rate is then bounded by:\n),(_),(_ )],(_),,(_max[ 1)( 21 21 labelsexamplesnblabelsexamplesnb labelsexamplesnblabelsexamplesnb BncyInconsiste BB BB + \u2212\u2264\n),(_),(_ )],(_),,(_min[)(\n21\n21 labelsexamplesnblabelsexamplesnb labelsexamplesnblabelsexamplesnbBncyInconsiste\nBB\nBB\n+ \u2264\u21d4\n\u2022 Inconsistency: The inconsistency rate computed by our\nmethod\nFor the irrelevant measure sets (Table III), the inconsistency rate is always equal to the proportion of the minority label, which shows that the measures bring no information about the knowledge. This statement is totally\nconsistent with the definition of these measure sets. We can note as well that no measure is used by the revised rules. At last, we can note that the performance after revision of the knowledge relative to the action application has been improved while no measure was used. In fact, the rule \u201calways apply the building removal/displacement action\u201d was replaced by the rule \u201cnever apply the building removal/displacement action\u201d, and the rule \u201calways apply the global building removal action\u201d was replaced by the rule \u201cnever apply the global building removal action\u201d. The replacement of these rules allowed to improve the efficiency of the system and thus its performance.\nFor the basic measure sets (Table IV), the inconsistency rates are still very close to the proportion of the minority label. The constraint satisfaction brings little information about the knowledge. The only elements of knowledge for which the inconsistency rate has significantly decreased are the stopping and the optimality criteria. It is for the optimality criterion that this rate has the most decreased and it is the only element of knowledge that uses measures and for which the performance has improved in comparison to the previous measure sets. Thus, we can state a correlation between the decrease of\nthe inconsistency rate and the fact that a relevant measure has been added to the measure set. At last, we can deduce from this second experiment that the use of measure sets uniquely composed of constraint satisfaction can be insufficient for knowledge definition.\nFor the complete measure sets (Table V), the adding of pertinent measures has allowed to decrease the inconsistency rate for some of the elements of knowledge. Concerning the elements of knowledge relative to the application of the Local building removal action and to the Building removal/displacement action, the rate remained (almost) equal to the proportion of examples of the minority label. In fact, the new measures did not bring much information about these elements of knowledge. For the other elements of knowledge, the inconsistency rate has decreased and rules using measures has been defined. We can note that these rules are pertinent because they allowed an improvement of the system performance. To conclude on this last group of measure sets, we can note that they are pertinent, but they can be improved. Typically, it could be interesting to define new measures for the element of knowledge relative to the building displacement action for which the inconsistency rate is still high.\nThese experiments allowed to show the relevance of our measure set evaluation method. Indeed, it allowed to diagnose which measure sets were pertinent for the definition of good knowledge. These experiments showed as well that the use of measure sets uniquely composed of the constraint satisfaction can be insufficient to define good knowledge. The adding of supplementary measures is often necessary. At last, these experiments allowed to point out the measure sets that needs more urgently to be improved.\nV. CONCLUSION In this paper, we proposed a method dedicated to the supervised evaluation of feature sets. This method allows\nto give a diagnostic of feature sets by analysing the inconsistency of example bases. We showed the pertinence of our method through a case-study carried out in the domain of geomatic.\nAs our method is generic and can be used as soon as an example base is built, an interesting perspective could consist in testing it for other applications domain than geomatic.\nAnother interesting perspective could be to define, from this feature set evaluation method, a complete methodology for feature set definition. Indeed, by detecting the need of new features, our method could be part of a broader feature set definition methodology that could help experts to define their own feature set."}], "references": [{"title": "A Practical Approach to Feature Selection", "author": ["K. Kira", "L.A. Rendell"], "venue": "Ninth International Workshop on Machine Learning\u201d", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial Intelligence 97(1-2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Practical feature subset selection for machine learning", "author": ["M.A. Hall", "L.A. Smith"], "venue": "Australian Computer Science Conference", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "An Introduction to Variable and Feature Selection", "author": ["E. Guyon", "A. Elisseff"], "venue": "Journal of Machine Learning Research 3", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "On the Handling of Continuous-Valued Attributed in Decision Tree Generation", "author": ["U. Fayyad", "K. Irani"], "venue": "Machine Learning 8", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "MODL : A Bayes optimal discretization method for continuous attributes", "author": ["M. Boull\u00e9"], "venue": "Machine learning 65(1)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Overcoming the Knowledge Acquisition Bottleneck in Map Generalization: the Role of Interactive Systems and Computational Intelligence", "author": ["R. Weibel", "S. Keller", "T. Reichenbacher"], "venue": "COSIT conference", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Methods for improving and updating the knowledge of a generalization system", "author": ["A. Ruas", "A. Dyevre", "D. Duch\u00eane", "P. Taillandier"], "venue": "Autocarto, Vancouver", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Experiments with Learning Techniques for Spatial Model Enrichment and Line Generalization", "author": ["C. Plazanet", "N. Martini Bigolin", "A. Ruas"], "venue": "GeoInformatica 2(4)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Interpretation of Spatial Data Bases using Machine Learning Techniques", "author": ["M. Sester"], "venue": "SDH", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "A method based on samples to capture user needs for generalisation", "author": ["F. Hubert", "A. Ruas"], "venue": "fifth workshop on progress in automated map generalisation", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "A review and conceptual framework of automated map generalisation", "author": ["K. Brassel", "R. Weibel"], "venue": "IJGIS 2(3)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1988}, {"title": "Integrating multi-agent", "author": ["M. Barrault", "N. Regnauld", "C. Duch\u00eane", "K. Haire", "C. Baeijs", "Y. Demazeau", "P. Hardy", "W. Mackaness", "A. Ruas", "R. Weibel"], "venue": "object-oriented, and algorithmic techniques for improved automated map generalization, ICC", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A prototype generalisation system based on the multi-agent system paradigm", "author": ["A. Ruas", "C. Duch\u00eane"], "venue": "W.A. Mackaness; A. Ruas & L.T. Sarjakoski, ed.,'Generalisation of Geographic information: cartographic modelling and applications', Elsevier Ltd", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Knowledge revision in systems based on an informed tree search strategy: application to cartographic generalisation", "author": ["P. Taillandier", "C. Duch\u00eane", "A. Drogoul"], "venue": "CSTST. Paris", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Automated sequencing of generalisation services based on collaborative filtering", "author": ["D. Burghardt", "M. Neun"], "venue": "GIScience", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic Knowledge Revision of a Generalisation System", "author": ["P. Taillandier"], "venue": "Workshop on generalisation and multiple representation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Other methods such as [1] propose to analysis the linked between the variations of a feature value and the label associated.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "For the second group of method, a classic strategy consists in evaluating the predictive model built from the example base (often by cross-validation [2]).", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "degree of redundancy between features [3].", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "In the literature, many supervised filtering algorithms were proposed (see [4]).", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "[5] or [6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5] or [6]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "[7] or [8]), for data enrichment (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] or [8]), for data enrichment (e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "[9] or [10]), system calibration (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[9] or [10]), system calibration (e.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "[11]), etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "One approach to solve it consists in using a local, step-by-step and knowledge-based method ([7] and [12]).", "startOffset": 93, "endOffset": 96}, {"referenceID": 11, "context": "One approach to solve it consists in using a local, step-by-step and knowledge-based method ([7] and [12]).", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "A classic generalisation model based on such approach is the AGENT model ([13] and [14]).", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "A classic generalisation model based on such approach is the AGENT model ([13] and [14]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "3) Procedural knowledge revision In [15], we proposed an approach to revise the procedural knowledge of systems based on the AGENT model.", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "[16] and [17]) \u2022 Complete measure set (C): composed of the constraint satisfaction and of additional measures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16] and [17]) \u2022 Complete measure set (C): composed of the constraint satisfaction and of additional measures.", "startOffset": 9, "endOffset": 13}], "year": 2010, "abstractText": "Nowadays, supervised learning is commonly used in many domains. Indeed, many works propose to learn new knowledge from examples that translate the expected behaviour of the considered system. A key issue of supervised learning concerns the description language used to represent the examples. In this paper, we propose a method to evaluate the feature set used to describe them. Our method is based on the computation of the consistency of the example base. We carried out a case study in the domain of geomatic in order to evaluate the sets of measures used to characterise geographic objects. The case study shows that our method allows to give relevant evaluations of measure sets. Supervised feature evaluation; consistency computation; geomatic", "creator": "'Certified by IEEE PDFeXpress at 07/24/2010 4:15:49 AM'"}}}