{"id": "1204.5043", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2012", "title": "Sparse Prediction with the $k$-Support Norm", "abstract": "now we derive a novel norm criterion that corresponds accurately to that the tightest strongly convex mesh relaxation of constraint sparsity combined solely with fitting an l2 size penalty and can directly also indirectly be essentially interpreted as a uniform group lasso norm with overlaps. specifically we mainly show moreover that this new norm provides one a tighter relaxation than the elastic net and suggest also using it as a replacement for choosing the lasso or the elastic net in sparse population prediction problems.", "histories": [["v1", "Mon, 23 Apr 2012 12:35:56 GMT  (317kb)", "https://arxiv.org/abs/1204.5043v1", null], ["v2", "Tue, 12 Jun 2012 08:59:52 GMT  (304kb)", "http://arxiv.org/abs/1204.5043v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["andreas argyriou", "rina foygel", "nathan srebro"], "accepted": true, "id": "1204.5043"}, "pdf": {"name": "1204.5043.pdf", "metadata": {"source": "CRF", "title": "Sparse Prediction with the k-Support Norm", "authors": ["Andreas Argyriou"], "emails": ["argyriou@ttic.edu", "rina@uchicago.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 4.\n50 43\nv2 ["}, {"heading": "1 Introduction", "text": "Regularizing with the \u21131 norm, when we expect a sparse solution to a regression problem, is often justified by \u2016w\u20161 being the \u201cconvex envelope\u201d of \u2016w\u20160 (the number of non-zero coordinates of a vectorw \u2208 Rd). That is, \u2016w\u20161 is the tightest convex lower bound on \u2016w\u20160. But we must be careful with this statement\u2014 for sparse vectors with large entries, \u2016w\u20160 can be small while \u2016w\u20161 is large. In order to discuss convex lower bounds on \u2016w\u20160, we must impose some scale constraint. A more accurate statement is that \u2016w\u20161 \u2264 \u2016w\u2016\u221e\u2016w\u20160, and so, when the magnitudes of entries in w are bounded by 1, then \u2016w\u20161 \u2264 \u2016w\u20160, and indeed it is the largest such convex lower bound. Viewed as a convex outer relaxation,\nS (\u221e) k := { w \u2223 \u2223 \u2016w\u20160 \u2264 k, \u2016w\u2016\u221e \u2264 1 } \u2286 { w \u2223 \u2223 \u2016w\u20161 \u2264 k } .\nIntersecting the right-hand-side with the \u2113\u221e unit ball, we get the tightest convex outer bound (convex hull) of S (\u221e) k :\n{ w \u2223 \u2223 \u2016w\u20161 \u2264 k, \u2016w\u2016\u221e \u2264 1 } = conv(S (\u221e) k ) .\nHowever, in our view, this relationship between \u2016w\u20161 and \u2016w\u20160 yields disappointing learning guarantees, and does not appropriately capture the success of the \u21131 norm as a surrogate for sparsity. In particular, the sample complexity\n1 of learning a linear predictor with k non-zero entries by empirical risk minimization inside this class (an NP-hard optimization problem) scales as2 O(k log d), but relaxing to the constraint \u2016w\u20161 \u2264 k yields a sample complexity which scales as O(k2 log d), because the sample complexity of \u21131-regularized learning scales quadratically with the \u21131 norm [11, 19].\nPerhaps a better reason for the \u21131 norm being a good surrogate for sparsity is that, not only do we expect the magnitude of each entry of w to be bounded, but we further expect \u2016w\u20162 to be small. In a regression setting, with a vector of features x, this can be justified when E[(x\u22a4w)2] is bounded (a reasonable assumption) and the features are not too correlated\u2014see, e.g. [16]. More broadly, especially in the presence of correlations, we might require this as a modeling assumption to aid in robustness and generalization. In any case, we have \u2016w\u20161 \u2264 \u2016w\u20162 \u221a\n\u2016w\u20160, and so if we are interested in predictors with bounded \u21132 norm, we can motivate the \u21131 norm through the following relaxation of sparsity, where the scale is now set by the \u21132 norm:\n{ w \u2223 \u2223 \u2016w\u20160 \u2264 k, \u2016w\u20162 \u2264 B } \u2286 { w \u2223 \u2223 \u2016w\u20161 \u2264 B \u221a k } .\nThe sample complexity when using the relaxation now scales as3 O(k log d).\nSparse + \u21132 constraint. Our starting point is then that of combining sparsity and \u21132 regularization, and learning a sparse predictor with small \u21132 norm. We are thus interested in classes of the form\nS (2) k := { w \u2223 \u2223 \u2016w\u20160 \u2264 k, \u2016w\u20162 \u2264 1 } .\nAs discussed above, the class {\u2016w\u20161 \u2264 \u221a k} (corresponding to the standard Lasso) provides a convex relaxation of S (2) k . But it is clear that we can get a\n1 We define this as the number of observations needed in order to ensure expected prediction error no more than \u01eb worse than that of the best k-sparse predictor, for an arbitrary constant \u01eb (that is, we suppress the dependence on \u01eb and focus on the dependence on the sparsity k and dimensionality d).\n2This is based on bounding the VC-subgraph dimension of this class, which is essentially the effective number of parameters.\n3More precisely, the sample complexity is O(B2k log d), where the dependence on B2 is to be expected. Note that if feature vectors are \u2113\u221e-bounded (i.e. individual features are bounded), the sample complexity when using only \u2016w\u20162 \u2264 B (without a sparsity or \u21131 constraint) scales as O(B2d). That is, even after identifying the correct support, we still need a sample complexity that scales with B2.\ntighter convex relaxation by keeping the \u21132 constraint as well:\nS (2) k \u2286\n{\nw \u2223 \u2223 \u2016w\u20161 \u2264 \u221a k, \u2016w\u20162 \u2264 1 } ( { w \u2223 \u2223 \u2016w\u20161 \u2264 \u221a k } . (1)\nConstraining (or equivalently, penalizing) both the \u21131 and \u21132 norms, as in (1), is known as the \u201celastic net\u201d [5, 20] and has indeed been advocated as a better alternative to the Lasso. In this paper, we ask whether the elastic net is the tightest convex relaxation to sparsity plus \u21132 (that is, to S (2) k ) or whether a tighter, and better, convex relaxation is possible.\nA new norm. We consider the convex hull (tightest convex outer bound) of S (2) k ,\nCk := conv(S (2) k ) = conv { w \u2223 \u2223 \u2016w\u20160 \u2264 k, \u2016w\u20162 \u2264 1 } . (2)\nWe study the gauge function associated with this convex set, that is, the norm whose unit ball is given by (2), which we call the k-support norm. We show that, for k > 1, this is indeed a tighter convex relaxation than the elastic net (that is, both inequalities in (1) are in fact strict inequalities), and is therefore a better convex constraint than the elastic net when seeking a sparse, low \u21132-norm linear predictor. We thus advocate using it as a replacement for the elastic net.\nHowever, we also show that the gap between the elastic net and the k-support norm is at most a factor of \u221a 2, corresponding to a factor of two difference in the sample complexity. Thus, our work can also be interpreted as justifying the use of the elastic net, viewing it as a fairly good approximation to the tightest possible convex relaxation of sparsity intersected with an \u21132 constraint. Still, even a factor of two should not necessarily be ignored and, as we show in our experiments, using the tighter k-support norm can indeed be beneficial.\nTo better understand the k-support norm, we show in Section 2 that it can also be described as the group lasso with overlaps norm [10] corresponding to all\n( d k ) subsets of k features. Despite the exponential number of groups in this description, we show that the k-support norm can be calculated efficiently in time O(d log d) and that its dual is given simply by the \u21132 norm of the k largest entries. We also provide efficient first-order optimization algorithms for learning with the k-support norm.\nRelated Work In many learning problems of interest, Lasso has been observed to shrink too many of the variables of w to zero. In particular, in many applications, when a group of variables is highly correlated, the Lasso may prefer a sparse solution, but we might gain more predictive accuracy by including all the correlated variables in our model. These drawbacks have recently motivated the use of various other regularization methods, such as the elastic net [20], which penalizes the regression coefficients w with a combination of \u21131 and \u21132 norms:\nmin\n{ 1\n2 \u2016Xw \u2212 y\u20162 + \u03bb1 \u2016w\u20161 + \u03bb2 \u2016w\u201622 : w \u2208 Rd\n}\n, (3)\nwhere for a sample of size n, y \u2208 Rn is the vector of response values, and X \u2208 Rn\u00d7d is a matrix with column j containing the values of feature j.\nThe elastic net can be viewed as a trade-off between \u21131 regularization (the Lasso) and \u21132 regularization (Ridge regression [9]), depending on the relative values of \u03bb1 and \u03bb2. In particular, when \u03bb2 = 0, (3) is equivalent to the Lasso. This method, and the other methods discussed below, have been observed to significantly outperform Lasso in many real applications.\nThe pairwise elastic net (PEN), proposed by [13], has a penalty function that accounts for similarity among features:\n\u2016w\u2016PENR = \u2016w\u201622 + \u2016w\u201621 \u2212 |w|\u22a4R|w| ,\nwhere R \u2208 [0, 1]p\u00d7p is a matrix with Rjk measuring similarity between features Xj and Xk. The trace Lasso [6] is a second method proposed to handle correlations within X , defined by\n\u2016w\u2016traceX = \u2016Xdiag(w)\u2016\u2217 ,\nwhere \u2016 \u00b7 \u2016\u2217 denotes the matrix trace-norm (the sum of the singular values) and promotes a low-rank solution. If the features are orthogonal, then both the PEN and the Trace Lasso are equivalent to the Lasso. If the features are all identical, then both penalties are equivalent to Ridge regression (penalizing \u2016w\u20162). Another existing penalty is OSCAR [3], given by\n\u2016w\u2016OSCARc = \u2016w\u20161 + c \u2211\nj<k\nmax{|wj |, |wk|} .\nLike the elastic net, each one of these three methods also \u201cprefers\u201d averaging similar features over selecting a single feature.\n2 The k-Support Norm\nOne argument for the elastic net has been the flexibility of tuning the cardinality k of the regression vector w. Thus, when groups of correlated variables are present, a larger k may be learned, which corresponds to a higher \u03bb2 in (3). A more natural way to obtain such an effect of tuning the cardinality is to consider the convex hull of cardinality k vectors,\nCk = conv(S (2) k ) = conv{w \u2208 Rd \u2223 \u2223 \u2016w\u20160 \u2264 k, \u2016w\u20162 \u2264 1}.\nClearly the sets Ck are nested, and C1 and Cd are the unit balls for the \u21131 and \u21132 norms, respectively. Consequently we define the k-support norm as the norm whose unit ball equals Ck (the gauge function associated with the Ck ball).\n4 An equivalent definition is the following variational formula:\n4 The gauge function \u03b3Ck : R d \u2192 R\u222a{+\u221e} is defined as \u03b3Ck (x) = inf{\u03bb \u2208 R+ : x \u2208 \u03bbCk}.\nDefinition 2.1. Let k \u2208 {1, . . . , d}. The k-support norm \u2016 \u00b7 \u2016spk is defined, for every w \u2208 Rd, as\n\u2016w\u2016spk := min { \u2211\nI\u2208Gk\n\u2016vI\u20162 : supp(vI) \u2286 I, \u2211\nI\u2208Gk\nvI = w\n}\n,\nwhere Gk denotes the set of all subsets of {1, . . . , d} of cardinality at most k.\nThe equivalence is immediate by rewriting vI = \u00b5IzI in the above definition, where \u00b5I \u2265 0, zI \u2208 Ck, \u2200I \u2208 Gk, \u2211\nI\u2208Gk \u00b5I = 1. In addition, this immediately implies that \u2016\u00b7\u2016spk is indeed a norm. In fact, the k-support norm is equivalent to the norm used by the group lasso with overlaps [10], when the set of overlapping groups is chosen to be Gk (however, the group lasso has traditionally been used for applications with some specific known group structure, unlike the case considered here).\nAlthough the variational definition 2.1 is not amenable to computation because of the exponential growth of the set of groups Gk, the k-support norm is computationally very tractable, with an O(d log d) algorithm described in Section 2.2.\nAs already mentioned, \u2016 \u00b7\u2016sp1 = \u2016 \u00b7\u20161 and \u2016 \u00b7\u2016spd = \u2016 \u00b7\u20162. The unit ball of this new norm in R3 for k = 2 is depicted in Figure 1. We immediately notice several differences between this unit ball and the elastic net unit ball. For example, at points with cardinality k and \u21132 norm equal to 1, the k-support norm is not differentiable, but unlike the \u21131 or elastic-net norm, it is differentiable at points with cardinality less than k. Thus, the k-support norm is less \u201cbiased\u201d towards sparse vectors than the elastic net and the \u21131 norm."}, {"heading": "2.1 The Dual Norm", "text": "It is interesting and useful to compute the dual of the k-support norm. We follow the notation of [2] for ordered vectors: for any w \u2208 Rd, |w| is the vector\nof absolute values, and w\u2193i is the i-th largest element of w. We have\n\u2016u\u2016sp \u2217 k = max {\u3008w, u\u3009 : \u2016w\u2016 sp k \u2264 1} = max\n \n\n( \u2211\ni\u2208I u2i\n) 1 2\n: I \u2208 Gk\n \n\n=\n( k\u2211\ni=1\n(|u|\u2193i )2 ) 1 2 =: \u2016u\u2016(2)(k) .\nThis is the \u21132-norm of the largest k entries in u, and is known as the 2-k symmetric gauge norm [2].\nNot surprisingly, this dual norm interpolates between the \u21132 norm (when k = d and all entries are taken) and the \u2113\u221e norm (when k = 1 and only the largest entry is taken). This parallels the interpolation of the k-support norm between the \u21131 and \u21132 norms.\nLike the \u2113p norms and elastic net, the k-support norm and its dual are symmetric gauge functions, that is, sign- and permutation-invariant norms. For properties of such norms, see [2]."}, {"heading": "2.2 Computation of the Norm", "text": "In this section, we derive an alternative formula for the k-support norm, which leads to computation of the value of the norm in O(d log d) steps.\nProposition 2.1. For every w \u2208 Rd,\n\u2016w\u2016spk =\n\n\nk\u2212r\u22121\u2211\ni=1\n(|w|\u2193i )2 + 1\nr + 1\n( d\u2211\ni=k\u2212r |w|\u2193i\n)2 \n\n1 2\n,\nwhere, letting |w|\u21930 denote +\u221e, r is the unique integer in {0, . . . , k\u22121} satisfying\n|w|\u2193k\u2212r\u22121 > 1\nr + 1\nd\u2211\ni=k\u2212r |w|\u2193i \u2265 |w|\u2193k\u2212r . (4)\nThis result shows that \u2016\u00b7\u2016spk trades off between the \u21131 and \u21132 norms in a way that favors sparse vectors but allows for cardinality larger than k. It combines the uniform shrinkage of an \u21132 penalty for the largest components, with the sparse shrinkage of an \u21131 penalty for the smallest components.\nProof of Proposition 2.1. We will use the inequality \u3008w, u\u3009 \u2264 \u3008w\u2193, u\u2193\u3009 [7].\nWe have\n1 2 (\u2016w\u2016spk )2 = max\n{\n\u3008u,w\u3009 \u2212 1 2 (\u2016u\u2016(2)(k))2 : u \u2208 Rd\n}\n= max\n{ d\u2211\ni=1\n\u03b1i|w|\u2193i \u2212 1\n2\nk\u2211\ni=1\n\u03b12i : \u03b11 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1d \u2265 0 }\n= max\n{ k\u22121\u2211\ni=1\n\u03b1i|w|\u2193i + \u03b1k d\u2211\ni=k\n|w|\u2193i \u2212 1\n2\nk\u2211\ni=1\n\u03b12i : \u03b11 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1k \u2265 0 } .\nLet Ar := d\u2211 i=k\u2212r |w|\u2193i for r \u2208 {0, . . . , k \u2212 1}. If A0 < |w|\u2193k\u22121 then the solution \u03b1 is given by \u03b1i = |w|\u2193i for i = 1, . . . , (k \u2212 1), \u03b1i = A0 for i = k, . . . , d. If A0 \u2265 |w|\u2193k\u22121 then the optimal \u03b1k, \u03b1k\u22121 lie between |w| \u2193 k\u22121 and A0, and have to be equal. So, the maximization becomes\nmax\n{ k\u22122\u2211\ni=1\n\u03b1i|w|\u2193i \u2212 1\n2\nk\u22122\u2211\ni=1\n\u03b12i +A1\u03b1k\u22121 \u2212 \u03b12k\u22121 : \u03b11 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1k\u22121 \u2265 0 } .\nIf A0 \u2265 |w|\u2193k\u22121 and |w| \u2193 k\u22122 > A1 2 then the solution is \u03b1i = |w| \u2193 i for i = 1, . . . , (k\u2212 2), \u03b1i = A1 2 for i = (k\u2212 1), . . . , d. Otherwise we proceed as before and continue this process. At stage r the process terminates if A0 \u2265 |w|\u2193k\u22121, . . . , Ar\u22121 r \u2265 |w|\u2193k\u2212r , Arr+1 < |w| \u2193 k\u2212r\u22121 and all but the last two inequalities are redundant. Hence the condition can be rewritten as (4). One optimal solution is \u03b1i = |w|\u2193i for i = 1, . . . , k\u2212 r\u2212 1, \u03b1i = Arr+1 for i = k\u2212 r, . . . , d. This proves the claim.\n2.3 Learning with the k-support norm\nWe thus propose using learning rules with k-support norm regularization. These are appropriate when we would like to learn a sparse predictor that also has low \u21132 norm, and are especially relevant when features might be correlated (that is, in almost all learning tasks) but the correlation structure is not known in advance. For regression problems with squared error loss, the resulting learning rule is of the form\nmin\n{ 1\n2 \u2016Xw \u2212 y\u20162 + \u03bb 2 (\u2016w\u2016spk ) 2 : w \u2208 Rd\n}\n(5)\nwith \u03bb > 0 a regularization parameter and k \u2208 {1, . . . , d} also a parameter to be tuned. As typical in regularization-based methods, both \u03bb and k can be selected by cross validation [8]. Although we have motivated this norm by considering S (2) k , the set of k-sparse unit vectors, the parameter k does not necessarily correspond to the sparsity level of the fitted vector of coefficients, and should be chosen via cross-validation independently of the desired sparsity level."}, {"heading": "3 Relation to the Elastic Net", "text": "Recall that the elastic net with penalty parameters \u03bb1 and \u03bb2 selects a vector of coefficients given by\nargmin\n{ 1\n2 \u2016Xw \u2212 y\u20162 + \u03bb1 \u2016w\u20161 + \u03bb2 \u2016w\u201622\n}\n. (6)\nFor ease of comparison with the k-support norm, we first show that the set of optimal solutions for the elastic net, when the parameters are varied, is the same as for the norm\n\u2016w\u2016elk := max { \u2016w\u20162, \u2016w\u20161\u221a\nk\n}\n,\nwhen k \u2208 [1, d], corresponding to the unit ball in (1) (note that k is not necessarily an integer). To see this, let w\u0302 be a solution to (6), and let k := (\u2016w\u0302\u20161/\u2016w\u0302\u20162)2 \u2208 [1, d] .\nThen for any w 6= w\u0302, if \u2016w\u2016elk \u2264 \u2016w\u0302\u2016elk , then \u2016w\u2016p \u2264 \u2016w\u0302\u2016p for p = 1, 2. Since w\u0302 is a solution to (6), therefore, \u2016Xw\u2212 y\u201622 \u2265 \u2016Xw\u0302\u2212 y\u201622. This proves that, for some constraint parameter B,\nw\u0302 = argmin\n{ 1\nn \u2016Xw \u2212 y\u201622 : \u2016w\u2016elk \u2264 B\n}\n.\nLike the k-support norm, the elastic net interpolates between the \u21131 and \u21132 norms. In fact, when k is an integer, any k-sparse unit vector w \u2208 Rd must lie in the unit ball of \u2016 \u00b7 \u2016elk . Since the k-support norm gives the convex hull of all k-sparse unit vectors, this immediately implies that\n\u2016w\u2016elk \u2264 \u2016w\u2016spk \u2200 w \u2208 Rd .\nThe two norms are not equal, however. The difference between the two is illustrated in Figure 1, where we see that the k-support norm is more \u201crounded\u201d.\nTo see an example where the two norms are not equal, we set d = 1+ k2 for some large k, and let w = (k1.5, 1, 1, . . . , 1)\u22a4 \u2208 Rd. Then\n\u2016w\u2016elk = max { \u221a k3 + k2, k1.5 + k2\u221a\nk\n} = k1.5 (\n1 + 1\u221a k\n)\n.\nTaking u = ( 1\u221a 2 , 1\u221a 2k , 1\u221a 2k , . . . , 1\u221a 2k )\u22a4, we have \u2016u\u2016(2)(k) < 1, and recalling this norm is dual to the k-support norm:\n\u2016w\u2016spk > \u3008w, u\u3009 = k1.5\u221a 2 + k2 \u00b7 1\u221a 2k = \u221a 2 \u00b7 k1.5 .\nIn this example, we see that the two norms can differ by as much as a factor of\u221a 2. We now show that this is actually the most by which they can differ. Proposition 3.1. \u2016 \u00b7 \u2016elk \u2264 \u2016 \u00b7 \u2016spk < \u221a 2 \u2016 \u00b7 \u2016elk .\nProof. We show that these bounds hold in the duals of the two norms. First, since \u2016 \u00b7 \u2016elk is a maximum over the \u21131 and \u21132 norms, its dual is given by\n\u2016u\u2016(el) \u2217\nk := inf a\u2208Rd\n{ \u2016a\u20162 + \u221a k \u00b7 \u2016u\u2212 a\u2016\u221e }\nNow take any u \u2208 Rd. First we show \u2016u\u2016(2)(k) \u2264 \u2016u\u2016 (el)\u2217 k . Without loss of generality, we take u1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 ud \u2265 0. For any a \u2208 Rd,\n\u2016u\u2016(2)(k) = \u2016u1:k\u20162 \u2264 \u2016a1:k\u20162 + \u2016u1:k \u2212 a1:k\u20162 \u2264 \u2016a\u20162 + \u221a k\u2016u\u2212 a\u2016\u221e .\nFinally, we show that \u2016u\u2016(el) \u2217 k < \u221a 2 \u2016u\u2016(2)(k). Taking a = (u1 \u2212 uk+1, . . . , uk \u2212 uk+1, 0, . . . , 0) \u22a4, we have\n\u2016u\u2016(el) \u2217 k \u2264 \u2016a\u20162 + \u221a k \u00b7 \u2016u\u2212 a\u2016\u221e =\n\u221a \u221a \u221a \u221a k\u2211\ni=1\n(ui \u2212 uk+1)2 + \u221a k|uk+1|\n\u2264\n\u221a \u221a \u221a \u221a k\u2211\ni=1\n(u2i \u2212 u2k+1) + \u221a k u2k+1 \u2264 \u221a 2 \u00b7\n\u221a \u221a \u221a \u221a k\u2211\ni=1\n(u2i \u2212 u2k+1) + k u2k+1\n= \u221a 2 \u2016u\u2016(2)(k) .\nFurthermore, this yields a strict inequality, because if u1 > uk+1, the nextto-last inequality is strict, while if u1 = \u00b7 \u00b7 \u00b7 = uk+1, then the last inequality is strict."}, {"heading": "4 Optimization", "text": "Solving the optimization problem (5) efficiently can be done with a first-order proximal algorithm. Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient. These methods require fast computation of the gradient \u2207f and the proximity operator\nprox\u03c9(x) := argmin\n{ 1\n2 \u2016u\u2212 x\u20162 + \u03c9(u) : u \u2208 Rd\n}\n.\nIn particular, accelerated first-order methods, proposed by Nesterov [14, 15] require two levels of memory at each iteration and exhibit an optimal O\n( 1 T 2 )\nconvergence rate for the objective after T iterations. To obtain a proximal method for k-support regularization, it suffices to compute the proximity map of g = 12L (\u2016 \u00b7 \u2016 sp k )\n2, for any L > 0. This can be done in O(d(k + log d)) steps with Algorithm 1.\nAlgorithm 1 Computation of the proximity operator.\nInput v \u2208 Rd Output q = prox 1\n2L (\u2016\u00b7\u2016sp k )2(v)\nFind r \u2208 {0, . . . , k \u2212 1}, \u2113 \u2208 {k, . . . , d} such that\n1 L+1zk\u2212r\u22121 > Tr,\u2113 \u2113\u2212k+(L+1)r+L+1 \u2265 1L+1zk\u2212r (7)\nz\u2113 > Tr,\u2113 \u2113\u2212k+(L+1)r+L+1 \u2265 z\u2113+1 (8)\nwhere z := |v|\u2193, z0 := +\u221e, zd+1 := \u2212\u221e, Tr,\u2113 := \u2113\u2211\ni=k\u2212r zi\nqi \u2190\n \n L L+1zi if i = 1, . . . , k \u2212 r \u2212 1 zi \u2212 Tr,\u2113\u2113\u2212k+(L+1)r+L+1 if i = k \u2212 r, . . . , \u2113 0 if i = \u2113 + 1, . . . , d\nReorder and change signs of q to conform with v\nAlgorithm 2 Accelerated k-support regularization.\nw1 = \u03b11 \u2208 Rd, \u03b81 \u2190 1 for t=1,2,. . . do\n\u03b8t+1 \u2190 1+ \u221a 1+4\u03b82t 2 wt+1 \u2190 prox \u03bb 2L (\u2016\u00b7\u2016sp k )2 ( \u03b1t \u2212 1LX\u22a4(X\u03b1t \u2212 y) ) using Algorithm 1\n\u03b1t+1 \u2190 wt+1 + \u03b8t\u22121\u03b8t+1 (wt+1 \u2212 wt) end for\nProof of Correctness of Algorithm 1. Since the support-norm is sign and permutation invariant, prox(v) has the same ordering and signs as v. Hence, without loss of generality, we may assume that v1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 vd \u2265 0 and require that q1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 qd \u2265 0, which follows from inequality (7) and the fact that z is ordered.\nNow, q = prox(v) is equivalent to Lz \u2212 Lq = Lv \u2212 Lq \u2208 \u2202 12 (\u2016 \u00b7 \u2016 sp k ) 2(q). It suffices to show that, for w = q, Lz\u2212Lq is an optimal \u03b1 in the proof of Proposition 2.1. Indeed, Ar corresponds to d\u2211\ni=k\u2212r qi =\n\u2113\u2211\ni=k\u2212r\n( zi \u2212 Tr,\u2113\u2113\u2212k+(L+1)r+L+1 ) =\nTr,\u2113 \u2212 (\u2113\u2212k+r+1)Tr,\u2113\u2113\u2212k+(L+1)r+L+1 = (r+ 1) LTr,\u2113 \u2113\u2212k+(L+1)r+L+1 and (4) is equivalent to condition (7). For i \u2264 k \u2212 r \u2212 1, we have Lzi \u2212 Lqi = qi. For k \u2212 r \u2264 i \u2264 \u2113, we have Lzi\u2212Lqi = 1r+1Ar. For i \u2265 \u2113+1, since qi = 0, we only need Lzi\u2212Lqi \u2264 1r+1Ar, which is true by (8).\nWe can now apply a standard accelerated proximal method, such as FISTA [1], to (5), at each iteration using the gradient of the loss and performing a prox step using Algorithm 1. The FISTA guarantee ensures us that, with appropriate\nstep sizes, after T such iterations, we have:\n1 2 \u2016XwT\u2212y\u20162+ \u03bb 2 (\u2016wT \u2016spk )\n2 \u2264 ( 1\n2 \u2016Xw\u2217\u2212y\u20162+\u03bb 2 (\u2016w\u2217\u2016spk ) 2\n)\n+ 2L\u2016w\u2217 \u2212 w1\u20162\n(T + 1)2 ."}, {"heading": "5 Empirical Comparisons", "text": "Our theoretical analysis indicates that the k-support norm and the elastic net differ by at most a factor of \u221a 2, corresponding to at most a factor of two difference in their sample complexities and generalization guarantees. We thus do not expect huge differences between their actual performances, but would still like to see whether the tighter relaxation of the k-support norm does yield some gains.\nSynthetic Data For the first simulation we follow [20, Sec. 5, example 4]. In this experimental protocol, the target (oracle) vector equals\nw\u2217 = (3, . . . , 3 \ufe38 \ufe37\ufe37 \ufe38\n15\n, 0 . . . , 0 \ufe38 \ufe37\ufe37 \ufe38\n25\n) ,\nwith y = (w\u2217)\u22a4x+N (0, 1). The input data X were generated from a normal distribution such that components 1, . . . , 5 have the same random mean Z1 \u223c N (0, 1), components 6, . . . , 10 have mean Z2 \u223c N (0, 1) and components 11, . . . , 15 have mean Z3 \u223c N (0, 1). A total of 50 data sets were created in this way, each containing 50 training points, 50 validation points and 350 test points. The goal is to achieve good prediction performance on the test data.\nWe compared the k-support norm with Lasso and the elastic net. We considered the ranges k = {1, . . . , d} for k-support norm regularization, \u03bb = 10i, i = {\u221215, . . . , 5}, for the regularization parameter of Lasso and k-support regularization and the same range for the \u03bb1, \u03bb2 of the elastic net. For each method, the optimal set of parameters was selected based on mean squared error on the validation set. The error reported in Table 5 is the mean squared error with respect to the oracle w\u2217, namely MSE = (w\u0302\u2212w\u2217)\u22a4V (w\u0302 \u2212w\u2217), where V is the population covariance matrix of Xtest.\nBeyond the predictive gains, to further illustrate the effect of the k-support norm, in Figure 5 we show the coefficients learned by each method, in absolute value. For each image, one row corresponds to the w learned for one of the 50 data sets. Whereas the elastic net can learn higher values at the relevant features, a better feature pattern with less variability emerges when using the k-support norm.\nSouth African Heart Data This is a classification task which has been used in [8]. There are 9 variables and 462 examples, and the response is presence/absence of coronary heart disease. We normalized the data so that each\npredictor variable has zero mean and unit variance. We then split the data 50 times randomly into training, validation, and test sets of sizes 400, 30, and 32 respectively. For each method, parameters were selected using the validation data. In Tables 5, we report the MSE and accuracy of each method on the test data. We observe that all three methods have identical performance.\n20 Newsgroups This is a binary classification version of 20 newsgroups created in [12] which can be found in the LIBSVM data repository.5 The positive class consists of the 10 groups with names of form sci.*, comp.*, or misc.forsale and the negative class consists of the other 10 groups. To reduce the number of features, we removed the words which appear in less than 3 documents. We randomly split the data into a training, a validation and a test set of sizes 14000,1000 and 4996, respectively. We report MSE and accuracy on the test data in Table 5. We found that k-support regularization gave improved prediction accuracy over both other methods.6"}, {"heading": "6 Summary", "text": "We introduced the k-support norm as the tightest convex relaxation of sparsity plus \u21132 regularization, and showed that it is tighter than the elastic net by\n5http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/ 6Regarding other sparse prediction methods, we did not manage to compare with OSCAR, due to memory limitations, or to PEN or trace Lasso, which do not have code available online.\nexactly a factor of \u221a 2. In our view, this sheds light on the elastic net as a close approximation to this tightest possible convex relaxation, and motivates using the k-support norm when a tighter relaxation is sought. This is also demonstrated in our empirical results.\nWe note that the k-support norm has better prediction properties, but not necessarily better sparsity-inducing properties, as evident from its more rounded unit ball. It is well understood that there is often a tradeoff between sparsity and good prediction, and that even if the population optimal predictor is sparse, a denser predictor often yields better predictive performance [20, 3, 10]. For example, in the presence of correlated features, it is often beneficial to include several highly correlated features rather than a single representative feature. This is exactly the behavior encouraged by \u21132 norm regularization, and the elastic net is already known to yield less sparse (but more predictive) solutions. The k-support norm goes a step further in this direction, often yielding solutions that are even less sparse (but more predictive) compared to the elastic net.\nNevertheless, it is interesting to consider whether compressed sensing results, where \u21131 regularization is of course central, can be refined by using the k-support norm, which might be able to handle more correlation structure within the set of features."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal of Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Matrix Analysis", "author": ["R. Bhatia"], "venue": "Graduate Texts in Mathematics. Springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR", "author": ["H.D. Bondell", "B.J. Reich"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Signal recovery by proximal forwardbackward splitting", "author": ["P.L. Combettes", "V.R. Wajs"], "venue": "Multiscale Modeling and Simulation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Elastic-net regularization in learning theory", "author": ["C. De Mol", "E. De Vito", "L. Rosasco"], "venue": "Journal of Complexity,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Trace lasso: a trace norm regularization for correlated designs", "author": ["E. Grave", "G.R. Obozinski", "F. Bach"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "P\u00f3lya. Inequalities", "author": ["G.H. Hardy", "J.E. Littlewood"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1934}, {"title": "The Elements of Statistical Learning: Data Mining, Inference and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Kennard. Ridge regression: Biased estimation for nonorthogonal problems", "author": ["R.W.A.E. Hoerl"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1970}, {"title": "Group Lasso with overlap and graph Lasso", "author": ["L. Jacob", "G. Obozinski", "J.P. Vert"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A modified finite Newton method for fast solution of large scale linear SVMs", "author": ["S.S. Keerthi", "D. DeCoste"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Exploiting covariate similarity in sparse regression via the pairwise elastic net", "author": ["A. Lorbert", "D. Eis", "V. Kostina", "D.M. Blei", "P.J. Ramadge"], "venue": "In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "CORE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Smoothness, low-noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Approximation accuracy, gradient methods, and error bound for structured convex optimization", "author": ["P. Tseng"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Covering number bounds of certain regularized linear function classes", "author": ["T. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}], "referenceMentions": [{"referenceID": 10, "context": "In particular, the sample complexity 1 of learning a linear predictor with k non-zero entries by empirical risk minimization inside this class (an NP-hard optimization problem) scales as O(k log d), but relaxing to the constraint \u2016w\u20161 \u2264 k yields a sample complexity which scales as O(k log d), because the sample complexity of l1-regularized learning scales quadratically with the l1 norm [11, 19].", "startOffset": 389, "endOffset": 397}, {"referenceID": 18, "context": "In particular, the sample complexity 1 of learning a linear predictor with k non-zero entries by empirical risk minimization inside this class (an NP-hard optimization problem) scales as O(k log d), but relaxing to the constraint \u2016w\u20161 \u2264 k yields a sample complexity which scales as O(k log d), because the sample complexity of l1-regularized learning scales quadratically with the l1 norm [11, 19].", "startOffset": 389, "endOffset": 397}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Constraining (or equivalently, penalizing) both the l1 and l2 norms, as in (1), is known as the \u201celastic net\u201d [5, 20] and has indeed been advocated as a better alternative to the Lasso.", "startOffset": 110, "endOffset": 117}, {"referenceID": 19, "context": "Constraining (or equivalently, penalizing) both the l1 and l2 norms, as in (1), is known as the \u201celastic net\u201d [5, 20] and has indeed been advocated as a better alternative to the Lasso.", "startOffset": 110, "endOffset": 117}, {"referenceID": 9, "context": "To better understand the k-support norm, we show in Section 2 that it can also be described as the group lasso with overlaps norm [10] corresponding to all ( d k ) subsets of k features.", "startOffset": 130, "endOffset": 134}, {"referenceID": 19, "context": "These drawbacks have recently motivated the use of various other regularization methods, such as the elastic net [20], which penalizes the regression coefficients w with a combination of l1 and l2 norms:", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "The elastic net can be viewed as a trade-off between l1 regularization (the Lasso) and l2 regularization (Ridge regression [9]), depending on the relative values of \u03bb1 and \u03bb2.", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": "The pairwise elastic net (PEN), proposed by [13], has a penalty function that accounts for similarity among features: \u2016w\u2016 R = \u2016w\u20162 + \u2016w\u20161 \u2212 |w|\u22a4R|w| , where R \u2208 [0, 1]p\u00d7p is a matrix with Rjk measuring similarity between features Xj and Xk.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "The pairwise elastic net (PEN), proposed by [13], has a penalty function that accounts for similarity among features: \u2016w\u2016 R = \u2016w\u20162 + \u2016w\u20161 \u2212 |w|\u22a4R|w| , where R \u2208 [0, 1]p\u00d7p is a matrix with Rjk measuring similarity between features Xj and Xk.", "startOffset": 161, "endOffset": 167}, {"referenceID": 5, "context": "The trace Lasso [6] is a second method proposed to handle correlations within X , defined by \u2016w\u2016 X = \u2016Xdiag(w)\u2016\u2217 , where \u2016 \u00b7 \u2016\u2217 denotes the matrix trace-norm (the sum of the singular values) and promotes a low-rank solution.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Another existing penalty is OSCAR [3], given by \u2016w\u2016 c = \u2016w\u20161 + c \u2211", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "In fact, the k-support norm is equivalent to the norm used by the group lasso with overlaps [10], when the set of overlapping groups is chosen to be Gk (however, the group lasso has traditionally been used for applications with some specific known group structure, unlike the case considered here).", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "We follow the notation of [2] for ordered vectors: for any w \u2208 R, |w| is the vector", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "This is the l2-norm of the largest k entries in u, and is known as the 2-k symmetric gauge norm [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "For properties of such norms, see [2].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "We will use the inequality \u3008w, u\u3009 \u2264 \u3008w\u2193, u\u2193\u3009 [7].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "As typical in regularization-based methods, both \u03bb and k can be selected by cross validation [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 3, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 14, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 16, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 17, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 13, "context": "In particular, accelerated first-order methods, proposed by Nesterov [14, 15] require two levels of memory at each iteration and exhibit an optimal O ( 1 T 2 )", "startOffset": 69, "endOffset": 77}, {"referenceID": 14, "context": "In particular, accelerated first-order methods, proposed by Nesterov [14, 15] require two levels of memory at each iteration and exhibit an optimal O ( 1 T 2 )", "startOffset": 69, "endOffset": 77}, {"referenceID": 0, "context": "We can now apply a standard accelerated proximal method, such as FISTA [1], to (5), at each iteration using the gradient of the loss and performing a prox step using Algorithm 1.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "South African Heart Data This is a classification task which has been used in [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 11, "context": "20 Newsgroups This is a binary classification version of 20 newsgroups created in [12] which can be found in the LIBSVM data repository.", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "It is well understood that there is often a tradeoff between sparsity and good prediction, and that even if the population optimal predictor is sparse, a denser predictor often yields better predictive performance [20, 3, 10].", "startOffset": 214, "endOffset": 225}, {"referenceID": 2, "context": "It is well understood that there is often a tradeoff between sparsity and good prediction, and that even if the population optimal predictor is sparse, a denser predictor often yields better predictive performance [20, 3, 10].", "startOffset": 214, "endOffset": 225}, {"referenceID": 9, "context": "It is well understood that there is often a tradeoff between sparsity and good prediction, and that even if the population optimal predictor is sparse, a denser predictor often yields better predictive performance [20, 3, 10].", "startOffset": 214, "endOffset": 225}], "year": 2012, "abstractText": "We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an l2 penalty. We show that this new k-support norm provides a tighter relaxation than the elastic net and is thus a good replacement for the Lasso or the elastic net in sparse prediction problems. Through the study of the k-support norm, we also bound the looseness of the elastic net, thus shedding new light on it and providing justification for its use.", "creator": "LaTeX with hyperref package"}}}