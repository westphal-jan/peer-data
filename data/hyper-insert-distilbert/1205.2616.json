{"id": "1205.2616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Bisimulation-based Approximate Lifted Inference", "abstract": "this there has never been potentially a great deal of initial recent human interest leading in efficient methods designing for performing lifted inference ; however, unfortunately most work of this numerical work assumes predictions that either the first - order model is widely given as human input stimuli to assess the system. here, we describe precise lifted choice inference query algorithms that fundamentally determine symmetries and automatically calculate lift the probabilistic model to speedup valid inference. in particular, basically we describe approximate lifted query inference techniques that allow the regression user to trade models off simplified inference accuracy for computational efficiency by deliberately using a handful of less tunable trace parameters, seemingly while keeping the error intervals bounded. our heavier algorithms typically are closely related to interpreting the graph - theoretic model concept of bisimulation. we report experiments on both synthetic and geometric real data models to show directly that in the presence also of algebraic symmetries, run - start times predicted for based inference can be improved significantly, combine with additional approximate lifted inference simulations providing orders of magnitude speedup over ground inference.", "histories": [["v1", "Wed, 9 May 2012 18:27:56 GMT  (250kb)", "http://arxiv.org/abs/1205.2616v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prithviraj sen", "amol deshpande", "lise getoor"], "accepted": false, "id": "1205.2616"}, "pdf": {"name": "1205.2616.pdf", "metadata": {"source": "CRF", "title": "Bisimulation-based Approximate Lifted Inference", "authors": ["Prithviraj Sen", "Amol Deshpande", "Lise Getoor"], "emails": ["sen@cs.umd.edu", "amol@cs.umd.edu", "getoor@cs.umd.edu"], "sections": [{"heading": null, "text": "There has been a great deal of recent interest in methods for performing lifted inference; however, most of this work assumes that the first-order model is given as input to the system. Here, we describe lifted inference algorithms that determine symmetries and automatically lift the probabilistic model to speedup inference. In particular, we describe approximate lifted inference techniques that allow the user to trade off inference accuracy for computational efficiency by using a handful of tunable parameters, while keeping the error bounded. Our algorithms are closely related to the graph-theoretic concept of bisimulation. We report experiments on both synthetic and real data to show that in the presence of symmetries, run-times for inference can be improved significantly, with approximate lifted inference providing orders of magnitude speedup over ground inference."}, {"heading": "1 Introduction", "text": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input. In this work, we study the alternate problem of identifying the symmetry present in an underlying probabilistic model, and show how this can be exploited to provide new lifted inference algorithms.\nOur work builds on recent results on efficient query evaluation in probabilistic databases from the database community. A query over a probabilistic database results in a very large graphical model which has many repeated factors. In prior work [18], we showed how, using the symmetry present in the probabilistic model and methods closely\nrelated to the graph-theoretic concept of bisimulation, it is possible to compile a compressed version of the inference problem. The compressed data-structure, called an rv-elim graph, can then be used to perform faster inference. In this paper, we show how the above techniques are generally applicable to arbitrary graphical models, and, more importantly, develop approximate lifted inference techniques that allow the user to trade off accuracy of inference for computational efficiency. We show how our approximate lifted inference techniques can compress the rv-elim graph well beyond the compression achieved by exact lifted inference, producing more impressive speedups while keeping the error bounded using certain tunable parameters.\nHere is a summary of our contributions and results:\n\u2022 We review the results from [18] and show how they are applicable to general probabilistic graphical models.\n\u2022 Using techniques based on approximate bisimulation, we extend these methods and introduce a tunable parameter to move from approximate inference with high speedups to exact inference with perfect accuracy.\n\u2022 We introduce a second approximation method to bin factors (or clique potentials) that are within a userspecified \u03b5 distance of each other into common partitions. Using these partitions, it is possible to compress the rv-elim graph and perform even faster inference.\n\u2022 We also show how to integrate our techniques with existing bounded complexity inference techniques (e.g., mini-buckets [5]) \u2013 that allows us to extend the use of our techniques to domains with unbounded treewidth.\n\u2022 We discuss how to integrate all of the above techniques into one single inference engine that allows combinations ranging from exact lifted inference to approximate inference based on approximate bisimulation and factor binning with the use of mini-buckets.\n\u2022 We experiment with synthetic and real-world data, and demonstrate how our techniques can achieve significant speedups of upto two orders of magnitude over ground inference and exact lifted inference with bounded error.\ns1 s3s2t1\ni3i2i1\n\u2227\u2227\u2227\nfs1 ft1 fs2 fs3\nfi1 fi2 fi3\nIn the next section, we review our earlier work on exact lifted inference [18], in Section 3 we present our techniques for approximate lifted inference, in Section 4 we evaluate our approaches on synthetic and real-world data, in Section 5 we review related work and we conclude with a few pointers for future work in Section 6."}, {"heading": "2 Background: Exact Lifted Inference with the RV-Elim Graph", "text": "This section reviews material from [18]. We begin with some notation. Let X denote a random variable that can be assigned a value from a pre-defined domain denoted by dom(X). Let f (X) denote a factor or clique potential that takes as arguments a set of random variables X = {X1, . . .Xn}. f (X) denotes a mapping f : dom(X1)\u00d7 . . .\u00d7 dom(Xn)\u2192 \u211c\u22650. Given a set of such factors, F = { f1, f2, . . . fm}, we can define a joint probability distribution over a set of random variables X by multiplying all the factors Pr(X ) = 1Z \u220f f\u2208F f (X f ) such that X f \u2286X denotes the arguments of f \u2208 F and Z denotes the partition function. Given such a joint probability distribution and a random variable X \u2208X , let \u00b5(X) denote its marginal probability distribution such that \u00b5(X) = \u2211X \\X Pr(X ).\nIn first-order probabilistic models, many factors come from grounding out first-order rules. The factors obtained from such rules map the same input to the same outputs and constitute symmetry in the model. Consider the friends and smokers domain where we want to infer the probability of a person being a smoker. Then a rule such as Smoker(P1)\u2227 Friend(P1,P2)\u21d2 Smoker(P2) would ground out to provide (n 2 ) factors with identical input-output mappings, assuming all pairs P1,P2 are friends. The notion of shared factors precisely captures this symmetry.\nWe refer to two factors f and f \u2032 as being shared, denoted f \u223c= f \u2032, if they contain the same input-output mappings, irrespective of whether they take the same random variables as arguments. More precisely, let X = {X1,X2, . . .Xn} and X\u2032 = {X \u20321,X \u20322 . . .X \u2032n} denote the arguments of f and f \u2032 respectively. Then, f (X) \u223c= f \u2032(X\u2032) iff dom(Xi) = dom(X \u2032i ),\u2200i = 1, . . .n, and f (x) = f \u2032(x),\u2200x \u2208 dom(X1)\u00d7 . . .dom(Xn).\nConsider the small example shown in Figure 1. All random variables are boolean valued. The priors on s1, s2,\n{[1,2],[2],2}\n{[1,2],[2],2}\n{[1,2],[2],2}\n{[1,2],[2],2}\n{[1,2],[2],2}\n(a) (b)\nFigure 2: (a) RV-Elim graph for the running example (vertices partitioned into 8 blocks, shading indicates partitioning), (b) corresponding compressed rv-elim graph.\ns3 and t1, denoted by fs1 , fs2 , fs3 and ft1 , respectively, are such that s1 and s2 are true with probability 0.8, s3 is true with probability 0.6 and t1 is true with probability 0.5. The three random variables i j,\u2200 j \u2208 {1,2,3} are each true when their corresponding parents s j and t1 are both true and this is enforced by the three factors fi j(i j,s j, t1) which return 1 iff i j \u21d4 s j \u2227 t1 and return 0 otherwise. Of the four priors, fs1(s1) and fs2(s2) represent a pair of shared factors since they contain the same input-output mappings ( fs1(true) = fs2(true) = 0.8); similarly, fi1 , fi2 and fi3 are also shared since they are all and factors.\u2217\nGiven a set of factors F , a set of random variables X whose marginal probabilities we are interested in computing, and an elimination order O which contains all the random variables to be summed over from F (we discuss how to construct elimination orders subsequently), it is straightforward to construct an rv-elim graph G = (V,E,LV ,LE) which is a directed acyclic graph (DAG) with vertex labels LV and edge labels LE such that:\n\u2022 If v is a root then it represents a factor f from F and its label LV (v) is such that \u2200 f \u2032 \u2208F , f \u2032 \u223c= f \u21d4LV (v\u2032) = LV (v) where v\u2032 denotes the vertex representing f \u2032.\n\u2022 If v is an internal vertex, then it represents an intermediate factor created during inference, denoting a variable elimination (summing over) operation formed by multiplying the factors represented by its parent vertices. The edge labels on edges with v as head denote the order in which the parents were multiplied and the label on v denotes how the arguments of its parents overlap.\nIn Figure 2(a), we show the rv-elim graph generated for our running example using the elimination order O = {t1,s3,s2,s1} (variables on the right are eliminated first). The vertex labels are shown next to each vertex. Notice that fs1 and fs2 have been assigned the same label \u201ca\u201d but fs3 has\n\u2217This is an example of a graphical model that may be constructed during query evaluation over probabilistic databases [18].\nbeen assigned a different label \u201cb\u201d; similarly, fi1 , fi2 and fi3 have been assigned the same label \u201cc\u201d. Each internal vertex corresponds to an elimination operation. For instance, ms1 denotes the intermediate factor produced by summing over s1: ms1(i1, t1) = \u2211s1 fs1(s1) fi1(i1,s1, t1)where fs1 is the first multiplicand and fi1 is the second based on the edge labels. The labels on internal vertices in Figure 2(a) denote how arguments across its parents overlap. We illustrate how the labels for the internal vertices were created by showing the construction for LV (ms1):\n\u2022 assign each random variable an id: s1 = 1, i1 = 2, t1 = 3 \u2022 begin constructing the label by going through each par-\nent\u2019s arguments list and forming a tuple composed of the arguments\u2019 ids assigned in the previous step: since fs1 is the first multiplicand and fi1 the second, we form our label by concatenating \u201c[1]\u201d with \u201c[2,1,3]\u201d,\n\u2022 add the id of the random variable being summed to the end of the string: append the string \u201c1\u201d to our label.\nThus, the complete label for ms1 is \u201c{[1], [2,1,3], 1}\u201d (Figure 2(a)). For this example, we are interested in computing the marginals for i1, i2 and i3 and these marginals are depicted by the leaf vertices.\nThe main goal of lifted inference is to avoid computing shared factors repeatedly; instead each shared factor should be computed once, and reused whenever required. For instance, in Figure 2(a), we observe thatms1 \u223c=ms2 since their parents form pairs of shared factors fs1 \u223c= fs2 and fi1 \u223c= fi2 . This is where the rv-elim graph is useful, it helps us determine the intermediate shared factors generated during inference before we actually compute them. Property 2.1. Vertices v1,v2 in rv-elim graph G represent shared factors, denoted v1 \u223c= v2, (i.e., fv1 \u223c= fv2 where fv denotes the factor represented by v), iff:\n\u2022 \u2200u1 i\u2192 v1,\u2203u2 i\u2192 v2 s.t. fu1 \u223c= fu2 and vice versa (the parents are pairwise shared, in order).\n\u2022 LV (v1) = LV (v2) (arguments overlap info. matches).\nIn [18] we observed that by adapting the graph-theoretic notion of bisimulation [11], one can determine the equivalence class partitioning of the vertices. Applying the bisimulation algorithm to an rv-elim graph partitions the set of vertices into blocks B j such that \u2200v1,v2 \u2208 B j, fv1 \u223c= fv2 . Once we have such a partition, it is easy to construct a compressed version of the rv-elim graph where each block B j is represented by a vertex and we introduce an edge B j i\u2192 B j\u2032 if \u2203v1 \u2208 B j,\u2203v2 \u2208 B j\u2032 s.t. v1\ni\u2192 v2. Figure 2(b) shows the compressed rv-elim graph constructed from Figure 2(a) where we denote the blocks in square braces next to each vertex; for instance, both ms1 and ms2 have been collapsed to the same vertex D. The compressed graph can then be used to perform inference efficiently.\nOne caveat about the above approach is that since factor\nAlgorithm 1: Exact Bisimulation [18] d(v) = {\n0, if v is a root 1+max{d(v\u2032)|v\u2032\u2192 v \u2208 E} /* compute depths */\n\u03c1 \u2190max{d(v)|v \u2208V} B0,l = {v|v is root\u2227LV (v) = l} /* compute initial partition */ C = {B0,l} Bi = {v|d(v) = i},\u2200i = 1 . . .\u03c1 for i = 1 . . .\u03c1 do\nforeach v \u2208 Bi do /* construct keys to partition on */ order parents by block-ids construct label LV (v) construct key kv with LV (v) and parents\u2019 blocks-ids\nadd Bi,k = {v \u2208 Bi|kv = k} toC returnC\nmultiplication is a commutative operation, the edge and the internal vertex labels, both of which depend on the order in which the factors are multiplied, can be dynamically altered by choosing a different order. In [18], we proposed ordering the parents of a node using their assigned blockids before assigning it to a block. This may lead to more symmetry and compression. Algorithm 1 depicts the complete bisimulation algorithm.\nFinally to choose the initial elimination order to generate the rv-elim graph, we run a bisimulation on the probabilistic model itself (vertices denote random variables, edges denote dependencies) to compress it. We then run a modified min-size heuristic [13] on the compressed graph, and replace the vertices with the corresponding sets of random variables to get an elimination order for inference."}, {"heading": "3 Approximate Lifted Inference", "text": "While the above approach to performing exact lifted inference can provide significant speedups when the probabilistic model contains moderate to large amounts of symmetry, in many cases we can do much better if we are willing to accept approximations in the marginal probability distributions computed. The main idea here is to explore looser versions of Property 2.1 so that we can partition the vertices of the rv-elim graph into bigger blocks and thus arrive at a smaller compressed rv-elim graph. In what follows, we describe two separate and orthogonal generalizations of Property 2.1 that can be used to implement approximate lifted inference. After that, we discuss how to combine our techniques with bounded complexity inference algorithms and finally, we discuss how to combine all of our proposed ideas together into one approximate lifted inference engine."}, {"heading": "3.1 Lifted Inference with Approximate Bisimulation", "text": "To introduce our first technique we require some notation. Given a vertex, edge labeled graph G = (V,E,LV ,LE) such as an rv-elim graph, let v0, . . .vn denote an n-length vertex path such that \u2200i= 0, . . .n : vi \u2208V and \u2200i= 0, . . . ,n\u2212\n1 : \u2203 j s.t. vi j\u2192 vi+1 \u2208 E. Further, we say that label path or simply, path, l0(l\u20320)l1(l \u2032 1) . . . ln(l \u2032 n)ln+1 matches vertex path v0, . . .vn+1 (and vice versa) if \u2200i= 0, . . . ,n+1 :LV (vi) = li and \u2200i = 0, . . .n : LE(vi\u2192 vi+1) = l\u2032i .\nWe will now revisit Property 2.1 and try to assign it a pathbased interpretation. Using a simple induction (and the fact that edges with the same head have distinct edge labels) it is possible to show that two vertices v1 and v2 in an rv-elim graph are bisimilar iff their incoming set of paths from the roots are identical. For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp., and the matching vertex paths for ms2 are fs2 ,ms2 and fi2 ,ms2 , resp. Notice that this path-based interpretation of Property 2.1 shows that it is a fairly stringent criteria (albeit necessary for exact inference). For instance, consider a case when two vertices deep in the rv-elim graph have large sets of long incoming paths and both sets are almost identical except for one incoming path to the second vertex which has that one label that does not allow it to match any incoming path to the first vertex; based on Property 2.1 these two vertices would be placed in different blocks of the final partition and the compressed rv-elim graph would be correspondingly bloated. This sort of behaviour is, in fact, on display in our running example where \u00b5i2 \u00b5i3 simply because, of the three incoming paths to \u00b5i3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u00b5i3 ) doesn\u2019t match any of \u00b5i2 \u2019s incoming paths.\nInstead of comparing sets of all incoming paths to vertices, we propose to relax Property 2.1 by comparing sets of only k-length (and less than k-length) incoming paths, where k is a tunable parameter we refer to as the path-length. Our compression algorithm permits high compression when the path-length is set to a low value and approaches exact bisimulation when we increase it. Figure 3(a) shows the result of partitioning vertices in our example rv-elim graph with k set to 0 where we simply partition vertices based on\ntheir labels. Figure 3(b) is more interesting where we have set k to 1 and so, compare incoming paths of length 1. Note how, in this case, ms3 has been differentiated from ms1 and ms2 since ms3 has an incoming path \u201cb(1){[1], [2,1,3],1}\u2032\u2032 (matching fs3 ,ms3 ) of length 1 which doesn\u2019t match any incoming 1-length path of ms1 or ms2 . In contrast, ms1 , ms2 and ms3 were all placed into the same block in Figure 3(a). Also notice that, in Figure 3(b), \u00b5i1 , \u00b5i2 and \u00b5i3 are still partitioned into the same block (leaf vertices tiled with green bricks) and this is because the only path that differentiates \u00b5i3 from \u00b5i1 and \u00b5i2 is a path of length 2 (vertex path fs3 ,ms3 ,\u00b5i3 ) which is beyond the scope of the current path-length setting of 1. This changes however, when we set path-length to 2 and obtain the results of exact bisimulation shown earlier in Figure 2(a).\nThe partitioning based on comparing incoming k-length paths can be obtained by computing k-bisimilarity [11] (for which algorithms are available) since these two properties are equivalent (this can be proved by induction). We formalize the k-bisimilarity property as follows:\nProperty 3.1. Given an rv-elim graph G= (V,E,LV ,LE), \u223c=k is defined inductively. For vertices v1,v2 \u2208V,\n\u2022 v1 \u223c=0 v2 iff LV (v1) = LV (v2).\n\u2022 v1 \u223c=k v2 (k > 0) iff LV (v1) = LV (v2) and \u2200u1 i\u2192\nv1,\u2203u2 i\u2192 v2 s.t. u1 \u223c=k\u22121 u2 and vice versa.\nThe algorithm for obtaining the partition based on \u223c=k, Algorithm 2, begins by computing the depth of each vertex d(v) and constructing an initial partition based on labels of the roots and the depths of internal vertices. Throughout Algorithm 2, we maintain two partitions, X and C. In the ith iteration, X maintains \u223c=i\u22121 and is used to update C where we construct \u223c=i. Note that the inner two loops can be performed in O(|E| logD+ |V |) time (not counting the time spent to construct the vertex labels), where D is the maximum in-degree in the rv-elim graph. Thus, Algorithm 2 runs in O(k(|E| logD+ |V |)) time (in contrast to Algorithm 1 which runs in O(|E| logD+ |V |) time). Note that\nconstructing the compressed rv-elim graph corresponding to\u223c=k is a bit more complicated now since we are no longer guaranteed that, if two internal vertices fall into the same block of the partition, then the parents will also have been placed into the same block (which holds for Property 2.1). Figure 3(c) (compressed graph obtained at k=1) illustrates this issue where all \u00b5\u2019s have been merged into one block but their 1st parents are not, thus G has two 1st parents D and F which is problematic if we want to use the compressed graph to run inference. Here, we simply get rid of the edge that corresponds to the smaller sized block (the dotted edge F \u2192 G in Figure 3(c) since F represents a block of size 1 versusDwhose block size is 2) to maximize the number of correct marginal probability computations."}, {"heading": "3.2 Lifted Inference with Factor Binning", "text": "We now introduce another way of implementing approximated lifted inference using an orthogonal generalization of Property 2.1. We begin by associating with Property 2.1 a distance-based interpretation. Recall that, Property 2.1 bins two factors into the same block of the partition when we can guarantee that their input-output mappings are exactly the same without actually computing them. Stated differently, given any user-defined distance measure that can measure the \u201cdistance\u201d between two factors, Property 2.1 deems that these factors belong to the same block only if the distance between them is zero. Note that the converse is not true. That is, it is possible for two internal vertices in the rv-elim graph to actually represent factors that comprise of identical input-output mappings but because their parents do not belong to the same blocks or because the parents\u2019 arguments don\u2019t overlap in the same fashion, Property 2.1 cannot bin these into the same block of the partition. We illustrate this with the following example:\n\u2211 Y  X Y f1 t t 0.8 t f 0.2 f t 0.4 f f 0.6 \u00d7 Y f2 t 0.5 f 0.5  = X mY t 0.5 f 0.5\n\u2211 Y \u2032  X \u2032 Y \u2032 f \u20321 t t 0.2 t f 0.8 f t 0.6 f f 0.4 \u00d7 Y \u2032 f \u20322 t 0.5 f 0.5  = X \u2032 mY \u2032 t 0.5 f 0.5\nwhere t and f denote true and false resp. Notice how factors f1 and f \u20321 have different input-output mappings ( f1(t,t) = 0.8 6= 0.2 = f \u20321(t,t)) and hence cannot be binned into the same block which means that it is not possible to determine that the resulting factors mY and mY \u2032 comprise of the same input-output mappings solely using Property 2.1. This, in turn, means that any intermediate factors derived from these two factors during the inference process will always be binned separately, thus leading to a\nAlgorithm 2: Approximate Bisimulation(k) d(v) = {\n0, if v is a root 1+max{d(v\u2032)|v\u2032\u2192 v \u2208 E}\n\u03c1 \u2190max{d(v)|v \u2208V} B0,l = {v|d(v) = 0\u2227LV (v) = l} Bi = {v|d(v) = i}\u2200i = 1 . . .\u03c1 C\u2190{B0,l}\u2200l \u222a{Bi} \u03c1 i=1 X \u2190C for j = 1 . . .k do\nfor i = 1 . . .\u03c1 do foreach B \u2208C at depth i do\norder parents by block-ids in X construct labels LV (v)\u2200v \u2208 B construct key kv\u2200v \u2208 B with LV (v), parent blocks-ids in X partition B based on keys kv replace B inC with new blocks\nX \u2190C returnC\nbloated compressed rv-elim graph.\nSuch symmetries can not be captured without actually looking into the factors and computing the distance between them (any distance measure such as KL-divergence or root mean squared distance would do). For this purpose, we ask the user for a separate parameter \u03b5 , that specifies an upper bound on the distance between two factors for them to be considered shared. Note that, unlike the previous algorithm, we can not compute distance between two intermediate factors without computing the factors.\nTo determine such a distance-based partitioning of the factors, we will need to solve the factor binning problem (FB):\nGiven: set of factors F = { f1, . . . fn} threshold \u03b5, distance function dist(\u00b7,\u00b7)\nReturn: argminF\u2286F |F| such that \u2200 fi \u2208F \\F \u2203 f \u2208 F s.t. dist( fi, f )\u2264 \u03b5\nWe will shortly show that the factor binning problem is equivalent to the dominating set problem (DS):\nGiven: graph G with vertex set V and edge set E denote by Nv neighborhood of vertex v\nReturn: argminD\u2286V|D| such that \u2200vi \u2208 V\\D \u2203v \u2208 D s.t. v \u2208 Nvi\nTheorem 3.2. FB is equivalent to DS.\nProof. The proof is in two parts, we first show that any instance of FB can be reduced to DS and vice versa. To show the first part, we specify the reduction to DS. Given an instance of FB, define the corresponding DS by setting:\nDSFB : V = F ,N fi = { fi}\u222a{ f |dist( fi, f )\u2264 \u03b5}\nAlgorithm 3: Factor Binning(\u03b5) d(v) = {\n0, if v is a root 1+max{d(v\u2032)|v\u2032\u2192 v \u2208 E}\n\u03c1 \u2190max{d(v)|v \u2208V} B0,l = {v|v is a root \u2227LV (v) = l}\nFB instantiate one factor per block B0,l FB Bhs0 \u2190 compute hitting set and construct new set of blocks\nby merging {B0,l} C = Bhs0 Bi = {v|d(v) = i},\u2200i = 1 . . .\u03c1 for i = 1 . . .\u03c1 do\nforeach v \u2208 Bi do order parents by block-ids construct label LV (v) construct key kv with LV (v) and parents\u2019 blocks-ids\nBi,k = {v \u2208 Bi|kv = k} FB instantiate one factor per new block Bi,k FB Bhsi \u2190 compute hitting set and construct new set of\nblocks by merging {Bi,k} C\u2190C\u222aBhsi\nreturnC\nNote that any solution toDSFB is a solution to FB.We show this by contradiction. Suppose solution D to DSFB is not a solution to FB, i.o.w., \u2203 fi \u2208F \\D s.t. dist( fi, f ) > \u03b5, \u2200 f \u2208 D. This implies N fi \u2229D = /0 which means that D is not a solution to DSFB and thus we have a contradiction. Similarly, any solution to FB is a solution to DSFB. Again, assume that solution F to FB is not a solution to DSFB. Thus, \u2203 fi \u2208F \\F s.t. N fi \u2229F = /0. This implies dist( fi, f ) > \u03b5, \u2200 f \u2208 F which means F is not a solution to FB and we have a contradiction. Given that solution spaces of FB and DSFB are same, and that the objective functions are also same, we have shown that FB can be solved by solving DSFB.\nThe reduction in the other direction is also easy. Given an instance of DS, define the corresponding FBDS by setting:\nFBDS : F = V,\u03b5 = 0 dist(vi,v j) = {\n0 if (vi,v j) \u2208 E 1 otherwise\nIt\u2019s easy to show that DS, FBDS share the same soln. space.\nDS is NP-Complete [7]. Further, Feige [6] showed that DS is not approximable to a factor of (1\u2212o(1))ln(|V|) unless NP has \u201cslightly super-polynomial time\u201d algorithms (or NP \u2282 DTIME(nlog(log(|V|)))). One way to solve DS is to utilize the fact that it is a special case of set cover and use the obvious greedy heuristic (described below) for set cover. This gives us an ln(|V|)-approximation algorithm [21]. Thus, for our experiments we use the same greedy approach to solve FB. FB is also equivalent to the \u03c1dominating set problem [2], which, in turn, is the converse of the classic k-center problem [12] where we are given a\ngraph from which we need to choose a subset of k vertices so that their distance from the other vertices is minimized. Note that, when the distance function satisfies special properties, better algorithms may be available. For instance, for euclidean spaces, near-optimal factor binning is possible [10], especially when the factor sizes are not large.\nThe algorithm to obtain the greedy solution for FB is to first construct each subset N fi (as defined above) and repeatedly pick fi corresponding to the current largest N fi to include into our solution. Every time we pick fi, we update all N f j \u2019s by deleting from them all factors that are within \u03b5 distance of fi. Another question we need to consider is whether to bin factors based on distance once and then run approximate lifted inference or whether to bin the intermediate factors based on distance also. For our experiments, we also binned the intermediate factors since this allows us to compress the rv-elim graph more agrressively. Algorithm 3 shows the complete algorithm to run approximate lifted inference using FB. Algorithm 3 is essentially Algorithm 1 with extra lines for FB computations (marked FB)."}, {"heading": "3.3 Bounded Complexity Lifted Inference", "text": "The approximation techniques we have introduced so far do not alleviate the worst-case complexity of the inference procedure. In other words, these techniques would not help if the ground inference procedure is associated with high treewidth (common with structured probabilistic graphical models). Next we show how to incorporate the mini-bucket scheme [5], a bounded complexity approximate (ground) inference algorithm, with our ideas. This allows us to keep a tight control over the complexity of inference incurred.\nThe mini-buckets scheme is a modification of the variable elimination algorithm [23] where at each step instead of eliminating a random variable by multiplying all factors it appears as argument in, one devises a set of mini-buckets each containing a (disjoint) subset of factors that contains that variable as argument and then eliminates the variable separately from each mini-bucket. More precisely, given a set of factors, one first constructs a canonical partition such that all subsumed factors are placed into the same bucket of the partition. A factor f is said to be subsumed by factor f \u2032 if any argument of f is also an argument of f \u2032. After constructing the canonical partition, the user has two choices:\n\u2022 construct mini-buckets by restricting the total number of arguments i (a user-defined parameter) in each minibucket. Since inference complexity is directly affected by the size of the largest factor encountered, this is one way to control the amount of computation incurred.\n\u2022 specify how many buckets m of the canonical partition to merge to form a mini-bucket. Again, this (indirectly) controls the size of the largest factor generated and keeps the complexity bounded.\nDechter and Rish [5] show how such a modification of the variable elimination algorithm provides an upper bound over the numbers produced in the resulting factors.\nIt is easy to combine our approaches with the mini-bucket scheme. Instead of building the rv-elim graph by introducing internal vertices corresponding to intermediate factors produced by multiplying all factors involving a certain random variable as argument, we simply introduce vertices corresponding to factors produced by the mini-bucket scheme. Since our approaches work on any rv-elim graph, this requires no change to the approaches presented earlier, while keeping the complexity of inference bounded."}, {"heading": "3.4 Unified Lifted Inference Engine", "text": "By interleaving the various steps, it is possible to combine all the ideas we have presented in this section into one unified approximate lifted inference engine. Our combined inference engine takes a set of eight parameters which define the combinations of techniques we would like to invoke (see Table 1). The experiments presented in the next section use this generic inference engine."}, {"heading": "4 Experimental Evaluation", "text": "We conducted experiments on synthetic and real data to determine how lifted inference with approximate bisimulation and factor binning perform on their own. We also report experiments with our unified lifted inference engine where we used both approaches in tandem. Each number we report is an average over 3 runs, our implementation is in JAVA and our experiments were performed on a machine with a 3GHz Xeon processor and 3GB RAM. We compare our results with two baseline algorithms: A ground inference procedure which is basically variable elimination [23] modified so that we obtain all marginals in a single pass, and the exact lifted inference procedure reviewed in Section 2. We report two metrics for each experiment: run times incurred by the various algorithms in seconds (Time) and error measured by computing the average number of marginal probabilities which were not within 10\u22128 of their correct values (Avg. #Probs. Incorrect)."}, {"heading": "4.1 Synthetic Bayesian Network Generator", "text": "We set up a synthetic Bayesian network (BN) generator to test various aspects of our algorithms. The generator produces BNs where the random variables are organized in layers and random variables from the ith layer randomly choose parents from the i\u22121th layer. For our experiments, we generated BNs with 3 layers: 1st layer contained 1000 random variables, 2nd 500 and 3rd 250. We introduced priors randomly for each variable in the first layer, every 25th prior was identical. The random variables in the last layer are our query variables for which we computed marginal\nprobabilities. All random variables had domain of size 30. To generate factors defining the dependency between random variables from the ith and i\u2212 1th layers, for each variable in the ith, we randomly chose 2 parents from the previous layer. Two children can choose the same parents, so we generated non-tree structured BNs. All factors with children from the ith layer are identical. This closely follows most structured probabilistic graphical models we have come across, where the priors usually closely resemble each other but may not be identical; whereas the factors defining dependencies between various random variables come from generic rules and are thus identical. We used a parameter to control how many times a random variable can be picked as a parent. This helps vary the complexity of the inference problem. We also used a parameter to add random noise after the factors are generated. We tried other parameter settings as well and the trends were as expected. For instance, increasing domain size increases the speedups obtained since with larger domains, we increase the time spent summing over random variables and multiplying factors while running ground inference \u2013 our lifted inference procedures are designed to save on this assuming the symmetry among factors is kept constant. Similarly, increasing the number of random variables with constant symmetry also increases speedups obtained."}, {"heading": "4.2 Lifted Inference with Approximate Bisimulation", "text": "Our first set of experiments tests our algorithm for lifted inference with approximate bisimulation. The results are reported in Figure 4 (a) and Figure 4 (d). The plots show that as we increase path-length (x-axis in these plots) the time for inference (Figure 4 (a)) slowly increases but error decreases (Figure 4 (d)). The solid line with triangles depict the results of running lifted inference with approximate bisimulation without mini-buckets, and with pathlength set to 3 we see that the error stands around 18%; the inference procedure took about 3 seconds to run, which is almost a 3 times speedup over exact lifted inference (which took 8.2 seconds) and almost a 9 times speedup over ground inference (which took 25.95 sec). All the other\nlines in the plots correspond to lifted inference with approximate bisimulation run with various mini-bucket schemes. Among these, the mini-bucket scheme with mini-buckets restricted by argument count at i = 3 seems to be a promising setting (dotted line with triangles) since it runs faster than lifted inference with approximate bisimulation but does not incur significantly higher error. Another interesting thing that shows up in these plots is that with minibuckets with i= 4 or m= 2 at path-length set to 3, the time taken to run inference goes up noticeably. This shows that at very low path-lengths, using mini-buckets could actually lead to loss of symmetry in the rv-elim graph."}, {"heading": "4.3 Lifted Inference with Factor Binning", "text": "Our second set of experiments tests our factor binning approach. The results are shown in Figure 4 (b) and Figure 4 (e). For these experiments, we used root mean squared distance to compare two factors. More precisely, given two factors f1 and f2 with a common joint domain D, dist( f1, f2) = \u221a 1 |D| \u2211x\u2208D( f1(x)\u2212 f2(x))2. The plots show that as we increase \u03b5 (on the x-axis) the times for inference go down (Figure 4 (b)), and the error goes up (Figure 4 (e)). On these experiments, ground inference took about 33 seconds and exact lifted inference took 25.24 seconds which means factor binning without mini-buckets (solid line with triangles) achieves a speedup of about 3.5 times over exact lifted inference and a speedup of almost 5 times over ground inference. Among the various mini-bucket\nschemes, once again i= 3 (dotted line with triangles) seems to be the best setting which gives small but noticeable reductions in run-times at almost no cost to accuracy. Notice that mini-buckets with small settings of either m or i tends to perform very poorly neither giving good accuracies nor providing good run-times and this is likely due to the sheer number of factors with which we are dealing. At such small settings, the mini-bucket scheme produces a lot of factors and computing the hitting set (which has a quadratic time complexity) becomes too expensive."}, {"heading": "4.4 Unified Lifted Inference Engine", "text": "In our last set of experiments, we used both approximate bisimulation (path-length=3) and factor binning (\u03b5 = 0.01) with mini-buckets (restricted by argument count i = 3). Here we report run-times for probabilistic models with varying number of random variables. The results are reported in Figure 4 (c) and Figure 4 (f). As should be clear from Figure 4 (c), with increasing size of the probabilistic model all three inference procedures, ground inference, exact lifted inference and approximate lifted inference, show an increase in run-time but there is an order of magnitude difference in times between ground inference and exact lifted inference (which partitions identical factors together) and another order of magnitude speedup over exact lifted inference for approximate lifted inference (which also bins nearly identical factors together) while keeping the accuracy within bounds. Thus approximate lifted inference is more than two orders of magnitude faster than ground in-\nference. The accuracies for approximate lifted inference for these experiments varied between 65-95%. For these complex networks, we could not run ground inference on models with more than 256 random variables due to memory limitations. Figure 4 (f) makes it clear how the runtime between exact lifted inference and approximate lifted inference varies. Here we set all priors in our probabilistic model similar to each other but varied the probability of two factors being identical to each other. The plot shows that as this probability increases, exact lifted inference captures the symmetry and does better, whereas approximate lifted inference keeps run-times low throughout."}, {"heading": "4.5 Experiments on Real-World Data", "text": "We experimented with a number of real world datasets. We first report results on the Cora [14] and CiteSeer [9] datasets. The Cora dataset contains 2708 machine learning papers with 5429 citations; each paper is labeled from one of seven topics. The CiteSeer dataset consists of 3316 publications with 4591 citations; each paper is labeled with one of 6 topics. The task is to predict the correct topic label of the papers. We divided each dataset into three roughly equal splits and performed three-fold cross valiation. For each experiment, we train on two splits and test on the third, randomly choosing 10% of the papers\u2019 class labels to be our query nodes. Each number we report is an average across all splits. Note that, for these experiments, using the citations in the datasets we produce Markov networks with unbounded treewidth and then perform collective classification [19], so we compare against ground inference with mini-buckets restricted to 6 arguments. Also, while testing on the third split, we include as evidence topic labels of the papers belonging to the training set linked to from the test set. We tried various parameter settings with our approximate lifted inference engine and report the best results. As\nTable 5 (a) shows, we obtained a 2.7 times speedup for Cora and 1.55 times speedup for CiteSeer with our approximate lifted inference engine over ground inference. The loss in accuracy was 4.8% for Cora and 1.9% for CiteSeer. These results were obtained with path length = 2, \u03b5 = 0.01 and using mini-buckets restricted to 6 arguments. We also show how much time was spent by each inference scheme to multiply factors and sum over random variables (arithmetic operations or \u201cArith. Ops.\u201d in Table 5 (a)) and the remaining operations (or \u201cRem. Ops.\u201d in Table 5 (a)). As should be clear from Table 5 (a), the various bisimulation\nalgorithms and hitting set computations do not really add much overhead on these datasets; we spend about 0.2 seconds, for Cora, and 0.9 seconds, for CiteSeer, more than ground inference to implement lifted inference.\nWe also experimented with the Cora dataset for entity resolution (Cora-ER) [1]. For this experiment, we used a Markov logic network with 46 distinct rules. Unfortunately, we could not get any noticeable speedup for this dataset. This dataset consists solely of random variables with domain size 2 (match/non-match). As a result, all the factors produced are extremely small in size (size of a factor is determined by the number of rows in it) which implies that the time spent performing arithmetic operations (multiplying factors and eliminating random variables) is not the bottleneck during inference. The techniques proposed in this paper are mainly directed towards reducing the time spent to perform arithmetic operations. However, we do present the precision-recall curve we obtained for Cora-ER (Figure 5 (b), increasing argument count restriction for the mini-buckets scheme reduces precision but increases recall) and we also counted the number of intermediate factors computed by ground and lifted inference for various samplings of the dataset consisting of 50-250 bibliographic citations to be deduplicated. Figure 5 (c) shows that lifted inference produces far fewer intermediate factors during inference than ground inference; recall that ground inference produces an intermediate factor everytime a random variable is eliminated but lifted inference saves on this computation by computing one factor for each block in the final partitioning. This, in turn, indicates that the dataset possesses symmetry which could lead to speedups if the domain sizes of the random variables and factors were large. Note that Figure 5 (c) also gives an idea of the reduced memory consumption for lifted inference."}, {"heading": "5 Related Work", "text": "Poole [17] was one of the first to show that variable elimination [23] can be modified to directly work with first-order representations of random variables and factors (or clique potentials) to avoid propositionalization. Subsequently, de Salvo Braz et al. [3] further developed on Poole\u2019s work and referred to it as inversion elimination. They also introduce another technique for lifted inference known as counting elimination which is more expensive than inversion elimination but can help in certain situations where\nthe ground model\u2019s treewidth renders ground inference infeasible. It is straightforward to show that the bisimulation approach to lifted inference subsumes inversion elimination (and partial inversion [4]). Given a computation of the form \u2211Y \u2211Xi \u220fi \u03c8(Xi,Y ) (all \u03c8\u2019s are shared factors), inversion elimination avoids the complexity of eliminating each Xi,\u2200i = 1, . . .n separately by pushing each summation of Xi against the corresponding \u03c8 , eliminating Xi once and then eliminating Y : \u2211Y \u2211Xi \u220f n i=1 \u03c8(Xi,Y ) = \u2211Y \u220fni=1 \u2211Xi \u03c8(Xi,Y ) = \u2211Y \u220f n i=1 \u03c8 \u2032(Y ) = \u2211Y \u03c8 \u2032n(Y ) = \u03c8 \u2032\u2032(). Figure 6 shows how our approach achieves the same.\nIn other related work, Singla and Domingos [20] propose an approach where they run a bisimulation-like algorithm on the factor graph representing the probabilistic model to find clusters of random variables that send and receive identical messages which helps speed up inference with loopy belief propagation (LBP) [22], a ground approximate inference algorithm. Our approaches differ from theirs on two counts: first, their approach requires as input the specification of the probabilistic model in first-order format (ours, in effect, determines the first-order representation) and second, as the authors acknowledge in their paper, LBP often has problems with convergence, whereas the approach we describe in Section 2 always returns exact marginals and the approach we describe in Section 3, even though it is approximate, is always guaranteed to converge."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we described light-weight, generally applicable approximation algorithms for lifted inference based on the graph theoretic concept of bisimulation. Essentially, our techniques are wrap-arounds for variable elimination [23] and can be used whenever variable elimination is applicable, including computing joint conditional probabilities and MAP assignments (by switching from the sumproduct operator to max-product). One interesting avenue of future work is to look for other bounded complexity inference algorithms (besides mini-buckets) that can be combined with the techniques introduced in this paper. Other avenues of future work are determining the optimal values of the various parameters (path-length and \u03b5) automatically and building the compressed rv-elim graph directly from the first-order description of the probabilistic model."}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF Grants No. IIS0438866 and IIS-0546136. We would also like to thank the anonymous reviewers for their comments and suggestions and Parag Singla for sharing with us the Cora-ER MLN.\nReferences [1] http://www.cs.umass.edu/\u02dcmccallum/data/cora-refs.tar.gz. [2] J. Bar-Ilan, G. Kortsarz, and D. Peleg. How to allocate net-\nwork centers. Journal of Algorithms, 1993. [3] R. de Salvo Braz, E. Amir, and D. Roth. Lifted first-order\nprobabilistic inference. In IJCAI, 2005. [4] R. de Salvo Braz, E. Amir, and D. Roth. MPE and partial in-\nversion in lifted probabilistic variable elimination. In AAAI, 2006.\n[5] R. Dechter and I. Rish. Mini-buckets: A general scheme for bounded inference. Journal of the ACM, 2003.\n[6] U. Feige. A threshold of ln(n) for approximating set cover. Journal of the ACM, 1998.\n[7] M. Garey and D. Johnson. Computers and Intractability: A guide to the theory of NP-Completeness. W.H.Freeman, \u201979.\n[8] L. Getoor and B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, 2007.\n[9] C. Giles, K. Bollacker, and S. Lawrence. Citeseer: An automatic indexing system. ACM Digital Libraries, 1998.\n[10] D. Hochbaum and W. Maass. Approximation schemes for covering and packing problems in image processing and VLSI. Journal of the ACM, 1985.\n[11] P. Kanellakis and S. Smolka. CCS expressions, finite state processes and three problems of equivalence. In PODC, \u201983.\n[12] O. Kariv and S. Hakimi. An algorithmic approach to network location problems I: The p-centers. SIAM Journal on Applied Mathematics, 1979.\n[13] U. Kjaerulff. Graph triangulation: Algorithms giving small total state space. Technical report, Univ. of Aalborg, 1990.\n[14] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. Automating the construction of internet portals with machine learning. Information Retrieval Journal, 2000.\n[15] B. Milch, L. Zettlemoyer, K. Kersting, M. Haimes, and L. Kaelbling. Lifted probabilistic inference with counting formulas. In AAAI, 2008.\n[16] A. Pfeffer, D. Koller, B. Milch, and K. Takusagawa. SPOOK: A system for probabilistic object-oriented knowledge representation. In UAI, 1999.\n[17] D. Poole. First-order probabilistic inference. In IJCAI, \u201903. [18] P. Sen, A. Deshpande, and L. Getoor. Exploiting shared\ncorrelations in probabilistic databases. In VLDB, 2008. [19] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and\nT. Eliassi-Rad. Collective classification in network data. AI Magazine, 29(3), 2008.\n[20] P. Singla and P. Domingos. Lifted first-order belief propagation. In AAAI, 2008.\n[21] V. Vazirani. Approximation Algorithms. Springer, 2001. [22] J. Yedidia, W. Freeman, and Y. Weiss. Generalized belief\npropagation. In NIPS, 2000. [23] N. Zhang and D. Poole. A simple approach to bayesian net-\nwork computations. In Canadian Conference on AI, 1994."}], "references": [{"title": "How to allocate network centers", "author": ["J. Bar-Ilan", "G. Kortsarz", "D. Peleg"], "venue": "Journal of Algorithms", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Lifted first-order probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "MPE and partial inversion in lifted probabilistic variable elimination", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Mini-buckets: A general scheme for bounded inference", "author": ["R. Dechter", "I. Rish"], "venue": "Journal of the ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "A threshold of ln(n) for approximating set cover", "author": ["U. Feige"], "venue": "Journal of the ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "editors", "author": ["L. Getoor", "B. Taskar"], "venue": "Introduction to Statistical Relational Learning. MIT Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Citeseer: An automatic indexing system", "author": ["C. Giles", "K. Bollacker", "S. Lawrence"], "venue": "ACM Digital Libraries", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Approximation schemes for covering and packing problems in image processing and VLSI", "author": ["D. Hochbaum", "W. Maass"], "venue": "Journal of the ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "An algorithmic approach to network location problems I: The p-centers", "author": ["O. Kariv", "S. Hakimi"], "venue": "SIAM Journal on Applied Mathematics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1979}, {"title": "Graph triangulation: Algorithms giving small total state space", "author": ["U. Kjaerulff"], "venue": "Technical report, Univ. of Aalborg", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Automating the construction of internet portals with machine learning", "author": ["A. McCallum", "K. Nigam", "J. Rennie", "K. Seymore"], "venue": "Information Retrieval Journal", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Lifted probabilistic inference with counting formulas", "author": ["B. Milch", "L. Zettlemoyer", "K. Kersting", "M. Haimes", "L. Kaelbling"], "venue": "AAAI", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "SPOOK: A system for probabilistic object-oriented knowledge representation", "author": ["A. Pfeffer", "D. Koller", "B. Milch", "K. Takusagawa"], "venue": "UAI", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploiting shared correlations in probabilistic databases", "author": ["P. Sen", "A. Deshpande", "L. Getoor"], "venue": "VLDB", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine, 29(3)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Lifted first-order belief propagation", "author": ["P. Singla", "P. Domingos"], "venue": "AAAI", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation Algorithms", "author": ["V. Vazirani"], "venue": "Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Generalized belief propagation", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "A simple approach to bayesian network computations", "author": ["N. Zhang", "D. Poole"], "venue": "Canadian Conference on AI", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}], "referenceMentions": [{"referenceID": 1, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 11, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 12, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 226, "endOffset": 229}, {"referenceID": 13, "context": "In prior work [18], we showed how, using the symmetry present in the probabilistic model and methods closely related to the graph-theoretic concept of bisimulation, it is possible to compile a compressed version of the inference problem.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "\u2022 We review the results from [18] and show how they are applicable to general probabilistic graphical models.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": ", mini-buckets [5]) \u2013 that allows us to extend the use of our techniques to domains with unbounded treewidth.", "startOffset": 15, "endOffset": 18}, {"referenceID": 13, "context": "In the next section, we review our earlier work on exact lifted inference [18], in Section 3 we present our techniques for approximate lifted inference, in Section 4 we evaluate our approaches on synthetic and real-world data, in Section 5 we review related work and we conclude with a few pointers for future work in Section 6.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "This section reviews material from [18].", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 1, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 1, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 a 2", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 a 2", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c [ fs1 , fs2 ] [ fi1 , fi2 , fi3 ] [ fs3 ]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c [ fs1 , fs2 ] [ fi1 , fi2 , fi3 ] [ fs3 ]", "startOffset": 7, "endOffset": 10}, {"referenceID": 13, "context": "\u2217This is an example of a graphical model that may be constructed during query evaluation over probabilistic databases [18].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "\u2022 begin constructing the label by going through each parent\u2019s arguments list and forming a tuple composed of the arguments\u2019 ids assigned in the previous step: since fs1 is the first multiplicand and fi1 the second, we form our label by concatenating \u201c[1]\u201d with \u201c[2,1,3]\u201d,", "startOffset": 262, "endOffset": 269}, {"referenceID": 1, "context": "\u2022 begin constructing the label by going through each parent\u2019s arguments list and forming a tuple composed of the arguments\u2019 ids assigned in the previous step: since fs1 is the first multiplicand and fi1 the second, we form our label by concatenating \u201c[1]\u201d with \u201c[2,1,3]\u201d,", "startOffset": 262, "endOffset": 269}, {"referenceID": 0, "context": "Thus, the complete label for ms1 is \u201c{[1], [2,1,3], 1}\u201d (Figure 2(a)).", "startOffset": 43, "endOffset": 50}, {"referenceID": 1, "context": "Thus, the complete label for ms1 is \u201c{[1], [2,1,3], 1}\u201d (Figure 2(a)).", "startOffset": 43, "endOffset": 50}, {"referenceID": 13, "context": "In [18] we observed that by adapting the graph-theoretic notion of bisimulation [11], one can determine the equivalence class partitioning of the vertices.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "One caveat about the above approach is that since factor Algorithm 1: Exact Bisimulation [18]", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "In [18], we proposed ordering the parents of a node using their assigned blockids before assigning it to a block.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "We then run a modified min-size heuristic [13] on the compressed graph, and replace the vertices with the corresponding sets of random variables to get an elimination order for inference.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 1, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 1, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 1, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 1, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1} [ms1 ,ms2 ]", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1} [ms1 ,ms2 ]", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 21, "endOffset": 28}, {"referenceID": 1, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 21, "endOffset": 28}, {"referenceID": 0, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 121, "endOffset": 128}, {"referenceID": 1, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 121, "endOffset": 128}, {"referenceID": 0, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 145, "endOffset": 152}, {"referenceID": 1, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 145, "endOffset": 152}, {"referenceID": 0, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 148, "endOffset": 155}, {"referenceID": 1, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 148, "endOffset": 155}, {"referenceID": 0, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 162, "endOffset": 167}, {"referenceID": 0, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "Note how, in this case, ms3 has been differentiated from ms1 and ms2 since ms3 has an incoming path \u201cb(1){[1], [2,1,3],1}\u2032\u2032 (matching fs3 ,ms3 ) of length 1 which doesn\u2019t match any incoming 1-length path of ms1 or ms2 .", "startOffset": 111, "endOffset": 118}, {"referenceID": 1, "context": "Note how, in this case, ms3 has been differentiated from ms1 and ms2 since ms3 has an incoming path \u201cb(1){[1], [2,1,3],1}\u2032\u2032 (matching fs3 ,ms3 ) of length 1 which doesn\u2019t match any incoming 1-length path of ms1 or ms2 .", "startOffset": 111, "endOffset": 118}, {"referenceID": 4, "context": "Further, Feige [6] showed that DS is not approximable to a factor of (1\u2212o(1))ln(|V|) unless NP has \u201cslightly super-polynomial time\u201d algorithms (or NP \u2282 DTIME(nlog(log(|V|)))).", "startOffset": 15, "endOffset": 18}, {"referenceID": 16, "context": "This gives us an ln(|V|)-approximation algorithm [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "FB is also equivalent to the \u03c1dominating set problem [2], which, in turn, is the converse of the classic k-center problem [12] where we are given a graph from which we need to choose a subset of k vertices so that their distance from the other vertices is minimized.", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "FB is also equivalent to the \u03c1dominating set problem [2], which, in turn, is the converse of the classic k-center problem [12] where we are given a graph from which we need to choose a subset of k vertices so that their distance from the other vertices is minimized.", "startOffset": 122, "endOffset": 126}, {"referenceID": 7, "context": "For instance, for euclidean spaces, near-optimal factor binning is possible [10], especially when the factor sizes are not large.", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "Next we show how to incorporate the mini-bucket scheme [5], a bounded complexity approximate (ground) inference algorithm, with our ideas.", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "The mini-buckets scheme is a modification of the variable elimination algorithm [23] where at each step instead of eliminating a random variable by multiplying all factors it appears as argument in, one devises a set of mini-buckets each containing a (disjoint) subset of factors that contains that variable as argument and then eliminates the variable separately from each mini-bucket.", "startOffset": 80, "endOffset": 84}, {"referenceID": 3, "context": "Dechter and Rish [5] show how such a modification of the variable elimination algorithm provides an upper bound over the numbers produced in the resulting factors.", "startOffset": 17, "endOffset": 20}, {"referenceID": 18, "context": "We compare our results with two baseline algorithms: A ground inference procedure which is basically variable elimination [23] modified so that we obtain all marginals in a single pass, and the exact lifted inference procedure reviewed in Section 2.", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "We first report results on the Cora [14] and CiteSeer [9] datasets.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "We first report results on the Cora [14] and CiteSeer [9] datasets.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Note that, for these experiments, using the citations in the datasets we produce Markov networks with unbounded treewidth and then perform collective classification [19], so we compare against ground inference with mini-buckets restricted to 6 arguments.", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "Poole [17] was one of the first to show that variable elimination [23] can be modified to directly work with first-order representations of random variables and factors (or clique potentials) to avoid propositionalization.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "[3] further developed on Poole\u2019s work and referred to it as inversion elimination.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "It is straightforward to show that the bisimulation approach to lifted inference subsumes inversion elimination (and partial inversion [4]).", "startOffset": 135, "endOffset": 138}, {"referenceID": 15, "context": "In other related work, Singla and Domingos [20] propose an approach where they run a bisimulation-like algorithm on the factor graph representing the probabilistic model to find clusters of random variables that send and receive identical messages which helps speed up inference with loopy belief propagation (LBP) [22], a ground approximate inference algorithm.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "In other related work, Singla and Domingos [20] propose an approach where they run a bisimulation-like algorithm on the factor graph representing the probabilistic model to find clusters of random variables that send and receive identical messages which helps speed up inference with loopy belief propagation (LBP) [22], a ground approximate inference algorithm.", "startOffset": 315, "endOffset": 319}, {"referenceID": 18, "context": "Essentially, our techniques are wrap-arounds for variable elimination [23] and can be used whenever variable elimination is applicable, including computing joint conditional probabilities and MAP assignments (by switching from the sumproduct operator to max-product).", "startOffset": 70, "endOffset": 74}], "year": 2009, "abstractText": "There has been a great deal of recent interest in methods for performing lifted inference; however, most of this work assumes that the first-order model is given as input to the system. Here, we describe lifted inference algorithms that determine symmetries and automatically lift the probabilistic model to speedup inference. In particular, we describe approximate lifted inference techniques that allow the user to trade off inference accuracy for computational efficiency by using a handful of tunable parameters, while keeping the error bounded. Our algorithms are closely related to the graph-theoretic concept of bisimulation. We report experiments on both synthetic and real data to show that in the presence of symmetries, run-times for inference can be improved significantly, with approximate lifted inference providing orders of magnitude speedup over ground inference.", "creator": "TeX"}}}