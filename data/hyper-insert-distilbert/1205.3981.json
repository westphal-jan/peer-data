{"id": "1205.3981", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2012", "title": "kLog: A Language for Logical and Relational Learning with Kernels", "abstract": "it klog interpreter is presumably a logical mapping and relational language for kernel - based learning. professionally it allows simulation users wishing to specify essentially logical and real relational learning problems consistently at a high level accuracy in a declarative conceptual way. somehow it uniquely builds considerably on most simple but powerful concepts : automated learning from interpretations, supervised entity / relationship data modeling, logic functional programming algorithms and deductive databases ( example prolog and datalog ), cognitive and correlation graph theory kernels. klog engine is probably a statistical relational graphical learning language system module but unlike indeed other popular statistical relational learning models, it does significantly not represent substantially a probability distribution directly. it is rather a refined kernel - based scientific approach to learning that currently employs features derived from a legally grounded entity / continuous relationship diagram. these features classes are historically derived further using already a novel interface technique theoretically called graphicalization : first, relational representations are transformed quickly into graph based mental representations ; so subsequently, analytic graph based kernels are essentially employed nationally for defining feature spaces. klog can use numerical and symbolic data, background knowledge in the form of prolog or datalog programs ( as in inductive static logic database programming systems ) thereafter and additionally several statistical procedures can continually be used worldwide to fit roughly the model parameters. both the sql klog experimental framework can - - in principle - - merely be partially applied to work tackle the generally same range of tasks that has made mainstream statistical learning relational learning modeling so intimately popular, typically including classification, regression, correlation multitask learning, and descriptive collective classification.", "histories": [["v1", "Thu, 17 May 2012 17:00:00 GMT  (423kb,D)", "https://arxiv.org/abs/1205.3981v1", null], ["v2", "Fri, 18 May 2012 12:46:57 GMT  (429kb,D)", "http://arxiv.org/abs/1205.3981v2", null], ["v3", "Fri, 22 Jun 2012 12:06:52 GMT  (424kb,D)", "http://arxiv.org/abs/1205.3981v3", null], ["v4", "Mon, 17 Feb 2014 11:47:55 GMT  (511kb,D)", "http://arxiv.org/abs/1205.3981v4", null], ["v5", "Mon, 28 Jul 2014 13:41:00 GMT  (512kb,D)", "http://arxiv.org/abs/1205.3981v5", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.PL", "authors": ["paolo frasconi", "fabrizio costa", "luc de raedt", "kurt de grave"], "accepted": false, "id": "1205.3981"}, "pdf": {"name": "1205.3981.pdf", "metadata": {"source": "CRF", "title": "kLog: A Language for Logical and Relational Learning with Kernels", "authors": ["Paolo Frasconia", "Fabrizio Costab", "Luc De Raedtc", "Kurt De Gravec"], "emails": ["p-f@dsi.unifi.it", "costa@informatik.uni-freiburg.de", "Luc.DeRaedt@cs.kuleuven.be", "Kurt.DeGrave@cs.kuleuven.be"], "sections": [{"heading": null, "text": "We introduce kLog, a novel approach to statistical relational learning. Unlike standard approaches, kLog does not represent a probability distribution directly. It is rather a language to perform kernel-based learning on expressive logical and relational representations. kLog allows users to specify learning problems declaratively. It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming, and deductive databases. Access by the kernel to the rich representation is mediated by a technique we call graphicalization: the relational representation is first transformed into a graph \u2014 in particular, a grounded entity/relationship diagram. Subsequently, a choice of graph kernel defines the feature space. kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of Prolog or Datalog programs as in inductive logic programming systems. The kLog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification. We also report about empirical comparisons, showing\nIPF was a visiting professor at KU Leuven and FC a postdoctoral fellow at KU Leuven while this work was initiated\n\u2217Corresponding author Email addresses: p-f@dsi.unifi.it (Paolo Frasconi),\ncosta@informatik.uni-freiburg.de (Fabrizio Costa), Luc.DeRaedt@cs.kuleuven.be (Luc De Raedt), Kurt.DeGrave@cs.kuleuven.be (Kurt De Grave)\nPreprint submitted to Artificial Intelligence July 29, 2014\nar X\niv :1\n20 5.\n39 81\nv5 [\ncs .A\nI] 2\nthat kLog can be either more accurate, or much faster at the same level of accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at http://klog.dinfo.unifi.it along with tutorials.\nKeywords: Logical and relational learning, Statistical relational learning, kernel methods, Prolog, Deductive databases"}, {"heading": "1. Introduction", "text": "The field of statistical relational learning (SRL) is populated with a fairly large number of models and alternative representations, a state of affairs often referred to as the \u201cSRL alphabet soup\u201d [1, 2]. Even though there are many differences between these approaches, they typically extend a probabilistic representation (most often, a graphical model) with a logical or relational one [3, 4]. The resulting models then define a probability distribution over possible worlds, which are typically (Herbrand) interpretations assigning a truth value to every ground fact.\nHowever, the machine learning literature contains \u2014 in addition to probabilistic graphical models \u2014 several other types of statistical learning methods. In particular, kernel-based learning and support vector machines are amongst the most popular and powerful machine learning systems that exist today. But this type of learning system has \u2014 with a few notable exceptions to relational prediction [5, 6] \u2014 not yet received a lot of attention in the SRL literature. Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning. It is precisely this gap that we ultimately want to fill.\nThis paper introduces the kLog language and framework for kernel-based logical and relational learning. The key contributions of this framework are threefold: 1) kLog is a language that allows users to declaratively specify relational learning tasks in a similar way as statistical relational learning and inductive logic programming approaches but it is based on kernel methods rather than on probabilistic modeling; 2) kLog compiles the relational domain and learning task into a graph-based representation using a technique called graphicalization; and 3) kLog uses a graph kernel to construct the feature space where eventually the learning takes place. This whole process\nis reminiscent of knowledge-based model construction in statistical relational learning. We now sketch these contributions in more detail and discuss the relationships with statistical relational learning.\nOne contribution of this paper is the introduction of the kLog language and framework for kernel-based logical and relational learning. kLog is embedded in Prolog (hence the name) and allows users to specify different types of logical and relational learning problems at a high level in a declarative way. kLog adopts, as many other logical and relational learning systems, the learning from interpretations framework [9]. In this way, the entities (or objects) and the relationships amongst them can be naturally represented. However, unlike typical statistical relational learning frameworks, kLog does not employ a probabilistic framework but is rather based on linear modeling in a kernel-defined feature space.\nkLog constitutes a first step into the direction of a general kernel-based SRL approach. kLog generates a set of features starting from a logical and relational learning problem and uses these features for learning a (linear) statistical model. This is not unlike Markov logic but there are two major differences. First, kLog is based on a linear statistical model instead of a Markov network. Second, the feature space is not immediately defined by the declared logical formulae but it is constructed by a graph kernel, where nodes in the graph correspond to entities and relations, some given in the data, and some (expressing background knowledge) defined declaratively in Prolog. Complexity of logical formulae being comparable, graph kernels leverage a much richer feature space than MLNs. In order to learn, kLog essentially describes learning problems at three different levels. The first level specifies the logical and relational learning problem. At this level, the description consists of an E/R-model describing the structure of the data and the data itself, which is similar to that of traditional SRL systems [10]. The data at this level is then graphicalized, that is, the interpretations are transformed into graphs. This leads to the specification of a graph learning problem at the second level. Graphicalization is the equivalent of knowledgebased model construction. Indeed, SRL systems such as PRMs and MLNs also (at least conceptually) produce graphs, although these graphs represent probabilistic graphical models. Finally, the graphs produced by kLog are turned into feature vectors using a graph kernel, which leads to a statistical learning problem at the third level. Again there is an analogy with systems such as Markov logic as the Markov network that is generated in knowledgebased model construction lists also the features. Like in these systems, kLog\nfeatures are tied together as every occurrence is counted and is captured by a single same parameter in the final linear model.\nIt is important to realize that kLog is a very flexible architecture in which only the specification language of the first level is fixed; at this level, we employ an entity/relationship (E/R) model. The second level is then completely determined by the choice of the graph kernel. In the current implementation of kLog that we describe in this paper, we employ the neighborhood subgraph pairwise distance kernel (NSPDK) [11] but the reader should keep in mind that other graph kernels can be incorporated. Similarly, for learning the linear model at level three we mainly experimented with variants of SVM learners but again it is important to realize that other learners can be plugged in. This situation is akin to that for other statistical relational learning representations, for which a multitude of different inference and learning algorithms has been devised (see also Section 8 for a discussion of the relationships between kLog and other SRL systems).\nIn the next section, we illustrate the main steps of kLog modeling with a complete example in a real world domain. In Section 3, we formalize the assumed statistical setting for supervised learning from interpretations, provide some background on statistical modeling from a relational learning point of view, and position kLog more clearly in the context of related systems such as Markov logic, M3N [6], etc. In Section 4, we formalize the semantics of the language and illustrate what types of learning problems can be formulated in the framework. Further examples are given in Section 5. The graphicalization approach and the graph kernel are detailed in Section 6. Some empirical evaluation is reported in Section 7 and, finally, the relationships to other SRL systems are discussed in Section 8."}, {"heading": "2. A kLog example", "text": "Before delving into the technical details of kLog, we illustrate the different steps on a real-life example using the UW-CSE dataset prepared by Domingos et al. for demonstrating the capabilities of MLNs [7]. Anonymous data was obtained from the University of Washington Department of Computer Science and Engineering. Basic entities include persons (students or professors), scientific papers, and academic courses. Available relations specify, e.g., whether a person was the author of a certain paper, or whether he/she participated in the teaching activities of a certain course. The learning task\nconsists of predicting students\u2019 advisors, i.e., to predict the binary relation advised_by between students and professors."}, {"heading": "2.1. Data format", "text": "Data comes in the form of true ground atoms, under the closed-world assumption. Since (first-order logic) functions are not allowed in the language, a ground atom is essentially like a tuple in a relational database, for example taught_by(course170,person211,winter_0102).\nkLog learns from interpretations. This means that the data is given as a set of interpretations (or logical worlds) where each interpretation is a set of ground atoms which are true in that world. In this example there are five interpretations: ai, graphics, language, systems, and theory, corresponding to different research groups in the department. For instance, a fragment of the interpretation ai is shown in Listing 1.\n1 advised_by(person21,person211). advised_by(person45,person211). 2 has_position(person211,faculty). has_position(person407,faculty). 3 in_phase(person14,post_generals). in_phase(person21,post_generals). 4 in_phase(person284,post_quals). in_phase(person45,post_generals). 5 professor(person211). professor(person407). 6 publication(title110,person14). publication(title110,person407). 7 publication(title25,person21). publication(title25,person211). 8 publication(title25,person284). publication(title25,person407). 9 publication(title44,person211). publication(title44,person415).\n10 publication(title44,person45). 11 student(person14). student(person21). 12 student(person284). student(person45). 13 ta(course12,person21,winter_0203). ta(course24,person21,spring_0203). 14 ta(course24,person70,autumn_0304). 15 taught_by(course12,person211,autumn_0001). 16 taught_by(course143,person211,winter_0001). 17 taught_by(course170,person211,winter_0102). 18 taught_by(course24,person211,autumn_0102). 19 taught_by(course24,person211,spring_0203). 20 taught_by(course24,person407,spring_0304). 21 taught_by(course82,person407,winter_0102).\nListing 1: Database D for the UW-CSE domain\nAll ground atoms in a particular interpretation together form an instance of a relational database and the overall data set consists of several (disjoint)\ndatabases."}, {"heading": "2.2. Data modeling and learning", "text": "The first step in kLog modeling is to describe the domain using a classic database tool: entity relationship diagrams. We begin by modeling two entity sets: student and professor, two unary relations: in_phase and has_position, and one binary relation: advised_by (which is the target in this example). The diagram is shown in Figure 1. The kLog data model is written using the fragment of code of Listing 2.\n1 signature has_position(professor_id::professor, position::property )::extensional. 2 signature advised_by(p1::student, p2::professor)::extensional. 3 signature student(student_id::self)::extensional. 4 signature professor(professor_id::self)::extensional.\nListing 2: Examples of signature declaration.\nEvery entity or relationship that kLog will later use to generate features (see feature generation below) is declared by using the special keyword signature. Signatures are similar to the declarative bias used in inductive logic programming systems. There are two kinds of signatures, annotated by the reserved words extensional and intensional. In the extensional case, all ground atoms have to be listed explicitly in the data file; in the intensional case, ground atoms are defined implicitly using Prolog definite clauses. A signature has a name and a list of arguments with types. A type is either the name of an entity set (declared in some other signature) or the special type property used for numerical or categorical attributes. In the ground atoms,\nconstants that are not properties are regarded as identifiers and are simply used to connect ground atoms (these constants disappear in the result of the graphicalization procedure explained below).1\nOne of the powerful features of kLog is its ability to introduce novel relations using a mechanism resembling deductive databases. Such relations are typically a means of injecting domain knowledge. In our example, it may be argued that the likelihood that a professor advises a student increases if the two persons have been engaged in some form of collaboration, such as co-authoring a paper, or working together in teaching activities. In Listing 3 we show two intensional signatures for this purpose. An intensional signature declaration must be complemented by a predicate (written in Prolog) which defines the new relation. When working on a given interpretation, kLog asserts all the true ground atoms in that interpretation in the Prolog database and collects all true groundings for the predicate associated with the intensional signature. Intensional signatures can also be effectively exploited to\n1 signature on_same_course(s::student,p::professor)::intensional. 2 on_same_course(S,P) :- 3 professor(P), student(S), 4 ta(Course,S,Term), taught_by(Course,P,Term).\n5 signature on_same_paper(s::student,p::professor)::intensional. 6 on_same_paper(S,P) :- 7 student(S), professor(P), 8 publication(Pub, S), publication(Pub,P).\n9 signature n_common_papers(s::student,p::professor,n::property )::intensional. 10 n_common_papers(S,P,N) :- 11 student(S), professor(P), 12 setof(Pub, (publication(Pub, S), publication(Pub,P)), CommonPapers), 13 length(CommonPapers,N).\nListing 3: Intensional signatures and associated Prolog predicates.\nintroduce aggregated attributes [9]. The last signature in Listing 3 shows for example how to count the number of papers a professor and a student have\n1 The special type name self is used to denote the name of the signature being declared, as in lines 7\u20138 of Listing 2. Thus, a signature containing an argument of type self introduces a new entity set while other signatures introduce relationships.\npublished together."}, {"heading": "2.3. Graphicalization", "text": "Graphicalization is our approach to capture the relational structure of the data by means of a graph. It enables the use of kernel methods in SRL via a graph kernel, which measures the similarity between two graphs. The procedure maps a set of ground atoms into a bipartite undirected graph whose nodes are true ground atoms and whose edges connect an entity atom to a relationship atom if the identifier of the former appears as an argument in the latter. The graph resulting from the graphicalization of the ai interpretation is shown in Figure 22. It is from this graph that kLog will generate propositional features (based on a graph kernel) for use in the learning procedure. The details of the graphicalization procedure and the kernel are given in Sections 6 and 6.2, respectively.\nNow that we have specified the inputs to the learning system, we still need to determine the learning problem. This is declared in kLog by designating\n2Note that interpretation predicates which exist in the data but have not a corresponding signature (e.g., publication) do not produce nodes in the graph. However these predicates may be conveniently exploited in the bodies of the intensional signatures (e.g., on_same_paper refers to publication).\none (or more) signature(s) as target (in this domain, the target relation is advised_by). Several library predicates are designed for training, e.g., kfold performs a k-fold cross validation. These predicates accept a list of target signatures which specifies the learning problem."}, {"heading": "3. Statistical setting", "text": "Our general approach to construct a statistical model is based on the learning from interpretations setting [9]. An interpretation, or logical world, is a set of ground atoms z. We assume that interpretations are sampled identically and independently from a fixed and unknown distribution D. We denote by {zi; i \u2208 I} the resulting data set, where I is a given index set (e.g., the first n natural numbers) that can be thought of as interpretation identifiers. Like in other statistical learning systems, the goal is to use a data sample to make some form of (direct or indirect) inference about this distribution.\nFor the sake of simplicity, throughout this paper we will mainly focus on supervised learning. In this case, it is customary to think of data as consisting of two separate portions: inputs (called predictors or independent variables in statistics) and outputs (called responses or dependent variables). In our framework, this distinction is reflected in the set of ground atoms in a given interpretation. That is, z is partitioned into two sets: x (input or evidence atoms) and y (output or query atoms).\nThe goal of supervised learning in this setting is to construct a prediction function f that maps the set of input atoms x (a partial interpretation) into a set of output atoms f(x). To this end, a feature vector3 \u03c6(x, y) is associated with each complete interpretation z = (x, y). A potential function based on the linear model F (x, y) = w>\u03c6(x, y) is then used to \u201cscore\u201d the interpretation. Prediction (or inference) is the process of maximizing F with respect to y, i.e. f(x) = argmaxy F (x, y). Learning is the process of fitting w to the available data, typically using some statistically motivated loss function that measures the discrepancy between the prediction f(xi) and the observed output yi on the i-th training interpretation. This setting is related to other kernel-based approaches to structured output learning (e.g., [12]), which may be developed without associating a probabilistic interpretation to F .\n3Our approach for constructing feature vectors is explained in Section 6.\nThe above perspective covers a number of commonly used algorithms ranging from propositional to relational learning. To see this, consider first binary classification of categorical attribute-value data. In this case, models such as naive Bayes, logistic regression, and support vector machines (SVM) can all be constructed to share exactly the same feature space. Using indicator functions on attribute values as features, the three models use a hyperplane as their decision function: f(x) = argmaxy\u2208{false,true}w>\u03c6(x, y) where for naive Bayes, the joint probability of (x, y) is proportional to exp(w>\u03c6(x, y)). The only difference between the three models is actually in the way w is fitted to data: SVM optimizes a regularized functional based on the hinge loss function, logistic regression maximizes the conditional likelihood of outputs given inputs (which can be seen as minimizing a smoothed version of the SVM hinge loss), and naive Bayes maximises the joint likelihood of inputs and outputs. The last two models are often cited as an example of generativediscriminative conjugate pairs because of the above reasons [13].\nWhen moving up to a slightly richer data type like sequences (perhaps the simplest case of relational data), the three models have well known extensions: naive Bayes extends to hidden Markov models (HMMs), logistic regression extends to conditional random fields (CRFs) [14], and SVM extends to structured output SVM for sequences [15, 12]. Note that when HMMs are used in the supervised learning setting (in applications such as part-of-speech tagging) the observation x is the input sequence, and the states form the output sequence y (which is observed in training data). In the simplest version of these three models, \u03c6(x, y) contains a feature for every pair of states (transitions) and a feature for every state-observation pair (emissions). Again, these models all use the same feature space (see, e.g., [14] for a detailed discussion).\nWhen moving up to arbitrary relations, the three above models can again be generalized but many complications arise, and the large number of alternative models suggested in the literature has originated the SRL alphabet soup mentioned at the beginning of the paper. Among generative models, one natural extension of HMMs is stochastic context free grammars [16], which in turn can be extended to stochastic logic programs [17]. More expressive systems include probabilistic relational models (PRMs) [8] and Markov logic networks (MLNs) [7], when trained generatively. Generalizations of SVM for relational structures akin to context free grammars have also been investigated [12]. Among discriminative models, CRFs can be extended from linear chains to arbitrary relations [14], for example in the form of discriminative\nMarkov networks [18] and discriminative Markov logic networks [7]. The use of SVM-like loss functions has also been explored in max-margin Markov networks (M3N) [6]. These models can cope with relational data by adopting a richer feature space. kLog contributes to this perspective as it is a language for generating a set of features starting from a logical and relational learning problem and using these features for learning a (linear) statistical model.\nTable 1 shows the relationships among some of these approaches. Methods on the same row use similar loss functions, while methods in the same column can be arranged to share the same feature space. In principle, kLog features can be used with any loss."}, {"heading": "4. The kLog language", "text": "A kLog program consists of:\n\u2022 a set of ground facts, embedded in a standard Prolog database, representing the data of the learning problem (see, e.g., Listing 1);\n\u2022 a set of signature declarations (e.g., Listing 2);\n\u2022 a set of Prolog predicates associated with intensional signatures (e.g., Listing 3);\n\u2022 a standard Prolog program which specifies the learning problem and makes calls to kLog library predicates."}, {"heading": "4.1. Data modeling and signatures", "text": "In order to specify the semantics of the language, it is convenient to formalize the domain of the learning problem as a set of constants (objects) C and a finite set of relations R. Constants are partitioned into a set of entity identifiers E (or identifiers for short) and set of property values V . Identifiers are themselves partitioned into k entity-sets E1, . . . , Ek. A ground atom\nr(c1, . . . , cn) is a relation symbol (or predicate name) r of arity n followed by an n-tuple of constant symbols ci. An interpretation (for the learning problem) is a finite set of ground atoms.\nThe signature for a relation r/m \u2208 R is an expression of the form\nr(name1 :: type1, . . . , namem :: typem) :: level\nwhere, for all j = 1, . . . ,m, typej \u2208 {E1, . . . , Ek,V} and namej is the name of the j-th column of r. If column j does not have type V , then its name can optionally include a role field using the syntax namej@rolej. If unspecified, rolej is set to j by default. The level of a signature is either intensional or extensional. In the extensional case, all the atoms which contribute to every interpretation are those and only those listed in the data. In the intensional case, ground atoms are those which result from the Prolog intensional predicates using Prolog semantics (optionally with tabling). In this way, users may take advantage of many of the extensions of definite clause logic that are built into Prolog.\nThe ability to specify intensional predicates through clauses (see an example in Listing 3) is most useful for introducing background knowledge in the learning process and common practice in inductive logic programming [9]. As explained in Section 6.2, features for the learning process are derived from a graph whose vertices are ground facts in the database; hence the ability to declare rules that specify relations directly translates into the ability to design and maintain features in a declarative fashion. This is a key characteristic of kLog and, in our opinion, one of the key reasons behind the success of related systems like Markov logic.\nIn order to ensure the well-definedness of the subsequent graphicalization procedure (see Section 6), we introduce two additional database assumptions. First, we require that the primary key of every relation consists of the columns whose type belongs to {E1, . . . , Ek} (i.e., purely of identifiers). This is perhaps the main difference between the kLog and the E/R data models. As it will become more clear in the following, identifiers are just placeholders and are kept separate from property values so that learning algorithms will not rely directly on their values to build a decision function4.\nThe relational arity of a relation is the length of its primary key. As a\n4To make an extreme case illustrating the concept, it makes no sense to predict whether a patient suffers from a certain disease based on his/her social security number.\nspecial case, relations of zero relational arity are admitted and they must consist of at most a single ground atom in any interpretation5. Second, for every entity-set Ei there must be a distinguished relation r/m \u2208 R that has relational arity 1 and key of type Ei. These distinguished relations are called E-relations and are used to introduce entity-sets, possibly with (m\u22121) attached properties as satellite data. The remaining |R| \u2212 k relations are called R-relations or relationships6, which may also have properties. Thus, primary keys for R-relations are tuples of foreign keys."}, {"heading": "4.2. Supervised learning jobs", "text": "A supervised learning job in kLog is specified as a set of relations. We begin defining the semantics of a job consisting of a single relation. Without loss of generality, let us assume that this relation has signature\nr(name1 ::Ei(1), . . . , namen ::Ei(n), namen+1 ::V , . . . , namen+m ::V) (1)\nwith i(j) \u2208 {1, . . . , k} for j = 1, . . . , n. Conventionally, if n = 0 there are no identifiers and if m = 0 there are no properties. Recall that under our assumptions the primary key of r must consist of entity identifiers (the first n columns). Hence, n > 0 and m > 0 implies that r represents a function with domain Ei(1)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Ei(n) and range Vm. If m = 0 then r can be seen as a function with a Boolean range.\nHaving specified a target relation r, kLog is able to infer the partition x \u222a y of ground atoms into inputs and outputs in the supervised learning setting. The output y consists of all ground atoms of r and all ground atoms of any intensional relation r\u2032 which depends on r. The partition is inferred by analyzing the dependency graphs of Prolog predicates defining intensional relations, using an algorithm reminiscent of the call graph computation in ViPReSS [19].\nWe assume that the training data is a set of complete interpretations7. During prediction, we are given a partial interpretation consisting of ground atoms x, and are required to complete the interpretation by predicting output\n5This kind of relations is useful to represent global properties of an interpretation (see Example 5.1).\n6Note that the word \u201crelationship\u201d specifically refers to the association among entities while \u201crelation\u201d refers the more general association among entities and properties.\n7kLog could be extended to deal with missing data by removing the closed world assumption and requiring some specification of the actual false groundings.\nground atoms y. For the purpose of prediction accuracy estimation, we will be only interested in the ground atoms of the target relation (a subset of y).\nSeveral situations may arise depending on the relational arity n and the number of properties m in the target relation r, as summarized in Table 2. When n = 0, the declared job consists of predicting one or more properties of an entire interpretation, when n = 1 one or more properties of certain entities, when n = 2 one or more properties of pairs of entities, and so on. When m = 0 (no properties) we have a binary classification task (where positive cases are ground atoms that belong to the complete interpretation). Multiclass classification can be properly declared by usingm = 1 with a categorical property, which ensures mutual exclusiveness of classes. Regression is also declared by usingm = 1 but in this case the property should be numeric. Note that property types (numerical vs. categorical) are automatically inferred by kLog by inspecting the given training data. An interesting scenario occurs when m > 1 so that two or more properties are to be predicted at the same time. A similar situation occurs when the learning job consists of several target relations. kLog recognizes that such a declaration defines a multitask learning job. However having recognized a multitask job does not necessarily mean that kLog will have to use a multitask learning algorithm capable of taking advantage of correlations between tasks (like, e.g., [20]). This is because, by design and in line with the principles of declarative languages, kLog separates \u201cwhat\u201d a learning job looks like and \u201chow\u201d it is solved by applying a particular learning algorithm. We believe that the separation of concerns at this level permits greater flexibility and extendability and facilitates plugging-in alternative learning algorithms (a.k.a. kLog models) that have the ability to provide a solution to a given job."}, {"heading": "4.3. Implementation", "text": "kLog is currently embedded in Yap Prolog [21] and consists of three main components: (1) a domain-specific interpreter, (2) a database loader, and (3) a library of predicates that are used to specify the learning task, to declare the graph kernel and the learning model, and to perform training, prediction, and performance evaluation. The domain-specific interpreter parses the signature declarations (see Section 4.1), possibly enriched with intensional and auxiliary predicates. The database loader reads in a file containing extensional ground facts and generates a graph for each interpretation, according to the procedure detailed in Section 6. The library includes common utilities for training and testing. Most of kLog is written in Prolog except feature\nvector generation and the statistical learners, which are written in C++. kLog incorporates LibSVM [22] and Stochastic gradient descent [23] and can interface with arbitrary (external) SVM solvers by coding appropriate data conversion wrappers."}, {"heading": "5. Examples", "text": "In Section 2 we have given a detailed example of kLog in a link prediction problem. The kLog setting encompasses a larger ensemble of machine learning scenarios, as detailed in the following examples, which we order according to growing complexity of the underlying learning task.\nExample 5.1. [Classification of independent interpretations] This is the simplest supervised learning problem with structured input data and scalar (unstructured) output. For the sake of concreteness, let us consider the problem of small molecule classification as pioneered in the relational learning setting in [24]. This domain is naturally modeled in kLog as follows. Each molecule corresponds to one interpretation; there is one E-relation, atom, that may include properties such as element and charge; there is one relationship of relational arity 2, bond, that may include a bond_type property to distinguish among single, double, and resonant chemical bonds; there is finally a zero-arity relationship, active, distinguishing between positive and negative interpretations. A concrete example is given in Section 7.1.\nExample 5.2. [Regression and multitask learning] The above example about small molecules can be extended to the case of regression where the task is\nto predict a real-valued property associated with a molecule, such as its biological activity or its octanol/water partition coefficient (logP) [25]. Many problems in quantitative structure-activity relationships (QSAR) are actually formulated in this way. The case of regression can be handled simply by introducing a target relation with signature activity(act::property).\nIf there are two or more properties to be predicted, one possibility is to declare several target relations, e.g., we might add logP(logp::property). Alternatively we may introduce a target relation such as:\ntarget_properties(activity::property,logp::property).\nMultitask learning can be handled trivially by learning independent predictors; alternatively, more sophisticated algorithms that take into account correlations amongst tasks (such as [26]) could be used.\nExample 5.3. [Entity classification] A more complex scenario is the collective classification of several entities within a given interpretation. We illustrate this case using the classic WebKB domain [27]. The data set consists of Web pages from four Computer Science departments and thus there are four interpretations: cornell, texas, washington, and wisconsin. In this domain there are two E-relations: page (for webpages) and link (for hypertextual links). Text in each page is represented as a bag-of-words (using the R-relation has) and hyperlinks are modeled by the R-relations link_to and link_from. Text associated with hyperlink anchors is represented by the R-relation has_anchor.\nThe goal is to classify each Web page. There are different data modeling alternatives for setting up this classification task. One possibility is to introduce several unary R-relations associated with the different classes, such as course, faculty, project, and student. The second possibility is to add a property to the entity-set page, called category, and taking values on the different possible categories. It may seem that in the latter case we are just reifying the R-relations describing categories. However there is an additional subtle but important difference: in the first modeling approach it is perfectly legal to have an interpretation where a page belongs simultaneously to different categories. This becomes illegal in the second approach since otherwise there would be two or more atoms of the E-relation page with the same identifier.\nFrom a statistical point of view, since pages for the same department are part of the same interpretation and connected by hyperlinks, the corresponding category labels are interdependent random variables and we formally have\nan instance of a supervised structured output problem [28], that in this case might also be referred to as collective classification [18]. There are however studies in the literature that consider pages to be independent (e.g., [29]).\nExample 5.4. [One-interpretation domains] We illustrate this case on the Internet Movie Database (IMDb) data set. Following the setup in [30], the problem is to predict \u201cblockbuster\u201d movies, i.e., movies that will earn more than $2 million in their opening weekend. The entity-sets in this domain are movie, studio, and individual. Relationships include acted_in(actor::individual, m::movie), produced(s::studio, m::movie), and directed(director::individual, m::movie). The target unary relation blockbuster(m::movie) collects positive cases. Training in this case uses a partial interpretation (typically movies produced before a given year). When predicting the class of future movies, data about past movies\u2019 receipts can be used to construct features (indeed, the count of blockbuster movies produced by the same studio is one of the most informative features [31]).\nA similar scenario occurs for protein function prediction. Assuming data for just a single organism is available, there is one entity set (protein) and a binary relation interact expressing protein-protein interaction [32, 33]."}, {"heading": "6. Graphicalization and feature generation", "text": "The goal is to map an interpretation z = (x, y) into a feature vector \u03c6(z) = \u03c6(x, y) \u2208 F (see Section 3). This enables the application of several supervised learning algorithms that construct linear functions in the feature space F . In this context, \u03c6(z) can be either computed explicitly or defined implicitly, via a kernel function K(z, z\u2032) = \u3008\u03c6(z), \u03c6(z\u2032)\u3009. Kernel-based solutions are very popular, sometimes computationally faster, and can exploit infinite-dimensional feature spaces. On the other hand, explicit feature map construction may offer advantages in our setting, in particular when dealing with large scale learning problems (many interpretations) and structured output tasks (exponentially many possible predictions). Our framework is based on two steps: first an interpretation z is mapped into an undirected labeled graph Gz; then a feature vector \u03c6(z) is extracted from Gz. Alternatively, a kernel function on pairs of graphs K(z, z\u2032) = K(Gz, Gz\u2032) could be computed. The corresponding potential function is then defined directly as F (z) = w>\u03c6(z) or as a kernel expansion F (z) = \u2211 i ciK(z, zi).\nThe use of an intermediate graphicalized representation is novel in the context of propositionalization, a well-known technique in logical and relational learning [9] that transforms graph-based or relational data directly into an attribute-value learning format, or possibly into a multi-instance learning one8. This typically results in a loss of information, cf. [9]. Our approach transforms the relational data into an equivalent graph-based format, without loss of information. After graphicalization, kLog uses the results on kernels for graph-based data to derive an explicit high-dimensional feature-based representation. Thus kLog directly upgrades these graph-based kernels to a fully relational representation. Furthermore, there is an extensive literature on graph kernels and virtually all existing solutions can be plugged into the learning from interpretations setting with minimal effort. This includes implementation issues but also the ability to reuse existing theoretical analyses. Finally, it is notationally simpler to describe a kernel and feature vectors defined on graphs, than to describe the equivalent counterpart using the Datalog notation.\nThe graph kernel choice implicitly determines how predicates\u2019 attributes are combined into features."}, {"heading": "6.1. Graphicalization procedure", "text": "Given an interpretation z, we construct a bipartite graph Gz([Vz, Fz], Ez) as follows (see Appendix B for notational conventions and Figure 2 for an example).\nVertices: there is a vertex in Vz for every ground atom of every E-relation, and there is a vertex in Fz for every ground atom of every R-relation. Vertices are labeled by the predicate name of the ground atom, followed by the list of property values. Identifiers in a ground atom do not appear in the labels but they uniquely identify vertices. The tuple ids(v) denotes the identifiers in the ground atom mapped into vertex v.\nEdges: uv \u2208 Ez if and only if u \u2208 Vz, v \u2208 Fz, and ids(u) \u2282 ids(v). The edge uv is labeled by the role under which the identifier of u appears in v (see Section 4.1).\n8In multi-instance learning [34], the examples are sets of attribute-value tuples or sets of feature vectors.\nNote that, because of our data modeling assumptions (see Section 4.1), the degree of every vertex v \u2208 Fz equals the relational arity of the corresponding R-relation. The degree of vertices in Vz is instead unbounded and may grow really large in some domains (e.g., social networks or World-Wide-Web networks). The graphicalization process can be nicely interpreted as the unfolding of an E/R diagram over the data, i.e., the E/R diagram is a template that is expanded according to the given ground atoms (see Figure 2). There are several other examples in the literature where a graph template is expanded into a ground graph, including the plate notation in graphical models [35], encoding networks in neural networks for learning data structures [36], and the construction of Markov networks in Markov logic [7]. The semantics of kLog graphs for the learning procedure is however quite different and intimately related to the concept of graph kernels, as detailed in the following section."}, {"heading": "6.2. Graph kernel", "text": "Learning in kLog is performed using a suitable graph kernel on the graphicalized interpretations. While in principle any graph kernel can be employed, there are several requirements that the chosen kernel has to meet in practice. On the one hand, computational efficiency is very important, both with respect to the number of graphs, and with respect to the graph size, as the grounding phase in the graphicalization procedure can yield very large graphs. On the other hand, we need a general purpose kernel with a flexible bias to adapt to a wide variety of application scenarios. In the case of independent interpretations (as in Examples 5.1 and 5.2), any graph kernel (several exist in the literature, see e.g. [37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation. However, when the task requires to make predictions about tuples of entities within the same interpretation (as in Section 2 or in Examples 5.3 and 5.4) an immediate application of existing graph kernels (such as those in the above references) is not straightforward. Additionally, if the graph has vertices with large degree (as in Example 5.3), kernels based on hard subgraph matching may severely overfit the data.\nIn the current implementation, we use an extension of NSPDK [11]. While the original kernel is suitable for sparse graphs with discrete vertex and edge labels, here we propose extensions to deal with soft matches (Section 6.2.4) and a larger class of graphs whose labels are tuples of mixed discrete and\nnumerical types (Section 6.2.5). In Section 6.3 we finally introduce a variant of NSPDK with viewpoints, which can handle the case of multiple predictions within the same interpretation. Similar extensions could be devised for other kinds of kernels but we do not discuss them in this paper."}, {"heading": "6.2.1. Kernel definition and notation", "text": "The NSPDK is an instance of a decomposition kernel, where \u201cparts\u201d are pairs of subgraphs (for more details on decomposition kernels see Appendix C). For a given graph G = (V,E), and an integer r \u2265 0, let N vr (G) denote the subgraph of G rooted in v and induced by the set of vertices\nV vr = {x \u2208 V : d?(x, v) \u2264 r}, (2)\nwhere d?(x, v) is the shortest-path distance between x and v 9. A neighborhood is therefore a topological ball with center v. Let us also introduce the following neighborhood-pair relation:\nRr,d = {(N vr (G), Nur (G), G) : d?(u, v) = d} (3)\nthat is, relation Rr,d identifies pairs of neighborhoods of radius r whose roots are exactly at distance d. We define \u03bar,d over graph pairs as the decomposition kernel on the relation Rr,d, that is:\n\u03bar,d(G,G \u2032) = \u2211 A,B \u2208 R\u22121r,d(G) A\u2032, B\u2032 \u2208 R\u22121r,d(G \u2032) \u03ba((A,B), (A\u2032, B\u2032)) (4)\nwhere R\u22121r,d(G) indicates the multiset of all pairs of neighborhoods of radius r with roots at distance d that exist in G.\nWe can now obtain a flexible parametric family of kernel functions by specializing the kernel \u03ba. The general structure of \u03ba is:\n\u03ba((A,B), (A\u2032, B\u2032)) = \u03baroot((A,B), (A \u2032, B\u2032))\u03basubgraph((A,B), (A \u2032, B\u2032)). (5)\nIn the following, we assume\n\u03baroot((A,B), (A \u2032, B\u2032)) = 1`(r(A))=`(r(A\u2032))1`(r(B))=`(r(B\u2032)) (6)\n9Conventionally d?(x, v) =\u221e if no path exists between x and v\nwhere 1 denotes the indicator function, r(A) is the root of A and `(v) the label of vertex v. The role of \u03baroot is to ensure that only neighborhoods centered on the same type of vertex will be compared. Assuming a valid kernel for \u03basubgraph (in the following Sections we give details on concrete instantiations), we can finally define the NSPDK as:\nK(G,G\u2032) = \u2211 r \u2211 d \u03bar,d(G,G \u2032). (7)\nFor efficiency reasons we consider the zero-extension of K obtained by imposing an upper bound on the radius and the distance parameter: Kr\u2217,d\u2217(G,G\u2032) =\u2211r\u2217\nr=0 \u2211d\u2217 d=0 \u03bar,d(G,G\n\u2032), that is, we limit the sum of the \u03bar,d kernels for all increasing values of the radius (distance) parameter up to a maximum given value r\u2217 (d\u2217). Furthermore we consider a normalized version of \u03bar,d, that is: \u03ba\u0302r,d(G,G\u2032) = \u03bar,d(G,G \u2032)\u221a\n\u03bar,d(G,G)\u03bar,d(G\u2032,G\u2032) , to ensure that relations induced by all\nvalues of radii and distances are equally weighted regardless of the size of the induced part sets.\nFinally, it is easy to show that the NSPDK is a valid kernel as: 1) it is built as a decomposition kernel over the countable space of all pairs of neighborhood subgraphs of graphs of finite size; 2) the kernel over parts is a valid kernel; 3) the zero-extension to bounded values for the radius and distance parameters preserves positive definiteness and symmetry; and 4) so does the normalization step."}, {"heading": "6.2.2. Subgraph kernels", "text": "The role of \u03basubgraph is to compare pairs of neighborhood graphs extracted from two graphs. The application of the graphicalization procedure to diverse relational datasets can potentially induce graphs with significantly different characteristics. In some cases (discrete property domains) an exact matching between neighborhood graphs is appropriate, in other cases however (continous properties domains) it is more appropriate to use a soft notion of matching.\nIn the following sections, we introduce variants of \u03basubgraph to be used when the atoms in the relational dataset can maximally have a single discrete or continuous property, or when more general tuples of properties are allowed."}, {"heading": "6.2.3. Exact graph matching", "text": "An important case is when the atoms, that are mapped by the graphicalization procedure to the vertex set of the resulting graph, can maximally\nhave a single discrete property. In this case, an atom r(c) becomes a vertex v, whose label is obtained by concatenation of the signature name and the attribute value. In this case, \u03basubgraph has the following form:\n\u03basubgraph((A,B), (A \u2032, B\u2032)) = 1A\u223c=A\u20321B\u223c=B\u2032 (8)\nwhere 1 denotes the indicator function and \u223c= isomorphism between graphs. Note that 1A\u223c=A\u2032 is a valid kernel between graphs under the feature map \u03c6cl that transforms A into \u03c6cl(A), a sequence of all zeros except the i-th element equal to 1 in correspondence to the identifier for the canonical representation of A [43, 44].\nEvaluating the kernel in Equation 8 requires as a subroutine graph isomorphism, a problem for which it is unknown whether polynomial algorithms exist. Algorithms that are in the worst case exponential but that are fast in practice do exist [43, 44]. For special graph classes, such as bounded degree graphs [45], there exist polynomial time algorithms. However, since it is hard to limit the type of graph produced by the graphicalization procedure (e.g., cases with very high vertex degree are possible as in general an entity atom may play a role in an arbitrary number of relationship atoms), we prefer an approximate solution with efficiency guarantees based on topological distances similar in spirit to [46].\nThe key idea is to compute an integer pseudo-identifier for each graph such that isomorphic graphs are guaranteed to bear the same number (i.e., the function is a graph invariant), but non-isomorphic graphs are likely to bear a different number. A trivial identity test between the pseudo-identifiers then approximates the isomorphism test. Appendix D details the computation of the pseudo-identifier."}, {"heading": "6.2.4. Soft matches", "text": "The idea of counting exact neighborhood subgraphs matches to express graph similarity is adequate when the graphs are sparse (that is, when the edge and the vertex set sizes are of the same order) and when the maximum vertex degree is low. However, when the graph is not sparse or some vertices exhibit large degrees, the likelihood that two neighborhoods match exactly quickly approaches zero, yielding a diagonal dominant kernel prone to overfitting10. In these cases a better solution is to relax the all-or-nothing\n10A concrete example is when text information associated to a document is modeled explicitly, i.e., when word entities are linked to a document entity: in this case the degree\ntype of match, allowing subgraphs to match partially or in a soft way. Although there exist several graph kernels that are based on this type of match, they generally suffer from very high computational costs [41]. To ensure efficiency, we use an idea introduced in the Weighted Decomposition Kernel [47]: given a subgraph, we consider only the multinomial distribution (i.e., the histogram) of the labels, discarding all structural information. In the soft match kernel, the comparison between two pairs of neighborhood subgraphs is replaced by11:\n\u03basubgraph((A,B), (A \u2032, B\u2032)) = \u2211 v \u2208 V (A) \u222a V (B) v\u2032 \u2208 V (A\u2032) \u222a V (B\u2032) 1`(v)=`(v\u2032) (9)\nwhere V (A) is the set of vertices of A. In words, for each pair of close neighborhoods, we build a histogram counting the vertices with the same label in either of the neighborhood subgraphs. The kernel is then computed as the dot product of the corresponding histograms."}, {"heading": "6.2.5. Tuples of properties", "text": "A standard assumption in graph kernels is that vertex and edge labels are elements of a discrete domain. However, in kLog the information associated with vertices is a tuple that can contain both discrete and real values. Here we extend NSPDK to allow both a hard and a soft match type over graphs with property tuples that can be discrete, real, or a mix of both types. While similar extensions can be conceived, in principle, for other graph kernels, the literature has mainly focused on kernels with single categorical labels. The key idea is to use the canonical vertex identifier (introduced in Section 6.2.3) to further characterize each property: in this way the kernel is defined only between tuples of vertices that correspond under isomorphism.\nThe general structure of the kernel on the subgraph can be written as:\n\u03basubgraph((A,B), (A \u2032, B\u2032)) = \u2211 v \u2208 V (A) \u222a V (B) v\u2032 \u2208 V (A\u2032) \u222a V (B\u2032) 1`(v)=`(v\u2032)\u03batuple(v, v \u2032) (10)\ncorresponds to the document vocabulary size. 11Note that the pair of neighborhood subgraphs are considered jointly, i.e., the label multisets are extracted independently from each subgraph in the pair and then combined together.\nwhere, for an atom12 r(c1, c2, . . . , cm) mapped into vertex v, `(v) returns the signature name r. \u03basubgraph is a kernel that is defined over sets of vertices (atoms) and can be decomposed in a part that ensures matches between atoms with the same signature name, and a second part that takes into account the tuple of property values. In particular, depending on the type of property values and the type of matching required, we obtain the following six cases. Soft match for discrete tuples. When the tuples contain only discrete elements and one chooses to ignore the structure in the neighborhood graphs, then each property is treated independently. Formally:\n\u03batuple(v, v \u2032) = \u2211 d 1propd(v)=propd(v\u2032) (11)\nwhere for an atom r(c1, c2, . . . , cd, . . . , cm) mapped into vertex v, propd(v)\n12We remind the reader that in the graphicalization procedure we remove the primary and foreign keys from each atom, hence the only information available at the graph level are the signature name and the properties values.\nreturns the property value cd. Hard match for discrete tuples. When the tuples contain only discrete elements and one chooses to consider the structure, then each property key is encoded taking into account the identity of the vertex in the neighborhood graph and all properties are required to match jointly. In practice, this is equivalent to the hard match of Section 6.2.3 where the property value is replaced with the concatenation of all property values in the tuple. Formally, we replace the label `(v) in Equation 10 with the labeling procedure Lv explained in Section 6.2.3. In this way, each vertex receives a canonical label that uniquely identifies it in the neighborhood graphs. The double summation of 1`(v)=`(v\u2032) in Eq. 10 is then performing the selection of the corresponding vertices v and v\u2032 in the two pairs of neighborhood that are being compared. Finally, we consider all the elements of the tuple jointly in order to identify a successful match:\n\u03batuple(v, v \u2032) = \u220f d 1propd(v)=propd(v\u2032). (12)\nSoft match for real tuples. To upgrade the soft match kernel to tuples of real values we replace the exact match with the standard product13. The kernel on the tuple then becomes:\n\u03batuple(v, v \u2032) = \u2211 c propc(v) \u00b7 propc(v\u2032). (13)\nHard match for real tuples. We proceed in an analogous fashion as for the hard match for discrete tuples, that is, we replace the label `(v) in Equation 10 with the labeling procedure Lv. In this case, however, we combine the real valued tuple of corresponding vertices with the standard product as in Equation 13:\n\u03batuple(v, v \u2032) = \u2211 c propc(v) \u00b7 propc(v\u2032). (14)\nSoft match for mixed discrete and real tuples. When dealing with tuples of mixed discrete and real values, the contribution of the kernels on\n13Note that this is equivalent to collecting all numerical properties of a vertex\u2019s tuple in a vector and then employ the standard dot product between vectors.\nthe separate collections of discrete and real attributes are combined via summation:\n\u03batuple(v, v \u2032) = \u2211 d 1propd(v)=propd(v\u2032) + \u2211 c propc(v) \u00b7 propc(v\u2032) (15)\nwhere indices d and c run exclusively over the discrete and continous properties respectively. Hard match for mixed discrete and real tuples. In an analogous fashion, provided that `(v) in Equation 10 is replaced with the labeling procedure Lv (see Section 6.2.3) we have:\n\u03batuple(v, v \u2032) = \u220f d 1propd(v)=propd(v\u2032) + \u2211 c propc(v) \u00b7 propc(v\u2032). (16)\nIn this way, each vertex receives a canonical label that uniquely identifies it in the neighborhood graph. The discrete labels of corresponding vertices are concatenated and matched for identity, while the real tuples of corresponding vertices are combined via the standard dot product."}, {"heading": "6.2.6. Domain knowledge bias via kernel points", "text": "At times it is convenient, for efficiency reasons or to inject domain knowledge into the kernel, to be able to explicitly select the neighborhood subgraphs. We provide a way to do so, declaratively, by introducing the set of kernel points, a subset of V (G) which includes all vertices associated with ground atoms of some specially marked signatures. We then redefine the relation Rr,d(A,B,G) used in Equation 4 like in Section 6.2.1 but with the additional constraints that the roots of A and B be kernel points.\nKernel points are typically vertices that are believed to represent information of high importance for the task at hand. Vertices that are not kernel points contribute to the kernel computation only when they occur in the neighborhoods of kernel points. In kLog, kernel points are declared as a list of domain relations: all vertices that correspond to ground atoms of these relations become kernel points."}, {"heading": "6.3. Viewpoints", "text": "The above approach effectively defines a kernel over interpretations\nK(z, z\u2032) = K(Gz, Gz\u2032)\nwhere Gz is the result of graphicalization applied to interpretation z. For learning jobs such as classification or regression on interpretations (see Table 2), this kernel is directly usable in conjunction with plain kernel machines like SVM. When moving to more complex jobs involving, e.g., classification of entities or tuples of entities, the kernel induces a feature vector \u03c6(x, y) suitable for the application of a structured output technique where f(x) = argmaxy w\n>\u03c6(x, y). Alternatively, we may convert the structured output problem into a set of independent subproblems as follows. For simplicity, assume the learning job consists of a single relation r of relational arity n. We call each ground atom of r a case. Intuitively, cases correspond to training targets or prediction-time queries in supervised learning. Usually an interpretation contains several cases corresponding to specific entities such as individual Web pages (as in Section 5.3) or movies (as in Section 5.4), or tuples of entities for link prediction problems (as in Section 2). Given a case c \u2208 y, the viewpoint of c, Wc, is the set of vertices that are adjacent to c in the graph. In order to define the kernel, we first consider the mutilated graph Gc where all vertices in y, except c, are removed (see Figure 5 for an illustration). We then define a kernel \u03ba\u0302 on mutilated graphs, following the same approach of the NSPDK, but with the additional constraint that the first endpoint must be in Wc. The decomposition is thus defined as\nR\u0302r,d = {(A,B,Gc) : N vr , Nur , v \u2208 Wc, d?(u, v) = d}.\nWe obtain in this way a kernel \u201ccentered\u201d around case c:\nK\u0302(Gc, G \u2032 c\u2032) = \u2211 r,d \u2211 A,B \u2208 R\u0302\u22121r,d(Gc) A\u2032, B\u2032 \u2208 R\u0302\u22121r,d(G \u2032 c\u2032 ) \u03b4(A,A\u2032)\u03b4(B,B\u2032)\nand finally we let K(G,G\u2032) = \u2211 c\u2208y,c\u2032\u2208y\u2032 K\u0302(Gc, G \u2032 c\u2032).\nThis kernel corresponds to the potential\nF (x, y) = w> \u2211 c \u03c6\u0302(x, c)\nwhich is clearly maximized by maximizing, independently, all sub-potentials w>\u03c6\u0302(x, c) with respect to c.\nBy following this approach, we do not obtain a collective prediction (individual ground atoms are predicted independently). Still, even in this reduced setting, the kLog framework can be exploited in conjunction with metalearning approaches that surrogate collective prediction. For example, Prolog predicates in intensional signatures can effectively be used as expressive relational templates for stacked graphical models [48] where input features for one instance are computed from predictions on other related instances. Results in Section 7.2 for the \u201cpartial information\u201d setting are obtained using a special form of stacking."}, {"heading": "6.4. Parameterization in kLog", "text": "The kernels presented in this section, together with the graphicalization procedure, yield a statistical model working in a potentially very highdimensional feature space. Although large-margin learners offer some robustness against high-dimensional representations, it is still the user\u2019s responsibility to choose appropriate kernel parameters (r\u2217 and d\u2217) to avoid overfitting. It should be noted that subgraphs in the NSPDK effectively act like templates which are matched against graphicalized interpretations for new data. Since identifiers themselves do not appear in the result of the graphicalization procedure, the same template can match multiple times realizing a form of parameter tying as in other statistical relational learning methods."}, {"heading": "7. kLog in practice", "text": "In this section, we illustrate the use of kLog in a number of application domains. All experimental results reported in this section were obtained using LibSVM in binary classification, multiclass, or regression mode, as appropriate."}, {"heading": "7.1. Predicting a single property of one interpretation", "text": "We now expand the ideas outlined in Example 5.1. Predicting the biological activity of small molecules is a major task in chemoinformatics and can help drug development [49] and toxicology [50, 51]. Most existing graph kernels have been tested on data sets of small molecules (see, e.g., [37, 38, 39, 41, 52]). From the kLog perspective the data consists of several interpretations, one for each molecule. In the case of binary classification (e.g., active vs. nonactive), there is a single target predicate whose truth state corresponds to the class of the molecule. To evaluate kLog we used two data sets. The Bursi data set [53] consists of 4,337 molecular structures with associated mutagenicity labels (2,401 mutagens and 1,936 nonmutagens) obtained from a short-term in vitro assay that detects genetic damage. The Biodegradability data set [54] contains 328 compounds and the regression task is to predict their half-life for aerobic aqueous biodegradation starting from molecular structure and global molecular measurements.\nListing 4 shows a kLog script for the Bursi domain. Relevant predicates in the extensional database are a/2, b/3 (atoms and bonds, respectively, extracted from the chemical structure), sub/3 (functional groups, computed by DMax Chemistry Assistant [55, 56]), fused/3, connected/4 (direct connection between two functional groups), linked/4 (connection between functional groups via an aliphatic chain). Aromaticity (used in the bond-type property of b/3) was also computed by DMax Chemistry Assistant. The intensional signatures essentially serve the purpose of simplifying the original data representation. For example atm/2 omits some entities (hydrogen atoms), and fg_fused/3 replaces a list of atoms by its length. In signature bnd(atom_1@b::atm,atom_2@b::atm) we use a role field b (introduced by the symbol @). Using the same role twice declares that the two atoms play the same role in the chemical bond, i.e. that the relation is symmetric. In this way, each bond can be represented by one tuple only, while a more traditional relational representation, which is directional, would require two tuples. While this may at first sight appear to be only syntactic sugar, it\n1 begin_domain. 2 signature atm(atom_id::self , element::property )::intensional. 3 atm(Atom, Element) :- a(Atom,Element), \\+(Element=h).\n4 signature bnd(atom_1@b::atm, atom_2@b::atm, type::property )::intensional. 5 bnd(Atom1,Atom2,Type) :- 6 b(Atom1,Atom2,NType), describeBondType(NType,Type), 7 atm(Atom1,_), atm(Atom2,_).\n8 signature fgroup(fgroup_id::self , group_type::property )::intensional. 9 fgroup(Fg,Type) :- sub(Fg,Type,_).\n10 signature fgmember(fg::fgroup, atom::atm)::intensional. 11 fgmember(Fg,Atom):- subat(Fg,Atom,_), atm(Atom,_).\n12 signature fg_fused(fg1@nil::fgroup, fg2@nil::fgroup, nrAtoms::property )::intensional. 13 fg_fused(Fg1,Fg2,NrAtoms):- fused(Fg1,Fg2,AtomList), length(AtomList,NrAtoms).\n14 signature fg_connected(fg1@nil::fgroup, fg2@nil::fgroup, 15 bondType::property )::intensional. 16 fg_connected(Fg1,Fg2,BondType):17 connected(Fg1,Fg2,Type,_AtomList),describeBondType(Type,BondType).\n18 signature fg_linked(fg::fgroup, alichain::fgroup, saturation::property )::intensional. 19 fg_linked(FG,AliChain,Sat) :- 20 linked(AliChain,Links,_BranchesEnds,Saturation), 21 ( Saturation = saturated -> Sat = saturated ; Sat = unsaturated ), 22 member(link(FG,_A1,_A2),Links).\n23 signature mutagenic::extensional. 24 end_domain.\nListing 4: Complete specification of the Bursi domain.\ndoes provide extra abilities for modeling which is important in some domains. For instance, when modeling ring-structures in molecules, traditional logical and relational learning systems need to employ either lists to capture all the elements in a ring structure, or else need to include all permutations of the atoms participating in a ring structure. For rings involving 6 atoms, this requires 6!=720 different tuples, an unnecessary blow-up. Also, working with lists typically leads to complications such as having to deal with a potentially infinite number of terms. The target relation mutagenic has relational arity zero since it is a property of the whole interpretation.\nAs shown in Table 3, results are relatively stable with respect to the choice of kernel hyperparameter (maximum radius and distance) and SVM regularization and essentially match the best results reported in [11] (AUROC 0.92\u00b10.02) even without composition with a polynomial kernel. These results are not surprising since the graphs generated by kLog are very similar in this case to the expanded molecular graphs used in [11].\nWe compared kLog to Tilde on the same task. A kLog domain specification can be trivially ported to background knowledge for Tilde. Both systems can then access the same set of Prolog atoms. Table 4 summarizes the results and can be compared with Table 3. Augmenting the language with the functional groups from [56] unexpectedly gave worse results in Tilde compared to a plain atom-bond language. The presence of some functional groups correlates well with the target and hence those tests are used near the root of the decision tree. Unfortunately, the greedy learner is unable to refine its hypothesis down to the atom level and relies almost exclusively on the coarse-grained functional groups. Such local-optimum traps can sometimes be escaped from by looking ahead further, but this is expensive and Tilde ran out of memory for lookaheads higher than 1. Tilde\u2019s built-in bagging can boost its results, especially when functional groups are used. A comparison between Tables 4 and 3 shows that Tilde with our language bias definition is not well suited for this problem.\nThe kLog code for biodegradability is similar to Bursi but being a regression task we have a target relation declared as\nsignature biodegradation(halflife::property)::extensional.\nWe estimated prediction performance by repeating five times a ten-fold cross validation procedure as described in [54] (using exactly the same folds in each trial). Results \u2014 rooted mean squared error (RMSE), squared correlation coefficient (SCC), and mean absolute percentage error (MAPE) \u2014 are reported in Table 5. For comparison, the best RMSE obtained by kFOIL on this data set (and same folds) is 1.14 \u00b1 0.04 (kFOIL was shown to outperform Tilde and S-CART in [5])."}, {"heading": "7.2. Link prediction", "text": "We report here experimental results on the UW-CSE domain that we have already extensively described in Section 2. To assess kLog behavior we evaluated prediction accuracy according to the leave-one-research-group-out setup of [7], using the domain description of Listings 2 and 3, together with a NSPDK kernel with distance 2, radius 2, and soft match. Comparative results with respect to Markov logic are reported in Figure 6 (MLN results published in [7]). The whole 5-fold procedure runs in about 20 seconds on a single core of a 2.5GHz Core i7 CPU. Compared to MLNs, kLog in the current implementation has the disadvantage of not performing collective assignment but the advantage of defining more powerful features thanks to the graph kernel. Additionally, MLN results use a much larger knowledge base. The advantage of kLog over MLN in Figure 6 is due to the more powerful feature space. Indeed, when setting the graph kernel distance and radius to 0 and 1, respectively, the feature space has just one feature for each ground signature, in close analogy to MLN. The empirical performance (area under recallprecision curve, AURPC) of kLog using the same set of signatures drops dramatically from 0.28 (distance 2, radius 2) to 0.09 (distance 1, radius 0).\nIn a second experiment, we predicted the relation advised_by starting from partial information (i.e., when relations Student (and its complement\nProfessor) are unknown, as in [7]). In this case, we created a pipeline of two predictors. Our procedure is reminiscent of stacked generalization [57]. In the first stage, a leave-one-research-group-out cross-validation procedure was applied to the training data to obtain predicted groundings for Student (a binary classification task on entities). Predicted groundings were then fed to the second stage which predicts the binary relation advised_by. The overall procedure was repeated using one research group at the time for testing. Results are reported in Figure 7 (MLN results published in [7]).\nSince kLog is embedded in the programming language Prolog, it is easy to use the output of one learning task as the input for the next one as illustrated in the pipeline. This is because both the inputs and the outputs are relations. Relations are treated uniformly regardless of whether they are defined intensionally, extensionally, or are the result of a previous learning run. Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62]. Such programming languages possess \u2014 in addition to the usual constructs \u2014 also primitives for learning, that is, to specify the inputs and the outputs of the learning problems. In this way, they support the development of software in which machine learning is embedded without requiring the developer to be a machine learning expert. According to Mitchell [60], the development of such languages is a long outstanding research question."}, {"heading": "7.3. Entity classification", "text": "The WebKB data set [27] has been widely used to evaluate relational methods for text categorization. It consists of academic Web pages from four computer science departments and the task is to identify the category (such as student page, course page, professor page, etc). Figure 8 shows the E/R diagram used in kLog. One of the most important relationships in this domain is has, that associates words to web pages. After graphicalization, vertices representing webpages have large degree (at least the number of words), making the standard NSPDK of [11] totally inadequate: even by setting the maximum distance d\u2217 = 1 and the maximum radius r\u2217 = 2, the hard match would essentially create a distinct feature for every page. In this domain we can therefore appreciate the flexibility of the kernel defined in Section 6.2. In particular, the soft match kernel creates histograms of word occurrences in the page, which is very similar to the bag-of-words (with counts)\nrepresentation that is commonly used in text categorization problems. The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.\nEmpirical results using only four Universities (Cornell, Texas, Washington, Wisconsin) in the leave-one-university-out setup are reported in Table 6. We compared kLog to MLN and to Tilde on the same task. For MLN we used the Alchemy system and the following set of formulae, which essentially encode the same domain knowledge exploited in kLog:\n1 Has(+w,p) => Topic(+t,p) 2 !Has(+w,p) => Topic(+t,p) 3 CsInUrl(p) => Topic(+t,p) 4 !CsInUrl(p) => Topic(+t,p) 5 Topic(+c1, p1) ^ (Exist a LinkTo(a, p1, p2)) => Topic(+c2, p2) 6 HasAnchor(+w,a) ^ LinkTo(a, p1, p2) => Topic(+c, p2)\nGround atoms for the predicate LinkTo were actually precalculated externally in Prolog (same code as for the kLog\u2019s intensional signature) since regular expressions are not available in Alchemy. We did not enforce mutually exclusive categories since results tended to be worse. For learning we used the preconditioned scaled conjugate gradient approach described in [63] and we\ntried a wide range of values for the learning rate and the number of iterations. The best results, reported in Table 6, used the trick of averaging MLN weights across all iterations as in [63]. MC-SAT inference was used during prediction. In spite of the advantage of MLN for using a collective inference approach, results are comparable to those obtained with kLog (MLN tends to overpredict the class \u201cstudent\u201d, resulting in a slightly lower average F1 measure, but accuracies are identical). Thus the feature extracted by kLog using the graph kernel are capable of capturing enough contextual information from the input portion of the data to obviate the lack of collective inference.\nIn the case of Tilde, we used the following language bias:\n1 rmode((has(+U,Word,+Page), Word = #Word)). 2 rmode((cs_in_url(+U,+Page,V), V = #V)). 3 rmode((link_to(+U,-Link,-From,+Page), has_anchor(+U,Word,Link), Word = #Word)).\nResults are reported in Table 8 and are slightly worse than those attained by kLog and MLN.\nAlthough the accuracies of the three methods are essentially comparable, their requirements in terms of CPU time are dramatically different: using a single core of a second generation Intel Core i7, kLog took 36s, Alchemy 27,041s (for 100 iterations, at which the best accuracy is attained), and Tilde: 5,259s."}, {"heading": "7.4. Domains with a single interpretation", "text": "The Internet Movie Database (IMDb) collects information about movies and their cast, people, and companies working in the motion picture industry. We focus on predicting, for each movie, whether its first weekend box-office receipts are over US$2 million, a learning task previously defined in [30, 64]. The learning setting defined so far (learning from independent interpretations) is not directly applicable since train and test data must necessarily occur within the same interpretation. The notion of slicing in kLog allows us to overcome this difficulty. A slice system is a partition of the true ground atoms in a given interpretation: z = {z(i1), . . . , z(in)} where the disjoint sets z(j) are called slices and the index set I = {i1, . . . , in} is endowed with a total order . For example, a natural choice for I in the IMDb domain is the set of movie production years (e.g., {1996, . . . , 2005}), where the index associated with a ground atom of an entity such as actor is the debut year.\nIn this way, given two disjoint subsets of I, T and S, such thatmax (T ) min (S), it is reasonable during training to use for some index t \u2208 T \\ {min (T )} the set of ground atoms {x(i) : i \u2208 T \u2227 i t}t \u222a {y(i) : i \u2208 T \u2227 i \u227a t}t (where i \u227a t iff i t and i 6= t) as the input portion of the data, and {y(t)}t as the output portion (targets). Similarly, during testing we can for each s \u2208 S use the set of ground atoms {x(i) : i \u2208 S \u2227 i s}s \u222a {y(i) : i \u2208 S \u2227 i \u227a s}s for predicting {y(s)}s.\nThe kLog data set was created after downloading the whole database from http://www.imdb.com. Adult movies, movies produced outside the US, and movies with no opening weekend data were discarded. Persons and companies with a single appearance in this subset of movies were also discarded. The resulting data set is summarized in Table 9. We modeled the domain in kLog using extensional signatures for movies, persons (actors, producers, directors), and companies (distributors, production companies, special effects companies). We additionally included intensional signatures counting, for each movie the number of companies involved also in other blockbuster movies. We sliced the data set according to production year, and starting from year y = 1997, we trained on the frame {y\u2212 1, y\u2212 2} and tested on the frame {y}. Results (area under the ROC curve) are summarized in Table 9, together with comparative results against MLN and Tilde. In all three cases we used the same set of ground atoms for training and testing, but in the case of MLN the Alchemy software does not differentiate between evidence and query ground atoms of the same predicate. We therefore introduced an extra predicate called PrevBlockbuster to inject evidence for years {y\u22123, y\u22124, . . . }, together with the hard rule\n1 PrevBlockbuster(m) <=> Blockbuster(m).\nand used Blockbuster as the query predicate when training on years {y \u2212 1, y \u2212 2} and testing on year y. MLN rules were designed to capture past performance of actors, directors, distribution companies, etc. For example:\n1 ActedIn(p,m1) ^ ActedIn(p,m2) ^ Blockbuster(m1) ^ m1 != m2 => Blockbuster(m2)\nIn the case of Tilde, we used as background knowledge exactly the same intensional signature definitions used in kLog. The MLN was trained using the discriminative weight learning algorithm implemented in Alchemy. MCMC (Gibbs sampling) was used during prediction to obtain probabilities for all query atoms.\nOn this data set, Tilde was the fastest system, completing all training and test phases in 220s, followed by kLog (1,394s) and Alchemy (12,812s). However, the AUC obtained by kLog is consistently higher across all prediction years."}, {"heading": "8. Related work", "text": "As kLog is a language for logical and relational learning with kernels it is related to work on inductive logic programming, to statistical relational learning, to graph kernels, and to propositionalization. We now discuss each of these lines of work and their relation to kLog.\nFirst, the underlying representation of the data that kLog employs at the first level is very close to that of standard inductive logic programming systems such as Progol [65], Aleph [66], and Tilde [67] in the sense that the input is essentially (a variation of) a Prolog program for specifying the data and the background knowledge. Prolog allows us to encode essentially any program as background knowledge. The E/R model used in kLog is related to the Probabilistic Entity Relationship models introduced by Heckerman et al. in [10]. The signatures play a similar role as the notion of a declarative bias in inductive logic programming [9]. The combined use of the E/R model and the graphicalization has provided us with a powerful tool for visualizing both the structure of the data (the E/R diagram) as well as specific cases (through\ntheir graphs). This has proven to be very helpful when preparing datasets for kLog. On the other hand, due to the adoption of a database framework, kLog forbids functors in the signature relations (though functors can be used inside predicates needed to compute these relations inside the background knowledge). This contrasts with some inductive logic programming systems such as Progol [65] and Aleph [66].\nSecond, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions. What kLog and statistical relational learning techniques have in common is that they both construct (implicitly or explicitly) graphs representing the instances. For statistical relational learning methods such as Markov logic [7], probabilistic relational models [8], and Bayesian logic programs [69] the knowledge-based model construction process will result in a graphical model (Bayesian or Markov network) for each instance representing a class of probability distributions, while in kLog the process of graphicalization results in a graph representing an instance by unrolling the E/R-diagram. Statistical relational learning systems then learn a probability distribution using the features and parameters in the graphical model, while kLog learns a function using the features derived by the kernel from the graphs. Notice that in both cases, the resulting features are tied together. Indeed, in statistical relational learning each ground instance of a particular template or expression that occurs in the graph has the same parameters. kLog features correspond to subgraphs that represent relational templates and that may match (and hence be grounded) multiple times in the graphicalization. As each such feature has a single weight, kLog also realizes parameter tying in a similar way as statistical relational learning methods. One difference between these statistical relational learning models and kLog is that the former do not really have a second level as does kLog. Indeed, the knowledge base model construction process directly generates the graphical model that includes all the features used for learning, while in kLog these features are derived from the graph kernel. While statistical relational learning systems have been commonly used for collective learning, this is still a question for further research within kLog. A combination of structured-output learning [12] and iterative approaches (as incorporated in the EM algorithm) can form the basis for further work in\nthis direction. Another interesting but more speculative direction for future work is concerned with lifted inference. Lifted inference has been the focus of a lot of attention in statistical relational learning; see [71] for an overview. One view on lifted inference is that it is trying to exploit symmetries in the graphical models that would be the result of the knowledge based model construction step, e.g., [72]. From this perspective, it might be interesting to explore the use of symmetries in graphs and features constructed by kLog.\nkLog builds also upon the many results on learning with graph kernels, see [73] for an overview. A distinguishing feature of kLog is, however, that the graphs obtained by graphicalizing a relational representation contain very rich labels, which can be both symbolic and numeric. This contrasts with the kind of graphs needed to represent for instance small molecules. In this regard, kLog is close in spirit to the work of [74], who define a kernel on hypergraphs, where hypergraphs are used to represent relational interpretations. A distinctive feature of kLog is automatic graphicalization of relational representations, which also allows users to naturally specify multitask and collective learning tasks.\nThe graphicalization approach introduced in kLog is closely related to the notion of propositionalization, a commonly applied technique in logical and relational learning [75, 9] to generate features from a relational representation. The advantage of graphicalization is that the obtained graphs are essentially equivalent to the relational representation and that \u2014 in contrast to the existing propositionalization approaches in logical and relational learning \u2014 this does not result in a loss of information. After graphicalization, any graph kernel can in principle be applied to the resulting graphs. Even though many of these kernels (such as the one used in kLog) compute \u2014 implicitly or explicitly \u2014 a feature vector, the dimensionality of the obtained vector is far beyond that employed by traditional propositionalization approaches. kFOIL [5] is one such propositionalization technique that has been tightly integrated with a kernel-based method. It greedily derives a (small) set of features in a way that resembles the rule-learning algorithm of FOIL [76].\nSeveral other approaches to relational learning and mining have employed graph-based encodings of the relational data, e.g., [77, 78, 79, 80]. kLog encodes a set of ground atoms into a bipartite undirected graph whose nodes are true ground atoms and whose edges connect an entity atom to a relationship atom if the identifier of the former appears as an argument in the latter. This differs from the usual encoding employed in graph-based approaches to relational learning and mining, which typically use labeled edges to directly\nrepresent the relationships between the nodes corresponding to the entities. Furthermore, these approaches typically use the graph-based representation as the single representation, and unlike kLog do neither consider the graphbased representation as an intermediate representation nor work at three levels of representation (logical, graph-based and feature-based).\nOther domain specific languages for machine learning have been developed whose goals are closely related to those of kLog. Learning based Java [62] was designed to specifically address applications in natural language processing. It builds on the concept of data-driven compilation to perform feature extraction and nicely exploits the constrained conditional model framework [81] for structured output learning. FACTORIE [82] allows users to concisely define features used in a factor graph and, consequently, arbitrarily connected conditional random fields. Like with MLN, there is an immediate dependency of the feature space on the sentences of the language, whereas in kLog this dependency is indirect since the exact feature space is eventually defined by the graph kernel."}, {"heading": "9. Conclusions", "text": "We have introduced a novel language for logical and relational learning called kLog. It tightly integrates logical and relational learning with kernel methods and constitutes a principled framework for statistical relational learning based on kernel methods rather than on graphical models. kLog uses a representation that is based on E/R modeling, which is close to representations being used by contemporary statistical relational learners. kLog first performs graphicalization, that is, it computes a set of labeled graphs that are equivalent to the original representation, and then employs a graph kernel to realize statistical learning. We have shown that the kLog framework can be used to formulate and address a wide range of learning tasks, that it performs at least comparably to state-of-the-art statistical relational learning techniques, and also that it can be used as a programming language for machine learning.\nThe system presented in this paper is a first step towards a kernel-based language for relational learning but there are unanswered questions and interesting open directions for further research. One important aspect is the possibility of performing collective classification (or structured output prediction). Structured output learning problems can be naturally defined within kLog\u2019s semantics: graphicalization followed by a graph kernel yields a joint\nfeature vector \u03c6(x, y) where y are the groundings of the output predicates. Collective prediction amounts to maximizing w>\u03c6(x, y) with respect to y. There are two important cases to consider. If groundings in y do not affect the graph structure (because they never affect the intensional signatures) then the collective classification problem is not more complicated than in related SRL systems. For example, if dependencies have a regular sequential structure, dynamic programming can be used for this step, exactly as in conditional random fields (indeed, collective classification has been succesfully exploited within kLog in an application to natural language test segmentation [83]). However, in general, changing y during search will also change the graph structure. In principle it is feasible to redo graphicalization from scratch during search, apply the graph kernel again and eventually evaluate w>\u03c6(x, y), but such a naive approach would of course be very inefficient. Developing clever and faster algorithms for this purpose is an interesting open issue. It should be remarked, however, that even without collective classification, kLog achieves good empirical results thanks to the fact that features produced by the graph kernel provide a wide relational context. Better understanding of generalization for structured prediction models has begun to emerge (see [84] and references therein) and a theoretical analysis of learning within the present kLog setting is another potential direction for future research.\nThe graph kernel that is currently employed in kLog makes use of the notion of topological distances to define the concept of neighborhoods. In this way, given a predicate of interest, properties of \u201cnearby\u201d tuples are combined to generate features relevant for that predicate. As a consequence, when topological distances are not informative (e.g., in the case of dense graphs with small diameter) then large fractions of the graph become accessible to any neighborhood and the features induced for a specific predicate cease to be discriminative. In these cases (typical when dealing with small-world networks), kernels with a different type of bias (e.g., flow-based kernels) are more appropriate. The implementation of a library of kernels suitable for different types of graphs, as well as the integration of other existing graph kernels (such as [40, 41, 42]) in the kLog framework, is therefore an important direction for future development.\nFurthermore, even though kLog\u2019s current implementation is quite performant, there are interesting implementation issues to be studied. Many of these are similar to those employed in statistical relational learning systems such as Alchemy [7] and ProbLog [85].\nkLog is being actively used for developing applications. We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90]."}, {"heading": "Acknowledgments", "text": "We thank Tom Schrijvers for his dependency analysis code. PF was supported partially by KU Leuven SF/09/014 and partially by Italian Ministry of University and Research PRIN 2009LNP494. KDG was supported partially by KU Leuven GOA/08/008 and partially by ERC Starting Grant 240186 \u201cMiGraNT\u201d. Finally, we would like to thank the anonymous reviewers and the associate editor for their very useful and constructive comments."}, {"heading": "Appendix A. Syntax of the kLog domain declaration section", "text": "A kLog program consists of Prolog code augmented by a domain declaration section delimited by the pair of keywords begin_domain and end_domain and one or more signature declarations. A signature declaration consists of a signature header followed by one or more Prolog clauses. Clauses in a signature declaration form the declaration of signature predicates and are automatically connected to the current signature header. There are a few signature predicates with a special meaning for kLog, as discussed in this section. A brief BNF description of the grammar of kLog domains is given in Figure A.9.\nAdditionally, kLog provides a library of Prolog predicates for handling data, learning, and performance measurement."}, {"heading": "Appendix B. Definitions", "text": "For the sake of completeness we report here a number of graph theoretical definitions used in the paper. We closely follow the notation in [91]. A graph G = (V,E) consists of two sets V and E. The notation V (G) and E(G) is\nused when G is not the only graph considered. The elements of V are called vertices and the elements of E are called edges. Each edge has a set of two elements in V associated with it, which are called its endpoints, which we denote by concatenating the vertices variables, e.g. we represent the edge between the vertices u and v with uv. In this case we say that u is adjacent to v and that uv is incident on u and v. The degree of a vertex is number of edges incident on it. A graph is bipartite if its vertex set can be partitioned into two subsets X and Y so that every edge has one end in X and the other in Y . A graph is rooted when we distinguish one of its vertices, called root. The neighborhood of a vertex v is the set of vertices that are adjacent to v and is indicated with N(v). The neighborhood of radius r of a vertex v is the set of vertices at a distance less than or equal to r from v and is denoted by Nr(v). In a graph G, the induced subgraph on a set of vertices W = {w1, . . . , wk} is a graph that has W as its vertex set and it contains every edge of G whose endpoints are in W . The neighborhood subgraph of radius r of vertex v is the subgraph induced by the neighborhood of radius r of v and is denoted by N vr . A labeled graph is a graph whose vertices and/or edges are labeled, possibly with repetitions, using symbols from a finite alphabet. We denote the function that maps the vertex/edge to the label symbol as `. Two simple graphs G1 = (V1, E1) and G2 = (V2, E2) are isomorphic, which we denote by G1 ' G2, if there is a bijection \u03c6 : V1 \u2192 V2, such that for any two vertices u, v \u2208 V1, there is an edge uv if and only if there is an edge \u03c6(u)\u03c6(v) in G2. An isomorphism is a structure-preserving bijection. Two labeled graphs are isomorphic if there is an isomorphism that preserves also the label information, i.e. `(\u03c6(v)) = `(v). An isomorphism invariant or graph invariant is a graph property that is identical for two isomorphic graphs (e.g. the number of vertices and/or edges). A certificate for isomorphism is an isomorphism invariant that is identical for two graphs if and only if they are isomorphic."}, {"heading": "Appendix C. Decomposition kernels", "text": "We follow the notation in [92]. Given a setX and a functionK : X\u00d7X \u2192 R, we say that K is a kernel on X \u00d7X if K is symmetric, i.e. if for any x and y \u2208 X K(x, y) = K(y, x), and if K is positive-semidefinite, i.e. if for any N \u2265 1 and any x1, . . . , xN \u2208 X, the matrix K defined by Kij = K(xi, xj) is positive-semidefinite, that is \u2211 ij cicjKij \u2265 0 for all c1, . . . , cN \u2208 R or equivalently if all its eigenvalues are nonnegative. It is easy to see that if\neach x \u2208 X can be represented as \u03c6(x) = {\u03c6n(x)}n\u22651 such that K is the ordinary l2 dot product K(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009 = \u2211 n \u03c6n(x)\u03c6n(y) then K is a kernel. The converse is also true under reasonable assumptions (which are almost always verified) on X and K, that is, a given kernel K can be represented as K(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009 for some choice of \u03c6. In particular it holds for any kernel K over X \u00d7X where X is a countable set. The vector space induced by \u03c6 is called the feature space. Note that it follows from the definition of positive-semidefinite that the zero-extension of a kernel is a valid kernel, that is, if S \u2286 X and K is a kernel on S\u00d7S then K may be extended to be a kernel on X \u00d7X by defining K(x, y) = 0 if x or y is not in S. It is easy to show that kernels are closed under summation, i.e. a sum of kernels is a valid kernel.\nLet now x \u2208 X be a composite structure such that we can define x1, . . . , xD as its parts14. Each part is such that xd \u2208 Xd for d = 1, . . . , D with D \u2265 1 where each Xd is a countable set. Let R be the relation defined on the set X1 \u00d7 . . .\u00d7XD \u00d7X, such that R(x1, . . . , xD, x) is true iff x1, . . . , xD are the parts of x. We denote with R\u22121(x) the inverse relation that yields the parts of x, that is R\u22121(x) = {x1, . . . , xD : R(x1, . . . , xD, x)}. In [92] it is demonstrated that, if there exist a kernel Kd over Xd \u00d7 Xd for each d = 1, . . . , D, and if two instances x, y \u2208 X can be decomposed in x1, . . . , xd and y1, . . . , yd, then the following generalized convolution:\nK(x, y) = \u2211\nx1, . . . , xm \u2208 R\u22121(x) y1, . . . , ym \u2208 R\u22121(y)\nM\u220f m=1 Km(xm, ym) (C.1)\nis a valid kernel called a convolution or decomposition kernel15. In words: a decomposition kernel is a sum (over all possible ways to decompose a structured instance) of the product of valid kernels over the parts of the instance."}, {"heading": "Appendix D. Graph invariant pseudo-identifier computation", "text": "We obtain the pseudo-identifier of a rooted neighborhood graph Gh by first constructing a graph invariant encoding Lg(Gh). Then we apply a hash\n14Note that the set of parts needs not be a partition for the composite structure, i.e. the parts may \u201coverlap\u201d.\n15To be precise, the valid kernel is the zero-extension of K to X \u00d7 X since R\u22121(x) is not guaranteed to yield a non-empty set for all x \u2208 X.\nfunction H(Lg(Gh))\u2192 N to the encoding. The algorithm was first described in [11] but we present it again here for self-sufficiency. Figure D.10 shows an overview.\nNote that we cannot hope to exhibit an efficient certificate for isomorphism in this way, and in general there can be collisions between two nonisomorphic graphs, either because these are assigned the same encoding or because the hashing procedure introduces a collision even when the encodings are different.\nHashing is not a novel idea in machine learning; it is commonly used, e.g., for creating compact representations of molecular structures [38], and has been advocated as a tool for compressing very high-dimensional feature spaces [93]. In the present context, hashing is mainly motivated by the computational efficiency gained by approximating the isomorphism test.\nThe graph encoding Lg(Gh) that we propose is best described by introducing two new label functions: one for the vertices and one for the edges, denoted Lv and Le respectively. Lv(v) assigns to vertex v a lexicographically sorted sequence of pairs composed by a topological distance and a vertex label, that is, Lv(v) returns a sorted list of pairs \u3008D(v, u), `(u)\u3009 for all u \u2208 Gh. Moreover, since Gh is a rooted graph, we can use the knowledge about the identity of the root vertex h and prepend to the returned list the additional\ninformation of the distance from the root node D(v, h). The new edge label is produced by composing the new vertex labels with the original edge label, that is Le(uv) assigns to edge uv the triplet \u3008Lv(u),Lv(v), `(uv)\u3009. Finally Lg(Gh) assigns to the rooted graph Gh the lexicographically sorted list of Le(uv) for all uv \u2208 E(Gh). In words: we relabel each vertex with a sequence that encodes the vertex distance from all other (labeled) vertices (plus the distance from the root vertex); the graph encoding is obtained as the sorted edge list, where each edge is annotated with the endpoints\u2019 new labels. For a proof that Lg(Gh) is graph invariant, see [94, p. 53].\nWe finally resort to a Merkle-Damg\u00e5rd construction-based hashing function for variable-length data to map the various lists to integers, that is, we map the distance-label pairs, the new vertex labels, the new edge labels and the new edge sequences to integers (in this order). Note that it is trivial to control the size of the feature space by choosing the hash codomain size (or alternatively the bit size for the returned hashed values). Naturally there is a tradeoff between the size of the feature space and the number of hash collisions."}], "references": [{"title": "Structured machine learning: the next ten years", "author": ["T. Dietterich", "P. Domingos", "L. Getoor", "S. Muggleton", "P. Tadepalli"], "venue": "Mach Learn 73 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "R", "author": ["L. De Raedt", "B. Demoen", "D. Fierens", "B. Gutmann", "G. Janssens", "A. Kimmig", "N. Landwehr", "T. Mantadelis", "W. Meert"], "venue": "Rocha, et al., Towards digesting the alphabet-soup of statistical relational learning ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "S", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting"], "venue": "Muggleton (Eds.), Probabilistic inductive logic programming: theory and applications, Vol. 4911 of Lecture notes in computer science, Springer, Berlin", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "B", "author": ["L. Getoor"], "venue": "Taskar (Eds.), Introduction to statistical relational learning, MIT Press, Cambridge, Mass.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast learning of relational kernels", "author": ["N. Landwehr", "A. Passerini", "L. De Raedt", "P. Frasconi"], "venue": "Machine learning 78 (3) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Max-margin Markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "in: S. Thrun, L. Saul, B. Sch\u00f6lkopf (Eds.), Proceedings of Neural Information Processing Systems, MIT Press, Cambridge, MA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "in: International Joint Conference on Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Logical and relational learning", "author": ["L. De Raedt"], "venue": "1st Edition, Cognitive technologies, Springer, New York", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic entity-relationship models", "author": ["D. Heckerman", "C. Meek", "D. Koller"], "venue": "PRMs, and plate models, in: L. Getoor, B. Taskar (Eds.), Introduction to Statistical Relational Learning, The MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast neighborhood subgraph pairwise distance kernel", "author": ["F. Costa", "K. De Grave"], "venue": "in: Proc. of the 27th International Conference on Machine Learning ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research 6 (2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "On discriminative vs", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes, in: T. Dietterich, S. Becker, Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems (NIPS) 14", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "An introduction to conditional random fields for relational learning", "author": ["C. Sutton", "A. McCallum"], "venue": "in: Getoor and Taskar", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Hidden markov support vector machines", "author": ["Y. Altun", "I. Tsochantaridis", "T. Hofmann"], "venue": "in: Twentieth International Conference on Machine Learning ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Applications of stochastic context-free grammars using the inside-outside algorithm", "author": ["K. Lari", "S. Young"], "venue": "Computer speech & language 5 (3) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Stochastic logic programs", "author": ["S. Muggleton"], "venue": "in: L. De Raedt (Ed.), Advances in Inductive Logic Programming, IOS Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "P. Abbeel", "D. Koller"], "venue": "in: A. Darwiche, N. Friedman (Eds.), Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, San Fransisco, CA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving Prolog programs: Refactoring for Prolog", "author": ["A. Serebrenik", "T. Schrijvers", "B. Demoen"], "venue": "Theory and Practice of Logic Programming 8 (2) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning 73 (3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "The Yap Prolog system", "author": ["V.S. Costa", "R. Rocha", "L. Damas"], "venue": "Theory and Practice of Logic Programming 12 (1-2) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "in: Y. Lechevallier, G. Saporta (Eds.), Proceedings of the 19th International Conference on Computational Statistics ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Mutagenesis: ILP experiments in a non-determinate biological domain", "author": ["A. Srinivasan", "S.H. Muggleton", "R. King", "M. Sternberg"], "venue": "in: Proceedings of the 4th International Workshop on Inductive Logic Programming, volume 237 of GMD-Studien", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "A new atom-additive method for calculating partition coefficients", "author": ["R. Wang", "Y. Fu", "L. Lai"], "venue": "J. Chem. Inf. Comput. Sci 37 (3) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research 6 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "S", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum", "T. Mitchell", "K. Nigam"], "venue": "Slattery, Learning to extract symbolic knowledge from the world wide web ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "S", "author": ["G.H. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar"], "venue": "V. N. Vishwanathan (Eds.), Predicting Structured Data, Neural Information Processing, The MIT Press", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to Classify Text using Support Vector Machines: Methods", "author": ["T. Joachims"], "venue": "Theory, and Algorithms, Kluwer Academic Publishers, Dordrecht", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Collective classification with relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "in: Workshop on Multi-Relational Data Mining ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature discovery with type extension trees", "author": ["P. Frasconi", "M. Jaeger", "A. Passerini"], "venue": "in: Proc. of the 18th Int. Conf. on Inductive Logic Programming, Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Global protein function prediction from protein-protein interaction networks", "author": ["A. Vazquez", "A. Flammini", "A. Maritan", "A. Vespignani"], "venue": "Nature biotechnology 21 (6) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernelbased data fusion and its application to protein function prediction in yeast", "author": ["G. Lanckriet", "M. Deng", "N. Cristianini", "M. Jordan", "W. Noble"], "venue": "in: Proceedings of the Pacific Symposium on Biocomputing, Vol. 9", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence 89 (1-2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "Adaptive computation and machine learning, MIT Press, Cambridge, MA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "A general framework for adaptive processing of data structures", "author": ["P. Frasconi", "M. Gori", "A. Sperduti"], "venue": "IEEE Transactions on Neural Networks 9 (5) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Cyclic pattern kernels for predictive graph mining", "author": ["T. Horv\u00e1th", "T. G\u00e4rtner", "S. Wrobel"], "venue": "in: Proceedings of KDD 04, ACM Press", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "Graph kernels for chemical informatics", "author": ["L. Ralaivola", "S. Swamidass", "H. Saigo", "P. Baldi"], "venue": "Neural Networks 18 (8) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Graph kernels for molecular structure-activity relationship analysis with support vector machines", "author": ["P. Mahe", "N. Ueda", "T. Akutsu", "J. Perret", "J. Vert"], "venue": "J. Chem. Inf. Model 45 (4) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Kernels for structured data", "author": ["T. G\u00e4rtner"], "venue": "Vol. 72 of Series in machine perception and artificial intelligence, World Scientific, Singapore", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph kernels", "author": ["S.V.N. Vishwanathan", "N.N. Schraudolph", "R. Kondor", "K.M. Borgwardt"], "venue": "J. Mach. Learn. Res. 99 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Weisfeiler-lehman graph kernels", "author": ["N. Shervashidze", "P. Schweitzer", "E.J. Van Leeuwen", "K. Mehlhorn", "K.M. Borgwardt"], "venue": "The Journal of Machine Learning Research 12 ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical graph isomorphism", "author": ["B.D. McKay"], "venue": "Congressus Numerantium 30 ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1981}, {"title": "J", "author": ["X. Yan"], "venue": "Han., gSpan: Graph-based substructure pattern mining, in: Proc. 2002 Int. Conf. Data Mining (ICDM \u030102)", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Isomorphism of graphs of bounded valence can be tested in polynomial time", "author": ["E.M. Luks"], "venue": "J. Comput. Syst. Sci 25 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1982}, {"title": "A parametric filtering algorithm for the graph isomorphism problem", "author": ["S. Sorlin", "C. Solnon"], "venue": "Constraints 13 ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Weighted decomposition kernels", "author": ["S. Menchetti", "F. Costa", "P. Frasconi"], "venue": "in: L. De Raedt, S. Wrobel (Eds.), Proceedings of the 22nd International Conference on Machine Learning, Vol. 119 of ACM International Conference Proceeding Series, ACM, New York, NY", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Stacked graphical models for efficient inference in markov random fields", "author": ["Z. Kou", "W. Cohen"], "venue": "in: Proceedings of the Seventh SIAM International Conference on Data Mining", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "ADMET in silico modelling: towards prediction paradise", "author": ["H. van de Waterbeemd", "E. Gifford"], "venue": "Nat Rev Drug Discov", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2003}, {"title": "A survey of the predictive toxicology challenge 2000\u20132001", "author": ["C. Helma", "S. Kramer"], "venue": "Bioinformatics 19 (10) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2003}, {"title": "Predictive Toxicology", "author": ["C. Helma"], "venue": "1st Edition, CRC Press", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "Classification of small molecules by two-and three-dimensional decomposition kernels", "author": ["A. Ceroni", "F. Costa", "P. Frasconi"], "venue": "Bioinformatics 23 (16) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "Derivation and validation of toxicophores for muta- genicity prediction", "author": ["J. Kazius", "R. McGuire", "R. Bursi"], "venue": "J. Med. Chem 48 (1) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "W", "author": ["H. Blockeel", "S. Dzeroski", "B. Kompare", "S. Kramer", "B. Pfahringer"], "venue": "Laer, Experiments in Predicting Biodegradability., Applied Artificial Intelligence 18 (2) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "Discovering H-Bonding Rules in Crystals with Inductive Logic Programming", "author": ["H.Y. Ando", "L. Dehaspe", "W. Luyten", "E.V. Craenenbroeck", "H. Vandecasteele", "L.V. Meervelt"], "venue": "Mol. Pharm. 3 (6) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2006}, {"title": "Molecular graph augmentation with rings and functional groups", "author": ["K. De Grave", "F. Costa"], "venue": "Journal of Chemical Information and Modeling 50 (9) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Stacked generalization", "author": ["D. Wolpert"], "venue": "Neural networks 5 (2) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1992}, {"title": "A perspective on inductive databases", "author": ["L. De Raedt"], "venue": "SIGKDD Explorations 4 (2) ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2002}, {"title": "Constraint-based data mining", "author": ["J.-F. Boulicaut", "B. Jeudy"], "venue": "in: O. Maimon, L. Rokach (Eds.), Data Mining and Knowledge Discovery Handbook, 2nd Edition, Springer", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "The discipline of machine learning", "author": ["T. Mitchell"], "venue": "Tech. Rep. CMU-ML- 06- 108, Carnegie-Mellon University ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards programming languages for machine learning and data mining (extended abstract)", "author": ["L. De Raedt", "S. Nijssen"], "venue": "in: Marzena Kryszkiewicz and Henryk Rybinski and Andrzej Skowron and Zbigniew W. Ras (Ed.), Foundations of Intelligent Systems - 19th International Symposium, IS- MIS 2011, Warsaw, Poland, June 28-30, 2011. Proceedings, Vol. 6804 of Lecture Notes in Computer Science, Springer", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning based java for rapid development of nlp systems", "author": ["N. Rizzolo", "D. Roth"], "venue": "in: Nicoletta Calzolari and Khalid Choukri and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis and Mike Rosner and Daniel Tapias (Ed.), Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May 2010, Valletta, Malta", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient weight learning for markov logic networks", "author": ["D. Lowd", "P. Domingos"], "venue": "in: Knowledge Discovery in Databases: PKDD 2007, Springer", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2007}, {"title": "A simple relational classifier", "author": ["S.A. Macskassy", "F. Provost"], "venue": "in: Proc. of the 2nd Workshop on Multi-Relational Data Mining", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2003}, {"title": "Inverse entailment and Progol", "author": ["S. Muggleton"], "venue": "New Generation Computing 13 (3\u20134) ", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1995}, {"title": "Top-down induction of first order logical decision trees", "author": ["H. Blockeel", "L. De Raedt"], "venue": "Artificial Intelligence 101 (1-2) ", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic similarity logic", "author": ["M. Br\u00f6cheler", "L. Mihalkova", "L. Getoor"], "venue": "in: Proc. of the 26th Conference on Uncertainty in Artificial Intelligence ", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian logic programming: theory and tool", "author": ["K. Kersting", "L. De Raedt"], "venue": "in: L. Getoor, B. Taskar (Eds.), An Introduction to Statistical Relational Learning, MIT Press", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2007}, {"title": "ProbLog: A probabilistic Prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "in: Proceedings of the 20th international joint conference on artificial intelligence", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}, {"title": "Lifted probabilistic inference", "author": ["K. Kersting"], "venue": "in: Proceedings of the 20th European Conference on Artificial Intelligence, Vol. 242 of Frontiers in Artificial Intelligence and Applications, IOS Press", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "Counting belief propagation", "author": ["K. Kersting", "B. Ahmadi", "S. Natarajan"], "venue": "in: Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, AUAI Press, Arlington, Virginia, United States", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey of kernels for structured data", "author": ["T. G\u00e4rtner"], "venue": "SIGKDD Explorations", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2003}, {"title": "Learning from interpretations: a rooted kernel for ordered hypergraphs", "author": ["G. Wachman", "R. Khardon"], "venue": "in: Proceedings of the 24th international conference on Machine learning", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2007}, {"title": "Propositionalization approaches to relational data mining", "author": ["S. Kramer", "N. Lavra\u010d", "P. Flach"], "venue": "in: S. D\u017eeroski, N. Lavra\u010d (Eds.), Relational Data Mining, Springer", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning logical definitions from relations", "author": ["J.R. Quinlan"], "venue": "Machine Learning 5 ", "citeRegEx": "76", "shortCiteRegEx": null, "year": 1990}, {"title": "Transforming graph data for statistical relational learning", "author": ["R.A. Rossi", "L.K. McDowell", "D.W. Aha", "J. Neville"], "venue": "Journal of Artificial Intelligence Research 45 (1) ", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2012}, {"title": "Relational retrieval using a combination of pathconstrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": "Machine learning 81 (1) ", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2010}, {"title": "Mining graph data", "author": ["D.J. Cook", "L.B. Holder"], "venue": "Wiley-Interscience", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2006}, {"title": "Mining heterogeneous information networks: principles and methodologies", "author": ["Y. Sun", "J. Han"], "venue": "Synthesis Lectures on Data Mining and Knowledge Discovery 3 (2) ", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning and inference with constraints", "author": ["M. Chang", "L. Ratinov", "N. Rizzolo", "D. Roth"], "venue": "in: Proceedings of AAAI", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "FACTORIE: Probabilistic programming via imperatively defined factor graphs", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": "in: Neural Information Processing Systems (NIPS)", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "A statistical relational learning approach to identifying evidence based medicine categories", "author": ["M. Verbeke", "V. Van Asch", "R. Morante", "P. Frasconi", "W. Daelemans", "L. De Raedt"], "venue": "in: Proc. of EMNLP-CoNLL", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2012}, {"title": "Collective stability in structured prediction: Generalization from one example", "author": ["B. London", "B. Huang", "B. Taskar", "L. Getoor"], "venue": "in: S. Dasgupta, D. Mcallester (Eds.), Proceedings of the 30th International Conference on Machine Learning (ICML-13), JMLR Workshop and Conference Proceedings", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "On the implementation of the probabilistic logic programming language problog", "author": ["A. Kimmig", "B. Demoen", "L. De Raedt", "V.S. Costa", "R. Rocha"], "venue": "Theory and Practice of Logic Programming 11 (2-3) ", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel-based logical and relational learning with klog for hedge cue detection", "author": ["M. Verbeke", "P. Frasconi", "V. Van Asch", "R. Morante", "W. Daelemans", "L. De Raedt"], "venue": "in: H. Muggleton, S. amd Watanabe, J. Tamaddoni- Nezhad A., Chen (Eds.), Inductive Logic Programming: Revised Selected Papers from the 21st International Conference ", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2011}, {"title": "Spatial relation extraction using relational learning", "author": ["P. Kordjamshidi", "P. Frasconi", "M. Van Otterlo", "M. Moens", "L. De Raedt"], "venue": "in: H. Muggleton, S. amd Watanabe, J. Tamaddoni-Nezhad A., Chen (Eds.), Inductive Logic Programming: Revised Selected Papers from the 21st International Conference ", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2011}, {"title": "A relational kernel-based framework for hierarchical image understanding", "author": ["L. Antanas", "P. Frasconi", "F. Costa", "T. Tuytelaars", "L. De Raedt"], "venue": "in: G. L. Gimel\u2019farb, E. R. Hancock, A. I. Imiya, A. Kuijper, M. Kudo, S. Shinichiro Omachi, T. Windeatt, K. Yamada (Eds.), Lecture Notes in Computer Science\u201e Springer", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2012}, {"title": "A relational kernel-based approach to scene classification", "author": ["L. Antanas", "M. Hoffmann", "P. Frasconi", "T. Tuytelaars", "L.D. Raedt"], "venue": "in: WACV", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2013}, {"title": "Employing logical languages for image understanding", "author": ["L. Antanas", "P. Frasconi", "T. Tuytelaars", "L. De Raedt"], "venue": "IEEE Workshop on Kernels and Distances for Computer Vision, International Conference on Computer Vision, Barcelona, Spain, 13 November 2011 ", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph Theory and Its Applications", "author": ["J.L. Gross", "J. Yellen"], "venue": "Second Edition (Discrete Mathematics and Its Applications), Chapman & Hall/CRC", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2005}, {"title": "Convolution kernels on discrete structures", "author": ["D. Haussler"], "venue": "Tech. Rep. 99- 10, UCSC-CRL ", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1999}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S. Vishwanathan"], "venue": "Journal of Machine Learning Research 10 ", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive quantitative structure-activity relationship models and their use for the efficient screening of molecules", "author": ["K. De Grave"], "venue": "Ph.D. thesis, Katholieke Universiteit Leuven ", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The field of statistical relational learning (SRL) is populated with a fairly large number of models and alternative representations, a state of affairs often referred to as the \u201cSRL alphabet soup\u201d [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 1, "context": "The field of statistical relational learning (SRL) is populated with a fairly large number of models and alternative representations, a state of affairs often referred to as the \u201cSRL alphabet soup\u201d [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 2, "context": "Even though there are many differences between these approaches, they typically extend a probabilistic representation (most often, a graphical model) with a logical or relational one [3, 4].", "startOffset": 183, "endOffset": 189}, {"referenceID": 3, "context": "Even though there are many differences between these approaches, they typically extend a probabilistic representation (most often, a graphical model) with a logical or relational one [3, 4].", "startOffset": 183, "endOffset": 189}, {"referenceID": 4, "context": "But this type of learning system has \u2014 with a few notable exceptions to relational prediction [5, 6] \u2014 not yet received a lot of attention in the SRL literature.", "startOffset": 94, "endOffset": 100}, {"referenceID": 5, "context": "But this type of learning system has \u2014 with a few notable exceptions to relational prediction [5, 6] \u2014 not yet received a lot of attention in the SRL literature.", "startOffset": 94, "endOffset": 100}, {"referenceID": 6, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 178, "endOffset": 184}, {"referenceID": 3, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 178, "endOffset": 184}, {"referenceID": 8, "context": "kLog adopts, as many other logical and relational learning systems, the learning from interpretations framework [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "At this level, the description consists of an E/R-model describing the structure of the data and the data itself, which is similar to that of traditional SRL systems [10].", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "In the current implementation of kLog that we describe in this paper, we employ the neighborhood subgraph pairwise distance kernel (NSPDK) [11] but the reader should keep in mind that other graph kernels can be incorporated.", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "In Section 3, we formalize the assumed statistical setting for supervised learning from interpretations, provide some background on statistical modeling from a relational learning point of view, and position kLog more clearly in the context of related systems such as Markov logic, MN [6], etc.", "startOffset": 285, "endOffset": 288}, {"referenceID": 6, "context": "for demonstrating the capabilities of MLNs [7].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "introduce aggregated attributes [9].", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Our general approach to construct a statistical model is based on the learning from interpretations setting [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 11, "context": ", [12]), which may be developed without associating a probabilistic interpretation to F .", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "The last two models are often cited as an example of generativediscriminative conjugate pairs because of the above reasons [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "When moving up to a slightly richer data type like sequences (perhaps the simplest case of relational data), the three models have well known extensions: naive Bayes extends to hidden Markov models (HMMs), logistic regression extends to conditional random fields (CRFs) [14], and SVM extends to structured output SVM for sequences [15, 12].", "startOffset": 270, "endOffset": 274}, {"referenceID": 14, "context": "When moving up to a slightly richer data type like sequences (perhaps the simplest case of relational data), the three models have well known extensions: naive Bayes extends to hidden Markov models (HMMs), logistic regression extends to conditional random fields (CRFs) [14], and SVM extends to structured output SVM for sequences [15, 12].", "startOffset": 331, "endOffset": 339}, {"referenceID": 11, "context": "When moving up to a slightly richer data type like sequences (perhaps the simplest case of relational data), the three models have well known extensions: naive Bayes extends to hidden Markov models (HMMs), logistic regression extends to conditional random fields (CRFs) [14], and SVM extends to structured output SVM for sequences [15, 12].", "startOffset": 331, "endOffset": 339}, {"referenceID": 13, "context": ", [14] for a detailed discussion).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "Among generative models, one natural extension of HMMs is stochastic context free grammars [16], which in turn can be extended to stochastic logic programs [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "Among generative models, one natural extension of HMMs is stochastic context free grammars [16], which in turn can be extended to stochastic logic programs [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "More expressive systems include probabilistic relational models (PRMs) [8] and Markov logic networks (MLNs) [7], when trained generatively.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "More expressive systems include probabilistic relational models (PRMs) [8] and Markov logic networks (MLNs) [7], when trained generatively.", "startOffset": 108, "endOffset": 111}, {"referenceID": 11, "context": "Generalizations of SVM for relational structures akin to context free grammars have also been investigated [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "Among discriminative models, CRFs can be extended from linear chains to arbitrary relations [14], for example in the form of discriminative", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "Markov networks [18] and discriminative Markov logic networks [7].", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "Markov networks [18] and discriminative Markov logic networks [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "The use of SVM-like loss functions has also been explored in max-margin Markov networks (MN) [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "The ability to specify intensional predicates through clauses (see an example in Listing 3) is most useful for introducing background knowledge in the learning process and common practice in inductive logic programming [9].", "startOffset": 219, "endOffset": 222}, {"referenceID": 18, "context": "The partition is inferred by analyzing the dependency graphs of Prolog predicates defining intensional relations, using an algorithm reminiscent of the call graph computation in ViPReSS [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 19, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": "Implementation kLog is currently embedded in Yap Prolog [21] and consists of three main components: (1) a domain-specific interpreter, (2) a database loader, and (3) a library of predicates that are used to specify the learning task, to declare the graph kernel and the learning model, and to perform training, prediction, and performance evaluation.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "kLog incorporates LibSVM [22] and Stochastic gradient descent [23] and can interface with arbitrary (external) SVM solvers by coding appropriate data conversion wrappers.", "startOffset": 25, "endOffset": 29}, {"referenceID": 22, "context": "kLog incorporates LibSVM [22] and Stochastic gradient descent [23] and can interface with arbitrary (external) SVM solvers by coding appropriate data conversion wrappers.", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "For the sake of concreteness, let us consider the problem of small molecule classification as pioneered in the relational learning setting in [24].", "startOffset": 142, "endOffset": 146}, {"referenceID": 24, "context": "to predict a real-valued property associated with a molecule, such as its biological activity or its octanol/water partition coefficient (logP) [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 25, "context": "Multitask learning can be handled trivially by learning independent predictors; alternatively, more sophisticated algorithms that take into account correlations amongst tasks (such as [26]) could be used.", "startOffset": 184, "endOffset": 188}, {"referenceID": 26, "context": "We illustrate this case using the classic WebKB domain [27].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "an instance of a supervised structured output problem [28], that in this case might also be referred to as collective classification [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "an instance of a supervised structured output problem [28], that in this case might also be referred to as collective classification [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": ", [29]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "Following the setup in [30], the problem is to predict \u201cblockbuster\u201d movies, i.", "startOffset": 23, "endOffset": 27}, {"referenceID": 30, "context": "When predicting the class of future movies, data about past movies\u2019 receipts can be used to construct features (indeed, the count of blockbuster movies produced by the same studio is one of the most informative features [31]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 31, "context": "Assuming data for just a single organism is available, there is one entity set (protein) and a binary relation interact expressing protein-protein interaction [32, 33].", "startOffset": 159, "endOffset": 167}, {"referenceID": 32, "context": "Assuming data for just a single organism is available, there is one entity set (protein) and a binary relation interact expressing protein-protein interaction [32, 33].", "startOffset": 159, "endOffset": 167}, {"referenceID": 8, "context": "The use of an intermediate graphicalized representation is novel in the context of propositionalization, a well-known technique in logical and relational learning [9] that transforms graph-based or relational data directly into an attribute-value learning format, or possibly into a multi-instance learning one8.", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "8In multi-instance learning [34], the examples are sets of attribute-value tuples or sets of feature vectors.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "There are several other examples in the literature where a graph template is expanded into a ground graph, including the plate notation in graphical models [35], encoding networks in neural networks for learning data structures [36], and the construction of Markov networks in Markov logic [7].", "startOffset": 156, "endOffset": 160}, {"referenceID": 35, "context": "There are several other examples in the literature where a graph template is expanded into a ground graph, including the plate notation in graphical models [35], encoding networks in neural networks for learning data structures [36], and the construction of Markov networks in Markov logic [7].", "startOffset": 228, "endOffset": 232}, {"referenceID": 6, "context": "There are several other examples in the literature where a graph template is expanded into a ground graph, including the plate notation in graphical models [35], encoding networks in neural networks for learning data structures [36], and the construction of Markov networks in Markov logic [7].", "startOffset": 290, "endOffset": 293}, {"referenceID": 36, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 37, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 38, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 39, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 40, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 41, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "In the current implementation, we use an extension of NSPDK [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 42, "context": "Note that 1A\u223c=A\u2032 is a valid kernel between graphs under the feature map \u03c6cl that transforms A into \u03c6cl(A), a sequence of all zeros except the i-th element equal to 1 in correspondence to the identifier for the canonical representation of A [43, 44].", "startOffset": 240, "endOffset": 248}, {"referenceID": 43, "context": "Note that 1A\u223c=A\u2032 is a valid kernel between graphs under the feature map \u03c6cl that transforms A into \u03c6cl(A), a sequence of all zeros except the i-th element equal to 1 in correspondence to the identifier for the canonical representation of A [43, 44].", "startOffset": 240, "endOffset": 248}, {"referenceID": 42, "context": "Algorithms that are in the worst case exponential but that are fast in practice do exist [43, 44].", "startOffset": 89, "endOffset": 97}, {"referenceID": 43, "context": "Algorithms that are in the worst case exponential but that are fast in practice do exist [43, 44].", "startOffset": 89, "endOffset": 97}, {"referenceID": 44, "context": "For special graph classes, such as bounded degree graphs [45], there exist polynomial time algorithms.", "startOffset": 57, "endOffset": 61}, {"referenceID": 45, "context": ", cases with very high vertex degree are possible as in general an entity atom may play a role in an arbitrary number of relationship atoms), we prefer an approximate solution with efficiency guarantees based on topological distances similar in spirit to [46].", "startOffset": 255, "endOffset": 259}, {"referenceID": 40, "context": "Although there exist several graph kernels that are based on this type of match, they generally suffer from very high computational costs [41].", "startOffset": 138, "endOffset": 142}, {"referenceID": 46, "context": "To ensure efficiency, we use an idea introduced in the Weighted Decomposition Kernel [47]: given a subgraph, we consider only the multinomial distribution (i.", "startOffset": 85, "endOffset": 89}, {"referenceID": 47, "context": "For example, Prolog predicates in intensional signatures can effectively be used as expressive relational templates for stacked graphical models [48] where input features for one instance are computed from predictions on other related instances.", "startOffset": 145, "endOffset": 149}, {"referenceID": 48, "context": "Predicting the biological activity of small molecules is a major task in chemoinformatics and can help drug development [49] and toxicology [50, 51].", "startOffset": 120, "endOffset": 124}, {"referenceID": 49, "context": "Predicting the biological activity of small molecules is a major task in chemoinformatics and can help drug development [49] and toxicology [50, 51].", "startOffset": 140, "endOffset": 148}, {"referenceID": 50, "context": "Predicting the biological activity of small molecules is a major task in chemoinformatics and can help drug development [49] and toxicology [50, 51].", "startOffset": 140, "endOffset": 148}, {"referenceID": 36, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 37, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 38, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 40, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 51, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 52, "context": "The Bursi data set [53] consists of 4,337 molecular structures with associated mutagenicity labels (2,401 mutagens and 1,936 nonmutagens) obtained from a short-term in vitro assay that detects genetic damage.", "startOffset": 19, "endOffset": 23}, {"referenceID": 53, "context": "The Biodegradability data set [54] contains 328 compounds and the regression task is to predict their half-life for aerobic aqueous biodegradation starting from molecular structure and global molecular measurements.", "startOffset": 30, "endOffset": 34}, {"referenceID": 54, "context": "Relevant predicates in the extensional database are a/2, b/3 (atoms and bonds, respectively, extracted from the chemical structure), sub/3 (functional groups, computed by DMax Chemistry Assistant [55, 56]), fused/3, connected/4 (direct connection between two functional groups), linked/4 (connection between functional groups via an aliphatic chain).", "startOffset": 196, "endOffset": 204}, {"referenceID": 55, "context": "Relevant predicates in the extensional database are a/2, b/3 (atoms and bonds, respectively, extracted from the chemical structure), sub/3 (functional groups, computed by DMax Chemistry Assistant [55, 56]), fused/3, connected/4 (direct connection between two functional groups), linked/4 (connection between functional groups via an aliphatic chain).", "startOffset": 196, "endOffset": 204}, {"referenceID": 10, "context": "As shown in Table 3, results are relatively stable with respect to the choice of kernel hyperparameter (maximum radius and distance) and SVM regularization and essentially match the best results reported in [11] (AUROC 0.", "startOffset": 207, "endOffset": 211}, {"referenceID": 10, "context": "These results are not surprising since the graphs generated by kLog are very similar in this case to the expanded molecular graphs used in [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 55, "context": "Augmenting the language with the functional groups from [56] unexpectedly gave worse results in Tilde compared to a plain atom-bond language.", "startOffset": 56, "endOffset": 60}, {"referenceID": 53, "context": "We estimated prediction performance by repeating five times a ten-fold cross validation procedure as described in [54] (using exactly the same folds in each trial).", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "04 (kFOIL was shown to outperform Tilde and S-CART in [5]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "To assess kLog behavior we evaluated prediction accuracy according to the leave-one-research-group-out setup of [7], using the domain description of Listings 2 and 3, together with a NSPDK kernel with distance 2, radius 2, and soft match.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "Comparative results with respect to Markov logic are reported in Figure 6 (MLN results published in [7]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Professor) are unknown, as in [7]).", "startOffset": 30, "endOffset": 33}, {"referenceID": 56, "context": "Our procedure is reminiscent of stacked generalization [57].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Results are reported in Figure 7 (MLN results published in [7]).", "startOffset": 59, "endOffset": 62}, {"referenceID": 57, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 101, "endOffset": 109}, {"referenceID": 58, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 101, "endOffset": 109}, {"referenceID": 59, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 282, "endOffset": 294}, {"referenceID": 60, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 282, "endOffset": 294}, {"referenceID": 61, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 282, "endOffset": 294}, {"referenceID": 59, "context": "According to Mitchell [60], the development of such languages is a long outstanding research question.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "Entity classification The WebKB data set [27] has been widely used to evaluate relational methods for text categorization.", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "After graphicalization, vertices representing webpages have large degree (at least the number of words), making the standard NSPDK of [11] totally inadequate: even by setting the maximum distance d\u2217 = 1 and the maximum radius r\u2217 = 2, the hard match would essentially create a distinct feature for every page.", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 1, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 2, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 3, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 4, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 5, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 6, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 7, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 8, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 62, "context": "For learning we used the preconditioned scaled conjugate gradient approach described in [63] and we", "startOffset": 88, "endOffset": 92}, {"referenceID": 62, "context": "The best results, reported in Table 6, used the trick of averaging MLN weights across all iterations as in [63].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "We focus on predicting, for each movie, whether its first weekend box-office receipts are over US$2 million, a learning task previously defined in [30, 64].", "startOffset": 147, "endOffset": 155}, {"referenceID": 63, "context": "We focus on predicting, for each movie, whether its first weekend box-office receipts are over US$2 million, a learning task previously defined in [30, 64].", "startOffset": 147, "endOffset": 155}, {"referenceID": 64, "context": "First, the underlying representation of the data that kLog employs at the first level is very close to that of standard inductive logic programming systems such as Progol [65], Aleph [66], and Tilde [67] in the sense that the input is essentially (a variation of) a Prolog program for specifying the data and the background knowledge.", "startOffset": 171, "endOffset": 175}, {"referenceID": 65, "context": "First, the underlying representation of the data that kLog employs at the first level is very close to that of standard inductive logic programming systems such as Progol [65], Aleph [66], and Tilde [67] in the sense that the input is essentially (a variation of) a Prolog program for specifying the data and the background knowledge.", "startOffset": 199, "endOffset": 203}, {"referenceID": 9, "context": "in [10].", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "The signatures play a similar role as the notion of a declarative bias in inductive logic programming [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 64, "context": "This contrasts with some inductive logic programming systems such as Progol [65] and Aleph [66].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 102, "endOffset": 105}, {"referenceID": 66, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 138, "endOffset": 142}, {"referenceID": 7, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 176, "endOffset": 179}, {"referenceID": 67, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 205, "endOffset": 209}, {"referenceID": 68, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 223, "endOffset": 227}, {"referenceID": 6, "context": "For statistical relational learning methods such as Markov logic [7], probabilistic relational models [8], and Bayesian logic programs [69] the knowledge-based model construction process will result in a graphical model (Bayesian or Markov network) for each instance representing a class of probability distributions, while in kLog the process of graphicalization results in a graph representing an instance by unrolling the E/R-diagram.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "For statistical relational learning methods such as Markov logic [7], probabilistic relational models [8], and Bayesian logic programs [69] the knowledge-based model construction process will result in a graphical model (Bayesian or Markov network) for each instance representing a class of probability distributions, while in kLog the process of graphicalization results in a graph representing an instance by unrolling the E/R-diagram.", "startOffset": 102, "endOffset": 105}, {"referenceID": 67, "context": "For statistical relational learning methods such as Markov logic [7], probabilistic relational models [8], and Bayesian logic programs [69] the knowledge-based model construction process will result in a graphical model (Bayesian or Markov network) for each instance representing a class of probability distributions, while in kLog the process of graphicalization results in a graph representing an instance by unrolling the E/R-diagram.", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "A combination of structured-output learning [12] and iterative approaches (as incorporated in the EM algorithm) can form the basis for further work in", "startOffset": 44, "endOffset": 48}, {"referenceID": 69, "context": "Lifted inference has been the focus of a lot of attention in statistical relational learning; see [71] for an overview.", "startOffset": 98, "endOffset": 102}, {"referenceID": 70, "context": ", [72].", "startOffset": 2, "endOffset": 6}, {"referenceID": 71, "context": "kLog builds also upon the many results on learning with graph kernels, see [73] for an overview.", "startOffset": 75, "endOffset": 79}, {"referenceID": 72, "context": "In this regard, kLog is close in spirit to the work of [74], who define a kernel on hypergraphs, where hypergraphs are used to represent relational interpretations.", "startOffset": 55, "endOffset": 59}, {"referenceID": 73, "context": "The graphicalization approach introduced in kLog is closely related to the notion of propositionalization, a commonly applied technique in logical and relational learning [75, 9] to generate features from a relational representation.", "startOffset": 171, "endOffset": 178}, {"referenceID": 8, "context": "The graphicalization approach introduced in kLog is closely related to the notion of propositionalization, a commonly applied technique in logical and relational learning [75, 9] to generate features from a relational representation.", "startOffset": 171, "endOffset": 178}, {"referenceID": 4, "context": "kFOIL [5] is one such propositionalization technique that has been tightly integrated with a kernel-based method.", "startOffset": 6, "endOffset": 9}, {"referenceID": 74, "context": "It greedily derives a (small) set of features in a way that resembles the rule-learning algorithm of FOIL [76].", "startOffset": 106, "endOffset": 110}, {"referenceID": 75, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 76, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 77, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 78, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 61, "context": "Learning based Java [62] was designed to specifically address applications in natural language processing.", "startOffset": 20, "endOffset": 24}, {"referenceID": 79, "context": "It builds on the concept of data-driven compilation to perform feature extraction and nicely exploits the constrained conditional model framework [81] for structured output learning.", "startOffset": 146, "endOffset": 150}, {"referenceID": 80, "context": "FACTORIE [82] allows users to concisely define features used in a factor graph and, consequently, arbitrarily connected conditional random fields.", "startOffset": 9, "endOffset": 13}, {"referenceID": 81, "context": "For example, if dependencies have a regular sequential structure, dynamic programming can be used for this step, exactly as in conditional random fields (indeed, collective classification has been succesfully exploited within kLog in an application to natural language test segmentation [83]).", "startOffset": 287, "endOffset": 291}, {"referenceID": 82, "context": "Better understanding of generalization for structured prediction models has begun to emerge (see [84] and references therein) and a theoretical analysis of learning within the present kLog setting is another potential direction for future research.", "startOffset": 97, "endOffset": 101}, {"referenceID": 39, "context": "The implementation of a library of kernels suitable for different types of graphs, as well as the integration of other existing graph kernels (such as [40, 41, 42]) in the kLog framework, is therefore an important direction for future development.", "startOffset": 151, "endOffset": 163}, {"referenceID": 40, "context": "The implementation of a library of kernels suitable for different types of graphs, as well as the integration of other existing graph kernels (such as [40, 41, 42]) in the kLog framework, is therefore an important direction for future development.", "startOffset": 151, "endOffset": 163}, {"referenceID": 41, "context": "The implementation of a library of kernels suitable for different types of graphs, as well as the integration of other existing graph kernels (such as [40, 41, 42]) in the kLog framework, is therefore an important direction for future development.", "startOffset": 151, "endOffset": 163}, {"referenceID": 6, "context": "Many of these are similar to those employed in statistical relational learning systems such as Alchemy [7] and ProbLog [85].", "startOffset": 103, "endOffset": 106}, {"referenceID": 83, "context": "Many of these are similar to those employed in statistical relational learning systems such as Alchemy [7] and ProbLog [85].", "startOffset": 119, "endOffset": 123}, {"referenceID": 84, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 79, "endOffset": 91}, {"referenceID": 81, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 79, "endOffset": 91}, {"referenceID": 85, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 79, "endOffset": 91}, {"referenceID": 86, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 115, "endOffset": 127}, {"referenceID": 87, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 115, "endOffset": 127}, {"referenceID": 88, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 115, "endOffset": 127}, {"referenceID": 89, "context": "We closely follow the notation in [91].", "startOffset": 34, "endOffset": 38}, {"referenceID": 90, "context": "We follow the notation in [92].", "startOffset": 26, "endOffset": 30}, {"referenceID": 90, "context": "In [92] it is demonstrated that, if there exist a kernel Kd over Xd \u00d7 Xd for each d = 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "The algorithm was first described in [11] but we present it again here for self-sufficiency.", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": ", for creating compact representations of molecular structures [38], and has been advocated as a tool for compressing very high-dimensional feature spaces [93].", "startOffset": 63, "endOffset": 67}, {"referenceID": 91, "context": ", for creating compact representations of molecular structures [38], and has been advocated as a tool for compressing very high-dimensional feature spaces [93].", "startOffset": 155, "endOffset": 159}], "year": 2014, "abstractText": "We introduce kLog, a novel approach to statistical relational learning. Unlike standard approaches, kLog does not represent a probability distribution directly. It is rather a language to perform kernel-based learning on expressive logical and relational representations. kLog allows users to specify learning problems declaratively. It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming, and deductive databases. Access by the kernel to the rich representation is mediated by a technique we call graphicalization: the relational representation is first transformed into a graph \u2014 in particular, a grounded entity/relationship diagram. Subsequently, a choice of graph kernel defines the feature space. kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of Prolog or Datalog programs as in inductive logic programming systems. The kLog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification. We also report about empirical comparisons, showing PF was a visiting professor at KU Leuven and FC a postdoctoral fellow at KU Leuven while this work was initiated \u2217Corresponding author Email addresses: p-f@dsi.unifi.it (Paolo Frasconi), costa@informatik.uni-freiburg.de (Fabrizio Costa), Luc.DeRaedt@cs.kuleuven.be (Luc De Raedt), Kurt.DeGrave@cs.kuleuven.be (Kurt De Grave) Preprint submitted to Artificial Intelligence July 29, 2014 ar X iv :1 20 5. 39 81 v5 [ cs .A I] 2 8 Ju l 2 01 4 that kLog can be either more accurate, or much faster at the same level of accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at http://klog.dinfo.unifi.it along with tutorials.", "creator": "LaTeX with hyperref package"}}}