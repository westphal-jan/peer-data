{"id": "1205.4477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences", "abstract": "discovering frequent episodes over event sequences accurately is an important automatic data resource mining automated task. : in many applications, scripted events potentially constituting or the total data sequence arrive as well a stream, consequently at furious rates, transient and recent occurrence trends ( or frequent disastrous episodes ) can change magnitude and drift due them to the dynamical dominant nature of the previous underlying event generation process. the ability to naturally detect streaks and track such are the changing sets sequences of frequent worst episodes can be incredibly valuable utilized in documenting many application scenarios. proposed current methods for introducing frequent and episode discovery are typically labeled multipass search algorithms, making them greatly unsuitable in the streaming memory context. in this review paper, we jointly propose a viable new continuous streaming regression algorithm for discovering frequent episodes over a window of recent events in the stream. our timing algorithm processes subsequent events as they arrive, one fewer batch at within a time, while discovering the top frequent disorder episodes grouped over a window consisting of several notable batches arriving in the almost immediate past. we derive approximation accuracy guarantees for our algorithm under modifying the imperative condition that no frequent sporadic episodes are approximately well - depth separated computed from infrequent ones expressed in every brief batch of studying the window. we sometimes present too extensive comprehensive experimental empirical evaluations of refining our coding algorithm on both real quantitative and synthetic data. we might also present comparisons with baselines and adaptations rates of streaming algorithms from itemset mining literature.", "histories": [["v1", "Mon, 21 May 2012 01:46:57 GMT  (872kb,D)", "http://arxiv.org/abs/1205.4477v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["debprakash patnaik", "naren ramakrishnan", "srivatsan laxman", "badrish chandramouli"], "accepted": false, "id": "1205.4477"}, "pdf": {"name": "1205.4477.pdf", "metadata": {"source": "CRF", "title": "Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences", "authors": ["Debprakash Patnaik", "Naren Ramakrishnan", "Srivatsan Laxman", "Badrish Chandramouli"], "emails": ["patnaik@vt.edu", "naren@vt.edu", "slaxman@microsoft.com", "badrishc@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The problem of discovering interesting patterns from large datasets has been well studied in the form of pattern classes such as itemsets, sequential patterns, and episodes with temporal constraints. However, most of these techniques deal with static datasets, over which multiple passes are performed.\nIn many domains like telecommunication and computer security, it is becoming increasingly difficult to store and process data at speeds comparable to their generation rate. A few minutes of call logs data in a telecommunication network can easily run into millions of records. Such data are referred to as data streams [12]. A data stream is an unbounded sequence where new data points or events arrive continuously and often at very high rates. Many traditional data mining algorithms are rendered useless in this context as one cannot hope to store the entire data and then process it. Any method for data streams must thus operate under the constraints of limited memory and processing time. In addition, the data must be processed faster than it is being generated. In this paper, we investigate the problem of mining temporal patterns called episodes under these constraints; while we focus on discovering frequent episodes from event streams, our method is general and adaptable to any class of patterns that might be of interest over the given data.\nIn several applications where frequent episodes have been found to be useful, share the streaming data characteristics. In neuroscience, multi electrode arrays are being used as implants to control artificial prosthetics. These interfaces interpret commands from the brain and direct external devices. Identifying controlling signals from brain is much like finding a needle in the hay stack. Large volumes of data need to be processed in real time to be able to solve this problem. Similar situations exist in telecom and computer networks where the network traffic and call logs must be analyzed to detect attacks or fraudulent activity.\nA few works exist in current literature for determining frequent itemsets from a stream of transactions (e.g. see [16]). However, they are either computationally impractical, due to worst-case assumptions, or\n\u2217Debprakash Patnaik is now with Amazon.com\nar X\niv :1\n20 5.\n44 77\nv1 [\ncs .L\nG ]\n2 1\nM ay\n2 01\nineffective due to strong independence assumptions. We make no statistical assumptions on the stream, independence or otherwise. We develop the error characterization of our algorithms by identifying two key properties of the data, namely, maximum rate of change and top-k separation. Our key algorithmic contribution is an adaptation of the border sets datastructures to reuse work done in previous batches when computing frequent patterns of the current batch. This reduces the candidate generation effort from F 2 to FFnew (where F denotes the number of frequent patterns of a particular size in the previous batch, while Fnew denotes the number of newly frequent patterns of that same size in the current batch). Experimental work demonstrates the practicality of our algorithms, both in-terms of accuracy of the returned frequent pattern sets as well as in terms of computational efficiencies."}, {"heading": "2 Preliminaries", "text": "In the framework of frequent episodes [9], an event sequence is denoted as \u3008(e1, \u03c41), . . . , (en, \u03c4n)\u3009, where (ei, \u03c4i) represents the ith event; ei is drawn from a finite alphabet E of symbols (called event-types) and \u03c4i denotes the time-stamp of the ith event, with \u03c4i+1 \u2265 \u03c4i, i = 1, . . . , (n \u2212 1). An `-node episode \u03b1 is defined by a triple \u03b1 = (V\u03b1, <\u03b1, g\u03b1), where V\u03b1 = {v1, . . . , v`} is a collection of ` nodes, <\u03b1 is a partial order over V\u03b1 and g\u03b1 : V\u03b1 \u2192 E is a map that assigns an event-type g\u03b1(v) to each node v \u2208 V\u03b1. There are two special classes of episodes: When <\u03b1 is total \u03b1 is called a serial episode and when it is empty, it is called a parallel episode. An occurrence of an episode \u03b1 is a map h : V\u03b1 \u2192 {1, . . . , n} such that eh(v) = g(v) for all v \u2208 V\u03b1 and for all pairs of nodes v, v\u2032 \u2208 V\u03b1 such that v <\u03b1 v\u2032 the map h ensures that \u03c4h(v) < \u03c4h(v\u2032). Two occurrences of an episode are non-overlapped [7] if no event corresponding to one appears in-between the events corresponding to the other. The maximum number of non-overlapped occurrences of an episode is defined as its frequency in the event sequence. The task in frequent episode discovery is to find all patterns whose frequency exceeds a user-defined threshold. Given a frequency threshold, Apriori-style level-wise algorithms [9, 1] can be used to obtain the frequent episodes in the event sequence. An important variant of this task is top-k episode mining, where, rather than issue a frequency threshold to the mining algorithm, the user supplies the number of top frequent episodes that need to be discovered.\nDefinition 1 (Top-k episodes of size `). The set of top-k episodes of size ` is defined as the collection of all `-node episodes with frequency greater than or equal to the frequency fk of the kth most frequent `-node episode in the given event sequence.\nNote that the number of top-k `-node episodes can exceed k, although the number of `-node episodes with frequencies strictly greater than fk is at most (k \u2212 1)."}, {"heading": "3 Problem Statement", "text": "The data available (referred to as an event stream) is a potentially infinite sequence of events:\nD = \u3008(e1, \u03c41), (e2, \u03c42), . . . , (ei, \u03c4i), . . . , (en, \u03c4n), . . .\u3009 (1)\nOur goal is to find all episodes that were frequent in the recent past and to this end, we consider a sliding window model for the window of interest of the user1. In this model, the user wants to determine episodes that are frequent over a window of fixed-size and terminating at the current time-tick. As new events arrive in the stream, the user\u2019s window of interest shifts, and the data mining task is to next report the frequent episodes in the new window of interest.\nTypically, the window of interest is very large and cannot be stored and processed in-memory. This straightaway precludes the use of standard multi-pass algorithms for frequent episode discovery over the window of interest. Events in the stream can be organized into batches such that at any given time only the new incoming batch needs to be stored and processed in memory. This is illustrated in Fig. 1. The current\n1Streaming patterns literature has also considered other models, such as the landmark and time-fading models [4], but we do not consider them in this paper.\nwindow of interest is denoted by Ws and the most recent batch, Bs, consists of a sequence of events in D with times of occurrence, \u03c4i, such that,\n(s\u2212 1)Tb \u2264 \u03c4i < sTb (2)\nwhere Tb is the time-span of each batch and s is the batch number (s = 1, 2, . . .) 2. The frequency of an episode \u03b1 in a batch Bs is referred to as its batch frequency f s(\u03b1). The current window of interest, Ws, consists of m consecutive batches ending in batch Bs, i.e.\nWs = \u3008Bs\u2212m+1, Bs\u2212m+2, . . . , Bs\u3009 (3)\nDefinition 2 (Window Frequency). The frequency of an episode \u03b1 over window Ws, referred to as its window frequency and denoted by fWs(\u03b1), is defined as the sum of batch frequencies of \u03b1 in Ws. Thus, if f j(\u03b1) denotes the batch frequency of \u03b1 in batch Bj, then the window frequency of \u03b1 is given by f\nWs(\u03b1) =\u2211 Bj\u2208Ws f j(\u03b1).\nIn summary, we are given an event stream (D), a time-span for batches (Tb), the number of consecutive batches that constitute the current window of interest (m), the desired size of frequent episodes (`) and the desired number of most frequent episodes (k). We are now ready to formally state the problem of discovering top-k episodes in an event stream.\nProblem 1 (Streaming Top-k Mining). For each n- ew batch, Bs, of events in the stream, find all `-node episodes in the corresponding window of interest, Ws, whose window frequencies are greater than or equal to the window frequency, fks , of k th most frequent `-node episode in Ws."}, {"heading": "4 Method", "text": "In general, the top-k episodes over a window may be quite different from the top-k episodes in the individual batches constituting the window. This is illustrated through an example in Fig. 2.\nExample 1 (Window Top-k v/s Batch Top-k). Let W be a window of four batches B1, . . . , B4. The episodes in each batch with corresponding batch frequencies are listed in Fig. 2. The corresponding window frequencies (sum of each episodes\u2019 batch frequencies) are listed in Table 1. The top-2 episodes in B1 are (PQRS) and (WXYZ). Similarly (EFGH) and (IJKL) are the top-2 episodes in B2, and so on. (ABCD) and (MNOP)\n2We assume that the number of events in any batch is bounded above and that we have sufficient memory to store and process all events that occur in a batch. For example, if time is integer-valued and if only one event occurs at any time-tick, then there are at most Tb events in any batch.\nPattern Count\nW X Y Z 12\nP Q R S 10\nA B C D 8\nM N O P 8\nE F G H 0\nI J K L 0\nPattern Count\nE F G H 15\nI J K L 12\nM N O P 10\nA B C D 9\nP Q R S 0\nW X Y Z 0\nPattern Count\nW X Y Z 12\nE F G H 10\nA B C D 10\nM N O P 8\nP Q R S 0\nI J K L 0\nPattern Count\nI J K L 11\nP Q R S 9\nM N O P 8\nA B C D 8\nE F G H 0\nW X Y Z 0\nB1 B2 B3 B4\nFigure 2: Batch frequencies in Example 1.\nhave the highest window frequencies but never appear in the top-2 of any batch \u2013 these episodes would \u2018fly below the radar\u2019 and go undetected if we considered only the top-2 episodes in every batch as candidates for the top-2 episodes over W . This example can be easily generalized to any number of batches and any k.\nExample 1 highlights the main challenge in the streaming top-k mining problem: while we can only store and process the most recent batch of events in the window of interest, the batchwise top-k episodes may not contain sufficient informative about the top-k over the entire window. It is obviously not possible to count and track all episodes (both frequent and infrequent) in every batch in the window, since the pattern space is typically very large. This brings us to the question of which episodes to select and track in every batch. How deep must we search within each batch for episodes that have potential to become top-k over the window? In this paper, we develop the formalism to answer this question. We identify two important properties of the underlying event stream which determine the design and analysis of our algorithms. These are stated in Definitions 3 & 4 below.\nDefinition 3 (Maximum Rate of Change, \u2206). The maximum change in batch frequency of any episode, \u03b1, across any pair of consecutive batches, Bs and Bs+1, is bounded above by \u2206(> 0), i.e.,\n|fs+1(\u03b1)\u2212 fs(\u03b1)| \u2264 \u2206, (4)\nand \u2206 is referred to as the maximum rate of change.\nIntuitively, \u2206 controls the extent of change that we may see from one batch to the next. It is trivially bounded above by the maximum number of events arriving per batch, and in practice, it is in fact much smaller.\nDefinition 4 (Top-k Separation of (\u03d5, )). A batch Bs of events is said to have a top-k separation of (\u03d5, ), \u03d5 \u2265 0, \u2265 0, if there are no more than (1 + )k episodes with batch frequency greater than or equal to (fsk \u2212 \u03d5\u2206), where fsk denotes the batch frequency of the kth most-frequent episode in Bs and \u2206 denotes the maximum rate of change as per Definition 3.\nThis is a measure of how well-separated the frequencies of the top-k episodes are relative to the rest of the episodes. We expect to see roughly k episodes with batch frequencies of at least fks and the separation can be considered to be high (or good) if can remain small even for relatively large \u03d5. We observe that is a non-decreasing function of \u03d5 and that top-k separation is measured relative to the maximum rate of change \u2206. Also, top-k separation of any given batch of events is characterized through not one but several pairs of (\u03d5, ) since \u03d5 and are essentially functionally related \u2013 is typically close to zero for \u03d5 = 0 and is roughly the size of the entire class of episodes (minus k) for \u03d5 \u2265 fks .\nWe now use the maximum rate of change property to design efficient streaming algorithms for top-k episode mining and show that top-k separation plays a pivotal role in determining the quality of approximation that our algorithms can achieve.\nLemma 1. Consider two consecutive batches, Bs and Bs+1, with a maximum rate of change \u2206. The batch frequencies of the kth most-frequent episodes in the corresponding batches are related as follows:\n|fs+1k \u2212 fsk | \u2264 \u2206 (5) Proof. There exist at least k episodes in Bs with batch frequency greater than or equal to f s k (by definition). Hence, there exist at least k episodes in Bs+1 with batch frequency greater than or equal to (f s k \u2212 \u2206) (since frequency of any episode can decrease by at most \u2206 going from Bs to Bs+1). Hence we must have fs+1k \u2265 (fsk \u2212 \u2206). Similarly, there can be at most (k \u2212 1) episodes in Bs+1 with batch frequency strictly greater than (fsk + \u2206). Hence we must also have f s+1 k \u2264 (fsk + \u2206).\nNext we show that if the batch frequency of an episode is known relative to f sk in the current batch Bs, we can bound its frequency in a later batch.\nLemma 2. Consider two batches, Bs and Bs+r, r \u2208 Z, located r batches away from each other. If \u2206 is the maximum rate of change (as per Definition 3) then the batch frequency of any episode \u03b1 in Bs+r must satisfy the following:\n1. If fs(\u03b1) \u2265 f sk , then f s+r(\u03b1) \u2265 fs+rk \u2212 2|r|\u2206 2. If fs(\u03b1) < fsk , then f\ns+r(\u03b1) < fs+rk + 2|r|\u2206 Proof. Since \u2206 is the maximum rate of change, we have fs+r(\u03b1) \u2265 (fs(\u03b1) \u2212 |r|\u2206) and from Lemma 1, we have fs+rk \u2264 (fsk + |r|\u2206). Therefore, if fs(\u03b1) \u2265 fsk , then fs+r(\u03b1) + |r|\u2206 \u2265 fs(\u03b1) \u2265 f sk \u2265 fs+rk \u2212 |r|\u2206 which implies fs+r(\u03b1) \u2265 fs+rk \u2212 2|r|\u2206. Similarly, if fs(\u03b1) < fsk , then fs+r(\u03b1)\u2212 |r|\u2206 \u2264 fs(\u03b1) < fsk \u2264 fs+rk + |r|\u2206 which implies fs+r(\u03b1) < fs+rk + 2|r|\u2206.\nLemma 2 gives us a way to track episodes that have potential to be in the top-k of future batches. This is an important property which our algorithm exploits and we recorded this as a remark below.\nRemark 1. The top-k episodes of batch, Bs+r, r \u2208 Z, must have batch frequencies of at least (fsk \u2212 2|r|\u2206) in batch Bs. Specifically, the top-k episodes of Bs+1 must have batch frequencies of at least (f s k \u2212 2\u2206) in Bs.\nBased on the maximum rate of change property we can derive a necessary condition for any episode to be top-k over a window. The following theorem prescribes the minimum batch frequencies that an episode must satisfy if it is a top-k episode over the window Ws.\nTheorem 1 (Exact Top-k over Ws). An episode, \u03b1, can be a top-k episode over window Ws only if its batch frequencies satisfy f s\n\u2032 (\u03b1) \u2265 (fs\u2032k \u2212 2(m\u2212 1)\u2206) \u2200Bs\u2032 \u2208Ws.\nProof. Consider an episode \u03b2 for which fs \u2032 (\u03b2) < (fs \u2032 k \u2212 2(m\u2212 1)\u2206) in batch Bs\u2032 \u2208Ws. Let \u03b1 be any top-k episode of Bs\u2032 . In any other batch Bp \u2208Ws, we have fp(\u03b1) \u2265 fs\u2032(\u03b1)\u2212 |p\u2212 s\u2032|\u2206 \u2265 fs\u2032k \u2212 |p\u2212 s\u2032|\u2206 (6) and\nfp(\u03b2) \u2264 f s\u2032(\u03b2) + |p\u2212 s\u2032|\u2206 < (fs \u2032 k \u2212 2(m\u2212 1)\u2206) + |p\u2212 s\u2032|\u2206 (7)\nApplying |p\u2212 s\u2032| \u2264 (m\u2212 1) to the above, we get fp(\u03b1) \u2265 fs\u2032k \u2212 (m\u2212 1)\u2206 > fp(\u03b2) (8)\nThis implies fWs(\u03b2) < fWs(\u03b1) for every top-k episode \u03b1 of Bs\u2032 . Since there are at least k top-k episodes in Bs\u2032 , \u03b2 cannot be a top-k episode over the window Ws.\nBased on Theorem 1 we can have the following simple algorithm for obtaining the top-k episodes over a window: Use a traditional level-wise approach to find all episodes with a batch frequency of at least (fk1 \u2212 2(m\u2212 1)\u2206) in the first batch (B1), simply accumulate their corresponding batch frequencies over all m batches of Ws and report the episodes with the k highest window frequencies over Ws. This approach is guaranteed to give us the exact top-k episodes over Ws. Further, in order to report the top-k over the next sliding window Ws+1, we need to consider all episodes with batch frequency of at least (f k 2 \u2212 2(m\u2212 1)\u2206) in the second batch and track them over all batches of Ws+1, and so on. Thus, an exact solution to Problem 1 would require running a level-wise episode mining algorithm in every batch, Bs, s = 1, 2, . . ., with a frequency threshold of (fks \u2212 2(m\u2212 1)\u2206).\n4.1 Class of (v, k)-Persistent Episodes\nTheorem 1 characterizes the minimum batchwise computation needed in order to obtain the exact top-k episodes over a sliding window. This is effective when \u2206 and m are small (compared to fks ). However, the batchwise frequency thresholds can become very low in other settings, making the processing time per-batch as well as the number of episodes to track over the window to become impractically high. To address this issue, we introduce a new class of episodes called (v, k)-persistent episodes which can be computed efficiently by employing higher batchwise thresholds. Further, we show that these episodes can be used to approximate the true top-k episodes over the window and the quality of approximation is characterized in terms of the top-k separation property (cf. Definition 4).\nDefinition 5 ((v, k)-Persistent Episode). A pattern is said to be (v, k)-persistent over window Ws if it is a top-k episode in at least v batches of Ws.\nProblem 2 (Mining (v, k)-Persistent Episodes). For each new batch, Bs, of events in the stream, find all `-node (v, k)-persistent episodes in the corresponding window of interest, Ws.\nTheorem 2. An episode, \u03b1, can be (v, k)-persistent over the window Ws only if its batch frequencies satisfy fs \u2032 (\u03b1) \u2265 (fs\u2032k \u2212 2(m\u2212 v)\u2206) for every batch Bs\u2032 \u2208Ws.\nProof. Let \u03b1 be (v, k)-persistent over Ws and let V\u03b1 denote the set of batches in Ws in which \u03b1 is in the top-k. For any Bq /\u2208 V\u03b1 there exists Bp\u0302(q) \u2208 V\u03b1 that is nearest to Bq. Since |V\u03b1| \u2265 v, we must have |p\u0302(q)\u2212 q| \u2264 (m\u2212 v). Applying Lemma 2 we then get f q(\u03b1) \u2265 f qk \u2212 2(m\u2212 v)\u2206 for all Bq /\u2208 V\u03b1.\nTheorem 2 gives us the necessary conditions for computing all (v, k)-persistent episodes over sliding windows in the stream. The batchwise threshold required for (v, k)-persistent episodes depends on the parameter v. For v = 1, the threshold coincides with the threshold for exact top-k in Theorem 1. The threshold increases linearly with v and is highest at v = m (when the batchwise threshold is same as the corresponding batchwise top-k frequency).\nThe algorithm for discovering (v, k)-persistent episodes follows the same general lines as the one described earlier for exact top-k mining, only that we now apply higher batchwise thresholds: For each new batch, Bs, entering the stream, use a standard level-wise episode mining algorithm to find all episodes with batch frequency of at least (fks \u2212 2(m\u2212 v)\u2206). (We provide more details of our algorithm later in Sec. 4.2). First, we investigate the quality of approximation of top-k that (v, k)-persistent episodes offer and show that the number of errors is closely related to the degree of top-k separation in the data.\n4.1.1 Top-k Approximation\nThe main idea here is that, under a maximum rate of change \u2206 and a top-k separation of (\u03d5, ), there cannot be too many distinct episodes which are not (v, k)-persistent, while having sufficiently high window frequencies. To this end, we first compute a lower-bound (fL) on the window frequencies of (v, k)-persistent episodes and an upper-bound (fU ) on the window frequencies of episodes that are not (v, k)-persistent (cf. Lemmas 3 & 4).\nLemma 3. If episode \u03b1 is (v, k)-persistent over a window, Ws, then its window frequency, f Ws(\u03b1), must satisfy the following lower-bound:\nfWs(\u03b1) \u2265 \u2211 Bs\u2032 fs \u2032 k \u2212 (m\u2212 v)(m\u2212 v + 1)\u2206 def = fL (9)\nProof. Consider episode \u03b1 that is (v, k)-persistent over Ws and let V\u03b1 denote the batches of Ws in which \u03b1 is in the top-k. The window frequency of \u03b1 can be written as\nfWs(\u03b1) = \u2211 Bp\u2208V\u03b1 fp(\u03b1) + \u2211 Bq\u2208Ws\\V\u03b1 f q(\u03b1)\n\u2265 \u2211 Bp\u2208V\u03b1 fpk + \u2211 Bq\u2208Ws\\V\u03b1 f qk \u2212 2|p\u0302(q)\u2212 q|\u2206\n= \u2211\nBs\u2032\u2208Ws\nfs \u2032 k \u2212 \u2211 Bq\u2208Ws\\V\u03b1 2|p\u0302(q)\u2212 q|\u2206 (10)\nwhere Bp\u0302(q) \u2208 V\u03b1 denotes the batch nearest Bq where \u03b1 is in the top-k. Since |Ws \\ V\u03b1| \u2264 (m\u2212 v), we must have \u2211\nBq\u2208Ws\\V\u03b1\n|p\u0302(q)\u2212 q| \u2264 (1 + 2 + \u00b7 \u00b7 \u00b7+ (m\u2212 v))\n= 1\n2 (m\u2212 v)(m\u2212 v + 1) (11)\nPutting together (10) and (11) gives us the lemma.\nLemma 4. If episode \u03b2 is not (v, k)-persistent over a window, Ws, then its window frequency, f Ws(\u03b2), must satisfy the following upper-bound:\nfWs(\u03b2) < \u2211 Bs\u2032 fs \u2032 k + v(v + 1)\u2206 def = fU (12)\nProof. Consider episode \u03b2 that is not (v, k)-persistent over Ws and let V\u03b2 denote the batches of Ws in which \u03b2 is in the top-k. The window frequency of \u03b2 can be written as:\nfWs(\u03b2) = \u2211 Bp\u2208V\u03b2 fp(\u03b2) + \u2211 Bq\u2208Ws\\V\u03b2 f q(\u03b2)\n< \u2211 Bp\u2208V\u03b2 fpk + 2|p\u0302(q)\u2212 q|\u2206 + \u2211 Bq\u2208Ws\\V\u03b2 f qk\n= \u2211\nBs\u2032\u2208Ws\nfs \u2032 k + \u2211 Bp\u2208V\u03b2 2|q\u0302(p)\u2212 p|\u2206 (13)\nwhere Bq\u0302(p) \u2208 Ws \\ V\u03b2 denotes the batch nearest Bp where \u03b2 is not in the top-k. Since |V\u03b2| < v, we must have \u2211\nBp\u2208V\u03b2\n|q\u0302(p)\u2212 p| \u2264 (1 + 2 + \u00b7 \u00b7 \u00b7+ (v \u2212 1))\n= 1\n2 v(v + 1) (14)\nPutting together (13) and (14) gives us the lemma.\nIt turns out that fU > fL \u2200v, 1 \u2264 v \u2264 m, and hence there is always a possibility for some episodes which are not (v, k)-persistent to end up with higher window frequencies than one or more (v, k)-persistent episodes. We observed a specific instance of this kind of \u2018mixing\u2019 in our motivating example as well (cf. Example 1). This brings us to the top-k separation property that we introduced in Definition 4. Intuitively, if there is sufficient separation of the top-k episodes from the rest of the episodes in every batch, then we would expect to see very little mixing. As we shall see, this separation need not occur exactly at kth most-frequent episode in every batch, somewhere close to it is sufficient to achieve a good top-k approximation.\nDefinition 6 (Band Gap Episodes, G\u03d5). In any batch Bs\u2032 \u2208 Ws, the half-open frequency interval [fks\u2032 \u2212 \u03d5\u2206, fks\u2032) is called the band gap of Bs\u2032. The corresponding set, G\u03d5, of band gap episodes over the window Ws, is defined as the collection of all episodes with batch frequencies in the band gap of at least one Bs\u2032 \u2208Ws.\nThe main feature of G\u03d5 is that, if \u03d5 is large-enough, then the only episodes which are not (v, k)-persistent but that can still mix with (v, k)-persistent episodes are those belonging to G\u03d5. This is stated formally in the next lemma.\nLemma 5. If \u03d52 > max{1, (1 \u2212 vm)(m \u2212 v + 1)}, then any episode \u03b2 that is not (v, k)-persistent over Ws, can have fWs(\u03b2) \u2265 fL only if \u03b2 \u2208 G\u03d5.\nProof. If an episode \u03b2 is not (v, k)-persistent over Ws then there exists a batch Bs\u2032 \u2208 Ws where \u03b2 is not in the top-k. Further, if \u03b2 /\u2208 G\u03d5 then we must have fs\u2032(\u03b2) < fks\u2032 \u2212 \u03d5\u2206. Since \u03d5 > 2, \u03b2 cannot be in the top-k of any neighboring batch of Bs\u2032 , and hence, it will stay below f k s\u2032 \u2212 \u03d5\u2206 for all Bs\u2032 \u2208Ws, i.e.,\nfWs(\u03b2) < \u2211\nBs\u2032\u2208Ws\nfks\u2032 \u2212m\u03d5\u2206.\nThe Lemma follows from the given condition \u03d52 > (1\u2212 vm)(m\u2212 v + 1).\nThe number of episodes in G\u03d5 is controlled by the top-k separation property, and since many of the non-persistent episodes which can mix with persistent ones must spend not one, but several batches in the band gap, the number of unique episodes that can cause such errors is bounded. Theorem 3 is our main result about quality of top-k approximation that (v, k)-persistence can achieve.\nTheorem 3 (Quality of Top-k Approximation). Let every batch Bs\u2032 \u2208Ws have a top-k separation of (\u03d5, ) with \u03d52 > max{1, (1 \u2212 vm)(m \u2212 v + 1)}. Let P denote the set of all (v, k)-persistent episodes over Ws. If |P| \u2265 k, then the top-k episodes over Ws can be determined from P with an error of no more than ( km \u00b5 ) episodes, where \u00b5 = min{m\u2212 v + 1, \u03d52 , 12( \u221a 1 + 2m\u03d5\u2212 1)}.\nProof. By top-k separation, we have a maximum of (1 + )k episodes in any batch Bs\u2032 \u2208 Ws, with batch frequencies greater than or equal to fks\u2032 \u2212 \u03d5\u2206. Since at least k of these must belong to the top-k of the Bs\u2032 , there are no more than k episodes that can belong to the band gap of Bs\u2032 . Thus, there can be no more than a total of km episodes over all m batches of Ws that can belong to G\u03d5.\nConsider any \u03b2 /\u2208 P with fWs(\u03b2) \u2265 fL \u2013 these are the only episodes whose window frequencies can exceed that of any \u03b1 \u2208 P (since fL is the minimum window frequency of any \u03b1). If \u00b5 denotes the minimum number of batches in which \u03b2 belongs to the band gap, then there can be at most ( km \u00b5 ) such distinct\n\u03b2. Thus, if |P| \u2265 k, we can determine the set of top-k episodes over Ws with error no more than ( km \u00b5 ) episodes.\nThere are now two cases to consider to determine \u00b5: (i) \u03b2 is in the top-k of some batch, and (ii) \u03b2 is not in the top-k of any batch.\nCase (i): Let \u03b2 be in the top-k of Bs\u2032 \u2208Ws. Let Bs\u2032\u2032 \u2208Ws be t batches away from Bs\u2032 . Using Lemma 2 we get fs\n\u2032\u2032 (\u03b2) \u2265 fks\u2032\u2032 \u2212 2t\u2206. The minimum t for which (fks\u2032\u2032 \u2212 2t\u2206 < fks \u2212 \u03d5\u2206) is (\u03d5 2 ) . Since \u03b2 /\u2208 P, \u03b2 is\nbelow the top-k in at least (m\u2212 v+ 1) batches. Hence \u03b2 stays in the band gap of at least min{m\u2212 v+ 1, \u03d52 } batches of Ws.\nCase (ii): Let VG denote the set of batches in Ws where \u03b2 lies in the band gap and let |VG| = g. Since \u03b2 does not belong to top-k of any batch, it must stay below the band gap in all the (m \u2212 g) batches of (Ws \\ VG). Since \u2206 is the maximum rate of change, the window frequency of \u03b2 can be written as follows:\nfWs(\u03b2) = \u2211\nBp\u2208VG\nfp(\u03b2) + \u2211\nBq\u2208Ws\\VG\nf q(\u03b2)\n< \u2211\nBp\u2208VG\nfp(\u03b2) + \u2211\nBq\u2208Ws\\VG\n(fkq \u2212 \u03d5\u2206) (15)\nLet Bq\u0302(p) denote the batch in Ws \\ VG that is nearest to Bp \u2208 VG. Then we have:\nfp(\u03b2) \u2264 f q\u0302(p)(\u03b2) + |p\u2212 q\u0302(p)|\u2206 < fkq\u0302(p) \u2212 \u03d5\u2206 + |p\u2212 q\u0302(p)|\u2206 < fkp \u2212 \u03d5\u2206 + 2|p\u2212 q\u0302(p)|\u2206 (16)\nwhere the second inequality holds because \u03b2 is below the band gap in Bq\u0302(p) and (16) follows from Lemma 1. Using (16) in (15) we get\nfWs(\u03b2) < \u2211\nBs\u2032\u2208Ws\nfks\u2032 \u2212m\u03d5\u2206 + \u2211\nBp\u2208VG\n2|p\u2212 q\u0302(p)|\u2206\n< \u2211\nBs\u2032\u2208Ws\nfks\u2032 \u2212m\u03d5\u2206 + 2(1 + 2 + \u00b7 \u00b7 \u00b7+ g)\u2206\n= \u2211\nBs\u2032\u2208Ws\nfks\u2032 \u2212m\u03d5\u2206 + g(g + 1)\u2206 = UB (17)\nThe smallest g for which (fWs(\u03b2) \u2265 fL) is feasible can be obtained by setting UB \u2265 fL. Since \u03d52 > (1\u2212 vm)(m\u2212 v + 1), UB \u2265 fL implies\u2211\nBs\u2032\u2208Ws\nfks\u2032 \u2212m\u03d5\u2206 + g(g + 1)\u2206 > \u2211\nBs\u2032\u2208Ws\nfks\u2032 \u2212 m\u03d5\u2206\n2\nSolving for g, we get g \u2265 12( \u221a\n1 + 2m\u03d5 \u2212 1). Combining cases (i) and (ii), we get \u00b5 = min{m \u2212 v + 1, \u03d52 , 1 2( \u221a 1 + 2m\u03d5\u2212 1)}.\nTheorem 3 shows the relationship between the extent of top-k separation required and quality of topk approximation that can be obtained through (v, k)-persistent episodes. In general, \u00b5 increases with \u03d52 until the latter starts to dominate the other two factors, namely, (m \u2212 v + 1) and 12( \u221a 1 + 2m\u03d5 \u2212 1). The theorem also brings out the tension between the persistence parameter v and the quality of approximation. At smaller values of v, the algorithm mines \u2018deeper\u2019 within each batch and so we expect fewer errors with respect to the true top-k epispodes. On the other hand, deeper mining within batches is computationally more intensive, with the required effort approaching that of exact top-k mining as v approaches 1. Finally, we use Theorem 3 to derive error-bounds for three special cases; first for v = 1, when the batchwise threshold is same as that for exact top-k mining as per Theorem 1; second for v = m, when the batchwise threshold is simply the batch frequency of the kth most-frequent episode in the batch; and third, for v = \u230a m+1 2 \u230b , when the batchwise threshold lies midway between the thresholds of the first two cases.\nCorollary 1. Let every batch Bs\u2032 \u2208Ws have a top-k separation of (\u03d5, ) and let Ws contain at least m \u2265 2 batches. Let P denote the set of all (v, k)-persistent episodes over Ws. If we have |P| \u2265 k, then the maximum number of errors in the top-k episodes derived from P, for three different choices of v, is given by:\n1. ( km m\u22121 ) , for v = 1, if \u03d52 > (m\u2212 1)\n2. ( km), for v = m, if \u03d52 > 1\n3. ( 4 km2\nm2\u22121\n) , for v = \u230a m+1 2 \u230b , if \u03d52 > 1 m \u2308 m\u22121 2 \u2309 \u2308 m+1 2 \u2309 Proof. We show the proof only for v = \u230a m+1 2 \u230b . The cases of v = 1 and v = m are obtained immediately upon application of Theorem 3. Fixing v = \u230a m+1 2 \u230b implies (m\u2212 v) = \u2308 m\u22121 2 \u2309 . For m \u2265 2, \u03d52 > 1m \u2308 m\u22121 2 \u2309 \u2308 m+1 2 \u2309 implies \u03d52 > max{1, (1\u2212 v m)(m\u2212 v + 1)}. Let tmin = min{m\u2212 v + 1, \u03d5 2 }. The minimum value of tmin is governed by\ntmin \u2265 min {\u2308 m+ 1\n2\n\u2309 , 1\nm\n\u2308 m\u2212 1\n2\n\u2309\u2308 m+ 1\n2 \u2309} = 1\nm\n\u2308 m\u2212 1\n2\n\u2309\u2308 m+ 1\n2 \u2309 \u2265 ( m2 \u2212 1\n4m\n) (18)\nLet gmin = 1 2( \u221a 1 + 2m\u03d5\u2212 1). \u03d5 > 2m \u2308 m\u22121 2 \u2309 \u2308 m+1 2 \u2309 implies gmin > ( m\u22121 2 ) . From Theorem 3 we have\n\u00b5 = min{tmin, gmin} \u2265 ( m2 \u2212 1\n4m ) and hence the number of errors is no more than ( 4 km2\nm2\u22121\n) ."}, {"heading": "4.2 Incremental Algorithm", "text": "In this section we present an efficient algorithm for incrementally mining patterns with frequency \u2265 (fsk\u2212\u03b8). From our formalism, the value of \u03b8 is specified by the type of patterns we want to mine. For (v, k) persistence, \u03b8 = 2(m\u2212 v)\u2206 whereas for mining the exact top-k the threshold is 2(m\u2212 1)\u2206.\nRecall that the goal of our mining task is to report frequent patterns of size `. After processing the data in the batch Bs\u22121, we desire all patterns with frequency greater than (f s\u22121 k \u2212 \u03b8). Algorithmically this is achieved by first setting a high frequency threshold and mining for patterns using the classical level wise Apriori method [2]. If the number of patterns of size-` is less than k, the support threshold is decreased and the mining repeated until atleast k `-size patterns are found. At this point fsk is known. The mining process\nis repeated once more with the frequency threshold (f sk \u2212 \u03b8). Doing this entire procedure for every new batch can be expensive and wasteful. After seeing the first batch of the data, whenever a new batch arrives we have information about the patterns that were frequent in the previous batch. This can be exploited to incrementally and efficiently update the set of frequent episodes in the new batch. The intuition behind this is that the frequencies of the majority of episodes do not change much from one batch to the next. As a result a small number of episode fall below the new support threshold in the new batch. There is also the possibility of some new episodes becoming frequent. This is illustrated in Figure 3. In order to efficiently find these sets of episodes, we need to maintain additional information that allows us to avoid full-blown candidate generation. We show that this state information is a by-product of Apriori algorithm and therefore any extra processing is unnecessary.\nIn the Apriori algorithm, frequent patterns are discovered iteratively, in ascending order of their size and it is often referred to as a levelwise procedure. The procedure alternates between counting and candidate generation. First a set Ci of candidate i-size patterns is created by joining the frequent (i\u2212 1)-size itemsets found in the previous iteration. Then the data is scanned for determining the frequency or count of each candidate pattern and the frequent i-size patterns are extracted from the candidates. An interesting observation is that all candidate episodes that are not frequent constitute the negative border of the frequent lattice. This is true because, in the Apriori algorithm, a candidate pattern is generated only when all its subpatterns are frequent. The usual approach is to discard the border. For our purposes, the patterns in the border contain the information required to identify the change in the frequent sets from one batch to the next.\nThe pseudocode for incrementally mining frequent patterns in batches is listed in Algorithm 1. Let the frequent episodes of size-i be denoted by F is. Similarly, the border episodes of size-i are denoted by Bis. The frequency threshold used in each batch is fsk \u2212 \u03b8. In the first batch of data, the top-k patterns are found by progressively lowering the frequency threshold fmin by a small amount (Lines 1-8). Once atleast k patterns of size ` are found, fsk is determined and the mining procedure repeated with a threshold of f s k \u2212 \u03b8. The border patterns generated during level wise mining are retained. For subsequent batches, first fsk is determined. As shown in Remark 1, if \u03b8 \u2265 2\u2206, then the set of frequent patterns F `s\u22121 in batch Bs\u22121 contains all patterns that can be frequent in the next batch Bs. Therefore simply updating the counts of all patterns in F `s\u22121 in the batch Bs and picking the kth highest frequency gives fsk (Lines 10-11). The new frequency threshold fmin is set to be f s k \u2212 \u03b8. The procedure, starting from bottom (size-1 patterns) updates the lattice for Bs. The data is scanned to determine the frequency of new candidates together with the frequent and border patterns from the lattice (Line 15-18). In the first level (patterns of size 1), the candidate set is empty. After counting, the patterns from the frequent set F `s\u22121 that continue to be frequent in the new batch are added to F `s . But if a pattern is no longer frequent it is marked as a border set and all its super episodes are deleted (Lines 19-24). This ensures that only border patterns are retained in the lattice. All patterns, either from the border set or the new candidate set, that are found to be frequent are added to F `s . Such episodes are also added to F inew. Any remaining infrequent patterns belong to border set because otherwise they would have atleast one of infrequent subpatterns and would have been deleted at a previous level (Line 24). These patterns are added to B`s (Line 30). The candidate generation step is required to fill out the missing parts of the frequent lattice. We want to avoid a full blown candidate generation. Note that if a pattern is frequent in Bs\u22121 and Bs then all its subpatterns are also frequent in both Bs and Bs\u22121. Any new pattern (6\u2208 F `s\u22121 \u222a B`s\u22121) that turns frequent in Bs, therefore, must have atleast one subpattern that was not frequent in Bs\u22121 but is frequent in Bs. All such patterns are listed in F inew. The candidate generation step (Line 31) for the next level generates only candidate patterns with atleast one subpattern \u2208 F inew. This greatly restricts the number of candidates generated at each level without compromising the completeness of the results.\nThe space and time complexity of the candidate generation is now O(|F inew|.|F is|) instead of O(|F is|2) and in most practical cases |F inew| |F is|. This is crucial in a streaming application where processing rate must match the data arrival rate.\nFor a window Ws ending in the batch Bs, the set of output patterns can be obtained by picking the top-k most frequent patterns from the set F `s . Each pattern also maintains a list that stores its batch-wise counts is last m batches. The window frequency is obtained by adding these entries together. The output\nAlgorithm 1 Mine top-k v-persistent patterns. Input: A new batch of events Bs, the lattice of frequent and border patterns (F\u2217s\u22121,B\u2217s\u22121), and parameters k and \u03b8 Output: The lattice of frequent and border patterns (F\u2217s ,B\u2217s)\n1: if s = 1 then 2: fmin = high value 3: while |F`s | < k do 4: Mine patterns with frequency \u2265 fmin 5: fmin = fmin \u2212 6: fsk = frequency of the k\nth most frequent pattern \u2208 F`s 7: Mine `-size patterns in Bs with frequency threshold fmin = f s k \u2212 \u03b8 8: Store the frequent and border patterns (of size = 1 . . . `) in (F\u2217s ,B\u2217s) 9: else\n10: CountPatterns(F`s\u22121, Bs) 11: Set fsk = frequency k\nth highest frequency (pattern \u2208 F`s\u22121) 12: Set frequency threshold for Bs, fmin = (f s k \u2212 \u03b8) 13: C1 = \u03d5 {New candidate patterns of size = 1} 14: for i = 1 . . . `\u2212 1 do 15: F is = \u03d5 {Frequent patterns of size i} 16: Bis = \u03d5 {Border patterns of size i} 17: F inew = \u03d5 {List of newly frequent Patterns} 18: CountPatterns(F is\u22121 \u222a Bis\u22121 \u222a Ci, Bs) 19: for \u03b1 \u2208 F is\u22121 do 20: if fs(\u03b1) \u2265 fmin then 21: F is = F is \u222a {\u03b1} 22: else 23: Bis = Bis \u222a {\u03b1} 24: Delete all its super-patterns from (F\u2217s\u22121,B\u2217s\u22121) 25: for \u03b1 \u2208 Bis\u22121 \u222a Ci do 26: if fs(\u03b1) \u2265 fmin then 27: F is = F is \u222a {\u03b1} 28: F inew = F i new \u222a {\u03b1} 29: else 30: Bis = Bis \u222a {\u03b1} 31: Ci+1 = GenerateCandidatei+1(F i new,F is) 32: return (F\u2217s ,B\u2217s)\npatterns are listed in decreasing order of their window counts.\nExample 2. In this example we illustrate the procedure for incrementally updating the frequent patterns lattice as a new batch Bs is processed (see Figure 4).\nFigure 4(A) shows the lattice of frequent and border patterns found in the batch Bs\u22121. ABCD is a 4-size frequent pattern in the lattice. In the new batch Bs, the pattern ABCD is no longer frequent. The pattern CDXY appears as a new frequent pattern. The pattern lattice in Bs is shown in Figure 4(B).\nIn the new batch Bs, AB falls out of the frequent set. AB now becomes the new border and all its super-patterns namely ABC, BCD and ABCD are deleted from the lattice.\nAt level 2, the border pattern XY turns frequent in Bs. This allows us to generate DXY as a new 3-size candidate. At level 3, DXY is also found to be frequent and is combined with CDX which is also frequent in Bs to generate CDXY as a 4-size candidate. Finally at level 4, CDXY is found to be frequent. This shows that border sets can be used to fill out the parts of the pattern lattice that become frequent in the new data."}, {"heading": "4.3 Estimating \u2206 dynamically", "text": "The parameter \u2206 in the bounded rate change assumption is a critical parameter in the entire formulation. But unfortunately the choice of the correct value for \u2206 is highly data-dependent. In the streaming setting, the characteristics of the data can change over time. Hence one predetermined value of \u2206 cannot be provided\nin any intuitive way. Therefore we estimate \u2206 from the frequencies of `-size episodes in consecutive windows. We compute the differences in frequencies of episodes that are common in consecutive batches. Specifically, we consider the value at the 75th percentile as an estimate of \u2206. We avoid using the maximum change as it tends to be noisy. A few patterns exhibiting large changes in frequency can skew the estimate and adversely affect the mining procedure."}, {"heading": "5 Results", "text": "In this section we present results both on synthetic data and data from real neuroscience experiments. We compare the performance of the proposed streaming episode mining algorithm on synthetic data to quantify the effect of different parameter choices and data characteristics on the quality of the top-k episodes reported by each method. Finally we show the quality of results obtained on neuroscience data.\nFor the purpose of comparing the quality of results we setup the following six variants of the mining frequent episodes:\nAlg 0: This is the naive brute force top-k mining algorithm that loads an entire window of events at a time and mines the top-k episode by repeatedly lowering the frequency threshold for mining. When a new batch arrives, events from the oldest batch are retired and mining process is repeated from scratch. This method acts as the baseline for comparing all other algorithms in terms of precision and recall.\nAlg 1: The top-k mining is done batch-wise. The top-k episodes over a window are reported from within the set of episodes that belong to the batch-wise top-k of atleast one batch in the window.\nAlg 2: Here the algorithm is same is above, but once an episodes enters the top-k in any of the batches in a window, it is tracked over several subsequent batches. An episode is removed from the list of episodes being tracked if it does not occur in the top-k of last m consecutive batches. This strategy helps obtaining a larger candidate set and also in getting more accurate counts of candidate patterns over the window.\nAlg 3: This algorithm uses a batch-wise frequency threshold fsk \u2212 2\u03b4 which ensures that the top-k episodes in the next batch Bs+1 are contained in the frequent lattice of Bs. This avoids multiple passes of the data while trying to obtain k most frequent episodes lowering the support threshold iteratively. The patterns with frequency between fsk and f s k \u2212 2\u03b4 also improve the overall precision and recall with\nrespect to the window.\nAlg 4: In this case the batch-wise frequency threshold is fsk \u2212 2(m\u2212 v)\u03b4 which guarantees finding all (v, k)persistent episodes in the data. We report results for v = 3m/4 and v = m/2.\nAlg 5: Finally, this last algorithm uses a heuristic batchwise threshold of f sk \u2212m(2\u2212 vm \u2212 ( vm)2)\u2206. Again we report results for v = 3m/4 and v = m/2."}, {"heading": "5.1 Synthetic Datasets", "text": "The datasets we used for experimental evaluation are listed in Table 2. The name of the data set is listed in Column 1, the length of the data set (or number of time-slices in the data sequence) in Column 2, the size of the alphabet (or total number of event types) in Column 3, the average rest firing rate in Column 4 and the number of patterns embedded in Column 5. In these datasets the data length is varied from - million to - million events, the alphabet size is varied from 1000 to 5000, the resting firing rate from 10.0 to 25.0, and the number of patterns embedded in the data from 25 to 50.\nData generation model: The data generation model for synthetic data is based on the inhomogeneous Poisson process model for evaluating the algorithm for learning excitatory dynamic networks [13]. We introduce two changes to this model. First, in order to mimic real data more closely in the events that constitute the background noise the event-type distribution follows a power law distribution. This gives the long tail characteristics to the simulated data.\nThe second modification was to allow the rate of arrival of episodes to change over time. As time progresses, the frequency of episodes in the recent window or batch slowly changes. We use a randomized scheme to update the connection strengths in the neuronal simulation model. The updates happen at the same timescale as the batch sizes used for evaluation."}, {"heading": "5.2 Comparison of algorithms", "text": "In Fig. 5, we compare the five algorithms\u2014Alg 1 through Alg 5\u2014that report the frequent episodes over the window looking at one batch at a time with the baseline algorithm Alg 0 that stores and processes the entire window at each window slide. The results are averaged over all 9 data sets shown in Table 2. We expect to marginalize the data characteristics and give a more general picture of each algorithm. The parameter settings for the experiments are shown in Table 3. Fig. 5 (a) plots the precision of the output of each algorithm compared to that of Alg 0 (treated as ground truth). Similarly, Fig 5 (b) shows the recall. Since the size of output of each algorithm is roughly k, the corresponding precision and recall numbers are almost the same. Average runtimes are shown in Fig. 5 (c) and average memory requirement in MB is shown in Fig. 5 (d).\nWe consistently observe that Alg 1 and 2 give lower precision and recall values compared with any other algorithm. This reinforces our observation that the top-k patterns in a window can be much different from the top-k patterns in the constituent batches. Alg 2 provides only a slight improvement over Alg 1 by tracking an episode once it enters the top-k over subsequent batches. This improvement can be attributed to the fact that window frequencies of patterns that were once in the top-k is better estimated. Alg 3 gives higher precision and recall compared to Alg 1 and 2. The frequency threshold used in Alg 3 is given by Theorem 1. Using this result we are able to estimate the value fsk \u2212 2\u03b4 by simply counting the episodes that are frequent in the previous batch. This avoids multiple iterations required in general for finding the top-k patterns. Fortunately this threshold also results in significantly higher support and precision.\nWe ran Alg 4 and 5 for two different values of v, viz. v = m/2 and v = 3m/4. Both these algorithms guarantee finding all (v, k)-persistent patterns for respective values of v. For the sake of comparison we also include episodes that exceeded the frequency threshold prescribed by (v, k)-persistence, but do not appear in top-k of atleast v batches. We observe that the precision and recall improves a little over Alg 3 with\nreasonable increase in memory and runtime requirements. For a higher value of v, this algorithm insists that the frequent patterns much persist over more batches. This raises the support threshold and as a result there is improvement in terms of memory and runtime, but a small loss in precision and recall. Note that the patterns missed by the algorithm are either not (v, k)-persistent or our estimation of \u03b4 has some errors in it. In addition, Alg 5 gives a slight improvement over Alg 4. This shows that our heuristic threshold is effective.\nOverall the proposed methods give atleast one order of magnitude improvement over the baseline algorithm in terms of both time and space complexity."}, {"heading": "5.2.1 Performance over time", "text": "Next we consider one dataset A2 with number of event types = 1000, the average resting firing rate as 10 Hz, and the number of embedded patterns = 50. On this data we show how the performances of the five algorithms change over time. The window size is set to be m = 10 and the batch size = 105 sec. Fig. 6 shows the comparison over 50 contiguous batches. Fig. 6 (a) and (b) show the way precision and recall evolve over time. Fig. 6 (c) and (d) show the matching memory usage and runtimes.\nThe data generation model allows the episode frequencies to change slowly over time. In the dataset used in the comparison we change the frequencies of embedded episodes at two time intervals: batch 15 to 20 and batch 35 to 42. In Fig. 6 (a) and (b), we have a special plot shown by the dashed line. This is listed as Alg 0 in the legend. What this line shows is the comparison of top-k episodes between consecutive window slides. In other words the top-k episodes in window Ws\u22121 are considered as the predicted output to obtain the precision and recall for Ws. The purpose of this curve is to show how the true top-k set changes with time and show how well the proposed algorithms track this change.\nAlg 1 and 2 perform poorly. On an average in the transient regions (batch 15 to 20 and batch 35 to 42) they perform 15 to 20% worse than any other method. Alg 3, 4 and 5 (for v=0.75m and v=0.5m) perform consistently above the reference curve of Alg 0. It expected of any reasonable algorithm to do better than the algorithm which uses the top-k of Ws\u22121 to predict the top-k of the window Ws. The precision and recall performance are in the order Alg 3 < Alg 4 v=0.75m < Alg 4 v=0.5m < Alg 5 v=0.75m < Alg 4 v=0.5m. This is in the same order as the frequency thresholds used by each method, and as expected.\nIn terms of runtime and memory usage, the changing top-k does not affect these numbers. The lowest runtimes are those of Alg 3. The initial slope in the runtimes and memory usage seen in Algo 0, is due to the fact that the algorithm loads the entire window, one batch at a time into memory. In this experiment the window consists of m = 10 batches. Therefore only after the first 10 batches one complete window span is available in memory."}, {"heading": "5.2.2 Effect of Data Characteristics", "text": "In this section we present results on synthetic data with different characteristics, namely, number of event types (or alphabet size), noise levels and number of patterns embedded in the data.\nIn Fig. 7 we report the effect alphabet size on the quality of result of the different algorithms. In datasets A1, A2 and A3 the alphabet size, i.e. the number of distinct event types, is varied from 500 to 5000. We observe that for smaller alphabet sizes the performance is better. Alg 1 and 2 perform consistently worse that the other algorithm for different alphabet sizes.\nIn this experiment we find that the quality of results for the proposed algorithms is not very sensitive to alphabet size. The precision and recall numbers drop by only 2-4%. This is quite different from the pattern mining setting where the user provides a frequency threshold. In our experience alphabet size is critical in the fixed frequency threshold based formulation. For low thresholds, large alphabet sizes can quickly lead to uncontrolled growth in the number of candidates. In our formulation the support threshold is dynamically readjusted and as a result the effect of large alphabet size is attenuated.\nNext, in Fig. 8, we show the effect of noise. The average rate of firing of the noise event-types (event types that do not participate in pattern occurrences) is varied from 2.0 Hz to 25 Hz. The precision and recall of Alg 1 and 2 degrade quickly with increase in noise. A small decrease in precision and recall of Alg 3 and Alg 4 is seen. But the performance of Alg 5 (for both v=0.75m and v=0.5m) stays almost at the\nsame level. It seems that the frequency threshold generated by Alg 5 is sufficiently low to finds the correct patterns even at higher noise level but not so low as to require significantly more memory (\u2248 400 MB) or runtime (\u2248 70 sec per batch at noise = 25.0 Hz for v=0.5m) as compared to other algorithms.\nIn Fig. 9, we change the number of patterns embedded in the data and study the effect. The number of embedded patterns vary from 10 to 50. Once again the performance of our proposed methods is fairly flat in all the metrics. Alg 3, 4 and 5 are seen to be less sensitive to the number of patterns embedded in the data than Alg 1 and 2."}, {"heading": "5.2.3 Effect of Parameters", "text": "So far the discussion has been about the effect of data characteristics of the synthetic data. The parameters of the mining algorithms were kept fixed. In this section we look at two important parameters of the algorithms, namely, the batch size Tb and the number of batches that make up a window, m.\nIn Fig. 10, the quality and performance metrics are plotted for three different batch sizes: 103, 104 and 105 (in sec). Batch size appears to have a significant effect on precision and recall. There is a 10% decrease in both precision and recall when batch size is reduced to 103 sec from 105. But note that a 100 fold decrease in batch size only changes quality of result by 10%.\nIt is not hard to imagine that for smaller batch sizes the episode statistics can have higher variability in different batches resulting in a lower precision and recall over the window. As the size of the batch grows the top-k in the batch starts to resemble the top-k of the window. Transient patterns will not be able to gather sufficient support in a large batch size.\nAs expected, the runtimes and memory usage are directly proportional to the batch size in all cases. The extra space and time is required only to handle more data. Batch size does not play a role in growth of number of candidates in the mining process.\nNext in Fig. 11, we show how the number of windows in a batch affect the performance. Precision and recall are observed to decrease linearly with the number of batches in a window in Fig. 11(a) and (b), whereas the memory and runtime requirements grow linearly with the number of batches. The choice of number of batches provides the trade-off between the window size over which the user desires the frequent persistent patterns and the accuracy of the results. For larger window sizes the quality of the results will be poorer. Note that the memory usage and runtime does not increase much for algorithms other than Alg 0 (see Fig. 11 (c) and (d)). Because these algorithms only process one batch of data irrespective of the number of batches in window. Although for larger windows the batch-wise support threshold decreases. In the synthetic datasets we see that this does not lead to unprecedented increase in the number of candidates."}, {"heading": "5.3 Multi-neuronal Data", "text": "Multi-electrode arrays provide high throughput recordings of the spiking activity in neuronal tissue and are hence rich sources of event data where events correspond to specific neurons being activated. We used the data from dissociated cortical cultures gathered by Steve Potter\u2019s laboratory at Georgia Tech [15] over several days. This is a rich collection of recordings from a 64-electrode MEA setup.\nWe show the result of mining frequent episodes in the data collected over several days from Culture 6 [15]. We use a batch size of 150 sec and all other parameters for mining are the same as that used for the synthetic data. The plots in Fig. 12 show the performance of the different algorithms as time progresses. Alg 1 and 2 give very low precision values which implies that the top-k in a batch is much different from the top-k in the window. Alg 3, 4 and 5 perform equally well over the MEA data with Alg 3 giving the best runtime performance.\nAt times Alg 3 requires slightly higher memory than the other algorithm (Alg 4 and 5). This may seem counter intuitive as Alg 4 and 5 use lower frequency threshold. But since \u03b4 is dynamically estimated from all episodes being tracked by the algorithm it can easily be the case that the \u03b4 estimates made by Alg 3 are looser and hence result in higher memory usage."}, {"heading": "6 Related work", "text": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3]. Some interesting algorithms have also been proposed for streaming motif mining in time-series data [11]. But these methods do not easily extend to other pattern classes like episodes and partial orders. To our knowledge, there has been very little work in the area of mining patterns in discrete event streams. In this section we discuss some of the existing methods for itemsets, sequential patterns, and motifs.\nKarp et al proposed a one pass streaming algorithm for finding frequent events in an item sequence [6]. The algorithm, at any given time, maintains a set K of event types and their corresponding counts. Initially, this set is empty. When an event is read from the input sequence, if the event type exists in the set then its count is incremented. Otherwise the event type is inserted into the set K with count 1. When the size of the set K exceeds b1/\u03b8c, the count of each event type in the set is decremented by 1 (and deleted from the set if count drops to zero). The key property is that any event type that occurs at least n\u03b8 times in the sequence is in the set K. Consider an event type that occurs f times in the sequence, but is not in K. Each occurrence of this event type is eliminated together with more than b1/\u03b8c \u2212 1 occurrences of other event types achieved by decrementing all counts by 1. Thus, at least a total of f/\u03b8 elements are eliminated. Thus f/\u03b8 < n, where n is the number of events in the sequences and hence, f < n\u03b8. This method guarantees no false negatives for a given support threshold. But the space and time complexity of this algorithm varies inversely with the support threshold chosen by the user. This can be a problem when operating at low support thresholds. In [5], this approach was extended to mine frequent itemsets.\nLossy counting constitutes another important class of streaming algorithms proposed by Manku and Motwani in 2002 [8]. In this work an approximate counting algorithm for itemsets is described. The algorithm stores a list of tuples which comprise an item or itemset, a lower bound on its count, and a maximum error term (\u2206). When processing the ith item, if it is currently stored then its count is incremented by one; otherwise, a new tuple is created with the lower bound set to one, and \u2206 set to bi c. Periodically, each tuple whose upper bound is less than bi c is deleted. This technique guarantees that the percentage error in reported counts is no more than and it is also shown that the space used by this algorithm is O(1 log n) for itemsets. Unfortunately, this method requires operating at very low support threshold in order to provide small enough error bounds. In [10], the pattern growth algorithm - PrefixSpan [14] for mining sequential patterns was extended to incorporate the idea of lossy counting.\nIn [3], the authors propose a new frequency measure for itemsets over data streams. The frequency of an itemset in a stream is defined as its maximal frequency over all windows in the stream from any point in the\npast until the current time that satisfy a minimal length constraint. They present an incremental algorithm that produces the current frequencies of all frequent itemsets in the data stream. The focus of this work is on the new frequency measure and its unique properties.\nIn [11] an online algorithm for mining time series motifs was proposed. The algorithm uses an interesting data structure to find a pair of approximately repeating subsequences in a window. The Euclidean distance measure is used to measure the similarity of the motif sequences in the window. Unfortunately this notion does not extend naturally to discrete patterns. Further, this motif mining formulation does not explicitly make use of a support or frequency threshold and returns exactly one pair of motifs that are found to be the closest in terms of distance.\nA particular sore point in pattern mining is coming up with a frequency threshold for the mining process. Choice of this parameter is key to the success of any effective strategy for pruning the exponential search space of patterns. Mining the top-k most frequent patterns has been proposed in the literature as a more\nintuitive formulation for the end user. In [13] we proposed an information theoretic principle for determining the frequency threshold that is ultimately used in learning a dynamic Bayesian network model for the data. In both cases the idea is to mine patterns at the highest possible support threshold to either outputs the top-k patterns or patterns that satisfy a minimum mutual information criteria. This is different from the approach adopted, for example, in lossy counting where the mining algorithm operates at support threshold proportional to the error bound. Therefore, in order to guarantee low errors, the algorithm tries to operate at the lowest possible threshold.\nAn episode or a general partial order pattern can be thought of as a generalization of itemsets where each item in the set is not confined to occur within the same transaction (i.e. at the same time tick) and there is additional structure in the form of ordering of events or items. In serial episodes, events must occur in exactly one particular order. Partial order patterns allow multiple orderings. In addition there could be repeated event types in an episode. The loosely coupled structure of events in an episode results in narrower separation between the frequencies of true and noisy patterns (i.e. resulting from random co-occurrences of events) and quickly leads to combinatorial explosion of candidates when mining at low frequency thresholds. Most of the itemset literature does not deal with the problem of candidate generation. The focus is on counting and not so much on efficient candidate generation schemes. In this work we explore ways of doing both counting and candidate generation efficiently. Our goal is to devise algorithms that can operate at as high frequency thresholds as possible and yet give certain guarantees about the output patterns."}, {"heading": "7 Conclusions", "text": "In this paper, we have studied the problem of mining frequent episodes over changing data streams. In particular our contribution in this work is three fold. We unearth an interesting aspect of temporal data mining where the data owner may desire results over a span of time in the data that cannot fit in the memory or be processed at a rate faster than the data generation rate. We have proposed a new sliding window model which slides forward in hops of batches. At any point only one batch of data is available for processing. We have studied this problem and identified the theoretical guarantees one can give and the necessary assumptions for supporting them.\nIn many real applications we find the need for characterizing pattern not just based on their frequency but also their tendency to persist over time. In particular, in neuroscience, the network structure underlying an ensemble of neurons changes much slowly in comparison to the culture wide periods bursting phenomenon. Thus separating the persistent patterns from the bursty ones can give us more insight into the underlying connectivity map of the network. We have proposed the notion of (v, k) persistent patterns to address this problem and outlined methods to mine all (v, k)-persistent patterns in the data. Finally we have provided detailed experimental results on both synthetic and real data to show the advantages of the proposed methods.\nFinally, we reiterate that although we have focused on episodes, the ideas presented in this paper could be applied to other pattern classes with similar considerations."}], "references": [{"title": "Discovering injective episodes with general partial orders", "author": ["A. Achar", "S. Laxman", "R. Viswanathan", "P.S. Sastry"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Fast Algorithms for Mining Association Rules in Large Databases", "author": ["R. Agrawal", "R. Srikant"], "venue": "In Proceedings of the 20th International Conference on Very Large Databases", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Mining frequent itemsets in a stream", "author": ["T. Calders", "N. Dexters", "B. Goethals"], "venue": "Proceedings of the 2007 Seventh IEEE International Conference on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "A survey on algorithms for mining frequent itemsets over data streams", "author": ["J. Cheng", "Y. Ke", "W. Ng"], "venue": "Knowledge and Information Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "An algorithm for in-core frequent itemset mining on streaming data", "author": ["R. Jin", "G. Agrawal"], "venue": "Proceedings of the Fifth IEEE International Conference on Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "A simple algorithm for finding frequent elements in streams and bags", "author": ["R.M. Karp", "S. Shenker", "C.H. Papadimitriou"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Discovering frequent episodes: Fast algorithms, Connections with HMMs and generalizations", "author": ["S. Laxman"], "venue": "PhD thesis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Approximate frequency counts over data streams", "author": ["G.S. Manku", "R. Motwani"], "venue": "In Proceedings of the 28th international conference on Very Large Data Bases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Discovery of frequent episodes in event sequences", "author": ["H. Mannila", "H. Toivonen", "A. Verkamo"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Stream sequential pattern mining with precise error bounds", "author": ["L. Mendes", "B. Ding", "J. Han"], "venue": "In Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Online discovery and maintenance of time series motif", "author": ["A. Mueen", "E. Keogh"], "venue": "In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Data streams: Algorithms and applications", "author": ["S. Muthukrishnan"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Discovering excitatory relationships using dynamic bayesian networks", "author": ["D. Patnaik", "S. Laxman", "N. Ramakrishnan"], "venue": "Knowledge and Information Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth", "author": ["J. Pei", "J. Han", "B. Mortazavi-Asl", "H. Pinto"], "venue": "In Proceedings of the 17th International Conference on Data Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "An extremely rich repertoire of bursting patterns during the development of cortical cultures", "author": ["D.A. Wagenaar", "J. Pine", "S.M. Potter"], "venue": "BMC Neuroscience,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Mining top-k frequent itemsets from data streams", "author": ["R.C.-W. Wong", "A.W.-C. Fu"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Such data are referred to as data streams [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "see [16]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "In the framework of frequent episodes [9], an event sequence is denoted as \u3008(e1, \u03c41), .", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Two occurrences of an episode are non-overlapped [7] if no event corresponding to one appears in-between the events corresponding to the other.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "Given a frequency threshold, Apriori-style level-wise algorithms [9, 1] can be used to obtain the frequent episodes in the event sequence.", "startOffset": 65, "endOffset": 71}, {"referenceID": 0, "context": "Given a frequency threshold, Apriori-style level-wise algorithms [9, 1] can be used to obtain the frequent episodes in the event sequence.", "startOffset": 65, "endOffset": 71}, {"referenceID": 3, "context": "The current Streaming patterns literature has also considered other models, such as the landmark and time-fading models [4], but we do not consider them in this paper.", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Algorithmically this is achieved by first setting a high frequency threshold and mining for patterns using the classical level wise Apriori method [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 12, "context": "Data generation model: The data generation model for synthetic data is based on the inhomogeneous Poisson process model for evaluating the algorithm for learning excitatory dynamic networks [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "We used the data from dissociated cortical cultures gathered by Steve Potter\u2019s laboratory at Georgia Tech [15] over several days.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "We show the result of mining frequent episodes in the data collected over several days from Culture 6 [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 4, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 7, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 2, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 10, "context": "Some interesting algorithms have also been proposed for streaming motif mining in time-series data [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Karp et al proposed a one pass streaming algorithm for finding frequent events in an item sequence [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "In [5], this approach was extended to mine frequent itemsets.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Lossy counting constitutes another important class of streaming algorithms proposed by Manku and Motwani in 2002 [8].", "startOffset": 113, "endOffset": 116}, {"referenceID": 9, "context": "In [10], the pattern growth algorithm - PrefixSpan [14] for mining sequential patterns was extended to incorporate the idea of lossy counting.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [10], the pattern growth algorithm - PrefixSpan [14] for mining sequential patterns was extended to incorporate the idea of lossy counting.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "In [3], the authors propose a new frequency measure for itemsets over data streams.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In [11] an online algorithm for mining time series motifs was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [13] we proposed an information theoretic principle for determining the frequency threshold that is ultimately used in learning a dynamic Bayesian network model for the data.", "startOffset": 3, "endOffset": 7}], "year": 2012, "abstractText": "Discovering frequent episodes over event sequences is an important data mining task. In many applications, events constituting the data sequence arrive as a stream, at furious rates, and recent trends (or frequent episodes) can change and drift due to the dynamical nature of the underlying event generation process. The ability to detect and track such the changing sets of frequent episodes can be valuable in many application scenarios. Current methods for frequent episode discovery are typically multipass algorithms, making them unsuitable in the streaming context. In this paper, we propose a new streaming algorithm for discovering frequent episodes over a window of recent events in the stream. Our algorithm processes events as they arrive, one batch at a time, while discovering the top frequent episodes over a window consisting of several batches in the immediate past. We derive approximation guarantees for our algorithm under the condition that frequent episodes are approximately well-separated from infrequent ones in every batch of the window. We present extensive experimental evaluations of our algorithm on both real and synthetic data. We also present empirical comparisons with baselines and adaptations of streaming algorithms from itemset mining literature.", "creator": "LaTeX with hyperref package"}}}