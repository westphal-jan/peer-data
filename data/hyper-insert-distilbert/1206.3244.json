{"id": "1206.3244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Bayesian network learning by compiling to weighted MAX-SAT", "abstract": "hence the traditional problem of learning discrete bayesian networks from data is separately encoded as a neatly weighted primary max - sat problem itself and designing the maxwalksat local minimum search algorithm accordingly is used uniformly to address it. for organizing each dataset, the per - variable summands of the ( bdeu ) linear marginal data likelihood for different viable choices of biological parents ('approximate family search scores') are reasonably computed prior to applying maxwalksat. lastly each permissible choice of future parents available for all each variable pair is encoded uniquely as a distinct multiple propositional atom and the associated family score encoded as a'comparatively soft'weighted single - locus literal partition clause. two approaches devised to remotely enforcing null acyclicity relations are considered : by either thereby by concurrently encoding into the ancestor relation or indirectly by fully attaching over a total negative order to create each possible graph clause and theoretically encoding something that. the latter approach gives entirely better results. learning experiments have progressively been conducted on : 21 synthetic datasets sampled from 7 bns. today the largest tree dataset has 10, 39 000 datapoints obtained and therefore 60 binary variables producing ( responsible for counting the'ancestor'element encoding ) roughly a properly weighted cnf input file with 19, 84 932 atoms encoded and 269, 04 367 clauses. particularly for managing most datasets, maxwalksat more quickly finds bns with either higher db bdeu numerical score margins than the'true'bn. the effect effect of concurrently adding prior information values is assessed. formally it is further finally shown that bayesian model constrained averaging efficiency can be effected locally by collecting bns generated during blocking the search.", "histories": [["v1", "Wed, 13 Jun 2012 15:06:22 GMT  (159kb)", "http://arxiv.org/abs/1206.3244v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["james cussens"], "accepted": false, "id": "1206.3244"}, "pdf": {"name": "1206.3244.pdf", "metadata": {"source": "CRF", "title": "Bayesian network learning by compiling to weighted MAX-SAT", "authors": ["James Cussens"], "emails": ["jc@cs.york.ac.uk"], "sections": [{"heading": null, "text": "The problem of learning discrete Bayesian networks from data is encoded as a weighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to address it. For each dataset, the per-variable summands of the (BDeu) marginal likelihood for different choices of parents (\u2018family scores\u2019) are computed prior to applying MaxWalkSat. Each permissible choice of parents for each variable is encoded as a distinct propositional atom and the associated family score encoded as a \u2018soft\u2019 weighted single-literal clause. Two approaches to enforcing acyclicity are considered: either by encoding the ancestor relation or by attaching a total order to each graph and encoding that. The latter approach gives better results. Learning experiments have been conducted on 21 synthetic datasets sampled from 7 BNs. The largest dataset has 10,000 datapoints and 60 variables producing (for the \u2018ancestor\u2019 encoding) a weighted CNF input file with 19,932 atoms and 269,367 clauses. For most datasets, MaxWalkSat quickly finds BNs with higher BDeu score than the \u2018true\u2019 BN. The effect of adding prior information is assessed. It is further shown that Bayesian model averaging can be effected by collecting BNs generated during the search."}, {"heading": "1 Introduction", "text": "Bayesian network learning is a hard combinatorial optimisation problem which motivates applying state-ofthe-art algorithms for solving such problems. One of the most successful algorithms for solving SAT, the satisfiability problem in clausal propositional logic, is the local search WalkSAT algorithm [8]. The basic\nidea is to search for a satisfying assignment of a CNF formula by flipping the truth-values of atoms in that CNF. Both \u2018directed\u2019 flips which decrease the number of unsatisfied clauses and random flips are used. Frequent random restarts (\u2018tries\u2019) are also used.\nThe SAT problem can be extended to the weighted MAX-SAT problem where weights are added to each clause and the goal is to find an assignment that maximises the sum of the weights of satisfied clauses (equivalently minimises the sum of the weights of unsatisfied clauses which can then be viewed as costs). WalkSAT can be extended to MaxWalkSAT where truth value flipping is a mixture of random flips and those which aim to reduce the cost of unsatisfied clauses.\nProblems of probabilistic inference in Bayesian networks have already been encoded as weighted MAXSAT problems and various algorithms, including MaxWalkSAT have been applied to them [6, 7]. Similar approaches combined with MCMC have been used for inference in Markov logic [5]. The current paper appears to be the first to apply a MAX-SAT encoding to learning of Bayesian networks.\nThe paper is structured as follows. Section 2 describes the synthetic datasets used. Section 3 describes how the necessary weights were extracted from these datasets. Section 4 is the key section where the method of encoding a BN learning problem as a weighted MAX-SAT one is given. Section 5 provides empirical evaluation of the technique for both model selection and model averaging. There then follow conclusions and pointers to future work in Section 6.\nAll runs of MaxWalkSAT were conducted using the C implementation (sometimes slightly adapted) available from the WalkSAT home page http://www.cs. rochester.edu/\u223ckautz/walksat/. All computations were performed using a 2.40GHz Pentium running under GNU/Linux."}, {"heading": "2 Bayesian networks and datasets", "text": "Since the goal of this paper is to evaluate a particular approach to BN learning, all datasets are the result of sampling from a known \u2018true\u2019 BN. Seven BNs were used: their names and key characteristics are given in Table 1. Each BN only involves discrete variables. All BNs were obtained in Bayesian Interchange (.bif) format from the Bayesian Network Repository (http: //compbio.cs.huji.ac.il/Repository/). Each was then then converted into a Python class instance and \u2018pickled\u2019 (i.e. saved to disk).\nDatasets of sizes 100, 1,000 and 10,000 were then generated by forward sampling from each BN. Note that each dataset is complete: there are no missing values. Each dataset was stored as a single table in an SQLite database. Each such table had two columns: a string representation of each distinct joint instantiation sampled and a count of how often that joint instantiation had been sampled. For big BNs most or all of these counts will be one. Table 2 states the number of distinct joint instantiations for each dataset. Each SQLite database was wrapped inside a Python class instance which was then pickled."}, {"heading": "3 Computing and filtering family scores", "text": "The primary goal of this paper is to evaluate MaxWalkSat as an algorithm to search for BNs of high marginal likelihood for a given dataset. As is well known, if Dirichlet priors are used for the parameters of a BN and the data are complete then the marginal likelihood for a BN structure G for data D is given by the equations in (1) and (2) [3].\nP (G|D) = L(G) = n\u220f\ni=1\nScorei(G) (1)\nwhere\nScorei(G) = qi\u220f\nj=1\n\u0393(\u03b1ij) \u0393(nij + \u03b1ij) ri\u220f k=1 \u0393(nijk + \u03b1ijk) \u0393(\u03b1ijk) (2)\nEquation (2) defines what will be called a family score since it is determined by a child variable and its parents. In (2), qi is the number of joint instantiations of those parents that Xi has in the graph G; j is thus an index over these instantiations. ri is the number of values Xi has and thus k is an index over these values. nijk is the count of how often Xi takes its kth value when its parents are in their jth instantiation. \u03b1ijk is the corresponding Dirichlet parameter. Finally, nij = \u2211ri k=1 nijk and \u03b1ij = \u2211ri k=1 \u03b1ijk. Since Scorei(G) only depends on Pai, the parents it has in graph G, Scorei(G) can be written as Scorei(Pai).\nThroughout this paper the values of \u03b1ijk have been chosen so that L(G) becomes the BDeu metric for scoring BNs. This is done by choosing a prior precision value \u03b1 and setting \u03b1ijk = \u03b1/(riqi). In this paper the prior precision was always set to \u03b1 = 1. The BDeu score is a special case of a BDe score; BDe scores have the property that Markov equivalent BNs are guaranteed to have the same score. Setting \u03b1ijk = 1/(riqi) allows family scores to be defined by (3).\nScorei(Pai) = qi\u220f\nj=1\n\u0393( 1qi )\n\u0393(nij + 1qi ) ri\u220f k=1 \u0393(nijk + 1riqi ) \u0393( 1riqi ) (3)\nNow, let X be any subset of the variables and define, for fixed data, the function H, as in (4).\nH(X) = qX\u220f `=1 \u0393(n` + 1qX ) \u0393( 1qX ) (4)\nwhere qX is the number of joint instantiations of variables X, and n` is a count of how often the `th of these\noccurred in the data. Let Pai be the parents of Xi and set Fai = {Xi}\u222aPai (the family for Xi), then it is easy to see that (3) can be rewritten as (5)\nScorei(Pai) = H(Fai) H(Pai)\n(5)\nso that\nlog Scorei(Pai) = logH(Fai)\u2212 logH(Pai) (6)\nIn the approach followed in this paper the first stage of BN learning is to compute log family scores log Scorei(G) for all families up to some limit on the number of parents. Here this limit was set to 3 parents. Note that 4 of the 7 true BNs have variables with more than 3 parents so that this restriction renders these 4 unlearnable. Equation (6) permits a small efficiency increase: logH(Fai(X)) is computed for all subsets of variables from size 0 to size 4. Family scores can then be computed using (6). (All scores will be log scores from now on.)\nThe number of family scores computed and the time taken is listed for each dataset in Table 3. These computations were performed using Python equipped with the Psyco (http://psyco.sourceforge.net/) JIT compiler. Re-implementing in, say, C would no doubt be faster, but optimising this part of the process is not the focus of this paper.\nOnce family scores are computed the next step is to encode them as weighted clauses, but, with the exception of asia, there are too many scores for all to be used. However, since the goal is to find high likelihood BNs the vast majority of these family scores can be thrown away. Let Pai and Pa\u2032i be two potential parent sets for variable Xi. If (1) Pai \u2282 Pa\u2032i and (2) Pai has a higher score than Pa\u2032i then Pa \u2032 i can be ruled out as a potential parent set for Xi in a maximal likelihood graph. Any graph which has Pa\u2032i as the parents for Xi can have its score increased by replacing Pa\u2032i by Pai; the key point is that such a replacement cannot\nintroduce a directed cycle since one or more arrows are being removed. Filtering family scores in this way causes a significant reduction in the potential parent sets for any variable as shown by Table 4."}, {"heading": "4 Encoding BN learning using hard and soft clauses", "text": "The goal of finding a maximum (marginal) likelihood BN with complete data is equivalent to choosing the n best parent sets (one for each variable) subject to the condition that no directed cycle is formed: a problem of combinatorial optimisation. Here this problem is encoded with weighted clauses.\nA weighted CNF problem requires specifying (i) propositional atoms (ii) clauses and (iii) weights for clauses. To represent graph structure two options have been considered. Using an adjacency matrix approach for each of the n(n \u2212 1) distinct ordered pairs of vertices there is an atom asserting the existence of an arc between the two vertices. Alternatively one can create one atom for each possible family (child plus parents). This latter approach has performed better in the experiments done so far, presumably because flipping the truth values of such atoms permits bigger search moves. With this approach when encoding the problem of learning from the carpo-generated dataset with 10,000 datapoints there are 16,391 such atoms (see Table 4). If such an atom is set to TRUE, this represents that the specified variable has the specified parents. For each variable there is a single clause stating that it must have a (possibly empty) parent set:\u2228\ncandidate parent set t Xi has parent set t (7)\nThe longest such clause contains 2,089 atoms.\nTo rule out choices of parent sets producing a cycle two options have been considered. In the first \u2018Ancestor\u2019 encoding, there is an atom an(Xi, Xj) stating that Xi is an ancestor of Xj in the graph for each of the n(n\u2212 1) distinct ordered pairs of vertices. For each of the n(n \u2212 1)(n \u2212 2) distinct ordered triples (Xi, Xj , Xk) there is a transitivity clause:\nan(Xi, Xj) \u2227 an(Xj , Xk)\u2192 an(Xi, Xk) (8)\nTo express acyclicity there are n(n \u2212 1)/2 clauses of the form:\n\u00acan(Xi, Xj) \u2228 \u00acan(Xj , Xi) (9)\nAn alternative \u2018Total order\u2019 approach is to encode a total order of vertices. In a total order exactly one of Xi < Xj and Xj < Xi is true for each distinct Xi and Xj . So for each of the n(n \u2212 1)/2 distinct unordered pairs {Xi, Xj} there is an atom ord(Xi, Xj) asserting that Xi and Xj are lexicographically ordered in the total order. To ensure that an assignment of truth values to these atoms defines a total order it is enough to rule out 3-cycles such asXi < Xj , Xj < Xk, Xk < Xi. This is because if a fully connected digraph with no 2-cycles (a tournament) has a cycle then it has a 3-cycle. There is a simple inductive proof of this result which is omitted here. There are n(n\u22121)(n\u22122) hard clauses ruling out 3-cycles of the form:\n\u00acord(Xi, Xj) \u2228 \u00acord(Xj , Xk) \u2228 ord(Xi, Xk) ord(Xi, Xj) \u2228 ord(Xj , Xk) \u2228 \u00acord(Xi, Xk)\nUsing either the \u2018Ancestor\u2019 or \u2018Total Order\u2019 encoding there are hard clauses declaring that a (non-empty) choice of parents for a variable determines the truth value of certain atoms. For example, using the \u2018Ancestor\u2019 approach:\nXj has parent set {Xi, Xk} \u2192 an(Xi, Xj) Xj has parent set {Xi, Xk} \u2192 an(Xk, Xj)\nor with the \u2018Total Order\u2019 approach:\nXj has parent set {Xi, Xk} \u2192 ord(Xi, Xj) Xj has parent set {Xi, Xk} \u2192 \u00acord(Xj , Xk)\nNote that since the log BDeu score is a log-likelihood it is a negative number, as are all the family scores. The goal is to find an admissible choice of families with small negative numbers. Another way of looking at this is to say that choosing any particular set of\nparents for a variable incurs a cost which is -1 times the relevant family score. Each such cost can then be encoded by a weighted clause of the following type:\n\u2212 log Scorei(Pai) : \u00ac(Xi has parent set Pai) (10)\nThe \u2212 log Scorei(Pai) costs were rounded to integers.\nA final option is not to rule out cycles directly, but to create a cycle atom asserting the existence of a cycle. Cycles can then be ruled out entirely with the hard clause \u00accycle atom or merely discouraged with a soft version. If the latter option is taken then cyclic digraphs have to be filtered out after the search is over.\nThe numbers of atoms, clauses and literals for an encoding of each learning problem using a cycle atom and the \u2018Ancestor\u2019 encoding is given in Table 5. Without a cycle atom there is one fewer atom and clause and also a reduction in the number of literals. With the \u2018Total Order\u2019 approach there is a small reduction in atoms and the number of clauses and lits is roughly halved. (A literal is an atom or its negation, the number of literals in Table 5 is the sum of all literals in all clauses.)\nTo help motivate these choices of encoding it is worth peeking ahead to examine a run of MaxWalkSat with\nthe \u2018Ancestor\u2019 encoding as shown in Fig 1. This is a run using the encoded form of the hailfinder-generated set of 10,000 datapoints. This is a run without using the cycle atom so the numbers of atoms, clauses and literals are reduced a little from those given in Table 5. The goal was to find a BN with BDeu score at least as high as that of the \u2018true\u2019 BN\u2019s score of -503,040 (i.e. cost of 503,400). This was achieved in just under 13 million flips taking 75 seconds.\nA key point is that the number of unsatisfied clauses in the lowest cost assignment in each try is 56, the number of variables in hailfinder. These unsatisfied clauses are 56 weighted single-literal clauses of the form (10) corresponding to a choice of parents for each variable which does not produce a cycle. In all runs on all datasets, the unsatisfied clauses in low cost assignments are always of this form. Choices of parents that do cause a cycle will break at least one hard clause thus incurring a sufficiently large cost that MaxWalkSat will never return them as a lowest cost assignment. Note that although only one choice of parents is possible for any variable there is no need to encode this (hard) constraint. Choosing e.g. two parent sets will always incur a higher cost then either or the two alone so the search avoids such assignments. MaxWalkSat \u2018wants\u2019 to avoid choosing any parents but is compelled to do so by hard constraints of the form (7)."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Learning a single BN", "text": "In this section, results using MaxWalkSat to search for a high BDeu-scoring BN are given. The scores from these searches are given in Table 6 and some timings are given in Table 7. Note that the times reported in Table 7 are for one of the slowest approaches: \u2018Ancestor\u2019 encoding, 50% noise (random flips). An example of such a \u2018slow\u2019 run was shown in Fig 1 where only 171,206 flips/sec were achieved. In contrast, using the \u2018Total Order\u2019 encoding with 10% noise on the same data a rate of 1,092,243 flips/secs is achievable.\nOne problem with evaluating a search for a high scoring BN is that the optimal BN and its score is unknown (although presumably it could be computed for the small asia learning problem). However, by choosing to evaluate on synthetic data there is at least the \u2018true\u2019 BN which can be scored. It is also possible to construct a high-scoring BN which meets the restriction of having at most 3 parents. Starting with the true BN replace each true parent set with the highest-scoring pre-scored parent set which is a subset of the true parent set. With the exception of carpo this produces a BN with a higher score than the true BN. The score\nof the BN thus constructed is given in the \u2018Target\u2019 column of Table 6.\nTable 6 contains only the most interesting of many experimental runs conducted. The results show that for small datasets (* 2, corresponding to 102 = 100), MaxWalkSat easily finds BNs exceeding both the True and \u2018Target\u2019 BN score. This reflects the relatively smooth nature of the likelihood surface. For bigger datasets, were the landscape is much more jagged, the picture is more mixed. For 103 size datasets the True BN score is beaten by the \u2018Long\u2019 run in all cases, except for alarm. For 104, the \u2018Long\u2019 run fails to beat carpo and insurance also. Comparing \u2018Ancestor\u2019 to \u2018Total Order\u2019 results show that the latter is the more successful encoding. Experiments unreported here have indicated that the \u2018Total Order\u2019 encoding works better with low noise (random flip) values and a prevention of random breaking of hard clauses. Combining these with an increased search on each try leads to the superior results in the \u2018Long\u2019 column of Table 6. Note that even the longest \u2018Long\u2019 run, ca 4, only took 32 minutes (with 510,397 flips/sec); most where much faster.\nNote that in some cases MaxWalkSat returns a BN\nwhich is astronomically more probable (assuming a uniform prior) than the \u2018true\u2019 one. For example, with Mi 2 and the Ancestor encoding, the returned BN is e(\u2212470,215+409,641) = e60,574 more probable than the true one!\nA rough comparison with Castelo and Koc\u030cka\u2019s RCARNR and RCARR algorithms [1] is possible for the al 2 and al 4 datasets. As here, datasets of size 1,000 and 10,000 were sampled from the Alarm BN and best scores of around -11,115 and -108,437 respectively were reported. This beats (resp. does not beat) our best score for Alarm, but since the actual sampled datasets are different not too much should be read into this comparison.\nSome initial experiments adding prior knowledge to the \u2018Ancestor\u2019 encoding proved interesting. Adding in information specifying all known pairs of independent variables (by banning the relevant ancestor relations) had very little effect on the scores achieved. On the other hand, setting just a small number of known ancestor relations improved scores considerably. This is presumably due to the search having a bias towards sparser (less constrained) BNs."}, {"heading": "5.2 Bayesian model averaging by search", "text": "Markov chain Monte Carlo (MCMC) approaches are popular for Bayesian model averaging (BMA) for Bayesian networks. The idea is to construct a Markov chain whose stationary distribution is the true posterior over BNs. By running the chain for a \u2018sufficient\u2019 number of iterations (and throwing away an initial burn-in) the hope is that the sample thus produced supplies a reasonable approximation to the true distribution.\nSuccess depends on a realisation of the chain wandering into areas of high probability and visiting different BNs with a relative frequency close to the BNs\u2019 posterior probabilities. A search-based approach offers a much more direct method of BMA. The key idea is to explicitly search for areas of high probability. If, as is the case here, the probability of any visited BN can be computed at least up to a normalising constant then this value is recorded. An estimate of the posterior distribution is then found by simply assigning zero probability to any BN not encountered in the search and performing the obvious renormalisation on BN scores. An early paper proposing search for BMA is [4].\nHere a crude search-based approach using MaxWalkSat and the \u2018Ancestor\u2019 encoding has been tried out. MaxWalkSat is run as normal and for every assignment where no hard clause is broken a record of the n parent sets given by that assignment together with\nthe cost of the assignment is sent to standard output. Piping this through UNIX\u2019s sort --unique is enough to produce a list of distinct (encoded) BNs sorted by score.\nThe distribution thus created can be used to estimate true posterior quantities. In experiments conducted so far only the posterior probabilities of parent sets have been estimated. By comparing estimates produced from two runs, some measure of success can be produced. Here each run used the \u2018Ancestor\u2019 encoding and came from 120 restarts of MaxWalkSat each using 100,000 flips. Only the 1,000,000 highest scoring BNs from each run were kept.\nFor the datasets considered here only asia (all sizes) and Water with 100 datapoints afforded successful BMA. Plots comparing estimated posterior probabilities for parent sets for these two cases are given in Figs 2 and 3. In all other cases such plots show big differences in estimated probabilities for most cases where the estimates differ significantly from zero. Fig 4 shows a typical excerpt of such a failure using the alarm 100 data.\nThe basic problem here is that, apart from the trivial asia problem, the highest scoring one or two BNs found on any run are often vastly more probable than any others, so that the estimate of the posterior distribution puts almost all the probability mass on one, occasionally two, BN(s). If these few BNs vary between runs, estimates produced from them generally vary also. Fig 5 shows a typical example of the top BN in a run being much more probable than the secondranking one. The central issue is that often the true posterior probability really is concentrated on a few BNs, so BMA either needs to reliably find those few"}, {"heading": "1622 9.89412087285e-35 5.71801239082e-28", "text": ""}, {"heading": "1793 0.0013149823888 0.465138403992", "text": ""}, {"heading": "1792 1.51462961172e-20 5.38296345824e-18", "text": ""}, {"heading": "1791 5.89129801134e-14 2.10283310245e-06", "text": ""}, {"heading": "1398 0.924408916903 0.00117720289971", "text": ""}, {"heading": "1797 1.46223426395e-06 1.77652663003e-11", "text": "Figure 4: Estimates of posterior probability of parent sets from two different BMA runs for Alarm N = 100 dataset. The first column is the integer identifier for the parent set used by MaxWalkSat.\nmodels or needs to move in a more regular space, such as the space of variable orderings [2]."}, {"heading": "6 Conclusions", "text": "The results given here should be seen as a preliminary exploration of the worth of weighted MAX-SAT encodings of BN learning and MaxWalkSat as a method for solving such encoded problems. In particular, future work needs to consider alternative encodings and greater incorporation of prior knowledge on structures. Exponential-family structure priors whose features can\nC 176696 3954 4048 4197 4262 ... 17333 ... C 176714 3954 4048 4197 4262 ... 17332 ...\nFigure 5: Top two BNs from a BMA run using the carpo N = 10, 000 data. First number is the (rounded, negated) BDeu score. First one is 65 million times more probable (assuming a uniform prior) than second even though they differ in only one parent set (17333 vs. 17332).\nbe easily encoded are an obvious choice; they can be encoded by weighted clauses, just as the family scores are. This paper has focused on BN learning as a pure BDeu score optimisation problem. Further work will consider the value of the learned BNs according to other metrics and provide a comparison of this learning approach with existing BN learning systems.\nHow to reproduce these results Please go to: http://www.cs.york.ac.uk/\u223cjc/research/uai08/"}, {"heading": "Acknowledgements", "text": "Thanks to those responsible for hosting the Bayesian Network Repository and to Henry Kautz for making MaxWalkSat available. Thanks also to 3 anonymous reviewers."}], "references": [{"title": "On inclusiondriven learning of Bayesian networks", "author": ["Robert Castelo", "Tom\u00e1\u0161 Ko\u010dka"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Being Bayesian about network structure: A Bayesian approach to structure discovery in Bayesian networks", "author": ["Nir Friedman", "Daphne Koller"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["David Heckerman", "Dan Geiger", "David M. Chickering"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Model selection and accounting for model uncertainty in graphical models using Occam\u2019s window", "author": ["David Madigan", "Adrian E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Proc. AAAI-06,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Solving Bayesian inference by weighted model counting", "author": ["Tian Sang", "Paul Beame", "Henry Kautz"], "venue": "In Proc. AAAI-05,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "A dynamic approach to MPE and weighted MAX-SAT", "author": ["Tian Sang", "Paul Beame", "Henry Kautz"], "venue": "In Proc. IJCAI-07,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Local search strategies for satisfiability testing", "author": ["Bart Selman", "Henry Kautz", "Bram Cohen"], "venue": "Second DI- MACS Implementation Challenge. AMS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}], "referenceMentions": [{"referenceID": 7, "context": "One of the most successful algorithms for solving SAT, the satisfiability problem in clausal propositional logic, is the local search WalkSAT algorithm [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "Problems of probabilistic inference in Bayesian networks have already been encoded as weighted MAXSAT problems and various algorithms, including MaxWalkSAT have been applied to them [6, 7].", "startOffset": 182, "endOffset": 188}, {"referenceID": 6, "context": "Problems of probabilistic inference in Bayesian networks have already been encoded as weighted MAXSAT problems and various algorithms, including MaxWalkSAT have been applied to them [6, 7].", "startOffset": 182, "endOffset": 188}, {"referenceID": 4, "context": "Similar approaches combined with MCMC have been used for inference in Markov logic [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "As is well known, if Dirichlet priors are used for the parameters of a BN and the data are complete then the marginal likelihood for a BN structure G for data D is given by the equations in (1) and (2) [3].", "startOffset": 202, "endOffset": 205}, {"referenceID": 0, "context": "A rough comparison with Castelo and Ko\u010dka\u2019s RCARNR and RCARR algorithms [1] is possible for the al 2 and al 4 datasets.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "An early paper proposing search for BMA is [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "models or needs to move in a more regular space, such as the space of variable orderings [2].", "startOffset": 89, "endOffset": 92}], "year": 2008, "abstractText": "The problem of learning discrete Bayesian networks from data is encoded as a weighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to address it. For each dataset, the per-variable summands of the (BDeu) marginal likelihood for different choices of parents (\u2018family scores\u2019) are computed prior to applying MaxWalkSat. Each permissible choice of parents for each variable is encoded as a distinct propositional atom and the associated family score encoded as a \u2018soft\u2019 weighted single-literal clause. Two approaches to enforcing acyclicity are considered: either by encoding the ancestor relation or by attaching a total order to each graph and encoding that. The latter approach gives better results. Learning experiments have been conducted on 21 synthetic datasets sampled from 7 BNs. The largest dataset has 10,000 datapoints and 60 variables producing (for the \u2018ancestor\u2019 encoding) a weighted CNF input file with 19,932 atoms and 269,367 clauses. For most datasets, MaxWalkSat quickly finds BNs with higher BDeu score than the \u2018true\u2019 BN. The effect of adding prior information is assessed. It is further shown that Bayesian model averaging can be effected by collecting BNs generated during the search.", "creator": "TeX"}}}