{"id": "1206.3261", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Learning When to Take Advice: A Statistical Test for Achieving A Correlated Equilibrium", "abstract": "obviously we study a static multiagent learning evaluation problem where agents can either actually learn via multiple repeated risky interactions, carefully or collectively can closely follow the advice of a supportive mediator who suggests possible actions to take. we present an option algorithmthat each hypothetical agent they can use persuasion so that, with high probability, they can verify whether or otherwise not practicing the mediator's advice is useful. thus in as particular, undoubtedly if receiving the persistent mediator'\\ s advice is useful then agents will reach potentially a correlated defensive equilibrium, but unlikely if applying the mediator'if s advice is not useful, then agents are often not effectively harmed by using simply our test,, and can likewise fall back to via their linear original learning reward algorithm. we then generalize along our inhibition algorithm and show sufficiently that finding in the positive limit it always correctly independently verifies the given mediator's advice.", "histories": [["v1", "Wed, 13 Jun 2012 15:33:53 GMT  (367kb)", "http://arxiv.org/abs/1206.3261v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.MA", "authors": ["greg hines", "kate larson"], "accepted": false, "id": "1206.3261"}, "pdf": {"name": "1206.3261.pdf", "metadata": {"source": "CRF", "title": "Learning When to Take Advice: A Statistical Test for Achieving A Correlated Equilibrium", "authors": ["Greg Hines"], "emails": ["ggdhines@cs.uwaterloo.ca", "klarson@cs.uwaterloo.ca"], "sections": [{"heading": null, "text": "We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who suggests possible actions to take. We present an algorithm that each agent can use so that, with high probability, they can verify whether or not the mediator\u2019s advice is useful. In particular, if the mediator\u2019s advice is useful then agents will reach a correlated equilibrium, but if the mediator\u2019s advice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our algorithm and show that in the limit it always correctly verifies the mediator\u2019s advice."}, {"heading": "1 Introduction", "text": "In settings where agents repeatedly interact with each other (for example, through a repeated game), there are great opportunities for learning since agents are able to adapt their strategies given the history of play. This problem has garnished a lot of attention from several research communities, including the AI community and the game theory community. While many criteria have been proposed for measuring the success of learning approaches, one commonly used measure is whether the agents learn how to best-respond to the strategies being played by the others. That is, does the learning process converge to an equilibrium.\nIn this paper we study the problem of agents interacting with each other in a repeated game setting, but we introduce a third party mediator or advisor who makes strategy suggestions to the agents. Ideally, by following the suggestions of the mediator, agents will be able to learn how to play against each other, possibly even reaching mutually beneficial outcomes which would not have been possible without the mediation. That is, our goal is for the agents to learn and adapt so that they find a correlated equilibrium [1].\nHowever, a mediator is only useful if it can make good sug-\ngestions. Even if a mediator tries to make good suggestions it may be prevented by coding errors, memory limitations, etc. For an agent to accept a mediator\u2019s suggestions, there must be some way for the agent to verify that the suggestions are reasonable. A mediator might not be willing to share its code with the agents, or be aware of its own limitations. Therefore, for a truly robust system, the agents themselves must have a way of checking the mediator\u2019s suggestions.\nThus, this paper introduces a statistical test based on hypothesis testing that, with high probability, can verify the mediator\u2019s suggestions. While hypothesis testing has been proposed in the multiagent learning literature as a tool that agents might use to learn how to play Nash equilibria [5], to the best of our knowledge it has never been applied for validating a mediator\u2019s advice. Based on our test, we propose an algorithm that allows agents to converge to the mediator\u2019s suggestion if it is a correlated equilibrium and otherwise, in the limit, be no worse off for having used our algorithm. We then generalize this algorithm to a more theoretical setting where we show that with probability one, in the limit, our test will always be able to correctly verify the mediator\u2019s suggestions. This provides a method for achieving convergence to a specific correlated equilibrium."}, {"heading": "2 Background", "text": "In this section we introduce the key concepts and assumptions used in this paper.\nA n-agent stage game is a tuple G = \u3008N, A = A1 \u00d7 . . . \u00d7 An, u1, . . . , un\u3009, where N = {1, . . . , n} is the set of agents, Ai is the set of possible actions for agent i and A is the set of possible joint actions, and ui : A \u2192 R is the utility function for agent i. Without loss of generality, we assume that all utilities are greater than or equal to 0. A specific action for agent i is ai \u2208 Ai, and a joint action is a = (a1, . . . , an). We assume that A is public knowledge but the agents\u2019 utility functions are private.\nEach agent chooses its actions according to some strategy. A strategy for agent i, \u03c3i, is a probability distribution over Ai, stating with what probability the agent will play each\npossible action. The set of all possible strategies for agent i is \u03a3i. The vector \u03c3 = (\u03c31, . . . ,\u03c3n) is a strategy profile which specifies a strategy for each agent and \u03a3 is the set of all possible strategy profiles. We use \u03c3\u2212i to denote (\u03c31, . . . ,\u03c3i\u22121,\u03c3i+1, . . . ,\u03c3n).\nGiven a strategy profile \u03c3, we define the expected utility for agent i as\nui(\u03c3) = \u2211\na=(a1,...,an)\u2208A\nui(a)\u03a0 n j=1\u03c3j(aj). (1)\nEach agent\u2019s utility is dependent not just on its own actions, but also on the actions taken by all other agents. We assume agents are rational, i.e., given \u03c3\u2212i, agent i will choose a strategy which maximizes its expected utility.\nIn our model we introduce a third-party mediator, M. The mediator knows the utility functions for all agents, but is not affected by the game\u2019s outcome. Instead M makes suggestions to each agent as to what action it should take, where these suggestions are instantiations of a correlated strategy. Definition 1. A correlated strategy, \u03c3A, is a probability distribution over A. We let s \u2208 A denote an instantiation of \u03c3A. The conditional correlated strategy \u03c3A\u2212i(s\u2212i|si) is the conditional probability of the joint signal (si, s\u2212i) given the signal si, and \u03c3A\u2212i(si) is the set of all conditional probabilities given si.\nNote that \u03c3i is a probability distribution over Ai while \u03c3A is a probability distribution over A.\nWe assume that M\u2019s correlated strategy is public knowledge, but the actual instantiation, s, is not. In particular we assume that M sends each agent i a private signal, si, based on s.\nThe agents are under no obligation to follow the mediator\u2019s signals. It is up to the mediator to pick a correlated strategy that a rational agent would be willing to follow. Note that our type of a mediator is different than Monderer and Tennenholtz\u2019s, where agents must agree to follow the mediator\u2019s suggested actions before knowing what they are [11]. Definition 2. A correlated strategy \u03c3\u2217A = {\u03c3A(a)|a \u2208 A} is a correlated equilibrium if for every agent i and every si \u2208 Ai,\n\u2211\ns\u2212i\u2208A\u2212i\n\u03c3\u2217A\u2212i(s\u2212i|si)ui(si, s\u2212i) (2)\n\u2265 \u2211\ns\u2212i\u2208A\u2212i\n\u03c3\u2217A\u2212i(s\u2212i|si)ui(a \u2032 i, s\u2212i),\nfor all a\u2032i \u2208 Ai [1]. The set of all correlated equilibria in G is C(G).\nIf all of agent i\u2019s opponents are following a correlated equilibrium \u03c3\u2217A, it is rational for agent i to also follow \u03c3\u2217A.\nIn this paper, we are interested in a setting where agents have the ability to learn and adapt to the actions taken by others. Thus, we study repeated games. A repeated game Gr = (G1, G2, . . .) is an infinite sequence of the stage game G played repeatedly. Agent i\u2019s action at time t is ati and the joint action at time t is at. The history of joint actions, hist(t) = {a1, . . . , at\u22121}, is a record of the joint action taken at each iteration until time t. The empirical, or observed, percentage of play of joint actions, \u03c3hist(t)A , is the percentage of time each joint action has been played as of time t. Agents may learn from previous iterations of the game to try and improve their strategy. Specifically, we assume that agent i has a learning algorithm Li : hist(t) \u2192 \u03a3i, that helps agent i select a strategy for time t.\nLet \u03c3tA be the actual correlated strategy at time t, i.e. the one agents are actually using and not necessarily the one based on M\u2019s suggestions. We say that \u03c3tA converges to a correlated equilibrium if for some \u03c3\u2217A \u2208 C(G), limt\u2192\u221e \u03c3tA = \u03c3 \u2217 A. Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8]."}, {"heading": "3 Setup", "text": "The setting for our paper is a repeated game Gr with a mediator, M. As illustrated for the two agent case in Figure 1, time t will begin with the mediator giving each agent a suggested action, sti. Agents will then simultaneously choose their action, ati, which may or may not be sti. If agent i chooses not to follow M\u2019s signal, it can instead use a learning algorithm, Li, which we assume is independent of M\u2019s signals, to select an action. Based on the actual joint action, each agent will then receive some utility and the process repeats. The mediator\u2019s signal to each agent is private information, known only to that agent and the mediator, as is the agent\u2019s utility function. However, the action set for each agent is public knowledge, as is the action taken by each agent during a turn.\nThe mediator\u2019s signals are based on a selected correlated strategy, \u03c3MA , which is constant throughout the repeated game. Although ideally the mediator will suggest a correlated strategy that is also a correlated equilibrium, each\nagent still needs to verify that the mediator has actually done so.\nOur aim is to design an algorithm that achieves the following goals.\nFirst goal: If \u03c3MA is a correlated equilibrium then \u03c3tA, the actual correlated strategy which is not necessarily \u03c3MA , will converge to \u03c3MA .\nSecond goal: If \u03c3MA is not a correlated equilibrium, agents should be no worse off, in the limit, for having used our algorithm.\nIn Section 4, we present an algorithm, \u039b, that achieves these goals with high probability. In Section 5, we generalize \u039b so that, with probability one, in the limit, it will achieve both goals. Since each agent will be using \u039b independently, we refer to \u039bi as the instance of the algorithm being run by agent i and \u039b as the joint algorithm.\nThe algorithm is based on the concept of givingM the benefit of the doubt; until there is reason to believe otherwise, agents assume that \u03c3MA is a correlated equilibrium and follow M\u2019s signals. Specifically, agents will assume that the following conditions hold.\nCondition 1: The correlated strategy \u03c3MA is a correlated equilibrium.\nCondition 2: All other agents are following the signals based on \u03c3MA .\nAgents test whether these conditions hold during an initial period of play called a sampling test which has a fixed length of lT . If, at the beginning of the sampling test, agent i decides that one of the conditions does not hold, it will not follow M\u2019s signals and instead will use an individual \u201cfall-back\u201d strategy, \u03b3i, chosen uniformly at random. At the end of the sampling test, all agents who still believe that both conditions hold will continue to follow M\u2019s signals. All other agents will start using their original learning algorithm. The algorithm \u039bi is correct if and only if, at the end of the sampling test, it correctly determines whether both conditions hold. The joint algorithm, \u039b, is correct if and only if \u039bi is correct for all i."}, {"heading": "4 The Initial Algorithm", "text": "In this section, we describe how our initial algorithm works. As a first step in \u039bi, agent i will check to see if Equation 2 holds for all si \u2208 Ai. If Equation 2 does not hold, agent i will know that Condition 1 cannot be true. In this case, agent i will use a \u201cfall-back\u201d strategy, \u03b3i \u2208 \u03a3i, picked uniformly at random, for the rest of the sampling test. If Equation 2 does hold, agent i must check to see if Condition 2 is true and will continue to follow M\u2019s signals throughout the sampling test.\nSince the utilities for each agent, as well as the signals they receive each turn, are private, there may be no way to prove or disprove Condition 2 with absolute certainty at any finite point during the game. The best \u039bi can do is reach a probabilistic conclusion. Since joint actions are public knowledge, \u039bi can compare the empirical percentages of play for the duration of the sampling test against the percentages predicted by \u03c3MA . If the difference between these two values is statistically significant, there is a high probability that at least one agent has stopped following the mediator\u2019s signals.\nTo test if there is a difference, agent i assumes there is some fixed but unknown correlated strategy \u03c3\u0303A that all agents were actually using for the sampling test, where \u03c3\u0303A may or may not be \u03c3MA . We are now able to use hypothesis testing, where our null hypothesis is that \u03c3MA is equal to \u03c3\u0303A, i.e.,\nH0 : \u03c3 M A = \u03c3\u0303A, (3)\nand our alternative hypothesis is that \u03c3MA is not equal to \u03c3\u0303A, i.e.,\nH1 : \u03c3 M A '= \u03c3\u0303A. (4)\nThe test statistic used is Pearson\u2019s \u03c72 test,\nT = \u2211\na\u2208A\u2032\n(X(a) \u2212 E(a))2\nE(a) , (5)\nwhere A\u2032 is any subset of A such that |A\u2032| = |A| \u2212 1, X(a) = lT\u03c3 hist(lT ) A (a) is the actual frequency of play of a \u2208 A\u2032 during the sampling test, E(a) = lT\u03c3MA (a) is the expected frequency of play according to \u03c3MA , and where lT is the length of the sampling period [12]. Note that \u03c3hist(lT )A is based on a sampling from \u03c3\u0303A of size lT . For now we assume that \u03c3MA (a) > 0 for all a \u2208 A. We relax this assumption later. The Pearson\u2019s \u03c72 test has (in the limit) a probability distribution function of\n\u03c72df + \u03c7 2 NCP,1, (6)\nwhere the first distribution has df = |A|\u22122 degrees of freedom, and the second distribution has 1 degree of freedom and a non-centrality parameter of NCP [9].\nIf H0 is true, NCP = 0. Assuming that H0 is true, we choose a significance level for rejection of the null hypothesis of \u03b1 < 1 and a corresponding critical value of c(\u03b1), i.e., we reject the null hypothesis when T \u2265 c(\u03b1). In this case, the probability of incorrectly rejecting H0 (known as a Type 1 error) is p1 = \u03b1. If H1 is actually true, we err when T < c(\u03b1) and we do not reject H0 (a Type 2 error). When H1 is true, NCP > 0. Since the non-centrality parameter determines how much the probability distribution in Equation 6 gets adjusted, determining NCP helps determine the probability of a Type 2 error.\nThe equation for NCP is NCP = t \u2217 \u03b4, where \u03b4, the sensitivity parameter, is a measure of the difference between \u03c3MA and \u03c3\u0303A given by\n\u03b4(\u03c3MA , \u03c3\u0303A) = \u2211\na\u2208A\n(\u03c3\u0303A(a) \u2212 \u03c3MA (a)) 2\n\u03c3MA (a) . (7)\nFor a given value of \u03b4, say \u03b4\u0302, if\n\u03b4(\u03c3MA , \u03c3\u0303A) \u2265 \u03b4\u0302, (8)\nthen the probability of a Type 2 error is bounded by some value \u03b2(\u03b4\u0302) < 1, whose value is normally found via numerical computation [9]. Since \u03b2 is also a function of lT and \u03b1, we refer to it as \u03b2(lT ,\u03b1, \u03b4).\nSince agents do not know whether their opponents are following the mediator\u2019s suggestions, agents do not know the exact value for \u03c3\u0303A, and therefore, it is impossible to choose an appropriate value for \u03b4\u0302 so that Equation 8 is guaranteed to hold. Instead, agents can consider a different question: what is the worst case situation under which Equation 8 does not hold? To answer this question, consider the set of all agents for whom Equation 2 does not hold, NB \u2286 N . Let (\u03c3MA\u2212NB , \u03b3NB ) be the actual correlated strategy for the duration of the sampling test, i.e., a combination of those agents who will follow M\u2019s signals and those who will rely on their fall-back strategy. Let \u03a3NB be the set of all possible joint strategies for agents in NB , and\n\u03a3NB (\u03c3 M A , \u03b4)\n= {\u03b3NB \u2208 \u03a3NB |\u03b4(\u03c3 M A , (\u03c3 M A\u2212NB , \u03b3NB )) < \u03b4} (9)\nbe the set of all possible joint strategies for agents in NB which would result in Equation 8 not holding. Let \u00b5(\u03a3NB ) and \u00b5(\u03a3NB (\u03c3MA , \u03b4)) be the Lebesgue measures of \u03a3NB and \u03a3NB(\u03c3MA , \u03b4), respectively. Then, since \u03b3i is chosen uniformly at random, the probability of \u03c3NB being in \u03a3NB (\u03c3 M A , \u03b4) is\n\u03c8(\u03a3NB ) = \u00b5(\u03a3NB (\u03c3 M A , \u03b4))\n\u00b5(\u03a3NB ) . (10)\nSince agents do not know NB , they consider the worst case scenario,\n\u03c8 = max N \u2032\u2286N \u03c8(\u03a3N \u2032). (11)\nIf we assume that whenever Equation 8 does not hold and \u03c3\u0303A '= \u03c3MA , a Type 2 error is always made, then the probability of a Type 2 error is at most\np2 \u2264 (1 \u2212 \u03c8) \u00b7 \u03b2(\u03b4\u0302) + \u03c8. (12)\nThat is, Equation 8 holds with at least a probability of \u03c8 and when it does, the probability of a Type 2 error is at most \u03b2(\u03b4\u0302) and with a probability of at most \u03c8, Equation 8 does not hold.\nIf we do not assume that \u03c3MA (a) > 0 for all a \u2208 A, then Equations 5 and 7 may contain division by zero. To deal with this, we ignore all a \u2208 A such that \u03c3M(a) = 0. If \u03b6 = {a \u2208 A|\u03c3M(a) = 0}, then the summations in Equations 5 and 7 need to exclude all a \u2208 \u03b6, and df in Equation 6 now equals |A| \u2212 2 \u2212 |\u03b6|. If the null hypothesis is correct then \u03c3MA (a) = 0 implies that \u03c3 hist(lT ) A (a) = 0 for all a \u2208 \u03b6. Alternatively, if there exists a\u2032 \u2208 A such that\n\u03c3hist(lT )A (a \u2032) > 0 while \u03c3MA (a\u2032) = 0, the alternative hypothesis must be correct. Hence, both of these cases do not present problems.\nThe only other case is if for all a \u2208 A such that \u03c3MA (a) = 0, \u03c3hist(lT )A (a) = 0 but, unknown to the agents, the alternative hypothesis is correct. In this case, a Type 2 error may occur. To find the probability of this case happening, we first determine the probability of at \u2208 \u03b6. Since any agent who rejects M\u2019s suggested strategy chooses its new strategy uniformly at random, the probability, P , that at \u2208 \u03b6 for t \u2264 lT is\nP \u2265 \u2211\na\u2208\u03b6\nmin N \u2032\u2286N \u03c3A \u2212N\u2032\n(a\u2212N \u2032) 1\n|AN \u2032 | , (13)\nwhere minN \u2032\u2286N is considered since agents do not know NB . Therefore, the probability that at '\u2208 \u03b6 for all t \u2264 lT is at most (1 \u2212 P)lT and the overall probability of a Type 2 error is at most\np2 \u2264 (1 \u2212 P) lT [(1 \u2212 \u03c8) \u00b7 \u03b2 + \u03c8] . (14)\nTo accommodate the worst case, we assume equality holds in Equation 14. Note that p1 has not changed. For simplicity, we assume that p1 = p2 = p, and refer to p as the overall probability of error.\nIt is possible to rearrange \u03b2(lT ,\u03b1, \u03b4) to express lT as a function of \u03b1, \u03b2 and \u03b4, i.e lT (\u03b1,\u03b2, \u03b4). As a result, lT is the sample size needed to perform the test with at most a probability of error (of either Type 1 or Type 2) of p.\nIf all agents are to use the same value for lT , they must also have the same value for \u03b2. This in turn requires them to have the same value for \u03c8. To achieve this, in Equations 11 and 13, agent i will consider all possible N \u2032, including those containing agent i."}, {"heading": "4.1 Examples", "text": "In this section we provide two examples to illustrate how our test would work.\nExample 1: Consider the game in Figure 2.\nLet A = {(a1,1, a2,1), (a1,1, a2,2), (a2,1, a2,1), (a1,2, a2,2)}. Suppose that M announces a correlated strategy, \u03c3MA = {1/18, 5/18, 2/18, 10/18}. Note that \u03c3MA is a correlated equilibrium.\nSuppose the agents choose p = 0.1 and \u03b4 = 0.01. Agents must now determine the critical value for rejection, c(\u03b1),\nand the length of the sampling test, lT . Since p1 = \u03b1, \u03b1 = 0.1. For 3 degrees of freedom, c(\u03b1) = 6.25. Since \u03c3MA (a) > 0 for all a, we can calculate \u03b2 by Equation 12. We calculate Equation 11 by numerical computation to find \u03c8 \u2248 0.09429. Therefore, \u03b2 = 0.0063. In practice, lT (\u03b1,\u03b2, \u03b4) would now be solved by some method of numerical computation [9]. For simplicity, we used the tables in Cohen to obtain a value of lT = 2100 [2].\nSuppose that after 2100 iterations, we have obtained an empirical frequency of play \u03b8hist(2101)A = {96, 601, 224, 1179}. Using Equation 5, we obtain a test statistic value of 4.678. Since this is lower than the critical value, both agents do not reject the null hypothesis and continue to use M\u2019s signals.\nExample 2: Consider a different example based on the same game where M announces a correlated strategy of \u03c3MA = {2/18, 10/18, 1/18, 5/18}. In this case, \u03c3MA is not a correlated equilibrium. Specifically, while Equation 2 is satisfied for Agent 1, it is not satisfied for Agent 2. Hence, Agent 2 will use a random fall-back strategy. Suppose \u03b32 = (3/4, 1/4).\nFor this example, the length of the test has not changed. Suppose we find an empirical frequency of \u03b8hist(2101)A = {1050, 350, 525, 175} after 2100 turns. Since Agent 2 already knows that \u03c3MA is not a correlated equilibrium, it will not perform the test. Agent 1 will obtain a test statistic value of 5953.3. This is well above the critical value and so Agent 1 will reject the null hypothesis, i.e., it will stop following the signals of the mediator.\nNote that, as we have stated our algorithm, Agent 1 will only know that there is a probability of at most 0.1 of incorrectly rejecting the null hypothesis. We have not accounted for the fact that the test statistic value is much higher than the critical value. An additional test that could be run after the null hypothesis is rejected is the calculation of the p-value. The p-value is the smallest \u03b1 value that would still allow us to reject the hypothesis [12]. In the case of the above example, the p-value would be very small, and Agent 1 could be very certain that \u03c3MA is not a correlated equilibrium."}, {"heading": "5 Repeated Testing", "text": "The limitation of our basic test is that there is always some positive probability of error. This is due to the need to pick values for 1\u2212p and \u03b4 that are both greater than 0. Since we can pick any such values for 1 \u2212 p and \u03b4, this is not much of a practical limitation, however we may wish to achieve a stronger theoretical result. Our goal is to have agents converge to playing \u03c3MA if it is a correlated equilibrium. If \u03c3MA is not a correlated equilibrium, then the agents\u2019 utility should be no worse off for having used our algorithm. This leads to the idea of repeated testing, where throughout the repeated game, agents will use multiple iterations of \u039bi.\nThe set of repeated sampling tests is R = {R1, R2, . . .},\nwhere Rj = {bRj , lRj}, bRj is the first time period in Rj , and lRj is the length of Rj . The instance of \u039bi during test Rj is denoted by \u039b Rj i . The repeated tests are not contiguous. A simple example is shown in Figure 3, where the timeline represents a repeated game up to 7 iterations. The grey areas represent sampling test iterations. For example, R2 = {bR2 , lR2} = {4, 2}, meaning that the second test iteration begins at time period 4 and lasts for 2 iterations of the repeated game.\nThe parameters, \u03b4 and p, can be set to depend on the test iteration, i.e. \u03b4(Rj) and p(Rj). Each test period must be identical for each agent, i.e. Rj must be the same for all agents. This means that \u03b4(Rj) and p(Rj) must be the same for all agents. The parameters are chosen such that\nlim j\u2192\u221e \u03b4(Rj) = 0, (15) \u221e \u2211\nj=1\np(Rj) < \u221e. (16)\nFor example, we can let \u03b4(Rj) = 1/j and p(Rj) = 1/2j. Finally, we assume that each agent\u2019s fall-back strategy is fixed. That is \u03b3Rji = \u03b3 Rj\u2032 i , for all j, j\u2032.\nOur first result is that an agent will not draw the wrong conclusion about the mediator too often. Theorem 1. In the limit, with probability one, there will only be a finite number of tests where \u039bRj is incorrect.\nProof. Let \u03c3MA be the correlated strategy suggested by M. Consider the following two cases:\n\u03c3MA is a correlated equilibrium: For test Rj , the probability of \u039bRji making a Type 1 error, p1(Rj), is equal to p(Rj). By the Borel-Cantelli lemma, with probability one, there will only be a finite number of times \u039bRji is incorrect, i.e. makes a Type 1 error. 1 This reasoning can be applied to all agents, and therefore with probability one there will only be a finite number of times \u039bRj is incorrect.\n\u03c3MA is not a correlated equilibrium: If \u03c3MA is not a correlated equilibrium, then some subset of agents, N \u2032 \u2286 N , will use their fall-back strategies instead of following the mediator\u2019s signals. The resulting correlated strategy for every test iteration will be (\u03c3MA \u2212N\u2032 , \u03b3N \u2032).\nSince \u03b3N \u2032 is fixed, by Equation 15, there exists a finite j\u2217\n1Borel-Cantelli Lemma: Let {Et}\u221e0 be a sequence of independent events and P (Et) be the probability of the event Et occurring. If P \u221e\nt=0 P (Et) < \u221e, then with probability one, only\na finite number of the events will occur.\nsuch that for all j \u2265 j\u2217,\n\u03b4(\u03c3MA , (\u03c3 M A \u2212N\u2032 , \u03b3N \u2032)) \u2265 \u03b4(Rj). (17)\nLet \u03c8(Rj) be the value of \u03c8, according to Equation 11, during the sampling test Rj . Starting at Rj\u2217 , we know that, with probability one, Equation 8 holds and therefore, since \u03c8(Rj) is the probability of Equation 8 not holding, \u03c8(Rj) = 0, for all j \u2265 j\u2217. Therefore, the probability of a Type 2 error starting at Rj\u2217 is\np2 = \u221e \u2211\nj=j\u2217\n(1 \u2212 P)lT \u03b2. (18)\nNote that P , lT and \u03b2 are all functions Rj , however we omit the notation (Rj) for clarity. Since \u03b2 is less than 1,\np2 \u2264 \u221e \u2211\nj=j\u2217\n(1 \u2212 P)lT [(1 \u2212 \u03c8) \u00b7 \u03b2 + \u03c8] (19)\n= \u221e \u2211\nj=j\u2217\np(Rj), (20)\nwhere \u03c8, as calculated by Equation 11, is also a function of Rj . Therefore, by Equation 16 and the Borel-Cantelli lemma, with probability one, there will only be a finite number of times \u039bRji is incorrect, i.e. makes a Type 2 error. Again, this reasoning can be generalized to all agents and therefore, there will only be a finite number of times \u039bRj is incorrect.\nWe now examine the behaviour of agents between sampling tests. The periods between test iterations are called free periods. The set of free periods is F = {F1, . . .} where Fj = {bFj , lFj}. Thus Gr = {R1, F1, R2, F2, . . .}. For example, in Figure 3, the first free period, F1, would be {bF1 , lF1} = {2, 2}. If \u039b Rj i did not reject the null hypothesis, agent i continues to follow M\u2019s signals for all of Fj . If \u039bRji did reject the null hypothesis, agent i relies on its learning algorithm Li for Fj . We assume that Li is flexible at the beginning of each free period [3]. Definition 3. The learning algorithm Li is flexible if at the beginning of every free period Fj ,\nLi(hist(bFj )) = Li(hist(1)). (21)\nTherefore, during each free period, Li does not base its actions on what has happened before time bFj .\nFor example, Li may be a trigger strategy, but that trigger may not be based on anything that has happened in a previous sampling test or free period.\nWe require that\nlim j\u2192\u221e\n\u2211j j\u2032=1 lRj\n\u2211j j\u2032=1 lFj\n= 0, (22)\nfor example lFj = l2Rj . This means that, in the limit, the length of the sampling periods is negligible compared to the length of the free periods. We also require that\nlim j\u2192\u221e lRj j = \u221e. (23)\nThis means that the length of the sampling tests grows at faster than a linear rate. The specific values for lRj and lFj would have to be agreed upon by all agents. Definition 4. Let \u03b8exp(t1,t2)A be the expected frequency of play from time t1 to t2, i.e., the expected number of times each joint action a \u2208 A gets played between times t1 and t2 inclusive. If t1 is not given, we assume t1 = 1. Similarly, let \u03b8exp(Fj ,...,Fj\u2032 )A be the expected frequency of play during the free periods Fj through Fj\u2032 , inclusive.\nSince the frequency of play depends on the algorithms the agents are using, let \u03b8exp(t)A (L) be the expected frequency of play from time 1 to t assuming that agents use the joint learning algorithm L for the whole period.\nFor simplicity in all of the following proofs, we assume that t always corresponds to the beginning of a sampling period. Let j(t) be the index of the last free period before t.\nThe first step is to show that if M suggests a correlated equilibrium, agents will converge to it. Theorem 2. If the correlated strategy suggested by M, \u03c3MA , is a correlated equilibrium, then with probability one,\nlim t\u2192\u221e\n\u03c3tA = \u03c3 M A . (24)\nProof. If \u03c3MA is a correlated equilibrium then by Theorem 1, with probability one, after some finite point \u039b will always correctly determine that \u03c3MA is a correlated equilibrium. As a result, with probability one, after some finite point, all agents will choose to follow the mediator\u2019s signals during the free periods.\nOur next result is a technical lemma which shows that in the limit, agents are not harmed by taking time out to do the sampling tests. Lemma 1. In the limit, there is no difference between the average utility from agents using L for the whole repeated game and just for the free periods, i.e.,\nlim t\u2192\u221e\n[\nui\n(\n\u03b8exp(t)A (L)\nt\n)\n\u2212 ui\n(\n\u03b8 exp(F1,...,Fj(t)) A (L)\nt\n)]\n= 0.\n(25)\nFurthermore, this is true even when excluding the first j\u2217 \u2212 1 free periods, for some j\u2217 > 1, i.e.,\nlim t\u2192\u221e\n[\nui\n(\n\u03b8exp(t)A (L)\nt\n)\n\u2212 ui\n(\n\u03b8 exp(Fj\u2217 ,...,Fj(t)) A (L)\nt\n)]\n= 0.\n(26)\nThe proof is given in the Appendix.\nFinally, we need to show that if \u03c3MA is not a correlated equilibrium, agents are no worse off, on average, for having used \u039b. Theorem 3. If the correlated strategy suggested by M, \u03c3MA , is not a correlated equilibrium, then with probability one,\nlim t\u2192\u221e\n[\nui\n(\n\u03b8exp(t)A (\u039b)\nt\n)\n\u2212 ui\n(\n\u03b8exp(t)A (L)\nt\n)]\n\u2265 0. (27)\nTherefore, in the limit, agent i will be no worse off for using \u039b instead of Li.\nProof. If \u03c3MA is not a correlated equilibrium, by Theorem 1, with probability one, starting at some sampling test, say Rj\u2217 , \u039b will always correctly determine that \u03c3MA is not a correlated equilibrium.\nConsider \u03b8A with respect to some arbitary a \u2208 A, denoted by \u03b8a. We start by breaking the game down into the sequence of sampling tests and free periods. That is, \u03b8exp(t)a (\u039b) = \u03b8 exp(R1,F1,...,F (t)) a (\u039b). For t \u2265 t(j\u2217), the utility can be split up into the utility for the sampling tests and free periods before Rj\u2217 and for those starting at Rj\u2217 i.e.,\nlim t\u2192\u221e\n[\nui\n(\n\u03b8 exp(R1,F1,...,Rj\u2217\u22121,Fj\u2217\u22121) a (\u039b)\nt\n)\n+ ui\n(\n\u03b8 exp(Rj\u2217 ,Fj\u2217 ,...,F (t)) a (\u039b)\nt\n)]\nSince \u03b8exp(R1,F1,...,Rj\u2217\u22121,Fj\u2217\u22121)A (\u039b) is constant, in the limit, the first term is 0, and so we are interested in\nlim t\u2192\u221e ui\n(\n\u03b8 (Rj\u2217 ,Fj\u2217 ,...,F (t)) a (\u039b)\nt\n)\nThe expected frequency can be split up into the expected frequency for the sampling periods and for the free periods. Since \u039b always determines that \u03c3MA is not a correlated equilibrium, during all the free periods agents will always use L, and so we are interested in\nlim t\u2192\u221e\n[\nui\n(\n\u03b8 (Rj\u2217 ,...,R(t)) a (\u039b)\nt\n)\n+ ui\n(\n\u03b8 (Fj\u2217 ,...,F (t)) a (L)\nt\n)]\nSince we assumed that all utilities are nonnegative, we may discard the first term, and thus have\nlim t\u2192\u221e ui\n(\n\u03b8 (Fj\u2217 ,...,F (t) a (L))\nt\n)\nTherefore, by Lemma 1, the theorem follows.\nTogether, Theorems 2 and 3 show that, with probability one, if \u03c3MA is a correlated equilibrium, agents will converge to it and if \u03c3MA is not a correlated equilibrium, agents will be no worse off in the long run for using \u039b."}, {"heading": "6 Conclusion", "text": "The setting for this paper was a repeated game with a mediator. The mediator makes suggestions to the agents as to what actions to take. We presented a test that agents could use so that, with high probability, they could determine if the mediator\u2019s suggestion was a correlated equilibrium. We then generalized our algorithm to incorporate repeated testing so that in the limit, with probability one, the test will always correctly determine whether the mediator\u2019s suggested strategy is a correlated equilibrium. As a result, if the mediator suggests a correlated equilibrium, then agents will converge to it, and otherwise, be no worse off in the long run for having used our algorithm.\nWe envision several directions for future research. First, it might be possible to extend our algorithm to work in radically uncoupled environments, where agents are not aware of the existence of others. This would significantly decrease the knowledge requirements of our test. Second, we would like to extend our approach so that the mediator receives feedback from the agents themselves, which can be used to help select appropriate correlated strategies. We believe that the incentive issues in such an approach will be challenging. It may also be interesting to apply our approach to other solution concepts such as mediated equilibria [11].\nIn a more applied direction, it might be possible to generalize our approach so it can be used in a stochastic game setting. Thus, our approach could be combined with methods such as Q-learning [7]. Correlated equilibria have also been used in graphical games, which can be used to model many different settings [10]. Hence, applying our technique to graphical games may yield some interesting results. For example, network games use graphical games to help represent a variety of problems, from public good provision and trade to information collection [6]. These models can be hindered by a \u201cfundamental theoretical problem: even the simplest games played on networks have multiple equilibrium[sic] which display a bewildering range of possible outcomes\u201d [6]. Our model may help integrate correlated equilibria as a possible solution to this problem."}, {"heading": "7 Acknowledgements", "text": "Our thanks to Gord Hines for his statistical advice."}, {"heading": "A Proof of Lemma 1", "text": "Proof. Consider \u03b8 with respect to a \u2208 A, denoted by \u03b8a. Since j\u2217 is fixed, \u03b8F1,...,Fj\u2217\u22121a (L) is constant, and therefore,\nlim t\u2192\u221e\n\u03b8 exp(F1,...,Fj\u2217\u22121) a (L)\nt = 0, (28)\nand therefore, Equations 25 and 26 are equivalent.\nSince the utility functions are linear transformations, proving the following is sufficient, although not necessary, to prove that Equation 25 holds,\nlim t\u2192\u221e\n\u03b8exp(t)a (L) \u2212 \u03b8 exp(F1,...,Fj(t)) a (L)\nt = 0. (29)\nSince L is flexible, it will, in expectation, always behave the same way during each free period. Specifically,\n\u03b8 exp(bFj ,bFj +lFj ) a (L) = \u03b8 exp(bF j\u2032 ,bF j\u2032 +lFj ) a (L), (30)\nfor all j\u2032 such that lFj\u2032 \u2265 lFj . This relationship can be represented graphically, as shown in Figure 4, where for simplicity, we let w(j) = exp(bFj + lFj\u22121 , bFj + lFj ), where lF0 = 0. Therefore,\n\u03b8 exp(F1,...,Fj(t)) a (L) =\nj(t) \u2211\nj=1\n(j(t) \u2212 j + 1)\u03b8w(j)a (L).\nNote that \u03b8w(j)a will be \u201crepresented\u201d more than \u03b8w(j \u2032)\na for j < j\u2032 and any finite t. In order for Equation 29 to hold, in the limit, all \u03b8w(j)a be must represented equally, i.e.\nlim t\u2192\u221e\nj(t) \u2212 j + 1\nt = lim t\u2192\u221e\nj(t) \u2212 j\u2032 + 1\nt , (31)\nfor all j, j\u2032. Consider t(j) = j\u22121(t), i.e. the first time index after the jth free period has ended:\nt(j) = j \u2211\nj\u2032=1\n(lRj + lFj ) \u2265 j \u2211\nj\u2032=1\nlRj . (32)\nBy Equation 23, limj\u2192\u221e t(j)j = \u221e, and therefore,\nlim t\u2192\u221e\nj(t) \u2212 j + 1\nt \u2264 lim t\u2192\u221e\nj(t)\nt = 0. (33)\nTherefore, in the limit, all \u03b8w(j \u2032)\na will be represented equally. However, since\n\u2211j(t) j=1 lFj < t, each \u03b8 w(j) a will\nbe \u201cunderrepresented\u201d compared to \u03b8ta(L) for any finite t. However, in the limit, this is not the case since,\nlim t\u2192\u221e\n\u2211j(t) j=1 lFj\nt = lim t\u2192\u221e\n\u2211j(t) j=1 lFj\n\u2211j(t) j=1(lRj + lFj )\n= lim t\u2192\u221e\n1 Pj(t)\nj=1 lRj Pj(t)\nj=1 lFj + 1\n= 1 (by Equation 22). (34)\nTherefore, in the limit \u03b8w(j)a will be represented equally compared to \u03b8ta(L)."}], "references": [{"title": "Subjectivity and correlation in randomized strategies", "author": ["R. Aumann"], "venue": "Journal of Mathematical Economics, 1:67\u201396,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1974}, {"title": "Statistical Power Analysis for the Behavioral Sciences", "author": ["J. Cohen"], "venue": "2nd edition,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "Combining expert advice in reactive environments", "author": ["D.P.D. Farias", "N. Megiddo"], "venue": "Journal of the ACM, 53(5):762\u2013799,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Calibrated learning and correlated equilibrium", "author": ["D.P. Foster", "R. Vohra"], "venue": "Games and Economic Behavior, 21:40\u201355,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning, hypothesis testing, and Nash equilibrium", "author": ["D.P. Foster", "H.P. Young"], "venue": "Games and Economic Behavior, 45:73\u201396,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Network games", "author": ["A. Galeotti", "S. Goyal", "M.O. Jackson", "F. Vega- Redondo", "L. Yariv"], "venue": "Unpublished, Jan", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Correlated Q-learning", "author": ["A. Greenwald", "K. Hall"], "venue": "Proceedings of ICML-2003, pages 242\u2013249, Washington, DC, USA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "A simple adaptive procedure leading to correlated equilibrium", "author": ["S. Hart", "A. Mas-Colell"], "venue": "Econometrica, 68:1127\u20131150,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Continuous Univariate Distributions, volume", "author": ["N. Johnson", "S. Kotz", "N. Balakrishnan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Correlated equilibria in graphical games", "author": ["S. Kakade", "M. Kearns", "J. Langford", "L. Ortiz"], "venue": "EC \u201903: Proceedings of the 4th ACM Conference on Electronic Commerce, pages 42\u201347, New York, NY, USA,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Strong mediated equilibrium", "author": ["D. Monderer", "M. Tennenholtz"], "venue": "Proceedings of the 21st American Association of Artificial Intelligence Conference, Boston, MA, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "All of Statistics", "author": ["L. Wasserman"], "venue": "Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "That is, our goal is for the agents to learn and adapt so that they find a correlated equilibrium [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "While hypothesis testing has been proposed in the multiagent learning literature as a tool that agents might use to learn how to play Nash equilibria [5], to the best of our knowledge it has never been applied for validating a mediator\u2019s advice.", "startOffset": 150, "endOffset": 153}, {"referenceID": 10, "context": "Note that our type of a mediator is different than Monderer and Tennenholtz\u2019s, where agents must agree to follow the mediator\u2019s suggested actions before knowing what they are [11].", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "for all ai \u2208 Ai [1].", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": "Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 11, "context": "where A is any subset of A such that |A| = |A| \u2212 1, X(a) = lT\u03c3 hist(lT ) A (a) is the actual frequency of play of a \u2208 A during the sampling test, E(a) = lT\u03c3 A (a) is the expected frequency of play according to \u03c3 A , and where lT is the length of the sampling period [12].", "startOffset": 266, "endOffset": 270}, {"referenceID": 8, "context": "The Pearson\u2019s \u03c72 test has (in the limit) a probability distribution function of \u03c7df + \u03c7 2 NCP,1, (6) where the first distribution has df = |A|\u22122 degrees of freedom, and the second distribution has 1 degree of freedom and a non-centrality parameter of NCP [9].", "startOffset": 255, "endOffset": 258}, {"referenceID": 8, "context": "then the probability of a Type 2 error is bounded by some value \u03b2(\u03b4\u0302) < 1, whose value is normally found via numerical computation [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "In practice, lT (\u03b1,\u03b2, \u03b4) would now be solved by some method of numerical computation [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "For simplicity, we used the tables in Cohen to obtain a value of lT = 2100 [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 11, "context": "The p-value is the smallest \u03b1 value that would still allow us to reject the hypothesis [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "We assume that Li is flexible at the beginning of each free period [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "It may also be interesting to apply our approach to other solution concepts such as mediated equilibria [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "Thus, our approach could be combined with methods such as Q-learning [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "Correlated equilibria have also been used in graphical games, which can be used to model many different settings [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "For example, network games use graphical games to help represent a variety of problems, from public good provision and trade to information collection [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "These models can be hindered by a \u201cfundamental theoretical problem: even the simplest games played on networks have multiple equilibrium[sic] which display a bewildering range of possible outcomes\u201d [6].", "startOffset": 198, "endOffset": 201}], "year": 2008, "abstractText": "We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who suggests possible actions to take. We present an algorithm that each agent can use so that, with high probability, they can verify whether or not the mediator\u2019s advice is useful. In particular, if the mediator\u2019s advice is useful then agents will reach a correlated equilibrium, but if the mediator\u2019s advice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our algorithm and show that in the limit it always correctly verifies the mediator\u2019s advice.", "creator": "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"}}}