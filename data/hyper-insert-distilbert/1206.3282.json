{"id": "1206.3282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Improving the Accuracy and Efficiency of MAP Inference for Markov Logic", "abstract": "\u2022 in designing this primary work we present cutting plane inference ( cpi ), a maximum a posteriori ( map ) function inference method proposed for statistical relational analytical learning. framed in pure terms of markov logic and inspired directly by the relational cutting plane truth method, wherein it originally can be seen as a meta reasoning algorithm that partially instantiates small parts errors of a characteristic large simple and nonlinear complex markov network information and we then solves these using a generalized conventional map method. so we evaluate cpi agents on almost two tasks, semantic role labelling estimation and joint entity resolution, while plugging in two equally different map inference design methods : with the current revised method of choice applied for map inference in markov logic, maxwalksat, generalized and fast integer linear grammar programming. usually we now observe that when used with cpi calculations both quantitative methods are significantly faster scoring than when used correctly alone. often in addition, cpi improves reduce the minimal accuracy times of static maxwalksat evaluation and considerably maintains the minimal exactness of integer linear algebra programming.", "histories": [["v1", "Wed, 13 Jun 2012 15:43:49 GMT  (234kb)", "http://arxiv.org/abs/1206.3282v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sebastian riedel"], "accepted": false, "id": "1206.3282"}, "pdf": {"name": "1206.3282.pdf", "metadata": {"source": "CRF", "title": "Improving the Accuracy and Efficiency of MAP Inference for Markov Logic", "authors": ["Sebastian Riedel"], "emails": [], "sections": [{"heading": null, "text": "In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori (MAP) inference method for Statistical Relational Learning. Framed in terms of Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta algorithm that instantiates small parts of a large and complex Markov Network and then solves these using a conventional MAP method. We evaluate CPI on two tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in two different MAP inference methods: the current method of choice for MAP inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We observe that when used with CPI both methods are significantly faster than when used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains the exactness of Integer Linear Programming."}, {"heading": "1 INTRODUCTION", "text": "Many tasks in Machine Learning are inherently relational: the label given to an object often depends on labels given to a set of related objects. For example, in Semantic Role Labelling [Carreras and Marquez, 2005] we are asked to label phrases with the role they play with respect to a given verb. Here the role given to one phrase depends on roles we have assigned to other phrases in the same sentence. It is, for instance, not possible to have two phrases both labelled as the agent of the same verb.\nStatistical Relational Learning [SRL, Ng and Subrahmanian, 1992, Koller, 1999] seeks to provide generic, solid and efficient means to solve such relational tasks. It typically uses variants of First Order Logic to describe Graphical Models with repetitive structure in a compact fashion. This has two main benefits.\nFirstly, the meta-information provided by the first order model can be used to avoid a full instantiation of the Graphical Model in inference and learning. This can yield faster runtime and higher accuracy [Koller, 1999, de Salvo Braz et al., 2005, Singla and Domingos, 2006b]. Secondly, an SRL language along with a powerful interpreter allows application developers to focus on models, and machine learning researchers to focus on foundations. This paradigm of decoupling applications and algorithms has increased the speed of development in many domains [Domingos, 2006].\nMarkov Logic [ML, Richardson and Domingos, 2005] is an expressive SRL language that combines First Order Logic and Markov Networks. It has been successfully used for several tasks such as Information Extraction [Poon and Domingos, 2007] and Entity Resolution [Singla and Domingos, 2006a].\nFor most Markov Logic applications we need to solve the Maximum A Posteriori (MAP) problem of finding the most likely solution given some observation. Richardson and Domingos [2005] proposed the use of MaxWalkSAT [MWS, Kautz et al., 1996] to tackle this problem. In our experiments we apply MWS to two rather simple ML models, one for Semantic Role Labelling and one for Joint Entity Resolution. Here we found MWS to be both slow and inaccurate. However, before languages like ML can ever be used to solve tasks like joint inference in large Natural Language Processing applications [Domingos, 2007] we surely need to able to efficiently and accurately solve comparatively simple ones.\nRather than investigating the use of other solvers such as Belief Propagation and its variants, which can perform quite poorly for the large, densely connected and partially deterministic networks we encounter, we focus on tackling this problem by introducing a meta algorithm: Cutting Plane Inference (CPI) inspired by the Cutting Plane Method [Dantzig et al., 1954].\nCPI incrementally instantiates only those portions of the complete Markov Network for which a current solution can be further optimised and solves these using\nan existing inference method. Often these partial problems are significantly smaller and less complex. Consequently, they are more easily solved than the complete problem.\nEmpirically we show that for Semantic Role Labelling CPI plus MWS is significantly faster and more accurate than MWS alone. When used with Integer Linear Programming (ILP), CPI achieves optimal accuracy due to the exactness of ILP, yet runs significantly faster than when using ILP alone. With this accuracy we are able to achieve state-of-the-art results in Semantic Role Labelling with minimal engineering effort. When tested on an Joint Entity Resolution model taken from from the Markov Logic literature [Singla and Domingos, 2005] CPI with MWS does again better than MWS alone both in terms of speed and accuracy. CPI with ILP allows us to perform efficient and exact inference on this task while ILP alone is infeasible.\nIn the next section of this paper we will present Markov Logic. Section 3 shows two ways of solving the MAP problem for the Markov Networks that Markov Logic describes: MWS and ILP. In section 4 Cutting Plane Inference is presented and we formally show how the accuracy of CPI depends on the accuracy of the base solver. Section 5 compares CPI in combination with MWS and ILP to plain MWS and ILP on two tasks. The first is Semantic Role Labelling, the second Joint Entity Resolution. We conclude with section 6."}, {"heading": "2 MARKOV LOGIC", "text": "Markov Logic [ML, Richardson and Domingos, 2005] is an SRL language based on First Order Logic and Markov Networks. It can be seen as a formalism that extends First Order Logic to allow formulae that can be violated with some penalty. From an alternative point of view, it is a expressive template language that uses First Order Logic formulae to instantiate Markov Networks of repetitive structure. 1\nLet us introduce some notation by example. Assume a simplified version of Semantic Role Labelling where we use an unary predicate agent to select the constituent that acts as agent for a given verb. We also maintain a set of additional predicates that provide information about constituents and their relation to the verb. For example, left can be a unary predicate that denotes constituents to the left of the verb. The set of all predicates will be called P . We also maintain a finite set C of constants representing constituents, words, tags etc.\nIn the following we will use np to denote the arity of a\n1Note that while this paper focuses on Markov Logic due to its expressive power and possibility of undirected dependencies, we note that much of the work reported here can be transferred to other formalisms.\npredicate p, and thus nleft = nagent = 1. In formulae we will denote logical variables using the letter v and some subscript such as v1. For example, in\n\u03c61 : agent (v1)\u21d2 left (v1)\nv1 is a variable and in\n\u03c62 : v1 6= v2 \u2227 agent (v1)\u21d2 \u00acagent (v2)\nv1 and v2 are variables. The number of free variables of a formula \u03c6 will be denoted with n\u03c6, thus n\u03c61 = 1 and n\u03c62 = 2. A grounding \u03c6 [ v1/c1, . . . , vn\u03c6/cn\u03c6 ] is generated by replacing each occurrence of each vi with the constant ci. We will often write \u03c6 [v/c] to mean \u03c6 [ v1/c1, . . . , vn\u03c6/cn\u03c6 ] . For example, \u03c62 [v/c] = c1 6= c2 \u2227 agent (c1)\u21d2 \u00acagent (c2).\nA formula that does not contain any variables is ground. A formula that contains a single predicate and nothing else is an atom. A set of ground atoms is called a possible world [Genesereth and Nilsson, 1987]. We say that a possible world W satisfies a formula \u03c6 and write W \u03c6 if \u03c6 is true inW . For example, the possible world {agent (c1)} does not satisfy \u03c61 [v1/c1]; the possible world {left (c1) , agent (c1)} does. In the following we will identify the binary vector y = ( yp(c) ) p\u2208P,c\u2208Cnp\nwith the possible world { p (c) |yp(c) = 1 } . For the sake of brevity we will often write ypc instead of yp(c). The set of all possible worlds we can construct using a set of predicates P and a set of constants C is YP,C .\nIn First Order Logic a knowledge base is a set of formulae. It describes the set of possible worlds that for which all its formulae are satisfied. In Markov Logic the equivalent of a first order knowledge base is a Markov Logic Network (MLN). Instead of classifying models as either consistent (all formulae are satisfied) or inconsistent (some are not) an MLN maps each possible world to a probability. This allows us to model uncertainty in our beliefs about the world. For example, a world in which agent (c1) \u21d2 left (c1) does not hold should not be impossible, it should just be a bit less likely because the agent of a verb tends to be on its left but can appear on its right in passive constructions.\nWe define an MLN M as set of pairs {(\u03c6i, wi)}i where each \u03c6i is a formula in First Order Logic and wi \u2208 R is a real number. Together with a finite set of constants C, an MLN M defines a log-linear probability distribution over possible worlds y \u2208 YP,C as follows\np (y) = 1 Z exp  \u2211 (\u03c6,w)\u2208M w \u2211 c\u2208Cn\u03c6 f\u03c6c (y)  (1) where the feature function f\u03c6c is defined as\nf\u03c6c (y) = I ( y \u03c6 [ v1/c1, . . . , vn\u03c6/cn\u03c6 ])\nZ is a normalisation constant, I (true) = 1 and I (false) = 0.\nThis distribution is strictly positive and corresponds to a Markov Network which is referred to as the Ground Markov Network. Note that we can represent hard constraints using very large weights.\nFor example, with M = {(\u03c61, 2.5) , (\u03c62, 1.2)} and the finite set of constants C = {n1, n2, . . .} that represent the nodes of the parse tree, the log-linear model would contain, among others, the feature f\u03c61n1 (y) = I ( y agent (n1)\u21d2 left (n1)) that returns 1 if the contained ground formula holds in the possible world y and 0 otherwise."}, {"heading": "3 MAP INFERENCE", "text": "In many settings we are given an MLNM and the state of a set of observed ground atoms ( xp(c) ) p\u2208O,c\u2208Cnp for a set of observable predicates O. We are then asked to find the set of hidden ground atoms y\u0302 \u2208 YH,C for a set of remaining predicates H = P \\O with maximum a posteriori probability (MAP)2\ny\u0302 = arg max y\u2208YH,C p (y|x) = arg max y\u2208YH,C s (y,x)\nwhere\ns (y,x) = \u2211\n(\u03c6,w)\u2208M\nw \u2211\nc\u2208Cn\u03c6 f\u03c6c (y,x) (2)\ncan be considered as a linear discriminant or scoring function that evaluates the goodness of a problem solution pair (x,y).\nFor example, we might be looking for the truth states of the H = {agent} atoms while knowing the state of all O = {left} ground atoms that indicate which constituents are placed to the left of the verb."}, {"heading": "3.1 MAXWALKSAT", "text": "Previous work [Richardson and Domingos, 2005] finds y\u0302 using MaxWalkSAT (MWS), an approximate RandomWalk method that has been very successfully used to solve Weighted SAT Problems [Kautz et al., 1996].\nIt starts by assigning a random state to all ground atoms and proceeds by repeatedly picking a random unsatisfied ground clause. With probability q a random ground atom of this clause is picked and its state is flipped. With probability 1 \u2212 q the ground atom which, when flipped, causes the largest increase in total weight s (y,x) is chosen to be flipped. The process is repeated until a fixed number nflips of flips is reached. Optionally one can try again nrestarts times to find a better y, each time starting at a new random solution.\n2In the case where multiple maxima exist we can pick any of these."}, {"heading": "3.2 INTEGER LINEAR PROGRAMMING", "text": "Integer Linear Programming [ILP, Winston and Venkataramanan, 2003] refers to the process of optimising a linear objective function under a set of linear inequalities and the constraint that all (or some) variables are integers. ILP has been used in many tasks to solve MAP problems [Roth and Yih, 2005, Clarke and Lapata, 2007] because of its exactness, its declarative nature and the availability of very efficient ILP solvers.\nHere we present a generic mapping from Ground Markov Networks in Markov Logic to ILPs.3 We start by replacing each feature function application f\u03c6c (y) in equation 2 with a binary variable \u03bb\u03c6c and constrain f\u03c6c (y) and \u03bb \u03c6 c to be equal. This leads to the optimisation problem\narg max y\u2208YH,C \u2211 (\u03c6,w)\u2208M w \u2211 c\u2208Cn\u03c6 \u03bb\u03c6c\ns.t \u03bb\u03c6c = f \u03c6 c (y,x)\u2200 (\u03c6,w) \u2208M, c \u2208 Cn\u03c6\nwith a linear objective function under a set of constraints.\nIn order to turn this into an ILP we need to transform each constraint into a set of linear constraints over y and the auxiliary variables ( \u03bb\u03c6c ) \u03c6,c\u2208Cn\u03c6 . This can be achieved by\n1. Mapping each constraint to a logical equivalence of auxiliary variable and ground formula, such as \u03bb\u03c6n1 \u21d4 agent (n1)\u21d2 left (n1)\n2. Replacing ground atoms by either true or false if they are observed or, if not, by their corresponding decision variable, as in \u03bb\u03c6n1 \u21d4 y agent n1 \u21d2 false\n3. Transforming the logical equivalence into Conjunctive Normal Form4, as in ( \u00ac\u03bb\u03c6n1 \u2228 y agent n1 ) \u2227(\n\u03bb\u03c6n1 \u2228 \u00acy agent n1 ) 4. Replacing each disjunction by a linear con-\nstraint [Williams, 1999], for example \u22121.0 \u00b7 \u03bb\u03c6n1 + 1.0 \u00b7 y agent n1 \u2265 0\nNote that we can significantly simplify the above program for hard constraints (i.e.., formulae with very large w) and formulae with only one hidden atom. We omit details for brevity.\n3In general it is always possible to map any Markov Network to an ILP [Taskar, 2004]. However, our mapping is tailor-made for Markov Logic and yields smaller programs.\n4If the formula contains universal (existential) quantified formulae we replace these with conjunctions (disjunctions) using the finite set of constants C."}, {"heading": "4 CUTTING PLANE INFERENCE", "text": "We will show in the empirical section of this paper that running inference using the full grounding of a Markov Logic Network can be slow and in the case of MWS also inaccurate. We will now present an algorithm that tries to overcome this problem by instantiating only portions of the complete Ground Markov Network and running an off-the-shelf inference method in this network."}, {"heading": "4.1 ALGORITHM", "text": "The proposed algorithm is a variant of the Cutting Plane approach from Operations Research [Dantzig et al., 1954]. Cutting Plane algorithms solve large scale constrained optimisation problems by only considering a subset of constraints. In each iteration the solution to a partial problem is provided to an oracle that returns a set of constraints5 the solution violates. The current problem is extended by these new constraints and re-solved. The process is repeated until no more violated constraints can be found.\nInstead of searching for violated constraints, Cutting Plane Inference (CPI) searches for feature-weight products in equation 2 that do not maximally contribute to the overall sum given the current solution. More precisely, for each formula \u03c6 and a given (y\u2032,x) we are looking for all tuples, Separate (\u03c6,w,y,x) \u2286 Cn\u03c6 , for which\nw \u00b7 f\u03c6c (y\u2032,x) < max y\u2208YH,C w \u00b7 f\u03c6c (y,x) (3)\nWe will say that the corresponding ground formulae are not maximally satisfied in the world y\u2032.\nIn the terminology of the Cutting Plane method this step is often referred to as separation: it finds a set of constraints that separates feasible solution from infeasible solutions. In our case this step will help to separate possible worlds with high score from those with low score.\nIt will be useful to define a partial grounding G = (G\u03c6)(\u03c6,w)\u2208M withG\u03c6 \u2286 C\nn\u03c6 that maps each first order formula \u03c6 to a set of tuples we ground it with. A partial grounding induces a partial score\nsG (y,x) = \u2211\n(\u03c6,w)\u2208M w \u2211 c\u2208G\u03c6 f\u03c6c (y,x) (4)\nCPI proceeds as described in algorithm 1. In each iteration i we maintain a partial grounding Gi. Initially G0 is filled with a small number of groundings. A natural choice are all groundings of formulae that only\n5In case of linear constraints these constraints form hyperplanes that further cut the space of feasible solution, hence the name.\ncontain one hidden predicate. In this case maximising sG0 is trivial because the hidden variables do not interact and often gives a very good first guess.\nIn step 5 we find a solution y that maximises the partial score sGi\u22121 (or approximately maximises it). For this we can pick our optimisation method of choice. In steps 9 and 10 we find the ground formulae which are not maximally satisfied in the current solution y and add them to the current partial grounding. We terminate if no more new ground formulae are foundor a maximum number of iterations is reached. This process calculates one solution y in each iteration. The final result is the solution y with highest score. Algorithm 1 CPI ( M,G0,x\n) 1: i\u2190 0 2: y\u2032 \u2190 0 3: repeat 4: i\u2190 i+ 1 5: y\u2190 solve ( Gi\u22121,x\n) 6: if s (y,x) > s (y\u2032,x) then 7: y\u2032 \u2190 y 8: end if 9: for each (\u03c6,w) \u2208M do 10: Gi\u03c6 \u2190 G i\u22121 \u03c6 \u222a Separate (\u03c6,w,y,x) 11: end for 12: until Gi\u03c6 = G i\u22121 \u03c6 or i > maxIterations 13: return y\u2032\nThe following theorem shows that when CPI returns the solution of iteration i the error is bound by the sum of the error of the base solver on the partial problem and the sum of absolute weights of newly found ground formulae at this iteration. In particular, for an iteration with no more newly found groundings the error is only bound by (in fact it is equal to) the error of the base solver on the partial problem, which is likely to be much smaller and easier to solver than the original one.\nThis also shows that if the base solver is exact (like ILP) and no more groundings are found, CPI will be exact. If we choose a solution for which new ground formulae were found the error bound is incremented by the sum of the absolute weights of these ground clauses. Thus we still do well if the remaining clauses have small weight. Theorem. Let y\u0302 be an optimal solution, y\u2032 the solution returned by CPI taken from iteration i, y\u0302Gi\u22121 an optimal solution for sGi\u22121 and b =\u2211\n(\u03c6,w) \u2211 c\u2208Gi\u03c6\\G i\u22121 \u03c6 |w| then\ns (y\u0302,x)\u2212 s (y\u2032,x) \u2264 sGi\u22121 (y\u0302Gi\u22121 ,x)\u2212 sGi\u22121 (y\u2032,x) + b\nProof. Let Gi \\ Gi\u22121 = ( Gi\u03c6 \\G i\u22121 \u03c6 ) \u03c6 be the newly\nadded groundings and Gi = ( Cn\u03c6 \\Gi\u03c6 ) \u03c6 the remaining groundings. We can split s (y\u0302,x) \u2212 s (y\u2032,x) into three parts, a score difference for the ground formulae in Gi\u22121, those in Gi \\ Gi\u22121 and Gi. We know that y\u2032 solves s\nGi optimally based on equation 3,\nthus s Gi (y\u0302,x) \u2212 s Gi (y\u2032,x) \u2264 0. Furthermore, in the worst case each term w \u00b7 f\u03c6c (y\u2032,x) in sGi/Gi\u22121 (y\u2032,x) is by |w| smaller than each corresponding term in sGi\\Gi\u22121 (y\u0302,x), leading to sGi\\Gi\u22121 (y\u0302,x) \u2212 sGi\\Gi\u22121 (y\u2032,x) \u2264 \u2211 (\u03c6,w) \u2211 c\u2208Gi\u03c6\\G i\u22121 \u03c6 |w|.\nNote that we do not make any claims about the runtime of the algorithm. Even without a limit on the number iterations it is guaranteed to converge in a finite number of steps due to the fact that the solution space is finite and we will either try each solution or return to a previous one. However, we cannot provide guarantees as to how many steps this will take. Thus we allow the algorithm to terminate before convergence is reached."}, {"heading": "4.2 SEPARATION", "text": "An integral part of CPI is the separation step, in which we need to find all groundings c of a formula \u03c6 and weight w which are not maximally satisfied (according to equation 3) for a given solution y\u2032. It is this step for which the Statistical Relational Learning paradigm comes into play. In a (propositional) Markov Network we do not have any higher order descriptions of its features. Performing separation then means evaluating all features of the network.\nIn Markov Logic, however, we can do better. There are two cases to consider. If w > 0 we have to find assignments c with f\u03c6c (y,x) = 0, that is, groundings for which y,x \u03c6 [v/c] is false. Correspondingly, for w < 0 we have to find c for which y,x \u03c6 [v/c] is true.\nWe cast this into a database query evaluation problem6 and store the atoms in y and x as rows in database tables. Then we convert the formula \u03c6 (or \u00ac\u03c6) to a database query which is executed during CPI. Such queries can often be processed very efficiently [Grohe et al., 2001]. In our experiments the cost of query evaluation was marginal when compared to the cost of numeric optimisation."}, {"heading": "4.3 RELATED WORK", "text": "The idea of Cutting Planes have been used in at least two ways. We can either use it to tackle ILP problems by solving their LP relaxation and, in case the solution is fractional, generate additional constraints\n6Alternatively we could frame this problem as an instance of theorem proving, but all axioms are ground atoms and we are looking for all groundings for which the formula holds \u2013 Database technology is optimised for this setting.\nthe integer solution is known to fulfil. Or we use it to solve problems with a large number of constraints, such as ILP formulations of the Travelling Salesman Problem [Dantzig et al., 1954], without having to include all of them.\nOur work follows previous research in MAP inference [Riedel and Clarke, 2006, Anguelov et al., 2004, Tromble and Eisner, 2006, Sontag and Jaakkola, 2007] that uses Cutting Planes to avoid including all constraints in advance. However, in this work we frame, implement and evaluate the approach more generally as a meta algorithm for MAP inference in Markov Logic Networks into which we can plug-in any existing propositional solver. This includes the introduction of a separation routine that does not require additional implementation efforts when applied to new tasks. Markov Logic Networks may also contain nondeterministic constraints. In contrast to previous work [Tromble and Eisner, 2006] our method handles these without the need to branch-and-bound.\nCPI is also similar in nature to LazySAT [Singla and Domingos, 2006b], a memory-efficient implementation of MWS: both methods avoid to instantiate the full ground network. However, while CPI only instantiates new parts of the ground network once the base solver has optimised the current partial network, LazySAT instantiates new parts of the network whenever they may be needed during the inner loop of MWS. Note that although CPI also reduces memory overhead, in this work we focus on its speed and accuracy and thus do not directly compare it to LazySAT, which inherits the speed and accuracy of MWS."}, {"heading": "5 EXPERIMENTS", "text": "We use two tasks to evaluate the utility of CPI as a meta MAP inference method for Markov Logic. The first is Semantic Role Labelling, the second Joint Entity Resolution. In both cases we want to investigate how CPI affects the runtime and accuracy of two base solvers: MWS and ILP. For all experiments we use our own Markov Logic implementation running on a Pentium 4 at 2.8Ghz with 4Gb RAM. All CPI systems use local formulae with only one hidden atom to create the initial grounding G0."}, {"heading": "5.1 SEMANTIC ROLE LABELLING", "text": "Semantic Role Labelling refers to the task of identifying and classifying the arguments and modifiers of verbs in natural language text, as in\n[A0He] [AM-MODwould] [A0n\u2019t] [Vaccept] [A1anything of value].\nfor the verb \u201caccept\u201d. Labels such as \u201cA0\u201d serve as placeholders for actual roles of the given verb, such\nas \u201cacceptor\u201d in the above case. The most effective approach to Semantic Role Labelling to date is based on the output of a constituent parser. Each constituent is labelled with the type of argument or modifier it represents with respect to the verb in question. We model the task using a (typed) binary label predicate defined over constituents and possible labels. Atoms of this predicate are hidden at test time.\nWe set up a knowledge base of rules that describe local features of constituents and global dependencies between labels, resembling previous work in Semantic Role Labelling [Punyakanok et al., 2005]. The rules we use are slightly more general than our examples \u03c61 and \u03c62 in section 2.\nWe learn the weights of this model using the CoNLL 2005 dataset [Carreras and Marquez, 2005] and the Online Learner MIRA [Crammer and Singer, 2003]. For inference during training we use CPI with ILP.\nFor testing we use the first 100 verb frames from the WSJ test set of the CoNLL 2005.7 In table 1 we show the following metrics: 1) the score delta to the optimal solution with respect to the soft clauses, \u2206ssoft = sGsoft (y\u0302,x)\u2212sGsoft (y\u2032,x) where Gsoft contains all ground formulae for each non-deterministic formula; 2) the number of violations of deterministic formulae; 3) the runtime taken; 4) the number of calls to the optimiser; 5) the F1 accuracy on the task. Note that the total score delta \u2206s = s (y\u0302,x) \u2212 s (y\u2032,x) is always dominated by the hard constraints as they have very large weights. Thus if system A produces one less violation than system B its total score delta \u2206s will be smaller.\nWe first note that using plain MWS with 100,000 flips (M-100k) and no restarts8 is less accurate in terms of soft model score and F1 accuracy when compared with CPI-MWS using the same number of flips (C-M100k). It is also significantly slower and produces some hard constraint violations while CPI & MWS does not. When using ILPwe achieve perfect model score since ILP returns exact solutions. Using ILP with CPI (CILP) is still exact; however, CPI speeds up the solver by almost two orders of magnitude.9\nWe also ran CPI-ILP on the full test set to compare our system with the state of the art, yielding 77.1 F1 measure. When compared to the entries in the CoNLL\n7The reason for not using more instances were memory problems we encountered when we were using MWS alone and grounding the complete network. These problems will likely disappear when using LazySAT instead of MWS.\n8Note that we also experimented with using restarts; however, differences to runs with equivalent total numbers of flips and no restarts were only marginal.\n9Note that the difference in runtime between MWS and ILP should be taken with caution: for ILP we use a wellestablished software library (lp-solve), for MWS our own implementation.\nshared task that only use the output of one parser our system would come out first [Carreras and Marquez, 2005].\nWe also wanted to investigate how CPI scales with problem size. Figure 1 shows the runtime of CPI-ILP against the number of candidate nodes. It seems that for this dataset and problem, CPI scales almost linearly with problem size up until 50 candidates. After 50 candidates a linear trend can only be guessed due to data sparseness. This linear dependency is interesting because the actual number of ground formulae rises quadratically (due to the no-overlap formula and others)."}, {"heading": "5.2 JOINT ENTITY RESOLUTION", "text": "Entity resolution is a crucial problem in many business, government and research projects. It can be described as the task of finding database records that refer to the same entity and is very similar to Coreference Resolution in NLP. In our experiments records are citations, and we search for citations describing the same publication; however, we not only want to find matching citations, we also want to jointly identify author or venue name strings referring to the same author or venue, respectively.\nWe use a knowledge base with 46 formulae provided\nby Singla and Domingos [2005] with predicates such as sameBib and sameAuthor that denote citation and author matches, respectively. The knowledge base states regularities such as \u201cif two authors names match the corresponding citations match\u201d or \u201cif the tdf-if distance between the titles is between 0.7 and 0.8 the titles match\u201d. It also contains the transitivity rule\nsameBib (v1, v2) \u2227 sameBib (v2, v3) \u21d2 sameBib (v1, v3)\nThis is a hard constraint and imposes a difficult problem for many generic inference methods [Poon and Domingos, 2006].\nIn our first set of experiments we used a cleaned version [Singla and Domingos, 2005] of the Cora Database [Bilenko and Mooney, 2003], containing approximately 1200 citations of computer science articles. In total these citations refer to about 120 unique publications. Following previous work [Singla and Domingos, 2006b], we tested and trained using a 10-fold leave-one-out procedure and PseudoLikelihood estimation while ensuring that folds do not contain split citation clusters [Singla and Domingos, 2005]. Each fold contains roughly 120 records.\nTable 2 shows our results for Entity Resolution. They are consistent with those in table 1: again CPI renders MWS faster and more accurate both in terms of violations, soft model score and F1 measure. However, this time we cannot directly compare plain ILP with CPI-ILP because the full ILP did not fit into memory. In other words, here CPI makes an infeasible method feasible. Note that in this case CPI-MWS did not converge, thus we terminated the algorithm after a predefined number of iterations (30).\nInterestingly, the F1 accuracy of CPI with MWS is significantly better than the F1 accuracy of MWS alone. This can be explained if we consider that CPI-MWS returns solutions with significantly higher soft model score. This score reflects what the model learnt to be a good matching, independent of the number of violations. CPI-MWS can achieve a higher soft score because it starts at a solution that maximises the local score without considering any hard constraints. MWS, on the other hand, starts at a random solution\nthat may contain many (locally) unlikely matchings and will spend most of its time making these unlikely matchings consistent.\nIn our second set of experiments we wanted to again evaluate the runtime behaviour of CPI when the problem size increases. For this purpose we used the Bibserv.org corpus and a model trained on the Cora dataset. Bibserv.org consists of about 20,000 citations. We used the same random subsets of size 50 to 500 in steps of 50 as Singla and Domingos [2006b]. All following results are averaged over 5 datasets of the same size.\nFigure 2 shows the runtime of CPI-ILP with increasing number of citation pairs \u2013 this corresponds to the number of decisions to make. Again CPI-ILP seems to scale linearly with the number of variables and thus quadratically with the number of citations. Yet, the number of features in the complete network scales at least cubically with the number of citations due to the transitivity clause."}, {"heading": "6 CONCLUSION", "text": "In this paper we presented Cutting Plane Inference (CPI), a novel method for finding MAP solutions in Markov Logic that incrementally solves partial versions of the complete Ground Markov Network based on a Cutting Plane approach. While Cutting Planes have been used for specific MAP inference problems before, this work shows how they can be generalised and incorporated into a Statistical Relational Learning framework where they become automatically available for a wide range of tasks. Our method essentially serves as a meta algorithm that alternates between deterministic first order query processing on one hand, and numeric optimisation of partial problems on the other.\nWe evaluated the proposed algorithm using two real-\nworld tasks for which we showed MWS to perform poorly: Joint Entity Resolution and Semantic Role Labelling. In both cases CPI makes an exact method (ILP) more efficient while remaining exact, and an approximate method (MWS) both faster and more accurate.\nHowever, exact MAP inference in general Graphical Models is PP-complete [Park, 2002]. Thus we obviously cannot expect Cutting Plane Inference to work for arbitrary formulae, weights and problems \u2013 at least not for ILP as base solver. Yet, we believe that both of the above tasks are instances of a larger class of problems that are not so much characterised by their network structure or strength of weights but by how well local formulae and weights predict the global goodness of a structure. CPI extends the applicability of SRL to this class and might therefore contribute to a more widespread use of SRL.\nIt will be important to investigate how to characterise the class of problems we can not solve with CPI. For example, consider a conjunctive formula like p (v1) \u2227 q (v2) with positive weight and sparsely populated predicates p and q in the current solution y. Separation will find all pairs of v1 and v2 for which the conjunction does not hold and here this set would be almost exhaustive, resulting in a problem not much smaller than the original one."}], "references": [{"title": "The correlated correspondence algorithm for unsupervised registration of nonrigid surfaces", "author": ["D. Anguelov", "D. Koller", "P. Srinivasan", "S. Thrun", "H.-C. Pang", "J. Davis"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Anguelov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Anguelov et al\\.", "year": 2004}, {"title": "Adaptive duplicate detection using learnable string similarity measures", "author": ["M. Bilenko", "R. Mooney"], "venue": "In Proc. of the Intl. Conf. on Knowledge Discovery and Data Mining,", "citeRegEx": "Bilenko and Mooney.,? \\Q2003\\E", "shortCiteRegEx": "Bilenko and Mooney.", "year": 2003}, {"title": "Introduction to the conll-2005 shared task: Semantic role labeling", "author": ["X. Carreras", "L. Marquez"], "venue": "In Proc. of the Conf. on Computational Natural Language Learning,", "citeRegEx": "Carreras and Marquez.,? \\Q2005\\E", "shortCiteRegEx": "Carreras and Marquez.", "year": 2005}, {"title": "Modelling compression with discourse constraints", "author": ["James Clarke", "Mirella Lapata"], "venue": "In Proc. of the 2007 Joint EMNLP/CoNLL Conf.,", "citeRegEx": "Clarke and Lapata.,? \\Q2007\\E", "shortCiteRegEx": "Clarke and Lapata.", "year": 2007}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer.,? \\Q2003\\E", "shortCiteRegEx": "Crammer and Singer.", "year": 2003}, {"title": "Solution of a large-scale traveling salesman problem", "author": ["G.B. Dantzig", "R. Fulkerson", "S.M. Johnson"], "venue": "Operations Research,", "citeRegEx": "Dantzig et al\\.,? \\Q1954\\E", "shortCiteRegEx": "Dantzig et al\\.", "year": 1954}, {"title": "Lifted first-order probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In Proc. of the 2005 Intl. Joint Conf. on Artificial Intelligence,", "citeRegEx": "Braz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Braz et al\\.", "year": 2005}, {"title": "Artificial Intelligence: The. First Hundred Years, chapter What\u2019s Missing in AI: The Interface Layer", "author": ["P. Domingos"], "venue": null, "citeRegEx": "Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Domingos.", "year": 2006}, {"title": "Structured machine learning: Ten problems for the next ten years", "author": ["Pedro Domingos"], "venue": "In Proc. of the Annual Intl. Conf. on Inductive Logic Programming,", "citeRegEx": "Domingos.,? \\Q2007\\E", "shortCiteRegEx": "Domingos.", "year": 2007}, {"title": "Logical Foundations of Artificial Intelligence", "author": ["Michael Genesereth", "Nils J. Nilsson"], "venue": null, "citeRegEx": "Genesereth and Nilsson.,? \\Q1987\\E", "shortCiteRegEx": "Genesereth and Nilsson.", "year": 1987}, {"title": "When is the evaluation of conjunctive queries tractable", "author": ["Martin Grohe", "Thomas Schwentick", "Luc Segoufin"], "venue": "In Proc. of the ACM Symposium on Theory of Computing,", "citeRegEx": "Grohe et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Grohe et al\\.", "year": 2001}, {"title": "A general stochastic approach to solving problems with hard and soft constraints", "author": ["H. Kautz", "B. Selman", "Y. Jiang"], "venue": "In The Satisfiability Problem: Theory and Applications,", "citeRegEx": "Kautz et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kautz et al\\.", "year": 1996}, {"title": "Probabilistic relational models", "author": ["D. Koller"], "venue": "In Proc. of the Intl. Conf. on Inductive Logic Programming,", "citeRegEx": "Koller.,? \\Q1999\\E", "shortCiteRegEx": "Koller.", "year": 1999}, {"title": "Probabilistic logic programming", "author": ["Raymond T. Ng", "V.S. Subrahmanian"], "venue": "Information and Computation,", "citeRegEx": "Ng and Subrahmanian.,? \\Q1992\\E", "shortCiteRegEx": "Ng and Subrahmanian.", "year": 1992}, {"title": "Map complexity results and approximation methods", "author": ["J. Park"], "venue": "In Proc. of the Conf. on Uncertainty in AI,", "citeRegEx": "Park.,? \\Q2002\\E", "shortCiteRegEx": "Park.", "year": 2002}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["H. Poon", "P. Domingos"], "venue": "In Proc. of the Natl. Conf. on Artificial Intelligence. AAAI Press,", "citeRegEx": "Poon and Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2006}, {"title": "Joint inference in information extraction", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Proc. of the Natl. Conf. on Artificial Intelligence,", "citeRegEx": "Poon and Domingos.,? \\Q2007\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2007}, {"title": "Generalized inference with multiple semantic role labeling systems", "author": ["V. Punyakanok", "D. Roth", "W. Yih"], "venue": "In Proc. of the Conf. on Computational Natural Language Learning,", "citeRegEx": "Punyakanok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2005}, {"title": "Markov logic networks", "author": ["Matthew Richardson", "Pedro Domingos"], "venue": "Technical report, University of Washington,", "citeRegEx": "Richardson and Domingos.,? \\Q2005\\E", "shortCiteRegEx": "Richardson and Domingos.", "year": 2005}, {"title": "Incremental integer linear programming for non-projective dependency parsing", "author": ["Sebastian Riedel", "James Clarke"], "venue": "In Proc. of the Conf. on Empirical Methods in Natural Language Processing,", "citeRegEx": "Riedel and Clarke.,? \\Q2006\\E", "shortCiteRegEx": "Riedel and Clarke.", "year": 2006}, {"title": "Integer linear programming inference for conditional random fields", "author": ["D. Roth", "W. Yih"], "venue": "In Proc. of the Intl. Conf. on Machine Learning,", "citeRegEx": "Roth and Yih.,? \\Q2005\\E", "shortCiteRegEx": "Roth and Yih.", "year": 2005}, {"title": "Discriminative training of markov logic networks", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proc. of the Natl. Conf. on Artificial Intelligence,", "citeRegEx": "Singla and Domingos.,? \\Q2005\\E", "shortCiteRegEx": "Singla and Domingos.", "year": 2005}, {"title": "Entity resolution with markov logic", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proc. of the Intl. Conf. on Data Mining,", "citeRegEx": "Singla and Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Singla and Domingos.", "year": 2006}, {"title": "Memory-efficient inference in relational domains", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proc. of the Natl. Conf. on Artificial Intelligence,", "citeRegEx": "Singla and Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Singla and Domingos.", "year": 2006}, {"title": "New outer bounds on the marginal polytope", "author": ["D. Sontag", "T. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sontag and Jaakkola.,? \\Q2007\\E", "shortCiteRegEx": "Sontag and Jaakkola.", "year": 2007}, {"title": "Learning Structured Prediction Models: a Large-Margin approach", "author": ["Ben Taskar"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Taskar.,? \\Q2004\\E", "shortCiteRegEx": "Taskar.", "year": 2004}, {"title": "A fast finite-state relaxation method for enforcing global constraints on sequence decoding", "author": ["Roy W. Tromble", "Jason Eisner"], "venue": "In Proc. of the Joint HLT/NAACL Conf.,", "citeRegEx": "Tromble and Eisner.,? \\Q2006\\E", "shortCiteRegEx": "Tromble and Eisner.", "year": 2006}, {"title": "Model Building in Mathematical Programming", "author": ["H. Paul Williams"], "venue": "Wiley, 4th edition,", "citeRegEx": "Williams.,? \\Q1999\\E", "shortCiteRegEx": "Williams.", "year": 1999}], "referenceMentions": [{"referenceID": 2, "context": "For example, in Semantic Role Labelling [Carreras and Marquez, 2005] we are asked to label phrases with the role they play with respect to a given verb.", "startOffset": 40, "endOffset": 68}, {"referenceID": 7, "context": "This paradigm of decoupling applications and algorithms has increased the speed of development in many domains [Domingos, 2006].", "startOffset": 111, "endOffset": 127}, {"referenceID": 16, "context": "It has been successfully used for several tasks such as Information Extraction [Poon and Domingos, 2007] and Entity Resolution [Singla and Domingos, 2006a].", "startOffset": 79, "endOffset": 104}, {"referenceID": 8, "context": "However, before languages like ML can ever be used to solve tasks like joint inference in large Natural Language Processing applications [Domingos, 2007] we surely need to able to efficiently and accurately solve comparatively simple ones.", "startOffset": 137, "endOffset": 153}, {"referenceID": 7, "context": "Richardson and Domingos [2005] proposed the use of MaxWalkSAT [MWS, Kautz et al.", "startOffset": 15, "endOffset": 31}, {"referenceID": 5, "context": "Rather than investigating the use of other solvers such as Belief Propagation and its variants, which can perform quite poorly for the large, densely connected and partially deterministic networks we encounter, we focus on tackling this problem by introducing a meta algorithm: Cutting Plane Inference (CPI) inspired by the Cutting Plane Method [Dantzig et al., 1954].", "startOffset": 345, "endOffset": 367}, {"referenceID": 21, "context": "When tested on an Joint Entity Resolution model taken from from the Markov Logic literature [Singla and Domingos, 2005] CPI with MWS does again better than MWS alone both in terms of speed and accuracy.", "startOffset": 92, "endOffset": 119}, {"referenceID": 9, "context": "A set of ground atoms is called a possible world [Genesereth and Nilsson, 1987].", "startOffset": 49, "endOffset": 79}, {"referenceID": 18, "context": "Previous work [Richardson and Domingos, 2005] finds \u0177 using MaxWalkSAT (MWS), an approximate RandomWalk method that has been very successfully used to solve Weighted SAT Problems [Kautz et al.", "startOffset": 14, "endOffset": 45}, {"referenceID": 11, "context": "Previous work [Richardson and Domingos, 2005] finds \u0177 using MaxWalkSAT (MWS), an approximate RandomWalk method that has been very successfully used to solve Weighted SAT Problems [Kautz et al., 1996].", "startOffset": 179, "endOffset": 199}, {"referenceID": 27, "context": "Replacing each disjunction by a linear constraint [Williams, 1999], for example \u22121.", "startOffset": 50, "endOffset": 66}, {"referenceID": 25, "context": "In general it is always possible to map any Markov Network to an ILP [Taskar, 2004].", "startOffset": 69, "endOffset": 83}, {"referenceID": 5, "context": "The proposed algorithm is a variant of the Cutting Plane approach from Operations Research [Dantzig et al., 1954].", "startOffset": 91, "endOffset": 113}, {"referenceID": 10, "context": "Such queries can often be processed very efficiently [Grohe et al., 2001].", "startOffset": 53, "endOffset": 73}, {"referenceID": 5, "context": "Or we use it to solve problems with a large number of constraints, such as ILP formulations of the Travelling Salesman Problem [Dantzig et al., 1954], without having to include all of them.", "startOffset": 127, "endOffset": 149}, {"referenceID": 26, "context": "In contrast to previous work [Tromble and Eisner, 2006] our method handles these without the need to branch-and-bound.", "startOffset": 29, "endOffset": 55}, {"referenceID": 17, "context": "We set up a knowledge base of rules that describe local features of constituents and global dependencies between labels, resembling previous work in Semantic Role Labelling [Punyakanok et al., 2005].", "startOffset": 173, "endOffset": 198}, {"referenceID": 2, "context": "We learn the weights of this model using the CoNLL 2005 dataset [Carreras and Marquez, 2005] and the Online Learner MIRA [Crammer and Singer, 2003].", "startOffset": 64, "endOffset": 92}, {"referenceID": 4, "context": "We learn the weights of this model using the CoNLL 2005 dataset [Carreras and Marquez, 2005] and the Online Learner MIRA [Crammer and Singer, 2003].", "startOffset": 121, "endOffset": 147}, {"referenceID": 2, "context": "shared task that only use the output of one parser our system would come out first [Carreras and Marquez, 2005].", "startOffset": 83, "endOffset": 111}, {"referenceID": 7, "context": "by Singla and Domingos [2005] with predicates such as sameBib and sameAuthor that denote citation and author matches, respectively.", "startOffset": 14, "endOffset": 30}, {"referenceID": 15, "context": "This is a hard constraint and imposes a difficult problem for many generic inference methods [Poon and Domingos, 2006].", "startOffset": 93, "endOffset": 118}, {"referenceID": 21, "context": "In our first set of experiments we used a cleaned version [Singla and Domingos, 2005] of the Cora Database [Bilenko and Mooney, 2003], containing approximately 1200 citations of computer science articles.", "startOffset": 58, "endOffset": 85}, {"referenceID": 1, "context": "In our first set of experiments we used a cleaned version [Singla and Domingos, 2005] of the Cora Database [Bilenko and Mooney, 2003], containing approximately 1200 citations of computer science articles.", "startOffset": 107, "endOffset": 133}, {"referenceID": 21, "context": "Following previous work [Singla and Domingos, 2006b], we tested and trained using a 10-fold leave-one-out procedure and PseudoLikelihood estimation while ensuring that folds do not contain split citation clusters [Singla and Domingos, 2005].", "startOffset": 213, "endOffset": 240}, {"referenceID": 7, "context": "We used the same random subsets of size 50 to 500 in steps of 50 as Singla and Domingos [2006b]. All following results are averaged over 5 datasets of the same size.", "startOffset": 79, "endOffset": 96}, {"referenceID": 14, "context": "However, exact MAP inference in general Graphical Models is PP-complete [Park, 2002].", "startOffset": 72, "endOffset": 84}], "year": 2008, "abstractText": "In this work we present Cutting Plane Inference (CPI), a Maximum A Posteriori (MAP) inference method for Statistical Relational Learning. Framed in terms of Markov Logic and inspired by the Cutting Plane Method, it can be seen as a meta algorithm that instantiates small parts of a large and complex Markov Network and then solves these using a conventional MAP method. We evaluate CPI on two tasks, Semantic Role Labelling and Joint Entity Resolution, while plugging in two different MAP inference methods: the current method of choice for MAP inference in Markov Logic, MaxWalkSAT, and Integer Linear Programming. We observe that when used with CPI both methods are significantly faster than when used alone. In addition, CPI improves the accuracy of MaxWalkSAT and maintains the exactness of Integer Linear Programming.", "creator": "TeX"}}}