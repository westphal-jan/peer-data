{"id": "1206.3295", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Refractor Importance Sampling", "abstract": "in this paper patent we currently introduce spectral refractor importance sampling ( ris ), an improvement suggested to reduce error variance in functional bayesian peripheral network importance gradient sampling propagation under evidential matrix reasoning. accordingly we accordingly prove explicitly the existence of a broader collection of maximal importance functions that are close to the optimal optimal minimum importance function under plain evidential reasoning. based on this theoretic computational result framework we derive the ris algorithm. successive ris iteration approaches match the traditional optimal chosen importance identity function by applying localized optical arc changes to minimize preserving the divergence between knowing the evidence - adjusted importance hash function and choose the appropriate optimal importance function. the validity and performance of new ris is empirically simply tested with merging a large standard setof and synthetic parameter bayesian networks simulated and two comparable real - world networks.", "histories": [["v1", "Wed, 13 Jun 2012 15:53:49 GMT  (295kb)", "http://arxiv.org/abs/1206.3295v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["haohai yu", "robert a van engelen"], "accepted": false, "id": "1206.3295"}, "pdf": {"name": "1206.3295.pdf", "metadata": {"source": "CRF", "title": "Refractor Importance Sampling", "authors": ["Haohai Yu"], "emails": ["hyu@cs.fsu.edu", "engelen@cs.fsu.edu"], "sections": [{"heading": null, "text": "In this paper we introduce Refractor Importance Sampling (RIS), an improvement to reduce error variance in Bayesian network importance sampling propagation under evidential reasoning. We prove the existence of a collection of importance functions that are close to the optimal importance function under evidential reasoning. Based on this theoretic result we derive the RIS algorithm. RIS approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function. The validity and performance of RIS is empirically tested with a large set of synthetic Bayesian networks and two realworld networks."}, {"heading": "1 Introduction", "text": "The Bayesian Network (BN) [Pearl, 1988] formalism is one of the dominant representations for modeling uncertainty in intelligent systems [Neapolitan, 1990, Russell and Norvig, 1995]. A BN is a probabilistic graphical model of a joint probability distribution over a set of statistical variables. Bayesian inference on a BN answers probabilistic queries about the variables and their influence relationships. The posterior probability distribution is computed using belief updating methods [Pearl, 1988, Guo and Hsu, 2002]. Exact inference is NP-hard [Cooper, 1990]. Thus, exact methods only admit relatively small networks or simple network configurations in the worst case. Approximations are also NP-hard [Dagum and Luby, 1993]. However, approximate inference methods have anytime [Garvey and Lesser, 1994] and/or anywhere [Santos et al., 1995] properties that make these methods more attractive compared to exact methods.\nStochastic simulation algorithms, also called stochastic sampling or Monte Carlo (MC) algorithms, form one of the most prominent subclasses of approximate inference algorithms of which Logic Sampling [Henrion, 1988] was the first and simplest sampling algorithm. Likelihood weighting [Fung and Chang, 1989] was designed to overcome the poor performance of logic sampling under evidential reasoning with unlikely evidence. Markov Chain Monte Carlo (MCMC) forms another important group of stochastic sampling algorithms. Examples in this group are Gibbs sampling, Metropolis sampling and hybrid-MC sampling [Geman and Geman, 1984, Gilks et al., 1996, MacKay, 1998, Pearl, 1987, Chavez and Cooper, 1990]. Stratified sampling [Bouckaert, 1994], hypercube sampling [Cheng and Druzdzel, 2000c], and quasi-MC methods [Cheng and Druzdzel, 2000b] generate random samples from uniform distributions using various methods to improve sampling results. The importance sampling methods [Rubinstein, 1981] are widely used in Bayesian inference. Self Importance Sampling (SIS) [Shachter and Peot, 1990] and Adaptive Importance Sampling (AIS-BN) [Cheng and Druzdzel, 2000a] are among the most effective algorithms.\nIn this paper we prove that the importance functions of an evidence-updated BN can only approach the optimal importance function when the BN graph structure is modified according to the observed evidence. This implies the existence of a collection of importance functions with minimum divergence to the optimal importance function under evidential reasoning. Based on this result we derive our Refractor Importance Sampling (RIS) class of algorithms. In contrast to AIS-BN and SIS methods, RIS removes the lower bound that prevents the updated importance function to approach the optimal importance function. This is achieved by a graphical structure \u201crefractor\u201d, consisting of a localized network structure change that minimizes the divergence between the evidence-adjusted importance function and the optimal importance function.\nThe remainder of this paper is organized as follows. Section 2 proves the existence of a lower bound on the divergence to the optimal importance function under evidential reasoning with a BN. The lower bound is used to derive the class of RIS algorithms introduced in Section 3. Section 4 empirically verifies the properties of the RIS algorithms on a large set of synthetic networks and two real-world networks, and compares the results to other importance sampling algorithms. Finally, Section 5 summarizes our conclusions and describes our future work."}, {"heading": "2 Importance Function Divergence", "text": "In this section we first give BN definitions and briefly review importance sampling. We then give a KLdivergence lower bound for importance sampling error variance. We prove the existence of a collection of importance functions that approach the optimal importance function by adjusting both the quantitative and qualitative components of a BN under dynamic updating with evidence."}, {"heading": "2.1 Definitions", "text": "The following definitions and notations are used.\nDef. 1 A Bayesian network BN = (G,Pr) is a DAG G = (V,A) with vertices V and arcs A, A \u2286 V\u00d7V. Pr is the joint probability distribution over the discrete random variables (vertices) V defined by Pr(V) =\u220f V \u2208V Pr(V | \u03c0(V )). The set of parents of a vertex V is \u03c0(V ). The conditional probability tables (CPT) of the BN assign values to Pr(V | \u03c0(V )) for all V \u2208 V.\nThe graph G induces the d-separation criterion [Pearl, 1988], denoted by \u3008X,Y | Z\u3009, which implies that X and Y are conditionally independent in Pr given Z, with X,Y,Z \u2286 V.\nDef. 2 Let BN = (G,Pr) be a Bayesian network.\n\u2022 The combined parent set of X \u2286 V is defined by \u03c0(X) = \u22c3 X\u2208X \u03c0(X ) \\X.\n\u2022 Let An(\u00b7) denote the transitive closure of \u03c0(\u00b7), i.e. the ancestor set of a vertex. The combined ancestor set of X \u2286 V is defined by An(X) =\u22c3\nX\u2208X An(X) \\X.\n\u2022 Let \u03b4 : V \u2192 IN denote a topological order of the vertices such that Y \u2208 An(X )\u2192 \u03b4(Y ) < \u03b4(X ). The ahead set of a vertex X \u2208 V given \u03b4 is defined by Ah(X) = {Y \u2208 V | \u03b4(Y ) < \u03b4(X)}."}, {"heading": "2.2 Importance Sampling", "text": "Importance sampling is an MC method to improve the convergence speed and reduce the error variance with probability density functions. Let g(X) be a function of m variables X = {X1, . . . , Xm} over domain \u2126 \u2286 IRm, such that computing g(X) for any X is feasible. Consider the problem of approximating I =\u222b\n\u2126 g(X)dX using a sampling technique. Importance\nsampling approaches this problem by rewriting I =\u222b \u2126 g(X) f(X)f(X)dX, where f(X) is a probability density function over \u2126, often referred to as the importance function. In order to achieve minimum error variance equal to \u03c32f(X) = ( \u222b \u2126 |g(X)|dX)2 \u2212 I2, the importance\nfunction should be f(X) = |g(X)|( \u222b\n\u2126 |g(X)|dX)\u22121, see\n[Rubinstein, 1981]. Note that when g(X) > 0 the optimal probability density function is f(X) = g(X)I\u22121 and \u03c32f(X) = 0. It is obvious that in most of cases it is impossible to obtain the optimal importance function.\nThe SIS [Shachter and Peot, 1990] and AIS-BN [Cheng and Druzdzel, 2000a] sampling algorithms are effective methods for approximate Bayesian inference. These methods attempt to approach the optimal importance function through learning by dynamically adjusting the importance function during sampling with evidence. To this end, AIS-BN heuristically changes the CPT values of a BN, a technique that has been shown to significantly improve the convergence rate of the approximation to the exact solution.\nWe use the following definitions for sake of exposition.\nDef. 3 Let BN = (G,Pr) be a Bayesian network with G = (V,A) and evidence e for variables E \u2286 V. A posterior BN e of the BN is some (new) network defined as BN e = (Ge,Pre) with graph Ge over variables V\\E, such that BN e exactly models the posterior joint probability distribution Pre = Pr(\u00b7 | e).\nA typical example of a posterior BN e is a BN combined with an updated posterior state as defined by exact inference algorithms, e.g. using evidence absorption [van der Gaag, 1996]. Approximations of BN e are used by importance sampling algorithms. These approximations consist of the original BN with all evidence vertices ignored from further consideration.\nDef. 4 Let BN = (G,Pr) be a Bayesian network with G = (V,A) and evidence e for variables E \u2286 V. The evidence-simplified ESBN e of BN is defined by ESBN e = (G\u2032e,Pr \u2032 e), where G \u2032 e = (V \u2032 e, A \u2032 e), V \u2032 e =\nV \\E, and A\u2032e = {(X,Y ) | (X,Y ) \u2208 A \u2227 X,Y /\u2208 E}.\nThe joint probability distribution Pr\u2032e of an evidencesimplified BN approximates Pre. For example, SIS and AIS-BN adjust the CPTs of the original BN."}, {"heading": "2.3 KL-Divergence Bounds", "text": "We give a lower bound on the KL-divergence [Kullback, 1959] of the evidence-simplified Pr\u2032e from the exact Pre. The lower bound is valid for all variations of Pr\u2032e, including those generated by importance sampling algorithms that adjust the CPT.\nTheorem 1 Let ESBN e = (G\u2032e,Pr \u2032 e) be an evidencesimplified BN given evidence e for E \u2286 V. If Pr\u2032e(V | \u03c0e(V )) = Pr(V | \u03c0(V ), e)) for all V \u2208 V then the KLdivergence between Pre and Pr\u2032e is minimal and given by \u2211\nX\u2208X \u2211 Cfg(X,\u03c0(X)) Pr(x, \u03c0(x) | e) ln Pr(x | \u03c0(x)) +\n\u2211 X\u2208X \u2211 Cfg(X,\u03c0(X)) Pr(x, \u03c0(x) | e) ln 1 Pr\u2032e(x | \u03c0e(x)) +\n\u2211 Cfg(\u03c0(E)) Pr(\u03c0(e) | e) ln \u220f e\u2208e Pr(e | \u03c0(e))\u2212 ln Pr(e) (1)\nwhere X = V \\E.\nProof. See Appendix A. 2\nTheorem 1 bounds the error variance from below, which is empirically verified for SIS and AIS-BN in the results Section 4. The divergence Eq. (1) is zero when specific conditions are met as stated below.\nCorollary 1 Let ESBN e = (G\u2032e,Pr \u2032 e) be an evidencesimplified BN given evidence e for E \u2286 V. If \u03c0(E) \u2229 (V \\E) = \u2205, then Pr\u2032e = Pre.\nProof. See Appendix B. 2\nHence, the optimal importance function is obtained when all evidence vertices are clustered as roots in G.\nWe will now show how Pr\u2032e can approach the optimal Pre without restrictions. For sake of explanation, the following widely-held assumptions are reiterated:\nAssumption 1 The topological order \u03b4 of a BN and its posterior version \u03b4e of BN e are consistent. That is, \u03b4e(Y ) < \u03b4e(X)\u2192 \u03b4(Y ) < \u03b4(X) for all X,Y \u2208 V \\E.\nAssumption 1 is reasonable for the following facts:\n1. According to chain rule, a BN can be built up in any topological order and all of them describe the same joint probability distribution.\n2. Although there has never been a widely accepted definition of what causality is, it is widely accepted that the fact of observing evidence for random variables should not change the causality relationship between the variables.\nTheorem 2 Let BN e(Ge,Pre) be the posterior of a BN = (G,Pr) given evidence e for E \u2286 V. If X /\u2208 An(E) for all X \u2208 V \\ E, then Pre(X | Ahe(X)) = Pr(X | \u03c0(X)). The evidence vertices in \u03c0(X) take configurations fixed by e, that is Pr(X | \u03c0(X)) = Pr(X | \u03c0(X) \\E, e1, . . . , em) for all ei \u2208 \u03c0(X) \u2229E.\nProof. See [Cheng and Druzdzel, 2000a]. 2\nHence, to compute the posterior probability of a vertex that is not an ancestor of an evidence vertex, there is no need to change the parents of the vertex or its CPT. For vertices that are ancestors of evidence vertices, we use Bayes\u2019 formula and d-separation to explore the effects of evidence on those vertices. Without loss of generality, only one evidence vertex is considered. The result applies to an evidence vertex set by transitivity.\nLemma 1 Let BN e(Ge,Pre) be the posterior of a BN = (G,Pr) given evidence e = {e} for E \u2208 V. Let X \u2208 An(E). Then, Pre(X | Ahe(X)) = Pr(e|X,Ah(X)) Pr(e|Ah(X)) Pr(X | \u03c0(X)).\nProof. Because Ahe(X) = Ah(X) by Assumption 1, we have Pre(X | Ahe(X)) = Pre(X,Ah(X)) Pre(Ah(X)) = Pr(X,Ah(X)|e)Pr(Ah(X)|e) = Pr(X,Ah(X),e) Pr(Ah(X),e) = Pr(e|X,Ah(X)) Pr(X,Ah(X)) Pr(e|Ah(X)) Pr(Ah(X)) = Pr(e|X,Ah(X)) Pr(e|Ah(X)) Pr(X | \u03c0(X)) by using Theorem 2. 2\nTheorem 2 and Lemma 1 show that if we have Pr(e | X,Ah(X)) and Pr(e | Ah(X)) for all X \u2208 An(E) we can derive Pre(X | Ahe(X)) to compute Pre(V) =\u220f V \u2208An(E) Pr(V | \u03c0(V )) \u220f V 6\u2208An(E) Pre(V | Ahe(V )) for the optimal importance function. However, there are two problems to derive Pre. Firstly, Ah(X) is too large to construct a posterior BNe for Pre in practice. Secondly, instead of the exact Pre(X | Ahe(X)) we have an estimate by importance sampling.\nThe parent sets of X \u2208 An(E ) can be minimized by exploiting d-separation. Let \u03b1e(X) \u2286 X \u222a Ah(X) denote the minimal vertex set that d-separates evidence E and X \u222a Ah(X), thus \u3008E,X \u222a Ah(X) | \u03b1e(X)\u3009. We will refer to \u03b1e(X) as the \u201cshield\u201d of X given E. Let \u03b2e(X) \u2286 Ah(X) denote the minimal vertex set that d-separates evidence E and Ah(X), thus \u3008E,Ah(X) | \u03b2e(X)\u3009. We explore the relationship between \u03b1e(X) and \u03b2e(X) below.\nLemma 2 Let BN e(Ge,Pre) be the posterior of a BN = (G,Pr) given evidence e = {e} for E \u2208 V. Then, \u03b2e(X) \u2286 (\u03b1e(X)\\X)\u222a\u03c0(X) for all X \u2208 An(E).\nProof. See Appendix C. 2\nTherefore, we can approach the optimal importance function Pre(X | Ahe(X)) by estimation of Pre(X | (\u03b1e(X) \\X) \u222a \u03c0(X)) from importance samples.\nInput: Evidence E \u2208 V and X \u2208 An(E) Output: The set S = \u03b1e(X) Data: array A, queue Q A\u2190 topSort\u03b4(Ah(X)); S \u2190 {X}; for i\u2190 |A| to 1 do\nQ\u2190 \u2205; push(Q, A[i]); while Q 6= \u2205 do\nV \u2190 pop(Q); if V = E then S \u2190 S \u222a {A[i]}; break; if V 6\u2208 S \u2227 V \u2208 An(E) \u2227X 6\u2208 An(V ) then\npush(children(V )); end\nend end\nAlgorithm 1: Computing the Shield \u03b1e(X)"}, {"heading": "3 Refractor Importance Sampling", "text": "The RIS algorithm modifies the BN structure according to the shield \u03b1e(X) for vertices X \u2208 An(E) by expanding the parent set of X and adjusting its CPT accordingly. Visually in the graph, RIS refracts arcs from the evidence vertices, which inspired the choice of name for the method. The algorithms and general procedure of RIS are introduced in this section."}, {"heading": "3.1 Computing the Shield", "text": "Alg. 1 computes \u03b1e(X) in O(|A|) worst-case time, assuming An(\u00b7) is determined in unit time (e.g. using a lookup table). Function topSort\u03b4 topologically sorts the set Ah(X) by topological order \u03b4 over V of the BN. Note that the shield \u03b1e(X) can be computed in advance for each X \u2208 V given evidence nodes E."}, {"heading": "3.2 Refractor Procedure", "text": "Alg. 2 modifies the graphical structure of BN. The time complexity of this algorithm is O(|V||A|) if |E| |V|, otherwise it is O(|V|2|A|). The CPT of a vertex X is updated by populating the expanded entries \u03b1e(X) \\ {X} using sampling data (described in Section 3.4).\nFig. 1 shows an example refractored BN using Alg. 2. E is the evidence node. Here, \u03b1e(C) = {A} and \u03b1e(B) = {A}. Arcs A \u2192 B and A \u2192 C are added. Note that arc A \u2192 B adjusts for the fact that the influence relationship between A and B has changed through evidence E. Arc E \u2192 D is no longer required and can be removed as in [van der Gaag, 1996].\nInput: BN = (G,Pr), evidence e for E \u2286 V Output: refractored BN e foreach E \u2208 E do\nforeach X \u2208 An(E) do expand \u03c0e(X) = (\u03b1e(X) \\ {X}) \u222a \u03c0(X); update the CPT of X;\nend end\nAlgorithm 2: Refractor Procedure"}, {"heading": "3.3 General RIS Procedure", "text": "RIS utilizes both the qualitative and quantitative properties of a BN to approach the optimal importance function. The general procedure of RIS is:\n1. The structure of the BN is modified by Algorithms 1 and 2. The CPTs of (a subset of) ancestor vertices of evidence vertices are expanded.\n2. Update the CPT values through some specific learning algorithm (see Section 3.4 for details).\n3. Sample the BN with an importance sampling algorithm using the new importance function."}, {"heading": "3.4 Variations of RIS", "text": "Step 1 modifies the BN structure significantly, especially when the ancestor sets of evidence vertices are large, e.g. when evidence vertices are leafs. This increases the complexity of the BN. However, the effect of evidence on other vertices is attenuated when the path length between the evidence and the vertices is increased [Henrion, 1989]. Therefore, instead of modifying all ancestors An(E) of evidence E in Step 1, it is generally sufficient to select a subset of ancestors such as the combined parent set \u03c0(E).\nSteps 1 and 2 are independent, because any importance function learning algorithm can be applied in Step 2. Steps 2 and 3 can be combined by using the same importance sampling algorithm for learning and inference. In our experiments, we used SIS and AISBN for both learning and inference (steps 2 and 3), referred to as RISSIS and RISAIS, respectively. AISBN will be referred to by AIS."}, {"heading": "4 Results", "text": "This section presents the experimental results of RISSIS and RISAIS compared to SIS and AIS for synthetic networks and two real-world networks."}, {"heading": "4.1 Measurement", "text": "The MSE (mean squared error) metric was used to measure the error of the importance sampling results compared to the exact solution:\nMSE = \u221a\u221a\u221a\u221a 1\u2211 Xi\u2208X ni \u2211 Xi\u2208X ni\u2211 j=1 (Pr\u2032e(xij)\u2212 Pre(xij))2 ,\nwhere X = V \\E. We also measured the KLdivergence of the approximate and exact posterior probability distributions:\nKL-divergence = \u2211\nCfg(X)\nPre(x) ln Pre(x) Pr\u2032e(x)\n.\nRecall that Theorem 1 gives a lower bound for the KLdivergence of the posterior probability distributions of SIS and AIS, which is indicated in the results by the PostKLD lower bound from Eq. (1).\nThe number of samples is taken as a measure of running time instead of CPU time in our experimental implementation. Recall that the overhead of RIS is fixed at startup when the evidence set can be predetermined. Furthermore, the RIS overhead is limited to collecting the updated CPT values during sampling (and learning in the case of RISAIS).\nThe reported sampling frequencies for AIS (and RISAIS) are for calculating the posterior results. Because AIS separates the importance function learning stage from the sampling stage, the actual number of samples taken for AIS (total sampling for importance function and sampling the results) is twice that of SIS. Recommended parameters [Cheng and Druzdzel, 2000a] are used in AIS and RISAIS."}, {"heading": "4.2 Test Cases", "text": "Because computing the MSE is expensive and PostKLD is exponential in the number of vertices, small-sized synthetic BNs with random variables with two or three states and |V| = 20 vertices and |A| = 30 arcs were evaluated in our experiments. The CPT for each variable is randomly generated with uniform distribution for the probability interval [0.1, 0.9] with bias for the extreme probabilities in intervals (0, 0.1) and (0.9, 1). For the experiments we generated 100 different synthetic BNs with these characteristics.\nWe also verified RIS with two real-world BNs: Alarm-37 [Beinlich et al., 1989] and HeparII-70 [Onisko, 2003]. The probability distributions of these networks are more extreme compared to the synthetic BNs. For each of the two BNs, 20 sets of evidence variables are randomly chosen, each with 10 evidence variables. For the Alarm-37 and HeparII-70 we choose to limit the refractoring to the parents nodes of the evidence set \u03c0(E) instead of An(E), see Section 3.4."}, {"heading": "4.3 Results for Synthetic Test Cases", "text": "We compared the MSE of four algorithms, AIS, RISAIS, SIS, and RISSIS. For this comparison a selection of 21 BNs from the generated synthetic test case suite was made. The other 79 test cases have PostKLD \u2264 0.1, which means according to Theorem 1 that the RIS advantage is limited.\nFig. 2 shows the results for the 21 synthetic BNs, where the sample frequency is varied from 1,000 to 19,000 in increments of 1,000. The dark column in the figures represent the ratio of lowest MSE cases for RISAIS versus AIS and RISSIS versus SIS. A ratio of 50% or higher indicates that the RIS algorithm has lower error variance than the non-RIS algorithm. For RISAIS this is the case for all but one of the 19 measurements taken In total, the MSE is lowest for RISAIS in 61.4% on average over all samples. For RISSIS this is the case\nfor all but four of the 19 measurements taken. In total, the MSE is lowest for RISSIS in 58.4% on average over all samples.\nIn fact, it is to be expected that the higher the PostKLD lower bound the better the RIS algorithms should perform. In order to determine the impact with increasing PostKLD, we selected all 100 synthetic BN test cases and measured the KL-divergence after 11,000 samples.\nFig. 3 shows the result for RISAIS, where the 100 BNs are ranked according to the PostKLD. Recall that the PostKLD is the lower bound on the KL-divergence of AIS. From the figure it can be concluded that AIS does not approach the exact solution for a significant number of test cases, whereas RISAIS is not limited by the bound due to the BN refractoring.\nIt should be noted that around points 1 and 26 in Fig. 3 the KL-divergence of RISAIS is worse compared to AIS. We believe the reason is that AIS heuristically changes the original CPT which has a negative impact on the RIS algorithm\u2019s ability to adjust the CPT to the optimal importance function.\nFig. 4 shows the result for RISSIS, where the 100 BNs are ranked according to the PostKLD. Interestingly,\nthe RISSIS and SIS results are better on average than RISAIS and AIS. Note that the PostKLD lower bound is the same for AIS and SIS. However, in this study SIS appears to approach the PostKLD closer than AIS. Also here we can conclude that SIS does not approach the exact solution for a significant number of test cases, whereas RISSIS is not limited by the bound due to the BN refractoring."}, {"heading": "4.4 Results for Alarm-37 and HeparII-70", "text": "Fig. 5 shows the results for Alarm-37 and HeparII70, where the sample frequency is varied from 1,000 to 19,000 in increments of 1,000. The dark column in the figures represent the ratio of lowest MSE cases for RISAIS versus AIS and RISSIS versus SIS. A ratio of 50% or higher indicates that the RIS algorithm has lower error variance than the non-RIS algorithm. For RISAIS this is the case for all but one of the 19 measurements taken. In total, the MSE is lowest for RISAIS in 56.7% on average over all samples. For RISSIS this is the case for all 19 measurements taken. In total, the MSE is lowest for RISSIS in 60.3% on average over all samples.\nThe combined results show that the RIS algorithms have reduced error variance for the synthetic networks\nand the two real-world networks. The PostKLD lower bound limits the ability of AIS and SIS to approach the optimal importance function. By contrast, the RIS approach successfully eliminates this limitation of AIS and SIS, thereby providing an improvement to reduce error variance in BN importance sampling propagation under evidential reasoning."}, {"heading": "5 Conclusions", "text": "In order to approach the optimal importance function for importance sampling propagation under evidential reasoning with a Bayesian network, a modification of the network\u2019s structure is necessary to eliminate the lower bound on the error variance. To this end, the proposed RIS algorithms refractor the network and adjust the conditional probability tables to minimize the divergence to the optimal importance function. The validity and performance of the RIS approach was empirically tested with a set of synthetic networks and two real-world networks.\nAdditional improvements of RIS are possible to achieve a better accuracy/cost ratio. The goal is to find an effective subset of the full shield size of an ancestor vertex of an evidence vertex or select a limited subset of the ancestors of evidence vertices that are refractored. Also some of the additionally introduced arcs could be removed with the arc removal algorithm [van Engelen, 1997] when they present a weak influence. Such strategies would reduce the complexity of the refractored network while still ensuring higher accuracy over current importance sampling algorithms."}, {"heading": "A Proof of Theorem 1", "text": "Proof. We use the KL-divergence (Cross Entropy) [Kullback, 1959] to measure the difference between the posterior BN e and ESBN e. The KL-divergence = E1[ln\nPr1(V) Pr2(V)\n] = \u2211\nCfg(V) Pr1(v) ln Pr1(v) Pr2(v) . Hence, the KL-divergence between posterior BN e and ESBN e is\u2211\nCfg(X) Pre(x) ln Pre(x) Pr\u2032e(x) where X = V \\E. This is further simplified as follows\u2211\nCfg(X) Pre(x) ln Pre(x) Pr\u2032e(x) =\u2211 Cfg(X) Pr(x | e) ln Pr(x|e) Pr\u2032e(x)\n=\u2211 Cfg(X) Pr(x | e) ln \u220f xj\u2208x Pr(xj |\u03c0(xj)) \u220f ei\u2208e Pr(ei|\u03c0(ei)) Pr(e) \u220f\nxj\u2208x Pr\u2032e(xj |\u03c0e(xj))\n= \u2211 Cfg(X) Pr(x | e) ln \u220f xj\u2208x Pr(xj |\u03c0(xj)) \u220f ei\u2208e\nPr(ei|\u03c0(ei))\u220f xj\u2208x Pr\u2032e(xj |\u03c0e(xj))\n+ln 1Pr(e) \u2211\nCfg(X) Pr(x | e) =\u2211 Cfg(X) Pr(x | e) ln \u220f xj\u2208x Pr(xj |\u03c0(xj)) \u220f ei\u2208e Pr(ei|\u03c0(ei))\u220f\nxj\u2208x Pr\u2032e(xj |\u03c0e(xj)) \u2212ln Pr(e)= \u2211 Cfg(X) Pr(x | e) \u2211 xj\u2208x ln Pr(xj |\u03c0(xj)) Pr\u2032e(xj |\u03c0e(xj))\n+ \u2211 Cfg(X) Pr(x | e) ln \u220f ei\u2208e Pr(ei | \u03c0(ei)) \u2212ln Pr(e)\n= \u2211 Xj\u2208X \u2211 Cfg(Xj ,\u03c0(Xj))\nln Pr(xj |\u03c0(xj))Pr\u2032e(xj |\u03c0e(xj))\u2211 Cfg(X\\{Xj ,\u03c0(Xj)}) Pr(x | e) + \u2211 Cfg(\u03c0(E))\nPr(\u03c0(e) | e) ln \u220f ei\u2208e Pr(ei | \u03c0(ei)) \u2212ln Pr(e)=\u2211\nXj\u2208X \u2211 Cfg(Xj ,\u03c0(Xj)) Pr(xj , \u03c0(xj) | e) ln Pr(xj |\u03c0(xj))Pr\u2032e(xj |\u03c0e(xj))\n+ \u2211 Cfg(\u03c0(E)) Pr(\u03c0(e) | e) ln \u220f ei\u2208e Pr(ei | \u03c0(ei))\n\u2212ln Pr(e) = \u2211 Xj\u2208X \u2211 Cfg(Xj ,\u03c0(Xj))\nPr(xj , \u03c0(xj) | e) ln Pr(xj | \u03c0(xj)) + \u2211 Xj\u2208X \u2211 Cfg(Xj ,\u03c0(Xj)) Pr(xj , \u03c0(xj) | e) ln 1Pr\u2032e(xj |\u03c0e(xj)) + \u2211 Cfg(\u03c0(E)) Pr(\u03c0(e) | e)ln \u220f ei\u2208e Pr(ei | \u03c0(ei)) \u2212ln Pr(e) (Eq. 1)\nThe first, third, and fourth terms in Eq. 1 are decided by the original probability distribution, so\u2211 Xj\u2208X \u2211 Cfg(Xj ,\u03c0(Xj))\nPr(xj , \u03c0(xj) | e) ln Pr(xj | \u03c0(xj)) + \u2211 Cfg(\u03c0(E)) Pr(\u03c0(e) | e) ln \u220f ei\u2208e Pr(ei | \u03c0(ei)) \u2212ln Pr(e) is constant. To minimize the difference between posterior BN e and ESBN e the only choice is to minimize the following term:\u2211 Xj\u2208X \u2211 Cfg(Xj ,\u03c0(Xj))\nPr(xj , \u03c0(xj) | e) ln 1Pr\u2032e(xj |\u03c0e(xj)) = \u2211 Xj\u2208X \u2211 Cfg(\u03c0(Xj)) \u2211 Cfg(Xj)\nPr(xj , \u03c0(xj) | e) ln 1Pr\u2032e(xj |\u03c0e(xj))\nThis is equal to minimizing the term for each Xj \u2208 X and each possible configuration of \u03c0(xj). \u2211 Cfg(Xj)\nPr(xj , \u03c0(xj) | e) ln 1Pr\u2032e(xj |\u03c0e(xj))= Pr(\u03c0(xj) | e) \u2211 Cfg(Xj)\nPr(xj | \u03c0(xj), e)ln 1Pr\u2032e(xj |\u03c0(xj)\\e) . We have \u2211 Cfg(Xj) Pr\u2032e(xj | \u03c0e(xj))\n= \u2211\nCfg(Xj) Pr\u2032e(xj | \u03c0(xj) \\ e) = 1. According to\nShannon\u2019s information theory [Shannon, 1956], to minimize \u2211 Cfg(Xj)\nPr(xj | \u03c0(xj), e) ln 1Pr\u2032e(xj |\u03c0(xj)\\e) we should set Pr\u2032e(xj | \u03c0(xj) \\ e) = Pr(xj | \u03c0(xj), e). This proves the Theorem 1. 2"}, {"heading": "B Proof of Corollary 1", "text": "Proof. Let X = V \\E, then\u2211 Cfg(V\\E) Pre(v \\ e) ln Pre(v\\e) Pr\u2032e(v\\e)\n=\u2211 Cfg(X) Pr(x | e) ln Pr(x|e) Pr\u2032e(x)\n=\u2211 Cfg(X) Pr(x | e) ln \u220f xj\u2208x Pr(xj |\u03c0(xj)) \u220f ei\u2208e Pr(ei|\u03c0(ei)) Pr(e) \u220f\nxj\u2208x Pr\u2032e(xj |\u03c0e(xj))\n.\nSince \u03c0(E) \u2229 (V \\E) = \u2205 \u21d2 \u2200Xj \u2208 X,Pr(xj | \u03c0(xj), e) = Pr(xj | \u03c0(xj)), from Theorem 1, set Pr\u2032e(xj | \u03c0e(xj)) = Pr(xj | \u03c0(xj)) to minimize the divergence, then\u2211\nCfg(X) Pr(x | e) ln \u220f xj\u2208x Pr(xj |\u03c0(xj)) \u220f ei\u2208e Pr(ei|\u03c0(ei)) Pr(e) \u220f\nxj\u2208x Pr\u2032e(xj |\u03c0e(xj))\n=\n\u2211 Cfg(X) Pr(x | e) ln\n\u220f ei\u2208e\nPr(ei|\u03c0(ei)) Pr(e) . Also\nfrom \u03c0(E) \u2229 (V \\E) = \u2205, \u2200Ei \u2208 E, \u03c0(Ei) \u2286 E \u21d2 \u220f ei\u2208e Pr(ei | \u03c0(ei)) = Pr(e), so\u2211\nCfg(X) Pr(x | e) ln \u220f ei\u2208e Pr(ei|\u03c0(ei)) Pr(e) = 0. The KL-divergence between Pre and Pr\u2032e is zero, thus Pre(v \\ e) = Pr\u2032e(v \\ e) according to [Kullback, 1959]. 2"}, {"heading": "C Proof of Lemma 2", "text": "Proof. \u2200Xk \u2208 Ah(Xj)\\\u03b2e(Xj), consider the following three cases.\nCase 1: If a path Xk \u2192 E exists then we show that this path is d-separated by \u03b1e(Xj). There are two possibilities. First, Xk \u2192 E bypasses Xj , so it must pass one of the parents of Xj . Then \u03c0(Xj) d-separates the path. Second, Xk \u2192 E does not passXj . Then the path must be d-separated by \u03b1e(Xj)\\Xj , so (\u03b1e(Xj)\\ Xj) \u222a \u03c0(Xj) d-separates the path.\nCase 2: If paths N \u2192 Xk and N \u2192 E exist, so N \u2208 Ah(Xj), and N d-separate the Xk and E, according to Case 1, (\u03b1e(Xj)\\Xj)\u222a\u03c0(Xj) d-separates path N \u2192 E.\nCase 3: If paths Xk \u2192 B and E \u2192 B exist, according to topological order {B, descendants ofB}\u2229((\u03b1e(Xj)\\ Xj)\u222a\u03c0(Xj)) = \u2205, so (\u03b1e(Xj)\\Xj)\u222a\u03c0(Xj) d-separates this path.\nFrom cases 1 to 3 we see that \u3008E,Ah(Xj) | ((\u03b1e(Xj) \\ Xj) \u222a \u03c0(Xj))\u3009, so \u03b2e(Xj) \u2286 (\u03b1e(Xj) \\Xj) \u222a \u03c0(Xj). 2"}], "references": [{"title": "The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks", "author": ["Beinlich et al", "I. 1989] Beinlich", "G. Suermondt", "R. Chavez", "G. Cooper"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al. et al\\.", "year": 1989}, {"title": "A randomized approximation algorithm for probabilistic inference on Bayesian belief networks. Networks, 20:661\u2013685", "author": ["Chavez", "Cooper", "R.M. 1990] Chavez", "G.F. Cooper"], "venue": null, "citeRegEx": "Chavez et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Chavez et al\\.", "year": 1990}, {"title": "AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000a] Cheng", "M.J. Druzdzel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Computational investigations of low-discrepancy sequences in simulation algorithms for Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000b] Cheng", "M.J. Druzdzel"], "venue": "In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Latin hypercube sampling in Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000c] Cheng", "M.J. Druzdzel"], "venue": "In Proceedings of the 13th International Florida Artificial Intelligence Research Symposium Conference", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Approximating probabilistic inference in Bayesian belief networks is NP-hard", "author": ["Dagum", "Luby", "P. 1993] Dagum", "M. Luby"], "venue": "Artificial Intelligence,", "citeRegEx": "Dagum et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dagum et al\\.", "year": 1993}, {"title": "Weighting and integrating evidence for stochastic simulation in Bayesian networks", "author": ["Fung", "Chang", "R. 1989] Fung", "K.C. Chang"], "venue": "In Proceedings of the 5th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Fung et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Fung et al\\.", "year": 1989}, {"title": "A survey of research in deliberative real-time artificial intelligence. Real-Time Systems, 6(3):317\u2013347", "author": ["Garvey", "Lesser", "A.J. 1994] Garvey", "V.R. Lesser"], "venue": null, "citeRegEx": "Garvey et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Garvey et al\\.", "year": 1994}, {"title": "Stochastic relaxation, Gibbs distribution and the Bayesian restoration of images", "author": ["Geman", "S. 1984] Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1984}, {"title": "A survey on algorithms for real-time Bayesian network inference. In In the joint AAAI-02/KDD-02/UAI02 workshop on Real-Time Decision Support and Diagnosis", "author": ["Guo", "Hsu", "H. 2002] Guo", "W. Hsu"], "venue": null, "citeRegEx": "Guo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2002}, {"title": "Artificial intelligence: A modern approach", "author": ["Russell", "Norvig", "S. 1995] Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Russell et al\\.", "year": 1995}, {"title": "On a distributed anytime architecture for probabilistic reasoning", "author": ["Santos et al", "E.J. 1995] Santos", "S.E. Shimony", "E. Solomon", "E. Williams"], "venue": "Technical report,", "citeRegEx": "al. et al\\.,? \\Q1995\\E", "shortCiteRegEx": "al. et al\\.", "year": 1995}, {"title": "Simulation approaches to general probabilistic inference on belief networks", "author": ["Shachter", "Peot", "R.D. 1990] Shachter", "M.A. Peot"], "venue": "In Proceedings of the 5th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Shachter et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Shachter et al\\.", "year": 1990}, {"title": "On evidence absorption for belief networks", "author": ["van der Gaag", "L. 1996] van der Gaag"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Gaag and Gaag,? \\Q1996\\E", "shortCiteRegEx": "Gaag and Gaag", "year": 1996}, {"title": "Approximating Bayesian belief networks by arc removal", "author": ["van Engelen", "R. 1997] van Engelen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Engelen and Engelen,? \\Q1997\\E", "shortCiteRegEx": "Engelen and Engelen", "year": 1997}], "referenceMentions": [], "year": 2008, "abstractText": "In this paper we introduce Refractor Importance Sampling (RIS), an improvement to reduce error variance in Bayesian network importance sampling propagation under evidential reasoning. We prove the existence of a collection of importance functions that are close to the optimal importance function under evidential reasoning. Based on this theoretic result we derive the RIS algorithm. RIS approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function. The validity and performance of RIS is empirically tested with a large set of synthetic Bayesian networks and two realworld networks.", "creator": "TeX"}}}