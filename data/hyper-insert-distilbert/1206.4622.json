{"id": "1206.4622", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training", "abstract": "simple item neighbourhood methods applying for collaborative sentiment filtering learn a weighted graph over only the set of items, often where each item is coincidentally connected uniquely to those it is chosen most essentially similar to. hopefully the prediction of a user's cumulative rating on such an item is then slowly given further by that rating of possible neighbouring items, whilst weighted across by twice their similarity. this paper result presents a new neighbourhood approach which proposed we freely call complex item fields, whereby an inverse undirected graphical model is formed over into the item graph. generally the resulting prediction rule type is of a simple neighbourhood generalization of the classical approaches, which takes into account more non - local information gains in reviewing the target graph, hopefully allowing roughly its best results to be adequately obtained accurately when using drastically consistently fewer edges significantly than other neighbourhood approaches. a rather fast approximate maximum entropy level training method always based on the weak bethe wolf approximation is not presented, which uses a simple gradient exponential ascent procedure. realized when using precomputed sufficient partition statistics on the movielens datasets, now our method is successively faster efficient than local maximum likelihood approaches typically by two orders upstream of magnitude.", "histories": [["v1", "Mon, 18 Jun 2012 15:05:52 GMT  (397kb)", "http://arxiv.org/abs/1206.4622v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.IR stat.ML", "authors": ["aaron defazio", "tib\u00e9rio s caetano"], "accepted": true, "id": "1206.4622"}, "pdf": {"name": "1206.4622.pdf", "metadata": {"source": "META", "title": "A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training", "authors": ["Aaron J. Defazio", "Tib\u00e9rio S. Caetano"], "emails": ["aaron.defazio@anu.edu.au", "tiberio.caetano@nicta.com.au"], "sections": [{"heading": "1. Introduction", "text": "Recommendation systems have presented new and interesting challenges to the machine learning community. The large scale and variability of data has meant that traditional approaches have not been applicable, particularly those that have quadratic or cubic running time.\nThis has led to the majority of research taking two tracks: (1) latent-factor models, and (2) neighbourhood models. Latent factor models embed both users\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nand items into a low dimensional space, from which predictions can be computed using linear operations. These include continuous approaches, such as low-rank approximate matrix factorization (Funk, 2006) and binary variable approaches, such as restricted Boltzmann machines (Salakhutdinov et al., 2007).\nThe second track, neighbourhood models, is the one explored in this work. Neighbourhood methods form a graph structure over either items or users, where edges connect items/users that are deemed similar. Rating predictions are performed under the assumption that users rate similar items similarly (for an item graph) or that similar users have similar preferences (for a user graph) using some form of weighted average (Sarwar et al., 2001).\nIn this paper we propose a neighbourhood model that treats the item graph as a undirected probabilistic graphical model. This allows us to compute distributions over ratings instead of the point estimates provided by alternative neighbourhood methods. Predictions for a user are performed by simply conditioning the model on her previous ratings, giving a distribution over the set of items she has yet to rate. The predictive rule takes into account non-local information in the item graph, allowing for smaller neighbourhood sizes than are used in other approaches.\nWe also present an efficient learning algorithm for our model, based on the Bethe entropy approximation. It exploits a decomposition of the variable matrix into diagonal and off-diagonal parts, where gradient ascent need only be performed for the diagonal elements. As our method loops over a set of edges instead of the full set of training data, training is orders of magnitude faster than stochastic gradient descent (SGD) approaches such as Koren (2010). For example, the item graph for the data-set we consider (see Section 6) has approximately 40 thousand edges, which is small compared to the 1 million data-points that are considered in each SGD iteration."}, {"heading": "2. The Item Graph", "text": "We begin by introducing the foundations of our model. We are given a set of users and items, along with a set of real valued ratings of items by users. Classical item neighbourhood methods (Sarwar et al., 2001) learn a graph structure over items i = 0 . . . N \u2212 1, along with a set of edge weights sij \u2208 R, so that if a query user u is presented, along with his ratings for the neighbours of item i (ruj , j \u2208 ne(i)), the predicted rating of item i is\nrui = \u00b5i +\n\u2211 j\u2208ne(i) sji (ruj \u2212 \u00b5j)\u2211\nj\u2208ne(i) sji ,\nwhere \u00b5i \u2208 R represent average ratings for that item over all users. See Figure 1 for an example of an actual neighbourhood for a movie recommendation system.\nIn order to use the above method, some principle or learning algorithm is needed to choose the neighbour weights. The earliest methods use the Pearson correlation between the items as the weights. In our notation, the Pearson correlation between two items is defined as\nsij = \u2211 u(rui \u2212 \u00b5i)(ruj \u2212 \u00b5j)\u221a\u2211\nu(rui \u2212 \u00b5i)2 \u221a\u2211 u(ruj \u2212 \u00b5j)2 .\nThe set of neighbours of each item is chosen as the k most similar, under the same similarity measure used for prediction. More sophisticated methods were developed for the NetFlix competition, including the work of Bell & Koren (2007), which identified the following problems with the above:\n\u2022 Bounded similarity scores can not handle deter-\nministic relations;\n\u2022 Interactions among neighbours are not accounted for, which greatly skews the results;\n\u2022 The weights sij cause over-fitting when none of the neighbours provide useful information.\nLearning the weights sij under an appropriate model can alleviate all of these problems, and provide superior predictions (Bell & Koren, 2007), with the only disadvantage being the computational time required for training the model. Learning the neighbourhood structure for such a model is not straightforward due to the potentially quadratic number of edges. In this paper we take the approach used by other neighbourhood methods, and assume that the neighbourhood structure is chosen by connecting each item to it\u2019s k most similar neighbours, using Pearson correlation as the similarity measure. We denote this undirected edge set E. Structure learning is in principle possible in our model, using variants of recently proposed methods for covariance selection (Duchi et al., 2008). Unfortunately such methods become unusable when the number of items considered exceeds a few thousand.\nIn the next section, we show that the edge weights can be interpreted as parameters of a distribution defined by a particular graphical model. This interpretation leads to fundamentally different rating and training methods, which we explore in Sections 4 and 5 respectively."}, {"heading": "3. The Item Field Model", "text": "Undirected graphical models are a general class of probability distributions defined by a set of feature functions over subsets of variables, where the functions return a local measure of compatibility. In our case the set of variables is simply the set of items, whose values we treat as continuous variables in the range 1 to 5. For any particular user, given the set of their items ratings (RK), we will predict their ratings on the remaining items (RU ) by conditioning on this distribution, namely computing expectations over P (RU |RK).\nThe most common feature domains used are simple tuples of variables (i, j), which can be equated with edges in the graphical model, in our case the item graph. We will additionally restrict ourselves to the class of log-linear models, which allows us to write the\ngeneral form of the distribution as\nP (r; \u0398) = 1\nZ(\u0398) exp \u2212 \u2211 (i,j)\u2208E \u0398ijfij(ri, rj) , where \u0398 is the set of features weights, and Z is the partition function, whose value depends on the parameters \u0398, and whose purpose is to ensure the distribution is correctly normalized. The choice of feature function for log-linear models is problem dependent, however in our case two choices stand out. We want functions that encourage smoothness, so that a discrepancy between the ratings of similar items is penalized. We propose the use of squared difference features\nfij(ri, rj) = 1\n2 ((ri \u2212 \u00b5i)\u2212 (rj \u2212 \u00b5j))2.\nThese features, besides being intuitive, have the advantage that for any choice of parameters \u0398 we can form a Gaussian log-linear model that defines that same distribution.\nIn a Gaussian log-linear model, pairwise features are defined for each edge as fij(ri, rj) = (ri\u2212\u00b5i)(rj\u2212\u00b5j), and unary features as fi(ri) = 1 2 (ri \u2212 \u00b5i)\n2. The pairwise feature weights \u0398ij correspond precisely with the off diagonal entries of the precision matrix (that is, the inverse of the covariance matrix). The unary feature weights \u03b8i correspond then to the diagonal elements. Thus we can map the squared difference features to a constrained Gaussian, where the diagonal elements are constrained so that for all i,\n\u0398ii = \u2212 \u2211\nj\u2208ne(i)\n\u0398ji.\nWe will denote the sparse symmetric matrix of weights for both the Gaussian and squared difference forms as \u0398, with the diagonal constrained in this way. This allows us to freely convert between the two forms.\nWe will impose an additional constraint on the allowable parameters, that each off-diagonal element is nonpositive; this constraint ensures that only similarity (as a opposed to dissimilarly) is modelled, and prevents numerical issues in the optimization discussed in Section 5."}, {"heading": "4. Prediction rule", "text": "The feature functions defined in the previous section appear somewhat arbitrary at first. We will now show that they are additionally motivated by a simple link to existing neighbourhood methods. Consider the case of predicting a rating rui, where for user u all ratings\nruj , j \u2208 ne(i) are known. These neighbours form the Markov blanket of node i. In this case the conditional distribution under the item field model is:\nN rui ; \u00b5i|\u2212i, 1 \u03c32 = \u2211\nj\u2208ne(i)\n\u0398ji  , where \u00b5i|\u2212i = \u00b5i \u2212 \u2211 j\u2208ne(i) \u0398ji (ruj \u2212 \u00b5j)\u2211\nj\u2208ne(i) \u0398ji .\nThis is a univariate Gaussian distribution, whose mean is given by a weighted sum of the same form as for traditional neighbourhood methods. In practice we rarely have ratings information for each item\u2019s complete neighbourhood, so this special case is just for illustrating the link with existing approaches.\nIn the general case, conditioning on a set of items K with known ratings rK , with the remaining items denoted U , we have:\nN ( rU ; \u00b5U |K ,\u0398UU ) , where\n\u00b5U |K = \u00b5U \u2212 [\u0398UU ] \u22121 \u0398UK (rK \u2212 \u00b5K) .\nThus computing the expected ratings requires nothing more than a few fast sparse matrix operations, including one sparse solve. If the prediction variances are required, both the variances and the expected ratings can be computed using belief propagation, which often requires fewer iterations than the sparse solve operation (Shental et al., 2008).\nThe linear solve in this prediction rule has the effect of diffusing the known ratings information over the graph structure, in a transitive manner. Figure 2 shows this\neffect which is sometimes known as spreading activation. Such transitive diffusion has been explored previously for collaborative filtering, in a more ad-hoc fashion (Huang et al., 2004).\nNote that this prediction rule is a bulk method, in that for a particular user, it predicts their ratings for all unrated items at once. This is the most common case in real world systems, as all item ratings are required in order to form user interface elements such as top 100 recommendation lists."}, {"heading": "5. Proposed Training Algorithm: Approximate Maximum Entropy Learning", "text": "The traditional way to train an undirected graphical model is using maximum likelihood. When exact inference is used, this is equivalent to maximum entropy training (Koller & Friedman, 2009), as one is the concave dual of the other. However, in practice variational approximations such as the Bethe approximation are used; for example when inference is performed using Belief propagation. In which case the approximate maximum entropy problem is not usually concave, and the solutions of the two are no longer necessarily equivalent.\nIn discrete models, the (constrained) approximate maximum entropy approach has been shown to learn superior models in some cases, at the expense of training time (Granapathi et al., 2008). In the case of the item field model we will establish that approximate maximum entropy learning is significantly faster, with comparable accuracy.\nWe first consider the case of a Gaussian model, with the variance-style features described in Section 2, which our constrained Gaussian model builds upon. Let \u03a3 denote the empirical rating covariance matrix, which we cap to non-zero only at locations (i, j) \u2208 E. The full covariance matrix is dense, however we only need to access the entries at these locations, and it is notationally convenient to treat the rest as zero. The optimization procedure is over the parameters of the beliefs B = {bi, bij}. The approximate maximum entropy objective is subject to the constraints:\n\u2200(i, j) \u2208 E Ebij [xixj ] = \u03a3ij (1) \u2200(i, j) \u2208 V Ebi [x2i ] = \u03a3ii (2)\n\u2200(i, j) \u2208 E \u222b xi\nbij(xi, xj) = bj(xj) (3)\u222b xj bij(xi, xj) = bi(xi) (4)\n\u2200(i) \u2208 V \u222b xi bi(xi) = 1 (5)\n\u2200(i, j) \u2208 E \u222b xi \u222b xj bij(xi, xj) = 1. (6)\nConstraints 3 and 4 are the marginal consistency constraints. When applied to Gaussian beliefs they simply assert that the covariance entries of beliefs with overlapping domains should be equal on that overlap. Constraints 5 and 6 are the normalizability constraints. For Gaussian beliefs they just enforce that the covariance matrices are all positive definite.\nIn general graphical models, the beliefs are separately parametrized distributions over the same domains as the factors, in our case they take the form of 2D and 1D mean zero Gaussian distributions. We will make use of a representation of the beliefs in a compact form of a single (sparse) symmetric matrix of the same structure as the covariance matrix, which we will denote C. This representation simply maps Cij = Ebij [xixj ], evaluated at any of the beliefs. This is well defined as the value will be the same for any belief, as noted above. This representation makes the consistency constraints implicit, using what is essentially variable substitution.\nThe Bethe entropy approximation of the beliefs in our notation is:\nHBethe(C) = \u2211\n(i,j)\u2208E\nlog ( CiiCjj \u2212 C2ij ) + \u2211 i\u2208V (1\u2212 deg(i)) logCii.\nNotice that log terms are undefined whenever the belief covariances are not positive definite. Thus the normalizability constraints are also extraneous. So for a purely Gaussian model, the approximate maximum entropy problem simplifies to:\nmaximize C HBethe(C)\ns.t. C = \u03a3.\nStated this way, the solution is trivial as the constraints directly specify C. However, we are interested in learning the weights \u0398, which are the Lagrange multipliers of the equality constraints. The Lagrangian is\nL\u03a3(C,\u0398) = HBethe(C) + \u3008\u0398,\u03a3\u2212 C\u3009 , (7)\nwhere \u3008\u00b7, \u00b7\u3009 is the standard inner product on matrices. The Lagrangian has derivatives:\nAlgorithm 1 Diagonal ascent algorithm for approximate maximum entropy learning\ninput: covariance \u03a3, size N , step size \u03b1 C = \u03a3 and k = 1 repeat\n# Compute \u0398, needed for each Cii gradient for i = 1 to N do\n\u0398ii = 1 Cii\nend for for (i, j) \u2208 E do\nif CiiCjj \u2212 C2ij > 0 and Cij > 0 then \u0398ij =\n\u2212Cij CiiCjj\u2212C2ij\n\u0398ii += Cjj CiiCjj\u2212C2ij \u2212 1Cii\nelse \u0398ij = 0\nend if end for # Take a gradient step on each Cii for i = 1 to N do Cii +=\n\u03b1\u221a k (\u0398ii + \u2211 j\u2208ne(i) \u0398ij)\nend for # Update the off-diagonal elements for (i, j) \u2208 E do Cij = \u03a3ij \u2212 \u03a3ii \u2212 \u03a3jj + Cii + Cjj end for k = k + 1\nuntil sufficient convergence return \u0398\n\u2202L\u03a3(C,\u0398) \u2202C = \u2202HBethe(C) \u2202C \u2212\u0398\n\u2202L\u03a3(C,\u0398)\n\u2202\u0398 = \u03a3\u2212 C.\nEquating the gradients to zero, gives the following equation for \u0398:\n\u0398 = \u2202HBethe(C)\n\u2202C |C=\u03a3,\nwhich gives the closed form solution\n\u0398ij = \u2212\u03a3ij\n\u03a3ii\u03a3jj \u2212 \u03a32ij\n\u0398ii = 1\n\u03a3ii + \u2211 j\u2208ne(i)\n( \u03a3jj\n\u03a3ii\u03a3jj \u2212 \u03a32ij \u2212 1 \u03a3ii\n) .\n(8)\nThe same solution for \u0398 can also be obtained using the pseudo-moment matching technique, as \u03a3 is a fixed point of the GaBP update equations for this parametrization. If we were applying a vanilla Gaussian model, we could use this result directly. However, for the item field model we have constraints on the\ndiagonal. Using variable substitution on the diagonal, we get the following Lagrangian:\nHBethe(C)+ \u2211\n(i,j)\u2208E\n\u0398ij (\u03a3ij \u2212 \u03a3ii \u2212 \u03a3jj)\n\u2212 \u2211\n(i,j)\u2208E\n\u0398ij (Cij \u2212 Cii \u2212 Cjj)\nwhich we denote L\u2032\u03a3(C,\u0398). It has gradients:\n\u2202L\u2032\u03a3(C,\u0398)\n\u2202Cij = \u2202HBethe(C) \u2202Cij \u2212\u0398ij\n\u2202L\u2032\u03a3(C,\u0398)\n\u2202Cii = \u2202HBethe(C) \u2202Cii + \u2211 j\u2208ne(i) \u0398ij\n\u2202L\u2032\u03a3(C,\u0398)\n\u2202\u0398ij =\u03a3ij \u2212 \u03a3ii \u2212 \u03a3jj \u2212 Cij + Cii + Cjj .\nIn order to optimize the diagonally constrained objective, we can take advantage of the closed form solution for the simpler unconstrained Gaussian. The procedure we use is given in Algorithm 1. It gives a quality solution in a small number of iterations (see Figure 3). The core idea is that if we fix the diagonal of C, the rest of the elements are determined. The gradient of the diagonal elements can be computed directly from \u0398, so we recalculate it at each iteration, then take a gradient step. The dual variables are used essentially as notation for the entropy gradients, not in any deeper sense."}, {"heading": "5.1. Missing Data & Kernel Functions", "text": "The training methods proposed take as input a sparse subset of a covariance matrix \u03a3, which contains the\nsufficient statistics required for training. It should be emphasized that we do not assume that the covariance matrix is sparse, rather our training procedure only needs to query the entries at the subset of locations where the precision matrix is assumed to be non-zero.\nAs our samples are incomplete (we do not know all item ratings for all users), the true covariance matrix is unknown. For our purposes, we form a covariance matrix by assuming the unrated items are rated at their item mean. More sophisticated methods of imputation are possible; we explored an ExpectationMaximization (EM) approach, which did not result in a significant improvement in the predictions made. It did however give better prediction covariances.\nIn general a kernel matrix can be used in place of the covariance matrix, which would allow the introduction of item meta-data through the kernel function. We left this avenue for future work."}, {"heading": "5.2. Conditional Random Field Variants", "text": "Much recent work in Collaborative filtering has concerned the handling of additional user meta-data, such as age and gender information usually collected by online systems (Stern et al., 2009). These attributes are naturally discrete, and so integrating them as part of the MRF model results in mixed discrete/continuous model. Approximate inference in such as model is no longer a simple linear algebra problem, and convergence becomes an issue. User attributes are better handled in a conditional random field (CRF) model, where the conditional distributions involve the continuous item variables only.\nUnfortunately the optimization technique described above does not extend readily to CRF models. Approximate maximum entropy training using Difference-of-convex methods has been applied to CRF training successfully (Granapathi et al., 2008), although such methods are slower than maximum likelihood. We explored CRF extensions using maximum likelihood learning, and while they did give better ratings predictions, training was slow due to the large number of belief propagation calls. While practical if the computation is distributed, the training time was still several hundred times slower than any of the other methods we tested."}, {"heading": "5.3. Maximum Likelihood Learning with Belief Propagation", "text": "An objective proportional to the negative loglikelihood for a Gaussian distribution under the Bethe approximation can be derived from the approximate\nentropy Lagrangian (Equation 7) using duality theory. First note that the Lagrangian can be split as follows:\nL\u03a3(\u0398, C) =HBethe(C) + \u3008\u0398,\u03a3\u2212 C\u3009 = \u3008\u0398,\u03a3\u3009 \u2212 (\u3008\u0398, C\u3009 \u2212HBethe(C)) ;\nThe dual is then formed by maximizing in terms of C:\n\u2234 \u2212 log p(\u03a3; \u0398) \u221d max C L\u03a3(\u0398, C) = \u3008\u0398,\u03a3\u3009 \u2212min C (\u3008\u0398, C\u3009 \u2212HBethe(C)) ;\nThe term inside of the minimization on the right is the Bethe free energy (Yedidia et al., 2000). By equating with the non-approximate likelihood, it can be seen that the log partition function is being approximated as:\nlogZ(\u0398) = \u2212min C (\u3008\u0398, C\u3009 \u2212HBethe(C))\nThe value of logZ(\u0398) and a (locally) minimizing C can be found efficiently using belief propagation (Cseke & Heskes, 2011). For diagonally dominant \u0398 belief propagation can be shown to always converge (Weiss & Freeman, 2001). The diagonal constraints as well as the non-positivity constraints on the off diagonal elements of \u0398 ensure diagonal dominance in this case.\nMaximum likelihood objectives for undirected graphical models are typically optimized using quasi-newton methods, and that is the approach we took here. The diagonal constraints are easily handled by variable substitution, and the non-positivity constraints are simple box constraints. We used the L-BFGS-B algorithm (Zhu et al., 1997) \u2013 a quasi-newton method that supports such constraints. The log-partition function logZ(\u0398) is convex if we are able to exactly solve the inner minimization over C, which is not the case in general."}, {"heading": "6. Experiments", "text": "For our comparison we tested on 2 representative datasets. The 1M ratings MovieLens dataset (GroupLens Research) consists of 3952 items and 6040 users. As there is no standard test/training data split for this dataset, we took the approach from Stern et al. (2009), where all data for 90% of the users is used for training, and the remaining users have their ratings split into a 75% training set and 25% test set. The 100K ratings MovieLens dataset involves 1682 items and 943 users. This dataset is distributed with five test/train partitions for cross validation purposes which we made use of.\nAll reported errors use the mean absolute error (MAE) measure ( 1N \u2211N i |\u00b5i\u2212ri|). All 2 datasets consist of ratings on a discrete 1 to 5 star scale. Each method we\ntested produced real valued predictions, and so some scheme was needed to reduce the predictions to real values in the interval 1 to 5. For our tests the values were simply clamped. Methods that learn a user dependent mapping from the real numbers into this interval have been explored in the literature (Stern et al., 2009).\nTable 1 shows the results for the two Movielens datasets. Comparisons are against our own implementation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochastic gradient descent for all factors) and the neighbourhood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective. All implementations were in python (using cython compilation), so timings are not comparable to fast implementations. We also show results for various methods of training the item field model besides the maximum entropy approach, including exact maximum likelihood training.\nThe item field model outperforms the other neighbourhood methods when sparse (10 neighbour) models are used. Increasing the neighbourhood size past roughly 10 actually starts to degrade the performance of the item field model: at 50 neighbours using maximum entropy training on the 1M dataset the MAE is 0.6866 vs 0.6772 at 10 neighbours. We found this occurred with the other training methods also. This may be caused\nby an over-fitting effect as restricting the number of neighbours is a form of regularization.\nThe latent factor model and the least squares neighbourhood model both use stochastic gradient descent for training. They required looping over the full set of training data each iteration. The maximum entropy method only loops over a sparse item graph each iteration which is why it is roughly two thousand times faster to train. Note that the dataset still has to be processed once to extract the neighbourhood structure and covariance values, the timing of which is indicated in the precomputation column. This is essentially the same for all the neighbourhood methods we compared. In an on-line recommendation system the covariance values can be updated as new ratings stream in, so the precomputation time is amortized. Training time is more crucial as multiple runs from a cold-start with varying regularization are needed to get the best performance (due to local minima)."}, {"heading": "7. Related Work", "text": "There has been previous work that applies undirected graphical models in recommendation systems. Salakhutdinov et al. (2007) used a bipartite graphical model, with binary hidden variables forming one part. This is essentially a latent factor model, and due to the hidden variables, requires different and less efficient training methods than those we apply in the\npresent paper. They apply a fully connected bipartite graph, in contrast to the sparse, non-bipartite model we use. Multi-scale conditional random fields models have also been applied to the more general social recommendation task with some success (Xin et al., 2009). Directed graphical models are commonly used as a modelling tool, such as in Salakhutdinov & Mnih (2008). While undirected models can be used in a similar way, the graph structures we apply in this work are far less rigidly structured.\nSeveral papers propose methods of learning weights of a neighbourhood graph (Koren, 2010) (Bell & Koren, 2007), however our model is the first neighbourhood method we are aware of which gives distributions over its predictions. Our model uses non-local, transitive information in the item graph for prediction. Nonlocal neighbourhood methods have been explored using the concept of spreading activation, typically on the user graph (Griffith et al., 2006) or on a bipartite user-item graph (Lie & Wang, 2009)."}, {"heading": "8. Conclusion", "text": "We have presented an undirected graphical model for collaborative filtering that naturally generalizes the prediction rule of previous neighbourhood methods by providing distributions over predictions rather than point estimates. We detailed an efficient training algorithm based on the approximate maximum entropy principle, which after preprocessing takes less than a second to train, and is two orders of magnitude faster than a maximum likelihood approach. Our model has fewer parameters than other comparable models, which is an advantage for interpretability and training."}, {"heading": "Acknowledgements", "text": "NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program."}], "references": [{"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["Bell", "Robert", "Koren", "Yehuda"], "venue": "In ICDM,", "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "Nonlinear programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas", "year": 1995}, {"title": "Properties of bethe free energies and message passing in gaussian models", "author": ["Cseke", "Botond", "Heskes", "Tom"], "venue": null, "citeRegEx": "Cseke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cseke et al\\.", "year": 2011}, {"title": "Projected subgradient methods for learning sparse gaussians", "author": ["Duchi", "John", "Gould", "Stephen", "Koller", "Daphne"], "venue": "In UAI,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Constrained approximate maximum entropy learning of markov random fields", "author": ["Funk", "Simon"], "venue": "Try this at home,", "citeRegEx": "Funk and Simon.,? \\Q2006\\E", "shortCiteRegEx": "Funk and Simon.", "year": 2006}, {"title": "A constrained spreading activation approach to collaborative filtering", "author": ["Griffith", "Josephine", "ORiordan", "Colm", "Sorensen", "Humphrey"], "venue": "In KES,", "citeRegEx": "Griffith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Griffith et al\\.", "year": 2006}, {"title": "Applying associative retrieval techniques to alleviate the sparsity problem in collaborative filtering", "author": ["Huang", "Zan", "Chen", "Hsinchun", "Zeng", "Daniel"], "venue": "ACM TOIS,", "citeRegEx": "Huang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2004}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Factor in the neighbors: Scalable and accurate collaborative filtering", "author": ["Koren", "Yehuda"], "venue": "TKDD,", "citeRegEx": "Koren and Yehuda.,? \\Q2010\\E", "shortCiteRegEx": "Koren and Yehuda.", "year": 2010}, {"title": "Improved collaborative filtering via information transformation", "author": ["Lie", "Jian-Guo", "Wang", "Bing-Hong"], "venue": "International Journal of Modern Physics C,", "citeRegEx": "Lie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lie et al\\.", "year": 2009}, {"title": "Bayesian probabilistic matrix factorization using mcmc", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["Sarwar", "Badrul", "Karypis", "George", "Konstan", "Joseph", "Riedl", "John"], "venue": "In WWW10,", "citeRegEx": "Sarwar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sarwar et al\\.", "year": 2001}, {"title": "Gaussian belief propagation solver for systems of linear equations", "author": ["Shental", "Ori", "Bickson", "Danny", "Siegel", "Paul H", "Wolf", "Jack K", "Dolev"], "venue": null, "citeRegEx": "Shental et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shental et al\\.", "year": 2008}, {"title": "Matchbox: large scale online bayesian recommendations", "author": ["Stern", "David", "Herbrich", "Ralf", "Graepel", "Thore"], "venue": "In WWW,", "citeRegEx": "Stern et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2009}, {"title": "Correctness of belief propagation in gaussian graphical models of arbitrary topology", "author": ["Weiss", "Yair", "Freeman", "William T"], "venue": "Neural Computation,", "citeRegEx": "Weiss et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2001}, {"title": "A social recommendation framework based on multiscale continuous conditional random fields", "author": ["Xin", "King", "Irwin", "Deng", "Hongbo", "Lyu", "Michael R"], "venue": "In CIKM,", "citeRegEx": "Xin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xin et al\\.", "year": 2009}, {"title": "Bethe free energy, kikuchi approximations and belief propagation algorithms", "author": ["Yedidia", "Jonathan S", "Freeman", "William T", "Weiss", "Yair"], "venue": "Technical report, Mitsubishi electric research,", "citeRegEx": "Yedidia et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2000}, {"title": "L-bfgs-b: Algorithm 778: Fortran routines for large scale bound constrained optimization", "author": ["C. Zhu", "H.Byrd", "Richard", "Lu", "Peihuang", "Nocedal", "Jorge"], "venue": "ACM TOMS,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 11, "context": "These include continuous approaches, such as low-rank approximate matrix factorization (Funk, 2006) and binary variable approaches, such as restricted Boltzmann machines (Salakhutdinov et al., 2007).", "startOffset": 170, "endOffset": 198}, {"referenceID": 12, "context": "Rating predictions are performed under the assumption that users rate similar items similarly (for an item graph) or that similar users have similar preferences (for a user graph) using some form of weighted average (Sarwar et al., 2001).", "startOffset": 216, "endOffset": 237}, {"referenceID": 12, "context": "Classical item neighbourhood methods (Sarwar et al., 2001) learn a graph structure over items i = 0 .", "startOffset": 37, "endOffset": 58}, {"referenceID": 3, "context": "Structure learning is in principle possible in our model, using variants of recently proposed methods for covariance selection (Duchi et al., 2008).", "startOffset": 127, "endOffset": 147}, {"referenceID": 13, "context": "If the prediction variances are required, both the variances and the expected ratings can be computed using belief propagation, which often requires fewer iterations than the sparse solve operation (Shental et al., 2008).", "startOffset": 198, "endOffset": 220}, {"referenceID": 6, "context": "Such transitive diffusion has been explored previously for collaborative filtering, in a more ad-hoc fashion (Huang et al., 2004).", "startOffset": 109, "endOffset": 129}, {"referenceID": 14, "context": "Much recent work in Collaborative filtering has concerned the handling of additional user meta-data, such as age and gender information usually collected by online systems (Stern et al., 2009).", "startOffset": 172, "endOffset": 192}, {"referenceID": 17, "context": "The term inside of the minimization on the right is the Bethe free energy (Yedidia et al., 2000).", "startOffset": 74, "endOffset": 96}, {"referenceID": 18, "context": "We used the L-BFGS-B algorithm (Zhu et al., 1997) \u2013 a quasi-newton method that supports such constraints.", "startOffset": 31, "endOffset": 49}, {"referenceID": 14, "context": "As there is no standard test/training data split for this dataset, we took the approach from Stern et al. (2009), where all data for 90% of the users is used for training, and the remaining users have their ratings split into a 75% training set and 25% test set.", "startOffset": 93, "endOffset": 113}, {"referenceID": 14, "context": "Methods that learn a user dependent mapping from the real numbers into this interval have been explored in the literature (Stern et al., 2009).", "startOffset": 122, "endOffset": 142}, {"referenceID": 12, "context": "Comparisons are against our own implementation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochastic gradient descent for all factors) and the neighbourhood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective.", "startOffset": 90, "endOffset": 111}, {"referenceID": 12, "context": "Comparisons are against our own implementation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochastic gradient descent for all factors) and the neighbourhood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective.", "startOffset": 91, "endOffset": 167}, {"referenceID": 12, "context": "Comparisons are against our own implementation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochastic gradient descent for all factors) and the neighbourhood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective.", "startOffset": 91, "endOffset": 281}, {"referenceID": 10, "context": "Salakhutdinov et al. (2007) used a bipartite graphical model, with binary hidden variables forming one part.", "startOffset": 0, "endOffset": 28}, {"referenceID": 16, "context": "Multi-scale conditional random fields models have also been applied to the more general social recommendation task with some success (Xin et al., 2009).", "startOffset": 133, "endOffset": 151}, {"referenceID": 16, "context": "Multi-scale conditional random fields models have also been applied to the more general social recommendation task with some success (Xin et al., 2009). Directed graphical models are commonly used as a modelling tool, such as in Salakhutdinov & Mnih (2008). While undirected models can be used in a similar way, the graph structures we apply in this work are far less rigidly structured.", "startOffset": 134, "endOffset": 257}, {"referenceID": 5, "context": "Nonlocal neighbourhood methods have been explored using the concept of spreading activation, typically on the user graph (Griffith et al., 2006) or on a bipartite user-item graph (Lie & Wang, 2009).", "startOffset": 121, "endOffset": 144}], "year": 2012, "abstractText": "Item neighbourhood methods for collaborative filtering learn a weighted graph over the set of items, where each item is connected to those it is most similar to. The prediction of a user\u2019s rating on an item is then given by that rating of neighbouring items, weighted by their similarity. This paper presents a new neighbourhood approach which we call item fields, whereby an undirected graphical model is formed over the item graph. The resulting prediction rule is a simple generalization of the classical approaches, which takes into account non-local information in the graph, allowing its best results to be obtained when using drastically fewer edges than other neighbourhood approaches. A fast approximate maximum entropy training method based on the Bethe approximation is presented, which uses a simple gradient ascent procedure. When using precomputed sufficient statistics on the Movielens datasets, our method is faster than maximum likelihood approaches by two orders of magnitude.", "creator": "LaTeX with hyperref package"}}}