{"id": "1206.4623", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "On the Size of the Online Kernel Sparsification Dictionary", "abstract": "further we best analyze from the proper size of the rendering dictionary constructed from purely online kernel sparsification, including using specifically a novel formula that expresses the closest expected determinant of removing the kernel gram root matrix efficiently in conventional terms of the eigenvalues of the covariance operator. using this formula, we are able to specifically connect the cardinality coefficients of understanding the dictionary with the generalized eigen - dependence decay law of the raw covariance residual operator. in particular, we nevertheless show that under certain application technical technical conditions, though the actual size difference of the computational dictionary will initially always grow constantly sub - linearly in the overall number of data points, and, as a consequence, by the browser kernel linear regressor constructed easily from components the internal resulting software dictionary is consistent.", "histories": [["v1", "Mon, 18 Jun 2012 15:06:34 GMT  (382kb)", "http://arxiv.org/abs/1206.4623v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yi sun", "faustino j gomez", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1206.4623"}, "pdf": {"name": "1206.4623.pdf", "metadata": {"source": "META", "title": "On the Size of the Online Kernel Sparsification Dictionary", "authors": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "emails": ["yi@idsia.ch", "tino@idsia.ch", "juergen@idsia.ch"], "sections": [{"heading": "1. Introduction", "text": "Kernel least squares (KLS) is a simple non-parametric regression method widely used in machine learning (e.g., see Scho\u0308lkopf and Smola, 2002). Standard KLS requires storing and computing the (pseudo) inverse of the Gram matrix, and thus its complexity scales at least quadratically in the number of data points, rendering the method intractable for large data sets. In order to reduce the computational cost and avoid overfitting, it is common to replace the Gram matrix with a low rank approximation formed by projecting all samples1 onto the span of a chosen subset or dictionary of samples. Using such an approximated Gram matrix can greatly reduce the computational cost of KLS, sometimes to linear in the number of data points. Generally speaking, there are two approaches to constructing the dictionary. The first is the Nystro\u0308m method\n1With a little abuse of notation, we use samples to refer to elements in some reproducing kernel Hilbert space (RKHS), i.e., samples are the images of the feature map of the data points.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n(Williams and Seeger, 2000), where a randomly selected subset is used. The second, which is the concern of this paper, is called Online Kernel Sparsification (OKS; Engel et al. 2004), where the dictionary is built up incrementally by incorporating new samples that cannot be represented well (in the least squares sense) using the current dictionary.\nSince being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006). Despite this empirical success, however, the theoretical understanding of OKS is still lacking. Most of the theoretical analysis has been done by Engel et al. (2004), who showed that the constructed dictionary is guaranteed to represent major fraction of the leading eigenvectors of the Gram matrix (Theorem 3.3, Engel et al. 2004). It was also proven that the dictionary stays finite if the set of possible samples is compact, and thus admits a finite covering number (Theorem 3.1, Engel et al. 2004). Yet, an important question remains open:\nHow does the size of the dictionary scale with the number of samples if the set of possible samples does not admit a finite covering number, or if the covering number is too large compared to the size of the data set?\nAnswering this question allows us to: (1) estimate the computational complexity of OKS, and therefore the associated KLS method, more accurately, and (2) characterize the generalization capability of the KLS regression function obtained, as the usual risk bounds are controlled by the quotient between the size of the dictionary and the number of samples (e.g., see Gyo\u0308rfi et al., 2004).\nIn this paper, we address this question theoretically. Our analysis proceeds in two steps:\n1. We provide a novel formula expressing the expected Gram determinant over a set of i.i.d. samples in terms of the eigenvalues of the covariance operator. We then prove that the expected Gram\ndeterminant diminishes with the cardinality of the set faster than any exponential function.\n2. We observe that the Gram determinant over the OKS dictionary is lower bounded by some exponential function in the size of the dictionary. However, since step 1 concludes that the chance of a finding a big Gram matrix with large determinant is exceedingly small, the size of the dictionary must also stay small with high probability. Specifically, we show that the size of the dictionary will always grow sub-linearly in the number of data points, which implies consistency of KLS regressors constructed from the dictionary.\nThe rest of the paper is organized as follows: Section 2 describes the first step of our analysis, establishing a number of theoretical properties concerning the Gram determinant, including its expectation, decay, and moments. In section 3, we proceed to step 2, and analyze the growth of the size of the dictionary in OKS using the results from section 2. Section 4 briefly discusses the results and directions for future research."}, {"heading": "2. The Determinant of a Gram matrix", "text": "Let H be a separable Hilbert space endowed with inner product \u3008\u00b7, \u00b7\u3009, and P be a distribution over H. Assume E\u03c6\u223cP \u2016\u03c6\u20162 < \u221e, and let C = E\u03c6\u223cP [\u03c6\u2297 \u03c6] be the (non-centered) covariance operator, where \u2297 denotes the tensor product. Let \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 be the eigenvalues of C sorted in descending order, then \u2211 \u03bbi <\u221e (Theorem 2.1, Blanchard et al. 2007).\nGiven i.i.d. samples \u03c61, . . . , \u03c6k \u223c P , let Gk (\u03c61:k) be2 the k\u00d7k Gram matrix with (i, j)-th entry \u3008\u03c6i, \u03c6j\u3009, and let detGk be the determinant of Gk. Clearly, detGk is a random variable from Hk to R. Moreover, detGk has bounded expectation since from Hadamard\u2019s inequality\n0 \u2264 E [detGk] \u2264 E [\u220fk\ni=1 \u2016\u03c6i\u20162\n] = ( E \u2016\u03c6\u20162 )k .\nLet \u03bb\u03031 \u2265 \u03bb\u03032 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb\u0303k denote the eigenvalues of k\u22121Gk (and thus those of the empirical covariance op-\nerator C\u0303k = k\u22121 \u2211k i=1 \u03c6i \u2297 \u03c6i) sorted in descending order. We assume the following condition. Assumption 1 limk\u2192\u221e \u2211\u221e i=1\n\u2223\u2223\u2223\u03bb\u0303i \u2212 \u03bbi\u2223\u2223\u2223 = 0, a.s., where we take \u03bb\u0303i = 0 for i > k.\nThe validity of this assumption will be discussed later in Section 4.1.\n2We use \u03c61:k as a short hand for \u03c61, . . . , \u03c6k."}, {"heading": "2.1. A Formula for the Expectation of the Gram Determinant", "text": "Before presenting our first main result (Theorem 1), we introduce some additional notation. The elementary symmetric polynomial3 of order k over n variables is defined as\n\u03bdn,k (\u03bb1:n) = k! \u2211\n1\u2264i1<i2<\u00b7\u00b7\u00b7<ik\u2264n\n\u03bbi1\u03bbi2 \u00b7 \u00b7 \u00b7\u03bbik ,\nwhere the summation runs over all k-subsets of {1, . . . , n}. We denote the infinite extension of \u03bdn,k as\n\u03bdk (\u03bb1, \u03bb2, . . . ) = k! \u2211\n1\u2264i1<i2<\u00b7\u00b7\u00b7<ik\n\u03bbi1\u03bbi2 \u00b7 \u00b7 \u00b7\u03bbik ,\nwhenever the infinite sum exists. For simplicity, \u03bdk and \u03bdn,k denote both the function and their respective values with default argument (\u03bb1, \u03bb2, . . . ), and we only write down the arguments when they differ from (\u03bb1, \u03bb2, . . . ). Some of the useful properties of \u03bdn,k and \u03bdk are summarized in the following Lemma.\nLemma 1 We have\na) \u03bdn,k \u2265 \u03bdn\u22121,k \u2265 0, and limn\u2192\u221e \u03bdn,k = \u03bdk.\nb) \u03bdn,k = k\u03bbn\u03bdn\u22121,k\u22121 + \u03bdn\u22121,k,\nc) \u03bd2k \u2265 \u03bdk\u22121\u03bdk+1 (Newton\u2019s inequality),\nd) \u03bd 1 k k \u2265 \u03bd 1 k+1 k+1 (Maclaurin\u2019s inequality).\nProof. We only prove the limit in a) exists. The other properties can be derived easily using the limit argument and the properties of elementary symmetric polynomials (e.g., see Niculescu, 2000). In particular, c) is a direct consequence of Newton\u2019s inequality, and d) is a rephrase of Maclaurin\u2019s inequality.\nNote that \u03bdn,k is a non-decreasing sequence of n. Moreover,\n\u03bdn,k = k! \u2211\n1\u2264i1<i2<\u00b7\u00b7\u00b7<ik\u2264n\n\u03bbi1 \u00b7 \u00b7 \u00b7\u03bbik < k! ( n\u2211 i=1 \u03bbi )k is bounded because \u2211 \u03bbi < \u221e. Therefore the limit exists.\nNote that property b) enables us to compute \u03bdn,k in O (nk) time using dynamic programming. More precisely, this is done by initializing i) \u03bd1,1 = \u03bb1, ii) \u03bdi,1 = \u03bdi\u22121,1 + \u03bbi for i = 1, . . . , n, and iii) \u03bdi,i = i\u03bbi\u03bdi\u22121,i\u22121 for i = 1, . . . , k, and then applying the recursion in b).\nThe following theorem gives an explicit representation of the expectation of detGk in terms of the eigenvalues of C.\n3Note that the standard definition does not have the k! term.\nTheorem 1 E [detGk (\u03c61:k)] = \u03bdk\nThat is, the expectation of the determinant of a Gram matrix built from k samples is equal to the k\u2212th order elementary symmetric polynomial over the eigenvalues of the covariance operator.\nProof. 4 Let \u03c61, . . . , \u03c6n \u223c P , and Gn = Gn (\u03c61:n) be the corresponding Gram matrix. Denote \u03bb\u03031, . . . , \u03bb\u0303n the eigenvalues of n\u22121Gn, so that n\u03bb\u0303i are the eigenvalues of Gn. The characteristic polynomial of Gn is given by f (\u03bb) = det (Gn \u2212 \u03bbI). By definition,\nf (\u2212\u03bb) = n\u220f i=1 (\u03bb+ n\u03bbi)\n= n\u2211 k=0 nk  \u2211 1\u2264i1<\u00b7\u00b7\u00b7<ik\u2264n \u03bbi1 \u00b7 \u00b7 \u00b7\u03bbik  \u00b7 \u03bbn\u2212k =\nn\u2211 k=0 nk \u03bdn,k\n( \u03bb\u03031:n ) k! \u00b7 \u03bbn\u2212k.\nAlternatively, we can express f (\u2212\u03bb) using the determinants of the principal submatrices (see for example Meyer, 2001, pp.494), which are Gram matrices by themselves:\nf (\u2212\u03bb) = n\u2211 k=0 \u2211 I\u2208[n]k detGk (\u03c6I) \u00b7 \u03bbn\u2212k,\nwhere [n]k is the family of k-subsets in {1, . . . , n}, and \u03c6I denotes {\u03c6i}i\u2208I . Divide the coefficients before \u03bbn\u2212k by binomial coefficient ( n k ) to get the identity:\n( n\nk )\u22121 \u2211 I\u2208[n]k detGk (\u03c6I) = (n\u2212 k)!nk n! \u03bdn,k ( \u03bb\u03031:n ) .\nThe l.h.s. is a U-statistic (Serfling, 1980) with kernel detGk. Since E [detGk] <\u221e, the law of large numbers for U-statistics (Hoeffding, 1961) asserts that\nE [detGk] = lim n\u2192\u221e\n( n\nk )\u22121 \u2211 I\u2208[n]k detGk (\u03c6I) , a.s.\nNow consider the r.h.s. For the first term\nlim n\u2192\u221e\n(n\u2212 k)!nk\nn! = lim n\u2192\u221e\nn n\u2212 1 \u00b7 \u00b7 \u00b7 n n\u2212 k + 1 = 1.\n4An alternative proof may be derived using the generator function of E [detGk] (Martin, 2007). Unfortunately, the result is only briefly alluded to in the slides, and no detailed documentation has been made available up to now.\nFor the second term, we have\n\u03bdn,k ( \u03bb\u03031:n ) \u2212 \u03bdk\n= n\u2211 i=1 ( \u03bdn,k ( \u03bb1:i\u22121, \u03bb\u0303i:n ) \u2212 \u03bdn,k ( \u03bb1:i, \u03bb\u0303i+1:n )) + (\u03bdn,k (\u03bb1, . . . , \u03bbn)\u2212 \u03bdk) .\nNote that\u2223\u2223\u2223\u03bdn,k (\u03bb1:i\u22121, \u03bb\u0303i:n)\u2212 \u03bdn,k (\u03bb1:i, \u03bb\u0303i+1:n)\u2223\u2223\u2223 = \u2223\u2223\u2223k (\u03bb\u0303i \u2212 \u03bbi) \u03bdn\u22121,k\u22121 (\u03bb1:i\u22121, \u03bb\u0303i+1:n)\u2223\u2223\u2223\n\u2264 k \u2223\u2223\u2223\u03bb\u0303i \u2212 \u03bbi\u2223\u2223\u2223 \u03bdn,k\u22121 (\u03bb1:i\u22121, \u03bbi, \u03bb\u0303i+1:n)\n\u2264 k \u2223\u2223\u2223\u03bb\u0303i \u2212 \u03bbi\u2223\u2223\u2223 \u03bd\u2217n,k\u22121,\nwhere\n\u03bd\u2217n,k\u22121 = \u03bdn,k\u22121\n( max { \u03bb\u03031, \u03bb1 } , . . . ,max { \u03bb\u0303n, \u03bbn }) is bounded as \u2211 max { \u03bb\u0303i, \u03bbi\n} <\u221e. Therefore\u2223\u2223\u2223\u03bdn,k (\u03bb\u03031:n)\u2212 \u03bdk\u2223\u2223\u2223 \u2264 k\u03bd\u2217n,k\u22121 n\u2211 i=1\n\u2223\u2223\u2223\u03bb\u0303i \u2212 \u03bbi\u2223\u2223\u2223+ |\u03bdn,k (\u03bb1, . . . , \u03bbn)\u2212 \u03bdk| \u2192 0, a.s.\nThe first summand vanishes because of Assumption 1, and the second one diminishes because of Lemma 1 a). As a result,\nlim n\u2192\u221e\n(n\u2212 k)!nk\nn! \u03bdn,k\n( \u03bb\u03031:n ) = \u03bdk.\n2.2. The Decaying Speed of E [detGk]\nIt is not immediately obvious how \u03bdk = E [detGk] behaves with increasing k. Here we provide a direct link between the speed with which \u03bdk approaches zero and the tail behavior of {\u03bbi}. The analysis is based on the following lemma.\nLemma 2 Let \u03bb(0) = \u2211 \u03bbj, and \u03bb (k) = \u2211 j>k \u03bbj. Then\nlog \u03bdk+s \u2212 log \u03bdk \u2264 s log \u03bb(k) + log ( k + s\nk\n) .\nProof. Note that\n\u03bdk+s = (k + s)! k!s! k! \u2211\n1\u2264i1<\u00b7\u00b7\u00b7<ik\n\u03bbi1 \u00b7 \u00b7 \u00b7\u03bbik \u00b7 s! \u2211\nik<j1<\u00b7\u00b7\u00b7js\n\u03bbj1 \u00b7 \u00b7 \u00b7\u03bbjs\n=\n( k + s\nk\n) k! \u2211\n1\u2264i1<\u00b7\u00b7\u00b7<ik\n\u03bbi1 \u00b7 \u00b7 \u00b7\u03bbik \u00b7 \u03bds (\u03bbik+1, \u03bbik+2, . . . )\nSince \u03bbi is decreasing and ik \u2265 k, we have for all ik\n\u03bds (\u03bbik+1, \u03bbik+2, . . . ) \u2264 \u03bds (\u03bbk+1, \u03bbk+2, . . . ) \u2264 (\u2211\nj>k \u03bbj\n)s ,\nwhere the last inequality is from Lemma 1 d). Therefore, \u03bdk+s \u2264 ( k + s\nk\n)( \u03bb(k) )s \u00b7 \u03bdk.\nTaking the logarithm gives the desired result.\nAn immediate consequence is that \u03bdk converges to 0 faster than any exponential function.\nCorollary 1 For any \u03b1 > 0, limk\u2192\u221e \u03b1 \u2212k\u03bdk = 0.\nProof. Assume k is fixed and s is large. From Stirling\u2019s formula\nlog\n( k + s\nk\n) = k log ( 1 + s\nk\n) + s log ( 1 + k\ns ) +O (log s) < s+O (log s) ,\nwhere we use log (1 + x) < x for all x > \u22121.\nBy Lemma 2,\nlog \u03bdk+s \u2212 (k + s) log\u03b1 \u2264 s [ 1\u2212 log\u03b1+ log \u03bb(k) ] +O (log s) .\nSince \u2211 \u03bbi < \u221e, we can pick a k\u2217 such that\nlog (\u2211\nj>k\u2217 \u03bbj\n) < \u22122 + log\u03b1, then\nlim k\u2192\u221e log \u03bdk \u2212 k log\u03b1 = lim s\u2192\u221e (log \u03bdk\u2217+s \u2212 (k\u2217 + s) log\u03b1)\n< lim s\u2192\u221e\n(\u2212s+O (log s)) = \u2212\u221e,\nand thus limk\u2192\u221e \u03b1 \u2212k\u03bdk = 0.\nRemark 1 We can also bound \u03bdk in terms of \u03bb (k)\nusing Lemma 2. For exponential decay, i.e., \u03bbi \u223c O ( \u03c3\u2212i ) , we take s = 1, then\nlog \u03bdk < \u2212 k2\n2 log \u03c3 + log k! +O (k) .\nThe bound is tight since for \u03bbi = \u03c3 \u2212i, direct computation gives\n\u03bdk = k! \u221e\u2211 i1=1 \u03bbi1 \u00b7 \u00b7 \u00b7 \u221e\u2211 ik=ik\u22121+1 \u03bbik = k!\u03c3\u2212k\u220fk i=1 (\u03c3 i \u2212 1) .\nTaking the logarithm and applying some algebra we get\nlog \u03bdk = \u2212 k2\n2 log \u03c3 + log k! +O (k) .\nFor polynomial decay, i.e., \u03bbi \u223c O ( i\u2212(1+p) ) ,\u2211\ni\u2265k i \u2212(1+p) \u223c k\n\u2212p p , we set s = k, then\nlog \u03bd2k \u2212 log \u03bdk \u2264 k log \u03bb(k) + log ( 2k\nk\n) .\nUsing Stirling\u2019s formula,\nlog (2k)!\u2212 2 log k! = k log 4 +O (log k) .\nTherefore,\nlog \u03bd2k \u03bdk \u2264 \u2212pk log k + k log 4 p +O (log k) ,\nwhich characterizes the convergence of \u03bdk."}, {"heading": "2.3. Bounding the Moments of the Gram Determinant", "text": "In this section we prove a simple result concerning the moment E [(detGk)m], with the additional assumption thatH is the reproducing kernel Hilbert space (RKHS) associated with some bounded Mercer kernel ` (x, x\u2032). Note that for any m \u2265 1, `(m) (x, x\u2032) = (` (x, x\u2032))m is still a bounded Mercer kernel. Let H(m) be the RKHS associated with `(m) and denote \u03bb\n(m) 1 \u2265 \u03bb (m) 2 \u2265 \u00b7 \u00b7 \u00b7 the\neigenvalues of the corresponding covariance operator in H(m). We have the following bound. Theorem 2 E [(detGk)m] \u2264 \u03bdk ( \u03bb (m) 1 , \u03bb (m) 2 , . . . ) for m = 2, 3, . . . .\nProof. Let A \u25e6B be the Hadamard product of A and B. We use the well-known fact: If A, B are positive semi-definite, then\ndet (A \u25e6B) \u2265 det (A) det (B) .\nRepeating the process in the proof of Theorem 1, and applying\ndetG (m) k \u2265 (detGk) m\ngives the result.\nRemark 2 Theorem 2 allows us to estimate empirically the bound of E [(detGk)m] without enumerating all subsets of size k. Moreover, for RBF and polynomial kernels, `(m) stays RBF and polynomial, respectively. However, it remains unknown how \u03bb (m) i behaves in the general case.\nOn the Size of the Online Kernel Sparsification Dictionary"}, {"heading": "3. Analyzing Online Kernel Sparsification", "text": "In OKS, the dictionary D is initially empty. When a new sample \u03c6 arrives5, it is added to the dictionary if\ndetGD\u222a{\u03c6}\ndetGD > \u03b1,\nwhere GD and GD\u222a{\u03c6} are the respective Gram matrices of D and D \u222a {\u03c6}, and \u03b1 > 0 is a user-defined constant controlling the approximation error. Note that our notation is equivalent to the form originally proposed by Engel et al. (2004) as\ndetGD\u222a{\u03c6}\ndetGD = \u3008\u03c6, \u03c6\u3009 \u2212 g>G\u2212Dg = min \u03c8\u2208spanD \u2016\u03c6\u2212 \u03c8\u20162\nwhere g = [ \u3008\u03c6, \u03c61\u3009 , . . . , \u2329 \u03c6, \u03c6|D| \u232a]> for D ={\n\u03c61, . . . , \u03c6|D| } , and G\u2212D is the inverse of GD. The new \u03c6 can be added in O(|D|2) time if G\u2212D is updated incrementally, for a total computational complexity O(|D|2 n) for n samples.\nOur analysis is based on the key observation that\ndetGD > \u03b1 |D|.\nSince we have shown in the previous section that \u03b1\u2212kE [detGk]\u2192 0, the chance of finding a subset with the property that detGD > \u03b1\n|D| will diminish as |D| grows, making a large dictionary unlikely.\nMore specifically, let \u03c61, . . . , \u03c6n be n i.i.d. samples from P , and let Dn be the dictionary constructed from \u03c61:n. Denote [n]k to be the family of all k-subsets of {1, . . . , n}. For A \u2208 [n]k, let\n\u03c1k (\u03c6A) = I [ detGk (\u03c6A) > \u03b1 k ] ,\nwhere I [\u00b7] is the indicator function. Define\nk\u2217n = argmax k  \u2211 A\u2208[n]k \u03c1k (\u03c6A) > 0  . Then clearly |Dn| < k\u2217n, and we may study k\u2217n instead of |D|. Intuitively, k\u2217n characterizes the dimensionality of the linear space spanned by \u03c61:n, because for any subset larger than k\u2217n there will be some \u03c6 which can be represented within error \u03b1 by the linear combination of \u03c61:n.\nTo characterize k\u2217n we study P [k\u2217n \u2265 k]. The following lemma shows that this probability is equal to the probability of the existence of k-subsets A with \u03c1k(A) = 1.\n5In practice \u03c6 are often features in some RKHS induced by a kernel, and we store samples in the original domain. However we assume D is made of features for conceptual simplicity.\nLemma 3 P [k\u2217n \u2265 k] = P [\u2211 A\u2208[n]k \u03c1k (\u03c6A) > 0 ] .\nProof. By definition\nP  \u2211 A\u2208[n]k \u03c1k (\u03c6A) > 0  \u2264 P  \u22c3 k\u2032\u2265k  \u2211 A\u2208[n]k\u2032 \u03c1k\u2032 (\u03c6A) > 0  \n= P [k\u2217n \u2265 k] .\nTherefore the equality is not trivial.\nFrom Theorem 5 in Cover and Thomas (1988),( detGk+1 (\u03c61:k+1)\n\u03b1k+1\n) 1 k+1\n\u2264 1 k + 1 \u2211 A\u2208[k+1]k ( detGk (\u03c6A) \u03b1k ) 1 k .\nTherefore, \u2211 A\u2208[n]k\n\u03c1k (\u03c6A) = 0 implies\u2211 A\u2208[n]k\u2032 \u03c1k\u2032 (\u03c6A) = 0 for all k \u2032 \u2265 k, and thus\nP  \u2211 A\u2208[n]k \u03c1k (\u03c6A) = 0  \u2264 P  \u22c2 k\u2032\u2265k  \u2211 A\u2208[n]k\u2032 \u03c1k\u2032 (\u03c6A) = 0   .\nTaking the complement on both sides,\nP  \u2211 A\u2208[n]k \u03c1k (\u03c6A) > 0  \u2265 P  \u22c3 k\u2032\u2265k  \u2211 A\u2208[n]k\u2032 \u03c1k\u2032 (\u03c6A) > 0  \n= P [k\u2217n \u2265 k] .\nWe may now proceed to bound k\u2217n, using basic tools from probability theory.\nTheorem 3 P [|Dn| \u2265 k] \u2264 P [k\u2217n \u2265 k] < \u03b1\u2212k ( n k ) \u03bdk.\nProof. Note that\nE  \u2211 A\u2282[n]k \u03c1k (\u03c6A)  = (n k ) E [\u03c1k] = ( n k ) P [ detGk > \u03b1 k ] .\nFrom Markov\u2019s inequality,\nP [ detGk > \u03b1 k ] <\nE [detGk] \u03b1k .\nIt then follows\nP [k\u2217n \u2265 k] = P  \u2211 A\u2208[n]k \u03c1k (\u03c6A) \u2265 1  \u2264 E\n \u2211 A\u2282[n]k \u03c1k (\u03c6A)  < (n k ) E [detGk] \u03b1k .\nHere we use the fact that \u03c1k is {0, 1}-valued, and apply Markov\u2019s inequality again.\nNote that the proof only uses Markov\u2019s inequality, which usually provides bounds that are by no means tight. The possibility of strengthening the bound is discussed in the next section. However, even with this simple analysis, some interesting results for the size of D can be obtained. The first is the following corollary.\nCorollary 2 For any \u03b5 \u2208 (0, 1],\nlim n\u2192\u221e P [ k\u2217n n \u2265 \u03b5 ] = 0.\nProof. For simplicity assume \u03b5n is an integer. Let k = n\u03b5, then (\nn\nk ) \u03bdk \u03b1k = ( \u03b5\u22121k k ) \u03bdk \u03b1k .\nUsing Stirling\u2019s formula,\nlog\n( \u03b5\u22121k\nk\n) = log ( \u03b5\u22121k ) !\u2212 log k!\u2212 log (( \u03b5\u22121 \u2212 1 ) k ) !\n= \u03b3k +O (log k) ,\nwhere\n\u03b3 = 1\n\u03b5 log\n1 \u03b5 \u2212 ( 1 \u03b5 \u2212 1 ) log ( 1 \u03b5 \u2212 1 ) .\nTherefore, following Corollary 1 and Theorem 3,\nlim n\u2192\u221e P [ k\u2217n n \u2265 \u03b5 ] < lim n\u2192\u221e,k=\u03b5n ( n k ) E [detGk] \u03b1k\n= lim k\u2192\u221e\n( \u03b5\u22121k\nk ) \u03bdk \u03b1k = 0.\nRemark 3 By definition, Corollary 2 indicates that n\u22121k\u2217n \u2192 0 in probability, or the size of the dictionary grows only sub-linearly with the number of samples. Assuming finite variance of the response variable, it immediately follows that the ordinary linear regressor constructed using features obtained from OKS is consistent, as the generalization error is controlled by n\u22121 |D| (e.g., see Gyo\u0308rfi et al., 2004).\nThe next corollary provides a bound given a finite number of samples.\nCorollary 3 For arbitrary \u03b4 > 0 and\nn < \u03b1k\ne\n( \u03b4\n\u03bdk\n) 1 k\n,\nwe have P [|Dn| > k] < \u03b4.\nRemark 4 It is possible to give a bound in k rather than n. However, such a bound requires the inversion of \u03bdk and complicates the notation.\nProof. Assume n = \u03b5\u22121k. Rewrite Theorem 3 as P [|D\u03b5\u22121k| \u2265 k] < \u03b1\u2212k ( \u03b5\u22121k\nk\n) \u03bdk.\nUsing the simple relation ( \u03b5\u22121k k ) < ( \u03b5\u22121e )k , we have\nlog\u03b1\u2212k ( \u03b5\u22121k\nk\n) \u03bdk < k (1\u2212 log\u03b1)\u2212 k log \u03b5+ log \u03bdk.\nLetting the r.h.s. equal log \u03b4, it follows that\nlog e\n\u03b1 +\n1 k log \u03bdk \u03b4 = log \u03b5, and \u03b5 = e \u03b1 (\u03bdk \u03b4 ) 1 k .\nUsing Corollary 3, an upper bound on the dictionary size can be derived using {\u03bbi}, and the impact of \u03b1 on the dictionary size can be analyzed.\nFrom the previous discussion, if \u03bbi \u2264 \u03c3\u2212i, then\n\u03bdk \u2264 k! (\u03c3) \u2212k\u220fk i=1 (\u03c3 i \u2212 1) =\nk! (\u03c3) \u2212k\n\u03c3 k(k+1) 2 \u220fk i=1 (1\u2212 \u03c3\u2212i) ,\nand some elementary manipulation gives\nk\u2211 i=1 log ( 1\u2212 \u03c3\u2212i ) > \u2212 \u03c3 (\u03c3 \u2212 1)2 .\nTherefore,\n1 k log \u03bdk > \u2212 k 2 log \u03c3 + log k \u2212 3 2 \u2212 log \u03c3 \u2212 1 k\n\u03c3\n(\u03c3 \u2212 1)2 .\nPlugging this into Corollary 3, n < \u03b1\u03b2 \u03b4 1 k \u03c3 k 2 , where \u03b2 is some constant depending on \u03c3, which implies k \u223c O (log (n)). Similarly, for polynomial decay n\u2212(1+p), we have for large k\n1 k log \u03bd2k \u03bdk < \u2212p log k + log 4 p ,\nand then n > \u03b1\u03b4 1 k k1+p. Therefore, the dictionary size grows approximately at the rate of n 1\n1+p . Note that the order of magnitude of these bounds coincides with the number of eigenvalues above certain threshold (Bach and Jordan, 2002, Table 3)."}, {"heading": "4. Discussion", "text": "This paper presented a rigorous theoretical analysis of how the dictionary in online kernel sparsification\nscales with respect to the number of samples, based on properties of the Gram matrix determinant. This work should lead to a better understanding of OKS, both in terms of its computational complexity, and the generalization capabilities associated with kernel least squares regressors. Three additional points are discussed below concerning a) the validity of Assumption 1, b) how our results relate to the Nystro\u0308m method, and c) how the analysis can be potentially developed further."}, {"heading": "4.1. On Assumption 1", "text": "Under the mild condition E\u03c6\u223cP \u2016\u03c6\u20162 < \u221e, it can be seen that \u3008\u00b7, \u00b7\u3009 is a Mercer kernel in the sense of Definition 2.15 in Braun (2005), and subsequently by Theorem 3.26 therein, it follows that the \u03b42 distance\n(Koltchinskii and Gine\u0301 2000, pp. 116) between { \u03bb\u0303i } and {\u03bbi} vanishes almost surely.\nHowever, the convergence of the spectrum in \u03b42 metric is insufficient for Theorem 1 to hold, and the stronger L1 convergence of the eigenspectrum is needed. It is possible to drop Assumption 1 altogether and base the discussion on limn\u2192\u221e \u03bdn,k instead, where the limit always exists and equals to E [detGk]. Otherwise, following the analysis by Gretton et al. (2009), we may provide sufficient conditions6 to Assumption 1 using the following extension of the Hoffman\u2013Wielandt inequality (Theorem 3, Bhatia and Elsner 1994)\u2211\ni \u2223\u2223\u2223\u03bb\u0303i \u2212 \u03bbi\u2223\u2223\u2223 \u2264 \u2225\u2225\u2225C\u0303k \u2212 C\u2225\u2225\u2225 1 ,\nwhere \u2016\u00b7\u20161 denotes the trace norm. Using Proposition 12 in Harchaoui et al. (2008), the convergence of\u2225\u2225\u2225C\u0303k \u2212 C\u2225\u2225\u2225\n1 to zero can be established provided that i)\nH is a separable RKHS (e.g., an RKHS induced by a continuous kernel over a separable metric space; Steinwart et al. 2006) induced by some bounded kernel, and\nii) the eigenspectrum of C satisfies \u2211 i \u03bb 1 2 i <\u221e."}, {"heading": "4.2. Comparison with Nystro\u0308m Method", "text": "A similar approach to OKS for reducing the computational cost of kernel methods is the Nystro\u0308m method (Williams and Seeger, 2000), where the dictionary consists of a subset of samples chosen at random. One distinction of the two methods, following from the analysis before, is that the dictionary from OKS satisfies detGD > \u03b1\n|D|, while the randomly selected subset D\u0303, satisfies detGD\u0303 \u03b1| D\u0303| for larger D. Therefore,\ndetGn detGD\ndetGn detGD\u0303 .\n6We thank the anonymous reviewers for pointing this out.\nFrom an information theoretic point of view, log detGndetGD\u0303 can be interpreted as the conditional entropy (Cover and Thomas, 1988), which indicates that D\u0303 captures less information about the data sets.\nThe theoretical study of the Nystro\u0308m method by Drineas and Mahoney (2005) suggests that O ( \u03b1\u22124k ) samples are needed to approximate the first k eigenvectors well, which is linear in k, irrespective of the sample size. A recent study (Jin et al., 2012) shows that assuming bounded kernel, the spectral norm of the approximation error between the true and the approximated Gram matrix scales at a rate\nof O ( n |D|\u2212 1 2 ) , and in the case of \u03bbi \u223c i\u2212p, an\nO ( n |D|1\u2212p ) rate may be obtained. In contrast, the\nresults in this paper are over the dictionary size |D|, and the approximation error is controlled by \u03b1. In particular, assuming bounded kernel, the (i, j)-th entry of the difference between the true and approximated Gram matrix using OKS is bounded by\n|\u3008\u03c6i, \u03c6j\u3009 \u2212 \u3008\u03a0D\u03c6i,\u03a0D\u03c6j\u3009| < 2 sup \u2016\u03c6\u2016 \u221a \u03b1,\nwhere \u03a0D denotes the projection operator into the space spanned by D and the inequality follows from the Cauchy-Schwartz inequality. Using the fact that \u2016A\u20162 \u2264 \u221a \u2016A\u20161 \u2016A\u2016\u221e for arbitrary matrix A, where \u2016\u00b7\u20162, \u2016\u00b7\u20161 and \u2016\u00b7\u2016\u221e respectively denote the spectral norm, maximum absolute column sum norm and maximum absolute row sum norm, we conclude that the spectral norm of the approximation error is controlled by O (n \u221a \u03b1), which is a non-probabilistic bound and does not explicitly depend on the dictionary size."}, {"heading": "4.3. On Strengthening the Bound", "text": "The proof of Theorem 2 uses Markov\u2019s inequality to bound both P [ detGk > \u03b1 k ] , and the probability of\u2211\nA\u2208[n]k \u03c1k (A) 6= 0. In practice, this bound is hardly satisfying. One possibility is to strengthen the bound by incorporating information from higher order moments (Philips and Nelson, 1995), i.e.,\nP [ detGk > \u03b1 k ] \u2264 inf m\u2208{1,2,...} E [detGmk ] \u03b1km\n\u2264 inf m\u2208{1,2,...}\n\u03bd ( \u03bb (m) 1 , \u03bb (m) 2 , . . . ) \u03b1km .\nHowever, analyzing \u03bb (m) i is difficult in general, and remains an open research question.\nIt is also possible to improve the second step, using concentration inequalities for configuration functions (Boucheron et al., 1999). Let \u03c81, . . . , \u03c8k be a subsequence of \u03c61:n. We say \u03c81:k is \u03b1-compatible, if for\nj = 1, . . . , k,\ndetG{\u03c81,...,\u03c8j}\ndetG{\u03c81,...,\u03c8j\u22121} > \u03b1.\nNote that the dictionary constructed by OKS is \u03b1compatible, and the property of \u03b1-compatibility is hereditary, i.e., \u03c81:k being \u03b1-compatible implies that all sub-sequences are also \u03b1-compatible. To see this, let \u03c8i1 , . . . , \u03c8is be a sub-sequence of \u03c81:k, then\ndetG{\u03c8i1 ,...,\u03c8is} detG{\u03c8i1 ,...,\u03c8is\u22121} = min \u03c8\u2208span{\u03c8i1 ,...,\u03c8is\u22121} \u2016\u03c8is \u2212 \u03c8\u2016 2\n\u2265 min \u03c8\u2208span{\u03c81,...,\u03c8is\u22121} \u2016\u03c8is \u2212 \u03c8\u2016 2\n= detG{\u03c81,...,\u03c8is}\ndetG{\u03c81,...,\u03c8is\u22121} > \u03b1.\nAs a result, let Zn denote the length of the longest subsequence in \u03c61:n that is \u03b1-compatible, then |Dn| < Zn. By Theorem 2 in Boucheron et al. (1999), Zn concentrates sharply around E [Zn]. Therefore, it is unlikely that |Dn| exceeds E [Zn] by much. However, providing tight bounds for E [Zn] is difficult and requires further study."}], "references": [{"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "JMLR, 3:1\u201348,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "The Hoffman-Wielandt inequality in infinite dimensions", "author": ["R. Bhatia", "L. Elsner"], "venue": "Proc. Indian Acad. Sci. (Math. Sci),", "citeRegEx": "Bhatia and Elsner.,? \\Q1994\\E", "shortCiteRegEx": "Bhatia and Elsner.", "year": 1994}, {"title": "Statistical properties of kernel principal component analysis", "author": ["G. Blanchard", "O. Bousquet", "L. Zwald"], "venue": "Machine Learning,", "citeRegEx": "Blanchard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2007}, {"title": "A sharp concentration inequality with applications", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "Technical Report 376,", "citeRegEx": "Boucheron et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 1999}, {"title": "Spectral properties of the kernel matrix and their relation to kernel methods in machine learning", "author": ["M.L. Braun"], "venue": "PhD thesis,", "citeRegEx": "Braun.,? \\Q2005\\E", "shortCiteRegEx": "Braun.", "year": 2005}, {"title": "Determinant inequalities via information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Cover and Thomas.,? \\Q1988\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 1988}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": null, "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Incremental sparsification for real-time online model learning", "author": ["N. Duy", "J. Peters"], "venue": "In AISTAT\u201910,", "citeRegEx": "Duy and Peters.,? \\Q2010\\E", "shortCiteRegEx": "Duy and Peters.", "year": 2010}, {"title": "Algorithms and representations for reinforcement learning", "author": ["Y. Engel"], "venue": "PhD thesis, Hebrew University,", "citeRegEx": "Engel.,? \\Q2005\\E", "shortCiteRegEx": "Engel.", "year": 2005}, {"title": "The kernel recursive least-squares algorithm", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Engel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2004}, {"title": "A fast, consistent kernel twosample test", "author": ["A. Gretton", "K. Fukumizu", "Z. Harchaoui", "B.K. Sriperumbudur"], "venue": "In NIPS\u201909,", "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "A distribution-free theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": null, "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2004}, {"title": "Testing for homogeneity with kernel Fisher discriminant analysis", "author": ["Z. Harchaoui", "F.R. Bach", "\u00c9. Moulines"], "venue": "In NIPS\u201908,", "citeRegEx": "Harchaoui et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Harchaoui et al\\.", "year": 2008}, {"title": "The strong law of large numbers for U-statistics", "author": ["W. Hoeffding"], "venue": "Technical Report 302, Department of statistics,", "citeRegEx": "Hoeffding.,? \\Q1961\\E", "shortCiteRegEx": "Hoeffding.", "year": 1961}, {"title": "Improved bound for the Nystr\u00f6m method and its application to kernel classification", "author": ["R. Jin", "T.-B. Yang", "M. Mahdavi", "Y.-F. Li", "Z.H. Zhou"], "venue": "Technical Report arXiv:1111.2262v3,", "citeRegEx": "Jin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2012}, {"title": "Random matrix approximation of spectra of integral operators", "author": ["V. Koltchinskii", "E. Gin\u00e9"], "venue": null, "citeRegEx": "Koltchinskii and Gin\u00e9.,? \\Q2000\\E", "shortCiteRegEx": "Koltchinskii and Gin\u00e9.", "year": 2000}, {"title": "The expected determinant of the random Gram matrix and its application to information retrieval", "author": ["J. Martin"], "venue": "URL http://dydan. rutgers.edu/Seminars/Slides/martin2.pdf", "citeRegEx": "Martin.,? \\Q2007\\E", "shortCiteRegEx": "Martin.", "year": 2007}, {"title": "Matrix analysis and applied linear algebra", "author": ["C.D. Meyer"], "venue": "SIAM: Society for Industrial and Applied Mathematics,", "citeRegEx": "Meyer.,? \\Q2001\\E", "shortCiteRegEx": "Meyer.", "year": 2001}, {"title": "A new look at Newton\u2019s inequalities", "author": ["C.P. Niculescu"], "venue": "Journal of Inequalities in Pure and Applied Mathematics,", "citeRegEx": "Niculescu.,? \\Q2000\\E", "shortCiteRegEx": "Niculescu.", "year": 2000}, {"title": "The moment bound is tighter than Chernoff\u2019s bound for positive tail probabilities", "author": ["T.K. Philips", "R. Nelson"], "venue": "The American Statistician,", "citeRegEx": "Philips and Nelson.,? \\Q1995\\E", "shortCiteRegEx": "Philips and Nelson.", "year": 1995}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Approximation theorems of mathematical statistics", "author": ["R.J. Serfling"], "venue": null, "citeRegEx": "Serfling.,? \\Q1980\\E", "shortCiteRegEx": "Serfling.", "year": 1980}, {"title": "Online kernel-based classification using adaptive projection algorithms", "author": ["K. Slavakis", "S. Theodoridis", "I. Yamada"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Slavakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Slavakis et al\\.", "year": 2008}, {"title": "An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels", "author": ["I. Steinwart", "D. Hush", "C. Scovel"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2006}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In NIPS\u201900,", "citeRegEx": "Williams and Seeger.,? \\Q2000\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2000}, {"title": "A sparse kernel-based least-squares temporal difference algorithm for reinforcement learning", "author": ["X. Xu"], "venue": "In Advances in Natural Computation,", "citeRegEx": "Xu.,? \\Q2006\\E", "shortCiteRegEx": "Xu.", "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": "(Williams and Seeger, 2000), where a randomly selected subset is used.", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "The second, which is the concern of this paper, is called Online Kernel Sparsification (OKS; Engel et al. 2004), where the dictionary is built up incrementally by incorporating new samples that cannot be represented well (in the least squares sense) using the current dictionary.", "startOffset": 87, "endOffset": 111}, {"referenceID": 7, "context": "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 22, "context": "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006).", "startOffset": 111, "endOffset": 134}, {"referenceID": 8, "context": ", 2008) and reinforcement learning (Engel, 2005; Xu, 2006).", "startOffset": 35, "endOffset": 58}, {"referenceID": 25, "context": ", 2008) and reinforcement learning (Engel, 2005; Xu, 2006).", "startOffset": 35, "endOffset": 58}, {"referenceID": 7, "context": "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006). Despite this empirical success, however, the theoretical understanding of OKS is still lacking. Most of the theoretical analysis has been done by Engel et al. (2004), who showed that the constructed dictionary is guaranteed to represent major fraction of the leading eigenvectors of the Gram matrix (Theorem 3.", "startOffset": 73, "endOffset": 353}, {"referenceID": 21, "context": "is a U-statistic (Serfling, 1980) with kernel detGk.", "startOffset": 17, "endOffset": 33}, {"referenceID": 13, "context": "Since E [detGk] <\u221e, the law of large numbers for U-statistics (Hoeffding, 1961) asserts that", "startOffset": 62, "endOffset": 79}, {"referenceID": 16, "context": "An alternative proof may be derived using the generator function of E [detGk] (Martin, 2007).", "startOffset": 78, "endOffset": 92}, {"referenceID": 8, "context": "Note that our notation is equivalent to the form originally proposed by Engel et al. (2004) as", "startOffset": 72, "endOffset": 92}, {"referenceID": 5, "context": "From Theorem 5 in Cover and Thomas (1988), ( detGk+1 (\u03c61:k+1) \u03b1k+1 ) 1 k+1 \u2264 1 k + 1 \u2211", "startOffset": 18, "endOffset": 42}, {"referenceID": 4, "context": "15 in Braun (2005), and subsequently by Theorem 3.", "startOffset": 6, "endOffset": 19}, {"referenceID": 9, "context": "Otherwise, following the analysis by Gretton et al. (2009), we may provide sufficient conditions to Assumption 1 using the following extension of the Hoffman\u2013Wielandt inequality (Theorem 3, Bhatia and Elsner 1994) \u2211", "startOffset": 37, "endOffset": 59}, {"referenceID": 23, "context": "(2008), the convergence of \u2225\u2225\u2225C\u0303k \u2212 C\u2225\u2225\u2225 1 to zero can be established provided that i) H is a separable RKHS (e.g., an RKHS induced by a continuous kernel over a separable metric space; Steinwart et al. 2006) induced by some bounded kernel, and ii) the eigenspectrum of C satisfies \u2211 i \u03bb 1 2 i <\u221e.", "startOffset": 109, "endOffset": 208}, {"referenceID": 12, "context": "Using Proposition 12 in Harchaoui et al. (2008), the convergence of \u2225\u2225\u2225C\u0303k \u2212 C\u2225\u2225\u2225 1 to zero can be established provided that i) H is a separable RKHS (e.", "startOffset": 24, "endOffset": 48}, {"referenceID": 24, "context": "A similar approach to OKS for reducing the computational cost of kernel methods is the Nystr\u00f6m method (Williams and Seeger, 2000), where the dictionary consists of a subset of samples chosen at random.", "startOffset": 102, "endOffset": 129}, {"referenceID": 5, "context": "From an information theoretic point of view, log detGn detGD\u0303 can be interpreted as the conditional entropy (Cover and Thomas, 1988), which indicates that D\u0303 captures less information about the data sets.", "startOffset": 108, "endOffset": 132}, {"referenceID": 14, "context": "A recent study (Jin et al., 2012) shows that assuming bounded kernel, the spectral norm of the approximation error between the true and the approximated Gram matrix scales at a rate of O ( n |D| 1 2 ) , and in the case of \u03bbi \u223c i\u2212p, an", "startOffset": 15, "endOffset": 33}, {"referenceID": 6, "context": "The theoretical study of the Nystr\u00f6m method by Drineas and Mahoney (2005) suggests that O ( \u03b1\u22124k ) samples are needed to approximate the first k eigenvectors well, which is linear in k, irrespective of the sample size.", "startOffset": 47, "endOffset": 74}, {"referenceID": 19, "context": "One possibility is to strengthen the bound by incorporating information from higher order moments (Philips and Nelson, 1995), i.", "startOffset": 98, "endOffset": 124}, {"referenceID": 3, "context": "It is also possible to improve the second step, using concentration inequalities for configuration functions (Boucheron et al., 1999).", "startOffset": 109, "endOffset": 133}, {"referenceID": 3, "context": "By Theorem 2 in Boucheron et al. (1999), Zn concentrates sharply around E [Zn].", "startOffset": 16, "endOffset": 40}], "year": 2012, "abstractText": "We analyze the size of the dictionary constructed from online kernel sparsification, using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator. Using this formula, we are able to connect the cardinality of the dictionary with the eigen-decay of the covariance operator. In particular, we show that under certain technical conditions, the size of the dictionary will always grow sublinearly in the number of data points, and, as a consequence, the kernel linear regressor constructed from the resulting dictionary is consistent.", "creator": "LaTeX with hyperref package"}}}