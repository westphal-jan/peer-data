{"id": "1206.4658", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Dirichlet Process with Mixed Random Measures: A Nonparametric Topic Model for Labeled Data", "abstract": "we broadly describe such a nonparametric descriptive topic recognition model for labeled viral data. the model extensively uses here a mixture of random combination measures ( mrm ) as a data base distribution of the dirichlet matching process ( pseudo dp ) of computing the hdp simulation framework, is so we please call it the dp - tagged mrm. to additionally model tightly labeled data, we define a matching dp linear distributed cumulative random measure tree for each item label, n and the resulting model generates an incomplete unbounded equal number basis of specified topics for each label. commercially we thereby apply dp - mrm on simple single - labeled and matrix multi - star labeled corpora of documents and compare the performance on antibody label prediction with medlda, additive lda - svm, diagnostic and labeled - dimensional lda. usually we further effectively enhance the model by incorporating random ddcrp and modeling together multi - labeled image images for product image segmentation and object reference labeling, incorporating comparing amongst the performance simulated with experimental ncuts pr and rddcrp.", "histories": [["v1", "Mon, 18 Jun 2012 15:27:40 GMT  (1424kb)", "http://arxiv.org/abs/1206.4658v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["dongwoo kim", "suin kim", "alice h oh"], "accepted": true, "id": "1206.4658"}, "pdf": {"name": "1206.4658.pdf", "metadata": {"source": "META", "title": "Dirichlet Process with Mixed Random Measures: A Nonparametric Topic Model for Labeled Data", "authors": ["Dongwoo Kim", "Suin Kim"], "emails": ["dw.kim@kaist.ac.kr", "suin.kim@kaist.ac.kr", "alice.oh@kaist.edu"], "sections": [{"heading": "1. Introduction", "text": "Topic models such as latent dirichlet allocation (LDA) (Blei et al., 2003) have been extended to incorporate side information such as authorship (Rosen-Zvi et al., 2004), spatial or temporal coordinates (Wang & Grimson, 2007; Wang et al., 2008), and document labels (Ramage et al., 2009). Most of these models are parametric topic models, and they cannot be simply converted to nonparametric counterparts which generally have various advantages over parametric models. In the Bayesian nonparametric (BNP) literature\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\non Dirichlet processes (DP), modeling unknown densities with covariates has been often done with dependent Dirichlet Processes (DDP), but extending DDP for topic modeling requires more complex model settings and posterior inferences (Srebro & Roweis, 2005).\nIn this paper, we propose a novel nonparametric topic model, Dirichlet process with mixed random measures (DP-MRM) for documents with an arbitrary amount of discrete side information such as labels. DP-MRM can be seen as a nonparametric extension of LabeledLDA (L-LDA) (Ramage et al., 2009) in terms of defining topic distributions over labels. Recent research shows that incorporating label information into topic models has advantages for topic interpretation as well as other practical uses such as user profiling in social media (Ramage et al., 2010). However, L-LDA assumes that each label corresponds to a single multinomial (i.e., topic), and a document is only generated by the topics of the observed labels. Consequently, the model imposes an overly limiting restriction on the topics with which to represent the documents. While L-LDA models each label with a single multinomial, DP-MRM models each label with a random measure which is defined over the entire topic space.\nThere are several supervised topic models including sLDA (Blei & McAuliffe, 2007), discLDA (LacosteJulien et al., 2008), and medLDA (Zhu et al., 2009), that also model data with labels. There are two major differences between those models and DP-MRM. First, in the former models which are designed specifically for classification, each label acts as the supervisor for learning. In L-LDA and DP-MRM which are designed with the focus on understanding the meaning of each label in terms of the latent topics, each label actually is the label for one (in L-LDA) or a set of (DP-MRM) topic(s). Second, the former models are restricted to\nmodeling data with one label per document and cannot model documents with multiple labels. To illustrate this second point, we evaluate DP-MRM on data with single labels as well as multiple labels.\nAnother view of DP-MRM is that it is a more general case of the HDP (Teh et al., 2006). Modeling the corpus with our model using a single label for all documents would produce the same results as the HDP. Viewed this way, DP-MRM can be used instead of the HDP in many BNP models that are extensions of HDP. We show an example of this by incorporating the ddCRP (Blei & Frazier, 2011) into our model for the task of image segmentation as done in rddCRP (Ghosh et al., 2011).\nThe paper is organized as follows. In section 2, we describe DP-MRM along with the stick-breaking and Po\u0301lya urn perspectives. In section 3, we derive a sampling method for the latent variables based on Gibbs sampling. In section 4, we demonstrate our approach on labeled documents for single-labeled and multi-labeled corpora and compare the performance of our model by label prediction and heldout likelihood against LDA-SVM and L-LDA. In section 5, we present a modification of our model for image segmentation and compare the performance with nCuts (Shi & Malik, 2000) and rddCRP (Ghosh et al., 2011) quantitatively and qualitatively."}, {"heading": "2. Dirichlet Process with Mixture of Random Measures", "text": "In this section, we describe our model, Dirichlet process with mixed random measures (DP-MRM) model. We first review the generative process of L-LDA, and then we show how DP-MRM incorporates label information within the BNP framework based on Dirichlet Processes (DP). Lastly, we present the stick breaking process and the Po\u0301lya urn scheme for DP-MRM."}, {"heading": "2.1. Model Definition", "text": "L-LDA is a supervised version of LDA for modeling multi-labeled documents. The generative process of L-LDA starts with a definition of a document specific function label(j), which returns a set of observed label indices for document j. Then, for each document j, a multinomial distribution \u03b8j over topics is randomly sampled from a Dirichlet with parameter rj\u03b1, where rj is a K dimensional vector whose kth value is 1 if k \u2208 label(j) and 0 if k /\u2208 label(j). Then, to generate the word i, a topic zji is chosen from this topic distribution, and a word, xji, is generated by randomly sampling from a topic-specific multinomial distribu-\ntion \u03c6zji . By using a document specific indicator vector rj , the model can specify the topic proportion of document \u03b8j over the |label(j)|\u22121 dimension simplex.\nWe now describe the generative process of Dirichlet process with mixed random measures. First, we define a DP distributed random measure G10, ..., G K 0 over K possible labels with a base distribution H as follows:\nH | \u03b2 \u2261 Dir(\u03b2) Gk0 | \u03b3k, H \u223c DP(\u03b3k, H), (1)\nwhere the base distribution H is assumed to be a symmetric Dirichlet distribution over the entire vocabulary dimension, and \u03b3k controls the variability of G k 0 . By defining one random measure per label, we place an infinite topic space for each label. For each document j, another DP distributed random measure Gj is defined with a mixture of labeled-random measure as follows:\n\u03bbj \u223c Dir(rj\u03b7) Gj |label(\u00b7), \u03b1, \u03bbj \u223c DP(\u03b1, \u2211\n{k;label(j)}\n\u03bbjkG k 0) (2)\nwhere \u03b1 is a concentration parameter, \u03bbjk is a mixing proportion of Gk0 , and \u03b7 controls the sparsity of \u03bbj . DP-MRM uses a mixture of random measures,\u2211 k \u03bbjkG k 0 , as the base distribution of Gj , the document specific measure. For the mixing proportion \u03bbjk of each Gk0 , we sample \u03bbj from a symmetric Dirichlet prior parameterized by rj and \u03b7. Hence, with the observed labels label(j), rj selectively specifies the mixing proportions of Gk0 over the |label(j)| \u2212 1 dimensional simplex.\nFor each word xji in document j, the probability of drawing a word xji is parameterized by a random variable \u03b8ji drawn from Gj with some family of distribution F . It is typically assumed to be a multinomial distribution,\n\u03b8ji | Gj \u223c Gj xji | \u03b8ji \u223c F (\u03b8ji) F (\u03b8ji) \u2261 Mult(\u03b8ji), (3)\nwhich makes F to be conjugate to the base distribution H, and so it is possible to integrate out the factors \u03b8ji.\nAs a result of the construction, the model chooses an appropriate number of topics for each label. Note that HDP can be viewed as a specialized instance of our model (Teh et al., 2006), where we assume there is a single \u2018unknown\u2019 label for all documents. Then the overall corpus is defined by a set of topics from the single \u2018unknown\u2019 label, Gunknown0 \u223cDP(\u03b3,H), and the random measure for document j is drawn from Gj \u223cDP(\u03b1,Gunknown0 ). A similar idea of using a mixture of random measures was proposed in (Antoniak,\n1974), but our model extends that idea into a hierarchical construction for the grouped clustering problem."}, {"heading": "2.2. Construction and Predictive Distribution", "text": "We now describe two perspectives that are important for the inference algorithms for DP-MRM: the stick breaking process and the Po\u0301lya urn scheme.\nStick breaking process The stick breaking process is a constructive definition for generating a Dirichlet process (Sethuraman, 1991). Same as the model definition in the previous section, the stick breaking process can be divided into two level DPs. For the first level random measure Gk0 , we follow the general stick breaking process, which is given by the following conditional distributions:\nvkl \u223c Beta(1, \u03b3k) \u03c0kl = vkl l\u22121\u220f d=1 (1\u2212 vkd)\n\u03c6kl \u223c H Gk0 = \u221e\u2211 l=0 \u03c0kl \u03b4\u03c6kl , (4)\nwhere \u03b4 is a Dirac delta measure. A general stick breaking process can be seen as two independent sequences of deciding the stick length \u03c0l by samples from i.i.d. Beta trials and deciding the atom of the lth stick \u03c6l by i.i.d. samples from H.\nThe second level stick breaking construction is given by the following conditional distributions:\n\u03bbj \u223c Dir(rj\u03b7)\nwjt \u223c Beta(1, \u03b1) \u03c0jt = wjt t\u22121\u220f d=1 (1\u2212 wjd) kjt \u223c Mult(\u03bbj) \u03c8jt \u223c G kjt 0\nGj = \u221e\u2211 t=0 \u03c0jt\u03b4\u03c8jt . (5)\nDeciding the length of each stick is the same as the general stick breaking process, but assigning atoms for\neach divided stick must be changed because there are K random measures for drawing \u03c8jt. We introduce kjt as an indicator to G k 0 where atom \u03c8jt is drawn.\nWe let \u03b8ji denote the random variable drawn from Gj , \u03c8jt the atom of Gj , and \u03c6 k l the atom of G k 0 . Note that each \u03b8ji is associated with one \u03c8jt (i.e., \u03b8ji = \u03c8jt), and each \u03c8jt is associated with one \u03c6 k l , thus they form a shared structure across the corpus. Figure 1 visualize a sharing structure between first and second level DPs.\nPo\u0301lya urn scheme A posterior perspective of the DP is the Po\u0301lya urn scheme which shows that draws from the DP are discrete and exhibit a clustering property. As Blackwell and MacQueen showed (Blackwell & MacQueen, 1973), our model can also be formed as a successive conditional distribution of \u03b8ji given \u03b8j1, ..., \u03b8ji\u22121.\nLet njt be the number of words for which factor \u03b8ji corresponds to \u03c8jt in document j, and mjkl be the number of \u03c8jt such that \u03c8jt = \u03c6 k l . Then the conditional distribution of \u03b8ji given \u03b8j1, ..., \u03b8ji\u22121, G 1 0, ..., G K 0 , and \u03b1, with Gj and \u03bbj marginalized out, is\n\u03b8ji|\u03b8j1, ..., \u03b8ji\u22121, \u03b1, \u03b7,G10, ..., GK0 (6) \u223c \u2211 t njt i\u2212 1 + \u03b1 \u03b4\u03c8jt + \u03b1 i\u2212 1 + \u03b1 \u2211 k mjk\u00b7 + rjk\u03b7 mj\u00b7\u00b7 + |rj |\u03b7 Gk0 ,\nwhere |rj | is the number of 1\u2019s in rj , and rjk is 1 if label k has been observed in document j. \u03b8ji can be sampled from the first term of RHS or the second term of RHS. When \u03b8ji is sampled from the first term, then it corresponds to one existing \u03c8jt, and when it is sampled from the second term, we choose Gk0 to draw \u03b8ji with probability proportional to mjk\u00b7 + \u03b7. After that, we can marginalize out Gk0 to proceed further and get the conditional distribution\n\u03c8jt|\u03c811, ..., \u03c8jt\u22121, \u03b3k, Hk \u223c \u2211 k m\u00b7kl m\u00b7k\u00b7 + \u03b3k \u03b4\u03c6kl + \u03b3k m\u00b7k\u00b7 + \u03b3k Hk. (7)"}, {"heading": "3. Inference via Gibbs Sampling", "text": "We propose a Gibbs sampler for DP-MRM, a Po\u0301lya urn scheme based on the marginalization of unknown dimensions (Escobar & West, 1995). For the collapsed Gibbs sampler, we marginalize out factors, \u03b8, \u03c8, \u03c6, mixing proportions, \u03bb, and random probability measures, Gj , G k 0 . As a result, we only need to sample the index of each latent variable. Let tji be the index variable such that \u03c8jt = \u03b8ji, and kjt be the index variable such that \u03c8jt \u223c Gk0 , and ljt be the index variable such that \u03c8jt = \u03c6 kjt l . Let njt be the number of \u03b8ji such that \u03b8ji = \u03c8jt, and let mjkl be the number of\n\u03c8jt such that \u03c8jt = \u03c6 k l . We use fkl(xji) to denote the conditional density of x under mixture component l of random measure Gk0 , given all items except xji,\nfkl(xji) =\n\u222b f(xji|\u03c6kl ) \u220f xj\u2032i\u2032\u2208xkl\nf(xj\u2032i\u2032 |\u03c6kl )h(\u03c6kl )d\u03c6kl\u222b \u220f xj\u2032i\u2032\u2208xkl f(xj\u2032i\u2032 |\u03c6kl )h(\u03c6kl )d\u03c6kl ,\nwhere xkl = {xji; kjtji = k, ljtji = l}.\nSampling t : The conditional density of word xji being assigned to \u03c8jt is\np(tji = t|t\u2212ji, rest)\n=\n{ njt\u00b7\nnj\u00b7\u00b7+\u03b1 fkjtljt(xji) existing t\n\u03b1 nj\u00b7\u00b7+\u03b1\n\u0393(xji) new t, (8)\nwhere \u0393(xji) = \u2211K k=1 mjk\u00b7+\u03b7 mj\u00b7\u00b7+K\u03b7 \u2211L l=1 m\u00b7kl m\u00b7k\u00b7+\u03b3k\nfkl(xji) + \u03b3k\nm\u00b7k\u00b7+\u03b3k fklnew(xji).\nSampling k and l : When new t is sampled, we need to sample kjtnew and ljtnew . However, sampling k and l cannot be done independently because given l the probability of k is always zero except one. The joint conditional density of k and l is\np(kjt = k, ljt = l|k\u2212jt, l\u2212jt, rest) (9)\n\u221d mjk\u00b7 + \u03b7 mj\u00b7\u00b7 +K\u03b7 \u00d7 m\u00b7kl m\u00b7k\u00b7 + \u03b3k fkl(xji) existing l\np(kjt = k, ljt = lnew|k\u2212jt, l\u2212jt, rest) (10)\n\u221d mjk\u00b7 + \u03b7 mj\u00b7\u00b7 +K\u03b7 \u00d7 \u03b3k m\u00b7k\u00b7 + \u03b3k fklnew(xji) new l.\nSampling k and l of existing t changes the component memberships of all data items xjt = {xji; tji = t}, and this sampling can be done with the conditional distribution of k and l given xjt."}, {"heading": "4. Application with Labeled Documents", "text": "We measure the performance of DP-MRM with three experiments. First, we compare the label prediction performance of DP-MRM and LDA-SVM on singlelabeled documents. Then, we compare the label prediction performance of DP-MRM and L-LDA on multilabeled documents. Finally, we compare the predictive\nperformance of DP-MRM and L-LDA on heldout data. For the label prediction experiments, we take a semisupervised approach: divide the corpus into training and test sets, infer the posterior distribution of the training set with the observed labels (i.e. rjk = 1 only when k \u2208 label(j)), and infer the posterior distribution of the test set with all possible K labels (i.e. rjk = 1 for all k).\nFor all evaluations, we run each model ten times with 5,000 iterations, the first 3,000 as burn-in and then using the samples thereafter with gaps of 100 iterations. For sampling the hyperparameters, we place Gamma(1,1) priors for \u03b3k, and \u03b1, and set \u03b2 to 0.5."}, {"heading": "4.1. Single-Labeled Documents", "text": "DP-MRM was designed to model multi-labeled documents, but it assumes that a label generates multiple topics, so this flexible assumption allows DP-MRM to be used for modeling single-labeled documents as well. Note that L-LDA for single-labeled documents would assign every word in a document to a single topic, and the document would thus be modeled as a mixture of unigrams (i.e., naive Bayes).\nTo measure the classification performance, we trained our model with five comp subcategories of newsgroup documents (20NG)1. Table 1 shows the details of our datasets. 90% of the documents were used with the labels, and the remaining 10% of documents were used without the labels. We classified each of the test documents by the label with the most number of words assigned. As a baseline, we trained a multi-class SVM with the topic proportions inferred by LDA (Blei et al., 2003). MedLDA (Zhu et al., 2009), one of supervised topic model, also used for the comparision. The results, shown in Figure 2, display a significant improvement of our model over the LDA-SVM approach and MedLDA."}, {"heading": "4.2. Multi-Labeled Documents", "text": "We compared the performance of L-LDA and DPMRM using two multi-labeled corpora: the Ohsumed dataset2, which is a subset of the MEDLINE corpus consisting of medical journals, and RCV1-V2 dataset (Lewis et al., 2004), a corpus of Reuters news articles. We randomly sampled a subset of each corpus, and the detailed descriptions are shown in Table 1. Again, 90% of documents were used with the labels, and the rest 10% of documents were used without the labels.\nL-LDA provides a systematic way of naming the dis-\n1http://people.csail.mit.edu/jrennie/20Newsgroups/ 2http://ir.ohsu.edu/ohsumed/ohsumed.html\nTable 2. Topics discovered by L-LDA and DP-MRM for the Infant label of the Ohsumed dataset, a corpus of medical journal articles, and for the Corporate/Industrial label of the RCV news articles corpus. We show the top ten probability words for each topic. L-LDA discovers exactly one topic per label, but DP-MRM discovers several topics per label.\nInfant Corporate/Industrial L-LDA DP-MRM L-LDA DP-MRM\nchildren children colon tumor compan million oil shar ton airlin infect infect aeruginosa patient million profit pow compan million air month infant express leukemia percent percent ga bank percent carg patient month gene cell market half compan percent produc flight\nag ag type chemotherapi produc expect produc million export servic infant antibodi dna dose stat compan plant invest crop airport studi hiv mutat therapi bank billion operat stock wheat carri vaccin vaccin ha-ra receiv invest result refin market grain plan viru viru excret treatment plan market unit stat juli operat antibodi test urinari remiss billion shar million plan sugar aircraft\n0\n0.25\n0.50\n0.75\n1.00\nos.ms-windows.misc sys.ibm.pc.hardware sys.mac.hardware windows.x graphics\nSingle label prediction\nLDA50-SVM LDA100-SVM MedLDA-50 MedLDA-100 MDPM\nFigure 2. Accuracies of DP-MRM, MedLDA, and LDASVM on classification of 20NG. DP-MRM outperforms LDA-SVM and MedLDA on average.\ncovered topics, and thus increases the interpretability of them. However, the assumption that a document is generated from a subset of topics specified by the observed labels limits the expressiveness of the model. DP-MRM was designed to keep the benefits of L-LDA while increasing the expressiveness, and we can see the consequences of the design in the discovered topics shown in Table 2. The table shows one label from each corpus and the corresponding topics. DP-MRM discovered multiple topics for the labels \u2018Infant\u2019 and \u2018Corporate/Industrial\u2019, and these are more detailed topics than the single topics discovered by L-LDA.\nFor the classification of multi-labeled documents based on the posterior samples, we counted the number of words assigned to each measure Gk0 and classified as label k with various threshold cuts based on normalized counts. We scored each model based on Micro F1 and Macro F1 measures. Micro F1 accounts for the proportion of each class, so large classes affect its results, whereas macro F1 assigns equal weights to all classes. Table 3 shows the classification results with different cuts, and our model performs better than LLDA in terms of micro average, but in macro average,\nthere are inconsistencies between the different cuts. In general, DP-MRM shows more stable performance with respect to the cuts, whereas L-LDA shows variable results depending on the cut."}, {"heading": "4.3. Predictive Performance", "text": "To compare the model fit, we measure the predictive performance of our model and L-LDA with heldout likelihood of the test set. For each model, posterior sampling was done with 90% of the words in each document while the test set performance was evaluated on the remaining 10% of the words. Given S samples from the posterior, the test set likelihood for our model is computed as follows:\np(xtest) = \u220f\nji\u2208xtest\n1\nS S\u2211 s=1 K\u2211 k=1 \u2211 l \u03b8 (s) jkl\u03c8 (s) klxji\nTable 3. Macro and micro F1 averages of L-LDA and DP-MRM for the two multi-label datasets. DP-MRM consistently performs better than L-LDA for micro F1, but not for macro F1.\nRCV Ohsumed Micro Average Macro Average Micro Average Macro Average\nCut DP-MRM L-LDA DP-MRM L-LDA DP-MRM L-LDA DP-MRM L-LDA 0.001 0.511 0.282 0.257 0.172 0.392 0.345 0.223 0.257 0.050 0.520 0.449 0.265 0.285 0.389 0.382 0.223 0.263 0.100 0.520 0.473 0.266 0.322 0.382 0.364 0.220 0.250 0.200 0.509 0.464 0.264 0.331 0.362 0.326 0.207 0.223 0.300 0.487 0.434 0.254 0.315 0.334 0.287 0.189 0.195 0.500 0.424 0.355 0.220 0.261 0.262 0.206 0.145 0.137\n\u03b8 (s) jkl = njkl\u00b7 + \u03b1{m\u00b7kl/(m\u00b7k\u00b7 + \u03b3k)} njk\u00b7\u00b7 + \u03b1\n\u03c8 (s) klxij\n= n\u00b7klxij + \u03b2\nn\u00b7kl\u00b7 +W\u03b2 ,\nwhere njklx is the number of words x corresponding to \u03c6kl in document j, and W is the vocabulary size. The test set likelihood for L-LDA was computed as follows:\np(xtest) = \u220f\nji\u2208xtest\n1\nS S\u2211 s=1 K\u2211 k=1 \u03b8 (s) jk \u03c8 (s) kxji\n\u03b8 (s) jk =\nnjk\u00b7 + \u03b1\nnj\u00b7\u00b7 +K\u03b1\n\u03c8 (s) kxji\n= n\u00b7kxji + \u03b2\nn\u00b7k\u00b7 +W\u03b2 ,\nwhere K is the total number of labels. Figure 3 shows the test set per-word log likelihood of both model with RCV dataset, our model performs better than L-LDA across ten folded dataset consistently."}, {"heading": "5. Image Segmentation with ddCRP", "text": "We describe an extension of DP-MRM, built by incorporating ddCRP, a nonparametric Bayesian prior that accounts for spatial dependencies, into DP-MRM. This illustrates the generality of DP-MRM that it may serve as a replacement for HDP for data with side information. We test this DP-MRM-ddCRP model on the task of image segmentation for multi-labeled images without manually segmented training data.\nImage segmentation is often done with manually segmented and labeled data (He et al., 2004; Gould et al., 2009). DP-MRM can also perform supervised segmentation, but such data are harder to obtain, whereas image collections with multiple labels and no segmentation are relatively easy to obtain (e.g., Picasa or Flickr). One recent paper has shown a Bayesian model for simultaneous image segmentation and annotation (Du et al., 2009) using a logistic stick-breaking process. While that model is specialized for image understand-\ning, DP-MRM is a general framework for modeling multi-labeled data including documents and images."}, {"heading": "5.1. Incorporating ddCRP into DP-MRM", "text": "The Chinese restaurant process (CRP) is an alternative formulation of the DP. CRP forms a clustering structure of customers by assigning each customer to an existing or a new table. ddCRP, however, forms a clustering structure of customers by linking customers, accounting for the distances between them; customers who are relatively close to each other are likely to be linked together than those who are far apart. Let ci be the assignment of customer i to the other customers, then the distribution of the customer assignment is\np(ci = i \u2032|c\u2212i, f,D, \u03b1) \u221d { f(dii\u2032) i 6= i\u2032 \u03b1 i = i\u2032 , (11)\nwhere dii\u2032 is the distance between customer i and i \u2032, and f(dii\u2032) is a decay function of the distance which mediates how the distances affect the resulting distribution over the partitions. There are many possible ways of defining the decay function, and in this paper, we follow (Ghosh et al., 2011) and use a window decay function which measures the distance between superpixels as a hop distance between them.\nBased on the conditional distribution of assignments, the Po\u0301lya urn scheme for the combined model is:\n\u03b8ji|\u03b8j1, ..., \u03b8ji\u22121, \u03b1, \u03b7,G10, ..., GK0 (12)\n\u223c i\u22121\u2211 i\u2032 f(dii\u2032) f isum + \u03b1 \u03b4\u03b8ji\u2032 +\n\u03b1\nf isum + \u03b1 \u2211 k mjk\u00b7 + rjk\u03b7 mj\u00b7\u00b7 + |rj |\u03b7 Gk0 ,\nwhere f isum = \u2211 i\u2032 6=i f(dii\u2032). This equation is similar to Equation (6), but we modify the equation based on the window decay function.\nFor posterior inference, we modify the posterior sampling Equation (8) based on the customer assignment scheme, but the changes only affect the local sampling results (within the document level), and can be employed by the algorithm for ddCRP mixture in previ-\nous work. The sampling scheme based on link structure among customers enhances the rapid mixing of sampler. See (Blei & Frazier, 2011) for a more detailed explanation of posterior inference."}, {"heading": "5.2. Image Segmentation with Multiple Labels", "text": "For image segmentation, we use the eight scene categories in (Oliva & Torralba, 2001) which are fully segmented and labeled by human subjects and available from the LabelMe dataset (Russell et al., 2008). A widely used method for representing images for inference is a codebook of images (Fei-Fei & Perona, 2005). To generate the codebook, each image is first divided into approximately 1,000 superpixels using the normalized cut algorithm (Shi & Malik, 2000). Each superpixel is described via local texton histogram (Martin et al., 2004) and HSV color histogram. By using kmeans, we quantize these histograms into 128 bins, and superpixel i in image j is summarized via these codewords xji = {xtji, xcji} indicating its texture xtji and color xcji. The base distribution H should be defined as H \u2261 Dir(\u03b7t)\u2297Dir(\u03b7c) for image segmentation.\nFigure 4 shows some examples of the labeled objects from posterior samples where DP-MRM segments images into objects and labels each object. We note again that we do not give any pixel-level information for each object during the posterior inference, but our model can successfully segment images and label segments simultaneously. The results indicate that DP-MRM succeeds in inferring both the segments and the corresponding labels by capturing the co-occurrence patterns of superpixels and labels.\nFigure 6 shows some examples of the image segmen-\ntation results comparing the original images, human segmented images, and DP-MRM segmented images. Figure 5 shows the quantitative performance of the segmentation via Rand Index, comparing DP-MRM with rddCRP (Ghosh et al., 2011) and normalized cuts (nCuts) (Shi & Malik, 2000), varying the number of segments from two to ten. We also vary the number of segments for each image, denoted as nCuts(*), where the number of segments are given as the number of labeled objects in each image. The result shows DPMRM performs better than both rddCRP and nCuts."}, {"heading": "6. Conclusion", "text": "In this paper, we presented our new model, DP-MRM, in which the base distribution of DP is a mixture of random measures. The applications with multi-labeled documents and images are shown with label prediction and image segmentation experiments. The results show that DP-MRM for labeled data produces\ninterpretable topics with more flexibility than the Labeled LDA. One promising extension of our model is to incorporate prior knowledge of external sources or domain experts into a Bayesian nonparametric topic model. It is beyond the scope of this paper, but our model can use different \u03b2k for each base distribution Hk, therefore using the structualized prior \u03b2k from domain experts (Andrzejewski et al., 2009) can be easily incorporated into our model."}, {"heading": "Acknowledgments", "text": "This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Tehcnology (2011-0026507)."}], "references": [{"title": "Incorporating domain knowledge into topic modeling via dirichlet forest priors", "author": ["D. Andrzejewski", "X. Zhu", "M. Craven"], "venue": "In ICML,", "citeRegEx": "Andrzejewski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Andrzejewski et al\\.", "year": 2009}, {"title": "Mixtures of dirichlet processes with applications to bayesian nonparametric problems", "author": ["C.E. Antoniak"], "venue": "The Annals of Statistics,", "citeRegEx": "Antoniak,? \\Q1974\\E", "shortCiteRegEx": "Antoniak", "year": 1974}, {"title": "Ferguson distributions via p\u00f3lya urn schemes", "author": ["D. Blackwell", "J.B. MacQueen"], "venue": "The Annals of Statistics,", "citeRegEx": "Blackwell and MacQueen,? \\Q1973\\E", "shortCiteRegEx": "Blackwell and MacQueen", "year": 1973}, {"title": "Distance dependent chinese restaurant processes", "author": ["D. Blei", "P. Frazier"], "venue": "JMLR, 12:2461\u20132488,", "citeRegEx": "Blei and Frazier,? \\Q2011\\E", "shortCiteRegEx": "Blei and Frazier", "year": 2011}, {"title": "Supervised topic models", "author": ["D. Blei", "J. McAuliffe"], "venue": "In NIPS,", "citeRegEx": "Blei and McAuliffe,? \\Q2007\\E", "shortCiteRegEx": "Blei and McAuliffe", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["D Blei", "A Ng", "M. Jordan"], "venue": "JMLR, pp", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A bayesian model for simultaneous image clustering, annotation and object segmentation", "author": ["L. Du", "L. Ren", "D. Dunson", "L. Carin"], "venue": "In NIPS,", "citeRegEx": "Du et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Du et al\\.", "year": 2009}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["M.D. Escobar", "M. West"], "venue": "JASA, pp", "citeRegEx": "Escobar and West,? \\Q1995\\E", "shortCiteRegEx": "Escobar and West", "year": 1995}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "In CVPR,", "citeRegEx": "Fei.Fei and Perona,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei and Perona", "year": 2005}, {"title": "Spatial distance dependent chinese restaurant processes for image segmentation", "author": ["S. Ghosh", "A.B. Ungureanu", "E.B. Sudderth", "D.M. Blei"], "venue": "In NIPS,", "citeRegEx": "Ghosh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2011}, {"title": "Multiscale conditional random fields for image labeling", "author": ["X. He", "R.S. Zemel", "M.A. Carreira-Perpin\u00e1n"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2004\\E", "shortCiteRegEx": "He et al\\.", "year": 2004}, {"title": "Disclda: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2008}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "JMLR, 5:361\u2013397,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture", "author": ["D.R. Martin", "C.C. Fowlkes", "J. Malik"], "venue": "cues. TPAMI,", "citeRegEx": "Martin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2004}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "Oliva and Torralba,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba", "year": 2001}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Ramage et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2009}, {"title": "Characterizing microblogs with topic models", "author": ["D. Ramage", "S. Dumais", "D. Liebling"], "venue": "In ICWSM. The AAAI Press,", "citeRegEx": "Ramage et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2010}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "In UAI,", "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Labelme: a database and web-based tool for image annotation", "author": ["B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": null, "citeRegEx": "Russell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2008}, {"title": "A constructive definition of dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1991\\E", "shortCiteRegEx": "Sethuraman", "year": 1991}, {"title": "Normalized cuts and image", "author": ["J. Shi", "J. Malik"], "venue": "segmentation. TPAMI,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Time-varying topic models using dependent dirichlet processes", "author": ["N. Srebro", "S. Roweis"], "venue": "UTML, TR# 2005,", "citeRegEx": "Srebro and Roweis,? \\Q2005\\E", "shortCiteRegEx": "Srebro and Roweis", "year": 2005}, {"title": "Hierarchical dirichlet processes", "author": ["Y. Teh", "M. Jordan", "M. Beal", "D. Blei"], "venue": null, "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Continuous Time Dynamic Topic Models", "author": ["C. Wang", "D.M. Blei", "D. Heckerman"], "venue": "In Proceedings of ICML,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Spatial latent dirichlet allocation", "author": ["X. Wang", "E. Grimson"], "venue": null, "citeRegEx": "Wang and Grimson,? \\Q2007\\E", "shortCiteRegEx": "Wang and Grimson", "year": 2007}, {"title": "Medlda: maximum margin supervised topic models for regression and classification", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "Topic models such as latent dirichlet allocation (LDA) (Blei et al., 2003) have been extended to incorporate side information such as authorship (Rosen-Zvi et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 17, "context": ", 2003) have been extended to incorporate side information such as authorship (Rosen-Zvi et al., 2004), spatial or temporal coordinates (Wang & Grimson, 2007; Wang et al.", "startOffset": 78, "endOffset": 102}, {"referenceID": 23, "context": ", 2004), spatial or temporal coordinates (Wang & Grimson, 2007; Wang et al., 2008), and document labels (Ramage et al.", "startOffset": 41, "endOffset": 82}, {"referenceID": 15, "context": ", 2008), and document labels (Ramage et al., 2009).", "startOffset": 29, "endOffset": 50}, {"referenceID": 15, "context": "DP-MRM can be seen as a nonparametric extension of LabeledLDA (L-LDA) (Ramage et al., 2009) in terms of defining topic distributions over labels.", "startOffset": 70, "endOffset": 91}, {"referenceID": 16, "context": "Recent research shows that incorporating label information into topic models has advantages for topic interpretation as well as other practical uses such as user profiling in social media (Ramage et al., 2010).", "startOffset": 188, "endOffset": 209}, {"referenceID": 25, "context": ", 2008), and medLDA (Zhu et al., 2009), that also model data with labels.", "startOffset": 20, "endOffset": 38}, {"referenceID": 22, "context": "Another view of DP-MRM is that it is a more general case of the HDP (Teh et al., 2006).", "startOffset": 68, "endOffset": 86}, {"referenceID": 9, "context": "We show an example of this by incorporating the ddCRP (Blei & Frazier, 2011) into our model for the task of image segmentation as done in rddCRP (Ghosh et al., 2011).", "startOffset": 145, "endOffset": 165}, {"referenceID": 9, "context": "In section 5, we present a modification of our model for image segmentation and compare the performance with nCuts (Shi & Malik, 2000) and rddCRP (Ghosh et al., 2011) quantitatively and qualitatively.", "startOffset": 146, "endOffset": 166}, {"referenceID": 22, "context": "Note that HDP can be viewed as a specialized instance of our model (Teh et al., 2006), where we assume there is a single \u2018unknown\u2019 label for all documents.", "startOffset": 67, "endOffset": 85}, {"referenceID": 19, "context": "Stick breaking process The stick breaking process is a constructive definition for generating a Dirichlet process (Sethuraman, 1991).", "startOffset": 114, "endOffset": 132}, {"referenceID": 5, "context": "As a baseline, we trained a multi-class SVM with the topic proportions inferred by LDA (Blei et al., 2003).", "startOffset": 87, "endOffset": 106}, {"referenceID": 25, "context": "MedLDA (Zhu et al., 2009), one of supervised topic model, also used for the comparision.", "startOffset": 7, "endOffset": 25}, {"referenceID": 12, "context": "We compared the performance of L-LDA and DPMRM using two multi-labeled corpora: the Ohsumed dataset, which is a subset of the MEDLINE corpus consisting of medical journals, and RCV1-V2 dataset (Lewis et al., 2004), a corpus of Reuters news articles.", "startOffset": 193, "endOffset": 213}, {"referenceID": 10, "context": "Image segmentation is often done with manually segmented and labeled data (He et al., 2004; Gould et al., 2009).", "startOffset": 74, "endOffset": 111}, {"referenceID": 6, "context": "One recent paper has shown a Bayesian model for simultaneous image segmentation and annotation (Du et al., 2009) using a logistic stick-breaking process.", "startOffset": 95, "endOffset": 112}, {"referenceID": 9, "context": "There are many possible ways of defining the decay function, and in this paper, we follow (Ghosh et al., 2011) and use a window decay function which measures the distance between superpixels as a hop distance between them.", "startOffset": 90, "endOffset": 110}, {"referenceID": 18, "context": "For image segmentation, we use the eight scene categories in (Oliva & Torralba, 2001) which are fully segmented and labeled by human subjects and available from the LabelMe dataset (Russell et al., 2008).", "startOffset": 181, "endOffset": 203}, {"referenceID": 13, "context": "Each superpixel is described via local texton histogram (Martin et al., 2004) and HSV color histogram.", "startOffset": 56, "endOffset": 77}, {"referenceID": 9, "context": "Figure 5 shows the quantitative performance of the segmentation via Rand Index, comparing DP-MRM with rddCRP (Ghosh et al., 2011) and normalized cuts (nCuts) (Shi & Malik, 2000), varying the number of segments from two to ten.", "startOffset": 109, "endOffset": 129}, {"referenceID": 0, "context": "It is beyond the scope of this paper, but our model can use different \u03b2k for each base distribution Hk, therefore using the structualized prior \u03b2k from domain experts (Andrzejewski et al., 2009) can be easily incorporated into our model.", "startOffset": 167, "endOffset": 194}], "year": 2012, "abstractText": "We describe a nonparametric topic model for labeled data. The model uses a mixture of random measures (MRM) as a base distribution of the Dirichlet process (DP) of the HDP framework, so we call it the DPMRM. To model labeled data, we define a DP distributed random measure for each label, and the resulting model generates an unbounded number of topics for each label. We apply DP-MRM on single-labeled and multi-labeled corpora of documents and compare the performance on label prediction with MedLDA, LDA-SVM, and Labeled-LDA. We further enhance the model by incorporating ddCRP and modeling multi-labeled images for image segmentation and object labeling, comparing the performance with nCuts and rddCRP.", "creator": "LaTeX with hyperref package"}}}