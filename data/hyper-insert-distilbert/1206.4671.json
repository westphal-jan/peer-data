{"id": "1206.4671", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling", "abstract": "\u2022 we develop dependent hierarchical model normalized model random measures and and we apply them simultaneously to dynamic topic computational modeling. the residual dependency arises via superposition, loop subsampling and in point transition reliance on the underlying the poisson processes consequences of these numerical measures. the these measures used include normalised generalised gamma processes that demonstrate power law extension properties, unlike dirichlet processes used previously prevalent in dynamic programming topic modeling. inference for the kernel model also includes adapting around a recently developed fast slice sampler to directly help manipulate the raw underlying poisson decomposition process. experiments performed relying on site news, blogs, academic media and twitter profile collections and demonstrate the technique gives us superior elastic perplexity over a number of underlying previous linear models.", "histories": [["v1", "Mon, 18 Jun 2012 15:35:02 GMT  (455kb)", "http://arxiv.org/abs/1206.4671v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["changyou chen", "nan ding", "wray l buntine"], "accepted": true, "id": "1206.4671"}, "pdf": {"name": "1206.4671.pdf", "metadata": {"source": "META", "title": "Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling", "authors": ["Changyou Chen", "Nan Ding", "Wray Buntine"], "emails": ["cchangyou@gmail.com", "ding10@purdue.edu", "Wray.Buntine@nicta.com.au"], "sections": [{"heading": "1. Introduction", "text": "Dirichlet processes and their variants are popular in recent years, with applications found in diverse discrete domains such as topic modeling (Teh et al., 2006), ngram modeling (Teh, 2006), clustering (Socher et al., 2011), and image modeling (Li et al., 2011). These models take as input a base distribution and produce as output another distribution which is somewhat similar. Moreover, they can be used hierarchically. Together this makes them ideal for modeling structured data such as text and images.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nWhen modeling dynamic data or data from multiple sources, dependent nonparametric Bayesian models (MacEachern, 1999) are needed in order to harness related or previous information. Among these models, the hierarchical Dirichlet process (HDP) (Teh et al., 2006) is the most popular one. However, a basic assumption underlying the HDP is the full exchangeability of the sample path, which is often violated in practice, e.g., we could assume the content of ICML depends on previous years\u2019 so order is important.\nTo overcome the full exchangeability limitation, several dependent Dirichlet process models have been proposed, for example, the dynamic HDP (Ren et al., 2008), the evolutionary HDP (Zhang et al., 2010), and the recurrent Chinese Restaurant process (Ahmed & Xing, 2010). Dirichlet processes are used because of simplicity and conjugacy (James et al., 2006). These models are constructed by incorporating the previous DP\u2019s into the base distribution of the current DP. Dependent DPs have also been constructed using the underlying Poisson processes (Lin et al., 2010). However, recent research has shown that many real datasets have the power-law property, e.g., in images (Sudderth & Jordan, 2008), in topic-word distributions (Teh, 2006) and in document topic (label) distributions (Rubin et al., 2011). This makes the Dirichlet process an improper tool for modeling these datasets.\nAlthough there also exists some dependent nonparametric models with power-law phenomena, their dependencies are limited. For example, Bartlett et al. (2010) proposed a dependent hierarchical PitmanYor process that only allows deletion of atoms, while Sudderth & Jordan (2008) construct the dependent Pitman-Yor process by only allowing dependencies between atoms.\nIn this paper, we use a larger class of stochastic processes called normalized random measures with independent increments (NRM) (James et al., 2009). While this includes the Dirichlet process as a special case, some other versions of NRMs have the powerlaw property. This class of discrete random measures can also be constructed from Poisson processes. Given this, following (Lin et al., 2010), we analogically define superposition, subsampling and point transition on these normalized random measures, and construct a time dependent hierarchical model for dynamic topic modeling. By this, the dependencies are flexibly controlled between both jumps and atoms of the NRMs. All proofs and some extended theories are available in (Chen et al., 2012)."}, {"heading": "2. Normalized Random Measures", "text": ""}, {"heading": "2.1. Background and Definitions", "text": "This background on random measures follows (James et al., 2009).\nLet (S,S) be a measure space where S is the \u03c3-algebra of S. Let \u03bd be a measure on it. A Poisson process on S is a random subset \u03a0 \u2208 S such that if N(A) is the number of points of \u03a0 in the measurable set A \u2286 S, then N(A) is a Poisson random variable with mean \u03bd(A), and N(A1), \u00b7 \u00b7 \u00b7 , N(An) are independent if A1, \u00b7 \u00b7 \u00b7 , An are disjoint.\nBased on the definition, we define a complete random measure (CRM) on (X,B(X)) to be a linear functional of the Poisson random measure N(\u00b7), with mean measure \u03bd(dt, dx) defined on a product space S = R+\u00d7X:\n\u00b5\u0303(B) = \u222b R+\u00d7B tN(dt,dx),\u2200B \u2208 B(X). (1)\nHere \u03bd(dt,dx) is called the Le\u0301vy measure of \u00b5\u0303.\nIt is worth noting that the CRM is usually written in the form \u00b5\u0303(B) = \u2211\u221e k=1 Jk\u03b4xk(B), where J1, J2, \u00b7 \u00b7 \u00b7 > 0 are called the jumps of the process, and x1, x2, \u00b7 \u00b7 \u00b7 are a sequence of independent random variables drawn from a base measurable space (X,B(X))1. A normalized random measure (NRM) on (X,B(X)) is defined as \u00b5 = \u00b5\u0303 \u00b5\u0303(X) . We always use \u00b5 to denote an NRM, and \u00b5\u0303 its unnormalized counterpart.\nTaking different Le\u0301vy measures \u03bd(dt, dx), we can obtain different NRMs, and the form we consider is described in Section 2.3. Here we consider the case \u03bd(dt, dx) = M\u03c1\u03b7(dt)H(dx), where H(dx) is the base probability measure, M is the total mass acting as a\n1B(X) means the \u03c3-algebra of X, we sometimes omit this and use X to denote the measurable space.\nconcentration parameter, and \u03b7 is the set of other hyperparameters, depending on the specific NRM\u2019s. We use NRM(M,\u03b7,H) to denote the corresponding normalized random measure."}, {"heading": "2.2. Slice sampling NRMs", "text": "We briefly introduce the ideas of slice sampling normalized random measures discussed in (\u201cSlice 1\u201d version, Griffin & Walker, 2011). It deals with the normalized random measure mixture of the type\n\u00b5(\u00b7) = \u221e\u2211 k=1 rk\u03b4\u03b8k(\u00b7), \u03b8si \u223c \u00b5(\u00b7), xi \u223c g0(\u00b7|\u03b8si) (2)\nwhere rk = Jk/ \u2211\u221e l=1 Jl, \u03b8k\u2019s are the component of the mixture model drawn i.i.d. from a parameter space H(\u00b7), si denotes the component that xi belongs to, and g0(\u00b7|\u03b8k) is the density function to generate data from component k. Given the observations ~x, a slice latent variable ui is introduced for each xi so that it only considers those components whose jump sizes Jk\u2019s are larger than the corresponding ui\u2019s. Furthermore, an auxiliary variable v is introduced to decouple each individual jump Jk and their infinite sum of the jumps\u2211\u221e l=1 Jl appeared in the denominators of rk\u2019s. It is shown in (Griffin & Walker, 2011) that the posterior of the infinite mixture model (2) with the above auxiliary variables is proportional to\nP\u00b5(~\u03b8, J1, \u00b7 \u00b7 \u00b7 , JK ,K, ~u, L,~s, v|~x,H, \u03c1\u03b7) \u221d\nexp { \u2212v\nK\u2211 k=1 Jk\n} exp { \u2212M \u222b L 0 (1\u2212 exp {\u2212vt}) \u03c1\u03b7(t)dt }\nvN\u22121p(J1, \u00b7 \u00b7 \u00b7 , JK) K\u220f k=1 h(\u03b8k) N\u220f i=1 1(Jsi > ui)g0(xi|\u03b8si), (3)\nwhere 1(a) is an indicator function returning 1 if a is true and 0 otherwise, h(\u00b7) is the density of H(\u00b7), L = min{~u}, and p(J1, \u00b7 \u00b7 \u00b7 , JK) = \u220fK k=1 \u03c1\u03b7(Jk)\u222b \u221e L \u03c1\u03b7(t)dt is the distribution for the jumps which are larger than L derived from the underlying Poisson process. Sampling for this mixture model iteratively cycles over {~\u03b8, (J1, \u00b7 \u00b7 \u00b7 , JK),K, ~u,~s, v} based on (3). Please refer to (Section 1.3 Chen et al., 2012) for more details."}, {"heading": "2.3. Normalized generalized Gamma processes", "text": "In this paper, we consider the normalized generalized Gamma processes. Generalized Gamma processes (Lijoi et al., 2007) (GGP) are random measures with the Le\u0301vy measure\n\u03bd(dt,dx) = M e\u2212bt\nt1+a H(dx), b > 0, 0 < a < 1. (4)\nBy normalizing the GGP, we obtain the normalized generalized Gamma process (NGG)2. One of the most familiar special cases is the Dirichlet process, which is a normalized Gamma process where a \u2192 0 and b = 1 and the concentration parameter appears as M .\nCrucially, unlike the DP, the NGG can produce the power-law phenomenon.\nProposition 1 ((Lijoi et al., 2007)) Let Kn be the number of components induced by the NGG with parameters a an b or the Dirichlet process with total mass M . Then for the NGG, Kn/n\na \u2192 Sab almost surely, where Sab is a strictly positive random variable parameterized by a and b. For the DP, Kn/ log(n)\u2192M .\nTherefore, in order to better analyze certain kinds of real data, we propose to use the NGG in place of the Dirichlet process. In the next section, we propose a dynamic topic model which extends two major advances of the Dirichlet process: the HDP (Teh et al., 2006) and the dependent Dirichlet process (Lin et al., 2010), to normalized random measures."}, {"heading": "3. Dynamic topic modeling with dependent hierarchical NRMs", "text": "Our main interest is to construct a dynamic topic model that inherits partial exchangeability, meaning that the documents within each time frame are exchangeable, while between time frames they are not. To achieve this, it is crucial to model the dependency of the topics between different time frames. In particular, a topic can either inherit from the topics of earlier time frames with certain transformation, or be a completely new one which is \u201dborn\u201d in the current time frame. The above idea can be modeled by a series of hierarchical NRMs, one per time frame. Between the time frames, these hierarchical NRMs depend on each other through three dependency operators: superposition, subsampling and point transition, which will be defined below. The corresponding graphical model is shown in Figure 1(left) and the generating process for the model is as follows:\n\u2022 Generating independent NRMs \u00b5m for time frame m = 1, \u00b7 \u00b7 \u00b7 , n:\n\u00b5m|H, \u03b70 \u223c NRM(M0, \u03b70, P0) (5)\nwhere H(\u00b7) = M0P0(\u00b7). M0 is the total mass for \u00b5m and P0 is the base distribution. In this paper, P0 is the Dirichlet distribution, \u03b70 is the set of\n2In NGG, b can be absolved into M , thus we usually set b = 1, see (Chen et al., 2012) for detail.\nhyperparameters of the corresponding NRM, e.g., in NGG, \u03b70 = {a, b}.\n\u2022 Generating dependent NRMs \u00b5\u2032m (from \u00b5m and \u00b5\u2032m\u22121), for time frame m > 1:\n\u00b5\u2032m = T (S q(\u00b5\u2032m\u22121))\u2295 \u00b5m . (6)\nwhere the three dependency operators superposition (\u2295), subsampling (Sq(\u00b7)) with acceptance rate q, and point transition (T (\u00b7)) are generalized from those of Dirichlet process (Lin et al., 2010). We will discuss them in more details in the following subsection.\n\u2022 Generating hierarchical NRM mixtures (\u00b5mj , \u03b8mji, xmji) for time frame m = 1, \u00b7 \u00b7 \u00b7 , n, document j = 1, \u00b7 \u00b7 \u00b7 , Nm, word i = 1, \u00b7 \u00b7 \u00b7 ,Wmj :\n\u00b5mj = NRM(Mm, \u03b7m, \u00b5 \u2032 m), (7) \u03b8mji|\u00b5mj \u223c \u00b5mj , xmji|\u03b8mji \u223c g0(\u00b7|\u03b8mji)\nwhere Mm is the total mass for \u00b5mj , g0(\u00b7|\u03b8mji) denotes the density function to generate data xmji from atom \u03b8mji."}, {"heading": "3.1. The three dependency operators", "text": "Adapting from the dependent Dirichlet process (Lin et al., 2010), the three dependency operators for the NRMs are defined as follows.\nSuperposition of normalized random measures Given n independent NRMs \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5n on X, the superposition (\u2295) is:\n\u00b51 \u2295 \u00b52 \u2295 \u00b7 \u00b7 \u00b7 \u2295 \u00b5n := c1\u00b51 + c2\u00b52 + \u00b7 \u00b7 \u00b7+ cn\u00b5n .\nwhere the weights cm = \u00b5\u0303m(X)\u2211 j \u00b5\u0303j(X) and \u00b5\u0303m is the unnormalized version of \u00b5m.\nSubsampling of normalized random measures Given a NRM \u00b5 = \u2211\u221e k=1 rk\u03b4\u03b8k on X, and a Bernoulli parameter q \u2208 [0, 1], the subsampling of \u00b5, is defined as\nSq(\u00b5) := \u2211 k:zk=1 rk\u2211 j zjrj \u03b4\u03b8k , (8)\nwhere zk \u223c Bernoulli(q) are Bernoulli random variables with acceptance rate q.\nPoint transition of normalized random measures Given a NRM \u00b5 = \u2211\u221e k=1 rk\u03b4\u03b8k on X, the point transition of \u00b5, is to draw atoms \u03b8\u2032k from a transformed base measure to yield a new NRM as T (\u00b5) := \u2211\u221e k=1 rk\u03b4\u03b8\u2032k .\nPoint transitions can be done in different ways with different transition kernels T (\u00b7). In this paper, following (Lin et al., 2010), when inheriting from NRM \u00b5, we draw atoms \u03b8\u2032k from the base measure as \u00b5 conditioned on its current statistics. Other ways of constructing transition kernels are left for further research."}, {"heading": "3.2. Properties of the dependency operators", "text": "The three dependency operators on the NRMs inherit some of the nice properties from the underlying Poisson process. It not only enables quantitatively controlling dependencies introduced after and before the operations, as is shown in (Section 4 Chen et al., 2012), but also maintains a nice equivalence relation between the NRM\u2019s and the corresponding CRM\u2019s. In the following theorem, we will use \u2295\u0303, S\u0303q(\u00b7) and T\u0303 (\u00b7) to denote the three operations on their corresponding CRM\u2019s3.\nTheorem 2 The following time dependent random measures (9) and (10) are equivalent:\n\u2022 Manipulate the normalized random measures:\n\u00b5\u2032m \u223c T (Sq(\u00b5\u2032m\u22121))\u2295 \u00b5m,m > 1. (9)\n\u2022 Manipulate the completely random measures:\n\u00b5\u0303\u2032m \u223c T\u0303 (S\u0303q(\u00b5\u0303\u2032m\u22121))\u2295 \u00b5\u0303m,m > 1. \u00b5\u2032m = \u00b5\u0303\u2032m\n\u00b5\u0303\u2032m(X) , (10)\n3The definitions of \u2295\u0303, S\u0303q(\u00b7) and T\u0303 (\u00b7) are similar to the NRMs\u2019, see (Section 1.5.1 Chen et al., 2012) for details.\nFurthermore, the resulting NRMs \u00b5\u2032m\u2019s give the following:\n\u00b5\u2032m = m\u2211 j=1\n( qm\u2212j \u00b5\u0303j ) (X)\u2211m\nj\u2032=1 (q m\u2212j\u2032 \u00b5\u0303j\u2032) (X)\nTm\u2212j(\u00b5j),m > 1\nwhere qm\u2212j \u00b5\u0303 is the random measure with Le\u0301vy measure qm\u2212j\u03bd(dt,dx) (\u03bd(dt,dx) is the Le\u0301vy measure of \u00b5\u0303). Tm\u2212j(\u00b5) denotes point transition on \u00b5 for (m\u2212j) times ."}, {"heading": "3.3. Reformulation of the proposed model", "text": "Theorem 2 in the last section allows us to first take superposition, subsampling, and point transition on the completely random measures \u00b5\u0303g\u2019s and then do the normalization. Therefore, we make use of Theorem 2 to obtain the dynamic topic model in Figure 1(right) by expanding the recusive formula in (10), which is equivalent to the left one.\nThe generating process of the new model is:\n\u2022 Generating independent CRM\u2019s \u00b5\u0303m for time frame m = 1, \u00b7 \u00b7 \u00b7 , n, following (1).\n\u2022 Generating \u00b5\u2032m for time frame m > 1, following (10).\n\u2022 Generating hierarchical NRM mixtures (\u00b5mj , \u03b8mji, xmji) following (7).\nThe reason for this reformulation is because the inference on the model in Figure 1(left) appears to be infeasible. In general, the posterior of an NRM introduce\ncomplex dependencies between jumps, thus sampling is unclear after taking the three dependency operators.\nOn the other hand, the model in Figure 1(right) is more amenable to computation because the NRMs and the three operators are decoupled. It allows us to first generate the dependent CRM\u2019s, then use the slice sampler introduced in Section 2.2 to sample the posterior of the corresponding NRMs. From now on, we will focus on the model in Figure 1(right). In the next section, we discuss its sampling procedure."}, {"heading": "4. Sampling", "text": "To introduce our sampling method we use the familiar Chinese restaurant metaphor (e.g. (Teh et al., 2006)) to explain key statistics. In this model customers for the variable \u00b5mj correspond to words in a document, restaurants to documents, and dishes to topics. In time frame m,\n\u2022 xmji: the customer i in the jth restaurant.\n\u2022 smji: the dish that xmji is eating.\n\u2022 nmjk: nmjk = \u2211 i \u03b4smji=k ,\nthe number of customers in \u00b5mj eating dish k.\n\u2022 tmjr: the table r in the jth restaurant.\n\u2022 \u03c8mjr: the dish that the table tmjr is serving.\n\u2022 n\u2032mk: n\u2032mk = \u2211 j \u2211 r \u03b4\u03c8mjr=k,\nthe number of customers4 in \u00b5\u2032m eating dish k.\n\u2022 n\u0303\u2032mk: n\u0303\u2032mk = n\u2032mk, the number of customers in \u00b5\u0303\u2032m eating dish k.\n\u2022 n\u0303mk: n\u0303mk = \u2211 m\u2032\u2265m n\u0303 \u2032 m\u2032k,\nthe number of customers in \u00b5\u0303m eating dish k.\nWe will do the sampling by marginalizing out \u00b5mj \u2019s. As it turns out, the remaining random variables that require sampling are smji, n \u2032 mk, as well as\n\u00b5\u0303m = \u2211 k Jmk\u03b4\u03b8k , \u00b5\u0303 \u2032 m = \u2211 k J \u2032mk\u03b4\u03b8k\nNote the tmjr and \u03c8mjr are not sampled as we sample the n\u2032mk directly. Thus our sampler deals with the following latent statistics and variables: smji, n \u2032 mk, Jmk, J \u2032 mk and some auxiliary variables are sampled to support these.\n4the customers in \u00b5\u2032m corresponds to the tables in \u00b5mj . For convenient, we also regard a CRM as a restaurant.\nSampling Jmk. Given n\u0303mk, we use the slice sampler introduced in (Griffin & Walker, 2011) to sample these jumps, with the posterior given in (3). Note that the mass Mm\u2019s are also sampled, see (Sec.1.3 Chen et al., 2012). The resulting {Jmk} are those jumps that exceed a threshold defined in the slice sampler, thus the number of jumps is finite.\nSampling J \u2032mk. J \u2032 mk is obtained by subsampling of {Jm\u2032k}m\u2032\u2264m5. By using a Bernoulli variable zmk,\nJ \u2032mk =\n{ Jm\u2032k if zmk = 1\n0 if zmk = 0.\nWe compute the posterior p(zmk = 1|\u00b5\u0303m, {n\u0303\u2032mk}) to decide whether to inherit this jump to \u00b5\u0303\u2032m or not. These posteriors are given in (Corollary 3 Chen et al., 2012). In practice, we found it mixes faster if we integrate out zmk\u2019s. (Lemma 9 Chen et al., 2012) shows that q-subsampling of a CRM with Le\u0301vy measure \u03bd(\u00b7) results in another CRM with Le\u0301vy measure q\u03bd(\u00b7), thus the jump sizes in the resultant CRM are scaled by q, meaning that J \u2032mk = q m\u2212m\u2032Jm\u2032k. After the sampling of {J \u2032mk}, we normalize it and obtain the NRM \u00b5\u2032m, \u00b5 \u2032 m = \u2211 k rmk\u03b4\u03b8k where rmk =\nJ \u2032mk/ \u2211 k\u2032 J \u2032 mk\u2032\nSampling smji, n \u2032 mk. The following procedures are similar to sampling an HDP. The only difference is that \u00b5mj and \u00b5 \u2032 m are NRMs instead of DPs. The sampling method goes as follows:\n\u2022 Sampling smji: We use a similar strategy as the sampling by direct assignment algorithm for the HDP (Teh et al., 2006), the conditional posterior of smji is:\np(smji = k|\u00b7) \u221d (\u03c9k + \u03c90Mmrmk)g0(xmji|\u03b8k)\nwhere \u03c90 and \u03c9k depend on the corresponding Le\u0301vy measure of \u00b5mj (see (Theorem 2 James et al., 2009)). When \u00b5mj is a DP, then \u03c9k \u221d nmjk and \u03c90 \u221d 1. When \u00b5mj is a NGG, \u03c9k \u221d nmjk \u2212 a and \u03c90 \u221d a(b + vmj)a, where vmj is the introduced auxiliary variables which can be sampled by an adaptive-rejection sampler using the posterior given in (Proposition 1 James et al., 2009).\n\u2022 Sampling n\u2032mk: Using the similar strategy as in (Teh et al., 2006), we sample n\u2032mk by simulating the (generalized) Chinese Restaurant Process, following the prediction rule (the probabilities of\n5 Since all the atoms across {\u00b5\u0303m\u2032} are unique, J \u2032mk is inherited from only one of {Jm\u2032k}.\ngenerating a new table or sitting on existing tables) of \u00b5mk in (Proposition 2 James et al., 2009)."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Power-law in the NGG", "text": "We first investigate the power-law phenomena in the NGG, we sample it using the scheme of (James et al., 2009) and compare it with the DP in Figure 2."}, {"heading": "5.2. Datasets", "text": "We tested our time dependent dynamic topic model on 9 datasets, removing stop-words and words appearing less than 5 times. ICML, JMLR, TPAMI are crawled from their websites and the abstracts are parsed. The preprocessed NIPS dataset is from (Globerson et al., 2007). The Person dataset is extracted from Reuters RCV1 using the query \u201cperson\u201d under Lucene. The Twitter datasets are updates from three sports twitter accounts: ESPN FirstTake (Twitter1), sportsguy33 (Twitter2) and SportsNation (Twitter3) obtained with the TweetStream API (http://pypi.python.org/pypi/tweetstream) to collect the last 3200 updates from each. The Daily Kos blogs (BDT) were pre-processed by (Yano et al., 2009). Statistics for the data sets are given in Table 1.\nIllustration: Figure 3 gives an example of topic evolutions in the Twitter2 dataset. We can clearly see that the three popular sports in the USA, i.e., basketball, football and baseball, evolve reasonably with time. For example, MLB starts in April each year, showing a peak in baseball topic, and then slowly evolves with decreasing topic proportions. Also, in August one foot-\nball topic is born, indicating a new season begins. Figure 4 gives an example of the word probability change in a single topic for the JMLR."}, {"heading": "5.3. Quantitative Evaluations", "text": "Comparisons We first compare our model with two popular dynamic topic models where the author\u2019s own code was available for our use: (1) the dynamic topic model by Blei and Lafferty (Blei & Lafferty, 2006) and (2) the hierarchical Dirichlet process, where we used a three level HDP, with the middle level DP\u2019s representing the base topic distribution for the documents in a particular time. For fair comparison, similar to (Blei & Lafferty, 2006), we held out the data in previous time but used their statistics to help the training of the current time data, this is implemented in the HDP code by Teh. Furthermore, we also tested the proposed model without power-law, which is to use a DP instead of an NGG. We tested our model on the 9 datasets, for each dataset we used 80% for training and held out 20% for testing. The hyperparameters for DHNGG is set to a = 0.2 in this set of experiments with subsampling rate being 0.9, which is found to work well in practice. The topic-word distributions are symmetric Dirichlet with prior set to 0.3. Table 2 shows the test\nlog-likelihoods for all these methods, which are calculated by first removing the test words from the topics and adding them back one by one and collecting the add-in probabilities as the testing likelihood (Teh et al., 2006). For all the methods we ran 2000 burn in iterations, followed by 200 iterations to collect samples. The results are averages over these samples.\nFrom Table 2 we see the proposed model DHNGG works best, with an improvement of 1%-3% in test log-likelihoods over the HDP model. In contrast the time dependent model iDTM of Ahmed & Xing (2010) only showed a 0.1% improvement over HDP on NIPS, implying the superiority of DHNRM over iDTM.\nHyperparameter sensitivity In NGG, there are hyperparameters a and b, where a controls the behavior of the power-law. In this section we study the influences of these two hyperparameters to the model. We varied a among (0.1, 0.2, 0.3, 0.5, 0.7, 0.9) while fixed the subsampling rate to 0.9 in this experiment. We run these settings on all these datasets, the training likelihoods are shown in Figure 5. From these results we consider a = 0.2 to be a good choice in practice.\nInfluence of the subsampling rate One of the distinct features of our model compared to other time dependent topic models is that the dependency comes partially from subsampling the previous time random measures, thus it is interesting to study the impact of subsampling rates to this model. In this experiment, we fixed a = 0.2, and varied the subsampling rate q among (0.1, 0.2, 0.3, 0.5, 0.7, 0.9, 1.0). The results are shown in Figure 6. From Figure 6, it is interesting to see that on the academic datasets, e.g., ICML,JMLR, the best results are achieved when q is approximately equal to 1; these datasets have higher correlations. While for the Twitter datasets, the best results are\nachieved when q is equal to 0.5 \u223c 0.7, indicating that people tend to discuss more changing topics in these datasets."}, {"heading": "6. Conclusion", "text": "We proposed dependent hierarchical normalized random measures. Specifically, we extend the three dependency operations for the Dirichlet process to normalized random measures and show how dependent models on NRMs can be implemented via dependent models on the underlying Poisson processes. Then we applied our model to dynamic topic modeling. Experimental results on different kinds of datasets demonstrate the superior performance of our model over existing models such as DTM, HDP and iDTM."}, {"heading": "Acknowledgments", "text": "We thank the reviewers for their valuable comments and Pinar Yanardag for collecting the Twitter data. NICTA\nis funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Center of Excellence program."}], "references": [{"title": "Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream", "author": ["A. Ahmed", "E.P. Xing"], "venue": "In UAI, pp", "citeRegEx": "Ahmed and Xing,? \\Q2010\\E", "shortCiteRegEx": "Ahmed and Xing", "year": 2010}, {"title": "Forgetting counts: constant memory inference for a dependent hierarchical Pitman-Yor process", "author": ["N. Bartlett", "D. Pfau", "F. Wood"], "venue": "In ICML \u201910", "citeRegEx": "Bartlett et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2010}, {"title": "Dynamic topic models", "author": ["D. Blei", "J. Lafferty"], "venue": "In ICML \u201906", "citeRegEx": "Blei and Lafferty,? \\Q2006\\E", "shortCiteRegEx": "Blei and Lafferty", "year": 2006}, {"title": "Theory of dependent hierarchical normalized random measures", "author": ["C. Chen", "W. Buntine", "N. Ding"], "venue": "Technical Report arXiv:1205.4159, ANU and NICTA,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Posterior simulation of normalized random measure mixtures", "author": ["J.E. Griffin", "S.G. Walker"], "venue": "J. Comput. Graph. Stat.,", "citeRegEx": "Griffin and Walker,? \\Q2011\\E", "shortCiteRegEx": "Griffin and Walker", "year": 2011}, {"title": "Conjugacy as a distinctive feature of the Dirichlet process", "author": ["L.F. James", "A. Lijoi", "I. Pr\u00fcnster"], "venue": "Scand. J. Stat.,", "citeRegEx": "James et al\\.,? \\Q2006\\E", "shortCiteRegEx": "James et al\\.", "year": 2006}, {"title": "Posterior analysis for normalized random measures with independent increments", "author": ["L.F. James", "A. Lijoi", "I. Pr\u00fcnster"], "venue": "Scand. J. Stat.,", "citeRegEx": "James et al\\.,? \\Q2009\\E", "shortCiteRegEx": "James et al\\.", "year": 2009}, {"title": "Controlling the reinforcement in Bayesian non-parametric mixture models", "author": ["A. Lijoi", "R.H. Mena", "I. Pr\u00fcnster"], "venue": "J. R. Stat. Soc. Ser. B,", "citeRegEx": "Lijoi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lijoi et al\\.", "year": 2007}, {"title": "Construction of dependent Dirichlet processes based on Poisson processes", "author": ["D. Lin", "E. Grimson", "J. Fisher"], "venue": "In NIPS", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Dependent nonparametric processes", "author": ["S.N. MacEachern"], "venue": "In Proc. of the SBSS", "citeRegEx": "MacEachern,? \\Q1999\\E", "shortCiteRegEx": "MacEachern", "year": 1999}, {"title": "The dynamic hierarchical Dirichlet process", "author": ["L. Ren", "D.B. Dunson", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Ren et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2008}, {"title": "Statistical topic models for multi-label document classification", "author": ["T. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Technical Report arXiv:1107.2462v2,", "citeRegEx": "Rubin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rubin et al\\.", "year": 2011}, {"title": "Spectral Chinese restaurant processes: Nonparametric clustering based on similarities", "author": ["R. Socher", "A. Maas", "C.D. Manning"], "venue": "In AISTATS", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Shared segmentation of natural scenes using dependent Pitman-Yor processes", "author": ["E.B. Sudderth", "M.I. Jordan"], "venue": "In NIPS", "citeRegEx": "Sudderth and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Sudderth and Jordan", "year": 2008}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Y.W. Teh"], "venue": "In ACL,", "citeRegEx": "Teh,? \\Q2006\\E", "shortCiteRegEx": "Teh", "year": 2006}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the ASA,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Predicting response to political blog posts with topic models", "author": ["T. Yano", "W. Cohen", "N.A. Smith"], "venue": "In Proc. of the NAACL-HLT,", "citeRegEx": "Yano et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yano et al\\.", "year": 2009}, {"title": "Evolutionary hierarchical Dirichlet processes for multiple correlated time-varying corpora", "author": ["J. Zhang", "Y. Song", "C. Zhang", "S. Liu"], "venue": "In KDD", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Dirichlet processes and their variants are popular in recent years, with applications found in diverse discrete domains such as topic modeling (Teh et al., 2006), ngram modeling (Teh, 2006), clustering (Socher et al.", "startOffset": 143, "endOffset": 161}, {"referenceID": 14, "context": ", 2006), ngram modeling (Teh, 2006), clustering (Socher et al.", "startOffset": 24, "endOffset": 35}, {"referenceID": 12, "context": ", 2006), ngram modeling (Teh, 2006), clustering (Socher et al., 2011), and image modeling (Li et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 9, "context": "When modeling dynamic data or data from multiple sources, dependent nonparametric Bayesian models (MacEachern, 1999) are needed in order to harness related or previous information.", "startOffset": 98, "endOffset": 116}, {"referenceID": 15, "context": "Among these models, the hierarchical Dirichlet process (HDP) (Teh et al., 2006) is the most popular one.", "startOffset": 61, "endOffset": 79}, {"referenceID": 10, "context": "To overcome the full exchangeability limitation, several dependent Dirichlet process models have been proposed, for example, the dynamic HDP (Ren et al., 2008), the evolutionary HDP (Zhang et al.", "startOffset": 141, "endOffset": 159}, {"referenceID": 17, "context": ", 2008), the evolutionary HDP (Zhang et al., 2010), and the recurrent Chinese Restaurant process (Ahmed & Xing, 2010).", "startOffset": 30, "endOffset": 50}, {"referenceID": 5, "context": "Dirichlet processes are used because of simplicity and conjugacy (James et al., 2006).", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "Dependent DPs have also been constructed using the underlying Poisson processes (Lin et al., 2010).", "startOffset": 80, "endOffset": 98}, {"referenceID": 14, "context": ", in images (Sudderth & Jordan, 2008), in topic-word distributions (Teh, 2006) and in document topic (label) distributions (Rubin et al.", "startOffset": 67, "endOffset": 78}, {"referenceID": 11, "context": ", in images (Sudderth & Jordan, 2008), in topic-word distributions (Teh, 2006) and in document topic (label) distributions (Rubin et al., 2011).", "startOffset": 123, "endOffset": 143}, {"referenceID": 1, "context": "For example, Bartlett et al. (2010) proposed a dependent hierarchical PitmanYor process that only allows deletion of atoms, while Sudderth & Jordan (2008) construct the dependent Pitman-Yor process by only allowing dependencies between atoms.", "startOffset": 13, "endOffset": 36}, {"referenceID": 1, "context": "For example, Bartlett et al. (2010) proposed a dependent hierarchical PitmanYor process that only allows deletion of atoms, while Sudderth & Jordan (2008) construct the dependent Pitman-Yor process by only allowing dependencies between atoms.", "startOffset": 13, "endOffset": 155}, {"referenceID": 6, "context": "In this paper, we use a larger class of stochastic processes called normalized random measures with independent increments (NRM) (James et al., 2009).", "startOffset": 129, "endOffset": 149}, {"referenceID": 8, "context": "Given this, following (Lin et al., 2010), we analogically define superposition, subsampling and point transition on these normalized random measures, and construct a time dependent hierarchical model for dynamic topic modeling.", "startOffset": 22, "endOffset": 40}, {"referenceID": 3, "context": "All proofs and some extended theories are available in (Chen et al., 2012).", "startOffset": 55, "endOffset": 74}, {"referenceID": 6, "context": "This background on random measures follows (James et al., 2009).", "startOffset": 43, "endOffset": 63}, {"referenceID": 7, "context": "Generalized Gamma processes (Lijoi et al., 2007) (GGP) are random measures with the L\u00e9vy measure", "startOffset": 28, "endOffset": 48}, {"referenceID": 7, "context": "Proposition 1 ((Lijoi et al., 2007)) Let Kn be the number of components induced by the NGG with parameters a an b or the Dirichlet process with total mass M .", "startOffset": 15, "endOffset": 35}, {"referenceID": 15, "context": "In the next section, we propose a dynamic topic model which extends two major advances of the Dirichlet process: the HDP (Teh et al., 2006) and the dependent Dirichlet process (Lin et al.", "startOffset": 121, "endOffset": 139}, {"referenceID": 8, "context": ", 2006) and the dependent Dirichlet process (Lin et al., 2010), to normalized random measures.", "startOffset": 44, "endOffset": 62}, {"referenceID": 3, "context": "In this paper, P0 is the Dirichlet distribution, \u03b70 is the set of In NGG, b can be absolved into M , thus we usually set b = 1, see (Chen et al., 2012) for detail.", "startOffset": 132, "endOffset": 151}, {"referenceID": 8, "context": "where the three dependency operators superposition (\u2295), subsampling (S(\u00b7)) with acceptance rate q, and point transition (T (\u00b7)) are generalized from those of Dirichlet process (Lin et al., 2010).", "startOffset": 176, "endOffset": 194}, {"referenceID": 8, "context": "Adapting from the dependent Dirichlet process (Lin et al., 2010), the three dependency operators for the NRMs are defined as follows.", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": "In this paper, following (Lin et al., 2010), when inheriting from NRM \u03bc, we draw atoms \u03b8\u2032 k from the base measure as \u03bc conditioned on its current statistics.", "startOffset": 25, "endOffset": 43}, {"referenceID": 15, "context": "(Teh et al., 2006)) to explain key statistics.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "\u2022 Sampling smji: We use a similar strategy as the sampling by direct assignment algorithm for the HDP (Teh et al., 2006), the conditional posterior of smji is: p(smji = k|\u00b7) \u221d (\u03c9k + \u03c90Mmrmk)g0(xmji|\u03b8k) where \u03c90 and \u03c9k depend on the corresponding L\u00e9vy measure of \u03bcmj (see (Theorem 2 James et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 15, "context": "\u2022 Sampling nmk: Using the similar strategy as in (Teh et al., 2006), we sample nmk by simulating the (generalized) Chinese Restaurant Process, following the prediction rule (the probabilities of 5 Since all the atoms across {\u03bc\u0303m\u2032} are unique, J \u2032 mk is inherited from only one of {Jm\u2032k}.", "startOffset": 49, "endOffset": 67}, {"referenceID": 6, "context": "Power-law in the NGG We first investigate the power-law phenomena in the NGG, we sample it using the scheme of (James et al., 2009) and compare it with the DP in Figure 2.", "startOffset": 111, "endOffset": 131}, {"referenceID": 16, "context": "The Daily Kos blogs (BDT) were pre-processed by (Yano et al., 2009).", "startOffset": 48, "endOffset": 67}, {"referenceID": 15, "context": "log-likelihoods for all these methods, which are calculated by first removing the test words from the topics and adding them back one by one and collecting the add-in probabilities as the testing likelihood (Teh et al., 2006).", "startOffset": 207, "endOffset": 225}, {"referenceID": 14, "context": "log-likelihoods for all these methods, which are calculated by first removing the test words from the topics and adding them back one by one and collecting the add-in probabilities as the testing likelihood (Teh et al., 2006). For all the methods we ran 2000 burn in iterations, followed by 200 iterations to collect samples. The results are averages over these samples. From Table 2 we see the proposed model DHNGG works best, with an improvement of 1%-3% in test log-likelihoods over the HDP model. In contrast the time dependent model iDTM of Ahmed & Xing (2010) only showed a 0.", "startOffset": 208, "endOffset": 566}], "year": 2012, "abstractText": "We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. The dependency arises via superposition, subsampling and point transition on the underlying Poisson processes of these measures. The measures used include normalised generalised Gamma processes that demonstrate power law properties, unlike Dirichlet processes used previously in dynamic topic modeling. Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process. Experiments performed on news, blogs, academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models.", "creator": "LaTeX with hyperref package"}}}