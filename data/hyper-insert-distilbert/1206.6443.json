{"id": "1206.6443", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Isoelastic Agents and Wealth Updates in Machine Learning Markets", "abstract": "hence recently, prediction value markets have emerged shown by considerable promise times for developing flexible mechanisms for applying machine centered learning. in this paper, agents with isoelastic utilities are considered. ) it furthermore is typically shown that the discount costs associated broadly with homogeneous comparison markets use of agents involved with two isoelastic utilities also produce equilibrium data prices both corresponding themselves to comparing alpha - l mixtures, those with a seemingly particular form of mixing information component relating to defining each comparison agent'natural s actual wealth. we notably also demonstrate that wealth accumulation for logarithmic clients and other isoelastic agents ( acquired through payoffs on initial prediction platforms of training rate targets ) who can implement both bayesian model updates and generalized mixture / weight updates by each imposing different market value payoff structures. an iterative algorithm is given for market equilibrium computation. we demonstrate that two inhomogeneous markets of agents with isoelastic utilities outperform state of well the art aggregate hierarchical classifiers schemes such as random forests, etc as for well as finite single classifiers ( neural gradient networks, interactive decision logic trees ) exist on a number thereof of robust machine / learning application benchmarks, evidence and simulations show conclude that isoelastic combination acquisition methods are generally better than their logarithmic counterparts.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (734kb)", "http://arxiv.org/abs/1206.6443v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Tue, 4 Sep 2012 17:50:18 GMT  (736kb)", "http://arxiv.org/abs/1206.6443v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.GT stat.ML", "authors": ["amos j storkey", "jono millin", "krzysztof geras"], "accepted": true, "id": "1206.6443"}, "pdf": {"name": "1206.6443.pdf", "metadata": {"source": "META", "title": "Isoelastic Agents and Wealth Updates in Machine Learning Markets", "authors": ["Amos J. Storkey", "Jonathan J. Millin", "Krzysztof J. Geras"], "emails": ["a.storkey@ed.ac.uk", "j.j.millin@sms.ed.ac.uk", "k.j.geras@sms.ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "This paper addresses the problem of classifier aggregation through the use of Machine Learning Markets. In supervised machine learning, many algorithms use simple averaging, weighted averaging, mixtures, mixing and log opinion pools for combining a variety of classifiers to form aggregate classifiers. On the other hand, prediction markets have been used for aggregation of simple beliefs in other fields (the market price is used as an aggregate probability). Some simple forms of theoretical prediction markets have been shown to be equivalent to weighted averaging (Barbu & Lay, 2011; Storkey, 2011). However, more generally, we can formulate prediction market mechanisms on the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nbasis of expected utility theory. We show that a whole spectrum of aggregation methods are then available, including and extending those provided by \u03b1-mixtures. We demonstrate the empirical benefits of these aggregation methods over those discussed above.\nA number of authors have recently examined the possibilities of market mechanisms for implementing machine learning methods. Desirable properties of markets include the fact that agents act independently, markets are inherently parallelisable and the fact that the form of models produced by markets is more general than explicitly defined probabilistic models. Market based machine learning approaches are being shown to be consistent with standard probabilistic machine learning formalisms (Storkey, 2011) as well providing regret bounds for market makers (Chen & Wortman Vaughan, 2010; Abernethy et al., 2011). Markets are also a well studied and pervasive part of our social and computational infrastructure.\nWe envisage the possibility of setting up online prediction markets for large scale multivariate prediction problems in which different algorithms compete. This generalises prediction markets, which usually focus on disconnected discrete events, and relates to other recent work in utilising algorithm crowdsourcing for machine learning (Abernethy & Frongillo, 2011). It would provide an alternative to, say, the current interest in competition and challenge environments (e.g. Netflix, Kaggle, PASCAL2), where individuals compete for the best personal performance. Experience in these domains has suggested that it is common for individual algorithms to be outperformed by competitors grouping together to produce model combinations (Bell & Koren, 2007). This mirrors the experience regarding aggregation of expert predictions (Dani et al., 2006).\nThe main novel contributions of this paper are:\n\u2022 Extending the set of standard agents from logarithmic agents and exponential agents to also include the various forms of isoelastic agents, and to show such agents can reproduce and extend the \u03b1-mixtures framework (Amari, 2007).\n\u2022 Demonstrating that online and batch agent wealth updates are equivalent to Bayesian posterior updates and mixture coefficient updates re-\nspectively.\n\u2022 Equating the multi-agent equilibrium (fixedpoint) market with a divergence minimisation. The result is an iterative process for establishing the market equilibrium values.\n\u2022 Demonstrating that Machine Learning Markets outperform individual strong classifiers when presented with equivalent data. Showing that the markets improve on state of the art classifiers, such as random forests, and provide better logprobability performance on classification test sets.\n\u2022 Demonstrating that inhomogeneous sets of isoelastic agents produce better performance than logarithmic agents. Hence aggregation by simple mixtures can be improved upon."}, {"heading": "2. Other Related Work", "text": "A number of previous papers address machine learning and prediction markets. In (Lay & Barbu, 2010) and (Barbu & Lay, 2011), the authors consider agents endowed with betting functions, and do experimental tests in the context of classifiers (leaves of the trees of a random forest) trained on bootstrapped samples. In (Storkey, 2011) the author develops a complementary approach, utilizing beliefs and utilities of individual agents. He shows that a variety of machine learning model combination methods, including model averaging, product models, factor graphs etc. can be implemented using Machine Learning Markets. In (Chen & Wortman Vaughan, 2010) the market is defined with respect to a global cost function, and they demonstrate achieving \u2018no regret\u2019 learning using information markets. From another perspective, Agrawal et al. (Agrawal et al., 2010) looked at the equilibrium conditions of prediction markets in various situations of matching buyers and sellers. The available information is important for market conditions. This was discussed in (Jumadinova & Dasgupta, 2011), where they used a multi-agent system to examine the various dependencies on information reliability, rate etc. It is the ability of prediction markets to aggregate belief that is key to their potential (Pennock & Wellman, 1997; Ottaviani & S\u00f8rensen, 2007), and much of the experimental work on prediction markets backs that up (Ledyard & Hanson, 2008).\nThe potential of prediction markets has been considered for some time. In (Arrow et al., 2008; Manski, 2006; Wolfers & Zitzewitz, 2004) the authors discuss the capability of practical prediction markets to capture accurate probabilities. In (Dani et al., 2006) the authors compared a number of different mechanisms for expert aggregation including a simple prediction market approach. Different market designs have different features, and ensuring good prediction market design with sufficient fluidity (Brahma et al., 2010)\nwill be critical for efficiently reaching equilibrium. In (Tseng et al., 2010) the authors examine the statistical properties of market agent models, whereas in (Lee & Moretti, 2009) the authors consider prediction markets in the context of Bayesian learning."}, {"heading": "3. Notation", "text": "Machine Learning Markets use prediction market mechanisms to generate machine learning models via the market price. The basic concepts and notation are now introduced."}, {"heading": "3.1. Goods", "text": "We consider a set of market goods enumerated by k = 1, 2, . . . NG, each corresponding to a specific outcome of a discrete random variable, denoted k. The good k will pay out one unit of currency in the event that the outcome for k is k. The market has a commonly agreed cost ck (0 < ck < 1) for each good k and we collect the costs into the cost vector c = (c1, c2, . . . , cNG)\nT . The cost vector c will be interpreted as the aggregate probabilistic belief provided by the market: the probability1 of k occurring is ck."}, {"heading": "3.2. Agent Actions", "text": "A number of agents, enumerated by i = 1, 2, . . . , NA, act in the market. Each agent has wealth Wi, and invests (or risks) an amount rik in stock k. We collect ri = (ri1, ri2, . . . , riNG) T . A no arbitrage assumption2\nimplies that \u2211 k ck = 1, meaning a probabilistic interpretation of ck is reasonable. Likewise, we can require all agents to spend all their wealth: if any agent wants to keep it as a risk free investment, that agent can simply purchase one of each stock instead. Hence, without loss of generality, we have\u2211\nk\nrik = Wi. (1)\nAgain without loss of generality, we will measure wealth in units such that the total wealth across all agents is 1: \u2211 iWi = 1. Hence \u2211 ik rik = 1.\nEach agent is also endowed with a utility function Ui(W ) denoting the utility of having wealth W . For the purposes of this paper we will only consider concave utility functions. Finally, each agent has a belief Pi, where Pi(k) denotes the probabilistic belief, for that particular agent i, that the outcome of k will be k. Necessarily, as k enumerates all possible outcomes,\u2211 k Pi(k) = 1 for all agents.\n1Strictly, the probability is ck divided by the unit of payout. This ensures dimensional consistency, both here and elsewhere.\n2No arbitrage: there is no opportunity for a risk free gain. If \u2211 k ck 6= 1 an agent can make a risk free win by buying (or selling) one of each stock which has a sure return (or debt) of 1 unit."}, {"heading": "3.3. Market", "text": "The agents jointly act in a market. The market transactions are subject to the macroscopic constraint\nNA\u2211 i=1 rik ck = NA\u2211 i=1 Wi = 1\u21d2 NA\u2211 i=1 rik = ck (2)\nwhere NA is the number of agents. This states that wealth must be conserved in the market: the total payout were that item to occur matches the total original wealth. Here rik/ck is the amount of good k bought by agent i (the amount invested divided by cost) and so is the amount received if event k occurs."}, {"heading": "3.4. Summary Table", "text": "i \u2192 agent k \u2192 good/outcome NA, NG \u2192 #agents/goods ri \u2192 investment\nWi \u2192 Wealth Ui \u2192 Utility c \u2192 cost vector Pi(k) \u2192 agent belief"}, {"heading": "4. Utility Maximisation", "text": "In Machine Learning Markets the market price c defines a probability distribution over possible outcomes, which can be used for prediction. The multiclass prediction problem is the focus for this paper."}, {"heading": "4.1. Investment Functions", "text": "A given utility function Ui induces a given investment function r\u2217i (Wi, c) via expected utility maximisation:\nr\u2217i = r \u2217 i (Wi, c) = arg max\nri \u2211 k Pi(k)Ui ( rik ck ) s.t.\n\u2211 k rik = Wi. (3)\nwhere we have used the fact that every agent must spend their whole wealth (1). The investment function indicates the amount an agent ideally would wish to invest in each good, given the costs c. The market constraints may mean this desire cannot be satisfied: the agent must find a buyer or seller to realise this desire. The optimum of (3) is given by\nr\u2217ik = ck(U \u2032 i) \u22121 ( \u03bbi(c)\nck Pi(k)\n) (4)\nwhere U \u2032 is the derivative of U and \u03bbi(c) is a Lagrange multiplier such that \u2211 k r \u2217 ik = Wi is satisfied.\nIn general, the equation for \u03bbi cannot be explicitly solved. However, for a number of utilities the investment function is analytic. Table 1 lists some important utility functions (exponential, logarithmic and isoelastic) and their corresponding investment functions.\nThe class of isoelastic functions are a very useful set of utilities. The isoelastic utilities get their name as they all have investment functions that are linear in the current wealth; this property is called the isoelastic property. Isoelastic utility functions are parameterised by \u03b7 > 0. Strictly, the logarithmic utility is also an\nisoelastic utility with the limiting value of \u03b7 = 1."}, {"heading": "4.2. Market Equilibrium", "text": "The Market must satisfy the market constraint (2). At the same time each agent attempts to maximise their individual utility. It is well known (Arrow & Debreu, 1954) that, if the individual utilities are concave (as is the case in this paper), there is a unique fixed price point for which agents all attain their maximum utility and the market constraints are satisfied. This is called the market equilibrium. However, the existence of a fixed point does not establish a means of obtaining it. The question of how a market might equilibrate formed part of the early discussion regarding equilibria, and led to Walras\u2019 concept of ta\u0302tonnement. This idea, as communicated by Samuelson (Samuelson, 1947), was that prices are differentially changed in the direction of the excess or deficit demand. However, the constraints on this formalism meant it was not established as a general purpose procedure. The are many analyses that involve formulating convex optimisation approaches or auction processes for obtaining market equilibria, e.g. (Deng et al., 2002; Devanur et al., 2008; Ye, 2006) \u2013 see (Vazirani, 2007) for more details. These algorithms typically require a complete optimisation procedure. Two exceptions are (Cole & Fleischer, 2007; Fleischer et al., 2008) which develop on the idea of ta\u0302tonnement.\nIn this paper an iterative ta\u0302tonnement-like approach is used for establishing market equilibria. Consider the fact that \u2211 k ck = 1 and \u2211 k( \u2211 i rik) = 1 means\nthat both ck and \u2211 i rik take probabilistic form. At\nequilibrium, we have \u2211 i rik = ck (see (2)) when all agents are allocated their optimal demand. Away from equilibrium there will be an excess or deficit demand in different goods, which is evident in the difference between \u2211 i rik and ck. Consider the KL divergence\nKL(c|| \u2211 i ri). This is minimised and zero only at equilibrium. To minimise this KL divergence we use Algorithm 1. This algorithm only terminates when the (unique) equilibrium is reached, when c is the equilibrium price. In all our empirical tests the equilibrium was always reached. Each pass of the algorithm is naively O(NA\u00d7NG) \u2013 each update is computationally equivalent to a mixture model update. In our experiments equilibria were reached in between 5 and 15 iterations.\nSatisfaction of the market constraints for given buying functions, or equivalently minimisation of KL(c|| \u2211 i ri), defines a fixed point that is a market equilibrium. These market equilibria can be explicitly computed for various agent utilities. For a market of identical logarithmic agents or a market of identical exponential agents we have\nck =\n\u2211 iWiPi(k)\u2211\niWi and ck \u221d NA\u220f i=1 Pi(k) 1 NA , (6)\nExponential U(W ) = \u2212 exp(\u2212W ) rik(Wi, c) = WiNG + ck log Pi(k) ck \u2212 1NG \u2211 k\u2032 ck\u2032 log Pi(k \u2032) ck\u2032\nLogarithmic U(W ) = { log(W ) for W > 0 \u2212\u221e otherwise rik(Wi, c) = WiPi(k)\nIsoelastic U(W ) = W 1\u2212\u03b7i\u22121 1\u2212\u03b7i rik = Wi  (ck) \u03b7i\u22121\u03b7i (Pi(k)) 1\u03b7i\u2211 k\u2032(ck\u2032) \u03b7i\u22121 \u03b7i (Pi(k\u2032)) 1 \u03b7i \nTable 1. Various utility functions and their corresponding investment functions. For the isoelastic utilities 0 < \u03b7.\nAlgorithm 1 Market Equilibrium\ninitialise c, initialise a (e.g. a = 0.1) define stopping criterion repeat\nCompute optimal ri for each agent ignoring market constraint set (for normalising Z1)\ncnewk = 1\nZ1 (\u2211 i rik ck )1\u2212a ck (5)\nif KL(cnew|| \u2211 i ri(c new)) \u2212 KL(c|| \u2211 i ri(c)) < 0 then discard cnewk and increase a.\nend if until KL(cnew|| \u2211 i ri(c new)) <\nrespectively. For logarithmic agents this takes the form of a model average or mixture of the agents\u2019 beliefs; for exponential agents it is a log opinion pool of beliefs. See (Storkey, 2011) for discussion of these. The term homogeneous will be used to refer to markets where all agents have identical utility functions."}, {"heading": "4.3. Equilibria for Isoelastic Agents", "text": "We cannot explicitly obtain the equilibrium for sets of isoelastic agents with \u03b7 6= 1, and so the optimisation procedure of the previous section needs to be employed. However we can say something about the form of solution we obtain for isoelastic agents. For example we can obtain the following equilibrium for homogeneous markets of isoelastic agents, all having the same \u03b7. This solution is not closed form:\nck = [\u2211 i ViPi(k) 1 \u03b7 ]\u03b7 (7)\nwhere Vi = Wi/Zi and Zi is the implicit solution to\nZi = \u2211 k \u2211 j Wj Zj Pj(k) 1 \u03b7 \u03b7\u22121 Pi(k) 1\u03b7 . (8) Equation (7) is precisely the equation for \u03b1-mixtures (Amari, 2007; Wu, 2009), but where Vi is defined implicitly in terms of a set of weights (or wealths) Wi. This particular expression for \u03b1-mixtures is interesting because of the isoelastic property: if a single component from the \u03b1-mixture is replaced by two iden-\ntical components with weight Wi/2, it results in exactly the same model. The precise number of components/agents does not matter, just the total wealth associated with each belief, regardless of the how many components share it. Hence, the wealths provide the natural measure for the mixing coefficients of an \u03b1mixture. This property does not hold for the Vi in Equation (7), except if \u03b7 = 1, when Vi = Wi.\nFigure 1 illustrates the distinction between an isoelastic market combination and a logarithmic combination (which is equivalent to a standard mixture). In the isolelastic market, for \u03b7 > 1, the individual beliefs are \u2018squashed\u2019 (raised to a fractional power) before being mixed, and are then \u2018unsquashed\u2019 again after mixing. The result of this is the areas of agreement between agents are emphasised relative to a standard mixture.\nAs an alternative to (7), we can also write ck = \u2211 i WiP \u03b7 ik(c) (9) where P \u03b7ik(c) is defined as\nP \u03b7ik(c) = ck\n( Pi(k) ck )1/\u03b7 \u2211 k\u2032 ck\u2032 ( Pi(k\u2032) ck\u2032\n)1/\u03b7 . (10) Again this is not closed form, but expresses the equilibrium ck as a weighted sum of the effective beliefs P \u03b7 ik that are associated with each agent once the impact of the combination with rest of the market is taken into account. Each effective belief is weighted by the agent\u2019s wealth Wi before aggregation.\nIn the discussion above, all the agents have utilities with the same value of \u03b7. This is a homogeneous market structure. However there is no restriction to homogeneity in the context of Machine Learning Markets. Here an inhomogeneous market structure can be used where different agents have different \u03b7 values. Hence the equilibria of Machine Learning Markets can implement a broader set of combination processes than standard \u03b1-mixtures, and so generalises the \u03b1-mixture formalism. We show in Section 6 that the use of an inhomogeneous market structure provides improvements over standard mixtures for a variety of standard machine learning benchmarks."}, {"heading": "5. Training and Wealth Allocation", "text": "We consider a classification problem, with a training dataset DTr consisting of covariates x\nn and corresponding classifications kn. We wish to learn the relationship between variables x and classifications k, so that given a test point x\u2217 we can provide a predictive distribution for the value that the corresponding class label k\u2217 will take. A test dataset DTe is used to evaluate the final performance."}, {"heading": "5.1. Agent Beliefs and Wealth Updates", "text": "Each agent has an individual belief Pi(k). In this paper we consider agents that have used standard machine learning algorithms on a training data set DTr to derive their beliefs.\nEach agent has a specific wealth Wi. The wealth affects the market influence of that agent. An agent\u2019s action in the market changes the wealth of the agent: the change of wealth after a single investment ri, and a return on that purchase is given by rik\u2217/ck\u2217 where k \u2217 denotes the index of the event that occurs. The agents make purchases in predictions across the whole training dataset, trading with the other agents. Payouts are then made and each agent makes a return on the investment. Agents that invest well (relatively) gain wealth, whereas agents that invest in goods that don\u2019t pay out lose wealth and hence market influence. This process can be repeated for a number of epochs.\nFor analytic purposes, we consider two wealth update schemes: an online and a batch scheme. All the empirical analyses are done using the batch scheme."}, {"heading": "5.1.1. Online: Bayesian Model Updates", "text": "Consider the case of all agents starting with wealth 1/NA, whereNA is the number of agents. Let T denote the first t data items and DT denote the ordered set of those data items, with Dt being the tth item. The total number of items is NTr. Let kt be the target for the tth data point.\nIn the online setting, the agent purchases predictions on the data points t = 1, 2, . . . , NTr one at a time. At\neach time point the outcome is then revealed and all bets are cashed in. This is an online update scheme. Let W ti denote the wealth of agent i after the target for data point t is known and the winnings are received.\nFor isoelastic agents (including logarithmic agents when \u03b7 = 1), at each data point t, each agent bets the whole wealth W ti and gains a return of rikt/ckt , leading to\nW t+1i = W ti P \u03b7 ikt (c)\u2211 i\u2032W t i\u2032P \u03b7 i\u2032kt (c) (11)\nusing (10) and (9), and rik from Table 1. If we equate W t+1i with the concept P (i|Dt+1) then this leads to\nW t+1i = P (i|DT+1) = P \u03b7iktP (i|DT )\u2211 i\u2032 P \u03b7 i\u2032kt P (i\u2032|DT ) (12) which gives the Bayesian update rule on observation of a new data point at time t for a single agent likelihood P \u03b7ikt . For logarithmic agents the belief P \u03b7 ikt\n= Pi(kt) and this just reduces to a Bayes update, treating each agent as an independent probabilistic model. For isoelastic agents it is still a Bayes update, but where Bayes rule uses the effective beliefs P \u03b7ikt as the component distributions, instead of the individual agent belief Pi(kt). The equilibrium cost ct for any item at time t after seeing data DT is a standard Bayesian model average, given by (9), as the weights are the posterior probabilities associated with each agent (12).\nThe fact that these Bayesian updates occur for logarithmic utilities (or equivalently log-loss) has been discussed in a different context in (Beygelzimer et al., 2012). Establishing the extension of this rule for isoelastic agents is a novel generalisation."}, {"heading": "5.1.2. Batch: Mixing Coefficient Updates", "text": "Bayesian model averaging is appropriate if we interpret each agent as an alternative competing hypothesis, where ultimately one agent has the correct belief. However, in many, or even most situations (see e.g. (Domingos, 2000; 1997; Minka, 2002) for a continued discussion), we may believe that the most appropriate model is a combination of beliefs rather than a single one. In those settings Bayesian model averaging is inappropriate. Rather, we may believe the data is best described by a mixture of probabilities, and we wish to determine optimal mixing proportions.\nConsider, instead, splitting the agent wealth equally across test cases, and requiring the agents to place bets on all test cases at once. In this case the wealth updates are equivalent to a single step of the mixture component updates. Specifically the return from considering data item t is\nP (i|t) def= WiP\n\u03b7 ikt (c)\u2211 i\u2032Wi\u2032P \u03b7 i\u2032k(c) . (13)\nEquation (13) is precisely the form of responsibility\ncalculation for a mixture model, but where we have used the effective beliefs for the isoelastic agents. The update rule Wi = \u2211 t P (i|t), which is simply the accumulated return over the whole dataset, matches the update rule for mixture coefficients.\nHence across the whole range of isoelastic agents, initialising agents with equal wealth and repeatedly applying the batch update rule reproduces the usual mixture coefficient updates applied to an \u03b1-mixture model. However multiple different values of \u03b1 can be used for different agents."}, {"heading": "6. Results", "text": "Machine Learning Markets with logarithmic and isoelastic agents were compared with decision trees, neural networks and random forests on a number of UCI datasets3. Experimental data was split into two sets, with 2/3 of the data being used for training and 1/3 for testing, with a maximum total dataset size of 3200 items. For large multiclass data, we used the Letter Recognition dataset. The markets of random forests were built using the Matlab random forest implementation, treebagger, with 20 decision trees pruned by requiring a minimum of ten of observations per tree leaf. Individual decision trees were extracted from the random forest after it has been trained on all of the training data, and were used to generate each of the 20 agents\u2019 beliefs. In our comparisons, market wealths were adapted on the complete training set with 1 training epoch. 30 iterations of each test were performed\n3Available at: http://archive.ics.uci.edu/ml/\nto generate meaningful statistics, with data being randomly permuted before each test. The same series of random seeds were used for each iteration of each test in order to fairly compare different utility functions. Wealth updates were performed using the batch mechanism described in Section 5.1.2.\nInhomogeneous isoelastic markets were created by sampling values for \u03b7 using (\u03b7 \u2212 1) \u223c \u0393(k, \u03b8), with shape parameter k = 3 and scale parameter \u03b8 = 1. This produces a diverse set of \u03b7 values for different agents, while ensuring \u03b7 > 1. More risk averse utility functions (\u03b7 > 1) were chosen as they emphasise regions of agreement between agents rather than regions of disagreement (see (7) and Figure 1).\nThe primary purpose of this analysis is to test different possible probabilistic combination methods against other single classifiers. Hence, we compute test loglikelihoods as the main evaluation metric. This is given\nby LL = \u2211T t log(P (kt|xt)), where P (kt|xt) is the probability of the true value kt given covariates xt. For a market we have P (kt|xt) = ckt where c is the equilibrium cost from the market given all agents in the market know the covariates x. Logs of test likelihood ratios are used when different models are compared."}, {"heading": "6.1. Relative Classifier Performance", "text": "We compare the methods used in Machine Learning Markets against other standard classifiers. Machine Learning Markets can utilise any probabilistic classifier as the beliefs for each agent, and so we compare a number of single classifiers with a market of those classifiers. The exception is for random forests, which is\nalready an aggregate classifier, and there we compare a random forest, against a market of trees that match the trees in the random forest. An inhomogeneous isoelastic market with batch updates is used for all comparisons. Table 2 presents the results along with standard deviations. The market approaches outperform all the standard approaches on all datasets, with clear statistical significance. This includes improvements over random forests. Interestingly, the market of neural networks performed better than the market of random trees in some settings."}, {"heading": "6.2. Isoelastic versus Logarithmic Markets", "text": "Figure 2 shows 1NTe log ( LISO LLOG ) , where NTe is the number of test points, and L denotes the test likelihood. This is referred to as the (scaled) log of the test likelihood-ratio between the inhomogeneous isoelastic market and the logarithmic market predictions.\nIn general, the log likelihood-ratios are positive, meaning that isoelastic markets have higher test performance than logarithmic markets. Further, they are positive to one standard deviation (the black dots in Figure 2), implying that isoelastic markets perform better (paired t-test p < 0.01 in all cases)."}, {"heading": "6.3. Varying the Parameter of Isoelasticity", "text": "Figure 3 shows that the test log-likelihood varies for homogeneous isoelastic markets with varying \u03b7. Searching for a good \u03b7 via a cross validation process can be computationally expensive. An alternative approach is to randomly allocate an \u03b7 to each of the agents, producing inhomogeneous markets, and perform market updates in order to tune the mixing proportions for the different agents. Figure 3 demonstrates that the inhomogeneous market provides results about as good as if we has known the optimal test \u03b7 but with significantly less computational cost."}, {"heading": "6.4. Batch Wealth Updates and Performance", "text": "Figure 4 shows that adapting wealth improves the test log-likelihood. This is true for both logarithmic and isoelastic utility functions. We have noticed that wealth adaptation does not make a significant difference on accuracy for small multiclass datasets, however, improvements in accuracy are observed on the large multiclass Letter Recognition data. In general, learning is more beneficial in cases where some agents are significantly poorer performers than others (e.g. they overfit, or are trained on biased data etc.)."}, {"heading": "7. Discussion", "text": "Machine Learning Markets can reflect many of the properties of principled probabilistic methods in handcrafted probabilistic models. The design of Machine Learning Markets allows the implicit definition of powerful models. We show that Baysesian model averaging and mixture model learning can be naturally implemented using market mechanisms. We show that different utility functions have a significant effect on the market combination results, and that isoelastic utilities are more effective in a number of tests than utilities that implement standard mixtures. The benefits of inhomogeneous markets of isoelastic agents over state of the art classifiers has been demonstrated, and the understanding of isoelastic utilities as encoding a generalisation of \u03b1-mixtures has been developed.\nThere are two immediate extensions to this work to be considered. First, the adaptability of markets means that tests of this approach in the context of dataset shift, or non-stationary environments, would be valuable. Another angle worthy of investigation is the mixture of expert setting. In the context of this paper, all agents had beliefs about the whole predictive dataset. It is likely that an agent will also learn about its own performance in the market: assessing what situations it is likely to generate a positive return on. Such agents would allocate different resources to different conditional situations akin to a mixture of experts."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Recently, prediction markets have shown<lb>considerable promise for developing flexible<lb>mechanisms for machine learning. In this pa-<lb>per, agents with isoelastic utilities are con-<lb>sidered. It is shown that the costs asso-<lb>ciated with homogeneous markets of agents<lb>with isoelastic utilities produce equilibrium<lb>prices corresponding to alpha-mixtures, with<lb>a particular form of mixing component relat-<lb>ing to each agent\u2019s wealth. We also demon-<lb>strate that wealth accumulation for logarith-<lb>mic and other isoelastic agents (through pay-<lb>offs on prediction of training targets) can im-<lb>plement both Bayesian model updates and<lb>mixture weight updates by imposing different<lb>market payoff structures. An iterative algo-<lb>rithm is given for market equilibrium compu-<lb>tation. We demonstrate that inhomogeneous<lb>markets of agents with isoelastic utilities out-<lb>perform state of the art aggregate classifiers<lb>such as random forests, as well as single clas-<lb>sifiers (neural networks, decision trees) on a<lb>number of machine learning benchmarks, and<lb>show that isoelastic combination methods are<lb>generally better than their logarithmic coun-<lb>terparts.", "creator": "LaTeX with hyperref package"}}}