{"id": "1206.6446", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Agglomerative Bregman Clustering", "abstract": "this classification manuscript subsequently develops especially the invariant theory of chaotic agglomerative clustering spectra with bregman temporal divergences. geometric smoothing coding techniques are developed to rapidly deal uniquely with progressively degenerate clusters. to allow conditions for stationary cluster smoothed models based on exponential families, with successively overcomplete continuous representations, bregman divergences are developed below for some nondifferentiable convex functions.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (481kb)", "http://arxiv.org/abs/1206.6446v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matus telgarsky", "sanjoy dasgupta"], "accepted": true, "id": "1206.6446"}, "pdf": {"name": "1206.6446.pdf", "metadata": {"source": "CRF", "title": "Agglomerative Bregman Clustering", "authors": ["Matus Telgarsky", "Sanjoy Dasgupta"], "emails": ["mtelgars@cs.ucsd.edu", "dasgupta@cs.ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "Starting with points {xi}mi=1 and a pairwise merge cost \u2206(\u00b7, \u00b7), classical agglomerative clustering produces a single hierarchical tree as follows (Duda et al., 2001).\n1. Start with m clusters: Ci := {xi} for each i.\n2. While at least two clusters remain:\n(a) Choose {Ci, Cj} with minimal \u2206(Ci, Cj). (b) Remove {Ci, Cj}, add in Ci \u222a Cj .\nIn order to build a hierarchy with low k-means cost, one can use the merge cost due to Ward (1963),\n\u2206w(Ci, Cj) := |Ci||Cj | |Ci|+ |Cj | \u2016\u03c4(Ci)\u2212 \u03c4(Cj)\u201622,\nwhere \u03c4(C) denotes the mean of cluster C.\nThe k-means cost, and thus the Ward merge rule, inherently prefer spherical clusters of common radius. To accommodate other cluster shapes and input domains, the squared Euclidean norm may be replaced with a relaxation sharing many of the same properties, a Bregman divergence.\nThis manuscript develops the theory of agglomerative clustering with Bregman divergences.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)."}, {"heading": "1.1. Bregman clustering", "text": "There is already a rich theory of clustering with Bregman divergences, and in particular the relationship of these divergences with exponential family distributions (Banerjee et al., 2005). The standard development has two shortcomings, the first of which is amplified in the agglomerative setting.\nDegenerate divergences. Many divergences lead to merge costs which are undefined on certain inputs. This scenario is exacerbated with small clusters; for instance, with Gaussian clusters, the corresponding divergence rule is the KL divergence, which demands full rank cluster covariances. This is impossible with \u2264 d points, but the agglomerative procedure above starts with singletons.\nMinimal representations. The standard theory of exponential families and its connections to Bregman divergences depend on minimal representations: there is just one way to write down any particular distribution. On the other hand, the natural encoding for many problems \u2014 e.g., Ising models, and many other examples listed in the textbook of Wainwright & Jordan (2008, Section 4) \u2014 is overcomplete, necessitating potentially tedious conversions to invoke the theory."}, {"heading": "1.2. Contribution", "text": "The approach of this manuscript is to carefully build a theory of Bregman divergences constructed from convex, yet nondifferentiable functions. Section 2 will present the basic definition, and verify this generalization satisfies the usual Bregman divergence properties.\nSection 3 will revisit the standard Bregman hard clustering model (Banerjee et al., 2005), and show how it naturally leads to a merge cost \u2206. Section 4 then constructs exponential families, demonstrating that nondifferentiable Bregman divergences, while permitting representations which are not minimal, still satisfy all the usual properties. To overcome the aforemen-\ntioned small-sample cases where divergences may not be well-defined, Section 5 presents smoothing procedures which immediately follow from the preceding technical development.\nTo close, Section 6 presents the final algorithm, and Section 7 provides experimental validation both by measuring cluster fit, and the suitability of cluster features in supervised learning tasks.\nThe various appendices contain all proofs, as well as some additional technical material and examples."}, {"heading": "1.3. Related work", "text": "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999). Furthermore, Merugu (2006) studied the same algorithm as the present work, phrased in terms of Bregman divergences. These preceding methods either do not explicitly mention divergence degeneracies, or circumvent them with Bayesian techniques, a connection discussed in Section 5.\nBregman divergences for nondifferentiable functions have been studied in a number of places. Remark 2.4 shows the relationship between the presented version, and one due to Gordon (1999). Furthermore, Kiwiel (1995) presents divergences almost identical to those here, but the manuscripts and focuses differ thereafter.\nThe development here of exponential families and related Bregman properties generalizes results found in a variety of sources (Brown, 1986; Azoury & Warmuth, 2001; Banerjee et al., 2005; Wainwright & Jordan, 2008); further bibliographic remarks will appear throughout, and in Appendix G. Finally, parallel to the completion of this manuscript, another group has developed exponential families under similarly relaxed conditions, but from the perspective of maximum entropy and convex duality (Csisza\u0301r & Matu\u0301s\u030c, 2012)."}, {"heading": "1.4. Notation", "text": "The following concepts from convex analysis are used throughout the text; the interested reader is directed to the seminal text of Rockafellar (1970). A set is convex when the line segment between any two of its elements is again within the set. The epigraph of a function f : Rn \u2192 R\u0304, where R\u0304 = R \u222a {\u00b1\u221e}, is the set of points bounded below by f ; i.e., the set\n{(x, r) : x \u2208 Rn, r \u2265 f(x)} \u2286 Rn \u00d7 R\u0304. A function is convex when its epigraph is convex, and closed when its epigraph is closed. The domain dom(f) of a function f : Rn \u2192 R\u0304 is the subset of inputs not mapping to +\u221e: dom(f) = {x \u2208 Rn : f(x) < \u221e}. A function is proper if dom(f) is nonempty, and f never takes on the value \u2212\u221e. The Bregman divergences in this manuscript will be generated from closed proper convex functions.\nThe conjugate of a function f is the function f\u2217(\u03c6) := supx \u3008\u03c6, x\u3009 \u2212 f(x); when f is closed proper convex, so is f\u2217, and moreover f\u2217\u2217 = f . A subgradient g to a function f at y \u2208 dom(f) provides an affine lower bound: for any x \u2208 Rn, f(x) \u2265 f(y) + \u3008g, x\u2212 y\u3009. The set of all subgradients at a point y is denoted by \u2202f(y) (which is easily empty). The directional derivative f \u2032(y; d) of a function f at y in direction d is limt\u21930(f(y + td)\u2212 f(y))/t.\nThe affine hull of a set S \u2286 Rn is the smallest affine set containing it. If S is translated and rotated so that its affine hull is some Rd \u2286 Rn, then its interior within Rd is its relative interior within Rn. Said another way, the relative interior ri(S) is the interior of S with respect to the Rn topology relativized to the affine hull of S. Although functions in this manuscript will generally be closed, their domains are often (relatively) open.\nConvex functions will be defined over Rn, but it will be useful to treat data as lying in an abstract space X , and a statistic map \u03c4 : X \u2192 Rn will embed examples in the desired Euclidean space. This map, which will also be overloaded to handle finite subsets of X , will eventually incorporate the smoothing procedure.\nThe cluster cost will be denoted by \u03c6, or \u03c6f,\u03c4 to make the underlying convex function and statistic map clear; similarly, the merge cost is denoted by \u2206 and \u2206f,\u03c4 ."}, {"heading": "2. Bregman divergences", "text": "Given a convex function f : Rn \u2192 R\u0304, the Bregman divergence Bf (\u00b7, y) is the gap between f and its linearization at y. Typically, f is differentiable, and so Bf (x, y) = f(x)\u2212 f(y)\u2212 \u3008\u2207f(y), x\u2212 y\u3009. Definition 2.1. Given a convex function f : Rn \u2192 R\u0304, the corresponding Bregman divergence between x, y \u2208 dom(f) is\nBf (x, y) := f(x)\u2212 f(y) + f \u2032(y; y \u2212 x). \u2666\nUnlike gradients, directional derivatives are welldefined whenever a convex function is finite, although they can be infinite on the relative boundary of dom(f) (Rockafellar, 1970, Theorems 23.1, 23.3, 23.4).\nNoting that f \u2032(y; y \u2212 x) \u2265 \u2212f \u2032(y;x \u2212 y) (Rockafellar, 1970, Theorem 23.1), it may seem closer to the original expression to instead use f(x) \u2212 f(y) \u2212 f \u2032(y;x \u2212 y) (which is thus bounded above by Bf (x, y)); however, it will later be shown that Bf (\u00b7, y) is convex, which fails if the directional derivative is flipped. This distinction is depicted in Figure 1.\nExample 2.2. In the case of the differentiable convex function f2 = \u2016 \u00b7 \u201622, Bf2(x, y) = \u2016x \u2212 y\u201622 follows by noting f \u20322(y; y \u2212 x) = \u30082y, y \u2212 x\u3009. To analyze the case of f1 = \u2016 \u00b7\u20161, first consider the univariate case h = | \u00b7 |. Either by drawing a picture or checking h\u2032(\u00b7; \u00b7) from definition, it follows that\nBh(x, y) :=\n{ 0 when xy > 0,\n2|x| otherwise.\nThen noting that f \u20321(\u00b7; \u00b7) decomposes coordinate-wise, it follows that Bf1(x, y) = \u2211 i Bh(xi, yi). Said another way, Bf1 is twice the l 1 distance from x to the farthest orthant containing y, which bears a resemblance to the hinge loss. \u2666\nBf can also be written in terms of subgradients. Proposition 2.3. Let a proper convex f and y \u2208 ri(dom(f)) be given. Then for any x \u2208 dom(f):\n\u2022 f \u2032(y; y \u2212 x) and Bf (x, y) are finite, and\n\u2022 Bf (x, y) := maxg\u2208\u2202f(y) f(x)\u2212 f(y)\u2212 \u3008g, x\u2212 y\u3009.\nThe above characterization will be extremely useful in proofs, where the existence of a maximizing subgradient g\u0304y,x will frequently be invoked.\nRemark 2.4. Given x \u2208 dom(f) and a dual element g \u2208 Rn, another nondifferentiable generalization of Bregman divergence, due to Gordon (1999), is\nDf (x, g) := f(x) + f \u2217(g)\u2212 \u3008g, x\u3009 .\nNow suppose there exists y \u2208 ri(dom(f)) with g \u2208 \u2202f(y); the Fenchel-Young inequality (Rockafellar, 1970, Theorem 23.5) grants Df (x, g) = f(x) \u2212 f(y) \u2212 \u3008g, x\u2212 y\u3009. Thus, by Proposition 2.3,\nBf (x, y) := max{Df (x, g) : g \u2208 \u2202f(y)}. \u2666\nTo sanity check Bf , Appendix A states and proves a number of key Bregman divergence properties, generalized to the case of nondifferentiability. The following list summarizes these properties; in general, f is closed proper convex, y \u2208 ri(dom(f)), and x \u2208 dom(f).\n\u2022 Bf (\u00b7, y) is convex, proper, nonnegative, and Bf (y, y) = 0.\n\u2022 When f is strictly convex, Bf (x, y) = 0 iff x = y.\n\u2022 Given gx \u2208 ri(dom(f\u2217)), supx\u2208\u2202f\u2217(gx) Bf (x, y) = supgy\u2208\u2202f(y) Bf\u2217(gy, gx).\n\u2022 Under some regularity conditions on f , a generalization of the Pythagorean theorem holds, with Bf replacing squared Euclidean distance.\nOver and over, this section depends on relative interiors. What\u2019s so bad about the relative boundary? The directional derivatives and subgradients break down. If y \u2208 relbd(dom(f)) and x \u2208 ri(dom(f)), then f \u2032(y; y \u2212 x) =\u221e = Bf (x, y), and there exists no maximizing subgradient as in Proposition 2.3; in fact, one can not in general guarantee the existence of any subgradients at all.\nIn just a moment, the cluster model will be developed, where it will be very easy for the second argument argument of Bf to lie on relbd(dom(f)), rendering the divergences infinite and cluster costs meaningless. Worse, it is frequently the case dom(f) is relatively open, meaning the relative boundary is not in dom(f)! The smoothing methods of Section 5 work around these issues. Their approach is simple enough: they just push relative boundary points inside the relative interior."}, {"heading": "3. Cluster model", "text": "Let a finite collection C of points {xi}mi=1 in some abstract space X \u2014 say, documents or vectors \u2014 be given. In order to cluster these with Bregman divergences, the first step is to map them into Rn.\nDefinition 3.1. A statistic map \u03c4 is any function from X to Rn. Given a finite set C \u2286 X , overload \u03c4 via averages: \u03c4(C) := |C|\u22121 \u2211 x\u2208C \u03c4(x). \u2666\nFor now, it suffices to think of \u03c4 as the identity map (with X = Rn), with an added convenience of computing means. Section 4, however, will rely on \u03c4 when constructing exponential families.\nDefinition 3.2. Given a statistic map \u03c4 : X \u2192 Rn and convex function f , the cost of a single cluster C is\n\u03c6f,\u03c4 (C) := \u2211 x\u2208C Bf (\u03c4(x), \u03c4(C)).\n(This cost was the basis for Bregman hard clustering (Banerjee et al., 2005).) \u2666\nExample 3.3 (k-means cost). Choose X = Rn, \u03c4(x) = x, and f = \u2016 \u00b7 \u201622. As discussed in Example 2.2, Bf (x, y) = \u2016x \u2212 y\u201622, and so \u03c6f,\u03c4 (C) =\u2211 x\u2208C \u2016x\u2212 \u03c4(C)\u201622, precisely the k-means cost. \u2666\nAs such, \u03c4(C) plays the role of a cluster center. While this may be intuitive for the k-means cost, it requires justification for general Bregman divergences. The following definition and results bridge this gap.\nDefinition 3.4. A convex function f is relatively (Ga\u0302teaux) differentiable if, for any y \u2208 ri(dom(f)), there exists g (necessarily any subgradient) so that, for any x \u2208 dom(f), f \u2032(y; y \u2212 x) = \u3008g, y \u2212 x\u3009. \u2666\nEvery differentiable function is relatively differentiable (with g = \u2207f(y)); fortunately, many relevant convex functions, in particular those used to construct Bregman divergences between exponential family distributions (cf. Proposition 4.5), will be relatively differentiable.\nUnder relative differentiability, Bregman divergences admit a bias-variance style decomposition, which immediately justifies the choice of centroid \u03c4(C).\nLemma 3.5. Let a proper convex relatively differentiable f , points {xi}mi=1 \u2282 Rn, and weights {\u03b1i}mi=1 \u2282 R be given, with \u00b5 := \u2211 i \u03b1ixi/( \u2211 j \u03b1j) \u2208 ri(dom(f)). Then, given any point y \u2208 ri(dom(f)), m\u2211 i=1 \u03b1iBf (xi, y) = m\u2211 i=1 \u03b1iBf (xi, \u00b5) + ( m\u2211 i=1 \u03b1i ) Bf (\u00b5, y).\nCorollary 3.6. Suppose the convex function f is relatively differentiable, let any statistic map \u03c4 and any finite cluster C \u2286 X be given. Then \u03c6f,\u03c4 (C) = infy\u2208Rn \u2211 x\u2208C B(\u03c4(x), y).\nProof. Use \u00b5 := \u03c4(C) = |C|\u22121 \u2211 x\u2208C \u03c4(x), Lemma 3.5, and Bf \u2265 0.\nContinuing, the stage is set to finally construct the Bregman merge cost.\nDefinition 3.7. Given two finite subsets C1, C2 of X , the cluster merge cost is simply growth in total cost:\n\u2206f,\u03c4 (C1, C2) = \u03c6f,\u03c4 (C1 \u222a C2)\u2212 \u2211\nj\u2208{1,2}\n\u03c6f,\u03c4 (Ci). \u2666\nThe above expression seems to imply that the computational cost of \u2206 scales with the number of points. But in fact, one need only look at the relevant centers.\nProposition 3.8. Let a proper convex relatively differentiable f and two finite subsets C1, C2 of X with \u03c4(Ci) \u2208 ri(dom(f)) be given. Then\n\u2206f,\u03c4 (C1, C2) = \u2211\nj\u2208{1,2}\n|Cj |Bf (\u03c4(Cj), \u03c4(C1 \u222a C2)).\nExample 3.9 (Ward/k-means merge cost). Continuing with the k-means cost from Example 3.3, note that for j \u2208 {1, 2},\n\u2016\u03c4(Cj)\u2212 \u03c4(C1 \u222a C2)\u20162 = |C3\u2212j | \u00b7 \u2016\u03c4(C1)\u2212 \u03c4(C2)\u20162\n|C1|+ |C2| .\nPlugging this into the simplification of \u2206f,\u03c4 provided by Proposition 3.8,\n\u2206f,\u03c4 (C1, C2) = \u2211\nj\u2208{1,2}\n|Cj ||C3\u2212j |2\n(|C1|+ |C2|)2 \u2016\u03c4(C1)\u2212 \u03c4(C2)\u201622\n= |C1||C2| |C1|+ |C2| \u2016\u03c4(C1)\u2212 \u03c4(C2)\u201622.\nThis is exactly the Ward merge cost. \u2666"}, {"heading": "4. Exponential families", "text": "So far, this manuscript has developed a mathematical basis to clustering with Bregman divergences. But what does it matter, if examples of meaningful Bregman divergences are few and far between?\nThe primary mechanism for constructing meaningful merge costs is to model the clusters as exponential family distributions. Throughout this section, let \u03bd be any measure over X , and further stipulate the statistic map \u03c4 is \u03bd-measurable.\nDefinition 4.1. Given a measurable statistic map \u03c4 and a vector \u03b8 \u2208 Rn of canonical parameters, the corresponding exponential family distribution has density\np\u03b8(x) := exp(\u3008\u03c4(x), \u03b8\u3009 \u2212 \u03c8(\u03b8)),\nwhere the normalization term \u03c8, typically called the cumulant or log partition function, is simply\n\u03c8(\u03b8) = ln \u222b exp(\u3008\u03c4(x), \u03b8\u3009)d\u03bd(x). \u2666\nMany standard distributions have this representation.\nExample 4.2. Choose X = Rd with \u03bd being Lebesgue measure, and n = d(d + 1), i.e. Rn = Rd(d+1). The map \u03c4(x) = (x, xx>) will provide for Gaussian densities. In particular, starting from the familiar form, with mean \u00b5 \u2208 Rd and positive definite covariance \u03a3 \u2208 Rd2 , the density at x, p\u03b8(x), is\nexp(\u2212(x\u2212 \u00b5)>\u03a3\u22121(x\u2212 \u00b5)/2)\u221a (2\u03c0)d|\u03a3|\n= exp ( \u2329 \u03c4(x), (\u03a3\u22121\u00b5,\u2212\u03a3\u22121/2) \u232a \u2212 1\n2 ln((2\u03c0)d|\u03a3| exp(\u00b5>\u03a3\u22121\u00b5))\n) .\nIn other words, \u03b8 = (\u03a3\u22121\u00b5,\u2212\u03a3\u22121/2). Notice that \u03c8 (here expanded as 12 ln(. . .)) and \u03b8 do not make sense if \u03a3 is merely positive semi-definite. \u2666\nSo far so good, but where\u2019s the convex function, and does the definition of p\u03b8 even make sense?\nProposition 4.3. Given a measurable statistic map \u03c4 , the function \u03c8 is well-defined, closed, convex, and never takes on the value \u2212\u221e. Remark 4.4. Notice that Proposition 4.3 did not provide that \u03c8 is proper, only that it is never \u2212\u221e. Unfortunately, more can not be guaranteed: if \u03bd is Lebesgue measure over R and \u03c4(x) = 0 for all x, then every parameter choice \u03b8 \u2208 R has \u03c8(\u03b8) = \u221e. It is therefore necessary to check, for any provided \u03c4 , whether dom(\u03c8) is nonempty. \u2666\nNot only is \u03c8 closed convex, it is about as well-behaved as any function discussed in this manuscript.\nProposition 4.5. Suppose dom(\u03c8) is nonempty. Then \u03c8 is relatively differentiable; in fact, given any \u03b8 \u2208 ri(dom(\u03c8)), any \u03c4\u0302 \u2208 \u2202\u03c8(\u03b8), and any \u03be \u2208 dom(\u03c8),\n\u03c8\u2032(\u03b8; \u03be \u2212 \u03b8) = \u3008\u03c4\u0302 , \u03be \u2212 \u03b8\u3009 = \u222b \u3008\u03c4(x), \u03be \u2212 \u03b8\u3009 p\u03b8(x)d\u03bd(x). If \u03c8 is fully differentiable at \u03b8,then \u2207\u03c8(\u03b8) = \u222b \u03c4p\u03b8. Since \u03c8 is closed, given \u03c4\u0302 \u2208 \u2202\u03c8(\u03b8), it follows that \u03b8 \u2208 \u2202\u03c8\u2217(\u03c4\u0302). There is still cause for concern that other subgradients at \u03c4\u0302 lead to different densities, but as will be shown below, this does not happen.\nNow that a relevant convex function \u03c8 has been identified, the question is whether B\u03c8 (or B\u03c8\u2217) provide a reasonable notion of distance amongst densities.\nThis will be answered in two ways. To start, recall the Kullback-Leibler divergence K between densities p, q:\nK(p, q) = \u222b p(x) ln ( p(x)\nq(x)\n) d\u03bd(x).\nTheorem 4.6. Let any \u03b81, \u03b82 \u2208 ri(dom(\u03c8)) and any \u03c4\u03021 \u2208 \u2202\u03c8(\u03b81), \u03c4\u03022 \u2208 \u2202\u03c8(\u03b82) be given, where \u2202\u03c8\u2217(\u03c4\u03022) \u2286 ri(dom(\u03c8)) (for instance, if dom(\u03c8) is relatively open). Then\nK(p\u03b81 , p\u03b82) = B\u03c8(\u03b82, \u03b81) = B\u03c8\u2217(\u03c4\u03021, \u03c4\u03022).\nFurthermore, if \u03b81 \u2208 \u2202\u03c8\u2217(\u03c4\u03022), then p\u03b81 = p\u03b82 \u03bd-a.e..\nMotivated by Proposition 4.5 and Theorem 4.6, the choice here is to base the cluster model on B\u03c8\u2217 .\nGiven two clusters {Ci}2i=1, set \u03c4\u0302i := \u03c4(Ci). When working with these clusters, it is entirely sufficient to store only these statistics and the cluster sizes, since \u03c4(C1 \u222a C2) = |C1 \u222a C2|\u22121(|C1|\u03c4\u03021 + |C2|\u03c4\u03022). Assuming for interpretability that \u03c8 is differentiable, since \u03c8 is closed, \u03c8\u2217\u2217 = \u03c8, and thus \u2207\u03c8(\u03b81) = \u222b \u03c4p\u03b81 = \u03c4\u03021; that is to say, these distributions have their (aptly named) mean parameterizations as their means. And as provided by Theorem 4.6, even if differentiability fails, various subgradients of the same mean all effectively represent the same distributions.\nExample 4.7. Suppose X is a finite set, representing a vocabulary with n words, and \u03bd is counting measure over X . The statistic map \u03c4 converts word k into the kth basis vector ek. Let \u03c4\u0302 \u2208 Rn++ represent the mean parameters of a multinomial over this set; observe that\np\u03b8(ei) = \u3008\u03c4(i), \u03c4\u0302\u3009 = exp(\u3008ei, ln \u03c4\u0302\u3009)\u2212 ln \u222b exp(\u3008\u03c4(k), ln \u03c4\u0302\u3009)d\u03bd(k).\nThat is to say, the canonical parameter vector is \u03b8 = ln \u03c4\u0302 , the coordinate-wise logarithm of the mean parameters. Proposition 4.5 can be verified directly: (\u2207\u03c8(\u03b8))i = e\u03b8i/ \u2211 k e\n\u03b8k = \u03c4\u0302 . Similarly, given another multinomial with mean parameters \u03c4\u0302 \u2032 \u2208 Rn++ and canonical parameters \u03b8\u2032 = ln \u03c4\u0302 \u2032,\nK(p\u03b8, p\u03b8\u2032) = n\u2211 i=1 \u03c4\u0302i ln ( \u03c4\u0302i \u03c4\u0302 \u2032i ) .\nThe notation Rn++ means strictly positive coordinates: no word can have zero probability. Without this restriction, it is not possible to map into the canonical parameter space. This is precisely the scenario the smoothing methods of Section 5 will work around: the provided clusters are on the relative boundary of dom(\u03c8\u2217), which is either not part of dom(\u03c8\u2217) at all, or as is the case here, causes degenerate Bregman divergences (infinite valued, and lacking subgradients). \u2666\nRemark 4.8. The multinomials in Example 4.7 have an overcomplete representation: scaling any canonical parameter vector by a constant gives the same\nmean parameter. In general, if two relative interior canonical parameters \u03b81 6= \u03b82 have a common subgradient \u03c4\u0302 \u2208 \u2202\u03c8(\u03b81) \u2229 \u2202\u03c8(\u03b82), then it follows that {\u03b81, \u03b82} \u2282 \u2202\u03c8\u2217(\u03c4\u0302) (Rockafellar, 1970, Theorem 23.5). That is to say: this scenario leads to mean parameters which have distinct subgradients, and are thus points of nondifferentiability within ri(dom(\u03c8\u2217)), which necessitate the generalized development of Bregman divergences in this manuscript. \u2666\nA further example of Gaussians appears in Appendix C.\nThe second motivation for \u2206\u03c8\u2217,\u03c4 is an interpretation in terms of model fit.\nTheorem 4.9. Fix some measurable statistic map \u03c4 , and let two finite subsets C1, C2 of X be given with mean parameters {\u03c4(C1), \u03c4(C2)} = {\u03c4\u03021, \u03c4\u03022} \u2286 ri(dom(\u03c8\u2217)). Choose any canonical parameters \u03b8i \u2208 \u2202\u03c8\u2217(\u03c4\u0302i), and for convenience set C3 := C1 \u222a C2, with mean parameter \u03c4\u03023 and any canonical parameter \u03b83 \u2208 \u2202\u03c8\u2217(\u03c4\u03023). Then\n\u2206\u03c8\u2217,\u03c4 (C1, C2) = \u2211\ni\u2208{1,2} \u2211 x\u2208Ci ln p\u03b8i(x)\u2212 \u2211 x\u2208C3 ln p\u03b83(x)."}, {"heading": "5. Smoothing", "text": "The final piece of the technical puzzle is the smoothing procedure: most of the above properties \u2014 for instance, that Bf (\u03c4(C1), \u03c4(C2)) < \u221e \u2014 depend on \u03c4(C2) \u2208 ri(dom(f)). Relative boundary points lead to degeneracies; for example, this characterizes the Gaussian degeneracy identified in the introduction.\nDefinition 5.1. Given a (nonempty) convex set S, a statistic map \u03c4 : X \u2192 Rn is a smoothing statistic map for S if, given any finite set C \u2286 X , \u03c4(C) \u2208 ri(S). \u2666\nIt turns out to be very easy to construct smoothing statistic maps.\nTheorem 5.2. Let a nonempty convex set S be given. Let \u03c40 : X \u2192 Rn be a statistic map satisfying, for any finite C \u2286 X , \u03c40(C) \u2208 cl(S). Let z \u2208 ri(S) and \u03b1 \u2208 (0, 1) be arbitrary. Given any finite set C \u2286 X , define the maps\n\u03c41(C) := (1\u2212 \u03b1)\u03c40(C) + \u03b1z, \u03c42(C) := \u03c40(C) + \u03b1z,"}, {"heading": "In general, \u03c41 is a smoothing statistic map for S. If additionally S is a convex cone, then \u03c42 is also a smoothing statistic map for S.", "text": "The following two examples smooth Gaussians and multinomials via Theorem 5.2. The parameters \u03b1 and\nz are chosen from data, and moreover satisfy \u2016\u03b1z\u2016 \u2193 0 as the total amount of available data grows; that is to say, \u03c4 will more and more closely match \u03c40.\nExample 5.3 (Smoothing multinomials.). The mean parameters to a multinomial lie within the probability simplex, a compact convex set. As discussed in Example 4.7, only the relative interior of the simplex provides canonical parameters. According to Theorem 5.2, all that remains in fixing this problem is to determine \u03b1z.\nThe approach here is to interpret the provided multinomial \u03c40(C) = \u03c4\u0302 as based on a finite sample of size m, and thus the true parameters lie within some confidence interval around \u03c4\u0302 ; crucially, this confidence interval intersects the relative interior of the probability simplex. One choice is a Bernstein-based upper confidence estimate \u03c4(C) = \u03c40(C) + O(1/m +\u221a p(1\u2212 p)/m), where p = 1/n. \u2666\nExample 5.4 (Smoothing Gaussians.). In the case of Gaussians, as discussed in Example 4.2, only positive definite covariance matrices are allowed. But this set is a convex cone, so Theorem 5.2 reduces the problem to finding a sensible element to add in.\nConsider the case of singleton clusters. Adding a fullrank covariance matrix in to the observed zero covariance matrix is like replacing this singleton with a constellation of points centered at it. Equivalently, each point is replaced with a tiny Gaussian, which is reminiscent of nonparametric density estimation techniques. Therefore one option is to use bandwidth selection techniques; the experiments of Section 7 use the \u201cnormal reference rule\u201d (Bowman & Azzalini, 1997, Section 2.4.2), trying both the approach of estimating a bandwidth for each coordinate (suffix -nd), and computing one bandwidth for every direction uniformly, and simply adding a rescaling of the identity matrix to the sample covariance (suffix -n). \u2666\nWhen there is a probabilistic interpretation of the clusters, and in particular \u03c4(C) may be viewed as a maximum likelihood estimate, another approach is to choose some prior over the parameters, and have \u03c4 produce a MAP estimate which also lies in the relative interior. As stated, this approach will differ slightly from the one presented here: the weight on the added element will scale with the cluster size, rather than the size of the full data, and the relationship of \u03c4(C1\u222aC2) to \u03c4(C1) and \u03c4(C2) becomes less clear."}, {"heading": "6. Clustering algorithm", "text": "The algorithm appears in Algorithm 1. Letting T\u2206f,\u03c4 denote an upper bound on the time to calculate a\nAlgorithm 1 Agglomerate. Input Merge cost \u2206f,\u03c4 , points {xi}mi=1 \u2286 X . Output Hierarchical clustering.\nInitialize forest as F := {{xi} : i \u2208 [m]}. while |F | > 1 do\nLet {Ci, Cj} \u2286 F be any pair minimizing \u2206f,\u03c4 (Ci, Cj), as computed by Proposition 3.8. Remove {Ci, Cj} from F , add in Ci \u222a Cj .\nend while return the single tree within F .\nsingle merge cost, a brute-force implementation (over m points) takes space O(m) and time O(m3T\u2206f,\u03c4 ), whereas caching merge cost computations in a minheap requires space O(m2) and time O(m2(lg(m) + T\u2206f,\u03c4 )). Please refer Appendix E for more notes on running times, and a depiction of sample hierarchies over synthetic data.\nIf Proposition 3.8 is used to compute \u2206f,\u03c4 , then only the sizes and means of clusters need be stored, and computing this merge cost involves just two Bregman divergences calculations. As the new mean is a convex combination of the two old means, computing it takes time O(n). The Bregman cost itself can be more expensive; for instance, as discussed with Gaussians in Appendix C, it is necessary to invert a matrix, meaning O(n3) steps."}, {"heading": "7. Empirical results", "text": "Trees generated by Agglomerate are evaluated in two ways. First, cluster compatibility scores are computed via dendrogram purity and initialization quality for EM upon mixtures of Gaussians. Secondly, cluster features are fed into supervised learners.\nThere are two kinds of data: Euclidean (points in some Rn), and text data. There are three Euclidean data sets: UCI\u2019s glass (214 points in R9); 3s and 5s from the mnist digit recognition problem (1984 training digits and 1984 testing digits in R49, scaled down from the original 28x28); UCI\u2019s spambase data (2301 train and 2300 test points in R57). Text data is drawn from the 20 newsgroups data, which has a vocabulary of 61,188 words; a difficult dichotomy (20n-h), the pair alt.atheism/talk.religion.misc (856 train and 569 test documents); an easy dichotomy (20n-e), the pair rec.sport.hockey/sci.crypt (1192 train and 794 test documents). Finally, 20n-b collects these four groups into one corpus.\nThe various trees are labeled as follows. s-link and c-link denote single and complete linkage, where l1\ndistance is used for text, and l2 distance is used for Euclidean data. km is the Ward/k-means merge cost. g-n fits full covariance Gaussians, whereas dg-nd fits diagonal covariance Gaussians; smoothing follows the data-dependent scheme of Example 5.4. multi fits multinomials, with the smoothing scheme of Example 5.3."}, {"heading": "7.1. Cluster compatibility", "text": "Table 1 contains cluster purity scores, a standard dendrogram quality measure, defined as follows. For any two points with the same label l, find the smallest cluster C in the tree which contains them both; the purity with respect to these two points is the fraction of C having label l. The purity of the dendrogram is the weighted sum, over all pairs of points sharing labels, of pairwise purities. The glass, spam, and 20newsgroups data appear in Heller & Ghahramani (2005); although a direct comparison is difficult, since those experiments used subsampling and randomized purity, the Euclidean experiments perform similarly, and the text experiments fare slightly better here.\nFor another experiment, now assessing the viability of Agglomerate as an initialization to EM applied to mixtures of Gaussians, please see Appendix F."}, {"heading": "7.2. Feature generation", "text": "The final experiment is to use dendrograms, built from training data, to generate features for classification tasks. Given a budget of features k, the top k clusters {Ci}ki=1 of a specified dendrogram are chosen, and for any example x, the ith feature is \u2206(Ci, {x}). Statistically, this feature measures the amount by which the model likelihood degrades when Ci is adjusted to accommodate x. The choice of tree was based on training set purity from Table 1. In all tests, the original features are discarded (i.e., only the k generated features are used).\nFigure 2 shows the performance of logistic regression classifiers using tree features, as well as SVD features. The stopping rule used validation set performance."}, {"heading": "Acknowledgements", "text": "This work was graciously supported by the NSF under grant IIS-0713540."}], "references": [{"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["Azoury", "Katy S", "Warmuth", "Manfred K"], "venue": "Machine Learning,", "citeRegEx": "Azoury et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Azoury et al\\.", "year": 2001}, {"title": "Clustering with Bregman divergences", "author": ["Banerjee", "Arindam", "Merugu", "Srujana", "Dhillon", "Inderjit S", "Ghosh", "Joydeep"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Convex Analysis and Nonlinear Optimization", "author": ["Borwein", "Jonathan", "Lewis", "Adrian"], "venue": null, "citeRegEx": "Borwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Borwein et al\\.", "year": 2000}, {"title": "Fundamentals of Statistical Exponential Families", "author": ["Brown", "Lawrence D"], "venue": "Insitute of Mathematical Statistics,", "citeRegEx": "Brown and D.,? \\Q1986\\E", "shortCiteRegEx": "Brown and D.", "year": 1986}, {"title": "Generalized minimizers of convex integral functionals, Bregman distance, Pythagorean identities", "author": ["Csisz\u00e1r", "Imre", "Mat\u00fa\u0161", "Frant\u01d0sek"], "venue": null, "citeRegEx": "Csisz\u00e1r et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Csisz\u00e1r et al\\.", "year": 2012}, {"title": "Real analysis: modern techniques and their applicatins", "author": ["Folland", "Gerald B"], "venue": "Wiley Interscience,", "citeRegEx": "Folland and B.,? \\Q1999\\E", "shortCiteRegEx": "Folland and B.", "year": 1999}, {"title": "Algorithms for model-based gaussian hierarchical clustering", "author": ["C. Fraley"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Fraley,? \\Q1998\\E", "shortCiteRegEx": "Fraley", "year": 1998}, {"title": "Hierarchical gaussian mixture model", "author": ["Garcia", "Vincent", "Nielsen", "Frank", "Nock", "Richard"], "venue": "In ICASSP,", "citeRegEx": "Garcia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2010}, {"title": "Approximate Solutions to Markov Decision Processes", "author": ["Gordon", "Geoff J"], "venue": "PhD thesis,", "citeRegEx": "Gordon and J.,? \\Q1999\\E", "shortCiteRegEx": "Gordon and J.", "year": 1999}, {"title": "Bayesian hierarchical clustering", "author": ["Heller", "Katherine A", "Ghahramani", "Zoubin"], "venue": "In ICML,", "citeRegEx": "Heller et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Heller et al\\.", "year": 2005}, {"title": "Fundamentals of Convex Analysis", "author": ["Hiriart-Urruty", "Jean-Baptiste", "Lemar\u00e9chal", "Claude"], "venue": null, "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 2001}, {"title": "Hierarchical bayesian clustering for automatic text classification", "author": ["Iwayama", "Makoto", "Tokunaga", "Takenobu"], "venue": "In IJCAI,", "citeRegEx": "Iwayama et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Iwayama et al\\.", "year": 1995}, {"title": "Proximal minimization methods with generalized Bregman functions", "author": ["Kiwiel", "Krzysztof C"], "venue": "SIAM journal on control and optimization,", "citeRegEx": "Kiwiel and C.,? \\Q1995\\E", "shortCiteRegEx": "Kiwiel and C.", "year": 1995}, {"title": "Distributed Learning using Generative Models", "author": ["Merugu", "Srujana"], "venue": "PhD thesis, University of Texas, Austin,", "citeRegEx": "Merugu and Srujana.,? \\Q2006\\E", "shortCiteRegEx": "Merugu and Srujana.", "year": 2006}, {"title": "A Survey of Recent Advances in Hierarchical Clustering Algorithms", "author": ["Murtagh", "Fionn"], "venue": "The Computer Journal,", "citeRegEx": "Murtagh and Fionn.,? \\Q1983\\E", "shortCiteRegEx": "Murtagh and Fionn.", "year": 1983}, {"title": "Convex Analysis", "author": ["Rockafellar", "R. Tyrrell"], "venue": null, "citeRegEx": "Rockafellar and Tyrrell.,? \\Q1970\\E", "shortCiteRegEx": "Rockafellar and Tyrrell.", "year": 1970}, {"title": "Agglomerative information bottleneck", "author": ["Slonim", "Noam", "Tishby", "Naftali"], "venue": "pp. 617\u2013623,", "citeRegEx": "Slonim et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Slonim et al\\.", "year": 1999}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["Wainwright", "Martin J", "Jordan", "Michael I"], "venue": "Now Publishers Inc.,", "citeRegEx": "Wainwright et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2008}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["Ward", "Joe H"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ward and H.,? \\Q1963\\E", "shortCiteRegEx": "Ward and H.", "year": 1963}], "referenceMentions": [{"referenceID": 1, "context": "There is already a rich theory of clustering with Bregman divergences, and in particular the relationship of these divergences with exponential family distributions (Banerjee et al., 2005).", "startOffset": 165, "endOffset": 188}, {"referenceID": 1, "context": "Section 3 will revisit the standard Bregman hard clustering model (Banerjee et al., 2005), and show how it naturally leads to a merge cost \u2206.", "startOffset": 66, "endOffset": 89}, {"referenceID": 6, "context": "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999).", "startOffset": 267, "endOffset": 401}, {"referenceID": 7, "context": "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999).", "startOffset": 267, "endOffset": 401}, {"referenceID": 6, "context": "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999). Furthermore, Merugu (2006) studied the same algorithm as the present work, phrased in terms of Bregman divergences.", "startOffset": 294, "endOffset": 430}, {"referenceID": 1, "context": "The development here of exponential families and related Bregman properties generalizes results found in a variety of sources (Brown, 1986; Azoury & Warmuth, 2001; Banerjee et al., 2005; Wainwright & Jordan, 2008); further bibliographic remarks will appear throughout, and in Appendix G.", "startOffset": 126, "endOffset": 213}, {"referenceID": 1, "context": "(This cost was the basis for Bregman hard clustering (Banerjee et al., 2005).", "startOffset": 53, "endOffset": 76}], "year": 2012, "abstractText": "This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondifferentiable convex functions.", "creator": "pdfsam-console (Ver. 2.0.6e)"}}}