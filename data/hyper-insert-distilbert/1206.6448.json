{"id": "1206.6448", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Online Alternating Direction Method", "abstract": "generalized online robust optimization has emerged theoretically as powerful tool in large integrated scale optimization. just in with this paper, we introduce efficient online algorithms based on the alternating directions search method ( rm adm ). further we introduce a new initial proof averaging technique employed for adm processing in the batch setting, which again yields the exact o ( 1 / 32 t ) expected convergence rate of adm and forms the basis of regret analysis in the online setting. subsequently we can consider all two reduction scenarios based in understanding the online convergence setting, ultimately based on selecting whether the solution needs to lie in the feasible option set data or not. now in both settings, we establish regret bounds for both the objective function as well as extreme constraint violation moments for fast general systems and strongly convex functions. preliminary results actually are always presented aiming to illustrate the intrinsic performance limitation of the clearly proposed network algorithms.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (589kb)", "http://arxiv.org/abs/1206.6448v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["huahua wang", "arindam banerjee"], "accepted": true, "id": "1206.6448"}, "pdf": {"name": "1206.6448.pdf", "metadata": {"source": "META", "title": "Online Alternating Direction Method", "authors": ["Huahua Wang", "Arindam Banerjee"], "emails": ["HUWANG@CS.UMN.EDU", "BANERJEE@CS.UMN.EDU"], "sections": [{"heading": "1. Introduction", "text": "In recent years, online learning (Zinkevich, 2003; Hazan et al., 2007) and its batch counterpart stochastic gradient descent (Juditsky et al., 2009) has contributed substantially to advances in large scale optimization techniques for machine learning. Online convex optimization has been generalized to handle time-varying and non-smooth convex functions (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010). Distributed optimization, where the problem is divided into parts on which progress can be made in parallel, has also contributed to advances in large scale optimization (Boyd et al., 2010; Bertsekas & Tsitsiklis, 1989; Censor & Zenios, 1998).\nImportant advances have been made based on the above ideas in the recent literature. Composite objective mirror descent (COMID) (Duchi et al., 2010) generalizes mirror descent (Beck & Teboulle, 2003) to the online setting. COMID also includes certain other proximal splitting methods such as FOBOS (Duchi & Singer, 2009) as special cases. Regularized dual averaging (RDA) (Xiao, 2010) general-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nizes dual averaging (Nesterov, 2009) to online and composite optimization, and can be used for distributed optimization (Duchi et al., 2011).\nFirst introduced in (Gabay & Mercier, 1976), the alternating direction method (ADM) has become popular in recent years due to its ease of applicability and empirical performance in a wide variety of problems, including composite objectives (Boyd et al., 2010; Eckstein & Bertsekas, 1992; Lin et al., 2009). The proof of convergence of ADM can be found in (Eckstein & Bertsekas, 1992; Boyd et al., 2010), although the rate of convergence rate has not been established. For further understanding of ADM, we refer the readers to the comprehensive review by (Boyd et al., 2010). An advantage of ADM is that it can handle linear equality constraints of the form {x, z|Ax+Bz = c}, which makes distributed optimization by variable splitting in a batch setting straightforward (Boyd et al., 2010). However, in an online or stochastic gradient descent setting, one obtains a double-loop algorithm where the inner loop ADM iterations have to be run till convergence after every new data point or function is revealed. As a result, ADM has not yet been generalized to the online setting.\nIn this paper, we consider optimization problems of the following form:\nmin x\u2208X ,z\u2208Z T\u2211 t=1 (ft(x) + g(z)) s.t. Ax + Bz = c , (1)\nwhere the functions ft, g are (non-smooth) convex functions, A \u2208 Rm\u00d7n1 ,B \u2208 Rm\u00d7n2 , c \u2208 Rm, x \u2208 X \u2208 Rn1\u00d71, z \u2208 Z \u2208 Rn2\u00d71, where X and Z are convex sets. In the sequel, we drop the convex sets X and Z for ease of exposition, noting that one can consider g and other additive functions to be the indicators of suitable convex feasible sets. The problem is studied both in the batch setting, where ft = f , and in the online setting for time-varying ft. We introduce a new proof technique for ADM in the batch setting, which establishes a O(1/T ) convergence rate of ADM based on variational inequalities (Facchinei & Pang, 2003). Further, the convergence analysis for the batch setting forms the basis of regret analysis in the online setting. We consider two scenarios in the online setting, based on whether or not the solution needs to lie in the feasible set\nin every iteration.\nWe propose efficient online ADM (OADM) algorithms for both scenarios which make a single pass through the update equations and avoid a double loop algorithm. In the online setting, while a single pass through the ADM update equations is not guaranteed to satisfy the linear constraints Axt + Bzt = c, we consider two types of regret: regret in the objective as well as regret in constraint violation. We establish both types of regret bounds for general and strongly convex functions. We also present preliminary experimental results illustrating the performance of the proposed OADM algorithms in comparison with FOBOS and RDA (Duchi & Singer, 2009; Xiao, 2010).\nThe key advantage of the OADM algorithms can be summarized as follows: Like COMID and RDA, OADM can solve online composite optimization problems, matching the regret bounds for existing methods. The ability to additionally handle linear equality constraints of the form Ax + Bz = c makes non-trivial variable splitting possible yielding efficient distributed online optimization algorithms based on OADM. Further, the notion of regret in both the objective as well as constraints may contribute towards development of suitable analysis tools for online constrained optimization problems (Mannor & Tsitsiklis, 2006; Mahdavi et al., 2011).\nThe rest of the paper is organized as follows. In Section 2, we analyze batch ADM and establish its convergence rate. In Section 3, we introduce the online optimization problem with linear constraints. The OADM algorithm is also given in Section 3. In Sections 4 and 5, we present the regret analysis in two different scenarios based on the constraints. We discuss connections to related work in Section 6, present preliminary experimental results in Section 7, and conclude in Section 8."}, {"heading": "2. Analysis for Batch ADM", "text": "We consider the batch ADM problem (1) where ft is fixed. The augmented Lagrangian for (1) is\nL\u03c1(x,y,z)=f(x)+g(z)+\u3008y,Ax+Bz\u2212c\u3009+ \u03c1\n2 \u2016Ax+Bz\u2212c\u20162, (2)\nwhere z is the primal variable and y is the dual variable, \u03c1 > 0 is the penalty parameter. Batch ADM executes the following three steps iteratively till convergence (Boyd et al., 2010):\nxt+1=argmin x\nf(x)+\u3008yt,Ax+Bzt\u2212c\u3009+ \u03c1\n2 \u2016Ax+Bzt\u2212c\u20162, (3)\nzt+1=argmin z\ng(z)+\u3008yt,Axt+1+Bz\u2212c\u3009+ \u03c1\n2 \u2016Axt+1+Bz\u2212c\u20162, (4)\nyt+1 = yt + \u03c1(Axt+1 + Bzt+1 \u2212 c) . (5)\nAt step (t + 1), the equality constraint is not necessarily satisfied in ADM. However, one can show that the equality constraint is satisfied in the long run such that limt\u2192\u221eAxt + Bzt \u2212 c \u2192 0. We first analyze the convergence of objective and constraint separately using a new proof technique, which plays an important role for the regret analysis in the online setting. Then, a joint analysis of the objective and constraint using a variational inequality (Facchinei & Pang, 2003) establishes the O(1/T ) convergence rate for ADM.\nWithout loss of generality, we assume that z0 = 0,y0 = 0. Denote \u2016y\u2217\u20162 = Dy, \u2016z\u2217\u20162 = Dz and \u03bbBmax as the largest eigenvalue of BTB."}, {"heading": "2.1. Bounds for Objective and Constraints", "text": "The following theorem shows that both the cumulative objective difference w.r.t. the optimal and the cumulative norms of the constraints, known as the primal and dual residuals (Boyd et al., 2010), are bounded by constants independent of the number of iterations T .\nTheorem 1 Let the sequences {xt, zt,yt} be generated by ADM. For any x\u2217, z\u2217 satisfying Ax\u2217 + Bz\u2217 = c, for any T , we have T\u2211 t=0 [f(xt+1)+g(zt+1)\u2212(f(x\u2217)+g(z\u2217))]\u2264 \u03bbBmaxD 2 z\u03c1 2 , (6)\nT\u2211 t=0 \u2016Axt+1+Bzt+1\u2212c\u201622+\u2016Bzt+1\u2212Bzt\u201622\u2264\u03bbBmaxD2z+ D2y \u03c12 .(7)\nIt is easy to verify that the KKT conditions of the augmented lagrangian (2) hold if (7) holds. The convergence of equality constraint and primal residual implies the convergence of ADM. A result similar to (7) has been shown in (Boyd et al., 2010), but our proof is different and selfcontained along with (6). Although (6) shows that the objective value converges to the optimal value, xt+1, zt+1 need not be feasible and the equality constraint is not necessarily satisfied."}, {"heading": "2.2. Rate of Convergence of ADM", "text": "We now prove the O(1/T ) convergence rate for ADM using a variational inequality (VI) based on the Lagrangian given in (2). Let \u2126 = X \u00d7 Z \u00d7 Rm. Any w\u2217 = (x\u2217, z\u2217,y\u2217) \u2208 \u2126 solves the original problem in (1) optimally if it satisfies the following variational inequality (Facchinei & Pang, 2003; Nemirovski, 2004):\n\u2200w \u2208 \u2126 , h(w)\u2212h(w\u2217)+(w\u2212w\u2217)TF (w\u2217) \u2265 0 , (8)\nwhere F (w)T = [yTA yTB \u2212 (Ax + Bz \u2212 c)T ] is the gradient of the last term of the Lagrangian, and h(w) = f(x)+g(z). Then, w\u0303 = (x\u0303, z\u0303, y\u0303) approximately solves the\nproblem with accuracy if it satisfies\n\u2200w \u2208 \u2126 , h(w\u0303)\u2212 h(w) + (w\u0303 \u2212w)TF (w\u0303) \u2264 . (9)\nWe show that after T iterations, the average w\u0304T = 1 T \u2211T t=1 wt, where wt = (xt, zt,yt) are from (3)-(5), satisfies the above inequality with = O(1/T ). Theorem 2 Let w\u0304T = 1T \u2211T t=1 wt, where wt = (xt, zt,yt) from (3)-(5). Then,\n\u2200w \u2208 \u2126, h(w\u0304T )\u2212h(w)+(w\u0304T\u2212w)TF (w\u0304T ) \u2264 O ( 1\nT\n) ."}, {"heading": "3. Online ADM", "text": "In this section, we extend the ADM to the online learning setting. Specifically, we focus on using online ADM (OADM) to solve the problem in (1). For our analysis, A and B are assumed to be fixed. At round t, we consider solving the following regularized optimization problem:\nxt+1 = argmin Ax+Bz=c ft(x) + g(z) + \u03b7B\u03c6(x,xt) , (10)\nwhere \u03b7 \u2265 0 is a learning rate and Bregman divergence B\u03c6(x,xt) \u2265 \u03b12 \u2016x \u2212 xt\u2016 2 2. If the above problem is solved in every step, standard analysis techniques (Hazan et al., 2007) can be suitably adopted to obtain sublinear regret bounds. While (10) can be solved by batch ADM, we essentially obtain a double loop algorithm where the function ft changes in the outer loop and the inner loop runs ADM iteratively till convergence so that the constraints are satisfied. Note that existing online methods, such as projected gradient descent and variants (Hazan et al., 2007; Duchi et al., 2010) do assume a black-box approach for projecting onto the feasible set, which for linear constraints may require iterative cyclic projections (Censor & Zenios, 1998).\nFor our analysis, instead of requiring the equality constraints to be satisfied at each time t, we only require the equality constraints to be satisfied in the long run, with a notion of regret associated with constraints. In particular, we consider the following online learning problem:\nmin xt,zt T\u2211 t=0 ft(xt) + g(zt)\u2212 min Ax+Bz=c T\u2211 t=0 ft(x) + g(z)\ns.t. T\u2211 t=1 \u2016Axt + Bzt \u2212 c\u201622 = o(T ) , (11)\nso that the cumulative constraint violation is sublinear in T . The augmented lagrangian function of (10) at time t is\nLt(x,y, z) =ft(x)+g(z)+\u3008y,Ax+Bz\u2212c\u3009+\u03b7B\u03c6(x,xt)\n+ \u03c1\n2 \u2016Ax + Bz\u2212 c\u20162 . (12)\nAt time t, our algorithm consists of just one pass through the following three update steps:\nxt+1 =argmin x\nft(x) + \u3008yt,Ax + Bzt \u2212 c\u3009\n+ \u03c1\n2 \u2016Ax + Bzt \u2212 c\u20162 + \u03b7B\u03c6(x,xt) , (13)\nzt+1 = argmin z\ng(z) + \u3008yt,Axt+1 + Bz\u2212 c\u3009\n+ \u03c1\n2 \u2016Axt+1 + Bz\u2212 c\u20162 , (14)\nyt+1 = yt + \u03c1(Axt+1 + Bzt+1 \u2212 c) . (15)\nThe x-update (13) has two penalty terms: a quadratic term and a Bregman divergence. If the Bregman divergence is not a quadratic function, it may be difficult to solve x efficiently. A common way is to linearize the objective such that\nxt+1 =argmin x \u3008f \u2032t(xt)+AT{yt+\u03c1(Axt+Bzt\u2212c)},x\u2212xt\u3009\n+ \u03b7B\u03c6(x,xt) . (16)\n(16) is known as inexact ADM (Boyd et al., 2010) if \u03c6 is a quadratic function. In the sequel, we focus on the algorithm using (13).\nOperationally, in round t, the algorithm presents a solution {xt, zt} as well as yt. Then, nature reveals function ft and we encounter two types of losses. The first type is the traditional loss measured by ft(xt) + g(zt). The second type is the residual of constraint violation, i.e., \u2016Axt+Bzt\u2212c\u20162. The goal is to establish sublinear regret bounds for both the objective and the constraint violation, which we do in Section 4. We consider another scenario, where in round t, we use a solution {x\u0302t, zt} based on zt such that Ax\u0302+Bzt = c. While (x\u0302t, zt) satisfies the constraint by design, the goal is to establish sublinear regret of the objective ft(x\u0302t) + g(zt) as well as the constraint violation for the true (xt, zt). For the second scenario, we use \u03b7 = 0 in (13) and present the results in Section 5. As the updates include the primal and dual variables, in line with batch ADM, we use a stronger regret Rc(T ) = \u2211T t=1R c t for constraint violation based on both primal and dual residuals, where\nRct =\u2016Axt+1 + Bzt+1\u2212 c\u201622 +\u2016Bzt+1 \u2212Bzt\u201622 . (17)\nBefore getting into the regret analysis, we discuss some example problems which can be solved using OADM. Like FOBOS and RDA, OADM can deal with machine learning methods where ft is a loss function and g is a regularizer, e.g. `1 or mixed norm, or an indicator function of a convex set. Examples include generalized lasso and group lasso (Boyd et al., 2010; Tibshirani, 1996; Xiao, 2010). OADM can also solve linear programs, e.g. MAP LP relaxation (Meshi & Globerson, 2011) and LP decoding (Barman et al., 2012), and non-smooth optimization,\ne.g. robust PCA (Lin et al., 2009) where ft is nuclear norm and g is `1 norm. Another promising scenario for OADM is consensus optimization (Boyd et al., 2010) where distributed local variables are updated separately and reach a global consensus in the long run. More examples can be found in (Boyd et al., 2010).\nIn the sequel, we need the following assumptions:\n(1) The norm of subgradient of ft(x) is bounded by Gf .\n(2) We assume g(z0) = 0 and g(z) \u2265 0.\n(3) x0 = 0,y0 = 0, z0 = 0. For any x\u2217, z\u2217 satisfying Ax\u2217 + Bz\u2217 = c, B\u03c6(x\u2217,0) = D2x, \u2016z\u2217\u20162 = Dz.\n(4) For any t, ft(xt+1)+g(zt+1)\u2212(ft(z\u2217)+g(z\u2217)) \u2265 \u2212F , which is true if the functions are lower bounded or Lipschitz continuous in the convex set (Mahdavi et al., 2011)."}, {"heading": "4. Regret Analysis of OADM", "text": "As discussed in Section 3, we consider two types of regret in OADM. The first type is the regret of the objective based on variable splitting, i.e.,\nR1(T )= T\u2211 t=0 ft(xt)+g(zt)\u2212 min Ax+Bz=c T\u2211 t=0 ft(x)+g(z) . (18)\nAside from using splitting variables, R1 is the standard regret in the online learning setting. The second is the regret of the constraint violation Rc defined in (17)."}, {"heading": "4.1. General Convex Functions", "text": "The following establishes the regret bounds for OADM.\nTheorem 3 Let the sequences {xt, zt,yt} be generated by OADM and assumptions (1)-(4) hold. For any x\u2217, z\u2217 satisfying Ax\u2217 + Bz\u2217 = c, setting \u03b7 = Gf \u221a T\nDx \u221a 2\u03b1 and \u03c1 =\n\u221a T ,\nwe have\nR1(T ) \u2264 \u03bbBmaxD2z \u221a T/2 + \u221a 2GfDx \u221a T/ \u221a \u03b1 , Rc(T ) \u2264 \u03bbBmaxD2z + \u221a 2DxGf/ \u221a \u03b1+ 2F \u221a T .\nNote the bounds are achieved without any explicit assumptions on A,B, c.1 The subgradient of ft is required to be bounded, but the subgradient of g is not necessarily bounded. Thus, the bounds hold for the case that g is an indicator function of a convex set. In addition to the O( \u221a T ) regret bound, OADM achieves the O( \u221a T ) bound for the constraint violation, which is not existent in the start-ofthe-art online learning algorithms (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010), since they do not explicitly handle linear constraints of the form Axt+Bz = c.\n1We do assume that Ax+Bz = c is feasible.\nThe bound for Rc could be reduced to a constant if additional assumptions on B and the subgradient of g are satisfied."}, {"heading": "4.2. Strongly Convex Functions", "text": "We assume both ft(x) and g are strongly convex. Specifically, we assume ft(x) is \u03b21-strongly convex with respect to a differentiable function \u03c6, i.e.,\nft(x \u2217)\u2265ft(x)+\u3008f \u2032t(x),x\u2217\u2212x\u3009+\u03b21B\u03c6(x\u2217,xt+1) , (19)\nwhere \u03b21 > 0, and g is a \u03b22-strongly convex function, i.e.,\ng(z\u2217)\u2265g(z)+\u3008g\u2032(z), z\u2217\u2212z\u3009+ \u03b22 2 \u2016z\u2217\u2212zt+1\u201622 , (20)\nwhere \u03b22 > 0. Then, logarithmic regret bounds can be established.\nTheorem 4 Let assumptions (1)-(4) hold. Assume ft(x) and g are strongly convex given in (19) and (20). For any x\u2217, z\u2217 satisfying Ax\u2217 + Bz\u2217 = c, setting \u03b7t = \u03b21t, \u03c1t = \u03b22t/\u03bb B max, we have\nR1(T ) \u2264 G2f log (T + 1)/(2\u03b1\u03b21) + \u03b22D2z/2 + \u03b21D2x , Rc(T )\u22642F\u03bbBmax log(T + 1)/\u03b22+\u03bbBmaxD2z+2\u03b21\u03bbBmaxD2x/\u03b22 .\nTo guarantee logarithmic regret bounds for both objective and constraints, OADM requires both ft and g to be strongly convex. FOBOS, COMID, and RDA only require g to be strongly convex although they do not consider linear constraints explicitly."}, {"heading": "5. Regret Analysis of OADM with \u03b7 = 0", "text": "We analyze the regret bound when \u03b7 = 0. In this case, OADM has the same updates as ADM. For the analysis, we consider zt to be the key primal variable, and compute x\u0302t using zt so that Ax\u0302t +Bzt = c. Since (x\u0302t, zt) satisfies the constraints by design, we consider the following regret:\nR2(T )= T\u2211 t=0 ft(x\u0302t)+g(zt)\u2212 min Ax+Bz=c T\u2211 t=0 ft(x)+g(z) . (21)\nwhere Ax\u0302t+Bzt = c. A common case we often encounter is when A = I,B = \u2212I, c = 0, thus x\u0302t = zt. While {x\u0302t, zt} satisfies the equality constraint, (xt, zt) need not satisfy Axt + Bzt \u2212 c = 0. Thus, in addition to R2(T ), we also consider bounds for Rc as defined in (17).\nTo guarantee that Ax\u0302t+Bzt = c,A \u2208 Rm\u00d7n1 is feasible, it implicitly requires the assumption m \u2264 n1. On the other hand, to establish a bound forR2, A should be full-column rank, i.e., rank(A) = n1. Therefore, we assume that A is a square and full rank matrix, i.e., A is invertible. Let \u03bbAmin be the smallest eigenvalue of AAT , then \u03bbAmin > 0."}, {"heading": "5.1. General Convex Functions", "text": "The following theorem shows the regret bounds.\nTheorem 5 Let \u03b7 = 0 in OADM and assumptions (1)-(4) and A is invertible hold. For any x\u2217, z\u2217 satisfying Ax\u2217 + Bz\u2217 = c, setting \u03c1 = Gf \u221a T\nDz \u221a \u03bbAmin\u03bb B max , we have\nR2(T ) \u2264 GfDz \u221a \u03bbBmaxT/\u03bb A min ,\nRc(T ) \u2264 \u03bbBmaxD2z + 2FDz \u221a \u03bbAmin\u03bb B maxT/Gf .\nWithout requiring an additional Bregman divergence, R2 achieves the \u221a T bound as R1. While R1 depends on xt which may not stay in the feasible set, R2 is defined on x\u0302t which always satisfies the equality constraint. The corresponding algorithm requires finding x\u0302t in each iteration such that Ax\u0302t = c\u2212Bzt, which involves solving a linear system. The algorithm will be efficient in some settings, e.g., consensus optimization where A = I."}, {"heading": "5.2. Strongly Convex Functions", "text": "The following theorem establishes the logarithmic regret bound under the assumption g is \u03b2-strongly convex given in (20).\nTheorem 6 Let \u03b7 = 0 in OADM. Assume that g(z) is \u03b22strongly convex, A is invertible, and assumptions (1)-(4) hold. Setting \u03c1t = \u03b22t/\u03bbBmax, we have\nR2(T ) \u2264 G2f\u03bb B max\n2\u03bbAmin\u03b22 (log(T + 1)) + \u03b22D\n2 z , (22)\nRc(T ) \u2264 \u03bbBmaxD2z + 2F\u03bbBmax log(T + 1)/\u03b22 . (23)\nUnlike Theorem 4, Theorem 6 shows that OADM can achieve the logarithmic regret bound without requiring ft to be strongly convex, which is in line with other online learning algorithms for composite objectives."}, {"heading": "6. Connections to Related Work", "text": "In this section, we assume \u03b7 = 0,A = I,B = \u2212I, c = 0, thus x = z. The three steps of OADM reduce to\nxt+1 =argmin x\nft(x)+\u3008yt,x\u2212zt\u3009+ \u03c1\n2 \u2016x\u2212zt\u20162 , (24)\nzt+1 =argmin z\ng(z)+\u3008yt,xt+1\u2212z\u3009+ \u03c1\n2 \u2016xt+1\u2212z\u20162, (25)\nyt+1 =yt + \u03c1(xt+1 \u2212 zt+1) . (26)\nLet f \u2032t(xt+1) \u2208 \u2202ft(x), g\u2032(zt+1) \u2208 \u2202g(z). The first order optimality conditions for (24) and (25) give\nf \u2032t(xt+1) + yt + \u03c1(xt+1 \u2212 zt) = 0 , g\u2032(zt+1)\u2212 yt \u2212 \u03c1(xt+1 \u2212 zt+1) = 0 .\nAdding them together yields\nzt+1 = zt \u2212 1\n\u03c1 (f \u2032t(xt+1) + g \u2032(zt+1)) . (27)\nOADM can be considered as taking the implicit subgradient of ft and g at the yet to be determined xt+1 and zt+1. FOBOS has the following update (Duchi & Singer, 2009):\nzt+1 = zt \u2212 1\n\u03c1 (f \u2032t(zt) + g \u2032(zt+1)) .\nFOBOS takes the explicit subgradient of ft at current zt.\nAs a matter of fact, FOBOS can be considered as an inexact OADM, which linearizes the objective of (24) at zt :\nxt+1 = argmin x \u3008f \u2032t(zt) + yt,x\u2212 zt\u3009+\n\u03c4 2 \u2016x\u2212 zt\u20162 .\nIt has the following closed-form solution:\nxt+1 = zt \u2212 1\n\u03c4 (f \u2032t(zt) + yt) . (28)\n(25) is equivalent to the following scaled form :\nzt+1 = argminz g(z) + \u03c1\n2 \u2016xt+1 \u2212 z +\n1 \u03c1 yt\u20162 . (29)\nLet \u03c1 = \u03c4 and zt+ 12 = xt+1 + 1 \u03c4 yt, we get FOBOS (Duchi & Singer, 2009). Furthermore, if g(z) is an indicator function of a convex set \u2126, substituting (28) into (29), we have\nzt+1 = argminz\u2208\u2126 \u03c1\n2 \u2016zt \u2212\n1 \u03c4 f \u2032t(zt)\u2212 z\u20162\n= Pz\u2208\u2126 [ zt \u2212 1\n\u03c4 f \u2032t(zt)\n] .\nWe recover the projected gradient descent (Hazan et al., 2007)."}, {"heading": "7. Experimental Results", "text": "In this section, we use OADM to solve the generalized lasso problems (Boyd et al., 2010), including lasso (Tibshirani, 1996) and total variation (TV)(Rudin et al., 1992). We present simulation results to show the convergence of objective as well as constraints in OADM. We also compare it with batch ADM and other two online learning algorithms: FOBOS and regularized dual averaging (RDA) in selecting sparse dimension in lasso and recovering data in total variation."}, {"heading": "7.1. Generalized Lasso", "text": "The generalized lasso problem is formulated as follows:\nmin x\n1\nN N\u2211 t=1 \u2016atx\u2212 bt\u201622 + \u03bb|Dx|1 , (30)\nwhere at \u2208 R1\u00d7n,x \u2208 Rn\u00d71,D \u2208 Rm\u00d7n and bt is a scalar. If D = I, (30) yields the lasso. If D is an upper bidiagonal matrix with diagonal 1 and off-diagonal\u22121, (30) becomes the total variation. The ADM form of (30) is:\nmin Dx=z\n1\nN N\u2211 t=1 \u2016atx\u2212 bt\u201622 + \u03bb|z|1 , (31)\nwhere z \u2208 Rm\u00d71. The three updates of OADM are:\nxt+1 = (a T t at + \u03c1D TD + \u03b7)\u22121v , (32) zt+1 = S\u03bb/\u03c1(x + u) , (33) ut+1 = ut + xt+1 \u2212 zt+1 , (34)\nwhere u = y/\u03c1, v = aTt bt + \u03c1btD T (z \u2212 u) + \u03b7x, and S\u03bb/\u03c1 denotes the shrinkage operation.\nFor lasso, the x-update is\nxt+1 = (v \u2212 (\u03b7 + \u03c1+ ataTt )\u22121aTt (atv))/(\u03b7 + \u03c1) .\nFor total variation, we set \u03b7 = 0 so that\nxt+1 = (Qv \u2212 (\u03c1+ atQaTt )\u22121QaTt (atQv))/\u03c1 ,\nwhere Q = (DTD)\u22121.\nIn both cases, the three updates (32)-(34) can be done in O(n) flops (Golub & Loan, 1996). In contrast, in batch ADM, the complexity of x-update could be as high as O(n3) or O(n2) by caching factorizations (Boyd et al., 2010). Here, we do not run them in parallel.\nFOBOS and RDA cannot directly solve the TV term. We first reformulate the total variation in the lasso form such that\nmin y\n1\nN N\u2211 t=1 \u2016atD\u22121y \u2212 b\u201622 + \u03bb|y|1 , (35)\nwhere y = Dx. FOBOS and RDA can solve the above lasso problem and get y. x can be recovered by using x = D\u22121y."}, {"heading": "7.2. Simulation", "text": "Our experiments follow the lasso and total variation examples in Boyd\u2019s website,2 although we modified the codes to accommodate our setup. We first randomly generated A with N examples of dimensionality n. A is then normalized along the column. Then, a true x0 is randomly generated with certain sparsity pattern for lasso and TV. b is calculated by adding gaussian noise to Ax0/N . In all experiments, N = 100, which facilitates the matrix inverse in ADM and will be gone through cyclically in the three online learning algorithms. For lasso, we keep the number of nonzeros (NNZs) k = 100 in x and try different combination of parameters from n = [1000, 5000], \u03c1 = [0.1, 1, 10] and q = [0.1, 0.5] for \u03bb = q\u00d7|AT b/N |\u221e. All experiments are implemented in Matlab.\nConvergence: We go through the examples 100 times using OADM. Figure 1(a) shows that NNZs converge to some value close to the actual k = 100 before t = 2000. Figure 1(b) shows the convergence of objective value. In Figure 1(c), the dashed lines are the stopping criteria used in ADM (Boyd et al., 2010). It shows that the equality constraint (top) and primal residual (bottom) are satisfied in the online setting. While the objective converges fast, the equality constraints relatively take more time to be satisfied.\nSparsity: We compare NNZs found by batch ADM and three online learning algorithms, including OADM, FOBOS, and RDA. We set \u03b7 = 1000 for OADM and \u03b3 = 1 for RDA. For FOBOS, we use a time varying parameter \u03c1t = \u03c1/ \u221a t. For online learning algorithms, we go through theN examples 100 times. We run the experiment 20 times and the average results are plotted. Due to the limited space, we only show the results for N = 100, n = 1000, q = 0.5 in Fig. 2. While ADM and RDA tend to give the sparsest results, OADM seems more conservative and converges to reasonably sparse solutions. Fig.2 shows OADM is closest to the actual NNZs 100. The NNZs in FOBOS is large and oscillates in a big range, which has also been observed in (Xiao, 2010).\n2http://www.stanford.edu/\u02dcboyd/papers/ admm/\nTotal Variation: We compare the patterns found by the four algorithms. For all algorithms, N = 100, n = 1000, \u03bb = 0.001 and \u03c1 is chosen through cross validation. In RDA, \u03b3 = 100. Recall that \u03b7 = 0 in OADM. While we use a fixed \u03c1 for OADM and RDA, FOBOS uses \u03c1t = \u03c1/ \u221a t. Figure 3 shows the three different patterns and results found by the algorithms. ADM seems to follow the pattern with obvious oscillation. OADM is smoother and generally follows the trend of the patterns. For the first two examples, FOBOS works well and the patterns found by RDA tend to be flat. In the last example, both FOBOS and RDA oscillate."}, {"heading": "8. Conclusions", "text": "In this paper, we propose an efficient online learning algorithm named online ADM (OADM). New proof techniques have been developed to analyze the convergence of ADM, which shows that ADM has a O(1/T ) convergence rate. Using the proof technique, we establish the regret bounds for the objective and constraint violation for general and strongly convex functions in OADM. Finally, we illustrate the efficacy of OADM in solving lasso and total variation."}, {"heading": "Acknowledgment", "text": "The research was supported by NSF CAREER award IIS0953274, and NSF grants IIS-0916750, IIS-0812183, and IIS-1029711."}, {"heading": "A. Proof of Rate of Convergence of ADM", "text": "Proof: We start by noting that the VI corresponding to the update of xt+1 in (3) is given by: \u2200x \u2208 X\nf(x)\u2212f(xt+1)+\u3008x\u2212xt+1,AT {yt+\u03c1(Axt+1+Bzt\u2212c)}\u3009\u22650 .\nUsing (5), \u2200x \u2208 X\nf(xt+1)\u2212 f(x) + \u3008xt+1 \u2212 x,ATyt+1\u3009 \u2264 \u03c1\u3008Ax\u2212Axt+1,Bzt \u2212Bzt+1\u3009 , (36)\nThe VI corresponding to the update of zt+1 in (4) is given by: \u2200z \u2208 Z ,\ng(z)\u2212g(zt+1)+\u3008z\u2212zt+1,BT {yt+\u03c1(Axt+1+Bzt+1\u2212c)}\u3009\u22650 .\nUsing (5), \u2200x \u2208 X\ng(zt+1)\u2212 g(z) + \u3008zt+1 \u2212 z,BTyt+1\u3009 \u2264 0 , (37)\nAdding (36) and (37) and denoting h(w) = f(x) + g(z), we have \u2200w \u2208 \u2126\nh(wt+1)\u2212h(w)+\u3008wt+1\u2212w,F (wt+1)\u3009 (38)\n\u2264 \u03c1\u3008Ax\u2212Axt+1,Bzt\u2212Bzt+1\u3009+ 1\n\u03c1 \u3008y\u2212yt+1,yt+1\u2212yt\u3009 .\nThe first term can be rewritten as\n2\u3008Ax\u2212Axt+1,Bzt \u2212Bzt+1\u3009 (39) = 2\u3008Ax\u2212 c\u2212 (Axt+1 \u2212 c),Bzt \u2212Bzt+1\u3009 = \u2016Ax + Bzt \u2212 c\u20162 \u2212 \u2016Ax + Bzt+1 \u2212 c\u20162\n+ \u2016Axt+1 + Bzt+1 \u2212 c\u20162 \u2212 \u2016Axt+1 + Bzt \u2212 c\u20162 .\nThe second term in (38) is equivalent to\n2\u3008y \u2212 yt+1,yt+1 \u2212 yt\u3009 (40) = \u2016y \u2212 yt\u20162 \u2212 \u2016y \u2212 yt+1\u20162 \u2212 \u2016yt \u2212 yt+1\u20162 .\nSubstituting (39) and (40) into (38) and summing over t,\nT\u2211 t=1 [h(wt)\u2212 h(w) + \u3008wt \u2212w, F (wt)\u3009] \u2264 L , (41)\nwhere the constant L = \u03c12\u2016Ax \u2212 c\u2016 2 2 + 1 2\u03c1\u2016y\u2016 2. Recall that h(w\u0303) is a convex function of w\u0303. Further, from the definition of F (w\u0303), \u3008w\u0303 \u2212 w, F (w\u0303)\u3009 is a convex function of w\u0303. Dividing both sides of (41) by T , recalling that w\u0304T = 1 T \u2211T t=1 wt, and using Jensen\u2019s inequality, we have\nh(w\u0304T )\u2212 h(w) + \u3008w\u0304T \u2212w, F (w\u0304T )\u3009\n\u2264 1 T T\u2211 t=1 h(wt)\u2212 h(w) + 1 T T\u2211 t=1 \u3008wt \u2212w, F (wt)\u3009\n\u2264 L T = O\n( 1\nT\n) ,\nwhich establishes convergence rate for ADM."}], "references": [{"title": "Decomposition methods for large scale LP decoding", "author": ["S. Barman", "X. Liu", "S. Draper", "B. Recht"], "venue": "In Arxiv,", "citeRegEx": "Barman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Barman et al\\.", "year": 2012}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Operations Research Letters,", "citeRegEx": "Beck and Teboulle,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2003}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1989}, {"title": "Parallel Optimization: Theory, Algorithms, and Applications", "author": ["Y. Censor", "S. Zenios"], "venue": null, "citeRegEx": "Censor and Zenios,? \\Q1998\\E", "shortCiteRegEx": "Censor and Zenios", "year": 1998}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "JMLR, 10:2873\u20132898,", "citeRegEx": "Duchi and Singer,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer", "year": 2009}, {"title": "Composite objective mirror descent", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "In COLT,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Dual averaging for distributed optimization: Convergence analysis and network", "author": ["J. Duchi", "A. Agarwal", "M. Wainwright"], "venue": "scaling. arXiv,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D.P. Bertsekas"], "venue": "Mathematical Programming,", "citeRegEx": "Eckstein and Bertsekas,? \\Q1992\\E", "shortCiteRegEx": "Eckstein and Bertsekas", "year": 1992}, {"title": "Finite-Dimensional Variational Inequalities and Complementarity Problems, volume I", "author": ["F. Facchinei", "Pang", "J.-S"], "venue": null, "citeRegEx": "Facchinei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Facchinei et al\\.", "year": 2003}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite-element approximations", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers and Mathematics with Applications,", "citeRegEx": "Gabay and Mercier,? \\Q1976\\E", "shortCiteRegEx": "Gabay and Mercier", "year": 1976}, {"title": "Matrix Computations,3rd ed", "author": ["G.H. Golub", "C.V. Loan"], "venue": null, "citeRegEx": "Golub and Loan,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan", "year": 1996}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Stochastic approximation approach to stochastic programming", "author": ["A. Juditsky", "G. Lan", "A. Nemirovski", "A. Shapiro"], "venue": "SIAM J. Optim.,", "citeRegEx": "Juditsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Juditsky et al\\.", "year": 2009}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG09-2215,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "The TV patterns found by OADM, ADM, FOBOS and RDA. OADM is the best in recovering", "author": ["M. Mahdavi", "R. Jin", "T. Yang"], "venue": "Arxiv,", "citeRegEx": "Mahdavi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2011}, {"title": "Online learning with constraints", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "In COLT, pp", "citeRegEx": "Mannor and Tsitsiklis,? \\Q2006\\E", "shortCiteRegEx": "Mannor and Tsitsiklis", "year": 2006}, {"title": "An alternating direction method for dual MAP LP relaxation", "author": ["O. Meshi", "A. Globerson"], "venue": "In ECML11,", "citeRegEx": "Meshi and Globerson,? \\Q2011\\E", "shortCiteRegEx": "Meshi and Globerson", "year": 2011}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM J. Optim.,", "citeRegEx": "Nemirovski,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski", "year": 2004}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov,? \\Q2009\\E", "shortCiteRegEx": "Nesterov", "year": 2009}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L. Rudin", "S.J. Osher", "E. Fatemi"], "venue": "Physica D,", "citeRegEx": "Rudin et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 1992}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "JMLR, 11:2543\u20132596,", "citeRegEx": "Xiao,? \\Q2010\\E", "shortCiteRegEx": "Xiao", "year": 2010}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "In recent years, online learning (Zinkevich, 2003; Hazan et al., 2007) and its batch counterpart stochastic gradient descent (Juditsky et al.", "startOffset": 33, "endOffset": 70}, {"referenceID": 11, "context": "In recent years, online learning (Zinkevich, 2003; Hazan et al., 2007) and its batch counterpart stochastic gradient descent (Juditsky et al.", "startOffset": 33, "endOffset": 70}, {"referenceID": 12, "context": ", 2007) and its batch counterpart stochastic gradient descent (Juditsky et al., 2009) has contributed substantially to advances in large scale optimization techniques for machine learning.", "startOffset": 62, "endOffset": 85}, {"referenceID": 5, "context": "Online convex optimization has been generalized to handle time-varying and non-smooth convex functions (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010).", "startOffset": 103, "endOffset": 157}, {"referenceID": 21, "context": "Online convex optimization has been generalized to handle time-varying and non-smooth convex functions (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010).", "startOffset": 103, "endOffset": 157}, {"referenceID": 5, "context": "Composite objective mirror descent (COMID) (Duchi et al., 2010) generalizes mirror descent (Beck & Teboulle, 2003) to the online setting.", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "Regularized dual averaging (RDA) (Xiao, 2010) general-", "startOffset": 33, "endOffset": 45}, {"referenceID": 18, "context": "izes dual averaging (Nesterov, 2009) to online and composite optimization, and can be used for distributed optimization (Duchi et al.", "startOffset": 20, "endOffset": 36}, {"referenceID": 6, "context": "izes dual averaging (Nesterov, 2009) to online and composite optimization, and can be used for distributed optimization (Duchi et al., 2011).", "startOffset": 120, "endOffset": 140}, {"referenceID": 13, "context": "First introduced in (Gabay & Mercier, 1976), the alternating direction method (ADM) has become popular in recent years due to its ease of applicability and empirical performance in a wide variety of problems, including composite objectives (Boyd et al., 2010; Eckstein & Bertsekas, 1992; Lin et al., 2009).", "startOffset": 240, "endOffset": 305}, {"referenceID": 21, "context": "We also present preliminary experimental results illustrating the performance of the proposed OADM algorithms in comparison with FOBOS and RDA (Duchi & Singer, 2009; Xiao, 2010).", "startOffset": 143, "endOffset": 177}, {"referenceID": 14, "context": "Further, the notion of regret in both the objective as well as constraints may contribute towards development of suitable analysis tools for online constrained optimization problems (Mannor & Tsitsiklis, 2006; Mahdavi et al., 2011).", "startOffset": 182, "endOffset": 231}, {"referenceID": 17, "context": "Any w\u2217 = (x\u2217, z\u2217,y\u2217) \u2208 \u03a9 solves the original problem in (1) optimally if it satisfies the following variational inequality (Facchinei & Pang, 2003; Nemirovski, 2004):", "startOffset": 123, "endOffset": 165}, {"referenceID": 11, "context": "If the above problem is solved in every step, standard analysis techniques (Hazan et al., 2007) can be suitably adopted to obtain sublinear regret bounds.", "startOffset": 75, "endOffset": 95}, {"referenceID": 11, "context": "Note that existing online methods, such as projected gradient descent and variants (Hazan et al., 2007; Duchi et al., 2010) do assume a black-box approach for projecting onto the feasible set, which for linear constraints may require iterative cyclic projections (Censor & Zenios, 1998).", "startOffset": 83, "endOffset": 123}, {"referenceID": 5, "context": "Note that existing online methods, such as projected gradient descent and variants (Hazan et al., 2007; Duchi et al., 2010) do assume a black-box approach for projecting onto the feasible set, which for linear constraints may require iterative cyclic projections (Censor & Zenios, 1998).", "startOffset": 83, "endOffset": 123}, {"referenceID": 20, "context": "Examples include generalized lasso and group lasso (Boyd et al., 2010; Tibshirani, 1996; Xiao, 2010).", "startOffset": 51, "endOffset": 100}, {"referenceID": 21, "context": "Examples include generalized lasso and group lasso (Boyd et al., 2010; Tibshirani, 1996; Xiao, 2010).", "startOffset": 51, "endOffset": 100}, {"referenceID": 0, "context": "MAP LP relaxation (Meshi & Globerson, 2011) and LP decoding (Barman et al., 2012), and non-smooth optimization,", "startOffset": 60, "endOffset": 81}, {"referenceID": 13, "context": "robust PCA (Lin et al., 2009) where ft is nuclear norm and g is `1 norm.", "startOffset": 11, "endOffset": 29}, {"referenceID": 14, "context": "(4) For any t, ft(xt+1)+g(zt+1)\u2212(ft(z)+g(z)) \u2265 \u2212F , which is true if the functions are lower bounded or Lipschitz continuous in the convex set (Mahdavi et al., 2011).", "startOffset": 143, "endOffset": 165}, {"referenceID": 5, "context": "In addition to the O( \u221a T ) regret bound, OADM achieves the O( \u221a T ) bound for the constraint violation, which is not existent in the start-ofthe-art online learning algorithms (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010), since they do not explicitly handle linear constraints of the form Axt+Bz = c.", "startOffset": 177, "endOffset": 231}, {"referenceID": 21, "context": "In addition to the O( \u221a T ) regret bound, OADM achieves the O( \u221a T ) bound for the constraint violation, which is not existent in the start-ofthe-art online learning algorithms (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010), since they do not explicitly handle linear constraints of the form Axt+Bz = c.", "startOffset": 177, "endOffset": 231}, {"referenceID": 11, "context": "We recover the projected gradient descent (Hazan et al., 2007).", "startOffset": 42, "endOffset": 62}, {"referenceID": 20, "context": ", 2010), including lasso (Tibshirani, 1996) and total variation (TV)(Rudin et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 19, "context": ", 2010), including lasso (Tibshirani, 1996) and total variation (TV)(Rudin et al., 1992).", "startOffset": 68, "endOffset": 88}, {"referenceID": 21, "context": "The NNZs in FOBOS is large and oscillates in a big range, which has also been observed in (Xiao, 2010).", "startOffset": 90, "endOffset": 102}], "year": 2012, "abstractText": "Online optimization has emerged as powerful tool in large scale optimization. In this paper, we introduce efficient online algorithms based on the alternating directions method (ADM). We introduce a new proof technique for ADM in the batch setting, which yields the O(1/T ) convergence rate of ADM and forms the basis of regret analysis in the online setting. We consider two scenarios in the online setting, based on whether the solution needs to lie in the feasible set or not. In both settings, we establish regret bounds for both the objective function as well as constraint violation for general and strongly convex functions. Preliminary results are presented to illustrate the performance of the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}