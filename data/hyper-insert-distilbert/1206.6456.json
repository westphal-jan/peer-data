{"id": "1206.6456", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Lognormal and Gamma Mixed Negative Binomial Regression", "abstract": "in regression analysis of regression counts, a particular lack of simple techniques and comparatively efficient algorithms for specialized posterior decision computation research has made frequent bayesian approaches appear altogether unattractive and thus underdeveloped. we propose identifying a dual lognormal sequence and direct gamma mixed negative binomial ( hybrid nb ) regression model for residual counts, and present incredibly efficient closed - form bayesian inference ; basically unlike past conventional simplified poisson models, virtually the overall proposed approach instead has deployed two free substitution parameters to include completely two different separate kinds of random random effects, and allows the incorporation of prior scattering information, such variations as spatial sparsity in the temporal regression coefficients. by gradually placing primarily a gamma decay distribution ignoring prior existing on infinitely the simpler nb dispersion parameter r, and so connecting a second lognormal distribution given prior with the logit ratio of precisely the nb residual probability selection parameter p, then efficient effective gibbs sampling and variational bayes inference are actually both successfully developed. only the closed - form updates are obtained by effectively exploiting error conditional filter conjugacy via both a compound poisson representation alternative and emerging a polya - gamma error distribution based reverse data augmentation approach. the proposed bayesian normal inference can be implemented routinely, while slowly being easily generalizable susceptible to assuming more weakly complex settings involving compact multivariate boundary dependence structures. the algorithms constructed are repeatedly illustrated using real time examples.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (477kb)", "http://arxiv.org/abs/1206.6456v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "stat.AP cs.LG stat.ME", "authors": ["mingyuan zhou", "lingbo li", "david b dunson", "lawrence carin"], "accepted": true, "id": "1206.6456"}, "pdf": {"name": "1206.6456.pdf", "metadata": {"source": "META", "title": "Lognormal and Gamma Mixed Negative Binomial Regression", "authors": ["Mingyuan Zhou", "Lingbo Li", "David Dunson"], "emails": ["mz1@ee.duke.edu", "ll83@duke.edu", "dunson@stat.duke.edu", "lcarin@ee.duke.edu"], "sections": [{"heading": "1. Introduction", "text": "In numerous scientific studies, the response variable is a count y = 0, 1, 2, \u00b7 \u00b7 \u00b7 , which we wish to explain with a set of covariates x = [1, x1, \u00b7 \u00b7 \u00b7 , xP ]T as E[y|x] = g\u22121(xT\u03b2), where \u03b2 = [\u03b20, \u00b7 \u00b7 \u00b7 , \u03b2P ]T are the regression coefficients and g is the canonical link function in generalized linear models (GLMs) (McCullagh\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n& Nelder, 1989; Long, 1997; Cameron & Trivedi, 1998; Agresti, 2002; Winkelmann, 2008). Regression models for counts are usually nonlinear and have to take into consideration the specific properties of counts, including discreteness and nonnegativity, and often characterized by overdispersion (variance greater than the mean). In addition, we may wish to impose a sparse prior in the regression coefficients for counts, which is demonstrated to be beneficial for regression analysis of both Gaussian and binary data (Tipping, 2001).\nCount data are commonly modeled with the Poisson distribution y \u223c Pois(\u03bb), whose mean and variance are both equal to \u03bb. Due to heterogeneity (difference between individuals) and contagion (dependence between the occurrence of events), the varance is often much larger than the mean, making the Poisson assumption restrictive. By placing a gamma distribution prior with shape r and scale p/(1\u2212p) on \u03bb, a negative binomial (NB) distribution y \u223c NB(r, p) can be generated as fY (y) = \u222b\u221e 0 Pois(y;\u03bb)Gamma ( \u03bb; r, p1\u2212p ) d\u03bb= \u0393(r+y) y!\u0393(r) (1\u2212 p)\nrpy, where \u0393(\u00b7) denotes the gamma function, r is the nonnegative dispersion parameter and p is a probability parameter. Therefore, the NB distribution is also known as the gamma-Poisson distribution. It has a variance rp/(1\u2212 p)2 larger than the mean rp/(1\u2212p), and thus it is usually favored over the Poisson distribution for modeling overdispersed counts.\nThe regression analysis of counts is commonly performed under the Poisson or NB likelihoods, whose parameters are usually estimated by finding the maximum of the nonlinear log likelihood (Long, 1997; Cameron & Trivedi, 1998; Agresti, 2002; Winkelmann, 2008). The maximum likelihood estimator (MLE), however, only provides a point estimate and does not allow the incorporation of prior information, such as sparsity in the regression coefficients. In addition, the MLE of the NB dispersion parameter r often lacks robustness and may be severely biased or even fail to converge if the sample size is small, the mean is small or if r is large (Saha & Paul, 2005; Lloyd-Smith, 2007).\nCompared to the MLE, Bayesian approaches are able to model the uncertainty of estimation and to incorporate prior information. In regression analysis of counts, however, the lack of simple and efficient algorithms for posterior computation has seriously limited routine applications of Bayesian approaches, making Bayesian analysis of counts appear unattractive and thus underdeveloped. For instance, for the NB dispersion parameter r, the only available closed-form Bayesian solution relies on approximating the ratio of two gamma functions using a polynomial expansion (Bradlow et al., 2002); and for the regression coefficients \u03b2, Bayesian solutions usually involve computationally intensive Metropolis-Hastings algorithms, since the conjugate prior for \u03b2 is not known under the Poisson and NB likelihoods (Chib et al., 1998; Chib & Winkelmann, 2001; Winkelmann, 2008).\nIn this paper we propose a lognormal and gamma mixed NB regression model for counts, with default Bayesian analysis presented based on two novel data augmentation approaches. Specifically, we show that the gamma distribution is the conjugate prior to the NB dispersion parameter r, under the compound Poisson representation, with efficient Gibbs sampling and variational Bayes (VB) inference derived by exploiting conditional conjugacy. Further we show that a lognormal prior can be connected to the logit of the NB probability parameter p, with efficient Gibbs sampling and VB inference developed for the regression coefficients \u03b2 and the lognormal variance parameter \u03c32, by generalizing a Polya-Gamma distribution based data augmentation approach in Polson & Scott (2011). The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. We illustrate the algorithm with real examples on univariate count analysis and count regression, and demonstrate the advantages of the proposed Bayesian approaches over conventional count models."}, {"heading": "2. Regression Models for Counts", "text": "The most basic regression model for counts is the Poisson regression model (Long, 1997; Cameron & Trivedi, 1998; Winkelmann, 2008), which can be expressed as\nyi \u223c Pois(\u03bbi), \u03bbi = exp(xTi \u03b2) (1)\nwhere xi = [1, xi1, \u00b7 \u00b7 \u00b7 , xiP ]T is the covariate vector for sample i. The Newton-Raphson method can be used to iteratively find the MLE of \u03b2 (Long, 1997). A serious constraint of the Poisson regression model is that it assumes equal-dispersion, i.e., E[yi|xi] = Var[yi|xi] = exp(xTi \u03b2). In practice, however, count data are often overdispersed, due to heterogeneity and contagion\n(Winkelmann, 2008). To model overdispersed counts, the Poisson regression model can be modified as\nyi \u223c Pois(\u03bbi), \u03bbi = exp(xTi \u03b2) i (2)\nwhere i is a nonnegative multiplicative random-effect term to model individual heterogeneity (Winkelmann, 2008). Using both the law of total expectation and the law of total variance, it can be shown that\nE[yi|xi] = exp(xTi \u03b2)E[ i] (3)\nVar[yi|xi] = E[yi|xi] + Var[ i]\nE2[ i] E2[yi|xi]. (4)\nThus Var[yi|xi] \u2265 E[yi|xi] and we obtain a regression model for overdispersed counts. We show below that both the gamma and lognormal distributions can be used as the nonnegative prior on i."}, {"heading": "2.1. The Negative Binomial Regression Model", "text": "The NB regression model (Long, 1997; Cameron & Trivedi, 1998; Winkelmann, 2008; Hilbe, 2007) is constructed by placing a gamma prior on i as\ni \u223c Gamma(r, 1/r) = rr\n\u0393(r) i r\u22121e\u2212r i (5)\nwhere E[ i] = 1 and Var[ i] = r\u22121. Marginalizing out i in (2), we have a NB distribution parameterized by mean \u00b5i = exp(x T i \u03b2) and inverse dispersion parameter \u03c6 (the reciprocal of r) as fY (yi) = \u0393(\u03c6\u22121+yi) yi!\u0393(\u03c6\u22121) ( \u03c6\u22121 \u03c6\u22121+\u00b5i )\u03c6\u22121( \u00b5i \u03c6\u22121+\u00b5i )yi , thus\nE[yi|xi] = exp(xTi \u03b2) (6) Var[yi|xi] = E[yi|xi] + \u03c6E2[yi|xi]. (7)\nThe MLEs of \u03b2 and \u03c6 can be found numerically with the Newton-Raphson method (Lawless, 1987)."}, {"heading": "2.2. The Lognormal-Poisson Regression Model", "text": "A lognormal-Poisson regression model (Breslow, 1984; Long, 1997; Agresti, 2002; Winkelmann, 2008) can be constructed by placing a lognormal prior on i as\ni \u223c lnN (0, \u03c32) (8)\nwhere E[ i] = e\u03c3 2/2 and Var[ i] = e \u03c32 ( e\u03c3 2 \u2212 1 ) . Us-\ning (3) and (4), we have\nE[yi|xi] = exp(xTi \u03b2 + \u03c32/2) (9) Var[yi|xi] = E[yi|xi] + ( e\u03c3 2 \u2212 1 ) E2[yi|xi]. (10)\nCompared to the NB model, there is no analytical form for the distribution of yi if i is marginalized out and the MLE is less straightforward to calculate, making\nit less commonly used. However, Winkelmann (2008) suggests to reevaluate the lognormal-Poisson model, since it is appealing in theory and may fit the data better. The inverse Gaussian distribution prior can also be placed on i to construct a heavier-tailed alternative to the NB model (Dean et al., 1989), whose density functions are shown to be virtually identical to the lognormal-Poisson model (Winkelmann, 2008)."}, {"heading": "3. The Lognormal and Gamma Mixed Negative Binomial Regression Model", "text": "To explicitly model the uncertainty of estimation and incorporate prior information, Bayesian approaches appear attractive. Bayesian analysis of counts, however, is seriously limited by the lack of efficient inference, as the conjugate prior for the regression coefficients \u03b2 is unknown under the Poisson and NB likelihoods (Winkelmann, 2008), and the conjugate prior for the NB dispersion parameter r is also unknown.\nTo address these issues, we propose a lognormal and gamma mixed NB regression model for counts, termed here the LGNB model, where a lognormal prior lnN (0, \u03c32) is placed on the multiplicative random effect term i and a gamma prior is placed on r. Denoting pi = e\u03c8i\n1+e\u03c8i = exp(xTi \u03b2) i 1+exp(xTi \u03b2) i , and logit(pi) = ln pi 1\u2212pi ,\nthe LGNB model is constructed as\nyi \u223c NB (r, pi) , \u03c8i = logit(pi) = xTi \u03b2 + ln i (11) i \u223c lnN (0, \u03d5\u22121), \u03d5 \u223c Gamma(e0, 1/f0) (12) \u03b2 \u223c P\u220f p=0 N (0, \u03b1\u22121p ), \u03b1p \u223c Gamma(c0, 1/d0) (13)\nr \u223c Gamma(a0, 1/h), h \u223c Gamma(b0, 1/g0) (14)\nwhere \u03d5 = \u03c3\u22122 and a0, b0, c0, d0, e0, f0 and g0 are gamma hyperparameters (they are set as 0.01 in experiments). Since yi \u223c NB (r, pi) in (11) can be augmented into a gamma-Poisson structure as yi \u223c Pois(\u03bbi), \u03bbi \u223c Gamma ( r, exp(xTi \u03b2) i ) , the LGNB model can also be considered as a lognormalgamma-gamma-Poisson regression model. Denoting X = [xT1 , \u00b7 \u00b7 \u00b7 ,xTN ]T , we may equivalently express \u03c8 = [\u03c81, \u00b7 \u00b7 \u00b7 , \u03c8N ]T in the above model as\n\u03c8 \u223c N (X\u03b2, \u03d5\u22121I). (15)\nIf we marginalize out h in (14), we obtain a beta prime distribution prior r \u223c \u03b2\u2032(a0, b0, 1, g0). If we marginalize out \u03b1p in (13), we obtain a Student-t prior for \u03b2p, the sparsity-promoting prior used in Tipping (2001); Bishop & Tipping (2000) for regression analysis of both Gaussian and binary data. Note that \u03b2 is connected to pi with a logit link, which is key to deriving efficient Bayesian inference."}, {"heading": "3.1. Model Properties and Model Comparison", "text": "Using the laws of total expectation and total variance and the moments of the NB distribution, we have\nE[yi|xi] = E i [E[yi|xi, i]]= exp(x T i \u03b2 + \u03c3 2/2 + ln r) (16) Var[yi|xi]=E i [Var[yi|xi, i]]+Var i [E[yi|xi, i]]\n= E[yi|xi] + ( e\u03c3 2 (1 + r\u22121)\u2212 1 ) E2[yi|xi]. (17)\nWe define the quasi-dispersion \u03ba as the coefficient associated with the mean quadratic term in the variance. As shown in (7) and (10), \u03ba=\u03c6 in the NB model and\n\u03ba = ( e\u03c3 2\u22121 ) in the lognormal-Poisson model. Ap-\nparently, they have different distribution assumptions on dispersion, yet there is no clear evidence to favor one over the other in terms of goodness of fit. In the proposed LGNB model, there are two free parameters r and \u03c32 to adjust both the mean in (16) and\ndispersion \u03ba = ( e\u03c3 2 (1+r\u22121)\u2212 1 ) , which become the\nsame as those of the NB model when \u03c32 = 0, and the same as those of the lognormal-Poisson model when \u03c6= r\u22121 = 0. Thus the LGNB model has one extra degree of freedom to incorporate both kinds of random effects, with their proportion automatically inferred."}, {"heading": "4. Default Bayesian Analysis Using Data Augmentation", "text": "As discussed in Section 3, the LGNB model has an advantage of having two free parameters to incorporate both kinds of random effects. We show below that it has an additional advantage in that default Bayesian analysis can be performed with two novel data augmentation approaches, with closed-form solutions and analytical update equations available for both Gibbs sampling and VB inference. One augmentation approach concerns the inference of the NB dispersion parameter r using the compound Poisson representation, and the other concerns the inference of the regression coefficients \u03b2 using the Polya-Gamma distribution."}, {"heading": "4.1. Inferring the Dispersion Parameter Under the Compound Poisson Representation", "text": "We first focus on inference of the NB dispersion parameter r and assume we know {pi}i=1,N and h, neglecting the remaining part of the LGNB model at this moment. We comment here that the novel Bayesian inference developed here can be applied to any other scenarios where the conditional posterior of r is proportional to \u220fN i=1 NB(yi; r, pi)Gamma(r; a0, 1/h), for which a hybrid Monte Carlo and a Metropolis-Hastings algorithms had been developed in Williamson et al. (2010) and Zhou et al. (2012), but VB solutions were not yet developed.\nAs proved in Quenouille (1949), y \u223c NB(r, p) can also be generated from a compound Poisson distribution as\ny= L\u2211 `=1 u`, L\u223cPois(\u2212r ln(1\u2212 p)), u` iid\u223c Log(p) (18)\nwhere Log(p) corresponds to the logarithmic distribution (Barndorff-Nielsen et al., 2010) with fU (k) = \u2212pk/[k ln(1 \u2212 p)], k \u2208 {1, 2, . . . }, whose probabilitygenerating function (PGF) is\nGU (z) = ln(1\u2212 pz)/ln(1\u2212 p), |z| < p\u22121. (19)\nUsing the conjugacy between the gamma and Poisson distributions, it is evident that the gamma distribution is the conjugate prior for r under this augmentation."}, {"heading": "4.1.1. Gibbs Sampling for r", "text": "Recalling (18), yi \u223c NB(r, pi) can also be generated from the random sum yi = \u2211Li `=1 ui` with\nui` iid\u223c Log(pi), Li \u223c Pois(\u2212r ln(1\u2212 pi)). (20)\nExploiting conjugacy between (14) and (20), given Li, we have the conditional posterior of r as\n(r|\u2212) \u223c Gamma ( a0+\nN\u2211 i=1 Li, 1 h\u2212 \u2211N i=1ln(1\u2212pi)\n) (21)\nwhere here and below expressions like (r|\u2212) correspond to random variable (RV) r, conditioned on all other RVs. The remaining challenge is finding the conditional posterior of Li. Denote wij = \u2211j `=1 ui`, j=1, \u00b7 \u00b7 \u00b7 , yi. Since wij is the summation of j iid Log(pi) distributed RVs, using (19), the PGF of wij is\nGWij(z)= [ln(1\u2212 piz)/ln(1\u2212 pi)] j , |z| < p\u22121i .\nTherefore, we have Li \u2261 0 if yi = 0 and for 1 \u2264 j \u2264 yi\nPr(Li = j|\u2212) \u221d Pr(wij = yi)Pois(j;\u2212r ln(1\u2212 pi))\n= G\n(yi) Wij (0)\nyi! Pois(j;\u2212r ln(1\u2212 pi))\n= dyi\ndzyi f ji (z) \u2223\u2223\u2223\u2223 z=0 rj j!yi! exp(r ln(1\u2212 pi))\n= F (yi, j)r jpyii exp(r ln(1\u2212 pi)) (22)\nwhere fi(z) = \u2212 ln(1\u2212piz) and F is a lower triangular matrix with F (1, 1) = 1, F (m, j) = 0 if j > m, and\nF (m, j) = m\u22121 m F (m\u22121, j)+ 1 m F (m\u22121, j\u22121) (23)\nif 1 \u2264 j \u2264 m. Using (22), we have\nPr(Li = j|\u2212) = Rr(yi, j), j = 0, \u00b7 \u00b7 \u00b7 , yi. (24)\nwhere Rr(0, 0) = 1 and\nRr(m, j) = F (m, j)r j / m\u2211 j\u2032=1 F (m, j\u2032)rj \u2032 . (25)\nThe values of F can be iteratively calculated and each row sums to one, e.g., the 4th and 5th rows of F are(\n6/4! 11/4! 6/4! 1/4! 0 0 \u00b7 \u00b7 \u00b7 24/5! 50/5! 35/5! 10/5! 1/5! 0 \u00b7 \u00b7 \u00b7\n) .\nNote that to obtain (22), we use the relationship proved in Lemma 1 of the supplementary material that\n1\nm!\ndm\ndzm f ji (z) \u2223\u2223\u2223\u2223 z=0 = F (m, j)j!pmi , 1 \u2264 j \u2264 m. (26)\nGibbs sampling for r proceeds by alternately sampling (24) and (21). Note that to ensure numerical stability when r > 1, instead of using (25), we may iteratively calculate Rr in the way we calculate F in (23). We show in Figure 1 of the supplementary material the matrices Rr for r = .1, 1, 10 and 100."}, {"heading": "4.1.2. Variational Bayes Inference for r", "text": "Using VB inference (Bishop & Tipping, 2000; Beal, 2003), we approximate the posterior p(r,L|X) with Q(r,L) = Qr(r) \u220fN i=1QLi(Li), and we have\nQLi(Li) = yi\u2211 j=0 Rr\u0303(yi, j)\u03b4j (27)\nQr(r) = Gamma(a\u0303, 1/h\u0303). (28)\nwhere \u3008x\u3009 = E[x], r\u0303=exp (\u3008ln r\u3009), \u03c8(x) is the digamma function, and\n\u3008ln r\u3009 = \u03c8(a\u0303)\u2212ln h\u0303, \u3008Li\u3009= yi\u2211 j=1 Rr\u0303(yi, j)j (29)\na\u0303 = a0 + N\u2211 i=1 \u3008Li\u3009, h\u0303 = h\u2212 N\u2211 i=1 \u3008ln(1\u2212 pi)\u3009. (30)\nEquations (29)-(30) constitute the VB inference for the NB dispersion parameter r, with \u3008r\u3009 = a\u0303/h\u0303."}, {"heading": "4.2. Inferring the Regression Coefficients Using the Polya-Gamma Distribution", "text": "Denote \u03c9i as a random variable drawn from the PolyaGamma (PG) distribution (Polson & Scott, 2011) as\n\u03c9i \u223c PG(yi + r, 0). (31) We have E\u03c9i [ exp(\u2212\u03c9i\u03c82i /2) ] = cosh\u2212(yi+r)(\u03c8i/2). Thus the likelihood of \u03c8i in (11) can be expressed as\nL(\u03c8i) \u221d (e\u03c8i)\nyi\n(1 + e\u03c8i)yi+r =\n2\u2212(yi+r) exp( yi\u2212r 2 \u03c8i)\ncoshyi+r(\u03c8i/2) \u221d exp (yi \u2212 r 2 \u03c8i ) E\u03c9i [ exp(\u2212\u03c9i\u03c82i /2) ] . (32)\nGiven the values of {\u03c9i}i=1,N and the prior in (15), the conditional posterior of \u03c8 can be expressed as\n(\u03c8|\u2212) \u221d N (\u03c8; X\u03b2, \u03d5\u22121I) N\u220f i=1 e \u2212\u03c9i 2 ( \u03c8i\u2212 yi\u2212r 2\u03c9i )2 (33)\nand given the values of \u03c8 and the prior in (31), the conditional posterior of \u03c9i can be expressed as\n(\u03c9i|\u2212) \u221d exp(\u2212\u03c9i\u03c82i /2)PG(\u03c9i; yi + r, 0). (34)"}, {"heading": "4.3. Gibbs Sampling Inference", "text": "Exploiting conditional conjugacy and the exponential tilting of the PG distribution in Polson & Scott (2011), we can sample in closed-form all latent parameters of the LGNB model described from (11) to (14) as\nSampling Li with (24), Sampling r with (21) (35) (\u03c9i|\u2212) \u223c PG(yi + r, \u03c8i) (36) (\u03c8|\u2212) \u223c N (\u00b5,\u03a3), (\u03b2|\u2212) \u223c N (\u00b5\u03b2,\u03a3\u03b2) (37) (h|\u2212) \u223c Gamma (a0 + b0, 1/(g0 + r)) (38)\n(\u03d5|\u2212) \u223c Gamma ( e0 + N\n2 ,\n1\nf0 + \u2016\u03c8 \u2212X\u03b2\u201622/2\n) (39)\n(\u03b1p|\u2212) \u223c Gamma ( c0 + 1/2, 1/ ( d0 + \u03b2 2 p/2 ))\n(40)\nwhere \u2126 = diag(\u03c91, \u00b7 \u00b7 \u00b7 , \u03c9N ), A = diag(\u03b10, \u00b7 \u00b7 \u00b7 , \u03b1P ), y = [y1, \u00b7 \u00b7 \u00b7 , yN ]T , \u03a3 = (\u03d5I + \u2126)\u22121, \u00b5 = \u03a3[(y \u2212 r)/2 + \u03d5X\u03b2], \u03a3\u03b2 = (\u03d5X TX + A)\u22121 and \u00b5\u03b2 = \u03d5\u03a3\u03b2X T\u03c8. Note that a PG distributed random variable can be generated from an infinite sum of weighted iid gamma random variables (Devroye, 2009; Polson & Scott, 2011). We provide in the supplementary material a method for accurately truncating the infinite sum."}, {"heading": "4.4. Variational Bayes Inference", "text": "Using VB inference (Bishop & Tipping, 2000; Beal, 2003), we approximate the posterior distribution with Q = Q\u03c8(\u03c8)Q\u03b2(\u03b2)Qr(r)Qh(h)Q\u03d5(\u03d5) \u220fP p=0Q\u03b1p(\u03b1p)\u220fN\ni=1[QLi(Li)Q\u03c9i(\u03c9i)]. To exploit conjugacy, defining QLi(Li) as in (27), Qr(r) as in (28), Q\u03c9i(\u03c9i) = PG(\u03b3i1, \u03b3i2), Q\u03c8(\u03c8) = N (\u00b5\u0303, \u03a3\u0303), Q\u03b2(\u03b2) = N (\u00b5\u0303\u03b2, \u03a3\u0303\u03b2), Qh(h) = Gamma(b\u0303, 1/g\u0303), Q\u03d5(\u03d5) = Gamma(e\u0303, 1/f\u0303) and Q\u03b1p(\u03b1p) = Gamma(c\u0303p, 1/d\u0303p), we have\na\u0303=a0+ N\u2211 i=1 \u3008Li\u3009, h\u0303= \u3008h\u3009+ N\u2211 i=1 \u3008ln(1 + e\u03c8i)\u3009 (41)\n\u03a3\u0303= ( \u3008\u03d5\u3009I+\u2126\u0303 )\u22121 , \u00b5\u0303=\u03a3\u0303 [(y\u2212\u3008r\u3009)/2+\u3008\u03d5\u3009X\u03b2] (42)\n\u03a3\u0303\u03b2 = ( \u3008\u03d5\u3009XTX+\u3008A\u0303\u3009 )\u22121 , \u00b5\u0303\u03b2 = \u3008\u03d5\u3009\u03a3\u0303\u03b2X T \u3008\u03c8\u3009 (43) b\u0303 = a0 + b0, g\u0303 = \u3008r\u3009+ g0, e\u0303 = e0 +N/2 (44) f\u0303=f0+ \u3008\u03c8T\u03c8\u3009\n2 \u2212\u3008\u03c8\u3009TX\u3008\u03b2\u3009+ tr[X\u3008\u03b2\u03b2 T \u3009XT ] 2\n(45)\nc\u0303p = c0 + 1/2, d\u0303p = d0 + \u3008\u03b22p\u3009/2 (46)\nwhere \u2126\u0303 = diag(\u3008\u03c91\u3009, \u00b7 \u00b7 \u00b7 , \u3008\u03c9N \u3009), A\u0303 = diag(\u03b1\u03030, \u00b7 \u00b7 \u00b7 , \u03b1\u0303P ), tr[\u03a3] is the trace of \u03a3, \u3008ln r\u3009 and \u3008Li\u3009 are calculated\nas in (29), \u3008r\u3009 = a\u0303/h\u0303, \u3008\u03c8T\u03c8\u3009 = \u00b5\u0303T \u00b5\u0303 + tr[\u03a3\u0303], \u3008\u03b2\u3009 = \u00b5\u0303\u03b2, \u3008\u03b2T\u03b2\u3009 = \u00b5\u0303T\u03b2 \u00b5\u0303\u03b2+ tr[\u03a3\u0303\u03b2], \u3008\u03b2\u03b2T \u3009 = \u00b5\u0303\u03b2\u00b5\u0303T\u03b2 + \u03a3\u0303\u03b2, \u3008h\u3009 = b\u0303/g\u0303, \u3008\u03d5\u3009 = e\u0303/f\u0303 and \u3008\u03b1p\u3009 = c\u0303p/d\u0303p. Although we do not have analytical forms for \u03b3i1 and \u03b3i2 in Q\u03c9i(\u03c9i), we can use (36) to calculate \u3008\u03c9i\u3009 as\n\u3008\u03c9i\u3009 = Er,\u03c8i [E[\u03c9i|r, \u03c8i, yi]] = (yi+\u3008r\u3009) \u2329 tanh(\u03c8i/2)\n2\u03c8i\n\u232a (47)\nwhere the mean property of the PG distribution1 (Polson & Scott, 2011) is applied. To calculate \u3008ln(1+e\u03c8i)\u3009 in (41) and \u2329 tanh(\u03c8i/2)\n2\u03c8i\n\u232a in (47), we use the Monte\nCarlo integration algorithm (Andrieu et al., 2003)."}, {"heading": "5. Example Results", "text": ""}, {"heading": "5.1. Univariate Count Data Analysis", "text": "The inference of the NB dispersion parameter r by itself plays an important role not only for the NB regression (Lawless, 1987; Winkelmann, 2008) but also for univariate count data analysis (Bliss & Fisher, 1953; Clark & Perry, 1989; Saha & Paul, 2005; Lloyd-Smith, 2007), and it also arises in some recently proposed latent variable models for count matrix factorization (Williamson et al., 2010; Zhou et al., 2012). Thus it is of interest to evaluate the proposed closed-form Gibbs sampling and VB inference for this parameter alone, before introducing the regression analysis part.\nWe consider a real dataset describing counts of red mites on apple leaves, given in Table 1 of Bliss & Fisher (1953). There were in total 172 adult female mites found in 150 randomly selected leaves, with a 0 count on 70 leaves, 1 on 38, 2 on 17, 3 on 10, 4 on 9, 5 on 3, 6 on 2 and 7 on 1. This dataset has a mean of 1.1467 and a variance of 2.2736, clearly overdispersed. We assume the counts are NB distributed and we intend to infer r with a hierarchical model as\nyi iid\u223c NB(r, p), r \u223c Gamma(a, 1/b), p \u223c Beta(\u03b1, \u03b2)\nwhere i = 1, \u00b7 \u00b7 \u00b7 , N and we set a = b = \u03b1 = \u03b2 = 0.01. We consider 20,000 Gibbs sampling iterations, with the first 10,000 samples discarded and every fifth sample collected afterwards. As shown in Figure 1, the autocorrelation of Gibbs samples decreases quickly as the lag increases, and the VB lower bound converges quickly even starting from a bad initialization (r is initialized two times the converged value).\nThe estimated posterior mean of r is 1.0812 with Gibbs sampling and 0.9988 with VB. Compared to the method of moments estimator (MME), MLE, and maximum quasi-likelihood estimator (MQLE) (Clark\n1There is a typo in B.2 Lemma 2 and other related equations of Polson & Scott (2011), where E(\u03c9) = a\nc tanh( c 2 )\nshould be corrected as E(\u03c9) = a 2c tanh( c 2 ).\n& Perry, 1989), which provides point estimates of 1.1667, 1.0246 and 0.99472, respectively, our algorithm is able to provide a full posterior distribution of r and is convenient to incorporate prior information. The calculating details of the MME, MLE and MQLE, the closed-form Gibbs sampling and VB update equations, and the VB lower bound are all provided in the supplementary material, omitted here for brevity."}, {"heading": "5.2. Regression Analysis of Counts", "text": "We test the full LGNB model on two real examples, with comparison to the Poisson, NB, lognormalPoisson and inverse-Gaussian-Poisson (IG-Poisson) regression models. The NASCAR dataset3, analyzed in Winner, consists of 151 NASCAR races during the 1975-1979 Seasons. The response variable is the number of lead changes in a race, and the covariates of a race include the number of laps, number of drivers and length of the track (in miles). The MotorIns dataset4, analyzed in Dean et al. (1989), consists of Swedish third-party motor insurance claims in 1977. Included in the data are the total number of claims for automobiles insured in each of the 315 risk groups, defined by a combination of DISTANCE, BONUS, and MAKE factor levels. The number of insured automobile-years for each group is also given. As in Dean et al. (1989), a 19 dimensional covariate vector is constructed for each group to represent levels of the factors. To test goodness-of-fit, we use the Pearson residuals, a metric widely used in GLMs (McCullagh & Nelder, 1989), calculated as\n2The inverse dispersion parameter \u03c6 = 1/0.9947 = 1.005 is mistakenly reported as the dispersion parameter r in Clark & Perry (1989) at Line 15, Page 314.\n3http://www.stat.ufl.edu/~winner/datasets.html 4http://www.statsci.org/data/general/motorins.\ntxt\nwhere \u00b5\u0302 and \u03ba\u0302 are the estimated mean and quasidispersion, respectively, whose calculations are described in detail in the supplementary material.\nThe MLEs for the Poisson and NB models are wellknown and the update equations can be found in Winner; Winkelmann (2008). The MLE results for the IGPoisson model on the MotorIns data were reported in Dean et al. (1989). For the lognormal-Poisson model, no standard MLE algorithms are available and we choose Metropolis-Hastings (M-H) algorithms for parameter estimation. We also consider a LGNB model under the special setting that r = 1000. As discussed in Section 3.1, this would lead to a model which is approximately the lognormal-Poisson model, yet with closed-form Gibbs sampling inference. We use both VB and Gibbs sampling for the LGNB model. We consider 20,000 Gibbs sampling iterations, with the first 10,000 samples discarded and every fifth sample collected afterwards. As described in the supplementary material, we sample from the PG distribution with a truncation level of 2000. We initialize r as 100 and other parameters at random. Examining the samples in Gibbs sampling, we find that the autocorrelations of model parameters generally reduce to below 0.2 at the lag of 20, indicating fast mixing.\nShown in Table 1 are the MLEs or posterior means of key model parameters. Note that \u03b20 of the LGNB model differs considerably from that of the Poisson and NB models, which is expected since \u03b20 +\u03c3 2/2 + ln r in\nthe LGNB model plays about the same role as \u03b20 in the Poisson and NB models, as indicated in (16).\nAs shown in Tables 2, in terms of goodness of fit measured by Pearson residuals, the Poisson model performs the worst due to its unrealistic equal-dispersion assumption; the NB model, assuming a gamma distributed multiplicative random effect term, significantly improves the performances compared to the Poisson model; the proposed LGNB model, modeling extra-Poisson variations with both the gamma and lognormal distributions, clearly outperforms both the Poisson and NB models. Since for the lognormalPoisson model with the M-H algorithm, we were not able to obtain comparable results even after carefully tuning the proposal distribution, we did not include it here for comparison. However, since the LGNB model reduces to the lognormal-Poisson model as r \u2192 \u221e, the results of the LGNB model with r \u2261 1000 would be able to indicate whether the lognormal distribution alone is appropriate to model the extra-Poisson variations. Despite the popularity of the NB model, which models extra-Poisson variations only with the gamma distribution, the results in Tables 2 suggest the benefits of incorporating the lognormal random effects. These observations also support the claim in (Winkelmann, 2008) that the lognormal-Poisson model should be reevaluated since it is appealing in theory and may fit the data better. Compared to the lognormalPoisson model, the LGNB model has an additional advantage that its parameters can be estimated with VB inference, which is usually much faster than sampling based methods.\nA clear advantage of the Bayesian inference over the MLE is that a full posterior distribution can be obtained, by utilizing the estimated posteriors of \u03c32, r and \u03b2. For example, shown in Figure 2 are the estimated posterior distributions of the quasi-dispersion \u03ba, represented with histograms. These histograms should be compared to \u03ba = 0 in the Poisson model, and the NB model\u2019s MLEs of \u03ba = 0.1905 and \u03ba = 0.0118, for the NASCAR and MotorIns datasets, respectively. We can also find that VB generally tends to overemphasize the regions around the mode of its estimated posterior distribution and consequently places low densities on the tails, whereas Gibbs sampling is able to explore a wider region. This is intuitive since VB relies on the assumption that the posterior distribution can be approximated with the product of independent Q functions, whereas Gibbs sampling only exploits conditional independence.\nThe estimated posteriors can also assist model interpretation. For example, based on \u03a3\u0303\u03b2 in VB for the\nNASCAR dataset, we can calculate the correlation matrix for (\u03b21, \u03b22, \u03b23)\nT as 1.0000 \u22120.4824 0.8933\u22120.4824 1.0000 \u22120.7171 0.8933 \u22120.7171 1.0000  which is typically not provided in MLE. Since \u03b21 (Laps) and \u03b23 (TrkLen) are highly positively correlated, we expect the corresponding covariates to be highly negatively correlated. This is confirmed, as the correlation coefficient between the number of laps and the track length is found to be as small as \u22120.9006."}, {"heading": "6. Conclusions", "text": "A lognormal and gamma mixed negative binomial (LGNB) regression model is proposed for regression analysis of overdispersed counts. Efficient closed-form Gibbs sampling and VB inference are both presented, by exploiting the compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. Model properties are examined, with comparison to the Poisson, NB and lognormal-Poisson models. As the univariate lognormal-Poisson regression model can be easily generalized to regression analysis of correlated counts, in which the derivatives and Hessian matrixes of parameters are used to construct multivariate normal proposals in a MetropolisHastings algorithm (Chib et al., 1998; Chib & Winkelmann, 2001; Ma et al., 2008; Winkelmann, 2008), the proposed LGNB model can be conveniently modified for multivariate count regression, in which we may be able to derive closed-form Gibbs sampling and VB inference. As the log Gaussian process can be used to model the intensity of the Poisson process, whose inference remains a major challenge (M\u00f8ller et al., 1998; Adams et al., 2009; Murray et al., 2010; Rao & Teh, 2011), we may link the log Gaussian process to the\nlogit of the NB probability parameter, leading to a log Gaussian NB process with tractable closed-form Bayesian inference. Furthermore, the NB distribution is shown to be important for the factorization of a term-document count matrix (Williamson et al., 2010; Zhou et al., 2012), and the multinomial logit has been used to model correlated topics in topic modeling (Blei & Lafferty, 2005; Paisley et al., 2011). Applying the proposed lognormal-gamma-NB framework and the developed closed-form Bayesian inference to these diverse problems is currently under active investigation."}, {"heading": "Acknowledgements", "text": "The research reported here has been supported in part by DARPA under the MSEE program."}], "references": [{"title": "Tractable nonparametric Bayesian inference in Poisson processes with Gaussian process intensities", "author": ["R. Adams", "I. Murray", "D. MacKay"], "venue": "In ICML,", "citeRegEx": "Adams et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2009}, {"title": "Categorical Data Analysis", "author": ["A. Agresti"], "venue": "Wiley-Interscience, 2nd edition,", "citeRegEx": "Agresti,? \\Q2002\\E", "shortCiteRegEx": "Agresti", "year": 2002}, {"title": "An introduction to MCMC for machine learning", "author": ["C. Andrieu", "N. de Freitas", "A. Doucet", "M.I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "Integer-valued L\u00e9vy processes and low latency financial econometrics", "author": ["O.E. Barndorff-Nielsen", "D.G. Pollard", "N. Shephard"], "venue": null, "citeRegEx": "Barndorff.Nielsen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barndorff.Nielsen et al\\.", "year": 2010}, {"title": "Variational Algorithms for Approximate Bayesian Inference", "author": ["M.J. Beal"], "venue": "PhD thesis,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "Variational relevance vector machines", "author": ["C.M. Bishop", "M.E. Tipping"], "venue": "In UAI,", "citeRegEx": "Bishop and Tipping,? \\Q2000\\E", "shortCiteRegEx": "Bishop and Tipping", "year": 2000}, {"title": "Correlated topic models", "author": ["D. Blei", "J.D. Lafferty"], "venue": "In NIPS,", "citeRegEx": "Blei and Lafferty,? \\Q2005\\E", "shortCiteRegEx": "Blei and Lafferty", "year": 2005}, {"title": "Fitting the negative binomial distribution to biological data", "author": ["C.I. Bliss", "R.A. Fisher"], "venue": null, "citeRegEx": "Bliss and Fisher,? \\Q1953\\E", "shortCiteRegEx": "Bliss and Fisher", "year": 1953}, {"title": "Bayesian inference for the negative binomial distribution via polynomial expansions", "author": ["E.T. Bradlow", "B.G.S. Hardie", "P.S. Fader"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Bradlow et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bradlow et al\\.", "year": 2002}, {"title": "Extra-Poisson variation in log-linear models", "author": ["N.E. Breslow"], "venue": "J. Roy. Statist. Soc., C,", "citeRegEx": "Breslow,? \\Q1984\\E", "shortCiteRegEx": "Breslow", "year": 1984}, {"title": "Regression Analysis of Count Data", "author": ["A.C. Cameron", "P.K. Trivedi"], "venue": null, "citeRegEx": "Cameron and Trivedi,? \\Q1998\\E", "shortCiteRegEx": "Cameron and Trivedi", "year": 1998}, {"title": "Markov chain Monte Carlo analysis of correlated count data", "author": ["S. Chib", "R. Winkelmann"], "venue": "Journal of Business & Economic Statistics,", "citeRegEx": "Chib and Winkelmann,? \\Q2001\\E", "shortCiteRegEx": "Chib and Winkelmann", "year": 2001}, {"title": "Posterior simulation and Bayes factors in panel count data models", "author": ["S Chib", "E Greenberg", "R. Winkelmann"], "venue": "Journal of Econometrics,", "citeRegEx": "Chib et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chib et al\\.", "year": 1998}, {"title": "A mixed Poisson-inverse-Gaussian regression model", "author": ["C. Dean", "J.F. Lawless", "G.E. Willmot"], "venue": "Canadian Journal of Statistics,", "citeRegEx": "Dean et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1989}, {"title": "On exact simulation algorithms for some distributions related to Jacobi theta functions", "author": ["L. Devroye"], "venue": "Statistics & Probability Letters,", "citeRegEx": "Devroye,? \\Q2009\\E", "shortCiteRegEx": "Devroye", "year": 2009}, {"title": "Negative Binomial Regression", "author": ["J.M. Hilbe"], "venue": null, "citeRegEx": "Hilbe,? \\Q2007\\E", "shortCiteRegEx": "Hilbe", "year": 2007}, {"title": "Negative binomial and mixed Poisson regression", "author": ["J.F. Lawless"], "venue": "Canadian Journal of Statistics,", "citeRegEx": "Lawless,? \\Q1987\\E", "shortCiteRegEx": "Lawless", "year": 1987}, {"title": "Maximum likelihood estimation of the negative binomial dispersion parameter for highly overdispersed data, with applications to infectious diseases", "author": ["J.O. Lloyd-Smith"], "venue": "PLoS ONE,", "citeRegEx": "Lloyd.Smith,? \\Q2007\\E", "shortCiteRegEx": "Lloyd.Smith", "year": 2007}, {"title": "Regression Models for Categorical and Limited Dependent Variables", "author": ["S.J. Long"], "venue": null, "citeRegEx": "Long,? \\Q1997\\E", "shortCiteRegEx": "Long", "year": 1997}, {"title": "A multivariate Poisson-lognormal regression model for prediction of crash counts by severity, using Bayesian methods", "author": ["J. Ma", "K.M. Kockelman", "P. Damien"], "venue": "Accident Analysis and Prevention,", "citeRegEx": "Ma et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2008}, {"title": "Generalized linear models", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "McCullagh and Nelder,? \\Q1989\\E", "shortCiteRegEx": "McCullagh and Nelder", "year": 1989}, {"title": "Elliptical slice sampling", "author": ["I. Murray", "R.P. Adams", "D.J.C. MacKay"], "venue": "In AISTATS,", "citeRegEx": "Murray et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2010}, {"title": "The discrete infinite logistic normal distribution for mixed-membership modeling", "author": ["J. Paisley", "C. Wang", "D.M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Paisley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2011}, {"title": "Default Bayesian analysis for multi-way tables: a data-augmentation approach", "author": ["N.G. Polson", "J.G. Scott"], "venue": null, "citeRegEx": "Polson and Scott,? \\Q2011\\E", "shortCiteRegEx": "Polson and Scott", "year": 2011}, {"title": "A relation between the logarithmic, Poisson, and negative binomial series", "author": ["M.H. Quenouille"], "venue": null, "citeRegEx": "Quenouille,? \\Q1949\\E", "shortCiteRegEx": "Quenouille", "year": 1949}, {"title": "Gaussian process modulated renewal processes", "author": ["V. Rao", "Y.W. Teh"], "venue": "In NIPS,", "citeRegEx": "Rao and Teh,? \\Q2011\\E", "shortCiteRegEx": "Rao and Teh", "year": 2011}, {"title": "Bias-corrected maximum likelihood estimator of the negative binomial dispersion parameter", "author": ["K. Saha", "S. Paul"], "venue": null, "citeRegEx": "Saha and Paul,? \\Q2005\\E", "shortCiteRegEx": "Saha and Paul", "year": 2005}, {"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Tipping,? \\Q2001\\E", "shortCiteRegEx": "Tipping", "year": 2001}, {"title": "The IBP compound Dirichlet process and its application to focused topic modeling", "author": ["S. Williamson", "C. Wang", "Heller", "Katherine A", "D.M. Blei"], "venue": "In ICML,", "citeRegEx": "Williamson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2010}, {"title": "Econometric Analysis of Count Data", "author": ["R. Winkelmann"], "venue": null, "citeRegEx": "Winkelmann,? \\Q2008\\E", "shortCiteRegEx": "Winkelmann", "year": 2008}, {"title": "Case Study \u2013 Negative Binomial Regression, NASCAR Lead Changes 1975-1979", "author": ["L. Winner"], "venue": "URL http://www. stat.ufl.edu/~winner/cases/nb_nascar.doc", "citeRegEx": "Winner,? \\Q2012\\E", "shortCiteRegEx": "Winner", "year": 2012}, {"title": "Betanegative binomial process and Poisson factor analysis", "author": ["M. Zhou", "L. Hannah", "D. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 27, "context": "In addition, we may wish to impose a sparse prior in the regression coefficients for counts, which is demonstrated to be beneficial for regression analysis of both Gaussian and binary data (Tipping, 2001).", "startOffset": 189, "endOffset": 204}, {"referenceID": 18, "context": "The regression analysis of counts is commonly performed under the Poisson or NB likelihoods, whose parameters are usually estimated by finding the maximum of the nonlinear log likelihood (Long, 1997; Cameron & Trivedi, 1998; Agresti, 2002; Winkelmann, 2008).", "startOffset": 187, "endOffset": 257}, {"referenceID": 1, "context": "The regression analysis of counts is commonly performed under the Poisson or NB likelihoods, whose parameters are usually estimated by finding the maximum of the nonlinear log likelihood (Long, 1997; Cameron & Trivedi, 1998; Agresti, 2002; Winkelmann, 2008).", "startOffset": 187, "endOffset": 257}, {"referenceID": 29, "context": "The regression analysis of counts is commonly performed under the Poisson or NB likelihoods, whose parameters are usually estimated by finding the maximum of the nonlinear log likelihood (Long, 1997; Cameron & Trivedi, 1998; Agresti, 2002; Winkelmann, 2008).", "startOffset": 187, "endOffset": 257}, {"referenceID": 17, "context": "In addition, the MLE of the NB dispersion parameter r often lacks robustness and may be severely biased or even fail to converge if the sample size is small, the mean is small or if r is large (Saha & Paul, 2005; Lloyd-Smith, 2007).", "startOffset": 193, "endOffset": 231}, {"referenceID": 8, "context": "For instance, for the NB dispersion parameter r, the only available closed-form Bayesian solution relies on approximating the ratio of two gamma functions using a polynomial expansion (Bradlow et al., 2002); and for the regression coefficients \u03b2, Bayesian solutions usually involve computationally intensive Metropolis-Hastings algorithms, since the conjugate prior for \u03b2 is not known under the Poisson and NB likelihoods (Chib et al.", "startOffset": 184, "endOffset": 206}, {"referenceID": 12, "context": ", 2002); and for the regression coefficients \u03b2, Bayesian solutions usually involve computationally intensive Metropolis-Hastings algorithms, since the conjugate prior for \u03b2 is not known under the Poisson and NB likelihoods (Chib et al., 1998; Chib & Winkelmann, 2001; Winkelmann, 2008).", "startOffset": 223, "endOffset": 285}, {"referenceID": 29, "context": ", 2002); and for the regression coefficients \u03b2, Bayesian solutions usually involve computationally intensive Metropolis-Hastings algorithms, since the conjugate prior for \u03b2 is not known under the Poisson and NB likelihoods (Chib et al., 1998; Chib & Winkelmann, 2001; Winkelmann, 2008).", "startOffset": 223, "endOffset": 285}, {"referenceID": 18, "context": "The most basic regression model for counts is the Poisson regression model (Long, 1997; Cameron & Trivedi, 1998; Winkelmann, 2008), which can be expressed as", "startOffset": 75, "endOffset": 130}, {"referenceID": 29, "context": "The most basic regression model for counts is the Poisson regression model (Long, 1997; Cameron & Trivedi, 1998; Winkelmann, 2008), which can be expressed as", "startOffset": 75, "endOffset": 130}, {"referenceID": 18, "context": "The Newton-Raphson method can be used to iteratively find the MLE of \u03b2 (Long, 1997).", "startOffset": 71, "endOffset": 83}, {"referenceID": 29, "context": "In practice, however, count data are often overdispersed, due to heterogeneity and contagion (Winkelmann, 2008).", "startOffset": 93, "endOffset": 111}, {"referenceID": 29, "context": "To model overdispersed counts, the Poisson regression model can be modified as yi \u223c Pois(\u03bbi), \u03bbi = exp(xi \u03b2) i (2) where i is a nonnegative multiplicative random-effect term to model individual heterogeneity (Winkelmann, 2008).", "startOffset": 208, "endOffset": 226}, {"referenceID": 18, "context": "The NB regression model (Long, 1997; Cameron & Trivedi, 1998; Winkelmann, 2008; Hilbe, 2007) is constructed by placing a gamma prior on i as i \u223c Gamma(r, 1/r) = r \u0393(r) i r\u22121e\u2212r i (5) where E[ i] = 1 and Var[ i] = r\u22121.", "startOffset": 24, "endOffset": 92}, {"referenceID": 29, "context": "The NB regression model (Long, 1997; Cameron & Trivedi, 1998; Winkelmann, 2008; Hilbe, 2007) is constructed by placing a gamma prior on i as i \u223c Gamma(r, 1/r) = r \u0393(r) i r\u22121e\u2212r i (5) where E[ i] = 1 and Var[ i] = r\u22121.", "startOffset": 24, "endOffset": 92}, {"referenceID": 15, "context": "The NB regression model (Long, 1997; Cameron & Trivedi, 1998; Winkelmann, 2008; Hilbe, 2007) is constructed by placing a gamma prior on i as i \u223c Gamma(r, 1/r) = r \u0393(r) i r\u22121e\u2212r i (5) where E[ i] = 1 and Var[ i] = r\u22121.", "startOffset": 24, "endOffset": 92}, {"referenceID": 16, "context": "The MLEs of \u03b2 and \u03c6 can be found numerically with the Newton-Raphson method (Lawless, 1987).", "startOffset": 76, "endOffset": 91}, {"referenceID": 9, "context": "A lognormal-Poisson regression model (Breslow, 1984; Long, 1997; Agresti, 2002; Winkelmann, 2008) can be constructed by placing a lognormal prior on i as i \u223c lnN (0, \u03c3) (8)", "startOffset": 37, "endOffset": 97}, {"referenceID": 18, "context": "A lognormal-Poisson regression model (Breslow, 1984; Long, 1997; Agresti, 2002; Winkelmann, 2008) can be constructed by placing a lognormal prior on i as i \u223c lnN (0, \u03c3) (8)", "startOffset": 37, "endOffset": 97}, {"referenceID": 1, "context": "A lognormal-Poisson regression model (Breslow, 1984; Long, 1997; Agresti, 2002; Winkelmann, 2008) can be constructed by placing a lognormal prior on i as i \u223c lnN (0, \u03c3) (8)", "startOffset": 37, "endOffset": 97}, {"referenceID": 29, "context": "A lognormal-Poisson regression model (Breslow, 1984; Long, 1997; Agresti, 2002; Winkelmann, 2008) can be constructed by placing a lognormal prior on i as i \u223c lnN (0, \u03c3) (8)", "startOffset": 37, "endOffset": 97}, {"referenceID": 13, "context": "The inverse Gaussian distribution prior can also be placed on i to construct a heavier-tailed alternative to the NB model (Dean et al., 1989), whose density functions are shown to be virtually identical to the lognormal-Poisson model (Winkelmann, 2008).", "startOffset": 122, "endOffset": 141}, {"referenceID": 29, "context": ", 1989), whose density functions are shown to be virtually identical to the lognormal-Poisson model (Winkelmann, 2008).", "startOffset": 100, "endOffset": 118}, {"referenceID": 28, "context": "However, Winkelmann (2008) suggests to reevaluate the lognormal-Poisson model, since it is appealing in theory and may fit the data better.", "startOffset": 9, "endOffset": 27}, {"referenceID": 29, "context": "Bayesian analysis of counts, however, is seriously limited by the lack of efficient inference, as the conjugate prior for the regression coefficients \u03b2 is unknown under the Poisson and NB likelihoods (Winkelmann, 2008), and the conjugate prior for the NB dispersion parameter r is also unknown.", "startOffset": 200, "endOffset": 218}, {"referenceID": 27, "context": "If we marginalize out \u03b1p in (13), we obtain a Student-t prior for \u03b2p, the sparsity-promoting prior used in Tipping (2001); Bishop & Tipping (2000) for regression analysis of both Gaussian and binary data.", "startOffset": 107, "endOffset": 122}, {"referenceID": 27, "context": "If we marginalize out \u03b1p in (13), we obtain a Student-t prior for \u03b2p, the sparsity-promoting prior used in Tipping (2001); Bishop & Tipping (2000) for regression analysis of both Gaussian and binary data.", "startOffset": 107, "endOffset": 147}, {"referenceID": 28, "context": "We comment here that the novel Bayesian inference developed here can be applied to any other scenarios where the conditional posterior of r is proportional to \u220fN i=1 NB(yi; r, pi)Gamma(r; a0, 1/h), for which a hybrid Monte Carlo and a Metropolis-Hastings algorithms had been developed in Williamson et al. (2010) and Zhou et al.", "startOffset": 288, "endOffset": 313}, {"referenceID": 28, "context": "We comment here that the novel Bayesian inference developed here can be applied to any other scenarios where the conditional posterior of r is proportional to \u220fN i=1 NB(yi; r, pi)Gamma(r; a0, 1/h), for which a hybrid Monte Carlo and a Metropolis-Hastings algorithms had been developed in Williamson et al. (2010) and Zhou et al. (2012), but VB solutions were not yet developed.", "startOffset": 288, "endOffset": 336}, {"referenceID": 24, "context": "As proved in Quenouille (1949), y \u223c NB(r, p) can also be generated from a compound Poisson distribution as", "startOffset": 13, "endOffset": 31}, {"referenceID": 3, "context": "where Log(p) corresponds to the logarithmic distribution (Barndorff-Nielsen et al., 2010) with fU (k) = \u2212p/[k ln(1 \u2212 p)], k \u2208 {1, 2, .", "startOffset": 57, "endOffset": 89}, {"referenceID": 4, "context": "Using VB inference (Bishop & Tipping, 2000; Beal, 2003), we approximate the posterior p(r,L|X) with Q(r,L) = Qr(r) \u220fN i=1QLi(Li), and we have", "startOffset": 19, "endOffset": 55}, {"referenceID": 14, "context": "Note that a PG distributed random variable can be generated from an infinite sum of weighted iid gamma random variables (Devroye, 2009; Polson & Scott, 2011).", "startOffset": 120, "endOffset": 157}, {"referenceID": 4, "context": "Using VB inference (Bishop & Tipping, 2000; Beal, 2003), we approximate the posterior distribution with Q = Q\u03c8(\u03c8)Q\u03b2(\u03b2)Qr(r)Qh(h)Q\u03c6(\u03c6) \u220fP p=0Q\u03b1p(\u03b1p) \u220fN i=1[QLi(Li)Q\u03c9i(\u03c9i)].", "startOffset": 19, "endOffset": 55}, {"referenceID": 2, "context": "To calculate \u3008ln(1+ei)\u3009 in (41) and \u3008 tanh(\u03c8i/2) 2\u03c8i \u3009 in (47), we use the Monte Carlo integration algorithm (Andrieu et al., 2003).", "startOffset": 109, "endOffset": 131}, {"referenceID": 16, "context": "The inference of the NB dispersion parameter r by itself plays an important role not only for the NB regression (Lawless, 1987; Winkelmann, 2008) but also for univariate count data analysis (Bliss & Fisher, 1953; Clark & Perry, 1989; Saha & Paul, 2005; Lloyd-Smith, 2007), and it also arises in some recently proposed latent variable models for count matrix factorization (Williamson et al.", "startOffset": 112, "endOffset": 145}, {"referenceID": 29, "context": "The inference of the NB dispersion parameter r by itself plays an important role not only for the NB regression (Lawless, 1987; Winkelmann, 2008) but also for univariate count data analysis (Bliss & Fisher, 1953; Clark & Perry, 1989; Saha & Paul, 2005; Lloyd-Smith, 2007), and it also arises in some recently proposed latent variable models for count matrix factorization (Williamson et al.", "startOffset": 112, "endOffset": 145}, {"referenceID": 17, "context": "The inference of the NB dispersion parameter r by itself plays an important role not only for the NB regression (Lawless, 1987; Winkelmann, 2008) but also for univariate count data analysis (Bliss & Fisher, 1953; Clark & Perry, 1989; Saha & Paul, 2005; Lloyd-Smith, 2007), and it also arises in some recently proposed latent variable models for count matrix factorization (Williamson et al.", "startOffset": 190, "endOffset": 271}, {"referenceID": 28, "context": "The inference of the NB dispersion parameter r by itself plays an important role not only for the NB regression (Lawless, 1987; Winkelmann, 2008) but also for univariate count data analysis (Bliss & Fisher, 1953; Clark & Perry, 1989; Saha & Paul, 2005; Lloyd-Smith, 2007), and it also arises in some recently proposed latent variable models for count matrix factorization (Williamson et al., 2010; Zhou et al., 2012).", "startOffset": 372, "endOffset": 416}, {"referenceID": 31, "context": "The inference of the NB dispersion parameter r by itself plays an important role not only for the NB regression (Lawless, 1987; Winkelmann, 2008) but also for univariate count data analysis (Bliss & Fisher, 1953; Clark & Perry, 1989; Saha & Paul, 2005; Lloyd-Smith, 2007), and it also arises in some recently proposed latent variable models for count matrix factorization (Williamson et al., 2010; Zhou et al., 2012).", "startOffset": 372, "endOffset": 416}, {"referenceID": 13, "context": "The MotorIns dataset, analyzed in Dean et al. (1989), consists of Swedish third-party motor insurance claims in 1977.", "startOffset": 34, "endOffset": 53}, {"referenceID": 13, "context": "The MotorIns dataset, analyzed in Dean et al. (1989), consists of Swedish third-party motor insurance claims in 1977. Included in the data are the total number of claims for automobiles insured in each of the 315 risk groups, defined by a combination of DISTANCE, BONUS, and MAKE factor levels. The number of insured automobile-years for each group is also given. As in Dean et al. (1989), a 19 dimensional covariate vector is constructed for each group to represent levels of the factors.", "startOffset": 34, "endOffset": 389}, {"referenceID": 13, "context": "The MotorIns dataset, analyzed in Dean et al. (1989), consists of Swedish third-party motor insurance claims in 1977. Included in the data are the total number of claims for automobiles insured in each of the 315 risk groups, defined by a combination of DISTANCE, BONUS, and MAKE factor levels. The number of insured automobile-years for each group is also given. As in Dean et al. (1989), a 19 dimensional covariate vector is constructed for each group to represent levels of the factors. To test goodness-of-fit, we use the Pearson residuals, a metric widely used in GLMs (McCullagh & Nelder, 1989), calculated as The inverse dispersion parameter \u03c6 = 1/0.9947 = 1.005 is mistakenly reported as the dispersion parameter r in Clark & Perry (1989) at Line 15, Page 314.", "startOffset": 34, "endOffset": 747}, {"referenceID": 28, "context": "The MLEs for the Poisson and NB models are wellknown and the update equations can be found in Winner; Winkelmann (2008). The MLE results for the IGPoisson model on the MotorIns data were reported in Dean et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 13, "context": "The MLE results for the IGPoisson model on the MotorIns data were reported in Dean et al. (1989). For the lognormal-Poisson model, no standard MLE algorithms are available and we choose Metropolis-Hastings (M-H) algorithms for parameter estimation.", "startOffset": 78, "endOffset": 97}, {"referenceID": 29, "context": "These observations also support the claim in (Winkelmann, 2008) that the lognormal-Poisson model should be reevaluated since it is appealing in theory and may fit the data better.", "startOffset": 45, "endOffset": 63}, {"referenceID": 12, "context": "As the univariate lognormal-Poisson regression model can be easily generalized to regression analysis of correlated counts, in which the derivatives and Hessian matrixes of parameters are used to construct multivariate normal proposals in a MetropolisHastings algorithm (Chib et al., 1998; Chib & Winkelmann, 2001; Ma et al., 2008; Winkelmann, 2008), the proposed LGNB model can be conveniently modified for multivariate count regression, in which we may be able to derive closed-form Gibbs sampling and VB inference.", "startOffset": 270, "endOffset": 349}, {"referenceID": 19, "context": "As the univariate lognormal-Poisson regression model can be easily generalized to regression analysis of correlated counts, in which the derivatives and Hessian matrixes of parameters are used to construct multivariate normal proposals in a MetropolisHastings algorithm (Chib et al., 1998; Chib & Winkelmann, 2001; Ma et al., 2008; Winkelmann, 2008), the proposed LGNB model can be conveniently modified for multivariate count regression, in which we may be able to derive closed-form Gibbs sampling and VB inference.", "startOffset": 270, "endOffset": 349}, {"referenceID": 29, "context": "As the univariate lognormal-Poisson regression model can be easily generalized to regression analysis of correlated counts, in which the derivatives and Hessian matrixes of parameters are used to construct multivariate normal proposals in a MetropolisHastings algorithm (Chib et al., 1998; Chib & Winkelmann, 2001; Ma et al., 2008; Winkelmann, 2008), the proposed LGNB model can be conveniently modified for multivariate count regression, in which we may be able to derive closed-form Gibbs sampling and VB inference.", "startOffset": 270, "endOffset": 349}, {"referenceID": 0, "context": "As the log Gaussian process can be used to model the intensity of the Poisson process, whose inference remains a major challenge (M\u00f8ller et al., 1998; Adams et al., 2009; Murray et al., 2010; Rao & Teh, 2011), we may link the log Gaussian process to the", "startOffset": 129, "endOffset": 208}, {"referenceID": 21, "context": "As the log Gaussian process can be used to model the intensity of the Poisson process, whose inference remains a major challenge (M\u00f8ller et al., 1998; Adams et al., 2009; Murray et al., 2010; Rao & Teh, 2011), we may link the log Gaussian process to the", "startOffset": 129, "endOffset": 208}, {"referenceID": 28, "context": "Furthermore, the NB distribution is shown to be important for the factorization of a term-document count matrix (Williamson et al., 2010; Zhou et al., 2012), and the multinomial logit has been used to model correlated topics in topic modeling (Blei & Lafferty, 2005; Paisley et al.", "startOffset": 112, "endOffset": 156}, {"referenceID": 31, "context": "Furthermore, the NB distribution is shown to be important for the factorization of a term-document count matrix (Williamson et al., 2010; Zhou et al., 2012), and the multinomial logit has been used to model correlated topics in topic modeling (Blei & Lafferty, 2005; Paisley et al.", "startOffset": 112, "endOffset": 156}, {"referenceID": 22, "context": ", 2012), and the multinomial logit has been used to model correlated topics in topic modeling (Blei & Lafferty, 2005; Paisley et al., 2011).", "startOffset": 94, "endOffset": 139}], "year": 2012, "abstractText": "In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.", "creator": "LaTeX with hyperref package"}}}