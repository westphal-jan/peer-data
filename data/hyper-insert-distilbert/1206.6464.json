{"id": "1206.6464", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Estimating the Hessian by Back-propagating Curvature", "abstract": "in this work : we both develop curvature propagation ( \u03b4 cp ), a general recognition technique allows for efficiently computing unbiased approximations of the generalized hessian of analyzing any geometric function approximation that is computed using a 2d computational graph. formulated at the cost margin of using roughly supplying two numerical gradient correlation evaluations, cp tests can give actually a rank - correct 1 approximation consisting of the whole hessian, and some can moreover be repeatedly applied to helping give increasingly very precise unbiased estimates each of any or all of the entries of the hessian. of particular interest is covering the diagonal of the hessian, for which still no general approach suitable is known to itself exist that is both entirely efficient and infinitely accurate. we show in experiments that cp turns n out to hardly work well just in practice, giving generally very accurate estimates of the hessian of neural networks, for the example, combined with a dramatically relatively small amount of work. often we also apply elementary cp regression to score data matching, but where a diagonal option of filling a certain hessian lattice plays an integral role in the statistical score quality matching objective, consequently and where it dominates is found usually computed exactly partly using inefficient scheduling algorithms themselves which do today not scale spontaneously to larger groups and lower more fundamentally complex models.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (748kb)", "http://arxiv.org/abs/1206.6464v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Tue, 4 Sep 2012 18:32:03 GMT  (703kb)", "http://arxiv.org/abs/1206.6464v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james martens", "ilya sutskever", "kevin swersky"], "accepted": true, "id": "1206.6464"}, "pdf": {"name": "1206.6464.pdf", "metadata": {"source": "META", "title": "Estimating the Hessian by Back-propagating Curvature", "authors": ["James Martens", "Ilya Sutskever", "Kevin Swersky"], "emails": ["JMARTENS@CS.TORONTO.EDU", "ILYA@CS.UTORONTO.CA", "KSWERSKY@CS.TORONTO.EDU"], "sections": [{"heading": "1. Introduction", "text": "There are many models and learning algorithms where it becomes necessary, or is at least very useful, to compute entries of the Hessian of some complicated function. For functions that can be computed using a computational graph there are automatic methods available for computing Hessian-vector products exactly (e.g. Pearlmutter, 1994). These can be used to recover specific columns of the Hessian, but are inefficient at recovering other parts of the matrix such as large blocks, or the diagonal. For the diagonal of the Hessian of a neural network training objective, there are deterministic approximations available such as that of Becker and Le Cun (1988), but these are not guaranteed to be accurate.\nRecently Chapelle and Erhan (2011) showed how to compute an unbiased estimate of the diagonal of the Gauss-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nNewton matrix, and used this to perform preconditioning within a Hessian-free Newton optimization algorithm (Martens, 2010). In this paper we build upon this idea and develop a family of algorithms, which we call Curvature Propagation (CP), for efficiently computing unbiased estimators of the Hessians of arbitrary functions. Estimating entries of the Hessian turns out to be strictly harder than doing the same for the Gauss-Newton matrix, and the resulting approach is necessarily more complex, requiring several additional ideas.\nAs with the algorithm of Chapelle and Erhan (2011), CP involves reverse sweeps of the computational graph of the function, which can be repeated to obtain higher-rank estimates of arbitrary accuracy. And when applied to a function which decomposes as the sum ofM terms, such as typical training objective functions, applying CP to the terms individually results in an estimate of rank M , at no additional expense than than applying it to the sum.\nThis is useful in several applications. The diagonal of the Hessian can be used as a preconditioner for first and second order nonlinear optimizers, which is the motivating application of Becker and Le Cun (1988) and Chapelle and Erhan (2011). Score Matching (Hyvarinen, 2006), a method for parameter estimation in Markov Random Fields, uses the diagonal of the Hessian within its objective, making it expensive to apply to all but the simplest models. As we will see, our work makes it possible to efficiently apply score matching to any model."}, {"heading": "2. Derivation of CP", "text": "In the following section we develop the Curvature Propagation method (CP) for functions that are defined in terms of general computational graphs. We will present one version of the approach that relies on the use of complex arithmetic, and later also give a version that uses only real arithmetic.\nAt a high level, we will define complex vector-valued linear function on the computational graph of our target function f , and then show through a series of lemmas that the expectation of the self outer-product of this function is in fact the Hessian matrix. This function can be computed by what amounts to a modification of reverse-mode automatic differentiation, where noise is injected at each node."}, {"heading": "2.1. Setting and notation", "text": "Let f : Rn \u2212\u2192 R be a twice differentiable function. We will assume that f can be computed via a computation graph consisting of a set of nodes N = {i : 1 \u2264 i \u2264 L} and directed edges E = (i, j) : i, j \u2208 N , where at each node i there is a vector valued output yi \u2208 Rni is computed via yi = fi(xi) for some twice-differentiable function fi. Here xi \u2208 Rmi is the total input to node i, and is given by the concatenation of vectors yk for k \u2208 Pi and Pi = {k : k is a parent of i} = {k : (k, i) \u2208 E}. We identify node 1 as input or \u201csource\u201d node (so that P1 = \u2205) and node L as the output or \u201csink\u201d node, with yL = f(y1) being the final output of the graph.\nLet Jab denote the Jacobian of a w.r.t. b where a and b are vectors, or in other words, \u2202a\n\u2202b . And let Hca,b de-\nnote the Hessian of the scalar function c w.r.t. a and then w.r.t. b (the order matters since it determines the dimension of the matrix). Note that if a and b are quantities associated with nodes i and j (resp.) in the computational graph, Jab and H c a,b will only be well-defined when j does not depend directly or indirectly on i, i.e. i 6\u2208 Aj , where Aj = {k : k is an ancestor of j}. Also note that when there is no dependency on b of a it will be the case that Jab = 0. Under this notation, the Hessian of f w.r.t. its input is denoted by Hfy1,y1 , but we will use the short-hand H for convenience.\nFor k \u2208 Pi, let Ri,k denote the projection matrix which maps the output yk of node k to the their positions in node i\u2019s input vector xi, so that we have xi = \u2211 k\u2208Pi Ri,kyk.\nSummarizing, we have the following set of recursive definitions for computing yL = f(y1) which are iterated for i ranging from 2 to L:\nxi = \u2211 k\u2208Pi Ri,kyk\nyi = fi(xi)\nNote that Ri,k need not appear explicitly as a matrix when implementing these recursions in actual code, but is merely the formal mathematical representation we will use to describe the projective mapping which is performed whenever outputs from a given computational node k are used as input to another node i."}, {"heading": "2.2. Computing gradients and Hessians", "text": "Reverse-mode automatic differentiation1 is a well known method for computing the gradient of functions which are defined in terms of computation graphs. It works by starting at the final node L and going backwards through the graph, recursively computing the gradient of f w.r.t. the yi for each i once the same has been done for all of i\u2019s\n1also known as back-propagation (Rumelhart et al., 1986) in the context of neural networks\nchildren. Using the vector-valued computational graph formalism and notation we have established, the recursions for computing the gradient \u2207f = Jfy1 (remembering that f \u2261 yL) are given by\nJfyL = 1 (1) Jfyi = \u2211 k\u2208Ci JfxkJ xk yi = \u2211 k\u2208Ci JfxkR > k,i (2)\nJfxi = J f yiJ yi xi (3)\nwhere Ci = {k : k is a child of i} and we have used the fact that Jxkyi = R > k,i.\nFor this method to yield a realizable algorithm, it is assumed that for each node i, the function fi is simple enough that direct computation of and/or multiplication by the \u201clocal\u201d Jacobian Jyixi = f \u2032 i(xi) is easy. If for a particular node i this is not the case, then the usual procedure is to split i into several new nodes which effectively break fi into several computationally simpler pieces.\nBy computing the vector derivative of both sides of each of the above equations w.r.t. yL, yj , and yj respectively (for j 6\u2208 Ai), the following recursions can be derived\nHfyL,yL = 0 (4) Hfyi,yj = \u2211 k\u2208Ci Rk,iH f xk,yj (5)\nHfxi,yj = J yi xi >Hfyi,yj +MiJ xi yj (6)\nwhere\nMi \u2261 ni\u2211 q=1 Jfyi,qH yi,q xi,xi (7)\nand where yi,q denotes the q-th component of yi. In deriving the above it is important to remember thatRk,i is a constant, so that its Jacobian w.r.t. yj is the zero matrix. Also note that Jfyi,q is a scalar and that H yi,q xi,xi is Hessian of the local nonlinearity fi. The overall Hessian of f , Hfy1,y1 can be obtained by applying these recursions in a backwards manner (assuming that the various Jacobians are already computed).\nThe additional Jacobian terms of the form Jxiyj which appear in eqn. 6 can be computed according to recursions analogous to those used to compute the gradient, which are given by the equations below:\nJxixi = Imi\u00d7mi (8) Jxixj = J xi yj J yj xj (9)\nJxiyj = \u2211 k\u2208Cj JxixkJ xk yj = \u2211 k\u2208Cj JxixkRk,j \u2200i 6\u2208 Aj (10)\nJxixj = 0 \u2200i \u2208 Aj (11)\nwhere, for convenience, we have defined Jxi,xj to be zero whenever i is an ancestor of j, whereas otherwise it would be undefined.\nIn general, using these recursions for direct computation of Hfy1,y1 will be highly impractical unless the computation tree for f involves a small total number of nodes, each with small associated output and input dimensions ni and mi. The purpose in giving them is to reveal how the \u201cstructure\u201d of the Hessian follows the computation tree, which will become critically important in both motivating the CP algorithm and then proving its correctness."}, {"heading": "2.3. The S function", "text": "We now define an efficiently computable function S that will allow us to obtain rank-1 estimates of the Hessian. Its argument consists of an ordered list of vectors V \u2261 {vi}Li=1 where vi \u2208 R`i , and its output is a n-dimensional vector (which may be complex valued). It will be defined as S(V ) \u2261 Sy1(V ), where Syi(V ) \u2208 Cni and Sxi(V ) \u2208 Cmi are vector-valued functions of V defined recursively via the equations\nSyL(V ) = 0 (12) Syi(V ) = \u2211 k\u2208Ci R>k,iSxk(V ) (13) Sxi(V ) = F > i vi + J yi xi >Syi(V ) (14)\nwhere each Fi is a (not necessarily square) complex-valued matrix in C`i\u00d7mi satisfying F>i Fi = Mi. Such an Fi is guaranteed to exist because Mi is symmetric, which follows from the fact that it is a linear combination of Hessian matrices.\nNote that these recursions closely resemble those given previously for computing the gradient (eqn. 1, 2, and 3). The multiplication by Jyixi\n> of the vector Syi(V ) at each stage of the recursion is easy to perform since this is precisely what happens at each stage of reverse-mode automatic differentiation used to compute the gradient of f . In general, the cost of computing S is similar to that of computing the gradient, which itself is similar to that of evaluating f . The practical aspects computing S(V ) will be discussed further in section 5."}, {"heading": "2.4. Properties of the S function with stochastic inputs", "text": "Suppose that the random variable V satisfies: \u2200i E [ viv > i ] = I and \u2200j 6= i, E [ viv > j ] = 0 (15)\nFor example, each vi could be drawn from a multivariate normal with mean 0 and covariance matrix I .\nWe will now give a result which establishes the usefulness of S(V ) as a tool for approximating H . The proof of this theorem and others will be located in the appendix/supplement. Theorem 2.1. S(V )S(V )> is an unbiased estimator of H(\u2261 Hfy1,y1)\nIn addition to being unbiased, the estimator S(V )S(V )> is will be symmetric and possibly complex-valued. To\nachieve a real valued estimate we can instead use only the real component of S(V )S(V )>, which itself will also be an unbiased estimator for Hfy1,y1 since the imaginary part of S(V )S(V )> is zero in expectation."}, {"heading": "2.5. Avoiding complex numbers", "text": "The factorization of the Mi\u2019s and resulting complex arithmetic associated with using these factors can be avoided if we redefine V so that each vi is of dimension mi (instead of `i), and we define the real vector-valued functions T (V ) \u2261 Ty1(V ) and U(V ) \u2261 Uy1(V ) according to the following recursions:\nTyL(V ) = 0 UyL(V ) = 0 Tyi(V ) = \u2211 k\u2208Ci Jxkyi >Txk (V ) Uyi(V ) = \u2211 k\u2208Ci Jxkyi >Uxk (V ) Txi(V ) = Mivi + J yi xi >Tyi(V ) Uxi(V ) = vi + J yi xi >Uyi(V )\nBoth these recursions for T and U are trivial modification of those given for S(V ), with the only difference being the matrix which multiplies vi (it\u2019s F>i for S, Mi for T , and I forU ). And because they do not involve complex quantities at any point, they will be real-valued. Theorem 2.2. T (V )U(V )> is an unbiased estimator ofH\nSince H is symmetric, it follows directly from this result that ( T (V )U(V )> )> = U(V )T (V )> is also an unbiased estimator of H . Note however that while both T (V )U(V )> and U(V )T (V )> will be symmetric in expectation (since Hfy1,y1 is), for any particular choice of V they generally will not be. This issue can be addressed by instead using the estimator 1\n2\n( T (V )U(V )> + U(V )T (V )> ) which will be symmet-\nric for any V . However, despite the fact that S(V )S(V )> and this alternative estimator are both symmetric for all V \u2019s and also unbiased, they will not, in general, be equal. While computing both T and U will require a total of 2 sweeps over the computational graph versus only the one required for S(V ), the total amount of work will be the same due to the doubly expensive complex-valued arithmetic required to evaluate S(V )."}, {"heading": "2.6. Matrix interpretation of S, T and U", "text": "Suppose we represent V as a large vector v \u2261 [v>1 . . . v>L ]> with dimension m \u2261 \u2211 imi. Then the functions S, T and U are linear in the vi\u2019s (a fact which follows from the recursive definitions of these functions) and hence v. Thus S, T , and U have an associated representation as matrices S\u0303 \u2208 Cn\u00d7m, T\u0303 \u2208 Rn\u00d7m, and U\u0303 \u2208 Rn\u00d7m w.r.t. the coordinate bases given by v\u0303.\nThen noting that S(V )S(V )> = S\u0303vv>S\u0303>, and that condition (15) is equivalent to E[vv>] = I , we obtain\nHfy1,y1 = E [ S\u0303vv>S\u0303> ] = S\u0303 E [ vv> ] S\u0303> = S\u0303S\u0303>\nand thus we can see that S\u0303 has an interpretation as \u201cfactor\u201d of Hfy1,y1 . Similarly we have T\u0303 U\u0303 > = Hfy1,y1 and U\u0303 T\u0303 > = Hfy1,y1 ."}, {"heading": "3. A simpler method?", "text": "At the cost of roughly two passes through the computational graph it is possible to compute the Hessian-vector Hw for an arbitrary vector w \u2208 Rn (e.g. Pearlmutter, 1994). This suggests the following simple approach to computing an unbiased rank-1 estimate ofH: draw w from a distribution satisfying E[ww>] = I and then take the outer product of Hw with w. It is easy to see that this is unbiased, since\nE [ HwwT ] = H E [ wwT ] = H (16)\nComputationally, this estimator is just as expensive as CP, but since there are several pre-existing methods computing Hessian vector products, it may be easier to implement. However, we will prove in the next section that the CP estimator will have much lower variance in most situations, and later confirm these findings experimentally. And in addition to this, there are certain situations, which arise frequently in machine learning applications, where vectorized implementations of CP will consume far less memory than similar vectorized implementations of this simpler estimator ever could, and we will demonstrate this in the specific case when f is a neural network training objective function.\nIt is also worth noting that this estimator underlies the Hessian norm estimation technique used in Rifai et al. (2011). That this is true is due to the equivalence between the stochastic finite-difference formulation used in that work and matrix-vector products with randomly drawn vectors. We will make this rigorous in the appendix/supplement."}, {"heading": "4. Covariance analysis", "text": "Let AB> be an arbitrary matrix factorization of H , with A,B \u2208 Cn\u00d7`. Given a vector valued random variable u \u2208 R` satisfying E[uu>] = I , we can use this factorization to produce an unbiased rank-1 estimate of the Hessian, HA,B \u2261 (Au)(Bu)> = Auu>B>. Note that the various CP estimators, as well as the simpler one discussed in the previous section are all of this form, and differ only in their choices of A and B.\nExpanding we have:\nE[HA,Bij H A,B kl ] = E \u2211 a,b Ai,auaubBj,b \u2211 c,d Ak,cucudBl,d  (17)\n= \u2211\na,b,c,d\nAiaBjbAkcBld E [uaubucud] (18)\nwhere here (and in the remainder of this section) the subscripts on u refer to scalar components of the vector u and\nnot elements of a collection of vectors.\nIf we assume u \u223c G \u2261 Normal(0, I), we can use the wellknow formula EG[uaubucud] = \u03b4ab\u03b4cd + \u03b4ac\u03b4bd + \u03b4ad\u03b4bc and simplify this further to:\n= \u2211\na,b,c,d\nAiaBjbAkcBld(\u03b4ab\u03b4cd + \u03b4ac\u03b4bd + \u03b4ad\u03b4bc)\n= (A>i Bj)(A > k Bl) + (A > i Ak)(B > j Bl) + (A > i Bl)(A > k Bj) = HijHkl + (A > i Ak)(B > j Bl) +HilHjk\nwhere Ai is a vector consisting of the i-th row of A, and similarly for Bi, and where we have used Hij = A>i Bj . Consequently, the variance is given by:\nCovG [ HA,Bij , H A,B kl ] = EG [ HA,Bij H A,B kl ] \u2212HijHkl\n= (A>i Ak)(B > j Bl) +HilHjk\nNote that when A = B = S\u0303, we have that (A>i Ak)(B > j Bl) = (S\u0303 > i S\u0303k)(S\u0303 > j S\u0303l) = HikHjl. Thus the estimator H S\u0303,S\u0303 has the following desirable property: its covariance depends only on H and not on the specific details of the computational graph used to construct the S function.\nIf on the other hand we assume that u \u223c K \u2261 Bernoilli({\u22121, 1})`, i.e. K is a multivariate distribution of independent Bernoulli random variables on {\u22121, 1}, we have EB [uaubucud] = \u03b4ab\u03b4cd + \u03b4ac\u03b4bd + \u03b4ad\u03b4bc \u2212 2\u03b4ab\u03b4bc\u03b4cd, which when plugged into (18) gives:\nCovK [ HA,Bij H A,B kl ] = (A>i Ak)(B > j Bl) +HilHjk\n\u2212 2 \u2211 a BiaAjaBkaAla\n= CovG [ HA,Bij H A,B kl ] \u2212 2 \u2211 a BiaAjaBkaAla\nOf particular interest is the self-variance of HA,Bij (i.e. Var [ HA,Bij ] = Cov [ HA,Bij , H A,B ij ] ). In this case we have that:\nVarK [ HA,Bij ] = VarG [ HA,Bij ] \u2212 2 \u2211 a (BiaAja) 2\nand we see that variance of estimator that uses K will always be strictly smaller than the one that uses G, unless \u2211 a(BiaAja)\n2 = 0 (which would imply that\u2211 aBiaAja = Hij = 0).\nReturning to the case that u \u223c G, we can prove the following result, which shows that when it comes to estimating the diagonal entries Hii of H , the estimator which uses A = B = S\u0303 has the lowest variance among all possible estimators of the form HA,B : Theorem 4.1. \u2200i and \u2200A,B s.t. AB> = H we have:\nVarG [ HA,Bii ] \u2265 VarG [ H S\u0303,S\u0303ii ] = 2H2ii\nMoreover, in the particular case of using the \u2018simple\u2019 estimator (which is given by A = H,B = I) the variance of the diagonal entries is given by:\nVarG [ HH,Iii ] = H>i Hi +H 2 ii = \u2211 j 6=i H2ij + VarG [ H S\u0303,S\u0303ii ] and so we can see that the CP estimator based on S always gives a lower variance, and is strictly lower in most cases."}, {"heading": "5. Practical aspects", "text": ""}, {"heading": "5.1. Computing and factoring the Mi\u2019s", "text": "Computing the matrices Mi for each node i is necessary in order to compute the S, T and U functions, and for S we must also be able to factor them. Fortunately, each Mi can be computed straightforwardly according to eqn. 7 as long as the operations performed at node i are simple enough. And each Hyi,qxi,xi is determined completely by the local function fi computed at node i. The Jacobian term Jfyi,q = [ Jfyi ] q\nwhich appears in the formula for Mi is just a scalar, and is the derivative of f w.r.t. yi,q . This can be made cheaply and easily available by performing, in parallel with the computation of S(V ), the standard backwards automatic differentiation pass for computing the gradient of f w.r.t. to y1, which will produce the gradient of f w.r.t. each yi along the way. Alternatively, this gradient information may be cached from a gradient computation which is performed ahead of time (which in many applications is done anyway).\nIn general, when Mi is block diagonal or banded diagonal, so too will Fi (with the same pattern), which will greatly reduce the associated computational and storage requirements. For example, when fi corresponds to the elementwise nonlinearities computed in a particular layer of a neural network, Mi will be diagonal and hence so will Fi, and these matrices can be stored as such. Also, if Mi happens to be sparse or low rank, without any other obvious special structure, there are algorithms which can compute factors Fi which will also be sparse or low-rank.\nAlternatively, in the most extreme case, the vector valued nodes in the graph can be sub-divided to produce a graph with the property that every node outputs only a scalar and has at most 2 inputs. In such a case, each Mi will be no bigger than 2\u00d7 2. Such an approach is best avoided unless deemed necessary since the vector formalism allows for a much more vectorized and thus efficient implementation in most situations which arise in practice. Another option to consider if it turns out thatMi is easy to work with but hard to factor, is to use the T,U based estimator instead of the S based one.\nIt may also be the case that Mi or Fi will have a special sparse form which makes sampling the entire vector vi unnecessary. For example, if a node copies a large input vector to its output and transforms a single entry by some nonlinear function, Mi will be all zeros with a single element\non the diagonal (and hence so will its factor Fi), making it possible to sample only the component of vi that corresponds to that entry."}, {"heading": "5.2. Increasing the rank", "text": "As with any unbiased estimator, the estimate can be made more accurate by collecting multiple samples. Fortunately, sampling and computing S(V ) for multiple V \u2019s is trivially parallelizeable. And it can be easily implemented in vectorized code for k samples by taking the defining recursions for S (eqn. 12, 13, and 14) and redefining Syi(V ) and Sxi(V ) to be matrix valued functions (with k columns) and vi to be a mi \u00d7 k matrix of column vectors which are generated from independent draws from the usual distribution for vi.\nIn the case where f is a sum of B similarly structured terms, which occurs frequently in machine learning such as when f is sum of regression errors or log-likelihood terms over a collection of training cases, one can apply CP individually to each term in the sum at almost no extra cost as just applying it to f , thus obtaining a rank-k estimate of f instead of a rank-1 estimate."}, {"heading": "5.3. Curvature Propagation for Diagonal Hessian Estimation in Feedforward Neural Networks", "text": "In this section we will apply CP to the specific example of computing an unbiased estimate diag(H\u0302) of the diagonal of H (diag(H)) of a feed-forward neural networks with ` layers. The pseudocode below computes the objective f of our neural network for a batch of B cases.\n1: Input: z1, a matrix of inputs (with B columns, one per case) 2: for all i from 1 to `\u2212 1 do 3: ui+1 \u2190Wizi 4: zi+1 \u2190 g(ui+1) 5: end for 6: f \u2190 \u2211B b=1 Lb(z`,b)/B 7: Output: f\nHere g(x) is a coordinate-wise nonlinearity, zi are matrices containing the outputs of the neuronal units at layer i for all the cases, and similarly the matrices ui contain their inputs. Lb denotes the loss-function associated with case b (the dependency on b is necessary so we can include targets). For simplicity we will assume that Lb is the standard squared loss given by Lb(z`,b) = 1/2\u2016z`,b \u2212 tb\u20162 for target vector tb (where t will denote the matrix of these vectors).\nThe special structure of this objective permits us to efficiently apply CP to each scalar term of the average\u2211B\nb=1 Lb(z`,b)/B, instead of to f directly. By summing the estimates of the diagonal Hessians for each Lb(z`,b)/B we thus obtain a rank-B estimate of H instead of merely a rank-1 estimate. That this is just as efficient as applying CP directly to f is due to the fact that the computations of each z`,b are performed independently of each other.\nFor ease of presentation, we will redefine V \u2261 {vi}Li=1 so that each vi is not a single vector, but a matrix of\nsuch vectors with B columns. We construct the computational graph so that the element-wise nonlinearities and the weight matrix multiplications performed at each of the ` layers each correspond to a node in the graph. We define Sui \u2261 Sxji (V ) where ji is the node corresponding to the computation of ui (from zi\u22121 and Wi\u22121), Szi \u2261 Sxki (V ) where ki is the node correspond to the computation of zi (from ui), and SWi \u2261 [Sy1(V )]Wi where [\u00b7]Wi denotes extraction of the rows in y1 corresponding to the i-th weightmatrix (Wi). Consistent with our mild redefinition/abuse of notation for V , each of Sui , Szi , Sy1 , and SWi will be matrix-valued with a column for each of the B training cases. The variables dzi and dui are the derivatives w.r.t. ui and zi computed with backpropagation and also have B columns. Finally, let a b be the element-wise product, a 2 be the element-wise power, outer(a, b) \u2261 ab>, outer2(a, b) \u2261 outer(a 2, b 2), and vec(\u00b7) be the vectorization operator. Under this notation, the algorithm below estimates the diagonal of the Hessian of f by estimating the sub-objective corresponding to each case, and then averaging the results. Like the pseudo-code for the neural network objective itself, it makes use of vectorization, which allows for an easily parallelized implementation.\n1: Sz` \u2190 vj` ; dz` \u2190 z` \u2212 t 2: Su` \u2190 Sz` ; du` \u2190 dz` 3: for all i from `\u2212 1 down to 1 do 4: Szi \u2190W>i Sui+1 ; dzi \u2190W>i dui+1 5: [diag(H\u0302)]Wi \u2190 vec(outer2 ( zi, Sui+1 ) /B) 6: Ki \u2190 g\u2032\u2032(ui) dzi 7: Sui \u2190 Szi g\u2032(ui) + vki K 1/2 i 8: dui \u2190 dzi g\u2032(ui) 9: end for\nFor i < `, each Ki is a B-columned matrix of vectors containing the diagonals for each training case of the local matrices Mki for each case occurring at node ki. Because Mj corresponds to an element-wise non-linear function, it is diagonal, and so K 1/2i will be a matrix of vectors corresponding to the diagonals of the factors Fki (which are themselves diagonal). Note that the above algorithm makes use of the fact that the local matrices Mji can be set to zero and the estimator of the diagonal will remain unbiased.\nAt no point in the above implementation do we need to store any matrix the size of SWi , as the computation of [diag(H\u0302)]Wi , which involves an element-wise square of SWi and sum over cases (as accomplished by line 5), can be performed as soon as the one and only contribution to SWi from other nodes in the graph is available. This is desirable since SWi will usually be much larger than the various other intermediate quantities which we need to store, such as zi or Sui . In functions f where large groups of parameters are accessed repeatedly throughout the computation graph, such as in the training objective of recurrent neural networks, we may have to temporally store some matrices the size Sy1 (or certain row-restrictions of this, like SWi ) as the contributions from different cases are collected and summed together, which can make CP less practical. No-\ntably, despite the structural similarities of back-prop (BP) to CP, this problem doesn\u2019t exist with BP since one can store incomplete contributions from each case in the batch into a single n dimensional vector, which is impossible in CP due to the need to take the entry-wise square of Sy1 before summing over cases."}, {"heading": "6. Hardness of exact computation", "text": "An approach like CP wouldn\u2019t be as useful if there was an efficient and exact algorithm for computing the diagonal of the Hessian of the function defined by an arbitrary computation graph. In this section we will argue why such an algorithm is unlikely to exist.\nTo do this we will reduce the problem of multiplying two matrices to that of computing (exactly) the diagonal of the Hessian of a certain function f , and then appeal to a hardness due to Raz and Shpilka (2001) which shows that matrix multiplication will require asymptotically more computation than CP does when it is applied to f . This result assumes a limited computational model consisting of bounded depth arithmetic circuits with arbitrary fan-in gates. While not a fully general model of efficient computation, it nonetheless captures most natural algebraic formulae and algorithms that one might try to use to compute the diagonal of f .\nThe function f will be defined by: f(y) \u2261 1/2y>W>ZWy, where Z \u2208 R2n\u00d72n is symmetric, and W \u2261 [P>Q]> with P \u2208 Rn\u00d7n and Q \u2208 Rn\u00d7n.\nNote that f may be easily evaluated in O(n2) time by multiplying y first by W , obtaining z, and then multiplying z by Z, obtaining Zz, and finally pre-multiplying by z> obtaining z>Zz = y>W>ZWy. Thus applying CP is relatively straight-forward, with the only potential difficulty being that the matrix Z, which is the local Hessian associated with the node that computes z>Zz, may not be easy to factorize. But using the T /U variant of CP gets around this issue, and achieves aO(n2) computational cost. Moreover, it is easy to see how the required passes could be implemented by a fixed-depth arithmetic circuit (with gates of arbitrary fan-in) with O(n2) edge-cost since the critical operations required are just a few matrix-vector multiplications. The goal of the next theorem is to show that there can be no such circuit of edge cost O(n2) for computing the exact Hessian of f . Theorem 6.1. Any family of bounded depth arithmetic circuits with arbitrary fan-in gates which computes the diagonal of f given inputs W and Z will have an edge count which is superlinear in n2.\nThe basic idea of the proof is to use the existence of such a circuit family to construct a family of circuits with bounded depth and edge count O(n2), that can multiply arbitrary n \u00d7 n matrices (which will turn out to be the matrices P andQ that parameterize f ), contradicting a theorem of Raz and Shpilka (2001) which shows that any such circuit fam-\nily must have edge count which is superlinear n2. The following lemma accomplishes this construction:\nLemma 6.2. If an arithmetic circuit with arbitrary fan-in gates computes the diagonal of the Hessian of f for arbitrary P , Q and Z then, there is also a circuit of twice the depth + O(1), and three times the number of edges + O(n2), which computes the product PQ for arbitrary input matrices P,Q \u2208 Rn\u00d7n.\nThe results presented in this section rule out, or make extremely unlikely, the possible existence of algorithms which could perform a constant number of backwards and forwards \u201cpasses\u201d through the computational graph of f to find its exact Hessian."}, {"heading": "7. Related work", "text": "The simplest way of computing the entries of the Hessian, including the diagonal, is by using an algorithm for Hessian-vector multiplication and running through the vectors ei for i = 1...n, recovering each column of H in turn. Unfortunately this method is too expensive in most situations, and in the example function f used in Section 6, would require O(n3) time.\nThe method of Chapelle and Erhan (2011) can be viewed as a special case of CP, where all the Mi\u2019s except for the Mi associated with the final nonlinearity are set to zero. Because of this, all of the results proved in this paper also apply to this approach, but with the Hessian replaced by the Gauss-Newton matrix.\nBecker and Le Cun (1988) gave an approach for approximating the diagonal of the Hessian of a neural network training objective using a deterministic algorithm which does several passes through the computation tree. This method applies recursions similar to (4)-(6), except that all the \u201cintermediate Hessians\u201d at each layer are approximated by their diagonals, thus producing a biased estimate (unless the intermediate Hessians really are diagonal). We numerically compare CP to this approach in Section 8.\nIn Bishop (1992), a method for computing entries of the Hessian of a feedforward neural network was derived. This method, while being exact, and more efficient than the naive approach discussed at the start of this section, is not practical for large networks, since it requires a number of passes which will be at least as big as the total number of hidden and outputs units. CP by contrast requires only 1 pass to obtain a single unbiased rank-B estimate, where B is the number of training cases."}, {"heading": "8. Experiments", "text": ""}, {"heading": "8.1. Accuracy Evaluation", "text": "In this section we test the accuracy of CP on a small neural network as we vary the number of samples. The network consists of 3 hidden layers, each with 20 units. The input\nand output layers are of size 256 and 10 respectively giving a total of 6190 parameters. We tested both a network with random weights set by Gaussian noise with a variance of 0.01, and one trained to classify handwritten digits from the USPS dataset 2. For the random vectors v, we tested both Gaussian and {\u22121, 1}-Bernoulli noise using the CP estimators based on using S and T /U , and the simpler estimator discussed in Section 3 based on using H/I . For the sake of comparison, we also included the deterministic method of (Becker and Le Cun, 1988). The experiments were carried out by picking a subset of 1000 data points from the USPS dataset and keeping it fixed. Note that sample size refers to the number of random vectors generated per data case. This means that a sample size of 1 corresponds to an aggregation of 1000 rank-1 estimates.\nOur results in 8.1 show that the accuracy of each estimator improves by roughly an order of magnitude for every order of magnitude increase in samples. It also shows that the Sbased estimator along with binary noise is by far the most efficient and the simple H/I based estimator is the least efficient by an order of magnitude."}, {"heading": "8.2. Score-Matching Experiments", "text": "To test the effectiveness of CP in a more practical scenario, we focus on estimating the parameters of a Markov random field using the score matching technique. Score matching is a simple alternative to maximum likelihood that has been widely used to train energy-based models (Ko\u0308ster and Hyva\u0308rinen, 2007; Swersky et al., 2011). One of its drawbacks is that the learning objective requires the diagonal Hessian of the log-likelihood with respect to the data, which can render it unreasonably slow for deep and otherwise complicated models.\nOur specific test involves learning the parameters of a covariance-restricted Boltzmann machine (cRBM; Ranzato et al., 2010). This can be seen as a two-layer network where the first layer uses the squared activation function followed by a second layer that uses the softplus activation function: log(1+exp(x)). The details of applying score matching to this model can be found in Swersky et al. (2011).\nIn this experiment, we attempted to train a cRBM using stochastic gradient descent on minibatches of size 100. Our setup is identical to Ranzato et al. (2010). In particular, our cRBM contained 256 factors and hidden units. We trained the model on 11000 image patches of size 16\u00d716 from the Berkeley dataset 3. For our training procedure, we optimize the first layer for 100 epochs, then freeze those weights and train the second layer for another 25 epochs.\nScore-matching requires the derivatives (w.r.t. the model parameters) of the sum of the diagonal entries of the Hes-\n2http://cs.nyu.edu/\u02dcroweis/data/usps_all. mat\n3http://www.cs.berkeley.edu/projects/ vision/grouping/segbench\nsian (w.r.t. the data). We can thus use CP to estimate the score-matching gradient by applying automatic differentiation to the CP estimator itself (sampling and then fixing the random noise V ), exploiting the facts that the linear sum over the diagonal respects expectation, and the derivative of the expectation over V is the expectation of the derivative, and so this will indeed produce an unbiased estimate of the required gradient.\nA random subset of covariance filters from the trained model are shown in Figure 8.2. As expected the filters appear Gabor-like, with various spatial locations, frequencies, and orientations. The second layer also reproduces the desired effect of pooling similar filters from the layer below.\nTo demonstrate that learning can proceed with no loss in accuracy we trained two different versions of the model, one where we use the exact minibatch gradient, and one where we use approximate gradients via our estimator. We plot the training loss versus epoch, and our results in Figure 1(c) show that the noise incurred from our unbiased approximation does not affect accuracy during learning with minibatches. Unfortunately, it is difficult to train for many epochs in the second layer because evaluating the exact objective is prohibitively expensive in this model."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Olivier Chapelle for his helpful discussions."}], "references": [{"title": "Improving the convergence of backpropagation learning with second order methods", "author": ["S. Becker", "Y. Le Cun"], "venue": "In Proceedings of the 1988 connectionist models summer school,", "citeRegEx": "Becker and Cun.,? \\Q1988\\E", "shortCiteRegEx": "Becker and Cun.", "year": 1988}, {"title": "Exact calculation of the hessian matrix for the multilayer perceptron", "author": ["C. Bishop"], "venue": "Neural Computation,", "citeRegEx": "Bishop.,? \\Q1992\\E", "shortCiteRegEx": "Bishop.", "year": 1992}, {"title": "Improved preconditioner for hessian free optimization", "author": ["O. Chapelle", "D. Erhan"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Chapelle and Erhan.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Erhan.", "year": 2011}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["A. Hyvarinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyvarinen.,? \\Q2006\\E", "shortCiteRegEx": "Hyvarinen.", "year": 2006}, {"title": "A two-layer ica-like model estimated by score matching", "author": ["U. K\u00f6ster", "A. Hyv\u00e4rinen"], "venue": "Artificial Neural Networks\u2013ICANN", "citeRegEx": "K\u00f6ster and Hyv\u00e4rinen.,? \\Q2007\\E", "shortCiteRegEx": "K\u00f6ster and Hyv\u00e4rinen.", "year": 2007}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "Pearlmutter.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter.", "year": 1994}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["M. Ranzato", "A. Krizhevsky", "G.E. Hinton"], "venue": "In Proc. Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Lower bounds for matrix product, in bounded depth circuits with arbitrary gates", "author": ["R. Raz", "A. Shpilka"], "venue": "Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Raz and Shpilka.,? \\Q2001\\E", "shortCiteRegEx": "Raz and Shpilka.", "year": 2001}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "In Proceedings of the ECML/PKDD", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "B.M. Marlin", "N. de Freitas"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "The complexity of partial derivatives", "author": ["B. Walter", "V. Strassen"], "venue": "Theoretical Computer Science,", "citeRegEx": "Walter and Strassen.,? \\Q1983\\E", "shortCiteRegEx": "Walter and Strassen.", "year": 1983}], "referenceMentions": [{"referenceID": 6, "context": "Pearlmutter, 1994). These can be used to recover specific columns of the Hessian, but are inefficient at recovering other parts of the matrix such as large blocks, or the diagonal. For the diagonal of the Hessian of a neural network training objective, there are deterministic approximations available such as that of Becker and Le Cun (1988), but these are not guaranteed to be accurate.", "startOffset": 0, "endOffset": 343}, {"referenceID": 2, "context": "Recently Chapelle and Erhan (2011) showed how to compute an unbiased estimate of the diagonal of the Gauss-", "startOffset": 9, "endOffset": 35}, {"referenceID": 5, "context": "Newton matrix, and used this to perform preconditioning within a Hessian-free Newton optimization algorithm (Martens, 2010).", "startOffset": 108, "endOffset": 123}, {"referenceID": 2, "context": "As with the algorithm of Chapelle and Erhan (2011), CP involves reverse sweeps of the computational graph of the function, which can be repeated to obtain higher-rank estimates of arbitrary accuracy.", "startOffset": 25, "endOffset": 51}, {"referenceID": 3, "context": "Score Matching (Hyvarinen, 2006), a method for parameter estimation in Markov Random Fields, uses the diagonal of the Hessian within its objective, making it expensive to apply to all but the simplest models.", "startOffset": 15, "endOffset": 32}, {"referenceID": 2, "context": "The diagonal of the Hessian can be used as a preconditioner for first and second order nonlinear optimizers, which is the motivating application of Becker and Le Cun (1988) and Chapelle and Erhan (2011). Score Matching (Hyvarinen, 2006), a method for parameter estimation in Markov Random Fields, uses the diagonal of the Hessian within its objective, making it expensive to apply to all but the simplest models.", "startOffset": 177, "endOffset": 203}, {"referenceID": 10, "context": "also known as back-propagation (Rumelhart et al., 1986) in the context of neural networks children.", "startOffset": 31, "endOffset": 55}, {"referenceID": 9, "context": "It is also worth noting that this estimator underlies the Hessian norm estimation technique used in Rifai et al. (2011). That this is true is due to the equivalence between the stochastic finite-difference formulation used in that work and matrix-vector products with randomly drawn vectors.", "startOffset": 100, "endOffset": 120}, {"referenceID": 8, "context": "To do this we will reduce the problem of multiplying two matrices to that of computing (exactly) the diagonal of the Hessian of a certain function f , and then appeal to a hardness due to Raz and Shpilka (2001) which shows that matrix multiplication will require asymptotically more computation than CP does when it is applied to f .", "startOffset": 188, "endOffset": 211}, {"referenceID": 8, "context": "The basic idea of the proof is to use the existence of such a circuit family to construct a family of circuits with bounded depth and edge count O(n), that can multiply arbitrary n \u00d7 n matrices (which will turn out to be the matrices P andQ that parameterize f ), contradicting a theorem of Raz and Shpilka (2001) which shows that any such circuit fam-", "startOffset": 291, "endOffset": 314}, {"referenceID": 2, "context": "The method of Chapelle and Erhan (2011) can be viewed as a special case of CP, where all the Mi\u2019s except for the Mi associated with the final nonlinearity are set to zero.", "startOffset": 14, "endOffset": 40}, {"referenceID": 1, "context": "In Bishop (1992), a method for computing entries of the Hessian of a feedforward neural network was derived.", "startOffset": 3, "endOffset": 17}, {"referenceID": 4, "context": "Score matching is a simple alternative to maximum likelihood that has been widely used to train energy-based models (K\u00f6ster and Hyv\u00e4rinen, 2007; Swersky et al., 2011).", "startOffset": 116, "endOffset": 166}, {"referenceID": 11, "context": "Score matching is a simple alternative to maximum likelihood that has been widely used to train energy-based models (K\u00f6ster and Hyv\u00e4rinen, 2007; Swersky et al., 2011).", "startOffset": 116, "endOffset": 166}, {"referenceID": 7, "context": "Our specific test involves learning the parameters of a covariance-restricted Boltzmann machine (cRBM; Ranzato et al., 2010).", "startOffset": 96, "endOffset": 124}, {"referenceID": 7, "context": "Our specific test involves learning the parameters of a covariance-restricted Boltzmann machine (cRBM; Ranzato et al., 2010). This can be seen as a two-layer network where the first layer uses the squared activation function followed by a second layer that uses the softplus activation function: log(1+exp(x)). The details of applying score matching to this model can be found in Swersky et al. (2011).", "startOffset": 103, "endOffset": 402}, {"referenceID": 7, "context": "Our setup is identical to Ranzato et al. (2010). In particular, our cRBM contained 256 factors and hidden units.", "startOffset": 26, "endOffset": 48}], "year": 2012, "abstractText": "In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show in experiments that CP turns out to work well in practice, giving very accurate estimates of the Hessian of neural networks, for example, with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models.", "creator": "LaTeX with hyperref package"}}}