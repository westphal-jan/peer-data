{"id": "1206.6475", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Split-Merge Framework for Comparing Clusterings", "abstract": "clustering variance evaluation measures are frequently frequently used to evaluate the inconsistent performance of algorithms. however, geographically most different measures are commonly not properly normalized and ignore some information merely in recognizing the generally inherent structure of smaller clusterings. we also model the relation between two clusterings as : a bipartite graph and systematically propose a general component - based decomposition formula jointly based on noting the components used of combining the respective graph. essentially most existing measures also are examples of providing this formula. in order to thoroughly satisfy consistency difficulties in the component, we even further propose a split - merge framework for loosely comparing distinct clusterings inclusive of different data sets. knowing our framework automatically gives measures assuming that measure are conditionally weakly normalized, as and since it perhaps can make use of data point information, including such, as discrete feature factor vectors and measured pairwise vertex distances. we use an robust entropy - network based instance process of calculating the framework and a coreference resolution data set to demonstrate empirically the improved utility of fitting our framework measured over other measures.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (280kb)", "http://arxiv.org/abs/1206.6475v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Tue, 4 Sep 2012 17:42:41 GMT  (280kb)", "http://arxiv.org/abs/1206.6475v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["qiaoliang xiang", "qi mao", "kian ming adam chai", "hai leong chieu", "ivor w tsang", "zhendong zhao"], "accepted": true, "id": "1206.6475"}, "pdf": {"name": "1206.6475.pdf", "metadata": {"source": "META", "title": "A Split-Merge Framework for Comparing Clusterings ", "authors": ["Qiaoliang Xiang", "Qi Mao", "Kian Ming A. Chai", "Hai Leong Chieu", "Wai-Hung Tsang", "Zhendong Zhao"], "emails": ["QIAOLIANGXIANG@GMAIL.COM", "QMAO1@NTU.EDU.SG", "CKIANMIN@DSO.ORG.SG", "CHAILEON@DSO.ORG.SG", "IVORTSANG@NTU.EDU.SG", "ZHENDONG.ZHAO@MQ.EDU.AU"], "sections": [{"heading": "1. Introduction", "text": "Hard partitional clustering groups data points into a set of disjoint clusters. There are three types of measures which could be used to evaluate a clustering: 1) an external measure that compares the clustering to a given true clustering; 2) an internal measure which only utilizes the information of feature vectors of data points; 3) a hybrid measure which takes both information into account. External measures are preferred because they better reflect human evaluation\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n(Strehl & Ghosh, 2003).\nClustering evaluation measures are commonly used to compare the performance of various algorithms, so they should be able to compare clusterings of different data sets. Unnormalized and asymmetric measures are inappropriate for comparing clusterings across data sets (Vinh et al., 2010; Wagner & Wagner, 2007). Therefore, measures should be normalized properly and be independent of some inherent structures of two clusterings (Meila\u0306, 2007). When a measure is used for comparing all possible clusterings with the true clustering, it is preferable that the similarity scores should be in the closed interval [0, 1] (Luo et al., 2009). Besides, a measure should not depend on some parameters such as the number of data points (Meila\u0306, 2007).\nExisting external measures can be grouped into three categories: pair counting, set matching and information theoretic. Pair counting measures are based on counting the pairs of points for which two clusterings agree or disagree. They are sensitive to parameters, such as the size of a cluster, the number of clusters, and the number of data points (Wagner & Wagner, 2007). Set matching measures find a maximum matching between two clusterings. They make no assumption on how clusterings are generated, but they ignore those unmatched clusters (Meila\u0306, 2007). Moreover, since the matching degree between two clusterings is always positive, such measures are not normalized. Information theoretic measures do not suffer from the problems of pair counting and set matching measures. They have been analyzed extensively and systematically in recent years (Meila\u0306, 2007; Vinh et al., 2010). Some measures tend to give high scores in practice, so adjusted measures, such as adjusted Rand index (Hubert & Arabie, 1985) and adjusted mutual information (Vinh et al., 2010) are proposed to address this issue, but they are not normalized because they may be negative (Meila\u0306, 2007).\nIn this paper, instead of focusing on designing a new clustering measure, we propose a split-merge framework that can be tailored to different applications (Guyon et al., 2009). The framework models two clusterings as a bipartite graph which is decomposed into connected components, and each component is further decomposed into subcomponents. Pairs of related subcomponents are then taken into consideration in designing a clustering similarity measure within the framework. The contributions of this paper are listed below.\n\u2022 We propose a general component-based decomposition formula based on the components of the bipartite graph. We find that most existing measures are special cases of that formula. \u2022 The framework can compare clusterings across data sets. It is join-weighted decomposable on components (Property 4.1), consistent between a component and its subcomponents (Property 4.4), and conditionally normalized (Property 5.1). Moreover, it satisfies many properties of the variation of information (Meila\u0306, 2007). \u2022 The framework is flexible and easy to use. It can be instantiated by providing a measure to score a subcomponent, which contains a cluster and a partition of the cluster. It is relatively easier to design such a measure. For example, one can either make use of an existing measure or additional information of data points, such as feature vectors or pairwise distances.\nThe rest of the paper is organized as follows. Section 2 introduces some relevant concepts and notations. Some representative measures are discussed in Section 3. We present the split-merge framework in Section 4 and compare it with other measures in Section 5. Experimental results are given in Section 6, and Section 7 concludes this work."}, {"heading": "2. Preliminaries", "text": "Let D = {1, 2, . . . , n} be a set of n data points, and let the feature vector of the i-th point be denoted by fi. A clustering is a set of clusters, and a cluster is a set of points. Let \u2126 be the set of all clusterings, L \u2208 \u2126 be the true clustering and C \u2208 \u2126 be the predicted clustering. L (resp. C) denotes any cluster of L (resp. C). Denote an empty clustering by \u2205 and an empty cluster by \u2205. A cluster is a singleton if it contains only one data point. The term ( n 2 ) = n(n\u2212 1)/2\nis the number of pairwise links between n points. The entropy of a clustering L is H(L) = \u2212\u2211L\u2208L |L|n log |L|n , while the joint entropy between two clusterings L and C is H(L,C) = \u2212\u2211L\u2208L\u2211C\u2208C |L\u2229C|n log |L\u2229C|n . The amount of information shared between L and C is I(L,C) = H(L) +H(C)\u2212H(L,C). The conditional entropy of C given L is H(C|L) = H(C)\u2212 I(C,L) (Cover & Thomas, 1991).\nWe introduce some relevant concepts from lattice theory (Gra\u0308tzer, 2011). Top > is the clustering that groups all the points into a cluster, and bottom \u22a5 is the clustering that treats each point as a singleton (Gra\u0308tzer, 2011). C refines L if C can be obtained by only splitting one or more clusters of L. The meet M = {L \u2229 C | L \u2208 L, C \u2208 C, L \u2229 C 6= \u2205} is the clustering that contains all nonempty intersections of every cluster from L with every cluster from C. The join J is the clustering with the greatest number of clusters that is refined by both L and C. J denotes any cluster of J. Note that both M and J are partitions of D."}, {"heading": "3. Related Work", "text": "In this paper, we focus on studying symmetric similarity measures. For a distance measure, we study its counterpart similarity measure by subtracting it from one. Meila\u0306 (2007); Wagner & Wagner (2007); Vinh et al. (2010) summarized and compared a large number of measures that have been proposed in the literature, and a few representative measures are discussed as follows."}, {"heading": "3.1. Pair Counting Measures", "text": "Rand Index A link is positive if the two points are within the same cluster, otherwise it is negative. There are P (L,C) = \u2211 L\u2208L \u2211 C\u2208C (|L\u2229C| 2 ) positive links and(\nn 2\n) \u2212 P (L,L)\u2212 P (C,C) + P (L,C) negative links common to two clusterings. Rand index is the fraction R(L,C) = ( ( n 2 ) \u2212 P (L,L)\u2212 P (C,C) + 2P (L,C))/ ( n 2\n) of links common to two clusterings (Rand, 1971). It is large when there are many clusters (Wagner & Wagner, 2007)."}, {"heading": "3.2. Set Matching Measures", "text": "Van Dongen Criterion In order to transform L to C, 2n\u2212\u2211L\u2208L maxC\u2208C |L \u2229 C| \u2212\u2211C\u2208C maxL\u2208L |L \u2229 C| point moves are required (Dongen, 2000). This metric can be constrained to a right-open interval [0, 1) by dividing by 2n (Meila\u0306, 2007). The similarity counterpart is N(L,C) = 1 2 \u2211 L\u2208L maxC\u2208C |L\u2229C| n + 1 2 \u2211 C\u2208C maxL\u2208L |L\u2229C| n . It is not normalized because its lower bound is nonzero.\nClassification Accuracy By considering clustering as a classification task, classification accuracy computes the fraction of points that are correctly classified (Meila\u0306 & Heckerman, 2001). Finding the best mapping between two clusterings is equivalent to solving a maximum weighted bipartite matching problem (Meila\u0306, 2005). The classification accuracy is A(L,C) = maxW \u2211 L \u2211 C W (L,C) |L\u2229C| n subject to\n\u2200L\u2200C,W (L,C) \u2208 {0, 1}; \u2200L,\u2211C W (L,C) = 1; and \u2200C,\u2211LW (L,C) = 1, and C (resp. L) ranges over C (resp. L). Its lower bound is 1/n instead of zero."}, {"heading": "3.3. Information Theoretic Measures", "text": "Normalized Mutual Information Vinh et al. (2010) advocated NMI(L,C) = I(L,C)/max{H(L), H(C)} after comparing several normalized variants of the mutual information. The issue with NMI(L,C) is that I(L,C) indicates the degree of statistical dependency between two clusterings, and this is not always consistent with their similarity. For example, I(>,C) = 0 means there is no dependency between > and any C \u2208 \u2126, but the actual similarity depends on the closeness of C to >.\nNormalized Variation of Information The variation of information VI(L,C) = H(C|L) +H(L|C) is the change in the amount of information when transforming L into C (Meila\u0306, 2007). Although VI(L,C) has certain desired properties, it is unnormalized. This can be rectified through dividing by the upper bound log n (Meila\u0306, 2007). Subtracting it from unity gives the similarity measure V (L,C) = 1\u2212VI(L,C)/ log n. However, this is not suitable for comparing clusterings across data sets due to the dependence on n (Meila\u0306, 2007). When both L and C have at most k \u2264 \u221an clusters, VI(L,C) is upper bounded by 2 log k (Meila\u0306, 2007). Thus, K(L,C) = 1\u2212 VI(L,C)/ log k2 is an alternative similarity measure that is suitable for comparing clusterings across data sets, but applying this in practice requires knowing k in advance (Meila\u0306, 2007)."}, {"heading": "4. The Split-Merge Framework", "text": "In Section 4.1, we model the relation between two clusterings as a bipartite graph that can be decomposed into connected components. A component is further decomposed into split and merge subcomponents in Section 4.2. The split and merge subcomponents can be combined into a derivation graph D that transforms L into C. In Section 4.3, we capture the essence of D by pairing split subcomponents with merge subcomponents. The similarity of each pair, called a subcomponent pair, is discussed in Section 4.4. It is also there that the precise definition for our split-merge framework is given. Two instances of our framework are given in Section 4.5."}, {"heading": "4.1. Connected Components of a Bipartite Graph", "text": "A bipartite graph governs the relation between L and C: Definition 4.1 (Bipartite Graph). Given clusterings L and C, a directed bipartite graph G = (L,C,E) is constructed: L and C are the two disjoint sets of vertices, and E = {\u3008L,C\u3009 | L \u2208 L, C \u2208 C, L \u2229 C 6= \u2205} is the set of directed edges from L to C.\nDefinition 4.2 (Induced Clustering). A clustering C gives an induced clustering CA = {A \u2229 C | C \u2208 C, A \u2229 C 6= \u2205}\nwhen acted upon by a cluster A (Meila\u0306, 2007).\nDefinition 4.3 (Induced Subgraph). GA = (LA,CA,EA) is a subgraph induced on G = (L,C,E) by a cluster A, where LA and CA are induced clusterings, and EA = {\u3008L,C\u3009 | L \u2208 LA, C \u2208 CA, L \u2229 C 6= \u2205} is the set of the remaining edges on the induced clusterings.\nProposition 4.1 (Component). {GJ | J \u2208 J} is the set of connected components of graph G.\nProof. Construct a graph G\u2032 by letting all directed edges of graph G be undirected. Constructing a clustering that is refined by both L and C requires that all reachable clusters of G\u2032 should be grouped together. Many such clusterings can be obtained, and the join J is the one with the largest number of clusters. So all non-reachable clusters of G\u2032 should not be grouped together. This process is equivalent to finding the connected components of graph G.\nThroughout the paper, component means weakly connected component. Figure 1 gives a bipartite graph and its components. Denote a general similarity measure by S(L,C). The similarity score of GJ is defined as the similarity S(LJ ,CJ) between LJ and CJ . Most measures on clusterings can be expressed as the component-based decomposition formula\nS(L,C) = \u2211 J\u2208J w(J, n)S(LJ ,CJ) + b(J, n), (1)\nwhere w(J, n) is the weight for component GJ , and b(J, n) is independent of the component scores. Table 1 lists some measures and their decompositions, which are shown in Section 2 of the supplementary material.\nIn this paper, we propose a restriction on the class of S(L,C) given in (1). We opine that S(L,C) should simply be the weighted average of the similarity scores of all components {GJ |J \u2208 J}. That is b(J, n) = 0. Moreover, the importance of component GJ is determined by the importance of all the points within J . In the absence of additional information, every point should be treated equally, so w(J, n) is to be proportional to the size of cluster J . In summary, we propose the following convex combination.\nProperty 4.1 (Join-weighted Decomposition). A similarity measure S(L,C) is join-weighted decomposable if\nS(L,C) = \u2211 J\u2208J |J | n S(LJ ,CJ). (2)\nIf C refines L, the above property becomes the similarity version of the convex additivity axiom (Meila\u0306, 2007, Axiom A3). Since J is the least clustering that is refined by both L and C, 1 \u2212 S(L,C) also satisfies the additivity of composition property (Meila\u0306, 2007, Property 8) if S(L,C) satisfies Property 4.1."}, {"heading": "4.2. Split and Merge Subcomponents", "text": "A component focuses on the clustering-clustering relation. It may be difficult to assign a score to such a relation. Hence, we further break a component into subcomponents, with the focus on the cluster-clustering relations. We define two kinds of subcomponents depending on whether cluster in the relation is from L or C. Definition 4.4 (Split/Merge Graph). The split graph \u3008L,C\u3009 is the complete directed bipartite graph from {L} to the induced clustering CL. The merge graph \u3008L, C\u3009 is the complete directed bipartite graph from the induced clustering LC to {C}.\nConceptually, a split graph maps a cluster L to one or more clusters of C which overlap with L, while a merge graph maps one or more clusters of L which overlap with C to C. For a component GJ , there can be one or more split/merge graphs, which we call its subcomponents. Definition 4.5 (Split/Merge Set/Subcomponent). The split set of component GJ = (LJ ,CJ ,EJ) is the set {\u3008L,CJ\u3009 | L \u2208 LJ} of split graphs. The merge set of the component is the set {\u3008C,LJ\u3009 | C \u2208 CJ} of merge graphs. Each element in the split (resp. merge) set is called a split (resp. merge) subcomponent of GJ .\nSometimes, we write \u3008L,C\u3009 instead of \u3008L,CJ\u3009 in the context of a split subcomponent of GJ since L \u2208 LJ , so CL is the same as (CJ)L. Similarly for the merge subcomponent.\nProposition 4.2. The set of sinks in the split set of GJ is the same as the set of sources in the merge set of GJ . This set is the meet MJ of LJ and CJ , and it is identical to the meet M of L and C induced by J .\nWith the above proposition, we can transform LJ to CJ via MJ . The transformation consists of splitting of clusters in LJ into clusters in MJ (if necessary), then merging into clusters in CJ (if necessary). The split and merge mappings are given by the split and merge set of GJ . Figure 2 gives an example. Formally, the transformation follows the derivation graph that combines the split and merge sets.\nDefinition 4.6 (Derivation Graph). The derivation graph D of G = (L,C,E) is a tripartite graph with parts L, C and their meet M. The set of subgraphs in D from L to M is the union of the split sets of the components of G (up to relabeling of the sinks in the union to the clusters in M); and the set of subgraphs from M to C is the union of the merge sets (up to relabeling). There is no edge between vertices in L and vertices in C.\nA subcomponent is between a cluster and a clustering, which can be assigned a score more easily than a component. Denote the similarity measure of a split (resp. merge) subcomponent \u3008L,C\u3009 (resp. \u3008L, C\u3009) by s(C|L) (resp. s(L|C)). We opine that these measures should include two factors: the number of clusters and the relative size of each cluster (Wagner & Wagner, 2007). Hence, we propose the following property.\nProperty 4.2 (Monotonically Decreasing). A subcomponent similarity measure is monotonically decreasing if it monotonically decreases as the number of clusters increases or the distribution of cluster sizes gets less skewed.\nThe above property becomes the cluster completeness (resp. cluster homogeneity) constraint (Amigo\u0301 et al., 2009) when applied to a split (resp. merge) subcomponent. These two constraints are important to clustering measures\n(Rosenberg & Hirschberg, 2007).\nTo ensure that s(C|L) and s(L|C) are normalized, we propose the following property.\nProperty 4.3 (Subcomponent-normalization). A split subcomponent similarity measure s(C|L) is normalized if\n1. s(C|L) = 1 if and only if CL = {L}; 2. s(C|L) = 0 if and only if CL has only singletons and L is not a singleton; and\n3. s(C|L) \u2208 (0, 1) otherwise. Similarly for the merge subcomponent measure s(L|C).\nProperty 4.2 and Property 4.3 ensure that the similarity measures can be used to score clusterings across data sets.\nA split subcomponent \u3008L,C\u3009 is a connected bipartite graph by definition. Hence, the similarity measure S(L,C) is also applicable to it. For consistency, a requirement is that the S({L},CL) evaluates to be the same as s(C|L). The same must hold for the merge subcomponent.\nProperty 4.4 (Subcomponent-consistency). A similarity measure S(L,C) is subcomponent-consistent if S({L},CL) = s(C|L) and S(LC , {C}) = s(L|C)."}, {"heading": "4.3. Subcomponent Pairs", "text": "Within a component GJ , a split (resp. merge) subcomponent may be paired with one or more merge (resp. split) subcomponents in the derivation graph D. If S(LJ ,CJ) were to be a direct combination of the similarity measures on the subcomponents of GJ , it might give the same value for the different sets of pairings of the subcomponents. This is undesirable. Instead, we propose to base the combination on pairs of split and merge subcomponents.\nDefinition 4.7 (Subcomponent Pair). In a component GJ , a subcomponent pair is the pair (\u3008L,CJ\u3009, \u3008LJ , C\u3009) such that L \u2208 LJ , C \u2208 CJ and L \u2229 C \u2208MJ .\nThis definition exploits that a split and a merge subcomponent are not disjoint in D only if a sink in the split subcomponent and a source in the merge subcomponent is the same cluster in MJ ; see Proposition 4.2. Proposition 4.3. For every M \u2208MJ there is one and only one subcomponent pair (\u3008L,CJ\u3009, \u3008LJ , C\u3009) such that L \u2229 C = M .\nProof. The existence of the pair is by definition of a subcomponent pair. For uniqueness, suppose there is another L\u2032 6= L where L\u2032 \u2208 LJ and L\u2032 \u2229 C \u2032 = M for some C \u2032 \u2208 CJ . Then L\u2032 \u2229 C \u2032 = L \u2229 C. Since L is a partition of D, L\u2032 \u2229 L = \u2205. So both L\u2032 \u2229 C \u2032 and L \u2229 C must be \u2205 for them to be equal. But M 6= \u2205 by the definition of meet. Hence a contradiction in this case. The other case where C \u2032 6= C gives a contradiction similarly.\nWith this proposition, we can exactly enumerate all the subcomponent pairs in a component GJ by enumerating the clusters in MJ . Moreover, MJ is a partition on J . These two properties suggest the following decomposition of S(LJ ,CJ) in the spirit of Property 4.1.\nS(LJ ,CJ) = \u2211\nM\u2208MJ\n|M | |J | \u2211 L\u2208LJ ,C\u2208CJ \u03b4(L \u2229 C,M)\u03c3(\u3008L,CJ\u3009, \u3008LJ , C\u3009),\nwhere \u03b4 is the Kronecker delta, and \u03c3(\u00b7, \u00b7) is the similarity measure on a subcomponent pair \u3008\u3008L,CJ\u3009, \u3008LJ , C\u3009\u3009. We will discuss the choice of \u03c3(\u00b7, \u00b7) in Section 4.4. Using Proposition 4.3 and |\u2205| = 0, the above decomposition can be simplified:\nS(LJ ,CJ) = \u2211 L\u2208LJ \u2211 C\u2208CJ |L \u2229 C| |J | \u03c3(\u3008L,CJ\u3009, \u3008LJ , C\u3009).\nThis can be directly substituted into (2). After subsuming the sum over the join into the sums over the clusterings L and C, we obtain the following convex combination. Property 4.5 (Meet-weighted Decomposition). A similarity measure S(L,C) is a meet-weighted decomposition if\nS(L,C) = \u2211 L\u2208L \u2211 C\u2208C |L \u2229 C| n \u03c3(\u3008L,C\u3009, \u3008L, C\u3009). (3)"}, {"heading": "4.4. Similarity on Subcomponent Pairs", "text": "We now discuss the choice of the similarity measure \u03c3 on a subcomponent pair. Given a pair (\u3008L,C\u3009, \u3008L, C\u3009), the general idea is to let \u03c3 be a function of the similarity measures s(C|L) and s(L|C) of the subcomponents. We discuss two functions: the product and the arithmetic mean. Our preference is the product because it satisfies subcomponentconsistency (Property 4.4), while the mean does not.\nProduct Our split-merge framework defines the meetweighted decomposed similarity\nS\u2217(L,C) = \u2211 L\u2208L \u2211 C\u2208C |L \u2229 C| n s(C|L)s(L|C). (4)\nwhere s(C|L) and s(L|C) are the subcomponentnormalized similarity measures of the split and merge subcomponents. It is subcomponent-consistent because\nS\u2217({L},CL) = \u2211\nC\u2208CL\n(|C|/|L|)s(CL|L)s({C}|C)\n= s(CL|L)  \u2211 C\u2208CL (|C|/|L|) s({C}|C)\ufe38 \ufe37\ufe37 \ufe38 =1  = s(C|L)\u00d71, where s(CL|L) = s(C|L) by definition of the split graph; and similarly for S\u2217(LC , {C}).\nArithmetic Mean One might also use other functions of s(C|L) and s(L|C) to define \u03c3. A natural choice is the arithmetic mean, which combined with (3) gives\nS\u2032(L,C) = 1\n2 \u2211 L\u2208L |L| n s(C|L) + 1 2 \u2211 C\u2208C |C| n s(L|C). (5)\nSome existing measures are instances of S\u2032. If we define s(C|L) = maxC\u2208C |L\u2229C||L| and s(L|C) = maxL\u2208L |L\u2229C| |C| , then S\u2032(L,C) becomes N(L,C). S\u2032(L,C) becomes K(L,C) if we define s(C|L) = 1\u2212H(CL)/ log k2 and s(L|C) = 1\u2212H(LC)/ log k2. The similarity S\u2032(L,C) directly combines the subcomponent similarities. Hence, it defeats the very purpose of subcomponent pairs. In contrast, the similarity given by (4) cannot be (linearly) decomposed further into subcomponent similarities. Moreover, S\u2032(L,C) is not subcomponentconsistent: S\u2032({L},CL) = s(C|L)/2 + 1/2 \u2265 s(C|L), with equality only when s(C|L) = 1."}, {"heading": "4.5. Examples", "text": "The split-merge framework (4) is flexible because the subcomponent similarity measures can be application specific. We give two examples.\nEntropy-based SH This example uses a normalized entropy of clustering. Let s(C|L) = 1\u2212H(CL)/ log |L| and s(L|C) = 1\u2212H(LC)/ log |C|. Substituting them into (4) gives a similarity measure in the split-merge framework, and we call this measure SH :\nSH(L,C) = \u2211 L\u2208L \u2211 C\u2208C |L \u2229 C| n ( 1\u2212 H(CL) log |L| )( 1\u2212 H(LC) log |C| ) .\nWe will compare this empirically to some existing similarity measures in Section 6.\nMean-squared-error-based The split-merge framework (4) can make use of the feature vectors of points or the distances between points when these are available. In contrast, previous external measures ignore such information (Coen et al., 2010). Many internal compactness and separation measures can be used as subcomponent measures (Liu et al., 2010). We give an example using the mean-squared-error to score a subcomponent. For a split subcomponent \u3008L,C\u3009, the mean-squared-error of CL is mse(CL) = \u2211 C\u2208CL \u2211 i\u2208C(fi \u2212 f\u0304C)2/n, where\nf\u0304C = \u2211\ni\u2208C fi/|C| is the center of cluster C. The similarity of the split subcomponent is defined to be\nMSE(C|L) = mse(CL) mse({L}) =\n\u2211 C\u2208CL \u2211 i\u2208C(fi \u2212 f\u0304C)2\u2211\ni\u2208L(fi \u2212 f\u0304L)2 ,\nwhere f\u0304L = \u2211 i\u2208L fi/|L| is the center of cluster L.\nUsing this subcomponent similarity measure, we can obtain a similarity measure between clusterings based on the splitmerge framework (4). It is efficient to compute because the meet of two clusterings can be computed in O(n) time when an appropriate data structure is used (Pantel & Lin, 2002). In contrast, the hybrid measure proposed by Coen et al. (2010) requires O(n2.6) time in the average case and O(n3 log n) time in the worst case."}, {"heading": "5. Comparisons with Existing Measures", "text": "We study some properties of our split-merge similarity framework S\u2217 given by (4) in comparison with other measures. There are other properties which we explore in Section 3 of the supplementary material."}, {"heading": "5.1. Conditional Normalization", "text": "To facilitate interpretation and comparison across different conditions (e.g., different data sets), the traditional normalization property focuses on the joint space of the two clusterings and requires that the range of a similarity measure should be normalized to a closed interval [0, 1] (Wagner & Wagner, 2007; Vinh et al., 2010), where the lower bound zero should be achievable. However, this does not take into account the fact that one clustering is typically the true clustering. Since one fundamental goal of a similarity measure is to rank clusterings against a true clustering, the similarities with respect to the true clustering should also be normalized (Luo et al., 2009): given a true clustering L, a similarity measure should be between zero and one with both extremes attainable. Luo et al. (2009) have found that some information theoretic measures did not satisfy this property, and they have proposed a normalization procedure using the extreme values attained by the original measures, which typically depends on L or n. Here, we propose conditional normalization based on a three-way partitioning of the set \u2126 of all possible clusterings of n data points given a true clustering L.\nProperty 5.1 (Conditional Normalization). A similarity measure S(L,C) is conditionally normalized if, given L,\n1. S(L,C) = 1 if and only if C = L; 2. S(L,C) = 0 if and only if C \u2208 \u2126L; and 3. S(L,C) \u2208 (0, 1) otherwise, i.e., C \u2208 \u039bL,\nwhere {{L},\u2126L,\u039bL} partitions \u2126 such that \u2126L = \u2205 if and only if n = 1, and \u039bL = \u2205 if and only if n \u2264 2.\nWe call C = L the best clustering, and it is the only clustering can be scored one against L. Each C \u2208 \u2126L is called a worst clustering, and the similarity between it and L must be zero. With these, the extremes of [0, 1] are realized. All other clusterings (i.e., those \u039bL) have similarities in (0, 1) with L. Our definition is more stringent than that afforded\nby the procedure of (Luo et al., 2009) in two ways. First, only clustering C = L can have the similarity one. This is to reflect that there is only one true clustering. Second, we demand that \u039bL be non-empty for n \u2265 3. This gives a gradation of similarities from one to zero as a clustering deteriorates from the best clustering to a worst clustering; it reflects how far a clustering is from the best clustering and a worst clustering.\nA similarity measure is not normalized if its lower bound (resp. upper bound) is not zero (resp. one). An unnormalized measure is also not conditionally normalized. To determine whether a normalized similarity measure is conditionally normalized, we need to determine the subset \u2126L for any L. This requires us to determine the lower bound of a similarity measure as well as its lower bound condition that indicates what kind of clusterings are considered to be the worst. The lower bound conditions of the similarity measures are summarized in Table 2, and they are derived in Section 1 of the supplementary material. \u2126L should not be empty for any L \u2208 \u2126 with n \u2265 1, so a normalized similarity measure is not conditionally normalized if there exists a clustering L such that the lower bound condition has no solution. The following proposition gives the normalization properties of a selection of similarity measures.\nProposition 5.1. N , A, and I are not normalized. R, NMI , V , and K are normalized but not conditionally normalized. S\u2217 is conditionally normalized.\nProof. N and A are not normalized because their lower bounds are (b\u221anc+ d\u221an e)/2n and 1/n, respectively. I is not normalized because its upper bound is not always one. The lower bounds of other similarity measures are zero. R can be zero only when L is > or \u22a5, so it is not conditionally normalized. The third category \u039b> of > is always empty because NMI(>,C) = 0 for any C \u2208 \u2126, so it is not conditionally normalized. When k = \u221a n, K becomes V . The lower bound condition of V has no solution when L = {{1, 2}, {3}}, so V and K are not conditionally normalized.\nWe prove that S\u2217 is conditionally normalized. First, it is\nclear that S\u2217(L,C) = 1 if and only if L = C. Second, its lower bound condition in Table 2 indicates that at least one worst-clustering can be constructed. Thirdly, the lower bound condition implies \u2126L 6= \u2126 \\ {L}, and at least a clustering can be obtained such that its similarity score is in (0, 1). For instance, if L 6= \u22a5, then \u22a5 is such a clustering because S\u2217(L,\u22a5) \u2208 (0, 1); and when L = \u22a5 and |L| > 2, such a clustering can be created by merging only two singletons."}, {"heading": "5.2. Join-weighted Decomposition and Consistency", "text": "We study whether some existing similarity measures satisfy Properties 4.1 and 4.4.\nProposition 5.2. Only N , A, K, and S\u2217 are join-weighted decomposable. A and S\u2217 are subcomponent-consistent.\nProof. That only N , A, K, and S\u2217 are join-weighted decomposable is shown in Table 1. Measures N and K are instances of S\u2032 in (5), so they are not subcomponent-consistent. That S\u2217({L},CL) is subcomponent-consistent is shown in Section 4.4. For A, we have A({L},CL) = maxC\u2208CL |C|/|L| = a(C|L) and A(LC , {C}) = a(L|C), where a is the relevant subcomponent measure."}, {"heading": "6. Experiments", "text": "We compare the conditional normalization and monotonically decreasing properties of various measures using the coreference resolution task. This task is to group noun phrases (data points) that refer into the same real-world entity (clusters) (Ng & Cardie, 2002). The \u03c63-CEAF measure (Luo, 2005) is frequently used to evaluate the performance of coreference algorithms, and it is the same as the classification accuracy. For a measure in our split-merge framework, we use the entropy-based SH in Section 4.5. Our experiments use a randomly selected document in the ACE-2005 English data set (Rahman & Ng, 2011).\nWe construct a series of clusterings from the best to a worst with respect to a given clustering. This series serves to evaluate similarity measures with respect to the following desideratum: a reasonable similarity score should decrease strictly from one to zero as the clustering \u201cworsens\u201d from the best to a worst. We use two operations to construct the series: (a) a binary split operation that splits the largest non-singleton cluster into two equal-sized clusters; and (b) a binary merge operation that either merges two true singletons into one cluster or merges a true singleton with a randomly selected cluster if there is only one singleton, where a true singleton is one that is in the true clustering. Given a true clustering, we first apply the binary split operation repeatedly to transform the true clustering to \u22a5.\nThen we apply the binary merge operation repeatedly to transform \u22a5 to a worst-clustering of the true clustering. Figure 3 plots the (normalized) similarities as the number of operations increases. Although all the measures decreases as the generated clustering worsens, only SH decreases from one to zero. This is because, among these measures, only SH is conditionally normalized; see Proposition 5.1. In addition, the other measures are rather far from zero at the worst-clustering. The figure also shows that SH is strictly decreasing. This is also satisfied by R, NMI and V but not by set matching measures A and N ."}, {"heading": "7. Conclusion", "text": "By modeling the intrinsic relation between two clusterings as a bipartite graph, we have proposed a split-merge framework that can be used to obtain similarity measures to compare clusterings on different data sets. In contrast with a representative selection of existing similarity measures, any measure obtained via the framework is conditionally normalized, join-weighted decomposable, and subcomponent-consistent. Conditional normalization is especially important because it allows comparing different clusterings of different data sets. In addition, our framework can also use feature vectors of data points or distances between data points."}, {"heading": "Acknowledgments", "text": "This work is supported by DSO grant DSOCL10021."}], "references": [{"title": "Comparing clusterings in space", "author": ["Coen", "Michael H", "Ansari", "M. Hidayath", "Fillmore", "Nathanael"], "venue": "In ICML, pp", "citeRegEx": "Coen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coen et al\\.", "year": 2010}, {"title": "Performance criteria for graph clustering and Markov cluster experiments", "author": ["Dongen", "Stijn"], "venue": "Technical report, National Research Institute for Mathematics and Computer Science,", "citeRegEx": "Dongen and Stijn.,? \\Q2000\\E", "shortCiteRegEx": "Dongen and Stijn.", "year": 2000}, {"title": "Lattice Theory: Foundation", "author": ["Gr\u00e4tzer", "George"], "venue": "Springer, 1st edition,", "citeRegEx": "Gr\u00e4tzer and George.,? \\Q2011\\E", "shortCiteRegEx": "Gr\u00e4tzer and George.", "year": 2011}, {"title": "Clustering: Science or art", "author": ["Guyon", "Isabelle", "Luxburg", "Ulrike Von", "Williamson", "Robert C"], "venue": "In NIPS Workshop on Clustering Theory,", "citeRegEx": "Guyon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2009}, {"title": "Comparing partitions", "author": ["Hubert", "Lawrence", "Arabie", "Phipps"], "venue": "Journal of Classification,", "citeRegEx": "Hubert et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Hubert et al\\.", "year": 1985}, {"title": "Understanding of internal clustering validation measures", "author": ["Liu", "Yanchi", "Li", "Zhongmou", "Xiong", "Hui", "Gao", "Xuedong", "Wu", "Junjie"], "venue": "In ICDM,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Information-theoretic distance measures for clustering validation: Generalization and normalization", "author": ["Luo", "Ping", "Xiong", "Hui", "Zhan", "Guoxing", "Wu", "Junjie", "Shi", "Zhongzhi"], "venue": "IEEE TKDE,", "citeRegEx": "Luo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2009}, {"title": "On coreference resolution performance metrics", "author": ["Luo", "Xiaoqiang"], "venue": "In HLT, pp", "citeRegEx": "Luo and Xiaoqiang.,? \\Q2005\\E", "shortCiteRegEx": "Luo and Xiaoqiang.", "year": 2005}, {"title": "Comparing clusterings: an axiomatic view", "author": ["Meil\u0103", "Marina"], "venue": "In ICML, pp", "citeRegEx": "Meil\u0103 and Marina.,? \\Q2005\\E", "shortCiteRegEx": "Meil\u0103 and Marina.", "year": 2005}, {"title": "Comparing clusterings \u2014 an information based distance", "author": ["Meil\u0103", "Marina"], "venue": "J. Multivar. Anal.,", "citeRegEx": "Meil\u0103 and Marina.,? \\Q2007\\E", "shortCiteRegEx": "Meil\u0103 and Marina.", "year": 2007}, {"title": "An experimental comparison of model-based clustering", "author": ["Meil\u0103", "Marina", "Heckerman", "David"], "venue": "methods. ML,", "citeRegEx": "Meil\u0103 et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Meil\u0103 et al\\.", "year": 2001}, {"title": "Improving machine learning approaches to coreference resolution", "author": ["Ng", "Vincent", "Cardie", "Claire"], "venue": "In ACL,", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Efficiently clustering documents with committees", "author": ["Pantel", "Patrick", "Lin", "Dekang"], "venue": "In PRICAI, pp", "citeRegEx": "Pantel et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2002}, {"title": "Narrowing the modeling gap: a cluster-ranking approach to coreference", "author": ["Rahman", "Altaf", "Ng", "Vincent"], "venue": "resolution. JAIR,", "citeRegEx": "Rahman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2011}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["Rand", "William M"], "venue": "JASA, 66(336):846\u2013850,", "citeRegEx": "Rand and M.,? \\Q1971\\E", "shortCiteRegEx": "Rand and M.", "year": 1971}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["Rosenberg", "Andrew", "Hirschberg", "Julia"], "venue": "In EMNLP-CoNLL, pp", "citeRegEx": "Rosenberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2007}, {"title": "Cluster ensembles \u2014 a knowledge reuse framework for combining multiple partitions", "author": ["Strehl", "Alexander", "Ghosh", "Joydeep"], "venue": "JMLR, 3:583\u2013617,", "citeRegEx": "Strehl et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2003}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Vinh", "Nguyen Xuan", "Epps", "Julien", "Bailey", "James"], "venue": null, "citeRegEx": "Vinh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vinh et al\\.", "year": 2010}, {"title": "Comparing clusterings \u2014 an overview", "author": ["Wagner", "Silke", "Dorothea"], "venue": null, "citeRegEx": "Wagner et al\\.,? \\Q1907\\E", "shortCiteRegEx": "Wagner et al\\.", "year": 1907}], "referenceMentions": [{"referenceID": 17, "context": "Unnormalized and asymmetric measures are inappropriate for comparing clusterings across data sets (Vinh et al., 2010; Wagner & Wagner, 2007).", "startOffset": 98, "endOffset": 140}, {"referenceID": 6, "context": "When a measure is used for comparing all possible clusterings with the true clustering, it is preferable that the similarity scores should be in the closed interval [0, 1] (Luo et al., 2009).", "startOffset": 172, "endOffset": 190}, {"referenceID": 17, "context": "They have been analyzed extensively and systematically in recent years (Meil\u0103, 2007; Vinh et al., 2010).", "startOffset": 71, "endOffset": 103}, {"referenceID": 17, "context": "Some measures tend to give high scores in practice, so adjusted measures, such as adjusted Rand index (Hubert & Arabie, 1985) and adjusted mutual information (Vinh et al., 2010) are proposed to address this issue, but they are not normalized because they may be negative (Meil\u0103, 2007).", "startOffset": 158, "endOffset": 177}, {"referenceID": 3, "context": "In this paper, instead of focusing on designing a new clustering measure, we propose a split-merge framework that can be tailored to different applications (Guyon et al., 2009).", "startOffset": 156, "endOffset": 176}, {"referenceID": 17, "context": "Meil\u0103 (2007); Wagner & Wagner (2007); Vinh et al. (2010) summarized and compared a large number of measures that have been proposed in the literature, and a few representative measures are discussed as follows.", "startOffset": 38, "endOffset": 57}, {"referenceID": 17, "context": "Normalized Mutual Information Vinh et al. (2010) advocated NMI(L,C) = I(L,C)/max{H(L), H(C)} after comparing several normalized variants of the mutual information.", "startOffset": 30, "endOffset": 49}, {"referenceID": 0, "context": "In contrast, previous external measures ignore such information (Coen et al., 2010).", "startOffset": 64, "endOffset": 83}, {"referenceID": 5, "context": "Many internal compactness and separation measures can be used as subcomponent measures (Liu et al., 2010).", "startOffset": 87, "endOffset": 105}, {"referenceID": 0, "context": "In contrast, the hybrid measure proposed by Coen et al. (2010) requires O(n) time in the average case and O(n log n) time in the worst case.", "startOffset": 44, "endOffset": 63}, {"referenceID": 17, "context": ", different data sets), the traditional normalization property focuses on the joint space of the two clusterings and requires that the range of a similarity measure should be normalized to a closed interval [0, 1] (Wagner & Wagner, 2007; Vinh et al., 2010), where the lower bound zero should be achievable.", "startOffset": 214, "endOffset": 256}, {"referenceID": 6, "context": "Since one fundamental goal of a similarity measure is to rank clusterings against a true clustering, the similarities with respect to the true clustering should also be normalized (Luo et al., 2009): given a true clustering L, a similarity measure should be between zero and one with both extremes attainable.", "startOffset": 180, "endOffset": 198}, {"referenceID": 6, "context": "Since one fundamental goal of a similarity measure is to rank clusterings against a true clustering, the similarities with respect to the true clustering should also be normalized (Luo et al., 2009): given a true clustering L, a similarity measure should be between zero and one with both extremes attainable. Luo et al. (2009) have found that some information theoretic measures did not satisfy this property, and they have proposed a normalization procedure using the extreme values attained by the original measures, which typically depends on L or n.", "startOffset": 181, "endOffset": 328}, {"referenceID": 6, "context": "by the procedure of (Luo et al., 2009) in two ways.", "startOffset": 20, "endOffset": 38}], "year": 2012, "abstractText": "Clustering evaluation measures are frequently used to evaluate the performance of algorithms. However, most measures are not properly normalized and ignore some information in the inherent structure of clusterings. We model the relation between two clusterings as a bipartite graph and propose a general component-based decomposition formula based on the components of the graph. Most existing measures are examples of this formula. In order to satisfy consistency in the component, we further propose a split-merge framework for comparing clusterings of different data sets. Our framework gives measures that are conditionally normalized, and it can make use of data point information, such as feature vectors and pairwise distances. We use an entropy-based instance of the framework and a coreference resolution data set to demonstrate empirically the utility of our framework over other measures.", "creator": "LaTeX with hyperref package"}}}