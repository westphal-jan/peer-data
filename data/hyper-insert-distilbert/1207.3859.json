{"id": "1207.3859", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2012", "title": "Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning", "abstract": "we consider the complex estimation curves of an actor i. i. lucius d. vector $ \\ xbf \\ in \\ r ^ \u03bb n $ reconstructed from measurements $ \\ ybf \\ gamma in \\ _ r ^ r m $ log obtained by a hierarchical general cascade regression model consisting ultimately of a known relative linear transform type followed by a probabilistic matching componentwise ( structurally possibly nonlinear ) measurement reference channel. presently a novel simulation method, called adaptive generalized approximate message passing ( adaptive gamp ), that presently enables joint constraint learning of the statistics of constructing the valued prior and measurement channel along with proportional estimation of the unknown vector $ \\ xbf $ is presented. the simplest proposed integration algorithm iteration is a generalization methods of being a recently recently - developed calculation technique by schwarz vila and schniter that uses expectation - maximization ( em ) iterations where the posteriors in the e - estimate steps rm are computed via functional approximate message passing. the proposed synthesis methodology can be applied specifically to probe a large spatial class e of learning constrained problems including calculating the learning of these sparse priors estimated in compressed sensing or identification learning of linear - coupled nonlinear cascade collision models in structural dynamical systems and neural programming spiking processes. we prove that for arbitrary large i. claudius i. or d. gaussian transform vector matrices the asymptotic componentwise behavior curve of f the simpler adaptive smoothing gamp reduction algorithm v is predicted by examining a simple set type of scalar optimal state evolution modeling equations. theoretically this hybrid analysis shows that the fully adaptive weighted gamp method can yield coherent asymptotically consistent generalized parameter estimates, which alternatively implies that the sampling algorithm achieves via a weak reconstruction gibbs quality equivalent furthermore to the oracle algorithm q that knows also the correct elastic parameter range values. the software adaptive gamp convergence methodology thus provides in a systematic, general consensus and computationally fully efficient method therefore applicable essentially to investigate a structurally large a range of socially complex linear - statistical nonlinear models with provable computation guarantees.", "histories": [["v1", "Tue, 17 Jul 2012 01:50:46 GMT  (105kb,D)", "https://arxiv.org/abs/1207.3859v1", "16 pages, 3 figures"], ["v2", "Thu, 19 Jul 2012 13:35:50 GMT  (101kb,D)", "http://arxiv.org/abs/1207.3859v2", "10 pages, 3 figures"], ["v3", "Sat, 1 Dec 2012 23:30:36 GMT  (180kb,D)", "http://arxiv.org/abs/1207.3859v3", "14 pages, 3 figures"]], "COMMENTS": "16 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT", "authors": ["ulugbek kamilov", "sundeep rangan", "alyson k fletcher", "michael unser"], "accepted": true, "id": "1207.3859"}, "pdf": {"name": "1207.3859.pdf", "metadata": {"source": "CRF", "title": "Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning", "authors": ["Ulugbek S. Kamilov", "Sundeep Rangan", "Alyson K. Fletcher"], "emails": ["ulugbek.kamilov@epfl.ch)", "srangan@poly.edu)", "afletcher@ucsc.edu)", "michael.unser@epfl.ch)"], "sections": [{"heading": null, "text": "I. INTRODUCTION Consider the estimation of a random vector x \u2208 Rn from the measurement model illustrated in Figure 1. The random vector x, which is assumed to have independent and identically distributed (i.i.d.) components xj \u223c PX , is passed through a known linear transform that outputs z = Ax. The components of y \u2208 Rm are generated by a componentwise transfer function PY |Z . This work addresses the cases where the distributions PX and PY |Z have some unknown parameters,\nThe work of S. Rangan was supported by the National Science Foundation under Grant No. 1116589. U. S. Kamilov and M. Unser were supported by the European Commission under Grant ERC-2010-AdG 267439-FUN-SP.\nU. S. Kamilov (email: ulugbek.kamilov@epfl.ch) is with Biomedical Imaging Group, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Switzerland\nS. Rangan (email: srangan@poly.edu) is with Polytechnic Institute of New York University, Brooklyn, NY.\nA. K. Fletcher (email: afletcher@ucsc.edu) is with the Department of Electrical Engineering, University of California, Santa Cruz.\nM. Unser (email: michael.unser@epfl.ch) is with Biomedical Imaging Group, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Switzerland\n\u03bbx and \u03bbz , that must be learned in addition to the estimation of x.\nSuch joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5]. Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]\u2013[8].\nWhen the distributions PX and PY |Z are known, or reasonably bounded, there are a number of methods available that can be used for the estimation of the unknown vector x. In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]\u2013[18]. These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17]. See, also the survey article [19]. The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z. The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17]. This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).\nHowever, although the current formulation of AMP and GAMP methods is attractive conceptually, in practice, one often does not know the prior and noise distributions exactly.\nar X\niv :1\n20 7.\n38 59\nv3 [\ncs .I\nT ]\n1 D\nec 2\n01 2\nTo overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al. [24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (\u03bbx, \u03bbz) along with the estimation of the vector x. While simulations indicate excellent performance, the analysis of these methods is difficult. This work provides a unifying analytic framework for such AMP-based joint estimation and learning methods. The main contributions of this paper are as follows: \u2022 Generalization of the GAMP method of [18] to a class\nof algorithms we call adaptive GAMP that enables joint estimation of the parameters \u03bbx and \u03bbz along with vector x. The methods are computationally fast and general with potentially large domain of application. In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]\u2013[25] as special cases. \u2022 Exact characterization of the asymptotic behavior of adaptive GAMP. We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations. \u2022 Demonstration of asymptotic consistency of adaptive GAMP with maximum-likelihood (ML) parameter estimation. Our main result shows that when the ML parameter estimation is computed exactly, the estimated parameters converge to the true values and the performance of adaptive GAMP asymptotically coincides with the performance of the oracle GAMP algorithm that knows the correct parameter values. Remarkably, this result applies to essentially arbitrary parameterizations of the unknown distributions PX and PY |Z , thus enabling provably consistent estimation on non-convex and nonlinear problems. \u2022 Experimental evaluation of the algorithm for the problems of learning of sparse priors in compressed sensing and identification of linear-nonlinear cascade models in neural spiking processes. Our simulations illustrate the performance gain of adaptive GAMP and its asymptotic consistency. Adaptive GAMP thus provides a computationallyefficient method for a large class of joint estimation and learning problems with a simple, exact performance characterization and provable conditions for asymptotic consistency."}, {"heading": "A. Related Literature", "text": "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]\u2013[25]. In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26]. The \u201cexpectation\u201d or E-step is performed by GAMP, which can approximately determine the marginal posterior distributions of the components xj given the observations y and the current parameter estimates of the GM distribution PX . A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs. Simulations in [22], [23] show remarkably\ngood performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing. Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.\nAs discussed in Section III-B, EM-GAMP is a special case of adaptive GAMP with a particular choice of the adaptation functions. Therefore, one contribution of this paper is to provide a rigorous theoretical justification of the EMGAMP methodology. In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods. However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27]. An interesting open question is whether the analysis methods in this paper can be extended to these scenarios as well.\nAn alternate method for joint learning and estimation has been presented in [28], which assumes that the distributions on the source and output channels are themselves described by graphical models with the parameters \u03bbx and \u03bbz appearing as unknown variables. The method in [28], called Hybrid-GAMP, iteratively combines standard loopy BP with AMP methods. One avenue of future work is to see if methodology in this paper can be applied to analyze the Hybrid-GAMP methods as well.\nFinally, it should be pointed out that while simultaneous recovery of unknown parameters is appealing conceptually, it is not a strict requirement. An alternate solution to the problem is to assume that the signal belongs to a known class of distributions and to minimize the maximal mean-squared error (MSE) for the class. This minimax approach [29] was proposed for AMP recovery of sparse signals in [13]. Although minimax approach results in the estimators that are uniformly good over the entire class of distributions, there may be a significant gap between the MSE achieved by the minimax approach and the oracle algorithm that knows the distribution exactly. Indeed, this gap was the main justification of the EMGAMP methods in [22], [23]. Due to its asymptotic consistency, adaptive GAMP provably achieves the performance of the oracle algorithm."}, {"heading": "B. Outline of the Paper", "text": "The paper is organized as follows: In Section II, we review the non-adaptive GAMP and corresponding state evolution equations. In Section III, we present adaptive GAMP and describe ML parameter learning. In Section IV, we provide the main theorems characterizing asymptotic performance of adaptive GAMP and demonstrating its consistency. In Section V, we provide numerical experiments demonstrating the applications of the method. Section VI concludes the paper. A conference version of this paper has appeared in [30]. This paper contains all the proofs, more detailed descriptions and additional simulations."}, {"heading": "II. REVIEW OF GAMP", "text": ""}, {"heading": "A. GAMP Algorithm", "text": "Before describing the adaptive GAMP algorithm, it is useful to review the basic (non-adaptive) GAMP algorithm of [18]. Consider the estimation problem in Fig. 1 where the componentwise distributions on the inputs and outputs have some parametric form,\nPX(x|\u03bbx), PY |Z(y|z, \u03bbz), (1)\nwhere \u03bbx \u2208 \u039bx and \u03bbz \u2208 \u039bz represent parameters of the distributions and \u039bx and \u039bz some parameter sets.\nThe GAMP algorithm of [18] can be seen as a class of methods for estimating the vectors x and z for the case when the parameters \u03bbx and \u03bbz are known. In contrast, the adaptive GAMP method that is discussed in Section III enables joint estimation of the parameters \u03bbx and \u03bbz along with the vectors x and z. In order that to understand how the adaptation works, it is best to describe the basic GAMP algorithm as a special case of the more general adaptive GAMP procedure.\nThe basic GAMP algorithm corresponds to the special case of Algorithm 1 when the adaptation functions Htx and H t z output fixed values\nHtz(p t,y, \u03c4 tp) = \u03bb\nt z, H t x(r t, \u03c4 tr) = \u03bb t x, (2)\nfor some pre-computed sequence of parameters \u03bb t x and \u03bb t\nz . By \u201cpre-computed\u201d, we mean that the values do not depend on the data through the vectors pt, yt, and rt. In the oracle scenario \u03bb t x and \u03bb t\nz are set to the true values of the parameters and do not change with the iteration number t.\nThe estimation functions Gtx, G t z and G t s determine the estimates for the vectors x and z, given the parameter values \u03bb\u0302tx and \u03bb\u0302 t z . As described in [18], there are two important sets of choices for the estimation functions, resulting in two variants of GAMP: \u2022 Sum-product GAMP: In this case, the estimation func-\ntions are selected so that GAMP provides a Gaussian approximation of sum-product loopy BP. The estimates x\u0302t and z\u0302t then represent approximations of the MMSE estimates of the vectors x and z. \u2022 Max-sum GAMP: In this case, the estimation functions are selected so that GAMP provides a quadratic approximation of max-sum loopy BP and x\u0302t and z\u0302t represent approximations of the MAP estimates.\nThe estimation functions of the sum-product GAMP are equivalent to scalar MMSE estimation problems for the components of the vectors x and z observed in Gaussian noise. For max-sum GAMP, the estimation functions correspond to scalar MAP problems. Thus, for both versions, the GAMP method reduces the vector-valued MMSE and MAP estimation problems to a sequence of scalar AWGN problems combined with linear transforms by A and AT . GAMP is thus computationally simple, with each iteration involving only scalar nonlinear operations followed by linear transforms. The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]\u2013 [35]. Appendix A reviews the equations for the sum-product\nGAMP. More details, as well as the equations for max-sum GAMP can be found in [18]."}, {"heading": "B. State Evolution Analysis", "text": "In addition to its computational simplicity and generality, a key motivation of the GAMP algorithm is that its asymptotic behavior can be precisely characterized when A is a large i.i.d. Gaussian transform. The asymptotic behavior is described by what is known as a state evolution (SE) analysis. By now, there are a large number of SE results for AMP-related algorithms [9], [11]\u2013[18]. Here, we review the particular SE analysis from [18] which is based on the framework in [16].\nAssumption 1: Consider a sequence of random realizations of the GAMP algorithm, indexed by the dimension n, satisfying the following assumptions: (a) For each n, the matrix A \u2208 Rm\u00d7n has i.i.d. components\nwith Aij \u223c N (0, 1/m) and the dimension m = m(n) is a deterministic function of n satisfying n/m\u2192 \u03b2 for some \u03b2 > 0 as n\u2192\u221e. (b) The input vectors x and initial condition x\u03020 are deterministic sequences whose components converge empirically with bounded moments of order s = 2k \u2212 2 as\nlim n\u2192\u221e\n(x, x\u03020) PL(s) = (X, X\u03020), (3)\nto some random vector (X, X\u03020) for some k \u2265 2. See Appendix B for the precise definition of this form of convergence.\n(c) The output vectors z and y \u2208 Rm are generated by\nz = Ax, and yi = h(zi, wi) for all i = 1, . . . ,m, (4)\nfor some scalar function h(z, w) and vector w \u2208 Rm representing an output disturbance. It is assumed that the output disturbance vector w is deterministic, but empirically converges as\nlim n\u2192\u221e\nw PL(s) = W, (5)\nwhere s = 2k\u22122 is as in Assumption 1 (b) and W is some random variable. We let PY |Z denote the conditional distribution of the random variable Y = h(Z,W ). (d) The estimation function Gtx(r, \u03c4r, \u03bbx) and its derivative with respect to r, is Lipschitz continuous in r at (\u03c4r, \u03bbx) = (\u03c4 t r, \u03bb t x), where \u03c4 t r is a deterministic parame-\nter from the SE equations below. A similar assumptions holds for Gtz(p, \u03c4p, \u03bbz). (e) The adaptation functions Htx and H t z are set to (2) for\nsome deterministic sequence of parameters \u03bb t x and \u03bb t\nz . Also, in the estimation steps in lines 7, 8 and 15 of Algorithm 1, the values of the \u03c4 tp and \u03c4 t r are replaced with the deterministic parameters \u03c4 tp and \u03c4 t r from the SE\nequations defined below. Assumption 1(a) simply states that we are considering large, Gaussian i.i.d. matrices A. Assumptions (b) and (c) state that the input vector x and output disturbance w are modeled as deterministic, but whose empirical distributions asymptotically appear as i.i.d. This deterministic model is one of key features\nof Bayati and Montanari\u2019s analysis in [16]. Assumption (d) is a mild continuity condition. Assumption (e) defines the restriction of adaptive GAMP to the non-adaptive GAMP algorithm. We will remove this final assumption later.\nNote that, for now, there is no assumption that the \u201ctrue\u201d distribution of X or the true conditional distribution of Y given Z must belong to the class of distributions (1) for any parameters \u03bbx and \u03bbz . The analysis can thus model the effects of model mismatch.\nNow, given the above assumptions, define the sets of vectors\n\u03b8tx := {(xj , rtj , x\u0302t+1j ), j = 1, . . . , n}, (6a) \u03b8tz := {(zi, z\u0302ti , yi, pti), i = 1, . . . ,m}. (6b)\nThe first vector set, \u03b8tx, represents the components of the the \u201ctrue,\u201d but unknown, input vector x, its GAMP estimate x\u0302t as well as rt. The second vector, \u03b8tz , contains the components the \u201ctrue,\u201d but unknown, output vector z, its GAMP estimate z\u0302t, as well as pt and the observed output y. The sets \u03b8tx and \u03b8tz are implicitly functions of the dimension n.\nThe main result of [18] shows that if we fix the iteration t, and let n\u2192\u221e, the asymptotic joint empirical distribution of the components of these two sets \u03b8tx and \u03b8 t z converges to random vectors of the form\n\u03b8 t\nx := (X,R t, X\u0302t+1), \u03b8\nt z := (Z, Z\u0302 t, Y, P t). (7)\nWe precisely state the nature of convergence momentarily (see Theorem 1). In (7), X is the random variable in Assumption 1(b), while Rt and X\u0302t+1 are given by\nRt = \u03b1tX + V t, V t \u223c N (0, \u03betr), (8a)\nX\u0302t+1 = Gtx(R t, \u03c4 tr, \u03bb\nt x) (8b)\nfor some deterministic constants \u03b1tr, \u03be t r, and \u03c4 t r that are defined below. Similarly, (Z,P t) \u223c N (0,Ktp) for some covariance matrix Ktp, and\nY = h(Z,W ), Z\u0302t = Gtz(P t, Y, \u03c4 tp, \u03bb\nt z), (9)\nwhere W is the random variable in (5) and Ktp contains deterministic constants.\nThe deterministic constants \u03b1tr, \u03be t r, \u03c4 t r and K t p represent\nparameters of the distributions of \u03b8 t x and \u03b8 t\nz and depend on both the distributions of the input and outputs as well as the choice of the estimation and adaptation functions. The SE equations provide a simple method for recursively computing these parameters. The equations are best described algorithmically as shown in Algorithm 2. In order that we do not repeat ourselves, in Algorithm 2, we have written the SE equations for adaptive GAMP: For non-adaptive GAMP, the updates (19b) and (20a) can be ignored as the values of \u03bb t\nz\nand \u03bb t\nx are pre-computed. With these definitions, we can state the main result from\n[18].\nTheorem 1 ( [18]): Consider the random vectors \u03b8tx and \u03b8 t z generated by the outputs of GAMP under Assumption 1. Let \u03b8 t x and \u03b8 t z be the random vectors in (7) with the parameters\nAlgorithm 1 Adaptive GAMP\nRequire: Matrix A, estimation functions Gtx, Gts and Gtz and adaptation functions Htx and H t z .\n1: Set t\u2190 0, st\u22121 \u2190 0 and select some initial values for x\u03020 and \u03c40x . 2: repeat 3: {Output node update} 4: \u03c4 tp \u2190 \u2016A\u20162F \u03c4 tx/m 5: pt \u2190 Ax\u0302t \u2212 st\u22121\u03c4 tp 6: \u03bb\u0302tz \u2190 Htz(pt,y, \u03c4 tp) 7: z\u0302ti \u2190 Gtz(pti, yi, \u03c4 tp, \u03bb\u0302tz) for all i = 1, . . . ,m 8: sti \u2190 Gts(pti, yi, \u03c4 tp, \u03bb\u0302tz) for all i = 1, . . . ,m 9: \u03c4 ts \u2190 \u2212(1/m) \u2211 i \u2202G t s(p t i, yi, \u03c4 t p, \u03bb\u0302 t z)/\u2202p t i\n10: 11: {Input node update} 12: 1/\u03c4 tr \u2190 \u2016A\u20162F \u03c4 ts/n 13: rt = xt + \u03c4 trA Tst 14: \u03bb\u0302tx \u2190 Htx(rt, \u03c4 tr) 15: x\u0302t+1j \u2190 Gtx(rtj , \u03c4 tr , \u03bb\u0302tx) for all j = 1, . . . , n 16: \u03c4 t+1x \u2190 (\u03c4 tr/n) \u2211 j \u2202G t x(r t j , \u03c4 t r , \u03bb\u0302 t x)/\u2202rj 17: until Terminated\ndetermined by the SE equations in Algorithm 2. Then, for any fixed t, the components of \u03b8tx and \u03b8 t z converge empirically with bounded moments of order k as\nlim n\u2192\u221e\n\u03b8tx PL(k) = \u03b8 t\nx, lim n\u2192\u221e\n\u03b8tz PL(k) = \u03b8 t z. (10)\nwhere \u03b8 t x and \u03b8 t\nz are given in (7). In addition, for any t, the limits\nlim n\u2192\u221e\n\u03c4 tr = \u03c4 t r, lim n\u2192\u221e \u03c4 tp = \u03c4 t p, (11)\nalso hold almost surely. The theorem shows that the behavior of any component of the vectors x and z and their GAMP estimates x\u0302t and z\u0302t are distributed identically to a simple scalar equivalent system with random variables X , Z, X\u0302t and Z\u0302t. This scalar equivalent model appears in several analyses and can be thought of as a single-letter characterization [36] of the system. Remarkably, this limiting property holds for essentially arbitrary distributions and estimation functions, even ones that arise from problems that are highly nonlinear or noncovex. From the singleletter characterization, one can compute the asymptotic value of essentially any componentwise performance metric such as mean-squared error or detection accuracy. Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]\u2013[40]."}, {"heading": "III. ADAPTIVE GAMP", "text": "As described in the previous section, the standard GAMP algorithm of [18] considers the case when the parameters \u03bbx and \u03bbz in the distributions in (1) are known. The adaptive GAMP method proposed in this paper, and shown in Algorithm 1, is an extension of the standard GAMP procedure that enables simultaneous identification of the parameters \u03bbx and \u03bbz along with estimation of the vectors x and z. The\nkey modification is the introduction of the two adaptation functions: Htz(p t,y, \u03c4 tp) and H t x(r\nt, \u03c4 tr). In each iteration, these functions output estimates, \u03bb\u0302tz and \u03bb\u0302 t x of the parameters based on the data pt, y, rt, \u03c4 tp and \u03c4 t r . We saw the standard GAMP method corresponds to the adaptation functions in (2) which outputs fixed values \u03bb t z and \u03bb t\nx that do not depend on the data, and can be used when the true parameters are known. For the case when the true parameters are not known, we will see that a simple maximum likelihood (ML) can be used to estimate the parameters from the data."}, {"heading": "A. ML Parameter Estimation", "text": "To understand how to estimate parameters via the adaptation functions, observe that from Theorem 1, we know that the distribution of the components of rt are distributed identically to the scalar Rt in (8). Now, the distribution of Rt only depends on three parameters \u2013 \u03b1tr, \u03be t r and \u03bbx. It is thus natural to attempt to estimate these parameters from the empirical distribution of the components of rt.\nTo this end, let \u03c6x(r, \u03bbx, \u03b1r, \u03ber) be the log likelihood\n\u03c6x(r, \u03bbx, \u03b1r, \u03ber) = log pR(r|\u03bbx, \u03b1r, \u03ber), (12)\nwhere the right-hand side is the probability density of a random variable R with distribution\nR = \u03b1rX + V, X \u223c PX(\u00b7|\u03bbx), V \u223c N (0, \u03ber).\nThen, at any iteration t, we can attempt to perform a maximum-likelihood (ML) estimate\n\u03bb\u0302tx = H t x(r t, \u03c4 tr)\n= arg max \u03bbx\u2208\u039bx max (\u03b1r,\u03ber)\u2208Sx(\u03c4tr)  1n n\u2211 j=1 \u03c6x(r t j , \u03bbx, \u03b1r, \u03ber)  .(13) Here, the set Sx(\u03c4 tr) is a set of possible values for the parameters \u03b1r, \u03ber. The set may depend on the measured variance \u03c4 tr . We will see the precise role of this set below.\nSimilarly, the joint distribution of the components of pt and y are distributed according to the scalar (P t, Y ) which depend only on the parameters Kp and \u03bbz . Thus, we can define the likelihood\n\u03c6z(p, y, \u03bbz,Kp) = log pP,Y (p, y|\u03bbz,Kp), (14)\nwhere the right-hand side is the joint probability density of (P, Y ) with distribution\nY \u223c PY |Z(\u00b7|Z, \u03bbz), (Z,P ) \u223c N (0,Kp).\nThen, we can attempt to estimate \u03bbz via the ML estimate\n\u03bb\u0302tz = H t z(p t,y, \u03c4 tp)\n= arg max \u03bbz\u2208\u039bz max Kp\u2208Sz(\u03c4tp)\n{ 1\nm m\u2211 i=1 \u03c6z(p t i, yi,Kp)\n} . (15)\nAgain, the set Sz(\u03c4 tp) is a set of possible covariance matrices Kp."}, {"heading": "B. Relation to EM-GAMP", "text": "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et. al. in [24], [25]. Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows. First, the algorithms use the sum-product version of the AMP / GAMP algorithms, so that the outputs can provide an estimate of the posterior distributions on the components of x given the current parameter estimates. Specifically, at any iteration t, define the distribution\nP\u0302 tj (xj |rtj , \u03c4 tr , \u03bb\u0302t\u22121x )\n= 1\nZ exp\n[ \u2212 1\n2\u03c4 tr |xj \u2212 rtj |2\n] PX(xj |\u03bb\u0302t\u22121x ). (16)\nFor the sum-product AMP or GAMP algorithms, it is shown in [18] that the SE equations simplify so that \u03b1tr = 1 and \u03betr = \u03c4 t r, if the parameters were selected correctly. Therefore, from Theorem 1, the conditional distribution P (xj |rtj) should approximately match the distribution (16) for large n. If, in addition, we treat rtj and \u03c4 t r as sufficient statistics for estimating xj given y and A, then P\u0302 tj can be treated as an approximation for the posterior distribution of xj given the current parameter estimate \u03bb\u0302t\u22121x . Some justification for this last step can be found in [11], [12], [17]. Using the approximation, we can approximately implement the EM procedure to update the parameter estimate via a maximization\n\u03bb\u0302tx = H t x(r t, \u03c4 tr)\n:= arg max \u03bbx\u2208\u039bx\n1\nn n\u2211 j=1 E [ logPX(xj |\u03bbx)|P\u0302 tj ] , (17)\nwhere the expectation is with respect to the distribution in (16). In [22], [23], the parameter update (17) is performed only once every few iterations to allow P\u0302 t to converge to the approximation of the posterior distribution of xj given the current parameter estimates. In [24], [25], the parameter estimate is updated every iteration. A similar procedure can be performed for the estimation of \u03bbz .\nWe thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions Htx and H t z . As a result, our analysis in Theorem 2 below can be applied to these algorithms to provide rigorous asymptotic characterizations of the EM-GAMP performance. However, at the current time, we can only prove the asymptotic consistency result, Theorem 3, for the ML adaptation functions (13) and (15) described above.\nThat being said, it should be pointed out that EM-GAMP update (17) is generally computationally much simpler than the ML updates in (13) and (15). For example, when PX(x|\u03bbx) is an exponential family, the optimization in (17) is convex. Also, the optimizations in (13) and (15) require searches over additional parameters such as \u03b1r and \u03ber. Thus, an interesting avenue of future work is to apply the analysis result, Theorem 3 below, to see if the EM-GAMP method or some similarly computationally simple technique can be developed which also provides asymptotic consistency.\nAlgorithm 2 Adaptive GAMP State Evolution Given the distributions in Assumption 1, compute the sequence of parameters as follows: \u2022 Initialization Set t = 0 with\nK0x = cov(X, X\u0302 0), \u03c40x = \u03c4 0 x , (18)\nwhere the expectation is over the random variables (X, X\u03020) in Assumption 1(b) and \u03c40x is the initial value in the GAMP algorithm. \u2022 Output node update: Compute the variables associated with the output nodes Compute the variables\n\u03c4 tp = \u03b2\u03c4 t x, K t p = \u03b2K t x, (19a) \u03bb t\nz = H t z(P t, Y, \u03c4 tp), (19b) \u03c4 tr = \u2212E\u22121 [ \u2202\n\u2202p Gts(P t, Y, \u03c4 tp, \u03bb t z)\n] , (19c)\n\u03betr = (\u03c4 t r) 2E [ Gts(P t, Y, \u03c4 tp, \u03bb t z) ] , (19d)\n\u03b1tr = \u03c4 t rE [ \u2202\n\u2202z Gts(P t, h(Z,W ), \u03c4 tp, \u03bb t z)\n] , (19e)\nwhere the expectations are over the random variables (Z,P t) \u223c N (0,Ktp) and Y is given in (9).\n\u2022 Input node update: Compute\n\u03bb t\nx = H t x(R t, \u03c4 tr), (20a) \u03c4 t+1x = \u03c4 t rE [ \u2202\n\u2202r Gtx(R t, \u03c4 tr, \u03bb t x)\n] , (20b)\nKt+1x = cov(X, X\u0302 t+1), (20c)\nwhere the expectations are over the random variables in (8)."}, {"heading": "IV. CONVERGENCE AND ASYMPTOTIC CONSISTENCY WITH GAUSSIAN TRANSFORMS", "text": ""}, {"heading": "A. General State Evolution Analysis", "text": "Before proving the asymptotic consistency of the adaptive GAMP method with ML adaptation, we first prove the following more general convergence result.\nAssumption 2: Consider the adaptive GAMP algorithm running on a sequence of problems indexed by the dimension n, satisfying the following assumptions: (a) Same as Assumption 1(a) to (c) with k = 2. (b) For every t, the adaptation function Htx(r, \u03c4r) can be\nregarded as a functional over r satisfying the following weak pseudo-Lipschitz continuity property: Consider any sequence of vectors r = r(n) and sequence of scalars \u03c4r = \u03c4 (n) r , indexed by n satisfying\nlim n\u2192\u221e\nr(n) PL(k)\n= Rt, lim n\u2192\u221e \u03c4 (n)r = \u03c4 t r,\nwhere Rt and \u03c4 tr are the outputs of the state evolution equations defined below. Then,\nlim n\u2192\u221e\nHtx(r (n), \u03c4 (n)r ) = H t x(R t, \u03c4 tr).\nSimilarly, Htz(y,p, \u03c4p) satisfies analogous continuity conditions in \u03c4p and (y,p). See Appendix B for a\ngeneral definition of weakly pseudo-Lipschitz continuous functionals. (c) The scalar function Gtx(r, \u03c4r, \u03bbx) and its derivative G\u2032\nt x(r, \u03c4r, \u03bbx) with respect to r are continuous in \u03bbx uniformly over r in the following sense: For every > 0, t, \u03c4\u2217r and \u03bb \u2217 x \u2208 \u039bx, there exists an open neighborhood U of (\u03c4\u2217r , \u03bb \u2217 x) such that for all (\u03c4r, \u03bbx) \u2208 U and r,\n|Gtx(r, \u03c4r, \u03bbx)\u2212Gtx(r, \u03c4\u2217r , \u03bb\u2217x)| < , |G\u2032tx(r, \u03c4r, \u03bbx)\u2212G\u2032 t x(r, \u03c4 \u2217 r , \u03bb \u2217 x)| < .\nIn addition, the functions Gtx(r, \u03c4r, \u03bbx) and G \u2032t x(r, \u03c4r, \u03bbx) must be Lipschitz continuous in r with a Lipschitz constant that can be selected continuously in \u03c4r and \u03bbx. The functions Gts(p, y, \u03c4p, \u03bbz), G t z(p, y, \u03c4p, \u03bbz) and their derivatives G\u2032ts(p, y, \u03c4p, \u03bbz) G \u2032t z(p, y, \u03c4p, \u03bbz) satisfy analogous continuity assumptions with respect to p, y, \u03c4p and \u03bbz .\nAssumptions (b) and (c) are somewhat technical, but mild, continuity conditions that can be satisfied by a large class of adaptation functionals and estimation functions. For example, from the definitions in Appendix B, the continuity assumption (d) will be satisfied for any functional given by an empirical average\nHtx(r, \u03c4r) = 1\nn n\u2211 j=1 \u03c6tx(rj , \u03c4r),\nwhere, for each t, \u03c6tx(rj , \u03c4r) is pseudo-Lipschitz continuous in r of order p and continuous in \u03c4r uniformly over r. A similar functional can be used for Htz . As we will see in Section IV-B, the ML functionals (13) and (15) will also satisfy the conditions of this assumption.\nTheorem 2: Consider the random vectors \u03b8tx and \u03b8 t z generated by the outputs of the adaptive GAMP under Assumption 2. Let \u03b8 t x and \u03b8 t\nz be the random vectors in (7) with the parameters determined by the SE equations in Algorithm 2. Then, for any fixed t, the components of \u03b8tx and \u03b8 t z converge empirically with bounded moments of order k = 2 as\nlim n\u2192\u221e\n\u03b8tx PL(k) = \u03b8 t\nx, lim n\u2192\u221e\n\u03b8tz PL(k) = \u03b8 t z. (21)\nwhere \u03b8 t x and \u03b8 t\nz are given in (7). In addition, for any t, the limits\nlim n\u2192\u221e\n\u03bbtx = \u03bb t\nx, lim n\u2192\u221e\n\u03bbtz = \u03bb t z, (22a)\nlim n\u2192\u221e\n\u03c4 tr = \u03c4 t r, lim n\u2192\u221e \u03c4 tp = \u03c4 t p, (22b)\nalso hold almost surely. Proof: See Appendix C.\nThe result is a natural generalization of Theorem 1 and provides a simple extension of the SE analysis to incorporate the adaptation. The SE analysis applies to essentially arbitrary adaptation functions. It particular, it can be used to analyze both the behavior of the adaptive GAMP algorithm with either ML and EM-GAMP adaptation functions in the previous section.\nThe proof is straightforward and is based on a continuity argument also used in [41]."}, {"heading": "B. Asymptotic Consistency with ML Adaptation", "text": "We cam now use Theorem 2 to prove the asymptotic consistency of the adaptive GAMP method with the ML parameter estimation described in Section III-A. The following two assumptions can be regarded as identifiability conditions.\nDefinition 1: Consider a family of distributions, {PX(x|\u03bbx), \u03bbx \u2208 \u039bx}, a set Sx of parameters (\u03b1r, \u03ber) of a Gaussian channel and function \u03c6x(r, \u03bbx, \u03b1r, \u03ber). We say that PX(x|\u03bbx) is identifiable with Gaussian outputs with parameter set Sx and function \u03c6x if: (a) The sets Sx and \u039bx are compact. (b) For any \u201ctrue\u201d parameters \u03bb\u2217x \u2208 \u039bx, and (\u03b1\u2217r , \u03be\u2217r ) \u2208 Sx,\nthe maximization\n\u03bb\u0302x = arg max \u03bbx\u2208\u039bx max (\u03b1r,\u03ber)\u2208Sx\nE [\u03c6x(\u03b1\u2217rX + V, \u03bbx, \u03b1r, \u03ber)|\u03bb\u2217x, \u03be\u2217r ] , (23)\nis well-defined, unique and returns the true value, \u03bb\u0302x = \u03bb\u2217x. The expectation in (23) is with respect to X \u223c PX(\u00b7|\u03bb\u2217x) and V \u223c N (0, \u03be\u2217r ). (c) For every \u03bbx and \u03b1r, \u03ber, the function \u03c6x(r, \u03bbx, \u03b1r, \u03ber) is pseudo-Lipschitz continuous of order k = 2 in r. In addition, it is continuous in \u03bbx, \u03b1r, \u03ber uniformly over r in the following sense: For every > 0 and \u03bb\u0302x, \u03b1\u0302r, \u03be\u0302r, there exists an open neighborhood U of \u03bb\u0302x, \u03b1\u0302r, \u03be\u0302r, such that for all (\u03bbx, \u03b1r, \u03ber) \u2208 U and all r,\n|\u03c6x(r, \u03bbx, \u03b1r, \u03ber)\u2212 \u03c6x(r, \u03bb\u0302x, \u03b1\u0302r, \u03be\u0302r)| < .\nDefinition 2: Consider a family of conditional distributions, {PY |Z(y|z, \u03bbz), \u03bbz \u2208 \u039bz} generated by the mapping Y = h(Z,W, \u03bbz) where W \u223c PW is some random variable and h(z, w, \u03bbz) is a scalar function. Let Sz be a set of covariance matrices Kp and let \u03c6z(y, p, \u03bbz,Kp) be some function. We say that conditional distribution family PY |Z(\u00b7|\u00b7, \u03bbz) is identifiable with Gaussian inputs with covariance set Sz and function \u03c6z if: (a) The parameter sets Sz and \u039bz are compact. (b) For any \u201ctrue\u201d parameter \u03bb\u2217z \u2208 \u039bz and true covariance\nK\u2217p, the maximization\n\u03bb\u0302z = arg max \u03bbz\u2208\u039bz max Kp\u2208Sz E [ \u03c6z(Y, P, \u03bbz,Kp)|\u03bb\u2217z,K\u2217p ] , (24)\nis well-defined, unique and returns the true value, \u03bb\u0302z = \u03bb\u2217z , The expectation in (24) is with respect to Y |Z \u223c PY |Z(y|z, \u03bb\u2217z) and (Z,P ) \u223c N (0,K\u2217p). (c) For every \u03bbz and Kp, the function \u03c6z(y, p, \u03bbz,Kp) is pseudo-Lipschitz continuous in (p, y) of order k = 2. In addition, it is continuous in \u03bbp,Kp uniformly over p and y.\nDefinitions 1 and 2 essentially require that the parameters \u03bbx and \u03bbz can be identified through a maximization. The functions \u03c6x and \u03c6z can be the log likelihood functions (12) and (14), although we permit other functions as well, since the maximization may be computationally simpler. Such functions are sometimes called pseudo-likelihoods. The existence of a such a function is a mild condition. Indeed, if such a function\ndoes not exists, then the distributions on R or (Y, P ) must be the same for at least two different parameter values. In that case, one cannot hope to identify the correct value from observations of the vectors rt or (y,pt).\nAssumption 3: Let PX(x|\u03bbx) and PY |Z(y|z, \u03bbz) be families of distributions and consider the adaptive GAMP algorithm, Algorithm 1, run on a sequence of problems, indexed by the dimension n satisfying the following assumptions: (a) Same as Assumption 1(a) to (c) with k = 2. In addition,\nthe distributions for the vector X is given by PX(\u00b7|\u03bb\u2217x) for some \u201ctrue\u201d parameter \u03bb\u2217x \u2208 \u039bx and the conditional distribution of Y given Z is given by PY |Z(y|z, \u03bb\u2217z) for some \u201ctrue\u201d parameter \u03bb\u2217z \u2208 \u039bz .\n(b) Same as Assumption 2(c). (c) The adaptation functions are set to (13) and (15).\nTheorem 3: Consider the outputs of the adaptive GAMP algorithm with ML adaptation as described in Assumption 3. Then, for any fixed t, (a) The components of \u03b8tx and \u03b8 t z in (6) converge empirically\nwith bounded moments of order k = 2 as in (21) and the limits (22) hold almost surely. (b) In addition, if (\u03b1tr, \u03be t r) \u2208 Sx(\u03c4 tr), and the family of dis-\ntributions PX(\u00b7|\u03bbx), \u03bbx \u2208 \u039bx is identifiable in Gaussian noise with parameter set Sx(\u03c4 tr) and pseudo-likelihood \u03c6x (see Definition 1), then\nlim n\u2192\u221e\n\u03bb\u0302tx = \u03bb t x = \u03bb \u2217 x (25)\nalmost surely. (c) Similarly, if Ktp \u2208 Sz(\u03c4 tp) for some t, and the family\nof distributions PY |Z(\u00b7|\u03bbz), \u03bbz \u2208 \u039bz is identifiable with Gaussian inputs with parameter set Sz(\u03c4 tp) and pseudolikelihood \u03c6z (see Definition 2) then\nlim n\u2192\u221e\n\u03bb\u0302tz = \u03bb t z = \u03bb \u2217 z (26)\nalmost surely. Proof: See Appendix D.\nThe theorem shows, remarkably, that for a very large class of the parameterized distributions, the adaptive GAMP algorithm is able to asymptotically estimate the correct parameters. Moreover, there is asymptotically no performance loss between the adaptive GAMP algorithm and a corresponding oracle GAMP algorithm that knows the correct parameters in the sense that the empirical distributions of the algorithm outputs are described by the same SE equations.\nThere are two key requirements: First, that the optimizations in (13) and (15) can be computed. These optimizations may be non-convex. Secondly, that the optimizations can be performed are over sufficiently large sets of Gaussian channel parameters Sx and Sz such that it can be guaranteed that the SE equations eventually enter these sets. In the examples below, we will see ways to reduce the search space of Gaussian channel parameters."}, {"heading": "V. NUMERICAL RESULTS", "text": ""}, {"heading": "A. Estimation of a Gauss-Bernoulli input", "text": "Recent results suggest that there is considerable value in learning of priors PX in the context of compressed sensing\n[42], which considers the estimation of sparse vectors x from underdetermined measurements (m < n) . It is known that estimators such as LASSO offer certain optimal minmax performance over a large class of sparse distributions [43]. However, for many particular distributions, there is a potentially large performance gap between LASSO and MMSE estimator with the correct prior. This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.\nHere, we illustrate the performance and asymptotic consistency of adaptive GAMP in a simple compressed sensing example. Specifically, we consider the estimation of a sparse vector x \u2208 Rn from m noisy measurements\ny = Ax + w = z + w,\nwhere the additive noise w is random with i.i.d. entries wi \u223c N (0, \u03c32). Here, the \u201coutput\u201d channel is determined by the statistics on w, which are assumed to be known to the estimator. So, there are no unknown parameters \u03bbz .\nAs a model for the sparse input vector x, we assumed the components are i.i.d. with the Gauss-Bernoulli distribution,\nxj \u223c {\n0 prob = 1\u2212 \u03c1, N (0, \u03c32x) prob = \u03c1\n(27)\nwhere \u03c1 represents the probability that the component is nonzero (i.e. the vector\u2019s sparsity ratio) and \u03c32x is the variance of the non-zero components. The parameters \u03bbx = (\u03c1, \u03c32x) are treated as unknown.\nIn the adaptive GAMP algorithm, we use the estimation functions Gx, Gs, and Gz corresponding to the sum-product GAMP algorithm. As described in Appendix A, for the sumproduce GAMP the SE equations simplify so that \u03b1tr = 1 and \u03betr = \u03c4 t r. Since the noise variance is known, the initial output noise variance \u03c40r obtained by adaptive GAMP in Algorithm 1 exactly matches that of oracle GAMP. Therefore, for t = 0, the parameters \u03b1tr and \u03be t r do not need to be estimated, and (13)\nconveniently simplifies to\nHx(r, \u03c4r) = arg max \u03bbx\u2208\u039bx  1n n\u2211 j=1 log pR(rj |\u03bbx, \u03c4r)  , (28) where \u039bx = [0, 1]\u00d7 [0,+\u221e). For iteration t > 0, we rely on asymptotic consistency, and assume that the maximization (28) yields the correct parameter estimates, so that \u03bb\u0302tx = \u03bbx. Then, in principle, for t > 0 adaptive GAMP uses the correct parameter estimates and we expect it to match the performance of oracle GAMP. In our implementation, we run EM update (17) until convergence to approximate the ML adaptation (28).\nFig. 2 illustrates the performance of adaptive GAMP on signals of length n = 400 generated with the parameters \u03bbx = (\u03c1 = 0.2, \u03c3 2 x = 5). The performance of adaptive GAMP is compared to that of LASSO with MSE optimal regularization parameter, and oracle GAMP that knows the parameters of the prior exactly. For generating the graphs, we performed 1000 random trials by forming the measurement matrix A from i.i.d. zero-mean Gaussian random variables of variance 1/m. In Figure 2(a), we keep the variance of the noise fixed to \u03c32 = 0.1 and plot the average MSE of the reconstruction against the measurement ratio m/n. In Figure 2(b), we keep the measurement ratio fixed to m/n = 0.75 and plot the average MSE of the reconstruction against the noise variance \u03c32. For completeness, we also provide the asymptotic MSE values computed via SE recursion. The results illustrate that GAMP significantly outperforms LASSO over the whole range of m/n and \u03c32. Moreover, the results corroborate the consistency of adaptive GAMP which achieves nearly identical quality of reconstruction with oracle GAMP. The performance results indicate that adaptive GAMP can be an effective method for estimation when the parameters of the problem are difficult to characterize and must be estimated from data.\nKAMILOV, RANGAN, FLETCHER AND UNSER 9 (a) (b)\nNoise variance ( )Measurement ratio ( )\nM S E (d B )\nM S E (d B )\n0.5 1 1.5 2 14\n13 12 11 10 9 8 7\nMeasurement ratio (m/n)\nM SE (d B)\nState Evolution LASSO Oracle GAMP Adaptive GAMP\n10 3 10 2 10 1 35\n30 25 20 15 10\ni V ri nce ( 2\nM SE (d B)"}, {"heading": "B. Estimation of a Nonlinear Output Classification Function", "text": "As a second example, we consider the estimation of the linear-nonlinear-Poisson (LNP) cascade model [8]. The model has been successfully used to characterize neural spike responses in early sensory pathways of the visual system. In the context of LNP cascade model, the vector x \u2208 Rn represents the linear filter, which models the linear receptive field of the neuron. AMP techniques combined with the parameter estimation have been recently proposed for neural receptive field estimation and connectivity detection in [44].\nAs in Section V-A, we model x as a Gauss-Bernoulli vector of unknown parameters \u03bbx = (\u03c1, \u03c32x). To obtain the measurements y, the vector z = Ax is passed through a componentwise nonlinearity u to result in\nu(z) = 1\n1 + e\u2212z . (29)\nLet the function \u03c8 \u2208 Rr denote a vector \u03c8(z) = ( 1, u(z), . . . , u(z)r\u22121 )T . (30)\nThen, the final measurement vector y is generated by a measurement channel with a conditional density of the form\npY |Z(yi|zi, \u03bbz) = f(zi)\nyi\ny! e\u2212f(zi), (31)\nwhere f denotes the nonlinearity given by f(z) = exp ( \u03bbTz \u03c8(z) ) .\nAdaptive GAMP can now be used to also estimate vector of polynomial coefficients \u03bbz , which together with x, completely characterizes the LNP system.\nThe estimation of \u03bbz is performed with ML estimator described in Section III-A. We assume that the mean and variance of the vector x are known at iteration t = 0. This implies that for sum-product GAMP the covariance K0p is initially known and the optimization (15) simplifies to\nHz(p,y, \u03c4p) = arg max \u03bbz\u2208\u039bz\n{ 1\nm m\u2211 i=1 log pY (yi|\u03bbz)\n} , (32)\nwhere \u039bz \u2282 Rr. The estimation of \u03bbx is performed as in Section V-A. As before, for iteration t > 0, we assume that the maximizations (28) and (32) yield correct parameter estimates \u03bb\u0302tx = \u03bbx and \u03bb\u0302 t z = \u03bbz , respectively. Thus we can conclude by induction that for t > 0 the adaptive GAMP algorithm should continue matching oracle GAMP for large enough n. In our simulations, we implemented (32) with a gradient ascend algorithm and run it until convergence.\nIn Fig. 3, we compare the reconstruction performance of adaptive GAMP against the oracle version that knows the true parameters (\u03bbx, \u03bbz) exactly. We consider the vector x generated with true parameters \u03bbx = (\u03c1 = 0.1, \u03c32x = 30). We consider the case r = 3 and set the parameters of the output channel to \u03bbz = [\u22124.88, 7.41, 2.58]. To illustrate the asymptotic consistency of the adaptive algorithm, we consider the signals of length n = 1000 and n = 10000. We perform 10 and 100 random trials for long and short signals, respectively, and plot the average MSE of the reconstruction against m/n. As expected, for large n, the performance of adaptive GAMP is nearly identical (within 0.15) to that of oracle GAMP."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "We have presented an adaptive GAMP method for the estimation of i.i.d. vectors x observed through a known linear transforms followed by an arbitrary, componentwise random transform. The procedure, which is a generalization of EMGAMP methodology of [22]\u2013[25] that estimates both the vector x as well as parameters in the source and componentwise output transform. In the case of large i.i.d. Gaussian transforms, it is shown that the adaptive GAMP method is provably asymptotically consistent in that the parameter estimates converge to the true values. This convergence result holds over a large class of models with essentially arbitrarily complex parameterizations. Moreover, the algorithm is computationally efficient since it reduces the vector-valued estimation problem to a sequence of scalar estimation problems in Gaussian noise. We believe that this method is applicable to a large class of linear-nonlinear models with provable guarantees can have applications in a wide range of problems. We have mentioned the use of the method for learning sparse priors in compressed sensing. Future work will include learning of parameters of output functions as well as possible extensions to non-Gaussian matrices."}, {"heading": "APPENDIX A SUM-PRODUCT GAMP EQUATIONS", "text": "As described in [18], the sum-product estimation can be implemented with the estimation functions\nGtx(r, \u03c4r, \u03bb\u0302x) := E[X|R = r, \u03c4r, \u03bb\u0302x], (33a) Gtz(p, y, \u03c4p, \u03bb\u0302z) := E[Z|P = p, Y = y, \u03c4p, \u03bb\u0302z], (33b) Gts(p, y, \u03c4p, \u03bb\u0302z) := 1\n\u03c4p\n( Gtz(p, y, \u03c4p, \u03bb\u0302z)\u2212 p ) , (33c)\nwhere the expectations are with respect to the scalar random variables\nR = X + Vx, Vx \u223c N (0, \u03c4r), X \u223c PX(\u00b7|\u03bb\u0302x), (34a) Z = P + Vz, Vz \u223c N (0, \u03c4p), Y \u223c PY |Z(\u00b7|Z, \u03bb\u0302z). (34b)\nThe paper [18] shows that the derivatives of these estimation functions for lines 9 and 16 are computed via the variances:\n\u03c4 r \u2202Gtx(r, \u03c4r, \u03bb\u0302x)\n\u2202r = var[X|R = r, \u03c4r, \u03bb\u0302x] (35a)\n\u2212\u2202G t s(p, y, \u03c4p, \u03bb\u0302z)\n\u2202p\n= 1\n\u03c4p\n( 1\u2212 var[Z|P = p, Y = y, \u03c4p, \u03bb\u0302z]\n\u03c4p\n) . (35b)\nThe estimation functions (33) correspond to scalar estimates of random variables in additive white Gaussian noise (AWGN). A key result of [18] is that, when the parameters are set to the true values (i.e. (\u03bb\u0302x, \u03bb\u0302z) = (\u03bbx, \u03bbz)), the outputs x\u0302t and z\u0302t can be interpreted as sum products estimates of the conditional expectations E(x|y) and E(z|y). The algorithm thus reduces the vector-valued estimation problem to a computationally simple sequence of scalar AWGN estimation problems along with linear transforms.\nMoreover, the SE equations in Algorithm 2 reduce to a particularly simple forms, where \u03c4 tr and \u03be t r in (19) are given by\n\u03c4 tr = \u03be t r = E\u22121\n[ \u22022\n\u2202p2 log pY |P (Y |P t)\n] , (36a)\nwhere the expectations are over the random variables (Z,P t) \u223c N (0,Ktp) and Y is given in (9). The covariance matrix Ktp has the form\nKtp =\n[ \u03b2\u03c4x0 \u03b2\u03c4x0 \u2212 \u03c4 tp\n\u03b2\u03c4x0 \u2212 \u03c4 tp \u03b2\u03c4x0 \u2212 \u03c4 tp\n] , (36b)\nwhere \u03c4x0 is the variance of X and \u03b2 > 0 is the asymptotic measurement ratio (see Assumption 1 for details). The scaling constant (19e) becomes \u03b1tr = 1. The update rule for \u03c4 t+1 x also simplifies to \u03c4 t+1x = E [ var ( X|Rt )] , (36c)\nwhere the expectation is over the random variables in (8)."}, {"heading": "APPENDIX B CONVERGENCE OF EMPIRICAL DISTRIBUTIONS", "text": "Bayati and Montanari\u2019s analysis in [16] employs certain deterministic models on the vectors and then proves convergence properties of related empirical distributions. To apply the same analysis here, we need to review some of their definitions. We say a function \u03c6 : Rr \u2192 Rs is pseudo-Lipschitz of order k > 1, if there exists an L > 0 such for any x, y \u2208 Rr,\n\u2016\u03c6(x)\u2212 \u03c6(y)\u2016 \u2264 L(1 + \u2016x\u2016k\u22121 + \u2016y\u2016k\u22121)\u2016x\u2212 y\u2016.\nNow suppose that for each n = 1, 2, . . ., v(n) is a set of vectors\nv(n) = {vi(n), i = 1, . . . , `(n)}, (37)\nwhere each element vi(n) \u2208 Rs and `(n) is the number of elements in the set. Thus, v(n) can itself be regarded as a vector with s`(n) components. We say that v(n) empirically converges with bounded moments of order k as n \u2192 \u221e to a\nrandom vector V on Rs if: For all pseudo-Lipschitz continuous functions, \u03c6, of order k,\nlim n\u2192\u221e\n1\nn n\u2211 i=1 \u03c6(vi(n)) = E(\u03c6(V)) <\u221e.\nWhen the nature of convergence is clear, we may write (with some abuse of notation)\nv(n) PL(k)\u2192 V as n\u2192\u221e,\nor lim n\u2192\u221e v(n) PL(k) = V.\nFinally, let Psk be the set of probability distributions on Rs with bounded kth moments, and suppose that H : Psk \u2192 \u039b is a functional Psk to some topological space \u039b. Given a set v(n) as in (37), write H(v) for H(Pv) where Pv is the empirical distribution on the components of v. Also, given a random vector V with distribution PV write H(V) for H(PV). Then, we will say that the functional H is weakly pseudo-Lipschitz continuous of order k if\nlim n\u2192\u221e\nv(n) PL(k)\n= V =\u21d2 lim n\u2192\u221e H(v(n)) = H(V),\nwhere the limit on the right hand side is in the topology of \u039b."}, {"heading": "APPENDIX C PROOF OF THEOREM 2", "text": "The proof follows along the adaptation argument of [41]. We use the tilde superscript on quantities such as x\u0303t, r\u0303t, \u03c4\u0303 tr , p\u0303 t, \u03c4 tp, s\u0303 t, and z\u0303t to denote values generated via a non-adaptive version of the GAMP. The non-adaptive GAMP algorithm has the same initial conditions as the adaptive algorithm (i.e. x\u03030 = x\u03020, \u03c4\u03030p = \u03c4 0 p , s\u0303 \u22121 = s\u22121 = 0), but with \u03bb\u0302tx and \u03bb\u0302 t z replaced by their deterministic limits \u03bb\u0304 t x and \u03bb\u0304 t z , respectively. That is, we replace lines 7, 8 and 15 with\nz\u0303ti = G t z(p t i, yi, \u03c4 t p, \u03bb\nt z), s\u0303 t i = G t s(p t i, yi, \u03c4 t p, \u03bb t z),\nx\u0303t+1j = G t x(r t j , \u03c4 t r , \u03bb\nt x).\nThis non-adaptive algorithm is precisely the standard GAMP method analyzed in [18]. The results in that paper show that the outputs of the non-adaptive algorithm satisfy all the required limits from the SE analysis. That is,\nlim n\u2192\u221e\n\u03b8\u0303tx PL(k) = \u03b8 t\nx, lim n\u2192\u221e\n\u03b8\u0303tz PL(k) = \u03b8 t z,\nwhere \u03b8\u0303tx and \u03b8\u0303 t z are the sets generated by the non-adaptive GAMP algorithm:\n\u03b8\u0303tx := { (xj , r\u0303 t j , x\u0303 t+1 j ) : j = 1, . . . , n } ,\n\u03b8\u0303tz = { (zi, z\u0303 t i , yi, p\u0303 t i) : i = 1, . . . ,m } .\nThe limits (21) are now proven through a continuity argument that shows that the adaptive and non-adaptive quantities must asymptotically agree with one another. Specifically, we will start by proving that the following limits holds almost surely for all t \u2265 0\nlim n\u2192\u221e \u2206tx = lim n\u2192\u221e\n1 n \u2016x\u0302t \u2212 x\u0303t\u2016kk = 0, , (38a)\nlim n\u2192\u221e \u2206t\u03c4p = limn\u2192\u221e |\u03c4 tp \u2212 \u03c4\u0303 tp| = 0 (38b)\nwhere \u2016 \u00b7 \u2016k is usual the k-norm. Moreover, in the course of proving (38), we will also show that the following limits hold almost surely\nlim m\u2192\u221e \u2206tp = lim m\u2192\u221e\n1 m \u2016pt \u2212 p\u0303t\u2016kk = 0, (39a)\nlim n\u2192\u221e \u2206tr = lim n\u2192\u221e\n1 n \u2016rt \u2212 r\u0303t\u2016kk = 0, (39b)\nlim m\u2192\u221e \u2206ts = lim m\u2192\u221e\n1 m \u2016st \u2212 s\u0303t\u2016kk = 0, (39c)\nlim m\u2192\u221e \u2206tz = lim m\u2192\u221e\n1 m \u2016z\u0302t \u2212 z\u0303t\u2016kk = 0, (39d)\nlim n\u2192\u221e \u2206t\u03c4r = limn\u2192\u221e |\u03c4 tr \u2212 \u03c4\u0303 tr | = 0, (39e)\nlim n\u2192\u221e\n\u03bb\u0302tx = \u03bb\u0304 t x, (39f)\nlim n\u2192\u221e\n\u03bb\u0302tz = \u03bb\u0304 t z, (39g)\nThe proof of the limits (38) and (39) is achieved by an induction on t. Although we only need to show the above limits for k = 2, most of the arguments hold for arbitrary k \u2265 2. We thus present the general derivation where possible.\nTo begin the induction argument, first note that the nonadaptive algorithm has the same initial conditions as the adaptive algorithm. Thus the limits (38) and (39c) hold for t = 0 and t = \u22121, respectively.\nWe now proceed by induction. Suppose that t \u2265 0 and the limits (38) and (39c) hold for some t and t\u2212 1, respectively. Since A has i.i.d. components with zero mean and variance 1/m, it follows from the Marc\u030cenko-Pastur Theorem [45] that that its 2-norm operator norm is bounded. That is, there exists a constant CA such that\nlim n\u2192\u221e \u2016A\u2016k \u2264 CA, lim n\u2192\u221e \u2016AT\u2016k \u2264 CA. (40)\nThis bound is the only part of the proof that specifically requires k = 2. From (40), we obtain\n\u2016pt \u2212 p\u0303t\u2016k = \u2016Ax\u0302t \u2212 \u03c4 tpst\u22121 \u2212Ax\u0303t + \u03c4\u0303 tps\u0303t\u22121\u2016k = \u2016A(x\u0302t \u2212 x\u0303t) + \u03c4 tp(s\u0303t\u22121 \u2212 st\u22121) + (\u03c4\u0303 tp \u2212 \u03c4 tp)s\u0303t\u22121\u2016k \u2264 \u2016A(x\u0302t \u2212 x\u0303t)\u2016k + |\u03c4 tp|\u2016s\u0303t\u22121 \u2212 st\u22121\u2016k + |\u03c4\u0303 tp \u2212 \u03c4 tp|\u2016s\u0303t\u22121\u2016k\n(a) \u2264\u2016A\u2016k\u2016x\u0302t \u2212 x\u0303t\u2016k + |\u03c4 tp|\u2016s\u0303t\u22121 \u2212 st\u22121\u2016k + |\u03c4\u0303 tp \u2212 \u03c4 tp|\u2016s\u0303t\u22121\u2016k \u2264 CA\u2016x\u0302t \u2212 x\u0303t\u2016k + |\u03c4 tp|\u2016st\u22121 \u2212 s\u0303t\u22121\u2016k + |\u03c4 tp \u2212 \u03c4\u0303 tp|\u2016s\u0303t\u22121\u2016k\n(41)\nwhere (a) is due to the norm inequality \u2016Ax\u2016k \u2264 \u2016A\u2016k\u2016x\u2016k. Since k \u2265 1, we have that for any positive numbers a and b\n(a+ b)k \u2264 2k(ak + bk). (42)\nApplying the inequality (42) into (41), we obtain\n1 m \u2016pt \u2212 p\u0303t\u2016kk \u2264 1 m ( CA\u2016x\u0302t \u2212 x\u0303t\u2016k + |\u03c4 tp|\u2016st\u22121 \u2212 s\u0303t\u22121\u2016k + \u2206t\u03c4p\u2016s\u0303 t\u22121\u2016k )k \u2264 2kCA n\nm \u2206tx + 2 k|\u03c4 tp|k\u2206t\u22121s + 2k(\u2206t\u03c4p) k\n( 1\nm \u2016s\u0303t\u22121\u2016kk\n) .\n(43)\nNow, since s\u0303t and \u03c4\u0303 tp are the outputs of the non-adaptive algorithm they satisfy the limits\nlim n\u2192\u221e\n1 m \u2016s\u0303t\u2016kk = lim n\u2192\u221e 1 m m\u2211 i=1 |s\u0303ti|k = E [ |St|k ] <\u221e, (44a)\nlim n\u2192\u221e\n\u03c4\u0303 tp = \u03c4 t p <\u221e. (44b)\nNow, the induction hypotheses state that \u2206tx, \u2206 t\u22121 s and \u2206 t \u03c4p \u2192 0. Applying these induction hypotheses along the bounds (44a), and the fact that n/m\u2192 \u03b2 we obtain (39a).\nTo prove (39g), we first prove the empirical convergence of (pt,y) to (P t, Y ). Towards this end, let \u03c6(p, y) be any pseudo-Lipschitz continuous function \u03c6 of order k. Then\u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 \u03c6(pti, yi)\u2212 E [ \u03c6(P t, Y )\n]\u2223\u2223\u2223\u2223\u2223 \u2264 1 m m\u2211 i=1\n\u2223\u2223\u03c6(pti, yi)\u2212 \u03c6(p\u0303ti, yi)\u2223\u2223 +\n\u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 \u03c6(p\u0303ti, yi)\u2212 E [ \u03c6(P t, Y ) ]\u2223\u2223\u2223\u2223\u2223 (a) \u2264 L m m\u2211 i=1 ( 1 + |pti|k\u22121 + |p\u0303ti|k\u22121 + |yi|k\u22121 ) |pti \u2212 p\u0303ti|\n+ \u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 \u03c6(p\u0303ti, yi)\u2212 E [ \u03c6(P t, Y ) ]\u2223\u2223\u2223\u2223\u2223 (b) \u2264LC\u2206tp + \u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 \u03c6(p\u0303ti, yi)\u2212 E [ \u03c6(P t, Y )\n]\u2223\u2223\u2223\u2223\u2223 . (45) In (a) we use the fact that \u03c6 is pseudo-Lipschitz, and in (b) we use Ho\u0308lder\u2019s inequality |x\u0302Ty| = \u2016x\u2016k\u2016y\u2016q with q = p/(p\u22121) and define C as\nC :=\n[ 1\nm m\u2211 i=1 ( 1 + |pti|k\u22121 + |p\u0303ti|k\u22121 + |yi|k\u22121\n)]k/(k\u22121)\n\u2264 1 m m\u2211 i=1 ( 1 + |pti|k\u22121 + |p\u0303ti|k\u22121 + |yi|k\u22121 )k/(k\u22121) \u2264 const\u00d7 [ 1 + ( 1\nm\n\u2225\u2225pt\u2225\u2225k k\n) k\u22121 k\n+\n( 1\nm\n\u2225\u2225p\u0303t\u2225\u2225k k\n) k\u22121 k\n+\n( 1\nm \u2016y\u2016kk\n) k\u22121 k ] , (46)\nwhere the first step is from Jensen\u2019s inequality. Since (p\u0303t,y) satisfy the limits for the non-adaptive algorithm we have:\nlim n\u2192\u221e\n1 m \u2016p\u0303t\u2016kk = lim n\u2192\u221e 1 m m\u2211 i=1 |p\u0303ti|k = E [ |P t|k ] <\u221e (47a)\nlim n\u2192\u221e\n1 m \u2016y\u2016kk = lim n\u2192\u221e 1 m m\u2211 i=1 |yi|k = E [ |Y |k ] <\u221e (47b)\nAlso, from the induction hypothesis (39a), it follows that the adaptive output must satisfy the same limit\nlim n\u2192\u221e\n1 m \u2016pt\u2016kk = lim n\u2192\u221e 1 m m\u2211 i=1 |pti|k = E [ |P t|k ] <\u221e. (48)\nCombining (45), (46), (47), (48), (39a) we conclude that for all t \u2265 0\nlim n\u2192\u221e\n(pt,y) PL(k) = (P t, Y ). (49)\nThe limit (49) along with (38b) and the continuity condition on Htz in Assumption 1(d) prove the limit in (39g).\nThe limit (39a) together with continuity conditions on Gtz in Assumptions 1 show that (39c), (39d) and (39e) hold for t. For example, to show (39d), we consider the limit m \u2192 \u221e of the following expression\n1 m \u2016z\u0302t \u2212 z\u0303t\u2016kk = 1 m \u2016Gtz(pt,y, \u03c4 tp, \u03bb\u0302tz)\u2212Gtz(p\u0303t,y, \u03c4 tp, \u03bb\u0304tz)\u2016kk\n(a) \u2264 L m \u2016pt \u2212 p\u0303t\u2016kk = L\u2206tp,\nwhere at (a) we used the Lipschitz continuity assumption. Similar arguments can be used for (39c) and (39e).\nTo show (39b), we proceed exactly as for (39a). Due to the continuity assumptions on Hx, this limit in turn shows that (39f) holds almost surely. Then, (38a) and (38b) follow directly from the continuity of Gx in Assumptions 1, together with (39b) and (39f). We have thus shown that if the limits (38) and (39) hold for some t, they hold for t+1. Thus, by induction they hold for all t.\nFinally, to show (21), let \u03c6 be any pseudo-Lipschitz continuous function \u03c6(x, r, x\u0302), and define\nt = \u2223\u2223\u2223\u2223\u2223\u2223 1n m\u2211 j=1 \u03c6(xj , r\u0303 t j , x\u0303 t+1 j )\u2212 E [ \u03c6(X,Rt, X\u0302t+1) ]\u2223\u2223\u2223\u2223\u2223\u2223 , (50) which, due to convergence of non-adaptive GAMP, can be made arbitrarily small by choosing n large enough. Then, consider\u2223\u2223\u2223\u2223\u2223\u2223 1n m\u2211 j=1 \u03c6(xj , r\u0302 t j , x\u0302 t+1 j )\u2212 E [ \u03c6(X,Rt, X\u0302t+1)\n]\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 tn + 1\nn n\u2211 j=1 \u2223\u2223\u03c6(xj , r\u0302tj , x\u0302t+1j )\u2212 \u03c6(xj , r\u0303tj , x\u0303t+1j )\u2223\u2223 (a) \u2264 tn + L\u2016rt \u2212 r\u0303t\u20161 + L\u2016x\u0302t+1 \u2212 x\u0303t+1\u20161\n+ L\u2032\nn n\u2211 j=1 ( |r\u0302tj |k\u22121 + |r\u0303tj |k\u22121 ) (|r\u0302tj \u2212 r\u0303tj |+ |x\u0302t+1j \u2212 x\u0303 t+1 j |)\n+ L\u2032\nn n\u2211 j=1 ( |x\u0302t+1j | k\u22121 + |x\u0303t+1j | k\u22121) (|r\u0302tj \u2212 r\u0303tj |+ |x\u0302t+1j \u2212 x\u0303t+1j |)\n(b) \u2264 tn + L ( \u2206tr ) 1 k + L ( \u2206tx ) 1 k\n+ L\u2032 ( \u2206tr ) 1 k ( (M\u0303 t+1x ) k\u22121 k + (M\u0302 t+1x ) k\u22121 k + (M\u0303 tr) k\u22121 k + (M\u0302 tr) k\u22121 k ) + L\u2032 ( \u2206tx ) 1 k ( (M\u0303 t+1x ) k\u22121 k + (M\u0302 t+1x ) k\u22121 k + (M\u0303 tr) k\u22121 k + (M\u0302 tr) k\u22121 k\n) (51)\nwhere L, L\u2032 are constants independent of n and\nM\u0302 t+1x = 1\nn\n\u2225\u2225x\u0302t+1\u2225\u2225k k , M\u0302 tr = 1\nn\n\u2225\u2225rt\u2225\u2225k k ,\nM\u0303 t+1x = 1\nn\n\u2225\u2225x\u0303t+1\u2225\u2225k k , M\u0303 tr = 1\nn\n\u2225\u2225r\u0303t\u2225\u2225k k\nIn (a) we use the fact that \u03c6 is pseudo-Lipshitz, in (b) we use `p-norm equivalence \u2016x\u20161 \u2264 n1\u22121/p\u2016x\u2016k and Ho\u0308lder\u2019s inequality |x\u0302Ty| = \u2016x\u2016k\u2016y\u2016q with q = p/(p\u22121). By applying of (38a), (39b) and since, M\u0302 t+1x , M\u0303 t+1 x , M\u0302 t r , and M\u0303 t r converge to a finite value we can obtain the first equation of (21) by taking n\u2192\u221e. The second equation in (21) can be shown in a similar way. This proves the limits (21).\nAlso, the first two limits in (22) are a consequence of (39f) and (39f). The second two limits follow from continuity assumptions in Assumption 1(e) and the convergence of the empirical distributions in (21). This completes the proof."}, {"heading": "APPENDIX D PROOF OF THEOREM 3", "text": "Part (a) of Theorem 3 is a direct application of the general result, Theorem IV-A. To apply the general result, first observe that Assumptions 3(a) and (c) immediately imply the corresponding items in Assumptions 2. So, we only need to verify the continuity condition in Assumption 2(b) for the adaptation functions in (13) and (15).\nWe begin by proving the continuity of Htz . Fix t, and let (y(n),p(n)) be a sequence of vectors and \u03c4 (n)p be a sequence of scalars such that\nlim n\u2192\u221e\n(y(n),p(n)) PL(p)\n= (Y, P t) lim n\u2192\u221e \u03c4 (n)p = \u03c4 t p, (52)\nwhere (Y, P t) and \u03c4 tp are the outputs of the state evolution equations. For each n, let\n\u03bb\u0302(n)z = H t z(y (n),p(n), \u03c4 (n)p ). (53)\nWe wish to show that \u03bb\u0302(n)z \u2192 \u03bb\u2217z , the true parameter. Since \u03bb\u0302 (n) z \u2208 \u039bz and \u039bz is compact, it suffices to show that, any limit point of any convergent subsequence is equal to \u03bb\u2217z . So, suppose that \u03bb\u0302(n)z \u2192 \u03bb\u0302z to some limit point \u03bb\u0302z on some subsequence \u03bb\u0302(n)z .\nFrom \u03bb\u0302(n)z and the definition (15) it follows that\n1\nm m\u2211 i=1 \u03c6z(y (n) i , p (n) i , \u03c4 (n) p , \u03bb\u0302 (n) z )\n\u2265 1 m m\u2211 i=1 \u03c6z(y (n) i , p (n) i , \u03c4 (n) p , \u03bb \u2217 z). (54)\nNow, since \u03c4 (n)p \u2192 \u03c4 tp and \u03bb\u0302 (n) z \u2192 \u03bb\u0302z , we can apply the continuity condition in Definition 2(c) to obtain\nlim inf n\u2192\u221e\n1\nm m\u2211 i=1 [ \u03c6z(y (n) i , p (n) i , \u03c4 t p, \u03bb\u0302z)\n\u2212\u03c6z(y(n)i , p (n) i , \u03c4 t p, \u03bb \u2217 z) ] \u2265 0. (55)\nAlso, the limit (52) and the fact that \u03c6z is psuedo-Lipschitz continuous of order k implies that\nE[\u03c6z(Y, P t, \u03c4 tp, \u03bb\u0302z)] \u2265 E[\u03c6z(Y, P t, \u03c4 tp, \u03bb\u2217z)]. (56)\nBut, property (b) of Definition 2 shows that \u03bb\u2217z is the maxima of the right-hand side, so\nE[\u03c6z(Y, P t, \u03c4 tp, \u03bb\u0302z)] = E[\u03c6z(Y, P t, \u03c4 tp, \u03bb\u2217z)]. (57)\nSince, by Definition 2(b), the maxima is unique, \u03bb\u0302z = \u03bb\u2217z . Since this limit point is the same for all convergent subsequences, we see that \u03bb\u0302(n)z \u2192 \u03bb\u2217z over the entire sequence. We have thus shown that given limits (52), the outputs of the adaptation function converge as\nHtz(y (n),p(n), \u03c4 (n)p ) = \u03bb\u0302 (n) z \u2192 \u03bb\u2217z = Htz(Y, P t, \u03c4 (n)p ).\nThus, the continuity condition on Htz in Assumption 2(b) is satisfied. The analogous continuity condition on Htx can be proven in a similar manner.\nTherefore, all the conditions of Assumption 2 are satisfied and we can apply Theorem 2. Part (a) of Theorem 3 immediately follows from Theorem 2.\nSo, it remains to show parts (b) and (c) of Theorem 3. We will only prove (b); the proof of (c) is similar. Also, since we have already established (22), we only need to show that the output of the SE equations matches the true parameter. That is, we need to show \u03bb t\nx = \u03bb \u2217 x. This fact follows immediately\nfrom the selection of the adaptation functions:\n\u03bb t\nx (a) = Htx(R t, \u03c4 tr) (b) = arg max\n\u03bbx\u2208\u039bx max (\u03b1r,\u03ber)\u2208Sx(\u03c4tr) E [ \u03c6x(R t, \u03bbx, \u03b1r, \u03ber) ]\n(c) = arg max\n\u03bbx\u2208\u039bx max (\u03b1r,\u03ber)\u2208Sx(\u03c4tr) E [ \u03c6x(\u03b1 t rX + V t, \u03bbx, \u03b1r, \u03ber)|\u03bb\u2217x, \u03betr ]\n(58) (d) = \u03bb \u2217 x (59)\nwhere (a) follows from the SE equation (20a); (b) is the definition of the ML adaptation function Htx(\u00b7) when interpreted as a functional on a random variable Rt; (c) is the definition of the random variable Rt in (8) where V t \u223c N (0, \u03betr); and (d) follows from Definition 1(b) and the hypothesis that (\u03b1\u2217r , \u03be \u2217 r ) \u2208 Sx(\u03c4 tr). Thus, we have proven that \u03bb t x = \u03bb \u2217 x, and this completes the proof of part (b) of Theorem 3. The proof of part (c) is similar."}], "references": [{"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M. Tipping"], "venue": "J. Machine Learning Research, vol. 1, pp. 211\u2013244, Sep. 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian factor regressionm models in the \u201clarge p, small n\u201d paradigm", "author": ["M. West"], "venue": "Bayesian Statistics, vol. 7, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparse Bayesian learning for basis selection", "author": ["D. Wipf", "B. Rao"], "venue": "IEEE Trans. Signal Process., vol. 52, no. 8, pp. 2153\u20132164, Aug. 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Bayesian compressive sensing", "author": ["S. Ji", "Y. Xue", "L. Carin"], "venue": "IEEE Trans. Signal Process., vol. 56, pp. 2346\u20132356, Jun. 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with compressible priors", "author": ["V. Cevher"], "venue": "Proc. NIPS, Vancouver, BC, Dec. 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Identification of systems containing linear dynamic and static nonlinear elements", "author": ["S. Billings", "S. Fakhouri"], "venue": "Automatica, vol. 18, no. 1, pp. 15\u201326, 1982.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1982}, {"title": "The identification of nonlinear biological systems: Wiener and Hammerstein cascade models", "author": ["I.W. Hunter", "M.J. Korenberg"], "venue": "Biological Cybernetics, vol. 55, no. 2\u20133, pp. 135\u2013144, 1986.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1986}, {"title": "Spiketriggered neural characterization", "author": ["O. Schwartz", "J.W. Pillow", "N.C. Rust", "E.P. Simoncelli"], "venue": "J. Vision, vol. 6, no. 4, pp. 484\u2013507, Jul. 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Iterative multiuser joint decoding: Unified framework and asymptotic analysis", "author": ["J. Boutros", "G. Caire"], "venue": "IEEE Trans. Inform. Theory, vol. 48, no. 7, pp. 1772\u20131793, Jul. 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Approximate belief propagation, density evolution, and neurodynamics for CDMA multiuser detection", "author": ["T. Tanaka", "M. Okada"], "venue": "IEEE Trans. Inform. Theory, vol. 51, no. 2, pp. 700\u2013706, Feb. 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Asymptotic mean-square optimality of belief propagation for sparse linear systems", "author": ["D. Guo", "C.-C. Wang"], "venue": "Proc. IEEE Inform. Theory Workshop, Chengdu, China, Oct. 2006, pp. 194\u2013198.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Random sparse linear systems observed via arbitrary channels: A decoupling principle", "author": ["\u2014\u2014"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Nice, France, Jun. 2007, pp. 946\u2013950.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Message-passing algorithms for compressed sensing", "author": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proc. Nat. Acad. Sci., vol. 106, no. 45, pp. 18 914\u201318 919, Nov. 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Message passing algorithms for compressed sensing I: motivation and construction", "author": ["\u2014\u2014"], "venue": "Proc. Info. Theory Workshop, Jan. 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Message passing algorithms for compressed sensing II: analysis and validation", "author": ["\u2014\u2014"], "venue": "Proc. Info. Theory Workshop, Jan. 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "The dynamics of message passing on dense graphs, with applications to compressed sensing", "author": ["M. Bayati", "A. Montanari"], "venue": "IEEE Trans. Inform. Theory, vol. 57, no. 2, pp. 764\u2013785, Feb. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimation with random linear mixing, belief propagation and compressed sensing", "author": ["S. Rangan"], "venue": "Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2010, pp. 1\u20136.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized approximate message passing for estimation with random linear mixing", "author": ["\u2014\u2014"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Saint Petersburg, Russia, Jul.\u2013Aug. 2011, pp. 2174\u20132178.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical model concepts in compressed sensing", "author": ["A. Montanari"], "venue": "Compressed Sensing: Theory and Applications, Y. C. Eldar and G. Kutyniok, Eds. Cambridge Univ. Press, Jun. 2012, pp. 394\u2013438.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian inference and optimal design for the sparse linear model", "author": ["M. Seeger"], "venue": "J. Machine Learning Research, vol. 9, pp. 759\u2013813, Sep. 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Expectation-maximization Bernoulli-Gaussian approximate message passing", "author": ["J.P. Vila", "P. Schniter"], "venue": "Conf. Rec. 45th Asilomar Conf. Signals, Syst. & Comput., Pacific Grove, CA, Nov. 2011, pp. 799\u2013803.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Expectation-maximization Gaussian-mixture approximate message passing", "author": ["\u2014\u2014"], "venue": "Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical physics-based reconstruction in compressed sensing", "author": ["F. Krzakala", "M. M\u00e9zard", "F. Sausset", "Y. Sun", "L. Zdeborov\u00e1"], "venue": "arXiv:1109.4424, Sep. 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices", "author": ["\u2014\u2014"], "venue": "arXiv:1206.3953, Jun. 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum-likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Roy. Statist. Soc., vol. 39, pp. 1\u201317, 1977.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1977}, {"title": "Informationtheoretically optimal compressed sensing via spatial coupling and approximate message passing", "author": ["D.L. Donoho", "A. Javanmard", "A. Montanari"], "venue": "December 2011, arXiv:1112.0708v1 [cs.IT].", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid generalized approximation message passing with applications to structured sparsity", "author": ["S. Rangan", "A.K. Fletcher", "V.K. Goyal", "P. Schniter"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Cambridge, MA, Jul. 2012, pp. 1241\u20131245.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Minimax risk over `p-balls for `qerror.", "author": ["D.L. Donoho", "I.M. Johnstone"], "venue": "Probab. Theory and Relat. Fields, vol. 99,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Approximate message passing with consistent parameter estimation and applications to sparse learning", "author": ["U.S. Kamilov", "S. Rangan", "A.K. Fletcher", "M. Unser"], "venue": "Proc. NIPS, Lake Tahoe, NV, Dec. 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M. Figueiredo"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 7, pp. 2479\u20132493, Jul. 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Augmented Lagrangian and Operator- Splitting Methods in Nonlinear Mechanics, ser", "author": ["R. Glowinski", "P.L. Tallec"], "venue": "SIAM Studies in Applied Mathematics. Philadelphia, PA: SIAM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1989}, {"title": "A new inexact alternating directions method for monotone variational inequalities", "author": ["B. He", "L.-Z. Liao", "D. Han", "H. Yang"], "venue": "Math. Program., vol. 92, no. 1, Ser A, pp. 103\u2013108, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Alternating direction augmented Lagrangian methods for semidefinite programming", "author": ["Z. Wen", "D. Goldfarb", "W. Yin"], "venue": "Math. Program. Comp., vol. 2, no. 3\u20134, pp. 203\u2013230, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian compressive sensing via belief propagation", "author": ["D. Baron", "S. Sarvotham", "R.G. Baraniuk"], "venue": "IEEE Trans. Signal Process., vol. 58, no. 1, pp. 269\u2013280, Jan. 2010.  14  APPROXIMATE MESSAGE PASSING WITH CONSISTENT PARAMETER ESTIMATION", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "A statistical-mechanics approach to large-system analysis of CDMA multiuser detectors", "author": ["T. Tanaka"], "venue": "IEEE Trans. Inform. Theory, vol. 48, no. 11, pp. 2888\u20132910, Nov. 2002.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Randomly spread CDMA: Asymptotics via statistical physics", "author": ["D. Guo", "S. Verd\u00fa"], "venue": "IEEE Trans. Inform. Theory, vol. 51, no. 6, pp. 1983\u20132010, Jun. 2005.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1983}, {"title": "Support recovery in compressed sensing: Information-theoretic bounds", "author": ["G. Caire", "S. Shamai", "A. Tulino", "S. Verd\u00fa"], "venue": "Proc. UCSD Workshop Inform. Theory & Its Applications, La Jolla, CA, Jan. 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Asymptotic analysis of MAP estimation via the replica method and applications to compressed sensing", "author": ["S. Rangan", "A. Fletcher", "V.K. Goyal"], "venue": "IEEE Trans. Inform. Theory, vol. 58, no. 3, pp. 1902\u20131923, Mar. 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1902}, {"title": "Iterative estimation of constrained rank-one matrices in noise", "author": ["S. Rangan", "A.K. Fletcher"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Cambridge, MA, Jul. 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressed sensing over `p-balls: Minimax mean square error", "author": ["D. Donoho", "I. Johnstone", "A. Maleki", "A. Montanari"], "venue": "Proc. ISIT, St. Petersburg, Russia, Jun. 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural reconstruction with approximate message passing (NeuRAMP)", "author": ["A.K. Fletcher", "S. Rangan", "L. Varshney", "A. Bhargava"], "venue": "Proc. Neural Information Process. Syst., Granada, Spain, Dec. 2011.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Distribution of eigenvalues for some sets of random matrices", "author": ["V.A. Mar\u010denko", "L.A. Pastur"], "venue": "Math. USSR\u2013Sbornik, vol. 1, no. 4, pp. 457\u2013 483, 1967.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1967}], "referenceMentions": [{"referenceID": 0, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 253, "endOffset": 256}, {"referenceID": 1, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 258, "endOffset": 261}, {"referenceID": 2, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 380, "endOffset": 383}, {"referenceID": 4, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 384, "endOffset": 387}, {"referenceID": 5, "context": "Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]\u2013[8].", "startOffset": 259, "endOffset": 262}, {"referenceID": 7, "context": "Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]\u2013[8].", "startOffset": 263, "endOffset": 266}, {"referenceID": 8, "context": "In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]\u2013[18].", "startOffset": 187, "endOffset": 190}, {"referenceID": 17, "context": "In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]\u2013[18].", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 66, "endOffset": 69}, {"referenceID": 10, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 18, "context": "See, also the survey article [19].", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z.", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z.", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 223, "endOffset": 227}, {"referenceID": 11, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 229, "endOffset": 233}, {"referenceID": 15, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 235, "endOffset": 239}, {"referenceID": 16, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 241, "endOffset": 245}, {"referenceID": 17, "context": "This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "To overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al.", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "To overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al.", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "[24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (\u03bbx, \u03bbz) along with the estimation of the vector x.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (\u03bbx, \u03bbz) along with the estimation of the vector x.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "\u2022 Generalization of the GAMP method of [18] to a class of algorithms we call adaptive GAMP that enables joint estimation of the parameters \u03bbx and \u03bbz along with vector x.", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]\u2013[25] as special cases.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]\u2013[25] as special cases.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]\u2013[25].", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]\u2013[25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs.", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "Simulations in [22], [23] show remarkably good performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Simulations in [22], [23] show remarkably good performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing.", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.", "startOffset": 54, "endOffset": 58}, {"referenceID": 23, "context": "In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods.", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods.", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 151, "endOffset": 155}, {"referenceID": 24, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 26, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 163, "endOffset": 167}, {"referenceID": 27, "context": "An alternate method for joint learning and estimation has been presented in [28], which assumes that the distributions on the source and output channels are themselves described by graphical models with the parameters \u03bbx and \u03bbz appearing as unknown variables.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "The method in [28], called Hybrid-GAMP, iteratively combines standard loopy BP with AMP methods.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "This minimax approach [29] was proposed for AMP recovery of sparse signals in [13].", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "This minimax approach [29] was proposed for AMP recovery of sparse signals in [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "Indeed, this gap was the main justification of the EMGAMP methods in [22], [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "Indeed, this gap was the main justification of the EMGAMP methods in [22], [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "A conference version of this paper has appeared in [30].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "Before describing the adaptive GAMP algorithm, it is useful to review the basic (non-adaptive) GAMP algorithm of [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The GAMP algorithm of [18] can be seen as a class of methods for estimating the vectors x and z for the case when the parameters \u03bbx and \u03bbz are known.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "As described in [18], there are two important sets of choices for the estimation functions, resulting in two variants of GAMP:", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]\u2013 [35].", "startOffset": 112, "endOffset": 116}, {"referenceID": 33, "context": "The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]\u2013 [35].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "More details, as well as the equations for max-sum GAMP can be found in [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "By now, there are a large number of SE results for AMP-related algorithms [9], [11]\u2013[18].", "startOffset": 74, "endOffset": 77}, {"referenceID": 10, "context": "By now, there are a large number of SE results for AMP-related algorithms [9], [11]\u2013[18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "By now, there are a large number of SE results for AMP-related algorithms [9], [11]\u2013[18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Here, we review the particular SE analysis from [18] which is based on the framework in [16].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Here, we review the particular SE analysis from [18] which is based on the framework in [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "of Bayati and Montanari\u2019s analysis in [16].", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "The main result of [18] shows that if we fix the iteration t, and let n\u2192\u221e, the asymptotic joint empirical distribution of the components of these two sets \u03b8 x and \u03b8 t z converges to random vectors of the form", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "With these definitions, we can state the main result from [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Theorem 1 ( [18]): Consider the random vectors \u03b8 x and \u03b8 t z generated by the outputs of GAMP under Assumption 1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "This scalar equivalent model appears in several analyses and can be thought of as a single-letter characterization [36] of the system.", "startOffset": 115, "endOffset": 119}, {"referenceID": 23, "context": "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]\u2013[40].", "startOffset": 97, "endOffset": 101}, {"referenceID": 35, "context": "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]\u2013[40].", "startOffset": 103, "endOffset": 107}, {"referenceID": 38, "context": "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]\u2013[40].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "As described in the previous section, the standard GAMP algorithm of [18] considers the case when the parameters \u03bbx and \u03bbz in the distributions in (1) are known.", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et.", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et.", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "in [24], [25].", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "in [24], [25].", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "For the sum-product AMP or GAMP algorithms, it is shown in [18] that the SE equations simplify so that \u03b1 r = 1 and \u03be r = \u03c4 t r, if the parameters were selected correctly.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "Some justification for this last step can be found in [11], [12], [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "Some justification for this last step can be found in [11], [12], [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Some justification for this last step can be found in [11], [12], [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "In [22], [23], the parameter update (17) is performed only once every few iterations to allow P\u0302 t to converge to the approximation of the posterior distribution of xj given the current parameter estimates.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [22], [23], the parameter update (17) is performed only once every few iterations to allow P\u0302 t to converge to the approximation of the posterior distribution of xj given the current parameter estimates.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "In [24], [25], the parameter estimate is updated every iteration.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [24], [25], the parameter estimate is updated every iteration.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "The proof is straightforward and is based on a continuity argument also used in [41].", "startOffset": 80, "endOffset": 84}, {"referenceID": 40, "context": "It is known that estimators such as LASSO offer certain optimal minmax performance over a large class of sparse distributions [43].", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "where \u039bx = [0, 1]\u00d7 [0,+\u221e).", "startOffset": 11, "endOffset": 17}, {"referenceID": 7, "context": "As a second example, we consider the estimation of the linear-nonlinear-Poisson (LNP) cascade model [8].", "startOffset": 100, "endOffset": 103}, {"referenceID": 41, "context": "AMP techniques combined with the parameter estimation have been recently proposed for neural receptive field estimation and connectivity detection in [44].", "startOffset": 150, "endOffset": 154}, {"referenceID": 21, "context": "The procedure, which is a generalization of EMGAMP methodology of [22]\u2013[25] that estimates both the vector x as well as parameters in the source and componentwise output transform.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "The procedure, which is a generalization of EMGAMP methodology of [22]\u2013[25] that estimates both the vector x as well as parameters in the source and componentwise output transform.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "As described in [18], the sum-product estimation can be implemented with the estimation functions", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "The paper [18] shows that the derivatives of these estimation functions for lines 9 and 16 are computed via the variances:", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "A key result of [18] is that, when the parameters are set to the true values (i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "Bayati and Montanari\u2019s analysis in [16] employs certain deterministic models on the vectors and then proves convergence properties of related empirical distributions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 39, "context": "The proof follows along the adaptation argument of [41].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "This non-adaptive algorithm is precisely the standard GAMP method analyzed in [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 42, "context": "components with zero mean and variance 1/m, it follows from the Mar\u010denko-Pastur Theorem [45] that that its 2-norm operator norm is bounded.", "startOffset": 88, "endOffset": 92}], "year": 2012, "abstractText": "We consider the estimation of an i.i.d. (possibly non-Gaussian) vector x \u2208 R from measurements y \u2208 R obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. A novel method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector x is presented. The proposed algorithm is a generalization of a recently-developed EM-GAMP that uses expectationmaximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The methodology can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. In addition, we show that when a certain maximum-likelihood estimation can be performed in each step, the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. Remarkably, this result applies to essentially arbitrary parametrizations of the unknown distributions, including ones that are nonlinear and non-Gaussian. The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.", "creator": "LaTeX with hyperref package"}}}