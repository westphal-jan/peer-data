{"id": "1207.4404", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2012", "title": "Better Mixing via Deep Representations", "abstract": "it has previously been slightly hypothesized, and supported with some experimental evidence, considering that deeper homogeneous representations, appearing when well parameter trained, tend to do largely a visibly better job at disentangling twice the specific underlying factors rate of orthogonal variation. we consequently study currently the following related conjecture : better shallow representations, in perhaps the sense of delivering better cross disentangling, processes can cannot be readily exploited to produce faster - mixing markov chains. consequently, mixing would naturally be more efficient at the higher surface levels of orthogonal representation. to become better understand why the and how this is partly happening, we propose - a particular secondary above conjecture : the progressively higher - level independent samples fill more positions uniformly unlike the level space they occupy and the high - density orthogonal manifolds otherwise tend tendency to unfold completely when represented at generally higher levels. simultaneously the paper itself discusses these these hypotheses and tests verified them experimentally through visualization tool and direct measurements of mixing curves and interpolating between samples.", "histories": [["v1", "Wed, 18 Jul 2012 16:07:36 GMT  (561kb,D)", "http://arxiv.org/abs/1207.4404v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "gr\u00e9goire mesnil", "yann dauphin", "salah rifai"], "accepted": true, "id": "1207.4404"}, "pdf": {"name": "1207.4404.pdf", "metadata": {"source": "CRF", "title": "Better Mixing via Deep Representations", "authors": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin"], "emails": [], "sections": [{"heading": "1 Introduction and Background", "text": "Deep learning algorithms attempt to discover multiple levels of representation of the given data (see (Bengio, 2009) for a review), with higher levels of representation defined hierarchically in terms of lower level ones. The central motivation is that higherlevel representations can potentially capture higher-level abstractions relevant to the distribution of interest. Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (Ha\u030astad, 1986; Ha\u030astad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011). The intuition behind these theoretical advantages is that lowerlevel features or latent variables can be re-used in many ways to construct higher-level ones, and the potential gain becomes exponential with respect to depth of the circuit that relates lower-level features and higher-level ones (thanks to the exponential number of paths in between). The ability of deep learning algorithms to construct abstract features or latent variables on top of the observed random variables therefore relies on this idea of re-use, which brings with it not only computational but also statistical advantages in terms of statistical power, e.g., as already exploited in prior machine learn-\nar X\niv :1\n20 7.\n44 04\nv1 [\ncs .L\nG ]\n1 8\ning work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).\nThere is another \u2013 less commonly discussed \u2013 motivation for deep representations, introduced in Bengio (2009): the idea that they may, to some extent, help to disentangle the underlying factors of variation. Clearly, if we had learning algorithms that could do a good job of discovering and separating out the underlying causes and factors of variation present in the data, it would make further processing (typically, taking decisions) much easier. One could even say that the ultimate goal of AI research is to build machines that can understand the world around us, i.e., disentangle the factors and causes it involves, so progress in that direction seems important. If learned representations do a good job of disentangling the underlying factors of variation, it is clear that learning (on top of these representations, e.g., towards specific tasks of interest) becomes substantially easier because disentangling counters the effects of the curse of dimensionality. In fact, in the extreme case, and where we also observe the target variables and effects of decisions, there is no need for further learning, only good inference. Several observations have been made and reported that suggest that some deep learning algorithms indeed help to disentangle the underlying factors of variation (Goodfellow et al., 2009; Glorot et al., 2011). However, it is not clear why, and to what extent in general (if any), different deep learning algorithms may sometimes help this disentangling.\nMost deep learning algorithms are based on some form of unsupervised learning, hence capturing salient structure in the data distribution. Whereas deep learning algorithms have mostly been used to learn features and exploit them for classification or regression tasks, their unsupervised nature also means that in several cases they can be used to generate samples. In general the associated sampling algorithms involve a Markov Chain and MCMC techniques, and these can notoriously suffer from a fundamental problem of mixing: it is difficult for the Markov chain to jump from one mode of the distribution to another, when these are separated by large low-density regions, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010). This hypothesis states that natural classes present in the data are associated with low-dimensional regions in input space (manifolds) near which the distribution concentrates, and that different class manifolds are well-separated by regions of very low density. Slow mixing means that consecutive samples tend to be correlated (belong to the same mode) and that it takes many consecutive sampling steps to go from one mode to another and even more to cover all of them, i.e., to obtain a large enough representative set of samples (e.g. to compute an expected value under the target distribution). This happens because these jumps through the empty low-density void between modes are unlikely and rare events. When a learner has a poor model of the data, e.g., in the initial stages of learning, the model tends to correspond to a smoother and higher-entropy (closer to uniform) distribution, putting mass in larger volumes of input space, and in particular, between the modes (or manifolds). This can be visualized in generated samples of images, that look more blurred and noisy. Mixing is therefore initially easy for such poor models. However, as the model improves and its corresponding distribution sharpens near where the data concentrate, mixing becomes considerably slower. Since sampling is an integral part of many learning\nalgorithms (e.g., to estimate the log-likelihood gradient), slower mixing then means slower or poorer learning, and one may even suspect that learning stalls at some point because of the limitations of the sampling algorithm. To improve mixing, a powerful idea that has been explored recently for deep learning algorithms (Desjardins et al., 2010; Cho et al., 2010; Salakhutdinov, 2010b,a) is tempering (Neal, 1994). The idea is use smoother densities (associated with higher temperature in a Boltzmann Machine or Markov Random Field formulation) to make quick but approximate jumps between modes, but use the sharp \u201ccorrect\u201d model to actually generate the samples of interest inside these modes, and allow samples to be exchanged between the different levels of temperature.\nHere we want to discuss another possibly related idea, and claim that mixing is easier when sampling at the higher levels of representation. The objective is not to propose a new sampling algorithm or a new learning algorithm, but rather to investigate this hypothesis through experiments using existing deep learning algorithms. The objective is to further our understanding of this hypothesis through more specific hypotheses aiming to explain why this would happen, using further experimental validation to test these more specific hypotheses. The idea that deeper generative models produce not only better features for classification but also better quality samples (in the sense of better corresponding to the target distribution being learned) is not novel and several observations support this hypothesis already, some quantitatively (Salakhutdinov and Hinton, 2009), some more qualitative (Hinton et al., 2006). The specific contributions of this paper is to focus on why the samples may be better, and in particular, why the chains may converge faster, based on the previously introduced idea that deeper representations can do a better job of disentangling the underlying factors of representation."}, {"heading": "2 Hypotheses", "text": "We first clarify the first hypothesis to be tested here.\nHypothesis H1: Depth vs Better Mixing. A successfully trained deeper architecture has the potential to yield representation spaces in which Markov chains mix faster.\nIf experiments validate that hypothesis, the most important next question is: why? The main explanation we conjecture is formalized in the following hypothesis.\nHypothesis H2: Depth vs Disentangling. Part of the explanation of H1 is that deeper representations can better disentangle the underlying factors of variation.\nWhy would that help to explain H1? Imagine an abstract (high-level) representation for object image data in which one of the factors is the \u201creverse video bit\u201d, which inverts black and white, e.g., flipping that bit replaces intensity x \u2208 [0, 1] by 1 \u2212 x. With the default value of 0, the foreground object is dark and the background light. Clearly, flipping that bit does not change most of the other semantic characteristics of the image, which could be represented in other high-level features. However, for\nevery image-level mode, there would be a reverse-video counterpart mode in which that bit is flipped: these two modes would be separated by vast \u201cempty\u201d (low density) regions in input space, making it very unlikely for any Markov chain in input space (e.g. Gibbs sampling in an RBM) to jump from one of these two modes to another, because that would require most of the input pixels or hidden units of the RBM to simultaneously flip their value. Instead, if we consider the high-level representation which has a \u201creverse video\u201d bit, flipping only that bit would be a very likely event under most Markov chain transition probabilities, since that flip would be a small change preserving high probability. As another example, imagine that some of the bits of the high-level representation identify the category of the object in the image, independently of pose, illumination, background, etc. Then simply flipping one of these object-class bits would also drastically change the raw pixel-space image, while keeping likelihood high. Jumping from an object-class mode to another would therefore be easy with a Markov chain in representation-space, whereas it would be much less likely in raw pixel-space.\nAnother point worth discussing (and which should be considered in future work) in H2 is the notion of degree of disentangling. Although it is somewhat clear what a completely disentangled representation would look like, deep learning algorithms are unlikely to do a perfect job of disentangling, and current algorithms do it in stages, with more abstract features being extracted at higher levels. Better disentangling would mean that some of the learned features have a higher mutual information with some of the known factors. One would expect at the same time that the features that are highly predictive of one factor be less so of other factors, i.e., that they specialize to one or a few of the factors, becoming invariant to others. Please note here the difference between the objective of learning disentangled representations and the objective of learning invariant features (i.e., invariant to some specific factors of variation which are considered to be like nuisance parameters). In the latter case, one has to know ahead of time what the nuisance factors are (what is noise and what is signal?). In the former, it is not needed: we only seek to separate out the factors from each other. Some features should be sensitive to one factor and invariant to the others.\nLet us now consider additional hypotheses that specialize H2.\nHypothesis H3: Disentangling Unfolds and Expands. Part of the explanation of H2 is that more disentangled representations tend to (a) unfold the manifolds near which raw data concentrates, as well as (b) expand the relative volume occupied by high-probability points near these manifolds.\nH3(a) says is that disentangling has the effect that the projection of high-density manifolds in the high-level representation space are smoother and easier to model than the corresponding high-density manifolds in raw input space. Let us again use an object recognition analogy. If we have perfectly disentangled object identity, pose and illumination, the high-density manifold associated with the distribution of features in high-level representation-space is flat: we can make large moves in that space (e.g., completely change the lighting) and yet stay in a high-probability region. In fact we can make any move inside some bounds and convex constraints and stay in a highprobability region, so that the distribution of high-level features may look locally more\nuniform, which is a consequence of H3(b). A good high-level representation does not need to allocate as much real estate (sets of values) for unlikely configurations. This is already what most unsupervised learning algorithms tend to do. For example, dimensionality reduction methods such as the PCA tend to define representations where most configurations are likely (but these only occupy a subspace of the possible raw-space configurations). Also, in clustering algorithms such as k-means, the training criterion is best minimized when clusters are approximately equally-weighted, i.e., the average posterior distribution over cluster identity is approximately uniform. Something similar is observed in the brain where different areas of somatosensory cortex correspond to different body parts, and the size of these areas adaptively depends (Flor, 2003) on usage of these (i.e., more frequent events are represented more finely and less frequent ones are represented more coarsely). Again, keep in mind that the actual representations learned by deep learning algorithms are not perfect, but what we will be looking for here is whether deeper representations correspond to more unfolded manifolds and to more locally uniform distributions, with high-probability points occupying an overall greater volume (compared to the available volume)."}, {"heading": "3 Representation-Learning Algorithms", "text": "The learning algorithms used in this paper to explore the preceding hypotheses are the Deep Belief Network or DBN (Hinton et al., 2006), trained by stacking Restricted Boltzmann Machines or RBMs, and the Contractive Auto-Encoder or CAE (Rifai et al., 2011a), for which a sampling algorithm was recently proposed (Rifai et al., 2012).\nSee Bengio (2009) for a detailed review of RBMs and DBNs. Each layer of the DBN is trained as an RBM, and a 1-layer DBN is just an RBM. An RBM defines a joint distribution between a hidden layer h and a visible layer v. Gibbs sampling at the top level of the DBN is used to obtain samples from the model: the sampled top-level representations are stochastically projected down to lower levels through the conditional distributions P (v|h) defined in each RBM. To avoid unnecessary additional noise, and like previous authors have done, at the last stage of this process (i.e. to obtain the raw-input level samples), only the mean-field values of the visible units are used, i.e.,E[v|h]. In the experiments on face data (where grey levels matter a lot), a Gaussian RBM is used at the lowest level.\nAn auto-encoder (LeCun, 1987; Hinton and Zemel, 1994) is parametrized through an encoder function f mapping an input-space vector x to a representation-space vector h, and a decoder function g mapping a representation-space vector h to an input-space reconstruction r. The experiments with the CAE are with h = f(x) = sigmoid(Wx+ b) and r = g(h) = sigmoid(WTh+ c). The CAE is a regularized auto-encoder, with tied weights (input to hidden weights are the transpose of hidden to reconstruction weights). The CAE is trained to minimize a reconstruction loss (cross-entropy here) plus a contractive regularization penalty \u03b1||\u2202f(x)\u2202x || 2 F (which is the sum of the elements of the Jacobian matrix). Like RBMs, CAE layers can be stacked to form deeper models, and one can either view them as deep auto-encoders (by composing the encoders together and the decoders together) or like in a DBN, as a top-level model (from which one can sample) coupled with encoding and decoding functions into the top level (by\ncomposing the lower-level encoders and decoders). A sampling algorithm was recently proposed for CAEs (Rifai et al., 2012). The idea is to alternate between projecting through the auto-encoder (i.e. performing a reconstruction) and adding Gaussian noise JJT in the directions of variation captured by the auto-encoder (in the Jacobian matrix J = \u2202f(x)\u2202x of the encoder function)."}, {"heading": "4 Experiments", "text": "The experiments have been performed on the MNIST digits dataset (LeCun et al., 1998) and the Toronto Face Database (Susskind et al., 2010), TFD. The former has been heavily used to evaluate many deep learning algorithms, while the latter is interesting because of the manifold structure it displays, and for which control factors (such as emotion and person identity) are known.\nThe DBNs tested on MNIST have 768-1024-1024 layer sizes (28\u00d728 input), and 2304-512-1024 on TFD (48\u00d748 input). The CAEs have sizes 768-1000-1000 and 2304-1000-1000 on MNIST and TFD respectively."}, {"heading": "4.1 Sampling at Different Depths", "text": ""}, {"heading": "4.1.1 Better Samples at Higher Levels", "text": "To test H1, we first plot sequences of samples at various depths. One can verify in Fig. 1 that samples obtained at deeper layers are visually more likely and mix faster.\nIn addition, we measure the quality of the obtained samples, using a procedure for the comparison of sample generators described in Breuleux et al. (2011). It measures the log-likelihood of a test set under the density computed from a Parzen window density estimator built on 10, 000 generated samples. Log-likelihoods for different models are presented in Table 1 (rightmost columns). Those results also suggest that the quality of the samples is higher if the Markov chain process used for sampling takes place in the upper layers.\nThis observation agrees with H3(b) that moving in higher-level representation spaces where the manifold has been expanded provides higher quality samples than moving in the raw input space where it may be hard to stay in high density regions."}, {"heading": "4.1.2 Visualizing Representation-Space by Interpolating Between Neighbors", "text": "According to H3(a), deeper layers tend to locally unfold the manifold near highdensities regions of the input space, while according to H3(b) there should be more relative volume taken by plausible configurations in representation-space. Both of these would imply that convex combinations of neighboring examples in representationspace correspond to more likely input configurations. Indeed, interpolating between points on a flat manifold should stay on the manifold. Furthermore, when interpolating between examples of different classes (i.e., different modes), H3(b) would suggest that most of the points in between (on the linear interpolation line) should correspond to plausible samples, which would not be the case in input space. In Fig. 2, we interpolate linearly between neighbors in representation space and visualize in the input space the\ninterpolated points obtained at various depths. One can see that interpolating at deeper levels gives visually more plausible samples."}, {"heading": "4.2 Measuring Mixing by Counting Number of Classes Visited", "text": "We evaluate here the ability of mixing among various classes. We consider sequences of length 10, 20 or 100 and compute histograms of number of different classes visited in a sequence, for the two different depths and learners, on TFD. Since classes typically are in different modes (manifolds), counting how many different classes are visited in an MCMC run tells us how quickly the chain mixes. Results in Fig. 3(c,f) show that the deeper architectures visit more classes and the CAE mixes faster than the DBN."}, {"heading": "4.3 Occupying More Volume Around Data Points", "text": "In these experiments (Fig. 3 (a,b,d,e)) we estimate the quality of samples whose representation is in the neighborhood of training examples, at different levels of representation. In the first setting (Fig. 3 (a,b)), the samples are interpolated at the midpoint between an example and its k-th nearest neighbor, with k on the x-axis. In the second case (Fig. 3 (d,e)), isotropic noise is added around an example, with noise standard deviation on the x-axis. In both cases, 500 samples are generated for each data point plotted on the figures, with the y-axis being the log-likelihood introduced earlier, i.e., estimating the quality of the samples. We find that on higher-level representations of both the CAE and DBN, a much larger proportion of the local volume is occupied by likely configurations, i.e., closer to the input-space manifold near which the actual data-generating distribution concentrates. Whereas the first experiment shows that this is true in the convex set between neighbors at different distances (i.e., in the directions of the manifold), the second shows that this is also true in random directions (but of course likelihoods are also worse there). The first result therefore agrees with H3(a) (unfolding) and H3(b) (volume expansion), while the second result mostly confirms H3(b)."}, {"heading": "4.4 Discriminative Ability vs Volume Expansion", "text": "The proposed hypotheses could arguably correspond to worse discriminative power. Indeed, if on the higher-level representations the different classes are \u201ccloser\u201d to each other (making it easier to mix between them), would it not mean that they are more confusable? We first confirm with the tested models (as a sanity check) that the deeper level features are conducive to better classification performance, in spite of their better generative abilities and better mixing.\nWe train a linear SVM on the concatenation of the raw input with the upper layers representations. Results presented in Table 1 show that the representation is more linearly separable if one increases the depth of the architecture and the information added by each layer is helpful for classification. Also, fine-tuning a MLP initialized with those weights is still the best way to reach state-of-the-art performances.\nTo explain the good discriminant abilities of the deeper layers (either when concatenated with lower layers or when fine-tuned discriminatively) in spite of the better mixing observed, we conjecture the help of a better disentangling of the underlying factors of variation, and in particular of the class factors. This would mean that the manifolds associated with different classes are more unfolded (as assumed by H3(a)) and possibly that different hidden units specialize more to specific classes than they would on lower layers. Hence the unfolding (H3(a)) and disentangling (H1) hypotheses reconcile better discriminative ability with expanded volume of good samples (H3(b))."}, {"heading": "5 Conclusion", "text": "The following hypotheses were tested: (1) deeper representations can yield better samples and better mixing; (2) this is due to better disentangling; (3) this is associated with\nunfolding of the manifold where data concentrate along with expansion of the volume good samples take in representation-space. The experimental results were in agreement with these hypotheses. They showed better samples and better mixing on higher levels, better samples obtained when interpolating between examples at higher levels, and better samples obtained when adding isotropic noise at higher levels. We also considered the potential conflict between the third hypothesis and better discrimination (confirmed on the models tested) and explained it away as a consequence of the second hypothesis.\nThis could be immediate good news for applications requiring to generate MCMC samples: by transporting the problem to deeper representations, better and faster results could be obtained. Future work should also investigate the link between better mixing and the process of training deep learners themselves, when they depend on an MCMC to estimate the log-likelihood gradient. One interesting direction is to investigate the relation between parallel tempering and the better mixing chains obtained from deeper layers."}], "references": [{"title": "A Bayesian/information theoretic model of learning via multiple task sampling", "author": ["J. Baxter"], "venue": "Machine Learning,", "citeRegEx": "Baxter,? \\Q1997\\E", "shortCiteRegEx": "Baxter", "year": 1997}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": null, "citeRegEx": "Bengio and Delalleau,? \\Q2011\\E", "shortCiteRegEx": "Bengio and Delalleau", "year": 2011}, {"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": null, "citeRegEx": "Bengio and LeCun,? \\Q2007\\E", "shortCiteRegEx": "Bengio and LeCun", "year": 2007}, {"title": "The curse of highly variable functions for local kernel machines", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "In NIPS\u20192005,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Quickly generating representative samples from an rbm-derived process", "author": ["O. Breuleux", "Y. Bengio", "P. Vincent"], "venue": "Neural Computation,", "citeRegEx": "Breuleux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Breuleux et al\\.", "year": 2011}, {"title": "Learning many related tasks at the same time with backpropagation", "author": ["R. Caruana"], "venue": "In NIPS\u201994,", "citeRegEx": "Caruana,? \\Q1995\\E", "shortCiteRegEx": "Caruana", "year": 1995}, {"title": "Algorithms for manifold learning", "author": ["L. Cayton"], "venue": "Technical Report CS2008-0923,", "citeRegEx": "Cayton,? \\Q2005\\E", "shortCiteRegEx": "Cayton", "year": 2005}, {"title": "Parallel tempering is efficient for learning restricted boltzmann machines", "author": ["K. Cho", "T. Raiko", "A. Ilin"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Tempered Markov chain monte carlo for training of restricted Boltzmann machine", "author": ["G. Desjardins", "A. Courville", "Y. Bengio", "P. Vincent", "O. Delalleau"], "venue": "In JMLR W&CP: Proc. AISTATS\u20192010,", "citeRegEx": "Desjardins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "Remapping somatosensory cortex after injury", "author": ["H. Flor"], "venue": "Advances in Neurology,", "citeRegEx": "Flor,? \\Q2003\\E", "shortCiteRegEx": "Flor", "year": 2003}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "A. Ng"], "venue": "In NIPS\u20192009,", "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Almost optimal lower bounds for small depth circuits", "author": ["J. H\u00e5stad"], "venue": "In STOC\u201986,", "citeRegEx": "H\u00e5stad,? \\Q1986\\E", "shortCiteRegEx": "H\u00e5stad", "year": 1986}, {"title": "On the power of small-depth threshold circuits", "author": ["J. H\u00e5stad", "M. Goldmann"], "venue": "Computational Complexity,", "citeRegEx": "H\u00e5stad and Goldmann,? \\Q1991\\E", "shortCiteRegEx": "H\u00e5stad and Goldmann", "year": 1991}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["G.E. Hinton", "R.S. Zemel"], "venue": null, "citeRegEx": "Hinton and Zemel,? \\Q1994\\E", "shortCiteRegEx": "Hinton and Zemel", "year": 1994}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "The development of the time-delay neural network architecture for speech recognition", "author": ["K.J. Lang", "G.E. Hinton"], "venue": "Technical Report CMU-CS-88-152,", "citeRegEx": "Lang and Hinton,? \\Q1988\\E", "shortCiteRegEx": "Lang and Hinton", "year": 1988}, {"title": "Mod\u00e8les connexionistes de l\u2019apprentissage", "author": ["Y. LeCun"], "venue": "Ph.D. thesis, Universite\u0301 de Paris VI", "citeRegEx": "LeCun,? \\Q1987\\E", "shortCiteRegEx": "LeCun", "year": 1987}, {"title": "Generalization and network design strategies", "author": ["Y. LeCun"], "venue": "Technical Report CRG-TR-89-4,", "citeRegEx": "LeCun,? \\Q1989\\E", "shortCiteRegEx": "LeCun", "year": 1989}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sample complexity of testing the manifold hypothesis", "author": ["H. Narayanan", "S. Mitter"], "venue": "In NIPS\u20192010", "citeRegEx": "Narayanan and Mitter,? \\Q2010\\E", "shortCiteRegEx": "Narayanan and Mitter", "year": 2010}, {"title": "Sampling from multimodal distributions using tempered transitions", "author": ["R.M. Neal"], "venue": "Technical Report 9421,", "citeRegEx": "Neal,? \\Q1994\\E", "shortCiteRegEx": "Neal", "year": 1994}, {"title": "Contracting auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Learning deep Boltzmann machines using adaptive MCMC", "author": ["R. Salakhutdinov"], "venue": "In ICML\u20192010", "citeRegEx": "Salakhutdinov,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov", "year": 2010}, {"title": "Learning in Markov random fields using tempered transitions", "author": ["R. Salakhutdinov"], "venue": "In NIPS\u20192010", "citeRegEx": "Salakhutdinov,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov", "year": 2010}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS\u20192009,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "The Toronto face dataset", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical Report UTML TR 2010-001,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Deep learning algorithms attempt to discover multiple levels of representation of the given data (see (Bengio, 2009) for a review), with higher levels of representation defined hierarchically in terms of lower level ones.", "startOffset": 102, "endOffset": 116}, {"referenceID": 14, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 15, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 4, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 3, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 2, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 6, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 48, "endOffset": 105}, {"referenceID": 0, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 48, "endOffset": 105}, {"referenceID": 9, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 48, "endOffset": 105}, {"referenceID": 18, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 158, "endOffset": 194}, {"referenceID": 20, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 158, "endOffset": 194}, {"referenceID": 13, "context": "Several observations have been made and reported that suggest that some deep learning algorithms indeed help to disentangle the underlying factors of variation (Goodfellow et al., 2009; Glorot et al., 2011).", "startOffset": 160, "endOffset": 206}, {"referenceID": 12, "context": "Several observations have been made and reported that suggest that some deep learning algorithms indeed help to disentangle the underlying factors of variation (Goodfellow et al., 2009; Glorot et al., 2011).", "startOffset": 160, "endOffset": 206}, {"referenceID": 7, "context": "In general the associated sampling algorithms involve a Markov Chain and MCMC techniques, and these can notoriously suffer from a fundamental problem of mixing: it is difficult for the Markov chain to jump from one mode of the distribution to another, when these are separated by large low-density regions, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010).", "startOffset": 380, "endOffset": 422}, {"referenceID": 22, "context": "In general the associated sampling algorithms involve a Markov Chain and MCMC techniques, and these can notoriously suffer from a fundamental problem of mixing: it is difficult for the Markov chain to jump from one mode of the distribution to another, when these are separated by large low-density regions, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010).", "startOffset": 380, "endOffset": 422}, {"referenceID": 0, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989). There is another \u2013 less commonly discussed \u2013 motivation for deep representations, introduced in Bengio (2009): the idea that they may, to some extent, help to disentangle the underlying factors of variation.", "startOffset": 64, "endOffset": 306}, {"referenceID": 23, "context": ", 2010; Salakhutdinov, 2010b,a) is tempering (Neal, 1994).", "startOffset": 45, "endOffset": 57}, {"referenceID": 29, "context": "The idea that deeper generative models produce not only better features for classification but also better quality samples (in the sense of better corresponding to the target distribution being learned) is not novel and several observations support this hypothesis already, some quantitatively (Salakhutdinov and Hinton, 2009), some more qualitative (Hinton et al.", "startOffset": 294, "endOffset": 326}, {"referenceID": 17, "context": "The idea that deeper generative models produce not only better features for classification but also better quality samples (in the sense of better corresponding to the target distribution being learned) is not novel and several observations support this hypothesis already, some quantitatively (Salakhutdinov and Hinton, 2009), some more qualitative (Hinton et al., 2006).", "startOffset": 350, "endOffset": 371}, {"referenceID": 11, "context": "Something similar is observed in the brain where different areas of somatosensory cortex correspond to different body parts, and the size of these areas adaptively depends (Flor, 2003) on usage of these (i.", "startOffset": 172, "endOffset": 184}, {"referenceID": 17, "context": "The learning algorithms used in this paper to explore the preceding hypotheses are the Deep Belief Network or DBN (Hinton et al., 2006), trained by stacking Restricted Boltzmann Machines or RBMs, and the Contractive Auto-Encoder or CAE (Rifai et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 26, "context": ", 2011a), for which a sampling algorithm was recently proposed (Rifai et al., 2012).", "startOffset": 63, "endOffset": 83}, {"referenceID": 19, "context": "An auto-encoder (LeCun, 1987; Hinton and Zemel, 1994) is parametrized through an encoder function f mapping an input-space vector x to a representation-space vector h, and a decoder function g mapping a representation-space vector h to an input-space reconstruction r.", "startOffset": 16, "endOffset": 53}, {"referenceID": 16, "context": "An auto-encoder (LeCun, 1987; Hinton and Zemel, 1994) is parametrized through an encoder function f mapping an input-space vector x to a representation-space vector h, and a decoder function g mapping a representation-space vector h to an input-space reconstruction r.", "startOffset": 16, "endOffset": 53}, {"referenceID": 1, "context": "See Bengio (2009) for a detailed review of RBMs and DBNs.", "startOffset": 4, "endOffset": 18}, {"referenceID": 26, "context": "A sampling algorithm was recently proposed for CAEs (Rifai et al., 2012).", "startOffset": 52, "endOffset": 72}, {"referenceID": 21, "context": "The experiments have been performed on the MNIST digits dataset (LeCun et al., 1998) and the Toronto Face Database (Susskind et al.", "startOffset": 64, "endOffset": 84}, {"referenceID": 30, "context": ", 1998) and the Toronto Face Database (Susskind et al., 2010), TFD.", "startOffset": 38, "endOffset": 61}, {"referenceID": 5, "context": "In addition, we measure the quality of the obtained samples, using a procedure for the comparison of sample generators described in Breuleux et al. (2011). It measures the log-likelihood of a test set under the density computed from a Parzen window density estimator built on 10, 000 generated samples.", "startOffset": 132, "endOffset": 155}], "year": 2012, "abstractText": "It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation. To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples.", "creator": "LaTeX with hyperref package"}}}