{"id": "1209.1360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2012", "title": "Multiclass Learning with Simplex Coding", "abstract": "usually in this paper we simply discuss a novel technical framework for approximate multiclass machine learning, mostly defined partly by modelling a suitable coding / decoding strategy, namely termed the simplex spectral coding, that progressively allows users to generalize to multiple linear classes a relaxation approach sometimes commonly thus used in traditional binary level classification. in this framework, a relaxation or error analysis can now be developed avoiding constraints on the considered hypotheses class. moreover, we show that in this setting providing it here is possible informally to derive the first generalized provably valid consistent regularized method with training / functional tuning selection complexity which is fundamentally independent to the number of classes. this tools synthesized from general convex analysis procedures are finally introduced that can therefore be used beyond expanding the restricted scope point of this paper.", "histories": [["v1", "Thu, 6 Sep 2012 18:22:25 GMT  (238kb,D)", "https://arxiv.org/abs/1209.1360v1", null], ["v2", "Fri, 14 Sep 2012 14:14:53 GMT  (170kb,D)", "http://arxiv.org/abs/1209.1360v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["youssef mroueh", "tomaso a poggio", "lorenzo rosasco", "jean-jacques e slotine"], "accepted": true, "id": "1209.1360"}, "pdf": {"name": "1209.1360.pdf", "metadata": {"source": "CRF", "title": "Multiclass Learning with Simplex Coding", "authors": ["Youssef Mroueh", "Tomaso Poggio", "Lorenzo Rosasco", "Jean-Jacques E. Slotine"], "emails": ["lrosasco@mit.edu", "jjs@mit.edu", "tp@ai.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "As bigger and more complex datasets are available, multiclass learning is becoming increasingly important in machine learning. While theory and algorithms for solving binary classification problems are well established, the problem of multicategory classification is much less understood. Practical multiclass algorithms often reduce the problem to a collection of binary classification problems. Binary classification algorithms are often based on a relaxation approach: classification is posed as a non-convex minimization problem and hence relaxed to a convex one, defined by suitable convex loss functions. In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations. Generalizing the above approach and results to more than two classes is not straightforward. Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21]. Indeed, most of the above methods can be interpreted as a kind of relaxation. Most proposed methods have complexity which is more than linear in the number of classes and simple one-vs all in practice offers a good alternative both in terms of performance and speed [15]. Much fewer works have focused on deriving theoretical guarantees. Results in this sense have been pioneered by [28, 20], see also [11, 7, 23]. In these works the error due to relaxation is studied asymptotically and under constraints on the function class to be considered. More quantitative results in terms of comparison inequalities are given in [4] under similar restrictions (see also [19]). Notably, the above results show that seemigly intuitive extensions of binary classification algorithms might lead to methods which are not consistent. Further, it is interesting to note that these restrictions on the function class, needed to prove the theoretical guarantees, make the computations in the corresponding algorithms more involved and are in fact often ignored in practice. In this paper we dicuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, in which a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this framework it is possible to derive the first provably consistent regularized method with training/tuning complexity which is independent to the number of classes. Interestingly, using the simplex coding, we can naturally generalize results, proof techniques and methods from the\nar X\niv :1\n20 9.\n13 60\nv2 [\nst at\n.M L\n] 1\n4 Se\nbinary case, which is recovered as a special case of our theory. Due to space restriction in this paper we focus on extensions of least squares, and SVM loss functions, but our analysis can be generalized to large class of simplex loss functions, including extension of logistic and exponential loss functions (used in boosting). Tools from convex analysis are developed in the longer version of the paper and can be useful beyond the scopes of this paper, and in particular in structured prediction.\nThe rest of the paper is organized as follow. In Section 2 we discuss problem statement and background. In Section 3 we discuss the simplex coding framework that we analyze in Section 4. Algorithmic aspects and numerical experiments are discussed in Section 5 and Section 6, respectively. Proofs and supplementary technical results are given in the longer version of the paper."}, {"heading": "2 Problem Statement and Previous Work", "text": "Let (X,Y ) be two random variables with values in two measurable spaces X and Y = {1 . . . T}, T \u2265 2. Denote by \u03c1X , the law of X on X , and by \u03c1j(x), the conditional probabilities for j \u2208 Y . The data is a sample S = (xi, yi)ni=1, from n identical and independent copies of (X,Y ).We can think of X as a set of possible inputs and of Y , as a set of labels describing a set of semantic categories/classes the input can belong to. A classification rule is a map b : X \u2192 Y , and its error is measured by the misclassification risk R(b) = P(b(X) 6= Y ) = E(1I[b(x) 6=y](X,Y )). The optimal classification rule that minimizes R is the Bayes rule, b\u03c1(x) = arg maxy\u2208Y \u03c1y(x), x \u2208 X . Computing the Bayes rule by directly minimizing the risk R, is not possible since the probability distribution is unknown. In fact one could think of minimizing the empirical risk (ERM), RS(b) = 1n \u2211n i=1 1I[b(x)6=y](xi, yi), which is an unbiased estimator of the R, but the corresponding optimization problem is in general not feasible. In binary classification, one of the most common way to obtain computationally efficient methods is based on a relaxation approach. We recall this approach in the next section and describe its extension to multiclass in the rest of the paper. Relaxation Approach to Binary Classification. If T = 2, we can set Y = \u00b11. Most modern machine learning algorithms for binary classification consider a convex relaxation of the ERM functional RS . More precisely: 1) the indicator function in RS is replaced by non negative loss V : Y \u00d7R\u2192 R+ which is convex in the second argument and is sometimes called a surrogate loss; 2) the classification rule b replaced by a real valued measurable function f : X \u2192 R. A classification rule is then obtained by considering the sign of f . It often suffices to consider a special class of loss functions, namely large margin loss functions V : R \u2192 R+ of the form V (\u2212yf(x)). This last expression is suggested by the observation that the misclassification risk, using the labels \u00b11, can be written as R(f) = E(\u0398(\u2212Y f(X))), where \u0398 is the heavy side step function. The quantity m = \u2212yf(x), sometimes called the margin, is a natural point-wise measure of the classification error. Among other examples of large margin loss functions (such as the logistic and exponential loss), we recall the hinge loss V (m) = |1 +m|+ = max{1 + m, 0} used in support vector machine, and the square loss V (m) = (1 +m)2 used in regularized least squares (note that (1\u2212 yf(x))2 = (y \u2212 f(x))2). Using surrogate large margin loss functions it is possible to design effective learning algorithms replacing the empirical risk with regularized empirical risk minimization\nE\u03bbS (f) = 1\nn n\u2211 i=1 V (yi, f(xi)) + \u03bbR(f), (1)\nwhereR is a suitable regularization functional and \u03bb is the regularization parameter, see Section 5."}, {"heading": "2.1 Relaxation Error Analysis", "text": "As we replace the misclassification loss with a convex surrogate\u2014 loss, we are effectively changing the problem: the misclassification risk is replaced by the expected loss, E(f) = E(V (\u2212Y f(X))) . The expected loss can be seen as a functional on a large space of functions F = FV,\u03c1, which depend on V and \u03c1. Its minimizer, denoted by f\u03c1, replaces the Bayes rule as the target of our algorithm. The question arises of the price we pay by a considering a relaxation approach: \u201cWhat is the relationship between f\u03c1 and b\u03c1?\u201d More generally, \u201cWhat is the approximation we incur into by estimating the expected risk rather than the misclassification risk?\u201d The relaxation error for a given loss function can be quantified by the following two\nrequirements: 1) Fisher Consistency. A loss function is Fisher consistent if sign(f\u03c1(x)) = b\u03c1(x) almost surely (this property is related to the notion of classification-calibration [2]). 2) Comparison inequalities. The excess misclassification risk, and the excess expected loss are related by a comparison inequality\nR(sign(f))\u2212R(b\u03c1) \u2264 \u03c8(E(f)\u2212 E(f\u03c1)),\nfor any function f \u2208 F , where \u03c8 = \u03c8V,\u03c1 is a suitable function that depends on V , and possibly on the data distribution. In particular \u03c8 should be such that \u03c8(s) \u2192 0 as s \u2192 0, so that if fn is a (possibly random) sequence of functions, such that E(fn) \u2192 E(f\u03c1) (possibly in probability), then the corresponding sequences of classification rules cn = sign(fn) is Bayes consistent, i.e. R(cn) \u2192 R(b\u03c1) (possibly in probability). If \u03c8 is explicitly known, then bounds on the excess expected loss yields bounds on the excess misclassification risk. The relaxation error in the binary case has been thoroughly studied in [2, 14]. In particular, Theorem 2 in [2] shows that if a large margin surrogate loss is convex, differentiable and decreasing in a neighborhood of 0, then the loss is Fisher consistent. Moreover, in this case it is possible to give an explicit expression of the function \u03c8. In particular, for the hinge loss the target function is exactly the Bayes rule and \u03c8(t) = |t|. For least squares, f\u03c1(x) = 2\u03c11(x)\u2212 1, and \u03c8(t) = \u221a t. The comparison inequality for the square loss can be improved for a suitable class of probability distribution satisfying the so called Tsybakov noise condition [22], \u03c1X ({x \u2208 X , |f\u03c1(x)| \u2264 s}) \u2264 Bqsq, s \u2208 [0, 1], q > 0. Under this condition the probability of points such that \u03c1y(x) \u223c 12 decreases polynomially. In this case the comparison inequality for the square loss is given by \u03c8(t) = cqt q+1 q+2 , see [2, 27]. Previous Works in Multiclass Classification. From a practical perspective, over the years, several computational solutions to multiclass learning have been proposed. Among others, we mention for example [10, 6, 5, 25, 1, 21]. Indeed, most of the above methods can be interpreted as a kind of relaxation of the original multiclass problem. Interestingly, the study in [15] suggests that the simple one-vs all schemes should be a practical benchmark for multiclass algorithms as it seems to experimentally achive performances that are similar or better to more sophisticated methods. As we previously mentioned from a theoretical perspective a general account of a large class of multiclass methods has been given in [20], building on results in [2] and [28]. Notably, these results show that seemingly intuitive extensions of binary classification algorithms might lead to inconsistent methods. These results, see also [11, 23], are developed in a setting where a classification rule is found by applying a suitable prediction/decoding map to a function f : X \u2192 RT where f is found considering a loss function V : Y \u00d7 RT \u2192 R+. The considered functions have to satisfy the constraint \u2211 y\u2208Y f\ny(x) = 0, for all x \u2208 X . The latter requirement is problematic since it makes the computations in the corresponding algorithms more involved and is in fact often ignored, so that practical algorithms often come with no consistency guarantees. In all the above papers relaxation is studied in terms of Fisher and Bayes consistency and the explicit form of the function \u03c8 is not given. More quantitative results in terms of explicit comparison inequality are given in [4] and (see also [19]), but also need to to impose the \u201dsum to zero\u201d constraint on the considered function class."}, {"heading": "3 A Relaxation Approach to Multicategory Classification", "text": "In this section we propose a natural extension of the relaxation approach that avoids constraining the class of functions to be considered, and allows to derive explicit comparison inequalities. See Remark 1 for related approaches.\nSimplex Coding. We start considering a suitable coding/decoding strategy. A coding map turns a label y \u2208 Y into a code vector. The corresponding decoding map given a vector returns a label in Y . Note that, this is what we implicitly did while treating binary classification encoding the label space Y = {1, 2} using the coding \u00b11, so that the naturally decoding strategy is simply sign(f(x)). The coding/decoding strategy we study is described by the following definition.\nDefinition 1 (Simplex Coding). The simplex coding is a map C : Y \u2192 RT\u22121, C(y) = cy, where the code vectors C = {cy | y \u2208 Y} \u2282 RT\u22121 satisfy: 1) \u2016cy\u20162 = 1, \u2200y \u2208 Y , 2)\u3008cy, cy\u2032\u3009 = \u2212 1T\u22121 , for y 6= y \u2032 with y, y\u2032 \u2208 Y , and\n3) \u2211 y\u2208Y cy = 0. The corresponding decoding is the map D : RT\u22121 \u2192 {1, . . . , T}, D(\u03b1) = arg maxy\u2208Y \u3008\u03b1, cy\u3009 , \u2200\u03b1 \u2208 RT\u22121.\nThe simplex coding corresponds to the T most separated vectors on the hypersphere ST\u22122 in RT\u22121, that is the vertices of the simplex (see Figure 1). For binary classification it reduces to the \u00b11 coding and the decoding map is equivalent to taking the sign of f . The decoding map has a natural geometric interpretation: an input point is mapped to a vector f(x) by a function f : X \u2192 RT\u22121, and hence assigned to the class having closer code vector (for y, y\u2032 \u2208 Y and \u03b1 \u2208 RT\u22121, we have \u2016cy \u2212 \u03b1\u20162 \u2265 \u2016cy\u2032 \u2212 \u03b1\u20162 \u21d4 \u3008cy\u2032 , \u03b1\u3009 \u2264 \u3008cy, \u03b1\u3009. Relaxation for Multiclass Learning. We use the simplex coding to propose an extension of the binary classification approach. Following the binary case, the relaxation can be described in two steps:\n1. using the simplex coding, the indicator function is upper bounded by a non-negative loss function V : Y \u00d7 RT\u22121 \u2192 R+, such that 1I[b(x) 6=y](x, y) \u2264 V (y, C(b(x))),for all b : X \u2192 Y , and x \u2208 X , y \u2208 Y ,\n2. rather than C \u25e6 b we consider functions with values in f : X \u2192 RT\u22121, so that V (y, C(b(x))) \u2264 V (y, f(x)), for all b : X \u2192 Y, f : X \u2192 RT\u22121 and x \u2208 X , y \u2208 Y .\nIn the next section we discuss several loss functions satisfying the above definitions and we study in particular the extension of the least squares and SVM loss functions. Multiclass Simplex Loss Functions. Several loss functions for binary classification can be naturally extended to multiple classes using the simplex coding. Due to space restriction, in this paper we focus on extensions of least squares, and SVM loss functions, but our analysis can be generalized to large class of simplex loss functions, including extension of logistic and exponential loss functions( used in boosting). The Simplex Least Square loss (SLS) is given by V (y, f(x)) = \u2016cy \u2212 f(x)\u20162, and reduces to the usual least square approach to binary classification for T = 2.One natural extension of the SVM\u2019s hinge loss in this setting would be to consider the Simplex Half space SVM loss (SH-SVM) V (y, f(x)) = |1\u2212 \u3008cy, f(x)\u3009|+. We will see in the following that while this loss function would induce efficient algorithms in general is not Fisher consistent unless further constraints are assumed. In turn, this latter constraint would considerably slow down the computations. Then we consider a second loss function Simplex Cone SVM (SC-SVM), related to the hinge loss, which is defined as V (y, f(x)) = \u2211 y\u2032 6=y \u2223\u2223\u2223 1T\u22121 + \u3008cy\u2032 , f(x)\u3009\u2223\u2223\u2223 + . The latter loss function is related to the one considered in the multiclass SVM proposed in [10]. We will see that it is possible to quantify the relaxation error of the loss function without requiring further constraints. Both the above SVM loss functions reduce to the binary SVM hinge loss if T = 2.\nRemark 1 (Related approaches). The simplex coding has been considered in [8],[26], and [16]. In particular, a kind of SVM loss is considered in [8] where V (y, f(x)) = \u2211 y\u2032 6=y |\u03b5\u2212 \u3008f(x), vy\u2032(y)\u3009|+ and vy\u2032(y) = cy\u2212cy\u2032 \u2016cy\u2212cy\u2032\u2016 , with \u03b5 = \u3008cy, vy \u2032(y)\u3009 =\n1\u221a 2 \u221a T T\u22121 . More recently [26] considered the loss function V (y, f(x)) = |\u03b5\u2212 \u2016cy \u2212 f(x)\u2016|+, and a simplex multi-class\nboosting loss was introduced in [16], in our notation V (y, f(x)) = \u2211 j 6=y e\n\u2212\u3008cy\u2212cy\u2032 ,f(x)\u3009. While all those losses introduce a certain notion of margin that makes use of the geometry of the simplex coding, it is not to clear how to derive explicit comparison theorems and moreover the computational complexity of the resulting algorithms scales linearly with the number of classes in the case of the losses considered in [16, 26] and O((nT )\u03b3), \u03b3 \u2208 {2, 3} for losses considered in [8] ."}, {"heading": "4 Relaxation Error Analysis", "text": "If we consider the simplex coding, a function f taking values in RT\u22121, and the decoding operator D, the misclassification risk can also be written as: R(D(f)) = \u222b X (1 \u2212 \u03c1D(f(x)))d\u03c1X (x). Then, following a relaxation approach we replace the misclassification loss by the expected risk induced by one of the loss functions V defined in the previous section. As in the binary case we consider the expected loss E(f) = \u222b V (y, f(x))d\u03c1(x, y). Let\nLp(X , \u03c1X ) = {f : X \u2192 RT\u22121 | \u2016f\u2016p\u03c1 = \u222b \u2016f(x)\u2016p d\u03c1X (x) <\u221e}, p \u2265 1.\nThe following theorem studies the relaxation error for SH-SVM, SC-SVM, and S-LS loss functions.\nTheorem 1. For SH-SVM, SC-SVM, and S-LS loss functions, there exists a p such that E : Lp(X , \u03c1X ) \u2192 R+ is convex and continuous. Moreover,\n1. The minimizer f\u03c1 of E over F = {f \u2208 Lp(X , \u03c1X ) | f(x) \u2208 K a.s.} exists and D(f\u03c1) = b\u03c1.\n2. For any f \u2208 F , R(D(f))\u2212R(D(f\u03c1)) \u2264 CT (E(f)\u2212 E(f\u03c1))\u03b1, where the expressions of p,K, f\u03c1, CT , and \u03b1 are given in Table 1.\nThe proof of this theorem is given in the longer version of the paper. The above theorem can be improved for Least Squares under certain classes of distribution . Toward this end we introduce the following notion of misclassification noise that generalizes Tsybakov\u2019s noise condition.\nDefinition 2. Fix q > 0, we say that the distribution \u03c1 satisfy the multiclass noise condition with parameter Bq , if\n\u03c1X\n({ x \u2208 X | 0 \u2264 min\nj 6=D(f\u03c1(x)) T \u2212 1 T\n( \u2329 cD(f\u03c1(x)) \u2212 cj , f\u03c1(x) \u232a ) \u2264 s }) \u2264 Bqsq, (2)\nwhere s \u2208 [0, 1].\nIf a distribution \u03c1 is characterized by a very large q, then, for each x \u2208 X , f\u03c1(x) is arbitrarily close to one of the coding vectors. For T = 2, the above condition reduces to the binary Tsybakov noise. Indeed, let c1 = 1, and c2 = \u22121, if f\u03c1(x) > 0, 12 (c1 \u2212 c2)f\u03c1(x) = f\u03c1(x), and if f\u03c1(x) < 0, 1 2 (c2 \u2212 c1)f\u03c1(x) = \u2212f\u03c1(x).\nThe following result improves the exponent of simplex-least square to q+1q+2 > 1 2 :\nTheorem 2. For each f \u2208 L2(X , \u03c1X ), if (2) holds, then for S-LS we have the following inequality,\nR(D(f))\u2212R(D(f\u03c1)) \u2264 K (\n2(T \u2212 1) T\n(E(f)\u2212 E(f\u03c1)) ) q+1 q+2 , (3)\nfor a constant K = ( 2 \u221a Bq + 1 ) 2q+2 q+2 .\nRemark 2. Note that the comparison inequalities show a tradeoff between the exponent \u03b1 and the constant C(T ), for S-LS and SVM losses. While the constant is order T for SVM it is order 1 for S-LS, on the other hand the exponent is 1 for SVM losses and 12 for S-LS. The latter could be enhanced to 1 for close to separable classification problems by virtue of the Tsybakov noise condition.\nRemark 3. Comparison inequalities given in Theorems 1 and 2 can be used to derive generalization bounds on the excess misclassification risk. For least square min-max sharp bound, for vector valued regression are easy to derive.Standard techniques for deriving sample complexity bound in binary classification extended for multi-class SVM losses could be found in [7] and could be adapted to our setting. The obtained bound are not known to be tight, better bounds akin to those in [18], will be subject to future work."}, {"heading": "5 Computational Aspects and Regularization Algorithms", "text": "In this section we discuss some computational implications of the framework we presented. Regularized Kernel Methods. We consider regularized methods of the form (1), induced by simplex loss functions and where the hypotheses space is a vector valued reproducing kernel Hilbert spaces (VV-RKHSs) and the regularizer the corresponding norm. See Appendix D.2 for a brief introduction to VV-RKHSs. In the following, we consider a class of kernels such that the corresponding RKHS H is given by the completion of the span {f(x) = \u2211N i=j \u0393(xj , x)aj , aj \u2208 RT\u22121, xi,\u2208 X , \u2200j = 1, . . . , N}, where we note that the coefficients are vectors in RT\u22121. While other choices are possible this is the kernel more directly related to a one vs all approach. We will discuss in particular the case where the kernel is induced by a finite dimensional feature map, k(x, x\u2032) = \u3008\u03a6(x),\u03a6(x\u2032)\u3009 , where \u03a6 : X \u2192 Rp, and \u3008\u00b7, \u00b7\u3009 is the inner product in Rp. In this case we can write each function inH as f(x) = W\u03a6(x), where W \u2208 R(T\u22121)\u00d7p. It is known [12, 3] that the representer theorem [9] can be easily extended to a vector valued setting, so that that minimizer of a simplex version of Tikhonov regularization is given by f\u03bbS (x) = \u2211n j=1 k(x, xj)aj , aj \u2208 RT\u22121, for all x \u2208 X , where the explicit expression of the coefficients depends on the considered loss function. We use the following notations: K \u2208 Rn\u00d7n,Kij = k(xi, xj),\u2200i, j \u2208 {1 . . . n}, A \u2208 Rn\u00d7(T\u22121), A = (a1, ..., an)T . Simplex Regularized Least squares (S-RLS). S-RLS is obtained considering the simplex least square loss in the Tikhonov functionals. It is easy to see [15] that in this case the coefficients must satisfy either (K + \u03bbnI)A = Y\u0302 or (X\u0302T X\u0302 + \u03bbnI)W = X\u0302T Y\u0302 in the linear case, where X\u0302 \u2208 Rn\u00d7p, X\u0302 = (\u03a6(x1), ...,\u03a6(xn))> and Y\u0302 \u2208 Rn\u00d7(T\u22121), Y\u0302 = (cy1 , ..., cyn)\n> . Interestingly, the classical results from [24] can be extended to show that the value fSi(xi), obtained computing the solution fSi removing the i\u2212 th point from the training set (the leave one out solution), can be computed in closed\nform. Let f\u03bbloo \u2208 Rn\u00d7(T\u22121), f\u03bbloo = (f\u03bbS1(x1), . . . , f \u03bb Sn (xn)). Let K(\u03bb) = (K + \u03bbnI)\u22121and C(\u03bb) = K(\u03bb)Y\u0302 . Define M(\u03bb) \u2208 Rn\u00d7(T\u22121), such that: M(\u03bb)ij = 1/K(\u03bb)ii, \u2200 j = 1 . . . T \u2212 1. One can show similarly to [15], that f\u03bbloo = Y\u0302 \u2212 C(\u03bb) M(\u03bb), where is the Hadamard product. Then, the leave-one-out error 1n \u2211n i=1 1Iy 6=D(fSi (x))(yi, xi), can be minimized at essentially no extra cost by precomputing the eigen decomposition of K (or X\u0302T X\u0302). Simplex Cone Support Vector Machine (SC-SVM). Using standard reasoning it is easy to show that (see Appendix C.2), for the SC-SVM the coefficients in the representer theorem are given by ai = \u2212 \u2211 y 6=yi \u03b1 y i cy, i = 1, . . . , n, where \u03b1i = (\u03b1 y i )y\u2208Y \u2208 RT , i = 1, . . . , n, solve the quadratic programming (QP) problem\nmax \u03b11,...,\u03b1n\u2208RT \u221212 \u2211 y,y\u2032,i,j \u03b1yiKijGyy\u2032\u03b1 y\u2032 j + 1 T \u2212 1 n\u2211 i=1 T\u2211 y=1 \u03b1yi  (4) subject to 0 \u2264 \u03b1yi \u2264 C0\u03b4y,yi , \u2200 i = 1, . . . , n, y \u2208 Y\nwhere Gy,y\u2032 = \u3008cy, cy\u2032\u3009 \u2200y, y\u2032 \u2208 Y and C0 = 12n\u03bb , \u03b1i = (\u03b1 y i )y\u2208Y \u2208 RT , for i = 1, . . . , n and \u03b4i,j is the Kronecker delta. Simplex Halfspaces Support Vector Machine (SH-SVM). A similar, yet more more complicated procedure, can be derived for the SH-SVM. Here, we omit this derivation and observe instead that if we neglect the convex hull constraint from Theorem 1, requiring f(x) \u2208 co(C) for almost all x \u2208 X , then the SH-SVM has an especially simple formulation at the price of loosing consistency guarantees. In fact, in this case the coefficients are given by ai = \u03b1icyi , i = 1, . . . , n, where \u03b1i \u2208 R, with i = 1, . . . , n solve the quadratic programming (QP) problem\nmax \u03b11,...,\u03b1n\u2208R \u22121 2 \u2211 i,j \u03b1iKijGyiyj\u03b1j + n\u2211 i=1 \u03b1i\nsubject to 0 \u2264 \u03b1i \u2264 C0, \u2200 i = 1 . . . n,\nwhere C0 = 12n\u03bb . The latter formulation could be trained at the same complexity of the binary SVM (worst case O(n3)) but lacks consistency. Online/Incremental Optimization The regularized estimators induced by the simplex loss functions can be computed by mean of online/incremental first order (sub) gradient methods. Indeed, when considering finite dimensional feature maps, these strategies offer computationally feasible solutions to train estimators for large datasets where neither a p by p or an n by n matrix fit in memory. Following [17] we can alternate a step of stochastic descent on a data point : Wtmp = (1 \u2212 \u03b7i\u03bb)Wi \u2212 \u03b7i\u2202(V (yi, fWi(xi))) and a projection on the Frobenius ball Wi = min(1,\n1\u221a \u03bb||Wtmp||F )Wtmp (See Algorithn C.5 for details.) The algorithm depends on the used loss function through the computation of the (point-wise) subgradient \u2202(V ). The latter can be easily computed for all the loss functions previously discussed. For the SLS loss we have \u2202(V (yi, fW (xi))) = 2(cyi \u2212Wxi)x>i , while for the SCSVM loss we have \u2202(V (yi, fW (xi))) = ( \u2211 k\u2208Ii ck)x > i where Ii = {y 6= yi| \u3008cy,Wxi\u3009 > \u2212 1T\u22121}. For the SH-SVM loss we have: \u2202(V (y, fW (xi))) = \u2212cyix>i if cyiWxi < 1 and 0 else ."}, {"heading": "5.1 Comparison of Computational Complexity", "text": "The cost of solving S-RLS for fixed \u03bb is in the worst case O(n3) (for example via Choleski decomposition). If we are interested into computing the regularization path for N regularization parameter values, then as noted in [15] it might be convenient to perform an eigendecomposition of the kernel matrix rather than solving the systems N times. For explicit feature maps the cost isO(np2), so that the cost of computing the regularization path for simplex RLS algorithm is O(min(n3, np2)) and hence independent of T . One can contrast this complexity with the one of a na\u0308ive One Versus all (OVa) approach that would lead to a O(Nn3T ) complexity. Simplex SVMs can be solved using solvers available for binary SVMs that are considered to have complexity O(n\u03b3) with \u03b3 \u2208 {2, 3}(actually the complexity scales with the number of support vectors) . For SC-SVM, though, we have nT rather than n unknowns and the complexity is (O(nT )\u03b3). SH-SVM where we omit the constraint, could be trained at the same complexity of the binary SVM (worst case O(n3)) but lacks consistency. Note that unlike for S-RLS, there is no straightforward way to compute the regularization path and the leave one out error for any of the above SVMs . The online algorithms induced by the different simplex loss functions are essentially the same, in particular each iteration depends linearly on the number of classes."}, {"heading": "6 Numerical Results", "text": "We conduct several experiments to evaluate the performance of our batch and online algorithms, on 5 UCI datasets as listed in Table 6, as well as on Caltech101 and Pubfig83. We compare the performance of our algorithms to on versus all svm (libsvm) , as well as the simplex based boosting [16]. For UCI datasets we use the raw features, on Caltech101 we use hierarchical features1 , and on Pubfig83 we use the feature maps from [13]. In all cases the parameter selection is based either on a hold out (ho) (80% training \u2212 20% validation) or a leave one out error (loo). For the model Selection of \u03bb in S-LS, 100 values are chosen in the range [\u03bbmin, \u03bbmax],(where \u03bbmin and \u03bbmax, correspond to the smallest and biggest eigenvalues of K). In the case of a Gaussian kernel (rbf) we use a heuristic that sets the width of the gaussian \u03c3 to the 25-th percentile of pairwise distances between distinct points in the training set. In Table 6 we collect the resulting classification accuracies:\nAs suggested by the theory, the consistent methods SC-SVM and S-LS have a big advantage over SH-SVM (where we omitted the convex hull constraint) . Batch methods are overall superior to online methods, with online SC-SVM achieving the best results. More generally, we see that rbf S- LS has the best performance among the simplex methods including the simplex boosting [16]. When compared to One Versus All SVM-rbf, we see that S-LS rbf achieves essentially the same performance."}], "references": [{"title": "Reducing multiclass to binary: a unifying approach for margin classifiers", "author": ["Erin L. Allwein", "Robert E. Schapire", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Optimal rates for regularized least-squares algorithm", "author": ["A. Caponnetto", "E. De Vito"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Consistency of multiclass empirical risk minimization methods based in convex loss", "author": ["D. Chen", "T. Sun"], "venue": "Journal of machine learning, X,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["Thomas G. Dietterich", "Ghulum Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Vc theory of large margin multi-category classiers", "author": ["Yann Guermeur"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "A framework for kernel-based multi-category classification", "author": ["Simon I. Hill", "Arnaud Doucet"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "A correspondence between bayesian estimation of stochastic processes and smoothing by splines", "author": ["G. Kimeldorf", "G. Wahba"], "venue": "Ann. Math. Stat., 41:495\u2013502,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1970}, {"title": "Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data", "author": ["Lee.Y", "L.Yin", "Wahba.G"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "On learning vector\u2013valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation, 17:177\u2013204,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Scaling-up biologically-inspired computer vision: A case-study on facebook", "author": ["N. Pinto", "Z. Stone", "T. Zickler", "D.D. Cox"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Composite binary losses", "author": ["M.D. Reid", "R.C. Williamson"], "venue": "JMLR, 11, September", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiclass boosting: Theory and algorithms", "author": ["Saberian.M", "Vasconcelos .N"], "venue": "In NIPS 2011,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In Proceedings of the 24th ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Information Science and Statistics. Springer, New York,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A moment bound for multicategory support vector machines", "author": ["Van de Geer.S Tarigan.B"], "venue": "JMLR 9,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "On the consistency of multiclass classification methods", "author": ["A. Tewari", "P.L. Bartlett"], "venue": "Proceedings of the 18th Annual Conference on Learning Theory, volume 3559, pages 143\u2013157. Springer,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, 6(2):1453\u20131484,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["Alexandre B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Composite multiclass losses", "author": ["Elodie Vernet", "Robert C. Williamson", "Mark D. Reid"], "venue": "In Proceedings of Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Spline models for observational data, volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics", "author": ["G. Wahba"], "venue": "SIAM, Philadelphia, PA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Support vector machine for multi class pattern recognition", "author": ["Weston", "Watkins"], "venue": "Proceedings of the seventh european symposium on artificial neural networks,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Multicategory vertex discriminant analysis for high-dimensional data", "author": ["Tong Tong Wu", "Kenneth Lange"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "On early stopping in gradient descent learning", "author": ["Y. Yao", "L. Rosasco", "A. Caponnetto"], "venue": "Constructive Approximation, 26(2):289\u2013315,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical analysis of some multi-category large margin classification methods", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research, 5:1225\u20131251,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Tong Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 23, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 11, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 25, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 14, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 268, "endOffset": 272}, {"referenceID": 8, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 4, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 21, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 0, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 17, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 24, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 45, "endOffset": 53}, {"referenceID": 16, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 45, "endOffset": 53}, {"referenceID": 5, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 64, "endOffset": 75}, {"referenceID": 19, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 64, "endOffset": 75}, {"referenceID": 3, "context": "More quantitative results in terms of comparison inequalities are given in [4] under similar restrictions (see also [19]).", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "More quantitative results in terms of comparison inequalities are given in [4] under similar restrictions (see also [19]).", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "A loss function is Fisher consistent if sign(f\u03c1(x)) = b\u03c1(x) almost surely (this property is related to the notion of classification-calibration [2]).", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "The relaxation error in the binary case has been thoroughly studied in [2, 14].", "startOffset": 71, "endOffset": 78}, {"referenceID": 11, "context": "The relaxation error in the binary case has been thoroughly studied in [2, 14].", "startOffset": 71, "endOffset": 78}, {"referenceID": 1, "context": "In particular, Theorem 2 in [2] shows that if a large margin surrogate loss is convex, differentiable and decreasing in a neighborhood of 0, then the loss is Fisher consistent.", "startOffset": 28, "endOffset": 31}, {"referenceID": 18, "context": "The comparison inequality for the square loss can be improved for a suitable class of probability distribution satisfying the so called Tsybakov noise condition [22], \u03c1X ({x \u2208 X , |f\u03c1(x)| \u2264 s}) \u2264 Bqs, s \u2208 [0, 1], q > 0.", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "The comparison inequality for the square loss can be improved for a suitable class of probability distribution satisfying the so called Tsybakov noise condition [22], \u03c1X ({x \u2208 X , |f\u03c1(x)| \u2264 s}) \u2264 Bqs, s \u2208 [0, 1], q > 0.", "startOffset": 205, "endOffset": 211}, {"referenceID": 1, "context": "In this case the comparison inequality for the square loss is given by \u03c8(t) = cqt q+1 q+2 , see [2, 27].", "startOffset": 96, "endOffset": 103}, {"referenceID": 23, "context": "In this case the comparison inequality for the square loss is given by \u03c8(t) = cqt q+1 q+2 , see [2, 27].", "startOffset": 96, "endOffset": 103}, {"referenceID": 8, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 21, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 0, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 17, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 16, "context": "As we previously mentioned from a theoretical perspective a general account of a large class of multiclass methods has been given in [20], building on results in [2] and [28].", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "As we previously mentioned from a theoretical perspective a general account of a large class of multiclass methods has been given in [20], building on results in [2] and [28].", "startOffset": 162, "endOffset": 165}, {"referenceID": 24, "context": "As we previously mentioned from a theoretical perspective a general account of a large class of multiclass methods has been given in [20], building on results in [2] and [28].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "These results, see also [11, 23], are developed in a setting where a classification rule is found by applying a suitable prediction/decoding map to a function f : X \u2192 R where f is found considering a loss function V : Y \u00d7 R \u2192 R.", "startOffset": 24, "endOffset": 32}, {"referenceID": 3, "context": "More quantitative results in terms of explicit comparison inequality are given in [4] and (see also [19]), but also need to to impose the \u201dsum to zero\u201d constraint on the considered function class.", "startOffset": 82, "endOffset": 85}, {"referenceID": 15, "context": "More quantitative results in terms of explicit comparison inequality are given in [4] and (see also [19]), but also need to to impose the \u201dsum to zero\u201d constraint on the considered function class.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "The latter loss function is related to the one considered in the multiclass SVM proposed in [10].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "The simplex coding has been considered in [8],[26], and [16].", "startOffset": 42, "endOffset": 45}, {"referenceID": 22, "context": "The simplex coding has been considered in [8],[26], and [16].", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "The simplex coding has been considered in [8],[26], and [16].", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "In particular, a kind of SVM loss is considered in [8] where V (y, f(x)) = \u2211 y\u2032 6=y |\u03b5\u2212 \u3008f(x), vy\u2032(y)\u3009|+ and vy\u2032(y) = cy\u2212cy\u2032 \u2016cy\u2212cy\u2032\u2016 , with \u03b5 = \u3008cy, vy \u2032(y)\u3009 =", "startOffset": 51, "endOffset": 54}, {"referenceID": 22, "context": "More recently [26] considered the loss function V (y, f(x)) = |\u03b5\u2212 \u2016cy \u2212 f(x)\u2016|+, and a simplex multi-class boosting loss was introduced in [16], in our notation V (y, f(x)) = \u2211 j 6=y e \u2212\u3008cy\u2212cy\u2032 ,f(x)\u3009.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "More recently [26] considered the loss function V (y, f(x)) = |\u03b5\u2212 \u2016cy \u2212 f(x)\u2016|+, and a simplex multi-class boosting loss was introduced in [16], in our notation V (y, f(x)) = \u2211 j 6=y e \u2212\u3008cy\u2212cy\u2032 ,f(x)\u3009.", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "While all those losses introduce a certain notion of margin that makes use of the geometry of the simplex coding, it is not to clear how to derive explicit comparison theorems and moreover the computational complexity of the resulting algorithms scales linearly with the number of classes in the case of the losses considered in [16, 26] and O((nT )), \u03b3 \u2208 {2, 3} for losses considered in [8] .", "startOffset": 329, "endOffset": 337}, {"referenceID": 22, "context": "While all those losses introduce a certain notion of margin that makes use of the geometry of the simplex coding, it is not to clear how to derive explicit comparison theorems and moreover the computational complexity of the resulting algorithms scales linearly with the number of classes in the case of the losses considered in [16, 26] and O((nT )), \u03b3 \u2208 {2, 3} for losses considered in [8] .", "startOffset": 329, "endOffset": 337}, {"referenceID": 6, "context": "While all those losses introduce a certain notion of margin that makes use of the geometry of the simplex coding, it is not to clear how to derive explicit comparison theorems and moreover the computational complexity of the resulting algorithms scales linearly with the number of classes in the case of the losses considered in [16, 26] and O((nT )), \u03b3 \u2208 {2, 3} for losses considered in [8] .", "startOffset": 388, "endOffset": 391}, {"referenceID": 0, "context": "where s \u2208 [0, 1].", "startOffset": 10, "endOffset": 16}, {"referenceID": 5, "context": "Standard techniques for deriving sample complexity bound in binary classification extended for multi-class SVM losses could be found in [7] and could be adapted to our setting.", "startOffset": 136, "endOffset": 139}, {"referenceID": 14, "context": "The obtained bound are not known to be tight, better bounds akin to those in [18], will be subject to future work.", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "It is known [12, 3] that the representer theorem [9] can be easily extended to a vector valued setting, so that that minimizer of a simplex version of Tikhonov regularization is given by f S (x) = \u2211n j=1 k(x, xj)aj , aj \u2208 RT\u22121, for all x \u2208 X , where the explicit expression of the coefficients depends on the considered loss function.", "startOffset": 12, "endOffset": 19}, {"referenceID": 2, "context": "It is known [12, 3] that the representer theorem [9] can be easily extended to a vector valued setting, so that that minimizer of a simplex version of Tikhonov regularization is given by f S (x) = \u2211n j=1 k(x, xj)aj , aj \u2208 RT\u22121, for all x \u2208 X , where the explicit expression of the coefficients depends on the considered loss function.", "startOffset": 12, "endOffset": 19}, {"referenceID": 7, "context": "It is known [12, 3] that the representer theorem [9] can be easily extended to a vector valued setting, so that that minimizer of a simplex version of Tikhonov regularization is given by f S (x) = \u2211n j=1 k(x, xj)aj , aj \u2208 RT\u22121, for all x \u2208 X , where the explicit expression of the coefficients depends on the considered loss function.", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "Interestingly, the classical results from [24] can be extended to show that the value fSi(xi), obtained computing the solution fSi removing the i\u2212 th point from the training set (the leave one out solution), can be computed in closed", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Following [17] we can alternate a step of stochastic descent on a data point : Wtmp = (1 \u2212 \u03b7i\u03bb)Wi \u2212 \u03b7i\u2202(V (yi, fWi(xi))) and a projection on the Frobenius ball Wi = min(1, 1 \u221a \u03bb||Wtmp||F )Wtmp (See Algorithn C.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "We compare the performance of our algorithms to on versus all svm (libsvm) , as well as the simplex based boosting [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "For UCI datasets we use the raw features, on Caltech101 we use hierarchical features1 , and on Pubfig83 we use the feature maps from [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "60% Simplex boosting [16] 86.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "More generally, we see that rbf S- LS has the best performance among the simplex methods including the simplex boosting [16].", "startOffset": 120, "endOffset": 124}], "year": 2012, "abstractText": "In this paper we discuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework, a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized method with training/tuning complexity which is independent to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper.", "creator": "TeX"}}}