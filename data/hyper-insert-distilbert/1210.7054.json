{"id": "1210.7054", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2012", "title": "Large-Scale Sparse Principal Component Analysis with Application to Text Data", "abstract": "thus sparse pca frequently provides a linear combination of small fixed number of mathematical features above that completely maximizes variance accuracy across data. although evaluating sparse discrete pca has hardly apparent advantages compared inherently to complete pca, such concepts as even better sample interpretability, itself it is apparently generally increasingly thought unanimously to be indeed computationally much more inherently expensive. in this paper, first we here demonstrate the surprising fact given that rendering sparse continuous pca methodology can greatly be easier than compact pca in practice, and such that it can be properly reliably smoothly applied close to handling very sufficiently large data sets. this comes from a practically rigorous feature elimination pre - processing result, coupled basically with increasing the favorable fact that features in complex real - continuous life data typically consistently have exponentially decreasing dimensional variances, which technically allows for many features to be essentially eliminated. we introduce a fast block coordinate ascent matrix algorithm with much better computational complexity than the normal existing first - order ones. perhaps we furthermore provide experimental results obtained based on text corpora involving millions of new documents and hundreds of supposedly thousands numbers of features. these results illustrate how sparse textual pca can help infants organize alongside a large corpus of typed text data in a user - interpretable way, providing humans an interesting attractive alternative approach approaches to creating topic models.", "histories": [["v1", "Fri, 26 Oct 2012 05:35:26 GMT  (46kb,D)", "http://arxiv.org/abs/1210.7054v1", "Appeared in the proceedings of NIPS 2011; The Neural Information Processing Systems Conference (NIPS), Granada, Spain, December 2011"]], "COMMENTS": "Appeared in the proceedings of NIPS 2011; The Neural Information Processing Systems Conference (NIPS), Granada, Spain, December 2011", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["youwei zhang", "laurent el ghaoui"], "accepted": true, "id": "1210.7054"}, "pdf": {"name": "1210.7054.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Sparse Principal Component Analysis with Application to Text Data", "authors": ["Youwei Zhang"], "emails": ["zyw@eecs.berkeley.edu", "elghaoui@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "The sparse Principal Component Analysis (Sparse PCA) problem is a variant of the classical PCA problem, which accomplishes a trade-off between the explained variance along a normalized vector, and the number of non-zero components of that vector.\nSparse PCA not only brings better interpretation [1], but also provides statistical regularization [2] when the number of samples is less than the number of features. Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6]. Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10]. These algorithms are based on non-convex formulations, and may only converge to a local optimum. The `1-norm based semidefinite relaxation DSPCA, as introduced in [1], does guarantee global convergence and as such, is an attractive alternative to local methods. In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS and SPCA algorithms, often underperform DSPCA. However, the first-order algorithm for solving DSPCA, as developed in [1], has a computational complexity ofO(n4 \u221a log n), with n the number of\nar X\niv :1\n21 0.\n70 54\nv1 [\nst at\n.M L\nfeatures, which is too high for many large-scale data sets. At first glance, this complexity estimate indicates that solving sparse PCA is much more expensive than PCA, since we can compute one principal component with a complexity of O(n2).\nIn this paper we show that solving DSPCA is in fact computationally easier than PCA, and hence can be applied to very large-scale data sets. To achieve that, we first view DSPCA as an approximation to a harder, cardinality-constrained optimization problem. Based on that formulation, we describe a safe feature elimination method for that problem, which leads to an often important reduction in problem size, prior to solving the problem. Then we develop a block coordinate ascent algorithm, with a computational complexity of O(n3) to solve DSPCA, which is much faster than the firstorder algorithm proposed in [1]. Finally, we observe that real data sets typically allow for a dramatic reduction in problem size as afforded by our safe feature elimination result. Now the comparison between sparse PCA and PCA becomes O(n\u03023) v.s. O(n2) with n\u0302 n, which can make sparse PCA surprisingly easier than PCA.\nIn Section 2, we review the `1-norm based DSPCA formulation, and relate it to an approximation to the `0-norm based formulation and highlight the safe feature elimination mechanism as a powerful pre-processing technique. We use Section 3 to present our fast block coordinate ascent algorithm. Finally, in Section 4, we demonstrate the efficiency of our approach on two large data sets, each one containing more than 100,000 features.\nNotation. R(Y ) denotes the range of matrix Y , and Y \u2020 its pseudo-inverse. The notation log refers to the extended-value function, with log x = \u2212\u221e if x \u2264 0."}, {"heading": "2 Safe Feature Elimination", "text": "Primal problem. Given a n\u00d7 n positive-semidefinite matrix \u03a3, the \u201csparse PCA\u201d problem introduced in [1] is :\n\u03c6 = max Z\nTr\u03a3Z \u2212 \u03bb\u2016Z\u20161 : Z 0, TrZ = 1 (1)\nwhere \u03bb \u2265 0 is a parameter encouraging sparsity. Without loss of generality we may assume that \u03a3 0. Problem (1) is in fact a relaxation to a PCA problem with a penalty on the cardinality of the variable:\n\u03c8 = max x\nxT\u03a3x\u2212 \u03bb\u2016x\u20160 : \u2016x\u20162 = 1 (2)\nWhere \u2016x\u20160 denotes the cardinality (number of non-zero elemements) in x. This can be seen by first writing problem (2) as:\nmax Z\nTr\u03a3Z \u2212 \u03bb \u221a \u2016Z\u20160 : Z 0, TrZ = 1,Rank(Z) = 1\nwhere \u2016Z\u20160 is the cardinality (number of non-zero elements) of Z. Since \u2016Z\u20161 \u2264 \u221a \u2016Z\u20160 \u2016Z\u2016F =\u221a\n\u2016Z\u20160, we obtain the relaxation max Z Tr\u03a3Z \u2212 \u03bb\u2016Z\u20161 : Z 0,TrZ = 1,Rank(Z) = 1\nFurther drop the rank constraint, leading to problem (1).\nBy viewing problem (1) as a convex approximation to the non-convex problem (2), we can leverage the safe feature elimination theorem first presented in [6, 12] for problem (2):\nTheorem 2.1 Let \u03a3 = ATA, where A = (a1, . . . , an) \u2208 Rm\u00d7n. We have\n\u03c8 = max \u2016\u03be\u20162=1 n\u2211 i=1 ((aTi \u03be) 2 \u2212 \u03bb)+.\nAn optimal non-zero pattern corresponds to indices i with \u03bb < (aTi \u03be) 2 at optimum.\nWe observe that the i-th feature is absent at optimum if (aTi \u03be) 2 \u2264 \u03bb for every \u03be, \u2016\u03be\u20162 = 1. Hence, we can safely remove feature i \u2208 {1, . . . , n} if\n\u03a3ii = a T i ai < \u03bb (3)\nA few remarks are in order. First, if we are interested in solving problem (1) as a relaxation to problem (2), we first calculate and rank all the feature variances, which takes O(nm) and O(n log(n)) respectively. Then we can safely eliminate any feature with variance less than \u03bb. Second, the elimination criterion above is conservative. However, when looking for extremely sparse solutions, applying this safe feature elimination test with a large \u03bb can dramatically reduce problem size and lead to huge computational savings, as will be demonstrated empirically in Section 4. Third, in practice, when PCA is performed on large data sets, some similar variance-based criteria is routinely employed to bring problem sizes down to a manageable level. This purely heuristic practice has a rigorous interpretation in the context of sparse PCA, as the above theorem states explicitly the features that can be safely discarded."}, {"heading": "3 Block Coordinate Ascent Algorithm", "text": "The first-order algorithm developed in [1] to solve problem (1) has a computational complexity of O(n4 \u221a log n). With a theoretical convergence rate of O( 1 ), the DSPCA algorithm does not converge fast in practice. In this section, we develop a block coordinate ascent algorithm with better dependence on problem size (O(n3)), that in practice converges much faster.\nFailure of a direct method. We seek to apply a \u201crow-by-row\u201d algorithm by which we update each row/column pair, one at a time. This algorithm appeared in the specific context of sparse covariance estimation in [13], and extended to a large class of SDPs in [14]. Precisely, it applies to problems of the form\nmin X\nf(X)\u2212 \u03b2 log detX : L \u2264 X \u2264 U, X 0, (4)\nwhereX = XT is a n\u00d7nmatrix variable, L,U impose component-wise bounds onX , f is convex, and \u03b2 > 0.\nHowever, if we try to update the row/columns ofZ in problem (1), the trace constraint will imply that we never modify the diagonal elements of Z. Indeed at each step, we update only one diagonal element, and it is entirely fixed given all the other diagonal elements. The row-by-row algorithm does not directly work in that case, nor in general for SDPs with equality constraints. The authors in [14] propose an augmented Lagrangian method to deal with such constraints, with a complication due to the choice of appropriate penalty parameters. In our case, we can apply a technique resembling the augmented Lagrangian technique, without this added complication. This is due to the homogeneous nature of the objective function and of the conic constraint. Thanks to the feature elimination result (Thm. 2.1), we can always assume without loss of generality that \u03bb < \u03c32min := min1\u2264i\u2264n \u03a3ii.\nDirect augmented Lagrangian technique. We can express problem (1) as 1\n2 \u03c62 = max X Tr\u03a3X \u2212 \u03bb\u2016X\u20161 \u2212\n1 2 (TrX)2 : X 0. (5)\nThis expression results from the change of variableX = \u03b3Z, with TrZ = 1, and \u03b3 \u2265 0. Optimizing over \u03b3 \u2265 0, and exploiting \u03c6 > 0 (which comes from our assumption that \u03bb < \u03c32min), leads to the result, with the optimal scaling factor \u03b3 equal to \u03c6. An optimal solution Z\u2217 to (1) can be obtained from an optimal solution X\u2217 to the above, via Z\u2217 = X\u2217/\u03c6. (In fact, we have Z\u2217 = X\u2217/Tr(X\u2217).)\nTo apply the row-by-row method to the above problem, we need to consider a variant of it, with a strictly convex objective. That is, we address the problem\nmax X\nTr\u03a3X \u2212 \u03bb\u2016X\u20161 \u2212 1\n2 (TrX)2 + \u03b2 log detX, : X 0, (6)\nwhere \u03b2 > 0 is a penalty parameter. SDP theory ensures that if \u03b2 = /n, then a solution to the above problem is -suboptimal for the original problem [15].\nOptimizing over one row/column. Without loss of generality, we consider the problem of updating the last row/column of the matrix variable X . Partition the latter and the covariance matrix S as\nX = ( Y y yT x ) , \u03a3 = ( S s sT \u03c3 ) ,\nwhere Y, S \u2208 R(n\u22121)\u00d7(n\u22121), y, s \u2208 Rn\u22121, and x, \u03c3 \u2208 R. We are considering the problem above, where Y is fixed, and (y, x) \u2208 Rn is the variable. We use the notation t := TrY . The conic constraint X 0 translates as yTY \u2020y \u2264 x, y \u2208 R(Y ), where R(Y ) is the range of the matrix Y . We obtain the sub-problem\n\u03c8 := max x,y\n( 2(yT s\u2212 \u03bb\u2016y\u20161) + (\u03c3 \u2212 \u03bb)x\u2212 12 (t+ x) 2\n+\u03b2 log(x\u2212 yTY \u2020y)\n) : y \u2208 R(Y ). (7)\nSimplifying the sub-problem. We can simplify the above problem, in particular, avoid the step of forming the pseudo-inverse of Y , by taking the dual of problem (7).\nUsing the conjugate relation, valid for every \u03b7 > 0: log \u03b7 + 1 = min\nz>0 z\u03b7 \u2212 log z,\nand with f(x) := (\u03c3 \u2212 \u03bb)x\u2212 12 (t+ x) 2, we obtain\n\u03c8 + \u03b2 = max y\u2208R(Y ) 2(yT s\u2212 \u03bb\u2016y\u20161) + f(x) + \u03b2min z>0\n( z(x\u2212 yTY \u2020y)\u2212 log z ) = min\nz>0 max y\u2208R(Y ) 2(yT s\u2212 \u03bb\u2016y\u20161 \u2212 \u03b2zyTY \u2020y) + max x (f(x) + \u03b2zx)\u2212 \u03b2 log z\n= min z>0 h(z) + 2g(z)\nwhere, for z > 0, we define h(z) := \u2212\u03b2 log z + max\nx (f(x) + \u03b2zx)\n= \u2212\u03b2 log z + max x ((\u03c3 \u2212 \u03bb+ \u03b2z)x\u2212 1 2 (t+ x)2)\n= \u22121 2 t2 \u2212 \u03b2 log z + max x ((\u03c3 \u2212 \u03bb\u2212 t+ \u03b2z)x\u2212 1 2 x2)\n= \u22121 2 t2 \u2212 \u03b2 log z + 1 2 (\u03c3 \u2212 \u03bb\u2212 t+ \u03b2z)2\nwith the following relationship at optimum: x = \u03c3 \u2212 \u03bb\u2212 t+ \u03b2z. (8)\nIn addition,\ng(z) := max y\u2208R(Y )\nyT s\u2212 \u03bb\u2016y\u20161 \u2212 \u03b2z\n2 (yTY \u2020y)\n= max y\u2208R(Y ) yT s+ min v : \u2016v\u2016\u221e\u2264\u03bb yT v \u2212 \u03b2z 2 (yTY \u2020y)\n= min v : \u2016v\u2016\u221e\u2264\u03bb max y\u2208R(Y ) (yT (s+ v)\u2212 \u03b2z 2 (yTY \u2020y))\n= min u : \u2016u\u2212s\u2016\u221e\u2264\u03bb max y\u2208R(Y ) (yTu\u2212 \u03b2z 2 (yTY \u2020y))\n= min u : \u2016u\u2212s\u2016\u221e\u2264\u03bb\n1\n2\u03b2z uTY u.\nwith the following relationship at optimum:\ny = 1\n\u03b2z Y u. (9)\nPutting all this together, we obtain the dual of problem (7): with\u03c8\u2032 := \u03c8+\u03b2+ 12 t 2, and c := \u03c3\u2212\u03bb\u2212t, we have\n\u03c8\u2032 = min u,z\n1 \u03b2z uTY u\u2212 \u03b2 log z + 1 2 (c+ \u03b2z)2 : z > 0, \u2016u\u2212 s\u2016\u221e \u2264 \u03bb.\nSince \u03b2 is small, we can avoid large numbers in the above, with the change of variable \u03c4 = \u03b2z:\n\u03c8\u2032 \u2212 \u03b2 log \u03b2 = min u,\u03c4\n1 \u03c4 uTY u\u2212 \u03b2 log \u03c4 + 1 2 (c+ \u03c4)2 : \u03c4 > 0, \u2016u\u2212 s\u2016\u221e \u2264 \u03bb. (10)\nSolving the sub-problem. Problem (10) can be further decomposed into two stages.\nFirst, we solve the box-constrained QP\nR2 := min u uTY u : \u2016u\u2212 s\u2016\u221e \u2264 \u03bb, (11)\nusing a simple coordinate descent algorithm to exploit sparsity of Y . Without loss of generality, we consider the problem of updating the first coordinate of u. Partition u, Y and s as\nu = ( \u03b7 u\u0302 ) , Y = ( y1 y\u0302 T\ny\u0302 Y\u0302\n) , s = ( s1 s\u0302 ) ,\nWhere, Y\u0302 \u2208 R(n\u22122)\u00d7(n\u22122), u\u0302, y\u0302, s\u0302 \u2208 Rn\u22122, y1, s1 \u2208 R are all fixed, while \u03b7 \u2208 R is the variable. We obtain the subproblem\nmin \u03b7\ny1\u03b7 2 + (2y\u0302T u\u0302)\u03b7 : \u2016\u03b7 \u2212 s1\u2016 \u2264 \u03bb (12)\nfor which we can solve for \u03b7 analytically using the formula given below.\n\u03b7 =  \u2212 y\u0302 T u\u0302 y1 if \u2016s1 + y\u0302 T u\u0302 y1 \u2016 \u2264 \u03bb, y1 > 0, s1 \u2212 \u03bb if \u2212 y\u0302 T u\u0302 y1\n< s1 \u2212 \u03bb, y1 > 0 or if y\u0302T u\u0302 > 0, y1 = 0, s1 + \u03bb if \u2212 y\u0302\nT u\u0302 y1 > s1 + \u03bb, y1 > 0 or if y\u0302T u\u0302 <= 0, y1 = 0.\n(13)\nNext, we set \u03c4 by solving the one-dimensional problem:\nmin \u03c4>0\nR2\n\u03c4 \u2212 \u03b2 log \u03c4 + 1 2 (c+ \u03c4)2.\nThe above can be reduced to a bisection problem over \u03c4 , or by solving a polynomial equation of degree 3.\nObtaining the primal variables. Once the above problem is solved, we can obtain the primal variables y, x, as follows. Using formula (9), with \u03b2z = \u03c4 , we set y = 1\u03c4 Y u. For the diagonal element x, we use formula (8): x = c+ \u03c4 = \u03c3 \u2212 \u03bb\u2212 t+ \u03c4 .\nAlgorithm summary. We summarize the above derivations in Algorithm 1. Notation: for any symmetric matrix A \u2208 Rn\u00d7n, let A\\i\\j denote the matrix produced by removing row i and column j. Let Aj denote column j (or row j) with the diagonal element Ajj removed.\nConvergence and complexity. Our algorithm solves DSPCA by first casting it to problem (6), which is in the general form (4). Therefore, the convergence result from [14] readily applies and hence every limit point that our block coordinate ascent algorithm converges to is the global optimizer. The simple coordinate descent algorithm solving problem (11) only involves a vector product and can take sparsity in Y easily. To update each column/row takes O(n2) and there are n such columns/rows in total. Therefore, our algorithm has a computational complexity of O(Kn3), where K is the number of sweeps through columns. In practice, K is fixed at a number independent of problem size (typically K = 5). Hence our algorithm has better dependence on the problem size compared to O(n4 \u221a log n) required of the first order algorithm developed in [1].\nFig 1 shows that our algorithm converges much faster than the first order algorithm. On the left, both algorithms are run on a covariance matrix \u03a3 = FTF with F Gaussian. On the right, the covariance matrix comes from a \u201dspiked model\u201d similar to that in [2], with \u03a3 = uuT +V V T /m, where u \u2208 Rn is the true sparse leading eigenvector, with Card(u) = 0.1n, V \u2208 Rn\u00d7m is a noise matrix with Vij \u223c N (0, 1) and m is the number of observations."}, {"heading": "4 Numerical Examples", "text": "In this section, we analyze two publicly available large data sets, the NYTimes news articles data and the PubMed abstracts data, available from the UCI Machine Learning Repository [16]. Both\nAlgorithm 1 Block Coordinate Ascent Algorithm Input: The covariance matrix \u03a3, and a parameter \u03c1 > 0. 1: Set X(0) = I 2: repeat 3: for j = 1 to n do 4: Let X(j\u22121) denote the current iterate. Solve the box-constrained quadratic program\nR2 := min u uTX (j\u22121) \\j\\j u : \u2016u\u2212 \u03a3j\u2016\u221e \u2264 \u03bb\nusing the coordinate descent algorithm 5: Solve the one-dimensional problem\nmin \u03c4>0\nR2\n\u03c4 \u2212 \u03b2 log \u03c4 + 1 2 (\u03a3jj \u2212 \u03bb\u2212TrX(j\u22121)\\j\\j + \u03c4) 2\nusing a bisection method, or by solving a polynomial equation of degree 3. 6: First set X(j)\\j\\j = X (j\u22121) \\j\\j , and then set both X (j)\u2019s column j and row j using\nX (j) j =\n1 \u03c4 X (j\u22121) \\j\\j u\nX (j) jj = \u03a3jj \u2212 \u03bb\u2212TrX (j\u22121) \\j\\j + \u03c4\n7: end for 8: Set X(0) = X(n) 9: until convergence\ntext collections record word occurrences in the form of bag-of-words. The NYTtimes text collection contains 300, 000 articles and has a dictionary of 102, 660 unique words, resulting in a file of size 1 GB. The even larger PubMed data set has 8, 200, 000 abstracts with 141, 043 unique words in them, giving a file of size 7.8 GB. These data matrices are so large that we cannot even load them into memory all at once, which makes even the use of classical PCA difficult. However with the preprocessing technique presented in Section 2 and the block coordinate ascent algorithm developed in Section 3, we are able to perform sparse PCA analysis of these data, also thanks to the fact that variances of words decrease drastically when we rank them as shown in Fig 2. Note that the feature elimination result only requires the computation of each feature\u2019s variance, and that this task is easy to parallelize.\nBy doing sparse PCA analysis of these text data, we hope to find interpretable principal components that can be used to summarize and explore the large corpora. Therefore, we set the target cardinality for each principal component to be 5. As we run our algorithm with a coarse range of \u03bb to search for\na solution with the given cardinality, we might end up accepting a solution with cardinality close, but not necessarily equal to, 5, and stop there to save computational time.\nThe top 5 sparse principal components are shown in Table 1 for NYTimes and in Table 2 for PubMed. Clearly the first principal component for NYTimes is about business, the second one about sports, the third about U.S., the fourth about politics and the fifth about education. Bear in mind that the NYTimes data from UCI Machine Learning Repository \u201chave no class labels, and for copyright reasons no filenames or other document-level metadata\u201d [16]. The sparse principal components still unambiguously identify and perfectly correspond to the topics used by The New York Times itself to classify articles on its own website.\nAfter the pre-processing steps, it takes our algorithm around 20 seconds to search for a range of \u03bb and find one sparse principal component with the target cardinality (for the NYTimes data in our current implementation on a MacBook laptop with 2.4 GHz Intel Core 2 Duo processor and 2 GB memory).\nA surprising finding is that the safe feature elimination test, combined with the fact that word variances decrease rapidly, enables our block coordinate ascent algorithm to work on covariance matrices of order at most n = 500, instead of the full order (n = 102660) covariance matrix for NYTimes, so as to find a solution with cardinality of around 5. In the case of PubMed, our algorithm only needs to work on covariance matrices of order at most n = 1000, instead of the full order (n = 141, 043)\ncovariance matrix. Thus, at values of the penalty parameter \u03bb that target cardinality of 5 commands, we observe a dramatic reduction in problem sizes, about 150 \u223c 200 times smaller than the original sizes respectively. This motivates our conclusion that sparse PCA is in a sense, easier than PCA itself."}, {"heading": "5 Conclusion", "text": "The safe feature elimination result, coupled with a fast block coordinate ascent algorithm, allows to solve sparse PCA problems for very large scale, real-life data sets. The overall method works especially well when the target cardinality of the result is small, which is often the case in applications where interpretability by a human is key. The algorithm we proposed has better computational complexity, and in practice converges much faster than, the first-order algorithm developed in [1]. Our experiments on text data also show that the sparse PCA can be a promising approach towards summarizing and organizing a large text corpus."}], "references": [{"title": "A direct formulation of sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M. Jordan", "G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["A.A. Amini", "M. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Rotation of principal components: choice of normalization constraints", "author": ["I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Loadings and correlations in the interpretation of principal components", "author": ["J. Cadima", "I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Spectral bounds for sparse PCA: exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "A modified principal component technique based on the LASSO", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Sparse Principal Component Analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational & Graphical Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["Haipeng Shen", "Jianhua Z. Huang"], "venue": "J. Multivar. Anal.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Sparse PCA: Convex relaxations, algorithms and applications", "author": ["Y. Zhang", "A. d\u2019Aspremont", "L. El Ghaoui"], "venue": "Handbook on Semide nite, Cone and Polynomial Optimization: Theory, Algorithms, Software and Applications. Springer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On the quality of a semidefinite programming bound for sparse principal component analysis", "author": ["L. El Ghaoui"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data", "author": ["O.Banerjee", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Row by row methods for semidefinite programming", "author": ["Zaiwen Wen", "Donald Goldfarb", "Shiqian Ma", "Katya Scheinberg"], "venue": "Technical report, Dept of IEOR, Columbia University,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Sparse PCA not only brings better interpretation [1], but also provides statistical regularization [2] when the number of samples is less than the number of features.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Sparse PCA not only brings better interpretation [1], but also provides statistical regularization [2] when the number of samples is less than the number of features.", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 4, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 5, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 6, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "The `1-norm based semidefinite relaxation DSPCA, as introduced in [1], does guarantee global convergence and as such, is an attractive alternative to local methods.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS and SPCA algorithms, often underperform DSPCA.", "startOffset": 30, "endOffset": 40}, {"referenceID": 1, "context": "In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS and SPCA algorithms, often underperform DSPCA.", "startOffset": 30, "endOffset": 40}, {"referenceID": 10, "context": "In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS and SPCA algorithms, often underperform DSPCA.", "startOffset": 30, "endOffset": 40}, {"referenceID": 0, "context": "However, the first-order algorithm for solving DSPCA, as developed in [1], has a computational complexity ofO(n \u221a log n), with n the number of", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Then we develop a block coordinate ascent algorithm, with a computational complexity of O(n) to solve DSPCA, which is much faster than the firstorder algorithm proposed in [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "Given a n\u00d7 n positive-semidefinite matrix \u03a3, the \u201csparse PCA\u201d problem introduced in [1] is : \u03c6 = max Z Tr\u03a3Z \u2212 \u03bb\u2016Z\u20161 : Z 0, TrZ = 1 (1) where \u03bb \u2265 0 is a parameter encouraging sparsity.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "By viewing problem (1) as a convex approximation to the non-convex problem (2), we can leverage the safe feature elimination theorem first presented in [6, 12] for problem (2): Theorem 2.", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "By viewing problem (1) as a convex approximation to the non-convex problem (2), we can leverage the safe feature elimination theorem first presented in [6, 12] for problem (2): Theorem 2.", "startOffset": 152, "endOffset": 159}, {"referenceID": 0, "context": "The first-order algorithm developed in [1] to solve problem (1) has a computational complexity of O(n \u221a log n).", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "This algorithm appeared in the specific context of sparse covariance estimation in [13], and extended to a large class of SDPs in [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "This algorithm appeared in the specific context of sparse covariance estimation in [13], and extended to a large class of SDPs in [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "The authors in [14] propose an augmented Lagrangian method to deal with such constraints, with a complication due to the choice of appropriate penalty parameters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "SDP theory ensures that if \u03b2 = /n, then a solution to the above problem is -suboptimal for the original problem [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "Therefore, the convergence result from [14] readily applies and hence every limit point that our block coordinate ascent algorithm converges to is the global optimizer.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "Hence our algorithm has better dependence on the problem size compared to O(n \u221a log n) required of the first order algorithm developed in [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "On the right, the covariance matrix comes from a \u201dspiked model\u201d similar to that in [2], with \u03a3 = uu +V V T /m, where u \u2208 R is the true sparse leading eigenvector, with Card(u) = 0.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "The algorithm we proposed has better computational complexity, and in practice converges much faster than, the first-order algorithm developed in [1].", "startOffset": 146, "endOffset": 149}], "year": 2012, "abstractText": "Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.", "creator": "LaTeX with hyperref package"}}}