{"id": "1211.2290", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2012", "title": "Dating Texts without Explicit Temporal Cues", "abstract": "this first paper tackles temporal resource resolution measures of memory documents, such endeavors as determining when composing a document is specifically about or assuming when it automatically was spontaneously written, based only on its text. meanwhile we apply techniques distinguished from information retrieval practices that effectively predict important dates via language models over a discretized short timeline. unlike most previous works, we efficiently rely { \\ it solely } reliance on temporal cues frequently implicit repeatedly in the text. importantly we additionally consider available both simplified document - likelihood compensation and divergence based techniques and several smoothing methods algorithms for both of guiding them. our best documented model scenario predicts the mid - term point of individuals'linear lives with a median exposure of 22 * and combined mean error of 36 + years for periodical wikipedia titled biographies conducted from 3800 - b. c. 1700 to the present ) day. meanwhile we also show that this smoothing approach commonly works well when training on such book biographies and predicting dates both account for describing non - textual biographical wikipedia for pages about actual specific years ( 500 b. c. 1910 to february 2010 a. 1990 d. ) and for publication dates of short fictional stories ( 1798 to mid 2008 ). together, our work shows that, even much in absence of temporal extraction resources, it is possible conversely to achieve remarkable local temporal argument locality consistently across physically a diverse set of printed texts.", "histories": [["v1", "Sat, 10 Nov 2012 05:12:31 GMT  (178kb,D)", "http://arxiv.org/abs/1211.2290v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["abhimanu kumar", "jason baldridge", "matthew lease", "joydeep ghosh"], "accepted": false, "id": "1211.2290"}, "pdf": {"name": "1211.2290.pdf", "metadata": {"source": "CRF", "title": "Dating Texts without Explicit Temporal Cues", "authors": ["Abhimanu Kumar", "Jason Baldridge", "Joydeep Ghosh"], "emails": ["abhimank@cs.cmu.edu", "jbaldrid@mail.utexas.edu", "ml@ischool.utexas.edu", "ghosh@ece.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "Temporal analysis of text has been an active area of research since the early days of text mining with different focus in different disciplines. In early computational linguistics research it was primarily concerned with the fine-grained ordering of temporal events (Allen, 1983; Vilain, 1982). Information retrieval research has focused largely on timesensitive document ranking (Dakka et al., 2008; Li\nand Croft, 2003), temporal organization of search results (Alonso et al., 2009), and how queries and documents change over time (Kulkarni et al., 2011).\nThis paper explores temporal analysis models that use ideas present in both computational linguistics and information retrieval. While some prior research has focused on extracting explicit mentions of temporal expressions (Alonso et al., 2009), we investigate the feasibility of using text alone to assign timestamps to documents. Following previous document dating work (de Jong et al., 2005; Kanhabua and N\u00f8rva\u030ag, 2008; Kumar et al., 2011), we construct supervised language models that capture the temporal distribution of words over chronons, which are contiguous atomic time spans used to discretized the timeline. Each chronon model is smoothed by interpolation with the entire training set collection. For each test document, a unigram language model is computed and used to find the document\u2019s similarity with each chronon\u2019s language model. This provides a ranking over chronons for the document, representing the document\u2019s likelihood of being similar to the time periods covered by each chronon (de Jong et al., 2005; Kanhabua and N\u00f8rva\u030ag, 2008).\nOur chronon models are learned from Wikipedia biographies spanning 3800 B.C. to 2010 A.D. Wikipedia-based training is advantageous since its recency enables us to control against stylistic vs. content factors influencing vocabulary use (e.g. consider the difference between William Mavor\u2019s 1796 discussion1 of Sir Walter Raleigh vs. a modern retrospective biography2). This contrasts with re-\n1http://bit.ly/lKR8Aa 2http://en.wikipedia.org/wiki/Walter_\nar X\niv :1\n21 1.\n22 90\nv1 [\ncs .C\nL ]\n1 0\nN ov\nsources such as the Google n-grams corpus (Michel et al., 2010), which is based on publication dates, and thus reflects information about when a document was written rather than what it is about.\nOur methods, all of which use the Wikipedia biographies for training models, are evaluated on three tasks. The first is matched to the training data: predict the mid-point of an individual\u2019s life based on the text in his or her Wikipedia biography. Our best model achieves a median error of 22 years and a mean error of 36 years. The second task is to predict the year for a set of events between 500 B.C. and 2010 A.D., using Wikipedia\u2019s pages for events in each year.3 The best model gives a mean error of 36 years and median error of 21 years. The final task is predicting the publication dates of short stories from the Gutenberg project from the period 1798 to 2008.4 In comparison to biographies, these stories have far fewer mentions of historical named entities that with peaked time signatures useful for prediction. This, plus the difference in genre between Wikipedia biographies (training) and works of fiction, stand to make this task more challenging. However, the distributions learned from the biographies prove to be quite robust here: our best model achieves a mean error of 20 years and a median error of 17 years from the true publication date.\nOur primary contribution is demonstrating the robustness and informativity of the implicit temporal cues available in text alone, across a diverse set of three prediction tasks. We do so for document collections spanning hundreds and thousands of years, whereas previous work has generally focused on relatively short periods (decades) for recent time spans. Note that we use a robust temporal expression identifier for English, Heidel-Time (Stro\u0308tgen and Gertz, 2010), to identify and remove dates from all texts for both training and testing. While one could exploit a resource such as Heidel-Time to perform rule-based document dating (possibly in combination with our methods and others such as (Chambers, 2012)), this work demonstrates that text-based techniques can be used effectively for languages for which such temporal extraction resources are not available (Heidel-\nRaleigh 3http://en.wikipedia.org/wiki/List_of_ years 4http://www.gutenberg.org\nTime has resources only for English, German and Dutch).\nA second contribution is a thorough exploration of the information retrieval approach for this task, including consideration of three different techniques for smoothing chronon language models and a comparison of generative (document-likelihood) and KL-divergence models for identifying the best chronon for a test document. We find that straightforward Jelenik-Mercer smoothing (basic linear interpolation) works the best, and that both document likelihood and KL-divergence based approaches perform similarly.\nA specific task of interest in digital humanities is to identify and visualize text sequences relating to the same time period across a collection of books. Our approach can be used to timestamp subsequences of documents, which could be booklength narratives or fictions, without explicit dates."}, {"heading": "2 Related Work", "text": "Corpora for temporal evaluation. With increased focus on temporal analysis, there have been efforts to create richly annotated corpora to train and evaluate temporal models, e.g. TimeBank (Pustejovsky et al., 2003) and Wikiwars (Mazur and Dale, 2010) were created to provide a common set of corpora for evaluating time-sensitive models. (2011) use the above corpora to resolve geographic and temporal references in text while (2008) use these to model event structures.\nSemantic based temporal models. Timesensitive models have also been developed using semantic properties of data. (Grishman et al., 2002) use semantic properties of web-data to create and automatically update a database on infectious disease outbreaks. Other simpler approaches have been explored to analyse literary and historical documents as well as recent datasets such as tweets and search queries. Time based analysis of historical texts provides important information as to how significant events happened in the past on a temporal scale. The Google N-Grams viewer5, which uses word counts from millions of books and corresponding publication date, provides plots of n-gram word sequences over a timeline (Michel et al., 2010). This gives use-\n5http://ngrams.googlelabs.com/\nful insights into historical trends of events/topics and writing styles. Time based analysis of tweets has gained popularity in recent years especially to capture current trending topics for tracking news items and market sentiment (Zhang et al., 2010).\nTime aware latent models. Another approach for temporal text analysis is latent variable based graphical models. Dynamic Topic Models (Blei and Lafferty, 2006) are used to analyze the evolution of topics over time in a large document collection (Wang et al., 2008). (2006) analyse variations in topic occurrences over a large corpora for a fixed time period. (2008) investigate the history of ideas in a research field though latent variable approaches. (2007) use graphical models for temporal analysis of blogs and Zhang et al.(Zhang et al., 2010) provide clustering techniques for time varying text corpora through hierarchical Dirichlet processes for modeling time sensitivity.\nTemporal analysis using conventional language models. Time based text analysis has been explored using conventional language model based approaches for various applications e.g. time-sensitive query interpretation (Li and Croft, 2003; Dakka et al., 2008), time-based presentation of search results (Alonso et al., 2009), and modelling query and document changes over time (Kulkarni et al., 2011). (Li and Croft, 2003), one of the early temporal language models, use explicit document dates to estimate a more informative document prior. More recently, (2008) propose models for identifying important time intervals likely to be of interest for a query incorporating document publication date into the ranking function. (2009) use explicit temporal metadata and expressions as attributes to cluster documents and create timelines for exploring search results.\nDocument dating\u2014the task of this paper\u2014is a closely related problem. (2005) follow a language model based approach to assign dates to Dutch newspaper articles from 1999-2005 by partitioning the timeline into discrete time periods. (2008) extend this work to incorporate temporal entropy and search statistics from Google Zeitgeist. These approaches (de Jong et al., 2005; Kanhabua and N\u00f8rva\u030ag, 2008) normalize the evidence for each chronon by the whole collection. (2012) improve over these by including linguistic constraints such\nas NERs, POS tagging and regular expression based temporal relation constraints (e.g. \u201cafter\u201d, \u201cbefore\u201d etc.) and using MaxEnt classifier for training. (2012) use linguistics features such as sentence length, context, entity list in a document etc. to discover events over twitter and assign time stamp by framing it as a binary classification problem with the two classes as relevant and non-relevant. But, all these approaches worked for a small time range (6- 10 years) but our datasets span around 5000 years and the evidence would die down after normalization. (2011) use divergence based methods and nonstandard smoothing on Wikipedia biographies for the same task. We perform our experiments on two of their datasets, Wikipedia biographies and Gutenberg short stories, and we compare their smoothing method with standard Jelinek-Mercer and Dirichlet smoothing."}, {"heading": "3 Document Collections", "text": "Our models are trained and evaluated on three datasets 6\nWikipedia biographies (wiki-bio). The Wikipedia dump of English on September 4, 2010 are used 7 to obtain biographies of individuals who lived between the years 3800 B.C. to 2010 A.D.\nWe extract the lifetime of each individual via each article\u2019s Infobox birth date and death date fields. We exclude biographies which do not specify one of the fields or which fall outside the year range considered. If the birth date is missing, we approximate it as 100 years before the death date (similarly and conversely when the death date is missing). We perform this only to estimate the word distributions in the training set. All such documents are discarded for validation and test. We treat the life span of each individual as the article\u2019s labeled time span. Note that the distribution of biographies is quite skewed toward recent times, as shown in Figure 1.\nThe resulting dataset contains a total of 280,867 Wikipedia biographies of individuals whose lifetimes begin and end within the year range consid-\n6All three will be released upon publication, including processing and extraction needed for replication of experiments.\n7http://download.wikimedia. org/enwiki/20100904/ enwiki-20100904-pages-articles.xml. bz2\nered (3800 B.C. to 2010 A.D.). These biographies are randomly split into subsets for training, development, and testing. We remove documents from development and test sets if either their birth date or death date missing. This leaves us with 224,476 training articles, 8,358 development articles and 8,440 test articles.\nWiki-year pages (wiki-year). Wikipedia has a collection of pages corresponding to various years that describe the events that occurred for a given year. 8. Each page has the corresponding year as its label and the text contains all the events that occurred in that year \u2013 some examples are shown in Table 1. Pages for years before 500 B.C. at times contain events that span several years, so we restrict the documents to be those from 500 B.C. to 2010 A.D.9 The 2,511 documents for this span are divided into even years for development (1256 documents) and odd years for testing (1255 documents).\nTable 1 shows random sample lines from four wiki-year pages. The lines are terse and the text as a whole contain very little temporal expressions.\nGutenberg short stories (gutss). We collected 678 English short stories published between 1798 to 2008, obtained from the Gutenberg Project. Whereas with Wikipedia biographies we use labeled time spans corresponding to lifetimes, Gutenberg stories are labeled by publication year. The average, minimum and maximum word count of these stories are (roughly) 14,000, 11,000 and 100,000 respectively. Stories are randomly split into a development and test set of 333 and 345 documents, respectively.\nNotation. We refer to biographies, stories and Wiki-Year pages alike as documents, and each dataset as defining a document collection c consisting of N documents: c = d1:N ."}, {"heading": "4 Model", "text": "Similar to previous work, we represent continuous time via discrete units. Our formalization most closely follows that of Alonso et al. (2009). The\n8http://en.wikipedia.org/wiki/List_of_ years\n9For example the events \u201cProto-Greek invasions of Greece.\u201d, \u201cMinoan Old Palace (Protopalatial) period starts in Crete.\u201d etc. are present in the text for 1878 as well as 1880 B.C. These occurred around 1880 B.C. but their exact occurrence date is unknown.\nsmallest temporal granularity we consider in this work is a single year."}, {"heading": "4.1 Estimation", "text": "Let a span of multiple, contiguous years be some interval \u03c4 = [ys, ye], where ys and ye refer to start and end years, respectively. As noted in \u00a73, we also know the year range covered by each document collection and restrict our overall timeline correspondingly to the span \u03c4o = [y0, yY ), covering a total of yY \u2212 y0 = \u2206 years.\nA chronon is an atomic interval x upon which a discrete timeline is constructed (Alonso et al., 2009). In this paper, a chronon consists of \u03b4 years, where \u03b4 is a tunable parameter. Given \u03b4, the timeline T\u03b4 is decomposed into a sequence of n contiguous, nonoverlapping chronons x = x1:n, where n = \u2206\u03b4 .\nA \u201cpseudo-document\u201d dx is created for each chronon x as the concatenation of all training documents whose labeled span overlaps x. For example, for a chronon size \u03b4=25 years, the biography of Abraham Lincoln (1809-1865) is included in pseudo-documents for each of the chronons representing 1800-1825, 1826-1850, and 1851-1875.\nA chronon model \u0398x is estimated from the pseudo-document dx and smoothed via interpolation with the collection. Chronon models are smoothed in three ways: a) Jelinek-Mercer smoothing (JM) (Zhai and Lafferty, 2004), b) Dirichlet smoothing (Zhai and Lafferty, 2004), and c) chronon-specific smoothing (CS) (Kumar et al., 2011). For all three, for each word w, \u0398\u0302dw can be computed as a mixture of document d and document collection cmaximumlikelihood (ML) estimates:\n\u0398\u0302dw = \u03bb fdw |d| + (1\u2212 \u03bb)f c w |c| , (1)\nwhere fdw and f c w denote the frequency of word w in the document or collection respectively, |d| and |c| are the document and collection lengths, and the parameter \u03bb specifies the smoothing strength. In case of Jelenik-Mercer smoothing, the value of \u03bb is chosen directly via tuning over values from zero to one.\nWith Dirichlet smoothing, \u03bb is chosen as:\n\u03bb = |d| |d|+ \u00b5\n(2)\n\u00b5 is a hyper-parameter tuned on the development set.\nChronon-specific smoothing, in turn, is a special case of Dirichlet smoothing where:\n\u00b5 = \u03be\n|Vdx \u222a Vdi | (3)\nwhere |Vdx\u222aVd| denotes the document-chronon specific vocabulary for some collection document di and pseudo-document dx and \u03be is a prior for hyperparameter \u00b5 that is tuned on the development set."}, {"heading": "4.2 Estimation", "text": "We calculate the affinity between each chronon x and a document d by estimating the discrete distribution P (x|d). In the next section, we use P (x|d) to infer affinity between d and different chronons. The mid-point of (see section 4.1) the most likely chronon is then returned as the predicted year by the model. We define two primary models for estimating P (x|d). The first approach estimates the likelihood of d for each chronon; via Bayes rule, this is combined with a chronon prior to calculate the likelihood of each chronon for d. The second approach ranks chronons based on the divergence between latent unigram distributions P (w|d) and P (w|x) (Lafferty and Zhai, 2001a).\nRanking by document likelihood The language modeling approach for information retrieval was originally formulated as query-likelihood (Ponte and Croft, 1998). For our task, the document is the \u201cquery\u201d for which we wish to rank chronons. We refer to this approach as document-likelihood (DL).\nWe estimate P (x|d) \u221d P (d|x)P (x) via Bayes Rule. Assuming unigram modeling, the likelihood of a test document d is given by:\nP (d|x) = \u220f w\u2208d \u0398xw (4)\nwhere the parameters of \u0398x are estimated from the chronon x\u2019s pseudo-document dx, as described in Section 4.1.\nJust as informed document priors (e.g. PageRank or document length) inform traditional document ranking in information retrieval, an informed prior over chronons has potential to benefit our task as well. We adopt a chronon prior intuitively informed by the distribution of training documents\nover chronons:\nP (x) = |dtrain \u2208 dx|\u2211 \u2200y |dtrain \u2208 dy|\n(5)\nwhere dtrain is a training document, dx is the pseudo-document for chronon x and |dtrain \u2208 dx| is the number of dated training documents overlapping with chronon x.\nRanking by model comparison Zhai and Lafferty (2001b) propose ranking via KL-divergence between a query and each collection document. Kumar et al. (2011) use this approach to compute P (x|d), which is estimated by computing the inverse KL-divergence of x and d and normalizing this value with the sum of inverse divergences with all chronons x1:n:\nP (x|d) = D(\u0398 d||\u0398x)\u22121\u2211\ny\u2208x1:n D(\u0398 d||\u0398y)\u22121\n(6)\nIt is straighforward to see that their formulation is rank equivalent to standard model comparison ranking with negative KL-divergence (de Jong et al., 2005; Kanhabua and N\u00f8rva\u030ag, 2008):\nP (x|d) \u221d D(\u0398d||\u0398x)\u22121 rank= \u2212DKL(\u0398d||\u0398x) (7)\nLafferty and Zhai showed such ranking is equivalent to generating the query (i.e. query-likelihood) assuming a uniform document prior and the query model being estimated by relative frequency (Lafferty and Zhai, 2001b). This means that for our task, if we adopt a uniform prior over chronons and estimate the document model by relative frequency, then KL-ranking and document-likelihood approaches will be rank equivalent.\nPrediction Having determined P (x|d), we choose the midpoint y\u0302 of the most likely chronon; for a chronon x = [y, y+\u03b4], the mid-point is y\u0302 = y+\u03b4/2."}, {"heading": "5 Experimental Setup", "text": "Data. To test the ability of word-based models to predict timestamps for documents, all temporal expressions identified in each document using the Heidel-Time temporal tagger (Stro\u0308tgen and Gertz, 2010) are removed. All numeric tokens and standard\nstopwords are also removed. The remaining tokens produce a vocabulary of 374,973 words for the entire Wikipedia biography corpus. Heidel-Time also provides the first two dates present in the text, which we use as a strong baseline for the biography task.\nTuning and smoothing For each model+task, we tune the parameters \u03b4, \u00b5, \u03be, and \u03bb over the development sets of the corresponding dataset. As in prior work (de Jong et al., 2005; Kanhabua and N\u00f8rva\u030ag, 2008; Kumar et al., 2011), we smooth chronon pseudo-document language models (for all models as well as smoothing techniques) but not document models. While smoothing both may potentially help, smoothing the former is strictly necessary for KL-divergence to prevent division by zero.\nTarget predictions For Wikipedia biographies, the predicted y\u0302 represents the mid-point of the individual\u2019s life span; for wiki-years, it is the year of the events on the page, and for Gutenberg short stories it is the publication date of the story. In later sections we will present the baseline predictions for y\u0302 for each dataset.\nError Measurement When predicting a single year for a document, a natural error measure between the predicted year y\u0302 (mid-point) and the actual year y\u2217 is the difference |y\u0302 \u2212 y\u2217|. We compute this difference for each document, then compute and report the mean y\u0304 and median y\u0303 of differences across documents. Similar distance error measures have also been used with document geolocation (Eisenstein et al., 2010; Wing and Baldridge, 2011).\nBaselines For Wikipedia biographies the first baseline (baseline-ht) is the mid-point of the first two temporal-dates extracted by HeidelTime (Stro\u0308tgen and Gertz, 2010). This is a highly effective baseline since it is often the case in Wikipedia biographies that the first two dates are the birth and death dates. The second baseline for biographies is to always predict the year that has greatest number of biographies spanning it, which is 1915 (baseline-1915). For Gutenberg stories, we take 1903, the midpoint of the range of publication dates (1798-2008) as the baseline (baseline-1903). For wiki-years, the baseline is the midpoint of the prediction range i.e. \u2212500+20102 = 755 (baseline755). This assumes that one knows a rough range of\npossible publication dates, which is reasonable for many applications and thus provides a good reference for comparison.\nWe also report oracle error which is the mean and median error which would occur if a model always picked the correct chronon. This error arises because chronons span multiple years; large chronons in particular will have higher oracle error (but may perform better for actual prediction due to better model estimation)."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Parameter tuning", "text": "We begin with year prediction experiments on the development sets to tune the parameters \u03b4, \u03be or \u00b5. We parametrize \u00b5 as a function of the average chronon size in the training set:\n\u00b5 = d\u03c1c\u0304e (8)\nc\u0304 is a constant whose value is dependent upon the model and the task. The value of \u03c1 is tuned over the validation set.\nFigure 4: Tuning for smoothing parameters (\u03be and \u03bb) over wiki-bio and wiki-years datasets for KL model.\nChoice of chronon size and smoothing parameters. We tune the chronon size (\u03b4) over the validation set and tune the smoothing parameters \u03bb, \u03c1, and \u03be (depending on the type of smoothing) for the best \u03b4 obtained. For \u03b4 tuning we assign an arbitrary value to the smoothing parameter \u03bb. The \u03b4 is tuned for each dataset and KL model with CS and JM smoothings. DL model with Dirichlet/JM smoothing and KL model with Dirichlet smoothing use the same best \u03b4 obtained for KL model with JM smoothing on the respective datasets. For each dataset, model and smoothing triad, the smoothing parameter \u03bb, \u03be, or \u03c1 is tuned. Tuning is performed to minimize the mean error on the development sets. The search space for smoothing parameters \u03be, \u03bb and \u03c1 includes { 1e \u2212 12, 1e \u2212 11, . . . , 0.1, 0.25, 0.75, 0.9, 0.99, . . . , 0.999999999 }\nFigures 2 and 4 shows the tuning of \u03b4 and smoothing parameters (\u03bb for JM and \u03be for CS) for the wiki-bio and wiki-years dataset. All triplets formed by KL/DL model \u00d7 JM/Dirichlet smoothing \u00d7 wiki-years/wiki-bio/gutss dataset use the optimum chronon-size obtained for the respective datasets\nfrom the KL model with JM smoothing. From Figure 4 the mean error curve is generally smooth for \u03bb and \u03be unlike the \u03b4, chronon-size parameter (figure 2). This makes smoothing the LMs robust to a range of values. The \u03b4 has more fluctuation even in the optimal neighborhood, which makes tuning chronon-size more critical. A straightforward strategy to reduce this sensitivity is to smooth chronon models based on the word distributions of neighboring chronons as well as interpolating with the collection model, which we intend to explore in future. The optimal chronon sizes for the three datasets are 10 years for wiki-bio and gutss and 50 years for wiki-year."}, {"heading": "6.2 Test results.", "text": "Table 2 shows the results for the various models on the test sets for all three datasets, using the parameters tuned on the corresponding development sets.\nWiki-bio The models beat both baselines easily. Note that baseline-ht is quite strong for a large number of documents: it gives a median error of zero since over half of the documents have birth and death dates as their first dates. Nonetheless, it fails entirely for many documents, and obviously has limited applicability. The models all reduce error by one half in comparison to baseline-1915. The best model (DL + JM smoothing) achieves a mean error of 37.4 years, which is quite strong given that the prediction range is 5810 years. The mean oracle error for the best model is 2.5 years. The mean and median error was 36.6 and 22.0 years for the best performing model (DL + JM smoothing) on the development set.\nWiki-years The models beat baseline-755 comfortably. Despite the fact that the documents are relatively short and that any given document contains a number of often unrelated events (and thus low counts per word type), the results are in line with those for wiki-bio, with mean error of 37.9 and median error of 20 years for the best models. The mean oracle error, 12.4 years, for this dataset is higher due to the larger chronon size. The KL model with JM smoothing provided the best mean and median error of 36.7 and 21.0 years respectively over development set.\nGutss All models except the one that uses chronon-specific smoothing with KL-divergence outperform baseline-1905 on mean error, and even that one is better on median. Since these are works of fiction with few historical entities mentioned, the mean error of 22.9 and median error of 19.0 of the best models indicate that the approach is quite capable of exploiting implicit temporal cues of basic vocabulary choices. Also, recall that the model is trained on Wikipedia; this demonstrates that this choice of training set works well as the basis for predictions on other domains. The mean oracle error (for chronons of 10 years) is 2.5 years. For the development set, the mean and median error was 20.4 and 17.0 years for the best performing model (DL + JM smoothing)."}, {"heading": "6.3 Output analysis", "text": "Using the output on the development set, we find interesting patterns in the predictions made by the\nmodels and the way they use the words as evidence.\nTime warps (2010) used geotags on Flickr images to identify wormholes\u2014locations that are not physically near but which are nonetheless similar to one another. We observe some similar patterns, in our case time warps, in our dataset. These are particularly prominent in wiki-year documents due to their terseness as these are list of events that happened in a given year. Besides the models trained on wiki-bio set add to this phenomenon as the context for the two datasets are slightly different. A cluster of dev event years from between 250 to 150 A.D. (e.g. wiki-years 234, 214, 152, 156 etc.) are predicted to be in 2nd century B.C. (200 B.C. to 150 B.C.) by our model. These event years are very short with an average length of 40-50 words per document. The discriminatory tokens present in these texts include: Roman, Empire, Kingdom, Han, Dynasty, China, Selucid, Greek, etc.. In the 200-150 B.C. period, all the documents in training set are about Greek/Selucid, Roman and Chinese (mostly from Han dynasty) emperors/personalities (e.g. Attalus I, Eratosthenes, Plautus, Emperor Gaozu of Han, Emperor Hui of Han, Zhang Qian, Emperor Wen of Han etc.) and contain similar prominent terms as the wiki-year event texts. This common collection of terms pushes the model to resolve wiki-year texts to 2nd century B.C. This happens because of the relative frequency of such terms in B.C. and A.D.: although these terms are present in the A.D. chronons, their proportion with respect to other terms is much smaller. Test documents that contain these terms are thus attracted to the B.C. chronons since they have these terms in generally higher proportion.\nAnother interesting cluster is short documents containing similar terms from 200-800 A.D. that are resolved to the mid-6th century A.D. The short wikiyear texts (e.g. years 246, 486, 750, 822, etc.) contain co-occurring set of terms like Byzantine, Empire, Roman, Arab, Conquest, Islam, and Caliphate. These short year events text contain events related to mostly Byzantine wars, emperors, Islamic/Arab conquest, Caliphates etc. These are resolved to the mid-6th century A.D. period that predominantly contains biographies of Islamic Caliphates (e.g. Abd al-Malik, Abu Bakr, Ali, Umar etc.) and Byzantine emperors and prominent personalities (e.g. Mau-\nrice, Fausta, Constans II etc.) which has predominant terms such as: Byzantine, Empire, Caliph, Islam, and conquest.\nDiscriminative Words Table 3 and 4 shows the top and bottom 25 words in the descending order of their strengths, where the predictive strength score of a word w is calculated as average prediction error of all the documents that contain the word w. The majority of the words that are most predictive are uncommon nouns, especially uncommon last names or famous titles e.g. capote, komatsu, and cranmer. Words such as tele, wavelength, electorates, teleplay, sap (the company) also have strong temporal connection as these have never been used before 19th century. The least predictive ones are mostly common words such as goodness, oneself, morality, tub, crates, and lantern. The uncommon words among the least predictive are generally present in just one or two documents for which our model performs very poorly. It is highly likely that these words might be inducing those warps due to their predominance and uniqueness."}, {"heading": "7 Conclusion", "text": "Using words alone, it is possible to identify the time period that a document is about (via the Wikipedia datasets) or the time period in which it was written (via the Gutenberg dataset). In the former case, the presence of named entities dominates the texts, and their names provide strong evidence for particular historical periods. For the latter, the texts are fictional (including science fiction), and they rarely mention historical entities. For these, general terms that are indicative of a given time period dominate the prediction. Interestingly, the models that are used (successfully) for this later task are trained on Wikipedia biographies about historical individuals, but which were written in the last decade.\nThe predictions made by our models provide a natural counterpart to other temporally sensitive models of word choice, such as Dynamic Topic Models (DTMs) (Blei and Lafferty, 2006). DTMs assume that documents are labeled with dates; our model could thus be used to create labels for an otherwise un-dated set of documents which can then be analyzed with DTMs. An important aspect of our work is that it opens opportunities for analyzing\nsub-parts of documents, such as chapters, sections and paragraphs of books. Consider, for example, Samuel Goodrich\u2019s \u201cThe Second Book of History\u201d from 1840, which covers thousands of years of history for many parts of the world.\nOf course, many texts include explicit dates, and exploiting their presence via approaches such as (2012) would only strengthen our predictions. Also, they create opportunities for using weaker, but more pin-pointed, supervision: strings identified as dates with high-confidence can be pivots for learning word distributions. This would obviate the need for labeled training material such as Wikipedia biographies, and thereby enable our methods to be used and adapted for a wide variety of genres. Given decent temporal expression identifiers for other languages, this could be used to bootstrap models for more languages as well."}, {"heading": "Acknowledgments", "text": "This work was partially supported by a grant from the Morris Memorial Trust Fund of the New York Community Trust and a Temple Fellowship."}], "references": [{"title": "Maintaining knowledge about temporal intervals", "author": ["James F. Allen."], "venue": "Commun. ACM, 26(11):832\u2013843, November.", "citeRegEx": "Allen.,? 1983", "shortCiteRegEx": "Allen.", "year": 1983}, {"title": "Clustering and exploring search results using timeline constructions", "author": ["Omar Alonso", "Michael Gertz", "Ricardo Baeza-Yates."], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, CIKM \u201909, pages 97\u2013106, New York, NY,", "citeRegEx": "Alonso et al\\.,? 2009", "shortCiteRegEx": "Alonso et al\\.", "year": 2009}, {"title": "Dynamic topic models", "author": ["David M. Blei", "John D. Lafferty."], "venue": "Proceedings of the 23rd international conference on Machine learning, ICML \u201906, pages 113\u2013120, New York, NY, USA. ACM.", "citeRegEx": "Blei and Lafferty.,? 2006", "shortCiteRegEx": "Blei and Lafferty.", "year": 2006}, {"title": "Jointly combining implicit constraints improves temporal ordering", "author": ["Nathanael Chambers", "Daniel Jurafsky."], "venue": "EMNLP, pages 698\u2013706. ACL.", "citeRegEx": "Chambers and Jurafsky.,? 2008", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Labeling documents with timestamps: Learning from their time expressions", "author": ["Nathanael Chambers."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98\u2013106, Jeju Island, Korea, July. Asso-", "citeRegEx": "Chambers.,? 2012", "shortCiteRegEx": "Chambers.", "year": 2012}, {"title": "Structural and temporal analysis of the blogosphere through community factorization", "author": ["Yun Chi", "Shenghuo Zhu", "Xiaodan Song", "Junichi Tatemura", "Belle L. Tseng."], "venue": "Proceedings of the 13th ACM SIGKDD international conference on Knowledge dis-", "citeRegEx": "Chi et al\\.,? 2007", "shortCiteRegEx": "Chi et al\\.", "year": 2007}, {"title": "Finding wormholes with flickr geotags", "author": ["Maarten Clements", "Pavel Serdyukov", "Arjen P. de Vries", "Marcel J.T. Reinders."], "venue": "Proceedings of the 32nd European conference on Advances in Information Retrieval, ECIR\u20192010, pages 658\u2013661, Berlin, Heidel-", "citeRegEx": "Clements et al\\.,? 2010", "shortCiteRegEx": "Clements et al\\.", "year": 2010}, {"title": "Answering general time sensitive queries", "author": ["Wisam Dakka", "Luis Gravano", "Panagiotis G. Ipeirotis."], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management, CIKM \u201908, pages 1437\u20131438, New York, NY, USA. ACM.", "citeRegEx": "Dakka et al\\.,? 2008", "shortCiteRegEx": "Dakka et al\\.", "year": 2008}, {"title": "Temporal Language Models for the Disclosure of Historical Text", "author": ["Franciska de Jong", "Henning Rode", "Djoerd Hiemstra."], "venue": "Humanities, computers and cultural heritage: Proceedings of the XVIth International Conference of the Association for History and Com-", "citeRegEx": "Jong et al\\.,? 2005", "shortCiteRegEx": "Jong et al\\.", "year": 2005}, {"title": "A latent variable model for geographic lexical variation", "author": ["Jacob Eisenstein", "Brendan O\u2019Connor", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Eisenstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2010}, {"title": "Real-time event extraction for infectious disease outbreaks", "author": ["Ralph Grishman", "Silja Huttunen", "Roman Yangarber."], "venue": "Proceedings of the second international conference on Human Language Technology Research, pages 366\u2013369, San Diego, California. Mor-", "citeRegEx": "Grishman et al\\.,? 2002", "shortCiteRegEx": "Grishman et al\\.", "year": 2002}, {"title": "Studying the history of ideas using topic models", "author": ["David Hall", "Daniel Jurafsky", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201908, pages 363\u2013371, Stroudsburg, PA, USA.", "citeRegEx": "Hall et al\\.,? 2008", "shortCiteRegEx": "Hall et al\\.", "year": 2008}, {"title": "Improving temporal language models for determining time of non-timestamped documents", "author": ["Nattiya Kanhabua", "Kjetil N\u00f8rv\u00e5g."], "venue": "Proceedings of the 12th European conference on Research and Advanced Technology for Digital Libraries, ECDL \u201908, pages", "citeRegEx": "Kanhabua and N\u00f8rv\u00e5g.,? 2008", "shortCiteRegEx": "Kanhabua and N\u00f8rv\u00e5g.", "year": 2008}, {"title": "Understanding temporal query dynamics", "author": ["Anagha Kulkarni", "Jaime Teevan", "Krysta M. Svore", "Susan T. Dumais."], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, WSDM \u201911, pages 167\u2013176, New York, NY,", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Supervised language modeling for temporal resolution of texts", "author": ["Abhimanu Kumar", "Matthew Lease", "Jason Baldridge."], "venue": "Proceeding of the 20th ACM Conference on Information and Knowledge Management (CIKM), pages 2069\u20132072.", "citeRegEx": "Kumar et al\\.,? 2011", "shortCiteRegEx": "Kumar et al\\.", "year": 2011}, {"title": "Document language models, query models, and risk minimization for information retrieval", "author": ["J. Lafferty", "C. Zhai."], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 111\u2013119.", "citeRegEx": "Lafferty and Zhai.,? 2001a", "shortCiteRegEx": "Lafferty and Zhai.", "year": 2001}, {"title": "Document language models, query models, and risk minimization for information retrieval", "author": ["John Lafferty", "Chengxiang Zhai."], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SI-", "citeRegEx": "Lafferty and Zhai.,? 2001b", "shortCiteRegEx": "Lafferty and Zhai.", "year": 2001}, {"title": "Time-based language models", "author": ["Xiaoyan Li", "W. Bruce Croft."], "venue": "Proceedings of the twelfth international conference on Information and knowledge management, CIKM \u201903, pages 469\u2013475, New York, NY, USA. ACM.", "citeRegEx": "Li and Croft.,? 2003", "shortCiteRegEx": "Li and Croft.", "year": 2003}, {"title": "Learning to resolve geographical and temporal references in text", "author": ["Vitor Loureiro", "Ivo Anastcio", "Bruno Martins."], "venue": "Isabel F. Cruz, Divyakant Agrawal, Christian S. Jensen, Eyal Ofek, and Egemen Tanin, editors, GIS, pages 349\u2013352. ACM.", "citeRegEx": "Loureiro et al\\.,? 2011", "shortCiteRegEx": "Loureiro et al\\.", "year": 2011}, {"title": "WikiWars: A new corpus for research on temporal expressions", "author": ["Pawel Mazur", "Robert Dale."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 913\u2013922, Cambridge, MA, October. Association for Computa-", "citeRegEx": "Mazur and Dale.,? 2010", "shortCiteRegEx": "Mazur and Dale.", "year": 2010}, {"title": "Identifying relevant temporal expressions for realworld events", "author": ["Sara Romano Nattiya Kanhabua", "Avar Stewart."], "venue": "Proceedings of The SIGIR 2012 Workshop on Time-aware Information Access, Portland, Oregon.", "citeRegEx": "Kanhabua and Stewart.,? 2012", "shortCiteRegEx": "Kanhabua and Stewart.", "year": 2012}, {"title": "A language modeling approach to information retrieval", "author": ["Jay M. Ponte", "W. Bruce Croft."], "venue": "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR \u201998, pages 275\u2013281, New York,", "citeRegEx": "Ponte and Croft.,? 1998", "shortCiteRegEx": "Ponte and Croft.", "year": 1998}, {"title": "The TimeBank corpus", "author": ["James Pustejovsky", "Patrick Hanks", "Roser Sauri", "Andrew See", "David Day", "Lisa Ferro", "Robert Gaizauskas", "Marcia Lazo", "Andrea Setzer", "Beth Sundheim."], "venue": "Corpus Linguistics, pages 647\u2013656.", "citeRegEx": "Pustejovsky et al\\.,? 2003", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Heideltime: High quality rule-based extraction and normalization of temporal expressions", "author": ["Jannik Str\u00f6tgen", "Michael Gertz."], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval \u201910, pages 321\u2013324, Stroudsburg, PA, USA.", "citeRegEx": "Str\u00f6tgen and Gertz.,? 2010", "shortCiteRegEx": "Str\u00f6tgen and Gertz.", "year": 2010}, {"title": "A system for reasoning about time", "author": ["Marc B. Vilain."], "venue": "David L. Waltz, editor, AAAI, pages 197\u2013201. AAAI Press.", "citeRegEx": "Vilain.,? 1982", "shortCiteRegEx": "Vilain.", "year": 1982}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["Xuerui Wang", "Andrew McCallum."], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201906, pages 424\u2013433, New York,", "citeRegEx": "Wang and McCallum.,? 2006", "shortCiteRegEx": "Wang and McCallum.", "year": 2006}, {"title": "Continuous time dynamic topic models", "author": ["Chong Wang", "David M. Blei", "David Heckerman."], "venue": "David A. McAllester and Petri Myllymki, editors, UAI, pages 579\u2013586. AUAI Press.", "citeRegEx": "Wang et al\\.,? 2008", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Simple supervised document geolocation with geodesic grids", "author": ["Benjamin Wing", "Jason Baldridge."], "venue": "Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, ACL, pages 955\u2013964. The Association for Computer Linguistics.", "citeRegEx": "Wing and Baldridge.,? 2011", "shortCiteRegEx": "Wing and Baldridge.", "year": 2011}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["Chengxiang Zhai", "John Lafferty."], "venue": "ACM Trans. Inf. Syst., 22(2):179\u2013 214.", "citeRegEx": "Zhai and Lafferty.,? 2004", "shortCiteRegEx": "Zhai and Lafferty.", "year": 2004}, {"title": "Evolutionary hierarchical dirichlet processes for multiple correlated time-varying corpora", "author": ["Jianwen Zhang", "Yangqiu Song", "Changshui Zhang", "Shixia Liu."], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data", "citeRegEx": "Zhang et al\\.,? 2010", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In early computational linguistics research it was primarily concerned with the fine-grained ordering of temporal events (Allen, 1983; Vilain, 1982).", "startOffset": 121, "endOffset": 148}, {"referenceID": 24, "context": "In early computational linguistics research it was primarily concerned with the fine-grained ordering of temporal events (Allen, 1983; Vilain, 1982).", "startOffset": 121, "endOffset": 148}, {"referenceID": 7, "context": "Information retrieval research has focused largely on timesensitive document ranking (Dakka et al., 2008; Li and Croft, 2003), temporal organization of search results (Alonso et al.", "startOffset": 85, "endOffset": 125}, {"referenceID": 17, "context": "Information retrieval research has focused largely on timesensitive document ranking (Dakka et al., 2008; Li and Croft, 2003), temporal organization of search results (Alonso et al.", "startOffset": 85, "endOffset": 125}, {"referenceID": 1, "context": ", 2008; Li and Croft, 2003), temporal organization of search results (Alonso et al., 2009), and how queries and documents change over time (Kulkarni et al.", "startOffset": 69, "endOffset": 90}, {"referenceID": 13, "context": ", 2009), and how queries and documents change over time (Kulkarni et al., 2011).", "startOffset": 56, "endOffset": 79}, {"referenceID": 1, "context": "poral expressions (Alonso et al., 2009), we investigate the feasibility of using text alone to assign timestamps to documents.", "startOffset": 18, "endOffset": 39}, {"referenceID": 12, "context": "Following previous document dating work (de Jong et al., 2005; Kanhabua and N\u00f8rv\u00e5g, 2008; Kumar et al., 2011), we construct supervised language models that capture the tempo-", "startOffset": 40, "endOffset": 109}, {"referenceID": 14, "context": "Following previous document dating work (de Jong et al., 2005; Kanhabua and N\u00f8rv\u00e5g, 2008; Kumar et al., 2011), we construct supervised language models that capture the tempo-", "startOffset": 40, "endOffset": 109}, {"referenceID": 12, "context": "This provides a ranking over chronons for the document, representing the document\u2019s likelihood of being similar to the time periods covered by each chronon (de Jong et al., 2005; Kanhabua and N\u00f8rv\u00e5g, 2008).", "startOffset": 156, "endOffset": 205}, {"referenceID": 23, "context": "Note that we use a robust temporal expression identifier for English, Heidel-Time (Str\u00f6tgen and Gertz, 2010), to identify and remove dates from all texts for both training and testing.", "startOffset": 82, "endOffset": 108}, {"referenceID": 4, "context": "While one could exploit a resource such as Heidel-Time to perform rule-based document dating (possibly in combination with our methods and others such as (Chambers, 2012)), this work demonstrates that text-based techniques can be used effectively for languages for which such temporal extraction resources are not available (Heidel-", "startOffset": 154, "endOffset": 170}, {"referenceID": 22, "context": "TimeBank (Pustejovsky et al., 2003) and Wikiwars (Mazur and Dale, 2010) were created to provide a common set of corpora for evaluating time-sensitive models.", "startOffset": 9, "endOffset": 35}, {"referenceID": 19, "context": ", 2003) and Wikiwars (Mazur and Dale, 2010) were created to provide a common set of corpora for evaluating time-sensitive models.", "startOffset": 21, "endOffset": 43}, {"referenceID": 19, "context": ", 2003) and Wikiwars (Mazur and Dale, 2010) were created to provide a common set of corpora for evaluating time-sensitive models. (2011) use the", "startOffset": 22, "endOffset": 137}, {"referenceID": 10, "context": "(Grishman et al., 2002) use semantic properties of web-data to create and automatically update a database on infectious disease outbreaks.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "and market sentiment (Zhang et al., 2010).", "startOffset": 21, "endOffset": 41}, {"referenceID": 2, "context": "Dynamic Topic Models (Blei and Lafferty, 2006) are used to analyze the evolution of topics over time in a large document collection (Wang et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 26, "context": "Dynamic Topic Models (Blei and Lafferty, 2006) are used to analyze the evolution of topics over time in a large document collection (Wang et al., 2008).", "startOffset": 132, "endOffset": 151}, {"referenceID": 29, "context": "(Zhang et al., 2010) provide clustering techniques for time varying text corpora through hierarchical Dirichlet processes for model-", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Dynamic Topic Models (Blei and Lafferty, 2006) are used to analyze the evolution of topics over time in a large document collection (Wang et al., 2008). (2006) analyse variations in topic occurrences over a large corpora for a fixed time period.", "startOffset": 22, "endOffset": 160}, {"referenceID": 2, "context": "Dynamic Topic Models (Blei and Lafferty, 2006) are used to analyze the evolution of topics over time in a large document collection (Wang et al., 2008). (2006) analyse variations in topic occurrences over a large corpora for a fixed time period. (2008) investigate the history of ideas in a research field though latent variable approaches.", "startOffset": 22, "endOffset": 253}, {"referenceID": 2, "context": "Dynamic Topic Models (Blei and Lafferty, 2006) are used to analyze the evolution of topics over time in a large document collection (Wang et al., 2008). (2006) analyse variations in topic occurrences over a large corpora for a fixed time period. (2008) investigate the history of ideas in a research field though latent variable approaches. (2007) use graphical models for temporal analysis of blogs and Zhang et al.", "startOffset": 22, "endOffset": 348}, {"referenceID": 17, "context": "time-sensitive query interpretation (Li and Croft, 2003; Dakka et al., 2008), time-based presentation of search results (Alonso et al.", "startOffset": 36, "endOffset": 76}, {"referenceID": 7, "context": "time-sensitive query interpretation (Li and Croft, 2003; Dakka et al., 2008), time-based presentation of search results (Alonso et al.", "startOffset": 36, "endOffset": 76}, {"referenceID": 1, "context": ", 2008), time-based presentation of search results (Alonso et al., 2009), and modelling query and document changes over time (Kulkarni et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 13, "context": ", 2009), and modelling query and document changes over time (Kulkarni et al., 2011).", "startOffset": 60, "endOffset": 83}, {"referenceID": 17, "context": "(Li and Croft, 2003), one of the early temporal language models, use explicit document dates to estimate a more informative document prior.", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "(Li and Croft, 2003), one of the early temporal language models, use explicit document dates to estimate a more informative document prior. More recently, (2008) propose models for identifying important time intervals likely to be of interest for a", "startOffset": 1, "endOffset": 162}, {"referenceID": 12, "context": "These approaches (de Jong et al., 2005; Kanhabua and N\u00f8rv\u00e5g, 2008) normalize the evidence for each", "startOffset": 17, "endOffset": 66}, {"referenceID": 1, "context": "Our formalization most closely follows that of Alonso et al. (2009). The", "startOffset": 47, "endOffset": 68}, {"referenceID": 1, "context": "A chronon is an atomic interval x upon which a discrete timeline is constructed (Alonso et al., 2009).", "startOffset": 80, "endOffset": 101}, {"referenceID": 28, "context": "(Zhai and Lafferty, 2004), b) Dirichlet smoothing (Zhai and Lafferty, 2004), and c) chronon-specific smoothing (CS) (Kumar et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 28, "context": "(Zhai and Lafferty, 2004), b) Dirichlet smoothing (Zhai and Lafferty, 2004), and c) chronon-specific smoothing (CS) (Kumar et al.", "startOffset": 50, "endOffset": 75}, {"referenceID": 14, "context": "(Zhai and Lafferty, 2004), b) Dirichlet smoothing (Zhai and Lafferty, 2004), and c) chronon-specific smoothing (CS) (Kumar et al., 2011).", "startOffset": 116, "endOffset": 136}, {"referenceID": 15, "context": "The second approach ranks chronons based on the divergence between latent unigram distributions P (w|d) and P (w|x) (Lafferty and Zhai, 2001a).", "startOffset": 116, "endOffset": 142}, {"referenceID": 21, "context": "Ranking by document likelihood The language modeling approach for information retrieval was originally formulated as query-likelihood (Ponte and Croft, 1998).", "startOffset": 134, "endOffset": 157}, {"referenceID": 27, "context": "Ranking by model comparison Zhai and Lafferty (2001b) propose ranking via KL-divergence between a query and each collection document.", "startOffset": 28, "endOffset": 54}, {"referenceID": 14, "context": "Kumar et al. (2011) use this approach to compute P (x|d), which is estimated by computing the inverse KL-divergence of x and d and normalizing this value with the sum of inverse divergences with all chronons x1:n:", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "rank equivalent to standard model comparison ranking with negative KL-divergence (de Jong et al., 2005; Kanhabua and N\u00f8rv\u00e5g, 2008):", "startOffset": 81, "endOffset": 130}, {"referenceID": 16, "context": "query-likelihood) assuming a uniform document prior and the query model being estimated by relative frequency (Lafferty and Zhai, 2001b).", "startOffset": 110, "endOffset": 136}, {"referenceID": 23, "context": "expressions identified in each document using the Heidel-Time temporal tagger (Str\u00f6tgen and Gertz, 2010) are removed.", "startOffset": 78, "endOffset": 104}, {"referenceID": 12, "context": "As in prior work (de Jong et al., 2005; Kanhabua and N\u00f8rv\u00e5g, 2008; Kumar et al., 2011), we smooth chronon pseudo-document language models (for all models as well as smoothing techniques) but not document models.", "startOffset": 17, "endOffset": 86}, {"referenceID": 14, "context": "As in prior work (de Jong et al., 2005; Kanhabua and N\u00f8rv\u00e5g, 2008; Kumar et al., 2011), we smooth chronon pseudo-document language models (for all models as well as smoothing techniques) but not document models.", "startOffset": 17, "endOffset": 86}, {"referenceID": 9, "context": "Similar distance error measures have also been used with document geolocation (Eisenstein et al., 2010; Wing and Baldridge, 2011).", "startOffset": 78, "endOffset": 129}, {"referenceID": 27, "context": "Similar distance error measures have also been used with document geolocation (Eisenstein et al., 2010; Wing and Baldridge, 2011).", "startOffset": 78, "endOffset": 129}, {"referenceID": 23, "context": "Baselines For Wikipedia biographies the first baseline (baseline-ht) is the mid-point of the first two temporal-dates extracted by HeidelTime (Str\u00f6tgen and Gertz, 2010).", "startOffset": 142, "endOffset": 168}, {"referenceID": 2, "context": "The predictions made by our models provide a natural counterpart to other temporally sensitive models of word choice, such as Dynamic Topic Models (DTMs) (Blei and Lafferty, 2006).", "startOffset": 154, "endOffset": 179}], "year": 2012, "abstractText": "This paper tackles temporal resolution of documents, such as determining when a document is about or when it was written, based only on its text. We apply techniques from information retrieval that predict dates via language models over a discretized timeline. Unlike most previous works, we rely solely on temporal cues implicit in the text. We consider both document-likelihood and divergence based techniques and several smoothing methods for both of them. Our best model predicts the mid-point of individuals\u2019 lives with a median of 22 and mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present day. We also show that this approach works well when training on such biographies and predicting dates both for nonbiographical Wikipedia pages about specific years (500 B.C. to 2010 A.D.) and for publication dates of short stories (1798 to 2008). Together, our work shows that, even in absence of temporal extraction resources, it is possible to achieve remarkable temporal locality across a diverse set of texts.", "creator": "TeX"}}}