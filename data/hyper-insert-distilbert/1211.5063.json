{"id": "1211.5063", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2012", "title": "On the difficulty of training recurrent neural networks", "abstract": "training recurrent neural networks function is more troublesome than existing feedforward branching ones because of the vanishing similarity and exploding gradient encoding problems detailed in bengio et al. ( 1994 ). in this paper we attempt to continually understand the elusive fundamental structural issues underlying the rapid exploding rapid gradient classification problem by exploring performing it from an adequately analytical, lacking a geometric representation and alternatively a functional dynamical system perspective. our analysis is used mainly to publicly justify the equally simple yet completely effective solution of norm clipping the rapidly exploded gradient. in the initial experimental coding section, highlighting the comparison between this vanishing heuristic solution solution and standard sgd provides empirical genetic evidence towards testing our hypothesis as well as it it shows that such enabling a unified heuristic is required to reach state far of the big art results on offering a unicode character prediction interactive task presentation and a 2d polyphonic universal music encoding prediction one.", "histories": [["v1", "Wed, 21 Nov 2012 15:40:11 GMT  (353kb,D)", "http://arxiv.org/abs/1211.5063v1", null], ["v2", "Sat, 16 Feb 2013 00:35:48 GMT  (447kb,D)", "http://arxiv.org/abs/1211.5063v2", "Improved description of the exploding gradient problem and description and analysis of the vanishing gradient problem"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["razvan pascanu", "tomas mikolov", "yoshua bengio"], "accepted": true, "id": "1211.5063"}, "pdf": {"name": "1211.5063.pdf", "metadata": {"source": "CRF", "title": "Understanding the exploding gradient problem", "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "emails": ["r.pascanu@gmail.com", "imikolov@fit.vutbr.cz", "bengioy@iro.umontreal.ca"], "sections": [{"heading": null, "text": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective. Our analysis is used to justify the simple yet effective solution of norm clipping the exploded gradient. In the experimental section, the comparison between this heuristic solution and standard SGD provides empirical evidence towards our hypothesis as well as it shows that such a heuristic is required to reach state of the art results on a character prediction task and a polyphonic music prediction one."}, {"heading": "1 Introduction", "text": "A recurrent neural network (RNN), see Figure 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series. The structure of the network is similar to that of a standard multilayer perceptron, with the distinction that we allow connections among hidden units associated with a time delay. Through these connections the model can retain information about the past inputs, enabling it to discover temporal correlations between events that are possibly far away from each other in the data (a crucial property for proper learning of time series).\nWhile in principle the recurrent network is a simple and powerful model, in practice, it is unfortunately hard to train properly. A plethora of training algorithms have been proposed in the literature, like Backpropagation Through Time (BPTT) (Rumelhart et al., 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc. Most of these are gradient based, though alternative approaches are also available (for example see Lukos\u030cevic\u030cius and Jaeger (2009)), and as shown in Atiya and Parlos (2000) most gradient-based methods behave qualitatively the same, providing little success in properly addressing complex tasks. Among the main reasons why this model is so unwieldy are the vanishing gradient and exploding gradient problems described in Bengio et al. (1994).\nIn this paper we will only address the exploding gradient problem and provide an understanding of the issues stochastic gradient descent (SGD) faces when training a recurrent network. We should note\nar X\niv :1\n21 1.\n50 63\nv1 [\ncs .L\nG ]\n2 1\nthat there are a few solutions proposed for this problem in the literature, amongst which the HessianFree learning (Sutskever et al., 2011), a second order method that seems promising, though more needs to be done to analyze its success, especially compared to other second order methods that seem to do less well in general. Long Short Term Memory networks (Hochreiter and Schmidhuber, 1997) is another approach, relying on a change in the structure of the model, and designed to help with the vanishing gradient problem. The extend to which this alteration limits the modeling power of the model is not clear, nor does the approach address the exploding gradient problem. We do not intend to directly compete with these solutions, but rather improve the limited understanding of why such approaches are required. To validate some of our hypotheses we devise a simple alteration of SGD which, given its simplicity, can be seen as a reliable alternative to SGD learning.\nThe structure of the paper is as follows. In subsection 1.1 we briefly formalise the concept of training recurrent networks. In section 2 we introduce and analyze the exploding gradient problem, while in section 3 we describe our proposed solution. Section 4 provides empirical experimentation on different datasets and finally in section 5 we have some final remarks."}, {"heading": "1.1 Training recurrent networks", "text": "A generic recurrent neural network is given by equation (1). In the theoretical section of this paper we will sometimes make use of the specific parametrization given by equation (2) 1 in order to provide more precise conditions and intuitions about the everyday use-case.\nxt = F (xt\u22121,ut, \u03b8) (1)\nxt = Wrec\u03c3(xt\u22121) + Winut + b (2)\nThe parameters of the model are given by the recurrent weight matrix Wrec, the biases b and input weight matrix Win, collected in \u03b8 for the general case. xt and ut represent the state and the input at time t, where x0 is provided by the user or set to zero (or learned), and \u03c3 is an element-wise function (usually\n1 This formulation is equivalent to the more widely known equation xt = \u03c3(Wrecxt\u22121 +Winut + b), and it was chosen for convenience.\nthe sigmoid or tanh). A cost E measures the performance of the network on some given task and it can be broken apart into individual costs for each step E = \u22111\u2264t\u2264T Et, where Et = L(xt).\nOne approach that can be used to compute the necessary gradients for learning is to represent the recurrent model as a deep multi-layer one (with an unbounded number of layers) and apply backpropagation on the unrolled model (see Figure 2).\nWe will diverge from the classical BPTT equations at this point and re-write the gradients (see equations (3), (4) and (5)) in order to better highlight the exploding gradient problem.\n\u2202E \u2202\u03b8\n= \u2211\n1\u2264t\u2264T\n\u2202Et \u2202\u03b8\n(3)\n\u2202Et \u2202\u03b8\n= \u2211\n1\u2264k\u2264t\n( \u2202Et \u2202xt \u2202xt \u2202xk \u2202+xk \u2202\u03b8 ) (4)\n\u2202xt \u2202xk = \u220f t\u2265i>k \u2202xi \u2202xi\u22121 = \u220f t\u2265i>k WTrecdiag(\u03c3 \u2032(xi\u22121))\n(5) where \u2202+ represents the partial derivative of the state at some point in time with respect to some parameter as a direct argument, i.e., without considering the recurrent path (i.e. when computing the derivative of xk with respect to \u03b8 we ignore the fact that xk is a function of xk\u22121 which in turn is a function of \u03b8). Equation (5) also provides the form of \u2202xt\u2202xk for the\nspecific parametrization given in equation (2), where diag converts a vector into a diagonal matrix, and \u03c3\u2032 computes the derivative of \u03c3 in an element-wise fashion.\nNote that each term \u2202Et\u2202\u03b8 from equation (3) has the same form and the behaviour of these individual terms determine the behaviour of the sum. Henceforth we will focus on one such generic term, calling it simply the gradient when there is no confusion.\nAny gradient component \u2202Et\u2202\u03b8 is also a sum (see equation (4)), whose terms we refer to as temporal contributions or temporal components. One can see that each such temporal contribution \u2202Et\u2202xt \u2202xt \u2202xk \u2202+xk \u2202\u03b8 measures how \u03b8 at step k affects the cost at step t. The factors \u2202xt\u2202xk (equation (5)) transport the error \u201cin time\u201c from step k to step t. We would further loosely distinguish between long term and short term contributions, where long term refers to components for which k t and short term to everything else."}, {"heading": "2 Exploding Gradients", "text": "As introduced in Bengio et al. (1994), the exploding gradient problems refers to the large increase in the norm of the gradient during training. Such events are caused by the explosion of the long term components, which can grow exponentially more then short term ones."}, {"heading": "2.1 The mechanics", "text": "To understand this phenomenon we need to look at the form of each temporal component, and in particular at the factors \u2202xt\u2202xk (see equation (5)) that take the form of a product of l Jacobian matrices, with l = t \u2212 k. Intuitively, these products can grow exponentially fast with l (in some direction v), leading to the explosion of long term components when l is large. Because the gradient is just a sum of these components, it follows that it should also grow exponentially fast following the long term component with k = 0 (for which l = t). In what follows we will try to formalize these intuitions (extending a similar derivation done in Bengio et al. (1994) where only a single hidden unit case was considered)."}, {"heading": "2.2 Linear model", "text": "Let us consider the term gTk = \u2202Et \u2202xt \u2202xt \u2202xk \u2202+xk \u2202\u03b8 for the linear version of the parametrization in equation (2) (i.e. set \u03c3 to the identity function) and assume t goes to infinity. We have that:\n\u2202xt \u2202xk\n= ( WTrec )l (6)\nBy employing the same approach as the power iteration method we can show that, given certain conditions, \u2202Et\u2202xt ( WTrec )l grows exponentially.\nProof Let Wrec have the eigenvalues \u03bb1, .., \u03bbn with |\u03bb1| > |\u03bb2| > .. > |\u03bbn| and the corresponding eigenvectors q1,q2, ..,qn which form a vector basis. We can now write the row vector \u2202Et\u2202xt into this basis:\n\u2202Et \u2202xt = \u2211N i=1 ciq T i\nIf j is such that cj 6= 0 and any j\u2032 < j, cj\u2032 = 0, using the fact that qTi ( WTrec )l = \u03bbliq T i we have that\n\u2202Et \u2202xt \u2202xt \u2202xk = cj\u03bb l jq T j + \u03bb l j n\u2211 i=j+1 ci \u03bbli \u03bblj qTi \u2248 cj\u03bbljqTj (7)\nWe used the fact that \u2223\u2223\u03bbi/\u03bbj \u2223\u2223 < 1 for i > j, which\nmeans that liml\u2192\u221e \u2223\u2223\u03bbi/\u03bbj \u2223\u2223l = 0. If |\u03bbj | > 1, it follows that \u2202xt\u2202xk grows exponentially fast with l, and it does so along the direction qj .\nThe proof assumes Wrec is diagonalizable for simplicity, though using the Jordan normal form of Wrec one can extend this proof by considering not just the eigenvector of largest eigenvalue but the whole subspace spanned by the the eigenvectors sharing the same (largest) eigenvalue.\nThis result provides a necessary condition for gradients to grow, namely that the spectral radius (the absolute value of the largest eigenvalue) of Wrec must be larger than 1.\nIf qj is not in the null space of \u2202+xk \u2202\u03b8 the entire temporal component grows exponentially with l. This approach extends easily to the entire gradient. If we re-write it in terms of the eigen-decomposition of W, we get:\n\u2202Et \u2202\u03b8 = n\u2211 j=1 ( t\u2211 i=k cj\u03bb t\u2212k j q T j \u2202+xk \u2202\u03b8 ) (8)\nWe can now pick j and k such that cjq T j \u2202+xk \u2202\u03b8 does not have 0 norm, while maximizing |\u03bbj |. If for the chosen j it holds that |\u03bbj | > 1 then \u03bbt\u2212kj cjqTj \u2202 +xk \u2202\u03b8 will dominate the sum and because this term grows exponentially fast to infinity with t, the same will happen to the sum."}, {"heading": "2.3 Nonlinear model", "text": "To generalize this proof to the nonlinear case, we define the concept of expanding and non-expanding matrices for some direction v. We say that the Jacobian matrix \u2202xi\u2202xi\u22121 expands along a vector v by \u03b1 > 1 if equation (9) holds.\n\u2200u, \u2223\u2223\u2223\u2223uT \u2202xi\u2202xi\u22121v \u2223\u2223\u2223\u2223 > \u03b1|uTv| (9) Intuitively, to achieve exponential growth, it is sufficient that most of the Jacobians \u2202xi\u2202xi\u22121 are expanding, such that their product expands exponentially with their number and also that there is no Jacobian matrix that kills off these exponentially large increases.\nWe formalize this by constructing the set P of matrices that are non-expanding, and considering a lower bound \u03b2 > 0 on how much these matrices shrink a vector in the direction v (see equation (10)).\n\u2200u, \u2223\u2223\u2223\u2223uT \u2202xi\u2202xi\u22121v \u2223\u2223\u2223\u2223 > \u03b2|uTv|, if \u2202xi\u2202xi\u22121 \u2208 P (10) This means that if \u03b1 is the least amount by which any matrix \u2202xi\u2202xi\u22121 6\u2208 P expands, \u2202xt \u2202xk should expand roughly by \u03b2|P |\u03b1t\u2212|P |. If the cardinality of P is bounded as t grows, it means this product grows exponentially fast with t\u2212 |P |.\nIt is worth mentioning that \u03b1 is bounded by the spectral radius of each matrix \u2202xi\u2202xi\u22121 (which is easy to\nsee as \u2225\u2225\u2225 \u2202xi\u2202xi\u22121v\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225 \u2202xi\u2202xi\u22121 \u2225\u2225\u2225 \u2016v\u2016). If we consider the parametrization in equation (2), this spectral radius is in its turn bounded by the product of the spectral radii \u03c1Wrec of Wrec and \u03c1\u03c3\u2032 of diag(\u03c3\n\u2032(xi\u22121)). We know that \u03c1\u03c3\u2032 < 1 for tanh and \u03c1\u03c3\u2032 <\n1/4 for sigmoid, and hence we recover the necessary condition for the gradients to explode, namely that \u03c1Wrec > 1 (with the tighter version for the sigmoid, \u03c1Wrec > 4).\nWe also need for equations (11) and (12) to hold, where the first equation implies that our chosen direction v is not in the null space of \u2202 +xk \u2202\u03b8 , while the second equation writes the vector \u2202Et\u2202xt in a orthonormal vector basis v1, ..,vN , where v1 = v and c (loss) i \u2208 R.\n\u2200u \u2208 RN , |uT \u2202 +xk \u2202\u03b8 v| \u2265 \u03b3k|uTv|, \u03b3k > 0 (11)\n\u2202Et \u2202xt = N\u2211 j=1 c (loss) j v T j , c (loss) 1 6= 0 (12)\nUsing these relations we can find a lower bound for |gTk v|, where gk is the temporal component corresponding to time step k. Equation (13) shows a few steps of this derivation, where without loss of generality we assigned the first element to P , but not the second one.\n|gTk v1| \u2265 \u2223\u2223\u2223( \u2202Et\u2202xt \u2202xt\u2202xk) \u2202+xk\u2202\u03b8 v1\u2223\u2223\u2223\n\u2265 \u03b3k \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 ( \u2202Et \u2202xt \u2202xt \u2202xk+1 ) \u2208P\ufe37 \ufe38\ufe38 \ufe37\u2202xk+1 \u2202xk v1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u03b3k\u03b2\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 ( \u2202Et \u2202xt \u2202xt \u2202xk+2 ) 6\u2208P\ufe37 \ufe38\ufe38 \ufe37\u2202xk+2 \u2202xk+1 v1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2265 \u03b3k\u03b2\u03b1\n\u2223\u2223\u2223( \u2202Et\u2202xt \u2202xt\u2202xk+3) \u2202xk+3\u2202xk+2v1\u2223\u2223\u2223 ..\n\u2265 \u03b3k\u03b2|P |\u03b1l\u2212|P | \u2223\u2223\u2223 \u2202Et\u2202xtv1\u2223\u2223\u2223 \u2265 \u03b3k\u03b2|P |\u03b1l\u2212|P ||c(loss)1 | \u2265 Ck\u03b1l\u2212|P |\n(13) Equation (13) ensures that long term components explode along v as long as the coefficient Ck (where Ck = \u03b3k\u03b2 |P ||c(loss)1 |) does not go exponentially fast to 0. This is mostly a constraint on \u03b3k (since |P | is bounded from our initial assumption), which is determined by \u2202 +xk \u2202\u03b8 . For a classical parametrization of the model the norm of the partial derivative \u2202 +xk \u2202\u03b8 is determined by the norm of the state and input at time k, where the constraint on the state roughly\ntranslates into not having the state going towards its saturated state faster than \u03b1l\u2212|P | (which can be satisfied for tanh and sigmoid).\nWe summarize this derivation by saying that what we achieve is to provide a lower bound of the gradient when it explodes, bound that grows exponentially with t. Further more, if we consider v such that we maximize \u03b1, given that |P | is fixed, in the limit of t\u2192\u221e the bound becomes tight and we can approximate the gradient by Ck\u03b1 t\u2212|P |v."}, {"heading": "2.4 The geometrical interpretation", "text": "Let us consider a simple one hidden unit model (equation (14)) where we provide an initial state x0 and train the model to have a specific target value after 50 steps. Note that for simplicity we assume no input.\nxt = w\u03c3(xt\u22121) + b (14)\nFigure 3 shows the error surface E50 = (\u03c3(x50) \u2212 0.7)2, where we use the initial state x0 = .5.\nSection 2.1 shows that when the gradient explodes, it is bounded by C\u03b1kv (see equation (13)). If we\nassume that this bound is tight, we can derive that when gradients explode so does the curvature along v, leading to a wall in the error surface as the one seen in 3.\nThis provides us with a hypothesis, which if it holds, gives us a simple solution to the exploding gradient problem depicted in 3.\nIf both the gradient and the leading eigenvector of the curvature are aligned with the exploding direction v, it follows that the error surface has a steep wall perpendicular to v (and consequently to the gradient). This means that when SGD reaches the wall and does a gradient descent step, it will be forced to jump across the valley moving perpendicular to the steep walls, possibly leaving the valley and disrupting the learning process.\nThe dashed arrows in Figure 3 correspond to ignoring the norm of this large step, ensuring that the model stays close to the wall. The key insight is that all the steps taken when the gradient explodes are aligned with v and ignore other descent direction (i.e. the model moves perpendicular to the wall). At the wall, small-norm step in the direction of the gradient therefore only pushes us back inside the smoother low-curvature region besides the wall. In that region, SGD is free to explore other descent directions.\nThe important addition in this scenario to the classical high curvature valley, is that we assume that the valley is wide, as we have a large region around the wall where if we land we can rely on first order methods to move towards the local minima. This justifies why just clipping the gradient might be sufficient and we are not necessarily constraint to use a second order method.\nOur hypothesis could also help to understand the recent success of the Hessian-Free approach compared to other second order methods. There are two key differences between Hessian-Free and most other second-order algorithms. First, it uses the full Hessian matrix and hence can deal with exploding directions that are not necessarily axis aligned. Second, it computes a new estimate of the Hessian matrix before each update step, which can take into account abrupt changes in curvature (such as the ones suggested by our hypothesis) while most other approaches rely on a smoothness assumption, averaging 2nd order signals\nover many steps."}, {"heading": "2.5 Drawing similarities with Dynamical Systems", "text": "One can consider yet another perspective, namely that of dynamical systems. Recurrent networks are universal approximators of dynamical systems (see for e.g. Siegelmann and Sontag (1995)) and as such it is sometimes useful to analyze them using dynamical systems tools which allow for better abstractions that we can use for reasoning about the behaviour of the model. Looking at dynamical systems theory for explaining the exploding gradient problem has been done before in Doya (1993); Bengio et al. (1993). In this paper we will extend and improve these previous observations.\nFor any parameter assignment \u03b8, depending on the initial state x0, the xt of an autonomous dynamical system converges, under the repeated application of the map F , to one of several possible different attractor states (e.g. point attractors). They describe the asymptotic behaviour of the model. The state space is divided into basins of attraction, one for each attractor. If the model is started in one basin of attraction it will converge to the corresponding attractor.\nThe next step is to consider how \u03b8 affects the asymptotic behaviour. Dynamical systems theory tells us that as \u03b8 changes slowly, the asymptotic behaviour changes smoothly almost everywhere except for certain crucial points where drastic changes occur (the new asymptotic behaviour is no more topologically equivalent with the old one). These points are called bifurcation boundaries and are caused by attractors that appear, disappear or change shape.\nSpecifically, if we re-consider the simple model defined by equation (14) from section 2.4, where we fix w to 5.0 and allow b to change we get its bifurcation diagram (with respect to b) of Figure 4. Such diagrams convey an abstract but complete picture of how the system can behave.\nThe x-axis covers the parameter b and the y-axis the asymptotic state x\u221e. The bold line follows the movement of the final point attractor, x\u221e, as b changes. At b1 we have a bifurcation boundary where a new attractor emerges (when b decreases from \u221e),\nwhile at b2 we have another that results in the disappearance of one of the two attractors. In the interval (b1, b2) we are in a rich regime, where there are two attractors and the change in position of boundary between them, as we change b, is traced out by a dashed line. The vector field (gray dashed arrows) describe the evolution of the state x if the network is initialized in that region.\nWe show that there are two types of events that could lead to a large change in xt, with t\u2192\u221e. One is crossing a boundary between basin of attraction (depicted with a unfilled circles), while the other is crossing a bifurcation boundary (filled circles). For large t, the \u2206xt resulting from a change in b will be large even for very small changes in b (as the system is attracted towards different attractors) which leads to a large gradient. In practice it is sufficient to be close to such a boundary for the gradient to be quite large (as F is smooth and hence it needs to gradually change direction, with the norm ||F \u2032|| of the Jacobian of F being 0 on the boundary).\nUsing these notions we can define a necessary and sufficient condition for the gradients to explode. The condition is for a boundary between basins of attraction to be crossed either by a change in x0 or a change in \u03b8. Not that we take the non-conventional approach of considering a change in \u03b8 to cause switching between basins of attraction by changing the position of the border. When crossing a bifurcation bound-\nary that leads to large change in xt, by an abuse of language, we say a boundary between basins of attractions was also crossed implicitly (as for e.g. the boundary between the old attractor and the emerging one), allowing us to unify the two different scenarios.\nIn Doya (1993) only crossing bifurcation boundaries are considered ignoring changes in the state or the position of the state relative a borders between basin of attraction. We argue that this is an incomplete view. Crossing a bifurcation implies a global change, but locally things could stay the same (i.e., after the bifurcation we can find ourselves in the same basin of attraction). Also a change in \u03b8 means a change in the position of the boundary between basins of attractions which could lead to crossing such a boundary, a scenario that is not considered by analyzing only bifurcations. Therefore we consider our proposed view more lucrative.\nAnother limitation of previous analysis is that they only consider autonomous systems. We propose to extend our analysis to input driven model by folding the input into the map. We consider the family of maps Ft, where we apply a different Ft at each step. Intuitively, for the gradients to explode we require the same behaviour as before, where (at least in some direction) the maps F1, .., Ft agree and change direction, for a small change in \u03b8 or x0 (even for the same input sequence). Figure 5 describes this behaviour.\nFor the specific parametrization provided by equation (2) we can take the analogy on step further by decomposing the maps Ft into a fixed map F\u0303 and\na time-varying one Ut. F (x) = Wrec\u03c3(x) + b corresponds to an input-less recurrent network, while Ut(x) = x + Winut describes the effect of the input. This is depicted in in Figure 6. Since Ut changes with time, it can not be analyzed using standard dynamical systems tools, but F\u0303 can. This means that when a boundary between basins of attractions is crossed for F\u0303 , the state will move towards a different attractor, which for large t can lead to a large discrepancy in xt. If Ut is bounded, that it can interfere with this behaviour only when the state is close to the boundary, but not far away. Therefore studying the asymptotic behaviour of F\u0303 can provide some information about where such events are likely to happen.\nThese derivations (specifically figure 6) provide an intuitive way of understanding equation 13, and the surrounding constraints (the set P for e.g. regards the behaviour of the maps Ui which could move against the exploding direction ensuring that gradients do not explodes). Another interesting connection is with our geometrical interpretation. If there is indeed such a boundary, that means not only that when crossed the norm of the gradient grows considerably, but it can do so quickly, i.e. the curvature has to be high as well. This speaks towards our hypothesis that both gradients and curvature tend to explode together."}, {"heading": "3 Dealing with the exploding", "text": "gradient"}, {"heading": "3.1 Previous solutions", "text": "One approach to avoid exploding gradients is to use L1 or L2 penalty on the recurrent weights. Given that the model is initialized with small numbers, the spectral radius of Wrec is probably smaller than 1, from which it follows that the gradient can not explode (see necessary condition found in section 2.1). The regularization term can ensure that during training the spectral radius never exceeds 1.\nThis approach limits the model to a simple regime (with a single point attractor at the origin), where any information inserted in the model has to die out exponentially fast in time. In such a regime we can not train a generator network, nor can we exhibit long term memory traces.\nThis suggest that solutions that exploit changes in the architecture to avoid vanishing gradients, such as LSTMs (Hochreiter and Schmidhuber, 1997) can deal with the exploding gradient by operating the recurrent model in a damping regime and relying exclusively on the highly specialized LSTM units to exhibit memory, thus justifying why the exploding gradient does not seem to be an issue in practice.\nDoya (1993) proposes to pre-program the model (to initialize the model in the right regime) or to use teacher forcing. The first proposal assumes that if the model exhibits from the beginning the same kind of asymptotic behaviour as the one required by the target, then there is no need to cross a bifurcation boundary. The downside is that one can not always know the required asymptotic behaviour, and, even if such information is known, it is not trivial to initialize a model in this specific regime. We should also note that such initialization does not prevent crossing the boundary between basins of attraction, which, as showed, could happen even though no bifurcation boundary is crossed.\nTeacher forcing is a more interesting and not very well understood solution. It can be seen as a way of initializing the model in the right regime and the right region of space. It has been showed that in practice it can reduce the chance that gradients explode, and even allow training generator models or models that\nwork with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991). One important downside is that it requires a target to be defined at every time step. It also requires expertise, as models trained with it have a tendency to be unstable.\nAnother approach was proposed by Tomas Mikolov recently described in his PhD thesis (Mikolov, 2012) involves clipping the gradient element-wise if the value exceeds in absolute value a fix threshold. This approach has been showed to do well in practice and it forms the backbone of our approach, which tries to justify this implementation trick."}, {"heading": "3.2 Scaling down the gradients", "text": "As suggested in section 2.4, one simple mechanism to deal with a sudden increase in the norm of the gradients is to rescale them whenever they go over a threshold (see algorithm 1).\nAlgorithm 1 Pseudo-code for norm clipping the gradients whenever they explode g\u0302\u2190 \u2202E\u2202\u03b8 if \u2016g\u0302\u2016 \u2265 threshold then g\u0302\u2190 threshold\u2016g\u0302\u2016 g\u0302\nend if\nThis algorithm is very similar to the one proposed by Tomas Mikolov and the only reason we diverged from his original proposal in an attempt to provide a better theoretical foundation (for e.g. we ensure that we always move in a descent direction), though in practice they behave similarly.\nThe proposed clipping is simple to implement and computationally efficient, but it does however introduce an additional hyper-parameter, namely the threshold. One good heuristic for setting this threshold is to look at statistics on the average norm over a sufficiently large number of updates. In our experiments we have noticed that for a given task and model size training is not very sensitive to this hyperparameter and the algorithm behaves well even for rather small thresholds.\nThe algorithm can also be thought of as adapting the learning rate based on the norm of the gradient.\nTable 1: Results on polyphonic music prediction in negative log likelihood per time step. Lower is better.\nData set Data fold BPTT Rescaling gradients\nPiano- train 7.06\u00b10.052 7.12\u00b10.027 midi.de test 7.79\u00b10.013 7.77\u00b10.019 Nottingham train 4.12\u00b10.330 3.71\u00b10.059 test 4.57\u00b10.718 4.05\u00b10.052 MuseData train 7.99\u00b10.768 7.02\u00b10.031\ntest 8.00\u00b10.426 7.24\u00b10.037\nCompared to other learning rate adaptation strategies, which focus on improving convergence by collecting statistics on the gradient (as for example in Duchi et al. (2011), or Moreira and Fiesler (1995) for an overview), we rely on the instantaneous gradient. This means that we can handle very abrupt changes in norm, while the other methods would not be able to do so. We do not ensure faster convergence though."}, {"heading": "4 Experiments and Results", "text": ""}, {"heading": "4.1 Polyphonic music prediction", "text": "The first task we consider is polyphonic music prediction, using the datasets Piano-midi.de, Nottingham and MuseData described in Boulanger-Lewandowski et al. (2012). In this case the vocabulary size is 88 different notes, with the distinction that multiple notes can be played at the same time.\nWe use a 100 tanh units model with biases in order to stay close to the original setup (BoulangerLewandowski et al., 2012). Each song is divided into non-overlapping sequences of 100 steps, and the hidden state is carried over only along the same song (and set to 0 for the first sequence of each song). We use a learning rate of .001 and a threshold of 120. The training and test scores reported in table 1 are average negative log likelihood per time step. We use 5 different runs to estimate these values.\nThese results are an improvement on the state of the art obtained using RNNs models(see BoulangerLewandowski et al. (2012)).\nTable 2: Results on the next character prediction task in entropy (bits/character)\nData set Data fold BPTT Rescaling gradients\ntext8 train 1.66\u00b10.010 1.67\u00b10.006 test 1.82\u00b10.067 1.80\u00b10.021\nPenn train 2.25\u00b10.960 1.41\u00b10.120 Treebank test 2.24\u00b10.942 1.44\u00b10.006"}, {"heading": "4.2 Next character prediction", "text": "The second task is next character prediction on three different datasets: Penn Treebank Corpus, Wikipedia \u2018text8\u2019. The same datasets had been considered in Mikolov et al. (2012).\nThe model used is a 500 sigmoid units RNN with no biases in order to have a similar setup as the one used in Mikolov et al. (2012). Each gradient is computed over non-overlapping sequences of 180 characters, where the hidden state is carried over from one sequence to the next one. Table 2 provides the training and test error for best validation score. These values are computed over 5 different random initializations and we report entropy (bits per character) as a measure of error. We used a threshold of 20 for \u2018text8\u2018 and 45 for Penn Treebank dataset.\nThese results (together with the one on polyphonic music prediction) suggest that clipping the gradients solves an optimization issue and does not act as a regularizer, as both the training and test error improve in general. Also results on Penn Treebank reach the state of the art (Mikolov et al., 2012), where those results were obtained using a different clipping algorithm similar to ours providing evidence that both behave similarly. For the \u2018text8\u2018 experiment, our model is much smaller than the one used for state of the art results (for computational reasons) and hence the numbers are higher."}, {"heading": "4.3 Temporal order task", "text": "In our final experiments we consider the synthetic problem proposed as task 6a in Hochreiter and Schmidhuber (1997) (see that paper for full details).\nWe use a 50 sigmoid units model with a learning rate of .001 and limit the number of training steps to 100k. We use mini-batch gradient descent with 1000 examples per batch. The task is considered solved if for 1000 consecutive inputs the model returns the correct answer. Figure 7 shows the success rate of standard BPTT and gradient rescaling for 50 different runs. Note that for sequences longer than 20 the vanishing gradient problem ensures that neither BPTT nor our algorithm can solve the task.\nThis task provides empirical evidence that the exploding gradient is linked with tasks that require long memory traces. We know that initially the model operates in the one-attractor regime (i.e. \u03c1Wrec < 1), in which the amount of memory is controlled by \u03c1Wrec . More memory means larger spectral radius, and, when this value crosses a certain threshold the model enters rich regimes where gradients are likely to explode. We see in Figure 7 that as long as the vanishing gradient problem does not become an issue, addressing the exploding gradient ensures a much larger success rate."}, {"heading": "5 Summary and Conclusions", "text": "The exploding gradient problem is just one facet of the difficulty of training recurrent networks. We provided different perspectives through which one can gain more insight into this issue, though these descriptions can easily be useful for also understanding the vanishing gradient problem. We propose a\nsolution that involves clipping the norm of the exploded gradients when it is too large. The algorithm is motivated by the assumption that when gradients explode, the curvature explodes as well, and we are faced with a specific pattern in the error surface, namely a valley with a single steep wall. In practice we show that this approach improves performance on all of the 6 tested tasks."}], "references": [{"title": "New results on recurrent network training: Unifying the algorithms and accelerating convergence", "author": ["A.F. Atiya", "A.G. Parlos"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Atiya and Parlos,? \\Q2000\\E", "shortCiteRegEx": "Atiya and Parlos", "year": 2000}, {"title": "The problem of learning long-term dependencies in recurrent networks. pages 1183\u20131195, San Francisco", "author": ["Y. Bengio", "P. Frasconi", "P. Simard"], "venue": "IEEE Press. (invited paper)", "citeRegEx": "Bengio et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1993}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Bifurcations of recurrent neural networks in gradient descent learning", "author": ["K. Doya"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Doya,? \\Q1993\\E", "shortCiteRegEx": "Doya", "year": 1993}, {"title": "Adaptive synchronization of neural and physical oscillators", "author": ["K. Doya", "S. Yoshizawa"], "venue": null, "citeRegEx": "Doya and Yoshizawa,? \\Q1991\\E", "shortCiteRegEx": "Doya and Yoshizawa", "year": 1991}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["J. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman,? \\Q1990\\E", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Computer Science Review ,", "citeRegEx": "Luko\u0161evi\u010dius and Jaeger,? \\Q2009\\E", "shortCiteRegEx": "Luko\u0161evi\u010dius and Jaeger", "year": 2009}, {"title": "Statistical Language Models based on", "author": ["T. Mikolov"], "venue": "Neural Networks. Ph.D. thesis, Brno University of Technology", "citeRegEx": "Mikolov,? \\Q2012\\E", "shortCiteRegEx": "Mikolov", "year": 2012}, {"title": "Subword language modeling with neural networks. preprint (http://www.fit.vutbr.cz/ imikolov/rnnlm/char.pdf)", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "Le", "H.-S", "S. Kombrink", "J. Cernocky"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Neural networks with adaptive learning rate and momentum terms. Idiap-RR Idiap-RR-04-1995, IDIAP, Martigny, Switzerland", "author": ["M. Moreira", "E. Fiesler"], "venue": null, "citeRegEx": "Moreira and Fiesler,? \\Q1995\\E", "shortCiteRegEx": "Moreira and Fiesler", "year": 1995}, {"title": "A neurodynamical model for working memory", "author": ["R. Pascanu", "H. Jaeger"], "venue": "Neural Netw.,", "citeRegEx": "Pascanu and Jaeger,? \\Q2011\\E", "shortCiteRegEx": "Pascanu and Jaeger", "year": 2011}, {"title": "Learning representations by backpropagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "On the computational power of neural nets", "author": ["H.T. Siegelmann", "E.D. Sontag"], "venue": "JOURNAL OF COMPUTER AND SYSTEM SCIENCES ,", "citeRegEx": "Siegelmann and Sontag,? \\Q1995\\E", "shortCiteRegEx": "Siegelmann and Sontag", "year": 1995}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks,", "citeRegEx": "Werbos,? \\Q1988\\E", "shortCiteRegEx": "Werbos", "year": 1988}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. Williams", "D. Zipser"], "venue": "Neural Comput.,", "citeRegEx": "Williams and Zipser,? \\Q1989\\E", "shortCiteRegEx": "Williams and Zipser", "year": 1989}], "referenceMentions": [{"referenceID": 1, "context": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective.", "startOffset": 146, "endOffset": 167}, {"referenceID": 14, "context": "A recurrent neural network (RNN), see Figure 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series.", "startOffset": 95, "endOffset": 146}, {"referenceID": 7, "context": "A recurrent neural network (RNN), see Figure 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series.", "startOffset": 95, "endOffset": 146}, {"referenceID": 17, "context": "A recurrent neural network (RNN), see Figure 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series.", "startOffset": 95, "endOffset": 146}, {"referenceID": 14, "context": "A plethora of training algorithms have been proposed in the literature, like Backpropagation Through Time (BPTT) (Rumelhart et al., 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 113, "endOffset": 151}, {"referenceID": 17, "context": "A plethora of training algorithms have been proposed in the literature, like Backpropagation Through Time (BPTT) (Rumelhart et al., 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 113, "endOffset": 151}, {"referenceID": 18, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 52, "endOffset": 79}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 107, "endOffset": 131}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc. Most of these are gradient based, though alternative approaches are also available (for example see Luko\u0161evi\u010dius and Jaeger (2009)), and as shown in Atiya and Parlos (2000) most gradient-based methods behave qualitatively the same, providing little success in properly addressing complex tasks.", "startOffset": 108, "endOffset": 269}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc. Most of these are gradient based, though alternative approaches are also available (for example see Luko\u0161evi\u010dius and Jaeger (2009)), and as shown in Atiya and Parlos (2000) most gradient-based methods behave qualitatively the same, providing little success in properly addressing complex tasks.", "startOffset": 108, "endOffset": 311}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc. Most of these are gradient based, though alternative approaches are also available (for example see Luko\u0161evi\u010dius and Jaeger (2009)), and as shown in Atiya and Parlos (2000) most gradient-based methods behave qualitatively the same, providing little success in properly addressing complex tasks. Among the main reasons why this model is so unwieldy are the vanishing gradient and exploding gradient problems described in Bengio et al. (1994).", "startOffset": 108, "endOffset": 579}, {"referenceID": 16, "context": "that there are a few solutions proposed for this problem in the literature, amongst which the HessianFree learning (Sutskever et al., 2011), a second order method that seems promising, though more needs to be done to analyze its success, especially compared to other second order methods that seem to do less well in general.", "startOffset": 115, "endOffset": 139}, {"referenceID": 8, "context": "Long Short Term Memory networks (Hochreiter and Schmidhuber, 1997) is another approach, relying on a change in the structure of the model, and designed to help with the vanishing gradient problem.", "startOffset": 32, "endOffset": 66}, {"referenceID": 1, "context": "As introduced in Bengio et al. (1994), the exploding gradient problems refers to the large increase in the norm of the gradient during training.", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": "In what follows we will try to formalize these intuitions (extending a similar derivation done in Bengio et al. (1994) where only a single hidden unit case was considered).", "startOffset": 98, "endOffset": 119}, {"referenceID": 12, "context": "Siegelmann and Sontag (1995)) and as such it is sometimes useful to analyze them using dynamical systems tools which allow for better abstractions that we can use for reasoning about the behaviour of the model.", "startOffset": 0, "endOffset": 29}, {"referenceID": 2, "context": "Looking at dynamical systems theory for explaining the exploding gradient problem has been done before in Doya (1993); Bengio et al.", "startOffset": 106, "endOffset": 118}, {"referenceID": 1, "context": "Looking at dynamical systems theory for explaining the exploding gradient problem has been done before in Doya (1993); Bengio et al. (1993). In this paper we will extend and improve these previous observations.", "startOffset": 119, "endOffset": 140}, {"referenceID": 4, "context": "In Doya (1993) only crossing bifurcation boundaries are considered ignoring changes in the state or the position of the state relative a borders between basin of attraction.", "startOffset": 3, "endOffset": 15}, {"referenceID": 8, "context": "This suggest that solutions that exploit changes in the architecture to avoid vanishing gradients, such as LSTMs (Hochreiter and Schmidhuber, 1997) can deal with the exploding gradient by operating the recurrent model in a damping regime and relying exclusively on the highly specialized LSTM units to exhibit memory, thus justifying why the exploding gradient does not seem to be an issue in practice.", "startOffset": 113, "endOffset": 147}, {"referenceID": 13, "context": "It has been showed that in practice it can reduce the chance that gradients explode, and even allow training generator models or models that work with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991).", "startOffset": 178, "endOffset": 230}, {"referenceID": 5, "context": "It has been showed that in practice it can reduce the chance that gradients explode, and even allow training generator models or models that work with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991).", "startOffset": 178, "endOffset": 230}, {"referenceID": 10, "context": "Another approach was proposed by Tomas Mikolov recently described in his PhD thesis (Mikolov, 2012) involves clipping the gradient element-wise if the value exceeds in absolute value a fix threshold.", "startOffset": 84, "endOffset": 99}, {"referenceID": 6, "context": "Compared to other learning rate adaptation strategies, which focus on improving convergence by collecting statistics on the gradient (as for example in Duchi et al. (2011), or Moreira and Fiesler (1995) for an overview), we rely on the instantaneous gradient.", "startOffset": 152, "endOffset": 172}, {"referenceID": 6, "context": "Compared to other learning rate adaptation strategies, which focus on improving convergence by collecting statistics on the gradient (as for example in Duchi et al. (2011), or Moreira and Fiesler (1995) for an overview), we rely on the instantaneous gradient.", "startOffset": 152, "endOffset": 203}, {"referenceID": 3, "context": "de, Nottingham and MuseData described in Boulanger-Lewandowski et al. (2012). In this case the vocabulary size is 88 different notes, with the distinction that multiple notes can be played at the same time.", "startOffset": 41, "endOffset": 77}, {"referenceID": 10, "context": "The same datasets had been considered in Mikolov et al. (2012).", "startOffset": 41, "endOffset": 63}, {"referenceID": 10, "context": "The model used is a 500 sigmoid units RNN with no biases in order to have a similar setup as the one used in Mikolov et al. (2012). Each gradient is computed over non-overlapping sequences of 180 characters, where the hidden state is carried over from one sequence to the next one.", "startOffset": 109, "endOffset": 131}, {"referenceID": 11, "context": "Also results on Penn Treebank reach the state of the art (Mikolov et al., 2012), where those results were obtained using a different clipping algorithm similar to ours providing evidence that both behave similarly.", "startOffset": 57, "endOffset": 79}, {"referenceID": 8, "context": "In our final experiments we consider the synthetic problem proposed as task 6a in Hochreiter and Schmidhuber (1997) (see that paper for full details).", "startOffset": 82, "endOffset": 116}], "year": 2012, "abstractText": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective. Our analysis is used to justify the simple yet effective solution of norm clipping the exploded gradient. In the experimental section, the comparison between this heuristic solution and standard SGD provides empirical evidence towards our hypothesis as well as it shows that such a heuristic is required to reach state of the art results on a character prediction task and a polyphonic music prediction one.", "creator": "LaTeX with hyperref package"}}}