{"id": "1212.0901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2012", "title": "Advances in Optimizing Recurrent Networks", "abstract": "\u2020 after a more than decade - long hiatus period of continuing relatively little research activity in the basic area of researching recurrent neural networks, several new technologies developments will nonetheless be separately reviewed about here that have allowed at substantial progress both in skills understanding and leadership in technical theoretical solutions towards more efficient training of recurrent communication networks. these advances worldwide have been motivated essentially by and related to the optimization neural issues surrounding deep spectral learning. although researching recurrent networks are presently extremely many powerful interested in achieving what they can in principle represent in terms of modelling probability sequences, their training is indeed plagued individually by two aspects of highlighting the actual same nonlinear issue regarding determining the slower learning of long - endurance term dependencies. experiments accurately reported here evaluate the use of long clipping gradients, actively spanning longer time uncertainty ranges with leaky elastic integration, engaging advanced particle momentum techniques, using more powerful sparse output length probability models, improving and encouraging shorter sparser approximation gradients to better help symmetry breaking and credit numerical assignment. the experiments are ultimately performed on archival text and music data and currently show off illustrating the combined potential effects incurred of exercising these techniques in generally improving both accurate training and test error.", "histories": [["v1", "Tue, 4 Dec 2012 23:25:34 GMT  (19kb)", "http://arxiv.org/abs/1212.0901v1", null], ["v2", "Fri, 14 Dec 2012 01:44:53 GMT  (19kb)", "http://arxiv.org/abs/1212.0901v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "nicolas boulanger-lewandowski", "razvan pascanu"], "accepted": false, "id": "1212.0901"}, "pdf": {"name": "1212.0901.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n21 2.\n09 01\nv1 [\ncs .L\nG ]\n4 D\nec 2\n01 2\nIndex Terms\u2014 Recurrent networks, deep learning, representation learning, long-term dependencies"}, {"heading": "1. INTRODUCTION", "text": "Machine learning algorithms for capturing statistical structure in sequential data face a fundamental problem [1, 2], called the difficulty of learning long-term dependencies. If the operations performed when forming a fixed-size summary of relevant past observations (for the purpose of predicting some future observations) are linear, this summary must exponentially forget past events that are further away, to maintain stability. On the other hand, if they are non-linear, then this non-linearity is composed many times, yielding a highly non-linear relationship between past events and future events. Learning such non-linear relationships turns out to be difficult, for reasons that are discussed here, along with recent proposals for reducing this difficulty.\nRecurrent neural networks [3] can represent such non-linear maps (F , below) that iteratively build a relevant summary of past observations. In their simplest form, recurrent neural networks (RNNs) form a deterministic state variable ht as a function of the present input observation xt and the past value(s) of the state variable, e.g., ht = F\u03b8(ht\u22121, xt), where \u03b8 are tunable parameters that control what will be remembered about the past sequence and what will be discarded. Depending on the type of problem at hand, a loss function L(ht, yt) is defined, with yt an observed random variable at time t and Ct = L(ht, yt) the cost at time t. The generalization objective is to minimize the expected future cost, and the training objective involves the average of Ct over observed sequences. In principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm [3] to compute a gradient), but it was observed early on [1, 2] that capturing dependencies that span a long interval was difficult, making the task of\noptimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for some tasks when the span of the dependencies of interest increases sufficiently. More precisely, using a local numerical optimization such as stochastic gradient descent or second order methods (which gradually improve the solution), the proportion of trials (differing only from their random initialization) falling into the basin of attraction of a good enough solution quickly becomes very small as the temporal span of dependencies is increased (beyond tens or hundreds of steps, depending of the task).\nThese difficulties are probably responsible for the major reduction in research efforts in the area of RNNs in the 90\u2019s and 2000\u2019s. However, a revival of interest in these learning algorithms is taking place, in particular thanks to [4] and [5]. This paper studies the issues giving rise to these difficulties and discusses, reviews, and combines several techniques that have been proposed in order to improve training of RNNs, following up on a recent thesis devoted to the subject [6]. We find that these techniques generally help generalization performance as well as training performance, which suggest they help to improve the optimization of the training criterion. We also find that although these techniques can be applied in the online setting, i.e., as add-ons to stochastic gradient descent (SGD), they allow to compete with batch (or large minibatch) second-order methods such as Hessian-Free optimization, recently found to greatly help training of RNNs [4]."}, {"heading": "2. LEARNING LONG-TERM DEPENDENCIES AND THE OPTIMIZATION DIFFICULTY WITH DEEP LEARNING", "text": "There has been several breakthroughs in recent years in the algorithms and results obtained with so-called deep learning algorithms (see [7] and [8] for reviews). Deep learning algorithms discover multiple levels of representation, typically as deep neural networks or graphical models organized with many levels of representationcarrying latent variables. Very little work on deep architectures occurred before the major advances of 2006 [9, 10, 11], probably because of optimization difficulties due to the high level of nonlinearity in deeper networks (whose output is the composition of the non-linearity at each layer). Some experiments [12] showed the presence of an extremely large number of apparent local minima of the training criterion, with no two different initializations going to the same function (i.e. eliminating the effect of permutations and other symmetries of parametrization giving rise to the same function). Furthermore, qualitatively different initialization (e.g., using unsupervised learning) could yield models in completely different regions of function space. An unresolved question is whether these difficulties are actually due to local minima or to ill-conditioning (which makes gradient descent converge so slowly as to appear stuck in a local minimum). Some ill-conditioning has clearly been shown to be involved, especially for the difficult problem of training deep auto-encoders, through comparisons [13] of stochastic gradient descent and Hessian-free optimization (a second order optimiza-\ntion method). These optimization questions become particularly important when trying to train very large networks on very large datasets [14], where one realizes that a major challenge for deep learning is the underfitting issue. Of course one can trivially overfit by increasing capacity in the wrong places (e.g. in the output layer), but what we are trying to achieve is learning of more powerful representations in order to also get good generalization.\nThe same questions can be asked for RNNs. When the computations performed by a RNN are unfolded through time, one clearly sees a deep neural network with shared weights (across the \u2019layers\u2019, each corresponding to a different time step), and with a cost function that may depends on the output of intermediate layers. Hessian-free optimization has been successfully used to considerably extend the span of temporal dependencies that a RNN can learn [4], suggesting that ill-conditioning effects are also at play in the difficulties of training RNN.\nAn important aspect of these difficulties is that the gradient can be decomposed [2, 15] into terms that involve products of Jacobians \u2202ht \u2202ht\u22121 over subsequences linking an event at time t1 and one at time t2: \u2202ht2 \u2202ht1 = \u220ft2 \u03c4=t1+1 \u2202h\u03c4 \u2202h\u03c4\u22121 . As t2 \u2212 t1 increases, the products of t2 \u2212 t1 of these Jacobian matrices tend to either vanish (when the leading eigenvalues of \u2202ht\n\u2202ht\u22121 are less than 1) or explode (when the\nleading eigenvalues of \u2202ht \u2202ht\u22121 are greater than 11). This is problematic because the total gradient due to a loss Ct2 at time t2 is a sum whose terms correspond to the effects at different time spans, which are weighted by\n\u2202ht2 \u2202ht1 for different t1\u2019s:\n\u2202Ct2 \u2202\u03b8\n= \u2211\nt1\u2264t2\n\u2202Ct2 \u2202ht2 \u2202ht2 \u2202ht1 \u2202ht1 \u2202\u03b8(t1)\nwhere \u2202ht1\n\u2202\u03b8(t1) is the derivative of ht1 with respect to the instantia-\ntion of the parameters \u03b8 at step t1, i.e., that directly come into the computation of ht1 in F . When the\n\u2202ht2 \u2202ht1 tend to vanish for increas-\ning t2\u2212 t1, the long-term term effects become exponentially smaller in magnitude than the shorter-term ones, making it very difficult to capture them. On the other hand, when\n\u2202ht2 \u2202ht1 \u201cexplode\u201d (becomes\nlarge), gradient descent updates can be destructive (move to poor configuration of parameters). It is not that the gradient is wrong, it is that gradient descent makes small but finite steps \u2206\u03b8 yielding a \u2206C, whereas the gradient measures the effect of \u2206C when \u2206\u03b8 \u2192 0. A much deeper discussion of this issue can be found in [15], along with a point of view inspired by dynamical systems theory and by the geometrical aspect of the problem, having to do with the shape of the training criterion as a function of \u03b8 near those regions of exploding gradient. In particular, it is argued that the strong non-linearity occurring where gradients explode is shaped like a cliff where not just the first but also the second derivative becomes large in the direction orthogonal to the cliff. Similarly, flatness of the cost function occurs simultaneously on the first and second derivatives. Hence dividing the gradient by the second derivative in each direction (i.e., pre-multiplying by the inverse of some proxy for the Hessian matrix) could in principle reduce the exploding and vanishing gradient effects, as argued in [4].\n1 Note that this is not a sufficient condition, but a necessary one. Further more one usually wants to operate in the regime where the leading eigenvalue is larger than 1 but the gradients do not explode."}, {"heading": "3. ADVANCES IN TRAINING RECURRENT NETWORKS", "text": ""}, {"heading": "3.1. Clipped Gradient", "text": "To address the exploding gradient effect, [16, 15] recently proposed to clip gradients above a given threshold. Under the hypothesis that the explosion occurs in very small regions (the cliffs in cost function mentioned above), most of the time this will have no effect, but it will avoid aberrant parameter changes in those cliff regions, while guaranteeing that the resulting updates are still in a descent direction. The specific form of clipping used here was proposed in [15] and is discussed there at much greater length: when the norm of the gradient vector g for a given sequence is above a threshold, the update is done in the direction threshold g\n||g|| . As argued in [15], this very\nsimple method implements a very simple form of second order optimization in the sense that the second derivative is also proportionally large in those exploding gradient regions."}, {"heading": "3.2. Spanning Longer Time Ranges with Leaky Integration", "text": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20]. Long-Short-Term Memory (LSTM) networks [21], which were shown to be able to handle much longer range dependencies, also benefit from a linearly self-connected memory unit with a near 1 self-weight which allows signals (and gradients) to propagate over long time spans.\nA different interpretation to this slow-changing units is that they behave like low-pass filter and hence they can be used to focus certain units on different frequency regions of the data. The analogy can be brought one step further by introducing band-pass filter units [22] or by using domain specific knowledge to decide on what frequency bands different units should focus. [?] shows that adding low frequency information as an additional input to a recurrent network helps improving the performance of the model.\nIn the experiments performed here, a subset of the units were forced to change slowly by using the following \u201cleaky integration\u201d state-to-state map: ht,i = \u03b1iht\u22121,i + (1 \u2212 \u03b1i)Fi(ht\u22121, xt). The standard RNN corresponds to \u03b1i = 0, while here different values of \u03b1i were randomly sampled from (0.02, 0.2), allowing some units to react quickly while others are forced to change slowly, but also propagate signals and gradients further in time. Note that because \u03b1 < 1, the vanishing effect is still present (and gradients can still explode via F ), but the time-scale of the vanishing effect can be expanded."}, {"heading": "3.3. Combining Recurrent Nets with a Powerful Output Probability Model", "text": "One way to reduce the underfitting of RNNs is to introduce multiplicative interactions in the parametrization of F , as was done successfully in [4]. When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24]. In the experiments performed here, we have experimented with a NADE output model for the music data."}, {"heading": "3.4. Sparser Gradients via Sparse Output Regularization and Rectified Outputs", "text": "[7] hypothesized that one reason for the difficulty in optimizing deep networks is that in ordinary neural networks gradients diffuse through the layers, diffusing credit and blame through many units, maybe making it difficult for hidden units to specialize. When the gradient on hidden units is more sparse, one could imagine that symmetries would be broken more easily and credit or blame assigned less uniformly. This is what was advocated in [26], exploiting the idea of rectifier non-linearities introduced earlier in [27], i.e., the neuron non-linearity is out = max(0, in) instead of out = tanh(in) or out = sigmoid(in). This approach was very successful in recent work on deep learning for object recognition [28], beating by far the state-of-the-art on ImageNet (1000 classes). Here, we apply this deep learning idea to RNNs, using an L1 penalty on outputs of hidden units to promote sparsity of activations. The underlying hypothesis is that if the gradient is concentrated in a few paths (in the unfolded computation graph of the RNN), it will reduce the vanishing gradients effect."}, {"heading": "3.5. Simplified Nesterov Momentum", "text": "Nesterov accelerated gradient (NAG) [29] is a first-order optimization method to improve stability and convergence of regular gradient descent. Recently, [6] showed that NAG could be computed by the following update rules:\nvt = \u00b5t\u22121vt\u22121 \u2212 \u01ebt\u22121\u2207f(\u03b8t\u22121 + \u00b5t\u22121vt\u22121) (1)\n\u03b8t = \u03b8t\u22121 + vt (2)\nwhere \u03b8t are the model parameters, vt the velocity, \u00b5t \u2208 [0, 1] the momentum (decay) coefficient and \u01ebt > 0 the learning rate at iteration t, f(\u03b8) is the objective function and \u2207f(\u03b8\u2032) is a shorthand notation for the gradient \u2202f(\u03b8)\n\u2202\u03b8 |\u03b8=\u03b8\u2032 . These equations have a form\nsimilar to standard momentum updates:\nvt = \u00b5t\u22121vt\u22121 \u2212 \u01ebt\u22121\u2207f(\u03b8t\u22121) (3)\n\u03b8t = \u03b8t\u22121 + vt (4)\n= \u03b8t\u22121 + \u00b5t\u22121vt\u22121 \u2212 \u01ebt\u22121\u2207f(\u03b8t\u22121) (5)\nand differ only in the evaluation point of the gradient at each iteration. This important difference, thought to counterbalance too high velocities by \u201cpeeking ahead\u201d actual objective values in the candidate search direction, results in significantly improved RNN performance on a number of tasks.\nIn this section, we derive a new formulation of Nesterov momentum differing from (3) and (5) only in the linear combination coefficients of the velocity and gradient contributions at each iteration, and we offer an alternative interpretation of the method. The key departure from (1) and (2) resides in committing to the \u201cpeekedahead\u201d parameters \u0398t\u22121 \u2261 \u03b8t\u22121 + \u00b5t\u22121vt\u22121 and backtracking by the same amount before each update. Our new parameters \u0398t updates become:\nvt = \u00b5t\u22121vt\u22121 \u2212 \u01ebt\u22121\u2207f(\u0398t\u22121) (6)\n\u0398t = \u0398t\u22121 \u2212 \u00b5t\u22121vt\u22121 + \u00b5tvt + vt\n= \u0398t\u22121 + \u00b5t\u00b5t\u22121vt\u22121 \u2212 (1 + \u00b5t)\u01ebt\u22121\u2207f(\u0398t\u22121) (7)\nAssuming a zero initial velocity v1 = 0 and velocity at convergence of optimization vT \u2243 0, the parameters \u0398 are a completely equivalent replacement of \u03b8.\nNote that equation (7) is identical to regular momentum (5) with different linear combination coefficients. More precisely, for an equivalent velocity update (6), the velocity contribution to the new parameters \u00b5t\u00b5t\u22121 < \u00b5t is reduced relatively to the gradient contribution (1 + \u00b5t)\u01ebt\u22121 > \u01ebt\u22121. This allows storing past velocities for a longer time with a higher \u00b5, while actually using those velocities more conservatively during the updates. We suspect this mechanism is a crucial ingredient for good empirical performance. While the \u201cpeeking ahead\u201d point of view suggests that a similar strategy could be adapted for regular gradient descent (misleadingly, because it would amount to a reduced learning rate \u01ebt), our derivation shows why it is important to choose search directions aligned with the current velocity to yield substantial improvement. The general case is also simpler to implement."}, {"heading": "4. EXPERIMENTS", "text": "In the experimental section we compare vanilla SGD versus SGD plus some of the enhancements discussed above. Specifically we use the letter \u2018C\u2018 to indicate that gradient clipping is used, \u2018L\u2018 for leaky-integration units, \u2018R\u2018 if we use rectifier units with L1 penalty and \u2018M\u2018 for Nesterov momentum."}, {"heading": "4.1. Music Data", "text": "We evaluate our models on the four polyphonic music datasets of varying complexity used in [24]: classical piano music (Pianomidi.de), folk tunes with chords instantiated from ABC notation (Nottingham), orchestral music (MuseData) and the four-part chorales by J.S. Bach (JSB chorales). The symbolic sequences contain high-level pitch and timing information in the form of a binary matrix, or piano-roll, specifying precisely which notes occur at each time-step. They form interesting benchmarks for RNNs because of their high dimensionality and the complex temporal dependencies involved at different time scales. Each dataset contains at least 7 hours of polyphonic music with an average polyphony (number of simultaneous notes) of 3.9.\nPiano-rolls were prepared by aligning each time-step (88 pitch labels that cover the whole range of piano) on an integer fraction of the beat (quarter note) and transposing each sequence in a common tonality (C major/minor) to facilitate learning. Source files and preprocessed piano-rolls split in train, validation and test sets are available on the authors\u2019 website2."}, {"heading": "4.1.1. Setup and Results", "text": "We select hyperparameters, such as the number of hidden units nh, regularization coefficients \u03bbL1, the choice of non-linearity function, or the momentum schedule \u00b5t, learning rate \u01ebt, number of leaky units nleaky or leaky factors \u03b1 according to log-likelihood on a validation set and we report the final performance on the test set for the best choice in each category. We do so by using random search [30] on the following intervals:\nnh \u2208 [100, 400] \u01ebt \u2208 [10 \u22124, 10\u22121] \u00b5t \u2208 [10 \u22123, 0.95] \u03bbL1 \u2208 [10\n\u22126, 10\u22123] nleaky \u2208 {0%, 25%, 50%} \u03b1 \u2208 [0.02, 2]\nThe cutoff threshold for gradient clipping is set based on the average norm of the gradient over one pass on the data, and we used 15 in this case for all music datasets. The data is split into sequences\n2www-etud.iro.umontreal.ca/\u02dcboulanni/icml2012\nof 100 steps over which we compute the gradient. The hidden state is carried over from one sequence to another if they belong to the same song, otherwise is set to 0.\nTable 1 presents log-likelihood (LL) and expected frame-level accuracy for various RNNs in the symbolic music prediction task.\nResults clearly show that these enhancements allow to improve on regular SGD in almost all cases; they also make SGD competitive with HF for the sigmoid recognition layers RNNs."}, {"heading": "4.2. Text Data", "text": "We use the Penn Treebank Corpus to explore both word and character prediction tasks. The data is split by using sections 0-20 as training data (5017k characters), sections 21-22 as validation (393k characters) and sections 23-24 as test data (442k characters).\nFor the word level prediction, we fix the dictionary to 10000 words, which we divide into 30 classes according to their frequency in text (each class holding approximately 3.3% of the total number of tokens in the training set). Such a factorization allows for faster implementation, as we are not required to evaluate the whole output layer (10000 units) which is the computational bottleneck, but only the output of the corresponding class [31]."}, {"heading": "4.2.1. Setup and Results", "text": "In the case of next word prediction, we compute gradients over sequences of 40 steps, where we carry the hidden state from one sequence to another. We use a small grid-search around the parameters\nused to get state of the art results for this number of classes [31], i.e., with a network of 200 hidden units yielding a perplexity of 134. We explore learning rate of 0.1, 0.01, 0.001, rectifier units versus sigmoid units, cutoff threshold for the gradients of 30, 50 or none, and no leaky units versus 50 of the units being sampled from 0.2 and 0.02.\nFor the character level model we compute gradients over sequences of 150 steps, as we assume that longer dependencies are more crucial in this case. We use 500 hidden units and explore learning rates of 0.5, 0.1 and 0.01.\nIn table 2 we have entropy (bits per character) or perplexity for varous RNNs on the word and character prediction tasks. Again, we observe substantial improvements in both training and test perplexity, suggesting that these techniques make optimization easier."}, {"heading": "5. CONCLUSIONS", "text": "Through our experiments we provide evidence that part of the issue of training RNN is due to the rough error surface which can not be easily handled by SGD. We follow an incremental set of improvements to SGD, and show that in most cases they improve both the training and test error, and allow this enhanced SGD to compete or even improve on a second-order method which was found to work particularly well for RNNs, i.e., Hessian-Free optimization."}, {"heading": "6. REFERENCES", "text": "[1] S. Hochreiter, \u201c Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. Mu\u0308nich,\u201d 1991.\n[2] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE T. Neural Nets, 1994.\n[3] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, \u201cLearning representations by back-propagating errors,\u201d Nature, vol. 323, pp. 533\u2013536, 1986.\n[4] J. Martens and I. Sutskever, \u201cLearning recurrent neural networks with Hessian-free optimization,\u201d in ICML\u20192011, 2011.\n[5] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in ICASSP 2011, 2011.\n[6] I. Sutskever, Training Recurrent Neural Networks, Ph.D. thesis, CS Dept., U. Toronto, 2012.\n[7] Yoshua Bengio, Learning deep architectures for AI, Now Publishers, 2009.\n[8] Y. Bengio, A. Courville, and P. Vincent, \u201cUnsupervised feature learning and deep learning: A review and new perspectives,\u201d Tech. Rep., arXiv:1206.5538, 2012.\n[9] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural Computation, vol. 18, pp. 1527\u20131554, 2006.\n[10] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy layer-wise training of deep networks,\u201d in NIPS\u20192006, 2007.\n[11] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun, \u201cEfficient learning of sparse representations with an energy-based model,\u201d in NIPS\u20192006, 2007.\n[12] D. Erhan, Y. Bengio, A. Courville, P. Manzagol, P. Vincent, and S. Bengio, \u201cWhy does unsupervised pre-training help deep learning?,\u201d J. Machine Learning Res., (11) 2010.\n[13] J. Martens, \u201cDeep learning via Hessian-free optimization,\u201d in ICML\u20192010, 2010, pp. 735\u2013742.\n[14] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. Chen, J. Dean, and A. Ng, \u201cBuilding high-level features using large scale unsupervised learning,\u201d in ICML\u20192012, 2012.\n[15] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, \u201cUnderstanding the exploding gradient problem,\u201d Tech. Rep., Universite\u0301 De Montre\u0301al, 2012, arXiv:arXiv:1211.5063.\n[16] Tomas Mikolov, Statistical Language Models based on Neural Networks, Ph.D. thesis, Brno University of Technology, 2012.\n[17] T. Lin, B. G. Horne, P. Tino, and C. L. Giles, \u201cLearning longterm dependencies is not as difficult with NARX recurrent neural networks,\u201d Tech. Rep. UMICAS-TR-95-78, U. Mariland, 1995.\n[18] S. ElHihi and Y. Bengio, \u201cHierarchical recurrent neural networks for long-term dependencies,\u201d in NIPS\u20191995, 1996.\n[19] Herbert Jaeger, Mantas Lukosevicius, Dan Popovici, and Udo Siewert, \u201cOptimization and applications of echo state networks with leaky- integrator neurons,\u201d Neural Networks, vol. 20, no. 3, pp. 335\u2013352, 2007.\n[20] I. Sutskever and G. Hinton, \u201cTemporal kernel recurrent neural networks,\u201d Neural Networks, vol. 23, no. 2, (23) 2, 2010.\n[21] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[22] Udo Siewert and Welf Wustlich, \u201cEcho-state networks with band-pass neurons: Towards generic time-scale-independent reservoir structures,\u201d \u00a1p\u00bfPreliminary Report\u00a1/p\u00bf, October 2007.\n[23] I. Sutskever, G. Hinton, and G. Taylor, \u201cThe recurrent temporal restricted Boltzmann machine,\u201d in NIPS\u20192008. 2009.\n[24] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent, \u201cModeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription,\u201d in ICML\u20192012, 2012.\n[25] H. Larochelle and I. Murray, \u201cThe Neural Autoregressive Distribution Estimator,\u201d in AISTATS\u20192011, 2011.\n[26] X. Glorot, A. Bordes, and Y. Bengio, \u201cDeep sparse rectifier neural networks,\u201d in AISTATS\u20192011, 2011.\n[27] V. Nair and G.E. Hinton, \u201cRectified linear units improve restricted Boltzmann machines,\u201d in ICML\u20192010, 2010.\n[28] A. Krizhevsky, I. Sutskever, and G. Hinton, \u201cImageNet classification with deep convolutional neural networks,\u201d in NIPS\u20192012. 2012.\n[29] Yu Nesterov, \u201cA method for unconstrained convex minimization problem with the rate of convergence o(1/k2),\u201d Doklady AN SSSR (translated as Soviet. Math. Docl.), vol. 269, pp. 543\u2013 547, 1983.\n[30] James Bergstra and Yoshua Bengio, \u201cRandom search for hyper-parameter optimization,\u201d J. Machine Learning Res., vol. 13, pp. 281\u2013305, 2012.\n[31] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in Proc. 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP 2011), 2011."}], "references": [{"title": "Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. M\u00fcnich", "author": ["S. Hochreiter"], "venue": "1991.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE T. Neural Nets, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, pp. 533\u2013536, 1986.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning recurrent neural networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "ICML\u20192011, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "ICASSP 2011, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Training Recurrent Neural Networks, Ph.D", "author": ["I. Sutskever"], "venue": "thesis, CS Dept., U. Toronto,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Now Publishers,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Tech. Rep., arXiv:1206.5538, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS\u20192006, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "NIPS\u20192006, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Manzagol", "P. Vincent", "S. Bengio"], "venue": "J. Machine Learning Res., (11) 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "ICML\u20192010, 2010, pp. 735\u2013742.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "G. Corrado", "K. Chen", "J. Dean", "A. Ng"], "venue": "ICML\u20192012, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Tech. Rep., Universit\u00e9 De Montr\u00e9al, 2012, arXiv:arXiv:1211.5063.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical Language Models based on", "author": ["Tomas Mikolov"], "venue": "Neural Networks, Ph.D. thesis, Brno University of Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning longterm dependencies is not as difficult with NARX recurrent neural networks", "author": ["T. Lin", "B.G. Horne", "P. Tino", "C.L. Giles"], "venue": "Tech. Rep. UMICAS-TR-95-78, U. Mariland, 1995.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. ElHihi", "Y. Bengio"], "venue": "NIPS\u20191995, 1996.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Optimization and applications of echo state networks with leaky- integrator neurons", "author": ["Herbert Jaeger", "Mantas Lukosevicius", "Dan Popovici", "Udo Siewert"], "venue": "Neural Networks, vol. 20, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Temporal kernel recurrent neural networks", "author": ["I. Sutskever", "G. Hinton"], "venue": "Neural Networks, vol. 23, no. 2, (23) 2, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Echo-state networks with band-pass neurons: Towards generic time-scale-independent reservoir structures", "author": ["Udo Siewert", "Welf Wustlich"], "venue": "\u00a1p\u00bfPreliminary Report\u00a1/p\u00bf, October 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "The recurrent temporal restricted Boltzmann machine", "author": ["I. Sutskever", "G. Hinton", "G. Taylor"], "venue": "NIPS\u20192008. 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ICML\u20192012, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "AISTATS\u20192011, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS\u20192011, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML\u20192010, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS\u20192012. 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Yu Nesterov"], "venue": "Doklady AN SSSR (translated as Soviet. Math. Docl.), vol. 269, pp. 543\u2013 547, 1983.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1983}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "J. Machine Learning Res., vol. 13, pp. 281\u2013305, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "Proc. 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP 2011), 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Machine learning algorithms for capturing statistical structure in sequential data face a fundamental problem [1, 2], called the difficulty of learning long-term dependencies.", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "Machine learning algorithms for capturing statistical structure in sequential data face a fundamental problem [1, 2], called the difficulty of learning long-term dependencies.", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "Recurrent neural networks [3] can represent such non-linear maps (F , below) that iteratively build a relevant summary of past observations.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "In principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm [3] to compute a gradient), but it was observed early on [1, 2] that capturing dependencies that span a long interval was difficult, making the task of optimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for some tasks when the span of the dependencies of interest increases sufficiently.", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "In principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm [3] to compute a gradient), but it was observed early on [1, 2] that capturing dependencies that span a long interval was difficult, making the task of optimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for some tasks when the span of the dependencies of interest increases sufficiently.", "startOffset": 171, "endOffset": 177}, {"referenceID": 1, "context": "In principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm [3] to compute a gradient), but it was observed early on [1, 2] that capturing dependencies that span a long interval was difficult, making the task of optimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for some tasks when the span of the dependencies of interest increases sufficiently.", "startOffset": 171, "endOffset": 177}, {"referenceID": 3, "context": "However, a revival of interest in these learning algorithms is taking place, in particular thanks to [4] and [5].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "However, a revival of interest in these learning algorithms is taking place, in particular thanks to [4] and [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "This paper studies the issues giving rise to these difficulties and discusses, reviews, and combines several techniques that have been proposed in order to improve training of RNNs, following up on a recent thesis devoted to the subject [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": ", as add-ons to stochastic gradient descent (SGD), they allow to compete with batch (or large minibatch) second-order methods such as Hessian-Free optimization, recently found to greatly help training of RNNs [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 6, "context": "There has been several breakthroughs in recent years in the algorithms and results obtained with so-called deep learning algorithms (see [7] and [8] for reviews).", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "There has been several breakthroughs in recent years in the algorithms and results obtained with so-called deep learning algorithms (see [7] and [8] for reviews).", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Very little work on deep architectures occurred before the major advances of 2006 [9, 10, 11], probably because of optimization difficulties due to the high level of nonlinearity in deeper networks (whose output is the composition of the non-linearity at each layer).", "startOffset": 82, "endOffset": 93}, {"referenceID": 9, "context": "Very little work on deep architectures occurred before the major advances of 2006 [9, 10, 11], probably because of optimization difficulties due to the high level of nonlinearity in deeper networks (whose output is the composition of the non-linearity at each layer).", "startOffset": 82, "endOffset": 93}, {"referenceID": 10, "context": "Very little work on deep architectures occurred before the major advances of 2006 [9, 10, 11], probably because of optimization difficulties due to the high level of nonlinearity in deeper networks (whose output is the composition of the non-linearity at each layer).", "startOffset": 82, "endOffset": 93}, {"referenceID": 11, "context": "Some experiments [12] showed the presence of an extremely large number of apparent local minima of the training criterion, with no two different initializations going to the same function (i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "Some ill-conditioning has clearly been shown to be involved, especially for the difficult problem of training deep auto-encoders, through comparisons [13] of stochastic gradient descent and Hessian-free optimization (a second order optimiza-", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "These optimization questions become particularly important when trying to train very large networks on very large datasets [14], where one realizes that a major challenge for deep learning is the underfitting issue.", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Hessian-free optimization has been successfully used to considerably extend the span of temporal dependencies that a RNN can learn [4], suggesting that ill-conditioning effects are also at play in the difficulties of training RNN.", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "An important aspect of these difficulties is that the gradient can be decomposed [2, 15] into terms that involve products of Jacobians \u2202ht \u2202ht\u22121 over subsequences linking an event at time t1 and one at time", "startOffset": 81, "endOffset": 88}, {"referenceID": 14, "context": "An important aspect of these difficulties is that the gradient can be decomposed [2, 15] into terms that involve products of Jacobians \u2202ht \u2202ht\u22121 over subsequences linking an event at time t1 and one at time", "startOffset": 81, "endOffset": 88}, {"referenceID": 14, "context": "A much deeper discussion of this issue can be found in [15], along with a point of view inspired by dynamical systems theory and by the geometrical aspect of the problem, having to do with the shape of the training criterion as a function of \u03b8 near those regions of exploding gradient.", "startOffset": 55, "endOffset": 59}, {"referenceID": 3, "context": ", pre-multiplying by the inverse of some proxy for the Hessian matrix) could in principle reduce the exploding and vanishing gradient effects, as argued in [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 15, "context": "To address the exploding gradient effect, [16, 15] recently proposed to clip gradients above a given threshold.", "startOffset": 42, "endOffset": 50}, {"referenceID": 14, "context": "To address the exploding gradient effect, [16, 15] recently proposed to clip gradients above a given threshold.", "startOffset": 42, "endOffset": 50}, {"referenceID": 14, "context": "The specific form of clipping used here was proposed in [15] and is discussed there at much greater length: when the norm of the gradient vector g for a given sequence is above a threshold, the update is done in the direction threshold g ||g|| .", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "As argued in [15], this very", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 218, "endOffset": 226}, {"referenceID": 18, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 218, "endOffset": 226}, {"referenceID": 19, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 236, "endOffset": 240}, {"referenceID": 20, "context": "Long-Short-Term Memory (LSTM) networks [21], which were shown to be able to handle much longer range dependencies, also benefit from a linearly self-connected memory unit with a near 1 self-weight which allows signals (and gradients) to propagate over long time spans.", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "The analogy can be brought one step further by introducing band-pass filter units [22] or by using domain specific knowledge to decide on what frequency bands different units should focus.", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "One way to reduce the underfitting of RNNs is to introduce multiplicative interactions in the parametrization of F , as was done successfully in [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 22, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 221, "endOffset": 229}, {"referenceID": 23, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 221, "endOffset": 229}, {"referenceID": 24, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 275, "endOffset": 283}, {"referenceID": 23, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 275, "endOffset": 283}, {"referenceID": 6, "context": "[7] hypothesized that one reason for the difficulty in optimizing deep networks is that in ordinary neural networks gradients diffuse through the layers, diffusing credit and blame through many units, maybe making it difficult for hidden units to specialize.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "This is what was advocated in [26], exploiting the idea of rectifier non-linearities introduced earlier in [27], i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 26, "context": "This is what was advocated in [26], exploiting the idea of rectifier non-linearities introduced earlier in [27], i.", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "This approach was very successful in recent work on deep learning for object recognition [28], beating by far the state-of-the-art on ImageNet (1000 classes).", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "Nesterov accelerated gradient (NAG) [29] is a first-order optimization method to improve stability and convergence of regular gradient descent.", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "Recently, [6] showed that NAG could be computed by the following update rules:", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "where \u03b8t are the model parameters, vt the velocity, \u03bct \u2208 [0, 1] the momentum (decay) coefficient and \u01ebt > 0 the learning rate at iteration t, f(\u03b8) is the objective function and \u2207f(\u03b8) is a shorthand notation for the gradient \u2202f(\u03b8) \u2202\u03b8 |\u03b8=\u03b8\u2032 .", "startOffset": 57, "endOffset": 63}, {"referenceID": 23, "context": "We evaluate our models on the four polyphonic music datasets of varying complexity used in [24]: classical piano music (Pianomidi.", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "We do so by using random search [30] on the following intervals:", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "nh \u2208 [100, 400] \u01ebt \u2208 [10 , 10]", "startOffset": 21, "endOffset": 30}, {"referenceID": 9, "context": "nh \u2208 [100, 400] \u01ebt \u2208 [10 , 10]", "startOffset": 21, "endOffset": 30}, {"referenceID": 9, "context": "95] \u03bbL1 \u2208 [10 , 10] nleaky \u2208 {0%, 25%, 50%} \u03b1 \u2208 [0.", "startOffset": 10, "endOffset": 19}, {"referenceID": 9, "context": "95] \u03bbL1 \u2208 [10 , 10] nleaky \u2208 {0%, 25%, 50%} \u03b1 \u2208 [0.", "startOffset": 10, "endOffset": 19}, {"referenceID": 30, "context": "Such a factorization allows for faster implementation, as we are not required to evaluate the whole output layer (10000 units) which is the computational bottleneck, but only the output of the corresponding class [31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "We use a small grid-search around the parameters used to get state of the art results for this number of classes [31], i.", "startOffset": 113, "endOffset": 117}], "year": 2017, "abstractText": "After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.", "creator": "LaTeX with hyperref package"}}}