{"id": "1212.1824", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2012", "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes", "abstract": "ordinary stochastic scale gradient descent ( sgd ) is already one of the now simplest and yet most sophisticated popular stochastic optimization methods. while it has already been largely theoretically studied for decades, the classical classical analysis therefore usually contains required complex non - trivial smoothness assumptions, qualities which seemingly do especially not apply to both many newer modern applications applications ways of making sgd with seemingly non - smooth objective smoothing functions internally such as support vector machines. in considering this paper, we investigate the cognitive performance aspects of reaching sgd \\ 0 emph { and without } such smoothness assumptions, efficiently as well well as a running average scheme tree to properly convert the sgd iterates path to a solution with optimal optimization accuracy. in this framework, when we prove furthermore that after t - rounds, thus the computational suboptimality error of performing the \\ emph { ; last } increasing sgd 0 iterate scales as \u03b4 o ( \\ log ( t ) / \\ sqrt { t } ) for our non - improper smooth convex } objective error functions, and o ( \\ log ( t ) / \u00ab t ) in the non - smooth strongly bounded convex case. to offer the best of protecting our abstract knowledge, realizing these are the remarkable first bounds of this initial kind, and then almost necessarily match precisely the minimax - optimal rates more obtainable considerably by appropriate numerical averaging research schemes. we frequently also correctly propose a sufficiently new and reasonably simple resource averaging model scheme, which often not only automatically attains optimal rates, physically but quickly can also be easily computed on - the - fly ( in contrast, rendering the random suffix averaging experimental scheme algorithm proposed late in \\ th citet { z rakhshasri12arxiv } / is much not as simple to yet implement ). finally, we may provide we some simplified experimental illustrations.", "histories": [["v1", "Sat, 8 Dec 2012 18:22:42 GMT  (24kb,D)", "http://arxiv.org/abs/1212.1824v1", "To appear in ICML 2013"], ["v2", "Fri, 28 Dec 2012 10:58:48 GMT  (24kb,D)", "http://arxiv.org/abs/1212.1824v2", "To appear in ICML 2013"]], "COMMENTS": "To appear in ICML 2013", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["ohad shamir", "tong zhang 0001"], "accepted": true, "id": "1212.1824"}, "pdf": {"name": "1212.1824.pdf", "metadata": {"source": "CRF", "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes", "authors": ["Ohad Shamir"], "emails": ["ohadsh@microsoft.com", "tzhang@stat.rutgers.edu"], "sections": [{"heading": null, "text": "\u221a T ) for non-smooth\nconvex objective functions, and O(log(T )/T ) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some experimental illustrations."}, {"heading": "1 Introduction", "text": "This paper considers one of the simplest and most popular stochastic optimization algorithms, namely Stochastic Gradient Descent (SGD). SGD can be used to optimize any convex function F over a convex domainW, given access only to unbiased estimates of F \u2019s subgradients. This feature makes it very useful for learning problems, where our goal is to minimize generalization error based only on a finite sampled training set. Moreover, SGD is extremely simple and highly scalable, making it particularly suitable for large-scale learning problems.\nThe algorithm itself proceeds in rounds, and can be described in just a few lines: We initialize w1 \u2208 W (following common practice, we will assume w1 = 0). At round t = 1, 2, . . ., we obtain a random estimate g\u0302t of a subgradient gt \u2208 \u2202F (wt) so that Eg\u0302t = gt, and update the iterate wt as follows:\nwt+1 = \u03a0W(wt \u2212 \u03b7tg\u0302t),\nwhere \u03b7t is a suitably chosen step-size parameter, and \u03a0W denotes projection on W. In terms of theoretical analysis, SGD has been studied for decades (for instance, see Kushner & Yin (2003) and references therein), but perhaps surprisingly, there are still important gaps left in our understanding of this method. First of all, most classical results look at asymptotic convergence rates, which do not apply to a fixed iteration budget T . In recent years, more attention has been devoted to non-asymptotic bounds (e.g., Bach & Moulines (2011)). However, these classical convergence bounds often make non-trivial smoothness\nar X\niv :1\n21 2.\n18 24\nv1 [\ncs .L\nG ]\n8 D\nec 2\nassumptions on the function F , such as Lipschitz-continuity of the gradient or higher-order derivatives. In modern applications, these assumptions often do not hold. For example, if SGD is used to solve the supportvector machine optimization problem (with the standard non-smooth hinge-loss) on a finite training set, then the underlying objective function F is essentially non-smooth, even at the optimal solution. In general, for machine learning applications F may be non-smooth whenever one uses a non-smooth loss function, and thus a smoothness-based analysis is not appropriate.\nWithout assuming smoothness, most of the existing analysis has been carried out in the context of online learning - a more difficult setting than our stochastic setting, where the subgradients are assumed to be provided by an adversary. Using online-to-batch conversion, it is possible to show that after T iterations, the average of the iterates, (w1 + . . . + wT )/T , has O(log(T )/T ) optimization error for strongly-convex F (see precise definition in Sec. 2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al. (2007); Hazan & Kale (2011). However, Rakhlin et al. (2011) showed that simple averaging is provably suboptimal in a stochastic setting. Instead, they proposed averaging the last \u03b1T iterates of SGD (where \u03b1 \u2208 (0, 1), e.g. 1/2), and showed that this averaging scheme has an optimal O(1/T ) convergence rate. In comparison, in the non-smooth setting, there are \u2126(1/ \u221a T ) and \u2126(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where g\u0302t = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011).\nIn this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions:\n\u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F .\nThese results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting.\n\u2022 We improve the existing expected error bound on the suffix averaging scheme of Rakhlin et al. (2011), from O((1 + log( 11\u2212\u03b1 ))/\u03b1T ) to O(log( 1 min{\u03b1,1\u2212\u03b1} )/T ).\n\u2022 We propose a new and very simple running average scheme, called polynomial-decay averaging, and prove that it enjoys optimal rates of convergence. Unlike suffix-averaging, this new running average scheme can be easily computed on-the-fly.\n\u2022 We provide a simple experimental study of the averaging schemes discussed in the paper.\nWe emphasize that although there exist other algorithms with O(1/T ) convergence rate in the strongly convex case (e.g. Hazan & Kale (2011); Ouyang & Gray (2012)), our focus in this paper is on the simple and widely-used SGD algorithm."}, {"heading": "2 Preliminaries", "text": "We use bold-face letters to denote vectors. We let F denote a convex function over a (closed) convex domain W, which is a subset of some Hilbert space with an induced norm \u2016 \u00b7 \u2016. We assume that F is minimized at some w\u2217 \u2208 W. Besides general convex F , we will also consider the important sub-class of strongly-convex functions. Formally, we say that a function F is \u03bb-strongly convex, if for all w,w\u2032 \u2208 W and any subgradient g of F at w, it holds that\nF (w\u2032) \u2265 F (w) + \u3008g,w\u2032 \u2212w\u3009+ \u03bb 2 \u2016w\u2032 \u2212w\u20162,\nwhere \u03bb > 0. For a general convex function, the above inequality can always be satisfied with \u03bb = 0. As discussed in the introduction, we consider the first-order stochastic optimization setting, where instead of having direct access to F , we only have access to an oracle, which given some w \u2208 W, returns a random vector g\u0302 such that E[g\u0302] \u2208 \u2202F (w). Our goal is to use a bounded number T of oracle calls, and compute some w\u0304 \u2208 W such that the optimization error, F (w\u0304)\u2212 F (w\u2217), is as small as possible. It is well-known that this framework can be applied to learning problems (see for instance Shalev-Shwartz et al. (2009)): given a hypothesis class W and a set of T i.i.d. examples, we wish to find a predictor w whose expected loss F (w) is close to optimal over W. Since the examples are chosen i.i.d., the subgradient of the loss function with respect to any individual example can be shown to be an unbiased estimate of a subgradient of F . We will mostly consider bounds on the expected error (over the oracle\u2019s and algorithm\u2019s randomness) for simplicity, although it is possible to obtain high-probability bounds in some cases.\nIn terms of the step-size \u03b7t in the strongly-convex case, we will generally assume it equals 1/(\u03bbt). We note that this is without much loss of generality, since if the step size is c/\u03bbt for some c \u2265 1, then it is equivalent to taking step sizes 1/(\u03bb\u2032t) where \u03bb\u2032 := \u03bb/c \u2264 \u03bb is a lower-bound on the strong convexity parameter. Since any \u03bb-strongly convex function is also \u03bb\u2032-strongly convex, we can use the analysis here to get upper bounds in terms of \u03bb\u2032, and if so desired, substitute \u03bb/c instead of \u03bb\u2032 in the final bound.\nWhen we run SGD, we let g\u0302t denote the random vector obtained at round t (when we query at wt), and let gt = E[g\u0302t] denote the underlying subgradient of F . To facilitate our convergence bounds, we assume that E[\u2016g\u0302t\u20162] \u2264 G2 for some fixed G. Also, when optimizing general convex functions, we will assume that the diameter of W, namely supw,w\u2032\u2208W \u2016w \u2212w\u2032\u2016, is bounded by some constant D."}, {"heading": "3 Convergence of Individual SGD Iterates", "text": "We begin by considering the case of strongly convex F , and prove the following bound on the expected error of any individual iterate wT . In this theorem as well as later ones, we did not attempt to optimize constants.\nTheorem 1. Suppose F is \u03bb-strongly convex, and that E[\u2016g\u0302t\u20162] \u2264 G2 for all t. Consider SGD with step sizes \u03b7t = 1/\u03bbt. Then for any T > 1, it holds that\nE[F (wT )\u2212 F (w\u2217)] \u2264 17G2(1 + log(T ))\n\u03bbT .\nProof. The beginning of the proof is standard. By convexity of W, we have the following for any w \u2208 W:\nE [ \u2016wt+1 \u2212w\u20162 ] = E[\u2016\u03a0W(wt \u2212 \u03b7tg\u0302t)\u2212w\u20162]\n\u2264 E [ \u2016wt \u2212 \u03b7tg\u0302t \u2212w\u20162 ] \u2264 E [ \u2016wt \u2212w\u20162 ] \u2212 2\u03b7tE[\u3008gt,wt \u2212w\u3009] + \u03b72tG2.\nLet k be an arbitrary element in {1, . . . , bT/2c}. Extracting the inner product, summing over all t =\nT \u2212 k, . . . , T , and rearranging, we get\nT\u2211 t=T\u2212k E[\u3008gt,wt \u2212w\u3009] \u2264 1 2\u03b7T\u2212k E[\u2016wT\u2212k \u2212w\u20162]\n+ T\u2211 t=T\u2212k+1 E[\u2016wt \u2212w\u20162] 2 ( 1 \u03b7t \u2212 1 \u03b7t\u22121 ) + G2 2 T\u2211 t=T\u2212k \u03b7t. (1)\nBy convexity of F , we can lower bound \u3008gt,wt \u2212w\u3009 by F (wt) \u2212 F (w). Plugging this in and substituting \u03b7t = 1/\u03bbt, we get\nE\n[ T\u2211\nt=T\u2212k\n(F (wt)\u2212 F (w)) ] \u2264 \u03bb(T \u2212 k)\n2 E[\u2016wT\u2212k \u2212w\u20162]\n+ \u03bb\n2 T\u2211 t=T\u2212k+1 E[\u2016wt \u2212w\u20162] + G2 2\u03bb T\u2211 t=T\u2212k 1 t . (2)\nNow comes the crucial trick: instead of picking w = w\u2217, as done in standard analysis (Hazan et al. (2007); Rakhlin et al. (2011)), we instead pick w = wT\u2212k. We also use the fact that E [ \u2016wt \u2212w\u2217\u20162 ] \u2264 4G 2\n\u03bb2t (Rakhlin et al. (2011), Lemma 1), which implies that for any t \u2265 T \u2212 k,\nE[\u2016wt \u2212wT\u2212k\u20162] \u22642E [ \u2016wt \u2212w\u2217\u20162 + \u2016wT\u2212k \u2212w\u2217\u20162 ] \u2264 8G 2\n\u03bb2\n( 1\nt +\n1\nT \u2212 k\n) \u2264 16G 2\n\u03bb2(T \u2212 k) \u2264 32G\n2\n\u03bb2T .\nPlugging this back into Eq. (2), we get\nE\n[ T\u2211\nt=T\u2212k\n(F (wt)\u2212 F (wT\u2212k)) ] \u2264 16G 2k\n\u03bbT + G2 2\u03bb T\u2211 t=T\u2212k 1 t .\nLet Sk = 1 k+1 \u2211T t=T\u2212k E[F (wt)] be the expected average value of the last k + 1 iterates. The bound above implies that\n\u2212E[F (wT\u2212k)] \u2264 \u2212E[Sk] + G2\n2\u03bb\n( 32\nT + T\u2211 t=T\u2212k\n1\n(k + 1)t\n) .\nBy the definition of Sk and the inequality above, we have\nkE[Sk\u22121] = (k + 1)E[Sk]\u2212 E[F (wT\u2212k)]\n\u2264 (k + 1)E[Sk]\u2212 E[Sk] + G2\n2\u03bb\n( 32\nT + T\u2211 t=T\u2212k\n1\n(k + 1)t\n) ,\nand dividing by k, implies\nE[Sk\u22121] \u2264 E[Sk] + G2\n2\u03bb\n( 32\nkT + T\u2211 t=T\u2212k\n1\nk(k + 1)t\n) . (3)\nUsing this inequality repeatedly and summing from k = 1 to k = bT/2c, we have\nE[F (wT )] = E[S0] \u2264 E[SbT/2c] + 16G2\n\u03bbT bT/2c\u2211 k=1 1 k\n+ G2\n2\u03bb bT/2c\u2211 k=1 T\u2211 t=T\u2212k\n1\nk(k + 1)t . (4)\nIt now just remains to bound these terms. E[ST/2] is the expected average value of the last bT/2c iterates, which was already analyzed in (Rakhlin et al. (2011), Theorem 5), yielding a bound of\nE[SbT/2c] \u2264 F (w\u2217) + 10G2\n\u03bbT\nfor T > 1. Moreover, we have bT/2c\u2211 k=1 (1/k) \u2264 1 + log(T/2).\nFinally, we have\nbT/2c\u2211 k=1 T\u2211 t=T\u2212k\n1 k(k + 1)t \u2264 bT/2c\u2211 k=1\n1\nk(T \u2212 k)\n= 1\nT bT/2c\u2211 k=1 ( 1 k + 1 T \u2212 k ) \u2264 (1 + log(T ))/T.\nThe result follows by substituting the above bounds into Eq. (4) and simplifying for readability.\nUsing a similar technique, we can also get an individual iterate bound, in the case of a general convex function F that may be non-smooth. We note that a similar technique was used in Zhang (2004), but for a different algorithm (one with constant learning rate), and the result was less explicit.\nTheorem 2. Suppose that F is convex, and that for some constants D,G, it holds that E[\u2016g\u0302t\u2016] \u2264 G2 for all t, and supw,w\u2032\u2208W \u2016w\u2212w\u2032\u2016 \u2264 D. Consider SGD with step sizes \u03b7t = c/ \u221a t where c > 0 is a constant. Then for any T > 1, it holds that\nE[F (wT )\u2212 F (w\u2217)] \u2264 ( D2\nc + cG2\n) 2 + log(T )\u221a\nT .\nProof. The proof begins the same as in Thm. 1 (this time letting k be an element in {1, . . . , T \u2212 1}), up to Eq. (1). Instead of substituting \u03b7t = c/\u03bbt, we substitute \u03b7t = c/ \u221a t, to get the, E[\u2016wt \u2212w\u20162] by D2, pick w = wT\u2212k and slightly simplify to get\nE [\u3008gt,wt \u2212wT\u2212k\u3009]\n\u2264 D 2\n2c\n(\u221a T \u2212 \u221a T \u2212 k ) + G2\n2 T\u2211 t=T\u2212k c\u221a t .\nBy convexity, we can lower bound \u3008gt,wt \u2212wT\u2212k\u3009 by F (wt)\u2212 F (wT\u2212k). Also, it is easy to verify (e.g. by\nintegration) that \u2211T t=T\u2212k 1\u221a t \u2264 2( \u221a T \u2212 \u221a T \u2212 k \u2212 1), hence\nE\n[ T\u2211\nt=T\u2212k\n(F (wt)\u2212 F (wT\u2212k))\n]\n\u2264 ( D2\n2c + cG2\n)(\u221a T \u2212 \u221a T \u2212 k \u2212 1 ) = ( D2\n2c + cG2\n) k + 1\u221a\nT + \u221a T \u2212 k \u2212 1 \u2264 ( D2\n2c + cG2 ) k + 1\u221a T . (5)\nAs in the proof of Thm. 1, let Sk = 1 k+1 \u2211T t=T\u2212k E[F (wt)] be the expected average value of the last K + 1 iterates. The bound above implies that\n\u2212E[F (wT\u2212k)] \u2264 \u2212E[Sk] + D2/2c+ cG2\u221a\nT .\nBy the definition of Sk and the inequality above, we have\nkE[Sk\u22121] = (k + 1)E[Sk]\u2212 E[F (wT\u2212k)]\n\u2264 (k + 1)E[Sk]\u2212 E[Sk] + D2/2c+ cG2\u221a\nT ,\nand dividing by k, implies\nE[Sk\u22121] \u2264 E[Sk] + D2/2c+ cG2\nk \u221a T\n.\nUsing this inequality repeatedly and by summing over k = 1, . . . , T \u2212 1, we have\nE[F (wT )] = E[S0] \u2264 E[ST\u22121] + D2/2c+ cG2\u221a\nT\nT\u22121\u2211 k=1 1 k . (6)\nIt now just remains to bound the terms on the right hand side. Using Eq. (1) with k = T \u2212 1 and w = w\u2217, and upper bounding the norms by D, it is easy to calculate that\nE[ST\u22121]\u2212 F (w\u2217) = 1\nT E [ T\u2211 t=1 E[F (wt)\u2212 F (w\u2217) ]\n\u2264 ( D2\nc + cG2 ) 1\u221a T .\nAlso, we have \u2211T\u22121 k=1 1/k \u2264 (1 + log(T )). Plugging these upper bounds into Eq. (6) and simplifying for readability, we get the required bound."}, {"heading": "4 Averaging Schemes", "text": "The bounds shown in the previous section imply that individual iterates wT have O(log(T )/T ) expected error in the strongly convex case, and O(log(T )/ \u221a T ) expected error in the convex case. These bounds are close but not the same as the minimax optimal rates, which are O(1/T ) and O(1/ \u221a T ) respectively. In this section, we consider averaging schemes, which rather than return individual iterates, return some weighted combination of all iterates w1, . . . ,wT , attaining the minimax optimal rates. We mainly focus here on the\nstrongly-convex case, since simple averaging of all iterates is already known to be optimal (up to constants) in the general convex case.\nWe first examine the case of \u03b1-suffix averaging, defined as the average of the last \u03b1T iterates (where \u03b1 \u2208 (0, 1) is a constant, and \u03b1T is assumed to be an integer):\nw\u0304\u03b1T = 1\n\u03b1T T\u2211 t=(1\u2212\u03b1)T+1 wt.\nIn Rakhlin et al. (2011), it was shown that this averaging scheme results in an optimization error of O((1 + log( 11\u2212\u03b1 ))/\u03b1T ), which is optimal in terms of T , but increases rapidly as we make \u03b1 smaller. The following theorem shows a tighter upper bound of O(log( 1min{\u03b1,1\u2212\u03b1} )/T ), which implies we can be much more flexible in choosing \u03b1. Besides being of independent interest, we will re-use this result in our proofs later on.\nTheorem 3. Under the conditions of Thm. 1, and assuming \u03b1T is an integer, it holds that E[F (w\u0304\u03b1T )\u2212F (w\u2217)] is at most 17G2 ( 1 + log (\n1 min{\u03b1,(1+1/T )\u2212\u03b1} )) \u03bbT .\nProof. Suppose first that \u03b1T \u2264 bT/2c. The proof is mostly identical to that of Thm. 1, except that instead of using Eq. (3) to bound E[S0], we use it to bound E[S\u03b1T\u22121] = 1\u03b1T \u2211T t=(1\u2212\u03b1)T+1 F (wt), which by convexity upper bounds F (w\u0304\u03b1T ). We get:\nE[S\u03b1T\u22121] \u2264 E[SbT/2c] + 16G2\n\u03bbT bT/2c\u2211 k=\u03b1T 1 k\n+ G2\n2\u03bb bT/2c\u2211 k=\u03b1T T\u2211 t=T\u2212k\n1\nk(k + 1)t ,\nUsing the same argument as in the proof of Thm. 1, and the fact that \u2211\u03b2T k=\u03b1T 1 k \u2264 1 + log(\u03b2/\u03b1) for any integers \u03b1T, \u03b2T that are no larger than T , we can obtain the upper bounds\nE[SbT/2c \u2264F (w\u2217) + 10G2/\u03bbT bT/2c\u2211 k=\u03b1T 1 k \u22641 + log(1/2\u03b1)\nand\nbT/2c\u2211 k=\u03b1T T\u2211 t=T\u2212k\n1 k(k + 1)t \u2264 1 T bT/2c\u2211 k=\u03b1T ( 1 k + 1 T \u2212 k )\n\u2264 1 T ((1 + log(1/2\u03b1)) + (1 + log(2(1\u2212 \u03b1)))) \u2264 1 T (2 + log(1/\u03b1)) .\nUsing the above estimates, with some simplifications for readability, we get that E[F (w\u0304\u03b1T ) \u2212 F (w\u2217)] is at most\n17 ( 1 + log ( 1\n\u03b1\n)) G2\n\u03bbT . (7)\nThis analysis assumed \u03b1T \u2264 bT/2c. If \u03b1 is larger, we can use the existing analysis (Rakhlin et al. (2011), Theorem 5), and get that E[F (w\u0304\u03b1T )\u2212 F (w\u2217)] is at most(\n4 + 5 log\n( 1\n1 + 1/T \u2212 \u03b1\n)) G2\n\u03bbT . (8)\nCombining Eq. (7) and Eq. (8) with a uniform upper bound which holds for all \u03b1, we get the required bound.\nWe note that in the general convex case without assuming strong convexity, one can use an analogous proof to show an upper bound of order log(1/\u03b1)/ \u221a T for \u03b1-suffix averaging. In contrast, existing techniques only imply a bound of order 1/ \u221a \u03b1T .\nAs discussed in the introduction, a limitation of suffix averaging is that unless we can store all iterates in memory, it requires us to guess the stopping time T in advance. For example, if we do 1/2-suffix averaging, we need to \u201cknow\u201d when we got to iterate T/2 and should start averaging. In practice, the stopping time T is often not known in advance and is determined empirically (e.g. by estimating the error on held-out data till satisfactory performance is obtained). One way to handle this is to decide in advance on a fixed schedule of stopping times T (e.g. T0, 2T0, 2 2T0, 2 3T0, . . . for some T0) and maintain suffix-averages only for those times. However, this is still not very flexible. In contrast, maintaining the average of all iterates up to time t can be done on-the-fly: we initialize w\u03041 = w1, and for any t > 1, we let\nw\u0304t =\n( 1\u2212 1\nt\n) w\u0304t\u22121 + 1\nt wt. (9)\nUnfortunately, returning the average of all iterates as in Eq. (9) is provably suboptimal and can harm performance Rakhlin et al. (2011). Alternatively, we can easily maintain and return the current iterate wt, but the bound we have for it is only O(log(t)/t), worse than the minimax-optimal O(1/t) we can get with suffix averaging.\nIn the following, we analyze a new and very simple running average scheme, denoted as polynomial-decay averaging, and show that it combines the best of both worlds: it can easily be computed on the fly, and it gives an optimal rate. It is parameterized by a number \u03b7 \u2265 0, which should be thought of as a small constant (e.g. \u03b7 = 3), and the procedure is defined as follows: w\u0304\u03b71 = w1, and for any t > 1,\nw\u0304\u03b7t =\n( 1\u2212 \u03b7 + 1\nt+ \u03b7\n) w\u0304\u03b7t\u22121 + \u03b7 + 1\nt+ \u03b7 wt. (10)\nNote that for \u03b7 = 0, this is exactly standard averaging (see Eq. (9)), whereas \u03b7 > 0 reduces the weight of earlier iterates compared to later ones. Moreover, w\u0304\u03b7t can be computed on-the-fly, just as easily as computing a standard average. An analysis of this averaging scheme in the strongly-convex case is provided in the theorem below.\nTheorem 4. Suppose F is \u03bb-strongly convex, and that E[\u2016g\u0302t\u20162] \u2264 G2 for all t. Consider SGD initialized with w1 and step-sizes \u03b7t = 1/\u03bbt. Also, let \u03b7 \u2265 2 be an integer. Then E [F (w\u03b7T )\u2212 F (w\u2217)] is at most\n17 ( 1 + \u03b7\nT\n)( \u03b7(\u03b7 + 1) + (\u03b7 + 0.5)3(1 + log(T ))\nT\n) G2\n\u03bbT\nAssuming that \u03b7 is an integer is merely for simplicity.\nProof. We can rewrite the recursion as\nw\u0304\u03b7t = t\u2212 1 t+ \u03b7 w\u0304\u03b7t\u22121 + \u03b7 + 1 t+ \u03b7 wt\nfor t \u2265 1 with w\u0304\u03b70 = 0. Unwrapping the recursion, we have that for any T \u2265 1, w\u0304 \u03b7 T = \u2211T t=1 \u03b1twt,, where\n\u03b1t = \u03b7 + 1\nt+ \u03b7 T\u220f j=t+1 j \u2212 1 j + \u03b7 ,\nand at t = T , the convention that \u220fT j=T+1((j \u2212 1)/(j + \u03b7)) = 1 is used.\nWe now denote F \u2032(w) = F (w)\u2212F (w\u2217). Since w\u0304\u03b7T is a weighted average of w1, . . . ,wT , where the weights \u03b1t sum up to be 1, it follows by the convexity of F and Jensen\u2019s inequality that F \u2032 (w\u0304\u03b7T ) \u2264 \u2211T t=1 \u03b1tF \u2032(wt).\nLet S\u2032k = \u2211T t=T\u2212k F \u2032(wt), and let \u03b10 = 0, then we have\nF \u2032 (w\u0304\u03b7T ) \u2264 T\u2211 t=1 (\u03b1t \u2212 \u03b1t\u22121)S\u2032T\u2212t. (11)\nIt is not difficult to check that for all t \u2265 1:\n\u03b1t \u2212 \u03b1t\u22121 = \u03b7(\u03b7 + 1)\n(t\u2212 1 + \u03b7)(t+ \u03b7) T\u220f j=t+1 j \u2212 1 j + \u03b7\n= \u03b7(\u03b7 + 1)\nT (T + 1) T+1\u220f j=t\nj\nj \u2212 1 + \u03b7\n\u2264  \u03b7(\u03b7+1)T (T+1) ( t\u22122+\u03b7 T+\u03b7 )\u03b7\u22121 if t \u2264 T + 2\u2212 \u03b7\n\u03b7(\u03b7+1) T (T+1) otherwise\n\u2264 \u03b7(\u03b7 + 1)(t+ \u03b7) T (T + 1)(T + 2) ,\nwhere the derivation of the last inequality has used \u03b7 \u2265 2. As to S\u2032T\u2212t in Eq. (11), note that the upper bound proof of Thm. 3 equally applies to 1T\u2212t+1S \u2032 T\u2212t. Using this bound and substituting in Eq. (11), we obtain\nF \u2032 (w\u0304\u03b7T )\n\u2264 T\u2211 t=1 \u03b1t(T \u2212 t+ 1) 17G2 log\n( Te\nmin{t,T\u2212t+1} ) \u03bbT\n\u2264 dT/2e\u2211 t=1 2\u03b7(\u03b7 + 1)(t+ \u03b7) T (T + 1)(T + 2) (T + \u03b7) 17G2 log (Te/t) \u03bbT \u226434G 2\u03b7(\u03b7 + 1)(T + \u03b7)\n\u03bbT 2(T + 1)(T + 2) (A+B + C), (12)\nwhere\nA = dT/2e\u2211 t=1 \u03b7 log(Te/t) \u2264 \u03b7T + 1 2 log(Te),\nand\nB = dT/2e\u2211 t=1 t log(Te) \u2264 0.5(dT/2e)(dT/2e+ 1) log(Te),\nand\nC \u2264\u2212 dT/2e\u2211 t=1 t log(t) \u2264 \u2212 \u222b dT/2e t=1 t log(t)dt\n=\u2212 [ 0.5t2 log t\u2212 0.25t2 ] \u2223\u2223dT/2e 1 =\u2212 0.5dT/2e2 log(T/2) + 0.25dT/2e2 \u2212 0.25.\nTherefore we have\nA+B + C\n\u2264(\u03b7 + 0.5)T + 1 2 log(Te)\n+ 0.5 (T + 1)2\n4 log(2e1.5)\u2212 0.25\n\u2264(\u03b7 + 0.5)T + 1 2 log(Te) + 0.5(T + 1)(T + 2).\nPlugging this estimate into Eq. (12) and simplifying, we obtain the desired bound.\nNote that for a constant \u03b7, the bound is essentially optimal. We end by noting that using an identical proof technique, it holds in the case of general convex F (with assumptions similar to Thm. 2) that\nE [F (w\u03b7T )\u2212 F (w \u2217)] \u2264 O\n( \u03b7(D2/c+ cG2)\u221a\nT\n) ,\nthis implies that polynomial-decay averaging is also optimal (up to constants) in the general convex case."}, {"heading": "5 Experiments", "text": "In this section, we study the behavior of the polynomial-decay averaging scheme on a few strongly-convex optimization problems. We chose the same 3 binary classification datasets ((ccat,cov1 and astro-ph) and experimental setup as in Rakhlin et al. (2011). For each dataset {xi, yi}mi=1, we ran SGD on the support vector machine optimization problem\nF (w) = \u03bb 2 \u2016w\u20162 + 1 m m\u2211 i=1 max{0, 1\u2212 yi\u3008xi,w\u3009},\nwith the domainW = Rd, where the stochastic gradient given wt was computed by taking a single randomly drawn training example (xi, yi) and computing the gradient with respect to that example, i.e. g\u0302t = \u03bbwt \u2212 1yi\u3008xi,wt\u3009\u22641yixi. All algorithms were initialized at w1 = 0. Following previous work, we chose \u03bb = 10\n\u22124 for ccat, \u03bb = 10\u22126 for cov1, and \u03bb = 5\u00d7 10\u22125 for astro-ph. The \u03b7 parameter of polynomial-decay averaging was set to 3. For comparison, besides polynomial-decay averaging, we also ran suffix averaging with \u03b1 = 1/2, and simple averaging of all iterates. The results are reported in the figure below. Each graph is a log-log plot representing the training error on one dataset over 10 repetitions, as a function of the number of iterations. We also experimented on the test set provided with each dataset, but omit the results as they are very similar.\nThe graphs below clearly indicate that polynomial-decay averaging work quite well. Achieving the best or almost-best performance in all cases. Suffix averaging performs performs similarly, although as discussed earlier, it is not as amenable to on-the-fly computation. Compared to these schemes, a simple average of all iterates is significantly suboptimal, matching the results of Rakhlin et al. (2011)."}, {"heading": "6 Discussion", "text": "In this paper, we investigated the convergence behavior of SGD, and the averaging schemes required to obtain optimal performance. In particular, we considered polynomial-decay averaging, which is as simple to compute as standard averaging of all iterates, but attains better performance theoretically and in practice. We also extended the existing analysis of SGD by providing new finite-sample bounds on individual SGD iterates, which hold without any smoothness assumptions, for both convex and strongly-convex problems.\nFinally, we provided new bounds for suffix averaging. While we focused on standard gradient descent, our techniques can be extended to the more general mirror descent framework and non-Euclidean norms.\nAn important open question is whether the O(log(T )/T ) rate we obtained on the individual iterate wT , for strongly-convex problems, is tight. This question is important, because running SGD for T iterations,\n0 2 4 6 8 10 12 \u22122\n0\n2\n4\n6\n8\n10\nlog(T)\nlo g(\nF (\u22c5)\n)\nASTRO\n0 2 4 6 8 10 12 \u22122\n0\n2\n4\n6\n8\nlog(T)\nlo g(\nF (\u22c5)\n)\nCCAT\n0 5 10 15\n0\n5\n10\nlog(T)\nlo g(\nF (\u22c5)\n)\nCOV1\nPoly. Decay Suffix Simple\nand returning the last iterate wT , is a very common heuristic. If the O(log(T )/T ) bound is tight, it means practitioners should not return the last iterate, since better O(1/T ) rates can be obtained by suffix averaging or polynomial-decay averaging. Alternatively, a O(1/T ) bound on the last iterate can indicate\nthat returning the last iterate is indeed justified. For a further discussion of this, see Shamir (2012). Another question is whether high-probability versions of our individual iterate bounds (Thm. 1 and Thm. 2) can be obtained, especially in the strongly-convex case. Again, this question has practical implications, since if a high-probability bound does not hold, it might imply that the last iterate can suffer from high variability, and should be used with caution. Finally, the tightness of Thm. 2 is still unclear. In fact, even for the simpler case of (non-stocahstic) gradient descent, we do not know whether the behavior of the last iterate proved in Thm. 2 is tight. In general, for an algorithm as simple and popular as SGD, we should have a better understanding of how it behaves and how it should be used in an optimal way."}], "references": [{"title": "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization", "author": ["A. Agarwal", "P. Bartlett", "P. Ravikumar", "M. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["F. Bach", "E. Moulines"], "venue": "In NIPS,", "citeRegEx": "Bach and Moulines,? \\Q2011\\E", "shortCiteRegEx": "Bach and Moulines", "year": 2011}, {"title": "Beyond the regret minimization barrier: An optimal algorithm for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "In COLT,", "citeRegEx": "Hazan and Kale,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2011}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["H. Kushner", "G. Yin"], "venue": null, "citeRegEx": "Kushner and Yin,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "Stochastic smoothing for nonsmooth minimizations: Accelerating sgd by exploiting structure", "author": ["H. Ouyang", "A. Gray"], "venue": "In ICML,", "citeRegEx": "Ouyang and Gray,? \\Q2012\\E", "shortCiteRegEx": "Ouyang and Gray", "year": 2012}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "CoRR, abs/1109.5647,", "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Pegasos: primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Is averaging needed for strongly convex stochastic gradient descent? Open problem presented at COLT", "author": ["O. Shamir"], "venue": null, "citeRegEx": "Shamir,? \\Q2012\\E", "shortCiteRegEx": "Shamir", "year": 2012}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In ICML,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement).", "startOffset": 186, "endOffset": 208}, {"referenceID": 4, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al.", "startOffset": 46, "endOffset": 63}, {"referenceID": 2, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al. (2007); Hazan & Kale (2011).", "startOffset": 64, "endOffset": 84}, {"referenceID": 2, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al. (2007); Hazan & Kale (2011). However, Rakhlin et al.", "startOffset": 64, "endOffset": 105}, {"referenceID": 2, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al. (2007); Hazan & Kale (2011). However, Rakhlin et al. (2011) showed that simple averaging is provably suboptimal in a stochastic setting.", "startOffset": 64, "endOffset": 137}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues.", "startOffset": 142, "endOffset": 164}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)).", "startOffset": 142, "endOffset": 431}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case.", "startOffset": 142, "endOffset": 593}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, .", "startOffset": 142, "endOffset": 1091}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F .", "startOffset": 142, "endOffset": 1600}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization.", "startOffset": 142, "endOffset": 2143}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting.", "startOffset": 142, "endOffset": 2453}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting. \u2022 We improve the existing expected error bound on the suffix averaging scheme of Rakhlin et al. (2011), from O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ) to O(log( 1 min{\u03b1,1\u2212\u03b1} )/T ).", "startOffset": 142, "endOffset": 2590}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting. \u2022 We improve the existing expected error bound on the suffix averaging scheme of Rakhlin et al. (2011), from O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ) to O(log( 1 min{\u03b1,1\u2212\u03b1} )/T ). \u2022 We propose a new and very simple running average scheme, called polynomial-decay averaging, and prove that it enjoys optimal rates of convergence. Unlike suffix-averaging, this new running average scheme can be easily computed on-the-fly. \u2022 We provide a simple experimental study of the averaging schemes discussed in the paper. We emphasize that although there exist other algorithms with O(1/T ) convergence rate in the strongly convex case (e.g. Hazan & Kale (2011); Ouyang & Gray (2012)), our focus in this paper is on the simple and widely-used SGD algorithm.", "startOffset": 142, "endOffset": 3123}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting. \u2022 We improve the existing expected error bound on the suffix averaging scheme of Rakhlin et al. (2011), from O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ) to O(log( 1 min{\u03b1,1\u2212\u03b1} )/T ). \u2022 We propose a new and very simple running average scheme, called polynomial-decay averaging, and prove that it enjoys optimal rates of convergence. Unlike suffix-averaging, this new running average scheme can be easily computed on-the-fly. \u2022 We provide a simple experimental study of the averaging schemes discussed in the paper. We emphasize that although there exist other algorithms with O(1/T ) convergence rate in the strongly convex case (e.g. Hazan & Kale (2011); Ouyang & Gray (2012)), our focus in this paper is on the simple and widely-used SGD algorithm.", "startOffset": 142, "endOffset": 3145}, {"referenceID": 7, "context": "It is well-known that this framework can be applied to learning problems (see for instance Shalev-Shwartz et al. (2009)): given a hypothesis class W and a set of T i.", "startOffset": 91, "endOffset": 120}, {"referenceID": 3, "context": "Now comes the crucial trick: instead of picking w = w\u2217, as done in standard analysis (Hazan et al. (2007); Rakhlin et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 3, "context": "Now comes the crucial trick: instead of picking w = w\u2217, as done in standard analysis (Hazan et al. (2007); Rakhlin et al. (2011)), we instead pick w = wT\u2212k.", "startOffset": 86, "endOffset": 129}, {"referenceID": 3, "context": "Now comes the crucial trick: instead of picking w = w\u2217, as done in standard analysis (Hazan et al. (2007); Rakhlin et al. (2011)), we instead pick w = wT\u2212k. We also use the fact that E [ \u2016wt \u2212w\u2217\u20162 ] \u2264 4G 2 \u03bb2t (Rakhlin et al. (2011), Lemma 1), which implies that for any t \u2265 T \u2212 k, E[\u2016wt \u2212wT\u2212k\u2016] \u22642E [ \u2016wt \u2212w\u2217\u20162 + \u2016wT\u2212k \u2212w\u2217\u20162 ] \u2264 8G 2 \u03bb2 ( 1 t + 1 T \u2212 k ) \u2264 16G 2 \u03bb2(T \u2212 k) \u2264 32G 2 \u03bb2T .", "startOffset": 86, "endOffset": 233}, {"referenceID": 6, "context": "E[ST/2] is the expected average value of the last bT/2c iterates, which was already analyzed in (Rakhlin et al. (2011), Theorem 5), yielding a bound of", "startOffset": 97, "endOffset": 119}, {"referenceID": 10, "context": "We note that a similar technique was used in Zhang (2004), but for a different algorithm (one with constant learning rate), and the result was less explicit.", "startOffset": 45, "endOffset": 58}, {"referenceID": 6, "context": "In Rakhlin et al. (2011), it was shown that this averaging scheme results in an optimization error of O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ), which is optimal in terms of T , but increases rapidly as we make \u03b1 smaller.", "startOffset": 3, "endOffset": 25}, {"referenceID": 6, "context": "If \u03b1 is larger, we can use the existing analysis (Rakhlin et al. (2011), Theorem 5), and get that E[F (w\u0304 T )\u2212 F (w\u2217)] is at most ( 4 + 5 log ( 1 1 + 1/T \u2212 \u03b1 )) G \u03bbT .", "startOffset": 50, "endOffset": 72}, {"referenceID": 6, "context": "(9) is provably suboptimal and can harm performance Rakhlin et al. (2011). Alternatively, we can easily maintain and return the current iterate wt, but the bound we have for it is only O(log(t)/t), worse than the minimax-optimal O(1/t) we can get with suffix averaging.", "startOffset": 52, "endOffset": 74}, {"referenceID": 6, "context": "We chose the same 3 binary classification datasets ((ccat,cov1 and astro-ph) and experimental setup as in Rakhlin et al. (2011). For each dataset {xi, yi}i=1, we ran SGD on the support vector machine optimization problem", "startOffset": 106, "endOffset": 128}, {"referenceID": 6, "context": "Compared to these schemes, a simple average of all iterates is significantly suboptimal, matching the results of Rakhlin et al. (2011).", "startOffset": 113, "endOffset": 135}], "year": 2017, "abstractText": "Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the last SGD iterate scales as O(log(T )/ \u221a T ) for non-smooth convex objective functions, and O(log(T )/T ) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some experimental illustrations.", "creator": "LaTeX with hyperref package"}}}