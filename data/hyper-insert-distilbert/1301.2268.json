{"id": "1301.2268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables", "abstract": "nonlinear global variational binary approximation methods encoded in graphical image models allow efficient approximate inference engines of complex posterior magnitude distributions by using a simpler model. the choice result of capturing the approximating model determines a tradeoff problem between the complex complexity estimates of the numerical approximation implementation procedure and enhance the quality of presenting the approximation. in this paper, we consider geometric variational approximations based on two classes combinations of quantitative models that are structurally richer than traditional standard filtered bayesian correlation networks, potentially markov networks \u2013 or mixture models. as such, these intermediate classes allow implementations to find better tradeoffs in particular the spectrum nature of approximations. eliminating the first class of models are chain graphs, examples which capture distributions that are partially directed. the second class of linear models are classical directed loop graphs ( bayesian prediction networks ) mixed with numerous additional latent variables. separating both latter classes allow ample representation of underlying multi - objective variable dependencies phenomena that cannot additionally be efficiently easily easily represented within representing a bayesian network.", "histories": [["v1", "Thu, 10 Jan 2013 16:23:26 GMT  (1178kb)", "http://arxiv.org/abs/1301.2268v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tal el-hay", "nir friedman"], "accepted": false, "id": "1301.2268"}, "pdf": {"name": "1301.2268.pdf", "metadata": {"source": "CRF", "title": "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables", "authors": ["Tal El-Hay", "Nir Friedman"], "emails": ["@cs.huji.ac.il"], "sections": null, "references": [{"title": "Tractable variational structures for approximating graphical models", "author": ["D. Barber", "W. Wiegerinck"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Elements of Informa\u00ad", "author": ["T.M. Cover", "J. A Thomas"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "Saul and Jordan [ 1 0] sug\u00ad gest to circumvent this problem by using structured variationa! approximation.", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 1, "context": "A common measure of distance is the KL divergence [2] between Q(T : 8) and the posterior distribution P(T I o).", "startOffset": 50, "endOffset": 53}], "year": 2011, "abstractText": "Global variational approximation methods in graphical models allow efficient approximate inference of com\u00ad plex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture mod\u00ad els. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are elwin graphs, which capture distributions that are partially directed. The second class of mod\u00ad els are directed graphs (Bayesian networks) with addi\u00ad tional latent variables. Both classes allow representa\u00ad tion of multi-variable dependencies that cannot be eas\u00ad ily represented within a Bayesian network.", "creator": "pdftk 1.41 - www.pdftk.com"}}}