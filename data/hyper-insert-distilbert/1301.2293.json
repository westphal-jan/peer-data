{"id": "1301.2293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Aggregating Learned Probabilistic Beliefs", "abstract": "we consider the possible task of collectively aggregating statistical beliefs consisted of severalexperts. we thus assume that these beliefs are essentially represented just as discrete probabilitydistributions. we cannot argue that modeling the evaluation requirements of any statistical aggregationtechnique depends on checking the overall semantic database context content of coordinating this entire task. we propose aframework, in which we assume suddenly that nature analytics generates samples from a ` true'distribution map and gradually different indicator experts form beliefs their beliefs correctly based onthe subsets component of the data they ultimately have within a chance taken to observe. naturally, generating theideal aggregate distribution would necessarily be the object one learned from generating thecombined sample sets. getting such a numerical formulation leads to finding a natural way statistical tomeasure the outcome accuracy ahead of the aggregation allocation mechanism. we show today that essentially the relatively well - known convex aggregation reasoning operator requiring linop iteration is ideallysuited well for that task. we propose a linop - based learning evaluation algorithm, inspired by pioneering the techniques developed for bayesian learning, when whichaggregates across the experts'approximate distributions adequately represented as bayesiannetworks. arguably our final preliminary experiments show that roughly this implementation algorithmperforms well matched in practice.", "histories": [["v1", "Thu, 10 Jan 2013 16:25:16 GMT  (914kb)", "http://arxiv.org/abs/1301.2293v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pedrito maynard-reid ii", "urszula chajewska"], "accepted": false, "id": "1301.2293"}, "pdf": {"name": "1301.2293.pdf", "metadata": {"source": "CRF", "title": "Aggregating Learned Probabilistic Beliefs", "authors": ["Pedrito Maynard-Reid"], "emails": ["urszula@cs.stanford.edu"], "sections": null, "references": [{"title": "Theory refinement on bayesian net\u00ad", "author": ["W. Buntine"], "venue": null, "citeRegEx": "Buntine.,? \\Q1991\\E", "shortCiteRegEx": "Buntine.", "year": 1991}, {"title": "A normative examination of ensemble learning algorithms", "author": ["D.M. Pennock", "P.E. Horvitz"], "venue": "In Proc. ICML'OO,", "citeRegEx": "Pennock and Horvitz.,? \\Q2000\\E", "shortCiteRegEx": "Pennock and Horvitz.", "year": 2000}], "referenceMentions": [], "year": 2011, "abstractText": "We consider the task of aggregating beliefs of sev\u00ad eral experts. We assume that these beliefs are rep\u00ad resented as probability distributions. We argue that the evaluation of any aggregation technique depends on the semantic context of this task. We propose a framework, in which we assume that nature generates samples from a 'true' distribution and different experts form their beliefs based on the subsets of the data they have a chance to observe. Naturally, the optimal ag\u00ad gregate distribution would be the one learned from the combined sample sets. Such a formulation leads to a natural way to measure the accuracy of the aggregation mechanism. We show that the well-known aggregation operator LinOP is ideally suited for that task. We propose a LinOP-based learning algorithm, inspired by the techniques developed for Bayesian learning, which aggregates the experts' distributions represented as Bayesian networks. We show experimentally that this algorithm performs well in practice.", "creator": "pdftk 1.41 - www.pdftk.com"}}}