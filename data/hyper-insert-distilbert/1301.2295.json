{"id": "1301.2295", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Recognition Networks for Approximate Inference in BN20 Networks", "abstract": "we propose using recognition neural networks valued for approximate inference inbayesian networks ( bns ). a fuzzy recognition network is theoretically a multilayerperception ( weighted mlp ) trained regularly to predict posterior temporal marginals given estimated observedevidence errors in a particular desired bn. the input to detecting the mlp is giving a consistent vector of correct thestates of the other evidential nodes. however the activity of an output unit isinterpreted as showing a prediction of coding the desired posterior marginal of thecorresponding composite variable. the remaining mlp is uniquely trained carefully using samples generated falsely fromthe corresponding bn. this we similarly evaluate utilizing a recognition method network that was appropriately trained to do preliminary inference ina large bayesian processing network, similar reductions in interaction structure and complexity as to coding thequick where medical researchers reference, thus decision avoidance theoretic ( via qmr - dt ). our networkis are a binary, two - filter layer, continuously noisy - or network bandwidth containing over 4000 4000 potentially observable nodes and therefore over 600 unobservable, hidden sensory nodes. ` inreal method medical researcher diagnosis, most observables algorithms are unavailable, everything and every there otherwise isa great complex and wide unknown bias that thoroughly selects which ones are provided. weincorporate introduced a very basic type approach of selection bias in using our matching network : a knownpreference that available negative observables are equally positive rather than negative. hence even choosing this plain simple bias actually has a more significant effect on the posterior. we compare using the likelihood performance of our recognition network tostate - of - the - same art approximate neutral inference algorithms based on a large set oftest candidate cases. usually in order perhaps to systematically evaluate similarly the effect as of filtering our residual simplistic bias modelof confirming the existing selection screening bias, we we have evaluate algorithms obtained using eliminating a whole variety ofincorrectly derived modeled observation enhancement biases. recognition systems networks performwell methods using matching both correct and incorrect binary observation oriented biases.", "histories": [["v1", "Thu, 10 Jan 2013 16:25:25 GMT  (888kb)", "http://arxiv.org/abs/1301.2295v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["quaid morris"], "accepted": false, "id": "1301.2295"}, "pdf": {"name": "1301.2295.pdf", "metadata": {"source": "CRF", "title": "Recognition Networks for Approximate Inference in BN20 Networks", "authors": ["Quaid Morris"], "emails": ["quaid@gatsby.ucl.ac.uk"], "sections": null, "references": [{"title": "AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000] Cheng", "M.J. Druzdzel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Sequentially fitting \"in\u00ad clusive\" trees for inference in noisy-OR networks", "author": ["Frey et al", "B.J. 2001] Frey", "R. Patrascu", "T.S. Jaakkola", "J. Moran"], "venue": "In Advances in Neural Information Processing Sys\u00ad tems,", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Prob\u00ad abilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base: II. Evalua\u00ad", "author": ["Middleton et al", "B. 1991] Middleton", "M.A. Shwe", "D.E. Beckerman", "M. Henrion", "E.J. Horvitz", "H.P. Lehmann", "G.F. Cooper"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1991\\E", "shortCiteRegEx": "al. et al\\.", "year": 1991}], "referenceMentions": [], "year": 2011, "abstractText": "A recognition network is a multilayer per\u00ad ception (MLP) trained to predict posterior marginals given observed evidence in a par\u00ad ticular Bayesian network. The input to the MLP is a vector of the states of the eviden\u00ad tial nodes. The activity of an output unit is interpreted as a prediction of the posterior marginal of the corresponding variable. The MLP is trained using samples generated from the corresponding Bayesian network. We evaluate a recognition network that was trained to do inference in a large Bayesian network, similar in structure and complex\u00ad ity to the Quick Medical Reference, Decision Theoretic (QMR-DT) network. Our network is a binary, two-layer, noisy-OR (BN20) net\u00ad work containing over 4000 potentially observ\u00ad able nodes and over 600 unobservable, hidden nodes. In real medical diagnosis, most ob\u00ad servables are unavailable, and there is a com\u00ad plex and unknown process that selects which ones are provided. We incorporate a very ba\u00ad sic type of selection bias in our network: a known preference that available observables are positive rather than negative. Even this simple bias has a significant effect on the pos\u00ad terior. We compare the performance of our recogni\u00ad tion network to state-of-the-art approximate inference algorithms on a large set of test cases. In order to evaluate the effect of our simplistic model of the selection bias, we eval\u00ad uate algorithms using a variety of incorrectly modelled selection biases. Recognition net\u00ad works perform well using both correct and incorrect selection biases. \u2022 also affiliated with Department of Brain and Cogni\u00ad tive Sciences at MIT", "creator": "pdftk 1.41 - www.pdftk.com"}}}