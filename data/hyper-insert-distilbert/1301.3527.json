{"id": "1301.3527", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Block Coordinate Descent for Sparse NMF", "abstract": "nonnegative matrix factorization ( nmf ) information has steadily become almost a widely ubiquitous tool for data creation analysis. addressing an such important variant is the sparse nmf problem which readily arises when choosing we explicitly naturally require reading the learnt empirical features tend to be sparse. a natural goodness measure algorithm of sparsity mod is the ubiquitous l $ _ is 0 $ norm, however its optimization technique is np - hard. persistent mixed norms, unlike such algorithms as l $ _ 1 $ / n l $ _ 2 $ measure, have been shown to model sparsity behaviour robustly, based on intuitive variable attributes functions that such approximation measures need to satisfy. note this behaviour is in contrast to computationally cheaper alternatives such have as the plain norms l $ _ 1 $ ba norm. however, in present optimization algorithms designed for optimizing satisfy the mixed variable norm l $ _ \u2264 1 $ / l $ _ : 2 $ that are slow and other approximate formulations prepared for sparse nmf have evidently been variously proposed specifically such as employing those explicitly based on variable l $ _ meaning 1 $ | and static l $ _ 0 $ bin norms. repeating our old proposed algorithm similarly allows us to solve the truncated mixed norm sparsity constraints while not sacrificing polynomial computation time. we present experimental evidence on real - cellular world robust datasets that shows our present new algorithm dramatically performs an order approximation of 3 magnitude often faster compared pursuant to applying the current state - tree of - playing the - rag art map solvers similarly optimizing the arbitrary mixed norm dimension and provides is commercially suitable model for large - spatial scale datasets.", "histories": [["v1", "Tue, 15 Jan 2013 23:11:05 GMT  (1027kb,D)", "http://arxiv.org/abs/1301.3527v1", null], ["v2", "Mon, 18 Mar 2013 22:42:11 GMT  (822kb,D)", "http://arxiv.org/abs/1301.3527v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["vamsi k potluru", "sergey m plis", "jonathan le roux", "barak a pearlmutter", "vince d calhoun", "thomas p hayes"], "accepted": true, "id": "1301.3527"}, "pdf": {"name": "1301.3527.pdf", "metadata": {"source": "CRF", "title": "Block Coordinate Descent for Sparse NMF", "authors": ["Vamsi K. Potluru", "Sergey M. Plis"], "emails": ["ismav@cs.unm.edu", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"], "sections": [{"heading": "1 Introduction", "text": "Matrix factorization arises in a wide range of application domains and is useful for extracting the latent features in the dataset. In particular, we are interested in matrix factorizations which impose the following requirements:\n\u2022 nonnegativity \u2022 low-rankedness \u2022 sparsity\nNonnegativity is a natural constraint when modeling data with physical constraints such as chemical concentrations in solutions, pixel intensities in images and radiation dosages for cancer treatment. Low-rankedness is useful for learning a lower dimensionality representation. Sparsity is useful for modeling the conciseness of the representation or that of the\nar X\niv :1\n30 1.\n35 27\nv1 [\ncs .L\nlatent features. Imposing all these requirements on our matrix factorization leads to the sparse nonnegative matrix factorization (SNMF) problem.\nSNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].\nHowever, algorithms [10, 7] for solving SNMF which utilize the mixed norm of L1/L2 as their sparsity measure are slow and do not scale well to large datasets. Thus, we develop an efficient algorithm to solve this problem and has the following ingredients:\n\u2022 An exact projection operator to enforce the user-defined sparsity as opposed to the previous heuristic approach [10]. \u2022 Novel sequential updates which provide the bulk of our speedup compared to the previously employed batch methods [10, 7]."}, {"heading": "2 Preliminaries and Previous Work", "text": "In this section, we give an introduction to the nonnegative matrix factorization (NMF) and SNMF problems. Also, we discuss some widely used algorithms from the literature to solve them.\nBoth these problems share the following problem and solution structure. At a high-level, given a nonnegative matrix X of size m \u00d7 n, we want to approximate it with a product of two nonnegative matrices W,H of sizes m\u00d7 r and r \u00d7 n, respectively:\nX \u2248WH.(1)\nThe nonnegative constraint on matrix H makes the representation a convex combination of features given by the columns of matrix W. In particular, NMF can result in sparse representations, or a parts-based representation, unlike other factorization techniques such as principal component analysis (PCA) and vector quantization (VQ). A common theme in the algorithms proposed for solving these problems is the use of alternating updates to the matrix factors, which is natural because the objective function to be minimized is convex in W and in H, separately, but not in both together. Much effort has been focused on optimizing the efficiency of the core step of updating one of W,H while the other stays fixed."}, {"heading": "2.1 Nonnegative Matrix Factorization", "text": "Factoring a matrix, all of whose entries are nonnegative, as a product of two low-rank nonnegative factors is a fundamental algorithmic challenge. This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2]. For further applications, see the references in the following papers [1, 4].\nWe will consider the following version of the NMF problem, which measures the reconstruction error using the Frobenius norm [17]:\nmin W,H\n1\n2 \u2016X\u2212WH\u20162F s.t. W \u2265 0, H \u2265 0, \u2016Wj\u20162 = 1, \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , r}(2)\nwhere \u2265 is element-wise. We use subscript to denote column elements. Simple multiplicative updates were proposed by Lee and Seung to solve the NMF problem. This is attractive for the following reasons:\n\u2022 Unlike additive gradient descent methods, there is no arbitrary learning rate parameter that needs to be set.\n\u2022 The nonnegativity constraint is satisfied automatically, without any additional projection step.\n\u2022 The objective function converges to a limit point and the values are non-increasing across the updates, as shown by Lee and Seung [17].\nAlgorithm 1 is an example of the kind of multiplicative update procedure used, for instance, by Lee and Seung [17]. The algorithm alternates between updating the matrices W and H (we have only shown the updates for H\u2014those for W are analogous).\nAlgorithm 1 nnls-mult(X,W,H)\nrepeat\nH = H W >X\nW>WH .\nuntil convergence Output: Matrix H.\nHere, indicates element-wise (Hadamard) product and matrix division is also elementwise. To remove the scaling ambiguity, the norm of columns of matrix W are set to unity. Also, a small constant, say 10\u22129, is added to the denominator in the updates to avoid division by zero.\nBesides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11]."}, {"heading": "2.2 Sparse Nonnegative Matrix Factorization", "text": "The nonnegative decomposition is in general not unique [6]. Furthermore, the features may not be parts-based if the data resides well inside the positive orthant. To address these issues, sparseness constraints have been imposed on the NMF problem.\nSparse NMF can be formulated in many different ways. From a user point of view, we can split them into two classes of formulations: explicit and implicit. In explicit versions of SNMF [10, 7], one can set the sparsities of the matrix factors W ,H directly. On the other hand, in implicit versions of SNMF [13, 22], the sparsity is controlled via a regularization parameter and is often hard to tune to specified sparsity values a priori. However, the algorithms for implicit versions tend to be faster compared to the explicit versions of SNMF.\nIn this paper, we consider the explicit sparse NMF formulation proposed by Hoyer [10]. To make the presentation easier to follow, we first consider the case where the sparsity is imposed on one of the matrix factors, namely the feature matrix W\u2014the analysis for the symmetric case where the sparsity is instead set on the other matrix factor H is analogous. The case where sparsity requirements are imposed on both the matrix factors is dealt with in the Appendix. The sparse NMF problem formulated by Hoyer [10] with sparsity on matrix W is as follows:\nmin W,H\nf(W ,H) = 1\n2 \u2016X\u2212WH\u20162F s.t. W \u2265 0,H \u2265 0,\n\u2016Wj\u20162 = 1, sp(Wj) = \u03b1, \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , r}(3) Sparsity measure for a d-dimensional vector x is given by:\nsp(x) = \u221a d\u2212 \u2016x\u20161/\u2016x\u20162\u221a\nd\u2212 1 (4)\nThe sparsity measure (4) defined above has many appealing qualities. Some of which are as follows:\n\u2022 The measure closely models the intuitive notion of sparsity as captured by the L0 norm. So, it easy for the user to specify sparsity constraints from prior knowledge of the application domain.\n\u2022 Simultaneously, it is able to avoid the pitfalls associated with directly optimizing the L0 norm. Desirable properties for sparsity measures have been previously explored [12] and it satisfies all of these properties for our problem formulation.\n\u2022 The above sparsity measure enables one to limit the sparsity for each feature to lie in a given range by changing the equality constraints in the SNMF formulation (3)\nto inequality constraints. This could be useful in scenarios like fMRI brain analysis, where we have prior knowledge on the sizes of brain regions. Our proposed algorithm can handle this formulation but we will not explore it further in this version.\nA gradient descent-based algorithm called Nonnegative Matrix Factorization with Sparseness Constraints (NMFSC) to solve SNMF was proposed [10]. Multiplicative updates were used for optimizing the matrix factor which did not have sparsity constraints specified. Heiler and Schno\u0308rr[7] proposed two new algorithms which also solved this problem by sequential cone programming and utilized general purpose solvers like MOSEK (http://www.mosek.com). We will consider the faster one of these called tangent-plane constraint (TPC) algorithm. However, both these algorithms, namely NMFSC and TPC, solve for the whole matrix of coefficients at once. In contrast, we propose a block coordinatedescent strategy which considers a sequence of vector problems where each one can be solved in closed form efficiently."}, {"heading": "3 The Sequential Sparse NMF Algorithm", "text": "We present our algorithm which we call Sequential Sparse NMF (SSNMF) to solve the SNMF problem as follows:\nFirst, we consider a problem of special form which is the building block (Algorithm 2) of our SSNMF algorithm and give an efficient, as well as exact, algorithm to solve it. Second, we describe our sequential approach (Algorithm 3) to solve the subproblem of SNMF. This uses the routine we developed in the previous step. Finally, we combine our routines developed in the previous two steps along with standard solvers (for instance Algorithm 1) to complete the SSNMF Algorithm (Algorithm 4)."}, {"heading": "3.1 Sparse-opt", "text": "Sparse-opt routine solves the following subproblem which arises when solving problem (3):\nmax y\u22650\nb>y s.t. \u2016y\u20161 = k, \u2016y\u20162 = 1(5)\nwhere vector b is of size m. This problem has been previously considered [10], and a heuristic to solve it was proposed which we will henceforth refer to as the Projection-heuristic. Observation 1. For any i, j, we have that if bi \u2265 bj, then yi \u2265 yj.\nLet us first consider the case when the vector b is sorted. Then by the previous observation, we have a transition point p that separates the zeros of the solution vector from the rest.\nObservation 2. By applying the Cauchy-Schwarz inequality on y and the all ones vector, we get p \u2265 k2.\nThe Lagrangian of the problem (5) is :\nL(y, \u00b5, \u03bb) = b>y + \u00b5 ( m\u2211 i=1 yi \u2212 k ) + \u03bb 2 ( m\u2211 i=1 y2i \u2212 1 ) Setting the partial derivatives of the Lagrangian to zero, we get by observation 1:\nm\u2211 i yi = k, m\u2211 i y2i = 1\nbi + \u00b5(p) + \u03bb(p)yi = 0,\u2200i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , p} yi = 0,\u2200i \u2208 {p+ 1, \u00b7 \u00b7 \u00b7 ,m}\nwhere we account for the dependence of the Lagrange parameters \u03bb, \u00b5 on the transition point p. We compute the objective value of problem (5) for all transition points p in the range from k2 to m and select the one with the highest value. In the case, where the vector b is not sorted, we just simply sort it and note down the sorting permutation vector. The complete algorithm is given in Algorithm 2. The dominant contribution to the running time of Algorithm 2 is the sorting of vector b and therefore can be implemented in O(m logm) time.\nAlgorithm 2 Sparse-opt(b, k)\n1: Set a = sort(b) and get the mapping \u03c0 such that ai = b\u03c0(i) and aj > aj+1 for all valid i, j. 2: Compute values of \u00b5(p), \u03bb(p) and obj(p) for p from dk2e to m as follows:\n\u03bb(p) = \u2212\n\u221a p \u2211p i=1 a 2 i \u2212 ( \u2211p i=1 ai) 2\n(p\u2212 k2) \u00b5(p) = \u2212 \u2211p i=1 ai p \u2212 k p \u03bb(p)\nobj(p) = \u2212\u03bb(p)(p\u2212 k2) + k\n\u2211p i=1 ai\np\n3: Set p\u2217 = arg maxp obj(p). 4: Set xi = \u2212ai+\u00b5(p \u2217) \u03bb(p\u2217) ,\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , p \u2217} and to zero otherwise. 5: Output: Solution vector y where y\u03c0(i) = xi."}, {"heading": "3.2 Sequential Approach \u2014Block Coordinate Descent", "text": "Previous approaches for solving SNMF [10, 7] use batch methods to solve for sparsity constraints. That is, the whole matrix is updated at once and projected to satisfy the constraints. We take a different approach of updating a column vector at a time. This gives us the benefit of being able to solve the subproblem (column) efficiently and exactly. Subsequent updates can benefit from the newly updated columns resulting in faster convergence as seen in the experiments.\nIn particular, consider the optimization problem (3) for a column j of the matrix W while fixing the rest of the elements of matrices W,H:\nmin Wj\u22650\nf\u0303(Wj) = 1\n2 g\u2016Wj\u201622 + u>Wj s.t. \u2016Wj\u20162 = 1, \u2016Wj\u20161 = k\nwhere g = H>j Hj and u = \u2212XH>j + \u2211 i 6=jWi(HH >)ij . This reduces to the problem (5) for which we have proposed an exact algorithm (Algorithm 2). We update the columns of the matrix factor W sequentially as shown in Algorithm 3. We call it sequential for we update the columns one at a time. Note that this approach can be seen as an instance of block coordinate descent methods by mapping features to blocks and the Sparse-opt projection operator to a descent step.\nAlgorithm 3 sequential-pass(X,W,H)\nC = \u2212XH> + WHH> G = HH> repeat for j = 1 to r (randomly) do Uj = Cj \u2212WjGjj t = Sparse-opt(\u2212Uj , k). C = C + (t\u2212Wj)G>i Wj = t.\nend for until convergence Output: Matrix W.\nTheorem 3. The routine in Algorithm 3 converges and the rate of convergence is at least E[f(W \u03c4 ,H l)] \u2212 f(W \u2217,H l) \u2264 C\u03c4 where \u03c4 is number of outer iterations (or equivalently number of matrix updates) for some constant C. The expectation is taken over the random choices of the column vectors in W and l is count of number of matrix updates to H.\nProof: To apply Theorem 5 from Nesterov\u2019s paper [21], we note the following transformations: Q = \u2297ri=1Qi where Qi \u2208 Rm and Qi = {\u2016Wj\u20162 \u2264 1 \u2229 \u2016Wj\u20161 = k \u2229W \u2265 0}. Note that Qi are closed and convex sets. Also, the function f given in equation (3) is smooth and satifies the smoothness condition because the Hessian is constant. So, applying theorem 5 on our problem where C is a constant dependent on {W l,H l}, we have the following:\nE[f(W T ,H l)]\u2212 f(W \u2217,H l) \u2264 rT rT + T (C)\n= C\nT + 1\nIf we assume strong convexity, we can get a better rate of convergence."}, {"heading": "3.3 SSNMF Algorithm for Sparse NMF", "text": "We are now in a position to present our complete Sequential Sparse NMF (SSNMF) algorithm. By combining Algorithms 1, 2 and 3, we obtain SSNMF (Algorithm 4).\nAlgorithm 4 ssnmf(X,W,H)\n1: repeat 2: W = sequential-pass(X,W,H) 3: H = nnls-mult(X,W,H) 4: until convergence 5: Output: Matrices W,H."}, {"heading": "4 Implementation Issues", "text": "For clarity of exposition, we presented the plain vanilla version of our SSNMF Algorithm 4. We now describe some of the actual implementation details.\n\u2022 Initialization: Generate a positive random vector v of size m and obtain z = Sparse-opt(v, k) where k = \u221a m \u2212 \u03b1 \u221a m\u2212 1 (from equation (4)). Use the solu-\ntion z and its random permutations to initialize matrix W. Initialize the matrix H to uniform random entries in [0, 1].\n\u2022 Incorporating faster solvers: We use multiplicative updates for a fair comparison with NMFSC and TPC. However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H. Empirical results (not reported here) show that this further speeds up the SSNMF algorithm.\n\u2022 Termination: In our experiments, we fix the number of alternate updates or equivalently the number of times we update matrix W . Other approaches include specifying total running time, relative change in objective value between iterations or approximate satisfaction of KKT conditions.\n\u2022 Sparsity constraints: We have primarly considered the sparse NMF model as formulated by Hoyer [10]. This has been generalized by Heiler and Schno\u0308rr [7] by relaxing the sparsity constraints to lie in user-defined intervals. Note that, we can handle this formulation [7] by making a trivial change to Algorithm 3."}, {"heading": "5 Experiments and Discussion", "text": "In this section, we compare the performance of our algorithm with the state-of-the-art NMFSC and TPC algorithms [10, 7]. Running times for the algorithms are presented when applied to one synthetic and three real-world datasets. Experiments report reconstruction error (\u2016X\u2212WH\u2016F ) instead of objective value for convenience of display. For all experiments on the datasets, we ensure that our final reconstruction error is always better than that of the other two algorithms. Our algorithm was implemented in MATLAB (http://www. mathworks.com) similar to NMFSC and TPC. All of our experiments were run on a 3.2Ghz Intel machine with 24GB of RAM and the number of threads set to one."}, {"heading": "5.1 Datasets", "text": "For comparing the performance of SSNMF with NMFSC and TPC, we consider the following synthetic and three real-world datasets :\n\u2022 Synthetic: 200 images of size 9\u00d7 9 as provided by Heiler and Schno\u0308rr in their code implementation.\n\u2022 ORL: Face dataset that consists of 400 images of size 112\u00d7 92 and can be obtained at http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.\n\u2022 Van Hateren: Natural images dataset that consists of 4000 images of size 1536\u00d7 1024 and can be obtained at http://www.kyb.tuebingen.mpg.de/bethge/ vanhateren/iml/. We use 400 of these images in our experiments.\n\u2022 sMRI: Structural MRI scans of 269 subjects taken at the John Hopkins University were obtained. The scans were taken on a single 1.5T scanner with the imaging parameters set to 35mm TR, 5ms TE, matrix size of 256 \u00d7 256. We segment these images into gray matter, white matter and cerebral spinal fluid images, using the software program SPM5 (http://www.fil.ion.ucl.ac.uk/spm/software/spm5/), followed by spatial smoothing with a Gaussian kernel of 10 \u00d7 10 \u00d7 10 mm. This results in images which are of size 105\u00d7 127\u00d7 46"}, {"heading": "5.2 Comparing Performances of Core Updates", "text": "We compare our Sparse-opt (Algorithm 2) routine with the competing Projectionheuristic [10]. In particular, we generate 40 random problems for each sparsity constraint\nin {0.2, 0.4, 0.6, 0.8} and a fixed problem size. The problems are of size 2i \u00d7 100 where i takes integer values from 0 to 12. Input coefficients are generated by drawing samples uniformly at random from [0, 1]. The mean values of the running times for Sparse-opt and the Projection-heuristic for each dimension and corresponding sparsity value are plotted in Figure 1."}, {"heading": "5.3 Comparing Overall Performances", "text": "SSNMF versus NMFSC and TPC: We plot the performance of SSNMF against NMFSC and TPC on the synthetic dataset provided by Heiler and Schno\u0308rr in Figure 2. We used the default settings for both NMFSC and TPC using the software provided by the authors. Our experience with TPC was not encouraging on bigger datasets and hence we show its performance only on the synthetic dataset. It is possible that the performance of TPC can be improved by changing the default settings but we found it non-trivial to do so.\nSSNMF versus NMFSC: To ensure fairness, we removed logging information from NMFSC code [10] and only computed the objective for equivalent number of matrix updates as SSNMF. We do not plot the objective values at the first iteration for convenience of display. However, they are the same for both algorithms because of the shared initialization . We ran the SSNMF and NMFSC on the ORL face dataset. The rank was fixed at 25 in both the algorithms. Also, the plots of running times versus objective values are shown in Figure 3 corresponding to sparsity values ranging from 0.1 to 0.7. Additionally, we ran our SSNMF algorithm and NMFSC algorithm on a large-scale dataset consisting of the structural MRI images by setting the rank to 40. The running times are shown in Figure 4. Finally, we evaluate the performance of our exact projection operator Sparse-opt versus the Projection-heuristic of Hoyer on the high-dimensional natural images dataset. We replace the Sparse-opt routine in SSNMF by Projection-heuristic and call it SSNMF+Proj. The running times for these algorithms for sparsity set to 0.5 are shown in Figure 1."}, {"heading": "5.4 Main Results", "text": "We compared the running times of our Sparse-opt routine versus the Projection-heuristic and found that on randomly generated problems we are faster on average. Also, we are guaranteed to converge monotonically and possibly to a lower objective using our Sparse-\nopt routine than if we used Projection-heuristic as illustrated by comparing the SSNMF and SSNMF+Proj algorithms (Figure 1) .\nOur results on switching the Sparse-opt routine with the Projection-heuristic did not slow down our SSNMF solver significantly for the datasets we considered. So, we conclude that the speedup is mainly due to the sequential nature of the updates (Algorithm 3).\nAlso, we converge faster than NMFSC for fewer number of matrix updates. This can be seen by noting that the plotted points in Figures 3 and 4 are such that the number of matrix updates are the same for both SSNMF and NMFSC. For some datasets, we noted a speedup of an order of magnitude making our approach attractive for computation purposes.\nFinally, we note that we recover a parts-based representation as shown by Hoyer. An example of the obtained features by NMFSC and ours is shown in Figure 5."}, {"heading": "6 Connections to Related Work", "text": "Other SNMF formulations have been considered by Hoyer [9], M\u00f8rup et al. [20] , Kim and Park [13], Pascual-Montano et al. [22] (nsNMF) and Peharz and Pernkopf [23].\nWe note that our sparsity measure has all the desirable properties, extensively discussed by Hurley and Rickard [12], except for one (\u201ccloning\u201d). Cloning property is satisfied when two vectors of same sparsity when concatenated maintain their sparsity value. Dimensions in our optimization problem are fixed and thus violating the cloning property is not an issue. Compare this with the L1 norm that satisfies only one of these properties (namely \u201crising tide\u201d). Rising tide is the property where adding a constant to the elements of a vector decreases the sparsity of the vector. Nevertheless, the measure used in Kim and Park is based on the L1 norm. The properties satisfied by the measure in Pascual-Montano et al. are unclear because of the implicit nature of the sparsity formulation.\nPascual-Montano et al. [22] claim that the SNMF formulation of Hoyer, as given by problem (3) does not capture the variance in the data. However, some transformation of the sparsity values is required to properly compare the two formulations [10, 22]. Preliminary results show that the formulation given by Hoyer is able to capture the variance in the data if the sparsity parameters are set appropriately. Peharz and Pernkopf [23] propose to tackle the L0 norm constrained NMF directly by projecting from intermediate unconstrained solutions to the required L0 constraint. This leads to the well-known problem of getting stuck in local minima. Indeed, the authors re-initialize their feature matrix with an NNLS solver to recover from the local suboptimum. Our formulation avoids the local minima associated with L0 norm by using a smooth surrogate."}, {"heading": "7 Conclusions", "text": "We have proposed a new efficient algorithm to solve the sparse NMF problem. Experiments demonstrate the effectiveness of our approach on real datasets of practical interest. Our algorithm is faster over a range of sparsity values and generally performs better when the\nsparsity is higher. The speed up is mainly because of the sequential nature of the updates in contrast to the previously employed batch updates of Hoyer. Also, we presented an exact and efficient algorithm to solve the problem of maximizing a linear objective with a sparsity constraint, which is an improvement over the heuristic approach in Hoyer.\nOur approach can be extended to other NMF variants [9]. Another possible application is the sparse version of nonnegative tensor factorization. A different research direction would be to scale our algorithm to handle large datasets by chunking [19] and/or take advantage of distributed computational settings."}, {"heading": "Acknowledgement", "text": "The first author would like to acknowledge the support from NIBIB grants 1 R01 EB 000840 and 1 R01 EB 005846. The second author was supported by NIMH grant 1 R01 MH07628201. The latter two grants were funded as part of the NSF/NIH Collaborative Research in Computational Neuroscience Program."}], "references": [{"title": "Computing a nonnegative matrix factorization \u2013 provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "In Proceedings of the 44th symposium on Theory of Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Color categories revealed by non-negative matrix factorization of munsell color spectra", "author": ["G. Buchsbaum", "O. Bloch"], "venue": "Vision research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Fast local algorithms for large scale nonnegative matrix and tensor factorizations", "author": ["A. Cichocki", "A.H. Phan"], "venue": "IEICE Transactions on Fundamentals of Electronics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Nonnegative ranks, decompositions, and factorizations of nonnegative matrices", "author": ["J. Cohen", "U. Rothblum"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1993}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Learning sparse representations by non-negative matrix factorization and sequential cone programming", "author": ["M. Heiler", "C. Schn\u00f6rr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "In Neural Networks for Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Non-negative matrix factorization with sparseness constraints", "author": ["P.O. Hoyer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Fast coordinate descent methods with variable selection for non-negative matrix factorization", "author": ["C.J. Hsieh", "I. Dhillon"], "venue": "ACM SIGKDD Internation Conference on Knowledge Discovery and Data Mining, page xx,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Comparing measures of sparsity", "author": ["N. Hurley", "S. Rickard"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis", "author": ["H. Kim", "H. Park"], "venue": "Bioinformatics, 23(12):1495\u20131502,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Toward faster nonnegative matrix factorization: A new algorithm and comparisons", "author": ["J. Kim", "H. Park"], "venue": "Data Mining, IEEE International Conference on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Self modeling curve resolution", "author": ["W. Lawton", "E. Sylvestre"], "venue": "Technometrics, pages 617\u2013633,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1971}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "S.H. Seung"], "venue": "In NIPS, pages 556\u2013562,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural Comp.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Approximate L0 constrained non-negative matrix and tensor factorization", "author": ["M. M\u00f8rup", "K.H. Madsen", "L.K. Hansen"], "venue": "In ISCAS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "CORE Discussion Papers,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Nonsmooth nonnegative matrix factorization (nsNMF)", "author": ["A. Pascual-Montano", "J.M. Carazo", "K. Kochi", "D. Lehmann", "R.D. Pascual-Marqui"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Sparse nonnegative matrix factorization with l-constraints", "author": ["R. Peharz", "F. Pernkopf"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Single-channel speech separation using sparse nonnegative matrix factorization", "author": ["M.N. Schmidt", "R.K. Olsson"], "venue": "In International Conference on Spoken Language Processing (INTERSPEECH),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 8, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 6, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 19, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 12, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 21, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 22, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 23, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 164, "endOffset": 172}, {"referenceID": 21, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 164, "endOffset": 172}, {"referenceID": 9, "context": "However, algorithms [10, 7] for solving SNMF which utilize the mixed norm of L1/L2 as their sparsity measure are slow and do not scale well to large datasets.", "startOffset": 20, "endOffset": 27}, {"referenceID": 6, "context": "However, algorithms [10, 7] for solving SNMF which utilize the mixed norm of L1/L2 as their sparsity measure are slow and do not scale well to large datasets.", "startOffset": 20, "endOffset": 27}, {"referenceID": 9, "context": "Thus, we develop an efficient algorithm to solve this problem and has the following ingredients: \u2022 An exact projection operator to enforce the user-defined sparsity as opposed to the previous heuristic approach [10].", "startOffset": 211, "endOffset": 215}, {"referenceID": 9, "context": "\u2022 Novel sequential updates which provide the bulk of our speedup compared to the previously employed batch methods [10, 7].", "startOffset": 115, "endOffset": 122}, {"referenceID": 6, "context": "\u2022 Novel sequential updates which provide the bulk of our speedup compared to the previously employed batch methods [10, 7].", "startOffset": 115, "endOffset": 122}, {"referenceID": 15, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "For further applications, see the references in the following papers [1, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 3, "context": "For further applications, see the references in the following papers [1, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 16, "context": "We will consider the following version of the NMF problem, which measures the reconstruction error using the Frobenius norm [17]:", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "\u2022 The objective function converges to a limit point and the values are non-increasing across the updates, as shown by Lee and Seung [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "Algorithm 1 is an example of the kind of multiplicative update procedure used, for instance, by Lee and Seung [17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 218, "endOffset": 222}, {"referenceID": 5, "context": "The nonnegative decomposition is in general not unique [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "In explicit versions of SNMF [10, 7], one can set the sparsities of the matrix factors W ,H directly.", "startOffset": 29, "endOffset": 36}, {"referenceID": 6, "context": "In explicit versions of SNMF [10, 7], one can set the sparsities of the matrix factors W ,H directly.", "startOffset": 29, "endOffset": 36}, {"referenceID": 12, "context": "On the other hand, in implicit versions of SNMF [13, 22], the sparsity is controlled via a regularization parameter and is often hard to tune to specified sparsity values a priori.", "startOffset": 48, "endOffset": 56}, {"referenceID": 21, "context": "On the other hand, in implicit versions of SNMF [13, 22], the sparsity is controlled via a regularization parameter and is often hard to tune to specified sparsity values a priori.", "startOffset": 48, "endOffset": 56}, {"referenceID": 9, "context": "In this paper, we consider the explicit sparse NMF formulation proposed by Hoyer [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "The sparse NMF problem formulated by Hoyer [10] with sparsity on matrix W is as follows:", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "Desirable properties for sparsity measures have been previously explored [12] and it satisfies all of these properties for our problem formulation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "A gradient descent-based algorithm called Nonnegative Matrix Factorization with Sparseness Constraints (NMFSC) to solve SNMF was proposed [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "Heiler and Schn\u00f6rr[7] proposed two new algorithms which also solved this problem by sequential cone programming and utilized general purpose solvers like MOSEK (http://www.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "This problem has been previously considered [10], and a heuristic to solve it was proposed which we will henceforth refer to as the Projection-heuristic.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "Previous approaches for solving SNMF [10, 7] use batch methods to solve for sparsity constraints.", "startOffset": 37, "endOffset": 44}, {"referenceID": 6, "context": "Previous approaches for solving SNMF [10, 7] use batch methods to solve for sparsity constraints.", "startOffset": 37, "endOffset": 44}, {"referenceID": 20, "context": "Proof: To apply Theorem 5 from Nesterov\u2019s paper [21], we note the following transformations: Q = \u2297i=1Qi where Qi \u2208 R and Qi = {\u2016Wj\u20162 \u2264 1 \u2229 \u2016Wj\u20161 = k \u2229W \u2265 0}.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "Initialize the matrix H to uniform random entries in [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 17, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 13, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 2, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 10, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 9, "context": "\u2022 Sparsity constraints: We have primarly considered the sparse NMF model as formulated by Hoyer [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "This has been generalized by Heiler and Schn\u00f6rr [7] by relaxing the sparsity constraints to lie in user-defined intervals.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "Note that, we can handle this formulation [7] by making a trivial change to Algorithm 3.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "In this section, we compare the performance of our algorithm with the state-of-the-art NMFSC and TPC algorithms [10, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": "In this section, we compare the performance of our algorithm with the state-of-the-art NMFSC and TPC algorithms [10, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 9, "context": "We compare our Sparse-opt (Algorithm 2) routine with the competing Projectionheuristic [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Input coefficients are generated by drawing samples uniformly at random from [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 9, "context": "SSNMF versus NMFSC: To ensure fairness, we removed logging information from NMFSC code [10] and only computed the objective for equivalent number of matrix updates as SSNMF.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Other SNMF formulations have been considered by Hoyer [9], M\u00f8rup et al.", "startOffset": 54, "endOffset": 57}, {"referenceID": 19, "context": "[20] , Kim and Park [13], Pascual-Montano et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[20] , Kim and Park [13], Pascual-Montano et al.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "[22] (nsNMF) and Peharz and Pernkopf [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[22] (nsNMF) and Peharz and Pernkopf [23].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "We note that our sparsity measure has all the desirable properties, extensively discussed by Hurley and Rickard [12], except for one (\u201ccloning\u201d).", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "[22] claim that the SNMF formulation of Hoyer, as given by problem (3) does not capture the variance in the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "However, some transformation of the sparsity values is required to properly compare the two formulations [10, 22].", "startOffset": 105, "endOffset": 113}, {"referenceID": 21, "context": "However, some transformation of the sparsity values is required to properly compare the two formulations [10, 22].", "startOffset": 105, "endOffset": 113}, {"referenceID": 22, "context": "Peharz and Pernkopf [23] propose to tackle the L0 norm constrained NMF directly by projecting from intermediate unconstrained solutions to the required L0 constraint.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "Our approach can be extended to other NMF variants [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 18, "context": "A different research direction would be to scale our algorithm to handle large datasets by chunking [19] and/or take advantage of distributed computational settings.", "startOffset": 100, "endOffset": 104}], "year": 2017, "abstractText": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L0 norm, however its optimization is NP-hard. Mixed norms, such as L1/L2 measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L1 norm. However, present algorithms designed for optimizing the mixed norm L1/L2 are slow and other formulations for sparse NMF have been proposed such as those based on L1 and L0 norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "creator": "LaTeX with hyperref package"}}}