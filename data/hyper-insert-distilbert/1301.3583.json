{"id": "1301.3583", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Big Neural Networks Waste Capacity", "abstract": "this article exposes the failure of some big fast neural networks to leverage newly added capacity to appropriately reduce additional underfitting. its past significant research suggest diminishing returns when increasing the size of neural client networks. but our experiments on imagenet computing lsvrc - 2010 calculations show that. this may be due to firstly the fact that among bigger networks currently underfit the larger training objective, probably sometimes thus performing worse increases on the training set than in smaller networks. evidently this suggests ways that the primary optimization method - first high order gradient descent - fails at fulfilling this specific regime. some directly attacking perform this evaluation problem, typically either through the global optimization inference method or against the frequent choices use of generalized parametrization, may allow to essentially improve twice the generalization error on large datasets, for predicting which, a large capacity hierarchy is required.", "histories": [["v1", "Wed, 16 Jan 2013 04:45:29 GMT  (38kb,D)", "http://arxiv.org/abs/1301.3583v1", null], ["v2", "Thu, 17 Jan 2013 18:11:34 GMT  (38kb,D)", "http://arxiv.org/abs/1301.3583v2", null], ["v3", "Wed, 27 Feb 2013 23:07:05 GMT  (38kb,D)", "http://arxiv.org/abs/1301.3583v3", null], ["v4", "Thu, 14 Mar 2013 20:49:20 GMT  (40kb,D)", "http://arxiv.org/abs/1301.3583v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yann n dauphin", "yoshua bengio"], "accepted": false, "id": "1301.3583"}, "pdf": {"name": "1301.3583.pdf", "metadata": {"source": "CRF", "title": "Big Neural Networks Waste Capacity", "authors": ["Yann N. Dauphin"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Deep learning and neural networks have achieved state-of-the-art results on vision 1, language 2 , and audio-processing tasks 3. All these cases involved fairly large datasets, but in all these cases, even larger ones could be used. One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efficiently training larger networks?\nPrior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error. These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011). Furthermore, Coates et al. (2011) shows that while neural net methods fail to leverage added capacity K-Means can. This has allowed K-Means to reach state-of-the-art performance on CIFAR-10 for methods that do not use artificial transformations. This is an unexpected result because K-Means is a much dumber unsupervised learning algorithm when compared with RBMs and regularized auto-encoders. Coates et al. (2011) argues that this is mainly due to K-Means making better use of added capacity, but it does not explain why the neural net methods failed to do this."}, {"heading": "2 Experimental Setup", "text": "We will perform experiments with the well known ImageNet LSVRC-2010 object detection dataset4. The subset used in the Large Scale Visual Recognition Challenge 2010 contains 1000 object categories and 1.2 million training images.\n1Krizhevsky et al. (2012) reduced by almost one half the error rate on the 1000-class ImageNet object recognition benchmark\n2Mikolov et al. (2011) reduced perplexity on WSJ by 40% and speech recognition absolute word error rate by > 1%.\n3For speech recognition, Seide et al. (2011) report relative word error rates decreasing by about 30% on datasets of 309 hours\n4http://www.image-net.org/challenges/LSVRC/2010/\nar X\niv :1\n30 1.\n35 83\nv1 [\ncs .L\nG ]\n1 6\nJa n\n20 13\nThis dataset has many attractive features:\n1. The task is difficult enough for current algorithms that there is still room for much improvement. For instance, Krizhevsky et al. (2012) was able to reduce the error by half recently. What\u2019s more the state-of-the-art is at 15.3% error. Assuming minimal error in the human labelling of the dataset, it should be possible to reach errors close to 0%.\n2. Improvements on ImageNet are thought to be a good proxy for progress in object recognition (Deng et al., 2009).\n3. It has a large number of examples. This is the setting that is commonly found in industry where datasets reach billions of examples. Interestingly, as you increase the amount of data, the training error converges to the generalization error. In other words, reducing training error is well correlated with reducing generalization error, when large datasets are available. Therefore, it stands to reason that resolving underfitting problems may yield significant improvements.\nWe use the features provided by the Large Scale Visual Recognition Challenge 20105. The images are convolved with SIFT features, then K-Means is used to form a visual vocabulary of 1000 visual words. Following the litterature, we report the Top-5 error rate only.\nThe experiments focus on the behavior of Multi-Layer Perceptrons (MLP) as capacity is increased. This is done by increasing the number of hidden units in the network. The final classification layer of the network is a softmax over possible classes (softmax(x) = e\u2212x/ \u2211 i e\n\u2212xi ). The hidden layers use the logistic sigmoid activation function (\u03c3(x) = 1/(1+e\u2212x)). We initialize the weights of the hidden layer according to the formula proposed by Glorot and Bengio (2010). The parameters of the classification layer are initialized to 0, along with all the bias (offset) parameters of the MLP.\nThe hyper-parameters to tune are the learning rate and the number of hidden units. We are interested in optimization performance so we cross-validate them based on the training error. We use a grid search with the learning rates taken from {0.1, 0.01} and the number of hiddens from {1000, 2000, 5000, 7000, 10000, 15000}. When we report the performance of a network with a given number of units we choose the best learning rate. We do not use any regularization because it would typically not help to decrease the training set error. The number of epochs is set to 300 so that it is large enough for the networks to converge.\nThe experiments are run on a cluster of Nvidia Geforce GTX 580 GPUs with the help of the Theano library (Bergstra et al., 2010). We make use of HDF5 (Folk et al., 2011) to load the dataset in a lazy fashion because of its large size. The shortest training experiment took 10 hours to run and the longest took 28 hours."}, {"heading": "3 Experimental Results", "text": "Figure 1 shows the evolution of the training error as the capacity is increased. The common intuition is that this increased capacity will help fit the training set - possibly to the detriment of generalization error. For this reason practitioners have focused mainly on the problem of overfitting the dataset when dealing with large networks - not underfitting. In fact, much research is concerned with proper regularization of such large networks (Hinton et al., 2012, 2006).\nHowever, our results show that adding capacity quickly fails to reduce underfitting. It becomes increasingly difficult for the networks to use additional units to fit the training set. As shown in Figure 2, the return on investment for additional units fades to nil without reaching 0 errors on the training set. This means there is an optimization problem that prevents us from effectively training big networks.\nFor reference, we also include the learning curves of the networks used for Figure 1 and 2 in Figure 3. We see that the curves for capacities above 5000 all converge towards the same point.\n5http://www.image-net.org/challenges/LSVRC/2010/download-public"}, {"heading": "4 Future directions", "text": "The stagnant underfitting observed in larger networks seems to be a failure of first order gradient descent.\nIn fact, we know that the first order approximation fails when there are a lot of interactions between hidden units. It may be that adding units increases the interactions between units and causes the Hessian to be ill-conditioned. This reasoning suggests two research directions:\n\u2022 methods that break interactions between large numbers of units. This helps the Hessian to be better conditioned and will lead to better effectiveness for first-order descent. This type of method can be implemented efficiently. Examples of this approach are sparsity and orthognality penalties.\n\u2022 methods that model interactions between hidden units. For example, second order methods (Martens, 2010) and natural gradient methods (Le Roux et al., 2008). Typically, these are expensive approaches and the challenge is in scaling them to large datasets, where stochas-\ntic gradient approaches may dominate. The ideal target is a stochastic natural gradient or stochastic second-order method.\nThe optimization failure may also be due to other reasons. For example, networks with more capacity have more local minima. Future work should investigate tests that help discriminate between ill-conditioning issues and local minima issues.\nFixing this optimization problem may be the key to unlocking better performance of deep networks. Based on past observations (Bengio, 2009; Erhan et al., 2010), we expect this optimization problem to worsen for deeper networks, and our experimental setup should be extended to measure the effect of depth. As we have noted earlier, improvements on the training set error should be well correlated with generalization for large datasets."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Unsupervised models of images by spike-andslab RBMs", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the Twenty-eight International Conference on Machine Learning (ICML\u201911)", "citeRegEx": "Courville et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Courville et al\\.", "year": 2011}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Manzagol", "P. Vincent", "S. Bengio"], "venue": "J. Machine Learning Res", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "An overview of the hdf5 technology suite and its applications", "author": ["M. Folk", "G. Heber", "Q. Koziol", "E. Pourmal", "D. Robinson"], "venue": "In Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases,", "citeRegEx": "Folk et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Folk et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In JMLR W&CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010),", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N. Le Roux", "Manzagol", "P.-A", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proceedings of the Twenty-seventh International Conference on Machine Learning", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernocky"], "venue": "In Proc. 12th annual conference of the international speech communication association (INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Higher order contractive auto-encoder. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "In Interspeech", "citeRegEx": "Seide et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 3, "context": "These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 2, "context": "One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efficiently training larger networks? Prior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error.", "startOffset": 330, "endOffset": 351}, {"referenceID": 2, "context": "One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efficiently training larger networks? Prior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error. These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011). Furthermore, Coates et al. (2011) shows that while neural net methods fail to leverage added capacity K-Means can.", "startOffset": 330, "endOffset": 649}, {"referenceID": 2, "context": "One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efficiently training larger networks? Prior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error. These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011). Furthermore, Coates et al. (2011) shows that while neural net methods fail to leverage added capacity K-Means can. This has allowed K-Means to reach state-of-the-art performance on CIFAR-10 for methods that do not use artificial transformations. This is an unexpected result because K-Means is a much dumber unsupervised learning algorithm when compared with RBMs and regularized auto-encoders. Coates et al. (2011) argues that this is mainly due to K-Means making better use of added capacity, but it does not explain why the neural net methods failed to do this.", "startOffset": 330, "endOffset": 1031}, {"referenceID": 10, "context": "For instance, Krizhevsky et al. (2012) was able to reduce the error by half recently.", "startOffset": 14, "endOffset": 39}, {"referenceID": 4, "context": "Improvements on ImageNet are thought to be a good proxy for progress in object recognition (Deng et al., 2009).", "startOffset": 91, "endOffset": 110}, {"referenceID": 0, "context": "We initialize the weights of the hidden layer according to the formula proposed by Glorot and Bengio (2010). The parameters of the classification layer are initialized to 0, along with all the bias (offset) parameters of the MLP.", "startOffset": 94, "endOffset": 108}, {"referenceID": 1, "context": "The experiments are run on a cluster of Nvidia Geforce GTX 580 GPUs with the help of the Theano library (Bergstra et al., 2010).", "startOffset": 104, "endOffset": 127}, {"referenceID": 6, "context": "We make use of HDF5 (Folk et al., 2011) to load the dataset in a lazy fashion because of its large size.", "startOffset": 20, "endOffset": 39}, {"referenceID": 12, "context": "For example, second order methods (Martens, 2010) and natural gradient methods (Le Roux et al.", "startOffset": 34, "endOffset": 49}, {"referenceID": 0, "context": "Based on past observations (Bengio, 2009; Erhan et al., 2010), we expect this optimization problem to worsen for deeper networks, and our experimental setup should be extended to measure the effect of depth.", "startOffset": 27, "endOffset": 61}, {"referenceID": 5, "context": "Based on past observations (Bengio, 2009; Erhan et al., 2010), we expect this optimization problem to worsen for deeper networks, and our experimental setup should be extended to measure the effect of depth.", "startOffset": 27, "endOffset": 61}], "year": 2017, "abstractText": "This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC-2010 show that this may be due to the fact that bigger networks underfit the training objective, sometimes performing worse on the training set than smaller networks. This suggests that the optimization method first order gradient descent fails at this regime. Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required.", "creator": "LaTeX with hyperref package"}}}