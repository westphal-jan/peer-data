{"id": "1301.3605", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks", "abstract": "both recent studies have additionally shown that interactive deep neural networks ( dnns ) perform significantly better evaluation than shallow networks and yield gaussian quantitative mixture intelligence models ( gmms ) on conventional large vocabulary speech recognition prediction tasks. first in following this paper we argue that the present difficulty in conducting speech structure recognition is caused primarily caused by simply the high variability included in subjective speech signals. dnns, which can often be considered alongside a joint model of testing a specific nonlinear feature transform operator and a log - th linear behavior classifier, achieve improved recognition accuracy by somehow extracting shorter discriminative memory internal audio representations that are less sensitive mainly to small perturbations in the input features. however, \" if no test labeled samples are very dissimilar to rough training individual samples, dnns brains perform poorly. we both demonstrate these properties that empirically conducted using a series of formal recognition modelling experiments on mixed speech narrowband and wideband noisy speech inputs and contrast speech distorted explained by environmental background noise.", "histories": [["v1", "Wed, 16 Jan 2013 07:23:19 GMT  (141kb,D)", "http://arxiv.org/abs/1301.3605v1", "ICLR 2013, 9 pages, 4 figures"], ["v2", "Mon, 21 Jan 2013 07:42:07 GMT  (144kb,D)", "http://arxiv.org/abs/1301.3605v2", "ICLR 2013, 9 pages, 4 figures"], ["v3", "Fri, 8 Mar 2013 19:42:37 GMT  (145kb,D)", "http://arxiv.org/abs/1301.3605v3", "ICLR 2013, 9 pages, 4 figures"]], "COMMENTS": "ICLR 2013, 9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["dong yu", "michael l seltzer", "jinyu li", "jui-ting huang", "frank seide"], "accepted": false, "id": "1301.3605"}, "pdf": {"name": "1301.3605.pdf", "metadata": {"source": "CRF", "title": "Feature Learning in Deep Neural Networks \u2013 Studies on Speech Recognition Tasks", "authors": ["Dong Yu", "Michael L. Seltzer", "Jinyu Li", "Jui-Ting Huang"], "emails": ["dongyu@microsoft.com", "mseltzer@microsoft.com", "jinyli@microsoft.com", "jthuang@microsoft.com", "fseide@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Automatic speech recognition (ASR) has been an active research area for more than five decades. However, the performance of the ASR systems is still far from satisfactory and the gap between ASR and human speech recognition is still large on most tasks. This gap becomes more pronounced as we move from speech recorded in controlled conditions to real-world applications that require recognizing noisy, far-field spontaneous speech, spoken with a huge vocabulary, sometimes in mixed languages.\nOne of the primary reasons speech recognition is a challenging classification task is because of the high variability in speech signals. It is virtually impossible to avoid some degree of mismatch between the training and testing conditions. For example, speakers may have different accents, use different dialects, pronounce words differently, speak in different styles, speak under different emotional states, and may converse spontaneously with coarticulation, reduction, and hesitation. Additional variability arises as a result of environmental noise, reverberation, different microphones and recording devices. To make matters worse, the various sources of variability are typically nonstationary and interact with the speech signal in a highly nonlinear way.\nRecently, significant progress has been made in applying deep neural networks (DNNs) to large vocabulary speech recognition tasks. By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].\nar X\niv :1\n30 1.\n36 05\nv1 [\ncs .L\nG ]\nIn this paper we show that DNNs can be considered a joint model of a nonlinear feature transformation and a log-linear classifier. Through many layers of nonlinear transforms, DNNs can convert the raw features into a highly invariant and discriminative representation which can then be effectively classified using a log-linear model.\nThe rest of the paper is organized as follows. In Section 2 we briefly describe DNNs and illustrate the feature learning interpretations of DNNs. In Section 3 we show that DNNs can learn invariant and discriminative features and demonstrate empirically that higher layer features are less sensitive to perturbations of the input. In Section 4 we point out that the feature generalization ability is effective only when test samples are small perturbations of training samples. When this assumption does not hold, DNNs performs poorly as indicated in our mixed-bandwidth experiments. We apply this analysis to the Aurora 4 noise robustness ASR task in Section 5 and show that a DNN can achieve performance equivalent to the current state of the art without requiring adaptation to the speaker or the environment. We conclude the paper in Section 6."}, {"heading": "2 Deep Neural Networks", "text": "A deep neural network (DNN) is conventional multi-layer perceptron (MLP) with many hidden layers (thus deep). If the input and output of the DNN are denoted as x and y, respectively, a DNN can interpreted as a directed graphical model that approximates the posterior probability py|x(y = s|x) of a class s given an observation vector x, as a stack of (L+1) layers of log-linear models. The first L layers model the posterior probabilities of hidden binary vectors hl given input vectors vl. If hl consists of N l hidden units, each denoted as hlj , the posterior probability can be expressed as\npl(hl|vl) = N l\u220f j=1\nez l j(v l)hlj\nez l j(v l)1 + ez l j(v l)0\nwhere zl(vl) = (W l)T vl + al\nand W l and al represent the weight matrix and bias vector in the lth layer, respectively. The output layer models the desired class posterior as a multinomial distribution\npL(y = s|vL) = e zLs (v L)\u2211 s\u2032 e zL s\u2032 (v L) = softmaxs\n( vL ) . (1)\nNote that in this model each hidden unit is a random variable that takes on values of 0 or 1. The precise modeling of py|x(y = s|x) requires marginalization over all possible values of hl across all layers which is infeasible. An effective practical trick is to replace the marginalization with the mean-field approximation [9] at each hidden layer. Given the observation x, we set v0 = x and compute the expected value of the output h0. The output of the first layer then becomes the input to the next layer, and so on. Specifically, the input to layer l + 1 is computed as the conditional expectation vl+1 = El{hl|vl} = \u03c3(zl(vl)), 0 \u2264 l < L (2) where \u03c3(z) is the sigmoid function applied element-wise. The activations are thus propagated forward through the network to the top layer. The class posterior probability py|x(y = s|x) can then be computed as py|x(y = s|x) = pL(y = s|vL) (3) using the softmax defined in (1)\nIn the DNN, the estimation of the posterior probability py|x(y = s|x) can also be considered a two-step deterministic process. In the first step, the observation vector x is transformed to another feature vector vL through L layers of non-linear transforms (2). In the second step, the posterior probability py|x(y = s|x) is estimated using the log-linear model (1) given the transformed feature vector vL. If we consider the first L layers fixed, learning the parameters in the softmax layer is equivalent to training a conditional maximum entropy (MaxEnt) model on features vL. In the conventional MaxEnt model, features are manually designed [10]. For example in [11] the first and second-order statistics of the observations are used as features. In DNNs, however, the features are\ndefined by the first L layers and are jointly learned with the MaxEnt model from the data. This not only eliminates the tedious and potentially erroneous process of manual feature extraction but also has the potential to automatically extract invariant and discriminative features, which are difficult to construct manually.\nIn all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task. The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [1\u20133]."}, {"heading": "3 Invariant and discriminative features", "text": ""}, {"heading": "3.1 Deeper is better", "text": "Using DNNs instead of shallow MLPs is a key component to the success of CD-DNN-HMMs. Table 1, which is extracted from [3], summarizes the word error rates (WER) on the Switchboard (SWB) [12] Eval 2000 test set using the 309-hour training set whose senone alignment label was generated from a maximum likelihood (ML) trained GMM system. Switchboard is an corpus of conversational telephone speech.\nFrom this table, we can observe that deeper networks outperform shallow ones. The WER decreases as the number of hidden layers increases, using a fixed layer size of 2000 hidden units. In other words, deeper models have stronger discriminative ability than shallow models. This is also reflected in the improvement of the training criterion (not shown). More interestingly, if architectures with an equivalent number of parameters are compared, the deep models consistently outperforms the shallow models. This is reflected in the last three lines of the table, which show the performance for deep networks and the equivalent single layer networks. Even if we further increase the size of an MLP with a single hidden layer to 16000 hidden units we can only achieve a WER of 22.1%, which is significantly worse than the 17.1% WER that is obtained using a 7 \u00d7 2k DNN under the same conditions. Note that as the number of hidden layers further increases, only limited additional gains are obtained and performance saturates after 9 hidden layers. In practice, a tradeoff needs to be made between the additional reduction in WER and the increased cost of training and decoding as the number of hidden layers is increased."}, {"heading": "3.2 DNN learns more invariant features", "text": "We have noticed that the biggest benefit of using DNNs over shallow models is that DNN learns more invariant and discriminative features. This is because many layers of simple nonlinear processing can generate a complicated nonlinear transform. To show that this nonlinear transform is robust to small variations in the input features, let\u2019s assume the output of layer l\u2212 1, or equivalently the input to the layer l is changed from vl to vl + \u03b4l, where \u03b4l is a small change. This change will cause the output of layer l, or equivalently the input to the layer l + 1 to change by\n\u03b4l+1 = \u03c3(zl(vl + \u03b4l))\u2212 \u03c3(zl(vl)) \u2248 diag ( \u03c3(zl(vl)) ) (wl)T \u03b4l.\nThe norm of the change \u03b4l+1 is\n\u2016\u03b4l+1\u2016 \u2248 \u2016diag(\u03c3\u2032(zl(vl)))((wl)T \u03b4l)\u2016 \u2264 \u2016diag(\u03c3\u2032(zl(vl)))(wl)T \u2016\u2016\u03b4l\u2016 = \u2016diag(vl+1 \u25e6 (1\u2212 vl+1))(wl)T \u2016\u2016\u03b4l\u2016 (4)\nwhere \u25e6 refers to an element-wise product. Note that the magnitude of the majority of the weights is typically very small. This is illustrated in Figure 1, which shows the distribution of weight magnitudes on a 6\u00d72k DNN trained using 30 hours of SWB data. With the exception of the weights in the first layer, 98% of the weights have magnitudes less than 0.5.\nWhile each element in vl+1 \u25e6 (1 \u2212 vl+1) is less than or equal to 0.25, the actual value is typically much smaller. This means that a large percentage of hidden neurons will not be active, as shown in Figure 2. As a result, the average norm \u2016diag(vl+1 \u25e6 (1 \u2212 vl+1))(wl)T \u2016 in (4) is smaller than one in all layers, as indicated in Figure 3. Since all hidden layer values are bounded in the same range of (0, 1), this indicates that when there is a small perturbation on the input, the perturbation shrinks at each higher hidden layer. In other words, features generated by higher hidden layers are more invariant to variations than those represented by lower layers. Note that the maximum norm is typically larger than one, as seen in Figure 3. This is necessary since the differences needs to be enlarged around the class boundaries to have discrimination ability."}, {"heading": "4 Learning by seeing", "text": "In Section 3, we showed empirically that the small perturbations in the input will be gradually shrunk as we move to the internal representation in the higher layers. In this section, we point out that the above result is only applicable to small perturbations around the training samples. When the test samples deviate significantly from the training samples, DNNs cannot accurately classify them. In other words, DNNs must see examples of representative variations in the data during training in order to generalize to similar variations in the test data.\nWe demonstrate this point using a mixed-bandwidth ASR study. Typical speech recognizers are trained on either narrowband speech signals, recorded at 8 kHz, or wideband speech signals, recorded at 16 kHz. It would be advantageous if a single system could recognize both narrowband and wideband speech, i.e. mixed-bandwidth ASR. One such system was recently proposed using a CD-DNN-HMM [13]. In that work, the following DNN architecture was used for all experiments. The input features were 29 mel-scale log filter-bank outputs together with dynamic features. An 11-frame context window was used generating an input layer with 29*3*11=957 nodes. The DNN has 7 hidden layers, each with 2048 nodes. The output layer has 1803 nodes, corresponding to the number of senones determined by the GMM system.\nThe 29-dimensional filter bank has two parts: the first 22 filters span 0-4 kHz and the last 7 filters span 4-8 kHz, with the central frequency of the first higher filter bank at 4 kHz. When the speech is wideband, all 29 filters have observed values. However, when the speech is narrowband, the highfrequency information was not captured so the final 7 filters are set to 0. Figure 4 illustrates the architecture of the mixed-bandwidth ASR system.\nExperiments were conducted on a mobile voice search (VS) corpus. This task consists of internet search queries made by voice on a smartphone.There are two training sets, VS-1 and VS-2, consisting of 72 and 197 hours of wideband audio data, respectively. These sets were collected during different times of year. The test set, called VS-T, has 26757 words in 9562 utterances. The narrow band training and test data were obtained by downsampling the wideband data.\nTable 2 summarizes the WER on the wideband and narrowband test sets when the DNN is trained with and without narrowband speech. From this table, it is clear that if all training data are wideband, the DNN performs well on the wideband test set (27.5% WER) but very poorly on the narrowband test set (53.5% WER). However, if we convert VS-2 to narrowband speech and train the same DNN using mixed-bandwidth data (second row), the DNN performs very well on both wideband and narrowband speech.\nTo understand the difference between these two scenarios, we take the output vectors at each layer for the wideband and narrowband input feature pairs and measure their Euclidean distance,\ndl(xwb, xnb) = \u221a\u221a\u221a\u221a N l\u2211 j=1 ( hlj(xwb)\u2212 hlj(xnb)\n)2 where the hidden units are shown as a function of the wideband features, xwb, or the narrowband features, xnb. For the top layer, whose output is the senone posterior probability, we calculate the KL-divergence in nats,\ndy (xwb, xnb) = NL\u2211 j=1 py|x(sj |xwb) log py|x(sj |xwb) py|x(sj |xnb)\nwhere NL is the number of senones, and sj is the senone id. Table 3 shows the statistics of dl and dy over 40k frames randomly sampled from the test set for the DNN trained using wideband speech only and the DNN trained using mixed-bandwidth speech.\nFrom Table 3 we can observe that in both DNNs, the distance between hidden layer vectors generated from the wideband and narrowband input feature pair is significantly reduced at the layers close to the output layer compared to that in the first hidden layer. Perhaps what is more interesting is that the average distances and variances in the data-mixed DNN are consistently smaller than those in the DNN trained on wideband speech only. This indicates that by using mixed-bandwidth training data, the DNN learns to consider the differences in the wideband and narrowband input features as irrelevant variations. These variations are suppressed after many layers of nonlinear transformation. The final representation is thus more invariant to this variation and yet still has the ability to distinguish between different class labels. This behavior is even more obvious at the output layer since the KL-divergence between the paired outputs is only 0.22 in the mixed-bandwidth DNN, much smaller than the 2.03 observed in the wideband DNN."}, {"heading": "5 Robustness to environmental distortions", "text": "In many speech recognition tasks, there are often cases where the despite the presence of variability in the training data, significant mismatch between training and test data persists. Environmental factors are common sources of such mismatch, e.g. ambient noise, reverberation, microphone type and capture device. GMM-based acoustic models are highly sensitive to environmental mismatch and as a result, several techniques to normalize of the input features or adapt the model parameters have been developed, such as Vector Taylor Series (VTS) adaption [14] and Maximum Likelihood\nLinear Regression (MLLR) [15]. In contrast, the analysis in the previous sections suggests that DNNs have the ability to generate internal representations that are robust with respect to variability seen in the training data. In this section, we evaluate the extent to which this invariance can be obtained with respect to distortions caused by the environment.\nWe performed a series of experiments on the Aurora 4 corpus [16], a 5000-word vocabulary task based on the Wall Street Journal (WSJ0) corpus. The experiments were performed with the 16 kHz multi-condition training set consisting of 7137 utterances from 83 speakers. One half of the utterances was recorded by a high-quality close-talking microphone and the other half was recorded using one of 18 different secondary microphones. Both halves include a combination of clean speech and speech corrupted by one of six different types of noise (street traffic, train station, car, babble, restaurant, airport) at a range of signal-to-noise ratios (SNR) between 10-20 dB.\nThe evaluation set consists of 330 utterances from 8 speakers. This test set was recorded by the primary microphone and a number of secondary microphones. These two sets are then each corrupted by the same six noises used in the training set at SNRs between 5-15 dB, creating a total of 14 test sets. These 14 test sets can then be grouped into 4 subsets, based on the type of distortion: none (clean speech), additive noise only, channel distortion only, and noise + channel. Notice that the types of noise are common across training and test sets but the SNRs of the data are not.\nThe DNN was trained using 24-dimensional log mel filterbank features with utterance-level mean normalization. The first- and second-order derivative features were appended to the static feature vectors. The input layer was formed from a context window of 11 frames creating an input layer of 792 input units. The DNN had 7 hidden layers with 2048 hidden units in each layer and the final softmax output layer had 3206 units, corresponding to the senones of the baseline HMM system. The network was initialized using layer-by-layer generative pre-training and then discriminatively trained using back propagation.\nIn Table 4, the performance obtained by the DNN acoustic model is compared to several other systems. The first system is a baseline GMM-HMM system, while the remaining systems are representative of the state of the art in acoustic modeling and noise and speaker adaptation. All systems used the same training set. To the authors\u2019 knowledge, these are the best published results on this task.\nThe second system combines Minimum Phone Error (MPE) discriminative training [17] and noise adaptive training (NAT) [18] using VTS adaptation to compensate for noise and channel mismatch [19]. The third system uses a hybrid generative/discriminative classifier [20] as follows . First, an adaptively trained HMM with VTS adaptation is used to generate features based on state likelihoods and their derivatives. Then, these features are input to a discriminative log-linear model to obtain the final hypothesis. The fourth system uses an HMM trained with NAT and combines VTS adaptation for environment compensation and MLLR for speaker adaptation [21]. Finally, the last row of the table shows the performance of the DNN system.\nIt is noteworthy that to obtain good performance, the GMM-based systems required complicated adaptive training procedures [18, 22] and multiple iterations of recognition in order to perform explicit environment and/or speaker adaptation. One of these systems required two classifiers. In contrast, the DNN system required only standard training and a single forward pass for classification. Yet, it outperforms the two systems that perform environment adaptation and matches the performance of a system that adapts to both the environment and speaker.\nFinally, we recall the results in Section 4, in which the DNN trained only on wideband data could not accurately classify narrowband speech. Similarly, a DNN trained only on clean speech has no ability to learn internal features that are robust to environmental noise. When the DNN for Aurora 4 is trained using only clean speech examples, the performance on the noise- and channel-distorted speech degrades substantially, resulting in an average WER of 30.6%. This further confirms our earlier observation that DNNs are robust to modest variations between training and test data but perform poorly if the mismatch is more severe."}, {"heading": "6 Conclusion", "text": "In this paper we demonstrated through a speech recognition experiments that DNNs can extract more invariant and discriminative features at the higher layers. In other words, the features learned by DNNs are less sensitive to small perturbations in the input features. This property enables DNNs to generalize better than shallow networks and enables CD-DNN-HMMs to perform speech recognition in a manner that is more robust to mismatches in speaker, environment, or bandwidth. On the other hand, DNNs cannot learn something from nothing. They require seeing representative samples to perform well. By using a multi-style training strategy and letting DNNs to generalize to similar patterns, we equaled the best result ever reported on the Aurora 4 noise robustness benchmark task without the need for multiple recognition passes and model adaptation."}], "references": [{"title": "Roles of pretraining and fine-tuning in context-dependent DBN- HMMs for real-world speech recognition", "author": ["D. Yu", "L. Deng", "G. Dahl"], "venue": "Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-dependent pretrained deep neural networks for large vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. Audio, Speech, and Lang. Proc., vol. 20, no. 1, pp. 33\u201342, Jan. 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Proc. Interspeech, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G.Li", "X. Chen", "D. Yu"], "venue": "Proc. ASRU, 2011, pp. 24\u201329.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["D. Yu", "F. Seide", "G.Li", "L. Deng"], "venue": "Proc. ICASSP, 2012, pp. 4409\u20134412.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "An application of pretrained deep neural networks to large vocabulary conversational speech recognition", "author": ["N. Jaitly", "P. Nguyen", "A. Senior", "V. Vanhoucke"], "venue": "Tech. Rep. Tech. Rep. 001, Department of Computer Science, University of Toronto, 2012. 8", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Making deep belief networks effective for large vocabulary continuous speech recognition", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran", "P. Fousek", "P. Novak", "A. r. Mohamed"], "venue": "Proc. ASRU, 2011, pp. 30\u201335.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Large vocabulary continuous speech recognition with context-dependent dbn-hmms", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Proc. ICASSP, 2011, pp. 4688\u20134691.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Lawrence Saul", "Tommi Jaakkola", "Michael I. Jordan"], "venue": "Journal of Artificial Intelligence Research, vol. 4, pp. 61\u201376, 1996.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Using continuous features in the maximum entropy model", "author": ["D. Yu", "L. Deng", "A. Acero"], "venue": "Pattern Recognition Letters, vol. 30, no. 14, pp. 1295\u20131300, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep-structured hidden conditional random fields for phonetic recognition", "author": ["D. Yu", "L. Deng"], "venue": "Proc. Interspeech, 2010, pp. 2986\u20132989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Switchboard-1 Release 2, Linguistic Data", "author": ["J. Godfrey", "E. Holliman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Improving wideband speech recognition using mixedbandwidth training data in CD-DNN-HMM", "author": ["J. Li", "D. Yu", "J.-T. Huang", "Y. Gong"], "venue": "Proc. SLT, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "HMM Adaptation Using Vector Taylor Series for Noisy Speech Recognition", "author": ["A. Acero", "L. Deng", "T. Kristjansson", "J. Zhang"], "venue": "Proc. of ICSLP, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["M.J.F. Gales"], "venue": "Computer Speech and Language, vol. 12, pp. 75\u201398, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Aurora working group: DSR front end LVCSR evaluation AU/384/02", "author": ["N. Parihar", "J. Picone"], "venue": "Tech. Rep., Inst. for Signal and Information Process, Mississippi State University.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Minimum phone error and i-smoothing for improved discriminative training", "author": ["D. Povey", "P.C. Woodland"], "venue": "Proc. ICASSP, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Noise adaptive training for robust automatic speech recognition", "author": ["O. Kalinli", "M.L. Seltzer", "J. Droppo", "A. Acero"], "venue": "IEEE Trans. on Audio, Speech, and Language Proc., vol. 18, no. 8, pp. 1889 \u20131901, Nov. 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1889}, {"title": "Discriminative adaptive training with VTS and JUD", "author": ["F. Flego", "M.J.F. Gales"], "venue": "Proc. ASRU, 2009, pp. 170\u2013175.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Derivative kernels for noise robust ASR", "author": ["A. Ragni", "M.J.F. Gales"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker and noise factorisation for robust speech recognition", "author": ["Y.-Q. Wang", "M.J.F. Gales"], "venue": "IEEE Trans. on Audio Speech and Language Proc., vol. 20, no. 7, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive training with joint uncertainty decoding for robust recognition of noisy data", "author": ["H. Liao", "M.J.F. Gales"], "venue": "Proc. of ICASSP, Honolulu, Hawaii, 2007. 9", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 1, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 2, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 3, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 4, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 5, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 6, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 7, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 8, "context": "An effective practical trick is to replace the marginalization with the mean-field approximation [9] at each hidden layer.", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "In the conventional MaxEnt model, features are manually designed [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "For example in [11] the first and second-order statistics of the observations are used as features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 1, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 2, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 3, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 0, "context": "The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [1\u20133].", "startOffset": 87, "endOffset": 92}, {"referenceID": 1, "context": "The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [1\u20133].", "startOffset": 87, "endOffset": 92}, {"referenceID": 2, "context": "The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [1\u20133].", "startOffset": 87, "endOffset": 92}, {"referenceID": 2, "context": "Table 1, which is extracted from [3], summarizes the word error rates (WER) on the Switchboard (SWB) [12] Eval 2000 test set using the 309-hour training set whose senone alignment label was generated from a maximum likelihood (ML) trained GMM system.", "startOffset": 33, "endOffset": 36}, {"referenceID": 11, "context": "Table 1, which is extracted from [3], summarizes the word error rates (WER) on the Switchboard (SWB) [12] Eval 2000 test set using the 309-hour training set whose senone alignment label was generated from a maximum likelihood (ML) trained GMM system.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "One such system was recently proposed using a CD-DNN-HMM [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "GMM-based acoustic models are highly sensitive to environmental mismatch and as a result, several techniques to normalize of the input features or adapt the model parameters have been developed, such as Vector Taylor Series (VTS) adaption [14] and Maximum Likelihood", "startOffset": 239, "endOffset": 243}, {"referenceID": 14, "context": "Linear Regression (MLLR) [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "We performed a series of experiments on the Aurora 4 corpus [16], a 5000-word vocabulary task based on the Wall Street Journal (WSJ0) corpus.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "The second system combines Minimum Phone Error (MPE) discriminative training [17] and noise adaptive training (NAT) [18] using VTS adaptation to compensate for noise and channel mismatch [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "The second system combines Minimum Phone Error (MPE) discriminative training [17] and noise adaptive training (NAT) [18] using VTS adaptation to compensate for noise and channel mismatch [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "The second system combines Minimum Phone Error (MPE) discriminative training [17] and noise adaptive training (NAT) [18] using VTS adaptation to compensate for noise and channel mismatch [19].", "startOffset": 187, "endOffset": 191}, {"referenceID": 19, "context": "The third system uses a hybrid generative/discriminative classifier [20] as follows .", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "The fourth system uses an HMM trained with NAT and combines VTS adaptation for environment compensation and MLLR for speaker adaptation [21].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "6 MPE-NAT + VTS [19] 7.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "3 NAT + Derivative Kernels [20] 7.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "8 NAT + Joint MLLR/VTS [21] 5.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "It is noteworthy that to obtain good performance, the GMM-based systems required complicated adaptive training procedures [18, 22] and multiple iterations of recognition in order to perform explicit environment and/or speaker adaptation.", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "It is noteworthy that to obtain good performance, the GMM-based systems required complicated adaptive training procedures [18, 22] and multiple iterations of recognition in order to perform explicit environment and/or speaker adaptation.", "startOffset": 122, "endOffset": 130}], "year": 2017, "abstractText": "Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper we argue that the difficulty in speech recognition is primarily caused by the high variability in speech signals. DNNs, which can be considered a joint model of a nonlinear feature transform and a log-linear classifier, achieve improved recognition accuracy by extracting discriminative internal representations that are less sensitive to small perturbations in the input features. However, if test samples are very dissimilar to training samples, DNNs perform poorly. We demonstrate these properties empirically using a series of recognition experiments on mixed narrowband and wideband speech and speech distorted by environmental noise.", "creator": "LaTeX with hyperref package"}}}