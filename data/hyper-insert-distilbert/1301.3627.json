{"id": "1301.3627", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Two SVDs produce more focal deep learning representations", "abstract": "a then key characteristic measure of work on deep learning and neural neural networks in general dimensions is that it relies commonly on representations all of the input types that support posterior generalization, robust inference, domain adaptation algorithm and other desirable knowledge functionalities. much then recent progress there in reaching the field has focused on efficient and effective methods reserved for hierarchical computing representations. because in this comprehensive paper, we propose an obviously alternative method used that is more obviously efficient effective than prior basic work and that produces fuzzy representations that otherwise have a property we call focality - - a property we first hypothesize to eventually be incredibly important for preserving neural network representations. hence the method consists of providing a quite simple application of two matching consecutive fuzzy svds trees and is supposedly inspired by anandkumar ( 2012 ).", "histories": [["v1", "Wed, 16 Jan 2013 08:37:39 GMT  (84kb)", "https://arxiv.org/abs/1301.3627v1", null], ["v2", "Sat, 11 May 2013 12:17:44 GMT  (41kb)", "http://arxiv.org/abs/1301.3627v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["hinrich schuetze", "christian scheible"], "accepted": false, "id": "1301.3627"}, "pdf": {"name": "1301.3627.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hs999@ifnlp.org", "scheibcn@ims.uni-stuttgart.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 1.\n36 27\nv2 [\ncs .C\nL ]\n1 1\nM ay\nA key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality \u2013 a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by (Anandkumar et al., 2012).\nIn this paper, we propose to generate representations for deep learning by two consecutive applications of singular value decomposition (SVD). In a setup inspired by (Anandkumar et al., 2012), the first SVD is intended for denoising. The second SVD rotates the representation to increase what we call focality. In this initial study, we do not evaluate the representations in an application. Instead we employ diagnostic measures that may be useful in their own right to evaluate the quality of representations independent of an application.\nWe use the following terminology. SVD1 (resp. SVD2) refers to the method using one (resp. two) applications of SVD; 1LAYER (resp. 2LAYER) corresponds to a single-hidden-layer (resp. twohidden-layer) architecture.\nIn Section 1, we introduce the two methods SVD1 and SVD2 and show that SVD2 generates better (in a sense to be defined below) representations than SVD1. In Section 2, we compare 1LAYER and 2LAYER SVD2 representations and show that 2LAYER representations are better. Section 3 discusses the results. We present our conclusions in Section 4.\n1 SVD1 vs. SVD2\nGiven a base representation of n objects in Rd, we first compute the first k dimensions of an SVD on the corresponding n\u00d7 d matrix C. Ck = USV T (where Ck is the rank-k approximation of C). We then use US to represent each object as a k-dimensional vector. Each vector is normalized to unit length because our representations are count vectors where the absolute magnitude of a count contains little useful information \u2013 what is important is the relative differences between the counts of different dimensions. This is the representation SVD1. It is motivated by standard arguments for representations produced by dimensionality reduction: compactness and noise reduction. Denoising is also the motivation for the first SVD in the method proposed by Anandkumar et al. (2012).\nWe then perform a second SVD on the resulting matrix C\u2032 of dimensionality n\u00d7 k. C\u2032 = U \u2032S\u2032V \u2032T (full-rank, no dimensionality reduction). We again use U \u2032S\u2032 to represent each object as a kdimensional vector. Each vector is normalized to unit length. This is the representation SVD2.\nNote that the operation we are applying is not equivalent to a single linear operation because of the lenght normalization that we perform between the two SVDs.\nSVD2 is intended to be a rotation of SVD1 that is more \u201cfocal\u201d in the following sense. Consider a classification problem f over a k-dimensional representation space R. Let M\u03b8(f,R) be the size k\u2032 of the smallest subset of the dimensions that support an accuracy above a threshold \u03b8 for f . Then a representation R is more focal than R\u2032 if M\u03b8(f,R) < M\u03b8(f,R\u2032). The intuition is that good deep learning representations have semantically interpretable hidden units that contribute input to a decision that is to some extent independent of other hidden units. We want the second SVD to rotate the representation into a \u201cmore focal\u201d direction.\nThe role of the second SVD is somewhat analogous to that of the second SVD in the approach of Anandkumar et al. (2012), where the goal also is to find a representation that reveals the underlying structure of the data set.\nThe architecture of the 1LAYER setup is depicted in Figure 1.\nExperimental setup. We use a corpus of movie review sentences (Pang and Lee, 2004). Following Schu\u0308tze (1995), we first compute a left vector and a right vector for each word. The dimensionality of the vectors is 250. Entry i for the left (right) vector of word w is the number of times that the word with frequency rank i occurred immediately to the left (right) of w. Vectors are then tf-idf weighted and length-normalized. We randomly select 100,000 unique trigrams from the corpus, e.g., \u201ctangled feelings of\u201d or \u201cas it pushes\u201d. Each trigram is represented as the concatentation of six vectors, the left and right vectors of the three words. This defines a matrix of dimensionality n\u00d7d (n = 100000, d = 1500). We then compute SVD1 and SVD2 on this matrix for k = 100.\nAnalysis of correlation coefficients. Figure 2 shows histograms of the 10,000 correlation coefficients of SVD1 (left) and SVD2 (right). Each correlation coefficient is the correlation of two columns in the corresponding 100000 \u00d7 100 matrix and is transformed using the function f(c) = log10 |c| to produce a histogram useful for the analysis. The histogram of SVD2 is shifted by about 0.5 to the left. This is a factor of 100.5 \u2248 3. Thus, SVD2 dimensions have correlations that are only a third as large as SVD1 correlations on average.\nWe take this to indicate that SVD2 representations are more focal than SVD1 representations because the distribution of correlation coefficients would change the way it changes from SVD2 to SVD1 if we took a focal representation (in the most extreme case one where each dimension by itself supported a decision) and rotated it.\nDiscrimination task. We randomly selected 200 words in the frequency range [25, 250] from the corpus; and randomly arranged them into 100 pairs. An example of such a pair is (documentary, special). For each pair, we first retrieved the SVD1 and SVD2 representations of all triples from the set of 100,000 in which one of the two words was the central word. For the example, \u201ctypical documentary footage\u201d, \u201cgood documentary can\u201d, and \u201creally special walk\u201d are such triples. Then we determined for each dimension i of the 100 dimensions (for both SVD1 and SVD2) the optimal discrimination value \u03b8 by exhaustive search; that is, we determined the threshold \u03b8 for which the accuracy of the classifer ~vi > \u03b8 (or ~vi < \u03b8) was greatest \u2013 where the discrimination task was to distinguish triples that had one word vs the other as their central word. So for \u201ctypical documentary footage\u201d and \u201cgood documentary can\u201d the classifier should predict class 1 (\u201cdocumentary\u201d), for \u201cre-\nally special walk\u201d the classifier should predict class 2 (\u201cspecial\u201d). Finally, of the 100 discrimination accuracies we chose the largest one for this word pair.\nOn this discrimination task, SVD2 was better than SVD1 55 times, the two were equal 15 times and SVD2 was worse 30 times. On average, discrimination accuracy of SVD2 was 0.7% better than that of SVD1. This is evidence that SVD2 is better for this discrimination task than SVD1.\nThis indicates again that SVD2 representations are more focal than SVD1 representations: each dimension is more likely to provide crucial information by itself as opposed to only being useful in conjunction with other dimensions.\nTo illustrate in more detail why this discrimination task is related to focality, assume that for a particular 100-dimensional representation r of trigrams t, the decision rule \u201cif r(t)27 > 0.2 then \u2018documentary\u2019 else \u2018special\u2019 \u201d (i.e., if the value of dimension 27 is greater than 0.2, then the trigram center is predicted to be \u201cdocumentary\u201d, else \u201cspecial\u201d) has an accuracy of 0.99; and that the decision rule \u201cif r(t)27 > 0.2 and r(t)91 < \u22120.1 then \u2018documentary\u2019 else \u2018special\u2019 \u201d has an accuracy of 1.0. Then M0.99(f, documentary-vs-special) = 1, M1.00(f, documentary-vs-special) = 2 and we can view the representation r as highly focal since a single dimension suffices for high accuracy and two dimensions achieve perfect classification results."}, {"heading": "2 1LAYER vs. 2LAYER", "text": "We compare two representations of a word trigram: (i) the 1LAYER representation from Section 1 and (ii) a 2LAYER representation that goes through two rounds of autoencoding, which is a deep learning representation in the sense that layer 2 represents more general and higher-level properties of the input than layer 1.\nThe architecture of the 2LAYER is depicted in Figure 3.\nTo create 2LAYER representations, we first create a vector for each of the 20701 word types occurring in the corpus. This vector is the concatenation of its left vector and its right vector. The resulting 20701 \u00d7 500 matrix is the input representation to SVD1. We again set k = 100. A trigram\nis then represented as the concatenation of three of these 100-dimensional vectors. We apply the SVD2 construction algorithm to the resulting 100000 \u00d7 300 matrix and truncate to k = 100.\nWe now have \u2013 for each trigram \u2013 two SVD2 representations, the 1LAYER representation from Section 1 and the 2LAYER representation we just described. We compare these two trigram representations, again using the task from Section 1: discrimination of the 100 pairs of words.\n2LAYER is better than 1LAYER 64 times on this task, the same in 18 cases and worse in 18 cases. This is statistically significant (p < .01, binomial test) evidence that 2LAYER SVD2 representations are more focal than 1LAYER SVD2 representations."}, {"heading": "3 Discussion", "text": ""}, {"heading": "3.1 Focality", "text": "One advantage of focal representations is that many classifiers cannot handle conjunctions of several features unless they are explicitly defined as separate features. Compare two representations ~x and ~x\u2032 where ~x\u2032 is a rotation of ~x (as it might be obtained by an SVD). Since one vector is a rotation of the other, they contain exactly the same information. However, if (i) an individual \u201chidden unit\u201d of the rotated vector ~x\u2032 can directly be interpreted as \u201cis verb\u201d (or a similar property like \u201cis adjective\u201d or \u201ctakes NP argument\u201d) and (ii) the same feature requires a conjunction of several hidden units for ~x, then the rotated representation is superior for many upstream statistical classifiers.\nFocal representations can be argued to be closer to biological reality than broadly distributed representations (Thorpe, 2010); and they have the nice property that they become categorical in the limit. Thus, they include categorical representations as a special case.\nA final advantage of focal representations is that in some convolutional architectures the input to the top-layer statistical classifier consists of maxima over HU (hidden unit) activations. E.g., one way to classify a sentence as having positive/negative sentiment is to slide a neural network whose input is a window of k words (e.g., k = 4) over it and to represent each window of k words as a vector of HU activations produced by the network. In a focal representation, the hidden units are more likely to have clear semantics like \u201cthe window contains a positive sentiment word\u201d. In this type of scenario, taking the maximum of activations over the n \u2212 k + 1 sliding windows of a sentence of length n results in hidden units with interpretable semantics like \u201cthe activation of the positive-sentiment HU of the window with the highest activation for this HU\u201d. These maximum values are then a good basis for sentiment classification of the sentence as a whole.\nThe notion of focality is similar to disentanglement (Glorot et al., 2011) \u2013 in fact, the two notions may be identical. However, Glorot et al. (2011) introduce disentanglement in the context of domain adaptation, focusing on the idea that \u201cdisentangled\u201d hidden units capture general cross-domain properties and for that reason are a good basis for domain adaptation. The contributions of this paper are: proposing a way of measuring \u201centanglement\u201d (i.e., measuring it as correlation), defining focality\nin terms of classification accuracy (a definition that covers single hidden units as well as groups of hidden units) and discussing the relationship to convolution and biological systems.\nIt is important to point out that we have not addressed how focality would be computed efficiently in a particular context. In theory, we could use brute force methods, but these would be exponential in the number of dimensions (systematic search over all subsets of dimensions). However, certain interesting questions about focality can be answered efficiently; e.g., if we have M(f,R) = 1 for one representation and M(f,R\u2032) > 1 for another, then this can be shown efficiently and in this case we have established that R is more focal than R\u2032."}, {"heading": "3.2 mSVD method", "text": "In this section, we will use the abbreviation mSVD to refer to a stacked applications of our method with an arbitrary number of layers even though we only experiment with m = 2 in this paper (2LAYER, 2-layer-stacking).\nSVD and other least squares methods are probably the most widely used dimensionality reduction techniques for the type of matrices in natural language processing that we work with in this paper (cf. (Turney and Pantel, 2010)). Stacking a second least squares method on top of the first has not been considered widely because these types of representations are usually used directly in vector classifiers such as Rocchio and SVM (however, see the discussion of (Chen et al., 2012) below). For this type of classifier, performing a rotation has no effect on classification performance. In contrast, our interest is to use SVD2 representations as part of a multilevel deep learning architecture where the hidden unit representations of any given layer are not simply interpreted as a vector, but decisions of higher layers can be based on individual dimensions.\nThe potential drawback of SVD and other least squares dimensionality reductions is that they are linear: reduced dimensions are linear combinations of orginal dimensions. To overcome this limitation many nonlinear methods have been introduced: probabilistic latent semantic indexing (Hofmann, 1999), kernel principal component analysis (Scho\u0308lkopf et al., 1998), matrix factorization techniques that obey additional constraints \u2013 such as non-negativity in the case of non-negative matrix factorization (Lee and Seung, 1999) \u2013 , latent dirichlet allocation (Blei et al., 2003) and different forms of autoencoding (Bengio, 2009; Chen et al., 2012). All of these can be viewed as dimension reduction techniques that do not make the simplistic assumptions of SVD and should therefore be able to produce better representation if these simplistic assumptions are not appropriate for the domain in question.\nHowever, this argument does not apply to the mSVD method we propose in this paper since it is also nonlinear. What should be investigated in the future is to what extent the type of nonlinearity implemented by mSVD offers advantages over other forms of nonlinear dimensionality reduction; e.g., if the quality of the final representations is comparable, then mSVD would have the advantage of being more efficient.\nFinally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al. (2012). Most deep learning representations are induced in a setting that also includes elements of supervised learning as is the case in contrastive divergence or when labeled data are available for adjusting initial representations produced by a process like autoencoding or dimensionality reduction.\nThis is the most important open question related to the research presented here: how can one modify hidden layer representations initialized by multiple SVDs in a meaningful way?\nThe work most closely related to what we are proposing is probably mDA (Chen et al., 2012) \u2013 an approach we only became aware of after the initial publication of this paper. There are a number of differences between mDA and mSVD. Non-linearity in mDA is achieved by classical deep learning encoding functions like tanh() whereas we renormalize vectors and then rotate them. Second, we do not add noise to the input vectors \u2013 mSVD is more efficient for this reasons, but it remains to be seen if it can achieve the same level of performance as mDA. Third, the mSVD architecture proposed here, which changes the objects to be represented from small frequent units to larger less frequent units when going one layer up, can be seen as an alternative (though less general since it\u2019s customized for natural language) way of extending to very high dimensions."}, {"heading": "4 Conclusion", "text": "As a next step a direct comparison should be performed of SVD2 with traditional deep learning (Hinton et al., 2006). As we have argued, SVD2 would be an interesting alternative to deep learning initialization methods currently used since SVD is efficient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable quality. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al. (2011), and Socher et al. (2011)."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Bengio", "Yoshua."], "venue": "Foundations and Trends in Machine", "citeRegEx": "Bengio and Yoshua.,? 2009", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Latent Dirichlet allocation", "author": ["Blei", "David M.", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "JMLR,", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A unified architecture for natural language processing", "author": ["Collobert", "Ronan", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Domain adaptation for large-scale", "author": ["Glorot", "Xavier", "Antoine Bordes", "Yoshua Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A fast learning algorithm for deep", "author": ["Hinton", "Geoffrey E", "Simon Osindero", "Yee-Whye Teh"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Probabilistic latent semantic indexing", "author": ["Hofmann", "Thomas."], "venue": "SIGIR, pages 50\u201357.", "citeRegEx": "Hofmann and Thomas.,? 1999", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "Learning the parts of objects by non-negative matrix", "author": ["Lee", "David D", "H. Sebastian Seung"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Learning word vectors for sentiment analysis", "author": ["Potts."], "venue": "ACL, pages 142\u2013150.", "citeRegEx": "Potts.,? 2011", "shortCiteRegEx": "Potts.", "year": 2011}, {"title": "A sentimental education: Sentiment analysis using subjectivity", "author": ["Pang", "Bo", "Lillian Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Distributional part-of-speech tagging", "author": ["Sch\u00fctze", "Hinrich."], "venue": "Conference of the European", "citeRegEx": "Sch\u00fctze and Hinrich.,? 1995", "shortCiteRegEx": "Sch\u00fctze and Hinrich.", "year": 1995}, {"title": "Grandmother cells and distributed representations", "author": ["Thorpe", "Simon."], "venue": "Kriegeskorte, Nikolaus", "citeRegEx": "Thorpe and Simon.,? 2010", "shortCiteRegEx": "Thorpe and Simon.", "year": 2010}, {"title": "Word representations: A simple and", "author": ["Turian", "Joseph", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models", "author": ["Turney", "Peter D", "Patrick Pantel"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "The notion of focality is similar to disentanglement (Glorot et al., 2011) \u2013 in fact, the two notions may be identical.", "startOffset": 53, "endOffset": 74}, {"referenceID": 3, "context": "The notion of focality is similar to disentanglement (Glorot et al., 2011) \u2013 in fact, the two notions may be identical. However, Glorot et al. (2011) introduce disentanglement in the context of domain adaptation, focusing on the idea that \u201cdisentangled\u201d hidden units capture general cross-domain properties and for that reason are a good basis for domain adaptation.", "startOffset": 54, "endOffset": 150}, {"referenceID": 1, "context": ", 1998), matrix factorization techniques that obey additional constraints \u2013 such as non-negativity in the case of non-negative matrix factorization (Lee and Seung, 1999) \u2013 , latent dirichlet allocation (Blei et al., 2003) and different forms of autoencoding (Bengio, 2009; Chen et al.", "startOffset": 202, "endOffset": 221}, {"referenceID": 4, "context": "Finally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 4, "context": "Finally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al.", "startOffset": 110, "endOffset": 160}, {"referenceID": 4, "context": "Finally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al. (2012). Most deep learning representations are induced in a setting that also includes elements of supervised learning as is the case in contrastive divergence or when labeled data are available for adjusting initial representations produced by a process like autoencoding or dimensionality reduction.", "startOffset": 110, "endOffset": 185}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006).", "startOffset": 93, "endOffset": 114}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006). As we have argued, SVD would be an interesting alternative to deep learning initialization methods currently used since SVD is efficient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable quality. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al.", "startOffset": 94, "endOffset": 484}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006). As we have argued, SVD would be an interesting alternative to deep learning initialization methods currently used since SVD is efficient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable quality. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al. (2011), and Socher et al.", "startOffset": 94, "endOffset": 504}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006). As we have argued, SVD would be an interesting alternative to deep learning initialization methods currently used since SVD is efficient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable quality. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al. (2011), and Socher et al. (2011).", "startOffset": 94, "endOffset": 530}], "year": 2013, "abstractText": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality \u2013 a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by (Anandkumar et al., 2012). In this paper, we propose to generate representations for deep learning by two consecutive applications of singular value decomposition (SVD). In a setup inspired by (Anandkumar et al., 2012), the first SVD is intended for denoising. The second SVD rotates the representation to increase what we call focality. In this initial study, we do not evaluate the representations in an application. Instead we employ diagnostic measures that may be useful in their own right to evaluate the quality of representations independent of an application. We use the following terminology. SVD (resp. SVD) refers to the method using one (resp. two) applications of SVD; 1LAYER (resp. 2LAYER) corresponds to a single-hidden-layer (resp. twohidden-layer) architecture. In Section 1, we introduce the two methods SVD and SVD and show that SVD generates better (in a sense to be defined below) representations than SVD. In Section 2, we compare 1LAYER and 2LAYER SVD representations and show that 2LAYER representations are better. Section 3 discusses the results. We present our conclusions in Section 4.", "creator": "LaTeX with hyperref package"}}}