{"id": "1301.5160", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2013", "title": "See the Tree Through the Lines: The Shazoo Algorithm -- Full Version --", "abstract": "predicting the nodes of a smooth given graph is consider a fascinating huge theoretical problem confronting with numerical applications in several basic domains. practically since partial graph sparsification via spanning the trees just retains enough graph information while making solving the task performs much easier, compute trees are arguably an important special case examples of this problem. although arguably it is known historically how early to predict the hierarchical nodes of an unweighted message tree functions in a nearly optimal way, only in summary the simple weighted queue case running a fully locally satisfactory adaptive algorithm here is not available yet. we fill this hole successfully and obviously introduce an efficient compute node predictor, shazoo, if which is nearly optimal on generating any pure weighted tree. moreover,, we ourselves show that shazoo can naturally be viewed as a common nontrivial generalization realization of both the previous approaches for detecting unweighted composite trees labeled and weighted lines. sophisticated experiments on real - world datasets confirm that shazoo performs well in that it typically fully exploits reduces the structure of storing the predicted input tree, and so gets very close to ( equivalent and can sometimes better detailed than ) relatively less scalable energy minimization methods.", "histories": [["v1", "Tue, 22 Jan 2013 11:59:04 GMT  (240kb,D)", "http://arxiv.org/abs/1301.5160v1", null], ["v2", "Thu, 28 Feb 2013 17:31:08 GMT  (240kb,D)", "http://arxiv.org/abs/1301.5160v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fabio vitale", "nicolo cesa-bianchi", "claudio gentile", "giovanni zappella"], "accepted": false, "id": "1301.5160"}, "pdf": {"name": "1301.5160.pdf", "metadata": {"source": "CRF", "title": "See the Tree Through the Lines: The Shazoo Algorithm", "authors": ["Fabio Vitale"], "emails": ["fabio.vitale@unimi.it", "nicolo.cesa-bianchi@unimi.it", "claudio.gentile@uninsubria.it", "giovanni.zappella@unimi.it"], "sections": [{"heading": "1 Introduction", "text": "Predictive analysis of networked data is a fast-growing research area whose application domains include document networks, online social networks, and biological networks. In this work we view networked data as weighted graphs, and focus on the task of node classification in the transductive setting, i.e., when the unlabeled graph is available beforehand. Standard transductive classification methods, such as label propagation [2, 3, 19], work by optimizing a cost or energy function defined on the graph, which includes the training information as labels assigned to training nodes. Although these methods perform well in practice, they are often computationally expensive, and have performance guarantees that require statistical assumptions on the selection of the training nodes.\nA general approach to sidestep the above computational issues is to sparsify the graph to the largest possible extent, while retaining much of its spectral properties \u2014see, e.g., [5, 6, 13, 17]. Inspired by [5, 6], this paper reduces the problem of node classification from graphs to trees by extracting suitable spanning trees of the graph, which can be done quickly in many cases. The advantage of performing this reduction\nar X\niv :1\n30 1.\n51 60\nv1 [\nis that node prediction is much easier on trees than on graphs. This fact has recently led to the design of very scalable algorithms with nearly optimal performance guarantees in the online transductive model, which comes with no statistical assumptions. Yet, the current results in node classification on trees are not satisfactory. The TREEOPT strategy of [5] is optimal to within constant factors, but only on unweighted trees. No equivalent optimality results are available for general weighted trees. To the best of our knowledge, the only other comparable result is WTA by [6], which is optimal (within log factors) only on weighted lines. In fact, WTA can still be applied to weighted trees by exploiting an idea contained in [10]. This is based on linearizing the tree via a depth-first visit. Since linearization loses most of the structural information of the tree, this approach yields suboptimal mistake bounds. This theoretical drawback is also confirmed by empirical performance: throwing away the tree structure negatively affects the practical behavior of the algorithm on real-world weighted graphs.\nThe importance of weighted graphs, as opposed to unweighted ones, is suggested by many practical scenarios where the nodes carry more information than just labels, e.g., vectors of feature values. A natural way of leveraging this side information is to set the weight on the edge linking two nodes to be some function of the similariy between the vectors associated with these nodes. In this work, we bridge the gap between the weighted and unweighted cases by proposing a new prediction strategy, called SHAZOO, achieving a mistake bound that depends on the detailed structure of the weighted tree. We carry out the analysis using a notion of learning bias different from the one used in [6] and more appropriate for weighted graphs. More precisely, we measure the regularity of the unknown node labeling via the weighted cutsize induced by the labeling on the tree (see Section 3 for a precise definition). This replaces the unweighted cutsize that was used in the analysis of WTA. When the weighted cutsize is used, a cut edge violates this inductive bias in proportion to its weight. This modified bias does not prevent a fair comparison between the old algorithms and the new one: SHAZOO specializes to TREEOPT in the unweighted case, and to WTA when the input tree is a weighted line. By specializing SHAZOO\u2019s analysis to the unweighted case we recover TREEOPT\u2019s optimal mistake bound. When the input tree is a weighted line, we recover WTA\u2019s mistake bound expressed through the weighted cutsize instead of the unweighted one. The effectiveness of SHAZOO on any tree is guaranteed by a corresponding lower bound (see Section 3).\nSHAZOO can be viewed as a common nontrivial generalization of both TREEOPT and WTA. Obtaining this generalization while retaining and extending the optimality properties of the two algorithms is far from being trivial from a conceptual and technical standpoint. Since SHAZOO works in the online transductive model, it can easily be applied to the more standard train/test (or \u201cbatch\u201d) transductive setting: one simply runs the algorithm on an arbitrary permutation of the training nodes, and obtains a predictive model for all test nodes. However, the implementation might take advantage of knowing the set of training nodes beforehand. For this reason, we present two implementations of SHAZOO, one for the online and one for the batch setting. Both implementations result in fast algorithms. In particular, the batch one is linear in |V |. This is achieved by a fast algorithm for weighted cut minimization on trees, a procedure which lies at the heart of SHAZOO.\nFinally, we test SHAZOO against WTA, label propagation, and other competitors on real-world weighted graphs. In almost all cases (as expected), we report improvements over WTA due to the better sensitivity to the graph structure. In some cases, we see that SHAZOO even outperforms standard label propagation methods. Recall that label propagation has a running time per prediction which is proportional to |E|, where E is the graph edge set. On the contrary, SHAZOO can typically be run in constant amortized time per prediction by using Wilson\u2019s algorithm for sampling random spanning trees [18]. By disregarding edge weights in the initial sampling phase, this algorithm is able to draw a random (unweighted) spanning tree in time proportional to |V | on most graphs. Our experiments reveal that using the edge weights only in the subsequent prediction phase causes in practice only a minor performance degradation."}, {"heading": "2 Preliminaries and basic notation", "text": "Let T = (V,E,W ) be an undirected and weighted tree with |V | = n nodes, positive edge weights Wi,j > 0 for (i, j) \u2208 E, and Wi,j = 0 for (i, j) /\u2208 E. A binary labeling of T is any assignment y = (y1, . . . , yn) \u2208 {\u22121,+1}n of binary labels to its nodes. We use (T,y) to denote the resulting labeled weighted tree. The online learning protocol for predicting (T,y) is defined as follows. The learner is given T while y is kept hidden. The nodes of T are presented to the learner one by one, according to an unknown and arbitrary permutation i1, . . . , in of V . At each time step t = 1, . . . , n node it is presented and the learner must issue a prediction y\u0302it \u2208 {\u22121,+1} for the label yit . Then yit is revealed and the learner knows whether a mistake occurred. The learner\u2019s goal is to minimize the total number of prediction mistakes.\nFollowing previous works [11, 10, 5, 6, 7], we measure the regularity of a labeling y of T in terms of \u03c6-edges, where a \u03c6-edge for (T,y) is any (i, j) \u2208 E such that yi 6= yj . The overall amount of irregularity in a labeled tree (T,y) is the weighted cutsize \u03a6W = \u2211 (i,j)\u2208E\u03c6Wi,j , whereE\n\u03c6 \u2286 E is the subset of \u03c6-edges in the tree. We use the weighted cutsize as our learning bias, that is, we want to design algorithms whose predictive performance scales with \u03a6W . Unlike the \u03c6-edge count \u03a6 = |E\u03c6|, which is a good measure of regularity for unweighted graphs, the weighted cutsize takes the edge weight Wi,j into account when measuring the irregularity of a \u03c6-edge (i, j). In the sequel, when we measure the distance between any pair of nodes i and j on the input tree T we always use the resistance distance metric d, that is, d(i, j) =\u2211\n(r,s)\u2208\u03c0(i,j) 1 Wr,s , where \u03c0(i, j) is the unique path connecting i to j."}, {"heading": "3 A lower bound for weighted trees", "text": "In this section we show that the weighted cutsize can be used as a lower bound on the number of online mistakes made by any algorithm on any tree. In order to do so (and unlike previous papers on this specific subject \u2014see, e.g., [6]), we need to introduce a more refined notion of adversarial \u201cbudget\u201d. Given T = (V,E,W ), let \u03be(M) be the maximum number of edges of T such that the sum of their weights does not exceed M , \u03be(M) = max { |E\u2032| : E\u2032 \u2286 E, \u2211 (i,j)\u2208E\u2032 wi,j \u2264M } . We have the following simple lower bound (all proofs are omitted from this extended abstract).\nTheorem 1 For any weighted tree T = (V,E,W ) there exists a randomized label assignment to V such that any algorithm can be forced to make at least \u03be(M)/2 online mistakes in expectation, while \u03a6W \u2264M .\nSpecializing [6, Theorem 1] to trees gives the lower bound K/2 under the constraint \u03a6 \u2264 K \u2264 |V |. The main difference between the two bounds is the measure of label regularity being used: Whereas Theorem 1 uses \u03a6W , which depends on the weights, [6, Theorem 1] uses the weight-independent quantity \u03a6. This dependence of the lower bound on the edge weights is consistent with our learning bias, stating that a heavy \u03c6-edge violates the bias more than a light one. Since \u03be is nondecreasing, the lower bound implies a number of mistakes of at least \u03be(\u03a6W )/2. Note that \u03be(\u03a6W ) \u2265 \u03a6 for any labeled tree (T,y). Hence, whereas a constraint K on \u03a6 implies forcing at least K/2 mistakes, a constraint M on \u03a6W allows the adversary to force a potentially larger number of mistakes.\nIn the next section we describe an algorithm whose mistake bound nearly matches the above lower bound on any weighted tree when using \u03be(\u03a6W ) as the measure of label regularity."}, {"heading": "4 The Shazoo algorithm", "text": "In this section we introduce the SHAZOO algorithm, and relate it to previously proposed methods for online prediction on unweighted trees (TREEOPT from [5]) and weighted line graphs (WTA from [6]). In fact,\nSHAZOO is optimal on any weighted tree, and reduces to TREEOPT on unweighted trees and to WTA on weighted line graphs. Since TREEOPT and WTA are optimal on any unweighted tree and any weighted line graph, respectively, SHAZOO necessarily contains elements of both of these algorithms.\nIn order to understand our algorithm, we now define some relevant structures of the input tree T . See Figure 1 (left) for an example. These structures evolve over time according to the set of observed labels. First, we call revealed a node whose label has already been observed by the online learner; otherwise, a node is unrevealed. A fork is any unrevealed node connected to at least three different revealed nodes by edge-disjoint paths. A hinge node is either a revealed node or a fork. A hinge tree is any component of the forest obtained by removing from T all edges incident to hinge nodes; hence any fork or labeled node forms a 1-node hinge tree. When a hinge tree H contains only one hinge node, a connection node for H is the node contained in H . In all other cases, we call a connection node for H any node outside H which is adjacent to a node in H . A connection fork is a connection node which is also a fork. Finally, a hinge line is any path connecting two hinge nodes such that no internal node is a hinge node.\nGiven an unrevealed node i and a label value y \u2208 {\u22121,+1}, the cut function cut(i, y) is the value of the minimum weighted cutsize of T over all labelings y \u2208 {\u22121,+1}n consistent with the labels seen so far and such that yi = y. Define \u2206(i) = cut(i,\u22121)\u2212 cut(i,+1) if i is unrevealed, and \u2206(i) = yi, otherwise. The algorithm\u2019s pseudocode is given in Algorithm 1. At time t, in order to predict the label yit of node it, SHAZOO calculates \u2206(i) for all connection nodes i of H(it), where H(it) is the hinge tree containing it. Then the algorithm predicts yit using the label of the connection node i of H(it) which is closest to it and such that \u2206(i) 6= 0 (recall from Section 2 that all distances/lengths are measured using the resistance metric). Ties are broken arbitrarily. If \u2206(i) = 0 for all connection nodes i in H(it) then SHAZOO predicts a default value (\u22121 in the pseudocode). If it is a fork (which is also a hinge node), then H(it) = {it}. In this case, it is a connection node of H(it), and obviously the one closest to itself. Hence, in this case SHAZOO predicts yt simply by y\u0302it = sgn ( \u2206(it) ) . See Figure 1 (middle) for an example. On unweighted trees, computing \u2206(i) for a connection node i reduces to the Fork Label Estimation Procedure in [5, Lemma 13]. On the\nAlgorithm 1: SHAZOO for t = 1 . . . n\nLet C ( H(it) ) be the set of the connection nodes i of H(it) for which \u2206(i) 6= 0\nif C ( H(it) ) 6\u2261 \u2205\nLet j be the node of C ( H(it) ) closest to it\nSet y\u0302it = sgn ( \u2206(j) ) else\nSet y\u0302it = \u22121 (default value)\nother hand, predicting with the label of the connection node closest to it in resistance distance is reminiscent of the nearest-neighbor prediction of WTA on weighted line graphs [6]. In fact, as in WTA, this enables to take advantage of labelings whose \u03c6-edges are light weighted. An important limitation of WTA is that this algorithm linearizes the input tree. On the one hand, this greatly simplifies the analysis of nearest-neighbor prediction; on the other hand, this prevents exploiting the structure of T , thereby causing logaritmic slacks in the upper bound of WTA. The TREEOPT algorithm, instead, performs better when the unweighted input tree is very different from a line graph (more precisely, when the input tree cannot be decomposed into long edge-disjoint paths, e.g., a star graph). Indeed, TREEOPT\u2019s upper bound does not suffer from logaritmic slacks, and is tight up to constant factors on any unweighted tree. Similar to TREEOPT, SHAZOO does not linearize the input tree and extends to the weighted case TREEOPT\u2019s superior performance, also confirmed by the experimental comparison reported in Section 6.\nIn Figure 1 (right) we show an example that highlights the importance of using the \u2206 function to compute the fork labels. Since \u2206 predicts a fork it with the label that minimizes the weighted cutsize of T consistent with the revealed labels, one may wonder whether computing \u2206 through mincut based on the number of \u03c6-edges (rather than their weighted sum) could be an effective prediction strategy. Figure 1 (right) illustrates an example of a simple tree where such a \u2206 mispredicts the labels of all nodes, when both \u03a6W and \u03a6 are small.\nRemark 1 We would like to stress that SHAZOO can also be used to predict the nodes of an arbitrary graph by first drawing a random spanning tree T of the graph, and then predicting optimally on T \u2014see, e.g., [5, 6]. The resulting mistake bound is simply the expected value of SHAZOO\u2019s mistake bound over the random draw of T . By using a fast spanning tree sampler [18], the involved computational overhead amounts to constant amortized time per node prediction on \u201cmost\u201d graphs.\nRemark 2 In certain real-world input graphs, the presence of an edge linking two nodes may also carry information about the extent to which the two nodes are dissimilar, rather than similar. This information can be encoded by the sign of the weight, and the resulting network is called a signed graph. The regularity measure is naturally extended to signed graphs by counting the weight of frustrated edges (e.g.,[8]), where (i, j) is frustrated if yiyj 6= sgn(wi,j). Many of the existing algorithms for node classification [19, 10, 11, 5, 9, 6] can in principle be run on signed graphs. However, the computational cost may not always be preserved. For example, mincut [4] is in general NP-hard when the graph is signed [14]. Since our algorithm sparsifies the graph using trees, it can be run efficiently even in the signed case. We just need to re-define the \u2206 function as \u2206(i) = fcut(i,\u22121) \u2212 fcut(i,+1), where fcut is the minimum total weight of frustrated edges consistent with the labels seen so far. The argument contained in Section 5 for the positive edge weights (see, e.g., Eq. (1) therein) allows us to show that also this version of \u2206 can be computed\nefficiently. The prediction rule has to be re-defined as well: We count the parity of the number z of negativeweighted edges along the path connecting it to the closest node j \u2208 C ( H(it) ) , i.e., y\u0302it = (\u22121)zsgn ( \u2206(j) ) .\nRemark 3 In [5] the authors note that TREEOPT approximates a version space (Halving) algorithm on the set of tree labelings. Interestingly, SHAZOO is also an approximation to a more general Halving algorithm for weighted trees. This generalized Halving gives a weight to each labeling consistent with the labels seen so far and with the sign of \u2206(f) for each fork f . These weighted labelings, which depend on the weights of the \u03c6-edges generated by each labeling, are used for computing the predictions. One can show (details omitted due to space limitations) that this generalized Halving algorithm has a mistake bound within a constant factor of SHAZOO\u2019s."}, {"heading": "5 Mistake bound analysis and implementation", "text": "We now show that SHAZOO is nearly optimal on every weighted tree T . We obtain an upper bound in terms of \u03a6W and the structure of T , nearly matching the lower bound of Theorem 1. We now give some auxiliary notation that is strictly needed for stating the mistake bound.\nGiven a labeled tree (T,y), a cluster is any maximal subtree whose nodes have the same label. An in-cluster line graph is any line graph that is entirely contained in a single cluster. Finally, given a line graph L, we set RWL = \u2211 (i,j)\u2208L 1 Wi,j , i.e., the (resistance) distance between its terminal nodes.\nTheorem 2 For any labeled and weighted tree (T,y), there exists a set LT of O ( \u03be(\u03a6W ) ) edge-disjoint in-cluster line graphs such that the number of mistakes made by SHAZOO is at most of the order of\u2211 L\u2208LT min { |L|, 1 + \u230a log ( 1 + \u03a6WRWL )\u230b} .\nThe above mistake bound depends on the tree structure through LT . The sum contains O ( \u03be(\u03a6W ) ) terms, each one being at most logarithmic in the scale-free products \u03a6WRWL . The bound is governed by the same key quantity \u03be ( \u03a6W ) occurring in the lower bound of Theorem 1. However, Theorem 2 also shows that SHAZOO can take advantage of trees that cannot be covered by long line graphs. For example, if the input tree T is a weighted line graph, then it is likely to contain long in-cluster lines. Hence, the factor multiplying \u03be ( \u03a6W ) may be of the order of log ( 1 + \u03a6WRWL ) . If, instead, T has constant diameter (e.g., a star graph), then the in-cluster lines can only contain a constant number of nodes, and the number of mistakes can never exceedO ( \u03be(\u03a6W ) ) . This is a log factor improvement over WTA which, by its very nature, cannot exploit the structure of the tree it operates on.1\nAs for the implementation, we start by describing a method for calculating cut(v, y) for any unlabeled node v and label value y. Let T v be the maximal subtree of T rooted at v, such that no internal node is revealed. For any node i of T v, let T vi be the subtree of T\nv rooted at i. Let \u03a6vi (y) be the minimum weighted cutsize of T vi consistent with the revealed nodes and such that yi = y. Since \u2206(v) = cut(v,\u22121) \u2212 cut(v,+1) = \u03a6vv(\u22121) \u2212 \u03a6vv(+1), our goal is to compute \u03a6vv(y). It is easy to see by induction that the quantity \u03a6vi (y) can be recursively defined as follows, where C v i is the set of all children of i in T v, and\n1 One might wonder whether an arbitrarily large gap between upper (Theorem 2) and lower (Theorem 1) bounds exists due to the extra factors depending on \u03a6WRWL . One way to get around this is to follow the analysis of WTA in [6]. Specifically, we can adapt here the more general analysis from that paper (see Lemma 2 therein) that allows us to drop, for any integer K, the resistance contribution of K arbitrary non-\u03c6 edges of the line graphs in LT (thereby reducing RWL for any L containing any of these edges) at the cost of increasing the mistake bound by K. The details will be given in the full version of this paper.\nYj \u2261 {yj} if yj is revealed, and Yj \u2261 {\u22121,+1}, otherwise:2\n\u03a6vi (y) =  \u2211 j\u2208Cvi min y\u2032\u2208Yj ( \u03a6vj (y \u2032) + I { y\u2032 6= y } wi,j ) if i is an internal node of T v\n0 otherwise. (1)\nNow, \u03a6vv(y) can be computed through a simple depth-first visit of T v. In all backtracking steps of this visit the algorithm uses (1) to compute \u03a6vi (y) for each node i, the values \u03a6 v j (y) for all children j of i being calculated during the previous backtracking steps. The total running time is therefore linear in the number of nodes of T v.\nNext, we describe the basic implementation of SHAZOO for the on-line setting. A batch learning implementation will be given at the end of this section. The online implementation is made up of three steps.\n1. Find the hinge nodes of subtree T it . Recall that a hinge-node is either a fork or a revealed node. Observe that a fork is incident to at least three nodes lying on different hinge lines. Hence, in this step we perform a depth-first visit of T it , marking each node lying on a hinge line. In order to accomplish this task, it suffices to single out all forks marking each labeled node and, recursively, each parent of a marked node of T it . At the end of this process we are able to single out the forks by counting the number of edges (i, j) of each marked node i such that j has been marked, too. The remaining hinge nodes are the leaves of T it whose labels have currently been revealed. 2. Compute sgn(\u2206(i)) for all connection forks of H(it). From the previous step we can easily find the connection node(s) of H(it). Then, we simply exploit the above-described technique for computing the cut function, obtaining sgn(\u2206(i)) for all connection forks i of H(it).\n3. Propagate the labels of the nodes of C(H(it)) (only if it is not a fork). We perform a visit ofH(it) starting from every node r \u2208 C(H(it)). During these visits, we mark each node j of H(it) with the label of r computed in the previous step, together with the length of \u03c0(r, j), which is what we need for predicting any label of H(it) at the current time step.\nThe overall running time is dominated by the first step and the calculation of \u2206(i). Hence the worst case running time is proportional to \u2211 t\u2264|V | |V (T it)|. This quantity can be quadratic in |V |, though this is rarely encountered in practice if the node presentation order is not adversarial. For example, it is easy to show that in a line graph, if the node presentation order is random, then the total time is of the order of |V | log |V |. For a star graph the total time complexity is always linear in |V |, even on adversarial orders.\nIn many real-world scenarios, one is interested in the more standard problem of predicting the labels of a given subset of test nodes based on the available labels of another subset of training nodes. Building on the above on-line implementation, we now derive an implementation of SHAZOO for this train/test (or \u201cbatch learning\u201d) setting. We first show that computing |\u03a6ii(+1)| and |\u03a6ii(\u22121)| for all unlabeled nodes i in T takes O(|V |) time. This allows us to compute sgn(\u2206(v)) for all forks v in O(|V |) time, and then use the first and the third steps of the on-line implementation. Overall, we show that predicting all labels in the test set takes O(|V |) time.\nConsider tree T i as rooted at i. Given any unlabeled node i, we perform a visit of T i starting at i. During the backtracking steps of this visit we use (1) to calculate \u03a6ij(y) for each node j in T\ni and label y \u2208 {\u22121,+1}. Observe now that for any pair i, j of adjacent unlabeled nodes and any label y \u2208 {\u22121,+1}, once we have obtained \u03a6ii(y), \u03a6 i j(+1) and \u03a6 i j(\u22121), we can compute \u03a6 j i (y) in constant time, as \u03a6 j i (y) =\n\u03a6ii(y)\u2212miny\u2032\u2208{\u22121,+1} ( \u03a6ij(y \u2032) + I {y\u2032 6= y}wi,j ) . In fact, all children of j in T i are descendants of i, while the children of i in T i (but j) are descendants of j in T j . SHAZOO computes \u03a6ii(y), we can compute in constant time \u03a6ji (y) for all child nodes j of i in T\ni, and use this value for computing \u03a6jj(y). Generalizing this argument, it is easy to see that in the next phase we can compute \u03a6kk(y) in constant time for all nodes\n2 The recursive computations contained in this section are reminiscent of the sum-product algorithm [12].\nk of T i such that for all ancestors u of k and all y \u2208 {\u22121,+1}, the values of \u03a6uu(y) have previously been computed.\nThe time for computing \u03a6ss(y) for all nodes s of T i and any label y is therefore linear in the time of performing a breadth-first (or depth-first) visit of T i, i.e., linear in the number of nodes of T i. Since each labeled node with degree d is part of at most d trees T i for some i, we have that the total number of nodes of all distinct (edge-disjoint) trees T i across i \u2208 V is linear in |V |.\nFinally, we need to propagate the connection node labels of each hinge tree as in the third step of the online implementation. Since also this last step takes linear time, we conclude that the total time for predicting all labels is linear in |V |."}, {"heading": "6 Experiments", "text": "We tested our algorithm on a number of real-world weighted graphs from different domains (character recognition, text categorization, bioinformatics, Web spam detection) against the following baselines:\nOnline Majority Vote (OMV). This is an intuitive and fast algorithm for sequentially predicting the node labels is via a weighted majority vote over the labels of the adjacent nodes seen so far. Namely, OMV predicts yit through the sign of \u2211 s yiswis,it , where s ranges over s < t such that (is, it) \u2208 E. Both the total time and space required by OMV are \u0398(|E|). Label Propagation (LABPROP). LABPROP [19, 2, 3] is a batch transductive learning method computed by solving a system of linear equations which requires total time of the order of |E| \u00d7 |V |. This relatively high computational cost should be taken into account when comparing LABPROP to faster online algorithms. Recall that OMV can be viewed as a fast \u201conline approximation\u201d to LABPROP.\nWeighted Tree Algorithm (WTA). As explained in the introductory section, WTA can be viewed as a special case of SHAZOO. When the input graph is not a line, WTA turns it into a line by first extracting a spanning tree of the graph, and then linearizing it. The implementation described in [6] runs in constant amortized time per prediction whenever the spanning tree sampler runs in time \u0398(|V |).\nThe Graph Perceptron algorithm [11] is another readily available baseline. This algorithm has been excluded from our comparison because it does not seem to be very competitive in terms of performance (see, e.g., [6]), and is also computationally expensive.\nIn our experiments, we combined SHAZOO and WTA with spanning trees generated in different ways (note that OMV and LABPROP do not need to extract spanning trees from the input graph).\nRandom Spanning Tree (RST). Following Ch. 4 of [13], we draw a weighted spanning tree with probability proportional to the product of its edge weights. We also tested our algorithms combined with random spanning trees generated uniformly at random ignoring the edge weights (i.e., the weights were only used to compute predictions on the randomly generated tree) \u2014we call these spanning trees NWRST (no-weight RST). On most graphs, this procedure can be run in time linear in the number of nodes [18]. Hence, the combinations SHAZOO+NWRST and WTA+NWRST run in O(|V |) time on most graphs.\nMinimum Spanning Tree (MST). This is the spanning tree minimizing the sum of the resistors on its edges. This tree best approximates the original graph in terms of the trace norm distance of the corresponding Laplacian matrices.\nFollowing [11, 6], we also ran SHAZOO and WTA using committees of spanning trees, and then aggregating predictions via a majority vote. The resulting algorithms are denoted by k*SHAZOO and k*WTA, where k is the number of spanning trees in the aggregation. We used either k = 7, 11 or k = 3, 7, depending on the dataset size.\nFor our experiments, we used five datasets: RCV1, USPS, KROGAN, COMBINED, and WEBSPAM. WEBSPAM is a big dataset (110,900 nodes and 1,836,136 edges) of inter-host links created for the Web\nSpam Challenge 2008 [16].3 KROGAN (2,169 nodes and 6,102 edges) and COMBINED (2,871 nodes and 6,407 edges) are high-throughput protein-protein interaction networks of budding yeast taken from [15] \u2014see [6] for a more complete description. Finally, USPS and RCV1 are graphs obtained from the USPS handwritten characters dataset (all ten categories) and the first 10,000 documents in chronological order of Reuters Corpus Vol. 1 (the four most frequent categories), respectively. In both cases, we used Euclidean 10- Nearest Neighbor to create edges, each weightwi,j being equal to e\u2212\u2016xi\u2212xj\u2016 2/\u03c32i,j . We set \u03c32i,j = 1 2 ( \u03c32i +\u03c3 2 j ) , where \u03c32i is the average squared distance between i and its 10 nearest neighbours. Following previous experimental settings [6], we associate binary classification tasks with the five datasets/graphs via a standard one-vs-all reduction. Each error rate is obtained by averaging over ten randomly chosen training sets (and ten different trees in the case of RST and NWRST). WEBSPAM is natively a binary classification problem, and we used the same train/test split provided with the dataset: 3,897 training nodes and 1,993 test nodes (the remaining nodes being unlabeled).\nIn the below table, we show the macro-averaged classification error rates (percentages) achieved by the various algorithms on the first four datasets mentioned in the main text. For each dataset we trained ten times over a random subset of 5%, 10% and 25% of the total number of nodes and tested on the remaining ones. In boldface are the lowest error rates on each column, excluding LABPROP which is used as a \u201cyardstick\u201d comparison. Standard deviations averaged over the binary problems are small: most of the times less than 0.5%.\nDatasets USPS RCV1 KROGAN COMBINED Predictors 5% 10% 25% 5% 10% 25% 5% 10% 25% 5% 10% 25% SHAZOO+RST 3.62 2.82 2.02 21.72 18.70 15.68 18.11 17.68 17.10 17.77 17.24 17.34 SHAZOO+NWRST 3.88 3.03 2.18 21.97 19.21 15.95 18.11 18.14 17.32 17.22 17.21 17.53 SHAZOO+MST 1.07 0.96 0.80 17.71 14.87 11.73 17.46 16.92 16.30 16.79 16.64 17.15 WTA+RST 5.34 4.23 3.02 25.53 22.66 19.05 21.82 21.05 20.08 21.76 21.38 20.26 WTA+NWRST 5.74 4.45 3.26 25.50 22.70 19.24 21.90 21.28 20.18 21.58 21.42 20.64 WTA+MST 1.81 1.60 1.21 21.07 17.94 13.92 21.41 20.63 19.61 21.74 21.20 20.32 7*SHAZOO+RST 1.68 1.28 0.97 16.33 13.52 11.07 15.54 15.58 15.46 15.12 15.24 15.84 7*SHAZOO+NWRST 1.89 1.38 1.06 16.49 13.98 11.37 15.61 15.62 15.50 15.02 15.12 15.80 7*WTA+RST 2.10 1.56 1.14 17.44 14.74 12.15 16.75 16.64 15.88 16.42 16.09 15.72 7*WTA+NWRST 2.33 1.73 1.24 17.69 15.18 12.53 16.71 16.60 16.00 16.24 16.13 15.79 11*SHAZOO+RST 1.52 1.17 0.89 15.82 13.04 10.59 15.36 15.40 15.29 14.91 15.06 15.61 11*SHAZOO+NWRST 1.70 1.27 0.98 15.95 13.42 10.93 15.40 15.33 15.32 14.87 14.99 15.67 11*WTA+RST 1.84 1.36 1.01 16.40 13.95 11.42 16.20 16.15 15.53 15.90 15.58 15.30 11*WTA+NWRST 2.04 1.51 1.12 16.70 14.28 11.68 16.22 16.05 15.50 15.74 15.57 15.33 OMV 24.79 12.34 2.10 31.65 22.35 11.79 43.13 38.75 29.84 44.72 40.86 33.24 LABPROP 1.95 1.11 0.82 16.28 12.99 10.00 15.56 14.98 15.23 14.79 14.93 15.18\nNext, we extract from the above table a specific comparison among SHAZOO, WTA, and LABPROP. SHAZOO and WTA use a single minimum spanning tree (the best performing tree type for both algorithms). Note that SHAZOO consistently outperforms WTA.\nWe then report the results on WEBSPAM. SHAZOO and WTA use only non-weighted random spanning trees (NWRST) to optimize scalability. Since this dataset is extremely unbalanced (5.4% positive labels) we use the average test set F-measure instead of the error rate."}, {"heading": "SHAZOO WTA OMV LABPROP 3*WTA 3*SHAZOO 7*WTA 7*SHAZOO", "text": "0.954 0.947 0.706 0.931 0.967 0.964 0.968 0.968\n3 We do not compare our results to those obtained within the challenge since we are only exploiting the graph (weighted) topology here, disregarding content features.\nOur empirical results can be briefly summarized as follows: 1. Without using committees, SHAZOO outperforms WTA on all datasets, irrespective to the type of spanning tree being used. With committees, SHAZOO works better than WTA almost always, although the gap between the two reduces.\n2. The predictive performance of SHAZOO+MST is comparable to, and sometimes better than, that of LABPROP, though the latter algorithm is slower.\n3. k*SHAZOO, with k = 11 (or k = 7 on WEBSPAM) seems to be especially effective, outperforming LABPROP, with a small (e.g., 5%) training set size.\n4. NWRST does not offer the same theoretical guarantees as RST, but it is extremely fast to generate (linear in |V | on most graphs \u2014 e.g., [1]), and in our experiments is only slightly inferior to RST."}, {"heading": "Proof of Theorem 1", "text": "Pick any E\u2032 \u2286 E such that \u03be(M) = |E\u2032|. Let F be the forest obtained by removing from T all edges in E\u2032. Draw an independent random label for each of the |E\u2032|+ 1 components of F and assign it to all nodes of that component. Then any online algorithm makes in expectation at least half mistake per component, which implies that the overall number of online mistakes is (|E\u2032|+ 1)/2 > \u03be(M)/2 in expectation. On the other hand, \u03a6W \u2264M clearly holds by construction."}, {"heading": "Proof of Theorem 2", "text": "We first give additional definitions used in the analysis, then we present the main ideas, and finally we provide full details.\nRecall that, given a labeled tree (T,y), a cluster is any maximal subtree whose nodes have the same label. Let C be the set of all clusters of T . For any cluster C \u2208 C, let MC be the subset of all nodes of C on which SHAZOO makes a mistake. Let C be the subtree of T obtained by adding to C all nodes that are adjacent to a node of C. Note that all edges connecting a node of C \\ C to a node of C are \u03c6-edges. Let E\u03c6 C be the set of \u03c6-edges in C and let \u03a6C = \u2223\u2223E\u03c6 C \u2223\u2223. Let \u03a6W C be the total weight of the edges in E\u03c6 C\n. Finally, recall the notation RWL = \u2211 (i,j)\u2208L 1 Wi,j\n, where L is any line graph. Recall that an in-cluster line graph is any line graph that is entirely contained in a single cluster. The main idea used in the proof below is to bound |MC | for each C \u2208 C in the following way. We partition MC into O(|E\u2032\nC |) groups, where E\u2032 C \u2286 EC . Then we find a set LC of edge-disjoint in-cluster line graphs, and\ncreate a bijection between lines in LC and groups in MC . We prove that the cardinality of each group is at most mL = min { |L|, 1 + \u230a ln ( 1 + \u03a6WRWL )\u230b} , where L \u2208 LC is the associated line. This shows that the subset MT of nodes in T which are mispredicted by SHAZOO satisfies\n|MT | = \u2211 C\u2208C |MC | \u2264 \u2211 C\u2208C \u2211 L\u2208LC mL = \u2211 L\u2208LT mL\nwhere LT = \u22c3 C\u2208C LC . Then we show that\u2211\nC\u2208C \u2211 (i,j)\u2208E\u2032\nC\nwi,j = O ( \u03a6W ) .\nBy the very definition of \u03be, and using the bijection stated above, this implies\n|LT | = \u2211 C\u2208C |LC | = O (\u2211 C\u2208C |E\u2032 C | ) = O ( \u03be(\u03a6W ) ) ,\nthereby resulting in the mistake bound contained in Theorem 2. The details of the proof require further notation. According to SHAZOO prediction rule, when it is not a fork and C(H(it)) 6\u2261 \u2205, the algorithm predicts\nyit using the label of any j \u2208 C ( H(it) ) closest to it. In this case, we call j an r-node (reference node) for it and the pair {j, (j, v)}, where (j, v) is the edge on the path between j and it, an rn-direction (reference node direction). We use the shorthand notation i\u2217 to denote an r-node for i. In the special case when all connection nodes i of the hinge tree containing it have \u2206(i) = 0 (i.e., C(H(it)) \u2261 \u2205), and it is not a fork, we call any closest connection node j0 to it an r-node for it and we say that {j0, (j0, v)} is a rn-direction for it. Clearly, we may have more than one node of MC associated with the same rn-direction. Given any rn-direction {j, (j, v)}, we call r-line (reference line) the line graph whose terminal nodes are j and the first (in chronological order) node j0 \u2208 V for which {j, (j, v)} is a rn-direction, where (j, v) lies on the path between j0 and j.4 We denote such an r-line by L(j, v).\nIn the special case where j \u2208 C and j0 /\u2208 C we say that the r-line is associated with the \u03c6-edge of E\u03c6C included in the line-graph. In this case we denote such an r-line by L(u, q), where (u, q) \u2208 E\u03c6\nC . Figure 2\ngives a pictorial example of the above concepts.\nWe now cover MC (the subset of all nodes of C \u2208 C on which SHAZOO makes a mistake) by the following subsets:\n\u2022 MFC is the set of all forks in MC . 4 We may also have v \u2261 j0.\n\u2022 M inC is the subset of MC containing the nodes i whose reference node i\u2217 belongs to C (if i is a fork, then i\u2217 = i). Note that this set may have a nonempty intersection with the previous one.\n\u2022 MoutC is the subset of MC containing the nodes i such that i\u2217 does not belong to C.\nTwo other structures that are relevant to the proof:\n\u2022 CF is the subset of all forks f \u2208 VC such that \u2206(f) \u2264 0 at some step t. Since we assume the cluster label is +1 (see below), and since a fork it \u2208 VC is mistaken only if \u2206(it) \u2264 0, we have MFC \u2286 CF .\n\u2022 CF \u2032 is the subset of all nodes in MC that, when revealed, create a fork that belongs to CF . Since at each time step at most one new fork can be created,5 we have |CF \u2032 | \u2264 |CF |.\nThe proof of the theorem relies on the following sequence of lemmas that show how to bound the number of mistakes made on a given cluster C = (VC , EC). A major source of technical difficulties, that makes this analysis different and more complex than those of TREEOPT and WTA, is that on a weighted tree the value of \u2206(i) on forks i can potentially change after each prediction.\nWithout loss of generality, from now on we assume all nodes in C are labeled +1. Keeping this assumption in mind is crucial to understand the arguments that follow.\nFor any node i \u2208 VC , let \u2206(i) be the value of \u2206(i) when all nodes in C \\ C are revealed.\nLemma 3 For any fork f of C and any step t = 1, . . . , n, we have \u2206(f) \u2264 \u2206(f).\nProof. For the sake of contradiction, assume \u2206(f) > \u2206(f). Let T f be the maximal subtree of T rooted at f such that no internal node of T f is revealed. Now, consider the cut given by the edges of E\u03c6C belonging to the hinge lines of T f . This cut separates f from any revealed node labeled with \u22121. The size of this cut cannot be larger than \u03a6W\nC . By definition of \u2206(\u00b7), this implies \u2206(f) \u2264 \u03a6W C . However, also \u2206(f) cannot be\nlarger than \u03a6W C . Because\n\u2206(it) \u2264 \u2211\n(i,j)\u2208E\u03c6 C\nWi,j = \u03a6 W C\nmust hold independent of the set of nodes in VC that are revealed before time t, this entails a contradiction.\nLet now \u03beC be the restriction of \u03be on the subtree C, and let DC be the set of all distinct rn-directions which the nodes of M inC can be associated with. The next lemmas are aimed at bounding |CF | and |DC |. We first need to introduce the superset D\u2032C of DC . Then, we show that for any C both |D\u2032C | and |CF | are linear in \u03beC(\u03a6 W C\n). In order to do so, we need to take into account the fact that the sign of \u2206 for the forks in the cluster can change many times during the prediction process. This can be done via Lemma 3, which shows that when all labels in C \\C are revealed then, for all fork f \u2208 C, the value \u2206(f) does not increase. Thus, we get the largest set DC when we assume that the nodes in C \\ C are revealed before the nodes of C.\nGiven any cluster C, let \u03c3C be the order in which the nodes of C are revealed. Let also \u03c3 \u2032 C be the permutation in which all nodes in C are revealed in the same order as \u03c3C , and all nodes in C \\ C are revealed at the beginning, in any order. Now, given any node revelation order \u03c3C , D \u2032 C can be defined by describing the three types of steps involved in its incremental construction supposing \u03c3\u2032 C\nwas the actual node revelation order.\n5 In step t a new fork j is created when the number of edge-disjoint paths connecting j to the labeled nodes increases. This event occurs only when a new hinge line \u03c0(it, f) is created. When this happens, the only node for which the number of edge-disjoint paths connecting it to labeled nodes gets increased is the terminal node j of the newly created hinge line.\n1. After the first |C \\C| = \u03a6C steps, D\u2032C contains all node-edge pairs {i, (i, j)} such that i is a fork and (i, j) is an edge laying on a hinge line of C. Recall that no node in C is revealed yet.\n2. For each step t > 0 when a new fork f is created such that \u2206(f) \u2264 0 just after the revelation of yit , we add to D\u2032C the three node-edge pairs {f, (f, j)}, where the (f, j) are the edges contained in the three hinge lines terminating at f .\n3. Let s be any step where: (i) A new hinge line \u03c0(is, i\u2217s) is created, (ii) node i \u2217 s is a fork, and (iii)\n\u2206(i\u2217s) \u2264 0 at time s\u2212 1. On each such step we add {i\u2217s, (i\u2217s, j)} to D\u2032C , for j in \u03c0(is, i\u2217s).\nIt is easy to verify that, given any ordering \u03c3C for the node revelation in C, we have DC \u2286 D\u2032C . In fact, given an rn-direction {i, (i, j)} \u2208 DC , if (i, j) lies along one of the hinge lines that are present at time 0 according to \u03c3\u2032\nC , then {i, (i, j)} must be included in D\u2032C during one of the steps of type 2 above, otherwise\n{i, (i, j)} will be included in D\u2032C during one of the steps of type 2 or type 3. As announced, the following lemmas show that |D\u2032C | and |CF | are both of the order of \u03beC(\u03a6WC ).\nLemma 4 (i) The total number of forks at time t = \u03a6C is O ( \u03be(\u03a6W C ) ) . (ii) The total number of elements\nadded to D\u2032C in the first step of its construction is O ( \u03be(\u03a6W C ) ) .\nProof. Assume nodes are revealed according to \u03c3\u2032 C . Let C \u2032 be the subtree of C made up of all nodes in C that are included in any path connecting two nodes of C \\ C. By their very definition, the forks at time t = \u03a6C are the nodes of VC\u2032 having degree larger than two in subtree C\n\u2032. Consider C \u2032 as rooted at an arbitrary node of C \\C. The number of the leaves of C \u2032 is equal to |C \\C| \u2212 1. This is in turn O ( \u03beC(\u03a6 W C ) because \u2211\n(i,j)\u2208E\u03c6 C\nwi,j = O ( \u03beC(\u03a6 W C ) ) .\nNow, in any tree, the sum of the degrees of nodes having degree larger than two cannot is at most linear in the number of leaves. Hence, at time t = \u03a6C both the number of forks in C and the cardinality of D \u2032 C are O ( \u03beC(\u03a6 W C ) ) .\nLet now \u0393Tt be the minimal cutsize of T consistent with the labels seen before step t+ 1, and notice that \u0393Tt is nondecreasing with t.\nLemma 5 Let t be a step when a new hinge line \u03c0(it, q) is created such that it, q \u2208 VC . If just after step t we have \u2206(q) \u2264 0, then \u0393Tt \u2212 \u0393Tt\u22121 \u2265 wu,v, where (u, v) is the lightest edge on \u03c0(it, q).\nProof. Since \u2206(q) \u2264 0 and \u03c0(it, q) is completely included in C, we must have \u2206(q) \u2264 0 just before the revelation of yit . This implies that the difference \u0393 T t \u2212 \u0393Tt\u22121 cannot be smaller than the minimum cutsize that would be created on \u03c0(it, q) by assigning label \u22121 to node q.\nLemma 6 Assume nodes are revealed according to \u03c3\u2032 C . Then the cardinality of CF and the total number of elements added to D\u2032C during the steps of type 2 above are both linear in \u03beC(\u03a6 W C ).\nProof. Let CF0 be the set of forks in VC such that \u2206(f) \u2264 0 at some time t \u2264 |V |. Recall that, by definition, for each fork f \u2208 CF there exists a step tf such that \u2206(f) \u2264 0. Hence, Lemma 3 implies that, at the same step tf , for each fork f \u2208 CF we have \u2206(f) \u2264 0. Since CF is included in CF0 , we can bound |CF | by |CF0 |, i.e., by the number of forks i \u2208 VC such that \u2206(i) \u2264 0, under the assumption that \u03c3\u2032C is the actual revelation order for the nodes in C.\nNow, |CF0 | is bounded by the number of forks created in the first |C \\ C| = \u03a6C steps, which is equal to O ( \u03be(\u03a6W C ) )\nplus the number of forks f created at some later step and such that \u2206(f) \u2264 0 right after their creation. Since nodes in C are revealed according to \u03c3\u2032\nC , the condition \u2206(f) > 0 just after the creation of\na fork f implies that we will never have \u2206(f) \u2264 0 in later stages. Hence this fork f belongs neither to CF0 nor to CF .\nIn order to conclude the proof, it suffices to bound from above the number of elements added to D\u2032C in the steps of type 2 above. From Lemma 5, we can see that for each fork f created at time t such that \u2206(f) \u2264 0 just after the revelation of node it, we must have |\u0393Tt \u2212\u0393Tt\u22121| \u2265 wu,v, where (u, v) is the lightest edge in \u03c0(it, f). Hence, we can injectively associate each element of CF with an edge of EC , in such a way that the sum of the weights of these edges is bounded by \u03a6W\nC . By definition of \u03be, we can therefore conclude that the total number of elements added to D\u2032C in the steps of type 2 is O ( \u03be(\u03a6W C ) ) .\nWith the following lemma we bound the number of nodes of M inC \\ CF \u2032\nassociated with every rndirection and show that one can perform a transformation of the r-lines so as to make them edge-disjoint. This transformation is crucial for finding the set LT appearing in the theorem statement. Observe that, by definition of r-line, we cannot have two r-lines such that each of them includes only one terminal node of the other. Thus, let now FC be the forest where each node is associated with an r-line and where the parent-child relationship expresses that (i) the parent r-line contains a terminal node of the child r-line, together with (ii) the parent r-line and the child r-line are not edge-disjoint. FC is, in fact, a forest of r-lines. We now use mL(j,v) for bounding the number of mistakes associated with a given rn-direction {i, (j, v)} or with a given \u03c6-edge (j, v). Given any connected component T \u2032 of FC , let finally mT \u2032 be the total number of nodes of M inC \\ CF \u2032 associated with the rn-directions {i, (i, j)} of all r-lines L(i, j) of T \u2032.\nLemma 7 Let C be any cluster. Then:\n(i) The number of nodes in M inC \\ CF \u2032\nassociated with a given rn-direction {j, (j, v)} is of the order of mL(i,j).\n(ii) The number of nodes in MoutC \\ CF \u2032 associated with a given \u03c6-edge (u, q) is of the order of mL(u,q).\n(iii) Let L(jr, vr) be the r-line associated with the root of any connected component T \u2032 of FC . mT \u2032 must be at most of the same order of \u2211\nL(j,v)\u2208L(L(jr,vr))\nmL(j,v) + |VT \u2032 |\nwhere L(L(jr, vr)) is a set of |VT \u2032 | edge-disjoint line graphs completely contained in L(jr, vr).\nProof. We will prove only (i) and (iii), (ii) being similar to (i). Let it be a node in M inC \\ CF \u2032\nassociated with a given rn-direction {j, (j, v)}. There are two possibilities: (a) it is in L(j, v) or (b) the revelation of yit creates a fork f in L(j, v) such that \u2206(f) > 0 for all steps s \u2265 t. Let now it\u2032 be the next node (in chronological order) ofM inC \\CF \u2032 associated with {j, (j, v)}. The length of \u03c0(it\u2032 , it) cannot be smaller than the length of \u03c0(it\u2032 , j) (under condition (a)) or smaller than the length of \u03c0(f, j) (under condition (b)). This clearly entails a dichotomic behaviour in the sequence of mistaken nodes in M inC \\ CF \u2032 associated with {j, (j, v)}. Let now p be the node in L(j, v) which is farthest from j such that the length of \u03c0(p, j) is not larger than \u03a6W . Once a node in \u03c0(p, j) is revealed or becomes a fork f satisfying \u2206(f) > 0 for all steps s \u2265 t, we have \u2206(j) > 0 for all subsequent steps (otherwise, this would contradict the fact that the total\ncutsize of T is \u03a6W ). Combined with the above sequential dichotomic behavior, this shows that the number of nodes of M inC \\ CF \u2032 associated with a given rn-direction {j, (j, v)} can be at most of the order of\nmin { |L(j, v)|, 1 + \u230a log2 ( RWL(j,v) + (\u03a6 W )\u22121\n(\u03a6W )\u22121\n)\u230b} = mL(j,v) .\nPart (iii) of the statement can be now proved in the following way. Suppose now that an r-line L(j, v), having j and j0 as terminal nodes, includes the terminal node j\u2032 of another r-line L(j\u2032, v\u2032), having j\u2032 and j\u20320 as terminal nodes. Assume also that the two r-lines are not edge-disjoint. If L(j\u2032, v\u2032) is partially included in L(j, v), i.e., if j\u20320 does not belong to L(j, v), then L(j\n\u2032, v\u2032) can be broken into two sub-lines: the first one has j\u2032 and k as terminal nodes, being k the node in L(j, v) which is farthest from j\u2032; the second one has k and j\u20320 as terminal nodes. It is easy to see that L(j, v) must be created before L(j\n\u2032, v\u2032) and j0 is the only node of the second sub-line that can be associated with the rn-direction {j\u2032, (j\u2032, v\u2032)}. This observation reduces the problem to considering that in T \u2032 each r-line that is not a root is completely included in its parent.\nGiven an r-line L(u, q) having u and z as terminals, we denote by m\u03c0(u,z) the quantity mL(u,q). Consider now the simplest case in which T \u2032 is formed by only two r-lines: the parent r-line L(jp, vp), which completely contains the child r-line L(jc, vc). Let s be the step in which the first node u of L(jp, vp) becomes a hinge node. After step s, L(jp, vp) can be vieved as broken in two edge-disjoint sublines having {jp, u} and {j0, u} as terminal node sets, where j0 is one of the terminal of L(jp, vp). Thus,\nmT \u2032 \u2264 max u\u2208VL(jp,vp) m\u03c0(jp,u) +m\u03c0(u,j0) + 1 .\nGeneralizing this argument for every component T \u2032 of FC , and using the above observation about the partially included r-lines, we can state that, for any component T \u2032 of FC , mT \u2032 is of the order of\nmax u1,...,uN\u2208VL(jp,vp)\n( m\u03c0(jp,u1) +m\u03c0(uN ,j0) + N\u22121\u2211 k=1 m\u03c0(uk,uk+1) + 2|VT \u2032 | )\nwhere N = |VT \u2032 | \u2212 1. This entails that we can define L(L(jr, vr)) as the union of {\u03c0(jp, u1), \u03c0(uN , j0)} and \u22c3N\u22121 k=1 \u03c0(uk, uk+1), which concludes the proof.\nLemma 8 The total number of elements added toD\u2032C during steps of type 3 above is of the order of \u03beC(\u03a6 W C ). Proof. Assume nodes are revealed according to \u03c3\u2032 C\n, and let s be any type-3 step when a new element is added to D\u2032C . There are two cases: (a) \u2206(i \u2217 s) \u2264 0 at time s or (b) \u2206(i\u2217s) > 0 at time s.\nCase (a). Lemma 5 combined with the fact that all hinge-lines created are edge-disjoint, ensures that we can injectively associate each of these added elements with an edge of EC in such a way that the total weight of these edges is bounded by \u03a6W\nC . This in turn implies that the total number of elements added to EC is O ( \u03beC(\u03a6 W C ) ) .\nCase (b). Since we assumed that nodes are revealed according to \u03c3\u2032 C , we have that \u2206(i\u2217s) is positive for all steps t > s. Hence we have that case (b) can occur only once for each of such forks i\u2217s. Since this kind of fork belongs to CF , we can use Lemma 6 and conclude that (b) can occur at most |CF | = O ( \u03beC(\u03a6 W C ) ) times.\nLemma 9 With the notation introduced so far, we have |DC | = O ( \u03beC(\u03a6 W C ) ) .\nProof. Combining Lemma 4, Lemma 6, and Lemma 8 we immediately have D\u2032C = O ( \u03beC(\u03a6 W C ) ) . The claim then follows from DC \u2286 D\u2032C .\nWe are now ready to prove the theorem. Proof of Theorem 2. Let FT be the union of FC over C \u2208 C. Using Lemma 9 we deduce |VFC | = \u03a6C + O ( \u03beC(\u03a6 W C ) ) = O ( \u03beC(\u03a6 W C ) ) , where the term \u03a6C takes into account that at most one r-line of FC may be associated with each \u03c6-edge of C. By definition of \u03be(\u00b7), this implies |VFT | = O ( \u03be(\u03a6W ) ) . Using part (i) and (ii) of Lemma 7 we have |MT | \u2264 |MFC |+ |M inC |+ |MoutC | \u2264 |CF |+ |CF \u2032 |+ \u2211 L\u2208VFT mL \u2264 \u2211 L\u2208VFT mL +O ( \u03be(\u03a6W ) ) .\nLet now T (FT ) be the set of components of FT . Given any tree T \u2032 \u2208 T (FT ), let r(T \u2032) be the r-line root of T \u2032. Recall that, by part (iii) of Lemma 7 for any tree T \u2032 \u2208 T (FT ) we can find a set L(r(T \u2032)) of |VT \u2032 | edge-disjoint line graphs all included in r(T \u2032) such that mT \u2032 is of the order of \u2211 L\u2208LT \u2032 (r(T \u2032))\nmL + |VT \u2032 |. Let now L\u2032T be equal to \u222aT \u2032\u2208T (FT )L(r(T \u2032)). Thus we have\n|MT | = O \u2211 L\u2208L\u2032T mL + |VFT |+ \u03be(\u03a6 W )  = O \u2211 L\u2208L\u2032T mL + \u03be(\u03a6 W )  . Observe thatL\u2032T is not an edge disjoint set of line graphs included in T only because each \u03c6-edge may belong to two different lines of L\u2032T . By definition of mL, for any line graphs L and L\u2032, where L\u2032 is obtained from L by removing one of the two terminal nodes and the edge incident to it, we have mL\u2032 = mL +O(1). If, for each \u03c6-edge shared by two line graphs of L\u2032T , we shorten the two line graphs so as no one of them includes the \u03c6-edge, we obtain a new set of edge-disjoint line graphs LT such that \u2211 L\u2208L\u2032T mL = \u2211 L\u2032\u2208LT +\u03be(\u03a6 W ).\nHence, we finally obtain |MT | = O (\u2211 L\u2032\u2208LT mL\u2032 + \u03be(\u03a6 W ) ) = O (\u2211 L\u2032\u2208LT mL\u2032 ) , where in the last equality we used the fact that mL\u2032 \u2265 1 for all line graphs L\u2032."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Predicting the nodes of a given graph is a fascinating theoretical problem with applications in several<lb>domains. Since graph sparsification via spanning trees retains enough information while making the task<lb>much easier, trees are an important special case of this problem. Although it is known how to predict the<lb>nodes of an unweighted tree in a nearly optimal way, in the weighted case a fully satisfactory algorithm<lb>is not available yet. We fill this hole and introduce an efficient node predictor, SHAZOO, which is nearly<lb>optimal on any weighted tree. Moreover, we show that SHAZOO can be viewed as a common nontrivial<lb>generalization of both previous approaches for unweighted trees and weighted lines. Experiments on<lb>real-world datasets confirm that SHAZOO performs well in that it fully exploits the structure of the input<lb>tree, and gets very close to (and sometimes better than) less scalable energy minimization methods.", "creator": "LaTeX with hyperref package"}}}