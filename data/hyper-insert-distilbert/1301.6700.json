{"id": "1301.6700", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "A New Model of Plan Recognition", "abstract": "we present a straightforward new abductive, probabilistic theory of supporting plan recognition. initially this model differs principally from classic previous plan recognition theories conventional in being centered solely around a model of terminating plan after execution : most previous methods have similarly been adopted based on explaining plans as formal objects or loosely on explicit rules describing the recognition process. we unfortunately show that recently our new model primarily accounts for operational phenomena easily omitted from most previous plan recognition theory theories : notably the conditional cumulative effect interpretation of a sequence of simulated observations showing of discrete partially - normally ordered, interleaved plans and the linear effect of context on managing plan adoption., the model community also supports inferences arising about affecting the evolution of the plan execution in some situations applicable where another agent intervenes in plan execution. this reference facility provides support for organisations using plan recognition to build signaling systems that theoretically will intelligently even assist identify a new user.", "histories": [["v1", "Wed, 23 Jan 2013 15:58:22 GMT  (365kb)", "http://arxiv.org/abs/1301.6700v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["robert p goldman", "christopher w geib", "christopher a miller"], "accepted": false, "id": "1301.6700"}, "pdf": {"name": "1301.6700.pdf", "metadata": {"source": "CRF", "title": "A New Model of Plan Recognition", "authors": ["Christopher A. Miller"], "emails": ["}@htc.honeywell.com"], "sections": [{"heading": null, "text": "We present a new abductive, probabilistic theory of plan recognition. This model dif fers from previous theories in being centered around a model of plan execution: most previous methods have been based on plans as formal objects or on rules describing the recognition process. We show that our new model accounts for phenomena omitted from most previous plan recognition theories: no tably the cumulative effect of a sequence of observations of partially-ordered, interleaved plans and the effect of context on plan adop tion. The model also supports inferences about the evolution of plan execution in situ ations where another agent intervenes in plan execution. This facility provides support for using plan recognition to build systems that will intelligently assist a user.\n1 Introduction\nIn this paper, we present a new theory of plan recogni tion. Our theory, which is abductive and probabilistic, differs from previous theories in treating plan execu tion as primary. This new perspective clarifies a num ber of difficult issues in plan recognition, permitting solutions to a wider class of plan recognition prob lems and allows plan recognition work to proceed on a firmer theoretical footing.\nIn 1986, Kautz and Allen (K&A) published an article, \"Generalized Plan Recognition,\" (Kautz & Allen 1986) that has framed the discussion of plan recognition ever since. K&A defined the problem of plan recognition as the problem of identifying a minimal set of top level actions sufficient to explain the set of observed actions. Plans were represented in a plan graph, with top-level actions as root nodes and other actions as nodes depending from the top-level actions. To a first\napproximation, the problem of plan recogmtwn was then a problem of graph covering. K&A formalized this view of plan recognition in terms of McCarthy's circumscription.\nUnlike K&A's, our abductive, prob\ufffdbilistic theory is centered around plan execution, instead of around plan graphs as formal objects. Our new framework clari fies a number of issues that were obscured by previ ous approaches. In particular, our approach handles partially-ordered plans; multiple, interleaved plans; the effect of context on plan choice and is able to cor rectly treat a system that is both recognizing actions of other agents, and acting on its own right.\nLike K&A, we offer only a formal model of the plan recognition problem. We do not pretend to provide a solution to either the algorithmic or implementa tion problems, only an understanding at the \"knowl edge level.\" We have encoded our plan recognition model in a language that has a computational inter pretation, and we have a proof-of-concept interpreter that has been used to test all examples given in this paper. However, we do not claim that this interpreter provides an efficient solution to the plan recognition problem. There are many ways our model could be implemented; we mention several possibilities in this paper. For that matter, we do not believe that there is a single best algorithm, appropriate for all plan recog nition domains.\nOur work on plan recognition is motivated by an in terest in procedure-based crisis management, specifi cally in industrial control. We are working on systems that will assist human agents executing crisis manage ment procedures. As a result, our model of plan exe cution is hierarchical and procedural, as distinguished from models in which there are only primitive actions, or models that assume agents are deliberative (e.g., game-theoretic models). This application requires us to account for phenomena omitted from most previous plan recognition theories: notably the cumulative ef fect of a sequence of observations of partially-ordered,\n246 Goldman, Geib, and Miller\ninterleaved plans; the effect of context on plan adop tion and the effect of interventions in the process of plan execution.\n2 Plans\nIn this paper, we use simple hierarchical (task decom position) plans, as most plan recognition work does. We assume that agents have a plan library that pro vides recipes for achieving goals. We give a sample plan library for a hypothetical space station example in Figure 1.\nWhen an agent wishes to achieve a goal like increasing available power (increase-power), the plan library provides a set of alternate methods that the agent can use. increase-power can be achieved either by by generating more power (gen-power) or by reduc ing power consumption (lower-power-use). In order to lower-power-use, the agent must do three steps: open access panel 2 (open-p2), shutoff experiment sub-system Xl (shutoff-XI), and shutoff experiment sub-system X2 (shutoff-X2). Note that the plan li brary may be viewed as an AND/OR tree, with goals as OR nodes and methods as AND nodes.\nOur space station plan library contains plans for three goals: increasing available power (increase-power), increasing the oxygen content of the air (raise-02level) and raising the temperature (raise-temp). The goals increase-power and raise-02-level each have two alternate methods: generate more power (gen-power) and reduce power consumption (lower power-use); and generate more 02 (gen-02) and reduce 02 consumption (lower-Or use), respectively. Since there is only one way of raising the temperature, there is no need for an actual method node for it.\nEach of the method nodes has a set of children con nected by \"and\" arcs representing the actions that are the steps of the method. In our simple plan library, the methods are all composed of primitive actions: open access panel 1 ( open-p 1), turn on generator B (start-gen-B), open-p2, shutoff-XI, shutoff-X2, start 02 generator (start-02-gen), open access panel 3 (open-p3), seal-off science module (seal-sci), check temperature (check-temp), and raise thermostat set point (raise-temp-set). In general, however, meth ods may introduce sub-goals. For example, in an al ternate library for this domain, gen-02 might have increase-power as a sub-goal in place of the two ac tions open-pi and start-gen-B.\nIn many cases, the steps of a method must be done in a particular order. Ordering constraints are represented by directed arcs. For example, open-p2 must precede shutoff-XI and shutoff-X2 for lower-power-use.\n7\ufffdwa M OR raise-o2-level \ufffd ge -o2\nNotice that the agent still has some freedom to choose the order of his/her actions.\nActions may be performed for more than one reason. For example, open-pl and start-gen-B are used in both gen-power and gen-02.\nFinally, notice that there are two different condi tions/ events that are tied to the goals. These dashed lines represent the fact that these events affect the like lihood that the agent will adopt the respective goal. For example if an EVA is to be prepared (EVA-prep), then the agent is more likely to adopt increase-power as a goal. Likewise, if the agent notices a drop in 02 level ( 02-drop), then he is likely to choose raise-02level as a goal.\n3 Plan Recognition\nPlan recognition is the process of inferring the goals of an agent from observations of an agent's actions. Cohen, Perrault and Allen (1981) distinguish between two kinds of plan recognition, keyhole and intended plan recognition. In keyhole recognition, the recog nizer is simply watching normal actions by the agent. In intended recognition, the agent is cooperative; its actions are done with the intent that they be under stood. Keyhole recognition is the problem that faces us in our applications, and this fact influences the structure of our model.\nTo the best of our knowledge, Charniak was the first to argue that plan recognition was best understood as a specific form of the general problem of abduction, or reasoning to the best explanation ( Charniak & Mc Dermott 1985).\nK&A's model of plan recognition (Kautz & Allen 1986) treated the problem as one of computing minimal ex planations, in the form of vertex covers based on the plan graph. For example, in their approach, if one\no2::_drop '\n'\nA New Model of Plan Recognition 247\nra\ufffdo2-level A\nA A se\nopen-pZ____;eal-sci check-temp raise-temp-set \"-----\" open-\ufffdrt-gen-B\nobserved open-pl and start-gen-B, the minimal ex planations would be\n(increase-power 1\\ gen-power) V (raise-02-level 1\\ gen-02)\nSee Figure 2. If, in addition, one observed check temp, then the system would have to postulate two top-level plans in order to explain (cover) all the ob servations, as shown in Figure 3.\nOne problem with this work is that it does not take into account differences in the a priori likelihood of different plans. Charniak and Goldman (C&G) (1993) argued that, since plan recognition involves abduction, it could best be done as Bayesian inference. Bayesian inference supports the preference for minimal explana tions, in the case of hypotheses that are equally likely, but also correctly handles explanations of the same complexity but different likelihoods.\nTwo plan recognition situations that are not handled by either K&A or C&G are the problems of influences from the state of the world and evidence from failure to observe. Clearly, the state of the world will influ ence an agent's decision to pursue plans. We see in our Figure l that the space station operator's decision to pursue increase-power will be affected substantially by whether or not an EVA is to be prepared (EVA prep). K&A could not take this into account, because they did not treat the relative likelihood of plans. Even for C&G, however, it is not simple to take this into ac count, because they defined their probability distribu tions over the plan tree. On the other hand, when we view plan recognition from the point of view of agent's pursuing plans, it becomes clear how to handle this.\nThe problem of evidence from failure to observe is a\nmore complex one. Consider what would happen if one observed open-pl and start-gen-B. Assuming that they were equally likely a priori, one would con clude that either increase-power (via gen-power) or raise-02-level (via gen-02) were equally good expla nations (see Figure 2). However, as time went by and one saw other actions, without seeing start-02-gen, the final action of gen-02, one would become more and more certain that increase-power/gen-power was the right explanation. Systems like those of C&G and K&A, are not capable of reasoning like this, be cause they do not consider plan recognition as a prob lem that evolves over time. They cannot represent the fact that an action has not been observed yet. They can only be silent about whether an action has oc curred - which just means that the system has failed to notice the action, not that the action hasn't oc curred - or assert that an action has not and will not occur. We will show that our approach handles evidence from failure to observe actions.\nVilain (1990) presented a theory of plan recognition as parsing, based on K&A's theory. Vilain does not actu ally propose parsing as a solution to the plan recogni tion problem. Instead, he uses the reduction of limited cases of plan recognition to parsing in order to investi gate the complexity of K&A's theory. The major prob lem with parsing as a model of plan recognition is that it does not treat partially-ordered plans or interleaved plans well. Indeed, even within a single method, par tial ordering (as in lower-power-use, where shutoff XI and shutoff-X2 can be done in any order, as long as open-p2 is done first), would cause an explosion in grammar s1ze.\nMore recently, Wellman and Pynadath (W&P) (1997)\n248 Goldman, Geib, and Miller\n7owcr\nM ANDA\nopen-pl start-gen-B check-temp\nOR A check-temp\nstart-gen-B\nFigure 3: A minimal explanation for a situation that requires two plans.\nhave proposed a plan recognition method that is both probabilistic and based on parsing. Unfortunately, this approach suffers from the same limitations on plan in terleaving as Vilain's. W&P propose that probabilistic context-sensitive grammars (PCSGs) might overcome this problem, but it is difficult to define a probability distribution for a PCSG (Pynadath & Wellman 1997). Difficulty in defining a probability distribution is also a problem for C&G 's approach. Overcoming this dif ficulty is one of the emphases of our own work.\nNone of the plan recognition systems that we know of properly handles actions taken by the recognizing agent. How should the recognizing agent, A, rea son about a situation in which it observes another agent, B, taking some actions, A infers a plan that it thinks B is pursuing, takes some actions on B's be half, and then sees further actions by B? Judea Pearl has provided the general solution for reasoning about such situations, with his theory of interventions (Pearl 1994). We incorporate interventions into our theory of plan recognition, to permit recognizing systems to work with the agents whose plans they watch.\nShortcomings in past approaches motivated the work described in this paper. Previous plan recognition sys tems have had trouble handling:\n\u2022 partially-ordered plans and plan interleaving;\n\u2022 evidence from failure to observe actions;\n\u2022 contextual influence on plan choice;\n\u2022 domains in which the recognizer and the recog nized both act.\nIn the following section, we will provide a novel model of plan recognition that provides solutions to these problems.\n4 Plan execution model\nOur model for plan execution is a simple one. The ex ecuting agent, at the start of the episode, chooses a set of plans to execute. The set of plans chosen determines the set of primitive actions that are pending. As the episode proceeds, the agent will repeatedly execute one of the pending actions, and generate a new set of pend ing actions from which further actions will be chosen. A new set of pending actions will be generated from the previous set by removing the action just executed and adding newly enabled actions. Actions become enabled when their predecessors are completed. This process is illustrated in Figure 5.\nThe model of plan execution provides a conceptual model for the generation of execution traces. In or der to use this model to perform plan recognition, we use this model and reason abductively.\nAs a way of motivating some of the issues this work is designed to confront this section will present some brief examples of the kinds of problems our system is designed to handle.\n4.1 Partial orderings\nMost encodings of plan recognition raise the prob lem of partial orders. Partial orders arise through tasks that have sub-tasks that are only partially or dered. Method lower-power-use provides an ex ample of partially-ordered sub-tasks: once the access panel has been opened, the two subsystems can be shut down in either order. Partial ordering also arises when an agent pursues multiple, independent tasks simulta neously. Of other plan recognition theories we have examined, only K&A's can properly handle partially ordered tasks.\n4.2 Overloaded actions\nMany plan recognition systems do not permit expla nations for an action in which that action is done as part of more than one plan. For example, open-pl and start-gen-B might be done as part of both the plans for increase-power and raise-02-level.\n4.3 Performing actions for their own sake\nMany previous plan recognition systems have required that their users make hard distinctions between \"top level\" actions - actions that can be done for their own sake - and other actions that are done only in service of higher level goals. Our applications show that this distinction is artificial and unrealistic. For example, operators of control systems, when not otherwise oc cupied, often glance at the current values of process variables. We would expect, e.g., check-temp to be performed occasionally \"for its own sake,\" rather than as part of raise-temp. Our model does not require a hard distinction between top-level and other actions.\n4.4 Negative Evidence\nPrevious plan recognition systems have not been able properly to take into account negative evidence, the confirming or disconfirming effect of failing to see some action. Previous systems didn't take negative evidence into account because they didn't treat observations as sequences.\n4.5 Context\nOur model allows us to take into account the state of the world when considering what goals the agent might be pursuing. As we stated earlier, if the agent has seen a drop in 02 level, then raise-02-level is a better explanation of open-pl than increase-power IS.\nA New Model of Plan Recognition 249\nFigure 5: A dynamic belief net illustrating the simple model of plan execution.\nAfter a more formal exposition of our model we will return to discuss these examples and the inference that is performed in our model.\n5 Formalizing the model\nIn this section of the paper we present a formal repre sentation of the plan execution model we have outlined above. As our notation we use Poole's logic of Prob abilistic Horn Abduction (PHA) (Poole 1993b). PHA provides logical rules entailing propositions and dis tinguished assumable propositions called hypotheses. These rules are in Prolog-like Horn form. An expla nation for a proposition is a set of hypotheses that, taken together with the rules, entail that proposition. Hypotheses have associated prior probabilities, so the logic supports a notion of best (most likely) explana tion. Poole shows that PHA can be used to describe arbitrary belief nets.\n5.1 Plans\nOur plan execution model works in concert with a li brary of plans that the agent may be executing. The library is made up of tasks. Tasks may be goals, meth ods and primitive actions. Caution: Please do not read too much into the terms \"goal\" and \"method.\" By these terms we mean nothing more than disjunc tive and conjunctive nodes in the plan graph (respec tively).\n5.2 The Model\nIn this section we will walk through the explanation of observed actions. Working backward from the ob servation, we must explain how the observed action is chosen from the set of pending actions. Then we must explain how that set of actions is assembled. The ex planation of the pending set covers the choice of sub tasks as means to achieving higher level goals. It also covers the evolution of a plan execution episode over time.\nRecall the discussion in the previous section - in or-\n250 Goldman, Geib, and Miller\nder to take into account the fact that our agents may be carrying out multiple, interleaved, partially-ordered plans, we simply stipulate that the action happening at any time is one that is chosen from the set of pend ing actions. This action of choosing from the current pending set corresponds to the hollow arrows in Fig ure 5. We will define the pending set later.\n(1) happen(X, T+1)+pending(P, T), X EP, tpick(X, P, T+1).\nRule 1 states that action X happens at time T + 1 if X is in the set of pending actions and is picked from that set. pick(X, P, T) is one of the distinguished hypothesis propositions with an associated probabil ity. (In our presentation of the rules, we will mark hypotheses with a t.)\nIn our current system, we simply assume that all of the pending actions are equally likely. It would be simple to provide a more complex model of the choice of next action to execute, for example, some sort of exponentially-increasing probability as the action con tinues to be pending.\nFor example, two of the explanations for happen(open-pl, 1), from the plan database shown in Figure 1 are: { 1) The pending set is { open pl, check-temp} and open-pl is picked; (2) The pending set is { open-pl, open-p2} and open-pl is picked.\nWe define the pending set recursively, first providing a basis case, that allows us to explain the contents of the pending set at time zero. The process of choosing the initial pending set from the plan library is shown in Figure 5 through the narrow arrows. The pending set is selected from the set of primitive actions (leaves):\n(2) pending(\u00a3, 0) +- leaves (Ls), select\ufffdeaves (Ls, L)).\nThe rule for select\ufffdeaves selects those leaves that are enabled.\nThe core of our model of plan execution is what it means for an action to be enabled. There are two reasons why an action may be enabled: either it is done for its own sake, or it is done as part of a parent plan.\n{3) enabled(A, T) +-for_own..sake(A, T).\n(4) enabled(A, T) +- \ufffd for_own..sake(A, T), for_parent(A, T).\nThe simple case is an action being done for its own case:\n{5) for_own..sake(A, T) + intendable(A), tintended(A, T).\nThe action A must be one of the set of actions that may be done for their own sake. Whether or not . a particular action meets this condition is indicated in the plan database. Note that there are no constraints on what kind of action may be done for its own sake; goals, methods and primitive actions are all eligible. In addition to it being possible that an action be done for its own sake, it must actually be done for its own sake. This is the second condition, that A be intended, one of the hypotheses whose probability must be assessed (this will be discussed later).\nThe other reason why an action might be enabled is that it is being done as part of a larger plan:\n(6) for_parent(A, T) + parents(A, Ps), :!PEPs I enabled..by(A, P, T).\nThere are two ways a child action, C, can be enabled as part of a parent plan, P {rules 7 and 10). First, the parent plan may be a method; a recipe that is a set of steps. In this case, in order for a particular action to be enabled, it must be a member of that set of steps, and all the preceding steps in that recipe must already have been done:\n(7) enabled..by(C, P, T) +- and..node(P), enabled(?, T), preds(C, P) = P, Vp E P [ prev_done(p, T) ].\nThe set of predecessors of a given node in a plan is de fined in the plan library. For example, in Figure 1, the predecessor set of start-02-gen in gen-02 is {start gen-B } .\nThe definition for prev __done for primitive actions is straightforward - a primitive action, A is prev __done at T iff happen(A, T') and T' :S T. For composite actions, it is more complex. Rule 8 gives the condition for methods and 9 for goals.\n(8) prev__cione(A, T + 1) +- and..node(A), expansion(A) =A, VaEA[ prev__cione(a, T+1) ].\n(9) prev__cione(A, T+ 1) +- or..node(A), enabled(A, T + 1), tchoose..expansion(M, A), prev __done ( M , T + 1) .\nThe second way a child, C can be enabled as part of a plan, P is if C is one of a set of methods for carrying out P. In this case, what is required is that the exe cuting agent choose C as the way she intends to do P:\n(10) enabled_by(C, P, T) for ...node ( P), choose_expansion(C, P), enabled(?, T).\nExpressions of the form choose_expansion(C, P) are hypotheses. In the examples we have worked, we have assumed that all of the methods for a given par ent, P, are equally likely, however this may easily be changed. One need only supply choice probabilities. A more substantial limitation is that this approach assumes that only a single method will be chosen by the executing agent as his means of achieving P. This assumption could be relaxed, but only at the cost of a more cumbersome model of method choice. So far, we have not found this necessary.\nSo far, we have provided only the rules necessary for explaining the initial contents of the set of pending actions and hence, explaining the first action an agent takes. As a plan execution progresses, the contents of the pending set changes (this process is shown in the shaded arrows in Figure 5):\n(11) pending(Pt+l\u2022 T + 1) f pending(Pt. T), leaves(Ls), progress (Ls, Pt, Pt+l, T + 1) .\nThe progress rule assembles a new pending list, Pt+l by considering the set of primitive actions, adding and subtracting appropriate actions. This is the most com plex of the rules, involving several cases:\n1. The most recent action is removed from the pend ing set;\n2. All other previously pending actions remain pend ing;\n3. Things that were not previously pending may be added.\nThe need for the rules to be mutually-exclusive and exhaustive makes the actual rules for progress some what cumbersome, so we have omitted them from the short version of this paper. The interesting case is the case of adding a primitive action to the pending set, Case 3. Primitive actions are added to the pending set if they have not already been done and if they are enabled:\n(12) add.Jlle(.4, T) f\ufffd prev..done(.4, T), enabled(.4, T) .\nRecall the two ways an action can become enabled: if it is the chosen method for a newly-enabled parent (10) or when is part of a method that is being executed and the last of its predecessors has just been completed (7). Also, recall that the definitions of enabled are recur-\nA New Model of Plan Recognition 251\nsive, applying not only to primitive actions, but also to methods and goals.\n5.3 Incorporating Context\nSo far, we have simply assumed that all plans have some fixed a priori probability of being chosen. We extend our model of plan execution to make plan adop tion conditional on facts that obtain at the start of the episode. We add rules that make plan adoption conditional on some facts about the environment. We provide an example of this later in the paper.\n5.4 Interacting with the Executing Agent\nWe wish to perform plan recognition in order to pro vide help to users. To do this properly, it is not enough to passively observe the process of plan execution. We must also be able to intervene into the process and predict the effects of our interventions. We would like to build intelligent systems that intervene by perform ing actions on the user's behalf.\nPearl (1994) provides a theoretical framework for causal interventions into processes described by dy namic Bayesian Networks like ours (Figure 5). The essence of his approach is that one can intervene by \"clamping\" a node in the network and cutting the causal links into that node.\nNow we revise the model of plan execution so that we permit two explanations for an action occurring: either the observed agent carries out the action, or the system intervenes to perform an action. We apply Pearl's approach to our plan execution model by re placing rule (1) above with the following two rules:\n(13) happen(X, T+l) f-intervene(X, T+l) . and the following revision of the original rule ( 1), to make it mutually exclusive and exhaustive with (13):1\n(14) happen(X, T + 1) f\ufffd intervene (T + 1), pending(L, T), member(X, L), pick(X, L, T+l).\nRule (13) indicates that an intervention makes the oc currance of action X at T + 1 independent of preceding events and the contents of the pending set. However, interventions do have causal effects on the future con tents of the pending set and hence on what later ac tions are performed by the agent.\n1 PHA requires all rules be mutually-exclusive and exhaustive.\n252 Goldman, Geib, and Miller\n5.5 Probabilities\nIn order to make the model here a probabilistic model, we need only specify probabilities for a restricted set of hypotheses. These are as follows:\n1. The probability that the agent will adopt a par ticular task for its own sake. These probabilities may either be simple, unconditional probabilities, or may be conditional on the agent's environment.\n2. The probability that the agent will choose a par ticular method when attempting to achieve a goal.\n3. The probability that the agent will choose a par ticular primitive action from a set of pending primitive actions.\nThese three parameters correspond to the three types of hypotheses, marked with a t in the rules above above.\nThe simple set of rules above, together with a plan li brary, provides a framework for abductive plan recog nition. We may generate abductive proofs for the oc currence of observed action sequences, and from these explanations extract user intentions. In the following section, we provide several worked examples. These serve both to illustrate our model and to show how it differs from previous work in this area.\n6 Plan recognition examples\nIn this section we provide examples that show how our model handles difficult plan recognition cases. In these examples, unless we state otherwise, we have assumed that all goals are equally likely and all methods are equally likely to be chosen by the agent. Most of these examples will be based on the plan library shown in Figure 1. We stress that these assumptions are not parts of our model - they are additional simplifica tions for the sake of clarity in presentation.\nWe abbreviate propositions like happen(open-pl, t) as open-pl1, yielding expressions like P(open p11): the probability that open-pl happens at time 1, and P(check-temp2lopen-pl1): the probabil ity that check-temp happens at time 2 given that open-pl happened at time 1. Likewise, we will use P(intend(increase-power)lopen-plt) to express the system's posterior belief that the agent intends to increase-power given that open-pl happened at time 1. Where it is possible without ambiguity, we will use w to represent the observations, as in P( intend(increase-power) lw).\n6.1 Interleaving plans for multiple goals\nConsider the case where the agent executes action open-pl and then action check-temp. Given this, the system can conclude that the agent must be pur suing raise-temp and one or both of increase-power and raise-02-level:\nPlan P( intend(Planlw ) ) raise-temp increase-power raise-02-level 1.0 0.6603 0.6603\nThe system's beliefs about the multiple goals of the agent allow it to assign higher probability to actions that contribute to the agent's possible plans. For ex ample, start-gen-B and raise-temp-set are the next steps in the respective plans. Therefore,the system be lieves one of these actions will almost certainly be the the next action executed:\nP(start-gen-Balw) = P(raise-temp-setalw) = .4748 .4748\n6.2 Partially ordered plans\nThe pending set provides our system the ability to han dle partially ordered plans. Consider the case of ac tions open-p2, shutoff-Xl and shutoff-X2, which make up lower-power-use. In this domain, action open-p2 is the only predecessor for both shutoff-XI and shutoff-X2. Thns once open-p2 has been exe cuted both shutoff-XI and shutoff-X2 are enabled and therefore are in the pending set until they are per formed.\nThe rules for construction of the pending set guarantee that ordering constraints are enforced. For example, action start-gen-B cannot be added to the pending set until action open-pl has been performed (see Fig ure 1).\n6.3 Overloaded actions\nIn Figure 1, open-pl is an example of an action that may be done as part of two plans, simultaneously. This possibility can be readily handled by our theory. Re call that in order to explain observations, we must find each pending set consistent with the evidence and con sider its possible explanations. If we ask for the proba bility that open-pl will be the first action performed, we must consider all of the possible goal sets the agent might be pursuing and all the possible expansions of those goals. As part of this process, we will find ex planations where action open-pl is used as part of two different methods: one for increase-power and another to raise-02-level.\n6.4 Performing actions for their own sake\nLet us compare two cases in which we observe check temp. In the first case, we make the conventional assumption that check-temp will only be done as part of a plan to raising the temperature. In this case, we can prove that the agent must be execut ing raise-temp. If, on the other hand, we consider the possibility that the operator is idly checking the temperature, then we will conclude that the proba bilities that the agent is pursuing raise-temp or just doing check-temp, is equal to their prior probabili ties. If the probability of doing check-temp \"for its own sake\" is relatively high, then we will not neces sarily conclude that the agent is trying to raising the temperature. If this plan recognition is being done as part of a system that might take actions to help the agent it is observing, one can see that getting this right is important.\n6.5 Negative Evidence\nConsider what happens if we were to see actions open pl and start-gen-B. All else being equal, we can conclude that increase-power (by means of gen power) and raise-02-level (via gen-02) are equally likely. But now consider what happens as we see ac tion check-temp, and then action raise-temp-set. As time goes by and we see other actions, we become more and more confident that we have seen method gen-power and not simply the beginning of gen-02 . The use of the pending set in our framework provides a simple and elegant solution to this problem. Recall that an action happens if it is in the set of pending ac tions, and is picked from that set. In our framework, as we see the sequence of actions that does not contain start-02-gen grow, we will gradually become more and more certain that start-02-gen is not a member of the pending set. This will undermine our previ ous belief in hypothesis raise-02-level and reinforce our belief in increase-power. Table 1 shows how the conditional probabilities for this problem evolve over time.\n6.6 Context\nOur model allows us to take into account the state of the world when considering what goals the agent might be pursuing. This capability is essential in supporting users whose tasks are influenced by the state of the world, e.g. users that are controlling a manufacturing plant.\nFigure 1 shows that the agent's intentions toward increase-power depend on the proposition EVA prep. We assume for simplicity's sake that the re-\nlationship between EVA-prep and increase-power is deterministic (this is not required by our formal ism): if EVA-prep is true then the agent intends increase-power and if \ufffdEVA-prep then the agent will not intend increase-power. If P(EVA-prep) = 0.5 then all three plans are equally likely a pri orz: ?(increase-power) = P(raise-02-level) = P(raise-temp) = 0.5\nThe probabilities of the initial actions for increase power are P(open-p11) = .2864 and P(open p2I) = .1458. However, if the system knows that EVA-prep is true, then its belief that the agent is pur suing increase-power shifts up to one and the prob ability of any action that contributes to this plan in creases. Thus P(open-p11IEVA-prep) = .3854 and P(open-p21JEVA-prep) = .2916.\n6. 7 Interventions\nConsider the set of actions given in Figure 6. Sup pose the agent is observed performing action a. The only plans that are consistent with performing a are p and q and these are equally likely. Therefore, the system concludes that the agent is doing either p or q with equal probability: P( intend(p) Ia I ) = P( intend( q) Ia I ) = .6666. It further concludes that the actions b and d are equally likely to appear next: P(b2lai) = P(d2la1) = .5.\nSuppose the agent were to perform b. Since p is the only reason to do b, the system can safely infer that the agent intends p. This also tends to explain away a, lowering the probability of q: P(intend(q)Ja1,b2) = .3333. The execution of b also enables c and provides evidence that it will be next: P( cala1 ,b2) = .8333.\nNow consider what would happen if, rather than the agent performing b, the system had done it for her (we write this as I(b2). The plan recognition algorithm's only treats b as evidence for the agent's pursuit of p if the agent herself does it. If the system performs b, then the system's belief in the probability of the agent pursuing p does not change: P(intend(p)iai) = P(intend(p)Ja1,I(b2)) = .6666\nHowever, the execution ofb by the system does licence\n254 Goldman, Geib, and Miller\nsome inferences. For example, since b has been exe cuted c is now enabled, and therefore will be added to the pending set. Since the system's execution of b does not change the system's beliefs about the goals the agent is persuing, the system believes that c is as likely to be the next action as b was before its execu tion: P (b2 jat) = P (ca lat,l(b2)) = .5.\n7 Implementation\nOur plan recognition theory has been implemented computationally. The implementation centers around a PHA abductive theorem prover. Our theorem-prover is based on David Poole's (Poole 1993a) but, it differs substantially in search strategy. All examples men tioned in this paper have been tested and work in our implementation.\nThe rules used by the program differ from those pre sented earlier only in the treatment of quantification and negation, neither of which are interpreted by the theorem prover. We expect to make our theorem prover and rule set publicly available.\n8 Concluding Remarks\nIn this paper we have presented a general model of plan recognition based on probabilistic abductive logic. The centerpiece of the model is a simple formal theory of plan execution. In comparison with previous work on plan recognition, our theory better handles se quences of actions generated by interleaved, partially ordered plans. Our theory can accommodate infor mation about the context in which plans are adopted. Finally, unlike previous theories, ours can incorporate interventions into the process of plan execution.\nIn future work, we would like to address some of the limitations of our model. One limitation is that in this model the agent does not interact with its surround ings. The actions done by the agent and the interven tions do not change the world state, because the world state is not represented. We are now working on a more elaborate plan execution model that takes this into account.\nAcknowledgements Thanks to David Poole for his PHA interpreter and help working with it.\nReferences\nCharniak, E., and Goldman, R. P. 1993. A Bayesian model of plan recognition. Artificial Intelligence 64(1):53-79.\nCharniak, E., and McDermott, D. 1985. Introduc tion to Artificial Intelligence. Reading, MA: Addison Wesley.\nCohen, P. R.; Perrault, C. R.; and Allen, J. F. 1981. Beyond question answering. In Lehnert, W., and Ringle, M., eds., Strategies for Natural Language Pro cessing. Hillsdale, NJ: Lawrence Erlbaum Associates. 245-274.\nde Mantaras, R. L., and Poole, D., eds. 1994. Un certainty in Artificial Intelligence, Proceedings of the Tenth Conference. Morgan Kaufmann.\nKautz, H., and Allen, J. F. 1986. Generalized plan recognition. In Proceedings of the Fifth National Con ference on Artificial Intelligence, 32-38.\nPearl, J. 1994. A probabilistic calculus of actions. In de Mantaras and Poole (1994), 454-462.\nPoole, D. 1993a. Logic programming, abduction and probability: a top-down anytime algorithm for stimat ing prior and posterior probabilities. New Generation Computing 11(3-4):377-400.\nPoole, D. 1993b. Probabilistic horn abduction and Bayesian networks. Artificial Intelligence 64:81-129.\nPynadath, D. V., and Wellman, M. P. 1997. Gener alized queries on probabilistic context-free grammars. To appear in IEEE PAM!.\nVilain, M. 1990. Getting serious about parsing plans: A grammatical analysis of plan recognition. In Pro ceedings of the Eighth National Conference on Arti ficial Intelligence, 190-197. Cambridge. MA: MIT Press.\nWellman, M. P., and Pynadath, D. V. 1997. Plan recognition under uncertainty. Unpublished web page."}], "references": [{"title": "A Bayesian model of plan recognition", "author": ["E. Charniak", "R.P. Goldman"], "venue": "Artificial Intelligence 64(1):53-79.", "citeRegEx": "Charniak and Goldman,? 1993", "shortCiteRegEx": "Charniak and Goldman", "year": 1993}, {"title": "Introduc\u00ad tion to Artificial Intelligence", "author": ["E. Charniak", "D. McDermott"], "venue": "Reading, MA: Addison Wesley.", "citeRegEx": "Charniak and McDermott,? 1985", "shortCiteRegEx": "Charniak and McDermott", "year": 1985}, {"title": "Beyond question answering", "author": ["P.R. Cohen", "C.R. Perrault", "J.F. Allen"], "venue": "Lehnert, W., and Ringle, M., eds., Strategies for Natural Language Pro\u00ad cessing. Hillsdale, NJ: Lawrence Erlbaum Associates. 245-274.", "citeRegEx": "Cohen et al\\.,? 1981", "shortCiteRegEx": "Cohen et al\\.", "year": 1981}, {"title": "Generalized plan recognition", "author": ["H. Kautz", "J.F. Allen"], "venue": "Proceedings of the Fifth National Con\u00ad ference on Artificial Intelligence, 32-38.", "citeRegEx": "Kautz and Allen,? 1986", "shortCiteRegEx": "Kautz and Allen", "year": 1986}, {"title": "A probabilistic calculus of actions", "author": ["J. Pearl"], "venue": "de Mantaras and Poole (1994), 454-462.", "citeRegEx": "Pearl,? 1994", "shortCiteRegEx": "Pearl", "year": 1994}, {"title": "Logic programming, abduction and probability: a top-down anytime algorithm for stimat\u00ad ing prior and posterior probabilities", "author": ["D. Poole"], "venue": "New Generation Computing 11(3-4):377-400.", "citeRegEx": "Poole,? 1993a", "shortCiteRegEx": "Poole", "year": 1993}, {"title": "Probabilistic horn abduction and Bayesian networks", "author": ["D. Poole"], "venue": "Artificial Intelligence 64:81-129.", "citeRegEx": "Poole,? 1993b", "shortCiteRegEx": "Poole", "year": 1993}, {"title": "Gener\u00ad alized queries on probabilistic context-free grammars", "author": ["D.V. Pynadath", "M.P. Wellman"], "venue": "To appear in IEEE PAM!.", "citeRegEx": "Pynadath and Wellman,? 1997", "shortCiteRegEx": "Pynadath and Wellman", "year": 1997}, {"title": "Getting serious about parsing plans: A grammatical analysis of plan recognition", "author": ["M. Vilain"], "venue": "Pro\u00ad ceedings of the Eighth National Conference on Arti\u00ad ficial Intelligence, 190-197. Cambridge. MA: MIT Press.", "citeRegEx": "Vilain,? 1990", "shortCiteRegEx": "Vilain", "year": 1990}, {"title": "Plan recognition under uncertainty", "author": ["M.P. Wellman", "D.V. Pynadath"], "venue": "Unpublished web page.", "citeRegEx": "Wellman and Pynadath,? 1997", "shortCiteRegEx": "Wellman and Pynadath", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Charniak and Goldman (C&G) (1993)", "startOffset": 0, "endOffset": 34}, {"referenceID": 9, "context": "More recently, Wellman and Pynadath (W&P) (1997)", "startOffset": 15, "endOffset": 49}, {"referenceID": 4, "context": "How should the recognizing agent, A, rea\u00ad son about a situation in which it observes another agent, B, taking some actions, A infers a plan that it thinks B is pursuing, takes some actions on B's be\u00ad half, and then sees further actions by B? Judea Pearl has provided the general solution for reasoning about such situations, with his theory of interventions (Pearl 1994).", "startOffset": 358, "endOffset": 370}, {"referenceID": 6, "context": "As our notation we use Poole's logic of Prob\u00ad abilistic Horn Abduction (PHA) (Poole 1993b).", "startOffset": 77, "endOffset": 90}, {"referenceID": 5, "context": "Our theorem-prover is based on David Poole's (Poole 1993a) but, it differs substantially in search strategy.", "startOffset": 45, "endOffset": 58}], "year": 2011, "abstractText": "We present a new abductive, probabilistic theory of plan recognition. This model dif\u00ad fers from previous theories in being centered around a model of plan execution: most previous methods have been based on plans as formal objects or on rules describing the recognition process. We show that our new model accounts for phenomena omitted from most previous plan recognition theories: no\u00ad tably the cumulative effect of a sequence of observations of partially-ordered, interleaved plans and the effect of context on plan adop\u00ad tion. The model also supports inferences about the evolution of plan execution in situ\u00ad ations where another agent intervenes in plan execution. This facility provides support for using plan recognition to build systems that will intelligently assist a user.", "creator": "pdftk 1.41 - www.pdftk.com"}}}