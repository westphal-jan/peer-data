{"id": "1301.7362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Tractable Inference for Complex Stochastic Processes", "abstract": "the monitoring and control ability of any dynamic voting system depends mostly crucially fundamentally on the ability to reason about its current status and its future exit trajectory. in the case instance of a stochastic stability system, these tasks \" typically involve the use factor of a belief threshold state - a probability random distribution over the hidden state units of the established process at hardly a given critical point in specified time. if unfortunately, testing the state spaces of highly complex processes are very unexpectedly large, significantly making an especially explicit analytic representation of a future belief state equally intractable. even conducted in realistic dynamic discrete bayesian networks ( r dbns ), regimes where where the believed process description itself appears can just be represented compactly, the spectral representation analysis of the simulated belief ground state is intractable. we rather investigate the idea of maintaining a compact accurate approximation to the assumed true general belief state, assess and analyze the historical conditions under evaluating which the errors due to computing the approximations taken over counting the lifetime of the process ultimately do almost not accumulate to dramatically make guiding our answers completely irrelevant. simply we show that the error in a regular belief state contracts exponentially less as the process evolves. thus, even with multiple approximations, modelling the error implicit in modeling our process remains bounded upon indefinitely. we show how greatly the additional statistical structure constraints of framing a possible dbn can be used to design using our approximation scheme, improving its performance significantly. moreover we demonstrate the applicability of our ideas in the current context description of a monitoring task, showing that numerous orders of equal magnitude relatively faster inference generation can be achieved with applying only a sudden small degradation in performance accuracy.", "histories": [["v1", "Wed, 30 Jan 2013 15:02:39 GMT  (430kb)", "http://arxiv.org/abs/1301.7362v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["xavier boyen", "daphne koller"], "accepted": false, "id": "1301.7362"}, "pdf": {"name": "1301.7362.pdf", "metadata": {"source": "CRF", "title": "Tractable Inference for Complex Stochastic Processes", "authors": ["Xavier Boyen", "Daphne Koller"], "emails": [], "sections": [{"heading": null, "text": "1 Introduction\nThe ability to model and reason about stochastic processes is fundamental to many applications [6, 8, 3, 14]. A number of formal models have been developed for describing situ ations of this type, including Hidden Markov Models [15], Kalman Filters [9], and Dynamic Bayesian Networks [4]. These very different models all share the same underlying Markov assumption, the fact that the future is conditionally independent of the past given the current state. Since the domain is stochastic and partially observable, the true state of the process is rarely known with certainty. However, most reasoning tasks can be performed by using a belief state, which is a probability distribution over the state of a system at a given time [1]. It follows from the Markov assumption that the belief state at time t completely cap tures all of our information about the past. In particular, it suffices both for predicting the probabilities of future tra jectories of the system, and for making optimal decisions\nabout our actions.\nConsider, for example, the task of monitoring an evolving system. Given a belief state at time t which summarizes all of our evidence so far, we can generate a belief state for timet+ 1 using a straightforward procedure: We propagate our current belief state through the state evolution model, resulting in a distribution over states at timet+ 1, and then condition that distribution on the observations obtained at time t + 1, getting our new belief state.\nThe effectiveness of this procedure (as well as of many oth ers) depends crucially on the representation of the belief state. Certain types of systems, e.g., Gaussian processes, admit a compact representation of the belief state and an ef fective update process (via Kalman .filtering [9]). However, in other cases matters are not so simple. Consider, for exam ple, a stochastic system represented as a dynamic Bayesian network (DBN). A DBN, like a Bayesian network, allows a decomposed representation of the state via state variables, and a compact representation of the probabilistic model by utilizing conditional independence assumptions. Here, a belief state is a distribution over some subset of the state variables at time t. In general, not all of the variables at timet must participate in the belief state [3]; however, (at least) every variable whose value at time t directly affects its value at time t + 1 must be included. In large DBNs, the obvious representation of a belief state (as a flat distri bution over its state space) is therefore typically infeasible, particularly in time-critical applications.\nHowever, our experience with Bayesian networks has led us to the tacit belief that structure is usually synonymous with easy inference. Thus, we may expect that, here also, the structure of the model would support a decomposed representation of the distribution, and thereby much more effective inference. Unfortunately, this hope turns out to be unfounded. Consider the DBN of Figure 1 (a), and assume to begin with that the evidence variable did not exist. At the initial time slice, all of the variables start out being in dependent. Furthermore, there are only a few connections between one variable and another. Nevertheless, at time 3, all the variables have become fully correlated: one can see from the unrolled DBN of Figure l (b) that no conditional independence relation holds among any of them. Observ ing the evidence variable makes things even worse: i_n this case it takes only 2 time slices for all the state vartables to become fully correlated. In general, unless a process decomposes into completely independent subprocesses, the\nbelief state will become fully correlated very early in time. As any factored decomposition of a distribution rests on some form of conditional independence, no decomposition of this belief state is possible. This phenomenon is per haps the most serious impediment to applying probabilistic reasoning methods to dynamic systems.\nRelated problems occur in other types of processes. For ex ample, in hybrid processes involving both continuous and discrete variables, we might wish to represent the belief state as a mixture of Gaussians. However, in most cases, the number of components in the mixture will grow expo nentially over time, making it impossible to represent or reason with an exact belief state.\nOne approach to dealing with this problem is to restrict the complexity of our belief state representation, allowing inference algorithms to operate on it effectively. Thus, rather than using an exact belief state which is very complex, we use a compactly represented approximate belief state. For example, in the context of a DBN, we might choose to represent an approximate belief state using a factored representation. (See [ 12] for some discussion of possible belief state representations for DBNs.) In the context of a hybrid process, we might choose to restrict the number of components in our Gaussian mixture.\nThis idea immediately suggests a new scheme for monitor ing a stochastic process: We first decide on some computa tionally tractable representation for an approximate belief state (e.g., one which decomposes into independent fac tors). Then we propagate the approximate belief state at time t through the transition model and condition it on our evidence at timet + I. In general, the resulting belief state for time t + I will not fall into the class which we have chosen to maintain. We therefore approximate it using one that does, and propagate further.\nThis general strategy must face the risk that our repeated approximations cause errors to build up out of control over extended periods of time, either by accumulation due to the repeated approximations, or (worse) by spontaneous ampli fication due to some sort of instability. In this paper, we show that this problem does not occur: the mere stochas ticity of the process serves to attenuate the effects of errors over time, fast enough to prevent the accumulated error from growing unboundedly. More specifically, we have proved the important (and to our knowledge) new result that stochastic processes (under certain assumptions) are a contraction for KL-divergence [2]: propagation of two dis tributions through a stochastic transition model results in a constant factor reduction of the KL-divergence between them. We believe that this result will have significant ap-\nplications in many other contexts.\nThis result applies to any discrete stochastic process and any approximate representation of the belief state. How ever, we show that even stronger results can be obtained in the case of structured processes equipped with an ap proximation scheme that is tailored to the structure of the process. Specifically, we consider processes that are com posed of several weakly interacting subprocesses, and an approximation scheme that decomposes each belief state as a product of independent belief states over the individual processes. Under these assumptions, our contraction rate improves dramatically. We also show a strong connection between the amount of interaction among the subprocesses and the quality of our bounds, We provide empirical ev idence for the success of our approach on two practical DBNs, showing that, by exploiting structure, we can achieve orders of magnitude faster inference with only a tiny degra dation in accuracy. As we mentioned above, DBNs have been a counter-example to our intuition that structured rep resentations leads to effective inference. Our results are the first to show how, at least for approximate inference, we can exploit the structure of a DBN for computational gain.\n2 Basic framework\nOur focus in this paper is on discrete-time finite-state Markov processes. Such processes can be modeled explic itly, as a Hidden Markov Model or, if additional structure is present, more compactly as a Dynamic Bayesian Network. A discrete time Markov process evolves by moving from one state to the other at consecutive times points. We use s(t) with s(t) E s = { S]' ... , sn} to denote the state at timet. In the case of a DBN, S(t) may be described as an assignment of values to some set of state variables. The Markov assumption, inherent to all of the models we con sider, asserts that the present state of the system contains enough information to make its future independent from its past, i.e., that P(S(t+i) I s<o)' ... , s<t)] = P(S(t+I) I s(t)]. For simplicity, we also assume that the process is time invariant, i.e., that the probability with which we have a transition from some state s; at time t to another state Sj at time t + I does not depend on t. Thus, we obtain that the process can be described via a transition model T: T[s;\"\"'sj] \ufffd P(s?+I) I s\ufffdt)], where we use s\ufffdt) to denote the event s< t) = s;. In the case of an HMM, T is of ten specified explicitly as an n x n matrix; for a DBN, T is described more compactly as a fragment of a Bayesian network (see Section 5.2).\nThe Markov process is typically hidden, or partially ob servable, meaning that its state is not directly observable. Rather, we observe a response R(t) E R = {r1, ... , rm}; in the case of a DBN, R(t) can be an assignment to some set of observable random variables. The response depends stochastically and exclusively on the state S(t); i.e. , R(t) is conditionally independent of any s<t') and R(t') given S(t). Using r\ufffdt) to denote R(t) = rh, we obtain that the observability aspect of the process can be described via an observation modelO: O[s;\ufffdrh] \ufffd P(r\ufffdt) I s\ufffdt)]. The Markov assumption implies that all the historical in formation needed to monitor or predict the system's evo-\nlution is contained in (the available knowledge about) its present state. This knowledge can be summarized in a be lief state-a probability distribution over the possible states. We distinguish between the prior and the posterior belief state:\nDefinition 1 The prior belief state at timet, denoted uC \u2022t), is the distribution over the state at t given the response history up to but not including timet. Letting r\ufffd\ufffd) denote the response observed at time k,\nc\u2022tl[ \u00b7l \ufffd P[ (t) I coJ ct-!Jl u s, - 8i rho ' ... , rht-1 \u00b7\nThe posterior belief state at time t, denoted uCt\u2022), is the distribution over the state at timet given the response history up to and including timet:\nuCt\u2022l[s;] \ufffd P[s\ufffdt) I r\ufffd\ufffdl, ... ,r\ufffdt\ufffd :),r\ufffdt')]. 0\nThe monitoring task is defined as the task of maintaining a belief state as time advances and new responses are ob served. In principle, the procedure is quite straightforward. Assume we have a posterior belief state uCt\u2022) at time t. Upon observing the response rh at timet+ 1, the new state distribution uCt+It) can be obtained via a two-stage compu tation, based on the two models T and 0. The prior belief state uC\u2022t+I) for the next time slice is obtained by propa gating uCt\u2022) through the stochastic transition model, while the posterior uCt+It) is obtained by conditioning uC \u2022t+I) on the response rh observed at timet + 1:\nn\ni=l\n2::\ufffd1 uC\u2022t+l)[sz] O[slc......,.rh] Abstractly, we view T[\u00b7] as a function mapping u(t\u2022) to uC \u2022t+I), and define Orh [\u00b7] as the function mapping uC \u2022t+I) to u(t+l\u2022) upon observing the response rh at timet+ 1. While exact monitoring is simple in principle, it can be quite costly. As we mentioned in the introduction, the belief state for a process represented compactly as a DBN is typically exponential in the number of state variables; it is therefore impractical in general even to feasibly store the belief state, far less to propagate it through the various update procedures described above.\nThus, we are interested in utilizing compactly represented approximate belief states in our inference algorithm. The risks associated with this idea are clear: the errors induced by our approximations may accumulate to make the results of our inference completely irrelevant. As we show in the next two sections, the stochasticity of the process prevents this problem from occurring.\n3 Simple contraction\nConsider the exact and estimated belief states uC t\u2022) and u(t\u2022). Intuitively, as we propagate each of them through the transition model it \"forgets\" some of its information; and as the two distributions forget about their differences, they\nbecome closer to each other. As we will see in Section 5, in order for our errors to remain bounded, their effect needs to dampen exponentially quickly. That is, we need to show that T reduces the distance between two belief states uCt\u2022) and uCt\u2022) by a constant factor. This result is known for T if we use \u00a32 norm (Eu clidean distance) as the distance between our distributions: II T[uCt\u2022)]- T[u(t\u2022)l l2:::; 1>-21\u00b7 II q(tt)- u(t\u2022) lh' where >.2 is the second largest eigenvalue ofT. Unfortunately, \u00a32 norm is inappropriate for our purposes. Recall that there are two main operations involved in updating a belief state: propagation through the transition model, and conditioning on an observation. Unfortunately, \u00a32 norm behaves badly with respect to conditioning: II Orh [uC\u2022t)]- Orh [uC\u2022t)JII2 can be arbitrarily larger than II uC\u2022t)- uC\u2022t) ll2- In fact, one can construct examples where the observation of any response rh will cause the \u00a32 distance to grow. Thus, we must search for an alternative distance measure for which to try and prove our contraction result. The obvious candidate is relative entropy, or KL divergence, which quantifies the loss or inefficiency incurred by using distribution .,P when the true distribution is if> [2, p. l8]:\nDefinition 2 If if> and t/J are two distributions over the same space Q, the relative entropy of if> to t/J is\nRelative entropy is, for a variety of reasons detailed in [2, ch.2], a very natural measure of discrepancy to use between a distribution and an approximation to it. Furthermore, and in contrast to \u00a32, it behaves very reasonably with respect to conditioning:"}, {"heading": "Factl For any t, Ep(t>[D[Orh[uC\u2022t)JIIOrh[u(tt)]]] :=:;", "text": "D[uC\u2022t) II uC\u2022t)], where p(t) = (uC\u2022t) 0) is the prior on the response at time t.\nUnfortunately, we seem to have simply shifted the problem from one place to another. While relative entropy is better behaved with respect to 0, there is no known contraction re sult forT. Indeed, until now, the only related properties that seem to have been known [2, p.34] are that relative entropy never increases by transition through a stochastic process (i.e., that D[T[u(t\u2022)JIIT[uCt\u2022)]] :::; D[uCt\u2022)II&Cttl]), and that it ultimately tends to zero for a very broad class of processes (i.e., D[Tk[uCt\u2022)]IITk[uCt\u2022)]] _,. 0 ask_,. oo when T is ergodic). Unfortunately, we need much stronger results if we wish to bound the accumulation of the error over time. One of the main contributions of this paper is the first proof (to our knowledge) that a stochastic transition contracts relative entropy at a geometric rate.\nWe now prove that a stochastic process T does, indeed, lead to a contraction in the relative entropy distance. It will be useful later on (for the case of DBNs) to consider a some what more general setting, where the sets of states 'before' and 'after' the stochastic transition are not necessarily the same. Thus, let Q = {w1, ... ,wn} be our anterior state space, and Q' = { w\ufffd, ... , w\ufffd,} be our ulterior state space. Let Q be ann x n' stochastic matrix, representing a random\nprocess from Q to Q'. Let cp and 1/J be two given anterior distributions over Q, and let cp' and 1/J' be the corresponding ulterior distributions induced over Q' by Q. Our goal is to measure the minimal extent to which the process Q causes the two distributions to become the same. In the worst case, there is no common starting point at all: all of the mass in one distribution cp is on some state w;, while all of the mass in the other distribution 1/J is on some other state w;2\u2022 However, the stochastic nature of the process causes each of them to place some weight on any posterior state wj: the probability cp'[wj] is Q[wj I w;,] while 1/J'(wj] is Q[wj I w;2]. Thus, while none of the probability mass of cp and 1/J was in agreement, cp' and 1/J' must agree on wj for a mass of min[Q[wj I w;,], Q[wj I w;2]]. Based on this insight, we have the following natural characterization of the mixing properties of our process:\nDefinition 3 For a Markov process with stochastic transi tion model Q, the minimal mixing rate of Q is\nn' IQ \ufffd min;1,i2'I: min[Q[wj I w;,], Q[wj I W;2]]. D\ni=I\nWe will show that stochastic propagation by Q is guaranteed to reduce relative entropy by a factor of I - IQ. The proof relies on the following lemma, which allows us to isolate the probability mass in the two distributions which is guaranteed to mix:\nLemma 2 For any 1 :::; IQ and any given cp and 1/J, the matrix Q admits an additive contraction decomposition Q = Qr + Q\"', where Qr and Q\"' are non-negative matrices such that, for all i, I:j \ufffd 1 QL = I\u00b7 and for all j, 2:::7=1 <,o[wi] Q[,j = 2:::7=1 1/J(wi] Qf,j.\nIntuitively, we have argued that, for at least a certain por tion of their probability mass, <p1 and 1/J' must agree. The matrix Qr captures that \"portion\" of the transition that uni fies the two. The two conditions state that this unification must happen with probability at least IQ, and that the two distributions must, indeed, agree on on that portion.\nBased on this lemma, our contraction result now follows easily. Essentially, the argument is based on a construction that makes explicit the different behavior of the process cor responding to the two parts of the contraction decomposi tion Qr, Q\"'. As suggested on Figure 2, we split the process into two separate phases: in the first, the process \"decides\" whether to contract, and in the second the appropriate tran sition occurs. That is, we define a new intermediate state space nt, which contains a new distinguished contraction state c and otherwise is identical to Q. We separate out the cases in which the process is guaranteed to contract cp and 1/J by having them correspond to an explicit transition to c. From c, the two processes behave identically, according to Qr, while from the remaining states in nt' they behave according to Q\"'. We are now in a position to prove our contraction theorem:\nTheorem 3 For Q, cp, 1/J, cp', 1/J' as above: D(cp'II'I/J']:::; (1 - \ufffd(Q)D(cpii'I/JJ.\nn n\u00b7 (a)\nFigure 2: Decomposition used in Theorem 3: (a) generic Markov transition process; (b) two-stage process equivalent to it for cp and TjJ. Here, arrows denote stochastic transitions between states.\nProof Fix cp and 1/J, and define a new two-phase process with one Markovian transition wr from Q to Qt and an other W\"' from nt to Q', where nt = {u!, ... , Un , c} is a new state set (see Figure 2). Intuitively, the state u; corre sponds to w; while the state c corresponds to the state ob tained if the process contracts. The process wr transitions to the contraction state with probability 1 and preserves its state with probability 1 - 1; i.e., wr(w; \"'-> c) = 1 while wr(w; \"'-> u;] = 1 - 1\u00b7 The process W\"' behaves like Q\"' from the states u;; from c, it duplicates the aggregate be havior of cp on Qr, i.e., W\"'[ u; \"'-\" wj] = Qt,j / ( 1 -1) while W\"'[c,wj] = Li cp[wi] QLh\u00b7 It is easy to show that the decomposed process is equivalent to the original process Q for cp and 1/J (this will not be the case for other distributions). To showthatD(cp'll 1/J'] :::; ( 1- I)D(cp II 1/J], we note thatD(cpt II 1/Jt] = (1-I)D(cp II 1/J]. As W\"' is Markovian, we also have D(cp'll 1/J'] :::; D(cpt II 1/J t], hence the claim. I\n4 Compound processes\nOur results so far apply to any discrete state Markovian pro cess. However, their potential impact is greatest in cases where an explicit representation of the belief state is infea sible. DBNs, which allow very complex processes to be specified compactly, often fall into this category. How well do our results apply to DBNs? Unfortunately, the answer is somewhat discouraging, even for very highly structured DBNs. As an extreme example, imagine a process com posed of N binary variables evolving independently, flip ping their value each time slice with probability 8. Each variable, viewed as a separate Markov process, has a mix ing rate of 28. Thus, one may expect the mixing rate of the process as a whole to be as good; indeed, since all of the processes are independent, we could hardly expect otherwise.\nHowever Theorem 3 tells a different story: computing 1 for the transition matrix of the compound process as a whole, one gets a discouragingly small value which is :::; ( 48)NI2. Is our definition of the mixing rate simply too pessimistic? Unfortunately not. The fallacy is in our assumption that local mixing properties would automatically carry over to the compound process. Each subprocess is rapidly mixing for belief states over its own variable only. If the belief state of the compound process involves dependencies be tween variables belonging to different subprocesses, then our contraction rate can, indeed, be as bad as our predic tion. Assume, for example, that the true distribution cp\ngives probability 1 to the state (i.e., assignment of values to variables) {0, ... , 0) while the approximate distribution 1/J gives probability p to that state and 1 - p to its opposite { 1, ... , 1). We can view the state space as a hypercube, and each of these distributions as a mass assignment to vertices of the hypercube. A single step through the transition ma trix \"diffuses\" the mass of the two distributions randomly around their starting points. However, the probability that the diffusion process around one point reaches the other is exponentially low, since all bits have to flip in one or the other of the two distributions.\nOne approach to improving our results for decomposed pro cesses is to make some additional assumptions about the structure of the belief state distributions, e.g., that they de compose. Clearly, such an assumption would be unreason able for the true belief state, as its lack of decomposability was the basis for our entire paper. Somewhat surprisingly, it suffices to make a decomposability assumption only for the approximate belief state. That is, we can show that if the process decomposes well, and the estimated belief state decomposes in a way that matches the structure of the pro cess, then we get significantly better bounds on the error contraction coefficient, regardless of the true belief state. Thus, as far as error contraction goes, the properties of the true belief state are not crucial: only the approximate be lief state and the process itself. This is very fortunate, as it is feasible to enforce decomposability properties on the approximate belief state, whereas we have no control over the true belief state.\nFormally, it is most convenient to describe our results in the framework of factored HMMs [ 17]; in the next section, we discuss how they can be applied to dynamic Bayesian networks. We assume that our system is composed of sev eral subprocesses 7i. Each subprocess has a state with a Markovian evolution model. The state of subprocess l at timet is written s?). The evolution model 7i is a stochastic mapping from the states of some set of processes at timet to the state of process l at timet + 1. We say that subprocess l depends on subprocess l' if7i depends on the value of s\ufffdt). Our model also allows a set of response variables at each time t, which can depend arbitrarily on the states of the processes at timet; however, as we are interested primarily in the contraction properties of our system, the properties of the response variables are irrelevant to our current analysis.\nWe begin by considering the simple case where our subpro cesses are completely independent, i.e., where subprocess l depends only on subprocess l. We also assume that our approximate belief state decomposes along the same lines. Clearly, this case is not interesting in itself: if our subpro cesses are really independent, our belief state would never become correlated in the first place, and we would not need to approximate it. However, this analysis lays the founda tion for the general case described below.\nTheorem 4 Let 'I-t, ... , TL be L independent subprocesses, let'\"'(l be the mixing rate of7;, andlet1 = minrn. Let <p and t/J be distributions over sit)' ... ' sr)' and assume that the sft) are marginally independent in t/J. Then D[<p'jj t/J') :S (1-!)D[<pllt/JJ.\nThus, if we have a set of independent subprocesses, each of which contracts, and our approximate belief state de-\ncomposes along the same lines as the process, then the contraction of the process as a whole is no worse than the contraction of the individual subprocesses. Since each sub process involves a much smaller number of states, its transi tion probabilities are likely to be reasonably large (assuming it is stochastic enough). This analysis usually results in a much better mixing rate.\nNow consider the more general case where the processes interact. Assume that subprocess l depends on subprocesses l1, ... , lf<. Then, 7i defines a probability P(Sr I Sr1, \u2022 \u2022 \u2022 , Srk ). This transition probability can be defined as a transition matrix, but one whose anterior and ulterior state spaces can be different. Luckily, in Section 3, we allowed for exactly this possibility, so that the mixing rate of 7i is well-defined. Let 11 be the mixing rate of 7i, and let 1 = minm. If our approximate belief state, as before, respects the process structure, then we can place a bound on the mixing rate of the entire process. This bound depends both on 1 and on the process structure, and is given by the following theorem.\nTheorem 5 Consider a system consisting of L subpro cesses 'I-t, ... , TL, and assume that each subprocess depends on at most r others; each subprocess influences at most q others; and each 7i has minimum mixing rate 11 :::: 'Y\u00b7 Let <p and t/J be distributions over s}t), ... , sit), where the s?) are independent in t/J. Then: Then D[<p'll t/J'J ::; (1-!*)D[<pllt/JJ, where1* = (!/r)1.\nProof idea We illustrate the basic construction for a simple example; generalization to arbitrary structures is straightforward. Consider two processes Tx and Ty as above, and assume that Ty depends on Tx. Our basic con struction follows the lines of the proof of Theorem 3: we split the transition of each process into two phases where the first chooses whether or not to contract and the second con cludes the transition in a way that depends on whether the process has contracted. Note, however, that the variable X plays a role both in Tx and in Ty, and that the transitions of these two subprocesses are conditionally independent given X. Thus, X cannot make a single decision to contract and apply it in the context of both processes. We therefore introduce two separate intermediate variables for Tx, X\ufffd and X\ufffd, where the first decides whether X contracts in the context of Tx and the second whether it contracts in the context of Ty. Our decomposition is as follows: ')\ufffd:\u00b7\u00b7\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 \ufffd,[ \ufffd '\n'\u00b7.\ufffd ) ... \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 . . . '\u00b7 .: \ufffd..: \u00b7\u00b7-\u2022 \u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022\u2022 \u2022\u2022\u2022 \u2022 \u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022 \u2022 \u2022\u2022\u2022\u2022 \u2022\u2022 . \u2022 \u2022 \u2022 \u2022 (?\" '=---=::.::-:.-:.7.7'.\":'\u00b7':\".;-.-:-.\ufffd \u2022...... . -\u00b7\u00b7\u00b7 \u00b7 t t+l t t+l\nOur transitions for the partitioned model are defined so as to make sure that the parts of the two-phase processes marked with the dotted lines induce the same behavior as their cor responding one-phase processes,for the distributions <p[X] and t/J[X]. (As in the proof of Theorem 3, this equivalence only holds for this pair of distributions.) The construction is essentially identical to that of Theorem 3, except that that the transition from X to the distinguished contraction state c\u00a7 of X\ufffd is taken with some probability .X1 to be specified\nbelow.\nTo get equivalence for the Ty process, we must deal with a slight subtlety: The transition from X\ufffd, yt to Y' must behave as if the first phase contracted if either of the two antecendents X\ufffd, yt is in the contraction state. (There is no choice, because if even one is in the contraction state, the process no longer has enough information to transition according to Q\ufffd .) Thus, if X ---> X\ufffd contracts with probability Az andY ---> yt with probability AJ, the process as a whole contracts with probability 1 - (I - Az)(I - A3). Therefore, in order for us to be able to construct a contraction decomposition Q\ufffd, Q\ufffd, we must select AJ , Az, A3 so as to satisfy AJ ::; IQx and 1- (I- Az)(I- AJ)::; IQy. Assuming that IQx = IQy = 1, we have that Az = A3 = I - VJ=7\" is one legitimate selection; observe that I - .;r=-7 2: 1/2. To analyze the actual contraction rate of the process as a whole, we first analyze the contraction from the initial variables X, Y to the intermediate variables X\ufffd, X\ufffd, Y t; the contraction for the two phases is no smaller. Our analy sis here uses a somewhat different process structure-the one shown using the dashed lines-than the one used to show the correctness of the partition. These processesWx from X to {X\ufffd, X\ufffd} and Wy from Y to yt_are independent (by design). Furthermore, we have assumed that X andY are independent in 1/J. Thus, the conditions of Theorem 4 apply1 and the contraction of the process from X, Y to X\ufffd, X\ufffd, yt is the minimum of the contraction of Wx and of Wy. Straightforwardly, the contraction of Wy is AJ. However, Wx contracts-loses all information about its original state-only when both X x and X y enter their contraction state. These events are independent, hence the probability that they both occur is the product AJAz. In general, if a subprocess Q1 has a contraction ratio 11 and depends on r1 subprocesses, we have to choose the contraction factor A for each influencing process (assum ing for simplicity that all are chosen to be equal) to be 1- \\YI -11 2: 1/r, resulting in a linear reduction in the contraction rate. On the other hand, each influence of a sub process Ql on another subprocess involves the construction of another intermediate variable, each of which contracts independently. The total contraction of Wx, is the prod uct of the individual contractions, which is 1* = A q, an exponential reduction. Putting it all together, the overall contraction rate admits the lower bound 1* = ( 1/ r )q. I\nWe see that interconnectivity between the processes costs us in terms of our contraction ratio. The cost of \"incom ing\" connectivity is a linear reduction in the contraction rate whereas the cost of \"outgoing\" connectivity is exponential. Intuitively, this phenomenon makes sense: if a process in fluences many others, it is much less likely that its value will be lost via contraction.\n1The theorem was stated for processes where the anterior and ulterior state space is identical, but the same proof applies to the more general case.\n5 Efficient monitoring\nAs we suggested throughout the paper, one of the main applications of our results is to the task of monitoring a complex process. As we mentioned, the exact monitoring task is often intractable, requiring that we maintain a very large belief state. We proposed an alternative approach, where we maintain instead an approximate belief state. As we now show, our contraction results allow us to bound the cumulative error arising from such an approximate monitor ing process. We also investigate one particular application of this approach, in the contex of DBNs.\n5.1 Approximate monitoring\nRecall the notation of Section 2 about belief states, and denote by iT(t) our compactly represented approximate be lief state at time t. The choice of compact representa tion depends on the process. If, for example, our pro cess is composed of some number of weakly interacting subprocesses-e.g., several cars on a freeway-it may be reasonable to represent our belief state at a given time using our marginal beliefs about its parts (e.g., individual vehi cles). In a hybrid process, as we said, we may want to use a fixed-size mixture of Gaussians.\nThe approximate belief state 17(t) is updated using the same process as u(t\u2022): we propagate it through the transition model, obtaining &( \u2022t+l), and condition on the current response, obtaining o-(t+l\u2022). However, o-(t+J.) does not usually fall into the same family of compactly-represented distributions to which we chose to restrict our belief states. In order to maintain the feasibility of our update process, we must approximate o-(t+l\u2022), typically by finding a \"nearby\" distribution that admits a compact representation; the result is our new approximate belief state u(t+l). In our free way domain, for example, we may compute our new beliefs about the state of each vehicle by projecting o-(t+l\u2022), and use the cross product of these individual belief states as our approximation; the resulting distribution is the closest (in terms of relative-entropy) to o-(t+l\u2022). In our continuous process, we could project back into our space of allowable belief states by approximating the distribution using a fixed number of Gaussians.\nWe begin by analyzing the error resulting from this type of strategy, i.e., the distance between the true posterior belief state u(t+h) and our approximation to it u(t+l). Intuitively, this error results from two sources: the \"old\" error which we \"inherited\" from the previous approxima tion iT(t), and the \"new\" error derived from approximating o-(t+l\u2022) using u(t+l). Suppose that each approximation in troduces an error of c, increasing the distance between the exact belief state and our approximation to it. However, the contraction resulting from the state transitions serves to drive them closer to each other, reducing the effect of old errors by a factor of f. The various observations move the two even closer to each other on expectation (averaged over the different possible responses). Therefore, the ex pected error accumulated up to time t would behave as c + ( I - I) c + ... + ( I - I) t- I c ::; c Li ( I - I) i = c h. To formalize this result, we first need to quantify the error\nresulting from our approximation. Our new approximate belief state a-Ct) is an approximation to uCt\u2022). Most obvi ously, we would define the error of the approximation as the relative entropy distance between them-D(u(t\u2022) II a-Ct)]. However, our error is measured relative to uCt\u2022) and not to u(t\u2022). Therefore, we use the following definition:\nDefinition 4 We say that an approximation a-Ct) of uCt\u2022) incurs error c: relative to u(a) if\nD(u(t\u2022) II a-Ct)]- D[u(t.) II u(t\u2022)] \ufffd c. D\nOur last theorem now follows easily by induction on t:\nTheorem 6 Let T be a stochastic process whose mixing rate is /, assume that we have an approximation scheme that, at each phase t, incurs error c: relative to uCt\u2022). Then for any t, we have:\nEp(I, ... ,t) [D(u(a) II a-Ct)]] \ufffd c:!J,\nwhere the expectation is taken over the possible response sequences rh1, \u2022\u2022\u2022 , rh., with the probability ascribed to them by the process T.\nOf course, it is not trivial to show that a particular ap proximation scheme will satisfy the accuracy require ment of c:, essentially because its definition involves the true belief uCt\u2022), which is usually not known. Nev ertheless, a sufficient condition for this requirement is max;ln [u(t\u2022)[s;]]/if(t)[s;] \ufffd c:; and the left-hand side quantity-the maximum log relative error caused by the approximation scheme at time t-is often easy to assess for a given approximation step.\nLet us understand the guarantees provided by this theorem. First, the bound involves relative entropy between the two entire belief states. In certain applications, we may be in terested in errors for individual variables or for subsets of variables. Fortunately, any bound on the entire distribution immediately carries over to any projection onto a subset of variables [2]. Furthermore, we note that bounds on rel ative entropy immediately imply bounds on the L, error, as I IIP -1/llh \ufffd (2ln2D[\ufffdPII1/1])112. Second, note that the bounds are on the expected error; the error for specific se quences of evidence are much weaker. In particular, the error after a very unlikely sequence of evidence might be quite large. Fortunately, our contraction result holds for arbitrary distributions, no matter how far apart. Thus, even if momentarily uCt\u2022) and a-(t) are very different, the con traction property will reduce this error exponentially.\n5.2 Monitoring in DBNs\nWe now consider a specific application of this general ap proach. We consider a process described as a DBN, and a factored belief state representation (where certain subsets of variables are forced to be independent). In this case, the process is specified in terms of an ordered set of state vari ables X1, \u2022.. , Xn. The probability model of a DBN is typ ically described using a 2-TBN (a two time-slice temporal Bayesian network), as shown in Figure 1. The 2-TBN asso ciates each variable with a conditional probability distribution P[Xkt+i) I Parents(Xkt+l))], where Parents(Xkt+'))\ncan contain any variable at time t and such variables at time t + 1 that preceed xk in the total ordering. This model represents the conditional distribution over the state at timet+ 1 given the variables at timet. Let C be the set {X: X(t) E UkParents(Xkt+l))}. To capture the idea of a subprocess, we partition the set C into disjoint subsets X 1, .\u2022. , XL. Our partition must satisfy the requirement that no X 1 may be affected by another X, within the same time slice; i.e., if X E X1, then X(t+l) cannot have as an ancestor a variable y(t+l) for Y E X1, # X I\u00b7 Note that a time slice may also contain non-persistent variables, e.g., sensor readings; but since none of them may ever be an ancestor of a canonical variable, we allow them to depend on any persistent variable in their time slice. Since the various clusters X 1 correspond to our subprocesses from Theorem 5, we shall maintain an approximate belief state in which the X 1 are independent, as prescribed. The approximate monitoring procedure for DBNs follows the same lines as the general procedure described in Sec tion 2: At each point in time, we have an approximate belief state a-C 1), in which the X 1 are all independent. We propa gate a-(t) through the transition model, and then condition the result on our observations at time t + 1. We then compute a-Ct+ I ) by projecting u(t+i\u2022) onto each X1; i.e., we define a-(t+l)[X1] = &(t+l\u2022)[X1], and the entire distribution as a product of these factors.\nIn the case of DBNs, we can actually accomplish this up date procedure quite efficiently. We first generate a clique tree [13] in which, for every /, some clique contains X\ufffdt) and some clique contains X \ufffdt+l). A standard clique tree propagation algorithm can then be used to compute the pos terior distribution over every clique. Once that is done, the distribution over x \ufffdt+l) is easily extracted from the appro priate clique. Further savings can be obtained if we assume a stationary DBN and a static approximation scheme.\nIn order to apply this generic procedure to a particular prob lem, we must define a partition of the canonical variables, i.e., choose a partition of the process into subprocesses. Our analysis in the previous sections can be used to evaluate the alternatives. The tradeoffs, however, are subtle: Subpro cesses with a small number of state variables allow more efficient inference. They also have a smaller transition ma trix and therefore their mixing rate is likely to be better. On the other hand, our subprocesses need to be large enough so that there are no edges between subprocesses within a sin gle time slice. Furthermore, making our subprocesses too small increases the error incurred by the approximation of assuming them to be independent. Specifically, if we have two (sets of) variables that are highly correlated, splitting them into two separate subprocesses is not a good idea. Our experimental results illustrate these tradeoffs.\n5.3 Experimental results\nWe validated this algorithm in the context of two real-life DBNs: the WATER network [8], used for monitoring the biological processes of a water purification plant; and the BAT network [6], used for monitoring freeway traffic (see Figure 3). We added a few evidence nodes to WATER, which did not have any; these duplicate a few of the state variables\nslice t slice t+l evidence slice l slice t+ l evidence\nFigure 3: The canonical fonn 2-TBNs for the DBNs used in our experiments: (a) the BAT network; (b) the WATER network. The dotted lines indicate some of the clusterings used in our approximations.\nwith added noise. The experimentation methodology is as follows: starting from the same initial prior (the uniform prior), we monitor the process evolution using our approxi mate method with some fixed decomposition, and compare it at every t to the exact inference, which is emulated by using our algorithm with the trivial partition (all canonical state variables in a single cluster). Observations are simu lated by sampling the evidence variables according to the exact distribution.\nFigure 4(a) shows the evolution of relative entropy for the BAT network on a typical run, using the shadowed nodes as evidence nodes. In this network, the belief state consists of 10 variables roughly partitioned in two weakly interact ing groups of 5: we choose this \"5+5\" partition for our approximation scheme. On an UltraSparc 2, the approxi mate monitoring took about 0.11 seconds per time slice, as compared to 1.72 for the exact inference, yielding a 15-fold speedup. In terms of accuracy, the error averages at 0.0007, remaining very low most of the time with, as we would ex pect, a few sparsely distributed spikes to somewhat larger values, peaking at 0.065. We also note that it does not appear to grow over the length of the run, as predicted by our analysis. Since in practical applications the emphasis is often on a few selected variables, we also computed the L1 errors for the beliefs over the two variables 'LateralAction' and 'ForwardAction' (i.e., the belief states marginalized over each of them). Their qualitative pattern was roughly similar to Figure 4(a), they respectively averaged 0.00013 and 0.0019, and remained bounded by 0.02 and 0.07 over the 1000-step run of our experiment.\nSimilar tests were conducted on the WATER network shown on Figure 3(b), and using a decomposed belief over the 3 clusters A-B, C-D-E-F, and G-H. Over a run of length 3000, the error remained bounded by 0.06 with the exception of one outlier to 0.14, and averaged 0.006 over that run. Running times were 0.19 sec/slice for the approximation, vs. 6.02 sec/slice for the reference (a 31-fold speedup).\nTo investigate the effect of our approximate belief state rep resentation, we ran the following experiment on three dif ferent projections: the \"5+5\" clustering already introduced, a \"3+2+4+ 1\" clustering obtained by further breaking down the 5-clusters (see Figure 3(a)), and a \"3+3+4\" clustering chosen irrespectively of the network topology. 2\nFirst, we used our theoretical tools to predict how well each\nof these would perform, compared with each other. The two determining factors here are the stepwise approximation er ror c and the overall mixing rate bounded by 1* . The former is directly linked to the expressiveness of the approxima tion; so according to this criterion we have in decreasing order of quality: \"5+5\" \ufffd \"3+3+4\"!:: \"3+2+4+1\". To as sess the mixing rates, we computed the vector f of mixing rates for all the clusters, and used it together with the topo logical quantities q and r to calculate 1* from Theorem 5. For the \"5+5\" clustering, this gave f = (0.00040, 0.0081), r = 2,q = 2,and thus\")'* = (/min/r)q = 4x10-8. For the \"3+2+4+ 1\" clustering, f = (0.00077, 0.080, 0.0081, 0.96), r = 3, q = 2, so/* = 7 X w-8\u2022 For the \"3+3+4\" clus tering, f = (0.0022, 0.020, 0.0034), r = 3, q = 3, so \")'* = 4 X w-10. For this criterion, we get \"3+2+4+1\" \ufffd \"5+5\" \ufffd \"3+3+4\": the latter is heavily penalized by the higher inter-connectivity of its clusters.\nThe second step was to compare this with the actual mon itoring accuracy of our algorithm in each case. The results are shown in Figure 4(b) (averaged over 8 different runs and plotted on logarithmic scale). The \"5+5\" clustering (lower curve) has an average error of 0.0006, its error is always lower than the \"3+2+4+1\" clustering (medium curve) for which the error averages 0.015. Both are clearly superior to the\"3+3+4\" clustering, for which the average error reaches 0.13.3 We observe that the errors obtained in practice is significantly lower than that predicted by the theoretical analysis. However, qualitatively, the behaviour of the dif ferent clusterings is consistent with the predictions of the theory. In particular, the quality of a clustering is very sensitive to its topology, as expected.\nInterestingly, the accuracy can be further improved by us ing conditionally independent approximations. In the WA TER network, for example, using a belief state decomposed into the overlapping clusters A-B-C-D-E, C-D-E-F-G, G-H yields, for the same sequence of observations as above, an\n2Strictly speaking, the \"3+2+4+1\" and \"3+3+4\" clusterings contain clusters that are not subprocesses according to our defi nition, as there are edges between clusters within the same time slice. However, it is impossible to find smaller clusterings in the BAT network that satisfy this restriction, and the empirical results still serve to illustrate the main points.\n3The speedups relative to exact inference were respectively 15, 20, 20 for the \"5+5\", \"3+2+4+ 1 \", \"3+3+4\" clusterings.\naverage error of just 0.0015. Also, the error now remains bounded by 0.018 throughout, reducing the maximum er ror by a factor of 8. Approximate inference here took 0.47 sec/slice, a 13-fold speedup over exact inference.\nWe also tested the effect of evidence on the quality of our approximation. Without evidence, the error curve is very smooth, and converges relatively soon to a constant error, corresponding to the error between the correct sta tionary distribution of the process and our approximation to it. With evidence, the error curve has a much higher variance, but its average is typically much lower, indicat ing that evidence further boosts contraction (as opposed to merely being harmless). The effect of evidence was more beneficial with better clusterings.\n6 Conclusion and extensions\nWe investigated the effect of approximate inference in a stochastic process. We showed that the stochastic nature of the process tends to make errors resulting from an approxi mation disappear rapidly as the process continues to evolve. Our analysis relies on a novel and important theorem prov ing a contraction result for relative entropy over stochastic processes. We also provided a more refined analysis for processes composed of weakly interacting subprocesses.\nWe applied the idea of approximate inference to the mon itoring task. Exact inference for this task is infeasible in complex stochastic processes with a large number of states. Even if the process is highly structured, exact inference (e.g., [11]) is forced into intractability by the full correla tion of the belief state that invariably occurs. We propose an approach whereby the algorithm maintains an approxi mate belief state with compact representation. This belief state is propagated forward from one time slice to the next, and the result is approximated so as to maintain a compact representation. Our analysis guarantees that the errors from our approximations do not accumulate.\nThis approach provides a general framework for approxi mate inference in stochastic processes. We also presented one instantiation of this general framework to the class of structured processes composed of weakly interacting sub processes. For this case, we provide a more refined analysis which results in significantly lower error bounds. We also presented experimental results showing that our approach works extremely well for real-life processes. Indeed, we get order of magnitude savings even for small processes, at\nthe cost of a a very low error in our approximation. For larger processes, we expect the savings to be much greater. Both our theoretical and our empirical results clearly show that our ability to execute accurate effective inference for a stochastic process depends heavily on its structure. Thus, our result is the first to show how the structure of a stochastic process can be exploited for inference.\nThere has been fairly little work on approximate inference in complex temporal models. 4 The early work of [ 14] consid ers a simple approach of using domain knowledge to simply eliminate some of the variables from each time slice. The random sampling approach of [ 1 0] can also be viewed as maintaining an approximate belief state, albeit one repre sented very naively as a set of weighted samples. The recent work of [12] extends this idea, using the random samples at each time slice as data in order to learn an approximate belief state. Both of these ideas can be viewed as falling into the framework described in this paper. However, nei ther contains any analysis nor an explicit connection to the structure of the process.\nThe recent work of [7] and [16] utilize mean field approxi mation in the context of various types of structured HMMs. Of these approaches, [7] is the closest to our work (to the part of it dealing with structured processes). In their ap proach, the compound process is approximated as being composed of independent subprocesses, whose parameters are chosen in a way that depends on the evidence. This approach differs from ours in several ways. First, it applies only to situations where the subprocesses are, in fact, inde pendent, and only become correlated due to observations. Our work applies to much richer models of partially de composed systems. Second, their approximation scheme is based on making the subprocesses independent throughout their length, whereas our approach accounts for the cor relations between the subprocesses at each time slice, and then marginalizes them out. Finally, their analysis proves that their approximation is the closest possible to the true distribution among the class of distributions represented by independent processes; our analysis, by contrast, provides explicit bounds (in expectation) on the total error.\n4Some approximate inference algorithms for nontemporal Bayesian networks can also be applied to this task. Specifically, the mini-bucket approach of [5] is somewhat related to ours, as it also uses a (different) form of factoring decomposition during the course of inference. However, none of the potentially relevant algorithms are associated with any performance guarantees on the error of approximation over time.\nOne important direction in which our results can be ex tended relates to different representations of the belief state and of the process. Our current analysis requires that the belief state representation make the states of the subpro cesses completely independent. Clearly, in many situations a more refined belief state representation is more appro priate. For example, as our experiments show, it can be very beneficial to make the states of two processes in our approximate belief state conditionally independent given a third; we would like to derive the formal conditions under which this is indeed an advantage. The results of [12] also demonstrate that other representations of a belief state might be useful, specifically ones that allow some context sensitivity of the structure of the distribution. They also show that it is beneficial to allow the structure of the belief state representation to vary over time, thus making it more appropriate to the specific properties of the current belief state. In general, our basic contraction result applies to any approximation scheme for belief state representation; we intend to experiment with different alternatives and evalu ate their contraction properties. Finally, we would like to apply our analysis to other types of processes requiring a compact representation of the belief state, e.g., processes with a continuous state space.\nAnother priority is to improve our analysis of the effect of evidence. Our current results only show that the evidence does not hurt too much. Our experiments, however, show (as we would expect) that the evidence can significantly reduce the overall error of our approximation.\nThere are many other tasks to which our techniques can be applied. For example, our analysis applies as is to the task of predicting the future evolution of the system. It also applies to tasks where the transition model depends on some action taken by an agent. Our contraction analysis is done on a phase-by-phase basis, and therefore does not require that the transition model be the same at each time slice. Thus, so long as the transition associated with each action is sufficiently stochastic, we can use our technique to predict the effects of a plan or a policy. A more speculative idea is to the task of constructing policies for POMDPs; there, a policy is a mapping from belief states to actions. Perhaps our more compact belief state representation will turn out to be a better basis for representing a policy.\nWe hope to generalize our contraction result to the case of reasoning backwards in time, allowing us to apply approxi mate inference to the task of computing our beliefs about a time slice given both past and future evidence. This infer ence task is the crucial component in algorithms that learn probabilistic models of stochastic processes from data. The learning process requires many iterations of this inference task, so that any improvement in efficiency could have ma jor impact. Furthermore, by showing that the influence of approximations in the future cannot significantly affect our beliefs about the past, we could obtain theoretical support for the simple and natural algorithm that computes beliefs about a time slice from a fairly small window on both sides. The applications of this idea to online learning are clear.\nAcknowledgements\nWe gratefully acknowledge Eric Bauer, Lise Getoor, and Uri Lerner for work on the software used in the experi-\nments, Raya Fratkina for help with the network files, and Tim Huang for providing us with the BAT network. Many thanks to Tom Cover, Nir Friedman, Leonid Gurvits, Alex Kozlov, Uri Lerner, and Stuart Russell for useful discussions and comments. This research was supported by ARO un der the MURI program \"Integrated Approach to Intelligent Systems\", grant number DAAH04-96-1-034I , by DARPA contract DACA 76-93-C-0025 under subcontract to Infor mation Extraction and Transport, Inc., and through the gen erosity of the Powell Foundation and the Sloan Foundation.\nReferences\n[ I ] K.J. Astrom. Optimal control of Markov decision processes with incomplete state estimation. J. Math. Anal. Applic. , I O, I 965.\n[2] T. Cover and J. Thomas. Elements of Information Theory. Wiley, 1991.\n[3] P. Dagum, A. Galper, and E. Horwitz. Dynamic net work models for forecasting. In Proc. UAI, 1992.\n[4] T. Dean and K. Kanazawa. A model for reasoning about persistence and causation. Comp. Int., 5(3), 1989.\n[5] R. Dechter and I. Rish. A scheme for approximating probabilistic inference. In Proc. UAI, 1997.\n[6] J. Forbes, T. Huang, K. Kanazawa, and S.J. Russell. The BATmobile: Towards a Bayesian automated taxi. In Proc. IJCAI, 1995.\n[7] Z. Ghahramani and M.I. Jordan. Factorial hidden Markov models. In Proc. NIPS, 1996.\n[8] F.V. Jensen, U. Kjrerulff, K.G. Olesen, and J. Ped ersen. An expert system for control of waste water treatment-a pilot project . Technical report, Judex Datasystemer A/S, Aalborg, 1989. In Danish.\n[9] R.E. Kalman. A new approach to linear filtering and prediction problems. J. of Basic Engineering, 1960.\n[ IO] K. Kanazawa, D. Koller, and S.J. Russell. Stochastic simulation algorithms for dynamic probabilistic net works. In Proc. UAI, I 995.\n[ I I ] U. Kj rerulff. A computational scheme for reasoning in dynamic probabilistic networks. In Proc. UAI, I 992.\n[ I2] D. Koller and R. Fratkina. Using learning for approx imation in stochastic processes. In Proc. ML, 1998.\n[ 1 3] S.L. Lauritzen and D.J. Spiegelhalter. Local compu tations with probabilities on graphical structures and their application to expert systems. J. Roy. Stat. Soc. , B 50, 1988.\n[14] G. Provan. Tradeoffs in constructing and evaluating temporal influence diagrams. In Proc. UAI, 1992.\n[15] L. Rabiner and B. Juang. An introduction to hidden Markov models. IEEE Acoustics, Speech & Signal Processing, 1986.\n[16] L.K. Saul and M.l. Jordan. Exploiting tractable sub structure in intractable networks. In Proc. NIPS, 1995.\n[17] P. Smyth, D. Beckerman, and M.I. Jordan. Prob abilistic independence networks for hidden Markov probability models. Neural Computation, 9(2), 1996."}], "references": [{"title": "Dynamic net\u00ad work models for forecasting", "author": ["P. Dagum", "A. Galper", "E. Horwitz"], "venue": "In Proc. UAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "A model for reasoning about persistence and causation", "author": ["T. Dean", "K. Kanazawa"], "venue": "Comp. Int.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "A scheme for approximating probabilistic inference", "author": ["R. Dechter", "I. Rish"], "venue": "In Proc. UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "The BATmobile: Towards a Bayesian automated taxi", "author": ["J. Forbes", "T. Huang", "K. Kanazawa", "S.J. Russell"], "venue": "In Proc. IJCAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Factorial hidden Markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "In Proc. NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Ped\u00ad ersen. An expert system for control of waste water treatment-a pilot project", "author": ["F.V. Jensen", "U. Kjrerulff", "K.G. Olesen"], "venue": "Technical report, Judex Datasystemer A/S,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "A new approach to linear filtering and prediction problems", "author": ["R.E. Kalman"], "venue": "J. of Basic Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1960}, {"title": "Using learning for approx\u00ad imation in stochastic processes", "author": ["D. Koller", "R. Fratkina"], "venue": "In Proc. ML,", "citeRegEx": "Koller and Fratkina.,? \\Q1998\\E", "shortCiteRegEx": "Koller and Fratkina.", "year": 1998}, {"title": "Local compu\u00ad tations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "J. Roy. Stat. Soc. , B", "citeRegEx": "Lauritzen and Spiegelhalter.,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter.", "year": 1988}, {"title": "Tradeoffs in constructing and evaluating temporal influence diagrams", "author": ["G. Provan"], "venue": "In Proc. UAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "An introduction to hidden Markov models", "author": ["L. Rabiner", "B. Juang"], "venue": "IEEE Acoustics, Speech & Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1986}, {"title": "Exploiting tractable sub\u00ad structure in intractable networks", "author": ["L.K. Saul", "M.l. Jordan"], "venue": "In Proc. NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Prob\u00ad abilistic independence networks for hidden Markov probability models", "author": ["P. Smyth", "D. Beckerman", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}], "referenceMentions": [], "year": 2011, "abstractText": "The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief state-a probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the be\u00ad lief state is intractable. We investigate the idea of maintaining a compact approximation to the true be\u00ad lief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approx\u00ad imations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the con\u00ad text of a monitoring task, showing that orders of mag\u00ad nitude faster inference can be achieved with only a small degradation in accuracy.", "creator": "pdftk 1.41 - www.pdftk.com"}}}