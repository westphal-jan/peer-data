{"id": "1302.1538", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "Sequential Update of Bayesian Network Structure", "abstract": "so there is an altogether obvious general need for improving the performance accuracy and conceptual accuracy of a bayesian network as new data is observed. chiefly because of behavior errors included in model construction behaviour and changes identified in describing the decision dynamics information of the various domains, we cannot afford to ignore the desired information in predicting new data. though while constructing sequential update behavior of global parameters for generating a fixed structure can be accomplished using obviously standard techniques, such sequential update of network global structure is really still an currently open problem. in this paper, we explicitly investigate sequential update of different bayesian networks were both behavioral parameters assumed and structure are expected both to progressively change. we introduce at a significantly new approach specifically that furthermore allows software for discussing the flexible manipulation operation of data the tradeoff between the quality of the continually learned networks and the amount of information obtained that is maintained about past observations. we accordingly formally describe completing our approach along including the necessary modifications to the scoring functions known for learning bayesian networks, evaluate its effectiveness through leading an influential empirical study, and extend it to the case solution of any missing data.", "histories": [["v1", "Wed, 6 Feb 2013 15:55:21 GMT  (1219kb)", "http://arxiv.org/abs/1302.1538v1", "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)"]], "COMMENTS": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["nir friedman", "moises goldszmidt"], "accepted": false, "id": "1302.1538"}, "pdf": {"name": "1302.1538.pdf", "metadata": {"source": "CRF", "title": "Sequential Update of Bayesian Network Structure", "authors": ["Nir Friedman", "Moises Goldszmidt"], "emails": ["nir@cs.berkeley.edu", "moises@erg.sri.com"], "sections": null, "references": [{"title": "A tutorial on learning Bayesian net\u00ad works", "author": ["D. Heckerman"], "venue": "Technical Report MSR-TR-95-06, Microsoft Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Learning Bayesian belief networks. An approach based on the MDL principle", "author": ["W. Lam", "F. Bacchus"], "venue": "Comp. lnt. ,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "A new view of the EM algorithm that justifies incremental and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Unpublished manuscript,", "citeRegEx": "Neal and Hinton.,? \\Q1994\\E", "shortCiteRegEx": "Neal and Hinton.", "year": 1994}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. of Stat. ,", "citeRegEx": "Schwarz.,? \\Q1978\\E", "shortCiteRegEx": "Schwarz.", "year": 1978}, {"title": "Sequential up\u00ad dating of conditional probabilities on directed graph\u00ad", "author": ["D.J. Spiegelhalter", "S.L. Lauritzen"], "venue": "ical structures. Networks,", "citeRegEx": "Spiegelhalter and Lauritzen.,? \\Q1990\\E", "shortCiteRegEx": "Spiegelhalter and Lauritzen.", "year": 1990}], "referenceMentions": [], "year": 2011, "abstractText": "There is an obvious need for improving the per\u00ad formance and accuracy of a Bayesian network as new data is observed. Because of errors in model construction and changes in the dynamics of the domains, we cannot afford to ignore the infor\u00ad mation in new data. While sequential update of parameters for a fixed structure can be accom\u00ad plished using standard techniques, sequential up\u00ad date of network structure is still an open problem. In this paper, we investigate sequential update of Bayesian networks were both parameters and structure are expected to change. We introduce a new approach that allows for the flexible ma\u00ad nipulation of the tradeoff between the quality of the learned networks and the amount of informa\u00ad tion that is maintained about past observations. We formally describe our approach including the necessary modifications to the scoring functions for learning Bayesian networks, evaluate its effec\u00ad tiveness through and empirical study, and extend it to the case of missing data.", "creator": "pdftk 1.41 - www.pdftk.com"}}}