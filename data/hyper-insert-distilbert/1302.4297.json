{"id": "1302.4297", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Feature Multi-Selection among Subjective Features", "abstract": "even when dealing with moderately subjective, noisy, unrelated or possible otherwise nebulous features, the \" wisdom task of crowds \" suggests that one may benefit simultaneously from multiple reward judgments'of listing the same feature on the other same measurement object. frequently we give theoretically - inwardly motivated ` estimated feature preference multi - selection'mathematical algorithms worldwide that choose, among a large large set of candidate features, not equally only about which features first to judge but the how terribly many likelihood times to explicitly judge each one. we demonstrate also the effectiveness required of this approach for controlling linear font regression on a crowdsourced window learning task of randomly predicting people's height and observed weight deviation from photos, using preferred features such as'gender ', and'estimated weight'as well as their culturally fraught subjective ones such as'relative attractive '.", "histories": [["v1", "Mon, 18 Feb 2013 15:00:47 GMT  (145kb)", "https://arxiv.org/abs/1302.4297v1", null], ["v2", "Wed, 17 Apr 2013 17:03:56 GMT  (148kb)", "http://arxiv.org/abs/1302.4297v2", "Published in Proceedings of the 30th International Conference on Machine Learning (ICML), 2013"], ["v3", "Tue, 14 May 2013 21:35:25 GMT  (136kb,D)", "http://arxiv.org/abs/1302.4297v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sivan sabato", "adam kalai"], "accepted": true, "id": "1302.4297"}, "pdf": {"name": "1302.4297.pdf", "metadata": {"source": "META", "title": "Feature Multi-Selection among Subjective Features", "authors": ["Sivan Sabato", "Adam Kalai"], "emails": ["SIVAN.SABATO@MICROSOFT.COM", "ADAM.KALAI@MICROSOFT.COM"], "sections": [{"heading": "1. Introduction", "text": "In this paper we consider prediction with subjective, vague, or noisy attributes (which are also termed \u2018features\u2019 throughout this paper). Such attributes can sometimes be useful for prediction, because they account for an important part of the signal that cannot be otherwise captured. In a crowdsourcing setting, the \u201cwisdom of crowds\u201d suggests that including multiple assessments of the same feature by different people may be useful. Henceforth, we refer to assessments of features as judgments. This paper introduces the problem of selecting, from a set of candidate features, which ones to use for prediction, and how many judgments to acquire for each, for a given budget limiting the total number of judgments. We give theoretically justified algorithms for this problem, and report crowdsourced experimental results, in which judgments of highly subjective features (even culturally fraught ones such as attractive) are helpful for prediction.\nAs a toy example, consider the problem of estimating the number of jelly beans in a jar based on an image of the jar. A linear regressor with multiple judgments of features might have the form,\ny\u0302 =0.95(est. number of beans)/5 \u2212 50(round jar)/2+ 100(monochromatic)/1 + 30(beautiful)/3.\nHere, for binary attributes, a/ra \u2208 [0, 1] denotes the fraction of positive judgments out of ra judgments of attribute a. For real-valued attributes, a/ra denotes the mean of ra judgments. The shape, number of colors, and attractiveness of the jar each help correct biases in the estimated number of beans, averaged across five people. Our goal is to choose a regressor that, as accurately as possible, estimates the labels (i.e., jelly bean counts) on future objects (i.e., jars) drawn from the same distribution, while staying within a budget of feature judgment resources per evaluated object at test time. In the example above, notice that even though the monochromatic coefficient is greater than the beautiful coefficient, fewer monochromatic judgments are used, because counting the number of colors is more objective, and hence further judgments are less valuable. While this example is contrived, similar phenomena are observed in the output of our algorithms (see 2).\nWe refer to the problem of selecting the number of repetitions, ra, of each attribute, as the feature multi-selection problem, because it generalizes the feature selection problem of choosing a subset of features, i.e., ra \u2208 {0, 1}, to choosing a multiset of features, i.e., ra \u2208 N. Since the feature selection problem is well known to be NP-hard (Natarajan, 1995), our problem is also NP-hard in the general case. (For a formal reduction, one simply considers the \u201cobjective\u201d case where all judgments of the same featureobject pair are identical.) Nonetheless, several successful approaches have been proposed for feature selection. The algorithms that we propose generalize two of these approaches to the problem of feature multi-selection.\nOur algorithms are theoretically motivated, and tested on synthetic and real-world data. The real world data are photos extracted from the publicly available Photographic Height/Weight Chart1, where people post pictures of themselves announcing their own height and weight.\nAs a more general motivation, consider a scientist who would like to use crowdsourcing as an alternative to themselves estimating a value for each of a large data set of objects. Say the scientist gathers multiple judgments of\n1http://www.cockeyed.com/photos/bodies/heightweight.html\nar X\niv :1\n30 2.\n42 97\nv3 [\ncs .L\nG ]\n1 4\nM ay\n2 01\n3\na number of binary or real-valued attributes for each object, and uses linear regression to predict the value of interest. In some cases, crowdsourcing is a natural source of judgments, as a great number of them may be acquired on demand, rapidly, and at very low cost. We assume the scientist has access to the following information:\n\u2022 A labeled set of objects (o, y) \u2208 O \u00d7 Y (with no judgments), where O is a set of objects and Y \u2286 R is a set of ground-truth labels drawn independently from a distribution D.\n\u2022 A crowd, which is a large pool of workers.\n\u2022 A possibly large set of candidate attributes A. For any attribute a \u2208 A and object o \u2208 O, the judgment of a random worker from the crowd may be queried at a cost.\n\u2022 A budget B, limiting the number of attribute judgments to be used when evaluating the regressor on a new unseen object.\nOur approach is as follows:\n1. Collect k \u2265 2 judgments for each candidate attribute in A, for each object in the labeled set.\n2. Based on this data and the budget, decide how many judgments of each attribute to use in the regressor.\n3. Collect additional judgments (as needed) on the labeled set so that each attribute has the number of judgments specified in the previous step.\n4. Find a linear predictor based on the average judgment of each feature.2\nStep 4 can be accomplished by simple least-squares regression. The goal in Step 2 (feature multi-selection) is to decide on a number of judgments per attribute that will hopefully yield the smallest squared error after Step 4.\nInterestingly, even given as few as k = 2 judgments per attribute, one can project an estimate of the squared error with more than k judgments of some features. We prove that these projections are accurate, for any fixed k \u2265 2, as the number of labeled objects increases. Our algorithms perform a greedy strategy for feature multi-selection, to attempt to minimize the projected loss. This greedy strategy can be seen as a generalization of the Forward Regression approach for standard feature selection (see e.g. Miller, 2002). The first algorithm operates under the assumption that different attributes are uncorrelated. In this case the\n2We focus on mean averaging, leaving to future work other aggregation statistics such as the median.\nprojection simplifies to a simple scoring rule, which incorporates attribute-label correlations as well as a natural notion of inter-rater reliability for each attribute. In this case, greedy selection is also provably optimal. While attributes are highly correlated in practice, the algorithm performs well in our experiments, possibly because Step 4 corrects for a small number of poor choices during feature multiselection. The second algorithm attempts to optimize the projection without any assumptions on the nature of correlations between features.\nWhile crowdsourcing is one motivation, the algorithms would be applicable to other settings such as learning from noisy sensor inputs, where one may place multiple sensors measuring each quantity, or social science experiments, where one may have multiple research assistants (rather than a crowd) judging each attribute.\nThe main contributions of this paper are: (a) introducing the feature multi-selection problem, (b) giving theoretically justified feature multi-selection algorithms, and (c) presenting experimental results, showing that feature multiselection can yield more accurate regressors, with different numbers of judgments for different attributes.\nRelated Work Related work spans a number of fields, including Statistics, Machine Learning, Crowdsourcing, and measurement in the social sciences. A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the \u2018true\u2019 regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements.\nA wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.\nTwo recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments. Some of their attributes are subjective, e.g., soothing. We employ their crowdsourcing protocol to label our binary attributes. Isola et al. (2011) study subjective and objective features for the task of estimating how memorable an image is, by taking the mean of 10 judgments per attribute for each im-\nage. They perform greedy feature selection over these attributes to find the best compact set of attributes for predicting memorability. The key difference between their algorithm and ours is that theirs does not choose how many judgments to average. Since that quantity is fixed for each attribute, their setting falls under the more standard feature selection umbrella. In our experiments we compare this approach to our algorithms.\nFinally, in the social sciences, a wide array of techniques have been developed for assessing inter-rater reliability of attributes, with the most popular perhaps being the \u03b1 coefficient (Cronbach, 1951). A principal use of such measures is determining, by some threshold, which features may be used in content analysis. For an overview of reliability theory, see (Krippendorff, 2012)."}, {"heading": "2. Preliminary assumptions and definitions", "text": "Let there be d candidate attributes called A = [d] = {1, 2, . . . , d}. We assume that, for any object o and attribute a, there is a distribution over judgments P[X[a] | O = o], and we assume that the judgments of attributeobject pairs are conditionally independent given the sets of attributes and objects. This represents an idealized setting in which a new random crowd worker is selected for each attribute-object judgment (In our experiments, we limit the total amount of work that any one worker may perform). We assume a distribution D over labeled objects, where labels are real numbers. We denote by DO the marginal distribution over objects drawn according to D. We let P[X[a]] = PO\u223cDO [X[a] | O]]. Labels y are assumed to be real valued. As is standard, we assume one \u201ctrue\u201d label yi for each object oi.\nFor notational ease, we assume that in the feature multiselection phase, exactly k \u2265 2 judgments for each feature are collected. Our analysis trivially generalizes to the setting in which different attributes are judged different numbers of times. Finally, each attribute a is assumed to have an expected value of E[X[a]] = 0, where the expectation is taken across objects and judgments of a. This is done for ease of presentation, so that we do not have to track the mean vectors as well as the variance. When discussing implementation details, we describe how to remove this assumption in practice without loss of generality.\nVectors will be boldface, e.g., x = (x[1], . . . , x[d]), random variables will be capitalized, e.g.,X , and matrices will be in black-board font, e.g., X. The i\u2019th standard unit vector is denoted by ei.\nLet r \u2208 Nd represent the number of judgments for each feature, so that attribute a is judged r[a] times, and we rep-\nresent the object\u2019s judgments by x, defined as:\nx = ( \u3008x[1](j)\u3009r[1]j=1, . . . , \u3008x[d](j)\u3009 r[d] j=1 ) ,\nwhere x[a](j) is the jth judgment of attribute a in x, and \u3008x[a](j)\u3009r[a]j=1 is a vector with x[a](j) in coordinate j. We say that r is the repeat vector of x. We denote the set of all possible representations with repeat vector r by R[r].\nWe denote by Dr the distribution which draws (X, Y ) \u2208 R[r] \u00d7 R by first drawing a labeled object (O, Y ) from D, and then drawing a random representation X \u2208 R[r] for this object. We denote by D\u221e the distribution that draws (X, Y ) where X \u2208 Rd by first drawing (O, Y ) fromD and then setting X[a] = E[X[a] | O]. We denote the expectation over Dr by Er = E(X,Y )\u223cDr . For D\u221e we denote E\u221e = E(X,Y )\u223cD\u221e .\nFor k \u2265 2, let k = (k, k, . . . , k) \u2208 Nd be the repeat vector used in the first training phase. The feature multi-selection algorithm receives as input a labeled training set S = ((x1, y1), . . . , (xm, ym)) where xi \u2208 Rkd and yi \u2208 R, drawn from Dk. This sample is generated by first drawing a set of labeled objects ((o1, y1), . . . , (om, ym)) i.i.d. from D, and then drawing a random representation xi for object oi. The algorithm further receives as input a budget B \u2208 N, which specifies the total number of feature judgments allowed for each unlabeled object at test (i.e., prediction) time. The output of the algorithm is a new vector of repeats r \u2208 RB , where,\nRB \u2261 { r \u2208 Nd | \u2211\na\u2208A r[a] \u2264 B\n} .\nLet o be an object with a true label y, and let y\u0302 be a prediction of the label of o. The squared loss for this prediction is `(y, y\u0302) = (y \u2212 y\u0302)2. Given a function f : Z \u2192 R for some domain Z, and a distribution D over Z \u00d7R, we denote the average loss of f on D by\n`(f,D) \u2261 E(Z,Y )\u223cD[`(f(Z), Y )].\nThe final goal of our procedure is to find a predictor with a low expected loss on labeled objects drawn from D. This predictor must use only B feature judgments for each object, as determined by the test repeat vector r. We consider linear predictors w \u2208 Rd that operate on the vector of average judgments of x \u2208 R[r], defined as follows:\nx\u0304[a] \u2261\n{ 1 r[a] \u2211r[a] j=1 x[a](j) if r[a] > 0,\n0 if r[a] = 0.\nFor an input representation x, the predictor w predicts the label \u3008w, x\u0304\u3009. For vector v \u2208 Rd, we denote by Diag(v) \u2208 Rd\u00d7d the diagonal matrix with v[a] in the ath position.\nFor a vector r \u2208 Nd and a matrix S \u2208 Rd\u00d7d, we denote by subr(S) the submatrix of S resulting from deleting all rows and columns a such that r[a] = 0. For a vector, subr(u) omits entries a such that r[a] = 0. Here subr(u) \u2208 Rd \u2032 and subr(S) \u2208 Rd \u2032\u00d7d\u2032 , where d\u2032 is the support size of r. We denote the pseudo-inverse of a matrix A \u2208 Rn\u00d7n (see e.g. Ben-Israel & Greville, 2003) by A+."}, {"heading": "3. Feature Multi-Selection Algorithms", "text": "The input to a feature multi-selection algorithm is a budget B andm labeled examples in which each attribute has been judged k times, and the output is a repeat vector r \u2208 RB . Our ultimate goal is to find r and a predictor w \u2208 Rd such that `(w, Dr) is minimal. We now give intuition about the derivation of the algorithms, but their formal definition is given in Alg. 1.\nDefine the loss of a repeat vector to be `(r) \u2261 minw\u2208Rd `(w, Dr). The goal is to minimize `(r) over r \u2208 RB . We give two forward-selection algorithms, both of which begin with r = (0, . . . , 0) and greedily increment r[a] for a that most decreases an estimate of `(r). The key question is how does one estimate this projected loss `(r) since the number of judgments can exceed k. We simplify notation by first considering only r which are positive, i.e., r[a] \u2265 1 for each a. We will shortly explain how to handle r[a] = 0. Define\nb = E[XY ], and \u03a3r = Er[X\u0304T X\u0304].\nWe call b[a] the correlation of a with the label. Note that b = Ek[X\u0304Y ], since linearity of expectation implies that b does not depend on k. Straightforward calculations show that, for any positive repeat vector r, If \u03a3r is non-singular,3\n`(r) = min w\nEr [ (wT X\u0304\u2212 Y )2 ] = Er[Y 2]\u2212 bT\u03a3\u22121r b.\nSince E[Y 2] does not depend on r, minimizing `(r) is equivalent to maximizing bT\u03a3\u22121r b (for positive r and nonsingular \u03a3r)."}, {"heading": "3.1. A Scoring Algorithm", "text": "The first algorithm that we propose is derived from the zero-correlation assumption, that E[X[a]X[a\u2032]] = 0 for a 6= a\u2032, or equivalently that the covariance matrix is diagonal. Perhaps the simplest approach to standard feature selection is to score each feature independently, based on its normalized empirical correlation with the label, and to select the B top-scoring features. If features are uncorrelated and the training sample is sufficiently large, then this efficient approach finds an optimal set of features. The feature multi-selection scoring algorithm that we propose\n3For singular \u03a3r, the pseudo-inverse \u03a3+r replaces \u03a3\u22121r .\nhenceforth is optimal under similar assumptions, however it is complicated by the fact that we may include multiple repetitions of each feature. Under the zero-correlation assumption, \u03a3r is diagonal, and its ath element, for r[a] > 0, can be expanded as\nEr[(X\u0304[a])2] = \u03c32[a] + v[a]\nr[a] , where\nv[a] \u2261 EO\u223cDO [Var[X[a] | O]] and \u03c32[a] \u2261 E\u221e [ (X[a])2 ] .\nWe refer to v[a] as the internal variance as it measures the \u201cinter-rater reliability\u201d of a, and we call \u03c32[a] the external variance as it is the inherent variance between examples. Hence for a diagonal \u03a3r, simple manipulation gives,\nE[Y 2]\u2212 `(r) = \u2211\na:r[a]>0\n(b[a])2\n\u03c32[a] + v[a]r[a] . (1)\nTherefore, when \u03a3r is diagonal, minimizing the projected loss is equivalent to maximizing the RHS above, a sum of independent terms that depend on the correlation and on the internal and external variance of each attribute, all of which can be estimated just once, for all possible repeat vectors. As one expects, greater correlation indicates a better feature, while a greater external variance indicates a worse feature. A larger internal variance indicates that more repeats are needed to achieve prediction quality.\nTo estimate Eq. (1) we estimate each of the components on the RHS. Unbiased estimation of b is straightforward, and unbiased estimation of v is also possible for k \u2265 2 samples per object, though importantly one should use the unbiased variance estimator,\nv\u0302[a] = 1\nm \u2211 i VarEst(xi[a](1), . . . , xi[a](j)), (2)\nVarEst(\u03b11, . . . , \u03b1n) \u2261 1 n\u2212 1 \u2211 j\u2208[n] (\u03b1j \u2212 1 n \u2211 j\u2032\u2208[n] \u03b1j\u2032) 2.\nUsing these estimates of v, we estimate the external variance using the equality \u03c32[a] = Ek [ (X\u0304[a])2 ] \u2212 v[a]k . A slight complication arises here, as this estimate might be negative for small samples, so we round it up to 0 when this happens. Another issue might seem to arise when the denominator of one of the summands in Eq. (1) is zero, however note that this can only occur if both the internal and the external variance are zero, which implies that the feature is constantly zero, thus zeroing its correlation as well. The same holds for the estimated ratio. In such cases we treat the ratio as equal to 0."}, {"heading": "3.2. The Full Multi-Selection Algorithm", "text": "The scoring algorithm is motivated by the assumption of zero correlation between features. However, this assump-\nAlgorithm 1 Feature multi-selection algorithms 1: Input: Budget B; ((x1, y1), . . . , (xm, ym)) \u2208 Rdk+1,\nAlgorithm type: Scoring/Full. 2: Output: A repeat vector r \u2208 RB . 3: x\u0304i[a]\u2190 1k \u2211 j\u2208[k] xi[a](j) for i \u2208 [m], a \u2208 A.\n4: b\u0302\u2190 1m \u2211 i yix\u0304i.\n5: v\u0302[a]\u2190 1m \u2211 i VarEst(xi[a](1), . . . , xi[a](k)). 6: if Scoring Algorithm then 7: \u2200a \u2208 A, \u03c3\u03022[a]\u2190 max { 0, 1m \u2211 i(x\u0304i[a]) 2 \u2212 v\u0302[a]k } .\n8: Define \u02c6obj(r) \u2261 \u2211 a:r[a]>0 b\u0302[a]\n2/(\u03c3\u03022[a] + v\u0302[a]r[a] ) 9: else\n10: \u03a3\u0302\u2190 MakePSD (\n1 m \u2211 i x\u0304 T i x\u0304i \u2212Diag(v\u0302)/k) ) 11: Mr \u2261 subr(\u03a3\u0302 + Diag( v\u0302[1]r[1] , . . . , v\u0302[d] r[d] )) 12: Define \u02c6obj(r) \u2261 subr(b\u0302)TM+r subr(b\u0302) 13: end if 14: r0 \u2190 (0, . . . , 0) \u2208 Nd 15: for t = 1 to B do 16: Find ibest \u2208 [d] such that \u02c6obj(rt\u22121+ei) is maximal. 17: rt \u2190 rt\u22121 + eibest . 18: end for 19: Return rB .\ntion rarely holds in practice. Building on and paralleling the definitions and derivation above, the Full Algorithm similarly maximizes bT\u03a3\u22121r b without this assumption. For positive r, one has\n\u03a3r = \u03a3 + Diag(v[1]/r[1], . . . , v[d]/r[d])\nWhere \u03a3 \u2261 E\u221e[XTX] is the external covariance matrix, and we estimate it based on the equality \u03a3 = \u03a3k \u2212 Diag(v)/k. Just as in the Scoring algorithm, the estimates of \u03c32[a] might be negative, in the full algorithm it is possible that the estimate of \u03a3 will not be positive semi-definite, so we analogously \u201cround up\u201d our estimate of \u03a3 to the nearest PSD matrix (see implementation details below). The estimate when some of the r[a]\u2019s are zero is formed by deleting the corresponding entries in the estimate of b and the corresponding rows and columns in the estimate of \u03a3r."}, {"heading": "3.3. Guarantees", "text": "Under our distributional assumptions, we show that the estimated objective functions used by our algorithms converge to E[Y 2] \u2212 `(r). Thus maximizing the estimated objective approximately minimizes `(r). Formally, let \u02c6objf (r) and \u02c6objs(r) be the objectives used in Alg. 1 for the full algorithm and the Scoring algorithm, respectively. Note that these objectives are implicitly functions of the training sample S. For a symmetric matrix S, let \u03bbmin(S) be the smallest eigenvalues of S. We define: \u03bb = minr\u2208RB \u03bbmin(subr(\u03a3)), and B\u0304 = min(B, d).\nTheorem 3.1. Suppose that all judgments and labels are in [\u22121, 1]. Then for any \u03b4 \u2208 (0, 1), with prob. at least 1\u2212 \u03b4 over m i.i.d. training samples from Dk, for all r \u2208 RB , for m \u2265 \u2126\u0303(B\u0304 ln(B\u0304d/\u03b4)/\u03bb2) we have\n| \u02c6objf (r)\u2212 (E[Y 2]\u2212 `(r))| \u2264 O ( B\u03043 ln(Bd/\u03b4)\n\u03bb2 \u221a m\n) ."}, {"heading": "If the external covariance matrix \u03a3 is diagonal, then for", "text": "m \u2265 \u2126\u0303(ln(d/\u03b4)/\u03bb2) we have\n| \u02c6objs(r)\u2212 (E[Y 2]\u2212 `(r))| \u2264 O ( ln(Bd/\u03b4)\n\u03bb2 \u221a m\n) .\nThe proof of this theorem is provided in Appendix A. The convergence rate for the full algorithm stems from two bounds: (1) If the norm of the minimizing w is at most \u03b1, then the convergence rate is at most B\u0304\u03b12/ \u221a m; (2) With high probability, the norm of the minimizing w is at most\u221a B\u0304/\u03bb. An additional factor of O(B\u0304 ln(Bd)) gets uniform convergence over r \u2208 RB . The components of this result are of the same order as the equivalent results for uniform convergence of standard least-squares regression. An improved rate of \u221a B\u0304\u03b12/m can be achieved for least-squares regression, if the algorithm exactly minimizes the sample squared loss (Srebro et al., 2010). However, our algorithm minimizes another objective, thus this result is not directly applicable. We leave it as a challenge for future work to find out whether a faster rate can be achieved in our case.\nAs always, these convergence rates are worst-case, and in practice a much smaller sample size is often sufficient to get meaningful results, as we have observed in our experiments. However, if the available training sample is too small to achieve reasonable results, one can limit the norm of the minimizer by adding regularization to the estimated covariance matrix, as in ridge regression (Hoerl & Kennard, 1970). This would allow faster convergence at the expense of a more limited class of predictors.\nAs Theorem 3.1 shows, when the zero-correlation assumption holds, the Scoring algorithm enjoys a much faster worst-case rate of convergence than the full algorithm. This is because it does not attempt to estimate the entire covariance matrix. This advantage is more significant for larger budgets. An additional advantage is that it finds the optimal value of r for its estimated objective:\nTheorem 3.2. The Scoring algorithm returns r \u2208 argmaxr\u2208RB \u02c6objs(r).\nTheorem 3.2 follows since f(r) = a/(b+ c/r) is concave and increasing in r and due to the following observation.\nLemma 3.3. Let r \u2208 Nd, and let f(r) = \u2211 i\u2208[d] gi(r[i]), where gi(\u00b7) : R+ \u2192 R are monotonic non-decreasing concave functions. Let B \u2208 N. The maximum of f(r) subject\nto r \u2208 RB is attained by a greedy algorithm which starts with r = (0, . . . , 0), and iteratively increases the coordinate which increases f the most.\nThe proof of this lemma is provided in Appendix A."}, {"heading": "3.4. Implementation", "text": "If our estimate of \u03a3 is not PSD, we use the procedure \u2018MakePSD\u2019, which takes a symmetric matrix A as input, and returns the PSD closest to A in Frobenius norm. This can be done by calculating the eigenvalue decomposition A = UDUT where U is orthogonal and D is diagonal, and returning UD\u0303UT , where D\u0303 is D with zeroed negative entries (Higham, 1988). If we assume a diagonal external covariance, then this procedure is equivalent to rounding up the estimate of \u03c32(a) to zero, as done in the Scoring algorithm. For a budget of B, the full algorithm performs Bd SVDs to calculate pseudo-inverses. Note, however, that the largest matrix that might be decomposed here is of size min(d,B)\u00d7min(d,B). Furthermore, in practice the matrices can be much smaller, since the algorithm might choose several repeats of the same features. In our experiments, the total time for decompositions, using standard libraries on a standard personal computer, has been negligible.\nOur description of the algorithms above assumes for simplicity that the mean of all features is zero. In practice, one adds a \u2018free\u2019 feature that is always 1, to allow for biased regressors. For the Scoring algorithm, one should further subtract the empirical mean from each feature. For the full algorithm, this not necessary, because when bias is allowed, adding a constant to any feature provably will not change the output of the full algorithm."}, {"heading": "4. Experiments", "text": "We tested our approach on three regression problems. In the first problem the feature judgments were simulated. In the second and third problem they were collected from the crowd using Amazon\u2019s Mechanical Turk.4\nFor the simulated experiment we used the UCI dataset \u2018Relative location of CT slices on axial axis Data Set\u2019 (Frank & Asuncion, 2010). In this dataset the features are histograms of spatial measurements in the image, and the label to predict is the relative location of the image on the axial axis. To simulate features with varying judgments, we collapsed each set of 8 adjacent histogram bins into a single feature, so that each judgment of the new feature was randomly chosen out of 8 possible values for this feature. The resulting dataset contained 48 noisy real-valued features per example.\nThe second and third problems were to predict the height and weight of people from a photo. 880 photos with selfdeclared height and weight were extracted from the publicly available Photographic Height/Weight Chart (Cockerham, 2013), where people post pictures of themselves announcing their own height and weight. We chose 37 attributes that we felt the crowd could judge and might be predictive. We collected judgments for these binary attributes, mainly following the judgment collection methodology of Patterson & Hays (2012), by batching the images into groups of 40, making labeling very efficient. To encourage honest workers, we promised (and delivered) bonuses for good work. We further limited the amount of work any one person could do. We used all of the collected judgments, regardless of whether the workers received bonuses for them or not. Our pay per hour was set\n4http://mturk.com. We will share our data upon request from other researchers, due to the sensitivity of judgments on people\u2019s images.\nto average to minimum wage. We collected numerical estimates of the height and the weight in a similar fashion. Binary judgments took about one second per judgment and their cost was a fraction of a cent per attribute judgment. The numerical estimates took about four times as long and we paid four times as much for them. Accordingly, we adjusted all the algorithms to count a single numerical judgment as equal to four binary attribute judgments.\nFigure 1 and Figure 2 show the normalized correlation (b\u0302[a]/\u03c3\u0302[a]) vs. the normalized inter-rater reliability (v\u0302[a]/\u03c3\u0302[a]) of selected attributes. These plots demonstrate that all combinations of useful/non-useful and stable/noisy attributes exist in this data. The full data listing all the attributes and their properties is provided in Table 1.\nTable 1 Lists all the attributes that were collected for the height and weight prediction problem, their internal variance and their normalized correlation for each of the prediction tasks.\nWe compared the test error of our algorithms, denoted \u2018Full\u2019 and \u2018Scoring\u2019 in the plots, to those of several plausible baselines. In all comparisons, we set k = 2. The first baseline, denoted \u2018Averages\u2019 in the plots, is based on the \u201cpredictive\u201d feature selection algorithm of Isola et al. (2011): We first average the 2 judgments per attribute to create a standard data set with one value for each objectattribute pair, and then greedily add attributes, one at a time, so as to minimize the least-squares error. The resulting regressor uses 2 judgments for each selected feature. The second baseline, denoted \u2018Copies\u2019, treats the 2 judgments of each feature-object pair as 2 different individual attributes, and again performs greedy forward selection on these features. Here the test repeat vector r was set according to the number of copies selected for each feature. Note that these baselines perform standard Machine Learning feature selection: \u2018Averages\u2019 considers d features and \u2018Copies\u2019 considers 2d features. For height and weight prediction, we compared the results also to the test error achieved by averaging only the height or weight estimates of the crowd, respectively. Since each numerical feature costs 4 times as much as a binary feature, we averaged over B/4 numerical judgments when the budget was set to B. We did not use regularization anywhere, thus our algorithms and the baselines are all parameter-free.\nThe test error presented in the plots was obtained as follows: r was selected based on a training set with k judgments. We then added judgments to features in the training set to get to r repeats. Finally we performed regular regression on the means of the enhanced training set to get a predictor. This predictor was then used to predict the labels of the test set with r judgments. In all the comparisons, each experiment was averaged over 50 random train/test splits. In all of the experiments, shown in figures 3-7, our\nfull algorithm achieved better test error than the baselines. The Scoring algorithm was usually somewhat worse than the Full Multi-Selection algorithm, and for small budgets also sometimes worse than the baselines, This is expected due to its zero-correlation assumption. However, when the sample size was small, the Scoring algorithm was sometimes better (see e.g., Figure 6), since it suffered from less over-fitting. This is consistent with our convergence analysis in Theorem 3.1. Analysis of training errors indicates that baseline algorithms suffer for two different reasons: (1) they are limited to a small number of repeats per feature; and (2) they suffer from greater over-fitting. The second reason is probably due to the fact that our algorithm tends to select a sparser r than do the baselines. Table 2 shows examples of predictors, with number of judgments for each attribute, learned by our full algorithm.\nIn our last experiment we tested the tradeoff between the number of training judgments per feature, and the number of training examples, in the following setting: Suppose we have a budget that allows us to collect a total of M judgments for training the feature multi-selection algorithm, and we have access to at least M/2d labeled exam-\nples. We can decide on a number k of judgments per feature, randomly select M/kd objects from our labeled pool to serve as the training set, and obtain kd judgments for each of these objects. What number k should we choose? Does this number depend on the total budget M? We compared the test error arising from different values of k over different values of M , for the slice dataset using both of our algorithms,. The results are shown in Figure 8. These results show a clear preference for a small k (which allows a large m on the same budget M ). Characterizing the optimal number k is left as an open question for future work."}, {"heading": "5. Conclusions", "text": "We introduce the problem of feature multi-selection and provide two algorithms for the case of regression with mean averaging of judgments. Future directions of research include other learning tasks, such as classification, and other types of feature aggregation, such as median averaging (which, for binary features, is equivalent to taking the majority). An additional important question for future\nwork is how to carry out feature multi-selection in an environment with a changing crowd."}, {"heading": "Acknowledgments", "text": "We wish to thank Edith Law and Haoqi Zhang for several helpful discussions."}, {"heading": "A. Analysis", "text": "A.1. Notations\nFor a symmetric matrix S, let \u03bbmax(S) and \u03bbmin(S) be the largest and the smallest eigenvalues of S, respectively. For functions \u03b1 and \u03b2, we say that \u03b1 \u2264 O(\u03b2) if for some constants C,C \u2032 > 0, \u03b1 \u2264 C\u03b2 + C \u2032. Similarly, \u03b1 \u2265 \u2126(\u03b2) indicates that for some constants C,C \u2032 > 0, \u03b1 \u2265 C\u03b2\u2212C \u2032. we say that \u03b1 \u2264 O\u0303(\u03b2) if for some constants C,C \u2032 > 0,\n\u03b1 \u2264 C\u03b2 ln(\u03b2) + C \u2032. Similarly, \u03b1 \u2265 \u2126\u0303(\u03b2) indicates that for some constants C,C \u2032 > 0, \u03b1 \u2265 C\u03b2 ln(\u03b2)\u2212 C \u2032.\nDenote by Zr a diagonal d\u00d7 d matrix whose i\u2019th diagonal entry is I[r[i] > 0]. Recall that v[i] is the internal variance of feature i. For a vector r \u2208 Nd, let Vr be the diagonal matrix such that its i\u2019th diagonal entry is zero if r[i] = 0 and equal to v[i]/r[i] otherwise. Let V\u0302r be defined similarly but using the sample estimate v\u0302[i], defined in Eq. (2), instead of v[i]. Denote by nr the number of non-zero entries in r \u2208 Nd.\nLet x \u2208 R[k] be an example with a repeat vector k such that k[i] \u2265 2.5 Let v\u0302(x)[i] = VarEst(x[i](1), . . . , x[i](k[i])). Given a repeat vector r, define the r-loss `r of the labeled example (x, y) by:\n`r(w,x, y) = (\u3008Zrw, x\u0304\u3009 \u2212 y)2 + \u2211\ni:r[i]>0\nw[i]2v\u0302(x)[i]( 1 r[i] \u2212 1 k[i] ).\nLet S = ((x1, y1), . . . , (xm, ym)) be a training set of labeled representations drawn i.i.d. from Dk. Denote the vector of training labels by y = (y1, . . . , yl). We denote the average of `r over S by\n`r(w, S) = 1\nm \u2211 l\u2208[m] `r(w,xl, yl).\nDefine\n\u03a3\u0302 = 1\nm \u2211 l\u2208[m] x\u0304lx\u0304 T l \u2212 V\u0302k,\n\u03a3\u0302r = Zr(\u03a3\u0302 + V\u0302r)Zr,\nb\u0302 = 1\nm \u2211 l\u2208[m] ylx\u0304l.\nNote that the notation for \u03a3\u0302 here is different than the one used in Alg. 1, since \u03a3\u0302 is not \u2018corrected\u2019 to be PSD. We denote the corrected estimate used in the algorithm by \u03a3\u0302p. Similarly, we denote by \u03a3\u0302pr the estimate for \u03a3r resulting from using \u03a3\u0302p instead of \u03a3\u0302. We have\n`r(w, S) = w T \u03a3\u0302rw \u2212 2wTZrb\u0302 +\n1 m yTy.\nDefine \u03bbr = \u03bbmin(subr(\u03a3)). Note that for \u03bb defined in the statement of Theorem 3.1, \u03bb = minr\u2208RB \u03bbr.\n5Alg. 1 can be applied to k with different values per feature by simply using k[a] instead of k in step 7 and Vk instead of Diag(v\u0302)/k in step 10. Our analysis holds for this more general algorithm.\nA.2. Proof of Theorem 3.1\nTo prove Theorem 3.1 we require several lemmas. All of the analysis below is under the assumption of Theorem 3.1, that all judgments and labels are in [\u22121, 1] with probability 1.\nFirst, the following lemma links the covariance matrix of the population when averaging r judgments to the covariance matrix of the population when using the expected values of the features for each object.\nLemma A.1. Let r \u2208 Nd. Then \u03a3r = Zr\u03a3Zr + Vr.\nProof. Consider a random object O drawn from DO, and define the vector \u00b5O such that for each coordinate i, \u00b5O[i] = E[X[i]|O]. Define \u03a3O = \u00b5O\u00b5TO, so that \u03a3 = EO\u223cDO [\u03a3O]. Similarly, define \u03a3r|O = Er[X\u0304Tr X\u0304 | O], so that \u03a3r = EO\u223cDO [\u03a3r|O]. Lastly, denote the variance of a feature on object O by vO[i] = E[(X[i]\u2212 E[X[i]|O])2|O], and let Vr|O be the diagonal matrix whose i\u2019th entry is zero if r[i] = 0, and equal to vO[i]/r[i] otherwise. Again Vr = EO[Vr|O]. We will prove that for any object O,\n\u03a3r|O = Zr\u03a3OZr + Vr|O. (3)\nThe desired equality will follow by averaging over O \u223c DO.\nDenote the entries of \u03a3r|O by sik. First, whenever r[i] = 0 or r[k] = 0, entry (i, k) is zero for both sides of Eq. (3). Now, consider a non-diagonal entry sik of \u03a3r|O for i 6= k, r[i] > 0 and r[k] > 0. The (i, k) entry on the right-hand side of Eq. (3) is \u00b5O[i]\u00b5O[k]. For the left-hand side we have\nsik = Er[X\u0304[i]X\u0304[k] | O] = Er[X\u0304[i] | O] \u00b7 Er[X\u0304[k] | O] = \u00b5O[i]\u00b5O[k].\nThus the equality holds for non-diagonal entries.\nNow, consider a diagonal entry sii of \u03a3r. If r[i] = 0 then both sides of Eq. (3) are zero. Assume r[i] > 0. We have\nsii = Er[X\u0304[i]2 | O] = Er[( 1\nr[i] \u2211 j\u2208[r[i]] X[i](j))2 | O]\n= 1 r[i]2 ( \u2211 j\u2208[r[i]] Er[(X[i](j))2 | O]+\n2 \u00b7 \u2211\nj<j\u2032,j,j\u2032\u2208[r[i]]\nEr[X[i](j) \u00b7X[i](j\u2032) | O] ) .\nConditioned onO, X[i](j) andX[i](j\u2032) are statistically independent. Therefore\nEr[X[i](j) \u00b7X[i](j\u2032) | O] = Er[X[i](j) | O] \u00b7 E[X[i](j\u2032) | O] = \u00b5O[i]2.\nIn addition, Er[(X[i](j))2] = vO[i] + \u00b5O[i]2. Combining the two equalities we get\nsii = 1\nr[i] (vO[i] + \u00b5O[i]\n2) + 1\nr[i]2 \u00b7 (r[i]2 \u2212 r[i])\u00b5O[i]2\n= \u00b5O[i] 2 + vO[i]/r[i].\nThis is exactly the value of entry (i, i) on the right-hand side of Eq. (3). We conclude that Eq. (3) holds for all types of entries, thus the lemma is proved.\nUsing Lemma A.1, we can show that `r(w, S) is an unbiased estimator of `(w, Dr).\nLemma A.2. Let k, r \u2208 Nd, so that \u2200i \u2208 [d], k[i] \u2265 2. Let S be a sample drawn i.i.d. from Dk. For any w \u2208 Rd, `(w, Dr) = ES [`r(w, S)].\nProof. We have\nES [`r(w, S)] = ES [wT \u03a3\u0302rw \u2212 2wTZrb\u0302 + 1\nm yTy]\n= wTES [\u03a3\u0302r]w \u2212 2wTZrES [b\u0302] + ES [ 1\nm yTy].\nFrom the definition of \u03a3\u0302r, we have\nES [\u03a3\u0302r] = ZrES [ 1\nm \u2211 l\u2208[m] x\u0304lx\u0304 T l + V\u0302r \u2212 V\u0302k]Zr\n= Zr(\u03a3k + Vr \u2212 Vk)Zr = \u03a3r,\nWhere the last inequality follows from Lemma A.1.\nIn addition, E[b\u0302] = b, and E[ 1my Ty] = E[Y 2]. Therefore\nES [`r(w, S)] = wT\u03a3rw \u2212 2wTZrb + E[Y 2].\nOn the other hand, we have\n`(w, Dr) = Er[(\u3008w, X\u0304\u3009 \u2212 Y )2] = Er[(wT X\u0304\u2212 Y )(X\u0304Tw \u2212 Y )] = wTE[X\u0304X\u0304T ]w \u2212 2wTEr[X\u0304Y ] + E[Y 2] = w\u03a3rw \u2212 2wTZrb + E[Y 2] = ES [`r(w, S)].\nThis completes the proof.\nThe next step is to bound the rate of convergence of minw\u2208Rd `r(w, S) to `(r) = minw\u2208Rd `(w, Dr). We show that as m grows, the difference between the two quantities approaches zero. The following lemma provides guarantees under the assumption that the two minimizers have a bounded norm. We will then go on to show that such a bound on the norm holds with high probability, where the bound depends on the external covariance matrix \u03a3.\nLemma A.3. Let \u03b1 > 0, and let W\u03b1 = {w \u2208 Rd | \u2016w\u2016 \u2264 \u03b1}. Let \u03b4 \u2208 (0, 1). Fix some w\u2217 \u2208 W\u03b1, and let w\u0302 \u2208 argminw\u2208Rd `r(w, S) such that \u2016w\u0302\u2016 is minimal. With probability at least 1 \u2212 \u03b4 over the draw of S, if \u2016w\u0302\u2016 \u2264 \u03b1, then\n|`(w\u2217, Dr)\u2212 `r(w\u0302, S)| \u2264 O ( \u03b12nr ln(e/\u03b4)\u221a\nm\n) .\nProof. By Lemma A.2, `(w, Dr) = Ek[`r(w,X, Y )]. By Rademacher complexity bounds (Bartlett & Mendelson, 2002), with probability 1\u2212 \u03b4, for all w \u2208W\u03b1,\nEr[`r(w,X, Y )] \u2264 (4) `r(w, S) +Rm(`r \u25e6W\u03b1, Dk) +O ( \u03b2 \u221a ln(1/\u03b4)/m ) ,\nwhereRm(`r \u25e6W\u03b1, Dk) is the expected Rademacher complexity of the function classW\u03b1 under `r, and \u03b2 is the maximal value of `r on the possible inputs. Under our assumptions, \u03b2 \u2264 \u03b12nr.\nWe wish to bound the Rademacher complexity for our function class, defined as\nRm(`r\u25e6W\u03b1, Dk) = 1\nm ES [E\u03c3[| sup\nw\u2208W\u03b1 \u2211 l\u2208[m] \u03c3l`r(w,xl, yl)|]],\nwhere \u03c3 = (\u03c31, . . . , \u03c3m) are m independent uniform {\u00b11}-valued variables, and S = ((x1, y1), . . . , (xm, ym)) is a random sample drawn i.i.d. from Dk. Denote the components of `r by\n`ar(w,x, y) = (\u3008Zrw, x\u0304\u3009 \u2212 y)2, `br(w,x, y) = \u2211\ni:r[i]>0\nw[i]2v\u0302(x)[i]( 1 r[i] \u2212 1 k[i] ).\nWe have\nRm(`r\u25e6W\u03b1, Dk) \u2264 Rm(`ar\u25e6W\u03b1, Dk)+Rm(`br\u25e6W\u03b1, Dk).\nWe bound each of these Rademacher complexities individually. The first term is the Rademacher complexity of the squared loss over the distribution generated by averaging over judgment vectors drawn from Dk. Standard application of the Lipschitz properties of the squared loss following Bartlett & Mendelson (2002) provides the following bound:\nRm(`ar \u25e6W\u03b1, Dk) \u2264 O ( \u03b12nr\u221a m ) .\nFor the second term, denote uw = Zr\u00b7(w[1]2, . . . , w[d]2)T , and v\u0302x = Zr \u00b7 (v\u0302(x)1, . . . , v\u0302(x)d)T . Then\n`br(w,x, y) = ( 1 r[i] \u2212 1 k[i] )\u3008uw, v\u0302x\u3009.\nSince all feature judgments are in [\u22121, 1], v\u0302x[i] \u2264 4, thus \u2016v\u0302x\u2016 \u2264 4 \u221a nr. In addition, \u2016uw\u2016 \u2264 \u2016w\u20162. Lastly, | 1r[i]\u2212 1 k[i] | \u2264 1. Thus, by standard Rademacher complexity bounds for the linear loss,\nRm(`br \u25e6W\u03b1, Dk) \u2264 O (\nsupw\u2208W\u03b1 \u2016uw\u2016 \u00b7 supx \u2016v\u0302x\u2016\u221a m ) \u2264 O ( \u03b12 \u221a nr\u221a m ) .\nSumming the two terms, we have shown that Rm(`r \u25e6W\u03b1, Dk) \u2264 O ( \u03b12nr\u221a m ) .\nCombining this with Eq. (4) and taking the minimum on both sides, we get\n`(w\u2217, Dr) \u2264 `r(w\u0302, S) +O ( \u03b12nr ln(e/\u03b4)\u221a\nm\n) .\nThis completes one side of the bound. To bound `r(w\u0302, S)\u2212 `(w\u2217, Dr), we note that, by the minimality of w\u0302 and by Hoeffding\u2019s inequality, with probability 1\u2212 \u03b4\n`r(w\u0302, S) \u2264 `r(w\u2217, S) \u2264 E[`r(w\u2217, S)] +O(\u03b2 \u221a ln(e/\u03b4)\nm ).\nCombining the two bounds and applying the union bound we get the statement of the lemma.\nWe now turn to bound the norm of any w considered by our algorithm. Let\n\u03a3\u0302 = 1\nm \u2211 l\u2208[m] x\u0304lx\u0304 T l \u2212 V\u0302k.\nFirst, we relate the smallest eigenvalue of \u03a3\u0302 to that of the true \u03a3.\nLemma A.4. Let \u03b4 \u2208 (0, 1). With probability at least 1\u2212\u03b4,\n\u03bbmin(subr(\u03a3\u0302)) \u2265 (5) \u03bbr \u2212 \u221a O(ln(1/\u03b4) + nr ln(nrm))/m.\nProof. It can be shown (see e.g. Sabato, 2012), by using an -net of vectors on the unit sphere, that for a symmetric random matrix S \u2208 Rn\u00d7n, and positive \u03b2, , \u03b3,\nP[\u03bbmin(S) \u2264 \u03b2 \u2212 \u03b3] \u2264 (6) P[\u03bbmax(S) > \u03b3] +O((n/ )n) min\nu:\u2016u\u2016=1 P[uTSu \u2264 \u03b2].\nIn our case, we have S = subr(\u03a3\u0302) and n = nr. Due to the boundedness of the judgments, all the entries of S are at most 1. Therefore \u03bbmax(S) \u2264 nr. We thus let \u03b3 = nr,\nso that P[\u03bbmax(S) > \u03b3] = 0. To bound P[uTSu \u2264 \u03b2] for u such that \u2016u\u2016 = 1, note that\nuTSu = 1\nm ( \u2211 l\u2208[m] \u3008u,Zrx\u0304l\u30092 \u2212 \u2211 i:r[i]>0 u[i]2v(xl)[i]/k[i] ) .\nBy McDiarmid\u2019s inequality (McDiarmid, 1989), for any t > 0,\nP[uTSu \u2264 E[uTSu]\u2212 t] \u2264 exp(\u22122t2/m\u22062),\nWhere \u2206 is the maximal difference between uTSu with some x1, . . . ,xm and uTSu with x[i] replaced by some x\u2032[i]. It is easy to verify that due to the boundedness of all xl, \u2206 \u2264 O(nr/m). Further, uTE[S]u = uT subr(\u03a3)u \u2265 \u03bbmin(subr(\u03a3)) = \u03bbr. Thus,\nP[uTSu \u2264 \u03bbr \u2212 t] \u2264 exp(\u22122mt2/n2r).\nSubstituting into Eq. (6), we get\nP[\u03bbmin(S) \u2264 \u03bbr \u2212 t\u2212 nr] \u2264 O((nr/ )nr) exp(\u22122mt2/n2r) = exp(\u22122mt2 + nr ln(nr/ )).\nLetting = t/nr and solving for \u03b4 we get the statement of the theorem.\nUsing Lemma A.4, we can now bound the norms of a minimizer of `(w, Dr) and of a minimizer of `r(w, S) with high probability.\nLemma A.5. Let w \u2208 Rd be a minimum-norm minimizer for `r(w, S), that is, let M be the set of minimizers for `r(w, S), and let w\u0302 \u2208 argminw\u2208M \u2016w\u2016. Let w\u2217 be a minimum-norm minimizer for `r(w, S). If m \u2265 \u2126\u0303((ln(1/\u03b4)+nr ln(nr))/\u03bb 2 r), then with probability at least 1\u2212 \u03b4, subr(\u03a3\u0302) is positive definite, and\n\u2016w\u0302\u2016 \u2264 \u221a nr ( \u03bbr \u2212 \u221a O(ln(1/\u03b4) + nr ln(nrm))\nm\n)\u22121 , and\n\u2016w\u2217\u2016 \u2264 \u221a nr/\u03bbr.\nFurther, this holds simultaneously for any vector r\u2032 \u2208 Nd with the same support as r.\nProof. Any minimum-norm minimizer for `r(w, S) has w[i] = 0 whenever r[i] = 0. Thus, we may assume w.l.o.g. that r[i] > 0 for all i by deleting the coordinates with r[i] = 0. We thus have\n`r(w, S) = w T \u03a3\u0302rw \u2212 2wT b\u0302 +\n1 m yTy.\nIf \u03a3\u0302r is positive definite, then the minimizer of `r(w, S) is w\u0302 = \u03a3\u0302\u22121r b\u0302. Thus\n\u2016w\u0302\u2016 \u2264 \u2016b\u0302\u2016\u03bbmax(\u03a3\u0302\u22121r ) (7)\n= \u2016b\u0302\u2016/\u03bbmin(\u03a3\u0302r) \u2264 \u221a nr/\u03bbr.\nNow, \u03a3\u0302r = \u03a3\u0302 + V\u0302r. Since V\u0302r 0, we have \u03bbmin(\u03a3\u0302r) \u2265 \u03bbmin(\u03a3\u0302). \u03bbmin(\u03a3\u0302) can be bounded from below by Lemma A.4. Substituting Eq. (5) in Eq. (7) we get the first desired inequality. For the second inequality, it suffices to note that if subr(\u03a3) is not singular, then w\u2217 = subr(\u03a3)\n\u22121b, thus \u2016w\u2217\u2016 \u2264 \u03bb\u22121min(subr(\u03a3))\u2016b\u2016 \u2264\u221a nr/\u03bbr.\nCombining Lemma A.3 and Lemma A.5 and applying the union bound we immediately get the following theorem.\nTheorem A.6. Let S be a training sample of size m drawn from Dk, where k[i] \u2265 2 for all i in [d]. Fix r \u2208 Nd. Let w\u0302 be a minimum-norm minimizer of `r(w, S). Let \u03b4 \u2208 (0, 1). Ifm \u2265 \u2126\u0303((ln(1/\u03b4)+nr ln(nr))/\u03bb2r), then with probability at least 1\u2212 \u03b4, subr(\u03a3\u0302) is positive definite and\n|`(r)\u2212 `r(w\u0302, S)| \u2264 O ( n2r\u03bb \u22122 r ln(e/\u03b4)\u221a\nm\n) .\nMoreover, the positive-definiteness holds simultaneously for all r\u2032 with the same support as r.\nFinally, we prove our main result, Theorem 3.1.\nProof of Theorem 3.1. We start by proving the result for the full algorithm. Assume that m \u2265 \u2126\u0303((ln(1/\u03b4) + nr ln(nr))/\u03bb 2), and consider a fixed r \u2208 RB . Recall that\n`r(w, S) = w T \u03a3\u0302rw \u2212 2wTZrb\u0302 +\n1 m yTy.\nThe full algorithm ignores, for each examined repeat vector, all the matrix and vector entries that correspond to features with zero repeats. Thus we may assume w.l.o.g. that for all i, r[i] > 0. Theorem A.6 guarantees with probability 1\u2212 \u03b4, that \u03a3\u0302 is positive definite with probability 1\u2212 \u03b4. It follows that \u03a3\u0302p = \u03a3\u0302, therefore \u03a3\u0302pr = \u03a3\u0302r. We thus have\n`r(w, S) = w T \u03a3\u0302prw \u2212 2wT b\u0302 +\n1 m yTy.\nThe minimizer of `r(w, S) is w\u0302 = (\u03a3\u0302pr) \u22121b\u0302. Substituting this solution in `r(w, S), we get minw\u2208Rd `r(w, S) = \u2212b\u0302T (\u03a3\u0302pr)\u22121b\u0302 + 1my Ty = \u2212 \u02c6objf (r) + 1my Ty. If the features are uncorrelated then\nmin w\u2208Rd\n`r(w, S) = \u2212b\u0302T (\u03a3\u0302pr)\u22121b\u0302 + 1\nm yTy\n= \u2212 \u02c6objf (r) + 1\nm yTy.\nSince all labels Y are bounded in [\u22121, 1], with probability 1\u2212 \u03b4, | 1my Ty \u2212 E[Y 2]| \u2264 O( \u221a\nln(1/\u03b4)/m). Thus it suffices to bound |`(r) \u2212 minw\u2208Rd `r(w, S)|. This bound is given by Theorem A.6. We now apply this bound simultaneously to all the possible r \u2208 RB . There are\u2211 i\u2208[B] ( d+ i\u2212 1 d\u2212 1 ) = \u2211 i\u2208[B] ( d+ i\u2212 1 i\u2212 1 ) \u2264 B(d+B)B\u0304\nsuch combinations, thus the bound in Theorem A.6 holds simultaneously for all r \u2208 RB , by dividing \u03b4 with this upper bound. For the requirement on the size of m we only need a union bound on the number of possible supports for r, which is bounded by dB\u0304 . Finally, by noting that for all r \u2208 RB , nr \u2264 B\u0304, we get the desired uniform convergence bound.\nNow, consider the Scoring algorithm. If \u03a3 is diagonal, then `(w, Dr) decomposes into a sum of nr independent losses over single-dimensional predictors with a singledimensional covariance \u2018matrix\u2019 equal to the scalar \u03c32[i] for feature i. The loss minimized by the Scoring algorithm decomposes similarly, using the covariance \u2018matrix\u2019 \u03c3\u03022[i]. Thus, we apply the convergence bound of Theorem A.6 to show convergence of each of the components individually, with nr = 1 and a union bound over B possible values of r[i], and then apply a union bound over d components to get simultaneous convergence of the parts of the loss. For the requirement on the size of m we only need a union bound over the number of components d.\nA.3. Proof of Theorem 3.2\nFirst, we prove the more general Lemma 3.3. We actually prove an equivalent mirror image of Lemma 3.3, by assuming the gi are convex non-increasing and proving that a greedy algorithm minimizes f(r). We formally define the greedy algorithm as follows:\n1. r0 \u2190 (0, . . . , 0).\n2. For t = 1 to B, let rt = rt\u22121 + ei, where i is the coordinate of r that decreases f(r) the most.\n3. Return rB .\nProof of Lemma 3.3. Let r\u2217 \u2208 argminr\u2208RB f(r). Since gi(\u00b7) are all non-increasing, we may choose r\u2217 such that\u2211 i\u2208[d] r\n\u2217[i] = B. Let r be a solution returned by the greedy algorithm listed in the theorem statement. Consider the iterations t1 < . . . < tn \u2208 [B] such that the index ik selected by the algorithm at iteration tk satisfies rtk [ik] > r\n\u2217[ik], so that it causes r to increase this coordinate more than its value in r\u2217. Let j1, . . . , jn \u2208 [d] be a\nseries of alternative indices such that r = r\u2217 + \u2211 k\u2208[n] eik \u2212 \u2211 k\u2208[n] ejk .\nDenote r\u2217L = r \u2217 + \u2211 k\u2208[L] eik \u2212 \u2211 k\u2208[L] ejk .\nNote that for all L < n, r[jL] < r\u2217L\u22121[jL].\nWe prove by induction that for all L \u2208 [n],\nf(r\u2217L) = f(r \u2217).\nWhen setting L = n we will get f(r) = f(r\u2217).\nThe claim trivially holds L = 0. Now assume it holds for L \u2212 1, and consider L. Denote for brevity t = tL\u22121, i = iL and j = jL. Since the algorithm selected i over j at iteration t+ 1, we have that\nf(rt + ei) \u2264 f(rt + ej).\nSubtracting \u2211 l\u2208[d] gl(rt[l]) from both sides we get\ngi(rt[i] + 1)\u2212 gi(rt[i]) \u2264 gj(rt[j] + 1)\u2212 gj(rt[j]).\nIt follows that\n0 \u2264 gi(rt[i])\u2212 gi(rt[i] + 1) + gj(rt[j] + 1)\u2212 gj(rt[j]).\nSince rt[i] \u2265 r\u2217L\u22121[i], by the convexity of gi,\ngi(rt[i])\u2212 gi(rt[i] + 1) \u2264 gi(r\u2217L\u22121[i])\u2212 gi(r\u2217L\u22121[i] + 1).\nIn addition, rt[j] + 1 \u2264 r\u2217L\u22121[j], therefore, again by convexity,\ngj(rt[j] + 1)\u2212 gj(rt[j]) \u2264 gj(r\u2217L\u22121[j])\u2212 gj(r\u2217L\u22121[j]\u2212 1).\nIt follows that\n0 \u2264gi(r\u2217L\u22121[i])\u2212 gi(r\u2217L\u22121[i] + 1)+ gj(r \u2217 L\u22121[j])\u2212 gj(r\u2217L\u22121[j]\u2212 1).\nTherefore\n0 \u2264 f(r\u2217L\u22121)\u2212 f(r\u2217L\u22121 + ei \u2212 ej).\nBy the induction hypothesis f(r\u2217) = f(r\u2217L\u22121). In addition, r\u2217L\u22121 + ei \u2212 ej = r\u2217L. it follows that\nf(r\u2217L) \u2264 f(r\u2217).\nSince r\u2217 is optimal for f , it follows that this inequality must hold with equality, thus proving the induction hypothesis.\nProof of Theorem 3.2. We have\n\u02c6obj(r) = \u2211\ni:r[i]>0\nb\u0302[i]2\n\u03c3\u03022[i] + v\u0302i/r[i] .\nDefine fi : N+ \u2192 R by\nfi(x) = x \u00b7 b\u0302[i]2\nx \u00b7 \u03c3\u03022[i] + v\u0302[i] .\nThen \u02c6obj(r) = \u2211 i:r[i]>0 fi(x).\nWe will define gi(x) : R \u2192 R so that each gi is concave and \u02c6obj(r) = \u2211 i\u2208[d] gi(r[i]). (8)\nWe consider two cases: (1) If v\u0302[i] > 0, then gi(x) is the natural extension of fi(x) to the reals. (2) If v\u0302[i] = 0, fi is a positive constant for all positive integers. Let gi(x) = fi(1) for all x \u2265 1, and let gi(x) = x/fi(1).\nIn both cases, gi(x) is concave with gi(x) = fi(x) for x \u2208 N+ and gi(0) = 0. Therefore Eq. (8) holds. In addition, in both cases gi(x) is monotonic non-decreasing. Therefore by Lemma 3.3, the greedy algorithm maximizes \u02c6obj(r) subject to r \u2208 RB ."}], "references": [{"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson", "year": 2002}, {"title": "Generalized inverses: theory and applications, volume 15", "author": ["A. Ben-Israel", "T.N.E. Greville"], "venue": null, "citeRegEx": "Ben.Israel and Greville,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville", "year": 2003}, {"title": "Efficient learning with partially observed attributes", "author": ["N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Statistical regression with measurement error, volume 6", "author": ["C. Cheng", "J.W. Van Ness"], "venue": "Arnold London,", "citeRegEx": "Cheng and Ness,? \\Q1999\\E", "shortCiteRegEx": "Cheng and Ness", "year": 1999}, {"title": "The photographic height and weight chart", "author": ["Cockerham", "Rob"], "venue": "http://www.cockeyed.com/photos/ bodies/heightweight.html,", "citeRegEx": "Cockerham and Rob.,? \\Q2013\\E", "shortCiteRegEx": "Cockerham and Rob.", "year": 2013}, {"title": "Coefficient alpha and the internal structure of tests", "author": ["L. Cronbach"], "venue": "Psychometrika, 16(3):297\u2013334,", "citeRegEx": "Cronbach,? \\Q1951\\E", "shortCiteRegEx": "Cronbach", "year": 1951}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "Dawid and Skene,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene", "year": 1979}, {"title": "Computing a nearest symmetric positive semidefinite matrix", "author": ["N.J. Higham"], "venue": "Linear algebra and its applications,", "citeRegEx": "Higham,? \\Q1988\\E", "shortCiteRegEx": "Higham", "year": 1988}, {"title": "Understanding the intrinsic memorability of images", "author": ["P. Isola", "D. Parikh", "A. Torralba", "A. Oliva"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Isola et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2011}, {"title": "Content analysis: An introduction to its methodology", "author": ["K. Krippendorff"], "venue": "Sage Publications,", "citeRegEx": "Krippendorff,? \\Q2012\\E", "shortCiteRegEx": "Krippendorff", "year": 2012}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "Surveys in Combinatorics, pp", "citeRegEx": "McDiarmid,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid", "year": 1989}, {"title": "Subset selection in regression", "author": ["A. Miller"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Miller,? \\Q2002\\E", "shortCiteRegEx": "Miller", "year": 2002}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM journal on computing,", "citeRegEx": "Natarajan,? \\Q1995\\E", "shortCiteRegEx": "Natarajan", "year": 1995}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["Patterson", "Genevieve", "Hays", "James"], "venue": "In Proceeding of the 25th Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Patterson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2012}, {"title": "Partial Information and DistributionDependence in Supervised Learning Models", "author": ["S. Sabato"], "venue": "PhD thesis,", "citeRegEx": "Sabato,? \\Q2012\\E", "shortCiteRegEx": "Sabato", "year": 2012}, {"title": "Feature multi-selection among subjective features", "author": ["S. Sabato", "A. Kalai"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Sabato and Kalai,? \\Q2013\\E", "shortCiteRegEx": "Sabato and Kalai", "year": 2013}, {"title": "Inferring ground truth from subjective labelling of venus images", "author": ["P. Smyth", "U.M. Fayyad", "M.C. Burl", "P. Perona", "P. Baldi"], "venue": "In NIPS,", "citeRegEx": "Smyth et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Smyth et al\\.", "year": 1994}, {"title": "Smoothness, lownoise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "The multidimensional wisdom of crowds", "author": ["Welinder", "Peter", "Branson", "Steve", "Belongie", "Serge", "Perona", "Pietro"], "venue": "In Neural Information Processing Systems Conference (NIPS),", "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "By Rademacher complexity bounds (Bartlett & Mendelson, 2002), with probability", "author": ["X Ek[`r(w"], "venue": null, "citeRegEx": "Ek.`r.w and ...,? \\Q2002\\E", "shortCiteRegEx": "Ek.`r.w and ...", "year": 2002}], "referenceMentions": [{"referenceID": 14, "context": "This work has been published in Sabato & Kalai (2013).", "startOffset": 32, "endOffset": 54}, {"referenceID": 12, "context": "Since the feature selection problem is well known to be NP-hard (Natarajan, 1995), our problem is also NP-hard in the general case.", "startOffset": 64, "endOffset": 81}, {"referenceID": 16, "context": "A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.", "startOffset": 125, "endOffset": 199}, {"referenceID": 18, "context": "A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.", "startOffset": 125, "endOffset": 199}, {"referenceID": 2, "context": "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein).", "startOffset": 255, "endOffset": 282}, {"referenceID": 2, "context": "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the \u2018true\u2019 regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements. A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses. Two recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments.", "startOffset": 255, "endOffset": 987}, {"referenceID": 2, "context": "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the \u2018true\u2019 regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements. A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses. Two recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments. Some of their attributes are subjective, e.g., soothing. We employ their crowdsourcing protocol to label our binary attributes. Isola et al. (2011) study subjective and objective features for the task of estimating how memorable an image is, by taking the mean of 10 judgments per attribute for each im-", "startOffset": 255, "endOffset": 1263}, {"referenceID": 5, "context": "Finally, in the social sciences, a wide array of techniques have been developed for assessing inter-rater reliability of attributes, with the most popular perhaps being the \u03b1 coefficient (Cronbach, 1951).", "startOffset": 187, "endOffset": 203}, {"referenceID": 9, "context": "For an overview of reliability theory, see (Krippendorff, 2012).", "startOffset": 43, "endOffset": 63}, {"referenceID": 17, "context": "An improved rate of \u221a B\u0304\u03b12/m can be achieved for least-squares regression, if the algorithm exactly minimizes the sample squared loss (Srebro et al., 2010).", "startOffset": 134, "endOffset": 155}, {"referenceID": 7, "context": "This can be done by calculating the eigenvalue decomposition A = UDU where U is orthogonal and D is diagonal, and returning UD\u0303U , where D\u0303 is D with zeroed negative entries (Higham, 1988).", "startOffset": 174, "endOffset": 188}, {"referenceID": 8, "context": "The first baseline, denoted \u2018Averages\u2019 in the plots, is based on the \u201cpredictive\u201d feature selection algorithm of Isola et al. (2011): We first average the 2 judgments per attribute to create a standard data set with one value for each objectattribute pair, and then greedily add attributes, one at a time, so as to minimize the least-squares error.", "startOffset": 113, "endOffset": 133}], "year": 2013, "abstractText": "When dealing with subjective, noisy, or otherwise nebulous features, the \u201cwisdom of crowds\u201d suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated feature multi-selection algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people\u2019s height and weight from photos, using features such as gender and estimated weight as well as culturally fraught ones such as attractive. This work has been published in Sabato & Kalai (2013).", "creator": "LaTeX with hyperref package"}}}