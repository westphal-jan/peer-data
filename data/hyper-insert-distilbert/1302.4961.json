{"id": "1302.4961", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Improved Sampling for Diagnostic Reasoning in Bayesian Networks", "abstract": "centralized bayesian networks offer great potential for use easily in automating large scale diagnostic unit reasoning tasks. gibbs sampling is the top main diagnostic technique potentially used to essentially perform diagnostic judgment reasoning surveys in large bandwidth richly interconnected clustered bayesian networks. unfortunately gibbs sampling can then take sometimes an excessive time exposure to generate a suitable representative sample. thoroughly in this paper we describe and test carefully a number of heuristic scaling strategies suggested for improving sampling in noisy - or bayesian networks. perhaps the strategies identified include monte carlo distributed markov polynomial chain specific sampling techniques other than simply gibbs model sampling. strategic emphasis is caution put lightly on strategies that could can be automatically implemented in distributed systems.", "histories": [["v1", "Wed, 20 Feb 2013 15:21:44 GMT  (402kb)", "http://arxiv.org/abs/1302.4961v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mark hulme"], "accepted": false, "id": "1302.4961"}, "pdf": {"name": "1302.4961.pdf", "metadata": {"source": "CRF", "title": "Improved Sampling for Diagnostic Reasoning in Bayesian Networks", "authors": ["Mark Hulme"], "emails": ["mrh@cs.rmit.edu.au"], "sections": [{"heading": null, "text": "1 Introduction This paper describe and tests heuristic strategies for improving sampling when performing diagnostic rea soning in large richly interconnected noisy-or Bayesian networks. Emphasis is put on strategies that can be implemented naturally in a distributed system.\nDiagnostic inferencing involves reasoning from a set of observations to a set of possible models, hypothesis or causes. Two common examples of diagnostic problems are medical diagnosis and mechanical fault diagnosis. The class of problems involving some component of di agnostic inferencing is not restricted to those labeled as diagnosis. For example, sensory perception (eg vision) and \"induction\" both involve reasoning from a set of observations to a set of possible models or causes.\nBayesian networks offer great potential for use in au tomating large scale diagnostic reasoning tasks. Un fortunately reasoning in large richly interconnected Bayesian networks can be difficult. Exact evaluation of a Bayesian network is NP-hard [Cooper, 1990]. Eval uation of a Bayesian network within probably approx imately correct bounds is also NP-hard [Dagum and Luby, 1993]. Diagnostic reasoning in Bayesian net work causes particular problems.\nMethods for evaluating Bayesian networks are typi cally broken up into two main classes; exact evaluation algorithms and simulation based methods. The exact evaluation algorithms include the Kim-Pearl poly-tree algorithm [Pearl, 1988] and Speigelhalter's clique tree algorithm [Lauritzen and Spiegelhalter, 1988]. All ex act evaluation algorithms have exponential time com plexity in the number of loops across the width of the network.\nThere are two main classes of simulation algorithms used in the Bayesian network community; forward sampling and Gibbs sampling. Forward sampling schemes [Henrion, 1986] have good performance even when applied to highly looped networks but only if all the evidence nodes are at the roots of the network. If the network is structured to represent a chain of causal events, as we often want to do when represent ing diagnostic problems, then forward sampling is ef ficient at reasoning from the causes to effects. Diag nosis involves reasoning from effect to cause. Forward sampling schemes have been described as having expo nential time complexity in the \"amount of diagnostic evidence\".\nGibbs sampling [Pearl, 1987; Hrycej, 1990; York, 1992] is the main method used when performing diagnostic inferencing in large richly interconnected Bayesian net works. Traditional Gibbs sampling schemes sample a single node at a time. A simple Gibbs sampling scheme would involve choosing a node at random and assign ing the node a value according to it's conditional prob ability given the state of the other nodes in the net work. This Gibbs sampling scheme defines a Markov chain where the states of the Markov chain represent states of the free nodes of the network. As long as the Markov chain is aperiodic and contains at most one ir reducible closed subset of persistent states, the Gibbs sampler is guaranteed to generate a sample that is rep resentative of the posterior probability of the states of the network in the long run. However, the samples are drawn dependently and may be highly biased in the short run.\nIn diagnostic problems the causes tend to have small prior probabilities. The effect of this is to set up com-\n316 Hulme\nP(b)=.!O\nP(vle,b)=.40 P(vle,b)=.60 P(vle,b)=.76\n::'i-;;; \"\u00b7\u00b7. .... / ..... \ufffd\ufffd 5 \u00b7.\n\\ ..... . . ... \ufffd\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7: \ufffd\u00b7\u00b7 \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00ae\ufffd- --\u00b7) : 1. . ........... .. ........... : 1\nA\"!\"! ,_ Jr .... -- -\ufffd::::::::::;\ufffd:::::::::\ufffd- \u00b7 \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 ... \ufffd\ufffd\ufffd .... .. . ) :... ...... :\ufffd)\nFigure 1: a. Diagnostic Network and b. Gibbs Sampling Markov Chain\npetition between possible causes not to have to ex plain positive pieces of evidence. A small diagnostic Bayesian network is shown in figure la. An earth quake and a boy are considered the only realistic ex planations for a vase being broken. Given we know that the vase is broken, one of the two events must have occurred. The small prior probabilities of these two explanations means that it is unlikely that both occurred.\nThe Markov chain defined if we choose one of the free nodes with equal probability and assign it a new value according to its current conditional probability is given in figure lb. If this Markov chain is traced then it is easy to see that the Gibbs sampler is likely to spend a long time in one of the two high probability states, e/\\b or b 1\\ e, before flipping to the other state. The only path between the high probability states is through the low probability state e 1\\ b. In many diagnostic reason ing tasks the Markov chain defined by simple Gibbs strategies involve high probability states separated by low probability states. The samples generated by such a Gibbs sampler will be highly dependent and biased in the short run.\nThe rest of the paper is broken up into four sections. Section 2 describes the diagnostic model examined in this paper and introduces some notation. Section 3 de scribes the improved sampling strategies investigated in this paper. Subsection 3.1 presents a simple scheme for identifying nodes in the network that need not be simulated. In section 3.2 it is argued that when sam pling a node we should only condition on the subset of Markov blanket nodes that d-separate the node from all evidence. Section 3.3 describes the Monte Carlo Markov chain (MCMC) sampling framework. Gibbs sampling is a kind of MCMC sampling [York, 1992]. The framework is used to engineer Markov chains that move between high probability states more easerly. Section 4 presents some empirical results on the strate gies described. Section 5 gives some conclusions and suggests some future work.\n2 Diagnostic Problem\nFigure 2 shows the form of the diagnostic problems ex amined in this paper. The nodes of the graph are split into two sets; model nodes and sensory nodes. The model nodes describe our \"world model\". We assume that the links between model nodes are structured to represent our views on uncertain causal interaction in the real world. The sensory nodes represent the set of possible direct observations of the world. Links be tween model nodes and sensory nodes describe how a state of the world effects what we expect to observe.\nNodes of arbitrary type are denoted by the symbol n. Model node are denoted using the symbol m. Joint states of the model nodes are denoted by the symbol 8. Sensory nodes are denoted by the symbol s. The set of observed evidence nodes is referred to with the symbol Y. All nodes in the network are modeled as binary noisy or units [Pearl, 1988; Peng and Reggia, 1990]. The noisy-or unit is derived when we assume that links represent independent causal events. If we assume that the probability of n; causing nj independent of all other events is Pij, then the equation below gives the probability that nj is true given the state of all its direct causes 11\"( nj).\nImproved Sampling for Diagnostic Reasoning in Bayesian Networks 317\nP(njJ7r(nj)) = 1- IT (1- p;jfi n;E,.(nj)\nTo evaluate this equation we need the set of all possi ble direct causes of a node. This is not as difficult as it may seem. If the set of direct causes of a node is not sufficiently complete, then we can add an extra node clamped true to represent all other true causes. The network structures drawn do not always explicitly rep resent these external causes but it is sometimes useful to acknowledge they exist.\nNoisy-or units are not the only unit used in Bayesian networks structured to represent chains of causal events. Other examples are noisy-and and noisy inhibitory units. Noisy-and units model situations where two events need to co-occur to have an uncertain causal effect on another event ( eg financial security and friendship lead to happiness). Noisy-inhibitory units model situations where uncertain cause effect relation ship may be stopped by a third event ( eg contact with flu and flu injection lead to no flu). Noise-or units are the most popular and we will concentrate on networks of these units.\n3 Improved Sampling In the next three sections some heuristic strategies are described for improving sampling when performing di agnostic reasoning. The first subsection describes a simple strategy for clamping nodes of the network be fore simulation begins. In the second subsection we argue that when sampling a network you need only conditioning on the nodes in the Markov blanket that d-separate the node from the evidence. The last sec tion looks at improving sampling of competing nodes by changing the structure of the Markov chain.\nAs described in the introduction, the fact that the prior probabilities of model nodes tend to be very small\ncreates problems when Gibbs sampling. This fact also creates opportunities. We can use the fact to help identify nodes of the network that are almost certainly false. If we assume that the prior probability of model nodes approaches zero, then it is easy to see that all nodes that are not ancestors of a true sensory node or descendants of ancestors of a true sensory node are almost certainly false.\nIt is easy to implement a strategy for identifying which nodes can be clamped in a distributed framework.\n1. All nodes are clamped to false.\n2. A signal is propagated backwards from all positive sensory nodes to ancestors to indicate that the network has positive diagnostic evidence and they should be undamped. In figure 3 nodes m1, m4, m5 and m7 receive this signal from s1.\n3. A signal is then propagated forward to all descen dents nodes to indicate possible increased posi tive causal support and that they should be un damped. In figure 3 nodes m2, s2 and sa will receive a signal from m5 and m7.\nNo other nodes in a network of noisy-or units receive positive sensory evidential support.\nIt is important to look at the impact of clamping nodes that are not absolutely false to false. The true poste rior probability of the nodes clamped false must be between zero and their prior probability . before any evidence arrived. The prior probability of nodes is easy to find by forward sampling. The effect of clamp ing on undamped nodes is more difficult to determine. For example, clamping node m6 will have an impact on node m5. This issue is investigated empirically in section 5.\nThe strategy presented above is easy to extend to net works involving noisy-and units. The forward propa gation rule needs be changed so that signals can only be sent forward through a noisy-and unit when all its' parents have received signals that they have positive causal or evidential support. Extension of the rule to inhibitory units would require a more involved signal process. When dealing with inhibitory units negative sensory evidence can increase the probability of model nodes. The fact that someone does not have a flu in creases your belief that they have had a flu injection slightly. If we know that there is a reasonable chance that they have had direct contact with someone with a flu then this raises the probability much further.\n3.2 Sampling the Evidence\nAs mentioned in the introduction there are two main types of simulation procedures used in Bayesian net works; forward sampling and Gibbs sampling. In for ward sampling when a node is simulated only the state of the parent nodes is conditioned on. In Gibbs sam pling when a node is simulated all nodes in the Markov\n318 Hulme\n&tem111l Cauae Nodes Model Nodes 1e.1\ufffdr:__ ;;\u00b7 e\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7r\ufffdr\n! l\nO\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7tT -\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7t\ufffd.!.\n\u00b7 .......................... } l\nSenscry Nodes\nH-Bj ,,g,\nFigure 4: Flow of all Evidence\nblanket are conditioned on. Forward sampling will only produce a representative sample if all the evi dence nodes are root nodes. The Gibbs sampler will produce a representative sample irrespective of where the evidence nodes are as long as the Markov chain defined by the sampler is irreducible. Intuitively the reason that the Gibbs sampler works irrespective of which nodes are evidence nodes is because it accounts for the evidence coming from all possible directions. This is often inefficient. We might do better to con sider the flow of evidence in the net when sampling.\nIn figure 4 s4 is an evidence node. Learning the value of s4 effects the probability of all ancestor nodes m3, m6 , ms and m9. The effect of the changed proba bilities of ancestor nodes also flows forward to all de scendants m2, s2 and s3. Node s4 is not the only evidence in the network. Root nodes can be thought of as having true evidence nodes representing true ex ternal causes of the node pointing into them. Evidence flows up all branches of the network from behind the root node. Some non-root nodes such as m4 will also be directly attached to a true external cause node.\nThe arrows on figure 4 show direction of all evidence flows. The nodes m3, m6, ms and mg receive evidence from all ancestors and some descendants nodes. This section of the graph should be Gibbs sampled. All other nodes only receive evidence from ancestor nodes and should be forward sampled. Simulation involves visiting each node in turn and calculating probabilities of the values of the node conditioned on the subset of nodes in the Markov blanket that d-separate it from all evidence nodes. The node is assigned a value according to these conditional probabilities.\nRecognizing the connection between forward sampling and Gibbs sampling allows us to use one equation to calculating the appropriate conditional probabilities. The probability of node n; having value j given the subset of Markov blanket nodes, e, that separate the node from the evidence nodes is given by the equation below. 1\n1 It should be noted that the whole of the Markov blan-\nIn the equation 1r( n;) stands for the set of parent nodes of n;, A( n;) stands for the set of child nodes of n; and .A e ( n;) is the set of child nodes carrying diagnostic evi dence. In figure 3 m3 is the only child node of m6 that carries diagnostic evidence. The states of nodes s3, m2 and ms should not be considered when simulating this node.\nSome caution must be used when applying the equa tion above. In Gibbs sampling we can visit nodes in any order. In forward sampling nodes are typically visited in a forward order and the state of the net work only registered when a complete pass through the network is made. If we are only interested in the marginal probability of nodes then we can visit nodes in any order subject to the restriction that when a node is sampled its' parents have been drawn based on the same causal evidence.\nMore important than the ability to write down a single equation for sampling nodes is the idea that there is a simple distributed mechanism that can be used to determine how a node should be sampled. As seen in figure 4 all that needs to be done is for the evidence nodes to send signals both forwards and backwards through the network.\n3.3 Markov Chain Engineering\nWhile the strategies in the previous two subsections improve sampling performance, they do not attack the key problem encountered when trying to perform diag nostic reasoning in noisy-or Bayesian networks. Nodes tend to compete not to have to explain positive evi dence. The result of this is that moving between high probability states of the network often requires chang ing the value of more than one node at a time. This can give diagnostic problem solving a combinatorial flavor.\nOne strategy designed to help overcome this problem is to merge highly correlated variables into a single variable. This has been suggested in both the statis tical literature [Hastings, 1970] and the Bayesian net work literature [Pearl, 1988; Jensen et al., 1995]. The strategy is known as blocking. In this subsection we describe the MCMC [Hastings, 1970] framework. It is shown that there is more freedom change the nature of the Markov chain than permanent blocking.\n3.3.1 The MCMC Framework\nIn the MCMC framework [Hastings, 1970] the goal is to set up a Markov chain that when simulated pro duces a sample representative of the distribution of\nket of a node needs to be considered if the network is to be annealed.\nImproved Sampling for Diagnostic Reasoning in Bayesian Networks 319\ninterest. It is assumed that the goal is to produce a sample representative of the distribution P(BIY). A Markov chain with transition probabilities P(B; =? Bj) is guaranteed to produce a sample representative of P(BIY) if it contains at most one irreducible closed subset of persistent states, it is aperiodic and it has P( BjY) as its' stationary distribution. The Markov chain has the stationary distribution P( BIY) if it sat isfies the equation below.\n(1)\nA sufficient condition for a Markov chain to have P( BIY) as a stationary distribution is that it satisfies the reversibility condition.\nHastings showed that a Markov chain that produces the correct sample when simulated can be constructed from a sequence of Markov chains. Consider a se quence of transition matrices, (P1 ... Pd), and their matrix multiple, P = f1f:::t P;. Assume each matrix P; has P(BIY) as one of stationary distributions. If P defines a Markov chain that is aperiodic and has at most one irreducible closed subset of persistent states, then the Markov chain will produce a sample when simulated that is representative of P(BIY) in the long run. Repeatedly simulating the sequence of transition matrices (P1 ... Pd) will also produce a sample repre sentative of P(BIY) in the long run. 3.3.2 MCMC strategies\nFrom the description above it is easy to see that there is considerable freedom in designing MCMC sampling strategies. In order to develop new sampling strategy we need to specify three components.\n\u2022 First, we need to choose the set of Markov chain structures to be used.\n\u2022 Second, we need to specify the transition proba bilities for the Markov chains.\n\u2022 Last, we need to define the sequence that the Markov chains are to be simulated in.\nWe will discuss how each of these components can be chosen to help improve diagnostic sampling below.\nChain Structures: If the Markov chains that we de velop are to satisfy the reversibility condition, then any chain that has a transition link from B; to Bj must also have a reverse link from Bj to B;. With this con straint we can see that developing a Markov chain in volves partitioning the set of all elementary states of our model into fully interconnected subsets.\nc \u00b76\n... -d ( ........ )\nq\u00a3_) 4--,\n\ufffd ..... .......... /\nFigure 5: Simulating a Single Variable\nFigure 5 shows the Markov chain defined when we sam ple node e of the broken vase network described in the introduction. As already discussed, if we simulate this network by only allowing the value of one variable to change at a time, then our sampler is likely to generate a highly biased sample in the short term.\nFigure 6: Blocking a Pair\nFigure 6 shows the effect of blocking the two competing causes in the broken vase example. The Markov chain now allows for direct movement between the two high probability states e 1\\ b and b 1\\ e and will therefore pro duce a much better quality sample. The problem with blocking is that it is computationally expensive. Sim ulating a single binary involves inspecting the Markov blanket of the node twice. Simulating a blocked pair of binary nodes involves conditioning on the joint Markov blanket of the node pair four times. The joint Markov blanket of a pair of nodes may be up to twice as large as the Markov blanket of a single node. In the worst case simulating n binary nodes involves conditioning on the joint Markov blanket of the n nodes 2n times.\nc\ufffd;\ufffd>:g\ufffd) ....\n.\u00b7\u00b7G\u00b7\u00b7 \u00b7\u00b7\u00b7\ufffd \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 ..... \\. ........... ) \\ ......... )\n320 Hulme .\nchain defined when we allow the two competing causes in the broken vase network to swap their values at the same time. Again there is a <!irect path between the two high probability states e 1\\ b and b 1\\ e. The Markov blanket of the two nodes has to be inspected twice for each simulation and the size of the Markov blanket is up to twice as large. This gives an equivalent of up to four single node Markov blanket calculations per simulation. In this paper we restrict ourselves to single node, paired node blocking and paired node swapping simulations.\nTransition Probabilities: The transition probabil ities attached to the Markov chain need to be speci fied so that the Markov chain satisfies the reversibility condition. Hastings [Hastings, 1970] derived a formula for the range of possible assignments of the transition probabilities. In this paper if the Markov chain allows us to transfer between a set of states 0m, then we take the transition probability of moving between ()i E 0m and ()j E 0m as P(Bil0m, Y). This reduces to the normal Gibbs sampling transition probabilities when sampling one node at a time. It is easy to show that this assignment of transition probabilities satisfies the reversibility condition.\n=\n= P(BdY)P(BiiY) Le,eem P(BziY) P(BiiY)Pm(BJ => 8;)\nThe equation below shows how this transition proba bility can be calculated. The symbol \ufffdm stands for the set of all random variables that do no have the same assignment in all elements of the set of states 0m. The symbol n\ufffd stands for the statement that the k node has the value given to it in 8; state.\nIt is sometimes argued that method of assigning trans fer probabilities developed by Metropolis is more effi cient than the one presented here because it encour ages more transitions of the Markov chain during sim ulation. If 0m = { B;, (}i}, then the transition probabil ity from (}ito (}i is given by min(1,P(Bi1Y)/P(O;IY)). The two methods of assigning transition probabilities are compared empirically in the next section.\nSequence of Markov Chains: When choosing a se quence of Markov chains to use in simulation we have to ensure that it is alway possible for the resulting sam pler to move between all non-zero probability states within a finite number of moves. The choice of se quence of Markov chains should be made in order to minimize the time it takes to sample the network and\nmaximize the quality of the sample produced. The fol lowing strategies are designed to ensure that blocking and swapping which is more expensive than ordinary sampling are used only where it is most needed.\n\u2022 Spouse nodes that receive positive diagnostic ev idence from a true sensory node are paired ran domly and block sampled. All other nodes are sampled normally.\n\u2022 Nodes that share a true common child are paired at random and block sampled. All other nodes are sampled normally.\n\u2022 Spouse nodes that receive positive diagnostic ev idence from a true sensory are randomly paired and swap sampled in 80% of the time. Any node that is not swap sampled is sampled normally.\n\u2022 Nodes that share a true common are paired at random and swap sampled in 80% of the time. Any node that is not swap sampled is sampled normally.\nWhen choosing a sequence of Markov chains it might also be useful to include chains that simulate difficult to simulate areas of the network more often.\n4 Empirical Results In this section some empirical results are presented on the different sampling strategies described in the pa per. The goal of this study is to see how well the strategies work at producing rough posterior probabil ities for model nodes in a limited time. It is assumed that it is better for an algorithm to give a rough indi cation of the posterior probability of all model nodes, rather than a precise indication of the probabilities of most model nodes. The two goals of robustness and precision can sometimes conflict. An estimate of the posterior of a model node, m;, is considered accurate enough if it is within \u00b1JP(m;IY)(1- P(m;IY))/5 of the true probability, P(m;IY). Notice that this bound allows probabilities near 0.5 to be estimated with less precision than probabilities near zero or one.\nThe strategies were tested on an engine fault network diagnosis network modeled using the fault diagnosis charts found in a standard car manual as a base [Gre gory's, 1990]. The network has 187 model nodes, 82 sensory nodes and 600 links. All nodes are considered to be linked to bias nodes which represent other causes. Five test cases were used. The number of pieces of ev idence in these cases ranged from 4 to 20. The number of pieces of positive evidence ranged from 2 to 9. Each test case was run at least 20 times per strategy.\nPosterior probabilities of model nodes were estimated by the average probability of the node being turned on whenever a Markov chain is applied that would allow the node to change value. The same strategy is used in the stochastic simulation with Markov Blanket scoring algorithm introduced in [Pearl, 1987].\nImproved Sampling for Diagnostic Reasoning in Bayesian Networks 321\nII Time II 5 Runs I 500 Runs I 1000 Runs I 2000 Runs I Gibbs sampling II t.oo II 51.9 I 51.5 I 50.8 I 5o.o I\nA summary of the results is given in table 1. It shows the comparative time each strategy took and the av erage number of errors made after varying numbers of runs. W hen interpreting these results it is important to remember that the efficiency and effectiveness of each strategy is going to be dependent on the network, the cases, hardware and software used. Testing of the strategies across a broader range of networks and cases is needed in the long term. The results presented in table 1 are discussed below.\n4.1 Gibbs Sampling\nThe first row of the table shows the performance of Gibbs sampling on the test problems. Little progress is made towards solving the problems. The error mea sure shown in the table is a bit deceptive. Gibbs sam pling occasionally performs reasonably well on one of the test problems, but performs badly on the great majority. This is an indication that the Gibbs sampler often gets stuck in bad local minima. Similar problems are reported in [Jensen et al., 1995].\n4.2 Clamp Nodes\nThe second row of the table shows the average number of errors after selected model nodes have been clamped according to the strategy described in section 3. The clamping strategy does not appear to lead to any sig nificant increase or decrease in the quality of the sam ple. The main benefit that comes from node clamp ing is the reduction in computational time. The node clamping strategy took 19% less time than Gibbs sam pling.\n4.3 Sampling from the Direction of Evidence\nThe third row of the table shows the effect of recog nizing that some nodes do not receive any diagnos tic evidence. This is the first strategy to make any progress towards solving the test cases within the 2000 run limit. Not only is quality of the sample improved but the time taken to sample the network is reduced 50% on the test cases.\n4.4 Blocking and Swapping Nodes\nRows four to seven of the table compare the various strategies for sampling competing nodes. All strategies lead to similar significant improvements in the quality of the sample. There is no detectable difference be tween the blocking and swapping strategy on the test cases. All blocking and swapping strategies take more time to run than Gibbs sampling. The most time effi cient strategy is to swap sibling nodes in pairs when a spouse is true.\n4.5 Choosing Transfer Probabilities\nThe eight rows of the table shows Metropolis sampling. This is equivalent to Gibbs sampling except Metropo lis transfer probabilities are used (see section 3). The runs shown in table 1 indicate that there is very little difference in performance between the two methods. Runs done on different problems show a slight differ ence in favor of Metropolis sampling.\n4.6 Concentrating Sampling\nThe last two rows of the table show combined strate gies. Both strategies involve using Metropolis trans fer probabilities in the Markov chain, node clamping, sampling from the direction of the evidence and spouse swap sampling when a parent is on. The first opti mized strategy visits nodes in a random order. The second optimized strategy alternates between a for ward pass and a backward pass through the network. In the second strategy nodes that do not receive any diagnostic evidence are only sampled on the forward pass. The argument for this is that node that receive diagnostic evidence tend to be much more difficult to simulate and should attract more of the computational resources. The forward-backward pass strategy pro duced the same quaility samples as the random selec tion strategy. The strategy runs in 66% of the time of the random selection strategy.\n322 Hulme\n4.7 Comment\nThe combined strategies go a long way towards solving the test cases within the 2000 runs. The combined strategies produce much higher quality samples than straight Gibbs sampling and in less than half the time.\n5 Conclusion and Further Work\nIn this paper we demonstrated that by being more sensitive to the structure of the network and the na ture of the problem to be solved the efficiency of the sampling procedure can be dramatically improved. All the suggestions in this paper could implemented in a distributed framework.\nThere is scope for further work in at least two areas. First, more work could be done on determining when it is worth simulating a node. The range of nodes that need to be simulated could be narrowed down further by recognizing that nodes that are not ances tors of unexplained true nodes, descendants of unex plained true nodes or descendants of ancestors unex plained true nodes may be treated as false. Consider the following scenario. 2\nA car arrives at a gas station. Two possible explanations for this are that the car is in need of gas or repairs. If steam is seen ris ing from the engine and the driver says that the car is overheating, then the reason for the cars arrival is explained. It is unlikely to be worth considering other explanations for the car arriving at the gas station. If modeled in a network, then any node that is not a possible cause of the observation that the car is overheating, a possible consequence of the car overheating or a consequence of a possi ble cause of the car overheating may not be worth simulating. After noticing that the ra diator is leaking it may be possible to focus simulation efforts further.\nThe focusing mechanism described above would be useful when modeling a broad domain at varying levels of granularity.\nSecond, there is plenty of room to develop more cre ative Markov chain engineering strategies. There is no need to restrict the shape of the Markov chains to single variable, paired blocking and paired swapping chains. There is no need to change the shape of the Markov chain used to simulate a subset of nodes just depending on the state of child or descendant nodes. The shape of chain used to simulate a subset of nodes could be varied according to the state of all surround ing nodes or conditional probabilities on the links of\n2Note that in section 3 we were interested in true sen sory nodes. Here we may also be interested in any model nodes that are almost certainly true. Simulation may need to be performed to see if a node is almost certainly true.\nthe network. The chain itself may be defined in terms of a heuristic ranking of the states of a subset of nodes. There is considerable work still to be done in exploring the space of different Markov chain engineering strate gies.\nSome preliminary results on further sampling improve ment strategies can be found in [Hulme, 1995].\nReferences\n[Cooper, 1990] Gregory F. Cooper. The computa tional complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42:393-405, 1990.\n[Dagum and Luby, 1993] Paul Dagum and Micheal Luby. Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial In telligence, 60:141-153, 1993.\n[Gregory's, 1990] Gregory's. Home Mechanic. Gre gory's Scientific Publication, 1990.\n[Hastings, 1970] W. K. Hastings. Monte Carlo sam pling methods using Markov chains and their appli cations. Biometrika, 57:97, 1970.\n[Henrion, 1986] M. Henrion. Propagating uncertainty in Bayesian networks by probabilistic logic sam pling. Uncertainty in Artificial Intelligence 2, pages 149-163, 1986.\n[Hrycej, 1990] Tomas Hrycej. Gibbs sampling in Bayesian networks. Artificial Intelligence, 46:351- 363, 1990.\n[Hulme, 1995] Mark Hulme. Sampling large diagnos tic Bayesian networks. Technical Report, 1995.\n[Jensen et al., 1995] Claus S. Jensen, Augustine Kong, and Uffe Kjrerulff. Blocking-Gibbs sampling in very large probabilistic expert systems. International Journal of Human-Computer Studies, 1995. Special Issue on Real-World Applications of Uncertain Reasoning. To appear.\n[Lauritzen and Spiegelhalter, 1988] S. L. Lauritzen and D. J. Spiegelhalter. Local computation with probabilities on graphical structures and their ap plication to expert systems. Journal of the Royal Statistics Society B, 50:157-224, 1988.\n[Pearl, 1987] Judea Pearl. Evidential reasoning using stochastic simulation of causal models. Artificial In telligence, 32:245-257, 1987.\n[Pearl, 1988] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer ence. Morgan Kaufmann, 1988.\n[Peng and Reggia, 1990] Yun Peng and James A. Reg gia. Abductive Inference Models for Diagnostic Problem-Solving. Springer-Verlag, 1990.\n(York, 1992] Jeremy York. Use of the Gibbs sampler in expert systems. Artificial Intelligence, 56:115-130, 1992."}], "references": [{"title": "Monte Carlo sam\u00ad pling methods using Markov chains and their appli\u00ad cations", "author": ["W.K. Hastings"], "venue": "Biometrika, 57:97", "citeRegEx": "Hastings. 1970", "shortCiteRegEx": null, "year": 1970}, {"title": "Propagating uncertainty in Bayesian networks by probabilistic logic sam\u00ad pling", "author": ["M. Henrion"], "venue": "Uncertainty in Artificial Intelligence 2, pages 149-163", "citeRegEx": "Henrion. 1986", "shortCiteRegEx": null, "year": 1986}, {"title": "Artificial Intelligence", "author": ["Tomas Hrycej. Gibbs sampling in Bayesian networks"], "venue": "46:351363,", "citeRegEx": "Hrycej. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Sampling large diagnos\u00ad tic Bayesian networks", "author": ["Mark Hulme"], "venue": "Technical Report,", "citeRegEx": "Hulme. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Blocking-Gibbs sampling in very large probabilistic expert systems", "author": ["Claus S. Jensen", "Augustine Kong", "Uffe Kjrerulff"], "venue": "International Journal of Human-Computer Studies,", "citeRegEx": "Jensen et al.. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Local computation with probabilities on graphical structures and their ap\u00ad plication to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistics Society B, 50:157-224", "citeRegEx": "Lauritzen and Spiegelhalter. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Artificial In\u00ad telligence", "author": ["Judea Pearl. Evidential reasoning using stochastic simulation of causal models"], "venue": "32:245-257,", "citeRegEx": "Pearl. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer\u00ad ence", "author": ["Judea Pearl"], "venue": "Morgan Kaufmann,", "citeRegEx": "Pearl. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Reg\u00ad gia", "author": ["Yun Peng", "James A"], "venue": "Abductive Inference Models for Diagnostic Problem-Solving. Springer-Verlag,", "citeRegEx": "Peng and Reggia. 1990", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 7, "context": "The exact evaluation algorithms include the Kim-Pearl poly-tree algorithm [Pearl, 1988] and Speigelhalter's clique tree algorithm [Lauritzen and Spiegelhalter, 1988].", "startOffset": 74, "endOffset": 87}, {"referenceID": 5, "context": "The exact evaluation algorithms include the Kim-Pearl poly-tree algorithm [Pearl, 1988] and Speigelhalter's clique tree algorithm [Lauritzen and Spiegelhalter, 1988].", "startOffset": 130, "endOffset": 165}, {"referenceID": 1, "context": "Forward sampling schemes [Henrion, 1986] have good performance even when applied to highly looped networks but only if all the evidence nodes are at the roots of the network.", "startOffset": 25, "endOffset": 40}, {"referenceID": 6, "context": "Gibbs sampling [Pearl, 1987; Hrycej, 1990; York, 1992] is the main method used when performing diagnostic inferencing in large richly interconnected Bayesian net\u00ad works.", "startOffset": 15, "endOffset": 54}, {"referenceID": 2, "context": "Gibbs sampling [Pearl, 1987; Hrycej, 1990; York, 1992] is the main method used when performing diagnostic inferencing in large richly interconnected Bayesian net\u00ad works.", "startOffset": 15, "endOffset": 54}, {"referenceID": 7, "context": "All nodes in the network are modeled as binary noisy\u00ad or units [Pearl, 1988; Peng and Reggia, 1990].", "startOffset": 63, "endOffset": 99}, {"referenceID": 8, "context": "All nodes in the network are modeled as binary noisy\u00ad or units [Pearl, 1988; Peng and Reggia, 1990].", "startOffset": 63, "endOffset": 99}, {"referenceID": 0, "context": "This has been suggested in both the statis\u00ad tical literature [Hastings, 1970] and the Bayesian net\u00ad work literature [Pearl, 1988; Jensen et al.", "startOffset": 61, "endOffset": 77}, {"referenceID": 7, "context": "This has been suggested in both the statis\u00ad tical literature [Hastings, 1970] and the Bayesian net\u00ad work literature [Pearl, 1988; Jensen et al., 1995].", "startOffset": 116, "endOffset": 150}, {"referenceID": 4, "context": "This has been suggested in both the statis\u00ad tical literature [Hastings, 1970] and the Bayesian net\u00ad work literature [Pearl, 1988; Jensen et al., 1995].", "startOffset": 116, "endOffset": 150}, {"referenceID": 0, "context": "In this subsection we describe the MCMC [Hastings, 1970] framework.", "startOffset": 40, "endOffset": 56}, {"referenceID": 0, "context": "In the MCMC framework [Hastings, 1970] the goal is to set up a Markov chain that when simulated pro\u00ad duces a sample representative of the distribution of", "startOffset": 22, "endOffset": 38}, {"referenceID": 0, "context": "Hastings [Hastings, 1970] derived a formula for the range of possible assignments of the transition probabilities.", "startOffset": 9, "endOffset": 25}, {"referenceID": 6, "context": "The same strategy is used in the stochastic simulation with Markov Blanket scoring algorithm introduced in [Pearl, 1987].", "startOffset": 107, "endOffset": 120}, {"referenceID": 4, "context": "Similar problems are reported in [Jensen et al., 1995].", "startOffset": 33, "endOffset": 54}, {"referenceID": 3, "context": "Some preliminary results on further sampling improve\u00ad ment strategies can be found in [Hulme, 1995].", "startOffset": 86, "endOffset": 99}], "year": 2011, "abstractText": "Bayesian networks offer great potential for use in automating large scale diagnostic rea\u00ad soning tasks. Gibbs sampling is the main technique used to perform diagnostic reason\u00ad ing in large richly interconnected Bayesian networks. Unfortunately Gibbs sampling can take an excessive time to generate a represen\u00ad tative sample. In this paper we describe and test a number of heuristic strategies for im\u00ad proving sampling in noisy-or Bayesian net\u00ad works. The strategies include Monte Carlo Markov chain sampling techniques other than Gibbs sampling. Emphasis is put on strate\u00ad gies that can be implemented in distributed systems.", "creator": "pdftk 1.41 - www.pdftk.com"}}}