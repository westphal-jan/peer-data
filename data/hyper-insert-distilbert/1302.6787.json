{"id": "1302.6787", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Approximation Algorithms for the Loop Cutset Problem", "abstract": "we thoroughly show thus how to find a unique small loop curser utility in a bayesian network. patiently finding alternatives such around a loop gap cutset is the first objective step in the method design of rat conditioning for inference. our modern algorithm guide for carefully finding a loop cutset, sometimes called mga, typically finds a loop loop cutset which is hence guaranteed in the worst case to contain times less heavily than twice doubled the true number sums of variables commonly contained in a minimum loop or cutset. traditionally we mostly test mga procedures on randomly precisely generated graphs and find that matching the average ratio between the number of instances immediately associated with is the algorithms'output and the number of instances associated with a specified minimum efficient solution is 1. 22.", "histories": [["v1", "Wed, 27 Feb 2013 14:14:02 GMT  (1088kb)", "http://arxiv.org/abs/1302.6787v1", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"]], "COMMENTS": "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)", "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["ann becker", "dan geiger"], "accepted": false, "id": "1302.6787"}, "pdf": {"name": "1302.6787.pdf", "metadata": {"source": "CRF", "title": "Approximation Algorithms for the Loop Cutset Problem", "authors": ["Ann Becker", "Dan Geiger"], "emails": ["anyuta@cs.technion.ac.il,", "dang@cs.technion.ac.il"], "sections": [{"heading": null, "text": "1 Introduction\nMost inference algorithms for the computation of a posterior probability in general Bayesian networks have two conceptual phases. One phase handles op erations on the graphical structure itself and the other performs probabilistic computations. For example, the clique tree algorithm requires us to first find a \"good\" clique tree and then perform probabilistic computa tions on the clique tree [LS88]. Pearl's method of con ditioning requires us first to find a \"good\" loop cutset and then perform a calculation for each loop cutset [Pe86, Pe88]. F inally, Shachter's algorithm requires us to find a \"good\" sequence of transformations and then, for each transformation, to compute some conditional probability tables [Sh86].\nIn the three algorithms just mentioned the first phase is to find a good discrete structure, namely, a clique tree, a cutset, or a sequence of transformations. The goodness of the structure depends on a chosen param eter that, if selected appropriately, reduces the proba bilistic computations done in the second phase. Find ing a structure that optimizes the selected parameter is usually NP-hard and thus heuristic methods are ap plied to find a reasonable structure. Most methods in the past had no guarantee of performance and per formed very badly when presented with an appropriate\nexample. For example, the greedy algorithms of [St90] and [SC90] for the method of conditioning may in the worst case perform as bad as a factor of n /4 where n is the number of variables in a Bayesian network. That is to say, the size of their solution instead of being 2 variables may include as many as n/2 variables-a dis astrous outcome. Similar situations occur with other inference algorithms.\nHowever, recently, among other results, Bar-Yehuda et al. (1994 ) have developed an algorithm that finds a loop cutset that is guaranteed in the worst case to contain less than 4 times the number of variables con tained by a minimum loop cutset. This guarantee is given only when the number of values of every vari able in the network is the same. Note that this result means that the number of instances associated with a loop cutset F found by their algorithm (e.g., ,IF I if the number of values of every variable is r) is no more than the number of instances associated with a mini mum loop cutset raised to the forth power. Note also that, the problem of finding a minimum loop cutset was shown to be NP-hard in [SC90].\nOur paper offers a new algorithm for finding a loop cutset, called MGA, that finds a loop cutset which is guaranteed in the worst case to contain less than twice the number of variables contained in an optimal loop cutset. That is, the number of instances associ ated with a loop cutset found by our algorithm is no more than the number of instances associated with an optimal loop cutset raised to the second power. The complexity of MGA is O(m + nlogn) where\ufffd and n are the number of edges and vertices respectively. Unlike [BGNR94], our result holds even when the ar ities of the variables are arbitrary. Like [BGNR94}, our solution is based on a reduction to the Weighted Vertex Feedback Set Problem, defined in the next sec tion. We should emphasize that all these performance guarantees are for the worst case.\nIn Section 4 we test MGA on randomly generated graphs and find that the average ratio between the number of instances associated with the algonthms' output and the number of instances associated with a minimum solution is 1.22.\nApproximation Algorithms for the Loop Cutset Problem 61\nFrom a theoretical point of view, Bar-Yehuda et. a!. ( 1994) note that as the number of variables grows to infinity the worst case ratio between the size of a loop cutset found by any polynomial algorithm and the size of an optimal loop cutset cannot be less than two un less the unlikely event that a similar result is obtained for the weighted vertex cover problem (WVC/. Conse quently, we conjecture that no polynomial algorithm for the loop cutset problem performs better in the worst case than the algorithm presented in this paper as graphs grow to infinity in size.\nThe rest of the paper is organized as follows. In Section 2 we outline the method of conditioning, ex plain the related loop cutset problem and describe the reduction from the loop cutset problem to the Weighted Vertex Feedback Set (WVFS) Problem. In Section 3 we provide two approximation algorithms for the WVFS problem which is by itself an NP-Complete problem [GJ79, pp. 191-192] . Finally, in Section 4 we present experiments that test the average performance of our algorithms.\n2 The Loop Cutset Problem\nPearl's method of conditioning is one of the known inference methods for Bayesian networks. A short overview of the method of conditioning and definitions of Bayesian networks are needed. The reader is re ferred to [ Pe88) for more details.\nLet P( u1, . . . , Un) be a probability Ji:stribution where each ui draws values from a finite set called the domain of Ui- A directed graph D with no directed cycles is called a Bayesian network of P if there is a 1-1 mapping between { u1, . . . , un} and vertices in D, such that u; is associated with vertex i and P can be written as follows:\n\" P(u1, . . . , un ) =IT P(u; I u;,, . . . , u;;,,) (1)\ni=l\nwhere i1, .. . , ij(i) are the source vertices of the incom ing edges to vertex i in D. Suppose now that some variables { v1, ... , v1} among {u1, . . . , Un } are assigned specific values {v1, . .. , vt} respectively. The updating problem is to compute the probability P( u; I v1 = v1, . . . , v, = v, ) for i = 1, . . . , n. A trail in a Bayesian network is a sub graph whose un derlying graph is a simple path. A vertex b is called a sink with respect to a trail t if there exist two consec utive edges a -+ b and b r- c on t . A trail t is active by a set of vertices Z if ( 1) every sink with respect to t either is in Z or has a descendant in Z and (2) every other vertex along t is outside Z. Otherwise, the trail is said to be blocked ( d-separated) by Z.\n1 The WVC problem is finding a set of vertices that con tains an endpoint of every edge in a given undirected graph and which has a. minimum weight among all such sets.\nVerma and Pearl [VP88] have proved that if D is a Bayesian network of P( u1, ... , un) and all trails between a vertex in { r1, .. . , r1} and a ver tex in {s1, ... ,sk } are blocked by {t1, ... ,tm}, then the corresponding sets of variables { Ur1, \u2022 . \u2022 , Ur1 } and { u$1, . \u2022 \u2022 , U3k } are independent conditioned on { Ut\" . . . ' Ut m } . Furthermore, Geiger and Pearl [ G P90] proved a converse to this theorem. Both results are presented and extended in [GV P90]. Using the close relationship between blocked trails and conditional independence, Kim and Pearl [K P83] de veloped an algorithm UPDATE-TREE that solves the updating problem on Bayesian networks in which ev ery two vertices are connected with at most one trail (singly-connected). Pearl then solved the updating problem on any Bayesian network as follows [Pe86]. First, a set of vertices S is selected such that any two vertices in the network are connected by at most one active trail in S U Z, where Z is any subset of ver tices. Then, UPDATE-TREE is applied once for each combination of value assignments to the variables cor responding to S, and, finally, the results are combined. This algorithm is called the method of conditioning and its complexity grows exponentially with the size of S. The setS is called a loop cutset. Note that when the domain size of the variables varies, then UPDATE TREE is called a number of times equal to the product of the domain sizes of the variables whose correspond ing vertices participate in the loop cutset. If we take the logarithm of the domain size (number of values) as the weight of a vertex, then finding a loop cutset such that the sum of its vertices weights is minimum optimizes Pearl's updating algorithm in the case where the domain sizes may vary.\nWe now give an alternative definition for a loop cutset S and then provide an approximation algorithm for finding it. This definition is borrowed from [BGNR94]. The underlying graph G of a directed graph D is the undirected graph formed by ignoring the directions of the edges in D. A cycle in G is a path whose two terminal vertices coincide. A loop in D is a subgraph of D whose underlying graph is a cycle. A vertex v is a sink with respect to a loop r if the two edges adjacent to v in r are directed into v. Every loop must contain at least one vertex that is not a sink with respect to that loop. Each vertex that is not a sink with respect to a loop r is called an allowed vertex with respect to r. A loop cutset of a directed graph D is a set of vertices that contains at least one allowed vertex with respect to each loop in D. The weight of a set of vertices X is denoted by w(X) and is equal to L, x w(v) where w(x) = log(lxl) and lxl is the size of tte domain associated with vertex x. A minimum loop cutset of a weighted directed graph D is a loop cutset F\u2022 of D for which w(F*) is minimum over all loop cutsets of G. The Loop Cutset Problem is defined as finding a minimum loop cutset of a given weighted directed graph D.\nThe approach we take is to reduce the weighted loop\n62 Becker and Geiger\ncutset problem to the weighted vertex feedback set problem, as done by [BGNR94]. We now define the weighted vertex feedback set problem and then the re duction.\nLet G = (V, E) be an undirected graph, and let w : V --+ m+ be a weight function on the vertices of G. A vertex feedback set of G is a subset of vertices F C V such that each cycle in G passes through at least \ufffdne vertex in F. In other words, a vertex feedback set F is a set of vertices of G such that by removing F from G, along with all the edges incident with F, we obtain a set of trees (i.e., a forest). The weight of a set of vertices X is denoted (as before) by w(X) and is equal to 2::v\u20acX w(v). A minim\u00b7um vertex feedback set of a weighted graph G with a weight function w is a vertex feedback set F* of G for which w(F*) is minimum over all vertex feedback sets of G. The Weighted Vertex Feedback Set (WVFS) Problem is defined as finding a minimum vertex feedback set of a given weighted graph G having a weight function w. Application of this problem for constraint satisfaction is described in [DP90]. In the next section we offer an algorithm, called MGA, for approximately solving the weighted vertex feedback set problem. The algorithm is guaranteed to output a weighted vertex set whose weight is less than twice the optimal weight.\nThe reduction is as follows. Given a weighted directed graph (D, w) (e.g., a Bayesian network), we define the splitting weighted undirected graph D, with a weight function w. as follows. Split each vertex v in D into two vertices V;n and Vout in D, such that all incoming edges to v in D become undirected incident edges with V;n in D s, and all outgoing edges from v in D become undirected incident edges with Vout in D,. In addition, connect V;0 and Vout in D, by an undirected edge. Now set w, (v;n) = oo and w,(vout ) = w(v). For a set of vertices X in D,, we define \"1/l(X) as the set obtained by replacing each vertex V;n or Vout in X by the respective vertex v in D from which these vertices originated.\nOur algorithm can now be easily stated.\nAlgorithm LC Input: A Bayesian network D; Output: A loop cutset of D;\n1. Construct the splitting graph D. with weight function w,;\n2. Apply MGA on (D5,w,) to obtain a vertex feedback set F;\n3. Output \"1/J(F).\nIt is immediately seen that if MGA outputs a vertex feedback set F whose weight is no more than twice the\nweight of a minimum vertex feedback set of Ds, then \"1/l(F) is a loop cutset of D with weight no more than twice the weight of a minimum loop cutset of D. This observation holds because there is an obvious one-to one and onto correspondence between loops in D and cycles in D, and because MGA never chooses a vertex that has an infinite weight.\n3 Algorithms For The WVFS problem\nRecall that the weighted vertex feedback set problem is defined as finding a minimum vertex feedback set of a given weighted graph G.\n3.1 The Greedy Algorithm\nWe first analyze the simplest of all approximation algo rithms for the weighted vertex feedback set problem the greedy algorithm. Assume we are given a weighted undirected graph G with a weight function w. The greedy algorithm starts with G after removing all ver tices with degree 0 or 1 and repeatedly chooses to in sert a vertex v into the constructed vertex feedback set if the ratio between v's weight w( v) and v 's degree d( v) in the current graph is minimal across all vertices in the current graph. When v is selected, it is removed from the current graph and then all vertices with de gree 0 or 1 are repeatedly removed as well. This step is repeated until the graph is exhausted. This algorithm and parts of its analysis are influenced by the work of Chvatal (1979) who analyzed the greedy algorithm for the Weighted Set Cover problem (WSC) and by Lovisz (1975) and Johnson (1974) who ana lyzed the unweighted version of this problem.\nALGORITHM GA\nInput: A weighted undirected graph G(V, E, w). Output: A vertex feedback set F.\nF.-0 i .- 1 Repeatedly remove all vertices with\ndegree 0 or 1 from V and insert the resulting graph into G;\nWhile G; is not the empty graph do 1. Pick a vertex v; for which\nw(v;) \u00b7 \u2022 \u00b7 \u00b7 G d(v\u2022) 1s mmtmum m i 2. F +- F u { Vj} 3. V ..- V \\ {v;} 4. i- i + 1 5. Repeatedly remove all vertices\nwith degree 0 or 1 from V and insert the resulting graph into G;\nend.\nIn the rest of this section we prove that the perfor mance ratio of this greedy algorithm is bounded by 2(log d + 1) where d = m axv EVd( v) is the degree of the\nApproximation Algorithms for the Loop Cutset Problem 63\ngraph. Recall that the performance ratio of an approx imation algorithm is the worst case ratio between the weight of the algorithm's output and the weight of an optimal solution. In Section 4, we show experimentally that even this simple algorithm when combined with the reduction algorithm LC convincingly outperforms the algorithms given by [SC90, St90].\nLet F* be an optimal weighted feedback set of G(V, E, w) and let Y = V\\F*. Note that the vertices in F (the output ofGA) are denoted by {v1, v2, . . . , vt} where v; are indexed in the order in which they are in serted into F by GA and where t = !FI- Let d;(v) denote the degree of vertex v in G;-the graph gen erated in iteration i of G A -and let V; be the set of vertices of G;. An edge is covered by the algorithm if for some i = 1, . . . , t, one of its endpoints is v; and the edge exists in G;. Let r 1 ( v) denote the set of edges in G1 for which at least one endpoint is v. Note that the set of vertex feedback sets of G and G1 is the same and that the degree of every vertex in G1 is smaller or equal to the degree of that vertex in G.\nLet c; = w(vi)jd;(v;) and let C(e) = c; for every edge e removed at iteration i. Note that for every j ::=; i we have w(vj )jdj (vj ) S w(vi)Jdj (v; ) because vertices are selected in decreasing order of these ratios. Also note that for j S i, dj(v;) 2 d;(v;) since the algorithm never adds edges. Thus,\nCj = w(vi)/di(vj) S w(v;)/d;(v;) = c; (2) for 1 S j SiS IFI, as originally claimed by [Ch79] in the context of the WSC problem. To analyze the performance ratio we use a lemma that bounds the number of edges in G; covered by the al gorithm until its termination. We need the following definitions. Let dx(v) be the number of edges whose one endpoint is v and the other is a vertex in X. De note F;* = F* n V; and F'; = Y n V;. A linkpoint is a vertex that has a degree 2 and A branchpoint is a vertex that has a degree larger than 2. (A self-loop adds 2 to the degree of a vertex) .\nLemma 1 t\nLdi(vj)::; 2 L d;(v), (3) j=i\nProof: We will actually prove that, t\nLdj(Vj ) S L (d;(v)- 2) +21Ft IS 2 L d;(v). j=i vEVi\n(4) According to our notations , L:vEV,(d;(v)- 2) equals\nL (dF:(v)- 2) + L dp; (v) + L (d;(v)- 2).\nFurthermore, the graph induced by F'; is a forest and since the number of edges in a forest is smaller\n(or equal) than the number of vertices, we have, L:vEF: dr,(v)/2 :'::: IF'; I. Thus L:vEr,(d:p;(v)- 2) :S 0. Consequently, L:vEv.(d;( v)- 2) +21Ft I is less than or equal to\nL dp; (v) + L d;( v) :S 2 L d;(v).\nThe proof of the first part of Eq. 4 is constructive. We repeatedly apply the following procedure on G; selecting in each step a vertex Vj E F; and showing that there are terms in the right hand side (RHS) of Eq. 4 that contribute dj (vi) to the RHS and have not been used for any other v E F;. Set H = G; and for k = i . . . t do as follows: Pick the vertex Vk. If Vk is a linkpoint in H then follow the two paths p1 and p2 in H emanating from vk until the first branchpoint on each side is found. There are three cases to consider. Either two distinct branchpoints b1 and b2 are found, one branchpoint b1 (in which case p1 and p2 define a cycle) or none (if the cycle is isolated). In the first case the two edges on Pl and P2 whose endpoints are b1 and b2, respec tively, are associated with the terms d\ufffdr: ( bl) - 2 > 0 and dk(b2)- 2 > 0 in the RHS and so each of these terms contributes 1 to the sum L:vEV (d;(v)- 2). In the second case, similarly, the two edges on p1 and P2 whose endpoints is b1 are associated with the term d\ufffdr:(bt)- 2 > 0 and so, if d\ufffdc(b1) > 3, this term con tributes 2 to the sum l:vEV;(d;(v) - 2). If dk(bl ) = 3 we continue to follow the third path from b1 (i.e. , not Pl or P2) until another branchpoint b2 is found and the last edge on that path is associated with dk(b2) - 2 which contributes the extra missing 1 to the RHS. Fi nally, if no branchpoint is found, then on the cycle in which Vk resides there must exist a vertex from F;* that resides on no other cycles of H. Now, if Vk is a branchpoint, then the term dk(vk) - 2 appears in both sides of the inequality. In this case, sequentially remove d\ufffdc(v\ufffd;:)- 2 of the d\ufffdc(v\ufffdc) edges adjacent to Vk such that after each removal the vertices with degree 0 or 1 are removed from H as well. Thus, Vk remains a linkpoint in which case the procedure for a linkpoint is applied. Finally, remove Vk, and repeatedly remove all the vertices with degree 0 or 1 from H. Repeat until F; is exhausted. D\nWe now show that w(F)::; 2 \u00b7 (logd + 1) \u00b7 w(F*).\nt t w(F) = L w(vi) = L c; \u00b7 d;(v;) =\ni=l i=1 t t t\nc1 L d;(v;) + L(c;- c;_l) L dj{Vj) i=l i=2 j=i\nSince c; 2:: Ci-1, we can apply Eq. 3 and so, t\n(5)\nw(F) S 2c1 L d1(v) + L2(c;- c;_l) L d;(v) = i=2\n64 Becker and Geiger\nt t-1 L 2c; L di(v)- L 2c; L di+l(v)\nThus, t\nw(F) :S L 2c; L d;(v)+\nt\nL2c; L i=l\nvEFi\\Fi+1 t-1\nd;(v)- L 2c;\n2(\ufffd (c; L d;(v) + c; L (d;(v)- d;+I(v))) i=l vEFi\\Fi+1 vEFi+1\n+ct L dt(v)) vEF;\"\nHowever, since the last sum on the right hand side merely counts the edge weights according to the iter ation they are assigned a weight, we get,\nw(F) :S 2 L L C(e) (6) vEF\" eEr1(v)\nNow, for every v E F*, H(d(v)) \u00b7 w(v) ;:: 2: C(e), (7)\neEr1(v)\nwhere H(m) = 2::::\ufffd1 1/i, as shown in [Ch79] using the following argument. Let s be the largest superscript such that d,(v) > 0 then\n\u2022 L C(e) = l:(d;(v)- d;+l(v)) \u00b7 (w(vi)jd;(vi))\neEr1(v) i=l s\n:S w(v) L(d;(v)- d;+t(v))jd;(v)\nwhere the inequality is due to Eq. 2. Furthermore, by induction,\n\" L C(e)::::; w(v) L[H(d;(v))- H(di+1(v))].\neEr1(v) i=l Since the right hand side is equal to w( v) \u00b7 H ( d( v)), Eq. 7 follows. Combining Eqs. 6, and 7 yields,\nw(F)::::; 2 L H(d(v) ) \u00b7 w(v) :S 2H(d) \u00b7 w(F*). vEF\"\nThus, since H(d) ::::; log d + 1 (equality holds only when d = 1),\nTheorem 2 The performance ratio of G A is bounded by 2(Iog d + 1).\nWe have an example in which the ratio between GA's output and the optimal output is 2log d. Our exam ple is similar to the example for the vertex cover prob lem given in [Mo92, pp. 47]. Consequently, the upper b ound given in Theorem 2 is rather tight.\n3.2 The Modified Greedy Algorithm\nWe now present a modified greedy algorithm, called MGA, whose performance ratio is bounded by the con stant 2. The changes we introduce into the greedy al gorithm are quite minor and so it is interesting that such a vast improvement in the performance ratio is obtained. A similar phenomenon is reported in the context of the weighted vertex cover problem [Cl83].\nMGA has two phases. In the first phase MGA repeat edly chooses to insert a vertex v into the constructed vertex feedback set if the ratio between v 's weight w( v) and v 's degree d( v) in the current graph is minimal across all vertices in the current graph. When v is se lected, it is removed from the current graph and then all vertices with degree 0 or 1 are repeatedly removed as well. For every edge removed in this process, a. weight of w(v)jd(v) is subtracted from its endpoint vertices. These steps are repeated until the graph is exhausted. The only difference between this phase and the plain greedy algorithm is the revision of some weights in each step instead of just revising the current degrees. The second phase removes redundant vertices from the constructed vertex feedback set.\nALGORITHM MGA\nInput: A weighted undirected graph G(V, E, w) . Output: A vertex feedback set F.\nend\nF' <--- 0 i<---1 Repeatedly remove all vertices with degree 0\nor 1 from V and their adjacent edges from E and insert the resulting graph into G;.\nWhile G; is not the empty graph do 1. Pick a vertex v; for which\n\ufffdt\ufffdS is minimum in G; 2. F' <--- F' U {vi} 3. V <--- V\\ {vi } 4. i <---i + 1 5. Repeatedly remove all vertices with\ndegree 0 or 1 from V and their adjacent edges from E and insert the resulting graph into G;.\nend F +-F1\nFor every edge e = (ut, u2) removed in this process do\nC(e) <--- \ufffd(\ufffd:$ w(ut) <--- w(ut)- C(e) w(u2) <--- w(u2)- C(e)\nFor i = IFI to 1 do {Phase 2} If every cycle in Gi that intersects\nwith { v;} also intersects with F \\ { v;} then,\nF ,__ F \\{vi} endfor\nApproximation Algorithms for the Loop Cutset Problem 65\nClearly F' computed at the first phase of MGA is a vertex feedback set of G and F created from F' by removing all redundant vertices is a minimal vertex feedback set of G, that is, if a vertex is removed from F, then F ceases to be a vertex feedback set of G. Fur thermore, as a result of removing redundant vertices the inequality 2::}=; dj(Vj) :S: 2 l:veF\u00b7 d;(v) (Eq. 3), proven to hold for the greedy algorith\ufffd becomes,\nL d;(v) :S: 2 L d;(v), vEFi veF;\n(8)\nwhere F;\" are the vertices in F that appear in graph G;. The proof of this equation is postponed to Sec tion 3.3. From the description of the algorithm we have for every vertex v in G1,\nL C(e) ::; w(v) (9) eer,(v)\nand if v E F equality must hold. Eq. 9 replaces the in equality l::eer,(v) C(e) :S: H(d(v))\u00b7w(v) (Eq. 7) proven for the greedy algorithm. By analogy with the previ ous section and using similar lines of reasoning, it is clear that Eqs. 8 and 9 which replace Eqs. 3 and 7 show that the bound on the performance ratio drops from 2 \u00b7 H(d) for the greedy algorithm to 2 for the modified greedy algorithm.\nTheorem 3 Algorithm MGA always outputs a vertex feedback set whose weight is no more than twice the weight of the optimal vertex feedback set.\nProof. As in Section 3.1, F* denotes a m1mmum feedback set of G(V, E, w) and Y ::: V \\ F*. Re call that the vertices in the constructed set F' are { Vt , v2, ... , Vt} where v; are indexed in the order in which they are inserted into F by M G A and t ::: I F'l\u00b7 Also, w; ( v) and d; ( v) denote the weight and degree, respectively, of vertex v in G;-the graph generated in iteration i of Step 5 of MGA-and Vi denotes the set of vertices of G;. As in the greedy algorithm, for every j ::; i we have Wj(Vj)/dj(Vj) :S: Wj(v;)/dj(v;) and also wi(v;)/d1(v;)::; w;(v;)jd;(v;) due to the way that the current weights and degrees are updated in the algo rithm. Thus,\nCj := Wj(Vj)/dj(Vj)::; W;(v;)/d;(v;) := Cj (10)\nfor 1 ::; j ::; i :S: I F'l\u00b7 We also have,\ni-1 L C(e) ::: c; \u00b7 d;(vi) + L:cj \u00b7 (dj(v;)- dH1(v;))\n(ll) because the right hand side simply groups edges ac cording to the iteration in which they are assigned a weight.\nLet a; ::: 1 if v; E F and a; ::: 0 if v; fl. F. That is, a; is 1 if v; is not removed from F in the final stage of M G A and 0 otherwise. We now prove that w(F) ::; 2 \u00b7 w(F\"').\nt t w(F) =I: a;\u00b7 w(v;) =La; L C(e)\ni=l\nNow, due to Eq. ll, w(F) is equal to\nt. o; \u00b7 [<; \u00b7 d;(v;) + \ufffd 'i \u00b7 (d;(v;)- d;+l(v;))l which in turn equals to\nt t t c1 I: a;\u00b7 d1(v;) + L(c;- c;_t) Lai \u00b7 d;(vi)\ni=l Furthermore,\nt\ni=i\nL ai \u00b7 di(vj) = L d;(v) :S: 2 L d;(v). (12) j=i\nSince c; 2: ci-t, we can apply Eq. 12 and so, analo gously to the derivation of Eq. 6, we get,\nw(F) :S t 2ct L dt(v) + L 2(c;- c;_l) L di(v) :S:\n2 I: I: C(e) (13)\nNow, Eqs. 9 and 13 yield the claimed inequality, w(F) :S: 2I:va\u2022 w(v)::: 2w(F*). 0\nThe complexity of the first phase of MGA is O(IEI + lVI log lVI) using a Fibonacci heap (e.g., [FT87]) be cause finding and deleting a vertex with minimum ra tio w(v)/d(v) from the heap is done lVI times at the cost of O(log lVI) and decreasing a weight from a ver tex in the heap is done lEI times at an amortized cost ofO(l). The complexity of the second phase ofMGA is also is O(IEI + lVI log lVI) using a simple implemen tation of the union-find algorithm because we need to do at most lVI union operations at an amortized cost of O(log lVI) and at most lEI find operations at the cost of 0(1) [CLR90, pp. 445]. Interestingly, if the second phase is removed from MGA (making MGA even closer to GA), then it can be shown that the performance ratio becomes 4 rather than 2. Hence the vast improvement in the worst case performance of MGA compared toGA stems from changing the vertices' weights in each step rather than from removing redundant vertices.\n3.3 A Theorem about Minimal Vertex Feedback Sets\nIn this section we prove Eq. 8 which has been used in the analysis of the modified greedy algorithm. Let\n66 Becker and Geiger\nG be a weighted graph for which every vertex has a degree strictly greater than 1, F be a minimal ver tex feedback set of G and F* be an arbitrary vertex feedback set of G (possibly a minimum weight vertex feedback set). Let d( v) be the degree of vertex v and dx(v) be the number of edges whose one endpoint is v and the other is in a set of vertices X.\nTheorem 4 Let G, F and F* be defined as above. Then, LvEF d(v) :S 2 LvEP d(v).\nThis theorem is interesting by its own sake since it relates the number of edges adjacent to any minimal weighted vertex feedback set to the number of edges adjacent to any minimum weighted vertex feedback set. Note that Ft is a minimal vertex feedback set of Gi and therefore Theorem 4 proves Eq. 8.\nTo prove this theorem we divide l:vEF d(v) into the sum 2IFI+ LvEF(d(v)-2) and provide an upper bound for each term.\nLemma 5 Let G, F and F* be defined as above. Then,\n2IFI::; L d(v)- 2IF n F*l + 2IF n F*l (14) vEF\nProof: First note that for every set of vertices B in G,\nL d(v)-21FnYn BI-2I(FnY)\\ BI (15) vEF\\B\nHowever, the degree of every vertex in G satisfies d(v)::::: 2 and therefore LveF\\B d(v)::::: 2I(FnY)\\ BI. Consequently,\nL d(v)-2jFnF*I::::: L d(v)-21FnYnBI. (16)\nThus, and since IF n F*l::::: IF n F* n Bland dB(v) ::; d( v), to prove the lemma it suffices to show that\n2IFI::; L dB(v)- 2IF n F n Bl + 2IF n F* n Bl,\n(17) or equivalently,\n2IFI :S L (dB(v)- 2) + 2IF\" n Bl, (18) vEFnB\nholds for some set of vertices B. We now define a set B for which this inequality can be proven. Since F is minimal, each vertex in F can be associated with a cycle in G that contains no other vertices of F. We define a graph H that consists of the union of these cycles-one cycle per each vertex. Note that every vertex in F is a linkpoint in H, i.e., a vertex with degree 2. Let B be the vertices of H.\nThe proof of Eq. 18 is constructive. We repeatedly apply the following procedure on H selecting in each step a vertex v E F and showing that there are terms in the right hand side (RHS) of Eq. 18 that contribute 2 to the RHS and have not been used for any other v E F. Set H' = H. Pick a vertex v E F and follow the two paths Pt and p2 in H' emanating from v (which is a linkpoint) until the first branchpoint on each side is found. There are three cases to consider. Either two distinct branchpoints bi and b2 are found, one branchpoint bt (in which case Pi and p2 define a cy cle) or none (if the cycle is isolated). In the first case the two edge\ufffdon Pi and P2 whose endpoints are bt E F and b2 E F, respectively, are associated with the terms dB(bi)- 2 > 0 and dB(b2 )-2 > 0 in the RHS and so each of these terms contributes 1 to the sum LvEFnB(dB(v) - 2). In the second case, simi larly, j_he two edges on Pi and p2 whose endpoints is bt E F are associated with the term dB(b1)- 2 > 0 and so, if dB(bt) > 3, this term contributes 2 to the sum LvEFnB(dB(v)- 2). If dB(bt) = 3 we continue to follow the third path from bi (i.e ., not Pi or P2) until another branchpoint b2 E F is found and the last edge on that path is associated with d8(b2) - 2 which contributes the extra missing 1 to the RHS. Finally, if no branchpoint is found, then on the cycle in which v resides there must exist a vertex from F* that resides on no other cycles of H'. Thus, the third case could not occur more than IF* n Bl times. Now remove the paths Pi and P2 from H' obtaining a graph in which still each vertex in F resides on a cycle that contains no other vertices of F. Continue the process until F is exhausted. D\nLemma 6 Let G, F and F* be defined as above. Then the sum LvEF(d(v)- 2) is upper bounded by,\nL dp\u2022(v)+ L (d(v)-2)- L (dr(v)-2) vEFnF'\nProof: First note that ,\nL(d(v)- 2) = L (dp- (v)- 2)+ vEF vEFnF'\nL dp(v) + L (d(v)- 2) + vEFilF\u2022 vEFnF\u2022\nL (dp- (v) - 2)- L (dF'(v)- 2) (19) vEFnF' vEFn'F\"\nWe now claim that Lv EFnF' ( dp-( v) - 2) + LvEFnr(dr(v)-2) is less or equal than 0 and there fore can be omitted from the inequality and conclude this proof. The graph induced by F is a forest and since the number of edges in a forest is smaller than the number of vertices, we have, Z::vEF' dr(v)/2 :S IYI. Thus l:vEF'\" ( dy\u2022 ( v) - 2) :S 0 which is equivalent to the stated claim. D\nApproximation Algorithms for the Loop Cutset Problem 67\nUsing the bounds given by Lemmas 5 and 6 we have,\nL d(v):::; L d(v)- 2 IF n Yl+ vEF vEF\n2IFnF*I+ L dF\u00b7(v) vEFn'F\"\n+ L (d(v)- 2)- L (dr(v)- 2) vEFnF\u2022\nHowever, LvEFnF\u2022 (d(v) - 2) + 2IF n F*l LvEFnF\u2022 d(v) and LvEFnF\"(d:p\u00b7(v)-2)+21Fn'F\n*I = LvEFnF\" dF\"(v). Thus, LvEF d(v) is bounded by\nL d(v) + L d(v)vEF vEFnF\u2022\nvEFnF\"\nNow, LvEFd(v)- LvEFnF\" dF\u2022(v) actually equals to L'FnF\" d(v) + LvEFnF\" dp(v) and therefore\nL d(v):::; L dF\u00b7(v) + L d(v)::; 2 L d(v) vEF\nwhich concludes the proof of Theorem 4.\n4 Experimental Results\nBelow we denote by A1 the algorithm described in [SC90] and by A2 the algorithm described in [St90]. We performed six experiments. In the first two ex periments we tested how the outputs of the four al gorithms, A1, A2, GA, and MGA, compare to a min imum loop cutset. In two additional experiments we checked how the algorithms' outputs compare to each other when given larger graphs for which a minimum loop cutset is hard to obtain. In the above four ex periments we have chosen all variables to be binary. The final two experiments compare the performance of these algorithms when the number of values in each vertex is randomly chosen between 2 and 6, 2 and 8, and between 2 and 10. Each instance of the six exper iments is based on 100 graphs generated as described by [SC90]. In the first experiment each of the 100 graphs gener ated had 15 vertices and 25 edges. MGA made only one mistake producing 6 vertices instead of the mini mum of 5 vertices. G A made 4 mistakes each by one vertex off. A2 made 7 mistakes one of which was two vertices off the minimum and the other six mistakes were one vertex off. A1 made 11 mistakes one of which was 2 vertices off and the other 10 mistakes were one vertex off. The minimum loop cutsets were between 3 and 6 vertices. Note that the ratio between the num ber of instances associated with a loop cutset found by MGA in this experiment and the number of instances associated with a minimum loop cutset is 1.002 which is far less than the theoretical ratios guaranteed by\nTheorem 4 for this experiment which lie between 8 when the minimum loop cutset contains 3 binary vari ables and 64 when the minimum loop cutset contains 6 binary variables. In the second experiment we generated 100 networks each with 25 vertices and 25 edges and tested how the output of the four algorithms compare to a minimum loop cutset when the graphs have a small number of loops. This case is interesting because the conditioning inference algorithm is most appropriate for these net works. MGA made no mistakes while the other three algorithms made between 4 and 5 mistakes each by one vertex (the minimum loop cutsets contained between 2 and 4 vertices). Next we tested larger graphs. The first portion of the table below compares between GA and A2 showing that GA performs better than A2 in 53 of the 61 graphs (87%) in which the algorithms disagree (out of 600 graphs tested). Each line in the table is based on 100 randomly generated graphs. The output columns show the number of graphs for which the two algorithms had an output of the same size and the number of graphs each algorithm performed better than the other. Thus even our simple greedy algorithm GA performs much better than A2. The reason for this is the reduction from the loop cutset problem to the weighted vertex feedback set problem which allows the algorithm to se lect vertices that have parents while A2 unjustifiably does not select such vertices (unless they have no pair of parents residing on the same loop). Similar empir ical results and the same explanation applies to Al. The second portion of the table shows that MGA per forms better than GA in 67 of the 75 graphs (89%) in which the algorithms disagreed. Comparing MGA and A2 in the same fashion (600 graphs) showed that MGA performed better than A2 in 109 of the 116 graphs in which the algorithms disagreed. Similarly, MGA per formed better than A1 in 135 of the 137 graphs in which these algorithms disagreed.\nlVI lEI A2 GA Eq. GA MGA Eq. 25 25 0 1 99 0 4 96 25 50 1 8 91 0 8 92 25 75 0 15 85 1 7 92 55 55 1 2 97 0 9 91 55 75 4 10 86 1 18 83 55 105 2 17 81 6 21 83\n8 53 539 8 67 525\nFinally, we repeated some of the experiments except that now each vertex was associated with a random number of values (between 2 and 6, 2 and 8, and 2 and 10). The results are summarized in the table be low. The two algorithms, A1 and MGA, output loop cutsets of the same size in 55% of the graphs and when the algorithms disagreed, then in 81% of these graphs MGA performed better than Al. The ratio obtained between the number of instances of the algorithms so lution and a minimum solution was 1.22 for MGA and\n68 Becker and Geiger\n1.44 for Al (using the 300 graphs in the table below for which the number of vertices is 15 and number of edges 25).\nJVI lEi values Al MGA Eq. 15 25 2-6 1 17 82 15 25 2-8 2 17 81 15 25 2-10 2 19 79 55 105 2-6 13 58 29 55 105 2-8 17 51 32 55 105 2-10 15 55 30\n50 2 17 333\nTo repeat this experiment with A2 required us to make a small change in A2 because it is not designed to run with vertices having different number of values. We adopted the approach of A1 which selects vertices (with at most one parent) according to their degree and if there are several candidates the one with the least number of values is selected for the loop cutset. Combining this idea with the A2 algorithm defines an algorithm we call the weighted A2 algorithm. The re sults obtained were that MGA performed better than WA2 in 175 of the 224 graphs in which the algorithms disagreed (out of 600). The ratio obtained between the number of instances of the algorithms' solution and a minimum solution was 1.22 for MGA and 1.33 for WA2.\nRemark.\nWhile this work was at its final stages of preparation we became aware of a different method for the WVFS problem that achieves a performance ratio of 2 [Be94]. A quick examination of our own work in light of this information revealed that our method also achieves a performance ratio of 2 .\nReferences\n[BGNR94] Bar-Yehuda R., Geiger D., Naor J . , and Roth R. Approximation algorithms for the vertex feedback set problems with applications to con straint satisfaction and Bayesian inference. In proc. of the 5th Annual A CM-Siam Symposium On Dis crete A lgorithms, Arlington, Virginia, January 1994.\n[Be94] Berman P. Pennsylvania state university. Per sonal communication . February 1 994.\n[Ch79] Chvatal V . A greedy heuristic for the set covering problem. Mathematics of operations re search, 4(3) , 1979, pp. 233-235.\n[Cl83] Clarkson K.L. A modification of the greedy al gorithm for vertex cover, Inform ation Processing Letters, 16 (1983), pp. 23-25.\n[CLR90] Carmen T.H., Leiserson C.E . , and Rivest R.L. Introduction t o algorithms, The MIT press, London, England, 1990.\n[DP90] Dechter R. and Pearl J . , schemes for constraint processing: learning, and cu tset decomposition, telligence, 41 (1990), 273-312. Enhancement backjumping, A rtificial In-\n[FT87] Fredman M. L. and Tarjan R.E . Fibonacci heaps and their uses in improved network optimiza tion algorithms. Journal of the ACM, 34(3) , 1987, pp. 596-6 15 .\n[GJ79] Garey M.R. and Johnson D.S., Computers and Intractability: A Guide to the Theory of NP comp/eteness, W. H. Freeman, San Francisco, Cali fornia, 1979.\n[GP90] Geiger, D. and Pearl, J ., On the logic of causal models, In Uncertainty in A rtifi cial Intelligence 4, Eds. Shachter R.D. , Levitt T.S. , Kana! L.N. , and Lemmer J.F. , North-Holland, New York , 1990, 3- 14.\n[GVP90] Geiger, D . , Verma, T .S. , and Pearl, J., Iden tifying independence in Bayesian networks, Net works, 20 (1990) , 507-534.\n[KP83] Kim H. and Pearl J., A computational model for combined causal and diagnostic reasoning in in ference systems, In Proceedings of the Eighth IJCAI, Morgan-Kaufmann, San Mateo, California, 1983, 190-193.\n[LS88] Lauritzen, S.L. and Spiegelhalter, D.J. Lo cal Computations with Probabilities on Graphical Structures and Their Application to Expert Systems (with discussion). Journal Royal Statistical Society, B, 1988, 50(2) :157-224.\n[Mo92] Motwani R.. Lecture notes on approximation algorithms. Computer Science Department, Stan ford University, Report STAN-CS-92-1435.\n[Pe88} Pearl, J ., Probabilistic reasoning in intelligent systems: Networks of plausible infere nce. Morgan Kaufmann, San Mateo, California, 1988.\n[Pe86] Pearl, J . , Fusion, propagation and structur ing in belief networks, A rtificial Intelligence, 29:3 (1986), 241-288.\n[Sh86] Shachter R.D . , Evaluating Influence Diagrams. Operations Research, 1986, 34 :871-882.\n[SC90] Suermondt H.J. and Cooper G.F., Probabilis tic inference in m ultiply connected belief networks using loop cu tsets, Int. J. Approx. Reasoning, 4 (1990) , 283-306.\n[St90] Stillman, J . , On heuristics for finding loop cut sets in multiply connected belief networks, In Pro ceedings of the Sixth Conference on Uncert ainty in A rtificial Int elligence, Cambridge, Massachusetts, 1990 , 265-272.\n[VP88] Verma, T. and Pearl, J. , Causal networks: Semantics and expressiveness, In Proceedings of Fourth Workshop on Uncertainty in Artificial Intel ligence, Minneapolis, Minnesota (published by the Association for Uncertainty in Artificial Intelligence, Mountain View, California) , 1988, 352\ufffd359."}], "references": [{"title": "A modification of the greedy al\u00ad gorithm for vertex cover", "author": ["K.L. Clarkson"], "venue": "Inform ation Processing Letters,", "citeRegEx": "Clarkson,? \\Q1983\\E", "shortCiteRegEx": "Clarkson", "year": 1983}, {"title": "Introduction t o algorithms, The MIT press", "author": ["T.H. Carmen", ". Leiserson C.E", "R.L. Rivest"], "venue": null, "citeRegEx": "Carmen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Carmen et al\\.", "year": 1990}, {"title": "On the logic of causal models, In Uncertainty in A rtifi cial Intelligence", "author": ["D. Geiger", "J Pearl"], "venue": null, "citeRegEx": "Geiger and Pearl,? \\Q1990\\E", "shortCiteRegEx": "Geiger and Pearl", "year": 1990}, {"title": "Lo\u00ad cal Computations with Probabilities on Graphical Structures and Their Application to Expert Systems (with discussion)", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal Royal Statistical Society,", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible infere nce", "author": ["J Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}], "referenceMentions": [], "year": 2011, "abstractText": "We show how to find a small loop cutset in a Bayesian network. Finding such a loop cutset is the first step in the method of condition\u00ad ing for inference. Our algorithm for finding a loop cutset, called MGA, finds a loop cut\u00ad set which is guaranteed in the worst case to contain less than twice the number of vari\u00ad ables contained in a minimum loop cutset. We test MGA on randomly generated graphs and find that the average ratio between the number of instances associated with the algo\u00ad rithms' output and the number of instances associated with a minimum solution is 1. 22.", "creator": "pdftk 1.41 - www.pdftk.com"}}}