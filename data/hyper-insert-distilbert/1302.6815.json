{"id": "1302.6815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data", "abstract": "we describe algorithms for learning bayesian networks from purely a genetic combination comprising of user knowledge and statistical data. the algorithms have two components : relatively a limited scoring metric and a search procedure. the smallest scoring metric takes neither a network structure, statistical relevance data, and replaces a user'existing s entire prior sensory knowledge, and efficiently returns thus a score explicitly proportional to calculating the posterior probability of the network structure given both the resulting data. the search procedure generates networks responsible for program evaluation described by the above scoring metric. usually our computed contributions are threefold. first, generally we identify a two unique important properties of metrics, which effectively we systematically call pure event equivalence code and specific parameter modularity. naturally these 2 properties have been mostly ignored, but historically when combined, humans greatly simplify for the procedural encoding of a needy user's intrinsic prior knowledge. when in particular, a naive user can express her knowledge - adequately for the most part - as equals a total single single prior bayesian network algorithms for the domain. second, we often describe reliable local search engines and error annealing algorithms to only be mostly used in conjunction with higher scoring metrics. in the special design case where essentially each noisy node apparently has at most one parent, we show that smaller heuristic search strategies can be easily replaced sometimes with a less polynomial algorithm to identify describing the networks with providing the appropriately highest score. third, we describe even a methodology solution for evaluating bayesian - network learning robust algorithms. we apply applying this approach continuously to a precise comparison of metrics and search procedures.", "histories": [["v1", "Wed, 27 Feb 2013 14:16:50 GMT  (1221kb)", "http://arxiv.org/abs/1302.6815v1", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"], ["v2", "Sat, 16 May 2015 23:46:48 GMT  (183kb)", "http://arxiv.org/abs/1302.6815v2", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"]], "COMMENTS": "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["david heckerman", "dan geiger", "david maxwell chickering"], "accepted": false, "id": "1302.6815"}, "pdf": {"name": "1302.6815.pdf", "metadata": {"source": "CRF", "title": "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data", "authors": ["David Heckerman", "Dan Geiger"], "emails": ["heckerma@microsoft.com,", "dang@cs.technion.ac.il,", "dmax@cs.ucla.edu"], "sections": null, "references": [], "referenceMentions": [], "year": 2011, "abstractText": "We describe scoring metrics for learning Bayesian networks from a combination of user knowledge and statistical data. We identify two important properties of metrics, which we call event equivalence and parame\u00ad ter modularity. These properties have been mostly ignored, but when combined, greatly simplify the encoding of a user's prior knowl\u00ad edge. In particular, a user can express his knowledge-for the most part-as a single prior Bayesian network for the domain.", "creator": "pdftk 1.41 - www.pdftk.com"}}}