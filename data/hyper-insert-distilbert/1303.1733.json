{"id": "1303.1733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2013", "title": "Multi-relational Learning Using Weighted Tensor Decomposition with Modular Loss", "abstract": "[ we propose a modular mathematical framework especially for multi - relational learning via tensor step decomposition. ` in demonstrating our learning parameter setting, the training data contains multiple types ) of sequence relationships among a set of objects, which we properly represent by essentially a sparse three - mode correlation tensor. \u00ab the fundamental goal is to repeatedly predict the values exactly of the minimal missing entries. to normally do so, we model each initial relationship closely as represents a function formed of representing a linear combination of constant latent factors. tomorrow we learn alternatively this latent representation by computing with a low - rank invariant tensor output decomposition, likewise using this quasi - newton binary optimization representation of precisely a weighted objective function. assessing sparsity in the widely observed data sets is captured by mapping the smooth weighted matrix objective, strongly leading to resulting improved accuracy when dynamic training raw data is limited. exploiting sparsity also improves efficiency, resulting potentially up to an objective order of magnitude satisfaction over unweighted approaches., in addition,, our framework accommodates arbitrary constant combinations of smooth, task - span specific resource loss functions, making way it better suited for learning different desired types of relations. for highlighting the typical cases of constrained real - scale valued functions and binary dependency relations, we propose combining several distinct loss functions and derive directly the associated local parameter gradients. we accordingly evaluate our method on synthetic and real business data, showing significant improvements today in producing both quantitative accuracy and scalability over related factorization reduction techniques.", "histories": [["v1", "Thu, 7 Mar 2013 16:10:44 GMT  (1086kb,D)", "https://arxiv.org/abs/1303.1733v1", null], ["v2", "Fri, 31 May 2013 21:09:20 GMT  (1086kb,D)", "http://arxiv.org/abs/1303.1733v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ben london", "theodoros rekatsinas", "bert huang", "lise getoor"], "accepted": false, "id": "1303.1733"}, "pdf": {"name": "1303.1733.pdf", "metadata": {"source": "CRF", "title": "Multi-relational Learning Using Weighted Tensor Decomposition with Modular Loss", "authors": ["Ben London", "Theodoros Rekatsinas", "Bert Huang"], "emails": ["blondon@cs.umd.edu", "thodrek@cs.umd.edu", "bert@cs.umd.edu", "getoor@cs.umd.edu"], "sections": [{"heading": null, "text": "We propose a modular framework for multirelational learning via tensor decomposition. In our learning setting, the training data contains multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a function of a linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using quasi-Newton optimization of a weighted objective function. Sparsity in the observed data is captured by the weighted objective, leading to improved accuracy when training data is limited. Exploiting sparsity also improves efficiency, potentially up to an order of magnitude over unweighted approaches. In addition, our framework accommodates arbitrary combinations of smooth, task-specific loss functions, making it better suited for learning different types of relations. For the typical cases of real-valued functions and binary relations, we propose several loss functions and derive the associated parameter gradients. We evaluate our method on synthetic and real data, showing significant improvements in both accuracy and scalability over related factorization techniques."}, {"heading": "1 Introduction", "text": "In network or relational data, one often finds multiple types of relations on a set of objects. For instance, in social networks, relationships between individuals may be personal, familial, or professional. We refer to this type of data as multi-relational. In this paper, we propose a tensor decomposition model for transduction on\nmulti-relational data. We consider a scenario in which we are given a fixed set of objects, a set of relations and a small training set, sampled from the full set of all potential pairwise relationships; our goal is to predict the unobserved relationships. The relations we consider may be binary-, discrete ordinal- or real-valued functions of the object pairs; for the binary-valued relationships, the training labels include both positive and negative examples.\nThere has been a growing interest in tensor methods within machine learning, partially due to their natural representation of multi-relational data (Kashima et al., 2009). Many contributions (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010) use the canonical polyadic (CP) decomposition, a generalization of singular value decomposition to tensors. Others (Bader et al., 2007) have proposed models based on decomposition into directional components (DEDICOM) (Harshman, 1978). We propose a similar decomposition (based on (Nickel et al., 2011)) which is more appropriate for multi-relational data, for reasons discussed in Section 3.2. Unlike these previous methods, we do not attempt to decompose the input tensor directly; rather, we explicitly model a mapping from the low-rank representation to the observed tensor, which is often better suited for prediction. For example, a binary relationship can be modeled as the sign of a latent representation; this gives the latent representation more freedom to increase the prediction margin, rather than reproduce {\u00b11} exactly. In this respect, approaches like maximum-margin matrix factorization (MMMF) (Srebro et al., 2005b; Rennie and Srebro, 2005a) and DEDICOM can be viewed as specializations of our framework.\nOur proposed method, multi-relational weighted tensor decomposition (see Section 3), assumes that the latent representation is determined by a linear combination of latent factors associated with each object. Learning these latent factors and their interactions in each relation thus becomes analogous to a weighted\nar X\niv :1\n30 3.\n17 33\nv2 [\ncs .L\nG ]\n3 1\nM ay\n2 01\ntensor decomposition (described in Section 3.1 and illustrated in Figure 1). We formulate this decomposition as a nonlinear optimization problem (Section 3.3), which may incorporate any combination of smooth, task-specific loss functions. These task-specific loss functions allow simultaneous learning of various relation types, such as binary- and continuous-valued. By weighting the objective function, we are able to learn from limited observed (training) relationships without fitting the unobserved (testing) ones, improving both accuracy and efficiency. We demonstrate the effectiveness of our approach in Section 4, using both real and synthetic data experiments. Our results indicate that our approach is both more accurate and efficient than competing factorizations when training data is sparse."}, {"heading": "2 Preliminaries", "text": "This section introduces our notation and defines the problem of multi-relational transduction.\nWe denote tensors and matrices using bold, uppercase letters; similarly, we use bold, lowercase letters to denote vectors. For a tensor X, let xi,j,k denote the (i, j)th element of the kth frontal slice. Denote by Xk the matrix comprising the kth frontal slice. We use to denote the Hadamard (i.e., element-wise) product, tr(\u00b7) for the trace operator and ||\u00b7||F for the Frobenius norm. For a matrix V and function f , let \u2207Vf denote the gradient of f with respect to V.\nFix a set of m objects and a set of n relations.1 To simplify our analysis, we assume that all relations are symmetric, though one can obtain an analogous derivation for asymmetric relations with only slightly more work. We are given a partially observed tensor Y \u2208 Rm\u00d7m\u00d7n, in which each observed entry yi,j,k is a (possibly noisy) measurement of a relationship and each unobserved entry is set to a null value.2 We are additionally given a nonnegative weighting tensor W \u2208 R+m\u00d7m\u00d7n, where each entry wi,j,k \u2208 [0, 1] corresponds to a user-defined confidence, or certainty, in the value of yi,j,k; if yi,j,k is unobserved, then wi,j,k is necessarily zero. The goal of multi-relational transduction in this tensor formulation is to infer the unobserved entries in Y."}, {"heading": "3 Proposed Method", "text": "This section introduces our proposed method, which we refer to as multi-relational weighted tensor decom-\n1Here, we use the term relation loosely to include not only strict relations, for which relationships are either present or not, but also real-valued functions.\n2For example, for binary-valued relations in {\u00b11}, the null value is 0.\nposition (MrWTD). We begin by describing our lowrank tensor representation of multi-relational data. We then define an optimization objective used to compute this representation and discuss how we solve the optimization."}, {"heading": "3.1 Representation as Tensor Decomposition", "text": "Our fundamental assumption is that each relationship is equal to a mapping \u03a6k applied to an element xi,j,k in an underlying low-rank tensor X \u2208 Rm\u00d7m\u00d7n. Each \u03a6k depends on the nature of the relation, and may differ across relations. For example, for binary relations in {\u00b11}, \u03a6k is the sign function. We further assume that each Xk can be factored as a rank-r decomposition\nXk = ARkA > + bk, (1)\nwhere A \u2208 Rm\u00d7r, Rk \u2208 Rr\u00d7r and bk \u2208 R. (Figure 1 illustrates this decomposition.) Note that there is a single A matrix, but n instances of Rk and bk. Also note that we place no constraints on A or Rk; the columns of A need not be linearly independent, and Rk need not be positive-semidefinite. To infer the values of the missing (or uncertain) entries, we predict each yi,j,k by computing xi,j,k = aiRka > j + bk, where ai and aj are the i th and jth row vectors of A, and then apply the appropriate mapping \u03a6k(xi,j,k).\nThe entries of A can be interpreted as the global latent factors of the objects, where the ith row ai corresponds to the latent factors of object i. Each Rk determines the interactions of A in the kth relation. Thus, each predicted relationship comes from a linear combination of the objects\u2019 latent factors. Because the latent factors are global, information propagates between relations during the decomposition, thus enabling collective learning. The addition of bk accounts for distributional bias within each relation."}, {"heading": "3.2 Related Models", "text": "Our tensor model is comparable to Harshman\u2019s DEDICOM (1978). Bader et al. (2007) applied the DEDICOM model to the task of temporal link prediction (in a single network), using the third mode as the time dimension. Recently, Nickel et al. (2011) proposed a relaxed DEDICOM, referred to as RESCAL, to solve several canonical multi-relational learning tasks. Of the previous approaches, our underlying decomposition is most similar to RESCAL, and Equation 1 would be identical to the RESCAL decomposition if not for the bias term. Beyond the decomposition, the key distinction is that RESCAL directly decomposes the input tensor, rather than modeling the mapping from X to Y. RESCAL also ignores the potential sparsity and uncertainty in the observations, whereas we explicitly\nmodel this. We demonstrate in Section 4 that our formulation produces more accurate predictions even when observed (training) data is limited.\nOther tensor factorization models have been proposed for multi-relational data, though they typically use the CP decomposition (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010). In the CP decomposition, each entry is the inner product of three vectors; this would be similar to our decomposition if each Rk slice were constrained to be diagonal. The richer interactions of the relaxed DEDICOM and the global latent representation of the objects often make it better suited for multi-relational learning, as was corroborated empirically by Nickel et al. (2011)."}, {"heading": "3.3 Objective", "text": "To compute the decomposition in Equation 1, we minimize the following regularized objective:\nf(A,R,b) , \u03bb\n2 ||A||2F\n+ n\u2211\nk=1\n\u03bb 2 ||Rk||2F + tr\n( Wk(`k(Yk,Xk)) >) , (2) where \u03bb \u2265 0 is a regularization parameter, Xk is computed by Equation 1, and `k is a loss function that is applied element-wise to the kth slice. (For brevity, we use f to denote f(A,R,b).) This ability to combine multiple loss functions is central to our approach, as the appropriate penalty depends on the mapping for each Xk to Yk. Though most matrix and tensor decompositions focus on minimizing the quadratic loss (defined below), this criterion may not be optimal for certain prediction tasks (such as binary prediction). By explicitly making the loss function for each slice task-specific, our framework offers more flexibility than related techniques. The only requirement (due to our optimization method) is that the loss function is smooth.\nIt is important to note our use of L2 regularization. Regularization effectively controls the complexity of\nthe model and thereby reduces the possibility of overfitting. This follows the traditional wisdom that \u201csimpler\u201d models will generalize better to unseen data\u2014in this case, the unobserved tensor entries. The rank of the decomposition can also be seen as a complexity parameter, since higher ranks will better fit the observed data. However, after a certain point, increasing the rank has a diminishing effect, since the regularizer seeks to minimize the Frobenius norm of the decomposition. We explore the effect of the rank parameter empirically in Section 4.4.\nTo minimize Equation 2, we require the gradients of f w.r.t. A, Rk and bk. Leveraging the symmetry of Rk, we derive3 these as\n\u2207Af = \u03bbA + n\u2211\nk=1\n2(Wk \u2207Xk`k(Yk,Xk))AR>k , (3)\n\u2207Rkf = \u03bbRk + A> (Wk \u2207Xk`k(Yk,Xk)) A, (4) \u2207bkf = tr ( Wk(\u2207Xk`k(Yk,Xk))> ) , (5)\nwhere denotes the Hadamard (i.e., element-wise) product, and \u2207Xk`k(Yk,Xk) is the gradient of `k w.r.t. Xk. Though this accommodates any differentiable loss function, we now present three that are applicable to many relational problems, and derive their corresponding loss gradients.\nQuadratic Loss: The most common loss function used in matrix and tensor factorization is the quadratic loss, which we denote by `q(y, x) , 12 (y \u2212 x)2. Minimizing the quadratic loss corresponds to the setting in which each relationship is directly approximated by a linear combination of latent factors; i.e., \u03a6k is the identity and Yk \u2248 Xk. For this loss function, the loss gradient is simply \u2207Xk`qk(Yk,Xk) = (Xk \u2212Yk).\nSmooth Hinge Loss: While the quadratic loss may be appropriate for learning real-valued functions, it\n3Due to space restrictions, we state the gradients without their derivation.\nis sometimes ill-suited for learning binary relations, which are essentially binary classifications. For binary classification, the goal is to complete a partially observed slice Yk \u2208 {\u00b11}m\u00d7m. Recall that the mapping \u03a6k is the sign function, and so yi,j,k \u2248 sgn(xi,j,k). Approximating {\u00b11} with a quadratic penalty may yield a \u201csmall-margin\u201d solution, since high-confidence predictions will push low-confidence predictions closer to the decision boundary. To get a \u201clarge-margin\u201d solution, we use the smooth hinge loss (Rennie and Srebro, 2005a), `h(y, x) , h(yx), where\nh(z) ,  1/2\u2212 z if z \u2264 0, (1\u2212 z)2/2 if 0 < z < 1, 0 if z \u2265 1.\nUnlike the standard hinge loss, the smooth hinge is differentiable everywhere. To obtain closed-form gradients, we define tensors P,Q \u2208 Rm\u00d7m\u00d7n, where\npi,j,k ,\n{ 1 if 0 < yi,j,kxi,j,k < 1,\n0 otherwise,\nand\nqi,j,k ,\n{ 1 if yi,j,kxi,j,k < 1,\n0 otherwise.\nWe can therefore express the smooth hinge as\nh(yi,j,kxi,j,k) = (pi,j,kx 2 i,j,k\u22122qi,j,kyi,j,kxi,j,k+qi,j,k)/2,\nwhich we can differentiate w.r.t. Xk to obtain \u2207Xk`hk(Yk,Xk) = (Pk Xk \u2212Qk Yk).\nLogistic Loss: For binary relations, we can also use the logistic loss (Rennie and Srebro, 2005b), defined as `l(y, x) , log(1 + e\u2212yx)). From a statistical perspective, this corresponds to the negative conditional log-likelihood of a logistic model. Note that this loss function also maximizes the binary prediction margin yx. The gradient of `l is easily derived as \u2207Xk`lk(Yk,Xk) = \u2212Yk Zk, where zi,j,k , (1 + eyx)\u22121."}, {"heading": "3.4 Weighting and Efficiency", "text": "The weighting tensor W is a particularly important component of our framework. Without W, the objective function would place equal importance on fitting both observed and unobserved values. If the observed tensor is very sparse (as it often is in real training data), this will result in fitting a large number of \u201cphantom zeros\u201d. The weighting tensor prevents this from happening by emphasizing only the observed (or certain) entries. We can thus train on a small number of observations without fitting the unobserved\ndata. This approach is similar to Acar et al.\u2019s (2010), though their analysis is limited to the minimizing the quadratic loss for a CP decomposition.\nWeighting the objective by W also leads to an improvement in efficiency. When W is sparse, the objective and gradient calculations are fairly lightweight, because any expression involving W can be computed using sparse arithmetic. For instance, Xk only appears in a Hadamard product with Wk, so Equation 1 can be implemented as a sparse outer product, where we only compute xi,j,k for any nonzero wi,j,k. In Equations 2\u20135, the only expressions that do not involve W are the regularization terms. Thus, when W has only c nonzero elements, the computational costs of these equations are O(ncr + nmr2). In contrast, methods that ignore the sparsity of the observed tensor take O(nm2r+nmr2) time. Assuming that m2 is the dominant term and that c grows much slower than m2 (e.g., in natural networks, c is often O(m)), the sparse computation can be an order of magnitude faster.\nAdditionally, since W can be real-valued (not just {0, 1}), we can adjust the entries to reduce the mistake penalty of certain examples. For instance, suppose an incorrect negative prediction is deemed more critical than an incorrect positive (as is often the case in medical diagnoses and certain link prediction tasks). One could multiplicatively increase the values of all {wi,j,k : yi,j,k = 1} or, alternatively, decrease the values of all {wi,j,k : yi,j,k = \u22121}. This would effectively penalize false negatives more severely than false positives, encouraging the optimization to satisfy positive examples."}, {"heading": "3.5 Optimization", "text": "To minimize the objective in Equation 2, we use limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) optimization. Since quasi-Newton methods, such as L-BFGS, avoid computing the Hessian, they are efficient for optimization problems involving many variables. All this requires is the objective function and the gradients in Equations 3\u20135. Since our optimization problem is non-convex, we are not guaranteed that L-BFGS will find the global minimum; in practice, however, the algorithm typically finds useful, though possibly local, minima.\nTo mitigate the possibility of finding local minima, we initialize the parameters using the eigendecomposition of each input slice, which is close to the desired factorization. This technique is similar to the initialization used by Bader et al. (2007) and Nickel et al. (2011), which have similar decompositions. For k = 1, . . . , n, let \u039bk , (\u03bb1,k, . . . , \u03bbr,k) denote the r largest eigenvalues of Yk, and let Vk , (v1,k, . . . ,vr,k) denote their\ncorresponding eigenvectors. We initialize Rk as a diagonal matrix with \u039bk along the diagonal, and A as the average of V1, . . . ,Vn. In practice, we find that this initialization converges faster, and often to a better solution, than random initialization.\nNote that when the objective function uses only quadratic loss, one can compute the parameter updates using the alternating simultaneous approximation, least squares and Newton (ASALSAN) algorithm (Bader et al., 2007), which produces an approximate solution and has been shown to converge quickly. Since our objective may contain a heterogeneous mixture of loss functions\u2014not all necessarily quadratic\u2014we do not use ASALSAN. Morever, we cannot use traditional convex programming techniques like semidefinite programming (SDP) because our objective is non-convex."}, {"heading": "4 Experiments", "text": "In this section, we compare variants of MrWTD with RESCAL (Nickel et al., 2011), MMMF (Rennie and Srebro, 2005a) and Bayesian probabilistic tensor factorization (BPTF) (Xiong et al., 2010) in several experiments, using both real and synthetic data. The real data sources are kinship data from the Australian Alyawarra tribe, and two social interaction datasets from the MIT Media Lab. The comparisons highlight the critical advantages of MrWTD: namely, the ability to learn from limited training data, handle a mixture of learning objectives, transfer information across relations for collective learning, and exploit sparsity for improved efficiency.\nTo test the effect of the rank parameter, we run an experiment varying only the rank of the decomposition over a range of values. The results support our hypothesis that L2 regularization reduces the impact of the rank, effectively controlling the model complexity.\nFinally, we perform a synthetic experiment to compare the running time of MrWTD to that of the above competing methods, demonstrating the significant scalability gains provided by exploiting sparsity.\nTo conserve space, certain figures and tables are provided in the supplementary material (Appendix A)."}, {"heading": "4.1 Compared Methods", "text": "To evaluate the performance of various loss functions, we compare several variants of MrWTD. The variant named MrWTD-Q uses the quadratic loss for all relations, regardless of their type. MrWTD-H and MrWTD-L use the quadratic loss for real-valued slices and the smooth hinge or logistic loss, respectively, for binary slices.\nThe RESCAL model approximates each slice of the input tensor as Yk \u2248 ARkA>. In (Nickel et al., 2011), binary relationships are represented by {0, 1}. Unfortunately, since RESCAL does not account for missing data, unobserved relationships are simply treated as negative examples. In order to distinguish between (un)observed relationships and negative examples, we use {\u00b11} for observed data and zeros elsewhere. In our experiments, we find that this modification improves RESCAL\u2019s performance over the original method. Since RESCAL uses the quadratic loss uniformly, it uses ASALSAN to compute the decomposition, with L2 regularization on A and Rk.\nMMMF is a tool for matrix reconstruction and, as such, is not designed for multi-relational data. That said, we can use it to reconstruct each slice of the tensor individually. Like MrWTD, MMMF approximates a binary input Y using the sign of a rank-r matrix decomposition, Y \u2248 sgn(UV>), where U,V \u2208 Rm\u00d7r. The \u201cfast\u201d variant of the algorithm (Rennie and Srebro, 2005a) adds a bias term and uses the smooth hinge loss. The optimization objective is very similar to ours, but with different gradients, due to the decomposition. Our implementation of fast MMMF differs from that of Rennie and Srebro (2005a) only in the way we solve the optimization (using L-BFGS, rather than conjugate gradient descent) and the fact that the input is assumed to be symmetric.\nBecause the synthetic data generator (described in Section 4.2) matches our decomposition and is slightly different than that of traditional MMMF, it is somewhat unfair to compare traditional MMMF to MrWTD. We therefore run a variant of MrWTD that decomposes each slice separately instead of jointly, using a separate Ak. This is meant to equalize the discrepancy in the decomposition, while isolating the deficiencies of non-collective learning. We refer to this model as MMMF+.\nBPTF is a fully Bayesian interpretation of the CP tensor factorization, originally designed for temporal prediction.4 We compare it to MrWTD to investigate the benefits and drawbacks of the Bayesian approach. BPTF assumes that all latent factors are sampled from a Gaussian distribution. The only user-defined properties are the rank of the decomposition and the hyperparameters. The parameters and latent factors are estimated using Gibbs sampling. One benefit of the Bayesian approach is that it avoids the model selection problem, which in our case is the choice of regulariza-\n4Sutskever et al. (2009) propose another fully Bayesian algorithm, Bayesian tensor factorization (BTF), whose decomposition is very similar to ours, though their framework only supports the quadratic loss. We were unable to compare MrWTD to this method at the time of submission.\ntion parameters.5 This can have a pronounced effect when training data is limited, which makes proper regularization critical. However, Gibbs sampling is computationally expensive, since it requires many iterations of sampling to converge to an accurate estimate. We analyze this trade-off between accuracy and efficiency in Section 4.5. Additionally, BPTF only supports the quadratic loss, since it has a natural probabilistic interpretation as the Gaussian likelihood and makes the model conjugate, making Gibbs sampling easier. No such interpretation exists for the (smooth) hinge loss, and the logistic loss has no conjugate prior.\nWe implement all of the above methods in MATLAB, using a third-party implementation of L-BFGS6, and the authors\u2019 implementation of BPTF7."}, {"heading": "4.2 Synthetic Data Experiments", "text": "To generate the synthetic data, we start by computing a low-rank tensor X\u0302 \u2208 Rm\u00d7m\u00d7n as X\u0302k \u2190 A\u0302R\u0302kA\u0302> + Ek, for k = 1, . . . , n, where A\u0302 \u2208 Rm\u00d7r and R\u0302k \u2208 Rr\u00d7r are sampled from a normal distribution, and Ek \u2208 Rm\u00d7m is low-level, normally-distributed noise. For the first experiment, we construct n = 3 binary relations (i.e., slices), over m = 500 objects, using rank r = 10. We refer to this dataset as Binary Synthetic. To generate a binary tensor Y \u2208 {\u00b11}m\u00d7m\u00d7n, we round the values of X\u0302 using the 90th percentile of its values as a threshold. This produces a heavy skew towards the negative class, as is typical in real multi-relational data. For the second experiment, we construct one binary relation and one real-valued relation, again over 500 objects, with rank 10. We normalize the realvalued relation such that the standard deviation is 1.0, giving it roughly the same scale as the binary slices. We refer to this dataset as Mixed Synthetic.\nWe evaluate over training sizes t \u2208 [3, 25] percent, averaging the results over 20 runs per size. In each run, we sample a random t \u00b7 ( m 2 ) pairs (and their symmetric counterparts) from each slice to use as the training set, and let the remaining pairs comprise the test set. We then hold out a random 25% from the training set as a validation set for a regularization parameter search, where we search over the range [10\u22123, 103] in logarithmic increments. For Binary Synthetic, we select the optimal parameter \u03bb\u2217 that maximizes the area under the precision-recall curve (AUPRC), averaged over all slices; for Mixed Synthetic, we maximize the harmonic mean of the AUPRC of the first slice and one minus the mean-squared error (MSE) of the second. We then\n5As the authors claim, the effect of tuning the hyperparameter priors is minimal.\n6www.di.ens.fr/~mschmidt/Software/minFunc.html 7www.cs.cmu.edu/~lxiong/bptf/bptf.html\nretrain on the full training set using \u03bb\u2217 and evaluate on the test set. For BPTF, we run Gibbs sampling for 200 iterations.\nThe results of the synthetic data experiments are given in Figure 2, reported as average AUPRC and MSE over 20 runs. On Binary Synthetic, MrWTD-L achieves a statistically significant8 lift over the competing methods for training sizes 5% and up, and all three variants showing significant lift for 10% and above. We attribute these results to two primary advantages: the weighted objective function, with its mixture of taskspecific loss functions, and the global latent factors. As discussed in Section 3.5, the weighted objective is necessary for exploiting small amounts of observed (i.e., training) data, without fitting the unobserved entries. Since RESCAL treats all entries as observed, it tends to fit the unobserved entries in sparsely populated tensors. Furthermore, though MMMF and MMMF+ use the same large-margin technique as MrWTD-H and MrWTD-L, they do not perform collective learning, since the latent factors are specific to each slice. In MrWTD, information from one slice is propagated to the others via the global latent factors. Note that BPTF and MrWTD-Q perform significantly worse the large-margin loss variants of MrWTD for sizes 10% and above, illustrating that the quadratic loss is not always appropriate for binary data. On Mixed Synthetic, MrWTD\u2019s improvement over RESCAL and MMMF, for both slices, is statistically significantly for all training sizes. MMMF+ is competitive with MrWTD on the real-valued slice, with significant lift for training sizes 3, 5%; yet its performance deteriorates on the binary slice, for all training sizes, since it is not able to transfer information between slices. BPTF is also competitive with MrWTD on the realvalued slice, and the binary slice for smaller training sizes, but falls slightly behind on the higher sizes."}, {"heading": "4.3 Real Data Experiments", "text": "We evaluate on several real multi-relational datasets. The first dataset consists of kinship data from the Australian Alyawarra tribe, as recorded by Denham and White (2005). This data has previously been used by Kemp et al. (2006) for multi-relational link prediction. The data contains m = 104 tribe members and n = 23 types of kinship (binary) relations.9 In total, the dataset includes 125,580 related pairs. This yields a tensor Y \u2208 {\u00b11}104\u00d7104\u00d723. The remaining datasets come from MIT\u2019s Human Dy-\n8We measure statistical significance in all experiments using a 2-sample t-test with rejection threshold 0.05.\n9The original data contains 26 relations, but relations 24\u201326 are extremely sparse, exhibiting fewer than 6 instances, so we omit them.\nnamics Laboratory. Both consist of human interaction data from students, faculty, and staff working on the MIT campus, recorded by a mobile phone application. From the first dataset, named Reality Mining (Eagle et al., 2009), we use the survey-annotated network, consisting of n = 3 types of binary relationships annotated by the subjects: friendship, in-lab interaction and out-of-lab interaction. These relationships are measured between m = 94 participants, providing a total of 13,395 related pairs. The resulting tensor is Y \u2208 {\u00b11}94\u00d794\u00d73. From the second dataset, named Social Evolution (Dong et al., 2011), we use the survey-annotated network, as well as several interaction relations derived from sensor data, resulting in n = 8 binary relations with 16,101 related pairs. The five surveyed relations are: close friendship, biweekly social interaction, political discussion, two types of social media interaction. The three derived relations are computed from: voice calls, SMS messaging and proximity. We binarize this data by a simple indicator of whether the given type of interaction occurred. In this case, the number of users is m = 84, results in a tensor Y \u2208 {\u00b11}84\u00d784\u00d78. For these experiments, we use the same methodology as the synthetic experiments, with rank r = 20. The results are also given in Figure 3. The three variants of MrWTD and BPTF achieve significant lift over RESCAL, MMMF and MMMF+ in nearly all experiments. MrWTD has a statistically significant advantage over the other methods for most training ratios on the Kinship data, while BPTF has an advantage on the Social Evolution data. Yet, as we show in the following section, MrWTD\u2019s estimation takes a small fraction of BPTF\u2019s running time. We therefore achieve results that are comparable to Bayesian methods in far\nless time. We refer the reader to Table 1 in the appendix for the complete set of results."}, {"heading": "4.4 Rank Experiment", "text": "To measure the effect of the rank parameter on the performance of each algorithm, we rerun the Social Evolution experiment, varying r = {5, 10, 20, 40} and keeping the training ratio is fixed at 25%. The results of this experiment are displayed in Figure 5, in the appendix. There is a small increase in AUC from r = 5 to r = 10, which is expected, since 5 is relatively low. However, we find that the effect of the rank is minimal for r \u2265 10; the standard deviation across all runs in this range is < 0.02 for each algorithm. This supports our hypothesis that, beyond a certain threshold, the regularizer is the primary controller of model complexity."}, {"heading": "4.5 Timing Experiment", "text": "Finally, we measure the running time of each of the above tensor methods to better understand their scalability in scenarios where training data is limited. We create a sequence of synthetic datasets (using the technique in Section 4.2), each with n = 3 binary slices, for sizes m = {500, 1000, 2000, 4000, 8000}. For training, we use a random 10% of the tensor. We compare the smooth hinge loss variant of MrWTD, RESCAL and BPTF, using predefined regularization and hyperparameters. We run these experiments on a machine with two 6-core Intel R\u00a9 Xeon R\u00a9 X5650 processors, running at 2.66 GHz, and 48 GB of RAM.\nThe timing results, averaged over 10 runs per problem size, are shown in Figure 4. BPTF takes considerably more time than the others, due to its Gibbs sampling\nestimation. Note that we could not run BPTF on the two largest problem sizes, due to out-of-memory exceptions. This illustrates the tradeoff between accuracy and efficiency in using Bayesian methods; one can reduce running time by reducing the number of iterations, but this would also affect the accuracy of the estimation. Due to the efficient, closed-form updates of the ASALSAN algorithm, RESCAL is the fastest for small problem sizes. However, MrWTD is significantly faster as the problem size grows. This is because RESCAL\u2019s objective function treats all tensor entries with equal importance, whereas MrWTD\u2019s weighted objective only requires the predictions of the observed entries, thus allowing us to skip prediction on the test data during estimation."}, {"heading": "5 Conclusion", "text": "In this paper, we present a modular framework for multi-relational learning via tensor decomposition. The decomposition we use provides an intuitive interpretation for the multi-relational domain, where objects have global latent representations and relationships are determined by a function of their linear combinations. We show that the global latent representations enable information to transfer between relation types during model estimation. Further, we demonstrate that our framework\u2019s weighted objective and support for multiple loss functions improves accuracy over similar models. Finally, we show that our method exploits the sparsity of limited training data to achieve an order of magnitude speedup over unweighted methods.\nWe plan to extend MrWTD to be able to learn from large-scale data by adapting hashing methods from matrix factorization literature (Karatzoglou et al.,\n2010). We would also like to compare our method to Sutskever et al.\u2019s BTF algorithm (2009), to further investigate the benefit of the Bayesian approach. We also intend to analyze the theoretical properties of our framework, such as generalization error, using existing learning theory literature (Srebro et al., 2005a; Cortes et al., 2008; El-Yaniv and Pechyony, 2009)."}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF CAREER grant 0746930 and NSF grant IIS1218488."}, {"heading": "A Supplementary Material", "text": "Here we report additional results from the experiments discussed in Section 4. Figure 5 shows the results of the rank experiment (Section 4.4). Table 1 shows the results of the timing experiment (Section 4.5). In Table 2 and Table 3, we list the full results of the synthetic (Section 4.2) and real data (Section 4.3) experiments."}], "references": [{"title": "Scalable tensor factorizations with missing data", "author": ["E. Acar", "D. Dunlavy", "T. Kolda", "M. M\u00f8rup"], "venue": "In Proc. of the 2010 SIAM International Conf. on Data Mining (SDM),", "citeRegEx": "Acar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2010}, {"title": "Temporal analysis of semantic graphs using ASALSAN", "author": ["B. Bader", "R. Harshman", "T. Kolda"], "venue": "In Proc. of the 7th IEEE International Conf. on Data Mining (ICDM),", "citeRegEx": "Bader et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bader et al\\.", "year": 2007}, {"title": "Stability of transductive regression algorithms", "author": ["C. Cortes", "M. Mohri", "D. Pechyony", "A. Rastogi"], "venue": "In Proc. of the 25th International Conf. on Machine Learning (ICML),", "citeRegEx": "Cortes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2008}, {"title": "Multiple measures of Alyawarra kinship", "author": ["W. Denham", "D. White"], "venue": "Field Methods,", "citeRegEx": "Denham and White.,? \\Q2005\\E", "shortCiteRegEx": "Denham and White.", "year": 2005}, {"title": "Multilinear algebra for analyzing data with multiple linkages", "author": ["D. Dunlavy", "T. Kolda", "W. Kegelmeyer"], "venue": "Technical Report,", "citeRegEx": "Dunlavy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dunlavy et al\\.", "year": 2006}, {"title": "Temporal link prediction using matrix and tensor factorizations", "author": ["D. Dunlavy", "T. Kolda", "E. Acar"], "venue": "ACM Trans. on Knowledge Discovery from Data,", "citeRegEx": "Dunlavy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dunlavy et al\\.", "year": 2011}, {"title": "Inferring friendship network structure by using mobile phone data", "author": ["N. Eagle", "A. Pentland", "D. Lazer"], "venue": "Proc. of the National Academy of Sciences,", "citeRegEx": "Eagle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eagle et al\\.", "year": 2009}, {"title": "Transductive Rademacher complexity and its applications", "author": ["R. El-Yaniv", "D. Pechyony"], "venue": "J. Artificial Intelligence Research (JAIR),", "citeRegEx": "El.Yaniv and Pechyony.,? \\Q2009\\E", "shortCiteRegEx": "El.Yaniv and Pechyony.", "year": 2009}, {"title": "Link pattern prediction with tensor decomposition in multirelational networks", "author": ["S. Gao", "L. Denoyer", "P. Gallinari"], "venue": "In IEEE Symposium on Comp. Intell. and Data Mining,", "citeRegEx": "Gao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Models for analysis of asymmetrical relationships", "author": ["R. Harshman"], "venue": "In First Joint Meeting of the Psychometric Society and the Society for Mathematical Psychology,", "citeRegEx": "Harshman.,? \\Q1978\\E", "shortCiteRegEx": "Harshman.", "year": 1978}, {"title": "Collaborative filtering on a budget", "author": ["A. Karatzoglou", "A. Smola", "M. Weimer"], "venue": "In Proc. of the 13th International Conf. on Artificial Intelligence and Statistics,", "citeRegEx": "Karatzoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Karatzoglou et al\\.", "year": 2010}, {"title": "Link propagation: a fast semisupervised learning algorithm for link prediction", "author": ["H. Kashima", "T. Kato", "Y. Yamanishi", "M. Sugiyama", "K. Tsuda"], "venue": "In SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Kashima et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kashima et al\\.", "year": 2009}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J. Tenenbaum", "T. Griffiths", "T. Yamada", "N. Ueda"], "venue": "In Proc. of the 21st National Conf. on Artificial Intelligence,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H. Kriegel"], "venue": "In Proc. of the 28th International Conf. on Machine Learning (ICML),", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["J. Rennie", "N. Srebro"], "venue": "In In Proc. of the 22nd International Conf. on Machine Learning (ICML),", "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Loss functions for preference levels: regression with discrete ordered labels", "author": ["J. Rennie", "N. Srebro"], "venue": "In IJCAI Multidisciplinary Workshop on Adv. in Preference Handling,", "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Generalization error bounds for collaborative prediction with lowrank matrices", "author": ["N. Srebro", "N. Alon", "T. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Maximummargin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Temporal collaborative filtering with Bayesian probabilistic tensor factorization", "author": ["L. Xiong", "X. Chen", "T. Huang", "J. Schneider", "J. Carbonell"], "venue": "In SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Xiong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "There has been a growing interest in tensor methods within machine learning, partially due to their natural representation of multi-relational data (Kashima et al., 2009).", "startOffset": 148, "endOffset": 170}, {"referenceID": 8, "context": "Many contributions (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010) use the canonical polyadic (CP) decomposition, a generalization of singular value decomposition to tensors.", "startOffset": 19, "endOffset": 85}, {"referenceID": 19, "context": "Many contributions (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010) use the canonical polyadic (CP) decomposition, a generalization of singular value decomposition to tensors.", "startOffset": 19, "endOffset": 85}, {"referenceID": 1, "context": "Others (Bader et al., 2007) have proposed models based on decomposition into directional components (DEDICOM) (Harshman, 1978).", "startOffset": 7, "endOffset": 27}, {"referenceID": 9, "context": ", 2007) have proposed models based on decomposition into directional components (DEDICOM) (Harshman, 1978).", "startOffset": 90, "endOffset": 106}, {"referenceID": 13, "context": "We propose a similar decomposition (based on (Nickel et al., 2011)) which is more appropriate for multi-relational data, for reasons discussed in Section 3.", "startOffset": 45, "endOffset": 66}, {"referenceID": 8, "context": "Our tensor model is comparable to Harshman\u2019s DEDICOM (1978). Bader et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 1, "context": "Bader et al. (2007) applied the DEDICOM model to the task of temporal link prediction (in a single network), using the third mode as the time dimension.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Bader et al. (2007) applied the DEDICOM model to the task of temporal link prediction (in a single network), using the third mode as the time dimension. Recently, Nickel et al. (2011) proposed a relaxed DEDICOM, referred to as RESCAL, to solve several canonical multi-relational learning tasks.", "startOffset": 0, "endOffset": 184}, {"referenceID": 8, "context": "Other tensor factorization models have been proposed for multi-relational data, though they typically use the CP decomposition (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010).", "startOffset": 127, "endOffset": 193}, {"referenceID": 19, "context": "Other tensor factorization models have been proposed for multi-relational data, though they typically use the CP decomposition (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010).", "startOffset": 127, "endOffset": 193}, {"referenceID": 4, "context": "Other tensor factorization models have been proposed for multi-relational data, though they typically use the CP decomposition (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010). In the CP decomposition, each entry is the inner product of three vectors; this would be similar to our decomposition if each Rk slice were constrained to be diagonal. The richer interactions of the relaxed DEDICOM and the global latent representation of the objects often make it better suited for multi-relational learning, as was corroborated empirically by Nickel et al. (2011).", "startOffset": 128, "endOffset": 577}, {"referenceID": 0, "context": "This approach is similar to Acar et al.\u2019s (2010), though their analysis is limited to the minimizing the quadratic loss for a CP decomposition.", "startOffset": 28, "endOffset": 49}, {"referenceID": 1, "context": "This technique is similar to the initialization used by Bader et al. (2007) and Nickel et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 1, "context": "This technique is similar to the initialization used by Bader et al. (2007) and Nickel et al. (2011), which have similar decompositions.", "startOffset": 56, "endOffset": 101}, {"referenceID": 1, "context": "Note that when the objective function uses only quadratic loss, one can compute the parameter updates using the alternating simultaneous approximation, least squares and Newton (ASALSAN) algorithm (Bader et al., 2007), which produces an approximate solution and has been shown to converge quickly.", "startOffset": 197, "endOffset": 217}, {"referenceID": 13, "context": "In this section, we compare variants of MrWTD with RESCAL (Nickel et al., 2011), MMMF (Rennie and Srebro, 2005a) and Bayesian probabilistic tensor factorization (BPTF) (Xiong et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 19, "context": ", 2011), MMMF (Rennie and Srebro, 2005a) and Bayesian probabilistic tensor factorization (BPTF) (Xiong et al., 2010) in several experiments, using both real and synthetic data.", "startOffset": 96, "endOffset": 116}, {"referenceID": 13, "context": "In (Nickel et al., 2011), binary relationships are represented by {0, 1}.", "startOffset": 3, "endOffset": 24}, {"referenceID": 14, "context": "The \u201cfast\u201d variant of the algorithm (Rennie and Srebro, 2005a) adds a bias term and uses the smooth hinge loss. The optimization objective is very similar to ours, but with different gradients, due to the decomposition. Our implementation of fast MMMF differs from that of Rennie and Srebro (2005a) only in the way we solve the optimization (using L-BFGS, rather than conjugate gradient descent) and the fact that the input is assumed to be symmetric.", "startOffset": 37, "endOffset": 299}, {"referenceID": 3, "context": "The first dataset consists of kinship data from the Australian Alyawarra tribe, as recorded by Denham and White (2005). This data has previously been used by Kemp et al.", "startOffset": 95, "endOffset": 119}, {"referenceID": 3, "context": "The first dataset consists of kinship data from the Australian Alyawarra tribe, as recorded by Denham and White (2005). This data has previously been used by Kemp et al. (2006) for multi-relational link prediction.", "startOffset": 95, "endOffset": 177}, {"referenceID": 6, "context": "From the first dataset, named Reality Mining (Eagle et al., 2009), we use the survey-annotated network, consisting of n = 3 types of binary relationships annotated by the subjects: friendship, in-lab interaction and out-of-lab interaction.", "startOffset": 45, "endOffset": 65}, {"referenceID": 2, "context": "We also intend to analyze the theoretical properties of our framework, such as generalization error, using existing learning theory literature (Srebro et al., 2005a; Cortes et al., 2008; El-Yaniv and Pechyony, 2009).", "startOffset": 143, "endOffset": 215}, {"referenceID": 7, "context": "We also intend to analyze the theoretical properties of our framework, such as generalization error, using existing learning theory literature (Srebro et al., 2005a; Cortes et al., 2008; El-Yaniv and Pechyony, 2009).", "startOffset": 143, "endOffset": 215}, {"referenceID": 14, "context": "We would also like to compare our method to Sutskever et al.\u2019s BTF algorithm (2009), to further investigate the benefit of the Bayesian approach.", "startOffset": 44, "endOffset": 84}], "year": 2013, "abstractText": "We propose a modular framework for multirelational learning via tensor decomposition. In our learning setting, the training data contains multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a function of a linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using quasi-Newton optimization of a weighted objective function. Sparsity in the observed data is captured by the weighted objective, leading to improved accuracy when training data is limited. Exploiting sparsity also improves efficiency, potentially up to an order of magnitude over unweighted approaches. In addition, our framework accommodates arbitrary combinations of smooth, task-specific loss functions, making it better suited for learning different types of relations. For the typical cases of real-valued functions and binary relations, we propose several loss functions and derive the associated parameter gradients. We evaluate our method on synthetic and real data, showing significant improvements in both accuracy and scalability over related factorization techniques.", "creator": "TeX"}}}