{"id": "1303.4434", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems", "abstract": "traditional non - convex sparsity - less inducing penalties have already recently received certain considerable general attentions mostly in sparse learning. recent theoretical investigations have steadily demonstrated algorithms their superiority potentially over detecting the convex counterparts in most several nonlinear sparse learning settings. again however, easy solving is the non - convex optimization for problems associated with finite non - convex np penalties unfortunately remains indeed a big big challenge. one a commonly globally used approach is the multi - smoothing stage ( ms ) convex interval relaxation ( delta or maximum dc programming ), formulation which relaxes the original non - convex reduction problem to a generic sequence representation of particular convex problems. naturally this approach is usually not found very practical for large - diameter scale accounting problems because solving its computational cost function is a multiple of solving a related single convex exclusion problem. \u2022 in using this paper, we propose a general solution iterative shrinkage ensemble and topological thresholding ( term gist ) algorithm to solve the nonconvex optimization problem procedure for avoiding a large class of non - regular convex penalties. the gist algorithm iteratively solves a proximal operator closure problem, every which has in turn has a complementary closed - form elimination solution for many highly commonly used penalties. approaching at best each minimum outer scaled iteration instance of writing the cochran algorithm, we use a dynamic line search initialized by constructing the extended barzilai - borwein ( sh bb ) rule that which allows individuals finding an appropriate step size quickly. the paper preparation also presents a detailed convergence simulator analysis of the basic gist algorithm. moreover the relative efficiency of the proposed nonlinear algorithm is recently demonstrated by having extensive estimation experiments on large - scale data sets.", "histories": [["v1", "Mon, 18 Mar 2013 21:41:53 GMT  (98kb)", "http://arxiv.org/abs/1303.4434v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA stat.CO stat.ML", "authors": ["pinghua gong", "changshui zhang", "zhaosong lu", "jianhua huang", "jieping ye"], "accepted": true, "id": "1303.4434"}, "pdf": {"name": "1303.4434.pdf", "metadata": {"source": "META", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems", "authors": ["Pinghua Gong", "Jianhua Z. Huang"], "emails": ["gph08@mails.tsinghua.edu.cn", "zcs@mail.tsinghua.edu.cn", "zhaosong@sfu.ca", "jianhua@stat.tamu.edu", "jieping.ye@asu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n44 34\nv1 [\ncs .L\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nBorwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets."}, {"heading": "1. Introduction", "text": "Learning sparse representations has important applications in many areas of science and engineering. The use of an \u21130-norm regularizer leads to a sparse solution, however the \u21130-norm regularized optimization problem is challenging to solve, due to the discontinuity and non-convexity of the \u21130-norm regularizer. The \u21131-norm regularizer, a continuous and convex surrogate, has been studied extensively in the literature (Tibshirani, 1996; Efron et al., 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al., 2008; Beck & Teboulle, 2009; Wright et al., 2009; Ye & Liu, 2012). Although the \u21131norm based sparse learning formulations have achieved great success, they have been shown to be suboptimal in many cases (Candes et al., 2008; Zhang, 2010b; 2012), since the \u21131-norm is a loose approximation of the \u21130-norm and often leads to an over-penalized problem. To address this issue, many non-convex regularizers, interpolated between the \u21130-norm and\nthe \u21131-norm, have been proposed to better approximate the \u21130-norm. They include \u2113q-norm (0 < q < 1) (Foucart & Lai, 2009), Smoothly Clipped Absolute Deviation (SCAD) (Fan & Li, 2001), Log-Sum Penalty (LSP) (Candes et al., 2008), Minimax Concave Penalty (MCP) (Zhang, 2010a), Geman Penalty (GP) (Geman & Yang, 1995; Trzasko & Manduca, 2009) and Capped-\u21131 penalty (Zhang, 2010b; 2012; Gong et al., 2012a).\nAlthough the non-convex regularizers (penalties) are appealing in sparse learning, it is challenging to solve the corresponding non-convex optimization problems. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm for a large class of non-convex penalties. The key step of the proposed algorithm is to compute a proximal operator, which has a closed-form solution for many commonly used non-convex penalties. In our algorithm, we adopt the Barzilai-Borwein (BB) rule (Barzilai & Borwein, 1988) to initialize the line search step size at each iteration, which greatly accelerates the convergence speed. We also use a non-monotone line search criterion to further speed up the convergence of the algorithm. In addition, we present a detailed convergence analysis for the proposed algorithm. Extensive experiments on largescale real-world data sets demonstrate the efficiency of the proposed algorithm."}, {"heading": "2. The Proposed Algorithm: GIST", "text": ""}, {"heading": "2.1. General Problems", "text": "We consider solving the following general problem:\nmin w\u2208Rd {f(w) = l(w) + r(w)} . (1)\nWe make the following assumptions on the above formulation throughout the paper:\nA1 l(w) is continuously differentiable with Lipschitz continuous gradient, that is, there exists a positive constant \u03b2(l) such that\n\u2016\u2207l(w)\u2212\u2207l(u)\u2016 \u2264 \u03b2(l)\u2016w \u2212 u\u2016, \u2200w,u \u2208 Rd.\nA2 r(w) is a continuous function which is possibly non-smooth and non-convex, and can be rewritten as the difference of two convex functions, that is,\nr(w) = r1(w)\u2212 r2(w),\nwhere r1(w) and r2(w) are convex functions.\nA3 f(w) is bounded from below.\nRemark 1 We say that w\u22c6 is a critical point of problem (1), if the following holds (Toland, 1979; Wright et al., 2009):\n0 \u2208 \u2207l(w\u22c6) + \u2202r1(w \u22c6)\u2212 \u2202r2(w \u22c6),\nwhere \u2202r1(w \u22c6) is the sub-differential of the function r1(w) at w = w \u22c6, that is,\n\u2202r1(w \u22c6) =\n{\ns : r1(w) \u2265 r1(w \u22c6) + \u3008s,w\u2212w\u22c6\u3009, \u2200w \u2208 Rd\n}\n.\nWe should mention that the sub-differential is nonempty on any convex function; this is why we make the assumption that r(w) can be rewritten as the difference of two convex functions."}, {"heading": "2.2. Some Examples", "text": "Many formulations in machine learning satisfy the assumptions above. The following least square and logistic loss functions are two commonly used ones which satisfy assumption A1:\nl(w) = 1\n2n \u2016Xw\u2212 y\u20162 or\n1\nn\nn \u2211\ni=1\nlog ( 1 + exp(\u2212yix T i w) ) ,\nwhere X = [xT1 ; \u00b7 \u00b7 \u00b7 ;x T n ] \u2208 R n\u00d7d is a data matrix and y = [y1, \u00b7 \u00b7 \u00b7 , yn]\nT \u2208 Rn is a target vector. The regularizers (penalties) which satisfy the assumption A2 are presented in Table 1. They are non-convex (except the \u21131-norm) and extensively used in sparse learning. The functions l(w) and r(w) mentioned above are nonnegative. Hence, f is bounded from below and satisfies assumption A3."}, {"heading": "2.3. Algorithm", "text": "Our proposed General Iterative Shrinkage and Thresholding (GIST) algorithm solves problem (1) by generating a sequence {w(k)} via:\nw(k+1) =argmin w l(w(k)) + \u3008\u2207l(w(k)),w \u2212w(k)\u3009\n+ t(k)\n2 \u2016w\u2212w(k)\u20162 + r(w), (2)\nIn fact, problem (2) is equivalent to the following proximal operator problem:\nw(k+1) = argmin w\n1 2 \u2016w\u2212 u(k)\u20162 + 1 t(k) r(w),\nwhere u(k) = w(k) \u2212 \u2207l(w(k))/t(k). Thus, in GIST we first perform a gradient descent along the direction \u2212\u2207l(w(k)) with step size 1/t(k) and then solve a proximal operator problem. For all the regularizers listed in Table 1, problem (2) has a closed-form solution (details are provided in the Appendix), although it may\nAlgorithm 1 GIST: General Iterative Shrinkage and Thresholding Algorithm\n1: Choose parameters \u03b7 > 1 and tmin, tmax with 0 < tmin < tmax; 2: Initialize iteration counter k \u2190 0 and a bounded starting point w(0); 3: repeat 4: t(k) \u2208 [tmin, tmax]; 5: repeat 6: w(k+1) \u2190 argmin\nw l(w(k)) + \u3008\u2207l(w(k)),w \u2212\nw(k)\u3009+ t (k)\n2 \u2016w \u2212w(k)\u20162 + r(w);\n7: t(k) \u2190 \u03b7t(k); 8: until some line search criterion is satisfied 9: k \u2190 k + 1 10: until some stopping criterion is satisfied\nbe a non-convex problem. For example, for the \u21131 and Capped \u21131 regularizers, we have closed-form solutions as follows:\n\u21131 : w (k+1) i = sign(u (k) i )max\n(\n0, |u (k) i | \u2212 \u03bb/t\n(k) )\n,\nCapped \u21131 : w (k+1) i =\n{\nx1, if hi(x1) \u2264 hi(x2), x2, otherwise,\nwhere x1 = sign(u (k) i )max(|u (k) i |, \u03b8), x2 = sign(u (k) i )min(\u03b8, [|u (k) i | \u2212 \u03bb/t (k)]+) and hi(x) = 0.5(x \u2212 u (k) i )\n2 + \u03bb/t(k) min(|x|, \u03b8). The detailed procedure of the GIST algorithm is presented in Algorithm 1. There are two issues that remain to be addressed: how to initialize t(k) (in Line 4) and how to select a line search criterion (in Line 8) at each outer iteration.\n2.3.1. The Step Size Initialization: 1/t(k)\nIntuitively, a good step size initialization strategy at each outer iteration can greatly reduce the line search cost (Lines 5-8) and hence is critical for the fast con-\nvergence of the algorithm. In this paper, we propose to initialize the step size by adopting the BarzilaiBorwein (BB) rule (Barzilai & Borwein, 1988), which uses a diagonal matrix t(k)I to approximate the Hessian matrix \u22072l(w) at w = w(k). Denote\nx(k) = w(k) \u2212w(k\u22121), y(k) = \u2207l(w(k))\u2212\u2207l(w(k\u22121)).\nThen t(k) is initialized at the outer iteration k as\nt(k) = argmin t\n\u2016tx(k) \u2212 y(k)\u20162 = \u3008x(k),y(k)\u3009\n\u3008x(k),x(k)\u3009 ."}, {"heading": "2.3.2. Line Search Criterion", "text": "One natural and commonly used line search criterion is to require that the objective function value is monotonically decreasing. More specifically, we propose to accept the step size 1/t(k) at the outer iteration k if the following monotone line search criterion is satisfied:\nf(w(k+1)) \u2264 f(w(k))\u2212 \u03c3\n2 t(k)\u2016w(k+1) \u2212w(k)\u20162, (3)\nwhere \u03c3 is a constant in the interval (0, 1).\nA variant of the monotone criterion in Eq. (3) is a nonmonotone line search criterion (Grippo et al., 1986; Grippo & Sciandrone, 2002; Wright et al., 2009). It possibly accepts the step size 1/t(k) even if w(k+1) yields a larger objective function value than w(k). Specifically, we propose to accept the step size 1/t(k), if w(k+1) makes the objective function value smaller than the maximum over previous m (m > 1) iterations, that is,\nf(w(k+1)) \u2264 max i=max(0,k\u2212m+1),\u00b7\u00b7\u00b7 ,k f(w(i))\n\u2212 \u03c3\n2 t(k)\u2016w(k+1) \u2212w(k)\u20162, (4)\nwhere \u03c3 \u2208 (0, 1)."}, {"heading": "2.3.3. Convergence Analysis", "text": "Inspired by Wright et al. (2009); Lu (2012a), we present detailed convergence analysis under both monotone and non-monotone line search criteria. We first present a lemma which guarantees that the monotone line search criterion in Eq. (3) is satisfied. This is a basic support for the convergence of Algorithm 1.\nLemma 1 Let the assumptions A1-A3 hold and the constant \u03c3 \u2208 (0, 1) be given. Then for any integer k \u2265 0, the monotone line search criterion in Eq. (3) is satisfied whenever t(k) \u2265 \u03b2(l)/(1\u2212 \u03c3).\nProof Since w(k+1) is a minimizer of problem (2), we have\n\u3008\u2207l(w(k)),w(k+1) \u2212w(k)\u3009+ t(k)\n2 \u2016w(k+1) \u2212w(k)\u20162\n+ r(w(k+1)) \u2264 r(w(k)). (5)\nIt follows from assumption A1 that\nl(w(k+1)) \u2264l(w(k)) + \u3008\u2207l(w(k)),w(k+1) \u2212w(k)\u3009\n+ \u03b2(l)\n2 \u2016w(k+1) \u2212w(k)\u20162. (6)\nCombining Eq. (5) and Eq. (6), we have\nl(w(k+1)) + r(w(k+1)) \u2264 l(w(k)) + r(w(k))\n\u2212 t(k) \u2212 \u03b2(l)\n2 \u2016w(k+1) \u2212w(k)\u20162.\nIt follows that\nf(w(k+1)) \u2264 f(w(k))\u2212 t(k) \u2212 \u03b2(l)\n2 \u2016w(k+1) \u2212w(k)\u20162.\nTherefore, the line search criterion in Eq. (3) is satisfied whenever (t(k) \u2212 \u03b2(l))/2 \u2265 \u03c3t(k)/2, i.e., t(k) \u2265 \u03b2(l)/(1\u2212 \u03c3). This completes the proof the lemma.\nNext, we summarize the boundedness of t(k) in the following lemma.\nLemma 2 For any k \u2265 0, t(k) is bounded under the monotone line search criterion in Eq. (3).\nProof It is trivial to show that t(k) is bounded from below, since t(k) \u2265 tmin (tmin is defined in Algorithm 1). Next we prove that t(k) is bounded from above by contradiction. Assume that there exists a k \u2265 0, such that t(k) is unbounded from above. Without loss of generality, we assume that t(k) increases monotonically to +\u221e and t(k) \u2265 \u03b7\u03b2(l)/(1\u2212\u03c3). Thus, the value t = t(k)/\u03b7 \u2265 \u03b2(l)/(1 \u2212 \u03c3) must have been tried at iteration k and does not satisfy the line\nsearch criterion in Eq. (3). But Lemma 1 states that t = t(k)/\u03b7 \u2265 \u03b2(l)/(1 \u2212 \u03c3) is guaranteed to satisfy the line search criterion in Eq. (3). This leads to a contradiction. Thus, t(k) is bounded from above.\nRemark 2 We note that if Eq. (3) holds, Eq. (4) is guaranteed to be satisfied. Thus, the same conclusions in Lemma 1 and Lemma 2 also hold under the the non-monotone line search criterion in Eq. (4).\nBased on Lemma 1 and Lemma 2, we present our convergence result in the following theorem.\nTheorem 1 Let the assumptions A1-A3 hold and the monotone line search criterion in Eq. (3) be satisfied. Then all limit points of the sequence { w(k) }\ngenerated by Algorithm 1 are critical points of problem (1).\nProof Based on Lemma 1, the monotone line search criterion in Eq. (3) is satisfied and hence\nf(w(k+1)) \u2264 f(w(k)), \u2200k \u2265 0,\nwhich implies that the sequence { f(w(k)) }\nk=0,1,\u00b7\u00b7\u00b7 is\nmonotonically decreasing. Let w\u22c6 be a limit point of the sequence { w(k) }\n, that is, there exists a subsequence K such that\nlim k\u2208K\u2192\u221e\nw(k) = w\u22c6.\nSince f is bounded from below, together with the fact that { f(w(k)) } is monotonically decreasing, limk\u2192\u221e f(w (k)) exists. Observing that f is continuous, we have\nlim k\u2192\u221e f(w(k)) = lim k\u2208K\u2192\u221e f(w(k)) = f(w\u22c6).\nTaking limits on both sides of Eq. (3) with k \u2208 K, we have\nlim k\u2208K\u2192\u221e\n\u2016w(k+1) \u2212w(k)\u2016 = 0. (7)\nConsidering that the minimizerw(k+1) is also a critical point of problem (2) and r(w) = r1(w) \u2212 r2(w), we have\n0 \u2208\u2207l(w(k)) + t(k)(w(k+1) \u2212w(k))\n+ \u2202r1(w (k+1))\u2212 \u2202r2(w (k+1)).\nTaking limits on both sides of the above equation with k \u2208 K, by considering the semi-continuity of \u2202r1(\u00b7) and \u2202r2(\u00b7), the boundedness of t\n(k) (based on Lemma 2) and Eq. (7), we obtain\n0 \u2208 \u2207l(w\u22c6) + \u2202r1(w \u22c6)\u2212 \u2202r2(w \u22c6),\nTherefore, w\u22c6 is a critical point of problem (1). This completes the proof of Theorem 1.\nBased on Eq. (7), we know that limk\u2208K\u2192\u221e \u2016w (k+1) \u2212 w(k)\u20162 = 0 is a necessary optimality condition of Algorithm 1. Thus, \u2016w(k+1) \u2212 w(k)\u20162 is a quantity to measure the convergence of the sequence {w(k)} to a critical point. We present the convergence rate in terms of \u2016w(k+1) \u2212w(k)\u20162 in the following theorem.\nTheorem 2 Let {w(k)} be the sequence generated by Algorithm 1 with the monotone line search criterion in Eq. (3) satisfied. Then for every n \u2265 1, we have\nmin 0\u2264k\u2264n\n\u2016w(k+1) \u2212w(k)\u20162 \u2264 2(f(w(0))\u2212 f(w\u22c6))\nn\u03c3tmin ,\nwhere w\u22c6 is a limit point of the sequence {w(k)}.\nProof Based on Eq. (3) with t(k) \u2265 tmin, we have\n\u03c3tmin 2 \u2016w(k+1) \u2212w(k)\u20162 \u2264 f(w(k))\u2212 f(w(k+1)).\nSumming the above inequality over k = 0, \u00b7 \u00b7 \u00b7 , n, we obtain\n\u03c3tmin 2\nn \u2211\nk=0\n\u2016w(k+1) \u2212w(k)\u20162 \u2264 f(w(0))\u2212 f(w(n+1)),\nwhich implies that\nmin 0\u2264k\u2264n\n\u2016w(k+1) \u2212w(k)\u20162 \u2264 2(f(w(0))\u2212 f(w(n+1)))\nn\u03c3tmin\n\u2264 2(f(w(0))\u2212 f(w\u22c6))\nn\u03c3tmin .\nThis completes the proof of the theorem.\nUnder the non-monotone line search criterion in Eq. (4), we have a similar convergence result in the following theorem (the proof uses an extension of argument for Theorem 1 and is omitted).\nTheorem 3 Let the assumptions A1-A3 hold and the non-monotone line search criterion in Eq. (4) be satisfied. Then all limit points of the sequence { w(k) } generated by Algorithm 1 are critical points of problem (1).\nNote that Theorem 1/Theorem 3 makes sense only if {\nw(k) }\nhas limit points. By considering one more mild assumption:\nA4 f(w) \u2192 +\u221e when \u2016w\u2016 \u2192 +\u221e,\nwe summarize the existence of limit points in the following theorem (the proof is omitted):\nTheorem 4 Let the assumptions A1-A4 hold and the monotone/non-monotone line search criterion in Eq. (3)/Eq. (4) be satisfied. Then the sequence { w(k) } generated by Algorithm 1 has at least one limit point."}, {"heading": "2.3.4. Discussions", "text": "Observe that l(w(k))+\u3008\u2207l(w(k)),w\u2212w(k)\u3009+ t (k)\n2 \u2016w\u2212\nw(k)\u20162 can be viewed as an approximation of l(w) at w = w(k). The GIST algorithm minimizes an approximate surrogate instead of the objective function in problem (1) at each outer iteration. We further observe that if t(k) \u2265 \u03b2(l)/(1 \u2212 \u03c3) > \u03b2(l) [the sufficient condition of Eq. (3)], we obtain\nl(w) \u2264l(w(k)) + \u3008\u2207l(w(k)),w \u2212w(k)\u3009\n+ t(k)\n2 \u2016w\u2212w(k)\u20162, \u2200w \u2208 Rd.\nIt follows that\nf(w) = l(w) + r(w) \u2264 M(w,w(k)), \u2200w \u2208 Rd,\nwhere M(w,w(k)) denotes the objective function of problem (2). We can easily show that\nf(w(k)) = M(w(k),w(k)).\nThus, the GIST algorithm is equivalent to solving a sequence of minimization problems:\nw(k+1) = argmin w M(w,w(k)), k = 0, 1, 2, \u00b7 \u00b7 \u00b7\nand can be interpreted as the well-known Majorization and Minimization (MM) technique (Hunter & Lange, 2000).\nNote that we focus on the vector case in this paper and the proposed GIST algorithm can be easily extended to the matrix case."}, {"heading": "3. Related Work", "text": "In this section, we discuss some related algorithms. One commonly used approach to solve problem (1) is the Multi-Stage (MS) convex relaxation (or CCCP, or DC programming) (Zhang, 2010b; Yuille & Rangarajan, 2003; Gasso et al., 2009). It equivalently rewrites problem (1) as\nmin w\u2208Rd f1(w)\u2212 f2(w),\nwhere f1(w) and f2(w) are both convex functions. The MS algorithm solves problem (1) by generating a sequence {w(k)} as\nw(k+1) =argmin w\u2208Rd f1(w)\u2212 f2(w (k))\n\u2212 \u3008s2(w (k)),w \u2212w(k)\u3009, (8)\nwhere s2(w (k)) denotes a sub-gradient of f2(w) at w = w(k). Obviously, the objective function in problem (8) is convex. The MS algorithm involves solving a\nsequence of convex optimization problems as in problem (8). In general, there is no closed-form solution to problem (8) and the computational cost of the MS algorithm is k times that of solving problem (8), where k is the number of outer iterations. This is computationally expensive especially for large scale problems.\nA class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1). The key step is by generating a sequence {w(k)} via solving problem (2). However, they require that the regularizer r(w) is convex and some of them even require that both l(w) and r(w) are convex. Our proposed GIST algorithm is a more general framework, which can deal with a wider range of problems including both convex and non-convex cases.\nAnother related algorithm called a Variant of Iterative Reweighted L\u03b1 (VIRL) is recently proposed to solve the following optimization problem (Lu, 2012a):\nmin w\u2208Rd\n{\nf(w) = l(w) + \u03bb\nd \u2211\ni=1\n(|wi| \u03b1 + \u01ebi) q/\u03b1\n}\n,\nwhere \u03b1 \u2265 1, 0 < q < 1, \u01ebi > 0. VIRL solves the above problem by generating a sequence {w(k)} as\nw(k+1) = argmin w\u2208Rd l(w(k)) + \u3008\u2207l(w(k)),w \u2212w(k)\u3009\n+ t(k)\n2 \u2016w\u2212w(k)\u20162 +\n\u03bbq\n\u03b1\nd \u2211\ni=1\n(|wki | \u03b1 + \u01ebi) q/\u03b1\u22121|wi| \u03b1.\nIn VIRL, t(k\u22121) is chosen as the initialization of t(k). The line search step in VIRL finds the smallest integer \u2113 with t(k) = t(k\u22121)\u03b7\u2113 (\u03b7 > 1) such that\nf(w(k+1)) \u2264 f(w(k))\u2212 \u03c3\n2 \u2016w(k+1) \u2212w(k)\u20162 (\u03c3 > 0).\nThe most related algorithm to our propose GIST is the Sequential Convex Programming (SCP) proposed by Lu (2012b). SCP solves problem (1) by generating a sequence {w(k)} as\nw(k+1) = argmin w\u2208Rd l(w(k)) + \u3008\u2207l(w(k)),w \u2212w(k)\u3009\n+ t(k)\n2 \u2016w\u2212w(k)\u20162 + r1(w) \u2212 r2(w (k))\u2212 \u3008s2,w\u2212w (k)\u3009,\nwhere s2 is a sub-gradient of r2(w) at w = w (k). Our algorithm differs from SCP in that the original regularizer r(w) = r1(w) \u2212 r2(w) is used in the proximal\noperator in problem (2), while r1(w) minus a locally linear approximation for r2(w) is adopted in SCP. We will show in the experiments that our proposed GIST algorithm is more efficient than SCP."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Experimental Setup", "text": "We evaluate our GIST algorithm by considering the Capped \u21131 regularized logistic regression problem, that is l(w) = 1n \u2211n i=1 log ( 1 + exp(\u2212yix T i w) ) and r(w) = \u03bb \u2211d\ni=1 min(|wi|, \u03b8). We compare our GIST algorithm with the Multi-Stage (MS) algorithm and the SCP algorithm in different settings using twelve data sets summarized in Table 2. These data sets are high dimensional and sparse. Two of them (news20, realsim)1 have been preprocessed as two-class data sets (Lin et al., 2008). The other ten2 are multi-class data sets. We transform the multi-class data sets into twoclass by labeling the first half of all classes as positive class, and the remaining classes as the negative class.\nAll algorithms are implemented in Matlab and executed on an Intel(R) Core(TM)2 Quad CPU (Q6600 @2.4GHz) with 8GB memory. We set \u03c3 = 10\u22125,m = 5, \u03b7 = 2, 1/tmin = tmax = 10\n30 and choose the starting points w(0) of all algorithms as zero vectors. We terminate all algorithms if the relative change of the two consecutive objective function values is less than 10\u22125 or the number of iterations exceeds 1000. The Matlab codes of the GIST algorithm are available online (Gong et al., 2013)."}, {"heading": "4.2. Experimental Evaluation and Analysis", "text": "We report the objective function value vs. CPU time plots with different parameter settings in Figure 1. From these figures, we have the following observations: (1) Both GISTbb-Monotone and GISTbbNonmonotone decrease the objective function value rapidly and they always have the fastest convergence speed, which shows that adopting the BB rule to initialize t(k) indeed greatly accelerates the convergence speed. Moreover, both GISTbb-Monotone and GISTbb-Nonmonotone algorithms achieve the smallest objective function values. (2) GISTbbNonmonotone may give rise to an increasing objective function value but finally converges and has a faster overall convergence speed than GISTbb-Monotone in most cases, which indicates that the non-monotone line search criterion can further accelerate the con-\n1http://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets/ 2http://www.shi-zhong.com/software/docdata.zip\nvergence speed. (3) SCPbb-Nonmonotone is comparable to GISTbb-Nonmonotone in several cases, however, it converges much slower and achieves much larger objective function values than those of GISTbbNonmonotone in the remaining cases. This demonstrates the superiority of using the original regularizer r(w) = r1(w) \u2212 r2(w) in the proximal operator in problem (2). (4) GIST-1 has a faster convergence speed than GIST-t(k\u22121) in most cases, which demonstrates that it is a bad strategy to use t(k\u22121) to initialize t(k). This is because {t(k)} increases monotonically in this way, making the step size 1/t(k) monotonically decreasing when the algorithm proceeds."}, {"heading": "5. Conclusions", "text": "We propose an efficient iterative shrinkage and thresholding algorithm to solve a general class of non-convex optimization problems encountered in sparse learning. A critical step of the proposed algorithm is the computation of a proximal operator, which has a closedform solution for many commonly used formulations. We propose to initialize the step size at each iteration using the BB rule and employ both monotone and non-monotone criteria as line search conditions, which greatly accelerate the convergence speed. Moreover, we provide a detailed convergence analysis of the proposed algorithm, showing that the algorithm converges under both monotone and non-monotone line search criteria. Experiments results on large-scale data sets demonstrate the fast convergence of the proposed algorithm.\nIn our future work, we will focus on analyzing the theoretical performance (e.g., prediction error bound, parameter estimation error bound etc.) of the solution obtained by the GIST algorithm. In addition, we plan to apply the proposed algorithm to solve the multitask feature learning problem (Gong et al., 2012a;b)."}, {"heading": "Acknowledgements", "text": "This work is supported partly by 973 Program (2013CB329503), NSFC (Grant No. 91120301, 61075004, 61021063), NIH (R01 LM010730) and NSF (IIS-0953662, CCF-1025177, DMS1208952)."}, {"heading": "Appendix: Solutions to Problem (2)", "text": "Observe that r(w) = \u2211d\ni=1 ri(wi) and problem (2) can be equivalently decomposed into d independent univariate optimization problems:\nw (k+1) i = argmin\nwi\nhi(wi) = 1\n2\n(\nwi \u2212 u (k) i\n)2\n+ 1\nt(k) ri(wi),\nwhere i = 1, \u00b7 \u00b7 \u00b7 , d and u (k) i is the i-th entry of u(k) = w(k) \u2212 \u2207l(w(k))/t(k). To simplify the notations, we unclutter the above equation by removing the subscripts and supscripts as follows:\nw(k+1) = argmin w\nhi(w) = 1\n2 (w \u2212 u)2 +\n1 t ri(w). (9)\n\u2022 \u21131-norm: w (k+1) = sign(u)max (0, |u| \u2212 \u03bb/t).\n\u2022 LSP: We can obtain an optimal solution of problem (9) via: w(k+1) = sign(u)x, where x is an optimal solution of the following problem:\nx = argmin w\n1 2 (w \u2212 |u|) 2 + \u03bb t log(1 + w/\u03b8)\ns.t. w \u2265 0.\nNoting that the objective function above is differentiable in the interval [0,+\u221e) and the minimum of the above problem is either a stationary point (the first derivative is zero) or an endpoint of the feasible region, we have\nx = argmin w\u2208C\n1 2 (w \u2212 |u|) 2 + \u03bb t log(1 + w/\u03b8),\nwhere C is a set composed of 3 elements or 1 element. If t2(|u| \u2212 \u03b8)2 \u2212 4t(\u03bb\u2212 t|u|\u03b8) \u2265 0,\nC = {0, [\nt(|u| \u2212 \u03b8) + \u221a t2(|u| \u2212 \u03b8)2 \u2212 4t(\u03bb\u2212 t|u|\u03b8)\n2t\n]\n+ [\nt(|u| \u2212 \u03b8)\u2212 \u221a t2(|u| \u2212 \u03b8)2 \u2212 4t(\u03bb\u2212 t|u|\u03b8)\n2t\n]\n+\n}\n.\nOtherwise, C = {0}.\n\u2022 SCAD: We can recast problem (9) into the following three problems:\nx1 = argmin w\n1 2 (w \u2212 u) 2 + \u03bb t |w| s.t. |w| \u2264 \u03bb,\nx2 = argmin w\n1 2 (w \u2212 u)2\n+ \u2212w2 + 2\u03b8(\u03bb/t)|w| \u2212 (\u03bb/t)2\n2(\u03b8 \u2212 1) s.t. \u03bb \u2264 |w| \u2264 \u03b8\u03bb,\nx3 = argmin w\n1 2 (w \u2212 u) 2 +\n(\u03b8 + 1)\u03bb2\n2t2 s.t.|w| \u2265 \u03b8\u03bb.\nWe can easily obtain that (x2 is obtained using the similar idea as LSP by considering that \u03b8 > 2):\nx1 = sign(u)min(\u03bb,max(0, |u| \u2212 \u03bb/t)),\nx2 = sign(u)min(\u03b8\u03bb,max(\u03bb, t|u|(\u03b8 \u2212 1)\u2212 \u03b8\u03bb\nt(\u03b8 \u2212 2) )),\nx3 = sign(u)max(\u03b8\u03bb, |u|)."}], "references": [{"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J.M. Borwein"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Barzilai and Borwein,? \\Q1988\\E", "shortCiteRegEx": "Barzilai and Borwein", "year": 1988}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Candes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2008}, {"title": "Signal recovery by proximal forward-backward splitting", "author": ["P.L. Combettes", "V.R. Wajs"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "Combettes and Wajs,? \\Q2005\\E", "shortCiteRegEx": "Combettes and Wajs", "year": 2005}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fan and Li,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li", "year": 2001}, {"title": "Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \u2264 1", "author": ["S. Foucart", "M.J. Lai"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Foucart and Lai,? \\Q2009\\E", "shortCiteRegEx": "Foucart and Lai", "year": 2009}, {"title": "Recovering sparse signals with a certain family of nonconvex penalties and dc programming", "author": ["G. Gasso", "A. Rakotomamonjy", "S. Canu"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Gasso et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gasso et al\\.", "year": 2009}, {"title": "Nonlinear image recovery with half-quadratic regularization", "author": ["D. Geman", "C. Yang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Geman and Yang,? \\Q1995\\E", "shortCiteRegEx": "Geman and Yang", "year": 1995}, {"title": "Multi-stage multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In NIPS, pp. 1997\u20132005,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Robust multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In SIGKDD,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Nonmonotone globalization techniques for the barzilai-borwein gradient method", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Grippo and Sciandrone,? \\Q2002\\E", "shortCiteRegEx": "Grippo and Sciandrone", "year": 2002}, {"title": "A nonmonotone line search technique for newton\u2019s method", "author": ["L. Grippo", "F. Lampariello", "S. Lucidi"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Grippo et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Grippo et al\\.", "year": 1986}, {"title": "A fixed-point continuation method for l1-regularized minimization with applications to compressed sensing", "author": ["E.T. Hale", "W. Yin", "Y. Zhang"], "venue": "CAAM TR07-07,", "citeRegEx": "Hale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hale et al\\.", "year": 2007}, {"title": "Quantile regression via an mm algorithm", "author": ["D.R. Hunter", "K. Lange"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Hunter and Lange,? \\Q2000\\E", "shortCiteRegEx": "Hunter and Lange", "year": 2000}, {"title": "Trust region newton method for logistic regression", "author": ["C.J. Lin", "R.C. Weng", "S.S. Keerthi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Iterative reweighted minimization methods for lp regularized unconstrained nonlinear programming", "author": ["Z. Lu"], "venue": "arXiv preprint arXiv:1210.0066,", "citeRegEx": "Lu,? \\Q2012\\E", "shortCiteRegEx": "Lu", "year": 2012}, {"title": "Sequential convex programming methods for a class of structured nonlinear programming", "author": ["Z. Lu"], "venue": "arXiv preprint arXiv:1210.3039,", "citeRegEx": "Lu,? \\Q2012\\E", "shortCiteRegEx": "Lu", "year": 2012}, {"title": "A simple and efficient algorithm for gene selection using sparse logistic regression", "author": ["S.K. Shevade", "S.S. Keerthi"], "venue": null, "citeRegEx": "Shevade and Keerthi,? \\Q2003\\E", "shortCiteRegEx": "Shevade and Keerthi", "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "A duality principle for non-convex optimisation and the calculus of variations", "author": ["Toland", "JF"], "venue": "Archive for Rational Mechanics and Analysis,", "citeRegEx": "Toland and JF.,? \\Q1979\\E", "shortCiteRegEx": "Toland and JF.", "year": 1979}, {"title": "Relaxed conditions for sparse signal recovery with general concave priors", "author": ["J. Trzasko", "A. Manduca"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Trzasko and Manduca,? \\Q2009\\E", "shortCiteRegEx": "Trzasko and Manduca", "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wright et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2008}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R. Nowak", "M. Figueiredo"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}, {"title": "Sparse methods for biomedical data", "author": ["J. Ye", "J. Liu"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Ye and Liu,? \\Q2012\\E", "shortCiteRegEx": "Ye and Liu", "year": 2012}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation,", "citeRegEx": "Yuille and Rangarajan,? \\Q2003\\E", "shortCiteRegEx": "Yuille and Rangarajan", "year": 2003}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["C.H. Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang,? \\Q2010\\E", "shortCiteRegEx": "Zhang", "year": 2010}, {"title": "Analysis of multi-stage convex relaxation for sparse regularization", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang,? \\Q2010\\E", "shortCiteRegEx": "Zhang", "year": 2010}, {"title": "Multi-stage convex relaxation for feature selection", "author": ["T. Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2012\\E", "shortCiteRegEx": "Zhang", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "The l1-norm regularizer, a continuous and convex surrogate, has been studied extensively in the literature (Tibshirani, 1996; Efron et al., 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al.", "startOffset": 107, "endOffset": 145}, {"referenceID": 5, "context": "The l1-norm regularizer, a continuous and convex surrogate, has been studied extensively in the literature (Tibshirani, 1996; Efron et al., 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al.", "startOffset": 107, "endOffset": 145}, {"referenceID": 24, "context": ", 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al., 2008; Beck & Teboulle, 2009; Wright et al., 2009; Ye & Liu, 2012).", "startOffset": 141, "endOffset": 247}, {"referenceID": 25, "context": ", 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al., 2008; Beck & Teboulle, 2009; Wright et al., 2009; Ye & Liu, 2012).", "startOffset": 141, "endOffset": 247}, {"referenceID": 2, "context": "Although the l1norm based sparse learning formulations have achieved great success, they have been shown to be suboptimal in many cases (Candes et al., 2008; Zhang, 2010b; 2012), since the l1-norm is a loose approximation of the l0-norm and often leads to an over-penalized problem.", "startOffset": 136, "endOffset": 177}, {"referenceID": 2, "context": "They include lq-norm (0 < q < 1) (Foucart & Lai, 2009), Smoothly Clipped Absolute Deviation (SCAD) (Fan & Li, 2001), Log-Sum Penalty (LSP) (Candes et al., 2008), Minimax Concave Penalty (MCP) (Zhang, 2010a), Geman Penalty (GP) (Geman & Yang, 1995; Trzasko & Manduca, 2009) and Capped-l1 penalty (Zhang, 2010b; 2012; Gong et al.", "startOffset": 139, "endOffset": 160}, {"referenceID": 25, "context": "Remark 1 We say that w is a critical point of problem (1), if the following holds (Toland, 1979; Wright et al., 2009): 0 \u2208 \u2207l(w) + \u2202r1(w )\u2212 \u2202r2(w ), where \u2202r1(w ) is the sub-differential of the function r1(w) at w = w , that is, \u2202r1(w ) = { s : r1(w) \u2265 r1(w ) + \u3008s,w\u2212w\u3009, \u2200w \u2208 R }", "startOffset": 82, "endOffset": 117}, {"referenceID": 13, "context": "(3) is a nonmonotone line search criterion (Grippo et al., 1986; Grippo & Sciandrone, 2002; Wright et al., 2009).", "startOffset": 43, "endOffset": 112}, {"referenceID": 25, "context": "(3) is a nonmonotone line search criterion (Grippo et al., 1986; Grippo & Sciandrone, 2002; Wright et al., 2009).", "startOffset": 43, "endOffset": 112}, {"referenceID": 22, "context": "Inspired by Wright et al. (2009); Lu (2012a), we present detailed convergence analysis under both monotone and non-monotone line search criteria.", "startOffset": 12, "endOffset": 33}, {"referenceID": 18, "context": "(2009); Lu (2012a), we present detailed convergence analysis under both monotone and non-monotone line search criteria.", "startOffset": 8, "endOffset": 19}, {"referenceID": 8, "context": "One commonly used approach to solve problem (1) is the Multi-Stage (MS) convex relaxation (or CCCP, or DC programming) (Zhang, 2010b; Yuille & Rangarajan, 2003; Gasso et al., 2009).", "startOffset": 119, "endOffset": 180}, {"referenceID": 4, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 14, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 25, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 17, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 18, "context": "The most related algorithm to our propose GIST is the Sequential Convex Programming (SCP) proposed by Lu (2012b). SCP solves problem (1) by generating a sequence {w} as w = argmin w\u2208R l(w) + \u3008\u2207l(w),w \u2212w\u3009", "startOffset": 102, "endOffset": 113}, {"referenceID": 16, "context": "Two of them (news20, realsim) have been preprocessed as two-class data sets (Lin et al., 2008).", "startOffset": 76, "endOffset": 94}], "year": 2013, "abstractText": "Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the BarzilaiProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s). Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.", "creator": "LaTeX with hyperref package"}}}