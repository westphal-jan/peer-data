{"id": "1303.5733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Investigation of Variances in Belief Networks", "abstract": "the belief network is a currently well - known graphical structure device for representing independences in computing a specified joint probability distribution. similarly the posterior methods, which perform subjective probabilistic inference in belief networks, collectively often treat thus the conditional distribution probabilities which are stored in the risk network as certain values. ( however, well if specifically one takes picture either observing a subjectivistic or a limiting frequency approach contribution to probability, one can never be uniquely certain of probability difference values. provided an initial algorithm should not query only and be certainly capable richer of reporting successively the predicted probabilities of the alternatives of remaining possible nodes present when admitting other alternative nodes are instantiated ; it should also be capable better of publicly reporting the smallest uncertainty there in both these uncertainty probabilities relative to increasing the uncertainty relative in the probabilities of which are stored in the network. earlier in this paper ) a proposed method for determining merging the variances in and inferred null probabilities is obtained under providing the assumption principle that detecting a uniformly posterior distribution variation on the uncertainty control variables can be favorably approximated generally by the prior distribution. it is even shown sharply that this alternative assumption is plausible beyond if their is taking a reasonable amount estimate of confidence in the calculated probabilities which are stored also in throughout the network. furthermore in this straightforward paper, a surprising sufficient upper bound for examining the largest prior variances corresponding in the probabilities of the alternatives of applying all internal nodes is obtained better in the case where the probability distributions of examining the probabilities used of the alternatives are beta distributions. furthermore it is shown too that the prior product variance in examining the probability at an alternative of exiting a node is bounded above versus by the least largest variance in allowing an element of the joint conditional probability distribution for using that output node.", "histories": [["v1", "Wed, 20 Mar 2013 15:31:56 GMT  (393kb)", "http://arxiv.org/abs/1303.5733v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["richard e neapolitan", "james kenevan"], "accepted": false, "id": "1303.5733"}, "pdf": {"name": "1303.5733.pdf", "metadata": {"source": "CRF", "title": "Investigation of Variances in Belief Networks", "authors": ["Richard E. Neapolitan", "James R. Ken evan"], "emails": [], "sections": [{"heading": null, "text": "1 INTRODUCTION\nMuch recent research in decision analysis and in ex pert systems which reason under uncertainty has fo-\ncused on belief networks. A belief network consists of a DAG = (V, E) in which each v E V represents a set of mutually exclusive and exhaustive events, along with a joint probability distribution, P, on the alterna tives of the nodes in V. The fundamental assumption in a belief network is that the value assumed by a node is probabilistically independent of the values assumed by all other nodes in the network, except the descen dents of the given node, given values of all parents of the node. It can be shown that, given this restric tion on P, P can be retrieved from the product of the conditional distributions of each node given values of its parents; it can further be shown that, if these conditional distributions are freely specified, the prod uct of those distributions, along with the DAG, con stitute a belief network. See Heckerman and Horvitz [1987), Neapolitan [1990), Pearl [1988), and Clemen [1991) for discussions of the importance of belief net works in representing problems in expert systems and decision analysis.\nTwo important problems in belief networks are prob ability propagation and abductive inference. Proba bility propagation is the determination of the values of all other nodes in the network given that certain nodes are instantiated for particular values or that ev idence is obtained for the values of certain nodes, while abductive inference is the determination of the most probable, second most probable, third most probable, and so on values of a specified set of nodes called the explanation set given that certain nodes are instanti ated or that evidence is obtained. Pearl [1986] and Lauritzen and Spiegelhalter [1988] have obtained effi cient algorithms for probability propagation for certain classes of networks, while Cooper [1984), Pearl [1987), and Peng and Reggia [1987] have obtained algorithms which perform abductive inference for certain classes of networks. This paper is concerned only with prob abilities obtained using probability propagation. Such probabilities will be called inferred probabilities. The development of efficient general purpose algorithms for probability propagation and abductive inference appears unlikely since Cooper [1988] has shown that both these problems are NP-hard. Recent research has\ntherefore centered on development of approximation, special case, and heuristic methods.\nThe above methods treat the conditional probabilities which are stored in the network as certain values. For example, Chavez and Cooper's [1990] approximation method computes an error relative to an exact value which would be obtained if exact probability propa gation were possible (e.g. using the method of Pearl [1986] or Lauritzen and Spiegelhalter [1988].) How ever, if one takes either a subjectivistic or a limiting frequency approach to probability, one can never be certain of probability values. Only a pure logical ap proach claims to know probabilities for certain. For example, if a coin were tossed 1000 times and 527 tosses came up heads, the frequentist would obtain a confidence interval for the probability value, while the subjectivist would obtain a posterior probability interval or a beta posterior distribution.\nAn exact algorithm for probability propagation should not only be capable of reporting inferred probabilities; it. should also be capable of reporting the uncertainty in these probabilities relative to the uncertainty in the conditional probabilities which are specified in the net work. An approximation algorithm should incorporate this uncertainty into the possible error which is re ported for the approximating values. The uncertainty in the conditional probabilities which are specified in the network can be expressed by probability distribu tions on the probabilities, and the uncertainty in the inferred probabilities can be determined by comput ing the variances in these probabilities relative to the joint distribution of these distributions. If a belief net work includes decision nodes and a value node, (such a belief network is called an influence diagram, see Clemen [1991]), and the system maximizes expected utility, there are two important reasons for reporting the variances. First, the variances can be used to mea sure the qualii.y of the system. If a decision is based on a probability of .8 when the 'correct' probability is .4, it may not be a good decision. Second, in an individual case, a large variance may indicate that the best deci sion would be to gather additional information which would decrease the variance. Howard [1970] discusses variances and the value of information. If the system does not maximize expected utility, the variances in the inferred probabilities inform the decision maker as to the system's uncertainty in its probabilities. This uncertainty should be taken into account before a de cision is made.\nResults of Zabel! [1982] show that in many of the situa tions involving repeatable experiments the uncertainty in probability values must be represented by Dirichlet distributions. Using a method developed by Spiegel halter [1988], Neapolitan [1990] showed how to 'dis cretize' the Dirichlet distributions and represent the uncertainty in the conditional probabilities which are specified in the network in the natural framework of\nInves tigation of Variances in Belief Networks 233\nthe belief network. For example, in Figure 1 the node C represents the uncertainty in the prior probability of A while D and E represent the uncertainty in the con ditional probability of B given A. Neapolitan [1990] further showed how to use one of the algorithms for ex act probability propagation to compute the variance in an inferred probability relative to the uncertainty in the probabilities which are stored in the network. Neapolitan noted, however, that the number of cal culations needed in this computation can grow expo nentially with the distance in the graph of a given node from the instantiated node. This is true even in sparsely connected networks for which exact prob ability propagation is computationally feasible.\nThus there still existed a need for a method for de termining the variances in inferred probabilities rela tive to the uncertainty in the stored probabilities. In Neapolitan and Kenevan [1990] a method is given for determining the prior variances (that is, the variances in the marginal probabilities before any evidence is obtained) of the probabilities of all nodes for the case where certain information is available concerning the probability distributions of the conditional probabili ties which are specified in the network. It is also shown how to obtain this information when the distributions are Dirichlet. A method for determining the variances of inferred probabilities appears very difficult even in the case of sparsely connected networks. In Section 3, an exact method is obtained for the case where a posterior distribution on the uncertainty variables can be approximated by the prior distribution. It is shown that this approximation is plausible if there is a reasonable amount of confidence in the probabili ties which are stored in the network. Note that, even\n234 Neapolitan and Kenevan\nif there is a reasonable amount of confidence in the stored probabilities, there may still be little confidence in the probabilities of the values of a particular node since the uncertainty in these probabilities is relative to the uncertainty in many of the probabilities which are stored in the network. Thus it is still necessary to compute the variances.\nOne fear concerning belief networks is that, when a probability value is computed for many of the proba bilities which are stored in the network, the variance in that probability value might be hopelessly large since it is relative to the uncertainty in many probabilities. Neapolitan and Kenevan [1990] show that if any prob ability is very large or very small, then the variance in that probability must be small. This is encourag ing since, in medical applications for example, infor mation is often obtained until the probability of some explanation is close to 1. However, what of the case where the probability is not large or small? Applica tions of algorithm in Neapolitan and Kenevan [1990] indicated that the prior variances did not, as one might expect, become hopelessly large as one went down the network. This indication led to the surprising result which is the main theorem at the end of Section 4 of this paper. Namely that, under certain assumptions, the prior variance in the probability of an alternative of a node is bounded above by the largest variance in an element of the conditional distribution which is speci fied for that node. Therefore, if we have confidence in the conditional probabilities which are specified in the network, we can have confidence in the prior probabil ities of all nodes.\n2 PRELIMINARY ASSUMPTIONS\nIt is assumed in what follows that probabilistic assess ments in the belief network are made independently. Thus the uncertainties in the assessed probabilities can be represented by a set of mutually independent aux iliary parent nodes. The auxiliary parent of a node, E, will be denoted U E. For example, in F igure 2, U E represents the uncertainty in the P(E), the prior probability of E, U F represents the uncertainty in the P(F \\E), the conditional probability of F given E, and Un represents the uncertainty in the P(D\\F, C). Each auxiliary node is actually a set of mutually indepen dent nodes, one for each combination of values of the true parents. For example, U E consists of one node, if E has three alternatives, U F consists of three nodes, and if F and C each have two alternatives, U D con sists of four nodes. U will be used to denote the set of all the uncertainty nodes. The underlying distri bution is then the joint probability distribution on the members of U and will be denoted by P(U). The prob abilistic assessments are random variables on this joint probability distribution. Small p will be used to de note these random variables. For example, p ( e; ) is the random variable for the prior probability of e; . This\nrandom variable will also be denoted by P(e;\\U) when it is convenient to do so. Similarly, p(f; \\ej) is the random variable for the conditional probability of /; given that E is equal to ej. This random variable will also be denoted by P(f;\\ej, U). It is assumed, for ex ample, that p(e1) is a function only of U E, and that p (fl Jel) is a function only of the first member of U F, and p(fi \\e2) is a function only of the second member of UF. Therefore these random variables are mutually independent. Note however that p(e; ) and p(ek) are not independent. For example, if E has two alterna tives and p(el) = .4, then p (e2) must equal .6.\nF igure 2: The auxiliary parent nodes represent the uncertainty in probabilities.\nThe random variable for the probability of a node which is not a root is computed from the assessed ran dom variables. For example,\np(fl) = 2::: p(fi\\e; )p( e; ) i\np(dl) = Li,kp (dl\\f; , ck)p(f; , ck) = Li,k p (dd/; , ck)P(f; )p(ck)\nsince p(f; ) and p( Ck) are independent random variables due to the network being singly connected.\nIf p; is a random variable for a probability value which is stored in the network (e.g., p; may be p(e;) or p (f; , ej)), it is assumed in this paper that the following information is available for p; , where E stands for the expected value:\nE(p;) E(pl) E(MJ) (1) Note that p; and PJ are random variables for the ith and jth alternatives of the same node. Neapolitan and Kenevan [1990] obtained this information in the case where the distributions are Dirichlet.\n3 DETERMINING THE\nVARIANCES IN INFERRED\nPROBABILITIES\nIn this section a method is obtained for computing the posterior variances (i.e., the variances in the inferred probabilities given that certain nodes, W, are instanti ated) under the assumption that a posterior distribu tion on the uncertainty variables, P(U\\V), can be ap proximated by the prior distribution, P(U). Neapoli tan (1991] shows that, when nodes are instantiated, the posterior distribution on the uncertainty variables can differ from the prior distribution by no more than a distribution based on the information in one addi tional trial. Therefore if there is a reasonable amount of confidence in the prior distribution, a posterior dis tribution can be approximated by the prior distribu tion. Essentially, we are assuming that we learn noth ing about the probabilities which are specified in the network from the current case.\nIn this section, it is assumed that a variable has ex actly two sons for the sake of clarity. The case of an arbitrary number of sons is a straightforward general ization. Furthermore, if W is the set of instantiated variables, Ew will be used to represent an expected value relative to the posterior distribution on the un certainty variables, while E will continue to represent an expected value relative to the prior distribution.\n3.1 THE CASE OF TREES\nIf W is a set of instantiated nodes, we are interested in the variance of the probability of an uninstantiated node, F, relative to the posterior distribution on the uncertainty variables. This variance is given by\nVw(p(f; IW)) = E w(p(f;IW)2)- (Ew(p(J;IWW Thus our goal is to compute these latter two quantities. Since\nEw(p(f; IW)) = L P(f;\\W, U)dP(U!W) = P(f;\\W), we could compute Ew(p(J; )W)) using one of the stan dard methods for computing probabilities. However, the method described here is a generalization of Pearl's [1986] method for computing probabilities, and the de termination of P(J; !W) is a by-product of the deter mination of E(p(f; /W)2). The propagation scheme is based on the following theorem.\nTheorem 1 Let F be an uninstantiated variable, W be the set of instantiated t\u2022ariables, Xp be the set of instantiated variables in the tree rooted at F 's left son, Vp be the set of instantiated variables in the tree rooted at F 's right son, and let Yp = W- Xp- Vp. Then, with the assumption that a posterior distribution on the uncerta inty variables can be approximated by the\nInvestigation of Vru:iances in Belief Networks 235\nprior distribution, it is the case that\nEw(p(f; )W)2) = E(p(Xp)J;)2)E(p(VF )J;)2)E(p(f; )Yp )2)\n(L:1 P(Xp)fJ)P(Vp)/j)P(/j)Yp)) 2\nand\nEw(p(J; /W)) = P(f;\\W)\n= P(Xp)f; )P(VF//; )P(f; IYF)\nLj P(Xp)fj )P(Vplfi )P(/j/Yp).\nProof Let U be the set of uncertainty variables, U x be the subset of those variables which are connected to F through F's left son, Uv be the subset which is connected to F through F's right son, and Uy be the remainder of the uncertainty variables. Then\nE w(p(J; \\W)2) = l P(f; /W,U)2dP(U/W) =\n{ P(W/f; , U)2P(J;/U)2 dP(U)W) Ju P(W/U)2 =\n( P(W)f;, U)2 P(/; )U)2dP(U)2 dP(U/W) Ju dP(U/W)2P(W)2 =\n{ P(W/f; , U)2P(J; \\U)2dP(U)dP(U) Ju dP(U)W)P(W)2 =\n( P(W/f; , U)2 P(J; \\U)2dP(U) dP(U) Ju dP(U)P(W)2 The second to the last equality is due to the assump tion that a posterior distribution on uncertainty vari ables can be approximated by the prior distribution. Thus we have that\nEw(p(f;JW)2 = P(YF )2 ( I 2 2 2 P(W)2 E p(Xp /; ) E(p(VF/f;) )E(p(f;\\YF) ).\nFinally,\nP(Yp _ P(J'p) _ P(YF) P(W)- P(XF,VF,YF)- P(XF.VFIYF)P(Yp)'\nAgain due to d-separation\nP(Xp, Vp!YF) = L: P(XF, Vp/Yp,/j)P(/j/YF) i\n= I: P(Xp)fi )P(VF/fi )P(/j JYp ), j\nwhich proves the first part of the theorem. The second part is proved in the same fashion and does not require the assumption that a posterior distribution on the uncertainty variables can be approximated by the prior distribution. 0\nDue to Theorem 1, the information which F needs from its parent is\nP(/;)Yp) and\n236 Neapolitan and Kenevan\nwhile the information which F needs from its sons is\nP(XFI!i), P(VFI!;), E(p(XFI/;)2), E(p(V FIJ;f).\nFirst we will show how to obtain the information which F needs from its sons. Suppose that the following information is available for each son, G, ofF:\nP(Xal9i), E(p(Xal9;)2), E(p(Xa l9;)(p(Xa l9i) ),\nP(Val9;), E(p(Val9;)2), E(p(Va l9;)p(Va l9i )) (2)\nNotice that p(Xal9;) and p(Xal9i) are not in general independent since they both depend on uncertainty variables in the tree rooted at G. Suppose that G is the left son of F. Then the tree rooted at G contains XF. We shall first consider the case where G is not in stantiated. In that case we have, due to d-separation, that\nE(p(XFI9i)2) = E(p(Xa, Val9;)2) = E(p(Xa I9;)2)E(p(Va 19;)2)\nand similarly that\nP(XFI9;) = P(Xai9;)P(Val9;) E(p(XFI9;)p(XFI9j)) =\nE(p(Xa l9;)p(Xa l9i ))E(p(Va l9;)p(Va IYi )).\nThus if the information listed in (2) is available for G we can compute E(p(XFI9;)2), P(XFI9;), and E(p(XFI9;)p(XF l9i )). We will now show that the first half of the information listed in (2) for F can be com puted from this latter information. First we have that\nP(XFIJ;) = L: P(XF1Yi)P(9ilf;). j\nSince P(9ilf;) = E(P(9i If;)), and this value is part of the information listed in ( 1) for P(9i If;), we see that we can compute P(XFI!;) from the information listed in (2) for G and the information listed in (1) for dis tributions which are specified in the network. Next we have that\nE(p(XFIJ;)p(XFI/i)) =\nE ( ( \ufffd p(XFI9k)P(9kl/;) (\ufffd p(XFI9k)P(9kl/i) )\nThis latter expression is the sum of the following kind of terms:\nE(p(XF l9k )2)E(p(9k lf;))E(p(9k 1/i )) and\nE(p(XF IYk)p(XF l9r ))E(p(9k IJ;))E(P(9r 1/i )).\nThe values in these terms are either part of the infor mation listed in (1) for distributions which are spec ified in the network or, as previously shown, can be computed from the information listed in (2) for G. Fi nally\nE (p(XFI/;)2) =\nE ( (\ufffd p(XFI9k)P(9klf;) (\ufffd p(XFI9k)P(9klf;) )\nThis latter expression contains the following kinds of terms:\nE (p(XFI9k)2) E (p(gkj/;)2) E(p(XF l9k)p(XF l9i ))E(p(9k If; )P(9j If;)).\nTherefore E(p(XFJ/;)2) can also be computed from the information listed in (2) for G and the information listed in (1) for distributions which are specified in the network.\nNext suppose that G is instantiated for 91\u00b7 In this case, due to d-separation, we have that\nP(Xa, Va,9df;) P(Xa IVa, 91, f;)P(Va IY1, /;)P(91, /;) P(Xa I91)P(Vai91)P(g1lf; ).\nSimilarly\nE (p(XFI/;)2) =\nE (p(Xal91)2) E (p(Val9!)2) E (P(91I/;)2) E (p(XFIJ;)p(XFI/i)) =\nE (p(Xal91)2) E (p(Val91)2) E (P(91//;)) E (p(91lfi )) .\nThus again the information listed in ( 2) for F can be computed from the information listed in (2) for the sons of F along with the information listed in ( 1) for distributions which are specified in the network.\nSince G d-separates F from the uncertainty variables connected to F through G, it does not seem correct, for example, that E(p(f;IW)2) depends on E(p(Xal91)2) and E(p(Val91)2) when G is instantiated for 91. This apparent dependence was caused by the assumption that a posterior distribution on the uncertainty vari ables can be approximated by the prior distribution. As shown in Neapolitan [1991] this dependence does not really exist, and when F's son, G, is instantiated for 91, XF can be set equal to {91} in Theorem 3.1 and\nP(XFI!;) E (p(XFI/;)2)\nE(p(XF lf;)p(XFI!i ))\nP(gl/f;) E (P(91IJ;)2) E(p(91l/;) )E(p(91l/i) ).\nThese latter values are simply the information listed in (1) for distributions which are specified in the network.\nNext we show how to obtain the information which a node, F, needs from its parent, E. We will show that the following information can be obtained from information listed in ( 1) for distributions which are stored in the network if E is instantiated or, if E is not instantiated, from the corresponding information for E along with the information listed in (2) for E which comes from E's son (since we are assuming that a posterior distribution on the uncertainty variables can be approximated by the prior distribution, notice that this information is the information listed in (1) conditional on YF ):\nP(f;\\YF), E (p(/;\\YF)2) , E(p(f; IYF )p(/j IYF )). (3)\nSince E d-separates F from all of F's ancestors and E's descendents through E's other children, if E is instantiated for q this information is simply equal to\nP(f; je; ) E(p(f; Jet)p(fj let)), which is the information listed in ( 1) for distributions which are specified in the network. Next assume that E is not instantiated and the information listed in (2) which comes from E's other son and the information listed in (3) is available for E. Due to d-separation\nP(f; IYF) = l: P(J;jej)P(eiJYF)\u00b7 j\nAssuming that F is the right son of E and that there fore the tree rooted at F contains V E, in the same way that Theorem 1 is proved it is possible to show that\nP( \u00b7JY) _ P(XEJej)P(ejiYE) e, F -\nLm P(XEiem)P(emiYE)' and therefore P(f;jY F) can be computed from the in formation listed in (2) which comes from E's other son and the information listed in (3) for E along with the information listed in (1) for distributions which are stored in the network. Next we have that\nThis latter expression is the sum of the following kinds of terms:\nand\nE(p(f; jei ))E(p(f; iek) )E(p( ei \\YF )p( ek IYF )). In the same way that Theorem 1 is proved it is possible to show that\nInvestigation of Variances in Belief Networks 237\nE (p(XEJej)2) E (p(ej!Y \u00a3)2} (Lm P(XEJe,.)P(emiY\u00a3))2\nE(p(eiiYF)p(ekJYF)) = E(p(XE lei )p(XE iek))E(p( ej IYE)P( ek lYE))\n(Lm P(XEiem)P(emiY \u00a3))2\nThus E(p(f; IYF )2) can be computed from the infor mation listed in (2) for E which comes from E's other son and the information list in (3) for E along with the information listed in (1) for distributions which are specified in the network. Finally\nE(p(f;jYF)(p(f;iYF)) =\nE ( ( \ufffd p(f;iem)p(emiYF)) ( \ufffd PCfilem)P(emiYF))) \u00b7\nThis latter expression includes the following types of terms:\nE(p(f; iem)P(/j lem))E(p( em IYF )2) and\nE(p(f; Jem))E(p(fj Jer ))E(p( em IYF )p( er IYF )). We have just shown that the information in these terms can be computed from the information listed in (2) which comes from E's other son and the infor mation listed in (3) for E along with the information listed in (1) for distributions which are specified in the network.\nUsing the theory developed above we can determine variances as follows. First initialize for every node, F, the values of\nP(XFI/; ), E(p(XFJ/; )2), E(p(XFJf;)p(XFI/i )),\nP(VFJf;), E(p(l!FI/;)2), E(p(V FJf;)p(VFI/i ))\nall to 1. Then since for the root, A,\nP(a; j<l>A) = P(a;), E(p(a;J<I>A)2) = E(p(a; JZ),\nE(p(a; j<I> A)p(aj j<I> A)) = E(p(a; )p(aj )), and these values are the information listed in ( 1) for distributions which are specified in the network, the prior variances can be computed by initiating a prop agation flow from the root. At each node the variances are computed using the formulas in Theorem 1. This result agrees with the variances obtained using the exact method described in Neapolitan and Kenevan [1990]. When a node, G, is instantiated, it initiates new propagation down by using the method described above for obtaining the information listed in ( 3) for each son of G from the son's parent in the case were\n238 Neapolitan and Kenevan\nthe parent is instantiated. G's uninstantiated children then continue the propagation flow using the method for the case where the parent is uninstantiated. (Note that instantiated nodes are dead ends for downward propagation.) G also initiates new propagation up by using the method described above to send its parent, F, the portion of the information listed in (2) for F which comes from G. As shown above, this informa tion is stored in the network. If F is uninstantiated, the information listed in (2) for F's parent, which comes from F, is then computed using the method described above for uninstantiated nodes. Instanti ated nodes are also dead ends for upward propagation. \"Vhen an uninstantiated node receives new informa tion from below it. not only must send new information up but also must send new information down to each of its other sons using the method described above for obtaining the information in (3) in the case where the parent is not instantiated.\n3.2 THE CASE OF SINGLY CONNECTED AND ARBITRARY BELIEF NETWORKS\nThe method described above can be extended to the case of singly connected networks. This extension ap pears in Neapolitan (1991] . The case of an arbitrary network can then be handled by using Pearl's (1988] method of clustering as discussed in Neapolitan and Kenevan (1990].\n4 OBTAINING AN UPPER\nBOUND FOR THE PRIOR\nVARIANCES\nAs noted in the introduction, Zabel! (1982] has shown that in many of t.he cases which are relevant to expert systems, the probability distribution of a probability must be Dirichlet. In this section it is assumed that there are two alternatives for each node and the dis tributions stored in the network are all Dirichlet. In the case of two alternatives, the Dirichlet distribution is called the beta distribution. It is given by\nJl.(p)::: <\u00b7:\ufffdtl)!p\u2022(l- p)b,\nwhere a and b are nonnegative parameters. It is straightforward to show that, in the case of the beta distribution,\nE(p) ::: .\ufffdt!2 E(p2) ::: .\ufffdt;a E(p)\nE(p(1- p)) = .!t!3 E(p) (4) where E stands for expected value. Some of these results are needed to obtain the proofs in this section.\nThe main theorem at the end of this section obtains the upper bounds for the prior variances of the prob abilities of all nodes in the network in the case where the network is a tree and the distributions stored in the network are beta. First we must obtain a number of preliminary results.\nLemma 1 If there are exactly two alternatives, then V(pl) ::: V(p2), however if there are three or more alternatives, the variances are not in general equal.\nProof Since\nV(Pl) = L p\ufffddP(U) - (fu p 1dP(U)) 2\nand\nV(p2)::: L (1- p1)2dP(U) - (fu (1- pl)dP(U)) 2\nthe first part of the lemma is proved with simple al gebraic manipulations. A counter example using a Dirichlet distribution proves the second part. 0\nLemma 2 Suppose there are exactly two alternatives. Let E == E(p1), S ::: E(pi), and T ::: E(p\u00a7). Then T::: 1 - 2E + S.\nProof Due to Lemma 1, V(pl) = V(P2). Set V be that variance. Since S ::: E2 + V and T ::: ( 1 - E)2 + V, the lemma is proved with straightforward algebraic manipulations. Box\nLemma 3 Suppose there are exactly two alternatives. Let P == E(p1p2), and E and S be as in Lemma 2. Then P = E- S.\nProof\nP = fu P1(l- p!)dP(U) = fu P1dP(U)- fu PidP(U)\n= E- S. o\nLemma 4 Suppose there are exactly two alternatives for both node A and node B and that A is the only parent of B. Let\nE::: E(ai), E1 = E(p(b1!al)), E2 == E(p(bda2)), V == V(al), V1 == V(p(b1ja1)), V2 = V(p(b1!a2))\nThen\nV(p(b1)) = V (V, + V2 + (E,- E2)2) + V2(1- E)2 + V1E2.\nProof Let S = E(p(ai)2), T = E(p(az)2), P = E\ufffdp(al)p(az)), S1 = E(p(bdai)2), and Sz = E(p(b1iaz) ) . We then have\nE (p(bi)2) = E ([p(hiaJ)p(a!) + p(b!laz)p(az)J2) which is easily seen to equal\nE (p(h la1)2) E (p(ai)2) + 2E(p(b1la1 ))E(p(b1iaz))E(p( ai)p(az))\n+ E (p(bdaz)2) E (p(a2)2) , and thus E (p(h)2) = S1S + 2E1E2P + S2T. (5) Similarly it can be shown that\nSince\n(E(p(h))2 == ErE2 + 2E1EE2-2E 1E2E2 +E?-2E?E +E?E2. (6)\nV(p(bJ)) = E (p(bi)2) -(E(p(bJ))2 , equations (5) and (6), applications of Lemmas 2 and 3, and some algebraic manipulations yield\nNow\nV(p(bi)) == Vz-2E1E2 V-2EV2 + S1S - E[E2 + S2S-E\ufffdE2. (7)\nS1S- EfE2 = S1S- S1E2 + S1E2-E[E2 = s1v + E2V1,\nand an identical result holds for S2S- E?E2. After inserting these results in equation (7) and performing some more algebraic manipulations we have that\nV(p(b!)) = V(S1 + Sz - 2E !E z) + Vz(1- E)2 + V1E2. (8)\nReplacing S1 and Sz in (8) by V1 + Er and Vz + Ei respectively proves the lemma. 0\nLemma 5 Let E , S, and V be as in Lemma 2. Then S \ufffd (E + E2)f2 implies that V \ufffd E-S.\nProof V \ufffd E -S means S-E2 \ufffd E-S, which is true if 2S \ufffd E + E2, which is the condition in the statement of the lemma. 0\nLemma 6 Let E and S be as in Lemma 2. If the distribution is beta, then\nS \ufffd (E + 2E2) /3.\nProof Let P be as in Lemma 3, and a and b be the parameters for the beta distribution. Then, due to equalities ( 4),\np = (...ll.L)E a+b+3 -(!!.\u00b1k\u00b1l.)(...ll.L)E - a+b+3 a+b+2\n== (!!.\u00b1k\u00b1l.) (1- E)E a+b+3 \ufffd 2 (E- E2) /3.\nInvestigation of Variances in Belief Networks 239\nThus due to Lemma 3 we have that E- S \ufffd 2(E E2)/3, and the lemma follows from some algebraic ma nipulations. 0\nLemma 7 If the distribution is beta, then E \ufffd ( 1 + .J1 - 12V)/2.\nProof In the same way that Lemma 5 was proved, Lemma 6 implies that\nV \ufffd (E- E2) /3, which implies\nE2-E + 3V \ufffd 0.\nThe expression E2 - E + 3V equals 0 at the point E == (I+.J1-12V)/2 and has positive derivative with respect to E at his point and to the right of it. There fore if E2-E+3V \ufffd 0, E must be\ufffd (l+V1-12V)/2. 0\nTheorem 2 Assume the conditions and notations in Lemma 4. Further assume that S \ufffd (E+E2)j2. Then\nV(p(b1)) \ufffd Maximum( VI, Vz).\nProof Suppose 1/1 \ufffd V2\u2022 Without loss of generality we can assume E 1 \ufffd E2\u2022 For, if this were not the case, we would have\nHowever, due to Lemma 1 and the assumption that V1 \ufffd Vz,\nV(p(bzia!)) = V(p(b1la1)) = V1 \ufffd Vz = V(p(b1la2)) = V(p(bzlaz)),\nand we could proceed in the proof using b2 instead of b1. Assuming now that E 1 \ufffd E z, due to Lemma 4 we have that\nV(p(bi)) = V (V1 + Vz + (E z- E 1)2) + Vz(1-E)2 + V1E2\n\ufffd V (2Vz + E?) + Vz(1 - E)2 + VzE2 \ufffd V (2V2 + (1 + .J1-12V2) /2)\n+ Vz(1 - E)2 + V2E2\nThe last inequality is due to Lemma 7 and the fact that Ei \ufffd E2. After some algebraic manipulations we have\nV(p(bi)) \ufffd V(1 + .J1- 121/ z)/2\n+ 1/2(1 + 21/- 2E + 2E2) \ufffd 1/(1 + .J1-121/ z)/2\n+ V(1 +2S- 2E2 - 2E+2E2) \ufffd V(1 + }1-121/2)/2 + Vz(1 - 2(E - S)) .\n240 Neapolitan and Kenevan\nDue to Lemma 5 and the assumptions of this theorem we then have\nV (p(b1)) :S V (1 + )1-12V2)/2 + V2(1- 2V ). Thus V (p(b1)) :S V2 if\nV (1 + J1- 12v2)/2 + V2(1 - 2V ) ::; v2.\nIf V = 0, the proof is trivial. Thus this latter inequal ity is true if 1 + )1- 12V2 :S 4V2, which is true if 1 - 12V2 :S 16Vl -SV2 + 1. Since 16Vl + 4 V2 ?: 0 the theorem is proved. 0\nTheorem 3 Assume the conditions and notation in Lemma 4. Furthermore let S' = E(p(h)2) and E' = E(p(b!)). Then S :S (E + E2)/2 implies that S' :S (E' + E'2)/2.\nProof Equation (5) from the proof of Lemma 4 states that S' = S1S + 2E1E2P+ S2T. It is straightforward that\n(E' + E'2) = [E1E + E2(1-E)+ (E1E + E2(1- E))2]/2.\nThus we need show that\nS1S + 2E1E2P + S2T :S [E1E + E2(1-E)+ (E1E + E2(1- E)f]/2.\nAlgebraic manipulations yield that this is equivalent to showing that\n(S1S + E1E2P) + (S2T + E1E2P) :S [E1E + EiE2 + E1EE2(l-E)]/2 + [E2(l-E)+ E5(1-E)2 + E1EE2(1- E)]/2.\nWe will accomplish this by showing that\nS1S + E1E2P :S [E1E+E[E2+E1EE2(1-E)]J2. (9)\nBy symmetry we will then also have shown that\nS2T+ E1E2P :S [E2(1 -E)+ Ei(I- Ef + E1EE2(1- E)]/2,\nand the theorem will be proved. To that end, due to Lemmas 3 and 6,\nS1S + E1E2P = S1S + E1E2(E- S) :S (E1 + 2Ef)S/3 + E1E2E-E1E2S.\nThus inequality (9) will hold if\nS(E1 + 2E[- 3E1E2)/3 + E1E2E :S [E1E + E[E2 + E1EE2(1- E)]/2.\nEliminating the trivial case when E1 = 0, algebraic manipulations yield that this last inequality is equiv alent to\nThere are two cases:\nCase 1: 1 + 2E1 - 3E2 ?: 0. In this casse due to the assumption that S :S (E + E2)/2, inequality (10) is true if\n(E + E2)(1 + 2El - 3E2) :S 3E(1 + E1E- EE2-E2),\nwhich, after performing algebraic manipulations and eliminating the trivial case when E = 0, is equivalent to E(1- E!) :S 2(1- EI) which proves Case 1.\nCase 2: 1 + 2E1 - 3E2 < 0. Inequality (10) is equiv alent to\nwhere the right side of this inequality is now positive due to the assumption in this case. Since V ?: 0, we have S ?: E2 and therefore the last inequality is true if\n3E(EE2 + E2- 1- E1E) :S 2E2(3E2-2E1- 1).\nAfter the trivial case when E = 0 is eliminated, alge braic manipulations yield that this inequality is equiv alent to\n2E(1- E2) + E(E1 - E2) :S 3(1-E2).\nClearly 2E(1 - E2) :S 2(1 -E2). Therefore we need only show that E(E1 - E2) :S 1 -E2. Due to the assumption in this case, E1 < (3E2- 1)/2. Thus we need only show that\nAlgebraic manipulations yields that this is equivalent to 0 :S (1-E2)(2 +E), which proves this case. 0\nTheorem 4 (Main Theorem) If the belief network is a tree, and all distributions stored in the network are beta, then for any node, TJ,\n( 11)\nwhere V1 = V (p(b1ia1)), V2 = V(p(b1ia2)), and A is B 's parent.\nProof Due to Lemma 6 and the fact that the distri bution for the probability of the root is assumed to be beta, it is easy to show that for the root\n(12)\nwhere S and E stand again for the expected value of a probability and the expected value of a probability squared respectively. Thus, due to Theorem 3 and an inductive argument, inequality (12) holds for all nodes in the network. Therefore, due to Theorem 2, inequality (11) holds for all nodes in the network. 0\n5 FUTURE RESEARCH\nThere exists a need for a great deal of additional re search in this area. First, it should be determined if the results in Section 4 hold when there are more than two alternatives and when the network is not tree. Second, of greater interest than the variance in prior probabil ities are the variances in inferred probabilities. If a node is instantiated in a tree, it is easy to see that the results above hold for the descendents of that node. However, in many cases we are more interested in the ancestors of the node. In medicine, for example, in stantiated nodes, which represent findings, are often near leaves while the nodes of interest, which repre sent diseases, are often near roots. It remains to be determined whether the variances become large as we propagate up the network from an instantiated node.\nReferences\nR. lvl. Chavez and G. F. Cooper [1990], \"A Random ized Approximation Algorithm for Probabilistic Infer ence on Bayesian Belief Networks,\" to appear in Jour nal of Networks.\nG. F. Cooper (1984], '\"NESTOR': A Computer-Based Medical Diagnostic that Integrates Causal and Prob abilistic Knowledge,\" Technical Report IIPP-84-48, Stanford University, Stanford, CA.\nG. F. Cooper (1988], \"Probabilistic Inference Using Belief Networks is NP-Hard,\" Technical Report KSL87-27, Stanford University, Stanford, CA.\nD. Heckerman and E. J. Horvitz (1987], \"On the Expressiveness of Rule-Based Systems for Reason ing with Uncertainty,\" Proceedings of AAAI, Seattle, \\Vashington.\nR. A. Howard (1970], \"Decision Analysis: Perspectives on Inference, Decision, and Experimentation,\" Pro ceedings of IEEE, Vol. 58, No. 5.\nS. L. Lauritzen and D. J. Spiegelhalter (1988], \"Local Computation with Probabilities on Graphical Struc tures and Their Applications to Expert Systems,\" Journal of the Royal Statistical Society B., Vol. 50, No. 2.\nR. E. Neapolitan (1990], Probabilistic Reasoning in Expert Systems: Theory and Algorithms, Wiley, New York.\nR. E. Neapolitan (1991], \"Using the Variance as a Mea sure of the Uncertainty in Inferred Probabilities in Be lief Networks,\" paper in progress.\nR. E. Neapolitan and J. R. Kenevan (1990], \"Compu tation of Variances in Causal Networks,\" Proceedings of 6th International Workshop on Uncertainty in Ar tificial Intelligence, MIT, Cambridge, MA.\nJ. Pearl (1986], \"Fusion, Propagation, and Structuring\nInvestigation of Variances in Belief Networks 241\nin Belief Networks,\" Artificial Intelligence, Vol. 29.\nJ. Pearl (1987], \"Distributed Revision of Composite Beliefs,\" Artificial Intelligence, Vol. 33.\nJ. Pearl (1988], Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, San Mateo, California.\nY. Peng and J. A. Reggia (1987], \"A Probabilistic Causal Model for Diagnostic Problem Solving- Parts I and II,\" IEEE Transactions on Systems, Man, and Cybernetics, Vol. SMC-17.\nR. D. Shachter (1988], \"Probabilistic Inference and In fluence Diagrams,\" Operations Research, Vol. 36, No. 4.\nD. J. Spiegelhalter (1988], \"Analysis of Softness,\" in L. N. Kana! and T. S. Levitt, Eels. , Uncertainty in Artificial Intelligence III, North-Holland, Amsterdam.\nS. L. Zabel! (1982], \"W. E. Johnson's 'Sufficientness' Postulate,\" The Annals of Statistics, Vol. 10, No. 4."}], "references": [{"title": "A Random\u00ad ized Approximation Algorithm for Probabilistic Infer\u00ad ence on Bayesian Belief Networks,\" to appear in Jour\u00ad nal of Networks", "author": ["R. lvl. Chavez", "G.F. Cooper"], "venue": null, "citeRegEx": "Chavez and Cooper,? \\Q1990\\E", "shortCiteRegEx": "Chavez and Cooper", "year": 1990}, {"title": "\"NESTOR': A Computer-Based Medical Diagnostic that Integrates Causal and Prob\u00ad abilistic Knowledge,", "author": ["G.F. Cooper"], "venue": "Technical Report IIPP-84-48,", "citeRegEx": "Cooper,? \\Q1984\\E", "shortCiteRegEx": "Cooper", "year": 1984}, {"title": "Probabilistic Inference Using Belief Networks is NP-Hard,", "author": ["G.F. Cooper"], "venue": "Technical Report KSL87-27,", "citeRegEx": "Cooper,? \\Q1988\\E", "shortCiteRegEx": "Cooper", "year": 1988}, {"title": "On the Expressiveness of Rule-Based Systems for Reason\u00ad ing with Uncertainty,", "author": ["D. Heckerman", "E.J. Horvitz"], "venue": "Proceedings of AAAI,", "citeRegEx": "Heckerman and Horvitz,? \\Q1987\\E", "shortCiteRegEx": "Heckerman and Horvitz", "year": 1987}, {"title": "Decision Analysis: Perspectives on Inference, Decision, and Experimentation,", "author": ["R.A. Howard"], "venue": "Pro\u00ad ceedings of IEEE,", "citeRegEx": "Howard,? \\Q1970\\E", "shortCiteRegEx": "Howard", "year": 1970}, {"title": "Local Computation with Probabilities on Graphical Struc\u00ad tures and Their Applications to Expert Systems,", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society B.,", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Probabilistic Reasoning in Expert Systems: Theory and Algorithms, Wiley, New York", "author": ["R.E. Neapolitan"], "venue": null, "citeRegEx": "Neapolitan,? \\Q1990\\E", "shortCiteRegEx": "Neapolitan", "year": 1990}, {"title": "Using the Variance as a Mea\u00ad sure of the Uncertainty in Inferred Probabilities in Be\u00ad lief Networks,\" paper in progress", "author": ["R.E. Neapolitan"], "venue": null, "citeRegEx": "Neapolitan,? \\Q1991\\E", "shortCiteRegEx": "Neapolitan", "year": 1991}, {"title": "Compu\u00ad tation of Variances in Causal Networks,", "author": ["R.E. Neapolitan", "J.R. Kenevan"], "venue": "Proceedings of 6th International Workshop on Uncertainty in Ar\u00ad tificial Intelligence,", "citeRegEx": "Neapolitan and Kenevan,? \\Q1990\\E", "shortCiteRegEx": "Neapolitan and Kenevan", "year": 1990}, {"title": "Distributed Revision of Composite Beliefs,", "author": ["J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "Pearl,? \\Q1987\\E", "shortCiteRegEx": "Pearl", "year": 1987}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "A Probabilistic Causal Model for Diagnostic Problem Solving- Parts I and II,", "author": ["Y. Peng", "J.A. Reggia"], "venue": "IEEE Transactions on Systems,", "citeRegEx": "Peng and Reggia,? \\Q1987\\E", "shortCiteRegEx": "Peng and Reggia", "year": 1987}, {"title": "Probabilistic Inference and In\u00ad fluence Diagrams,", "author": ["R.D. Shachter"], "venue": "Operations Research,", "citeRegEx": "Shachter,? \\Q1988\\E", "shortCiteRegEx": "Shachter", "year": 1988}, {"title": "Analysis of Softness,\" in L", "author": ["J. D"], "venue": "Eels. ,", "citeRegEx": "D.,? \\Q1988\\E", "shortCiteRegEx": "D.", "year": 1988}, {"title": "W. E. Johnson's 'Sufficientness' Postulate,", "author": ["S.L. Zabel"], "venue": "The Annals of Statistics,", "citeRegEx": "Zabel,? \\Q1982\\E", "shortCiteRegEx": "Zabel", "year": 1982}], "referenceMentions": [{"referenceID": 3, "context": "See Heckerman and Horvitz [1987), Neapolitan [1990), Pearl [1988), and Clemen [1991) for discussions of the importance of belief net\u00ad works in representing problems in expert systems and decision analysis.", "startOffset": 4, "endOffset": 33}, {"referenceID": 3, "context": "See Heckerman and Horvitz [1987), Neapolitan [1990), Pearl [1988), and Clemen [1991) for discussions of the importance of belief net\u00ad works in representing problems in expert systems and decision analysis.", "startOffset": 4, "endOffset": 52}, {"referenceID": 3, "context": "See Heckerman and Horvitz [1987), Neapolitan [1990), Pearl [1988), and Clemen [1991) for discussions of the importance of belief net\u00ad works in representing problems in expert systems and decision analysis.", "startOffset": 4, "endOffset": 66}, {"referenceID": 3, "context": "See Heckerman and Horvitz [1987), Neapolitan [1990), Pearl [1988), and Clemen [1991) for discussions of the importance of belief net\u00ad works in representing problems in expert systems and decision analysis.", "startOffset": 4, "endOffset": 85}, {"referenceID": 6, "context": "Pearl [1986] and Lauritzen and Spiegelhalter [1988] have obtained effi\u00ad cient algorithms for probability propagation for certain classes of networks, while Cooper [1984), Pearl [1987), and Peng and Reggia [1987] have obtained algorithms which perform abductive inference for certain classes of networks.", "startOffset": 0, "endOffset": 13}, {"referenceID": 3, "context": "Pearl [1986] and Lauritzen and Spiegelhalter [1988] have obtained effi\u00ad cient algorithms for probability propagation for certain classes of networks, while Cooper [1984), Pearl [1987), and Peng and Reggia [1987] have obtained algorithms which perform abductive inference for certain classes of networks.", "startOffset": 17, "endOffset": 52}, {"referenceID": 1, "context": "Pearl [1986] and Lauritzen and Spiegelhalter [1988] have obtained effi\u00ad cient algorithms for probability propagation for certain classes of networks, while Cooper [1984), Pearl [1987), and Peng and Reggia [1987] have obtained algorithms which perform abductive inference for certain classes of networks.", "startOffset": 156, "endOffset": 170}, {"referenceID": 1, "context": "Pearl [1986] and Lauritzen and Spiegelhalter [1988] have obtained effi\u00ad cient algorithms for probability propagation for certain classes of networks, while Cooper [1984), Pearl [1987), and Peng and Reggia [1987] have obtained algorithms which perform abductive inference for certain classes of networks.", "startOffset": 156, "endOffset": 184}, {"referenceID": 1, "context": "Pearl [1986] and Lauritzen and Spiegelhalter [1988] have obtained effi\u00ad cient algorithms for probability propagation for certain classes of networks, while Cooper [1984), Pearl [1987), and Peng and Reggia [1987] have obtained algorithms which perform abductive inference for certain classes of networks.", "startOffset": 156, "endOffset": 212}, {"referenceID": 1, "context": "Pearl [1986] and Lauritzen and Spiegelhalter [1988] have obtained effi\u00ad cient algorithms for probability propagation for certain classes of networks, while Cooper [1984), Pearl [1987), and Peng and Reggia [1987] have obtained algorithms which perform abductive inference for certain classes of networks. This paper is concerned only with prob\u00ad abilities obtained using probability propagation. Such probabilities will be called inferred probabilities. The development of efficient general purpose algorithms for probability propagation and abductive inference appears unlikely since Cooper [1988] has shown that both these problems are NP-hard.", "startOffset": 156, "endOffset": 597}, {"referenceID": 0, "context": "For example, Chavez and Cooper's [1990] approximation method computes an error relative to an exact value which would be obtained if exact probability propa\u00ad gation were possible (e.", "startOffset": 13, "endOffset": 40}, {"referenceID": 0, "context": "For example, Chavez and Cooper's [1990] approximation method computes an error relative to an exact value which would be obtained if exact probability propa\u00ad gation were possible (e.g. using the method of Pearl [1986] or Lauritzen and Spiegelhalter [1988].", "startOffset": 13, "endOffset": 218}, {"referenceID": 0, "context": "For example, Chavez and Cooper's [1990] approximation method computes an error relative to an exact value which would be obtained if exact probability propa\u00ad gation were possible (e.g. using the method of Pearl [1986] or Lauritzen and Spiegelhalter [1988].) How\u00ad ever, if one takes either a subjectivistic or a limiting frequency approach to probability, one can never be certain of probability values.", "startOffset": 13, "endOffset": 256}, {"referenceID": 4, "context": "Howard [1970] discusses variances and the value of information.", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "Results of Zabel! [1982] show that in many of the situa\u00ad tions involving repeatable experiments the uncertainty in probability values must be represented by Dirichlet distributions.", "startOffset": 11, "endOffset": 25}, {"referenceID": 11, "context": "Results of Zabel! [1982] show that in many of the situa\u00ad tions involving repeatable experiments the uncertainty in probability values must be represented by Dirichlet distributions. Using a method developed by Spiegel\u00ad halter [1988], Neapolitan [1990] showed how to 'dis\u00ad cretize' the Dirichlet distributions and represent the uncertainty in the conditional probabilities which are specified in the network in the natural framework of Inves tigation of Variances in Belief Networks 233", "startOffset": 157, "endOffset": 233}, {"referenceID": 6, "context": "Using a method developed by Spiegel\u00ad halter [1988], Neapolitan [1990] showed how to 'dis\u00ad cretize' the Dirichlet distributions and represent the uncertainty in the conditional probabilities which are specified in the network in the natural framework of Inves tigation of Variances in Belief Networks 233", "startOffset": 52, "endOffset": 70}, {"referenceID": 6, "context": "Neapolitan [1990] further showed how to use one of the algorithms for ex\u00ad act probability propagation to compute the variance in an inferred probability relative to the uncertainty in the probabilities which are stored in the network.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "In Neapolitan and Kenevan [1990] a method is given for determining the prior variances (that is, the variances in the marginal probabilities before any evidence is obtained) of the probabilities of all nodes for the case where certain information is available concerning the probability distributions of the conditional probabili\u00ad ties which are specified in the network.", "startOffset": 3, "endOffset": 33}, {"referenceID": 6, "context": "Neapolitan and Kenevan [1990] show that if any prob\u00ad ability is very large or very small, then the variance in that probability must be small.", "startOffset": 0, "endOffset": 30}, {"referenceID": 6, "context": "Neapolitan and Kenevan [1990] show that if any prob\u00ad ability is very large or very small, then the variance in that probability must be small. This is encourag\u00ad ing since, in medical applications for example, infor\u00ad mation is often obtained until the probability of some explanation is close to 1. However, what of the case where the probability is not large or small? Applica\u00ad tions of algorithm in Neapolitan and Kenevan [1990] indicated that the prior variances did not, as one might expect, become hopelessly large as one went down the network.", "startOffset": 0, "endOffset": 430}, {"referenceID": 6, "context": "Neapolitan and Kenevan [1990] obtained this information in the case where the distributions are Dirichlet.", "startOffset": 0, "endOffset": 30}, {"referenceID": 9, "context": "However, the method described here is a generalization of Pearl's [1986] method for computing probabilities, and the de\u00ad termination of P(J; !W) is a by-product of the deter\u00ad mination of E(p(f; /W)2).", "startOffset": 58, "endOffset": 73}, {"referenceID": 6, "context": "As shown in Neapolitan [1991] this dependence does not really exist, and when F's son, G, is instantiated for 91, XF can be set equal to {91} in Theorem 3.", "startOffset": 12, "endOffset": 30}, {"referenceID": 6, "context": "This result agrees with the variances obtained using the exact method described in Neapolitan and Kenevan [1990]. When a node, G, is instantiated, it initiates new propagation down by using the method described above for obtaining the information listed in ( 3) for each son of G from the son's parent in the case were", "startOffset": 83, "endOffset": 113}, {"referenceID": 6, "context": "This extension ap\u00ad pears in Neapolitan (1991] .", "startOffset": 28, "endOffset": 46}, {"referenceID": 6, "context": "This extension ap\u00ad pears in Neapolitan (1991] . The case of an arbitrary network can then be handled by using Pearl's (1988] method of clustering as discussed in Neapolitan and Kenevan (1990].", "startOffset": 28, "endOffset": 125}, {"referenceID": 6, "context": "This extension ap\u00ad pears in Neapolitan (1991] . The case of an arbitrary network can then be handled by using Pearl's (1988] method of clustering as discussed in Neapolitan and Kenevan (1990].", "startOffset": 28, "endOffset": 192}, {"referenceID": 13, "context": "As noted in the introduction, Zabel! (1982] has shown that in many of t.", "startOffset": 30, "endOffset": 44}], "year": 2011, "abstractText": "The belief network is a well-known graphi\u00ad cal structure for representing independences in a joint probability distribution. The meth\u00ad ods, which perform probabilistic inference in belief networks, often treat the conditional probabilities which are stored in the network as certain values. However, if one takes ei\u00ad ther a subjectivistic or a limiting frequency approach to probability, one can never be certain of probability values. An algorithm should not only be capable of reporting the probabilities of the alternatives of remain\u00ad ing nodes when other nodes are instanti\u00ad ated; it should also be capable of reporting the uncertainty in these probabilities relative to the uncertainty in the probabilities which are stored in the network. In this paper a method for determining the variances in in\u00ad ferred probabilities is obtained under the as\u00ad sumption that a posterior distribution on the uncertainty variables can be approximated by the prior distribution. It is shown that this assumption is plausible if their is a reason\u00ad able amount of confidence in the probabili\u00ad ties which are stored in the network. Fur\u00ad thermore in this paper, a surprising upper bound for the prior variances in the proba\u00ad bilities of the alternatives of all nodes is ob\u00ad tained in the case where the probability dis\u00ad tributions of the probabilities of the alterna\u00ad tives are beta distributions. It is shown that the prior variance in the probability at an al\u00ad ternative of a node is bounded above by the largest variance in an element of the condi\u00ad tional probability distribution for that node.", "creator": "pdftk 1.41 - www.pdftk.com"}}}