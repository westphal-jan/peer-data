{"id": "1303.5752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "About Updating", "abstract": "survey arise of several forms of complicated updating, with a practical illustrative scheme example. similarly we study several classical updating ( conditioning ) schemes that ideally emerge entirely naturally from studying a common scenarion to indirectly provide some descriptive insights involved into their meaningful meaning. updating methods is principally a particular subtle analytical operation component and there is no single method, requiring no single'good'rule. the choice parameter of matching the appropriate rule must always be given careful due consideration. planchet ( 1989 ) presents often a mathematical survey ahead of many rules. altogether we normally focus closely on realising the most practical, meaning constraints of these rules. generally after lightly summarizing the several rules for conditioning, today we present such an efficient illustrative example in which the possible various forms of conditioning can best be explained.", "histories": [["v1", "Wed, 20 Mar 2013 15:33:33 GMT  (270kb)", "http://arxiv.org/abs/1303.5752v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["philippe smets"], "accepted": false, "id": "1303.5752"}, "pdf": {"name": "1303.5752.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Philippe Smets"], "emails": [], "sections": [{"heading": null, "text": "We study several updating (conditioning) schemes that emerge naturally from a common scenario to provide some insights into their meaning. Updating is a subtle operation and there is no single method, no single 'good' rule. The choice of the appropriate rule must always be given due consideration. Planchet (1989) presents a mathematical survey of many rules.We focus on the practical meaning of these rules. After summarizing the several rules for conditioning, we present an illustrative example in which the various forms of conditioning can be explained.\n1. CONDITIONING RULES FOR BELIEF FUNCTIONS.\nLet 11 be a finite set with elements rot. ro2, ... ron. The roi are mutually exclusive possible answers to a given question. Let bel: 211\ufffd[0,1] be a belief function over 11, with m its corresponding basic belief mass assignment. For A <;;;11, bel(A) quantifies our degree of belief that the true answer to the question is in A. If the roi are an exhaustive list of possible answers, the closed-world assumption prevails (as no solution exists outside Q), otherwise the open-world assumption prevails (Smets 1988)\nSuppose \ufffdnew piec\ufffd of evidence that says that the answer is not in A, where A is the set of roi that are elements of 11, but not of A. To say that the answer is not in A does not mean that the answer is in A. Under closed-world assumption both statements are equivalent. Under open world assumption, they are not. Here, the answer might be non\ufffd of the elements of 11. When I say ':!_he answer is not in A\", I only eliminate the elements of A as possible answer. When I say \"the \ufffdnswer is in A\", not only I eliminate the elements of A, but I also claim that the answer is an element of A - a much stronger claim. This distinction is at the origin of the difference between the open-world and the closed-world assumptions.\nThe updating of \ufffdI induced by the information that the answer is not in A can be performed variously. It will depend on the interpretation of the problem that bel is supposed to model.\nConditioning C.l. The Unnormalized Dempster's Rule of Conditioning.\nFor all X<;;;11, the basic belief masses m(X) given to X is transferred to An X. The basic belief masses assignment IDA , the belief function belA and the plausibility function piA obtained after conditioning on A are:\nIDA(B) = L m(BuX) for B<;;;A X<;;;A\nIDA(B) = 0 for B<J;A belA (B) = bel(BuA)- bel( A) for B<;;;11 plA(B) = pl(BnA) for B<;;;Q\nThis solution is always applicable, even when pl(A) = 0. Notice that m A (0) might be non null with this solution. In fact, one has: mA(0) = m(0) + bel(A).\nThis rule is the one described in the transferable belief model under open world assumption (Smets 1988).\nConditioning C.2. The Normalized Dempster's Rule of Conditioning.\nMasses are transferred as in conditioning C. 1, but the result is then proportionally normalized to cope with the masses that would be transferred to the empty set. This avoids ending with a positive mass on 0, and guarantees that bel(Q) = 1. After conditioning, one gets:\nIDA(B) =c L m(Bu X ) for B<;;;A,Bt0 x<;;;A\nIDA(B) = 0 and for B<;;;11\nbelA (B) = c (bel(BuA)- bel(A) ) plA(B) = c pl(BnA)\nwhere c\u00b71 = bel(11) - bel(A) = pi( A).\nfor B<J;A\nThis solution applies only when pl(A)>O. No solution is provided when pl(A)=O.\nThis rule is the one described in the transferable belief model under closed world assumption. It is the one initially proposed by Shafer (1976a).\nConditioning C.2'. Bayesian solution.\nThe classical solution with probability function is, _ P(BnA) c PA(B)- P(A) for B _Q. This solution applies only if P(A)>O. If P(A)=O, no solution is provided.\nThis is a particular case of conditioning C.2. It is obtained when the belief function bel is a probability function.\nConditioning C.3. Yager-Kohlas's Solution.\nThe basic belief masses are transferred as in C.1, but the masses that could be transferred to the empty set are reallocated to A, so:\nmA(B) = I. m(BuX) VB<:A,Bo<A,B,.t0 x<:A mA(A) = L m(AuX) + bel(A) X<:A\nmA(B) = 0 VB\ufffdA be!A(B) = bel(BuA) - bel(A) VB<:A,Bo<A belA(A) = bel(Q) plA(B) = pl(BnA) + bel(A) VB<:A\nThis conditioning might seem artificial, but it will be shown that it can be observed sometimes. It was proposed in Yager (1985) and Kohlas (1989). It applies normally only to normalized belief functions (i.e., those with m(0)=0 or equivalently with bel(Q)=l)\nConditioning C.4. Geometric Rule of Conditioning.\nOne way to condition belief functions consists in deciding that all basic belief masses not given to subsets of A are nullified, and those given to subsets of A are proportionally normalized so their sum remains one. Then mA, belA and piA become:\n_!!!@__ =bel( A) =0\nbel (B) = bei(BnA) A bel(\ufffd p!A(B) =\npl(BuA) - p l(A) pl(Q) - pl(A)\nif B<:A\notherwise\nVB<:Q\nVB<:Q\nAbout Updating 379\nThese relations are described under the closed-world assumption. Their extensions under open-world assumption are obtained by suppressing the denominators in mA , belA and piA . The geometrical rule of conditioning has been discussed in Suppes and Zanotti (1977) and Shafer (1976b).\nConditioning C.S. Specialization.\nKruse (1990) has studied a conditioning rule that applies to the transferable belief model and is probably the most general form of conditioning that ties in with the idea of basic belief masses that quantify the part of belief that supports a set A of Q and cannot support a more specific subset of A through lack of information. The idea is that the basic belief mass given to a set A is distributed among the subsets B of A. This concept was also studied by Yager (1986) and Dubois and Prade (1986) when they introduced the ideas of belief inclusions. These authors work under closed-world assumption. Our presentation is made under the open-world assumption.\nFor each subset X<:Q, let c(B, X) VX,B<:Q be non negative coefficients such that:\nc(B,X) = O if B\ufffdX L c(B,X) = 1\nB<:X Then the basic belief masses m* obtained by a specialization based on the c(B, X) coefficients are:\nm*(B) = I, c(B, X) m(X) VB<:Q B<:X\nThe Dempster and the geometric rule of conditioning are specializations. Suppose we known that the answer to the question about Q is not in A<:Q.\nOne obtains the unnormalized Dempster's rule of conditioning when c(B, BuY) = 1, Y<:A. Then:\nmA(B) = I. c(B, BuY) m(BuY) vBc;;;;A y<;;;Q\nOne obtains the unnormalized geometric rule of conditioning if c@, B) = 1 whenever B<: A and c(0, B) = 1 whenever BnAo<0 otherwise. Normalized rules of conditioning are obtained by further rescaling.\nYager-Kohlas conditioning_js not a specialization as the masses given to subsets of A are transferred to A.\nRemark: the unnormalized Dempster's rule of combination is also a specialization. Suppose there exist two basic belief masses assignment m 1 and mz on Q. Let m 12 = m 1 almz and let c(X, Z) = m 1 (XIZ). Then (Smets 1991b):\n380 Smets\nm12(X) = I. m1(XuY) m2(XuZ) = YnZ=0 Y,Z\ufffdA = I. m 1 (XIZ) m2(Z)\nZ\ufffdO\nConditioning C.6. Imaging.\nLewis (1976) considers that probabilities are given to worlds, so each roi represents a world. If one learns that a set A of world is impossible (does not contain th\ufffd answer), then the probabilities given to the worlds in A are transferred to the 'closest' worlds in A.\nSuppose the 'closest' relation n(ro, A): OxO\ufffdO with n(ro, A) being the world in A that is the closest to world ro. So n(ro, A)e A and n(ro, A)=ro if roe A, i.e., the worlds in A are the closest to themselves.\nAfter conditioning on A by the imaging method, the probability P(ro) given to a world ro is transferred to the world n(ro, A).\nLet F(roilroj) = 1 if roi= n(roj> A) = 0 otherwise Then P A(roi) = L F(roilroj) P(roj) \\I roie 0. ro\u00b7e 0 J - Note that P A (roi) = 0 if roie A. We use the conditional probability notation for F to enhance the fact that F behaves as a conditional probability function.\nOf course the idea of 'closest' world needs to be defined and most of the criticism against this conditioning rule focusses on criticism of the idea of closeness between worlds (see Gardenfors 1988).\nGardenfors (1988) generalizes Lewis's imaging. He considers that the probability given to a world that is learned to be impossible is distributed among the remaining worlds according to some probability distribution that somehow reflect closeness between worlds. F(roilroj) will repres\ufffdt the portion of the probability given to world roje A that is transferred to world roie A. So let:\nand\nF(roilroj) = 0 =I =0 \ufffd0\nL F(roilroj) = 1 (l)iE 0\nif (l)iE A if (l)jE A, (l)i = (l)j if (l)jE!, (l)i \"# (l)j if roje A and roie A\nVroje 0\nConditioning on A leads to the relations:\nNote that P A(roi) = 0 if roie A.\nThese relations were described for probability function (under closed-world assumption). They can be general\ufffded in two ways: 1) the requirement F(roilroj) = 0 if roie A is dropped, and 2) the domain from 0 to 20 is generalized (one assimilates the basic belief masses to probabilities on the power set 20). Let the relation F:20x20\ufffd[0, 1] with: I. F(BIX) = 1 VX\ufffdO\nB\ufffdO\nAfter conditioning on 'not in A', one gets: mA(B) = I. F(BIX) m(X) VB\ufffdO X\ufffdO This conditioning relation subsumes all those presented so far.\nSpecialization is obtained if: F(BIX) = 0 VBnA-#0 Yager-Kohlas conditioning is \ufffdbtained if: F(BIX) = 0 VBnA;t0\n= 1 B\ufffdA.X=BuYVY\ufffdA = 1 B=A,X\ufffdA\nConditioning C.7. Upper and Lower Bayesian Conditioning.\nSuppose I know only that an unknown probability function P over 0 belongs to a convex subset <J> of the set IP of the probability functions defined on 0. The upper and lower probabilities P* and P* define P uniquely, where \\I A\ufffdO P*(A) =max (P(A) : Pe !1')\nP*(A) = min (P(A) : Pe <J>).\nP(AnB) Suppose one asks for the value of P(BIA) = P(A) . All that can be said is that P(BIA) is between the upper and lower conditional probabilities P*(BIA) and P*(BIA),\nwhere \\I A \ufffd0 (Smets 1987): P*(BIA) =max (P(BIA) : Pe <J>) P*(BIA) = min (P(BIA) : Pe <J>).\nIt can be shown that (see Dempster 1967, Fagin and Halpern 1990, Jaffray 1990, Zhang 1989):\nP*(BIA) P *(AnB)\nP*(AnB)+P*(AnB)\nP*(AnB) P*(BIA) = ----'-----' \ufffd\ufffd P*(AnB)+P*(AnB)\nFagin and Halpern (1990), Jaffray (1990) and Zhang (1989) show that if P*(.) is a belief function, then P*(.IA) is also a belief function. Jaffray (1990) also provides the basic belief masses derived from P*(.IA) by the inverse Moebius transform.\nIt can be shown that the Upper and Lower Bayesian Conditioning is not a specialization. It is a generalized imaging where the coefficents depend on the basic belief masses of the initial belief function.\n2. THE SCENARIO: THE VOTING INTENTIONS STUDY.\nTo illustrate the meaning of the various rules of conditioning we have described, we present a scenario that deals with objective data, induced objective proportions, and where the various forms of conditioning can be described, depending on the contextual information.\nSuppose I organize a study on how people will vote in the next election. Let Q = (a, b, c, d, e) be the set of candidates. One candidate must be selected by I 00 voters. Each voter may vote for only one candidate. Voting will be next Sunday and today, Monday, I shall ask each potential voter to indicate for whom he intends to vote. But voters' opinions, today, are not firmly established and some voters can only point to a subset A of Q that contains the name of the candidate they will vote for, but they have not yet decided definitively among these candidates in set A. We accept that voters will always vote for one of the candidates belonging to the set they provided to on Monday; opinions can only be made more specific.\nThe voting intentions of the 100 voters are summarized in table 1. Note that the data do not result from a survey based on a sample, but from an exhaustive study of the\nAbout Updating 381\nwhole population. This prevents problems related to sampling variations.\nOn Sunday, the 100 voters will vote and their votes will generate a frequency distribution over Q. Let IP be the set of frequency distributions Prop over Q, where Prop( a) for aE Q is the proportion of voters who vote a, and Prop( A) = L Prop(a) for Ac;;;n1. We will neglect the fact that\naE A the population is finite, and accept that Prop( A) for A c;;;n can take any value in [0, 1).\nOn Sunday, one element of IP will be selected, the one corresponding to the distribution of votes. But today we do not for instance know Prop(a). We know that Prop(a) is at least 5%, at most 78%. Any value in between is acceptable. So all the Monday data says is that Prop belongs to a subset fP of IP where fP contains all those frequency distributions on n compatible with the observed frequencies given in table 1. The set fP is uniquely defined by the upper and lower proportions Prop* and Prop* where 'if A c;;; n Prop*(A) = max{ Prop(A): PropE ff>} and Prop*(A) =min { Prop( A): PropE ff>} Some upper and lower proportions induced by the data of table 1 are given in table 2. The proportions Prop( A) will be such that 'if A c;;; n:\nProp*( A) :>Prop( A) :> Prop*(A)\nSet {a}\n{a, b) {a, b, c}\n{c) {d)\n{c, d} {c,d, e)\n5=5% 5+8 = 13% 5+8+15 = 28% 0=0% 0=0% 0 = 0%\n22 = 22%\nProp* 5+8+15+29- 57% 5+8+15+21+29 = 78% 5+8+15+21+29 = 78%\n15+21+29 = 65% 21+29+22 = 72% 15+21+29+22 = 87% 15+21+29+22 = 87%\nI collected my data on Monday, but on Tuesday I learn candidates a and b were killed in a car accident during the night. What can I say about Sunday's elections results?\nI We speak of proportions, not of probablity, in order to avoid any confusion. Probability admits many definitions. Using proportions, and later beliefs, will prevent - hopefully - any confusion.\n382 Smets\nThe 15 voters who answered (a, b, c) will have to vote for c as c is the only remaining candidate and these 15 voters had indicated their willingness to consider a candidate in {a, b, c). Identically, the 21 and 29 voters who had answered {b, c, d) or {a, b, c, d) will vote for a candidate in ( c, d) . The impact of the conditioning information, a and b are dead, results in a transfer of frequencies similar to the one described in the transferable belief model. The frequency given to a set X is transferred _!o the set Xn Y once we know that the truth is not in Y. But what about the 13 voters who answered (a) or (a, b)?\nSeveral scenarios can be considered that lead to the different solutions we wish to illustrate. Some may appear somewhat artificial, but that is not the point. We only wish to give a meaning to each conditioning rule.\nThe labels of the scenarios are those of the conditioning rules described in section 2.\nScenario Cl : Compulsory Voting.\nSuppose we are in Belgium where voting is compulsory (absentees are fined). The 13 voters must cast their votes. If blank voting is allowed, we might consider they will cast blank votes. Table C.1 presents some upper and lower proportions induced after conditioning on ( c, d, e) . Note that 13 blank votes will be collected, so Prop*(0)= Prop*(0) = 13%.\nSet (c) {d)\n(c,d) (c, d,e)\n15 = 15% 0 = 0%\n15+21+29 = 65% 65+22 = 87%\nProp* 15+21+29 = 65% 21+29+22 = 72% 15+21+29+22 = 87% 15+21+29+22 = 87%\nTable C.l. Values for some upper and lower proportions induced by the frequencies of table 1 and the conditioning C. l . (65 = 15+21+29)\nThis conditioning rule is similar to Dempster's rule of conditioning under open world assumption.\nScenario C.2. Free Voting.\nSame as Scenario C. l , but we compute proportions among those who do not abstain. This is also the French situation where there is no obligation to vote, in which case the 13 voters would not vote on Sunday. Table C.2 presents some upper and lower proportions induced after conditioning on ( c, d, e)\nThis conditioning rule is similar to Dempster's rule of conditioning under closed-world assumption. It is identical with the C.l conditioning rule except for the normalization factor (the division by 87).\nScenario C.3. Compulsory Choice.\nBack to Belgium context C. 1 (compulsory voting), but blank votes are not allowed. So all I know about the 13 voters that pose a problem is that they will vote for a candidate in ( c, d, e) . Table C.3 presents some upper and lower proportions induced after conditioning on ( c, d, e) .\nThis conditioning is similar to Yager-Kohlas rule.\nScenario C.4: Geometrical Rule.\nSuppose that each voter who had given an answer that contained a or b is so depressed that he commits suicide. Then only 22 voters are left. The proportions among those who had answered a subset of ( c, d, e) will be proportionally rescaled. Table C.4 presents some upper and lower proportions induced after conditioning on { c, d, e).\nThis conditioning corresponds to the geometrical rule of conditioning, a form also encountered naturally in scenarios based on random sets (Smets 1990a)\nScenario C.S: Specialization\nSpecialization is obtained when I consider that the information \"a and b are dead' allows me to reconsider each voter's answer. For each type of answer, I consider that those who select it will be distributed among its subsets not containing a and b according to a known distribution. The coefficients c of the specialization are these probabilities. As an example, suppose I know (by my knowledge of the political links among the candidates) that: 1) among those who answered (b, c, d), one third will vote c, one third will vote d, one third is still undecided, 2) among those who answered (a, b, c, d), half will vote d, the other half is still undecided, and 3) among those who answer ( d, e), half will vote d, the others will vote e. The 13 voters who answer (a) and (a, b) will cast blank votesas in scenario C.1 (normalization can be introduced as in scenario C2). Table C.5 presents some upper and lower proportions induced after conditioning on (c, d, e).\nSet\n(c) (d)\n(c,d) (c,d,e)\n15+7=22% 7+ 14.5+ 11=32.5%\n65+11=76% 65+22=87%\nProp*\n15+ 14+ 14.5=43.5% 14+29+ 11=54%\n65+11=76% 65+22=87%\nTable C.S. Values for some upper and lower proportions induced by the frequencies of table 1 and the conditioning C.5.\nThis conditioning form corresponds to the specialization.\nScenario C.6.1. Introspective Realloca-tion: level 1.\nMaybe I know more about the candidates than their names. I know that a and b had similar political orientations and that among c, d and e, c is politically the closest to a and b. Then I might consider that the 13 voters without a candidate will vote for c. Table C.6.1 presents some upper and lower proportions induced after conditioning on ( c, d, e)\nSet\n(c) {d)\n(c,d) (c, d, e)\n15+13=28% 0=()%\n65+13=78% 65+22+ 13=100%\nProp*\n15+21+29+13-78% 21+29+22=72%\n65+22+13=100% 65+ 22+ 13= 100%\nTable C.6.1. Values for some upper and lower proportions induced by the frequencies of table 1 and the conditioning C.6.1.\nAb out Updating 383\nThis conditioning generalizes the imaging conditioning introduced by Lewis where:\nVY\ufffdA F(BIBuY) = l B\ufffdA = 0 otherwise\nScenario C.6.2. Introspective Realloca-ion: level 2.\nGeneralizing Scenario C.6. 1, we might consider c as politically very close to a and b, d as quite similar to a and b, and e totally different. So we might accept that there is a certain proportion of the 13 voters that will vote for c and the remainder will vote for c or d. Let the proportion of people that will vote for c given they had decided to vote for a or b be 0.4, and the others will vote for (c,d).\nTable C.6.2 presents some upper and lower proportions induced after conditioning on ( c, d, e).\nSet\n(c) {d)\n(c, d) [c, d, e)\n15+5.2=20.2% 0=0%\n65+13=78% 65+22+ 13=100%\nProp*\n15+21+29+13-78% 2 1+29+22+7.8=80%\n65+22+13=100% 65+22+13=100%\nTable C.6.2. Values for some upper and lower proportions induced by the frequencies of table 1 and the conditioning C.6.2.\nThis form of conditioning generalizes Giirdenfors's Imaging to power sets, where\nVY\ufffdA F(BIY) \ufffd0 B\ufffdA = 0 otherwise\nScenario C.6.3. Introspective Realloca-tion: level 3.\nYou might be even more subtle in your opinion than in scenario C.6.2. You might consider differently the 5 voters who answered (a) and the 8 who answered (a, b).\nSet\n(c) {d)\n(c,d) (c,d,e)\n15+6=21% 0=0% 65+11=76% 65+22+ 13=100%\nProp*\n65+13-78% 21+29+22+5=77%\n65+22+13=100% 65+22+13=100%\nTable C.6.3. Values for some upper and lower proportions induced by the frequencies of table 1 and the conditioning C.6.3.\nThey are not identical and you might consider that the distribution of the 5 among ( c, d, e) is different from the distribution of the 8. For instance, you might consider that 1) among the 5 voters who answered (a) Monday,\n384 Smets\n40% (=2) would answer {c) Tuesday, the other (=3) (c, d), and 2) among the 8 voters who answered {a, b}, half (=4) would answer {c) , one quarter (=2) would answer {c, d) and the last quarter (=2) would answer {c, e). Table C.6.3 presents some upper and lower proportions induced after conditioning on { c, d, e) .\nThis corresponds to the generalization of the imaging where\nF(BIB) = 1 F(BIX) :?: 0\n=0\nif B\ufffdA if B\ufffdA, X\ufffdA otherwise\nScenario C.7. Upper and Lower Bayesian Conditioning.\nBefore learning that a and b are dead, i.e., Monday evening, I would like to assess the proportion of those who will vote for c among those who will vote for c or d or e on Sunday. Should I know the frequency distribution of the Sunday votes, I would compute Prop({c)) Prop({c}l{c, d, e})=p ({ d ) ) rop c, , e\nBut I do not know these values. All I know are upper and lower limits for each Prop. I know that Prop is in a subset fJ> of IP, the set of frequency distributions on Q. For each Prop in fJ>, I can compute Prop({c) l{c, d, e)).\nConsequently I know that the upper lower limits of Prop( {c) I { c, d, e}) are between the upper and lower conditional proportions Prop*(.! { c, d, e}) and Prop*(.! { c, d, e)) where * Prop({c)) m Prop (AI{c,d,e}) = max{p ({ d ) ) : Prope or ) rop c, , e Prop*({ c)) = Prop*({c) )+Prop*({d, e}) . Prop({c)) m Prop*(AI{c,d,e}) = mm{Prop({c, d, e) ):\nPrope or l Prop*({c})\n= Prop*( {c) )+Prop*( { d, e) )\nThis conditioning corresponds to the upper and lower bayesian conditioning.\n4. BELIEFS INDUCED BY THE\nPROPORTIONS.\nSuppose a voter is going to be selected randomly among the 100 voters (with equiprobability for each voter). The question is to bet on who is the Sunday candidate of this randomly selected voter.\nGiven the available data, all I can say is that the proportion Prop( A) of voters who will vote for a candidate in set A\ufffdQ on Sunday is included between Prop*(A) and Prop*( A). So the probability P(A) that the selected voters will vote for a candidate in A is included between Prop*( A) and Prop*(A). Thus I can build upper and lower probabilities P*(A) and P*(A) for P(A) where P*(A) = Prop*(A) and P*(A) = Prop*(A). Given this set of upper and lower probabilities, I can build the pignistic probabilities BetP of the fact that the randomly selected voter will vote for a candidate in A (Smets 1990b), (see also Smets (199Ib) for a practical justification of the pignistic transformation).\nBetP(A) = L m(X) IX\ufffdt l X\ufffdQ\nwhere lXI is the number of candidates in set X\ufffdQ. BetP is a probability function, and all bets on Q are built on it.\nBut the know ledge of these upper and lower probabilities can also induce a belief in me about the candidate for which the randomly selected voter will vote. As shown in Smets (1991a), the belief function that quantifies my belief about Q is numerically equal to the lower probabilities function (because mathematically the lower probability happens to be a belief function in the present scenario). This is based on the maximal-minimal isopignistic transformation described in Smets (1991a). The pignistic transformation of this belief function is of course the same as the one derived from the upper and lower probabilities. So analysing the problem directly from the upper and lower probabilities point of view or through the belief function induced by these upper and lower probabilities leads to the same results.\nA bet on who will be the Sunday winner is not analysed. It requires a study of our belief concerning the subsets of IP. The belief that x is the winner is equal to the belief allocated to those frequency distributions in ff> \ufffd IP where x is the most frequent observation. That is a completely different problem altogether and will not be dealt with.\n5. CONCLUSIONS.\nWe have shown that conditioning can be performed by many rules, and through an illustrative example, we have provided scenarios that lead to each form of conditioning. This study is not exhaustive as other forms of updating can - and have been - suggested (Cano and Moral 1990, Moral and De Campos 1990). We hope that these illustrative examples will help the user understand the meaning of the various conditioning rules we have studied.\nAcknowledgements.\nThe following text presents some research results of the Belgian National incentive-programme for fundamental research in artificial intelligence initiated by the Belgian State, Prime Minister's Office, Science Policy Programming. Scientific responsibility is assumed by the author. Research work has been partly supported by the DRUMS project which is funded by a grant from the Commission of the European Communities under the ESPRIT II-Program, Basic Research Project 3085.\nBibliography.\nCANO J.E. and MORAL S. (1990) Combination of incomplete probabilistic information. (unpublished manuscript) DEGROOT M.H. (1970) Optimal statistical decisions. McGraw-Hill, New York. DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping. Ann. Math. Statistics 38: 325-339. DUBOIS D. and PRADE H. (1986) A set theoretical view of belief functions. Int. J. Gen. Systems, 12:193-226. FAGIN R. and HALPERN J. (1990) A new approach to uptdating beliefs. 6th Conf. on Uncertainty in AI. GARDENFORS P. (1988) Knowledge in flux. Modelling the dynamics of epistemuic states. MIT Press, Cambridge, Mass. JAFFREY J.Y. (1990) Bayesian conditioning and belief functions. IPMU-1990. KOHLAS J. (1989) The reliability of reasoning with unreliable arguments. Inst. Automation and Oper. Research, Univ. Freiburg, Technical report 168. KRUSE R. and SCHWECKE E. (1990) Specialization: a new concept for uncertainty handling with belief functions. Int. J. Gen. Systems (to appear) LEWIS D. (1976) Probabilities of conditionals and conditional probabilities. Philosophical Review 85: 297- 315. MORAL S. and DE CAMPOS L.M. (1990) Updating uncertain information. (unpublished manuscript) PLANCHET B. (1989) Credibility and conditioning. J. Theor. Probabil. 2:289-299.\nAbout Updating 385\nSHAFER G. (1976a) A mathematical theory of evidence. Princeton Univ. Press. Princeton, NJ. SHAFER G. (1976b) A theory of statistical evidence. in Foundations of probability theory, statistical inference, and statistical theories of science. Harper and Hooker ed. Reidel, Doordrecht-Holland. SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kanal L.N. and Lemmer J.F. eds, North Holland, Amsterdam, , 2940. SMETS Ph. (1991a)Belief induced by the knowledge of some probabilities. Submitted for publication. SMETS Ph. (1991b) Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem. (submitted for publication) SUPPES P. and ZANOTTI M. (1977) On using random relations to generate upper and lower probabilities. Synthesis 36:427-440. YAGER R.R. (1985) On the Dempster-Shafer framework and new combination rules. Technical report MII-504, Machine Intelligence Institute, Iona College. YAGER R.R. (1986) The entailment principle for Dempster-Shafer granules. Int. J. Intell. Systems 1:247- 262 ZHANG L (1989) A new proof to theorem 3.2 of Fagin and Halpern's paper. Unpublisshed Memorandum, University of Kansas, Business School."}], "references": [{"title": "Combination of incomplete probabilistic information", "author": ["J.E. CANO", "S. MORAL"], "venue": "(unpublished manuscript) DEGROOT M.H", "citeRegEx": "CANO and MORAL,? \\Q1990\\E", "shortCiteRegEx": "CANO and MORAL", "year": 1990}, {"title": "Upper and lower probabilities induced by a multplevalued mapping", "author": ["A.P. DEMPSTER"], "venue": "Ann. Math. Statistics", "citeRegEx": "DEMPSTER,? \\Q1967\\E", "shortCiteRegEx": "DEMPSTER", "year": 1967}, {"title": "A new approach to uptdating beliefs", "author": ["R. FAGIN", "J. HALPERN"], "venue": "6th Conf. on Uncertainty in AI. GARDENFORS P", "citeRegEx": "FAGIN and HALPERN,? \\Q1990\\E", "shortCiteRegEx": "FAGIN and HALPERN", "year": 1990}, {"title": "Bayesian conditioning and belief functions. IPMU-1990", "author": ["J.Y. JAFFREY"], "venue": "KOHLAS J", "citeRegEx": "JAFFREY,? \\Q1990\\E", "shortCiteRegEx": "JAFFREY", "year": 1990}, {"title": "Specialization: a new concept for uncertainty handling with belief functions", "author": ["R. KRUSE", "E. SCHWECKE"], "venue": "Int. J. Gen. Systems (to appear) LEWIS D", "citeRegEx": "KRUSE and SCHWECKE,? \\Q1990\\E", "shortCiteRegEx": "KRUSE and SCHWECKE", "year": 1990}, {"title": "Updating uncertain information", "author": ["S. MORAL", "L.M. DE CAMPOS"], "venue": "(unpublished manuscript) PLANCHET B", "citeRegEx": "315", "shortCiteRegEx": "315", "year": 1990}, {"title": "The transferable belief model and random sets", "author": ["SMETS Ph"], "venue": "To appear in Int. J. Intell. Systems. SMETS Ph", "citeRegEx": "Ph.,? \\Q1990\\E", "shortCiteRegEx": "Ph.", "year": 1990}, {"title": "1991a)Belief induced by the knowledge of some probabilities. Submitted for publication", "author": ["Shachter R.D", "Kanal L.N", "Lemmer J.F. eds", "North Holland", "Amsterdam", "2940. SMETS Ph"], "venue": "SMETS Ph", "citeRegEx": "R.D. et al\\.,? \\Q1991\\E", "shortCiteRegEx": "R.D. et al\\.", "year": 1991}, {"title": "On the Dempster-Shafer framework and new combination rules. Technical report MII-504, Machine Intelligence Institute, Iona College", "author": ["R.R. YAGER"], "venue": "YAGER R.R", "citeRegEx": "YAGER,? \\Q1985\\E", "shortCiteRegEx": "YAGER", "year": 1985}, {"title": "A new proof to theorem 3.2 of Fagin and Halpern's paper", "author": ["L ZHANG"], "venue": "Unpublisshed Memorandum,", "citeRegEx": "ZHANG,? \\Q1989\\E", "shortCiteRegEx": "ZHANG", "year": 1989}], "referenceMentions": [{"referenceID": 1, "context": "DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping. Ann. Math. Statistics 38: 325-339. DUBOIS D. and PRADE H. (1986) A set theoretical view of belief functions.", "startOffset": 0, "endOffset": 152}, {"referenceID": 1, "context": "DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping. Ann. Math. Statistics 38: 325-339. DUBOIS D. and PRADE H. (1986) A set theoretical view of belief functions. Int. J. Gen. Systems, 12:193-226. FAGIN R. and HALPERN J. (1990) A new approach to uptdating beliefs.", "startOffset": 0, "endOffset": 261}, {"referenceID": 1, "context": "DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping. Ann. Math. Statistics 38: 325-339. DUBOIS D. and PRADE H. (1986) A set theoretical view of belief functions. Int. J. Gen. Systems, 12:193-226. FAGIN R. and HALPERN J. (1990) A new approach to uptdating beliefs. 6th Conf. on Uncertainty in AI. GARDENFORS P. (1988) Knowledge in flux.", "startOffset": 0, "endOffset": 351}, {"referenceID": 1, "context": "DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping. Ann. Math. Statistics 38: 325-339. DUBOIS D. and PRADE H. (1986) A set theoretical view of belief functions. Int. J. Gen. Systems, 12:193-226. FAGIN R. and HALPERN J. (1990) A new approach to uptdating beliefs. 6th Conf. on Uncertainty in AI. GARDENFORS P. (1988) Knowledge in flux. Modelling the dynamics of epistemuic states. MIT Press, Cambridge, Mass. JAFFREY J.Y. (1990) Bayesian conditioning and belief functions.", "startOffset": 0, "endOffset": 463}, {"referenceID": 1, "context": "DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping. Ann. Math. Statistics 38: 325-339. DUBOIS D. and PRADE H. (1986) A set theoretical view of belief functions. Int. J. Gen. Systems, 12:193-226. FAGIN R. and HALPERN J. (1990) A new approach to uptdating beliefs. 6th Conf. on Uncertainty in AI. GARDENFORS P. (1988) Knowledge in flux. Modelling the dynamics of epistemuic states. MIT Press, Cambridge, Mass. JAFFREY J.Y. (1990) Bayesian conditioning and belief functions. IPMU-1990. KOHLAS J. (1989) The reliability of reasoning with unreliable arguments.", "startOffset": 0, "endOffset": 535}, {"referenceID": 1, "context": "DEMPSTER A.P. (1967) Upper and lower probabilities induced by a multplevalued mapping. Ann. Math. Statistics 38: 325-339. DUBOIS D. and PRADE H. (1986) A set theoretical view of belief functions. Int. J. Gen. Systems, 12:193-226. FAGIN R. and HALPERN J. (1990) A new approach to uptdating beliefs. 6th Conf. on Uncertainty in AI. GARDENFORS P. (1988) Knowledge in flux. Modelling the dynamics of epistemuic states. MIT Press, Cambridge, Mass. JAFFREY J.Y. (1990) Bayesian conditioning and belief functions. IPMU-1990. KOHLAS J. (1989) The reliability of reasoning with unreliable arguments. Inst. Automation and Oper. Research, Univ. Freiburg, Technical report 168. KRUSE R. and SCHWECKE E. (1990) Specialization: a", "startOffset": 0, "endOffset": 698}, {"referenceID": 5, "context": "Philosophical Review 85: 297315. MORAL S. and DE CAMPOS L.M. (1990) Updating uncertain information.", "startOffset": 28, "endOffset": 68}, {"referenceID": 5, "context": "Philosophical Review 85: 297315. MORAL S. and DE CAMPOS L.M. (1990) Updating uncertain information. (unpublished manuscript) PLANCHET B. (1989) Credibility and conditioning.", "startOffset": 28, "endOffset": 144}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions.", "startOffset": 6, "endOffset": 17}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions.", "startOffset": 6, "endOffset": 182}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets.", "startOffset": 6, "endOffset": 377}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty.", "startOffset": 6, "endOffset": 480}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kanal L.N. and Lemmer J.F. eds, North Holland, Amsterdam, , 2940. SMETS Ph. (1991a)Belief induced by the knowledge of some probabilities.", "startOffset": 6, "endOffset": 711}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kanal L.N. and Lemmer J.F. eds, North Holland, Amsterdam, , 2940. SMETS Ph. (1991a)Belief induced by the knowledge of some probabilities. Submitted for publication. SMETS Ph. (1991b) Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem.", "startOffset": 6, "endOffset": 810}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kanal L.N. and Lemmer J.F. eds, North Holland, Amsterdam, , 2940. SMETS Ph. (1991a)Belief induced by the knowledge of some probabilities. Submitted for publication. SMETS Ph. (1991b) Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem. (submitted for publication) SUPPES P. and ZANOTTI M. (1977) On using random relations to generate upper and lower probabilities.", "startOffset": 6, "endOffset": 962}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kanal L.N. and Lemmer J.F. eds, North Holland, Amsterdam, , 2940. SMETS Ph. (1991a)Belief induced by the knowledge of some probabilities. Submitted for publication. SMETS Ph. (1991b) Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem. (submitted for publication) SUPPES P. and ZANOTTI M. (1977) On using random relations to generate upper and lower probabilities. Synthesis 36:427-440. YAGER R.R. (1985) On the Dempster-Shafer framework and new combination rules.", "startOffset": 6, "endOffset": 1071}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kanal L.N. and Lemmer J.F. eds, North Holland, Amsterdam, , 2940. SMETS Ph. (1991a)Belief induced by the knowledge of some probabilities. Submitted for publication. SMETS Ph. (1991b) Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem. (submitted for publication) SUPPES P. and ZANOTTI M. (1977) On using random relations to generate upper and lower probabilities. Synthesis 36:427-440. YAGER R.R. (1985) On the Dempster-Shafer framework and new combination rules. Technical report MII-504, Machine Intelligence Institute, Iona College. YAGER R.R. (1986) The entailment principle for Dempster-Shafer granules.", "startOffset": 6, "endOffset": 1221}, {"referenceID": 6, "context": "SMETS Ph. (1988) Belief functions. in SMETS Ph, MAMDANI A., DUBOIS D. and PRADE H. ed. Non standard logics for automated reasoning. Academic Press, London p 253-286. SMETS P. (1987) Upper and lower probability functions versus belief functions. Proc. International Symposium on Fuzzy Systems and Knowledge Engineering, Guangzhou, China, July 10-16, pg 17-21. SMETS Ph. (1990a) The transferable belief model and random sets. To appear in Int. J. Intell. Systems. SMETS Ph. (1990b) Construucting the pignistic probability function in a context of uncertainty. Uncertainty in Artificial Intelligence 5, Henrion M., Shachter R.D., Kanal L.N. and Lemmer J.F. eds, North Holland, Amsterdam, , 2940. SMETS Ph. (1991a)Belief induced by the knowledge of some probabilities. Submitted for publication. SMETS Ph. (1991b) Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem. (submitted for publication) SUPPES P. and ZANOTTI M. (1977) On using random relations to generate upper and lower probabilities. Synthesis 36:427-440. YAGER R.R. (1985) On the Dempster-Shafer framework and new combination rules. Technical report MII-504, Machine Intelligence Institute, Iona College. YAGER R.R. (1986) The entailment principle for Dempster-Shafer granules. Int. J. Intell. Systems 1:247262 ZHANG L (1989) A new proof to theorem 3.", "startOffset": 6, "endOffset": 1324}], "year": 2011, "abstractText": "Survey of several forms of updating, with a practical illustrative example. We study several updating (conditioning) schemes that emerge naturally from a common scenario to provide some insights into their meaning. Updating is a subtle operation and there is no single method, no single 'good' rule. The choice of the appropriate rule must always be given due consideration. Planchet (1989) presents a mathematical survey of many rules.We focus on the practical meaning of these rules. After summarizing the several rules for conditioning, we present an illustrative example in which the various forms of conditioning can be explained. 1. CONDITIONING RULES FOR", "creator": "pdftk 1.41 - www.pdftk.com"}}}