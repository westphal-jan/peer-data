{"id": "1303.5759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "An Efficient Implementation of Belief Function Propagation", "abstract": "surprisingly the local logic computation technique ( shafer liang et al. peng 1987, shafer and shenoy 1988, shenoy and shafer 1986 ) is used occasionally for propagating belief functions in so called scenarios a markov hash tree. proven in this paper, today we efficiently describe an efficient feasible implementation characterized of improved belief & function internal propagation depend on the internal basis of how the local computation technique technique. usually the presented method avoids modification all by the externally redundant dependent computations provided in the propagation process, and so makes yet the computational complexity almost decrease with respect to other better existing implementations ( chen hsia and wei shenoy feng 1989, zarley et ot al. anderson 1988 ). we systematically also give hence a combined algorithm p for both alternative propagation and any re - intervening propagation cycles which makes the re - propagation process more efficient when one or more of randomly the corresponding prior belief functions is changed.", "histories": [["v1", "Wed, 20 Mar 2013 15:34:07 GMT  (453kb)", "http://arxiv.org/abs/1303.5759v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hong xu"], "accepted": false, "id": "1303.5759"}, "pdf": {"name": "1303.5759.pdf", "metadata": {"source": "CRF", "title": "An Efficient Implementation of Belief Function Propagation", "authors": [], "emails": ["r01505@bbrbfu01.bitnet"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nDempster-Shafer theory (Shafer 1976, Smets 1988) has been considered as one of the tools for dealing with the problem of uncertain information by the Artificial Intelligence community. The computational complexity of Dempster's rule of combination, the pivot mechanism of the theory, however, is the main obstacle to its effective use. However, several implementations of the Dempster-Shafer theory have recently been developed (Hsia and Shenoy 1989, Zarley 1988, Zarley et a!. 1988) based on the observation that an arbitrary belief function network can be represented as a hypergraph (Kong 1986), which can also be embedded in so called a Markov tree (Zhang 1988). These implementations use the local computation technique (Shafer et a!. 1987, Shafer and Shenoy 1988, Shenoy and Shafer 1986) for propagating belief functions in the Markov tree. According to this technique, the belief function propagation can be described as a message-passing scheme: each node in the Markov tree sends its message to one of its neighbours after it has received the messages from all of its other neighbours, and the result of propagation on each node is computed by combining its own belief function (prior belief function) and the messages from all of its neighbours. After the results for all the nodes have been computed, one may want to change one or more of the prior belief functions. Then we have to\nre-propagate the impact of the changes to all the other nodes. In general, there may be repeated computations during propagation and re-propagation process, which may greatly affect the efficiency of the computation. The goal of this paper is to present an efficient method for the implementation of belief function propagation. The main advantage of this scheme is that it avoids all the redundant computations during propagation, resulting in a reduced computational complexity with respect to that of other existing implementations. It is also shown that making full use of the stored messages passed between the nodes and of stored intermediate information, we can just re-propagate the changed values when some prior belief functions are changed.\nThe paper is organized as follows. In section 2, some basic concepts about belief function networks are reviewed. In section 3, we describe a straightforward implementation of belief functions propagation using local computation. In section 4, we present our implementation scheme. In section 5, we discuss the problem of updating messages when one or more inputs is changed, and give a combined algorithm for both propagation and re-propagation. Finally, some conclusions are given in section 6."}, {"heading": "2 SOME BASIC CONCEPTS ABOUT BELIEF FUNCTION NETWORKS", "text": "Dempster-Shafer theory (Shafer 1976, Smets 1988), is concerned with the problem of representing and manipulating incomplete knowledge. In this section, we recall some basic concepts and definitions about belief functions and belief function networks. This presentation follows (Shafer and Shenoy 1988, Shenoy 1989).\nvariables and Confjguratjons We use the symbol '\\.If X for the set of possible values of a variable X, and we call '\\.If x the frame for X. Given a finite non-empty set h of variables, let 'Ufh denote the Cartesian product of '\\.If x for X in h: 'Ufh =X{'UI xI Xeh}. We call 'Uih the frame for h. We refer to elements of '\\.If h as configurations of h.\nBasjc Probability Assignments A basic probability assignment (bpa) m on X, is a function which assigns a\n426 Xu\nvalue in [0, I] to every subset a of 'II! x and satisfies the following axioms: (i) m(0) = 0; and (ii) L{m(a) I a\ufffd'U! xl = 1\nBelief Functions A belief function Bel associated with a bpa m, is a function that assigns a value in [0, 1] to every non-empty subset a of 'II! X\u2022 called \"degree of belief in a\", defmedby\nBel(a) = L{m{b) lb\ufffda)\nThe subsets a for which m(a)>O are called focal elements of Bel. The simplest belief function is the one with m('U! x) =1, called vacuous belief function.\nProjection and Extension If g and h are sets of variables, h \ufffd g, and x is a configuration of g, then we let x.l.h denote the projection of x to 'U! h\u00b7 x.l.h is always a\nconfiguration of h. If 9 is a non-empty subset of 'II! g. then the projection of IJ to h, denoted by 9 .I. h, is obtained by projecting elements of9 to 'U!h, i.e. \ufffd;\ufffd.l.h= (x.l.h lxe\ufffd;\ufffd). By extension of a subset of a frame to a subset of a larger frame, we mean a cylinder set extension. If g and h are sets of variables, h\ufffdg, h>'g, and h is a subset of 'U!h, then the extension of h to g , denoted by h tg, is flx'Uf g-h\u00b7\nDempster's Rule of Combjnatjon Dempster's Rule of Combination is a rule for producing a new bpa from two bpa's. Considering two bpa's m1 and m2 on g and h, we let m = m 1 Ell m2 be the bpa on guh defined by\nm{0) = Oand m{c) = K-lr(m1(a)m2{b) 1 (at(guh) n b t(guh)) = c) where K= l-L(m1(a) m2(b) I (a t(guh) nb t(guh)) = 0}\nK is a normalizing factor, which intuitively measures how much m 1 and m2 are conflicting. If K = 0, then we say that mJ and m2 are not combinable.\nMarginalization Suppose m is a bpa on g and suppose\nh \ufffd g, h>'0. The marginal of m for h, denoted by mJ.h, is a bpa defmed by m.l.h(a)=L{m{b)l) \ufffd'II! g. &.l.h:a.} for all subsets a of 'II! h.\nA Belief Function Network Using Dempster-Shafer theory, the problem can be represented as a finite set of variables X. A finite frame 'II! x is associated with each\nvariable X of X and the elements of 'II! x are mutually exclusive and exhaustive.The relationships of the variables are expressed by subsets of X. The knowledge about the variable X is encoded in the belief function over 'II! x or over 'U!h where h = (X). The knowledge about relation among the variables is encoded in the belief function over 'U!h of subset h in w hich the variables are included. We call these belief functions as prior belief functions. So, a belief function network consists of X, a set of subsets of X: K, and a fmite\ncollection of independent belief functions (BelJ, Bel2, ... , Belk) where each belief function Beli is the prior belief function on some subset h, and is stored as a set of focal elements with their values.\nEvaluation of a Belief Function Network Suppose we are given a belief function network. To evaluate a belief function network, we have to: (i) combine all Beli in the network, the resulting belief function is called global belief function; (ii) compute the marginal of the global belief function for each variable in the network.\nBecause the computational complexity of Dempster's combination is exponential with the size of the frame of belief functions being combined, it will not be feasible to compute the global belief function when there are a large number of variables. In the next section, we will describe an alternative way to evaluate the belief function network by using the local computation technique proposed in (Shafer et al. 1987, Shafer and Shenoy 1988, Shenoy and Shafer 1986)."}, {"heading": "3 BELIEF FUNCTION PROPAGATION USING LOCAL COMPUTATION", "text": "It has been shown that if the belief function network can be represented as certain kind of tree, called Markov tree, the belief functions can be \"propagated\" in the Markov tree by a local message-passing scheme, producing as a result in the marginals of the global belief function for each of the nodes. We first look at what Markov tree is and how a belief function network can be represented as a Markov tree.\nGiven a tree G=('JTI., e) where each nooe (also called vertex) ve m, is a non-empty subset of a finite set V, e is the set of edges in G. Then G is Markov if for any ve m., such that v separates two other distinct nodes Vi and Vj in G, (vinvj)\ufffdv. Given three distinct nodes v, Vi and Vjo we say that v separates Vi and Vj if v is on the path between Vi and Vj-\nLet :t{ and X be as defined in the previous section. In the language of graph theory, K is called a hypergraph on X and each element of K is called a hyperedge. In order to use local computation for propagation, the hypergraph should be arranged in a Markov tree where V=X and 'JTI. ;;2K. We can always find a method to arrange a hypergraph in a Markov tree. Algorithms for constructing a Markov tree for a hypergraph can be found in (Kong 1986, Mellouli 1987, Zhang 1988). Two examples of Markov tree representatives (on the right hand side of Fig 3.1) for hypergraphs (on the left hand side of Fig 3.1) are shown below. In Example!,\nX1 =(a, b, c), K1 = ((a}, (b), (c), (a, b}, (b, c)), Th1 = K1; In Example2, X2 = (p, q, s, t, r}, K2 = ( (s), (t), (p), (q), (r), (s, p), (p, t), (t, q), (s, q), (p, r) ), where Th2 = K2 u ( (p, q, t), (s, p, q) l, where (p, q, t) and (s, p, q) are the new nodes added for constructing the Markov tree.\nAn Efficient Implementation of Belief Function Propagation 427\nExample!\n~ Example2\nFig3.1: Markov tree representatives of hypergraphs\nIn the rest of this section, we discuss Shafer, Shenoy and Mellouli's propagation scheme using local computation (Shafer et al. 1987). Suppose we have already arranged the hypergraph in a Markov tree G=(Th, e). For each node V, we let Nv={ Vkl(Vk. v)e e} be the set of neighbours of V, Belv the prior belief function on v, and Bel.!. v the marginal of the global belief function for v. Let L(G) be the leaves of G given some designated node as the root of G. During propagation, each node sends a belief function to each of its neighbours. The belief function sent by v to Vi is referred as a \"message\" and is denoted by Mv\ufffdvi. We define it as:\nMv\ufffdvi=((Belvel(Gl {MVk\ufffdVlVkE (Nv-{Vi}) })).j. (vrwi)) fvj (3.1)\nBecause a leaf v has only one neighbour, say vi, then the above expression reduces to:\nThus, when the propagation starts, the leaves of the Markov tree can send messages to their neighbours right away. The others send a message to one neighbour after they have received messages by all but that one neighbour. And when a node receives a message from that one neighbour, it appropriately (i.e. by using (3.1)) sends messages back to the remaining neighbours. All the messages can be transmitted through the Markov tree in this way.\nAfter node v has received the messages from all the neighbours, the marginal Bel.j.v for v is given by\n(3.2)\nBecause all the variables are included in the Markov tree, as\nFig 3.1 illustrated, we can simultaneously compute the marginals of the global belief function for every variables in the belief function network. The whole propagation process is shown in Fig 3 .2. For more detail about this propagation scheme, see (Saffiotti 1989, Shafer et al. 1987, Shafer and Shenoy 1988, Shenoy and Shafer 1986).\n..\ne,\ufffd\\o 9'\u00a2\ufffd\ufffd, g 9 11 12 13 __ t \ufffd \ufffdstep3 \ufffd2 I\ufffd ,'; \ufffd \ufffd\u00b7\ufffd- 2 14\n9 \u2022 !t:f.\\., \ufffd ofj\\ Hlta _,,,c .\u2022 \ufffd\ufffd rfbo'&b \u2022\u2022\u2022\u2022u 8 9 11 12 13 8 9 11 12 13 8 9 11 12 13 step4 step5 step6 on the paths \u2022. :\ufffdmessages are sent from nodes \ufffdnodes 8 Fig3.2: the message-passing scheme for simultaneous belief\nfunction propagation\nAccording to the scheme described above, a typical message passing situation can be illustrated as in Fig 3.3. A straightforward way for computing the messages between the nodes and the marginals for the nodes is as follows.\n\ufffd- I ,;\ufffd\ufffd -! ...\nFig3.3: a typical message-passing situation\nExample 3 1 \u00b7 Suppose nodes d, e and f have received messages from all their respective neighbours except node g. Suppose now we want to compute the marginals for d, e and f. According to (3.1), we first compute all Mi\ufffdg. ie { d, e, f)\nMi\ufffdg=((BeliGl {e:JMZ\ufffdilzeNi-{g) )).l.(ing))fg (3.3)\nNow g has received the messages by d, e and f, so it can send message to h. Again using (3.1), we have:\nLater, after g has received the message from h, it can send messages back to d, e and f, and the marginals for d, e and f will be computed. Using (3.2), we have\nBel.j.f=Belfel(GlMz\ufffdflzeNf-(g) )$ ((BelgGlMd\ufffdgt$Me\ufffdgt$Mh\ufffdg).j.(gnf)) tf (3.5)\n428 Xu\nBel.l.e=BeJe(B{EflMz\ufffdlze Ne-{g) )Efl ((Belg(l)Md-->g(l)Mf-->gEflMh-->g).l.(gne)) te (3.6)\nBc!.l.d:BeJct(B( (BMZ--KiizeNct-{g) )Efl ((BelgEflMe--)g(l)Mf--)gEf)Mh--)g)J.(gnd)) td (3.7)\nSome repeated computations are found here. e.g. Bel fEEl { Efl MZ-)flze Nf-{g)) is computed four times in (3.3),(3.5), (3.6) and (3.7). Belg(l)Mf--)gEf)Mh--)g is computed twice in (3.6) and (3.7). The solution will be discussed in the next section.\n4 A More Efficient Implementation\nIt is well known that the computation of Dempster's combination involves the most computational expense during the whole propagation process. So the redundant combinations during the propagation should be avoided as much as possible. In this section, we present an algorithm for belief function propagation using local computation which avoids those repeated computations described above.\nIn our implementation, we assume that once we have chosen a Markov tree representative G = ('JT1,,e), G will not change unless the belief function network which it represents is changed. We still make use of the notations defined in the preceding section. We choose one node of G, say vr. to be the root of the tree, thus the edges in G can be seen as direct edges: we say that an edge (v, vi) in G is directed from v to Vi whenever node Vi is on the path between node v and vr. In other words, we can defme the parent-children relationship for each node v: let Chv={ck I CkENv. v is on the path between Ck and vr) be the children of v and 1'v be the parent of v if 1've N v and 1'v is on the path between v and vr. We also assume that there is an order (arbitrary but fixed) for the elements in Chv. and let Ch'v denote the same set as Chv but with reverse order. For each Ck e Chv. we let Lsbck={ Cj I CjE Chv\u2022 j<k) denote the left hand siblings of Ck, and :R.sbck={ Cj I CjE Chv\u2022 j>k) denote the right hand siblings of Ck. Furthermore, to remove redundant computations, we associate three intermediate variables Cu.rv, '\\.ntmv and Rv with each node v. Suppose that for a given node v, it is Chv=(q, c2 .... , cml\u00b7 Then we give below the formulas for computing these intermediate variables:\n(4.1)\n(4.2)\nThen we compute the marginal for Ci, one of the children of v, as follows:\nBel.l.ci:eu.rc;Efl(('\\.ntfncj(l)Rci(vnci)) tc; =Belc;Efl { EflMZ--)Cilze Chc;)Efl\n((Belv9 { EflMCk-->VICkE Lsbc; )EflM1'v-->V Ef) { EflMCk--)VICkE :R.sbc;))J.(vnci)) tc ;\n= Belc;Efl{ 9Mz--)Cilze Chc;) Efl((Belv(B { Efl\ufffdk--)VICkE Nv-{ c;)) )J.(vnc ;)) tc; =Belc;Efl { EflMZ--)Cilze Chc;) Efl{ Mz--)Cilz=P c;l =Belc;Efl { EflMVj--)Cilvje N Ci)\nwhich is what (3.2) requires to be the case. For the root vr. because Chvr = N vro so, when node vr receives the messages from all of its children, the marginal for vr can be computed immediately, i.e. BelJ.vr = Cu.rvr .\nFrom the analysis in Example 4.1, we will see how the number of applications of Dempster's combination can be reduced to the least. The typical message-passing situation shown in Fig 3.3 above is now as in Fig 4.1. The arrows in the edges are the directions of edges, and two more storages, Cu.q and '\\.ntmi. are required at each node. Because Ri is used just once, we do not store it at each node, but regard it as a temporary variable.\nFig4.1: a typical message-passing situation\nExample 4.1: In this example, we consider the same situation as in Example 3.1. Because nodes d, e and f have received all the messages from their children, Cu.r ct. Cu.r e and Cu.rf can be computed according to (4.1). Then the messages Mi--)g, i = d, e, f, can be computed as follows:\nNow g can receive the messages from its children d, e and f in sequence:\n'\\.ntmct = Belg '\\.ntme = '\\.ntmct Efl Md--)g '\\.ntmf = '\\.ntme Efl Me--)g Cu.r g = '\\.ntmf 9 Mf -)g\nAn Efficient Implementation of Belief Function Propagation 429\nMg\ufffdh = ((Cu.r g)J.(gnh)) th\nAfter h has sent the message back to g, g can send the messages back to its children f, e and d, and the marginals for d, e and f can be computed.\nRr=Mh\ufffdg Mg\ufffdf = (('Lntmr Ell Rf )J.(gnf))tf Be!J.f = Cu.rf E!l Mg\ufffdf Re = Rf Ell Mf\ufffdg Mg\ufffde = (('Lntme Ell Re )J.(gne)) te\nBe1J.e =Cure Ell Mg\ufffde Rd = Re Ell Me\ufffdg Mg\ufffd = ((tntmd Ell \ufffd )J.(gnd)) td Bel.J.d =Curd Ell Mg\ufffd\nTable4.1 Comparison of the number of combinations used by our approach and the straightforward one\nto compute Example 3.1 Example 4.1 comparison (1) (2) (1) (2) '/ \ufffd/ (3) Mg -lh ' (3)\n(5)\ufffd/ (4) same $/ (4) '/ (6)y \u2022\nMg -->f (1) (2)\n'\\/ (3) (6) (l) using (6) <\ufffdY\ninstead of and v (l) re-com:flutinJ BeJU\n(8\ufffd ')( \u2022 (1)E!l( )Ell() \u2022\nMg ->e (IV) (l) (4) (l) \ufffd)\nv (\ufffd) J>\nusing (5)\n(9)v instead of\nand re-com:fluting Be!J.e y (\ufffd_,. (1)E!l ( ) \u2022 Mg -..1 (l) j> (10) (3) using (10)\n\"\"\ufffd\nv (1) instead of and ' (1)\no\ufffdy re-comiluting\nBeJ.!.d (\ufffd/ (7)E!l ( ) \u2022 \u2022\nThe numbers correspond to: (1): Belg. (2): Md\ufffdg. (3): Me\ufffd g. (4): Mf\ufffdg. (5): tntm.e. (6): tntmf, (7): Mh\ufffdg, (8): Belf(fl(EJlMz\ufffdflze:Nf\u00b7(g)). (8'): Cu.rr. (9): BeleE!l(EJlMz\ufffdlze:Ne\u00b7(g)), (9'): Cu.re. (10): Re. (11): BeldEll(EJlMz-Milze:Nd-(g)), (11'): Cur d\u2022\nThe following algorithm implements the simultaneous belief functions propagation according to the scheme described above. Note that Chv and Pv used here always refer to the children and the parent of node v in the same chosen G. In the algorithm, we use \"nil\" to represent the vacuous belief function. Before propagation, Cu.r v is initiated to Belv. The process begins by calling Propagate ('11\\., t).\nif 'Jil.' ;t 0 then L := L('Jil. ', e\u00b7)\nPropagate ('Jil.', e\u00b7)\nI* get the leaves of G' = ('Jil. \u00b7,e') *I if 'Jil.' = (r) then L := 'Jil.' end-if\nI* r is the root of G. *I for i e L I* for every v in L, do the followings: * 1\nfor k e Chi I* receive the messages from all its children: *I 'Lntmk := Cuq I* store intermediate result at k*l Mk\ufffdi := ((Curk)J.(kni))ti\nI* compute message from k to i *I Curi := Curi Ell Mk\ufffdi !* combine all the messages from children *I\n430 Xu\nend-for end-for Propagate('JTI.'-L, t'-((i,j) I iEL})\n/* delete the leaves, continue propagation in the remaining tree *I\nfor i E L /* for every v in L, do the following: *I if Pi exists then R := MPi---7i\n/* get the message from the parent*/ else R := nil /* only root has not parent. */ Bel.l.i := Cuq\n/* marginal for the root node is computed*/ end-if Q := nil for k E Ch'i\n/* send messages back to every child and compute the marginals for them:* I\nR := R Gl Q !* combine some messages from i's neighbours */\nMHk :=((1.nttnkGlR)Hink)) tk (4.3) !* compute message from i to k by using intermediate\nresults*/ Bel.l.k := Curk Gl Mi-?k /* marginal for k is computed*/ Q := Mj-?i\n/* get message from k to i for further computation */ end-for\nend-for end-if\nAlgorithm 1 belief function propagation\nWe can distinguish two parts in the Algorithm 1, separated by the recursive call. In the first part, each node receives the messages from all of its children, and combines them with its own belief function, because the messages are sent starting from the leaf nodes until the root of G is reached, we call this part \"propagation-up\". In the second part, after each node has received the message from its parent, it sends messages back to its children and computes the marginals for them; as the messages are sent back from the root until the leaves of G are reached, we call this part \"propagation-down\". Because the leaves of G have no children to receive messages from, the entire propagation can be invoked by calling. Propagate('lll. -L('IIl. , e ),e -{(v,vi)l vE L('JTI. ,e)}).\nFrom Algorithm 1, we find that the number of applications of Dempster's combination at each node is related to the number of its children. Let lSI denote the size of the set S. Generally, in \"propagation-up\", there are IChvl combinations at each non-leaf node v; In \"propagation-down\", there are 21Chvrl-1 combinations at the root vr. because vr has no parent and 31Chvl- l combinations at node v which is neither leaf nor root, thus there are altogether 1Chvl+(2+1 (Pv }I)* IChvl-1 combinations at non -leaf node v. No combination is needed at leave nodes. In Example 3.1, there are IChvl*( l +\nIN'vi+L(IChCjiiCjEChy, i=l, ... , m , m=IChyl}) combinations at each node.\n5 UPDATING MESSAGES\nSuppose we have already computed the marginals for all the nodes and the Markov tree G is still the same, and we now want to change one or more of the prior belief functions for some reason. Because some of the previous computed intermediate information are stored at each nodes, we can update the marginals for all the nodes without redoing all the work during re-propagation.\nFor the sake of simplicity, in this section, we use number i to refer to node Vi in the Markov tree, as shown in Fig. 5.1. Node 1 is by convention the root of the tree.\nFig 5.1 a Markov tree representation for a belief function network\nSuppose we change one prior belief function, say Bel12\u00b7 According to (3.1), the generic message Mi-?j depends on Beli and on Mk---7 i (kEN i\u00b7 ko'j), thus all the messages Mi-t :Pi (if Pi exists) from any i on the path between node 12 and 1, including node!, will be discarded; Moreover, all the messages MP j-?j (if P j exists) for all j not lying on the path between 12 and 1, will be discarded as well. The remaining half of the messages can be retained. Now suppose we need to compute the marginals for all nodes again. If we have stored all the previous messages, then only the changed messages should be recomputed, while the unchanged ones can be retained.\nAs an illustration, let's now focus on the computation of M5---71, a message which has been discarded by the change in BelJ2\u00b7 If there were no 1.nttn 10 stored at node 10, we would have to compute M5---7l as follows:\nCurs = Bels Curs = Curs Gl M6---75 Gl M7---75 Gl Ml 0---75 MS---71 = ((Curs).l.(5nl))tl\ni.e. three combinations are needed here.\nAn Efficient Implementation of Belief Function Propagation 431\nBy using the stored 'Lntm.10, we compute MS\ufffd1 as:\nCurs = Bels Curs='Lntm.lQE9M10\ufffdS ;as 'Lntm.10 is not changed. MS\ufffd1=((Curs).!.(Sn1))f 1\ni.e. just one combination is needed here.\nFormally, suppose that one input Beli is changed. According to (3.1), all the messages Mk\ufffdPk (iU'k exists) from any k on the path between node i and r (the root), including i, will be discarded; Moreover, all the messages MP j\ufffd j (if P j exists) for all j not lying on the path between i and r, will be discarded as well. According to (4.1), Curk of node k stores the combination of all the messages sent from the children of k with its own belief function. So only Curk of k lying on the path between i and r, including i and r, will be discarded. According to (4.2), 'Lntm.j depends on Belpj and M k\ufffdPj (ke Lsb j). so for any k on the path between i and r, including i, if je :R.sbk, then 'Lntm.j will be discarded. This point is illustrated in Fig. S.2 by showing some cases when one prior belief function is changed.\n@) input is changed @ 1-ntmi is discarded Cu.r;. M ->Pi of the vertices on the path/ are discarded M\ufffd-> i of the vertices on the other paths are discarded\nFigS .2: cases for the changes of the messages\nSuppose that for the node i, Cur i is not changed, i.e. Beli and all the Mk \ufffd i (keCh i) are not changed. As a consequence, all 'Lntm.k (keChi) are not changed. So whenever Cuq is unchanged, we can skip the \"propagation up'' part for i during re-propagation. e.g. We want to compute Bel.l.14 now. Because M14\ufffd1 does not change, it is desirable not to re-compute Bel14$M1S\ufffd14E!)M16\ufffd14 for computing Bel.l-14. By using Cuq4, we just avoid this computation.\nSynthesizing all the cases discussed in these two sections, we give a combined algorithm for both simultaneous propagation and re-propagation. This algorithm is based on the assumption that the structure of the Markov tree is not changed when re-propagating and that Mi\ufffdj. 'Lntm.i, Cuq are only discarded when necessary as explained before. In this\nAlgorithm, we will use Bel.!.i temporarily to compute Cuq in the propagation-up part. Initially, for each node i, Bel.l.i is initiated to Beli and for the first child CJ of i, 'Lntm.q is initiated to Beli. Then the propagation can begin by calling Propagate('Jll. -L('Jll. ,e), e-{(iJ)Iie L('Jll. ,e)}).\nPropagate ( 'Jn,'' e ') if 'Jlt'\"' 0 then\nL := L('Jlt ', e\u00b7) !*get the leaves of G' = ('111. ',e') *I if '111.'= (r) then L := '111.' end-if\n/* r is the root of G. *I for i e L /* for every v in L, do the followings: *I\nif Cuq does not exist then /* if Cuq exists, the first part is skipped for i */\nCh :=Chi /* otherwise: *I for j e Chi\n!* find the first child i whose message is discarded *I if W\ufffdi exists then Ch := Ch - (j}\nend-if end-for\nelse Bel.l.i := 'Lntm.j exit loop-for\nif Ch=0 then Ch :=Chi end-if !* if all the messages from the children is not changed,\nas Curi is discarded, Beli is changed. *I for k e Ch\n/* compute the messages from the rest of children. *I 'Lntm.k:=Bel.l.i /*store intermediate result at k*/ if Mk\ufffdi does not exist then Mk\ufffdi:=((Curk).l.(ink)) ti end-if /* compute message from k to i if necessary *I Bel.l.i := Bel.!.i E9 Mk\ufffdi /* combine all the messages from children */\nend-for Curi := Bel.!.i\n/* store the combination of all the messages from children of i with the prior belief function of i at node i *I\nend-if end-for Propagate('Jlt '-L, e\u00b7-( (i, j) I ie L))\n/* delete the leaves, continue propagation in the remaining tree *I\nfor i E L('Jlt', e\u00b7) !* for every v in L, do the following: *I\nif Pi exists then R := Mpi\ufffdi /* get the message from the parent *I\nelse R :=nil end-if Q :=nil for k e Ch'i\n/* send messages back to every child and compute the marginals for them:*/\nR := R (!) Q /*combine some messages from i's neighbours */\n432 Xu\nif Mi\ufffdk does not exist then Mi\ufffdk:= (('Lntt11-kEBR)J.(ink)) fk end-if\n/* compute message from i to k by using intermediate results*/\nBel.J.k := Cu.rk Ee Mi\ufffdk /*marginal for k is computed*/\nQ :=Mk\ufffdi /* get message from k to i for further computation *I\nend-for end-for\nend-if\nAlgorithm2 combined algorithm for simultaneous propagation and re-propagation\n6 CONCLUSIONS\nWe have presented an algorithm (Algorithm 1) for belief function propagation based on the local computation technique proposed by Shafer, Shenoy and Mellouli. The advantage of Algorithm 1 is that it decreases the number of applications of Dempster's combination rule during the propagation, thus reducing the overall complexity, which is one of the most serious problems in implementing Dempster-Shafer theory. Moreover, Algorithm 2 makes full use of the already computed messages and intermediate results for re-propagating when one or more of the prior belief functions is changed. A propagation system* bas been implemented in Allegro Common Lisp with Common Windows (by Franz Inc) according to Algorithm 2. It runs on a SUN-3/60 Workstation under SUN Operating System 4.0.3. It has shown that the speed of computation can be greatly increased in comparison with an existing implementation such as MacEvidence (Hsia and Shenoy 1989). Because of the generality of the local computation technique (Saffiotti 1989, Shenoy 1989), this approach is not just specific to belief function propagation, but may be used for any case in which the local computation technique may be applied.\nAcknowledgements\nI am grateful for the support, help and suggestions from Professor Philippe Smets. For the useful discussions and worthy comments on the drafts, I greatly thank Alessandro Saffiotti, Yen-Teh Hsia, Robert Kennes and Elisabeth Umkehrer .This work has been supported by the grant of IRIDIA, Universite Libre de Bruxelles.\n*The implementation is a property of IRIDIA-Universite Libre de Bruxelles, and it is freely available for strictly non commercial use.\nReferences\nHsia Y. and Shenoy P. P. (1989) \"MacEvidence: A Visual Environment for Constructing & Evaluating Evidential Systems\" Working Paper No. 211, School of Business, University of Kansas, Lawrence, KS.\nKong A. (1986) \"Multivariate Belief Functions & Graphical Models\" Ph.D dissertation, Department of Statistics, Harvard University, Cambridge, MA.\nMellouli K. (1987) \"On the Propagation of Beliefs in"}, {"heading": "Network Using the Dempster-Shaler Theory of", "text": "Evidence\" Ph.D dissertation, School of Business, University of Kansas, Lawrence, KS.\nSaffiotti A. (1989) \"De Propagationibus\" Technical Note IRIDINARCHON/TN-005, IRIDIA, Universite Libre de Bruxelles, Brussels.\nShafer G. (1976) \"A Mathematical Theory of Evidence\" Princeton University Press.\nShafer G., Shenoy P. P. and Mellouli K. (1987) \"Propagating Belief Functions in Qualitative Markov Trees\" International Journal of Approximate Reasoning, 1:349-400.\nShafer G.and Shenoy P. P. (1988) \"Local Computation in Hypertrees\" Working Paper No. 201, School of Business, University of Kansas, Lawrence, KS.\nShenoy P. P. and Shafer G. (1986) \"Propagating Belief Functions with Local Computations\" IEEE Expert, 1(3), 43-52.\nShenoy P. P. (1989) \"A Valuation-Based Language for Expert Systems\" International Journal of Approximate Reasoning, 3:383-411.\nSmets Ph. (1988) \"Belief Functions\" in \"Non Standard Logics for Automated Reasoning\" edited by Smets Ph., Mamdani A., Dubois D. and Prade H., Academic Press, London, 253-286.\nZarley D. (1988) \"An Evidential Reasoning System\" Working Paper No. 206, School of Business, University of Kansas, Lawrence, KS.\nZarley D., Hsia Y. and Shafer G. (1988) \"Evidential Reasoning Using DELIEF\" in \"Proceedings of the seventh National Conference on Artificial Intelligence\" edited by Paul St., MN, 1, 205-209.\nZhang L. (1988) \"Studies on Finding Hypertree Covers of Hypergraphs\" Working Paper No. 198, School of Business, University of Kansas, Lawrence, KS."}], "references": [{"title": "MacEvidence: A Visual Environment for Constructing & Evaluating Evidential Systems", "author": ["Y. Hsia", "P. Shenoy P"], "venue": "Working Paper No. 211, School of Business,", "citeRegEx": "Hsia and P.,? \\Q1989\\E", "shortCiteRegEx": "Hsia and P.", "year": 1989}, {"title": "Multivariate Belief Functions & Graphical Models", "author": ["A. Kong"], "venue": "Ph.D dissertation,", "citeRegEx": "Kong,? \\Q1986\\E", "shortCiteRegEx": "Kong", "year": 1986}, {"title": "On the Propagation of Beliefs in Network Using the Dempster-Shaler Theory of Evidence\" Ph.D dissertation, School of Business, University of Kansas, Lawrence, KS", "author": ["K. Mellouli"], "venue": null, "citeRegEx": "Mellouli,? \\Q1987\\E", "shortCiteRegEx": "Mellouli", "year": 1987}, {"title": "De Propagationibus\" Technical Note IRIDINARCHON/TN-005, IRIDIA, Universite Libre de Bruxelles, Brussels", "author": ["A. Saffiotti"], "venue": null, "citeRegEx": "Saffiotti,? \\Q1989\\E", "shortCiteRegEx": "Saffiotti", "year": 1989}, {"title": "A Mathematical Theory of Evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "Shafer,? \\Q1976\\E", "shortCiteRegEx": "Shafer", "year": 1976}, {"title": "Propagating Belief Functions in Qualitative Markov Trees", "author": ["G. Shafer", "P. Shenoy P", "K. Mellouli"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Shafer et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Shafer et al\\.", "year": 1987}, {"title": "Local Computation in Hypertrees", "author": ["G. Shafer", "P. Shenoy P"], "venue": "Working Paper No. 201, School of Business,", "citeRegEx": "Shafer and P.,? \\Q1988\\E", "shortCiteRegEx": "Shafer and P.", "year": 1988}, {"title": "Propagating Belief Functions with Local Computations", "author": ["P. Shenoy P", "G. Shafer"], "venue": "IEEE Expert,", "citeRegEx": "P. and Shafer,? \\Q1986\\E", "shortCiteRegEx": "P. and Shafer", "year": 1986}, {"title": "A Valuation-Based Language for Expert Systems", "author": ["P. Shenoy P"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "P.,? \\Q1989\\E", "shortCiteRegEx": "P.", "year": 1989}, {"title": "Belief Functions\" in \"Non Standard Logics for Automated Reasoning\" edited by Smets Ph", "author": ["Smets Ph"], "venue": null, "citeRegEx": "Ph.,? \\Q1988\\E", "shortCiteRegEx": "Ph.", "year": 1988}, {"title": "An Evidential Reasoning System", "author": ["D. Zarley"], "venue": "Working Paper No. 206, School of Business,", "citeRegEx": "Zarley,? \\Q1988\\E", "shortCiteRegEx": "Zarley", "year": 1988}, {"title": "Studies on Finding Hypertree Covers of Hypergraphs", "author": ["L. Zhang"], "venue": "Working Paper No. 198, School of Business,", "citeRegEx": "Zhang,? \\Q1988\\E", "shortCiteRegEx": "Zhang", "year": 1988}], "referenceMentions": [{"referenceID": 1, "context": "1988) based on the observation that an arbitrary belief function network can be represented as a hypergraph (Kong 1986), which can also be embedded in so\u00ad called a Markov tree (Zhang 1988).", "startOffset": 108, "endOffset": 119}, {"referenceID": 11, "context": "1988) based on the observation that an arbitrary belief function network can be represented as a hypergraph (Kong 1986), which can also be embedded in so\u00ad called a Markov tree (Zhang 1988).", "startOffset": 176, "endOffset": 188}, {"referenceID": 5, "context": "In the rest of this section, we discuss Shafer, Shenoy and Mellouli's propagation scheme using local computation (Shafer et al. 1987).", "startOffset": 113, "endOffset": 133}], "year": 2011, "abstractText": "The local computation technique (Shafer et a!. 1987, Shafer and Shenoy 1988, Shenoy and Shafer 1986) is used for propagating belief functions in so\u00ad called a Markov Tree. In this paper, we describe an efficient implementation of belief function propagation on the basis of the local computation technique. The presented method avoids all the redundant computations in the propagation process, and so makes the computational complexity decrease with respect to other existing implementations (Hsia and Shenoy 1989, Zarley et a!. 1988). We also give a combined algorithm for both propagation and re-propagation which makes the re-propagation process more efficient when one or more of the prior belief functions is changed.", "creator": "pdftk 1.41 - www.pdftk.com"}}}