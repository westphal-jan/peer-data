{"id": "1305.3120", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2013", "title": "Optimization with First-Order Surrogate Functions", "abstract": "in this subsequent paper, we consequently study structural optimization methods consisting respectively of successively iteratively minimizing surrogates expressions of an objective function. namely by proposing out several algorithmic variants and simple convergence index analyses, consequently we make two main contributions. first, we cannot provide a unified viewpoint for several first - order partial optimization techniques such as linear accelerated proximal correlation gradient, vector block coordinate descent, elimination or direct frank - wolfe expansion algorithms. second, whereas we subsequently introduce a simple new standard incremental analysis scheme that experimentally means matches matched or outperforms state - up of - living the - art puzzle solvers purely for large - time scale competitive optimization problems typically incorrectly arising in machine class learning.", "histories": [["v1", "Tue, 14 May 2013 11:49:34 GMT  (152kb)", "http://arxiv.org/abs/1305.3120v1", "to appear in the proceedings of ICML 2013; the arxiv paper contains the 9 pages main text followed by 26 pages of supplemental material. International Conference on Machine Learning (ICML 2013) (2013)"]], "COMMENTS": "to appear in the proceedings of ICML 2013; the arxiv paper contains the 9 pages main text followed by 26 pages of supplemental material. International Conference on Machine Learning (ICML 2013) (2013)", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["julien mairal"], "accepted": true, "id": "1305.3120"}, "pdf": {"name": "1305.3120.pdf", "metadata": {"source": "META", "title": "Optimization with First-Order Surrogate Functions", "authors": ["Julien Mairal"], "emails": ["julien.mairal@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 5.\n31 20\nv1 [\nst at\n.M L\n] 1\n4 M\nay 2\n01 3"}, {"heading": "1. Introduction", "text": "The principle of iteratively minimizing a majorizing surrogate of an objective function is often called majorization-minimization (Lange et al., 2000). Each iteration drives the objective function downhill, thus giving the hope of finding a local optimum. A large number of existing procedures can be interpreted from this point of view. This is for instance the case of gradient-based or proximal methods (see Nesterov, 2007; Beck & Teboulle, 2009; Wright et al., 2009), EM algorithms (see Neal & Hinton, 1998), DC programming (Horst & Thoai, 1999), boosting (Collins et al., 2002; Della Pietra et al., 2001), and some variational Bayes techniques (Wainwright & Jordan, 2008; Seeger & Wipf, 2010). The concept of \u201csurrogate\u201d has also been used successfully in the signal processing literature about sparse optimization (Daubechies et al., 2004; Gasso et al., 2009) and matrix factorization (Lee & Seung, 2001; Mairal et al., 2010).\nIn this paper, we are interested in generalizing the majorization-minimization principle. Our goal is both to discover new algorithms, and to draw connections\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nwith existing methods. We focus our study on \u201cfirstorder surrogate functions\u201d, which consist of approximating a possibly non-smooth objective function up to a smooth error. We present several schemes exploiting such surrogates, and analyze their convergence properties: asymptotic stationary point conditions for non-convex problems, and convergence rates for convex ones. More precisely, we successively study:\n\u2022 a generic majorization-minimization approach; \u2022 a randomized block coordinate descent algorithm (see Tseng & Yun, 2009; Shalev-Shwartz & Tewari, 2009; Nesterov, 2012; Richta\u0301rik & Taka\u0301c\u030c, 2012);\n\u2022 an accelerated variant for convex problems inspired by Nesterov (2004); Beck & Teboulle (2009);\n\u2022 a generalization of the \u201cFrank-Wolfe\u201d conditional gradient method (see Zhang, 2003; Harchaoui et al., 2013; Hazan & Kale, 2012; Zhang et al., 2012);\n\u2022 a new incremental scheme, which we call MISO.1\nWe present in this work a unified view for analyzing a large family of algorithms with simple convergence proofs and strong guarantees. In particular, all the above optimization methods except Frank-Wolfe have linear convergence rates for minimizing strongly convex objective functions. This is remarkable for MISO, the new incremental scheme derived from our framework; to the best of our knowledge, only two recent incremental algorithms share such a property: the stochastic average gradient method (SAG) of Le Roux et al. (2012), and the stochastic dual coordinate ascent method (SDCA) of Shalev-Schwartz & Zhang (2012). Our scheme MISO is inspired in part by these two works, but yields different update rules than SAG or SDCA.\nAfter we present and analyze the different optimization schemes, we conclude the paper with numerical experiments focusing on the scheme MISO. We show that in most cases MISO matches or outperforms cutting-edge solvers for large-scale \u21132- and \u21131-regularized logistic regression (Bradley et al., 2011; Beck & Teboulle, 2009; Le Roux et al., 2012; Fan et al., 2008; Bottou, 2010).\n1Minimization by Incremental Surrogate Optimization."}, {"heading": "2. Basic Optimization Scheme", "text": "Given a convex subset \u0398 of Rp and a continuous function f : Rp \u2192 R, we are interested in solving\nmin \u03b8\u2208\u0398 f(\u03b8),\nwhere we assume, to simplify, that f is bounded below. Our goal is to study the majorization-minimization scheme presented in Algorithm 1 and its variants. This procedure relies on the concept of surrogate functions, which are minimized instead of f at every iteration.2\nAlgorithm 1 Basic Scheme input \u03b80 \u2208 \u0398; N (number of iterations). 1: for n = 1, . . . , N do 2: Compute a surrogate function gn of f near \u03b8n\u22121; 3: Update solution: \u03b8n \u2208 argmin\u03b8\u2208\u0398 gn(\u03b8). 4: end for output \u03b8N (final estimate);\nFor this approach to be successful, we intuitively need surrogates that approximate well the objective f and that are easy to minimize. In this paper, we focus on \u201cfirst-order surrogate functions\u201d defined below, which will be shown to have \u201cgood\u201d theoretical properties.\nDefinition 2.1 (First-Order Surrogate). A function g : Rp \u2192 R is a first-order surrogate of f near \u03ba in \u0398 when the following conditions are satisfied:\n\u2022 Majorization: we have g(\u03b8\u2032) \u2265 f(\u03b8\u2032) for all \u03b8\u2032 in argmin\u03b8\u2208\u0398 g(\u03b8). When the more general condition g \u2265 f holds, we say that g is a majorant function; \u2022 Smoothness: the approximation error h , g \u2212 f is differentiable, and its gradient is L-Lipschitz continuous. Moreover, we have h(\u03ba) = 0 and \u2207h(\u03ba) = 0. We denote by SL(f, \u03ba) the set of such surrogates, and by SL,\u03c1(f, \u03ba) the subset of \u03c1-strongly convex surrogates.\nFirst-order surrogates have a few simple properties, which form the building block of our analyses:\nLemma 2.1 (Basic Properties - Key Lemma). Let g be in SL(f, \u03ba) for some \u03ba in \u0398. Define h , g\u2212f and let \u03b8\u2032 be in argmin\u03b8\u2208\u0398 g(\u03b8). Then, for all \u03b8 in \u0398,\n\u2022 |h(\u03b8)| \u2264 L2 \u2016\u03b8 \u2212 \u03ba\u201622; \u2022 f(\u03b8\u2032) \u2264 f(\u03b8) + L2 \u2016\u03b8 \u2212 \u03ba\u201622."}, {"heading": "Assume that g is in SL,\u03c1(f, \u03ba), then, for all \u03b8 in \u0398,", "text": "\u2022 f(\u03b8\u2032) + \u03c12\u2016\u03b8\u2032 \u2212 \u03b8\u201622 \u2264 f(\u03b8) + L2 \u2016\u03b8 \u2212 \u03ba\u201622. 2Note that this concept differs from the machine learning terminology, where a \u201csurrogate\u201d often denotes a fixed convex upper bound of the nonconvex (0\u22121)-loss.\nThe proof of this lemma is relatively simple but for space limitation reasons, all proofs in this paper are provided as supplemental material. With Lemma 2.1 in hand, we now study the properties of Algorithm 1."}, {"heading": "2.1. Convergence Analysis", "text": "For general non-convex problems, proving convergence to a global (or local) minimum is out of reach, and classical analyses study instead asymptotic stationary point conditions (see, e.g., Bertsekas, 1999). To do so, we make the mild assumption that for all \u03b8, \u03b8\u2032 in \u0398, the directional derivative \u2207f(\u03b8, \u03b8\u2032 \u2212 \u03b8) of f at \u03b8 in the direction \u03b8\u2032\u2212 \u03b8 exists. A classical necessary first-order condition (see Borwein & Lewis, 2006) for \u03b8 to be a local minimum of f is to have \u2207f(\u03b8, \u03b8\u2032\u2212\u03b8) non-negative for all \u03b8\u2032 in \u0398. This naturally leads us to consider the following asymptotic condition to assess the quality of a sequence (\u03b8n)n\u22650 for non-convex problems:\nDefinition 2.2 (Asymptotic Stationary Point). A sequence (\u03b8n)n\u22650 satisfies an asymptotic stationary point condition if\nlim inf n\u2192+\u221e inf \u03b8\u2208\u0398 \u2207f(\u03b8n, \u03b8 \u2212 \u03b8n) \u2016\u03b8 \u2212 \u03b8n\u20162 \u2265 0.\nIn particular, if f is differentiable on Rp and \u0398 = Rp, this condition implies limn\u2192+\u221e \u2016\u2207f(\u03b8n)\u20162 = 0.\nBuilding upon this definition, we now give a first convergence result about Algorithm 1.\nProposition 2.1 (Non-Convex Analysis). Assume that the surrogates gn from Algorithm 1 are in SL(f, \u03b8n\u22121) and are majorant or strongly convex. Then,(f(\u03b8n))n\u22650 monotonically decreases and (\u03b8n)n\u22650 satisfies an asymptotic stationary point condition.\nConvergence results for non-convex problems are by nature weak. This is not the case when f is convex. In the next proposition, we obtain convergence rates by following a proof technique from Nesterov (2007) originally designed for proximal gradient methods. Proposition 2.2 (Convex Analysis for SL(f, \u03ba)). Assume that f is convex and that for some R > 0,\n\u2016\u03b8\u2212 \u03b8\u22c6\u20162 \u2264 R for all \u03b8 \u2208 \u0398 s.t. f(\u03b8) \u2264 f(\u03b80), (1) where \u03b8\u22c6 is a minimizer of f on \u0398. When the surrogate gn in Algorithm 1 are in SL(f, \u03b8n\u22121), we have\nf(\u03b8n)\u2212 f\u22c6 \u2264 2LR2\nn+ 2 for all n \u2265 1,\nwhere f\u22c6 , f(\u03b8\u22c6). Assume now that f is \u00b5-strongly convex. Regardless of condition (1), we have\nf(\u03b8n)\u2212 f\u22c6 \u2264 \u03b2n(f(\u03b80)\u2212 f\u22c6) for all n \u2265 1, where \u03b2 , L\u00b5 if \u00b5 > 2L or \u03b2 , ( 1\u2212 \u00b54L ) otherwise.\nThe result of Proposition 2.2 is interesting in the sense that it provides sharp theoretical results without making strong assumption on the surrogate functions. The next proposition shows that slightly better rates can be obtained when the surrogates are strongly convex. Proposition 2.3 (Convex Analysis for SL,\u03c1(f, \u03ba)). Assume that f is convex and let \u03b8\u22c6 be a minimizer of f on \u0398. When the surrogates gn of Algorithm 1 are in SL,\u03c1(f, \u03b8n\u22121) with \u03c1 \u2265 L, we have for all n \u2265 1,\nf(\u03b8n)\u2212 f\u22c6 \u2264 L\u2016\u03b80 \u2212 \u03b8\u22c6\u201622\n2n .\nWhen f is \u00b5-strongly convex, we have for all n \u2265 1, \n\n\n\u2016\u03b8n \u2212 \u03b8\u22c6\u201622 \u2264 ( L \u03c1+\u00b5\n)n\n\u2016\u03b80 \u2212 \u03b8\u22c6\u201622 f(\u03b8n)\u2212 f\u22c6 \u2264 ( L \u03c1+\u00b5 )n\u22121 L\u2016\u03b80\u2212\u03b8\u22c6\u201622 2 .\nNote that the condition \u03c1 \u2265 L is relatively strong; it can indeed be shown that f is necessarily (\u03c1\u2212L)strongly convex if \u03c1>L, and convex if \u03c1=L. The fact that making stronger assumptions yields better convergence rates suggests that going beyond first-order surrogates could provide even sharper results. This is confirmed in the next proposition:\nProposition 2.4 (Second-Order Surrogates). Make similar assumptions as in Proposition 2.2, and also assume that the error functions hn , gn\u2212f are twice differentiable, that their Hessians \u22072hn are M - Lipschitz, and that \u22072hn(\u03b8n\u22121) = 0 for all n. Then,\nf(\u03b8n)\u2212 f\u22c6 \u2264 9MR3\n2(n+ 3)2 for all n \u2265 1.\nIf f is \u00b5-strongly convex, the convergence rate is superlinear with order 3/2.\nConsistently with this proposition, similar rates were obtained by Nesterov & Polyak (2006) for the Newton method with cubic regularization, which involve second-order surrogates. In the next section, we focus again on first-order surrogates, and present simple mechanisms to build them. The proofs of the different claims are provided in the supplemental material."}, {"heading": "2.2. Examples of Surrogate Functions", "text": ""}, {"heading": "Lipschitz Gradient Surrogates.", "text": "When f is differentiable and \u2207f is L-Lipschitz, f admits the following majorant surrogate in S2L,L(f, \u03ba):\ng : \u03b8 7\u2192 f(\u03ba) +\u2207f(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + L 2 \u2016\u03b8 \u2212 \u03ba\u201622.\nIn addition, when f is convex, g is in SL,L(f, \u03ba), and when f is \u00b5-strongly convex, g is in SL\u2212\u00b5,L(f, \u03ba). Note also that minimizing g amounts to performing a classical classical gradient descent step \u03b8\u2032 \u2190 \u03ba\u2212 1L\u2207f(\u03ba)."}, {"heading": "Proximal Gradient Surrogates.", "text": "Assume that f splits into f = f1 + f2, where f1 is differentiable with a L-Lipschitz gradient. Then, f admits the following majorant surrogate in S2L(f, \u03ba):\ng : \u03b8 7\u2192 f1(\u03ba) +\u2207f1(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + L\n2 \u2016\u03b8 \u2212 \u03ba\u201622 + f2(\u03b8).\nThe approximation error g \u2212 f is indeed the same as in the previous paragraph and thus:\n\u2022 when f1 is convex, g is in SL(f, \u03ba). If f2 is also convex, g is in SL,L(f, \u03ba).\n\u2022 when f1 is \u00b5-strongly convex, g is in SL\u2212\u00b5(f, \u03ba). If f2 is also convex, g is in SL\u2212\u00b5,L(f, \u03ba).\nMinimizing g amounts to performing a proximal gradient step (see Nesterov, 2007; Beck & Teboulle, 2009)."}, {"heading": "DC Programming Surrogates.", "text": "Assume that f = f1 + f2, where f2 is concave and differentiable with a L2-Lipschitz gradient. Then, the following function g is a majorant surrogate in SL2(f, \u03ba):\ng : \u03b8 7\u2192 f1(\u03b8) + f2(\u03ba) +\u2207f2(\u03ba)\u22a4(\u03b8 \u2212 \u03ba).\nSuch a surrogate forms the root of DC- (difference of convex functions)-programming (see Horst & Thoai, 1999). It is also indirectly used in reweighted-\u21131 algorithms (Cande\u0300s et al., 2008) for minimizing on Rp+ a cost function of the form \u03b8 7\u2192 f1(\u03b8)+\u03bb \u2211p i=1 log(\u03b8i+\u03b5)."}, {"heading": "Variational Surrogates.", "text": "Let f be a real-valued function defined on Rp1 \u00d7 Rp2 . Let \u03981 \u2286 Rp1 and \u03982 \u2286 Rp2 be two convex sets. Define f\u0303 as f\u0303(\u03b81) , min\u03b82\u2208\u03982 f(\u03b81, \u03b82) and assume that\n\u2022 \u03b81 7\u2192 f(\u03b81, \u03b82) is differentiable for all \u03b82 in \u03982; \u2022 \u03b82 7\u2192 \u22071f(\u03b81, \u03b82) is L-Lipschitz for all \u03b81 in Rp1 ;3 \u2022 \u03b81 7\u2192 \u22071f(\u03b81, \u03b82) is L\u2032-Lipschitz for all \u03b82 in \u03982; \u2022 \u03b82 7\u2192 f(\u03b81, \u03b82) is \u00b5-strongly convex for all \u03b81 in Rp1 .\nLet us fix \u03ba1 in \u03981. Then, the following function is a majorant surrogate in S2L\u2032\u2032(f\u0303 , \u03ba) for some L\u2032\u2032 > 0:\ng : \u03b81 7\u2192 f(\u03b81, \u03ba\u22c62) with \u03ba\u22c62 , argmin \u03b82\u2208\u03982 f\u0303(\u03ba1, \u03b82).\nWhen f is jointly convex in \u03b81 and \u03b82, f\u0303 is itself convex and we can choose L\u2032\u2032 = L\u2032. Algorithm 1 becomes a block-coordinate descent procedure with two blocks."}, {"heading": "Saddle Point Surrogates.", "text": "Let us make the same assumptions as in the previous paragraph but with the following differences:\n3The notation \u22071 denotes the gradient w.r.t. \u03b81.\n\u2022 \u03b82 7\u2192f(\u03b81, \u03b82) is \u00b5-strongly concave for all \u03b81 in Rp1 ; \u2022 \u03b81 7\u2192f(\u03b81, \u03b82) is convex for all \u03b82 in \u03982; \u2022 f\u0303(\u03b81) , max\u03b82\u2208\u03982 f(\u03b81, \u03b82).\nThen, f\u0303 is convex and the function below is a majorant surrogate in S2L\u2032\u2032(f\u0303 , \u03ba1):\ng : \u03b81 7\u2192 f(\u03b81, \u03ba\u22c62) + L\u2032\u2032\n2 \u2016\u03b81 \u2212 \u03ba1\u201622,\nwhere L\u2032\u2032 , max(2L2/\u00b5, L\u2032). When \u03b81 7\u2192 f(\u03b81, \u03b82) is affine, we can instead choose L\u2032\u2032 , L2/\u00b5."}, {"heading": "Jensen Surrogates.", "text": "Jensen\u2019s inequality provides a natural mechanism to obtain surrogates for convex functions. Following the presentation of Lange et al. (2000), we consider a convex function f : R 7\u2192 R, a vector x in Rp, and define f\u0303 : Rp \u2192 R as f\u0303(\u03b8) , f(x\u22a4\u03b8) for all \u03b8. Let w be a weight vector in Rp+ such that \u2016w\u20161 = 1 and wi 6= 0 whenever xi 6=0. Then, we define for any \u03ba in Rp\ng : \u03b8 7\u2192 p \u2211\ni=1\nwif\n(\nxi wi (\u03b8i \u2212 \u03bai) + x\u22a4\u03ba ) ,\nWhen f is differentiable with an L-Lipschitz gradient, and wi , |xi|\u03bd/\u2016x\u2016\u03bd\u03bd, then g is in SL\u2032(f\u0303 , \u03ba) with\n\u2022 L\u2032 = L\u2016x\u20162\u221e\u2016x\u20160 for \u03bd = 0; \u2022 L\u2032 = L\u2016x\u2016\u221e\u2016x\u20161 for \u03bd = 1; \u2022 L\u2032 = L\u2016x\u201622 for \u03bd = 2.\nAs far as we know, the convergence rates we provide when using such surrogates are new. We also note that Jensen surrogates have been successfully used in machine learning. For instance, Della Pietra et al. (2001) interpret boosting procedures under this point of view through the concept of auxiliary functions."}, {"heading": "Quadratic Surrogates.", "text": "When f is twice differentiable and admits a matrix H such that H\u2212\u22072f is always positive definite, the following function is a first-order majorant surrogate:\ng : \u03b8 7\u2192 f(\u03ba) +\u2207f(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + 1 2 (\u03b8 \u2212 \u03ba)\u22a4H(\u03b8 \u2212 \u03ba).\nThe Lipschitz constant of \u2207(g\u2212f) is the largest eigenvalue of H \u2212 \u22072f(\u03b8) over \u0398. Such surrogates appear frequently in the statistics and machine learning literature (Bo\u0308hning & Lindsay, 1988; Khan et al., 2010).\nWe have shown that there are many rules to build first-order surrogates. Choosing one instead of another mainly depends on how easy it is to build the surrogate (do we need to estimate an a priori unknown Lipschitz constant?), and on how cheaply it can be minimized."}, {"heading": "3. Block Coordinate Scheme", "text": "In this section, we introduce a block coordinate descent extension of Algorithm 1 under the assumptions that\n\u2022 \u0398 is separable\u2014that is, it can be written as a Cartesian product \u0398 = \u03981 \u00d7\u03982 \u00d7 . . .\u00d7\u0398k;\n\u2022 the surrogates gn are separable into k components:\ngn(\u03b8) = k \u2211\ni=1\ngin(\u03b8 i) for \u03b8 = (\u03b81, . . . , \u03b8k) \u2208 \u0398.\nWe present a randomized procedure in Algorithm 2 following Tseng & Yun (2009); Shalev-Shwartz & Tewari (2009); Nesterov (2012); Richta\u0301rik & Taka\u0301c\u030c (2012).\nAlgorithm 2 Block Coordinate Descent Scheme\ninput \u03b80 = (\u03b8 1 0 , . . . , \u03b8 k 0 ) \u2208 \u0398 = (\u03981 \u00d7 . . .\u00d7\u0398k); N .\n1: for n = 1, . . . , N do 2: Choose a separable surrogate gn of f near \u03b8n\u22121; 3: Randomly pick up one block \u0131\u0302n and update \u03b8 \u0131\u0302n n :\n\u03b8\u0131\u0302nn \u2208 argmin \u03b8\u0131\u0302n\u2208\u0398\u0131\u0302n g \u0131\u0302nn (\u03b8 \u0131\u0302n).\n4: end for output \u03b8N = (\u03b8 1 N , . . . , \u03b8 k N ) (final estimate);\nAs before, we first study the convergence for nonconvex problems. The next proposition shows that similar guarantees as for Algorithm 1 can be obtained.\nProposition 3.1 (Non-Convex Analysis). Assume that the functions gn are majorant surrogates in SL(f, \u03b8n\u22121). Assume also that \u03b80 is the minimizer of a majorant surrogate function in SL(f, \u03b8\u22121) for some \u03b8\u22121 in \u0398. Then, the conclusions of Proposition 2.1 hold with probability one.\nUnder convexity assumptions on f , the next two propositions give us expected convergence rates. Proposition 3.2 (Convex Analysis for SL(f, \u03ba)). Make the same assumptions as in Proposition 2.2 and define \u03b4 , 1k . When the surrogate functions gn in Algorithm 2 are majorant and in SL(f, \u03b8n\u22121), the sequence (f(\u03b8n))n\u22650 almost surely converges to f\u22c6 and\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 2LR2\n2 + \u03b4(n\u2212 n0) for all n \u2265 n0,\nwhere n0 , \u2308 log ( 2(f(\u03b80)\u2212f\u22c6) LR2 \u2212 1 ) / log ( 1 1\u2212\u03b4 )\u2309 if f(\u03b80)\u2212 f\u22c6 > LR2 and n0 , 0 otherwise. Assume now that f is \u00b5-strongly convex. Then, we have instead an expected linear convergence rate\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 ((1 \u2212 \u03b4) + \u03b4\u03b2)n(f(\u03b80)\u2212 f\u22c6),\nwhere \u03b2 , L\u00b5 if \u00b5 > 2L or \u03b2 , ( 1\u2212 \u00b54L ) otherwise.\nProposition 3.3 (Convex Analysis for SL,\u03c1(f, \u03ba)). Assume that f is convex. Define \u03b4 , 1k . Choose majorant surrogates gn in SL,\u03c1(f, \u03b8n\u22121) with \u03c1 \u2265 L, then (f(\u03b8n))n\u22650 almost surely converges to f\u22c6 and we have\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 C0\n(1\u2212 \u03b4) + \u03b4n for all n \u2265 1,\nwith C0 , (1\u2212\u03b4)(f(\u03b80)\u2212f\u22c6)+ (1\u2212\u03b4)\u03c1+\u03b4L2 \u2016\u03b80\u2212\u03b8\u22c6\u201622. Assume now that f is \u00b5-strongly convex, then we have an expected linear convergence rate \n\n\nL 2 E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u201622] \u2264 C0\n( (1 \u2212 \u03b4) + \u03b4 L\u03c1+\u00b5 )n\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 C0\u03b4 ( (1 \u2212 \u03b4) + \u03b4 L\u03c1+\u00b5 )n\u22121 .\nThe quantity \u03b4= 1/k represents the probability for a block to be updated during an iteration. Note that updating all blocks (\u03b4=1) gives the same results as in Section 2. Linear convergence for strongly convex objectives with block coordinate descent is classical since the works of Tseng & Yun (2009); Nesterov (2012). Results of the same nature have also been obtained by Richta\u0301rik & Taka\u0301c\u030c (2012) for composite functions."}, {"heading": "4. Frank-Wolfe Scheme", "text": "In this section, we show how to use surrogates to generalize the Frank-Wolfe method, an old convex optimization technique that has regained some popularity in machine learning (Zhang, 2003; Harchaoui et al., 2013; Hazan & Kale, 2012; Zhang et al., 2012). We present this approach in Algorithm 3.\nAlgorithm 3 Frank-Wolfe Scheme input \u03b80 \u2208 \u0398; N (number of iterations). 1: for n = 1, . . . , N do 2: Let gn be a majorant surrogate in SL,L(f, \u03b8n\u22121). 3: Compute a search direction:\n\u03bdn \u2208 argmin \u03b8\u2208\u0398\n[ gn(\u03b8)\u2212 L\n2 \u2016\u03b8 \u2212 \u03b8n\u22121\u201622\n]\n.\n4: Line search: \u03b1\u22c6,argmin \u03b1\u2208[0,1] gn(\u03b1\u03bdn+(1\u2212\u03b1)\u03b8n\u22121). 5: Update solution: \u03b8n , \u03b1 \u22c6\u03bdn + (1\u2212 \u03b1\u22c6)\u03b8n\u22121.\n6: end for output \u03b8N (final estimate);\nWhen f is smooth and the \u201cgradient Lipschitz based surrogates\u201d from Section 2.2 are used, Algorithm 3 becomes the classical Frank-Wolfe method.4 Our point of view is however more general since it allows for example to use \u201cproximal gradient surrogates\u201d. The next proposition gives a convergence rate.\n4Note that the classical Frank-Wolfe algorithm performs in fact the line search over the function f and not gn.\nProposition 4.1 (Convex Analysis). Assume that f is convex and that \u0398 is bounded. Call R , max\u03b81,\u03b82\u2208\u0398 \u2016\u03b81 \u2212 \u03b82\u20162 the diameter of \u0398. Then, the sequence (f(\u03b8n))n\u22650 provided by Algorithm 3 converges to the minimum f\u22c6 of f over \u0398 and\nf(\u03b8n)\u2212 f\u22c6 \u2264 2LR2\nn+ 2 for all n \u2265 1.\nOther extensions of Algorithm 3 can also easily be designed by using our framework. We present for instance in the supplemental material a randomized block Frank-Wolfe algorithm, revisiting the recent work of Lacoste-Julien et al. (2013)."}, {"heading": "5. Accelerated Scheme", "text": "A popular scheme for convex optimization is the accelerated proximal gradient method (Nesterov, 2007; Beck & Teboulle, 2009). By using surrogate functions, we exploit similar ideas in Algorithm 4. When using the \u201cLipschitz gradient surrogates\u201d of Section 2.2, Algorithm 4 is exactly the scheme 2.2.19 of Nesterov (2004). When using the \u201cproximal gradient surrogate\u201d and when \u00b5 = 0, it is equivalent to the FISTA method of Beck & Teboulle (2009). Algorithm 4 consists of iteratively minimizing a surrogate computed at a point \u03ban\u22121 extrapolated from \u03b8n\u22121 and \u03b8n\u22122. It results in better convergence rates, as shown in the next proposition by adapting a proof technique of Nesterov (2004).\nAlgorithm 4 Accelerated Scheme input \u03b80 \u2208 \u0398; N ; \u00b5 (strong convexity parameter); 1: Initialization: \u03ba0 , \u03b80; a0 = 1; 2: for n = 1, . . . , N do 3: Choose a surrogate gn in SL,L+\u00b5(f, \u03ban\u22121); 4: Update solution: \u03b8n , argmin\u03b8\u2208\u0398 gn(\u03b8); 5: Compute an \u2265 0 such that:\na2n = (1 \u2212 an)a2n\u22121 + \u00b5L+\u00b5an;\n6: Set \u03b2n , an\u22121(1\u2212an\u22121)\na2 n\u22121 +an and update \u03ba:\n\u03ban , \u03b8n + \u03b2n(\u03b8n \u2212 \u03b8n\u22121);\n7: end for output \u03b8N (final estimate);\nProposition 5.1 (Convex Analysis). Assume that f is convex. When \u00b5 = 0, the sequence (\u03b8n)n\u22650 provided by Algorithm 4 satisfies for all n \u2265 1,\nf(\u03b8n)\u2212 f\u22c6 \u2264 2L\u2016\u03b80 \u2212 \u03b8\u22c6\u201622\n(n+ 2)2 .\nWhen f is \u00b5-strongly convex, we have instead a linear\nconvergence rate: for n \u2265 1,\nf(\u03b8n)\u2212 f\u22c6 \u2264 ( 1\u2212 \u221a \u00b5\nL+ \u00b5\n)n\u22121 L\u2016\u03b80 \u2212 \u03b8\u22c6\u201622\n2 ."}, {"heading": "6. Incremental Scheme", "text": "This section is devoted to objective functions f that split into many components:\nf(\u03b8) = 1\nT\nT \u2211\nt=1\nf t(\u03b8). (2)\nThe most classical method exploiting such a structure when f is smooth is probably the stochastic gradient descent (SGD) and its variants (see Bottou, 2010). It consists of drawing at iteration n an index t\u0302n and updating the solution as \u03b8n\u2190\u03b8n\u22121\u2212\u03b7n\u2207f t\u0302n(\u03b8n\u22121) with a scalar \u03b7n. Another popular algorithm is the stochastic mirror descent (see Juditsky & Nemirovski, 2011) for general non-smooth convex problems, a setting we do not consider in this paper since non-smooth functions do not always admit first-order surrogates.\nRecently, it was shown by Shalev-Schwartz & Zhang (2012) and Le Roux et al. (2012) that linear convergence rates could be obtained for strongly convex functions f t. The SAG algorithm of Le Roux et al. (2012) for smooth unconstrained optimization is an approximate gradient descent strategy, where an estimate of \u2207f is incrementally updated at each iteration. The work of Shalev-Schwartz & Zhang (2012) for composite optimization is a dual coordinate ascent method called SDCA which performs incremental updates in the primal (2). Unlike SGD, both SAG and SDCA require storing information about past iterates.\nIn a different context, incremental EM algorithms have been proposed by Neal & Hinton (1998), where surrogates of a log-likelihood are incrementally updated. By using similar ideas, we present in Algorithm 5 a scheme for solving (2), which we call MISO. In the next propositions, we study its convergence properties.\nAlgorithm 5 Incremental Scheme MISO input \u03b80 \u2208 \u0398; N (number of iterations). 1: Choose surrogates gt0 of f\nt near \u03b80 for all t; 2: for n = 1, . . . , N do 3: Randomly pick up one index t\u0302n and choose a\nsurrogate gt\u0302nn of f t\u0302n near \u03b8n\u22121. Set gtn , g t n\u22121\nfor t 6= t\u0302n; 4: Update solution: \u03b8n \u2208 argmin\n\u03b8\u2208\u0398 1 T \u2211T t=1 g t n(\u03b8).\n5: end for output \u03b8N (final estimate);\nProposition 6.1 (Non-Convex Analysis). Assume that the surrogates gt\u0302nn from Algorithm 5 are majorant and are in SL(f t\u0302n , \u03b8n\u22121). Then, the conclusions of Proposition 2.1 hold with probability one.\nProposition 6.2 (Convex Analysis). Assume that f is convex. Define f\u22c6 , min\u03b8\u2208\u0398 f(\u03b8) and \u03b4, 1T . When the surrogates g t n in Algorithm 5 are majorant and in SL,\u03c1(f t, \u03b8n\u22121) with \u03c1\u2265L, we have\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 L\u2016\u03b8\u22c6 \u2212 \u03b80\u201622\n2\u03b4n for all n \u2265 1.\nAssume now that f is \u00b5-strongly convex. For all n\u22651, \n\n\nE[\u2016\u03b8\u22c6\u2212\u03b8n\u201622] \u2264 ( (1\u2212\u03b4)+\u03b4 L\u03c1+\u00b5 )n\n\u2016\u03b8\u22c6 \u2212 \u03b80\u201622 E[f(\u03b8n)\u2212f\u22c6] \u2264 ( (1\u2212\u03b4)+\u03b4 L\u03c1+\u00b5 )n\u22121 L\u2016\u03b8\u22c6\u2212\u03b80\u201622 2 .\nInterestingly, the proof and the convergence rates of Proposition 6.2 are similar to those of the block coordinate scheme. For both schemes, the current iterate \u03b8n can be shown to be the minimizer of an approximate surrogate function which splits into different parts. Each iteration randomly picks up one part, and updates it. Like SAG or SDCA, we obtain linear convergence for strongly convex functions f , even though the upper bounds obtained for SAG and SDCA are better than ours.\nIt is also worth noticing that for smooth unconstrained problems, MISO and SAG yield different, but related, update rules. Assume for instance that \u201cLipschitz gradient surrogates\u201d are used. At iteration n of MISO, each function gtn is a surrogate of f\nt near some \u03batn\u22121. The update rule of MISO can be shown to be \u03b8n \u2190 1 T \u2211T t=1\u03ba t n\u22121\u2212 1TL \u2211T t=1\u2207f t(\u03batn\u22121); in comparison, the update rule of SAG is \u03b8n\u2190\u03b8n\u22121\u2212 1TL \u2211T\nt=1\u2207f t(\u03batn\u22121). The next section complements the theoretical analysis of the scheme MISO by numerical experiments and practical implementation heuristics."}, {"heading": "7. Experiments", "text": "In this section, we show that MISO is efficient for solving large-scale machine learning problems."}, {"heading": "7.1. Experimental Setting", "text": "We consider \u21132- and \u21131- logistic regression without intercept, and denote by m the number of samples and by p the number of features. The corresponding optimization problem can be written\nmin \u03b8\u2208Rp\n1\nm\nm \u2211\nt=1\nlog(1 + e\u2212ytx t\u22a4\u03b8) + \u03bb\u03c8(\u03b8), (3)\nwhere the regularizer \u03c8 is either the \u21131- or squared \u21132-norm. The yt\u2019s are in {\u22121,+1} and the xt\u2019s are vectors in Rp with unit \u21132-norm. We use four classical datasets described in the following table:\nname m p storage size (GB) alpha 250 000 500 dense 1 rcv1 781 265 47 152 sparse 0.95 covtype 581 012 54 dense 0.11 ocr 2 500 000 1 155 dense 23.1\nThree datasets, alpha, rcv1 and ocr were obtained from the 2008 Pascal large scale learning challenge.5 The dataset covtype is available from the LIBSVM website.6 We have chosen to test several software packages including LIBLINEAR 1.93 (Fan et al., 2008), the ASGD and SGD implementations of L. Bottou (version 2)7, an implementation of SAG kindly provided to us by the authors of Le Roux et al. (2012), the FISTA method of Beck & Teboulle (2009) implemented in the SPAMS toolbox8, and SHOTGUN (Bradley et al., 2011). All these softwares are coded in C++ and were compiled using gcc. Experiments were run on a single core of a 2.00GHz Intel Xeon CPU E5-2650 using 64GB of RAM, and all computations were done in double precision. All the timings reported do not include data loading into memory. Note that we could not run the softwares SPAMS, LIBLINEAR and SHOTGUN on the dataset ocr because of index overflow issues."}, {"heading": "7.2. On Implementing MISO", "text": "The objective function (3) splits into m components f t : \u03b8 7\u2192 log(1 + e\u2212ytxt\u22a4\u03b8) + \u03bb\u03c8(\u03b8). It is thus natural to consider the incremental scheme of Section 6 together with the proximal gradient surrogates of Section 2.2. Concretely, we build at iteration n of MISO a surrogate gt\u0302nn of f\nt\u0302n as follows: gt\u0302nn : \u03b8 7\u2192 lt\u0302n(\u03b8n\u22121)+ \u2207lt\u0302n(\u03b8n\u22121)\u22a4(\u03b8\u2212\u03b8n\u22121)+L2 \u2016\u03b8\u2212\u03b8n\u22121\u201622+\u03bb\u03c8(\u03b8), where lt is the logistic function \u03b8 7\u2192 log(1 + e\u2212ytxt\u22a4\u03b8). After removing the dependency over n to simplify the notation, all the surrogates can be rewritten as gt : \u03b8 7\u2192 at + zt\u22a4\u03b8+ L2 \u2016\u03b8\u201622 +\u03bb\u03c8(\u03b8), where at is a constant and zt is a vector in Rp. Therefore, all surrogates can be \u201csummarized\u201d by the pair (at, z\nt), quantities which we keep into memory during the optimization. Then, finding the estimate \u03b8n amounts to minimizing a function of the form \u03b8 7\u2192 z\u0304\u22a4n \u03b8 + L2 \u2016\u03b8\u201622 + \u03bb\u03c8(\u03b8), where z\u0304n is the average value of the quantities z\nt at iteration n. It is then easy to see that obtaining z\u0304n+1\n5http://largescale.ml.tu-berlin.de. 6http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 7http://leon.bottou.org/projects/sgd. 8http://spams-devel.gforge.inria.fr/.\nfrom z\u0304n can be done in O(p) operations with the following update: z\u0304n+1 \u2190 z\u0304n + (zt\u0302nnew \u2212 zt\u0302nold)/m. One issue is that building the surrogates gt requires choosing some constant L. An upper bound on the Lipschitz constants of the gradients \u2207lt could be used here. However, we have observed that significantly faster convergence could be achieved by using a smaller value, probably because a local Lipschitz constant may be better adapted than a global one. By studying the proof of Proposition 6.2, we notice indeed that our convergence rates can be obtained without majorant surrogates, when we simply have: E[f t(\u03b8n)] \u2264 E[gtn(\u03b8n)] for all t and n. This motivates the following heuristics:\n\u2022 MISO1: start by performing one pass over \u03b7=5% of the data to select a constant L\u2032 yielding the smallest decrease of the objective, and set L = L\u2032\u03b7;\n\u2022 MISO2: in addition to MISO1, check the inequalities f t\u0302n(\u03b8n\u22121)\u2264 gt\u0302nn\u22121(\u03b8n\u22121) during the optimization. After each pass over the data, if the rate of satisfied inequalities drops below 50%, double the value of L.\nFollowing these strategies, we have implemented the scheme MISO in C++. The resulting software package will be publicly released with an open source license."}, {"heading": "7.3. \u21132-Regularized Logistic Regression", "text": "We compare LIBLINEAR, FISTA, SAG, ASGD, SGD, MISO1, MISO2 and MISO2 with T = 1000 blocks (grouping some observations into minibatches). LIBLINEAR was run using the option -s 0 -e 0.000001. The implementation of SAG includes a heuristic line search in the same spirit as MISO2, introduced by Le Roux et al. (2012). Every method was stopped after 50 passes over the data. We considered three regularization regimes, high (\u03bb= 10\u22123), medium (\u03bb= 10\u22125) and low (\u03bb=10\u22127). We present in Figure 1 the values of the objective function during the optimization for the regime medium, both in terms of passes over the data and training time. The regimes low and high are provided as supplemental material only. Note that to reduce the memory load, we used a minibatch strategy for the dataset rcv1 with T = 10 000 blocks.\nOverall, there is no clear winner from this experiment, and the preference for an algorithm depends on the dataset, the required precision, or the regularization level. The best methods seem to be consistently MISO, ASGD and SAG and the slowest one FISTA. Note that this apparently mixed result is a significant achievement. We have indeed focused on state-ofthe-art solvers, which already significantly outperform a large number of other baselines (see Bottou, 2010; Fan et al., 2008; Le Roux et al., 2012)."}, {"heading": "7.4. \u21131-Regularized Logistic Regression", "text": "Since SAG, SGD and ASGD cannot deal with \u21131regularization, we compare here LIBLINEAR, FISTA, SHOTGUN and MISO. We use for LIBLINEAR the option -s 6 -e 0.000001. We proceed as in Section 7.3, considering three regularization regimes yielding different sparsity levels. We report the results for one of them in Figure 2 and provide the rest as supplemental material. In this experiment, our method outperforms other competitors, except LIBLINEAR on the dataset rcv1 when a high precision is required (and the regularization is low). We also remark that a low precision solution is often achieved quickly using the minibatch scheme (MISO2 b1000), but this strategy is outperformed by MISO1 and MISO2 for high precisions."}, {"heading": "8. Conclusion", "text": "In this paper, we have introduced a flexible optimization framework based on the computation of \u201csurrogate functions\u201d. We have revisited numerous schemes and discovered new ones. For each of them, we have studied convergence guarantees for non-convex problems and convergence rates for convex ones. Our methodology led us in particular to the design of an in-\nEffective passes over data / Dataset alpha\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nTraining time (sec) / Dataset alpha\nD is\nta nc\ne to\no pt\nim um\n100 101 102 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nFISTA LIBLINEAR SHOTGUN MISO1 MISO2 MISO2 b1000\nEffective passes over data / Dataset rcv1\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nTraining time (sec) / Dataset rcv1\nD is\nta nc\ne to\no pt\nim um\n100 101 102 103 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nFISTA LIBLINEAR SHOTGUN MISO1 b10000 MISO2 b10000 MISO2 b1000\nEffective passes over data / Dataset covtype\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nTraining time (sec) / Dataset covtype\nD is\nta nc\ne to\no pt\nim um\n10\u22121 100 101 102 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nFISTA LIBLINEAR SHOTGUN MISO1 MISO2 MISO2 b1000\nEffective passes over data / Dataset ocr\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nTraining time (sec) / Dataset ocr\nD is\nta nc\ne to\no pt\nim um\n101 102 103 10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nMISO1 MISO2 MISO2 b1000"}, {"heading": "Acknowledgments", "text": ""}, {"heading": "Bradley, J.K., Kyrola, A., Bickson, D., and Guestrin, C.", "text": "Parallel coordinate descent for l1-regularized loss minimization. In Proc. ICML, 2011.\nCande\u0300s, E.J., Wakin, M., and Boyd, S.P. Enhancing sparsity by reweighted \u21131 minimization. J. Fourier Anal. Appl., 14(5):877\u2013905, 2008.\nCollins, M., Schapire, R.E., and Singer, Y. Logistic regression, AdaBoost and Bregman distances. Mach. Learn., 48(1):253\u2013285, 2002.\nDaubechies, I., Defrise, M., and De Mol, C. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Commun. Pur. Appl. Math., 57 (11):1413\u20131457, 2004."}, {"heading": "Della Pietra, S., Della Pietra, V., and Lafferty, J. Duality", "text": "and auxiliary functions for Bregman distances. Technical report, CMU-CS-01-109, 2001.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. LIBLINEAR: A library for large linear classification. J. Mach. Learn. Res., 9:1871\u20131874, 2008."}, {"heading": "Gasso, G., Rakotomamonjy, A., and Canu, S. Recovering", "text": "sparse signals with non-convex penalties and DC programming. IEEE T. Signal Process., 57(12):4686\u20134698, 2009.\nHarchaoui, Z., Juditsky, A., and Nemirovski, A. Conditional gradient algorithms for norm-regularized smooth convex optimization. preprint arXiv:1302.2325v4, 2013.\nHazan, E. and Kale, S. Projection-free online learning. In Proc. ICML, 2012.\nHorst, R. and Thoai, N.V. DC programming: overview. J. Optim. Theory App., 103(1):1\u201343, 1999.\nJuditsky, A. and Nemirovski, A. First order methods for nonsmooth convex large-scale optimization, I: General purpose methods. In Optimization for Machine Learning. MIT Press, 2011.\nKhan, E., Marlin, B., Bouchard, G., and Murphy, K. Variational bounds for mixed-data factor analysis. In Adv. NIPS, 2010."}, {"heading": "Lacoste-Julien, S., Jaggi, M., Schmidt, M., and Pletscher,", "text": "P. Block-coordinate Frank-Wolfe optimization for structural SVMs. In Proc. ICML, 2013."}, {"heading": "Lange, K., Hunter, D.R., and Yang, I. Optimization", "text": "transfer using surrogate objective functions. J. Comput. Graph. Stat., 9(1):1\u201320, 2000."}, {"heading": "Le Roux, N., Schmidt, M., and Bach, F. A stochastic", "text": "gradient method with an exponential convergence rate for finite training sets. In Adv. NIPS, 2012."}, {"heading": "Lee, D.D. and Seung, H.S. Algorithms for non-negative", "text": "matrix factorization. In Adv. NIPS, 2001."}, {"heading": "Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online", "text": "learning for matrix factorization and sparse coding. J. Mach. Learn. Res., 11:19\u201360, 2010.\nNeal, R.M. and Hinton, G.E. A view of the EM algorithm that justifies incremental, sparse, and other variants. Learning in graphical models, 89:355\u2013368, 1998.\nNesterov, Y. Introductory lectures on convex optimization. Kluwer Academic Publishers, 2004.\nNesterov, Y. Gradient methods for minimizing composite objective functions. Technical report, CORE Discussion Paper, 2007.\nNesterov, Y. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM J. Optimiz., 22 (2):341\u2013362, 2012."}, {"heading": "Nesterov, Y. and Polyak, B.T. Cubic regularization of", "text": "Newton method and its global performance. Math. Program., 108(1):177\u2013205, 2006.\nRichta\u0301rik, P. and Taka\u0301c\u030c, M. Iteration complexity of randomized block coordinate descent methods for minimizing a composite function. Math. Program., 2012.\nSeeger, M.W. and Wipf, D.P. Variational Bayesian inference techniques. IEEE Signal Proc. Mag., 27(6):81\u201391, 2010."}, {"heading": "Shalev-Schwartz, S. and Zhang, T. Proximal stochastic", "text": "dual coordinate ascent. preprint arXiv 1211.2717v1, 2012."}, {"heading": "Shalev-Shwartz, S. and Tewari, A. Stochastic methods for", "text": "\u21131 regularized loss minimization. In Proc. ICML, 2009.\nTseng, P. and Yun, S. A coordinate gradient descent method for nonsmooth separable minimization. Math. Program., 117:387\u2013423, 2009."}, {"heading": "Wainwright, M.J. and Jordan, M.I. Graphical models,", "text": "exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1\u2013305, 2008.\nWright, S., Nowak, R., and Figueiredo, M. Sparse reconstruction by separable approximation. IEEE T. Signal Process., 57(7):2479\u20132493, 2009.\nZhang, T. Sequential greedy approximation for certain convex optimization problems. IEEE T. Inform. Theory, 49 (3):682\u2013691, 2003.\nZhang, X., Yu, Y., and Schuurmans, D. Accelerated training for matrix-norm regularization: a boosting approach. In Adv. NIPS, 2012.\nSupplementary Material\nOptimization with First-Order Surrogate Functions\nOutline. In Appendix A, we present simple mathematical definitions. Appendix B contains useful mathematical results, which are used in the paper. In Appendix C, we present various mechanisms to build first-order surrogate functions; it is in fact a more rigorous version of Section 2.2, where all claims are proved. In Appendix D, we present the block Frank-Wolfe optimization scheme. Finally, all proofs of propositions are given in Appendix E, and Appendix F contains additional experimental results."}, {"heading": "A. Mathematical Background", "text": "For self-containedness purposes, we introduce in this section some mathematical definitions. Most of them can be found in classical textbooks on optimization (e.g., Bertsekas, 1999; Boyd & Vandenberghe, 2004; Borwein & Lewis, 2006; Nocedal & Wright, 2006; Nesterov, 2004).\nDefinition A.1 (Directional Derivative). Let us consider a function f : \u0398 \u2286 Rp \u2192 R, where \u0398 is a convex set, and \u03b8, \u03b8\u2032 be in \u0398. When it exists, the following limit is called the directional derivative of f at \u03b8 in the direction \u03b8\u2032 \u2212 \u03b8:\n\u2207f(\u03b8, \u03b8\u2032 \u2212 \u03b8) , lim t\u21920+ f(\u03b8 + t(\u03b8\u2032 \u2212 \u03b8))\u2212 f(\u03b8) t .\nWhen f is differentiable at \u03b8, directional derivatives always exist and we have \u2207f(\u03b8, \u03b8\u2032 \u2212 \u03b8) = \u2207f(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8). Definition A.2 (Feasible Direction). Let \u0398 \u2286 Rp be a convex set and \u03b8 be a point in \u0398. A vector z in Rp is a feasible direction if \u03b8 + z is in \u0398. In other words, z can be written as \u03b8\u2032 \u2212 \u03b8, where \u03b8\u2032 is in \u0398. Definition A.3 (Stationary Point). Let us consider a function f : \u0398 \u2286 Rp \u2192 R, where \u0398 is a convex set, such that f admits directional derivatives everywhere in \u0398 for every feasible direction. Let \u03b8 be a point in \u0398. We say that \u03b8 is a stationary point if for all \u03b8\u2032 6= \u03b8 in \u0398,\n\u2207f(\u03b8, \u03b8\u2032 \u2212 \u03b8) \u2265 0. (4)\nWhen f is differentiable and \u03b8 is in the interior of \u0398, this condition reduces to \u2207f(\u03b8) = 0. When f is convex and \u03b8 is also in the interior of \u0398, this condition reduces to 0 \u2208 \u2202f(\u03b8), where \u2202f is the subdifferential of f .\nProof. Let us assume that \u03b8 is a stationary point and f is differentiable at \u03b8. Then, for all \u03b8\u2032 in \u0398, \u2207f(\u03b8, \u03b8\u2032\u2212\u03b8) = \u2207f(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8) \u2265 0. In particular, since \u03b8 is in the interior of \u0398, we can find \u03b8\u2032 such that \u03b8\u2032 \u2212 \u03b8 = \u2212\u03b4\u2207f(\u03b8) for some \u03b4 > 0 small enough. Thus, we necessarily have \u2207f(\u03b8) = 0. The converse is trivial. The equivalence between (4) and 0 \u2208 \u2202f(\u03b8) when f is convex but non-differentiable can be found in Borwein & Lewis (2006, Proposition 3.1.6).\nDefinition A.4 (Lipschitz Continuity). A function f : \u0398 \u2286 Rp \u2192 R is called Lipschitz if there exists a constant L > 0 such that for all \u03b8, \u03b8\u2032 in \u0398, we have\n|f(\u03b8\u2032)\u2212 f(\u03b8)| \u2264 L\u2016\u03b8\u2212 \u03b8\u2032\u20162.\nIn that case, we say that the function is L-Lipschitz.\nDefinition A.5 (Strong Convexity). Let \u0398 be a convex set. A function f : \u0398 \u2286 Rp \u2192 R is called \u00b5-strongly convex when there exists a constant \u00b5 > 0 such that for all \u03b8\u2032 in \u0398, the function \u03b8 7\u2192 f(\u03b8) \u2212 \u00b52 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 is convex. This definition is equivalent to having for all \u03b1 in [0, 1] and \u03b8, \u03b8\u2032 in \u0398,\nf(\u03b1\u03b8 + (1\u2212 \u03b1)\u03b8\u2032) \u2264 \u03b1f(\u03b8) + (1 \u2212 \u03b1)f(\u03b8\u2032)\u2212 \u00b5 2 \u03b1(1 \u2212 \u03b1)\u2016\u03b8 \u2212 \u03b8\u2032\u201622. (5)\nNote that the value \u00b5 = 0 leads to the classical definition of convex functions.\nProof. Let us consider \u03b8\u2032 in \u0398 and define the function g : \u03b8 7\u2192 f(\u03b8)\u2212 \u00b52 \u2016\u03b8 \u2212 \u03b8\u2032\u201622. This function is convex if and only if for all \u03b1 in [0, 1], we have\ng(\u03b1\u03b8 + (1\u2212 \u03b1)\u03b8\u2032) \u2264 \u03b1g(\u03b8) + (1\u2212 \u03b1)g(\u03b8\u2032).\nIn other words, if and only if\nf(\u03b1\u03b8 + (1 \u2212 \u03b1)\u03b8\u2032)\u2212 \u00b5 2 \u03b12\u2016\u03b8 \u2212 \u03b8\u2032\u201622 \u2264 \u03b1 ( f(\u03b8)\u2212 \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 ) + (1\u2212 \u03b1)f(\u03b8\u2032),\nwhich is equivalent to (5)."}, {"heading": "B. Useful Mathematical Results", "text": "We provide in this section a few propositions and lemmas which are used in this paper.\nLemma B.1 (Convex Surrogate for Functions with Lipschitz Gradient). Let f : Rp \u2192 R be differentiable and \u2207f be L-Lipschitz continuous. Then, for all \u03b8, \u03b8\u2032 in Rp,\n|f(\u03b8\u2032)\u2212 f(\u03b8)\u2212\u2207f(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8)| \u2264 L 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622. (6)\nProof. This lemma is classical (see Nesterov, 2004, Lemma 1.2.3 and its proof).\nNote that Eq. (6) does not imply the gradient of a differentiable function f to be L-Lipschitz continuous. The equivalence is only true in some cases, as shown in the following lemma.\nLemma B.2 (Relation between Quadratic Surrogates and Lipschitz Constants). Let f : Rp \u2192 R be a differentiable function. Assume that for all \u03b8, \u03b8\u2032 in Rp, inequality (6) holds. Then, \u2207f is L-Lipschitz continuous when one of the following conditions is true:\n1. f is convex;\n2. f is twice differentiable;\n3. \u2207f is Lipschitz continuous (the lemma then provides the Lipschitz constant L)."}, {"heading": "Proof.", "text": ""}, {"heading": "First point:", "text": "a proof of the first point can be found in Nesterov (2004, Theorem 2.1.5)."}, {"heading": "Second point:", "text": "To prove the second point, we upper-bound the extremal eigenvalues of the Hessian matrix. Let us fix \u03b8 in Rp. Since f is twice differentiable at \u03b8, we have for all \u03b8\u2032 in Rp\n\u2207f(\u03b8\u2032)\u2212\u2207f(\u03b8) = \u22072f(\u03b8)(\u03b8\u2032 \u2212 \u03b8) + o(\u2016\u03b8\u2032 \u2212 \u03b8\u20162),\nand thus (\u03b8\u2032 \u2212 \u03b8)\u22a4(\u2207f(\u03b8\u2032)\u2212\u2207f(\u03b8)) = (\u03b8\u2032 \u2212 \u03b8)\u22a4\u22072f(\u03b8)(\u03b8\u2032 \u2212 \u03b8) + o(\u2016\u03b8\u2032 \u2212 \u03b8\u201622). (7)\nSumming twice Eq. (6) without the absolute values when exchanging the roles of \u03b8 and \u03b8\u2032 gives\n(\u03b8\u2032 \u2212 \u03b8)\u22a4(\u2207f(\u03b8\u2032)\u2212\u2207f(\u03b8)) \u2264 L\u2016\u03b8 \u2212 \u03b8\u2032\u201622.\nPlugging Eq. (7) into this inequality yields \u2016\u22072f(\u03b8)\u20162 \u2264 L. To conclude, we use a mean value theorem, as done for example in Nesterov (2004, Lemma 1.2.2)\n\u2016\u2207f(\u03b8\u2032)\u2212\u2207f(\u03b8)\u20162 = \u2225 \u2225 \u2225\n\u2225\n\u222b 1\nt=0\n\u22072f(\u03b8 + t(\u03b8\u2032 \u2212 \u03b8))(\u03b8\u2032 \u2212 \u03b8)dt \u2225 \u2225 \u2225\n\u2225\n2\n\u2264 \u222b 1\nt=0\n\u2016\u22072f(\u03b8 + t(\u03b8\u2032 \u2212 \u03b8))\u20162dt\u2016\u03b8\u2032 \u2212 \u03b8\u20162\n\u2264 L\u2016\u03b8\u2212 \u03b8\u2032\u20162."}, {"heading": "Third point:", "text": "Proving the third point is more difficult due to the lack of smoothness assumptions on f . However, when making the explicit assumption that \u2207f is Lipschitz continuous, we can show that Eq. (6) provides us a Lipschitz constant. The proof exploits some results from nonsmooth analysis developed by Clarke (1983). We essentially use a mean value theorem for multi-dimensional Lipschitz functions (Clarke, 1983, Proposition 2.6.5), exploiting the fact that a Lipschitz function is differentiable almost everywhere (Rademacher theorem). This allows us to follow a similar proof as for the twice differentiable case.\nMore precisely, we have that \u2207f is Lipschitz continuous and thus differentiable almost everywhere on \u0398. Let us call the Hessian matrix \u22072f(\u03b8) at a point \u03b8 in Rp, when it exists. Then, we have at such a point \u2016\u22072f(\u03b8)\u20162 \u2264 L, following the beginning of the second point\u2019s proof. Then, it turns out that for all \u03b8 in Rp, the following mean value theorem holds for almost all \u03b8\u2032 (see Clarke, 1983, proof of Proposition 2.6.5):\n\u2207f(\u03b8\u2032)\u2212\u2207f(\u03b8) = \u222b 1\nt=0\n\u22072f(\u03b8 + t(\u03b8\u2032 \u2212 \u03b8))(\u03b8\u2032 \u2212 \u03b8)dt.\nThis comes from the fact that for almost all \u03b8\u2032, the intersection of the line segment [\u03b8, \u03b8\u2032] and the set where \u22072f is not defined has 0 one-dimensional measure (see again Clarke, 1983, Proposition 2.6.5). We therefore have for almost all \u03b8\u2032 (and a fixed \u03b8), \u2016\u2207f(\u03b8) \u2212 \u2207f(\u03b8\u2032)\u20162 \u2264 L\u2016\u03b8 \u2212 \u03b8\u2032\u20162 and the general result comes from a continuity argument.\nLemma B.3 (Surrogate for Functions with Lipschitz Hessian). Let f : Rp \u2192 R be a twice differentiable function with M -Lipschitz continuous Hessian. Then for all \u03b8, \u03b8\u2032 in Rp,\n\u2223 \u2223 \u2223 \u2223 f(\u03b8\u2032)\u2212 f(\u03b8)\u2212\u2207f(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8)\u2212 1 2 (\u03b8\u2032 \u2212 \u03b8)\u22a4\u22072f(\u03b8)(\u03b8\u2032 \u2212 \u03b8) \u2223 \u2223 \u2223 \u2223 \u2264 M 6 \u2016\u03b8 \u2212 \u03b8\u2032\u201632.\nProof. This is again a classical lemma (see Nesterov, 2004, Lemma 1.2.4).\nLemma B.4 (Lower Surrogate for Strongly Convex Functions). Let f : Rp \u2192 R be a \u00b5-strongly convex function. Suppose that f is differentiable, then the following inequality holds for all \u03b8, \u03b8\u2032 in Rp:\nf(\u03b8\u2032) \u2265 f(\u03b8) +\u2207f(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8) + \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622.\nProof. \u03b8\u2032 7\u2192 f(\u03b8\u2032)\u2212 \u00b52 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 is convex and differentiable and is therefore above its tangent at \u03b8, immediately leading to the desired inequality.\nLemma B.5 (Second-Order Growth Property). Let f : Rp \u2192 R be a \u00b5-strongly convex function and \u0398 \u2286 Rp be a convex set. Let \u03b8\u22c6 be the minimizer of f on \u0398. Then, the following condition holds for all \u03b8 in \u0398:\nf(\u03b8) \u2265 f(\u03b8\u22c6) + \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u22c6\u201622.\nProof. Let us define the function g : \u03b8 7\u2192 f(\u03b8) \u2212 \u00b52 \u2016\u03b8 \u2212 \u03b8\u22c6\u201622. We show that \u03b8\u22c6 is a minimizer of the convex function g by looking at first-order optimality conditions based on directional derivatives. For all \u03b8 in \u0398, we have\n\u2207g(\u03b8\u22c6, \u03b8 \u2212 \u03b8\u22c6) = lim t\u21920+ f(\u03b8\u22c6 + t(\u03b8 \u2212 \u03b8\u22c6))\u2212 f(\u03b8\u22c6)\u2212 \u00b5t22 \u2016\u03b8 \u2212 \u03b8\u22c6\u201622 t\n= lim t\u21920+ f(\u03b8\u22c6 + t(\u03b8 \u2212 \u03b8\u22c6))\u2212 f(\u03b8\u22c6) t = \u2207f(\u03b8\u22c6, \u03b8 \u2212 \u03b8\u22c6) \u2265 0,\nwhere \u2207f(\u03b8\u22c6, \u03b8 \u2212 \u03b8\u22c6) is non-negative because \u03b8\u22c6 is a stationary point of f on \u0398. Thus, \u03b8\u22c6 is also a stationary point of the function g on \u0398, and is a minimizer of g on \u0398 since g is convex (Borwein & Lewis, 2006, Proposition 2.1.2). This is sufficient to conclude.\nLemma B.6 (Lipschitz Continuity of Minimizers for Parameterized Functions). Let f : Rp1 \u00d7\u03982 \u2192 R be a function of two variables where \u03982 \u2286 Rp2 is a convex set. Assume that\n\u2022 \u03b81 7\u2192 f(\u03b81, \u03b82) is differentiable for all \u03b82 in \u03982; \u2022 \u03b82 7\u2192 \u22071f(\u03b81, \u03b82) is L-Lipschitz continuous for all \u03b81 in Rp1 ; \u2022 \u03b82 7\u2192 f(\u03b81, \u03b82) is \u00b5-strongly convex for all \u03b81 in Rp1 .\nThen, the function \u03b81 7\u2192 argmin\u03b82\u2208\u03982 f(\u03b81, \u03b82) is well defined and L\u00b5 -Lipschitz.\nProof. Let us consider \u03b81, \u03b8 \u2032 1 in R p1 and the corresponding (unique by strong convexity) solutions \u03b8\u22c62 , argmin\u03b82\u2208\u03982 f(\u03b81, \u03b82) and \u03b8 \u2032\u22c6 2 , argmin\u03b82\u2208\u03982 f(\u03b8 \u2032 1, \u03b82). From the second-order growth condition of Lemma B.5, we have \u00b5\n2 \u2016\u03b8\u22c62 \u2212 \u03b8\u2032\u22c62 \u201622 \u2264 f(\u03b81, \u03b8\u2032\u22c62 )\u2212 f(\u03b81, \u03b8\u22c62),\nand \u00b5\n2 \u2016\u03b8\u22c62 \u2212 \u03b8\u2032\u22c62 \u201622 \u2264 f(\u03b8\u20321, \u03b8\u22c62)\u2212 f(\u03b8\u20321, \u03b8\u2032\u22c62 ),\nDefine the function g : \u03ba 7\u2192 f(\u03ba, \u03b8\u2032\u22c62 )\u2212 f(\u03ba, \u03b8\u22c62) and sum the above inequalities. We obtain\n\u00b5\u2016\u03b8\u22c62 \u2212 \u03b8\u2032\u22c62 \u201622 \u2264 g(\u03b81)\u2212 g(\u03b8\u20321).\nWe notice that the gradient of g is bounded: for all \u03ba in Rp1 , \u2016\u2207g(\u03ba)\u20162 = \u2016\u22071f(\u03ba, \u03b8\u2032\u22c62 ) \u2212 \u22071f(\u03ba, \u03b8\u22c62)\u2016 \u2264 L\u2016\u03b8\u2032\u22c62 \u2212 \u03b8\u22c62\u20162. We use here the fact that \u22071f is L-Lipschitz with respect to its second argument. Thus, g is Lipschitz with constant L\u2016\u03b8\u2032\u22c62 \u2212 \u03b8\u22c62\u20162 and\n\u00b5\u2016\u03b8\u22c62 \u2212 \u03b8\u2032\u22c62 \u201622 \u2264 L\u2016\u03b8\u22c62 \u2212 \u03b8\u2032\u22c62 \u20162\u2016\u03b81 \u2212 \u03b8\u20321\u20162.\nThis is sufficient to conclude."}, {"heading": "Lemma B.7 (Differentiability of Optimal Value Functions).", "text": "Let us consider a function f defined as in Lemma B.6 and with the same properties. Define the optimal value function f\u0303(\u03b81) , min\u03b82\u2208\u03982 f(\u03b81, \u03b82). Then, f\u0303 is differentiable and \u2207f\u0303(\u03b81) = \u22071f(\u03b81, \u03b8\u22c62), where \u03b8\u22c62 , argmin\u03b82\u2208\u03982 f(\u03b81, \u03b82). Moreover,\n1. when f is convex and \u03b81 7\u2192 \u22071f(\u03b81, \u03b82) is L\u2032-Lipschitz continuous for all \u03b82 in \u03982, the function f\u0303 is convex and \u2207f\u0303 is Lipschitz continuous with constant L\u2032;\n2. when \u03b81 7\u2192 f(\u03b81, \u03b82) is concave for all \u03b82 in \u03982, the function f\u0303 is concave and \u2207f\u0303 is Lipschitz continuous with constant 2L 2\n\u00b5 ;\n3. when \u03b81 7\u2192 f(\u03b81, \u03b82) is affine for all \u03b82 in \u03982, the function f\u0303 is concave and \u2207f\u0303 is Lipschitz continuous with constant L 2\n\u00b5 .\nProof. Note that this lemma is a variant of a theorem introduced by Danskin (1967). We first prove the differentiability of f before detailing how to obtain the Lipschitz constants.\nDifferentiability of f : Let us consider \u03b81 and \u03b8 \u2032 1 in R\np1 , and let us use the same notation and definitions as in the proof of Lemma B.6. Then, we have\nf\u0303(\u03b8\u20321)\u2212 f\u0303(\u03b81) = f(\u03b8\u20321, \u03b8\u2032\u22c62 )\u2212 f(\u03b81, \u03b8\u22c62) = f(\u03b8\u20321, \u03b8 \u2032\u22c6 2 )\u2212 f(\u03b8\u20321, \u03b8\u22c62) + f(\u03b8\u20321, \u03b8\u22c62)\u2212 f(\u03b81, \u03b8\u22c62)\n= g(\u03b8\u20321) + f(\u03b8 \u2032 1, \u03b8 \u22c6 2)\u2212 f(\u03b81, \u03b8\u22c62) = g(\u03b8\u20321) +\u22071f(\u03b81, \u03b8\u22c62)\u22a4(\u03b8\u20321 \u2212 \u03b81) + o(\u2016\u03b8\u20321 \u2212 \u03b81\u20162),\n(8)\nwhere g is defined in the proof of Lemma B.6. Recall that the function g is Lipschitz with constant L\u2016\u03b8\u2032\u22c62 \u2212 \u03b8\u22c62\u20162 (see the proof of Lemma B.6). Thus,\n|g(\u03b8\u20321)| \u2264 |g(\u03b81)\u2212 g(\u03b8\u20321)| \u2264 L\u2016\u03b8\u2032\u22c62 \u2212 \u03b8\u22c62\u20162\u2016\u03b8\u20321 \u2212 \u03b81\u20162 \u2264 L2\n\u00b5 \u2016\u03b8\u20321 \u2212 \u03b81\u201622, (9)\nwhere the first inequality uses the fact that g(\u03b8\u20321) \u2264 0 and g(\u03b81) \u2265 0. The last inequality uses Lemma B.6. We can now show that f\u0303(\u03b8\u20321) = f\u0303(\u03b81) +\u22071f(\u03b81, \u03b8\u22c62)\u22a4(\u03b8\u20321 \u2212 \u03b81) + o(\u2016\u03b8\u20321 \u2212 \u03b81\u20162). The function f\u0303 thus admits a first-order Taylor expansion and is differentiable. Moreover, we have \u2207f\u0303(\u03b81) = \u22071f(\u03b81, \u03b8\u22c62)."}, {"heading": "Proof of the first point:", "text": "When f is jointly convex in \u03b81 and \u03b82, it is easy to show that f\u0303 is also convex (Boyd & Vandenberghe, 2004, Section 3.2.5).\nBy explicitly upper-bounding the quantity o(\u2016\u03b8\u20321 \u2212 \u03b81\u20162) in Eq. (8) using the L\u2032-Lipschitz continuity of \u22071f in its first argument and the inequality g(\u03b8\u20321) \u2264 0, we have\n0 \u2264 f\u0303(\u03b8\u20321)\u2212 f\u0303(\u03b81)\u2212\u2207f\u0303(\u03b81)\u22a4(\u03b8\u20321 \u2212 \u03b81) \u2264 L\u2032\n2 \u2016\u03b8\u20321 \u2212 \u03b81\u201622,\nwe can apply Lemma B.2 to ensure that \u2207f\u0303 is L\u2032-Lipschitz continuous."}, {"heading": "Proof of the second point:", "text": "\u2212f\u0303 is a pointwise supremum of convex functions and is therefore convex (see Boyd & Vandenberghe, 2004, Section 3.2.3). Then, we have from Eq. (8) and using the concavity of \u03b81 7\u2192 f(\u03b81, \u03b8\u22c62):\nf\u0303(\u03b8\u20321)\u2212 f\u0303(\u03b81) \u2265 g(\u03b8\u20321) +\u2207f\u0303(\u03b81)\u22a4(\u03b8\u20321 \u2212 \u03b81).\nThus,\n0 \u2264 \u2212f\u0303(\u03b8\u20321) + f\u0303(\u03b81) +\u2207f\u0303(\u03b81)\u22a4(\u03b8\u20321 \u2212 \u03b81) \u2264 |g(\u03b8\u20321)| \u2264 L2\n\u00b5 \u2016\u03b81 \u2212 \u03b8\u20321\u201622,\nwhere the last inequality was shown in Eq. (9). We can then apply Lemma B.2 to the convex function \u2212f\u0303 and we obtain the desired Lipschitz constant 2L 2\n\u00b5 ."}, {"heading": "Proof of the third point:", "text": "When \u03b81 7\u2192 f(\u03b81, \u03b82) is affine, \u22071f(\u03b81, \u03b82) is independent of \u03b81.\n\u2016\u2207f\u0303(\u03b8\u20321)\u2212\u2207f\u0303(\u03b81)\u20162 = \u2016\u22071f(\u03b8\u20321, \u03b8\u2032\u22c62 )\u2212\u22071f\u0303(\u03b81, \u03b8\u22c62)\u20162\n= \u2016\u2207g(\u03b81)\u20162 \u2264 L\u2016\u03b82 \u2212 \u03b8\u22c62\u20162 \u2264 L2\n\u00b5 \u2016\u03b81 \u2212 \u03b8\u20321\u20162,\nwhere the upper-bound on the gradient of g was shown in the proof of Lemma B.6.\nLemma B.8 (Pythagoras Relation). Let \u03b8, \u03ba, \u03bd in Rp. Then \u2016\u03ba\u2212 \u03b8\u201622 + 2(\u03ba\u2212 \u03b8)\u22a4(\u03b8 \u2212 \u03bd) = \u2016\u03ba\u2212 \u03bd\u201622 \u2212 \u2016\u03b8 \u2212 \u03bd\u201622. Lemma B.9 (Regularity of Residual Functions). Let f, g : Rp \u2192 R be two functions. Define the difference function h , g \u2212 f . Then,\n1. if g is \u03c1-strongly convex and f differentiable with L-Lipschitz continuous gradient, with \u03c1 \u2265 L, the function h is (\u03c1\u2212 L)-strongly convex;\n2. if g and f are convex and differentiable with L-Lipschitz continuous gradient, \u2207h is L-Lipschitz continuous.\n3. if g and f are \u00b5-strongly convex and differentiable with L-Lipschitz continuous gradient, \u2207h is (L \u2212 \u00b5)Lipschitz continuous."}, {"heading": "Proof.", "text": ""}, {"heading": "Proof of the first point:", "text": "Let \u03b8\u2032 be in Rp, and define l : \u03b8 7\u2192 g(\u03b8)\u2212 f(\u03b8)\u2212 (\u03c1\u2212L)2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622. Then,\nl(\u03b8) = ( g(\u03b8)\u2212 \u03c1 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 ) +\n(\nL 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 \u2212 f(\u03b8)\n)\n,\nThe left term inside parentheses is convex by definition of strong convexity. Let us call the right term l\u2032 : \u03b8 7\u2192 L 2 \u2016\u03b8\u2212\u03b8\u2032\u201622\u2212f(\u03b8). The function l\u2032 is differentiable and we can show that it is above its tangent, therefore convex. Let us indeed fix \u03ba in Rp:\nl\u2032(\u03b8) = L\n2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 \u2212 f(\u03b8) \u2265 \u2212f(\u03ba)\u2212\u2207f(\u03ba)\u22a4(\u03b8 \u2212 \u03ba)\u2212\nL 2 \u2016\u03b8 \u2212 \u03ba\u201622 + L 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622\n=\n(\nL 2 \u2016\u03ba\u2212 \u03b8\u2032\u201622 \u2212 f(\u03ba)\n)\n+ L(\u03ba\u2212 \u03b8\u2032)\u22a4(\u03b8 \u2212 \u03ba)\u2212\u2207f(\u03ba)\u22a4(\u03b8 \u2212 \u03ba)\n= l\u2032(\u03ba) +\u2207l\u2032(\u03ba)\u22a4(\u03b8 \u2212 \u03ba).\nThe first inequality comes from Lemma B.1 applied to the function f at \u03ba. The second equality is simply due to the trivial relation described in Lemma B.8.\nProof of the second and third points: We simply prove the third point, and then obtain the second point by choosing \u00b5 = 0. We have for all \u03b8 and \u03b8\u2032 in Rp, according to Lemma B.1 and B.4\n\u00b5 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 \u2264 f(\u03b8\u2032)\u2212 f(\u03b8)\u2212\u2207f(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8) \u2264 L 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622,\nand\n\u2212L 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 \u2264 \u2212g(\u03b8\u2032) + g(\u03b8) +\u2207g(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8) \u2264 \u2212 \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622.\nSumming the two inequalities we have that\n|h(\u03b8\u2032)\u2212 h(\u03b8)\u2212\u2207h(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8)| \u2264 L\u2212 \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622.\nwhere h , g\u2212 f . Since h is differentiable with a Lipschitz gradient, the result follows from Lemma B.2 (whether h is convex or not)."}, {"heading": "C. Mechanisms to Construct First-Order Surrogate Functions", "text": "We provide here some details and justifications to Section 2.2. We start with a basic lemma, which gives us elementary techniques to combine surrogate functions.\nLemma C.1 (Combination Rules for Majorant First-Order Surrogates). Let us consider two functions f : Rp \u2192 R and f \u2032 : Rp \u2192 R, and majorant surrogate functions g in SL(f, \u03ba) and g\u2032 in SL(f \u2032, \u03ba) for some \u03ba in \u0398. Then, the following combination rules hold:\n\u2022 Linear combination: for all \u03b1, \u03b2 > 0, \u03b1g+\u03b2g\u2032 is a majorant surrogate function in S\u03b1L+\u03b2L\u2032(\u03b1f+\u03b2f \u2032, \u03ba); \u2022 Transitivity: consider g\u2032\u2032 a majorant surrogate in SL\u2032\u2032(g, \u03ba). Then, g\u2032\u2032 is a majorant surrogate in\nSL+L\u2032\u2032(f, \u03ba); \u2022 Negation: the function g\u2032\u2032 : \u03b8 7\u2192 \u2212g(\u03b8) + L2 \u2016\u03b8 \u2212 \u03ba\u201622 is a majorant surrogate in S2L(\u2212f, \u03ba).\nProof. The first two points are easy to check. For the last one, we have for all \u03b8 in \u0398, g(\u03b8) \u2212 f(\u03b8) \u2264 L2 \u2016\u03b8 \u2212 \u03ba\u201622 according to Lemma 2.1. The proposed surrogate is therefore majorant for \u2212f . We can now define the approximation error function h\u2032\u2032 : \u03b8 7\u2192 f(\u03b8) \u2212 g(\u03b8) + L2 \u2016\u03b8 \u2212 \u03ba\u201622, which is differentiable with 2L-Lipschitz continuous gradient and g\u2032\u2032 is in S2L(\u2212f, \u03ba) (we have used the fact that \u03b8 7\u2192 L2 \u2016\u03b8\u2212 \u03ba\u201622 and h , g \u2212 f are both differentiable and their gradients are L-Lipschitz).\nIn the next paragraphs, we justify the different surrogates we have introduced in Section 2.2."}, {"heading": "Lipschitz Gradient Surrogates.", "text": "When f is differentiable and \u2207f is L-Lipschitz, we consider the following surrogate:\ng : \u03b8 7\u2192 f(\u03ba) +\u2207f(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + L 2 \u2016\u03b8 \u2212 \u03ba\u201622.\nBy applying Lemma B.1 and studying the approximation error h , g \u2212 f , we immediately obtain that g is a majorant surrogate in S2L,L(f, \u03ba). When f is convex, we can use Lemma B.9 to prove that g is in SL,L(f, \u03ba) and SL\u2212\u00b5,L(f, \u03ba) when f is \u00b5-strongly convex."}, {"heading": "Proximal Gradient Surrogates.", "text": "Assume that f splits into f = f1 + f2, where f1 is differentiable with a L-Lipschitz gradient. Then, we have presented the following surrogate\ng : \u03b8 7\u2192 f1(\u03ba) +\u2207f1(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + L\n2 \u2016\u03b8 \u2212 \u03ba\u201622 + f2(\u03b8).\nFollowing the same arguments as in the previous paragraph, we have that g is in S2L(f, \u03ba). Moreover, when f1 is convex, g is in SL(f, \u03ba). If f2 is also convex, g is in SL,L(f, \u03ba). When f1 is \u00b5-strongly convex, g is in SL\u2212\u00b5(f, \u03ba). If f2 is also convex, g is in SL\u2212\u00b5,L(f, \u03ba)."}, {"heading": "DC Programming Surrogates.", "text": "Assume that f = f1 + f2, where f2 is concave and differentiable with a L2-Lipschitz gradient. Then, we have presented the following surrogate\ng : \u03b8 7\u2192 f1(\u03b8) + f2(\u03ba) +\u2207f2(\u03ba)\u22a4(\u03b8 \u2212 \u03ba).\nIt is easy to see that g is a majorant surrogate since f2 is concave and below its tangents. It is also easy to see that the approximation error g \u2212 f has a L2-Lipschitz continuous gradient."}, {"heading": "Variational Surrogates.", "text": "Let f be a function defined on Rp1 \u00d7 Rp2 . Let \u03981 \u2286 Rp1 and \u03982 \u2286 Rp2 be two convex sets. Define f\u0303 as f\u0303(\u03b81) , min\u03b82\u2208\u03982 f(\u03b81, \u03b82) and assume that\n\u2022 \u03b81 7\u2192 f(\u03b81, \u03b82) is differentiable for all \u03b82 in \u03982; \u2022 \u03b82 7\u2192 \u22071f(\u03b81, \u03b82) is L-Lipschitz for all \u03b81 in Rp1 ; \u2022 \u03b81 7\u2192 \u22071f(\u03b81, \u03b82) is L\u2032-Lipschitz for all \u03b82 in \u03982;\n\u2022 \u03b82 7\u2192 f(\u03b81, \u03b82) is \u00b5-strongly convex for all \u03b81 in Rp1 .\nLet us fix \u03ba1 in \u03981. Then, we can show that the following function is a majorant surrogate in SL\u2032\u2032(f\u0303 , \u03ba) for some L\u2032\u2032 > 0:\ng : \u03b81 7\u2192 f(\u03b81, \u03ba\u22c62) with \u03ba\u22c62 , argmin \u03b82\u2208\u03982 f\u0303(\u03ba1, \u03b82).\nWe can indeed apply Lemma B.7 which ensures that f\u0303 is differentiable with \u2207f\u0303(\u03b81) = \u22071f(\u03b81, \u03b8\u22c62) and \u03b8\u22c62 , argmin f(\u03b81, \u03b82). Considering the approximation error function h , g \u2212 f\u0303 , we indeed have that h(\u03ba1) = 0, \u2207h(\u03ba1) = 0 and since \u03b8\u22c62 as a function of \u03b81 is Lipschitz according to Lemma B.6, we also have that \u2207h is Lipschitz continuous.\nWhen f is jointly convex in \u03b81 and \u03b82, f\u0303 is itself convex and \u2207f\u0303 is L\u2032-Lipschitz continuous according to Lemma B.7. We can then apply Lemma B.9 to obtain that \u2207h is L\u2032-Lipschitz continuous such that we can choose L\u2032\u2032 = L\u2032."}, {"heading": "Saddle Point Surrogates.", "text": "Let us make the same assumptions as in the previous paragraph with the following exceptions\n\u2022 \u03b82 7\u2192f(\u03b81, \u03b82) is \u00b5-strongly concave for all \u03b81 in Rp1 ; \u2022 \u03b81 7\u2192f(\u03b81, \u03b82) is convex for all \u03b82 in \u03982; \u2022 f\u0303(\u03b81) , max\u03b82\u2208\u03982 f(\u03b81, \u03b82).\nThen, f\u0303 is convex as the pointwise supremum of convex functions (see Boyd & Vandenberghe, 2004) and we can show that the function below is a majorant surrogate in S2L\u2032\u2032(f\u0303 , \u03ba1):\ng : \u03b81 7\u2192 f(\u03b81, \u03ba\u22c62) + L\u2032\u2032\n2 \u2016\u03b81 \u2212 \u03ba1\u201622,\nwhere L\u2032\u2032 , max(2L2/\u00b5, L\u2032). When \u03b81 7\u2192 f(\u03b81, \u03b82) is affine, we can instead choose L\u2032\u2032 , L2/\u00b5. We indeed apply the same methodology as in the previous paragraph. Lemma B.7 tells us that the function \u2212f\u0303 is differentiable with 2L2/\u00b5-Lipschitz continuous gradient (only L2/\u00b5 in the affine case). Then, we have that the function \u03b81 7\u2192 \u2212f(\u03b81, \u03ba\u22c62) is in SL\u2032\u2032(\u2212f\u0303 , \u03ba1) by using Lemma B.9. We then apply the negation rule of Lemma C.1 to conclude."}, {"heading": "Jensen Surrogates.", "text": "Let us recall the definition of Jensen surrogates. Following Lange et al. (2000), we consider a convex function f : R 7\u2192 R, a vector x in Rp and define f\u0303 : Rp \u2192 R as f\u0303(\u03b8) , f(x\u22a4\u03b8) for all \u03b8. Let w in Rp+ be a weight vector such that w \u2265 0, \u2016w\u20161 = 1 and wi 6= 0 whenever xi 6=0. Then, we consider the following function g for any \u03ba in Rp\ng : \u03b8 7\u2192 p \u2211\ni=1\nwif\n(\nxi wi (\u03b8i \u2212 \u03bai) + x\u22a4\u03ba ) ,\nAssume that f is differentiable with a L-Lipschitz gradient and wi , |xi|\u03bd/\u2016x\u2016\u03bd\u03bd for some \u03bd \u2265 0.9 \u2207f is obviously Lipschitz with constant L\u2016x\u201622. g is also convex, differentiable with Lipschitz continuous gradient with constant L\u2032 obtained below with simple calculations:\n\u2022 if \u03bd = 0, L\u2032 = L\u2016x\u20162\u221e\u2016x\u20160; \u2022 if \u03bd = 1, L\u2032 = L\u2016x\u2016\u221e\u2016x\u20161; \u2022 if \u03bd = 2, L\u2032 = L\u2016x\u201622.\nThe fact that g is majorant is a simple application of Jensen inequality. It is also obvious that g(\u03ba) = f(\u03ba) and that \u2207g(\u03ba) = \u2207f(\u03ba). We now apply Lemma B.9, noticing that we always have L\u2032 greater than L\u2016x\u201622, and we have that g is in SL\u2032(f\u0303 , \u03ba).\n9With an abuse of notation, \u2016x\u201600 denotes the \u21130-pseudo norm, also denoted by \u2016x\u20160."}, {"heading": "Quadratic Surrogates.", "text": "When f is twice differentiable and admits a matrix H in such that \u22072f \u2212 H is always positive definite, the following function is a first-order majorant surrogate:\ng : \u03b8 7\u2192 f(\u03ba) +\u2207f(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + 1 2 (\u03b8 \u2212 \u03ba)\u22a4H(\u03b8 \u2212 \u03ba).\nThe fact that it is majorant is simply an application of the mean-value theorem."}, {"heading": "D. Additional Optimization Scheme: Block Frank-Wolfe", "text": "We provide in this section an additional optimization scheme, combining the ideas of Sections 4 and 3 with separability assumptions on the surrogates gn and \u0398. It results in a block coordinate version of the Frank-Wolfe optimization scheme presented in Algorithm 6 generalizing a procedure recently introduced by Lacoste-Julien et al. (2013). More precisely, the algorithm of Lacoste-Julien et al. (2013) corresponds to using a quadratic surrogate as provided by Lemma B.1 when f is smooth with L-Lipschitz gradient, and performing a line search on the function f instead of gn. Our approach on the other hand can afford to have a non-smooth component in f and in that sense is more general. Note that Lacoste-Julien et al. (2013) also presents duality gap guarantees and various extensions and applications, which we do not consider in our paper.\nAlgorithm 6 Block Frank-Wolfe Scheme\ninput \u03b80 = (\u03b8 1 0 , . . . , \u03b8 k 0 ) \u2208 \u0398 = \u03981 \u00d7 . . .\u00d7\u0398k (initial point); N (number of iterations).\n1: for n = 1, . . . , N do 2: Compute a separable majorant surrogate function gn = \u2211k i=1 g i n in SL,L(f, \u03b8n\u22121); 3: Randomly pick one block \u0131\u0302n in {1, . . . , k} and compute a search direction:\n\u03bd \u0131\u0302nn \u2208 argmin \u03b8\u0131\u0302n\u2208\u0398\u0131\u0302n\n[\ng \u0131\u0302nn (\u03b8 \u0131\u0302n)\u2212 L\n2 \u2016\u03b8\u0131\u0302n \u2212 \u03b8\u0131\u0302nn\u22121\u201622\n]\n.\n4: Line search: \u03b1\u22c6 , argmin\n\u03b1\u2208[0,1] g \u0131\u0302nn ((1 \u2212 \u03b1)\u03b8\u0131\u0302nn\u22121 + \u03b1\u03bd \u0131\u0302nn ).\n5: Update \u03b8\u0131\u0302nn : \u03b8\u0131\u0302nn , (1\u2212 \u03b1\u22c6)\u03b8\u0131\u0302nn\u22121 + \u03b1\u22c6\u03bd \u0131\u0302nn .\n6: end for output \u03b8N = (\u03b8 1 N , . . . , \u03b8 k N ) (final estimate);\nProposition D.1 (Convergence Rate for Algorithm 6). Let f be convex, bounded below and f\u22c6 be the minimum of f on \u0398 = \u03981 \u00d7 . . .\u00d7 \u0398k. Assume that \u0398 is bounded and call R , max\u03b81,\u03b82\u2208\u0398 \u2016\u03b81 \u2212 \u03b82\u20162 its diameter. The sequence (f(\u03b8n))n\u22650 provided by Algorithm 6 converges almost surely to f\u22c6 and we have for all n \u2265 1,\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 2LR2\n2 + \u03b4(n\u2212 n0) ,\nwhere \u03b4 , 1/k and n0 , \u2308 log ( 2(f(\u03b80)\u2212f\u22c6) LR2 \u2212 1 ) / log ( 1 1\u2212\u03b4 )\u2309 if f(\u03b80)\u2212 f\u22c6 > LR2 and n0 , 0 otherwise.\nThe proof is given in Appendix E."}, {"heading": "E. Proofs of Lemmas and Propositions", "text": "We present in this section the proofs of the different lemmas and propositions in the paper."}, {"heading": "E.1. Proof of Lemma 2.1", "text": "Proof. The first inequality is a direct applications of Lemma B.1 applied to the function h at the point \u03ba when noticing that h(\u03ba) = 0 and \u2207h(\u03ba) = 0. Then, for all \u03b8 in \u0398, we have\nf(\u03b8\u2032) \u2264 g(\u03b8\u2032) \u2264 g(\u03b8) = f(\u03b8) + h(\u03b8),\nand we obtain the second inequality from the first one. When g is \u03c1-strongly convex, we can in addition exploit the second-order growth property of g presented in Lemma B.5, and obtain\nf(\u03b8\u2032) + \u03c1\n2 \u2016\u03b8\u2032 \u2212 \u03b8\u201622 \u2264 g(\u03b8\u2032) +\n\u03c1 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622 \u2264 g(\u03b8) = f(\u03b8) + h(\u03b8),\nand the third inequality follows from the second one."}, {"heading": "E.2. Proof of Proposition 2.1", "text": "Proof. The fact that (f(\u03b8n))n\u22650 is non-increasing and convergent because bounded below is clear:\nf(\u03b8n) \u2264 gn(\u03b8n) \u2264 gn(\u03b8n\u22121) = f(\u03b8n\u22121),\nwhere the first inequality and the last equality come from Definition 2.1. The second inequality comes from the definition of \u03b8n. Denote by f\n\u22c6 the limit of the sequence (f(\u03b8n))n\u22650 and by hn , gn \u2212 f the approximation error functions. The latter are differentiable and their gradient are L-Lipschitz continuous according to the definitions of the surrogate functions. Then,\nf(\u03b8n) + hn(\u03b8n) = gn(\u03b8n) \u2264 f(\u03b8n\u22121),\nand thus, by summing over n, \u221e \u2211\nn=1\nhn(\u03b8n) \u2264 f(\u03b80)\u2212 f\u22c6,\nand the non-negative sequence (hn(\u03b8n))n\u22650 necessarily converges to zero.\nWe have then two possibilities (according to the assumptions made in the proposition):\n\u2022 if the gn\u2019s are majorant surrogates, plugging \u03b8\u2032 = \u03b8n \u2212 1L\u2207hn(\u03b8n) in Lemma B.1 yields\nhn(\u03b8 \u2032) \u2264 hn(\u03b8n)\u2212\n1\n2L \u2016\u2207hn(\u03b8n)\u201622,\nand therefore,\n\u2016\u2207hn(\u03b8n)\u201622 \u2264 2L(hn(\u03b8n)\u2212 hn(\u03b8\u2032)) \u2264 2Lhn(\u03b8n) \u2212\u2192 n\u2192+\u221e 0,\nwhere we use the fact that hn(\u03b8 \u2032) \u2265 0 because gn is majorant.\n\u2022 otherwise, the functions gn are \u03c1-strongly convex and we can exploit some inequalities of Lemma 2.1. Notably,\n\u03c1 2 \u2016\u03b8n \u2212 \u03b8n\u22121\u201622 \u2264 f(\u03b8n\u22121)\u2212 f(\u03b8n).\nSumming this inequality over n yields that \u2016\u03b8n \u2212 \u03b8n\u22121\u201622 necessarily converges to zero, and\n\u2016\u2207hn(\u03b8n)\u20162 = \u2016\u2207hn(\u03b8n)\u2212\u2207hn(\u03b8n\u22121)\u20162 \u2264 L\u2016\u03b8n \u2212 \u03b8n\u22121\u20162 \u2212\u2192 n\u2192+\u221e 0,\nsince \u2207hn(\u03b8n\u22121) = 0 according to Definition 2.1.\nWe can now compute directional derivatives of f at a point \u03b8n and a direction \u03b8 \u2212 \u03b8n, where \u03b8 is in \u0398:\n\u2207f(\u03b8n, \u03b8 \u2212 \u03b8n) = \u2207gn(\u03b8n, \u03b8 \u2212 \u03b8n)\u2212\u2207hn(\u03b8n)\u22a4(\u03b8 \u2212 \u03b8n).\nNote that \u03b8n minimizes gn on \u0398 and therefore \u2207gn(\u03b8n, \u03b8 \u2212 \u03b8n) \u2265 0. Therefore,\n\u2207f(\u03b8n, \u03b8 \u2212 \u03b8n) \u2265 \u2212\u2016\u2207hn(\u03b8n)\u20162\u2016\u03b8 \u2212 \u03b8n\u20162,\nusing Cauchy-Schwarz inequality. Then,\nlim inf n\u2192+\u221e inf \u03b8\u2208\u0398 \u2207f(\u03b8n, \u03b8 \u2212 \u03b8n) \u2016\u03b8 \u2212 \u03b8n\u20162 \u2265 \u2212 lim n\u2192+\u221e \u2016\u2207hn(\u03b8n)\u20162 = 0."}, {"heading": "E.3. Proof of Proposition 2.2", "text": "Proof. We separately prove the two parts of the proposition. Non-strongly convex case: Let us define hn , gn \u2212 f the approximation error function at iteration n. From Lemma 2.1 (with g= gn, \u03ba= \u03b8n\u22121, \u03b8\u2032=\u03b8n), we have\nf(\u03b8n) \u2264 min \u03b8\u2208\u0398\n[\nf(\u03b8) + L\n2 \u2016\u03b8 \u2212 \u03b8n\u22121\u201622\n]\n.\nThen, following a similar proof technique as Nesterov (2007, Theorem 4), we have\nf(\u03b8n) \u2264 min \u03b1\u2208[0,1]\nf(\u03b1\u03b8\u22c6 + (1\u2212 \u03b1)\u03b8n\u22121) + L\u03b12\n2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622,\n\u2264 min \u03b1\u2208[0,1]\n\u03b1f(\u03b8\u22c6) + (1 \u2212 \u03b1)f(\u03b8n\u22121) + L\u03b12\n2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622,\n(10)\nwhere the minimization over \u0398 in the previous equation is replaced by a minimization on the line segment \u03b1\u03b8\u22c6 + (1 \u2212 \u03b1)\u03b8n\u22121 : \u03b1 \u2208 [0, 1]. Then, because the sequence (f(\u03b8n))n\u22650 is monotonically decreasing we can use the bounded level set assumption, which yields\nf(\u03b8n)\u2212 f\u22c6 \u2264 min \u03b1\u2208[0,1]\n(1\u2212 \u03b1)(f(\u03b8n\u22121)\u2212 f\u22c6) + LR2\u03b12\n2 .\n\u2022 if f(\u03b8n\u22121)\u2212 f\u22c6 \u2265 LR2, then we have the optimal value \u03b1\u22c6 = 1 and f(\u03b8n)\u2212 f\u22c6 \u2264 LR 2\n2 ;\n\u2022 otherwise \u03b1\u22c6 = f(\u03b8n\u22121)\u2212f \u22c6\nLR2 . Denoting by rn , f(\u03b8n)\u2212 f\u22c6, we have\nrn \u2264 rn\u22121 ( 1\u2212 rn\u22121 2LR2 ) .\nThus, r\u22121n \u2265 r\u22121n\u22121 ( 1\u2212 rn\u221212LR2 )\u22121 \u2265 r\u22121n\u22121+ 12LR2 , where the second inequality comes from the convexity inequality (1\u2212 x)\u22121 \u2265 1 + x for x \u2208 (0, 1).\nThen, we have seen that if r0 \u2265 LR2, then r1 \u2264 LR 2 2 and thus r \u22121 n \u2265 r\u221211 + n\u221212LR2 \u2265 n+32LR2 . Otherwise, we have r\u22121n \u2265 r\u221210 + n2LR2 \u2265 n+22LR2 , which is sufficient to conclude. \u00b5-strongly convex case: Let us now assume that f is \u00b5-strongly convex, and drop the bounded level sets assumption. The proof again follows Nesterov (2007) for computing the convergence rate of proximal gradient methods. We start from (10). We can then use the second-order growth property of f (Lemma B.5) which states that f(\u03b8n\u22121) \u2265 f\u22c6+ \u00b52 \u2016\u03b8n\u22121\u2212\u03b8\u22c6\u201622 and obtain\nf(\u03b8n)\u2212 f\u22c6 \u2264 (\nmin \u03b1\u2208[0,1]\n1\u2212 \u03b1+ L\u03b1 2\n\u00b5\n)\n(f(\u03b8n\u22121)\u2212 f\u22c6).\nAt this point, it is easy to show that\n\u2022 if \u00b5 \u2265 2L, then the previous binomial is minimized for \u03b1\u22c6 = 1 and\nf(\u03b8n)\u2212 f\u22c6 \u2264 L\n\u00b5 (f(\u03b8n\u22121)\u2212 f\u22c6);\n\u2022 if \u00b5 \u2264 2L, then we have \u03b1\u22c6 = \u00b52L and\nf(\u03b8n)\u2212 f\u22c6 \u2264 ( 1\u2212 \u00b5 4L ) (f(\u03b8n\u22121)\u2212 f\u22c6),\nwhich is sufficient to conclude."}, {"heading": "E.4. Proof of Proposition 2.3", "text": "Proof. We separately prove the two parts of the proposition."}, {"heading": "Non-strongly convex case:", "text": "From Lemma 2.1 (with g=gn, \u03ba=\u03b8n\u22121, \u03b8\u2032=\u03b8n, \u03b8=\u03b8\u22c6), we have\nf(\u03b8n)\u2212 f(\u03b8\u22c6) \u2264 L\n2 \u2016\u03b8n\u22121 \u2212 \u03b8\u22c6\u201622 \u2212\n\u03c1 2 \u2016\u03b8n \u2212 \u03b8\u22c6\u201622 \u2264 L 2 \u2016\u03b8n\u22121 \u2212 \u03b8\u22c6\u201622 \u2212 L 2 \u2016\u03b8n \u2212 \u03b8\u22c6\u201622. (11)\nBy summing this inequality, we have\nn(f(\u03b8n)\u2212 f(\u03b8\u22c6)) \u2264 n \u2211\nk=1\n(f(\u03b8k)\u2212 f(\u03b8\u22c6)) \u2264 L\n2 (\u2016\u03b80 \u2212 \u03b8\u22c6\u201622 \u2212 \u2016\u03b8n \u2212 \u03b8\u22c6\u201622) \u2264 L\u2016\u03b80 \u2212 \u03b8\u22c6\u201622 2 ,\nwhere the first inequality comes from the fact that f(\u03b8k) \u2265 f(\u03b8n) for all k \u2264 n. This is sufficient to prove (2.3). Note that finding telescopic sums to prove convergence rates is a classical technique (see Beck & Teboulle, 2009).\n\u00b5-strongly convex case: Let us now prove the second part of the proposition and assume that f is \u00b5-strongly convex. The strong convexity implies the second-order growth property of Lemma B.5: f(\u03b8n)\u2212f\u22c6 \u2265 \u00b52 \u2016\u03b8n\u2212\u03b8\u22c6\u201622 for all n. Combined with (11), this yields\n\u00b5+ \u03c1\n2 \u2016\u03b8n \u2212 \u03b8\u22c6\u201622 \u2264\nL 2 \u2016\u03b8n\u22121 \u2212 \u03b8\u22c6\u201622,\nand thus\nf(\u03b8n)\u2212 f(\u03b8\u22c6) \u2264 L\n2 \u2016\u03b8n\u22121 \u2212 \u03b8\u22c6\u201622 \u2264\n(\nL\n\u03c1+ \u00b5\n)n\u22121 L\u2016\u03b80 \u2212 \u03b8\u22c6\u201622\n2 ."}, {"heading": "E.5. Proof of Proposition 2.4", "text": "Proof. We separately prove the two parts of the proposition."}, {"heading": "Non-strongly convex case:", "text": "Following a similar scheme as in Proposition 2.2 and using Lemma B.3 on the approximation error functions hn instead of Lemma 2.1, we have\nf(\u03b8n) \u2264 min \u03b1\u2208[0,1]\nf(\u03b1\u03b8\u22c6 + (1\u2212 \u03b1)\u03b8n\u22121) + M\u03b13\n6 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201632, (12)\nThen, again following the proof of Proposition 2.2,\nf(\u03b8n)\u2212 f\u22c6 \u2264 min \u03b1\u2208[0,1]\n(1\u2212 \u03b1)(f(\u03b8n\u22121)\u2212 f\u22c6) + \u03b13MR3\n6 .\nDenoting by rn , f(\u03b8n)\u2212 f\u22c6 and by \u03b1\u22c6 the solution of this optimization problem, we have\n\u2022 if rn\u22121 \u2265 MR3/2, then \u03b1\u22c6 = 1 and rn \u2264 MR3/6; \u2022 otherwise, \u03b1\u22c6 = \u221a 2rn\u22121/(MR3), and\nrn \u2264 rn\u22121 ( 1\u2212 \u221a\n8rn\u22121 9MR3\n)\n.\nIt follows that r \u22121/2 n \u2265 r\u22121/2n\u22121\n(\n1\u2212 \u221a\n8rn\u22121 9MR3 )\u22121/2 \u2265 r\u22121/2n\u22121 + \u221a 2 9MR3 , where the last inequality comes from the\nconvexity inequality (1\u2212 x)\u22121/2 \u2265 1 + x/2.\nThen, we have seen that if r0 \u2265 MR3/2, then r1 \u2264 MR3/6 and thus r\u22121/2n \u2265 r\u22121/21 + (n \u2212 1) \u221a 2 9MR3 \u2265 \u221a\n2 9MR3 (n\u2212 1 + 3\n\u221a 3) \u2265 \u221a\n2 9MR3 (n+ 4). Otherwise, we have r \u22121/2 n \u2265 r\u22121/20 + n\n\u221a\n2 9MR3 \u2265\n\u221a\n2 9MR3 (n+ 3). This\nis sufficient to obtain the first part of the proposition.\n\u00b5-strongly convex case: Let us now assume that f is \u00b5-strongly convex, and drop the bounded level sets assumption. Starting again from (12),\nf(\u03b8n) \u2264 min \u03b1\u2208[0,1]\nf(\u03b1\u03b8\u22c6 + (1\u2212 \u03b1)\u03b8n\u22121) + M\u03b13\n6 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201632,\nand using the second-order growth property of f , we have\nrn \u2264 min \u03b1\u2208[0,1]\n(1\u2212 \u03b1)rn\u22121 + \u03b3\u03b13\n3 r 3/2 n\u22121.\n\u2022 when \u03b3\u221arn\u22121 \u2265 1, we have \u03b1\u22c6 = 1\u221a \u03b3 \u221a rn\u22121 and the desired inequality follows;\n\u2022 otherwise, \u03b1\u22c6 = 1 and rn \u2264 32r 3/2 n\u22121 \u2264 rn\u22121 3 ."}, {"heading": "E.6. Proof of Proposition 3.1", "text": "Proof. We proceed in several steps and adapt the convergence proof of Proposition 2.1 to our new setting."}, {"heading": "Definition of an approximate surrogate g\u0304n:", "text": "We define recursively the sequence of functions (g\u0304n)n\u22650 as follows:\ng\u0304n , g\u0304n\u22121 + g \u0131\u0302n n \u2212 g\u0304 \u0131\u0302nn\u22121,\nwhere the surrogate gn and the index \u0131\u0302n are chosen in the algorithm. We also define g\u0304\u22121 as a majorant separable surrogate function such that \u03b80 \u2208 argmin\u03b8\u2208\u0398 g\u0304\u22121(\u03b8) (we have assumed in the proposition that such a surrogate function exists). Then, it is easy to see that g\u0304n is constructed in such a way that \u03b8n is a minimizer of g\u0304n over \u0398 for all n \u2265 0 and that g\u0304n \u2265 f . Almost sure convergence of (f(\u03b8n))n\u22650 and consequences: We have f(\u03b8n) \u2264 gn(\u03b8n) = \u2211k i=1 g k n(\u03b8 k n) \u2264 \u2211k i=1 g k n(\u03b8 k n\u22121) = gn(\u03b8n\u22121) = f(\u03b8n\u22121) since we have g \u0131\u0302n n (\u03b8 \u0131\u0302n n ) \u2264 g \u0131\u0302nn (\u03b8 \u0131\u0302n n\u22121) and g i n(\u03b8 i n) = g i n(\u03b8 i n\u22121) for i 6= \u0131\u0302n. Thus, (f(\u03b8n))n\u22650 is monotonically decreasing and converges almost\nsurely. We also have\nE[g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121)] = E[g\u0304n(\u03b8n)\u2212 g\u0304n(\u03b8n\u22121)] + E[g\u0304n(\u03b8n\u22121)\u2212 g\u0304n\u22121(\u03b8n\u22121)] = E[g\u0304n(\u03b8n)\u2212 g\u0304n(\u03b8n\u22121)] + E[g \u0131\u0302nn (\u03b8\u0131\u0302nn\u22121)\u2212 g\u0304 \u0131\u0302nn\u22121(\u03b8\u0131\u0302nn\u22121)] = E[g\u0304n(\u03b8n)\u2212 g\u0304n(\u03b8n\u22121)] + E[E[g \u0131\u0302nn (\u03b8\u0131\u0302nn\u22121)\u2212 g\u0304 \u0131\u0302nn\u22121(\u03b8\u0131\u0302nn\u22121)|\u03b8n\u22121]]\n= E[g\u0304n(\u03b8n)\u2212 g\u0304n(\u03b8n\u22121)] + 1\nk E[gn(\u03b8n\u22121)\u2212 g\u0304n\u22121(\u03b8n\u22121)]\n= E[g\u0304n(\u03b8n)\u2212 g\u0304n(\u03b8n\u22121)] + 1\nk E[f(\u03b8n\u22121)\u2212 g\u0304n\u22121(\u03b8n\u22121)].\nNote that both terms g\u0304n(\u03b8n)\u2212 g\u0304n(\u03b8n\u22121) and f(\u03b8n\u22121)\u2212 g\u0304n\u22121(\u03b8n\u22121) are non-positive with probability one and thus the sequence (E[g\u0304n(\u03b8n)])n\u22650 is non-increasing, bounded below and convergent. The term E[g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121)] is therefore the summand of a converging sum, and so are E[g\u0304n(\u03b8n) \u2212 g\u0304n(\u03b8n\u22121)] and E[f(\u03b8n\u22121) \u2212 g\u0304n\u22121(\u03b8n\u22121)]. Then, we have by using Beppo-Levi theorem\n+\u221e \u2211\nn=0\nE[g\u0304n(\u03b8n)\u2212 f(\u03b8n)] = E [ +\u221e \u2211\nn=0\ng\u0304n(\u03b8n)\u2212 f(\u03b8n) ] < +\u221e.\nThus, the term g\u0304n(\u03b8n)\u2212 f(\u03b8n) converges almost surely to 0. Asymptotic stationary point conditions: Let us denote by h\u0304n , g\u0304n \u2212 f which is differentiable with L-Lipschitz continuous gradient. Then, for all \u03b8 in \u0398,\n\u2207f(\u03b8n, \u03b8 \u2212 \u03b8n) = \u2207g\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n)\u2212\u2207h\u0304n(\u03b8n)\u22a4(\u03b8 \u2212 \u03b8n).\nWe have \u2207g\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n) \u2265 0 since \u03b8n is a minimizer of g\u0304n, and \u2016\u2207h\u0304n(\u03b8n)\u201622 \u2264 2Lh\u0304n(\u03b8n), following a similar argument as in the proof of Proposition 2.1. Since we have shown that h\u0304n(\u03b8n) almost surely converges to zero, we conclude using the Cauchy-Schwarz inequality as in the proof of Proposition 2.1."}, {"heading": "E.7. Proof of Proposition 3.2", "text": "Proof. The fact that (f(\u03b8n))n\u22650 almost surely converges follows the beginning of Proposition 3.1. To show the convergence rates of (E[f(\u03b8n)])n\u22650, we adapt the proof of Proposition 2.2 to out stochastic block setting. Let us denote by \u03b8\u22c6n a minimizer of the surrogate function gn over \u0398. Since the indices \u0131\u0302n are picked up uniformly at random, we have the following conditional probabilities\nP(\u03b8in = \u03b8 \u22c6i n |\u03b8n\u22121) = \u03b4 and P(\u03b8in = \u03b8in\u22121|\u03b8n\u22121) = 1\u2212 \u03b4.\nWe can then obtain the following inequalities for all \u03b8 in \u0398\nE[f(\u03b8n)|\u03b8n\u22121] \u2264 E[gn(\u03b8n)|\u03b8n\u22121] = k \u2211\ni=1\nE[gin(\u03b8 i n)|\u03b8n\u22121] =\nk \u2211\ni=1\n(1\u2212 \u03b4)gin(\u03b8in\u22121) + \u03b4gin(\u03b8\u22c6in )\n= (1\u2212 \u03b4)gn(\u03b8n\u22121) + \u03b4gn(\u03b8\u22c6n) \u2264 (1\u2212 \u03b4)f(\u03b8n\u22121) + \u03b4gn(\u03b8) \u2264 (1\u2212 \u03b4)f(\u03b8n\u22121) + \u03b4 ( f(\u03b8) + L\n2 \u2016\u03b8 \u2212 \u03b8n\u22121\u201622\n)\n,\n(13)\nwhere we have used the conditional probabilities computed above and the fact that |gn(\u03b8)\u2212f(\u03b8)| \u2264 L2 \u2016\u03b8\u2212\u03b8n\u22121\u201622 according to Lemma 2.1. Let us now follow the proof of Proposition 2.2:\nE[f(\u03b8n)|\u03b8n\u22121] \u2264 (1 \u2212 \u03b4)f(\u03b8n\u22121) + \u03b4 (\nmin \u03b1\u2208[0,1]\nf(\u03b1\u03b8\u22c6 + (1\u2212 \u03b1)\u03b8n\u22121) + L\u03b12\n2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622\n)\n.\nWe can now proceed by considering two different cases.\nCase 1: without strong convexity: To simplify the notation, we now introduce the quantities rn , f(\u03b8n) \u2212 f\u22c6 and following again the proof of Proposition 2.2, we have\nE[rn|\u03b8n\u22121] \u2264 (1\u2212 \u03b4)rn\u22121 + \u03b4 (\nmin \u03b1\u2208[0,1]\n(1\u2212 \u03b1)rn\u22121 + LR2\u03b12\n2\n)\n.\nThe term in parenthesis on the right is a concave function of rn\u22121 as a pointwise infimum of concave functions (in fact, pointwise infimum of linear functions). By taking the expectation and using Jensen inequality, we thus have\nE[rn] \u2264 (1 \u2212 \u03b4)E[rn\u22121] + \u03b4 (\nmin \u03b1\u2208[0,1]\n(1\u2212 \u03b1)E[rn\u22121] + LR2\u03b12\n2\n)\n.\nBy following again the proof of Proposition 2.2, we have\nE[rn] \u2264 (1 \u2212 \u03b4)E[rn\u22121] + \u03b4 { LR2 2 if E[rn\u22121] > LR 2\nE[rn\u22121] ( 1\u2212 E[rn\u22121]2LR2 ) otherwise.\nWe also notice that the inequality E[rn] \u2264 (1 \u2212 \u03b4)E[rn\u22121] + \u03b4LR 2\n2 is always true. This yields after simple\ncalculations E[rn] \u2264 (1\u2212 \u03b4)nr0 + (1\u2212 (1\u2212 \u03b4)n)LR 2 2 for all n \u2265 1. We also remark that the definition of n0 in the proposition implies that (1\u2212 \u03b4)n0r0 +(1\u2212 (1\u2212 \u03b4)n0)LR 2\n2 \u2264 LR2 after some short calculations. Thus, we have for all n > n0 E[rn] \u2264 E[rn\u22121] ( 1\u2212 \u03b4E[rn\u22121]2LR2 ) and thus E[rn] \u22121 \u2265 E[rn0 ]\u22121 + (n\u2212n0)\u03b42LR2 \u2265 2+(n\u2212n0)\u03b4 2LR2 , following similar derivations as in Proposition 2.2. This is sufficient to conclude.\nCase 2: under strong convexity assumptions: We proceed similarly as in case 1, but upper-bound instead \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622 by 2rn\u22121/\u00b5. This leads us to a similar relation as in the proof of Proposition 2.2:\nE[rn] \u2264 (1\u2212 \u03b4)E[rn\u22121] + \u03b4 (\nmin \u03b1\u2208[0,1]\n1\u2212 \u03b1+ L\u03b1 2\n\u00b5\n)\nE[rn\u22121],\nand following again the proof of Proposition 2.2, we have\nE[rn] \u2264 ((1\u2212 \u03b4) + \u03b4\u03b2)E[rn\u22121],\nyielding the desired convergence rate."}, {"heading": "E.8. Proof of Proposition 3.3", "text": "Proof. The fact that (f(\u03b8n))n\u22650 almost surely converges follows the beginning of Proposition 3.1. We then separately prove the two remaining parts of the proposition.\nWithout strong convexity assumptions: Using the same notation as in the proof of Proposition 3.2, we can replace the inequality gn(\u03b8 \u22c6 n) \u2264 gn(\u03b8) in Eq. (13) by gn(\u03b8 \u22c6 n) \u2264 gn(\u03b8)\u2212 \u03c12\u2016\u03b8\u22c6n \u2212 \u03b8\u201622 (using Lemma B.5), and we obtain\nE[f(\u03b8n)|\u03b8n\u22121] \u2264 (1\u2212 \u03b4)f(\u03b8n\u22121) + \u03b4 ( f\u22c6 + L\n2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622 \u2212\n\u03c1 2 \u2016\u03b8\u22c6 \u2212 \u03b8\u22c6n\u201622\n)\n,\nWe also remark that\nE [ \u2016\u03b8\u22c6 \u2212 \u03b8n\u201622|\u03b8n\u22121 ] =\nk \u2211\ni=1\nE [ \u2016\u03b8\u22c6i \u2212 \u03b8in\u201622|\u03b8n\u22121 ] =\nk \u2211\ni=1\n(1\u2212 \u03b4)\u2016\u03b8\u22c6i \u2212 \u03b8in\u22121\u201622 + \u03b4\u2016\u03b8\u22c6i \u2212 \u03b8\u22c6in \u201622\n= (1\u2212 \u03b4)\u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622 + \u03b4\u2016\u03b8\u22c6 \u2212 \u03b8\u22c6n\u201622.\nCombining the two previous inequalities yields\nE\n[ f(\u03b8n) + \u03c1\n2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u201622\n\u2223 \u2223 \u2223 \u03b8n\u22121 ] \u2264 (1 \u2212 \u03b4)f(\u03b8n\u22121) + \u03b4f\u22c6 + (1\u2212 \u03b4)\u03c1+ \u03b4L\n2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622.\nLet us now define rn , E[f(\u03b8n)\u2212 f\u22c6]. Taking the expectation in the previous inequality gives\nrn \u2212 (1 \u2212 \u03b4)rn\u22121 \u2264 \u03b4L+ (1 \u2212 \u03b4)\u03c1\n2 E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622]\u2212\n\u03c1 2 E[\u2016\u03b8\u22c6 \u2212 \u03b8\u22c6n\u201622]\n\u2264 \u03b4L+ (1 \u2212 \u03b4)\u03c1 2 ( E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622]\u2212 E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u201622] ) .\n(14)\nSumming these inequalities and using the fact that rn \u2264 rn\u22121 yields\nn\u03b4rn + (1\u2212 \u03b4)(rn \u2212 r0) \u2264 n \u2211\nk=1\nrk \u2212 (1\u2212 \u03b4)rk\u22121 \u2264 \u03b4L+ (1 \u2212 \u03b4)\u03c1\n2 \u2016\u03b8\u22c6 \u2212 \u03b80\u201622,\nwhich gives the desired convergence rate.\nWith strong convexity assumptions: Assume now that f is \u00b5-strongly convex. To simplify the notation, we introduce the quantity \u03ben , 1 2E[\u2016\u03b8\u22c6\u2212\u03b8n\u201622]. We can now rewrite the first inequality in (14) as\nrn + \u03c1\u03ben \u2264 (1\u2212 \u03b4)rn\u22121 + ((1\u2212 \u03b4)\u03c1+ \u03b4L)\u03ben\u22121. We are going to exploit two inequalities. Since we have the second-order growth property rn \u2265 \u00b5\u03ben, for all \u03b2 in [0, 1], \u03b2rn + (\u03c1+ (1\u2212 \u03b2)\u00b5)\u03ben \u2264 (1\u2212 \u03b4)rn\u22121 + ((1\u2212 \u03b4)\u03c1+ \u03b4L)\u03ben\u22121. By choosing \u03b2 , (1\u2212\u03b4)(\u03c1+\u00b5)(1\u2212\u03b4)(\u03c1+\u00b5)+\u03b4L , it is easy to show that\n(1 \u2212 \u03b4)rn + ((1 \u2212 \u03b4)\u03c1+ \u03b4L)\u03ben \u2264 (1 \u2212 \u03b4)(\u03c1+ \u00b5) + \u03b4L\n\u03c1+ \u00b5 ((1\u2212 \u03b4)rn\u22121 + ((1\u2212 \u03b4)\u03c1+ \u03b4L)\u03ben\u22121) .\nThus, we have by induction\n(1\u2212 \u03b4)rn + ((1 \u2212 \u03b4)\u03c1+ \u03b4L)\u03ben \u2264 ( (1\u2212 \u03b4)(\u03c1+ \u00b5) + \u03b4L \u03c1+ \u00b5\n)n\n((1\u2212 \u03b4)r0 + ((1\u2212 \u03b4)\u03c1+ \u03b4L)\u03be0) ,\nand again, since \u00b5\u03ben \u2264 rn, we obtain the convergence rate of (\u03ben)n\u22650\n\u03ben \u2264 C ( (1\u2212 \u03b4)(\u03c1+ \u00b5) + \u03b4L \u03c1+ \u00b5\n)n\n\u2264 \u03b1n ( (1\u2212 \u03b4)r0 L + \u03be0 ) ,\nwhere we have defined the quantities \u03b1 , (1\u2212\u03b4)(\u03c1+\u00b5)+\u03b4L\u03c1+\u00b5 and C , (1\u2212\u03b4)r0+((1\u2212\u03b4)\u03c1+\u03b4L)\u03be0 (1\u2212\u03b4)(\u03c1+\u00b5)+\u03b4L . We now compute the convergence rate of (rn)n\u22650 by induction. Suppose that rn\u22121 \u2264 C\u2032\u03b1n\u22122 for some constant C\u2032 and some n \u2265 2. We have shown in (14) that rn \u2264 (1 \u2212 \u03b4)rn\u22121 + L\u03ben\u22121. By using the induction hypothesis, we have rn \u2264 ((1\u2212\u03b4)C\u2032/\u03b1+LC)\u03b1n\u22121. We therefore study under which conditions we have both ((1\u2212\u03b4)C\u2032/\u03b1+LC) \u2264 C\u2032 and r1 \u2264 C\u2032, which are sufficient conditions to have by induction rn \u2264 C\u2032\u03b1n\u22121 for all n. It is easy to show that the quantity C\u2032 , (1\u2212\u03b4)r0+((1\u2212\u03b4)\u03c1+\u03b4L)\u03be0\u03b4 satisfies such conditions."}, {"heading": "E.9. Proof of Proposition 4.1", "text": "Proof. We have from the strong convexity of gn:\nf(\u03b8n) \u2264 gn(\u03b8n) \u2264 min \u03b1\u2208[0,1]\n(1\u2212 \u03b1)gn(\u03b8n\u22121) + \u03b1gn(\u03bdn)\u2212 L\n2 \u03b1(1 \u2212 \u03b1)\u2016\u03b8n\u22121 \u2212 \u03bdn\u201622,\nwhere \u03bdn is defined in Algorithm 3. Let us now consider \u03b8 \u22c6 such that f(\u03b8\u22c6) = f\u22c6. Then, we have\nf(\u03b8n) \u2264 min \u03b1\u2208[0,1]\n(1 \u2212 \u03b1)f(\u03b8n\u22121) + \u03b1 ( gn(\u03bdn)\u2212 L\n2 \u2016\u03bdn \u2212 \u03b8n\u22121\u201622\n)\n+ L\n2 (\u03b1\u2212 \u03b1(1 \u2212 \u03b1))\u2016\u03bdn \u2212 \u03b8n\u22121\u201622.\n\u2264 min \u03b1\u2208[0,1]\n(1 \u2212 \u03b1)f(\u03b8n\u22121) + \u03b1 ( gn(\u03b8 \u22c6)\u2212 L\n2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622\n)\n+ \u03b12LR2\n2\n\u2264 min \u03b1\u2208[0,1]\n(1 \u2212 \u03b1)f(\u03b8n\u22121) + \u03b1f\u22c6 + \u03b12LR2\n2 ,\n(15)\nwhere we have first used the equality gn(\u03b8n\u22121) = f(\u03b8n\u22121), then the second inequality exploits gn(\u03bdn)\u2212 L2 \u2016\u03bdn \u2212 \u03b8n\u22121\u201622 \u2264 gn(\u03b8\u22c6) \u2212 L2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622 from the definition of \u03bdn. Finally, we use the fact that gn(\u03b8\u22c6) = f\u22c6 + hn(\u03b8\u22c6) where hn is the approximation error function gn \u2212 f with |hn(\u03b8\u22c6)| \u2264 L2 \u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622 is ensured by Lemma 2.1. Minimizing (15) with respect to \u03b1 and denoting by rn , f(\u03b8n)\u2212 f\u22c6 yields\nrn \u2264 { LR2 2 if rn\u22121 \u2264 LR2 rn\u22121 ( 1\u2212 rn\u221212LR2 ) otherwise .\nThese are the same relations used in the proof of Proposition 2.2, leading therefore to the same convergence rate."}, {"heading": "E.10. Proof of Proposition 5.1", "text": "Proof. We follow the proof techniques introduced by Nesterov (2004) using the so called \u201cestimate sequences\u201d, and more precisely we adapt the proof of Nesterov (2004, Theorem 2.2.8) to deal with our surrogate functions."}, {"heading": "Preliminaries:", "text": "We rely heavily on Lemma 2.1, which we recall and expand here. Let us define \u03c1 , L+ \u00b5. Then, for all \u03b8 in \u0398,\nf(\u03b8n) \u2264 f(\u03b8) + L\n2 \u2016\u03b8 \u2212 \u03ban\u22121\u201622 \u2212\n\u03c1 2 \u2016\u03b8 \u2212 \u03b8n\u201622\n= f(\u03b8) + L\n2 \u2016\u03b8 \u2212 \u03ban\u22121\u201622 \u2212\n\u03c1 2 \u2016\u03b8 \u2212 \u03ban\u22121 + \u03ban\u22121 \u2212 \u03b8n\u201622\n= f(\u03b8)\u2212 \u00b5 2 \u2016\u03b8 \u2212 \u03ban\u22121\u201622 \u2212 \u03c1 2 \u2016\u03b8n \u2212 \u03ban\u22121\u201622 + \u03c1(\u03b8 \u2212 \u03ban\u22121)\u22a4(\u03b8n \u2212 \u03ban\u22121).\n(16)\nTo simplify the notation in the sequel, we introduce the quantity \u03ben , \u03b8n \u2212 \u03ban\u22121, which Nesterov (2004) calls \u201cgradient mapping\u201d, up to a multiplicative constant. Then, (16) can be rewritten\nf(\u03b8n) \u2264 f(\u03b8)\u2212 \u00b5\n2 \u2016\u03b8 \u2212 \u03ban\u22121\u201622 \u2212\n\u03c1 2 \u2016\u03ben\u201622 + \u03c1(\u03b8 \u2212 \u03ban\u22121)\u22a4\u03ben. (17)\nDefinition of the estimate sequence by induction: Keeping in mind this key quantity, let us now proceed by induction to prove the main result. The recursion hypothesis Hn for n \u2265 1 is the existence of a function g\u0304n : Rp \u2192 R such that\n\n  \n  \ng\u0304n(\u03b8) = g\u0304 \u22c6 n + \u03b3n 2 \u2016\u03b8 \u2212 vn\u201622 g\u0304n(\u03b8) \u2264 f(\u03b8) + An2 \u2016\u03b8 \u2212 \u03b80\u201622 \u2200\u03b8 \u2208 \u0398 f(\u03b8n) \u2264 g\u0304\u22c6n (\u03c1an + \u03b3n)\u03ban = \u03c1an\u03b8n + \u03b3nvn , (Hn)\nfor some vn and some values An, \u03b3n recursively defined as follows: Ak = Ak\u22121(1 \u2212 ak\u22121) and \u03b3k = (1 \u2212 ak\u22121)\u03b3k\u22121 + \u00b5ak\u22121 for all k \u2265 2, and A1 = L, \u03b31 = \u03c1. We recall that the scalars ak are also defined in the algorithm. The functions g\u0304n which we are going to recursively define are related to the \u201cestimate sequences\u201d introduced by Nesterov (2004). Along with the quantity An, they indeed reflect the convergence rate of the algorithm, since Hn implies that f(\u03b8n)\u2212 f\u22c6 \u2264 An2 \u2016\u03b8\u22c6 \u2212 \u03b80\u201622."}, {"heading": "Initialization of the induction for n = 1:", "text": "Let us first initialize the induction, by showing that H1 is true. We remark that A1 = L and \u03b31 = \u03c1 are chosen such that We can thus define\ng\u03041(\u03b8) = f(\u03b81) + \u03b31 2 \u2016\u03b8 \u2212 \u03b81\u201622.\nIn other words, we define v1 , \u03b81 and g\u0304 \u22c6 1 , f(\u03b81), and we obviously have the first and third conditions of H1. The second one is simply an application of Lemma 2.1, when noticing that \u03ba0 = \u03b80. The last condition is also satisfied because \u03ba1 = \u03b81 = v1 (since \u03b21 = 0 in the algorithm)."}, {"heading": "Induction argument:", "text": "Since we have shown that H1 is true, we now assume Hn\u22121 for n \u2265 2 and show Hn. We define g\u0304n : Rp \u2192 R such that for all \u03b8 in Rp\ng\u0304n(\u03b8) = (1 \u2212 an\u22121)g\u0304n\u22121(\u03b8) + an\u22121 ( f(\u03b8n) + \u00b5\n2 \u2016\u03b8 \u2212 \u03ban\u22121\u201622 +\n\u03c1 2 \u2016\u03ben\u201622 \u2212 \u03c1(\u03b8 \u2212 \u03ban\u22121)\u22a4\u03ben ) . (18)\nBecause of (17), the term between parenthesis on the right is smaller than f(\u03b8) and thus, we have g\u0304n(\u03b8) \u2264 f(\u03b8) + (1 \u2212 an\u22121)An\u221212 \u2016\u03b8 \u2212 \u03b80\u201622 = f(\u03b8) + An2 \u2016\u03b8 \u2212 \u03b80\u201622, by definition of An. Thus, the second condition of Hn is true. The function g\u0304n is moreover quadratic and the first condition is easy to check (using appropriate values for g\u0304\u22c6n and vn). Let us now check the third condition, namely that f(\u03b8n) \u2264 min\u03b8\u2208\u0398 g\u0304n(\u03b8). We first remark that\ng\u0304n\u22121(\u03b8) = g\u0304 \u22c6 n\u22121 + \u03b3n\u22121 2 \u2016\u03b8 \u2212 vn\u22121\u201622\n\u2265 f(\u03b8n\u22121) + \u03b3n\u22121 2 \u2016\u03b8 \u2212 vn\u22121\u201622 \u2265 f(\u03b8n) + \u03c1\n2 \u2016\u03ben\u201622 \u2212 \u03c1(\u03b8n\u22121 \u2212 \u03ban\u22121)\u22a4\u03ben + \u03b3n\u22121 2 \u2016\u03b8 \u2212 vn\u22121\u201622,\nThe first inequality comes from the induction hypothesis Hn\u22121 and the second inequality comes from (17). Then, we can combine this inequality with (18).\ng\u0304n(\u03b8) \u2265 f(\u03b8n) + \u03c1 2 \u2016\u03ben\u201622 \u2212 (1\u2212 an)\u03c1(\u03b8n\u22121 \u2212 \u03ban\u22121)\u22a4\u03ben +B(\u03b8), (19)\nwhere\nB(\u03b8) , (1 \u2212 an\u22121)\u03b3n\u22121\n2 \u2016\u03b8 \u2212 vn\u22121\u201622 +\nan\u22121\u00b5\n2 \u2016\u03b8 \u2212 \u03ban\u22121\u201622 \u2212 \u03c1an\u22121(\u03b8 \u2212 \u03ban\u22121)\u22a4\u03ben.\nNote that B(\u03b8) is also the part of g\u0304n dependent on \u03b8, and such that vn = argmin\u03b8\u2208Rp B(\u03b8). Minimizing B(\u03b8) yields\nvn = 1\n\u03b3n ((1\u2212 an\u22121)\u03b3n\u22121vn\u22121 + an\u22121\u00b5\u03ban\u22121) + \u03c1an\u22121 \u03b3n \u03ben, (20)\nwhere we recall that \u03b3n = (1\u2212 an\u22121)\u03b3n\u22121 + \u00b5an\u22121. Moreover, we have the convexity inequality\nB(\u03b8) = \u03b3n 2\n(\n(1\u2212 an\u22121)\u03b3n\u22121 \u03b3n \u2016\u03b8 \u2212 vn\u22121\u201622 + an\u22121\u00b5 \u03b3n \u2016\u03b8 \u2212 \u03ban\u22121\u201622\n)\n\u2212 \u03c1an\u22121(\u03b8 \u2212 \u03ban\u22121)\u22a4\u03ben\n\u2265 \u03b3n 2\n\u2225 \u2225 \u2225 \u2225 \u03b8 \u2212 ( (1\u2212 an\u22121)\u03b3n\u22121 \u03b3n vn\u22121 + an\u22121\u00b5 \u03b3n \u03ban\u22121 )\u2225 \u2225 \u2225 \u2225 2\n2\n\u2212 \u03c1an\u22121(\u03b8 \u2212 \u03ban\u22121)\u22a4\u03ben.\nand thus, using the closed form of vn computed in (20), we have\nB(vn) \u2265 \u03b3n 2\n\u2225 \u2225 \u2225 \u2225 \u03c1an\u22121 \u03b3n \u03ben \u2225 \u2225 \u2225 \u2225 2\n2\n\u2212 \u03c1an\u22121(1\u2212 an\u22121)\u03b3n\u22121 \u03b3n (vn\u22121 \u2212 \u03ban\u22121)\u22a4 \u03ben \u2212 \u03c12a2n\u22121 \u03b3n \u2016\u03ben\u201622\n= \u2212\u03c1 2a2n\u22121 2\u03b3n \u2016\u03ben\u201622 \u2212 \u03c1an\u22121(1\u2212 an\u22121)\u03b3n\u22121 \u03b3n (vn\u22121 \u2212 \u03ban\u22121)\u22a4 \u03ben.\nWe can now obtain the following lower-bound on g\u0304\u22c6n , min\u03b8\u2208Rp g\u0304n(\u03b8), plugging the value of B(vn) into (19),\ng\u0304\u22c6n \u2265 f(\u03b8n) + ( \u03c1 2 \u2212 \u03c1 2a2n\u22121 2\u03b3n ) \u2016\u03ben\u201622 \u2212 (1\u2212 an\u22121)\u03c1 ( \u03b8n\u22121 \u2212 \u03ban\u22121 + an\u22121\u03b3n\u22121 \u03b3n (vn\u22121 \u2212 \u03ban\u22121) )\u22a4 \u03ben.\nGiven the definitions of \u03b3n and an, and the fact that \u03c1a 2 0 = \u03b31, we also obviously have the relation \u03c1a 2 n\u22121 = \u03b3n for all n \u2265 0. This cancels the factor in front of \u2016\u03ben\u201622. It is also easy to show that the fourth condition of Hn\u22121 implies \u03b8n\u22121 \u2212 \u03ban\u22121 + an\u22121\u03b3n\u22121\u03b3n (vn\u22121 \u2212 \u03ban\u22121) = 0. Since we have shown the three first conditions of Hn, it remains to show the last one, namely that (\u03c1an+\u03b3n)\u03ban = \u03c1an\u03b8n + \u03b3nvn. We first remark that (20) can be rewritten\n\u03b3nvn = (1 \u2212 an\u22121)\u03b3n\u22121vn\u22121 + an\u22121(\u00b5\u2212 \u03c1)\u03ban\u22121 + \u03c1an\u22121\u03b8n.\nCombining with the fourth condition of Hn\u22121, we have\n\u03b3nvn = (1\u2212 an\u22121) ((\u03c1an\u22121 + \u03b3n\u22121)\u03ban\u22121 \u2212 \u03c1an\u22121\u03b8n\u22121) + an\u22121(\u00b5\u2212 \u03c1)\u03ban\u22121 + \u03c1an\u22121\u03b8n = \u2212(1\u2212 an\u22121)an\u22121\u03c1\u03b8n\u22121 + \u03c1an\u22121\u03b8n\n= \u03b3n\n(\n\u03b8n\u22121 + 1\nan\u22121 (\u03b8n \u2212 \u03b8n\u22121)\n)\n,\nwhere we use the relation \u03c1a2n\u22121 = \u03b3n and the recursive relation between \u03b3n and \u03b3n\u22121 to remove the terms depending on \u03ban\u22121 in the first equation. Now that we have a simple form describing vn, we can finally show,\n\u03c1an\u03b8n + \u03b3nvn \u03c1an + \u03b3n = \u03b8n + \u03b3n(1/an\u22121 \u2212 1) \u03c1an + \u03b3n (\u03b8n \u2212 \u03b8n\u22121).\nAnd some simple computation shows that the right part of this equation is equal to \u03ban. In other words, the factor in front of (\u03b8n \u2212 \u03b8n\u22121) is equal to \u03b2n, and the last condition of Hn is satisfied."}, {"heading": "Obtaining the convergence rate:", "text": "Since Hn is true for all n \u2265 1, we have f(\u03b8n)\u2212f\u22c6 \u2264 An2 \u2016\u03b8\u22c6\u2212\u03b80\u201622 and thus it remains to compute the convergence rate of the sequence An to prove the main result. We follow here the proof of Nesterov (2004, Lemma 2.2.4). Let us first look at the case \u00b5 = 0. It is easy to show by induction that for all n \u2265 1, we have An = La2n\u22121. Moreover\n1 an \u2212 1 an\u22121 = an\u22121 \u2212 an an\u22121an = a2n\u22121 \u2212 a2n an\u22121an(an\u22121 + an) = a2n\u22121an an\u22121an(an\u22121 + an) = an\u22121 an\u22121 + an \u2265 1 2\nwhere we use the relation a2n = (1\u2212 an)a2n\u22121 and and the fact that an \u2264 an\u22121 for all n \u2265 1. Thus, we have\n1 an \u2212 1 a0 = 1 an \u2212 1 \u2265 n 2 ,\nand an \u2264 2/(n+ 2). Since An = La2n\u22121, this gives us the desired convergence rate. When \u00b5 > 0, we have the relation a2n = (1 \u2212 an)a2n\u22121 \u2212 \u00b5\u03c1an. It is then easy to show by induction that for all n \u2265 0, we have an \u2265 \u221a \u00b5 \u03c1 . Thus, An \u2264 ( 1\u2212 \u221a \u00b5 \u03c1 )n\u22121 A1. Since A1 = L, we have obtain the second convergence rate."}, {"heading": "E.11. Proof of Proposition 6.1", "text": "Proof. The proof is very similar to the one of Proposition 3.1. We proceed in several steps."}, {"heading": "Almost sure convergence of f(\u03b8n):", "text": "Let us denote by g\u0304n , 1 T \u2211T t=1 g t n. We have the following recursion relation\ng\u0304n = g\u0304n\u22121 + g t\u0302n n \u2212 gt\u0302nn\u22121,\nwhere the surrogates and the index t\u0302n are chosen in the algorithm. This allows us to obtain the following inequalities, which hold with probability one\ng\u0304n(\u03b8n) \u2264 g\u0304n(\u03b8n\u22121) = g\u0304n\u22121(\u03b8n\u22121) + gt\u0302nn (\u03b8n\u22121)\u2212 gt\u0302nn\u22121(\u03b8n\u22121) = g\u0304n\u22121(\u03b8n\u22121) + f t\u0302n(\u03b8n\u22121)\u2212 gt\u0302nn\u22121(\u03b8n\u22121) \u2264 g\u0304n\u22121(\u03b8n\u22121).\nThe first inequality is true by definition of \u03b8n and the second one because g\u0304 t\u0302n n\u22121 is a majorant surrogate of f t\u0302n . The sequence (g\u0304n(\u03b8n))n\u22650 is thus monotonically decreasing, bounded below with probability one and thus converges almost surely. Note now that the previous inequalities imply\nE[g\u0304n(\u03b8n)]\u2212 E[g\u0304n\u22121(\u03b8n\u22121)] \u2264 E[f t\u0302n(\u03b8n\u22121)\u2212 gt\u0302nn\u22121(\u03b8n\u22121)]. (21)\nThe non-positive term E[g\u0304n(\u03b8n)] \u2212 E[g\u0304n\u22121(\u03b8n\u22121)] is the summand of a converging sum. Thus, the non-positive\nterms E[f t\u0302n(\u03b8n\u22121)\u2212 gt\u0302nn\u22121(\u03b8n\u22121)] is also the summand of a converging sum and we have\nE\n[ +\u221e \u2211\nn=0\ngt\u0302n+1n (\u03b8n)\u2212 f t\u0302n+1(\u03b8n) ] = +\u221e \u2211\nn=0\nE[gt\u0302n+1n (\u03b8n)\u2212 f t\u0302n+1(\u03b8n)]\n=\n+\u221e \u2211\nn=0\nE[E[gt\u0302n+1n (\u03b8n)\u2212 f t\u0302n+1(\u03b8n)|\u03b8n]]\n=\n+\u221e \u2211\nn=0\nE[g\u0304n(\u03b8n)\u2212 f(\u03b8n)]\n= E\n[ +\u221e \u2211\nn=0\ng\u0304n(\u03b8n)\u2212 f(\u03b8n) ] < +\u221e,\nwhere we use two times Beppo-Le\u0301vy theorem to exchange the expectation and the sum signs in front of nonnegative quantities. As a result, the term g\u0304n(\u03b8n)\u2212 f(\u03b8n) converges almost surely to 0, implying the almost sure convergence of f(\u03b8n).\nAsymptotic stationary point conditions: Let us denote by h\u0304n , g\u0304n \u2212 f which is differentiable with L-Lipschitz continuous gradient. Then, for all \u03b8 in \u0398,\n\u2207f(\u03b8n, \u03b8 \u2212 \u03b8n) = \u2207g\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n)\u2212\u2207h\u0304n(\u03b8n)\u22a4(\u03b8 \u2212 \u03b8n).\nWe have \u2207g\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n) \u2265 0 by definition of \u03b8n, and \u2016\u2207h\u0304n(\u03b8n)\u201622 \u2264 2Lh\u0304n(\u03b8n), following a similar argument as in the proof of Proposition 2.1. Since we have shown that h\u0304n(\u03b8n) almost surely converges to zero, we conclude as in the proof of Proposition 3.1."}, {"heading": "E.12. Proof of Proposition 6.2", "text": "Proof. The almost sure convergence of f(\u03b8n) was shown in Proposition 6.1. We now prove the proposition in several steps and start with some preliminaries."}, {"heading": "Preliminaries:", "text": "Let us denote by \u03batn\u22121 the point in \u0398 such that g t n is in SL,\u03c1(f t, \u03batn\u22121). We remark that such points are drawn according to the following conditional probability distribution:\nP(\u03batn\u22121 = \u03b8n\u22121|\u03b8n\u22121) = \u03b4 and P(\u03batn\u22121 = \u03batn\u22122|\u03b8n\u22121) = 1\u2212 \u03b4,\nwhere \u03b4 , 1/T . Thus we have for all t in {1, . . . , T } and all n \u2265 1,\nE[\u2016\u03b8\u22c6 \u2212 \u03batn\u22121\u201622] = E[E[\u2016\u03b8\u22c6 \u2212 \u03batn\u22121\u201622|\u03b8n\u22121]] = \u03b4E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u22121\u201622] + (1\u2212 \u03b4)E[\u2016\u03b8\u22c6 \u2212 \u03batn\u22122\u201622]. (22)\nThe other relation we need is an extension of Lemma 2.1 to the incremental setting. For all \u03b8 in \u0398, we have\nf(\u03b8n) \u2264 f(\u03b8) + 1\nT\nT \u2211\nt=1\n(\nL 2 \u2016\u03b8 \u2212 \u03batn\u22121\u201622 \u2212 \u03c1 2 \u2016\u03b8 \u2212 \u03b8n\u201622\n)\n. (23)\nThe proof of this relation is similar to that of Lemma 2.1, exploiting to \u03c1-strong convexity of g\u0304n. We can now study the first part of the proposition."}, {"heading": "Monotonic decrease of E[f(\u03b8n)]:", "text": "Note that E[gt\u0302nn\u22121(\u03b8n\u22121)] = E[E[g t\u0302n n\u22121(\u03b8n\u22121)|\u03b8n\u22121]] = E[g\u0304n\u22121(\u03b8n\u22121)]. Applying this relation to Eq. (21), we have\nE[f(\u03b8n)] \u2264 E[g\u0304n(\u03b8n)] \u2264 E[f t\u0302n(\u03b8n\u22121)] = E[E[f t\u0302n(\u03b8n\u22121)|\u03b8n\u22121]] = E[f(\u03b8n\u22121)],\nwhere the first inequality comes from the fact that f \u2264 g\u0304n (see proof of Proposition 6.1).\nNon-strongly convex case (\u03c1 = L); convergence rate: Denote by An , E[ 1 2T \u2211T t=1 \u2016\u03b8\u22c6 \u2212 \u03batn\u201622] and by \u03ben , 12E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u201622]. Then, we have from (23) and by taking the expectation\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 LAn\u22121 \u2212 L\u03ben.\nIt follows from (22) that An = \u03b4\u03ben + (1\u2212 \u03b4)An\u22121 and thus\nE[f(\u03b8n)\u2212 f\u22c6] \u2264 L\n\u03b4 (An\u22121 \u2212An).\nBy summing the above inequalities, and using the fact that E[f(\u03b8n)\u2212 f\u22c6] is monotonically decreasing, we obtain that\nnE[f(\u03b8n)\u2212 f\u22c6] \u2264 LA0 \u03b4 ,\nleading to the convergence rate of Eq. (6.2), since A0 = 1 2\u2016\u03b8\u22c6 \u2212 \u03b80\u201622.\n\u00b5-strongly convex case: Suppose now that f is \u00b5-strongly convex. We will prove the proposition by induction. Assume that for some n \u2265 1, we have An\u22121 \u2264 \u03b2n\u22121\u03be0 with \u03b2 , (1\u2212\u03b4)(\u03c1+\u00b5)+\u03b4L\u03c1+\u00b5 . We have from (23) and the second-order growth condition of Lemma B.5\n\u00b5\u03ben \u2264 E[f(\u03b8n)\u2212 f\u22c6] \u2264 LAn\u22121 \u2212 \u03c1\u03ben,\nwhich is true for all n \u2265 1. Combining the previous inequality, Eq. (22), and the induction hypothesis, we have\nAn = \u03b4\u03ben + (1\u2212 \u03b4)An\u22121 \u2264 ( \u03b4L\n\u00b5+ \u03c1 + (1\u2212 \u03b4)\n)\n\u03b2n\u22121\u03be0 = \u03b2 n\u03be0.\nSince we have A0 = \u03be0, the induction hypothesis is true for all n \u2265 0. Since we have from (23) \u03ben \u2264 L\u03c1+\u00b5An\u22121, and E[f(\u03b8n)\u2212 f\u22c6] \u2264 LAn\u22121, we finally have shown the desired convergence rate (6.2)."}, {"heading": "E.13. Proof of Proposition D.1", "text": "Proof. Let us denote by \u03bd\u22c6n , argmin\u03b8\u2208\u0398 [ gn(\u03b8) \u2212 L2 \u2016\u03b8 \u2212 \u03b8n\u22121\u201622 ]\n. Because of the separability of the surrogate function gn, we have after a few calculations\nE[f(\u03b8n)|\u03b8n\u22121] \u2264 E[gn(\u03b8n)|\u03b8n\u22121] \u2264 (1\u2212 \u03b4)f(\u03b8n\u22121) + \u03b4 min \u03b1\u2208[0,1] gn((1 \u2212 \u03b1)\u03b8n\u22121 + \u03b1\u03bd\u22c6n).\nFollowing the proof of Proposition 4.1, we have\nE[f(\u03b8n)|\u03b8n\u22121] \u2264 (1\u2212 \u03b4)f(\u03b8n\u22121) + \u03b4 min \u03b1\u2208[0,1]\n[\n(1 \u2212 \u03b1)f(\u03b8n\u22121) + \u03b1f\u22c6 + \u03b12LR2\n2\n]\n.\nTaking the expectation and defining rn , E[f(\u03b8n)\u2212 f\u22c6], we have\nrn \u2264 (1\u2212 \u03b4)rn\u22121 + \u03b4 min \u03b1\u2208[0,1]\n(1\u2212 \u03b1)rn\u22121 + \u03b12LR2\n2 ,\nwhere we have used Jensen inequality similarly as in the proof of Proposition 3.2.\nMinimizing with respect to \u03b1 yields\nrn \u2264 (1\u2212 \u03b4)rn\u22121 + \u03b4 { LR2 2 if rn\u22121 > LR 2\nrn\u22121 ( 1\u2212 rn\u221212LR2 ) otherwise.\nThis is the same recursive relations as in the proof of Proposition 3.2, and we therefore obtain the same convergence rate."}, {"heading": "F. Additional Experimental Results", "text": "Figures 3 and 4 presents benchmarks for \u21132-logistic regressions with a different regularization parameter than Figure 1. Similarly, we present \u21131-logistic regressions benchmarks in Figures 5 and 6 with a different sparsity level than Figure 2."}, {"heading": "Supplementary References", "text": "Boyd, S.P. and Vandenberghe, L. Convex Optimization. Cambridge University Press, 2004.\nClarke, F.H. Optimization and Nonsmooth Analysis. John Wiley, 1983.\nDanskin, J. M. The theory of max-min, and its application to weapons allocation problems. O\u0308konometrie und Unternehmensforschung, 1967.\nNocedal, J. and Wright, S.J. Numerical optimization. Springer Verlag, 2006. 2nd edition."}], "references": [{"title": "Optimization and Nonsmooth Analysis", "author": ["F.H. Clarke"], "venue": "John Wiley,", "citeRegEx": "Clarke,? \\Q1983\\E", "shortCiteRegEx": "Clarke", "year": 1983}, {"title": "The theory of max-min, and its application to weapons allocation problems", "author": ["J.M. Danskin"], "venue": "O\u0308konometrie und Unternehmensforschung,", "citeRegEx": "Danskin,? \\Q1967\\E", "shortCiteRegEx": "Danskin", "year": 1967}, {"title": "Numerical optimization", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The proof exploits some results from nonsmooth analysis developed by Clarke (1983). We essentially use a mean value theorem for multi-dimensional Lipschitz functions (Clarke, 1983, Proposition 2.", "startOffset": 69, "endOffset": 83}, {"referenceID": 1, "context": "Note that this lemma is a variant of a theorem introduced by Danskin (1967). We first prove the differentiability of f before detailing how to obtain the Lipschitz constants.", "startOffset": 61, "endOffset": 76}], "year": 2013, "abstractText": "In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions. First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or FrankWolfe algorithms. Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.", "creator": "LaTeX with hyperref package"}}}