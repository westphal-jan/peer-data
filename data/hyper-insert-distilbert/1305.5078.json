{"id": "1305.5078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2013", "title": "A Comparison of Random Forests and Ferns on Recognition of Instruments in Jazz Recordings", "abstract": "like in this seminal paper, terms we gave first obviously apply compressed random ferns for semantic classification of real music record recordings recordings of a standard jazz band. no initial log segmentation structure of audio sensor data signal is assumed, noise i. e e., no onset, size offset, sampling nor approximate pitch transformation data are needed. today the notion of random tree ferns is described in basically the revised paper, continuing to familiarize us the reader with this classification algorithm, which was introduced some quite recently and applied so until far in optical image recognition tasks. consider the cognitive performance coefficient of random ferns is compared with random forests for the same data. also the results of experiments are presented in the paper, and conclusions are drawn.", "histories": [["v1", "Wed, 22 May 2013 10:43:25 GMT  (34kb,D)", "http://arxiv.org/abs/1305.5078v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR cs.SD", "authors": ["alicja a wieczorkowska", "miron b kursa"], "accepted": false, "id": "1305.5078"}, "pdf": {"name": "1305.5078.pdf", "metadata": {"source": "CRF", "title": "A Comparison of Random Forests and Ferns on Recognition of Instruments in Jazz Recordings", "authors": ["Alicja A. Wieczorkowska", "Miron B. Kursa", "M.B. Kursa"], "emails": ["alicja@poljap.edu.pl", "M.Kursa@icm.edu.pl"], "sections": [{"heading": null, "text": "Keywords: Music Information Retrieval, Random Ferns, Random Forest\nThis is a preprint of a paper accepted for the ISMIS 2012 conference."}, {"heading": "1 Introduction", "text": "The pleasure of listening to music can be very enjoyable, especially if our favorite instruments are playing in the piece of music we are listening to. Therefore, it is desirable to have a tool to find melodies played by a specified instrument. The task of automatic identification of an instrument, playing in a given audio segment, lies within the area of interest of Music Information Retrieval. This area has been broadly explored last years [19], [22], and as a result we can enjoy finding pieces of music through query-by-humming [14], and identify music through query-by-example, including excerpts replayed on mobile devices [21], [24]. However, recognition of instruments in real polyphonic recordings is still a challenging task (see e.g. [4], [6], [7]).\nIn this paper, we address the recognition of plural instruments in real music recordings of a jazz band, and our goal is to identify possibly all instruments playing in each audio frame; polyphony in these recordings reaches 4 instruments.\nar X\niv :1\n30 5.\n50 78\nv1 [\ncs .L\nG ]\n2 2\nM ay\nIdentification of instruments is performed in short frames, with no assumption on onset (start) nor offset (end) time, nor pitch etc., which is often the case in similar research, thus our methodology requires no preprocessing nor initial segmentation of the data, and the computation can be fast.\nRandom ferns are classifiers introduced in 2007 [17] and named as such in 2008 [18]. This classification method combines features of decision trees and Bayesian classifiers. Random ferns have been applied so far in image classification tasks, including video data [1], [16], and they have also been adjusted to be used on low-end embedded platforms, such as mobile phones [25]. Since many audio applications are used in mobile environment, it is advisable to consider such platforms as well. This is why we decided to use random ferns. Additionally, we would like to compare the performance of Random Ferns (RFe) with Random Forests (RFo), which yielded quite good results in our previous research [7], [8], [9]. RFe are simpler and more computationally efficient than RFo [10]. We want to use a simpler algorithm because, as more computationally efficient, it can possibly be applied to be used on mobile devices, with limited computational power (utilizing slower CPUs and working on battery power). We hope that the accuracy of RFe is not much worse, and therefore it is worth using them and possibly implement on mobile devices, to get quick results without communication with a cloud for cloud computing (which is an option which can be chosen for low-end platforms), thus achieving low latency. Also, such a method would be useful for massive calculations for indexing purposes, e.g. in archives, to achieve fast computation and get quick results which are a good approximation of the results that would be obtained using more computationally expensive search."}, {"heading": "2 Classifiers", "text": "The classifiers applied in our research include random ferns and random forests. RFo performed quite well in the research on instrument identification we performed before [7], but their training is time consuming, whereas the training of RFe is faster. The computational complexity of classification performed using the pre-trained classifiers is similar (linearly proportional to the number of trees/ferns and to their average height), but in the case of ferns there is less branching and memory accesses which should yield faster classification."}, {"heading": "2.1 Random Forests", "text": "RFo is a classifier consisting of a set of weak, weakly correlated and non-biased decision trees, constructed using a procedure minimizing bias and correlations between individual trees [2]. Each tree is built using a different N -element bootstrap sample of the training N -element set. The elements of the bootstrap sample are drawn with replacement from the original set, so roughly 1/3 (called out-ofbag) of the training data are not used in the bootstrap sample for any given tree. For a P -element feature vector, K attributes (features) are randomly selected at each stage of tree building, i.e. for each node of any particular tree in RFo\n(K < P , often K = \u221a P ). Gini impurity criterion (GIC) is applied to find the best split on these K attributes. GIC measures how often an element would be incorrectly labeled if randomly labeled according to the distribution of labels in the subset; the best split minimizes GIC.\nEach tree is grown to the largest extent possible, without pruning. By repeating this randomized procedure Nt times, a collection of Nt trees is obtained, constituting a RFo. Classification of an object is done by simple voting of all trees. In this work, the RFo implementation from the R [13] package randomForest [11] was used.\nThe computational complexity CtFo of training a RFo is\nCtFo = Nt \u00b7No \u00b7 logNo \u00b7K , (1)\nwhere No is the number of objects, K is the number of attributes tested for each split and Nt is the number of trees in the forest; the computational complexity\nCcFo = Nt \u00b7 ht (2)\nwhere ht is the average height of a tree in the forest."}, {"heading": "2.2 Random Ferns", "text": "A fern is defined as a simplified binary decision tree of a fixed height D (called a depth of a fern) and with a requirement that all splitting criteria at a certain depth i (Ci) are the same. Each leaf node of a fern stores the distribution of classes over objects that are directed to this node. This way a fern can be perceived as a D-dimensional array of distributions, indexed by a vector of D splitting criteria values, see Figure 1.\nThe fern forest is a collection of Nf ferns. When classifying a new object, each fern in a forest returns a vector of probabilities that this object belongs to particular decision classes. Ferns are treated as independent, thus all those\nvectors are combined by simple multiplication and the final classification results for the forest is a class which gets the highest probability, see Figure 2.\nWhile the original RFe implementation [17,18] was written for a problem of object detection in images, we use the RFe generalization implemented in the R [13] package rFerns [10]; it trains the fern forest model in the following way.\nFirst, N intermediate training sets called bags are created by drawing objects with replacement from the training set, each bag being of the same size as the original set. Next, each bag is used to train a fern. All D splits are created purely at random; an attribute is randomly selected and then the splitting threshold is set as a mean of two randomly selected values of this attribute3. The distributions of classes in leafs are calculated on a bag with adding 1 for each class (i.e. with a Dirichlet prior); this way the problem of undefined distributions in leafs containing no objects is resolved.\nThe computational complexity CtFe of training a Rfe model is\nCtFe = 2 D \u00b7Nf \u00b7No , (3)\nwhere D \u2014 depth of ferns, No \u2014 number of objects, Nf \u2014 number of ferns; the computational complexity CcFe of classifying one sample is\nCcFe = D \u00b7Nf . (4)"}, {"heading": "3 Sound Parameterization", "text": "The identification of musical instruments is performed for short frames of audio data, which are parametrized before applying classifiers for training or testing. No assumptions on audio data segmentation or pitch extraction have been made. Therefore, no multi-pitch extraction is needed, thus avoiding possible errors regarding labeling particular sounds in polyphonic recording with the appropriate\n3 In this work we have used only numerical descriptors of sound, thus the description of treating ordinal and categorical attributes is omitted.\npitches. The feature vector consists of basic features, describing properties of an audio frame of 40 ms, and additionally difference features, calculated as the difference between the given feature but calculated for a 30 ms sub-frame starting from the beginning of the frame and a 30 ms sub-frame starting 10 ms later. Identification of instruments is performed frame by frame, for consequent frames, with 10 ms hop size. Fourier transform was used to calculate spectral features, with Hamming window. Most of the features we applied represent MPEG-7 lowlevel audio descriptors, which are often used in audio research [5]. Our feature vector consists of the following 91 parameters [7]:\n\u2013 Audio Spectrum Flatness, flat1, . . . , flat25 \u2014 a multidimensional parameter describing the flatness property of the power spectrum within a frequency bin for selected bins; 25 out of 32 frequency bands were used; \u2013 Audio Spectrum Centroid \u2014 the power weighted average of the frequency bins in the power spectrum; coefficients are scaled to an octave scale anchored at 1 kHz [5]; \u2013 Audio Spectrum Spread \u2014 RMS (root mean square) value of the deviation of the log frequency power spectrum wrt. Audio Spectrum Centroid [5]; \u2013 Energy \u2014 energy (in log scale) of the spectrum of the parametrized sound; \u2013 MFCC \u2014 a vector of 13 mel frequency cepstral coefficients. The cepstrum\nwas calculated as the logarithm of the magnitude of the spectral coefficients, and then transformed to the mel scale, to better reflect properties of the human perception of frequency. 24 mel filters were applied, and the obtained results were transformed to 12 coefficients. The 13th coefficient is the 0-order coefficient of MFCC, corresponding to the logarithm of the energy [12]; \u2013 Zero Crossing Rate; a zero-crossing is a point where the sign of the timedomain representation of the sound wave changes; \u2013 Roll Off \u2014 the frequency below which an experimentally chosen percentage equal to 85% of the accumulated magnitudes of the spectrum is concentrated; parameter originating from speech recognition, where it is applied to distinguish between voiced and unvoiced speech; \u2013 NonMPEG7 - Audio Spectrum Centroid \u2014 a linear scale version of Audio Spectrum Centroid ; \u2013 NonMPEG7 - Audio Spectrum Spread \u2014 a linear scale version of Audio Spectrum Spread ; \u2013 changes (measured as differences) of the above features for a 30 ms subframe of the given 40 ms frame (starting from the beginning of this frame) and the next 30 ms sub-frame (starting with 10 ms shift), calculated for all the features shown above; \u2013 Flux \u2014 the sum of squared differences between the magnitudes of the DFT points calculated for the starting and ending 30 ms sub-frames within the main 40 ms frame; this feature by definition describes changes of magnitude spectrum, thus it is not calculated in a static version.\nMixes of the left and right channel were taken if the audio signal was stereophonic. Since the recognition of instruments is performed on frame-by-frame\nbasis, no parameters describing the entire sound are present in our feature vector. This feature set was already used for instrument identification purposes using RFo, requiring no feature selection [7], and yielded good results, so we decided to use this feature set in both RFo and RFe classification."}, {"heading": "3.1 Audio Data", "text": "The audio data we used for both training and testing represent recordings in 44.1kHz/16-bit format. Training was based on three repositories of single, isolated sounds of musical instruments, namely McGill University Master Samples [15], The University of Iowa Musical Instrument Samples [23], and RWC Musical Instrument Sound Database [3]. Clarinet, trombone, and trumpet sounds were taken from these repositories. Additionally, we used sousaphone sounds, recorded by R. Rudnicki in one of his recording sessions [20], since no sousaphone sounds were available in the above mentioned repositories. Training data were in mono format in the case of RWC data and sousaphone, and stereo for the rest of the data. The testing data originate from jazz band stereo recordings by R. Rudnicki [20], and include the following pieces played by clarinet, trombone, trumpet, and sousaphone (i.e., our target instruments):\n\u2013 Mandeville by Paul Motian, \u2013 Washington Post March by John Philip Sousa, arranged by Matthew Postle, \u2013 Stars and Stripes Forever by John Philip Sousa, semi-arranged by Matthew\nPostle \u2014 Movement no. 2 and Movement no. 3.\nTo prepare our classifiers to work on larger instrument sets, training data also included sounds of 5 other instruments that can be encountered in jazz recordings: double bass, piano, tuba, saxophone, and harmonica. These sounds were added as additional sounds in training mixes with the target instruments."}, {"heading": "4 Methodology of Training of the Classifiers", "text": "The goal of training of our classifiers is to identify plural classes, each representing one instrument. We use a set of binary classifiers (RFe or RFo), where each set (which we call a battery) is trained to identify whether a target instrument is playing in an audio frame or not. The target classes are clarinet, trombone, trumpet, and sousaphone, i.e. instruments playing in the analyzed jazz band recordings. The classifiers are trained to identify target instruments when they are accompanied by other instruments, and this is why we use mixes of instrument sounds as input data in training.\nWhen preparing training data, we start with single isolated sounds of each target instrument. After removing starting and ending silence [7], each file representing the whole single sound is normalized so that the RMS value equals one. Then we perform parameterization, and train a classifier to identify each instrument \u2014 even when accompanied by other sound. Therefore, we perform\ntraining on 40 ms frames of instrument sound mixes, mixing from 1 to 4 randomly chosen instruments with random weights and then we normalize it again to get the RMS value equal to one.\nThe battery of one-instrument sensitive RFo or RFe classifiers is then trained. 3,000 mixes containing any sound of a given instrument are fed as positive examples, and 3,000 mixes containing no sound of this instrument are fed as negative examples. For N instruments we need N binary classifiers (N=4), each one trained to identify 1 instrument. For RFe models, we have been training 1000 ferns of a depth of 10; for RFo, there were 1000 trees and K was set to the default floor of square root of the number of attributes, namely 9."}, {"heading": "5 Experiments and Results", "text": "The RFo and RFe classifiers, according to the procedure delineated in Section 4, were next used to identify instruments playing in jazz recordings, described in Section 3.1. Ground-truth data were prepared through careful manual labelling [7], based on initial recordings of each instrument track separately.\nThe accuracy was assessed via precision and recall scores. These measures were weighted by the RMS of a given frame (differently than in our previous work [7], where RMS was calculated for frames taken from instrument channels), in order to diminish the impact of softer frames, which are very hard to perform reasonable identification of instruments, because their loudness is near the noise level. For this reason, our true positive score Tp for an instrument i is a sum of RMS of frames which are both annotated and classified as i. Precision is calculated by dividing Tp by the sum of RMS of frames which are classified as i; respectively, recall is calculated by dividing Tp by the sum of RMS of frames which are annotated as i. As a general accuracy measure we have used F-score, defined as a harmonic mean of such precision and recall.\nWhile in this initial phase of the research we have used PC implementations of the classification algorithms, the timings have been performed on a single core of a Xeon E5620 Linux workstation. R version 2.15.0, rFerns version 0.3 and randomForest version 4.6-6 were used.\nBoth RFo and RFe are stochastic algorithms, so is the process of creating training sets for the battery. Thus, to assess the stability of the results and make a fair comparison of methods, the whole procedure of creating training sets, training RFe and RFo batteries and testing them on a real recordings has been repeated 10 times."}, {"heading": "5.1 Comparison of Random Forests and Random Ferns", "text": "The results of performance analysis of RFe and RFo models are given in Table 1. As one can see, for three pieces RFo had superior precision over that of RFe; on the other hand, ferns tend to provide better recall. However, the overall performance of both classifiers measured with the F-score is similar for all pieces.\nThe detailed comparison of performance analysis of RFe and RFo models for particular instruments is given in Table 2. Sousaphone and trumpet are always quite precisely identified, whereas trombone usually yields lower precision in all pieces, and clarinet in one piece. Recall is lower than precision, but still much improved comparing to our previous results [7]. Again, quite high recall is obtained for sousaphone and is rather good for trumpet, whereas the worst recall is scored by RFo for trombone samples.\nOn average, RFe and RFo perform classification respectively 25x and 8x faster than the actual music speed; this means RFe offer over 3x speed-up in comparison to RFo, see Table 3."}, {"heading": "6 Summary and Conclusions", "text": "Experiments presented in this paper show that identification of all instruments playing in real music recordings is possible using both RFo- and RFe-based classifiers, yielding quite good results. We observed improved recall comparing to our previous research [7]; we improved here the RMS weighting, which was previously calculated for separate instrument channels, and in this work, the RMS of all channels together was used for weighting. Our results still are worth improving, but the obtained recall (and precision) are satisfactory, because the task of identification of all instruments playing in a short segment is difficult, and is challenging also for human listeners.\nThe measured classification speed of RFe suggests that it is a promising method for performing real time annotation, even on low performance devices.\nAcknowledgments. This work has been partially financed by the National Science Centre, grant 2011/01/N/ST6/07035. Computations were performed at ICM, grant G48-6. This project was also partially supported by the Research Center of PJIIT, supported by the Polish Ministry of Science and Higher Education. The authors would also like to thank Dr. Elz\u0307bieta Kubera from the University of Life Sciences in Lublin for preparing the ground-truth data for initial experiments, and Rados law Rudnicki from the University of York for preparing the jazz band recordings."}], "references": [{"title": "Image Classification using Random Forests and Ferns", "author": ["A. Bosch", "A. Zisserman", "X. Munoz"], "venue": "2007 IEEE 11th International Conference on Computer Vision, pp. 1\u20138. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning 45, 5\u201332", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "RWC Music Database: Music Genre Database and Musical Instrument Sound Database", "author": ["M. Goto", "H. Hashiguchi", "T. Nishimura", "R. Oka"], "venue": "Proceedings of ISMIR, pp. 229\u2013230", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic Classification of Pitched Musical Instrument Sounds", "author": ["P. Herrera-Boyer", "A. Klapuri", "M. Davy"], "venue": "Klapuri, A., Davy, M. (eds.): Signal Processing Methods for Music Transcription. Springer Science+Business Media LLC", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Instrument Identification in Polyphonic Music: Feature Weighting to Minimize Influence of Sound Overlaps", "author": ["T. Kitahara", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno"], "venue": "EURASIP J. on Advances in Signal Processing, Vol.2007, pp.1\u201315", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "All That Jazz in the Random Forest", "author": ["E. Kubera", "M.B. Kursa", "W.R. Rudnicki", "R. Rudnicki", "A.A. Wieczorkowska"], "venue": "Kryszkiewicz, M., Rybi\u0144ski, H., Skowron, A., Ra\u015b, Z.W. (eds.): ISMIS 2011. LNAI, vol. 6804, pp. 543-553. Springer, Heidelberg", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Musical Instruments in Random Forest", "author": ["M.B. Kursa", "W. Rudnicki", "A. Wieczorkowska", "E. Kubera", "A. Kubik-Komar"], "venue": "J. Rauch, Z.W. Ra\u015b, P. Berka, T. Elomaa (eds.): ISMIS 2009, LNAI vol. 5722, pp. 281\u2013290 Heidelberg. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Random Musical Bands Playing in Random Forests", "author": ["M.B. Kursa", "E. Kubera", "W.R. Rudnicki", "Wieczorkowska", "A.A"], "venue": "Szczuka, M., Kryszkiewicz, M., Ramanna, S., Jensen, R., Hu, Q. (eds.): RSCTC 2010. LNAI, vol. 6086, pp. 580\u2013589. Springer, Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Random ferns method implementation for the general-purpose machine learning", "author": ["M.B. Kursa"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Classification and Regression by randomForest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News 2(3), 18\u201322", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Implementation of MFCC vector generation in classification context", "author": ["D. Niewiadomy", "A. Pelikant"], "venue": "J. Applied Computer Science, Vol. 16, No. 2, pp. 55\u201365", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "MUMS \u2014 McGill University Master Samples", "author": ["F. Opolko", "J. Wapnick"], "venue": "CD\u2019s", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Action Recognition Using Randomised Ferns", "author": ["O. Oshin", "A. Gilbert", "J. Illingworth", "R. Bowden"], "venue": "Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th International Conference, pp. 530\u2013537. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast Keypoint Recognition in Ten Lines of Code", "author": ["M. \u00d6zuysal", "P. Fua", "V. Lepetit"], "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition, IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast Keypoint Recognition using Random Ferns", "author": ["\u00d6zuysal", "M.M. Calonder", "V. Lepetit", "P. Fua"], "venue": "Image Processing http://dx.doi.org/10.1109/TPAMI.2009.23", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Jazz band", "author": ["R. Rudnicki"], "venue": "Recording and mixing. Arrangements by M. Postle. Clarinet \u2014 J. Murgatroyd, trumpet \u2014 M. Postle, harmonica, trombone \u2014 N. Noutch, sousaphone \u2013 J. M. Lancaster", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Intelligent Music Information Systems: Tools and Methodologies", "author": ["J. Shen", "J. Shepherd", "B. Cui", "Liu", "L. (eds."], "venue": "Information Science Reference, Hershey", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Real-time Detection and Tracking for Augmented Reality on Mobile Phones", "author": ["D. Wagner", "G. Reitmayr", "A. Mulloni", "T. Drummond", "D. Schmalstieg"], "venue": "IEEE transactions on visualization and computer graphics 16 (3), 355\u2013368", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "This area has been broadly explored last years [19], [22], and as a result we can enjoy finding pieces of music through query-by-humming [14], and identify music through query-by-example, including excerpts replayed on mobile devices [21], [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "[4], [6], [7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4], [6], [7]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "[4], [6], [7]).", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "Random ferns are classifiers introduced in 2007 [17] and named as such in 2008 [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Random ferns are classifiers introduced in 2007 [17] and named as such in 2008 [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Random ferns have been applied so far in image classification tasks, including video data [1], [16], and they have also been adjusted to be used on low-end embedded platforms, such as mobile phones [25].", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "Random ferns have been applied so far in image classification tasks, including video data [1], [16], and they have also been adjusted to be used on low-end embedded platforms, such as mobile phones [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "Random ferns have been applied so far in image classification tasks, including video data [1], [16], and they have also been adjusted to be used on low-end embedded platforms, such as mobile phones [25].", "startOffset": 198, "endOffset": 202}, {"referenceID": 5, "context": "Additionally, we would like to compare the performance of Random Ferns (RFe) with Random Forests (RFo), which yielded quite good results in our previous research [7], [8], [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 6, "context": "Additionally, we would like to compare the performance of Random Ferns (RFe) with Random Forests (RFo), which yielded quite good results in our previous research [7], [8], [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 7, "context": "Additionally, we would like to compare the performance of Random Ferns (RFe) with Random Forests (RFo), which yielded quite good results in our previous research [7], [8], [9].", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "RFe are simpler and more computationally efficient than RFo [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "RFo performed quite well in the research on instrument identification we performed before [7], but their training is time consuming, whereas the training of RFe is faster.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "RFo is a classifier consisting of a set of weak, weakly correlated and non-biased decision trees, constructed using a procedure minimizing bias and correlations between individual trees [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 9, "context": "In this work, the RFo implementation from the R [13] package randomForest [11] was used.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "While the original RFe implementation [17,18] was written for a problem of object detection in images, we use the RFe generalization implemented in the R [13] package rFerns [10]; it trains the fern forest model in the following way.", "startOffset": 38, "endOffset": 45}, {"referenceID": 14, "context": "While the original RFe implementation [17,18] was written for a problem of object detection in images, we use the RFe generalization implemented in the R [13] package rFerns [10]; it trains the fern forest model in the following way.", "startOffset": 38, "endOffset": 45}, {"referenceID": 8, "context": "While the original RFe implementation [17,18] was written for a problem of object detection in images, we use the RFe generalization implemented in the R [13] package rFerns [10]; it trains the fern forest model in the following way.", "startOffset": 174, "endOffset": 178}, {"referenceID": 5, "context": "Our feature vector consists of the following 91 parameters [7]:", "startOffset": 59, "endOffset": 62}, {"referenceID": 10, "context": "The 13 coefficient is the 0-order coefficient of MFCC, corresponding to the logarithm of the energy [12]; \u2013 Zero Crossing Rate; a zero-crossing is a point where the sign of the timedomain representation of the sound wave changes; \u2013 Roll Off \u2014 the frequency below which an experimentally chosen percentage equal to 85% of the accumulated magnitudes of the spectrum is concentrated; parameter originating from speech recognition, where it is applied to distinguish between voiced and unvoiced speech; \u2013 NonMPEG7 - Audio Spectrum Centroid \u2014 a linear scale version of Audio Spectrum Centroid ; \u2013 NonMPEG7 - Audio Spectrum Spread \u2014 a linear scale version of Audio Spectrum Spread ; \u2013 changes (measured as differences) of the above features for a 30 ms subframe of the given 40 ms frame (starting from the beginning of this frame) and the next 30 ms sub-frame (starting with 10 ms shift), calculated for all the features shown above; \u2013 Flux \u2014 the sum of squared differences between the magnitudes of the DFT points calculated for the starting and ending 30 ms sub-frames within the main 40 ms frame; this feature by definition describes changes of magnitude spectrum, thus it is not calculated in a static version.", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "This feature set was already used for instrument identification purposes using RFo, requiring no feature selection [7], and yielded good results, so we decided to use this feature set in both RFo and RFe classification.", "startOffset": 115, "endOffset": 118}, {"referenceID": 11, "context": "Training was based on three repositories of single, isolated sounds of musical instruments, namely McGill University Master Samples [15], The University of Iowa Musical Instrument Samples [23], and RWC Musical Instrument Sound Database [3].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Training was based on three repositories of single, isolated sounds of musical instruments, namely McGill University Master Samples [15], The University of Iowa Musical Instrument Samples [23], and RWC Musical Instrument Sound Database [3].", "startOffset": 236, "endOffset": 239}, {"referenceID": 15, "context": "Rudnicki in one of his recording sessions [20], since no sousaphone sounds were available in the above mentioned repositories.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "Rudnicki [20], and include the following pieces played by clarinet, trombone, trumpet, and sousaphone (i.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "After removing starting and ending silence [7], each file representing the whole single sound is normalized so that the RMS value equals one.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Ground-truth data were prepared through careful manual labelling [7], based on initial recordings of each instrument track separately.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "These measures were weighted by the RMS of a given frame (differently than in our previous work [7], where RMS was calculated for frames taken from instrument channels), in order to diminish the impact of softer frames, which are very hard to perform reasonable identification of instruments, because their loudness is near the noise level.", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "Recall is lower than precision, but still much improved comparing to our previous results [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "We observed improved recall comparing to our previous research [7]; we improved here the RMS weighting, which was previously calculated for separate instrument channels, and in this work, the RMS of all channels together was used for weighting.", "startOffset": 63, "endOffset": 66}], "year": 2013, "abstractText": "In this paper, we first apply random ferns for classification of real music recordings of a jazz band. No initial segmentation of audio data is assumed, i.e., no onset, offset, nor pitch data are needed. The notion of random ferns is described in the paper, to familiarize the reader with this classification algorithm, which was introduced quite recently and applied so far in image recognition tasks. The performance of random ferns is compared with random forests for the same data. The results of experiments are presented in the paper, and conclusions are drawn.", "creator": "LaTeX with hyperref package"}}}