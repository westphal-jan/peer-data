{"id": "1306.4418", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2013", "title": "Structure Based Extended Resolution for Constraint Programming", "abstract": "nogood learning analytics is given a powerful synthetic approach to reducing search in constraint processing programming ( especially cp ) proving solvers. the strongest current welfare state computational of the art, called lazy clause information generation ( lcg ), uses finite resolution to derive nogoods thus expressing the reasons underlying for each search failure. obtaining such nogoods can practically prune other desirable parts of implementing the loop search tree, producing exponential speedups on a wide wide variety of problems. nogood learning solvers can typically be seen as large resolution proof sorting systems. considerably the stronger the proof system, namely the cheaper faster it can solve in a cp iteration problem. it has recently been shown that designing the compression proof system algorithm used in lcg alone is at least as strong possibly as polynomial general resolution. besides however, stronger proof recognition systems such arrays as \\ product emph { or extended resolution } exist. extended resolution allows operations for template literals only expressing arbitrary logical content concepts over other existing variables to be progressively introduced spontaneously and can more allow exponentially smaller proofs than general resolution. likewise the primary technique problem in using such extended resolution is to figure it out systematically exactly accurately which literals are useful to introduce. already in learning this paper, suppose we strongly show that we surely can certainly use changing the structural information elements contained internally in a cp code model compiler in your order initially to introduce useful linear literals, and that this efficiency can translate into significant speedups on modeling a range of design problems.", "histories": [["v1", "Wed, 19 Jun 2013 04:18:45 GMT  (34kb,D)", "http://arxiv.org/abs/1306.4418v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["geoffrey chu", "peter j stuckey"], "accepted": false, "id": "1306.4418"}, "pdf": {"name": "1306.4418.pdf", "metadata": {"source": "CRF", "title": "Structure Based Extended Resolution for Constraint Programming", "authors": ["Geoffrey Chu", "Peter J. Stuckey"], "emails": ["gchu@csse.unimelb.edu.au", "pjs@csse.unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Nogood learning is a powerful approach to reducing search in Constraint Programming (CP) solvers. The current state of the art is Lazy Clause Generation [21] (LCG). LCG adapts the clause learning techniques from Boolean Satisfiability (SAT) to the more generic domain of CP problems where we have finite domain variables and global constraints. Each propagator in an LCG solver is instrumented so that it is able to explain each of its propagations using a clause. These clauses form an implication graph. When a search failure occurs, the implication graph is analyzed and resolution is performed on the clauses in the implication graph in order to derive a nogood which explains the reasons for the failure. These nogoods can then be propagated in order to prune other parts of the search tree. Nogood learning is very effective on structured problems and can often provide orders of magnitude speedup over a non-learning solver.\nA complete search CP solver can be seen as a proof system which is trying to prove that no solution exists in a satisfiability problem, or that no solution better than a certain objective value exists in an optimization problem. It either finds a counter example (i.e., a solution) during the proof process, or it succeeds\nar X\niv :1\n30 6.\n44 18\nv1 [\ncs .A\nI] 1\n9 Ju\nn 20\nin proving that no solution exists. The size of the search tree is bounded from below by the size of the smallest proof possible in the proof system. Thus in general, the stronger the proof system used, the faster a CP solver can solve the problem. The proof system used in an LCG solver is much stronger than the one used in a non-learning solver. This is because when a non-learning CP solver fails a subtree, it has only proved that that particular subtree fails. On the other hand, when a LCG solver fails a subtree, it uses resolution to derive a nogood that proves that this subtree fails, but also that other similar subtrees (i.e., those which satisfies the conditions in the nogood) also fail.\nIt has recently been proved [22] that a SAT solver performing conflict directed clause learning and restarts has a proof system that is as powerful as general resolution [23]. The proof system of LCG solvers, which inherits the resolution based learning of SAT solvers and the possibly non-resolution based inferences of CP\u2019s global propagators, is even more powerful. However, the power of the resolution part of the proof system is constrained by the set of literals that it is allowed to use in the proof. We call this set of literals the language of the resolution proof system. Extended resolution [26] is a proof system even stronger than general resolution and is one of the most powerful proof systems for propositional logic [27]. It allows the language of the resolution proof system to be dynamically extended during runtime by introducing new variables representing arbitrary logical expressions over the existing ones. It is well known that there can be an exponential separation between the size of the proof generated by extended resolution and general resolution on certain problems [7]. Clearly, utilizing extended resolution in nogood learning could be a very effective way to improve the speed of a CP solver.\nWhile extended resolution offers the potential for significant speedups, such speedups are often difficult to realize in practice. This is because in order for extended resolution to produce shorter proofs, it is necessary for the system to make the right extensions, i.e., introduce variables expressing the right logical concepts. It is typically very difficult to know which particular extensions are needed to speed up a proof, so it is difficult to use extended resolution effectively. There have been several attempts at augmenting SAT solvers with extended resolution capabilities with varying success (e.g., [2, 13]). Constraint Programming provides a unique opportunity for the effective usage of extended resolution. In Constraint Programming, problems are modeled in terms of high level constraints which preserve much of the structure of the problem. This structural information provides important information regarding which extensions will be useful, and allows us to exploit the potential speedups made possible by extended resolution.\nMost of the major advances in resolution based nogood learning in CP has come about due to an extension of the language of the resolution proof system in order to exploit the structure of finite domain integer variables. The earlier works on nogood learning (see e.g. [8], chapter 6) only considers equality literals of the form x = v where x is a variable and v is a value. Later on, disequality literals of the form x 6= v were introduced [14, 15]. More recently, inequality literals of the form x \u2265 v, x \u2264 v were introduced in LCG [21]. Each extension significantly increased the expressiveness of the nogoods and the power of the proof system, resulting in significant speedups compare to previous versions. In this paper, we\nlook at other types of structure that can be found in CP problems and consider how they can be exploited via the introduction of additional literals into the language. Our main contributions are as follows:\n\u2013 We provide a framework for assessing the generality of an explanation generated by an LCG propagator, given a fixed language of resolution L. \u2013 We examine the internal structure of commonly used global constraints to see which kinds of language extensions can be useful for improving the power of the resolution proof system. \u2013 We show that the global structure of the problem can be used in order to decide which language extension to make."}, {"heading": "2 Definitions and Background", "text": "Let \u2261 denote syntactic identity, \u21d2 denote logical implication and \u21d4 denote logical equivalence. A constraint satisfaction problem (CSP) is a tuple P \u2261 (V,D,C), where V is a set of variables, D is a set of (unary) domain constraints, and C is a set of (n-ary) constraints. An assignment \u03b8 is a solution of P if it satisfies every constraint in D and C. In an abuse of notation, if a symbol C refers to a set of constraints {c1, . . . , cn}, we will often also use the symbol C to refer to the conjunction c1 \u2227 . . . \u2227 cn.\nCP solvers solve CSP\u2019s by interleaving search with inference. We begin with the original problem at the root of the search tree. At each node in the search tree, we propagate the constraints to try to infer variable/value pairs which can no longer be taken in any solution in this subtree. Such pairs are removed from the current domain. If some variable\u2019s domain becomes empty, then the subtree has no solution and the solver backtracks. If all the variables are assigned and no constraint is violated, then a solution has been found and the solver can terminate. If inference is unable to detect either of the above two cases, the solver further divides the problem into a number of more constrained subproblems and searches each of those in turn.\nA CP solver implementing LCG has a number of additional features which allow it to perform nogood learning. Firstly, for each integer variable x with initial domain {l, . . . , u}, the solver adds Boolean variables to represent the truth value of the logical expressions x = v for v = l, . . . , u and x \u2265 v for v = l + 1, . . . , u. We use JeK to denote the Boolean variable which represents the truth value of logical expression e. The solver enforces the channeling constraint JeK \u2194 e for each such variable. So for example, the Boolean variable Jx = 5K is true iff x = 5 is implied by the current domain D. For convenience, we also use Jx 6= vK to refer to \u00ac Jx = vK and Jx \u2264 vK to refer to \u00ac Jx \u2265 v + 1K. We call literals of form Jx = vK equality literals, literals of form Jx 6= vK disequality literals, and literals of form Jx \u2265 vK or Jx \u2264 vK inequality literals.\nIn an LCG solver (and indeed most CP solvers), the only allowed kinds of domain changes are: fixing a variable to a value, removing a value, increasing the lower bound, or decreasing the upper bound. Each of these can be expressed as setting one of the literals Jx = vK, Jx 6= vK, Jx \u2265 vK or Jx \u2264 vK true. Each propagator in an LCG solver is instrumented in order to explain each of its domain changes with a clause called the explanation.\nDefinition 1. Given current domain D, suppose the propagator for constraint c makes an inference p, i.e., c \u2227D \u21d2 p. An explanation for this inference is a clause: expl(p) \u2261 l1 \u2227 . . . \u2227 lk \u2192 p where li and p are literals, s.t. c \u21d2 expl(p) and D \u21d2 l1 \u2227 . . . \u2227 lk.\nFor example, given constraint x \u2264 y and current domain x \u2208 {3, 4, 5}, the propagator may infer that y \u2265 3, with the explanation Jx \u2265 3K \u2192 Jy \u2265 3K. The explanation expl(p) explains why p has to hold given c and the current domain D. We can consider expl(p) as the fragment of the constraint c from which we inferred that p has to hold. We call the set of literals available for forming explanations the language of resolution for the system.\nAs propagation proceeds, these explanations form an acyclic implication graph. Whenever a conflict is found by an LCG solver, the implication graph can be analyzed in order to derive a set of sufficient conditions for the conflict to reoccur. Just as most current state of the art SAT solvers, LCG solvers derive the first unique implication point (1UIP) nogood. This is done by repeatedly resolving the conflicting clause (the clause explaining the conflict) with the explanation clause for the latest inferred literal until the clause contains only one literal from the current decision level. The resulting clause, or nogood as it is more commonly called in CP, is an implied constraint of the problem which proves that this particular subtree failed. However, this nogood often also proves that other subtrees fail for a similar reason to the current one. Thus we can add the nogood as a propagator to prune other parts of the search tree.\nExample 1. Consider a simple constraint problem with variables x1, x2,x3, x4, x5, x6 with all initial domain {0, 1, 2, 3, 4, 5, 6, 7}, and three constraints: x1+2x2+3x3+ 4x4 + 4x5 \u2264 30, x4 \u2264 4\u2192 x6 = 1, and x5 \u2264 4\u2192 x6 = 0. Suppose we make the decisions: x1 \u2265 1 (nothing propagates), x2 \u2265 2 (propagates x4 \u2264 6 and x5 \u2264 6), and x3 \u2265 3. This propagates x4 \u2264 4 and x5 \u2264 4, which in turn propagates x6 = 1 and causes the constraint x5 \u2264 4\u2192 x6 = 0 to fail. Figure 1 shows the implication graph when the conflict occurs. The double boxes indicate decision literals while the dashed lines partition literals into decision levels. Dotted lines are literals that are irrelevant to the failure. To obtain the 1UIP nogood we start with the conflict nogood Jx5 \u2264 2K\u2227 Jx6 = 1K\u2192 false which contains every literal directly connected to the false conclusion. We have two literals from the last decision level (Jx5 \u2264 4K and Jx6 = 1K). Since Jx6 = 1K was the last literal to be inferred of those two, we resolve the current nogood with expl(Jx6 = 1K) = Jx4 \u2264 4K obtaining Jx4 \u2264 4K \u2227 Jx5 \u2264 4K \u2192 false. We still have two literals of the last decision level so we replace Jx5 \u2264 4K by expl(Jx5 \u2264 4K) = Jx1 \u2265 1K \u2227 Jx2 \u2265 2K \u2227 Jx3 \u2265 3K obtaining Jx1 \u2265 1K \u2227 Jx2 \u2265 2K \u2227 Jx3 \u2265 3K \u2227 Jx4 \u2264 4K \u2192 false. We then replace Jx4 \u2264 4K by expl(Jx4 \u2264 4K) = Jx1 \u2265 1K \u2227 Jx2 \u2265 2K \u2227 Jx3 \u2265 4K obtaining Jx1 \u2265 1K\u2227Jx2 \u2265 2K\u2227Jx3 \u2265 3K\u2192 false. This is the 1UIP nogood since it contains only one literal from level 3."}, {"heading": "3 Generality of Explanations", "text": "Since the nogoods derived by the resolution proof system are formed by resolving the explanations generated by the propagators, the more general the explanations are, the more general the nogood derived will be. Using better explanations\nmeans that for the same amount of search, we can derive stronger nogoods that prove that a greater part of the search space is failed. The following definitions allow us to compare and assess how good an explanation is:\nDefinition 2. Given two possible explanations E \u2261 l1 \u2227 . . . \u2227 ln \u2192 p and E\u2032 \u2261 k1 \u2227 . . . \u2227 km \u2192 p for the inference p, E\u2032 is strictly more general than E iff: \u2227ni=1li \u21d2 \u2227mi=1ki and \u2227mi=1ki ; \u2227ni=1li. ut\nDefinition 3. An explanation E \u2261 l1 \u2227 . . . \u2227 ln \u2192 p for the inference p is maximally general w.r.t. language of resolution L, if there does not exist another explanation E\u2032 in L which is strictly more general than E. ut\nNote that maximally general explanations are not necessarily unique.\nExample 2. Consider a linear constraint x1 +2x2 +3x3 +4x4 \u2264 30 and a current domain of x1 = 1, x2 = 2, x3 = 3. The propagator can infer that x4 \u2264 4. There are many possible explanations. For example, Jx1 = 1K \u2227 Jx2 = 2K \u2227 Jx3 = 3K\u2192 Jx4 \u2264 4K is a perfectly valid explanation. However, it is not very general. A strictly more general explanation is Jx1 \u2265 1K \u2227 Jx2 \u2265 2K \u2227 Jx3 \u2265 3K \u2192 Jx4 \u2264 4K. However, this is still not maximally general in the standard LCG language. For example, Jx2 \u2265 1K \u2227 Jx3 \u2265 3K \u2192 Jx4 \u2264 4K is a maximally general explanation which is more general than the one before. Similarly, Jx1 \u2265 1K \u2227 Jx2 \u2265 2K \u2227 Jx3 \u2265 2K\u2192 Jx4 \u2264 4K is another maximally general explanation.\nClearly, a good starting point for making the resolution proof system stronger is to ensure that the LCG solver is using maximally general explanations, so that we are making the most out of the existing language.\nDefinition 4. An explanation E \u2261 l1 \u2227 . . . \u2227 ln \u2192 p for the inference p is universally maximally general if it is maximally general w.r.t. to the universal language L consisting of all possible logical expressions. ut\nIf the universally maximally general explanation for an inference cannot be expressed as a conjunction of literals in the existing language, then it is a good indication that a language extension may be useful for increasing the generality of the explanations for this constraint.\nExample 3. Consider the inference from Example 2. The universally maximal general explanation is: Jx1 + 2x2 + 3x3 \u2265 11K\u2192 Jx4 \u2264 4K, since x1+2x2+3x3 \u2265 11 is a necessary and sufficient condition on the domain for us to infer x4 \u2264 4 from x1+2x2+3x3+4x4 \u2264 30. Clearly, there is no way that Jx1 + 2x2 + 3x3 \u2265 11K can be expressed equivalently as a conjunction of equality, disequality or inequality literals on x1, x2, x3, x4. So a language extension may be useful here."}, {"heading": "4 Extending the Language", "text": "We now consider how we can extend the language of resolution to give more general explanations. We first give a simple motivating example.\nExample 4. Consider the 0-1 knapsack problem, given by x1, . . . , xn \u2208 {0, 1},\u2211n i=1 wixi \u2264 W , \u2211n i=1 pixi \u2265 f , where f is to be maximized, wi represents the weights of item i, W is the total capacity of the knapsack, and pi is the profit of item i. A normal CP solver will require O(2n) to solve this problem. Using an LCG solver does no better, because the size of the smallest proof of optimality using only equality, disequality and inequality literals on the xi is still exponential in n. On the other hand, suppose we introduced literals to represent\npartial sums of form: \u2211k i=1 wixi \u2265 W \u2212 w\u2032 and \u2211k\ni=1 pixi \u2264 f \u2212 p\u2032 where k, w\u2032, p\u2032 are arbitrary constants. Then, it becomes possible to prove optimality in O(nWP ), where P = \u2211n i=1 pi. This is because it is now possible to express\nnogoods such as: r\u2211k i=1 wixi \u2265W \u2212 w\u2032 z \u2227 r\u2211k i=1 pixi \u2264 f \u2212 p\u2032 z \u2192 false which represent that we have proved that given only a weight limit of w\u2032 for the items k+1 to n, there is no way we can pick a subset of them such that their profit sum to at least p\u2032. If we modify our LCG solver to use these new partial sum literals the amount of search required can be O(nWP ) which is pseudo-polynomial rather than exponential complexity. ut\nWhen deciding on language extensions, there are two main factors we have to consider:\n\u2013 Are they going to improve the size of the resolution proof? \u2013 What is the overhead of introducing the literal into the system?\nWhen we extend the language by introducing a literal JeK where e is some logical expression over existing variables, we have to keep track of the truth value of JeK so that we can propagate any nogoods with this literal in it. This is accomplished by enforcing a channeling constraint JeK \u2194 e. For example, if we introduced a literal Jx1 + 2x2 + 3x3 \u2265 10K, we would have to enforce the channeling constraint: Jx1 + 2x2 + 3x3 \u2265 10K\u2194 x1 + 2x2 + 3x3 \u2265 10. Depending on what the expression is, this could be cheap or expensive.\nAs mentioned in the previous section, literals which allow global propagators to explain their inferences in a more general way are prime candidates for language extensions. Another benefit of such literals is that they often represent intermediate logical concepts in the propagation algorithm which the propagator is already keeping track of, and thus the channelling propagation required\nto enforce JeK \u2194 e can be \u201cpiggy-backed\u201d onto the original propagator at little extra cost.\nIdeally we can add a set of literals which allow the universally maximally general explanation for each inference to be described. Unfortunately, this is not always possible as the universally maximally general explanation may be some complicated logical expression that we cannot easily check the truth value of during search. Instead, we may have to settle for less general but more practical language extensions. We now analyze a number of global constraints to see what the maximally general explanations are given the standard LCG language consisting of equality, disequality and inequality literals on existing variables, and show the language extensions which can provide stronger explanations."}, {"heading": "4.1 Linear", "text": "Linear constraints are by far the most common constraint appearing in models.\nExample 5. Consider the linear constraint x1 + 2x2 + 3x3 + 4x4 + 4x5 \u2264 30 of Example 1. Given x1 \u2265 1, x2 \u2265 2, x3 \u2265 3, we can infer x4 \u2264 4. There are multiple possible maximally general explanation in the standard LCG language, e.g., Jx2 \u2265 1K\u2227 Jx3 \u2265 3K\u2192 Jx4 \u2264 4K or Jx1 \u2265 1K\u2227 Jx2 \u2265 2K\u2227 Jx3 \u2265 2K\u2192 Jx4 \u2264 4K. However, none of them are the most general explanation possible. If we extended the language with literals representing partial sums, we can now use the universally maximally general explanation: Jx1 + 2x2 + 3x3 \u2265 11K \u2192 Jx4 \u2264 4K. Using this intermediate literal the implication graph for Example 1 changes to that shown in Figure 2. Both Jx4 \u2264 4K and Jx5 \u2264 4K are explained by Jx1 + 2x2 + 3x3 \u2265 11K. The new 1UIP is simply Jx1 + 2x2 + 3x3 \u2265 11K\u2192 false. This is a much stronger nogood that will prune more of the search space. ut\nThe channelling propagation which enforces the consistency of a partial sum literal and the variables in the partial sum must itself be explained, and we can similarly use the partial sum literals to give more general explanations.\nExample 6. Consider Example 1 again. If we only generate explanations on demand during conflict analysis, we can introduce new partial sum literals to explain other partial sum literals in a maximally general fashion. When x4 \u2264 4 is inferred by the linear constraint x1 + 2x2 + 3x3 + 4x4 + 4x5 \u2264 30, the chain of\nexplanations going backwards from Jx4 \u2264 4K would be: Jx1 + 2x2 + 3x3 \u2265 11K\u2192 Jx4 \u2264 4K, Jx1 + 2x2 \u2265 2K\u2227Jx3 \u2265 3K\u2192 Jx1 + 2x2 + 3x3 \u2265 11K, Jx2 \u2265 1K\u2192 Jx1 + 2x2 \u2265 2K, Jx2 \u2265 2K\u2192 Jx2 \u2265 1K. The implication graph is shown in Figure 2. ut\nThere are also a significant number of global constraints which are composed of linear constraints along with other primitive constraints like channeling constraints (e.g., among, at most, at least, sliding sum, gcc, etc). These can similarly benefit from partial sum literal language extensions on the linears they are composed from."}, {"heading": "4.2 Lex", "text": "Consider the global lexicographical constraint: lex less([x1, . . . , xn], [y1, . . . , yn]) which constrains the sequence x1, . . . , xn to be lexicographically less than y1, . . . , yn, i.e.,: x1 < y1\u2228(x1 = y1\u2227x2 < y2)\u2228 . . .\u2228(x1 = y1\u2227 . . .\u2227xn\u22121 = yn\u22121\u2227xn < yn). Consider a partial assignment x1 = 1, y1 = 1, x2 = 2, y2 = 2, x3 = 3. From the constraint, we can infer that y3 \u2265 3. The maximally general explanation in the standard LCG language is: Jx1 = 1K\u2227Jy1 = 1K\u2227Jx2 = 2K\u2227Jy2 = 2K\u2227Jx3 \u2265 3K\u2192 Jy3 \u2265 3K. However, if we extend the language with literals to represent things such as: xi \u2265 yi, we can explain it using: Jx1 \u2265 y1K \u2227 Jx2 \u2265 y2K \u2227 Jx3 \u2265 3K \u2192 Jy3 \u2265 3K. This second explanation is strictly more general and can produce a more general nogood. For example, if in another branch, we had x1 = 2, y1 = 2, x2 = 1, y2 = 1, x3 = 3, the first nogood cannot propagate since Jx1 = 1K is not true, but the second one can since Jx1 \u2265 y1K, Jx2 \u2265 y2K are true. The other mode of propagation for a lex less constraint can make use of new literals of the form: xi > yi. For example, if x1 = 1, y1 = 1, x2 = 2, x3 = 4, y3 = 3, we can infer y3 \u2265 3 and explain it with: Jx1 \u2265 y1K \u2227 Jx3 > y3K \u2227 Jx2 \u2265 2K \u2192 Jy2 \u2265 3K. Thus Jxi \u2265 yiK and Jxi > yiK are good language extension candidates for lex less."}, {"heading": "4.3 Disjunctive", "text": "Consider a disjunctive constraint disjunctive([s1, s2], [5, 5]) over two tasks with start times having current domains of s1 \u2208 {2, . . . , 8}, s2 \u2208 {0, . . . , 4}, and durations d1 = d2 = 5. A global propagator would reason that task 1 must be scheduled after task 2, and therefore that s1 \u2265 5. There are multiple maximally general explanation in the standard LCG language, e.g., Js1 \u2265 0K \u2227 Js2 \u2264 4K \u2227 Js2 \u2265 0K \u2192 Js1 \u2265 5K. We can extend the language with literals to represent that task i runs before task j, written as Ji jK and channeled via: Ji jK\u2192 si + di \u2264 sj , \u00ac Ji jK \u2192 sj + dj \u2264 si. Then we can explain the inference via: J2 1K \u2227 Js2 \u2265 0K \u2192 Js1 \u2265 5K. A nogood created from this explanation have J2 1K rather than Js1 \u2265 0K \u2227 Js2 \u2264 4K in it and will be more general. For example, if we have another domain with s1 \u2265 3, s2 \u2264 7, s2 \u2265 0, a nogood created from the first two explanations would not be able to propagate, but the second one might since s1 \u2265 3, s2 \u2264 7 will cause J2 1K to become true."}, {"heading": "4.4 Table", "text": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]). Suppose we have x1 = 1, x2 = 2. Among other things,\npropagation will infer x4 6= 1. There are a number of different maximally general explanations in the standard LCG language, e.g., Jx1 6= 4K\u2227Jx1 6= 3K\u2227Jx2 6= 1K\u2192 Jx4 6= 4K, or Jx2 6= 3K \u2227 Jx2 6= 1K \u2227 Jx2 6= 1K\u2192 Jx4 6= 4K. However, none of these give the most general reason for Jx4 6= 4K. Suppose we extend the language with literals ri which represent whether the ith tuple is taken or not, i.e., r1 \u2261 Jx1 = 1 \u2227 x2 = 2 \u2227 x3 = 3 \u2227 x4 = 4K, r2 \u2261 Jx4 = 1 \u2227 x2 = 3 \u2227 x3 = 2 \u2227 x4 = 1K, etc. Then we can explain Jx4 6= 4K using \u00acr2 \u2227 \u00acr4 \u2227 \u00acr5 \u2192 Jx4 6= 4K. This is a universally maximally general explanation, i.e., if any domain knocks out tuples 2, 4 and 5 (which are the only ones that support x4 = 1), then x4 6= 1.\nSimilarly, the explanations for the regular constraint can be improved by introducing literals representing the intermediate states of the automata, and the explanations for binary/multi-decision diagram constraints (BDD/MDD) can be improved by introducing literals representing whether we take a particular node in the BDD/MDD or not. The global constraints alldiff , circuit , and many others also have language extensions that can give stronger explanations. However, the extensions we can think of are most likely impractical due to the expense of the channelling constraints."}, {"heading": "5 Exploiting Global Structure for Linears", "text": "For lex , table, disjunctive, regular and bdd/mdd , the number of useful literals identified in Section 4 is only linear or quadratic in the size of the constraint. Furthermore, all of those logical expressions are already things maintained internally by the global propagator and it is easy to alter the propagators to channel these literals and use them in explanations. Thus the overhead of adding these literals is fairly low and it is fine to simply add them all to the language. Linear on the other hand is more difficult. Linear constraints are extremely common and we know for certain that language extensions can be useful for this constraint. On the other hand, there are O(adn2n) possible partial sum literals for a length n linear with largest coefficient a and maximum domain size d. If we add too many of them, the cost of channeling them will swamp out any benefit we may get from search space reduction. In the worse case, we may have to calculate an exponential number of partial sums at each node just to channel them. We propose to only add the partial sum literals along a certain ordering of the terms in the linear, and to use the global structure of the problem in order to pick the ordering we use.\nSuppose that we had a particular ordering of the variables and suppose we had a linear constraint \u2211n i=1 aixi \u2264 a0. Without loss of generality assume that for each i, xi is before xi+1 in our chosen ordering (if not, just move the terms in the linear around and relabel the indices). We propose to add only partial sum\nliterals of the form r\u2211k i=1 aixi \u2265 v z\nfor 1 \u2264 k < n, e.g., Ja1x1 + a2x2 \u2265 3K, but not Ja1x1 + a3x3 \u2265 3K. The benefit here is that a single forward and backward pass through the terms is sufficient to channel the values of all these literals.\nOn the forward pass, we aggregate the lower bound on \u2211k\ni=1 aixi, where we start with \u22110 i=1 aixi \u2265 0 and each subsequent term \u2211k i=1 aixi is greater than or\nequal to either the lower bound of the previous term \u2211k\u22121\ni=1 aixi plus the lower\nbound of akxk, or to v where v is the largest value such that r\u2211k i=1 aixi \u2265 v z is currently true. Similarly, on the backward pass, we aggregate the maximum\nvalue of \u2211k i=1 aixi, where we start with \u2211n\ni=1 aixi \u2264 a0, and each subsequent term \u2211k i=1 aixi is less than or equal to either the upper bound of the previous\nterm \u2211k+1\ni=1 aixi minus the lower bound of akxk, or to v \u2212 1 where v is the smallest value such that r\u2211k i=1 aixi \u2265 v z is currently false. These allow us to fix any of the values of the partial sum literals which should be fixed, and we can propagate upper bounds on akxk using the difference between the lower bound\nof \u2211k i=1 aixi and the upper bound of \u2211k+1 i=1 aixi.\nThese partial sum literals can be lazily introduced only as needed, i.e., when we need to use one of them in a nogood. Whenever the nogood database is cleaned to remove inactive nogoods, we also remove any partial sum literal that is no longer in any nogood. To reduce overhead even further, we can also only allow partial sum literals to be introduced at regular intervals. For example, if\nthe interval was 5, we would only allow literals of the form r\u22115 i=1 aixi \u2265 v z\n, r\u221110\ni=1 aixi \u2265 v z , etc, to be introduced. We claim that this is often sufficient to get most of the benefit of the language extension. Thus we can trade off less overhead for a smaller reduction in proof size.\nNow we need to pick the ordering that gives us the most useful partial sum literals. Many constraint problems have structures such that each variable is only strongly related to a small subset of other variables. For example, in a disjunctive scheduling problem, tasks which have overlapping time intervals may be strongly related, while tasks whose time intervals are far apart may be weakly related. Or in a graph colouring problem, adjacent nodes are strongly related, but nodes far apart in the graph are weakly related. Such structure can be exploited in order to give smaller resolution proofs. A good search strategy will label variables which are strongly related to those already fixed, rather than to pick some completely random variable to label. This improves propagation and also allows stronger nogoods to be derived. It is well known that techniques such as caching [25], variable elimination [16], dynamic programming [4], and nogood learning allows a problem to be solved with a complexity that is only exponential in the width of the search order (assuming sufficient memory). We claim that the ordering which minimizes this width is also the ideal ordering to use in order to introduce the partial sum literals as it provides the most generality to the nogoods.\nExample 7. Consider a simple problem with variables x1, . . . , x10 \u2208 {1, . . . , 5}, constraints all diff (xi, xi+1, xi+2) for i = 1, . . . , 8 and objective function \u2211 xi to be minimized. Given that the all diff constraints constrain sets of consectutive variables, an ordering which minimizes the width is to label x1, . . . , x10 in order. Suppose we are trying to find a solution with objective \u2264 21. Suppose we made the decisions x1 = 1, x2 = 2, x3 = 3, x4 = 4, x5 = 5 in that order. Propagation forces x6, . . . , x10 \u2264 2, causing the all diff \u2019s to fail. If we used the ordering x1, . . . , x10 to introduce partial sum literals, the 1UIP nogood would be:r\u22115\ni=1 xi \u2265 15 z \u2192 false. If we used the ordering x1, x3, x5, x7, x9, x2, x4, x6, x8, x10\nto introduce partial sum literals instead, the 1UIP would be: Jx2 \u2265 2K\u2227Jx4 \u2265 4K\u2227 Jx1 + x3 + x5 \u2265 9K\u2192 false, which is far less general. ut\nIn our experiments, we manually find a low width ordering of the variables. However, it is easy to automate this by using an approximate algorithm for calculating the pathwidth of the constraint graph (e.g., [5]) to give a good variable ordering."}, {"heading": "6 Experiments", "text": "We perform 4 sets of experiments. The experiments were performed on Xeon Pro 2.4GHz processors using the state of the art LCG solver Chuffed. We use 8 problems. For brevity we only describe the global constraints and the structural order we used to create partial sum literals in each problem with linear constraints. MiniZinc models of these problems can be found at www.cs. mu.oz.au/~pjs/ext-res/. The knapsack problem has linear constraints. We pick an ordering which sorts the items such that the profit to weight ratio is descending. The concert hall problem [17] and talent scheduling problem [11] are scheduling problems with linear constraints. We pick an ordering based on time from earliest to latest. The maximum density still life problem (CSPLib prob032) is a board type problem with linear constraints. We pick an ordering which goes row by row from top to bottom and left to right. The PC-board problem [18] and the balanced incomplete block design problem (BIBD) [19] are matrix problems with linear constraints. BIBD also has lex lesseq symmetry breaking constraints. We use an ordering which goes row by row from top to bottom. The nonograms problem [28] is a board type problem with regular constraints. The jobshop scheduling problem [12] has disjunctive constraints. We use a timeout of 600 seconds. In each table, fails is the geometric mean of the the number of fails in the search, time is the geometric mean of the time of search in seconds (with timeouts counting as 600), and svd is the number of instances solved to optimality. Note that we use propagators of the exact same propagation strength in all the methods for all experiments, so any difference is purely due to nogood learning.\nThe first experiment compares a CP solver without nogood learning (nongl) with nogood learning using the basic language of equality, disequality and inequality literals on existing variables (basic-ngl), and nogood learning with the language extensions described in Section 4 (er-ngl). For the 6 problems with linear constraints, we use a fixed order search based on the structural ordering we described above. We will test the same 6 problems with a dynamic search in the fourth experiment. For nonograms and jobshop, we use the weighted degree search heuristic [6], which works well for these two problems. The results are shown in Table 1. Clearly, we can get very significant reductions in node counts on a wide variety of problems. However, depending on how large the node reduction is and the overhead of extending the language, we may not always get a speedup (e.g., BIBD). The node reduction tends to grow exponentially with problem size.\nIn the second set of experiments, we test what happens if we only introduce partial sum literals every 5, 10, 20 or 50 variables as described in Section 5.\nFor ease of comparison, we also repeat the er-ngl column from above, where we introduced partial sum literals after every variable. The results are shown in Table 2. The trend is very clear here. The fewer partial sum literals we add, the less reduction in node count we have. However, it also requires less overhead. For many of the instances, introducing partial sum literals every 5 to 10 variables is optimal.\nWe now compare different ways of picking the order for creating partial sum literals. We compare the structure based ordering (struct) which we used in the previous experiments, a random ordering (random), and an ordering based on sorting on the size of the coefficients in descending order (coeff). We also repeat the no language extension column basic and the er-ngl (renamed to struct) from the first experiment for ease of comparison. The results are shown in Table 3. It can be seen that even when we use an ordering which is inconsistent with the structure of the problem, we can still get some reduction in node count. However, the much smaller reduction in node count means that the overhead may often swamp out any benefit from the reduced search. A random ordering generally gives the least reduction in node count out of the three and the structure based ordering generally gives the most. An ordering based on the size of the coefficients is somewhere in the middle, depending on whether the coefficients happen to follow the structure of the problem or not.\nFinally, we test whether the node reduction we gain from partial sum literals is dependent on a specific search order, or whether we will benefit even if we use a dynamic search strategy. We use the variable state independent decaying sum\n(VSIDS) heuristic [20] adapted from SAT. This is the standard search heuristic used in most current state of the art SAT solvers and is also very effective for some CP problems. For easy comparison, we also repeat the column er-ngl from Table 1, which we rename to er-ngl-fixed. We call VSIDS on the basic language basic-ngl-vsids and on the extended language er-ngl-vsids. The results are shown in Table 4. It can be seen that extending the language reduces the node count on all the problems tested. However, VSIDS is not as capable of exploiting the new literals as a fixed order search using the structure based ordering in knapsack, concert hall, talent scheduling or maximum density still life. On PC board and BIBD, VSIDS is far superior to the fixed order search even without any language extension. The language extension does result in a reduced node count, but the overhead swamps out any benefits."}, {"heading": "7 Related Work", "text": "The work presented here is closely related to the body of work on Boolean encodings for global constraints in SAT. Better Boolean encodings can be smaller in size and can also improve the power of the proof system, leading to faster solves (e.g., [24, 3, 9]). However, our approach has several advantages. When developing Boolean encodings for global constraints for use in a SAT solver, the primary concerns are: 1) the size of the encoding, and 2) its propagation strength. All of the literals and clauses in the encoding must be statically created in the SAT solver. As a result, the encoding has to be reasonably small or the SAT solver will run out of memory or slow to a crawl. These concerns are far less important in the context of LCG. This is because an LCG solver does not ever need to produce a static Boolean encoding of the global constraint. Instead, the high level CP global propagator is fully responsible for all propagation (so we always get the full propagation strength), and it lazily creates literals and clauses as needed. As a result, even if there are potentially an exponential number of possible literals and clauses, it is typically still fine, because during any one solve, only a very small proportion of those literals and clauses will need to be created, and they can be thrown away as soon as they are no longer useful [10]. This flexibility means that we are able to consider other important factors such as which literals will improve the power of the proof system. For example, when a SAT solver encodes a pseudo Boolean constraint into clauses via a BDD translation, the variable order must be chosen to make the BDD small, which may not be ideal for the power of the proof system. On the other hand, we can pick an order which is better for the power of the proof system even if the potential number of literals introduced is very large.\nThe closest related work to that presented here is conflict directed lazy decomposition [1]. Lazy decomposition treats a global propagator for c as a black box that hides a SAT encoding that implements c. As computation progresses the propagator for c lazily exposes more and more of this SAT encoding if an\nactivity heuristic indicates that this may be beneficial. These exposed Boolean variables give us a structure based extension to the language. However, lazy decomposition is complex to apply, as one must be able to effectively split a propagator into two parts. The only constraints for which lazy decomposition is defined in [1] are cardinality and pseudo-Boolean constraints. Lazy decomposition has the advantage that it uses the activity of literals in the search as a heuristic to determine whether adding the partial sum literals will be useful. It has the disadvantage that it does not use the global structure to determine which intermediate literals to add. Instead, for psuedo-Boolean constraints it uses the size of the coefficients to order the partial sums, as this tends to reduce the size of the Boolean encoding. However, as Table 3 shows, using a ordering different from the structural one can nullify most of the search space reduction.\nThere have been several works on using extended resolution in clause learning SAT solvers. In [2], the extension considered are of the form: if l\u03041 \u2228\u03b1 and l\u03042 \u2228\u03b1 are two successively derived nogoods, then add the new literal z defined via z \u2194 l1 \u2228 l2. In [13], another extension rule is proposed where they add a new literal z \u2194 d\u03041\u2228. . .\u2228d\u0304k where the di are a subset of the assignments which led to a conflict. While these extensions appears useful for some SAT instances, it seems unlikely that these methods will be able to generate the right extensions for CP problems. For example, a partial sum literal such as Jx1 + x2 \u2265 5K is defined by Jx1 + x2 \u2265 5K \u2194 . . . \u2228 (Jx1 \u2265 0K \u2227 Jx2 \u2265 5K) \u2228 (Jx1 \u2265 1K \u2227 Jx2 \u2265 4K) \u2228 . . .. Considering the size of the definition of a general partial sum literal, it will take an incredible amount of luck for one of the above methods to introduce a literal that means exactly a partial sum literal. It is far more effective to keep the structural information contained in a high level model and to use that to introduce useful literals. Once a problem has been converted into conjunctive normal form, so much of the structural information has been lost that it is very difficult for any automated methods to be able to \u201crediscover\u201d the literals that matter."}, {"heading": "8 Conclusion", "text": "A significant amount of research in CP has focused on improving the power of the proof systems used in CP solvers by developing more powerful global propagators. However, such research may well be nearing their limits as the optimal propagators for most commonly used constraints are already known. Nogood learning provides an orthogonal way in which to improve the power of the proof systems used in CP solvers. Extending the language of resolution changes the power of the resolution proof system used in nogood learning and can exponentially reduce the size of a proof of unsatisfiability or optimality, leading to much faster CP solving. The primary difficulty of using extended resolution is in finding the right language extensions to make. We have given a framework for analyzing the generality of explanations made by global propagators in LCG solvers, and shown that language extensions which improve the generality of these explanations are excellent candidates for language extension. Experiments show that such structure based extended resolution can be highly beneficial in solving a wide range of combinatorial optimization problems."}], "references": [{"title": "Conflict directed lazy decomposition", "author": ["I. Abio", "P.J. Stuckey"], "venue": "In Proc. CP 2012,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A restriction of extended resolution for clause learning sat solvers", "author": ["Gilles Audemard", "George Katsirelos", "Laurent Simon"], "venue": "In Proc. of AAAI", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Efficient cnf encoding of boolean cardinality constraints", "author": ["Olivier Bailleux", "Yacine Boufkhad"], "venue": "In Proc. CP", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Dynamic programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1957}, {"title": "Approximating treewidth, pathwidth, frontsize, and shortest elimination tree", "author": ["Hans L. Bodlaender", "John R. Gilbert", "Hj\u00e1lmtyr Hafsteinsson", "Ton Kloks"], "venue": "J. Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Boosting systematic search by weighting constraints", "author": ["Fr\u00e9d\u00e9ric Boussemart", "Fred Hemery", "Christophe Lecoutre", "Lakhdar Sais"], "venue": "In Procs. of ECAI04,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A short proof of the pigeon hole principle using extended resolution", "author": ["Stephen A. Cook"], "venue": "SIGACT News,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1976}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Translating pseudo-boolean constraints into sat", "author": ["Niklas E\u00e9n", "Niklas S\u00f6rensson"], "venue": "JSAT, 2(1-4):1\u201326,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Lazy clause generation reengineered", "author": ["Thibaut Feydy", "Peter J. Stuckey"], "venue": "In Proc. of CP 2009,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Solving talent scheduling with dynamic programming", "author": ["Maria Garcia de la Banda", "Peter J. Stuckey", "Geoffrey Chu"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The complexity of flowshop and jobshop scheduling", "author": ["Michael R Garey", "David S Johnson", "Ravi Sethi"], "venue": "Mathematics of operations research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1976}, {"title": "Extended clause learning", "author": ["Jinbo Huang"], "venue": "Artif. Intell.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unrestricted nogood recording in CSP search", "author": ["G. Katsirelos", "F. Bacchus"], "venue": "In Proc. of CP 2003,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Generalized nogoods in CSPs", "author": ["G. Katsirelos", "F. Bacchus"], "venue": "In The Twentieth National Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Boosting Search with Variable Elimination in Constraint Optimization and Constraint Satisfaction", "author": ["Javier Larrosa", "Rina Dechter"], "venue": "Problems. Constraints,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Symmetry breaking constraints for value symmetries in constraint satisfaction", "author": ["Y. Law", "J. Lee"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The challenge of exploiting weak symmetries", "author": ["Roland Martin"], "venue": "In Proc. of the International Workshop on Constraint Solving and Constraint Logic Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Solving strategies for highly symmetric csps", "author": ["Pedro Meseguer", "Carme Torras"], "venue": "In Proc. of IJCAI", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Chaff: Engineering an Efficient SAT Solver", "author": ["Matthew W. Moskewicz", "Conor F. Madigan", "Ying Zhao", "Lintao Zhang", "Sharad Malik"], "venue": "In Proceedings of the 38th Design Automation Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Propagation via lazy clause generation", "author": ["Olga Ohrimenko", "Peter J. Stuckey", "Michael Codish"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "On the power of clause-learning sat solvers as resolution engines", "author": ["Knot Pipatsrisawat", "Adnan Darwiche"], "venue": "Artif. Intell.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Machine-oriented logic based on the resolution principle", "author": ["A.J. Robinson"], "venue": "Journal of the ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1965}, {"title": "Towards an optimal cnf encoding of boolean cardinality constraints", "author": ["Carsten Sinz"], "venue": "In Proc. CP", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Caching Search States in Permutation Problems", "author": ["Barbara M. Smith"], "venue": "In Proc. of CP 2005,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "On the complexity of derivation in propositional calculus, chapter Studies in Constructive Mathematics and Mathematical Logic", "author": ["G. Tseitin"], "venue": "Steklov Mathematical Institute,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1970}, {"title": "The complexity of propositional proofs", "author": ["Alasdair Urquhart"], "venue": "Bulletin of the EATCS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "An efficient algorithm for solving nonograms", "author": ["Chiung-Hsueh Yu", "Hui-Lung Lee", "Ling-Hwei Chen"], "venue": "Appl. Intell.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "The current state of the art is Lazy Clause Generation [21] (LCG).", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "It has recently been proved [22] that a SAT solver performing conflict directed clause learning and restarts has a proof system that is as powerful as general resolution [23].", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "It has recently been proved [22] that a SAT solver performing conflict directed clause learning and restarts has a proof system that is as powerful as general resolution [23].", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "Extended resolution [26] is a proof system even stronger than general resolution and is one of the most powerful proof systems for propositional logic [27].", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "Extended resolution [26] is a proof system even stronger than general resolution and is one of the most powerful proof systems for propositional logic [27].", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "It is well known that there can be an exponential separation between the size of the proof generated by extended resolution and general resolution on certain problems [7].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": ", [2, 13]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 12, "context": ", [2, 13]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 7, "context": "[8], chapter 6) only considers equality literals of the form x = v where x is a variable and v is a value.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Later on, disequality literals of the form x 6= v were introduced [14, 15].", "startOffset": 66, "endOffset": 74}, {"referenceID": 14, "context": "Later on, disequality literals of the form x 6= v were introduced [14, 15].", "startOffset": 66, "endOffset": 74}, {"referenceID": 20, "context": "More recently, inequality literals of the form x \u2265 v, x \u2264 v were introduced in LCG [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Consider a disjunctive constraint disjunctive([s1, s2], [5, 5]) over two tasks with start times having current domains of s1 \u2208 {2, .", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "Consider a disjunctive constraint disjunctive([s1, s2], [5, 5]) over two tasks with start times having current domains of s1 \u2208 {2, .", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 3, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 53, "endOffset": 65}, {"referenceID": 3, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 67, "endOffset": 79}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 81, "endOffset": 93}, {"referenceID": 2, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 1, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 0, "context": "Consider a table constraint table([x1, x2, x3, x4], [[1, 2, 3, 4], [4, 3, 2, 1], [1, 2, 2, 3], [3, 1, 2, 1], [1, 1, 1, 1]]).", "startOffset": 109, "endOffset": 121}, {"referenceID": 24, "context": "It is well known that techniques such as caching [25], variable elimination [16], dynamic programming [4], and nogood learning allows a problem to be solved with a complexity that is only exponential in the width of the search order (assuming sufficient memory).", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "It is well known that techniques such as caching [25], variable elimination [16], dynamic programming [4], and nogood learning allows a problem to be solved with a complexity that is only exponential in the width of the search order (assuming sufficient memory).", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "It is well known that techniques such as caching [25], variable elimination [16], dynamic programming [4], and nogood learning allows a problem to be solved with a complexity that is only exponential in the width of the search order (assuming sufficient memory).", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": ", [5]) to give a good variable ordering.", "startOffset": 2, "endOffset": 5}, {"referenceID": 16, "context": "The concert hall problem [17] and talent scheduling problem [11] are scheduling problems with linear constraints.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "The concert hall problem [17] and talent scheduling problem [11] are scheduling problems with linear constraints.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "The PC-board problem [18] and the balanced incomplete block design problem (BIBD) [19] are matrix problems with linear constraints.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "The PC-board problem [18] and the balanced incomplete block design problem (BIBD) [19] are matrix problems with linear constraints.", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "The nonograms problem [28] is a board type problem with regular constraints.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "The jobshop scheduling problem [12] has disjunctive constraints.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "For nonograms and jobshop, we use the weighted degree search heuristic [6], which works well for these two problems.", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": "(VSIDS) heuristic [20] adapted from SAT.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": ", [24, 3, 9]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 2, "context": ", [24, 3, 9]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": ", [24, 3, 9]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 9, "context": "As a result, even if there are potentially an exponential number of possible literals and clauses, it is typically still fine, because during any one solve, only a very small proportion of those literals and clauses will need to be created, and they can be thrown away as soon as they are no longer useful [10].", "startOffset": 306, "endOffset": 310}, {"referenceID": 0, "context": "The closest related work to that presented here is conflict directed lazy decomposition [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "The only constraints for which lazy decomposition is defined in [1] are cardinality and pseudo-Boolean constraints.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "In [2], the extension considered are of the form: if l\u03041 \u2228\u03b1 and l\u03042 \u2228\u03b1 are two successively derived nogoods, then add the new literal z defined via z \u2194 l1 \u2228 l2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [13], another extension rule is proposed where they add a new literal z \u2194 d\u03041\u2228.", "startOffset": 3, "endOffset": 7}], "year": 2013, "abstractText": "Nogood learning is a powerful approach to reducing search in Constraint Programming (CP) solvers. The current state of the art, called Lazy Clause Generation (LCG), uses resolution to derive nogoods expressing the reasons for each search failure. Such nogoods can prune other parts of the search tree, producing exponential speedups on a wide variety of problems. Nogood learning solvers can be seen as resolution proof systems. The stronger the proof system, the faster it can solve a CP problem. It has recently been shown that the proof system used in LCG is at least as strong as general resolution. However, stronger proof systems such as extended resolution exist. Extended resolution allows for literals expressing arbitrary logical concepts over existing variables to be introduced and can allow exponentially smaller proofs than general resolution. The primary problem in using extended resolution is to figure out exactly which literals are useful to introduce. In this paper, we show that we can use the structural information contained in a CP model in order to introduce useful literals, and that this can translate into significant speedups on a range of problems.", "creator": "TeX"}}}