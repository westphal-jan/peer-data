{"id": "1306.4650", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2013", "title": "Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization", "abstract": "from majorization - polynomial minimization our algorithms must consist of : iteratively minimizing a symmetric majorizing optimal surrogate sample of an objective smooth function. because of its simplicity and showing its very wide practical applicability, repeating this principle thereof has been very immensely popular in statistics and in signal function processing. in finishing this paper, we indeed intend to make this principle scalable. we actually introduce and study a stochastic majorization - minimization scheme, which indeed is able precisely to deal informally with most large - geometric scale or least possibly nonlinear infinite infinite data dense sets. right when applied to totally convex supervised optimization problems under externally suitable optimization assumptions, accordingly we show, that because it achieves an expected convergence data rate of weights o ( : 1 / \\ sqrt { \u03c9 n } ) after n * iterations, \u03c6 and o ( 1 / n ) consistency for strongly augmented convex functions. equally important, our scheme almost of surely converges to three stationary points chosen for a large weight class domain of non - convex computing problems. we must derive from to our framework several infinitely efficient algorithms.! first, we propose with a familiar new stochastic proximal slope gradient representation method, engine which experimentally matches state - products of - the - art solvers available for large - scale l1 - distributed logistic regression. > second, we develop an appropriate online local dc programming algorithm for non - convex sparse estimation. finally,, we demonstrate the expected effectiveness results of our approximate technique methodology for solving generalized large - numerical scale weighted structured matrix data factorization problems.", "histories": [["v1", "Wed, 19 Jun 2013 19:21:48 GMT  (292kb)", "https://arxiv.org/abs/1306.4650v1", null], ["v2", "Tue, 10 Sep 2013 12:29:41 GMT  (299kb)", "http://arxiv.org/abs/1306.4650v2", "accepted for publication for Neural Information Processing Systems (NIPS) 2013. This is the 9-pages version followed by 16 pages of appendices. The title has changed compared to the first technical report"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["julien mairal"], "accepted": true, "id": "1306.4650"}, "pdf": {"name": "1306.4650.pdf", "metadata": {"source": "CRF", "title": "Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization", "authors": ["Julien Mairal"], "emails": ["julien.mairal@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n46 50\nv2 [\nst at\n.M L\n] 1\n0 Se\n\u221a n) after n iterations, and of O(1/n) for strongly convex functions.\nEqually important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale \u21131logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems."}, {"heading": "1 Introduction", "text": "Majorization-minimization [15] is a simple optimization principle for minimizing an objective function. It consists of iteratively minimizing a surrogate that upper-bounds the objective, thus monotonically driving the objective function value downhill. This idea is used in many existing procedures. For instance, the expectation-maximization (EM) algorithm (see [5, 21]) builds a surrogate for a likelihood model by using Jensen\u2019s inequality. Other approaches can also be interpreted under the majorization-minimization point of view, such as DC programming [8], where \u201cDC\u201d stands for difference of convex functions, variational Bayes techniques [28], or proximal algorithms [1, 23, 29].\nIn this paper, we propose a stochastic majorization-minimization algorithm, which is is suitable for solving large-scale problems arising in machine learning and signal processing. More precisely, we address the minimization of an expected cost\u2014that is, an objective function that can be represented by an expectation over a data distribution. For such objectives, online techniques based on stochastic approximations have proven to be particularly efficient, and have drawn a lot of attraction in machine learning, statistics, and optimization [3\u20136, 9\u201312, 14, 16, 17, 19, 22, 24\u201326, 30].\nOur scheme follows this line of research. It consists of iteratively building a surrogate of the expected cost when only a single data point is observed at each iteration; this data point is used to update the surrogate, which in turn is minimized to obtain a new estimate. Some previous works are closely related to this scheme: the online EM algorithm for latent data models [5, 21] and the online matrix factorization technique of [19] involve for instance surrogate functions updated in a similar fashion. Compared to these two approaches, our method is targeted to more general optimization problems.\nAnother related work is the incremental majorization-minimization algorithm of [18] for finite training sets; it was indeed shown to be efficient for solving machine learning problems where storing\ndense information about the past iterates can be afforded. Concretely, this incremental scheme requires to store O(pn) values, where p is the variable size, and n is the size of the training set.1 This issue was the main motivation for us for proposing a stochastic scheme with a memory load independent of n, thus allowing us to possibly deal with infinite data sets, or a huge variable size p.\nWe study the convergence properties of our algorithm when the surrogates are strongly convex and chosen among the class of first-order surrogate functions introduced in [18], which consist of approximating the possibly non-smooth objective up to a smooth error. When the objective is convex, we obtain expected convergence rates that are asymptotically optimal, or close to optimal [14, 22]. More precisely, the convergence rate is of order O(1/ \u221a n) in a finite horizon setting, and O(1/n) for a strongly convex objective in an infinite horizon setting. Our second analysis shows that for nonconvex problems, our method almost surely converges to a set of stationary points under suitable assumptions. We believe that this result is equally valuable as convergence rates for convex optimization. To the best of our knowledge, the literature on stochastic non-convex optimization is rather scarce, and we are only aware of convergence results in more restricted settings than ours\u2014see for instance [3] for the stochastic gradient descent algorithm, [5] for online EM, [19] for online matrix factorization, or [9], which provides stronger guarantees, but for unconstrained smooth problems.\nWe develop several efficient algorithms based on our framework. The first one is a new stochastic proximal gradient method for composite or constrained optimization. This algorithm is related to a long series of work in the convex optimization literature [6,10,12,14,16,22,25,30], and we demonstrate that it performs as well as state-of-the-art solvers for large-scale \u21131-logistic regression [7]. The second one is an online DC programming technique, which we demonstrate to be better than batch alternatives for large-scale non-convex sparse estimation [8]. Finally, we show that our scheme can address efficiently structured sparse matrix factorization problems in an online fashion, and offers new possibilities to [13, 19] such as the use of various loss or regularization functions.\nThis paper is organized as follows: Section 2 introduces first-order surrogate functions for batch optimization; Section 3 is devoted to our stochastic approach and its convergence analysis; Section 4 presents several applications and numerical experiments, and Section 5 concludes the paper."}, {"heading": "2 Optimization with First-Order Surrogate Functions", "text": "Throughout the paper, we are interested in the minimization of a continuous function f : Rp \u2192 R:\nmin \u03b8\u2208\u0398 f(\u03b8), (1)\nwhere\u0398 \u2286 Rp is a convex set. The majorization-minimization principle consists of computing a majorizing surrogate gn of f at iteration n and updating the current estimate by \u03b8n \u2208 argmin\u03b8\u2208\u0398 gn(\u03b8). The success of such a scheme depends on how well the surrogates approximate f . In this paper, we consider a particular class of surrogate functions introduced in [18] and defined as follows:\nDefinition 2.1 (Strongly Convex First-Order Surrogate Functions). Let \u03ba be in \u0398. We denote by SL,\u03c1(f, \u03ba) the set of \u03c1-strongly convex functions g such that g \u2265 f , g(\u03ba) = f(\u03ba), the approximation error g \u2212 f is differentiable, and the gradient \u2207(g \u2212 f) is LLipschitz continuous. We call the functions g in SL,\u03c1(f, \u03ba) \u201cfirst-order surrogate functions\u201d.\nAmong the first-order surrogate functions presented in [18], we should mention the following ones: \u2022 Lipschitz Gradient Surrogates. When f is differentiable and \u2207f is L-Lipschitz, f admits the following surrogate g in S2L,L(f, \u03ba):\ng : \u03b8 7\u2192 f(\u03ba) +\u2207f(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + L 2 \u2016\u03b8 \u2212 \u03ba\u201622.\nWhen f is convex, g is in SL,L(f, \u03ba), and when f is \u00b5-strongly convex, g is in SL\u2212\u00b5,L(f, \u03ba). Minimizing g amounts to performing a classical classical gradient descent step \u03b8 \u2190 \u03ba\u2212 1\nL \u2207f(\u03ba).\n\u2022 Proximal Gradient Surrogates. Assume that f splits into f = f1 + f2, where f1 is differentiable, \u2207f1 is L-Lipschitz, and f2 is\n1To alleviate this issue, it is possible to cut the dataset into \u03b7 mini-batches, reducing the memory load to O(p\u03b7), which remains cumbersome when p is very large.\nconvex. Then, the function g below is in S2L,L(f, \u03ba):\ng : \u03b8 7\u2192 f1(\u03ba) +\u2207f1(\u03ba)\u22a4(\u03b8 \u2212 \u03ba) + L\n2 \u2016\u03b8 \u2212 \u03ba\u201622 + f2(\u03b8).\nWhen f1 is convex, g is in SL,L(f, \u03ba). If f1 is \u00b5-strongly convex, g is in SL\u2212\u00b5,L(f, \u03ba). Minimizing g amounts to a proximal gradient step [1, 23, 29]: \u03b8 \u2190 argmin\u03b8 12\u2016\u03ba\u2212 1L\u2207f1(\u03ba)\u2212 \u03b8\u201622 + 1Lf2(\u03b8). \u2022 DC Programming Surrogates. Assume that f = f1 + f2, where f2 is concave and differentiable, \u2207f2 is L2-Lipschitz, and g1 is in SL1,\u03c11(f1, \u03ba), Then, the following function g is a surrogate in SL1+L2,\u03c11(f, \u03ba): g : \u03b8 7\u2192 f1(\u03b8) + f2(\u03ba) +\u2207f2(\u03ba)\u22a4(\u03b8 \u2212 \u03ba). When f1 is convex, f1 + f2 is a difference of convex functions, leading to a DC program [8].\nWith the definition of first-order surrogates and a basic \u201cbatch\u201d algorithm in hand, we now introduce our main contribution: a stochastic scheme for solving large-scale problems."}, {"heading": "3 Stochastic Optimization", "text": "As pointed out in [4], one is usually not interested in the minimization of an empirical cost on a finite training set, but instead in minimizing an expected cost. Thus, we assume from now on that f has the form of an expectation:\nmin \u03b8\u2208\u0398\n[ f(\u03b8) , Ex[\u2113(x, \u03b8)] ] , (2)\nwhere x from some set X represents a data point, which is drawn according to some unknown distribution, and \u2113 is a continuous loss function. As often done in the literature [22], we assume that the expectations are well defined and finite valued; we also assume that f is bounded below.\nWe present our approach for tackling (2) in Algorithm 1. At each iteration, we draw a training point xn, assuming that these points are i.i.d. samples from the data distribution. Note that in practice, since it is often difficult to obtain true i.i.d. samples, the points xn are computed by cycling on a randomly permuted training set [4]. Then, we choose a surrogate gn for the function \u03b8 7\u2192 \u2113(xn, \u03b8), and we use it to update a function g\u0304n that behaves as an approximate surrogate for the expected cost f . The function g\u0304n is in fact a weighted average of previously computed surrogates, and involves a sequence of weights (wn)n\u22651 that will be discussed later. Then, we minimize g\u0304n, and obtain a new estimate \u03b8n. For convex problems, we also propose to use averaging schemes, denoted by \u201coption 2\u201d and \u201coption 3\u201d in Alg. 1. Averaging is a classical technique for improving convergence rates in convex optimization [10, 22] for reasons that are clear in the convergence proofs.\nAlgorithm 1 Stochastic Majorization-Minimization Scheme input \u03b80 \u2208 \u0398 (initial estimate); N (number of iterations); (wn)n\u22651, weights in (0, 1];\n1: initialize the approximate surrogate: g\u03040 : \u03b8 7\u2192 \u03c12\u2016\u03b8 \u2212 \u03b80\u201622; \u03b8\u03040 = \u03b80; \u03b8\u03020 = \u03b80; 2: for n = 1, . . . , N do 3: draw a training point xn; define fn : \u03b8 7\u2192 \u2113(xn, \u03b8); 4: choose a surrogate function gn in SL,\u03c1(fn, \u03b8n\u22121); 5: update the approximate surrogate: g\u0304n = (1\u2212 wn)g\u0304n\u22121 + wngn; 6: update the current estimate:\n\u03b8n \u2208 argmin \u03b8\u2208\u0398 g\u0304n(\u03b8);\n7: for option 2, update the averaged iterate: \u03b8\u0302n , (1\u2212 wn+1)\u03b8\u0302n\u22121 + wn+1\u03b8n; 8: for option 3, update the averaged iterate: \u03b8\u0304n ,\n(1\u2212wn+1)\u03b8\u0304n\u22121+wn+1\u03b8n\u2211n+1 k=1 wk ;\n9: end for output (option 1): \u03b8N (current estimate, no averaging); output (option 2): \u03b8\u0304N (first averaging scheme); output (option 3): \u03b8\u0302N (second averaging scheme).\nWe remark that Algorithm 1 is only practical when the functions g\u0304n can be parameterized with a small number of variables, and when they can be easily minimized over \u0398. Concrete examples are discussed in Section 4. Before that, we proceed with the convergence analysis."}, {"heading": "3.1 Convergence Analysis - Convex Case", "text": "First, We study the case of convex functions fn : \u03b8 7\u2192 \u2113(\u03b8,xn), and make the following assumption:\n(A) for all \u03b8 in \u0398, the functions fn are R-Lipschitz continuous. Note that for convex functions, this is equivalent to saying that subgradients of fn are uniformly bounded by R.\nAssumption (A) is classical in the stochastic optimization literature [22]. Our first result shows that with the averaging scheme corresponding to \u201coption 2\u201d in Alg. 1, we obtain an expected convergence rate that makes explicit the role of the weight sequence (wn)n\u22651. Proposition 3.1 (Convergence Rate). When the functions fn are convex, under assumption (A), and when \u03c1 = L, we have\nE[f(\u03b8\u0304n\u22121)\u2212 f\u22c6] \u2264 L\u2016\u03b8\u22c6 \u2212 \u03b80\u201622 + R\n2\nL \u2211n k=1 w 2 k\n2 \u2211n k=1 wk for all n \u2265 1, (3)\nwhere \u03b8\u0304n\u22121 is defined in Algorithm 1, \u03b8\u22c6 is a minimizer of f on \u0398, and f\u22c6 , f(\u03b8\u22c6).\nSuch a rate is similar to the one of stochastic gradient descent with averaging, see [22] for example. Note that the constraint \u03c1 = L here is compatible with the proximal gradient surrogate. From Proposition 3.1, it is easy to obtain a O(1/ \u221a n) bound for a finite horizon\u2014that is, when the total number of iterations n is known in advance. When n is fixed, such a bound can indeed be obtained by plugging constant weights wk = \u03b3/ \u221a n for all k \u2264 n in Eq. (3). Note that the upper-\nboundO(1/ \u221a n) cannot be improved in general without making further assumptions on the objective function [22]. The next corollary shows that in an infinite horizon setting and with decreasing weights, we lose a logarithmic factor compared to an optimal convergence rate [14,22] of O(1/ \u221a n).\nCorollary 3.1 (Convergence Rate - Infinite Horizon - Decreasing Weights). Let us make the same assumptions as in Proposition 3.1 and choose the weights wn = \u03b3/ \u221a n. Then,\nE[f(\u03b8\u0304n\u22121)\u2212 f\u22c6] \u2264 L\u2016\u03b8\u22c6 \u2212 \u03b80\u201622\n2\u03b3 \u221a n\n+ R2\u03b3(1 + log(n))\n2L \u221a n\n, \u2200n \u2265 2.\nOur analysis suggests to use weights of the form O(1/ \u221a n). In practice, we have found that choosing wn = \u221a n0 + 1/ \u221a n0 + n performs well, where n0 is tuned on a subsample of the training set."}, {"heading": "3.2 Convergence Analysis - Strongly Convex Case", "text": "In this section, we introduce an additional assumption:\n(B) the functions fn are \u00b5-strongly convex.\nWe show that our method achieves a rate O(1/n), which is optimal up to a multiplicative constant for strongly convex functions (see [14, 22]). Proposition 3.2 (Convergence Rate). Under assumptions (A) and (B), with \u03c1 = L+ \u00b5. Define \u03b2 , \u00b5\n\u03c1 and wn , 1+\u03b2 1+\u03b2n . Then,\nE[f(\u03b8\u0302n\u22121)\u2212 f\u22c6] + \u03c1\n2 E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u201622] \u2264 max\n(\n2R2\n\u00b5 , \u03c1\u2016\u03b8\u22c6 \u2212 \u03b80\u201622\n)\n1\n\u03b2n+ 1 for all n \u2265 1,\nwhere \u03b8\u0302n is defined in Algorithm 1, when choosing the averaging scheme called \u201coption 3\u201d.\nThe averaging scheme is slightly different than in the previous section and the weights decrease at a different speed. Again, this rate applies to the proximal gradient surrogates, which satisfy the constraint \u03c1 = L+ \u00b5. In the next section, we analyze our scheme in a non-convex setting."}, {"heading": "3.3 Convergence Analysis - Non-Convex Case", "text": "Convergence results for non-convex problems are by nature weak, and difficult to obtain for stochastic optimization [4, 9]. In such a context, proving convergence to a global (or local) minimum is out of reach, and classical analyses study instead asymptotic stationary point conditions, which involve directional derivatives (see [2, 18]). Concretely, we introduce the following assumptions:\n(C) \u0398 and the support X of the data are compact; (D) The functions fn are uniformly bounded by some constant M ; (E) The weights wn are non-increasing, w1 = 1, \u2211 n\u22651 wn=+\u221e, and \u2211 n\u22651 w 2 n \u221a n<+\u221e; (F) The directional derivatives \u2207fn(\u03b8, \u03b8\u2032 \u2212 \u03b8), and \u2207f(\u03b8, \u03b8\u2032 \u2212 \u03b8) exist for all \u03b8 and \u03b8\u2032 in \u0398.\nAssumptions (C) and (D) combined with (A) are useful because they allow us to use some uniform convergence results from the theory of empirical processes [27]. In a nutshell, these assumptions ensure that the function class {x 7\u2192 \u2113(x, \u03b8) : \u03b8 \u2208 \u0398} is \u201csimple enough\u201d, such that a uniform law of large numbers applies. The assumption (E) is more technical: it resembles classical conditions used for proving the convergence of stochastic gradient descent algorithms, usually stating that the weights wn should be the summand of a diverging sum while the sum of w2n should be finite; the constraint \u2211\nn\u22651 w 2 n\n\u221a n < +\u221e is slightly stronger. Finally, (F) is a mild assumption, which is\nuseful to characterize the stationary points of the problem. A classical necessary first-order condition [2] for \u03b8 to be a local minimum of f is indeed to have \u2207f(\u03b8, \u03b8\u2032\u2212\u03b8) non-negative for all \u03b8\u2032 in \u0398. We call such points \u03b8 the stationary points of the function f . The next proposition is a generalization of a convergence result obtained in [19] in the context of sparse matrix factorization.\nProposition 3.3 (Non-Convex Analysis - Almost Sure Convergence). Under assumptions (A), (C), (D), (E), (f(\u03b8n))n\u22650 converges with probability one. Under assumption (F), we also have that\nlim inf n\u2192+\u221e inf \u03b8\u2208\u0398 \u2207f\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n) \u2016\u03b8 \u2212 \u03b8n\u20162 \u2265 0,\nwhere the function f\u0304n is a weighted empirical risk recursively defined as f\u0304n = (1\u2212wn)f\u0304n\u22121+wnfn. It can be shown that f\u0304n uniformly converges to f .\nEven though f\u0304n converges uniformly to the expected cost f , Proposition 3.3 does not imply that the limit points of (\u03b8n)n\u22651 are stationary points of f . We obtain such a guarantee when the surrogates that are parameterized, an assumption always satisfied when Algorithm 1 is used in practice.\nProposition 3.4 (Non-Convex Analysis - Parameterized Surrogates). Let us make the same assumptions as in Proposition 3.3, and let us assume that the functions g\u0304n are parameterized by some variables \u03ban living in a compact set K of Rd. In other words, g\u0304n can be written as g\u03ban , with \u03ban in K. Suppose there exists a constant K > 0 such that |g\u03ba(\u03b8) \u2212 g\u03ba\u2032(\u03b8)| \u2264 K\u2016\u03ba\u2212 \u03ba\u2032\u20162 for all \u03b8 in \u0398 and \u03ba, \u03ba\u2032 in K. Then, every limit point \u03b8\u221e of the sequence (\u03b8n)n\u22651 is a stationary point of f\u2014that is, for all \u03b8 in \u0398,\n\u2207f(\u03b8\u221e, \u03b8 \u2212 \u03b8\u221e) \u2265 0.\nFinally, we show that our non-convex convergence analysis can be extended beyond first-order surrogate functions\u2014that is, when gn does not satisfy exactly Definition 2.1. This is possible when the objective has a particular partially separable structure, as shown in the next proposition. This extension was motivated by the non-convex sparse estimation formulation of Section 4, where such a structure appears.\nProposition 3.5 (Non-Convex Analysis - Partially Separable Extension). Assume that the functions fn split into fn(\u03b8) = f0,n(\u03b8)+ \u2211K\nk=1 fk,n(\u03b3k(\u03b8)), where the functions \u03b3k :R\np\u2192R are convex and R-Lipschitz, and the fk,n are non-decreasing for k \u2265 1. Consider gn,0 in SL0,\u03c11(f0,n, \u03b8n\u22121), and some non-decreasing functions gk,n in SLk,0(fk,n, \u03b3k(\u03b8n\u22121)). Instead of choosing gn in SL,\u03c1(fn, \u03b8n\u22121) in Alg 1, replace it by gn,\u03b8 7\u2192g0,n(\u03b8)+gk,n(\u03b3k(\u03b8)). Then, Propositions 3.3 and 3.4 still hold."}, {"heading": "4 Applications and Experimental Validation", "text": "In this section, we introduce different applications, and provide numerical experiments. A C++/Matlab implementation is available in the software package SPAMS [19].2 All experiments were performed on a single core of a 2GHz Intel CPU with 64GB of RAM.\n2http://spams-devel.gforge.inria.fr/."}, {"heading": "4.1 Stochastic Proximal Gradient Descent Algorithm", "text": "Our first application is a stochastic proximal gradient descent method, which we call SMM (Stochastic Majorization-Minimization), for solving problems of the form:\nmin \u03b8\u2208\u0398 Ex[\u2113(x, \u03b8)] + \u03c8(\u03b8), (4)\nwhere \u03c8 is a convex deterministic regularization function, and the functions \u03b8 7\u2192 \u2113(x, \u03b8) are differentiable and their gradients are L-Lipschitz continuous. We can thus use the proximal gradient surrogate presented in Section 2. Assume that a weight sequence (wn)n\u22651 is chosen such that w1 = 1. By defining some other weights win recursively as w i n , (1\u2212wn)wi\u22121n for i < n and wnn,wn, our scheme yields the update rule:\n\u03b8n \u2190 argmin \u03b8\u2208\u0398\nn \u2211\ni=1\nwin [ \u2207fi(\u03b8i\u22121)\u22a4\u03b8 + L2 \u2016\u03b8 \u2212 \u03b8i\u22121\u201622 + \u03c8(\u03b8) ] . (SMM)\nOur algorithm is related to FOBOS [6], to SMIDAS [25] or the truncated gradient method [16] (when \u03c8 is the \u21131-norm). These three algorithms use indeed the following update rule:\n\u03b8n \u2190 argmin \u03b8\u2208\u0398 \u2207fn(\u03b8n\u22121)\u22a4\u03b8 + 12\u03b7n \u2016\u03b8 \u2212 \u03b8n\u22121\u2016 2 2 + \u03c8(\u03b8), (FOBOS)\nAnother related scheme is the regularized dual averaging (RDA) of [30], which can be written as\n\u03b8n \u2190 argmin \u03b8\u2208\u0398\n1\nn\nn \u2211\ni=1\n\u2207fi(\u03b8i\u22121)\u22a4\u03b8 + 12\u03b7n \u2016\u03b8\u2016 2 2 + \u03c8(\u03b8). (RDA)\nCompared to these approaches, our scheme includes a weighted average of previously seen gradients, and a weighted average of the past iterates. Some links can also be drawn with approaches such as the \u201capproximate follow the leader\u201d algorithm of [10] and other works [12, 14].\nWe now evaluate the performance of our method for \u21131-logistic regression. In summary, the datasets consist of pairs (yi,xi)Ni=1, where the yi\u2019s are in {\u22121,+1}, and the xi\u2019s are in Rp with unit \u21132norm. The function \u03c8 in (4) is the \u21131-norm: \u03c8(\u03b8) , \u03bb\u2016\u03b8\u20161, and \u03bb is a regularization parameter; the functions fi are logistic losses: fi(\u03b8) , log(1 + e\u2212yix \u22a4\ni \u03b8). One part of each dataset is devoted to training, and another part to testing. We used weights of the form wn , \u221a\n(n0 + 1)/(n+ n0), where n0 is automatically adjusted at the beginning of each experiment by performing one pass on 5% of the training data. We implemented SMM in C++ and exploited the sparseness of the datasets, such that each update has a computational complexity of the order O(s), where s is the number of non zeros in \u2207fn(\u03b8n\u22121); such an implementation is non trivial but proved to be very efficient. We consider three datasets described in the table below. rcv1 and webspam are obtained from the 2008 Pascal large-scale learning challenge.3 kdd2010 is available from the LIBSVM website.4\nname Ntr (train) Nte (test) p density (%) size (GB) rcv1 781 265 23 149 47 152 0.161 0.95 webspam 250 000 100 000 16 091 143 0.023 14.95 kdd2010 10 000 000 9 264 097 28 875 157 10\u22124 4.8\nWe compare our implementation with state-of-the-art publicly available solvers: the batch algorithm FISTA of [1] implemented in the C++ SPAMS toolbox and LIBLINEAR v1.93 [7]. LIBLINEAR is based on a working-set algorithm, and, to the best of our knowledge, is one of the most efficient available solver for \u21131-logistic regression with sparse datasets. Because p is large, the incremental majorization-minimization method of [18] could not run for memory reasons. We run every method on 1, 2, 3, 4, 5, 10 and 25 epochs (passes over the training set), for three regularization regimes, respectively yielding a solution with approximately 100, 1 000 and 10 000 non-zero coefficients. We report results for the medium regularization in Figure 1 and provide the rest as supplemental material. FISTA is not represented in this figure since it required more than 25 epochs to achieve reasonable values. Our conclusion is that SMM often provides a reasonable solution after one epoch, and outperforms LIBLINEAR in the low-precision regime. For high-precision regimes, LIBLINEAR should be preferred. Such a conclusion is often obtained when comparing batch and stochastic algorithms [4], but matching the performance of LIBLINEAR is very challenging.\n3http://largescale.ml.tu-berlin.de. 4http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/."}, {"heading": "4.2 Online DC Programming for Non-Convex Sparse Estimation", "text": "We now consider the same experimental setting as in the previous section, but with a non-convex regularizer \u03c8 : \u03b8 7\u2192 \u03bb\u2211pj=1 log(|\u03b8[j]| + \u03b5), where \u03b8[j] is the j-th entry in \u03b8. A classical way for minimizing the regularized empirical cost 1\nN\n\u2211N\ni=1 fi(\u03b8) +\u03c8(\u03b8) is to resort to DC programming. It consists of solving a sequence of reweighted-\u21131 problems [8]. A current estimate \u03b8n\u22121 is updated as a solution of min\u03b8\u2208\u0398 1N \u2211N i=1 fi(\u03b8) + \u03bb \u2211p j=1 \u03b7j |\u03b8[j]|, where \u03b7j , 1/(|\u03b8n\u22121[j]|+ \u03b5).\nIn contrast to this \u201cbatch\u201d methodology, we can use our framework to address the problem online. At iteration n of Algorithm 1, we define the function gn according to Proposition 3.5:\ngn : \u03b8 7\u2192 fn(\u03b8n\u22121) +\u2207fn(\u03b8n\u22121)\u22a4(\u03b8 \u2212 \u03b8n\u22121) + L2 \u2016\u03b8 \u2212 \u03b8n\u22121\u201622 + \u03bb \u2211p j=1 |\u03b8[j]| |\u03b8n\u22121[j]|+\u03b5 ,\nWe compare our online DC programming algorithm against the batch one, and report the results in Figure 2, with \u03b5 set to 0.01. We conclude that the batch reweighted-\u21131 algorithm always converges after 2 or 3 weight updates, but suffers from local minima issues. The stochastic algorithm exhibits a slower convergence, but provides significantly better solutions. Whether or not there are good theoretical reasons for this fact remains to be investigated. Note that it would have been more rigorous to choose a bounded set \u0398, which is required by Proposition 3.5. In practice, it turns not to be necessary for our method to work well; the iterates \u03b8n have indeed remained in a bounded set."}, {"heading": "4.3 Online Structured Sparse Coding", "text": "In this section, we show that we can bring new functionalities to existing matrix factorization techniques [13, 19]. We are given a large collection of signals (xi)Ni=1 in R m, and we want to find a\ndictionary D in Rm\u00d7K that can represent these signals in a sparse way. The quality of D is measured through the loss \u2113(x,D) , min\n\u03b1\u2208RK 1 2\u2016x\u2212D\u03b1\u201622+\u03bb1\u2016\u03b1\u20161+ \u03bb22 \u2016\u03b1\u201622, where the \u21131-norm\ncan be replaced by any convex regularizer, and the squared loss by any convex smooth loss.\nThen, we are interested in minimizing the following expected cost:\nmin D\u2208Rm\u00d7K Ex [\u2113(x,D)] + \u03d5(D),\nwhere \u03d5 is a regularizer for D. In the online learning approach of [19], the only way to regularize D is to use a constraint set, on which we need to be able to project efficiently; this is unfortunately not always possible. In the matrix factorization framework of [13], it is argued that some applications can benefit from a structured penalty \u03d5, but the approach of [13] is not easily amenable to stochastic optimization. Our approach makes it possible by using the proximal gradient surrogate\ngn : D 7\u2192 \u2113(xn,Dn\u22121) + Tr ( \u2207D\u2113(xn,Dn\u22121)\u22a4(D\u2212Dn\u22121) ) + L2 \u2016D\u2212Dn\u22121\u20162F + \u03d5(D). (5) It is indeed possible to show that D 7\u2192 \u2113(xn,D) is differentiable, and its gradient is Lipschitz continuous with a constant L that can be explicitly computed [18, 19].\nWe now design a proof-of-concept experiment. We consider a set of N =400 000 whitened natural image patches xn of size m = 20 \u00d7 20 pixels. We visualize some elements from a dictionary D trained by SPAMS [19] on the left of Figure 3; the dictionary elements are almost sparse, but have some residual noise among the small coefficients. Following [13], we propose to use a regularization function \u03d5 encouraging neighbor pixels to be set to zero together, thus leading to a sparse structured dictionary. We consider the collection G of all groups of variables corresponding to squares of 4 neighbor pixels in {1, . . . ,m}. Then, we define\u03d5(D) , \u03b31 \u2211K j=1 \u2211\ng\u2208G maxk\u2208g |dj [k]|+\u03b32\u2016D\u20162F, where dj is the j-th column of D. The penalty \u03d5 is a structured sparsity-inducing penalty that encourages groups of variables g to be set to zero together [13]. Its proximal operator can be computed efficiently [20], and it is thus easy to use the surrogates (5). We set \u03bb1 =0.15 and \u03bb2 =0.01; after trying a few values for \u03b31 and \u03b32 at a reasonable computational cost, we obtain dictionaries with the desired regularization effect, as shown in Figure 3. Learning one dictionary of size K = 256 took a few minutes when performing one pass on the training data with mini-batches of size 100. This experiment demonstrates that our approach is more flexible and general than [13] and [19]. Note that it is possible to show that when \u03b32 is large enough, the iterates Dn necessarily remain in a bounded set, and thus our convergence analysis presented in Section 3.3 applies to this experiment."}, {"heading": "5 Conclusion", "text": "In this paper, we have introduced a stochastic majorization-minimization algorithm that gracefully scales to millions of training samples. We have shown that it has strong theoretical properties and some practical value in the context of machine learning. We have derived from our framework several new algorithms, which have shown to match or outperform the state of the art for solving large-scale convex problems, and to open up new possibilities for non-convex ones. In the future, we would like to study surrogate functions that can exploit the curvature of the objective function, which we believe is a crucial issue to deal with badly conditioned datasets."}, {"heading": "Acknowledgments", "text": "This work was supported by the Gargantua project (program Mastodons - CNRS)."}, {"heading": "A Mathematical Background and Useful Results", "text": "In this paper, we use subdifferential calculus for convex functions. The definition of subgradients and directional derivatives can be found in classical textbooks, see, e.g., [2], [37]. We denote by \u2202f(\u03b8) the subdifferential of a convex function f at a point \u03b8. Other definitions can be found in the appendix of [18], which uses a similar notation as ours.\nIn this section, we present several classical optimization and probabilistic tools, which we use in our paper. The first lemma is a classical quadratic upper-bound for differentiable functions with a Lipschitz gradient. It can be found for instance in Lemma 1.2.3 of [35], or in the appendix of [18].\nLemma A.1 (Convex Surrogate for Functions with Lipschitz Gradient). Let f : Rp \u2192 R be differentiable and \u2207f be L-Lipschitz continuous. Then, for all \u03b8, \u03b8\u2032 in Rp,\n|f(\u03b8\u2032)\u2212 f(\u03b8)\u2212\u2207f(\u03b8)\u22a4(\u03b8\u2032 \u2212 \u03b8)| \u2264 L 2 \u2016\u03b8 \u2212 \u03b8\u2032\u201622. (6)\nThe next lemma is a simple relation, which will allow us to identify the subdifferential of a convex function with the one of its surrogate at a particular point.\nLemma A.2 (Surrogate Functions and Subdifferential). Assume that f, g : Rp \u2192 R are convex, and that h , g \u2212 f is differentiable at \u03b8 in Rp with \u2207h(\u03b8) = 0. Then, \u2202f(\u03b8) = \u2202g(\u03b8).\nProof. It is easy to show that g and f have the same directional derivatives at \u03b8 since h is differentiable and \u2207h(\u03b8) = 0. This is sufficient to conclude that \u2202g(\u03b8) = \u2202f(\u03b8) by using Proposition 3.1.6 of [2], a simple lemma relating directional derivatives and subgradients.\nThe following lemma is a lower bound for strongly convex functions. It can be found for instance in [36].\nLemma A.3 (Lower Bound for Strongly Convex Functions). Let f : Rp \u2192 R be a \u00b5-strongly convex function. Let z be in \u2202f(\u03ba) for some \u03ba in Rp. Then, the following inequality holds for all \u03b8 in Rp:\nf(\u03b8) \u2265 f(\u03ba) + z\u22a4(\u03b8 \u2212 \u03ba) + \u00b5 2 \u2016\u03b8 \u2212 \u03ba\u201622.\nProof. The function l : \u03b8 7\u2192 f(\u03b8)\u2212 \u00b52 \u2016\u03b8\u2212\u03ba\u201622 is convex by definition of strong convexity, and l\u2212f is differentiable with \u2207(l\u2212 f)(\u03ba) = 0. We apply Lemma A.2, which tells us that z is in \u2202l(\u03ba). This is sufficient to conclude, by noticing that a convex function is always above its tangents.\nThe next lemma is also classical (see the appendix of [18]).\nLemma A.4 (Second-Order Growth Property). Let f : Rp \u2192 R be a \u00b5-strongly convex function and \u0398 \u2286 Rp be a convex set. Let \u03b8\u22c6 be the minimizer of f on \u0398. Then, the following condition holds for all \u03b8 in \u0398:\nf(\u03b8) \u2265 f(\u03b8\u22c6) + \u00b5 2 \u2016\u03b8 \u2212 \u03b8\u22c6\u201622.\nWe now introduce a sequence of probabilistic tools, which we use in our convergence analysis for non-convex functions. The first one is a classical theorem on quasi-martingales, which was used in [3] for proving the convergence of the stochastic gradient descent algorithm.\nTheorem A.1 (Convergence of Quasi-Martingales.). This presentation follows [3] and Proposition 9.5 and Theorem 9.4 of [34]. The original theorem is due to [33]. Let (Fn)n\u22650 be an increasing family of \u03c3-fields. Let (Xn)n\u22650 be a real stochastic process such that every random variable Xn is bounded below by a constant independent of n, and Fn-measurable. Let\n\u03b4n ,\n{\n1 if E[Xn+1 \u2212Xn|Fn] > 0, 0 otherwise.\nIf the series \u2211\u221e\nn=0 E[\u03b4n(Xn+1 \u2212 Xn)] converges, then (Xn)n\u22650 is a quasi-martingale and converges almost surely to an integrable random variable X\u221e. Moreover,\n\u221e \u2211\nn=0\nE [ |E[Xn+1 \u2212Xn|Fn]| ] < \u221e.\nThe next lemma is simple, but useful to prove the convergence of deterministic algorithms."}, {"heading": "Lemma A.5. Deterministic Lemma on Non-negative Converging Series.", "text": "Let (an)n\u22651, (bn)n\u22651 be two non-negative real sequences such that the series \u2211\u221e n=1 an diverges, the series \u2211\u221e\nn=1 anbn converges, and there exists K > 0 such that |bn+1 \u2212 bn| \u2264 Kan. Then, the sequence (bn)n\u22651 converges to 0.\nProof. The proof is inspired by the one of Proposition 1.2.4 of [31]. Since the series \u2211\nn\u22651 an diverges, we necessarily have lim infn\u2192+\u221e bn = 0. Otherwise, it would be easy to contradict the assumption \u2211\nn\u22651 anbn < +\u221e. Let us now proceed by contradiction and assume that lim supn\u2192+\u221e bn = \u03bb > 0. We can then build two sequences of indices (mj)j\u22651 and (nj)j\u22651 such that\n\u2022 mj < nj < mj+1,\n\u2022 \u03bb3 < bk, for mj \u2264 k < nj ,\n\u2022 bk \u2264 \u03bb3 , for nj \u2264 k < mj+1.\nLet \u03b5 = \u03bb 2\n9K and \u0303 be large enough such that\n\u221e \u2211\nn=m\u0303\nanbn < \u03b5.\nThen, we have for all j \u2265 \u0303 and all m with mj \u2264 m \u2264 nj \u2212 1,\n|bnj \u2212 bm| \u2264 nj\u22121 \u2211\nk=m\n|bk+1 \u2212 bk| \u2264 3K\n\u03bb\nnj\u22121 \u2211\nk=m\nak \u03bb 3 \u2264 3K \u03bb\nnj\u22121 \u2211\nk=m\nakbk \u2264 3K\n\u03bb\n+\u221e \u2211\nk=m\nakbk\n\u2264 3K\u03b5 \u03bb \u2264 \u03bb 3 .\nTherefore, by using the triangle inequality,\nbm \u2264 bnj + \u03bb 3 \u2264 2\u03bb 3 .\nand finally, for all m \u2265 \u0303, bm \u2264 2\u03bb\n3 ,\nwhich contradicts lim supn\u2192+\u221e bn = \u03bb > 0. Therefore, bn \u2212\u2192 n\u2192+\u221e 0.\nWe now provide a stochastic version of Lemma A.6."}, {"heading": "Lemma A.6. Stochastic Lemma on Non-negative Converging Series.", "text": "Let (Xn)n\u22651 be a sequence of non-negative measurable random variables on a probability space. Let also an, bn be two non-negative sequences such that \u2211 n\u22651 an = +\u221e and \u2211\nn\u22651 anbn < +\u221e. Assume that there exists a constant C such that for all n \u2265 1, E[Xn] \u2264 bn and |Xn+1\u2212Xn| \u2264 Can almost surely. Then Xn almost surely converges to zero.\nProof. The following series is convergent\nE\n\n\n\u2211 n\u22651 anXn\n\n = \u2211\nn\u22651 E [anXn] \u2264\n\u2211 n\u22651 anbn < +\u221e,\nwhere we use the fact that the random variables are non-negative to interchange the sum and the expectation. We thus have that \u2211\nn\u22651 anXn converges with probability one. Then, let us call a\u2032n = an and b \u2032 n = Xn; the conditions of Lemma A.5 are satisfied for a \u2032 n and b \u2032 n with probability one, and Xn almost surely converges to zero."}, {"heading": "B Auxiliary Lemmas", "text": "In this section, we present auxiliary lemmas for our convex and non-convex analyses. We start by presenting a lemma which is useful for both of them, and which is in fact a core component for all results presented in [18]. The proof of this lemma is simple and available in [18].\nLemma B.1 (Basic Properties of First-Order Surrogate Functions). Let g be in SL,\u03c1(f, \u03ba) for some \u03ba in \u0398. Define the approximation error function h , g\u2212f and let \u03b8\u2032 be the minimizer of g over \u0398. Then, for all \u03b8 in \u0398,\n\u2022 \u2207h(\u03ba) = 0;\n\u2022 |h(\u03b8)| \u2264 L2 \u2016\u03b8 \u2212 \u03ba\u201622;\n\u2022 f(\u03b8\u2032) \u2264 g(\u03b8\u2032) \u2264 f(\u03b8) + L2 \u2016\u03b8 \u2212 \u03ba\u201622 \u2212 \u03c1 2\u2016\u03b8 \u2212 \u03b8\u2032\u201622."}, {"heading": "B.1 Convex Analysis", "text": "We introduce, for all n \u2265 0, the quantity \u03ben , 12E[\u2016\u03b8\u22c6 \u2212 \u03b8n\u201622], where \u03b8\u22c6 is a minimizer of f on \u0398. Our analysis also involves several quantities that are defined recursively for all n \u2265 1:\n\n    \n    \nAn , (1\u2212 wn)An\u22121 + wn\u03ben\u22121 Bn , (1\u2212 wn)Bn\u22121 + wnE[f(\u03b8n\u22121)] Cn , (1\u2212 wn)Cn\u22121 + (Rwn) 2\n2\u03c1\ng\u0304n , (1\u2212 wn)g\u0304n\u22121 + wngn f\u0304n , (1\u2212 wn)f\u0304n\u22121 + wnfn\n, (7)\nwhere A0 , 1L(\u03c1\u03be0 \u2212 f\u22c6), B0 , 0, C0 , 0, g\u03040 = f\u03040 , \u03b8 7\u2192 \u03c1 2\u2016\u03b8\u2212 \u03b80\u201622. Note that g\u03040 is \u03c1-strongly convex, and is minimized by \u03b80. The choice for A0, B0, C0 is driven by technical reasons, which appear in the proof of Lemma B.4, a stochastic version of Lemma B.1.\nNote that we also assume here that all the expectations above are well defined and finite-valued. In other words, we do not deal with measurability or integrability issues for simplicity, as often done in the literature [22].\nLemma B.2 (Auxiliary Lemma for Convex Analysis). When the functions fn are convex, and the surrogates gn are in SL,\u03c1(fn, \u03b8n\u22121), we have under assumption (A) that for all n \u2265 1,\ng\u0304n(\u03b8n\u22121) \u2264 g\u0304n(\u03b8n) + (Rwn)\n2\n2\u03c1 .\nProof. First, we remark that the subdifferentials of gn and fn at \u03b8n\u22121 coincide by applying Lemma A.2. Then, we choose zn in \u2202gn(\u03b8n\u22121) = \u2202fn(\u03b8n\u22121), which is bounded by R accord-\ning to assumption (A), and we have\ng\u0304n(\u03b8n) = (1\u2212 wn)g\u0304n\u22121(\u03b8n) + wngn(\u03b8n)\n\u2265 (1\u2212wn) ( g\u0304n\u22121(\u03b8n\u22121)+ \u03c1\n2 \u2016\u03b8n\u2212\u03b8n\u22121\u201622\n)\n+wn\n(\ngn(\u03b8n\u22121)+z \u22a4 n (\u03b8n\u2212\u03b8n\u22121)+\n\u03c1 2 \u2016\u03b8n\u2212\u03b8n\u22121\u201622 )\n= g\u0304n(\u03b8n\u22121) + wnz \u22a4 n (\u03b8n \u2212 \u03b8n\u22121) +\n\u03c1 2 \u2016\u03b8n \u2212 \u03b8n\u22121\u201622\n\u2265 g\u0304n(\u03b8n\u22121)\u2212Rwn\u2016\u03b8n \u2212 \u03b8n\u22121\u20162 + \u03c1\n2 \u2016\u03b8n \u2212 \u03b8n\u22121\u201622\n\u2265 g\u0304n(\u03b8n\u22121)\u2212 (Rwn)\n2\n2\u03c1 .\nThe first inequality uses Lemma A.4 and Lemma A.3 since gn is \u03c1-strongly convex by definition (and by induction g\u0304n is \u03c1-strongly convex as well); the second inequality uses Cauchy-Schwarz\u2019s inequality and the fact that the subgradients of the functions fn are bounded by R.\nLemma B.3 (Another Auxiliary Lemma for Convex Analysis). When the functions fn are convex, and the surrogates gn are in SL,\u03c1(fn, \u03b8n\u22121), we have under assumption (A) that for all n \u2265 0,\nBn \u2264 E[g\u0304n(\u03b8n)] + Cn, (8)\nProof. We proceed by induction, and start by showing that Eq. (8) is true for n = 0.\nB0 = 0 = E[g\u03040(\u03b80)] = E[g\u03040(\u03b80)] + C0.\nLet us now assume that it is true for n\u2212 1, and show that it is true for n. Bn = (1\u2212 wn)Bn\u22121 + wnE[f(\u03b8n\u22121)]\n\u2264 (1\u2212 wn)(E[g\u0304n\u22121(\u03b8n\u22121)] + Cn\u22121) + wnE[f(\u03b8n\u22121)] = (1\u2212 wn)(E[g\u0304n\u22121(\u03b8n\u22121)] + Cn\u22121) + wnE[fn(\u03b8n\u22121)] = (1\u2212 wn)(E[g\u0304n\u22121(\u03b8n\u22121)] + Cn\u22121) + wnE[gn(\u03b8n\u22121)] = E[g\u0304n(\u03b8n\u22121)] + (1\u2212 wn)Cn\u22121\n\u2264 E[g\u0304n(\u03b8n)] + (Rwn)\n2\n2\u03c1 + (1\u2212 wn)Cn\u22121\n= E[g\u0304n(\u03b8n)] + Cn.\nThe first inequality uses the induction hypothesis; the last inequality uses Lemma B.2 and the definition of Cn. We also used the fact that E[fn(\u03b8n\u22121)] = E[E[fn(\u03b8n\u22121)]|Fn\u22121]] = E[E[f(\u03b8n\u22121)]|Fn\u22121]] = E[f(\u03b8n\u22121)], where Fn\u22121 corresponds to the filtration induced by the past information before time n, such that \u03b8n\u22121 is deterministic given Fn\u22121.\nThe next lemma is important; it is the stochastic version of Lemma B.1 for first-order surrogates.\nLemma B.4 (Basic Properties of Stochastic First-Order Surrogates). When the functions fn are convex and the functions gn are in SL,\u03c1(fn, \u03b8n\u22121), we have under assumption (A) that for all n \u2265 0,\nBn \u2264 f\u22c6 + LAn \u2212 \u03c1\u03ben + Cn,\nProof. According to Lemma B.3, it is sufficient to show that E[g\u0304n(\u03b8n)] \u2264 f\u22c6 + LAn \u2212 \u03c1\u03ben for all n \u2265 0. Since g\u0304n is \u03c1-strongly convex and \u03b8n is the minimizer of g\u0304n over \u0398, we have E[g\u0304n(\u03b8n)] \u2264 E[g\u0304n(\u03b8\n\u22c6)] \u2212 \u03c1\u03ben, by using Lemma A.4. Thus, it is in fact sufficient to show that E[g\u0304n(\u03b8\u22c6)] \u2264 f\u22c6 + LAn. For n = 0, this inequality holds since E[g\u03040(\u03b8\u22c6)] = \u03c1\u03be0 = f\u22c6 + LA0. We can then proceed again by induction: assume that E[g\u0304n\u22121(\u03b8\u22c6)] \u2264 f\u22c6 + LAn\u22121. Then,\nE[g\u0304n(\u03b8 \u22c6)] = (1\u2212 wn)E[g\u0304n\u22121(\u03b8\u22c6)] + wnE[gn(\u03b8\u22c6)]\n\u2264 (1\u2212 wn)(f\u22c6 + LAn\u22121) + wn(E[fn(\u03b8\u22c6)] + L\u03ben\u22121) = (1\u2212 wn)(f\u22c6 + LAn\u22121) + wn(f\u22c6 + L\u03ben\u22121) = f\u22c6 + LAn,\nwhere we have used Lemma B.1 to upper-bound the difference E[gn(\u03b8\u22c6)]\u2212E[fn(\u03b8\u22c6)] by \u03ben\u22121.\nFor strongly-convex functions, we also have the following simple but useful relation between An and Bn.\nLemma B.5 (Relation between An and Bn). Under assumption (B), if w1 = 1, we have for all n \u2265 1,\nf\u22c6 + \u00b5An \u2264 Bn.\nProof. This relation is true for n = 1 since we have f\u22c6 + \u00b5A1 = f\u22c6 + \u00b5\u03be0 \u2264 f(\u03b80) = B1 by applying Lemma A.4, since f is \u00b5-strongly convex according to assumption (B). The rest follows by induction."}, {"heading": "B.2 Non-convex Analysis", "text": "When the functions fn are not convex, the convergence analysis becomes more involved. One key tool we use is a uniform convergence result when the function class {x 7\u2192 \u2113(\u03b8,x) : \u03b8 \u2208 \u0398} is \u201csimple enough\u201d in terms of entropy. Under the assumptions made in our paper, it is indeed possible to use some results from empirical processes [27], which provides us the following lemma.\nLemma B.6 (Uniform Convergence). Under assumptions (A), (C), and (D), we have the following uniform law of large numbers:\nE\n[\nsup \u03b8\u2208\u0398\n\u2223 \u2223 \u2223 \u2223 \u2223 1 n n \u2211\ni=1\nfi(\u03b8)\u2212 f(\u03b8) \u2223 \u2223 \u2223 \u2223\n\u2223\n]\n\u2264 C\u221a n , (9)\nwhere C is a constant, and sup\u03b8\u2208\u0398 \u2223 \u2223 1 n \u2211n i=1 fi(\u03b8) \u2212 f(\u03b8) \u2223 \u2223 converges almost surely to zero.\nProof. We simply refer to Lemma 19.36 and Example 19.7 of [27], where assumptions (C) and (D) ensure uniform boundness and squared integrability conditions. Note that we assume that the quantities sup\u03b8\u2208\u0398 \u2223 \u2223 1 n \u2211n i=1 fi(\u03b8) \u2212 f(\u03b8) \u2223\n\u2223 are measurable. This assumption does not incur a loss of generality, since measurability issues for empirical processes can be dealt with rigorously [27].\nThe next lemma shows that uniform convergence applies to the weighted empirical risk f\u0304n, defined in Eq. (7), but with a different rate.\nLemma B.7 (Uniform Convergence for f\u0304n). Under assumptions (A), (C), (D), and (E), we have for all n \u2265 1,\nE\n[\nsup \u03b8\u2208\u0398\n\u2223 \u2223f\u0304n(\u03b8) \u2212 f(\u03b8) \u2223 \u2223\n] \u2264 Cwn \u221a n,\nwhere C is the same as in Lemma B.6, and sup\u03b8\u2208\u0398 \u2223 \u2223f\u0304n(\u03b8)\u2212 f(\u03b8) \u2223 \u2223 converges almost surely to zero.\nProof. We prove the two parts of the lemma separately. As in Lemma B.6, we assume all the quantities of interest to be measurable."}, {"heading": "First part of the lemma:", "text": "Let us fix n > 0. It is easy to show that f\u0304n can be written as f\u0304n = \u2211n i=1 w i nfi for some non-negative weights win with w n n = wn. Let us also define the empirical cost Fi , 1 n\u2212i+1 \u2211n j=i fj . According to (9), we have E [sup\u03b8\u2208\u0398 |Fi(\u03b8) \u2212 f(\u03b8)|] \u2264 C\u221an\u2212i+1 . We now remark that\nf\u0304n \u2212 f = n \u2211\ni=1\n(win \u2212 wi\u22121n )(n\u2212 i+ 1)(Fi \u2212 f),\nwhere we have defined w0n , 0. This relation can be proved by simple calculation. We obtain the first part by using the triangle inequality, and the fact that win \u2265 wi\u22121n for all i:\nE\n[\nsup \u03b8\u2208\u0398\n|f\u0304n(\u03b8)\u2212 f(\u03b8)| ] \u2264 E [ n \u2211\ni=1\n(win \u2212 wi\u22121n )(n\u2212 i+ 1) sup \u03b8\u2208\u0398\n|Fi(\u03b8)\u2212 f(\u03b8)| ]\n=\nn \u2211\ni=1\n(win \u2212 wi\u22121n )(n\u2212 i+ 1)E [\nsup \u03b8\u2208\u0398\n|Fi(\u03b8) \u2212 f(\u03b8)| ]\n\u2264 n \u2211\ni=1\n(win \u2212 wi\u22121n )C \u221a n\u2212 i+ 1\n\u2264 \u221a nC n \u2211\ni=1\n(win \u2212 wi\u22121n )\n= C \u221a nwn.\nThis is unfortunately not sufficient to show that E [ sup\u03b8\u2208\u0398 |f\u0304n(\u03b8) \u2212 f(\u03b8)| ]\nconverges to zero almost surely. We will show this fact by using Lemma A.6."}, {"heading": "Second part of the lemma:", "text": "We call Xn = sup\u03b8\u2208\u0398 |f\u0304n(\u03b8)\u2212 f(\u03b8)|. We have\nXn \u2212Xn\u22121 = sup \u03b8\u2208\u0398 |(1\u2212 wn)(f\u0304n\u22121(\u03b8) \u2212 f(\u03b8)) + wn(fn(\u03b8)\u2212 f(\u03b8))| \u2212Xn\u22121\n\u2264 sup \u03b8\u2208\u0398 wn|fn(\u03b8) \u2212 f(\u03b8)| \u2212 wnXn\u22121 \u2264 2Mwn\nLet us denote by \u03b8\u22c6n a point in \u0398 such that Xn = |f\u0304n(\u03b8\u22c6n)\u2212 f(\u03b8\u22c6n)|. We also have\nXn \u2212Xn\u22121 = sup \u03b8\u2208\u0398 |(1\u2212 wn)(f\u0304n\u22121(\u03b8) \u2212 f(\u03b8)) + wn(fn(\u03b8)\u2212 f(\u03b8))| \u2212Xn\u22121\n\u2265 (1\u2212 wn)Xn\u22121 + wn(fn(\u03b8\u22c6n\u22121)\u2212 f(\u03b8\u22c6n\u22121))\u2212Xn\u22121 \u2265 \u2212wnXn\u22121 + wn(fn(\u03b8\u22c6n\u22121)\u2212 f(\u03b8\u22c6n\u22121)) \u2265 \u2212wn4M,\nwhere we use again the fact that all functions fn, f\u0304n and f are bounded by M . Thus, we have shown that |Xn \u2212 Xn\u22121| \u2264 4Mwn. Call an = wn and bn = wn \u221a n, then the conditions of Lemma A.6 are satisfied, and Xn converges almost surely to zero.\nFinally, the next lemma illustrates why the strong convexity of the surrogates is important.\nLemma B.8 (Stability of the Estimates). When gn is in SL,\u03c1(f, \u03b8n\u22121),\n\u2016\u03b8n \u2212 \u03b8n\u22121\u20162 \u2264 2Rwn\n\u03c1 .\nProof. Because the surrogates gn are \u03c1-strongly convex, we have from Lemma A.4\n\u03c1 2 \u2016\u03b8n \u2212 \u03b8n\u22121\u201622 \u2264 g\u0304n(\u03b8n\u22121)\u2212 g\u0304n(\u03b8n)\n= wn (gn(\u03b8n\u22121)\u2212 gn(\u03b8n)) + (1 \u2212 wn) (g\u0304n\u22121(\u03b8n\u22121)\u2212 g\u0304n\u22121(\u03b8n)) \u2264 wn (gn(\u03b8n\u22121)\u2212 gn(\u03b8n)) \u2264 wn (fn(\u03b8n\u22121)\u2212 fn(\u03b8n)) \u2264 Rwn\u2016\u03b8n \u2212 \u03b8n\u22121\u20162.\nThe second inequality comes from the fact that \u03b8n\u22121 is a minimizer of g\u0304n\u22121; the third inequality is because gn(\u03b8n\u22121) = fn(\u03b8n\u22121) and gn \u2265 fn. This is sufficient to conclude."}, {"heading": "C Proofs of the Main Lemmas and Propositions", "text": ""}, {"heading": "C.1 Proof of Proposition 3.1", "text": "Proof. According to Lemma B.4, we have for all n \u2265 1,\nwnBn\u22121 \u2264 wnf\u22c6 + LwnAn\u22121 \u2212 Lwn\u03ben\u22121 + wnCn\u22121.\nBy using the relations (7), this is equivalent to\nBn\u22121 \u2212 Bn + wnE[f(\u03b8n\u22121)] \u2264 wnf\u22c6 + L(An\u22121 \u2212An) + Cn\u22121 \u2212 Cn + (Rwn)\n2\n2L .\nBy summing these inequalities between 1 and n, we obtain\nB0 \u2212Bn + n \u2211\nk=1\nwkE[f(\u03b8k\u22121)] \u2264 ( n \u2211\nk=1\nwk\n)\nf\u22c6 + LA0 \u2212 LAn \u2212 Cn + n \u2211\nk=1\n(Rwk) 2\n2L .\nNote that we also have\nBn \u2264 f\u22c6 + LAn + Cn = LAn + Cn +B0 \u2212 LA0 + L\u03be0.\nTherefore, by combining the two previous inequalities,\nn \u2211\nk=1\nwkE[f(\u03b8k\u22121)] \u2264 ( n \u2211\nk=1\nwk\n)\nf\u22c6 + L\u03be0 +\nn \u2211\nk=1\n(Rwk) 2\n2L ,\nand by using Jensen\u2019s inequality,\nE[f(\u03b8\u0304n\u22121)\u2212 f\u22c6] \u2264 L\u03be0 +\nR2 2L \u2211n k=1 w 2 k\n\u2211n k=1 wk\n."}, {"heading": "C.2 Proof of Corollary 3.1", "text": ""}, {"heading": "Proof.", "text": "We choose weights of the form wn ,\n\u03b3\u221a n . Then, we have\nn \u2211\nk=1\nw2k \u2264 \u03b32(1 + logn),\nby using the fact that \u2211n k=1 1 k \u2264 1 + log(n). We also have for n \u2265 2,\nn \u2211\nk=1\nwk \u2265 2\u03b3( \u221a n+ 1\u2212 1) \u2265 \u03b3 \u221a n,\nwhere we use the fact that \u2211n k=1 1\u221a k \u2265 2(\n\u221a n+ 1\u2212 1), and the fact that 2( \u221a n+ 1\u2212 1) \u2265 \u221an for\nall n \u2265 2. Plugging this inequalities into (3) yields the desired result."}, {"heading": "C.3 Proof of Proposition 3.2", "text": "Proof. We proceed in several steps, proving the convergence rates of several quantities of interest.\nConvergence rate of Cn: Let us show by induction that we have Cn \u2264 R 2 \u03c1 wn for all n \u2265 1. This is obviously true for n = 1\nby definitions of w1 = 1 and C1 = R 2 2\u03c1 . Let us now assume that it is true for n\u2212 1. We have\nCn = (1\u2212 wn)Cn\u22121 + R2\n2\u03c1 w2n\n\u2264 R 2\n\u03c1 wn\n(\n(1 \u2212 wn) wn\u22121 wn + wn 2\n)\n\u2264 R 2\n\u03c1 wn\n(\n\u03b2(n\u2212 1) \u03b2n+ 1 \u03b2n+ 1 \u03b2(n\u2212 1) + 1 + 1 \u03b2n+ 1\n)\n\u2264 R 2\n\u03c1 wn\n(\n\u03b2(n\u2212 1) \u03b2(n\u2212 1) + 1 +\n1\n\u03b2(n\u2212 1) + 1\n)\n= R2\n\u03c1 wn.\n(10)\nWe conclude by induction that this is true for all n \u2265 1. Convergence rate of An: From Lemma B.5 and B.4, we have for all n \u2265 2,\n\u00b5An\u22121 \u2264 LAn\u22121 \u2212 \u03c1\u03ben\u22121 + Cn\u22121.\nMultiplying this inequality by wn,\n2\u00b5wnAn\u22121 \u2264 \u03c1wn(An\u22121 \u2212 \u03ben\u22121) + wnCn\u22121,\nwhere the factor 2 comes from the fact that \u03c1 = L+ \u00b5. By using the definition of An in Eq. (7), we obtain the relation\nAn \u2264 ( 1\u2212 2\u00b5wn \u03c1 ) An\u22121 + wn \u03c1 Cn\u22121.\nLet us now show by induction that we have, for all n \u2265 1, the convergence rate An \u2264 \u03b4wn, where \u03b4 , max ( R2\n\u03c1\u00b5 , \u03be0\n)\n. For n = 1, we have that and w1 = 1, and thus A1 = \u03be0 \u2264 \u03b4. Assume now that we have An\u22121 \u2264 \u03b4wn\u22121 for some n \u2265 1. Then, by using the convergence rate (10) and the induction hypothesis,\nAn \u2264 \u03b4wn (( 1\u2212 2\u00b5wn \u03c1 ) wn\u22121 wn + R2wn\u22121 \u03c12\u03b4 )\n\u2264 \u03b4wn (( 1\u2212 2\u00b5wn \u03c1 ) wn\u22121 wn + \u00b5 wn\u22121 \u03c1 )\n\u2264 \u03b4wn ( \u03b2n+ 1\u2212 2\u00b5(1+\u03b2) \u03c1\n\u03b2n+ 1\n\u03b2n+ 1 \u03b2(n\u2212 1) + 1 + \u00b5(1+\u03b2) \u03c1 \u03b2(n\u2212 1) + 1\n)\n= \u03b4wn\n(\n\u03b2n+ 1\u2212 \u00b5(1+\u03b2) \u03c1\n\u03b2(n\u2212 1) + 1\n)\n\u2264 \u03b4wn.\nThe last inequality uses the fact that \u00b5(1+\u03b2) \u03c1 \u2265 \u03b2 because \u03b2 \u2264 \u00b5 L\n. we conclude by induction that"}, {"heading": "An \u2264 \u03b4wn for all n \u2265 1.", "text": ""}, {"heading": "Convergence rate of E[f(\u03b8\u0302n)\u2212 f\u22c6] + \u03c1\u03ben:", "text": "We use again Lemma B.4:\nBn \u2212 f\u22c6 + \u03c1\u03ben \u2264 LAn + Cn,\nand we consider two possible cases\n\u2022 If R2 \u03c1\u00b5 \u2265 \u03be0, then\nBn \u2212 f\u22c6 + \u03c1\u03ben \u2264 R2\n\u03c1\n(\n1 + L\n\u00b5\n)\nwn\n= R2\n\u00b5 wn\n\u2264 2R 2\n\u00b5(\u03b2n+ 1) ,\nwhere we simply use the convergence rates of An and Cn computed before.\n\u2022 If instead R2 \u03c1\u00b5 < \u03be0, then\nBn \u2212 f\u22c6 + \u03c1\u03ben \u2264 ( R2\n\u03c1 + L\u03be0\n)\nwn\n\u2264 \u03c1\u03be0wn\n\u2264 2\u03c1\u03be0 \u03b2n+ 1 .\nIt is then easy to prove that E[f(\u03b8\u0302n)\u2212f\u22c6] \u2264 Bn by using Jensen\u2019s inequality, which allows us to conclude."}, {"heading": "C.4 Proof of Proposition 3.3", "text": "Proof. We generalize the proof of convergence for online matrix factorization of [19]. The proof exploits Theorem A.1 about the convergence of quasi-martingales [33], similarly as [3] for proving the convergence of the stochastic gradient descent algorithm for non-convex functions."}, {"heading": "Almost sure convergence of (g\u0304n(\u03b8n))n\u22651:", "text": "The first step consists of applying a convergence theorem for the sequence (g\u0304n(\u03b8n))n\u22651 by bounding its positive expected variations. Define Yn , g\u0304n(\u03b8n). For n \u2265 2, we have\nYn\u2212Yn\u22121 = g\u0304n(\u03b8n)\u2212g\u0304n(\u03b8n\u22121)+g\u0304n(\u03b8n\u22121)\u2212g\u0304n\u22121(\u03b8n\u22121) = (g\u0304n(\u03b8n)\u2212g\u0304n(\u03b8n\u22121))+wn(gn(\u03b8n\u22121)\u2212g\u0304n\u22121(\u03b8n\u22121)) = (g\u0304n(\u03b8n)\u2212g\u0304n(\u03b8n\u22121))+wn(f\u0304n\u22121(\u03b8n\u22121)\u2212g\u0304n\u22121(\u03b8n\u22121))+wn(gn(\u03b8n\u22121)\u2212f\u0304n\u22121(\u03b8n\u22121)) = (g\u0304n(\u03b8n)\u2212g\u0304n(\u03b8n\u22121))+wn(f\u0304n\u22121(\u03b8n\u22121)\u2212g\u0304n\u22121(\u03b8n\u22121))+wn(fn(\u03b8n\u22121)\u2212f\u0304n\u22121(\u03b8n\u22121)) \u2264 wn(fn(\u03b8n\u22121)\u2212f\u0304n\u22121(\u03b8n\u22121)).\n(11)\nThe final inequality comes from the inequality g\u0304n \u2265 f\u0304n, which is easy to show by induction starting from n = 1 since w1 = 1. It follows,\nE[g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121)|Fn\u22121] \u2264 wnE[fn(\u03b8n\u22121)\u2212 f\u0304n\u22121(\u03b8n\u22121)|Fn\u22121] = wn(f(\u03b8n\u22121)\u2212 f\u0304n\u22121(\u03b8n\u22121)) \u2264 wn sup\n\u03b8\u2208\u0398 |f(\u03b8)\u2212 f\u0304n\u22121(\u03b8)|,\nwhere Fn\u22121 is the filtration representing the past information before time n. Call now\n\u03b4n ,\n{\n1 if E[g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121)|Fn\u22121] > 0 0 otherwise.\nThen, the series below with non-negative summands converges: \u221e \u2211\nn=1\nE[\u03b4n(g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121))] = \u221e \u2211\nn=1\nE[\u03b4nE[(g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121))|Fn\u22121]]\n\u2264 \u221e \u2211\nn=1\nE\n[\nwn sup \u03b8\u2208\u0398\n|f(\u03b8)\u2212 f\u0304n\u22121(\u03b8)| ]\n\u2264 \u221e \u2211\nn=1\nCw2n \u221a n < +\u221e,\nThe second inequality comes from Lemma B.7. Since in addition g\u0304n is bounded below by some constant independent of n, we can apply Theorem A.1. This theorem tells us that (g\u0304n(\u03b8n))n\u22651 converges almost surely to an integrable random variable g\u22c6 and that\n\u2211\u221e n=1 E[|E[g\u0304n(\u03b8n) \u2212\ng\u0304n\u22121(\u03b8n\u22121)|Fn\u22121]|] converges almost surely."}, {"heading": "Almost sure convergence of (f\u0304n(\u03b8n))n\u22651:", "text": "We will show by using Lemma A.5 that the non-positive term f\u0304n(\u03b8n) \u2212 g\u0304n(\u03b8n) almost surely converges to zero, and thus (f\u0304n(\u03b8n))n\u22651 is also converging almost surely to g\u22c6.\nWe observe that \u221e \u2211\nn=1\nE[|E[g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121)|Fn\u22121]|] = E [ \u221e \u2211\nn=1\n|E[g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121)|Fn\u22121]| ] < +\u221e.\nThus, the series \u2211\u221e\nn=1 |E[g\u0304n(\u03b8n) \u2212 g\u0304n\u22121(\u03b8n\u22121)|Fn\u22121]| is absolutely convergent with probability one, and the series\n\u2211\u221e n=1 E[g\u0304n(\u03b8n)\u2212 g\u0304n\u22121(\u03b8n\u22121)|Fn\u22121] is also almost surely convergent.\nWe also remark that, using Lemma B.7,\nE\n[ +\u221e \u2211\nn=1\nwn|f(\u03b8n\u22121)\u2212 f\u0304n\u22121(\u03b8n\u22121)| ] \u2264 C +\u221e \u2211\nn=1\nw2n \u221a n < +\u221e,\nand thus wn(f(\u03b8n\u22121)\u2212 f\u0304n\u22121(\u03b8n\u22121)) is the summand of an absolutely convergent series with probability one.\nTaking the expectation of Eq. (11) conditioned on Fn\u22121, it remains that the non-positive term wn(f\u0304n\u22121(\u03b8n\u22121)\u2212 g\u0304n\u22121(\u03b8n\u22121)) is also necessarily the summand of an almost surely convergent series, since all other terms in the equation are summands of almost surely converging sums. This is not sufficient to immediately conclude that f\u0304n(\u03b8n)\u2212g\u0304n(\u03b8n) converges to zero almost surely, and thus we will use Lemma A.5. We have that\n\u2211+\u221e n=1 wn diverges, that \u2211+\u221e n=1 wn(g\u0304n\u22121(\u03b8n\u22121)\u2212 f\u0304n\u22121(\u03b8n\u22121))\nconverges almost surely. Define Xn , (g\u0304n\u22121(\u03b8n\u22121) \u2212 f\u0304n\u22121(\u03b8n\u22121)). By definition of the surrogate functions, the differences hn , gn \u2212 fn are differentiable and their gradients are L-Lipschitz continuous. Since in addition \u0398 is compact and \u2207hn(\u03b8n\u22121) = 0, \u2207hn is bounded by some constant R\u2032 independent of n, and the function hn is R\u2032-Lipschitz. This is therefore also the case for h\u0304n = g\u0304n \u2212 f\u0304n.\n|Xn+1 \u2212Xn| = |h\u0304n(\u03b8n)\u2212 h\u0304n\u22121(\u03b8n\u22121)| \u2264 |h\u0304n(\u03b8n)\u2212 h\u0304n(\u03b8n\u22121)|+ |h\u0304n(\u03b8n\u22121)\u2212 h\u0304n\u22121(\u03b8n\u22121)| \u2264 R\u2032\u2016\u03b8n \u2212 \u03b8n\u22121\u20162 + |h\u0304n(\u03b8n\u22121)\u2212 h\u0304n\u22121(\u03b8n\u22121)|\n\u2264 2RR \u2032\n\u03c1 wn + wn|hn(\u03b8n\u22121)\u2212 h\u0304n\u22121(\u03b8n\u22121)|\n= 2RR\u2032\n\u03c1 wn + wn|h\u0304n\u22121(\u03b8n\u22121)|\n\u2264 O(wn). The second inequality uses the fact that h\u0304n is R\u2032-Lipschitz; The second inequality uses Lemma B.8; the last equality uses the fact that the functions hn are also bounded by some constant independent of n (using the fact that \u2207hn is uniformly bounded). We can now apply Lemma A.5, and Xn converges to zero with probability one. Thus, (f\u0304n(\u03b8n))n\u22651 converges almost surely to g\u22c6."}, {"heading": "Almost sure convergence of (f(\u03b8n))n\u22651:", "text": "Since (f\u0304n(\u03b8n))n\u22651 converges almost surely, we simply use Lemma A.6, which tells us that f\u0304n converges uniformly to f . Then, (f(\u03b8n))n\u22651 converges almost surely to g\u22c6."}, {"heading": "Asymptotic Stationary Point Condition:", "text": "Let us call h\u0304n , g\u0304n \u2212 f\u0304n, which can be shown to be differentiable with a L-Lipschitz gradient by definition of the surrogate gn. For all \u03b8 in \u0398,\n\u2207f\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n) = \u2207g\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n)\u2212\u2207h\u0304n(\u03b8n)\u22a4(\u03b8 \u2212 \u03b8n). Since \u03b8n is the minimizer of g\u0304n, we have \u2207g\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n) \u2265 0. Since h\u0304n is differentiable and its gradient is L-Lipschitz continuous, we can apply Lemma A.1 to \u03b8 = \u03b8n and \u03b8\u2032 = \u03b8n \u2212 1L\u2207h\u0304n(\u03b8n), which gives h\u0304n(\u03b8\u2032) \u2264 h\u0304n(\u03b8n)\u2212 12L\u2016\u2207h\u0304n(\u03b8n)\u201622. Since we have shown that h\u0304n(\u03b8n) = g\u0304n(\u03b8n)\u2212 f\u0304(\u03b8n) converges to zero and h\u0304n(\u03b8\u2032) \u2265 0, we have that \u2016\u2207h\u0304n(\u03b8n)\u20162 converges to zero. Thus,\ninf \u03b8\u2208\u0398 \u2207f\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n) \u2016\u03b8 \u2212 \u03b8n\u20162 \u2265 \u2212\u2016\u2207h\u0304n(\u03b8n)\u20162 \u2212\u2192 n\u2192+\u221e 0 a.s."}, {"heading": "C.5 Proof of Proposition 3.4", "text": "Proof. Since \u0398 is compact according to assumption (C), the sequence (\u03b8n)n\u22651 admits limit points. Let us consider a converging subsequence (nk)k\u22651 to a limit point \u03b8\u221e in \u0398. In this converging subsequence, we can also find a subsequence (nk\u2032 )k\u2032\u22651 such that \u03bank\u2032 converges to a point \u03ba\u221e in K (which is compact). For the sake of simplicity, and without loss of generality, we remove the indices k and k\u2032 from the notation and assume that \u03b8n converges to \u03b8\u221e, while \u03ban converges to \u03ba\u221e. It is then easy to see that the functions g\u0304n converge uniformly to g\u0304\u221e , g\u03ba\u221e , given the assumptions made in the proposition.\nDefining h\u0304\u221e , g\u0304\u221e \u2212 f , we have for all \u03b8 in \u0398: \u2207f(\u03b8\u221e, \u03b8 \u2212 \u03b8\u221e) = \u2207g\u0304\u221e(\u03b8\u221e, \u03b8 \u2212 \u03b8\u221e)\u2212\u2207h\u0304\u221e(\u03b8\u221e, \u03b8 \u2212 \u03b8\u221e).\nTo prove the proposition, we will first show that \u2207g\u0304\u221e(\u03b8\u221e, \u03b8\u2212\u03b8\u221e) \u2265 0 and then that \u2207h\u0304\u221e(\u03b8\u221e, \u03b8\u2212 \u03b8\u221e) = 0.\nProof of \u2207g\u0304\u221e(\u03b8\u221e, \u03b8 \u2212 \u03b8\u221e) \u2265 0: It is sufficient to show that \u03b8\u221e is a minimizer of g\u0304\u221e. This is straightforward, by taking the limit when n goes to infinity of g\u0304n(\u03b8) \u2265 g\u0304n(\u03b8n), where we use the uniform convergence of g\u0304n.\nProof of \u2207h\u0304\u221e(\u03b8\u221e, \u03b8 \u2212 \u03b8\u221e) = 0: Since both f\u0304n and g\u0304n converges uniformly (according to Lemma B.7 for f\u0304n), we have that h\u0304n converges uniformly to h\u0304\u221e. Since h\u0304n is differentiable with a L-Lipschitz gradient, we have for all vector z in Rp, h\u0304n(\u03b8n + z) = h\u0304n(\u03b8n) +\u2207h\u0304n(\u03b8n)\u22a4z+O(\u2016z\u201622), where the constant in O is independent of n. By taking the limit when n goes to infinity, it remains\nh\u0304\u221e(\u03b8\u221e + z) = h\u0304\u221e(\u03b8\u221e) +O(\u2016z\u201622), since we have shown in the proof of Proposition 3.3 that \u2016\u2207h\u0304n(\u03b8n)\u20162 converges to zero. Since h\u0304\u221e admits a first order extension around \u03b8\u221e it is differentiable at this point and furthermore, \u2207h\u0304\u221e(\u03b8\u221e) = 0. This is sufficient to conclude."}, {"heading": "C.6 Proof of Proposition 3.5", "text": "Proof. First we notice that\n\u2022 gn \u2265 fn;\n\u2022 gn(\u03b8n\u22121) = fn(\u03b8n\u22121);\n\u2022 gn is \u03c11-strongly convex since \u03b8 7\u2192 gk,n(\u03b3k(\u03b8)) can be shown to be convex, following elementary composition rules for convex functions (see [32], Section 3.2.4).\nThus, the only property missing is the smoothness of the approximation error hn , gn\u2212 fn. Rather than writing again a full proof, we now simply review the different places where this property is used, and which modifications should be made to the proofs of Propositions 3.3 and 3.4.\nIn the second step of this proof, we require the functions hn to be uniformly Lipschitz and uniformly bounded. It easy to check that it is still the case with the assumptions we made in Proposition 3.5.\nThe last step about the asymptotic point condition is however more problematic, where we cannot show anymore that the quantity \u2207h\u0304n(\u03b8n) converges to zero (since h\u0304n is not differentiable anymore). Instead, we need to show that the directional derivative \u2207h\u0304n(\u03b8n,\u03b8\u2212\u03b8n)\u2016\u03b8\u2212\u03b8n\u2016 uniformly converges to zero on \u0398.\nWe will show the result for K = 1; it will be easy to extend it to any arbitrary K > 2. We remark that\n\u2207h\u0304n(\u03b8n, \u03b8 \u2212 \u03b8n) = \u2207h\u03040,n(\u03b8n)\u22a4(\u03b8 \u2212 \u03b8n) + lim t\u21920+ h\u03041,n(\u03b31(\u03b8n + t(\u03b8 \u2212 \u03b8n)))\u2212 h\u03041,n(\u03b31(\u03b8n)) t ,\nwhere h\u03040,n and h\u03041,n are defined similarly as h\u0304n for the functions h0,n , g0,n \u2212 f0,n and h1,n , g1,n \u2212 f1,n respectively. Since h\u0304n(\u03b8n) is shown to converge to zero, we have that the non-negative quantities h\u03040,n(\u03b8n) and h\u03041,n(\u03b31(\u03b8n)) converge to zero as well. Since h\u03040,n and h\u03041,n are differentiable and their gradients are Lipschitz, we use similar arguments as in the proof of Proposition 3.3, and we have that \u2207h\u03040,n(\u03b8n) and h\u0304\u20321,n(\u03b31(\u03b8n)) converge to zero (where h\u0304\u20321,n is the derivative of h\u03041,n. Concerning the second term, we can make the following Taylor expansion for h\u03041,n:\nh\u03041,n(\u03b31(\u03b8n+z)) = h\u03041,n(\u03b31(\u03b8n))+h\u0304 \u2032 1,n(\u03b31(\u03b8n))(\u03b31(\u03b8n+z)\u2212\u03b31(\u03b8n))+O ( (\u03b31(\u03b8n + z)\u2212 \u03b31(\u03b8n))2 ) ,\nwhere the constant in the O notation is independent of \u03b8n and z (since the derivative is L1-Lipschitz). Plugging z , t(\u03b8 \u2212 \u03b8n) in this last equation, and using the Lipschitz property of \u03b31, we have\nlim t\u21920+\n\u2223 \u2223 \u2223 \u2223 h\u03041,n(\u03b31(\u03b8n + t(\u03b8 \u2212 \u03b8n)))\u2212 h\u03041,n(\u03b31(\u03b8n)) t \u2223 \u2223 \u2223 \u2223 \u2264 |h\u0304\u20321,n(\u03b31(\u03b8n))|\u2016\u03b8 \u2212 \u03b8n\u2016.\nSince h\u0304\u20321,n(\u03b31(\u03b8n)) converges to zero, we can conclude the proof of the modified Proposition 3.3.\nThe proof of Proposition 3.4 can be modified with similar arguments."}, {"heading": "D Additional Experimental Results", "text": "We present in Figures 4 and 5 some additional experimental comparisons, which complement the ones of Section 4.1. Figures 6 and 7 present additional plots from the experiment of Section 4.2. Finally, we present three dictionaries corresponding to the experiment of Section 4.3 in Figures 8, 9 and 10."}, {"heading": "Supplementary References", "text": "[31] D.P. Bertsekas. Nonlinear programming. Athena Scientific Belmont, 1999. 2nd edition.\n[32] S.P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\n[33] D. L. Fisk. Quasi-martingales. T. Am. Math. Soc., 120(3):359\u2013388, 1965.\n[34] M. Me\u0301tivier. Semi-martingales. Walter de Gruyter, 1983.\n[35] Y. Nesterov. Introductory lectures on convex optimization. Kluwer Academic Publishers, 2004.\n[36] Y. Nesterov and J.-P. Vial. Confidence level solutions for stochastic programming. Automatica, 44(6):1559\u20131568, 2008.\n[37] J. Nocedal and S.J. Wright. Numerical optimization. Springer Verlag, 2006. 2nd edition."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with largescale or possibly infinite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence rate of O(1/ \u221a n) after n iterations, and of O(1/n) for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale l1logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems.", "creator": "LaTeX with hyperref package"}}}