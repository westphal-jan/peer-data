{"id": "1307.4689", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2013", "title": "DASH: Dynamic Approach for Switching Heuristics", "abstract": "complete pattern tree invariant search engine is a locally highly research effective method for tackling mip running problems, and over like the years, a plethora of branching heuristics simulations have already been introduced competing to independently further refine how the coding technique for varying simulation problems. recently, individual portfolio algorithms have had taken the process a different step further, trying to predict independently the best linear heuristic for investigating each instance solution at hand. while however, the functional motivation behind algorithm selection can be taken further still, and even used to dynamically choose through the most appropriate statistical algorithm for treating each repeatedly encountered subproblem. in this paper we identify roughly a feature structure space hypothesis that captures both ways the evolution of defeating the maze problem displayed in designing the branching complexity tree type and the unique similarity gained among subproblems properties of instances from each the same mip models. we show how to actively exploit integrating these typical features to decide the authors best first time to switch the branching heuristic and best then both show quickly how such for a system method can be trained most efficiently. experiments on a more highly heterogeneous web collection of varying mip instances show significant gains improving over the larger pure algorithm selection approach so that for a each given instance uses only a single given heuristic throughout the search.", "histories": [["v1", "Wed, 17 Jul 2013 16:31:14 GMT  (136kb)", "http://arxiv.org/abs/1307.4689v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["giovanni di liberto", "serdar kadioglu", "kevin leo", "yuri malitsky"], "accepted": false, "id": "1307.4689"}, "pdf": {"name": "1307.4689.pdf", "metadata": {"source": "CRF", "title": "DASH: Dynamic Approach for Switching Heuristics", "authors": ["Giovanni Di Liberto", "Serdar Kadioglu", "Kevin Leo", "Yuri Malitsky"], "emails": ["dilibert@dei.unipd.it,", "y.malitsky@umail.ucc.ie", "serdrk@gmail.com", "kevin.leo@monash.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n46 89\nv1 [\ncs .A\nI] 1\n7 Ju\nMixed Integer Programming (MIP) is a powerful problem representation that is ubiquitous in the modern world. The problem is represented as the maximization of an objective function while maintaining the specified linear inequalities and restricting some variables to only take integer values while others are allowed to take on any real value.\nMaximize : cTx\nsubject to : Ax \u2264 b\nl \u2264 x \u2264 u\nxj integer \u2200j \u2208 D , where D \u2286 {1..n}\nThrough this straight forward formulation it is possible to define a wide variety of problems, ranging from scheduling [1] to production planning [2] to network design [3] to auctions [4] and many others.\nIn practice, these problems are typically approached using branch and bound or branch and cut techniques [5,6]. Here the main idea is to perform deterministic\nand inductive reasoning to lower the domains of the variables. When this is no longer possible, a variable is selected and assigned a value based on some guiding heuristic. Once such a decision is made the search proceeds to function deterministically. If or when it is later found that the decision led to an infeasible or sub-optimal solution, the search backtracks, returning to the parent node to try an alternate assignment.\nThe key behind the success or failure of this complete search approach is the order in which the variables are selected and the order the values are assigned to them. Choosing certain variables can significantly reduce the domains of all other variables, allowing the deterministic analysis to quickly find a contradiction or determine that no improving solution can exist in the subproblem. Alternatively, choosing the wrong variables can lead to exponentially longer run times.\nDue to the critical importance of the selection of the branching variable and value, there have been a number of heuristics presented [7,8,9]. Several of them are based on simple rules, eg. Most/Least Infeasible Branching base their decisions on the variable\u2019s fractionality. Other heuristics, like Pseudocost Branching, can adapt over time while others, like Strong Branching, test which of the fractional candidates gives the best progress before actually committing to any of them. Finally, there are also hybrid techniques, eg. Reliability Branching, that put together the positive aspects of Strong Branching and Pseudocost Branching. A good overview of these and other heuristics can be found in [9].\nThe efficiency of the search, however, can be much improved if we could use the correct heuristic at the correct time in the search. Work with portfolios has already shown that there is often no single solver or approach that works optimally on every instance [10,11,12]. We also know that throughout the branching process, as certain variables get assigned and the domains of others are changed, the underlying structure of the subproblems changes. In this paper, we show how to identify changes in the problem structure and therefore how to make a decision of when it is the best time to switch the employed guiding heuristic.\nWhile a similar approach was recently introduced in [13], this work expands the research from the set partitioning problem with problem dependent heuristics, to the much more general problem of MIP. We also provide a detailed analysis of how the problem structure changes over time and clearly demonstrate the effectiveness of the proposed approach on real benchmarks that are of interest to the community."}, {"heading": "1 Dynamic Switching", "text": "The objective motivating this work is to create a solver that dynamically adjusts its search strategy, selecting the most appropriate heuristic for the subproblem at hand. In a high-level overview, we want the solver to perform a standard branch and bound procedure, but before choosing the next branching variable and value, it will analyze the structure of the current subproblem using a set of representative features. Using this structural information, the solver would be able to predict that a specific heuristic is likely better than the alternatives, and\nAlgorithm 1 DASH - branch callback\n1: procedure branchCallback(subproblem, parent, centers, heuristics) 2: if depth < maxDepth and depth % interval == 0 then 3: x\u2190 featuresComputation(subproblem) 4: for all center c in cs do 5: distancei \u2190 euclideanDistance(x, centers) 6: end for 7: cluster \u2190 argmin(distance) 8: heuristic\u2190 heuristicscluster 9: else 10: heuristic\u2190 parent.heuristic 11: end if 12: ExecuteBranching(subproblem,heuristic) 13: end procedure\nemploy it to make the next decision. We refer to such a strategy as a Dynamic Approach for Switching Heuristics (DASH).\nThe specifics of DASH are described in Algorithm 1. Modeled after the ISAC approach [10], DASH assumes that instances that have similar features share the same structure and so will yield to the same algorithm. We will therefore employ clustering to identify these groups of instances. DASH is provided the current subproblem, the heuristic employed by the parent node, the centers of the known clusters, and the list of available heuristics. Because determining the feature can be computationally expensive and because switching heuristics at lower depths of the search tree has a smaller impact on the quality of the search, DASH only chooses to switch the guiding heuristic up to a certain depth and only at predetermined intervals, choosing the parent\u2019s heuristic in all other cases. When a decision does need to be made, the approach computes the features of the provided subproblem and determines the nearest cluster based on the Euclidean distance. In theory, any distance metric can be used here, but in practice we found that Euclidean works well in the general case. In the end, DASH, employs the heuristic that has been determined best for that cluster.\nAs can be inferred from this algorithm, the key component that determines the success or failure of DASH, is the correct assignment of heuristic to cluster. To train this, we follow a similar procedure first described in [13]. For each instance in the training set we compute an assortment of subproblems that are observed when using each of our heuristics. This extended problem set allows us to get a better overview of the type of subproblems DASH will be encountering, as opposed to just using the original training instances. Computing the features of the extended problem set, we cluster the instances. For this we employ gmeans [14], a general clustering approach that automatically determines the best number of clusters for the dataset in question. In particular, this clustering approach assumes that a good cluster is one that has a gaussian distribution around the cluster center. Starting with all instances being in a single cluster, g-means iteratively calls 2-means, to split a cluster in two. If the new clusters\nare more gaussian than the original, the split is accepted and the procedure continues. Once all the instances are clustered, the clusters with fewer instances than a certain threshold are absorbed by the nearest clusters.\nOnce all the subproblems are clustered, we have to determine which heuristic is best in which scenario. However, an important caveat to this is that the decision of a using a heuristic for a certain cluster also affects all other decisions. This is because DASH can switch heuristics several times, and the types of subproblems observed after applying one heuristic will likely be different then when another one has been applied. Therefore, we employ the parameter tuner GGA [15] to simultaneously assign heuristics to all clusters, using only the original instances for training."}, {"heading": "2 Experimental Setup", "text": "In order to set the stage for DASH, three things are necessary. First, we must have a descriptive feature set that can correctly distinguish between different classes of instances, but also do this with minimal overhead. Second, there must be a diverse set of heuristics each of which performs well on different kinds of instances. Finally, there must be a heterogeneous domain, with a large number of benchmark instances. We touch on all three of these components in this section.\nWe implement our feature computation and heuristics through extending the state-of-the-art MIP solver Cplex version 12.5 [16]. Here, we only modify the built in branching strategy by implementing a branch callback function based on Algorithm 1. Because all the tested approaches require this branch callback to be enabled, the comparability of the results is guaranteed4. Finally, in order to obtain reliable results, we run each Cplex execution in the single core version. The experiments were run on dual Intel Xeon E5430 quad-core processors (2.66Ghz) computers with 12GB of DDR-2 FB-DIMM 667MHz memory."}, {"heading": "2.1 Feature Space", "text": "The features have to capture as many aspects of the problems as possible without becoming too expensive to compute. To do this, we gather statistics about the problem definition of the remaining subproblem, a process similar to the one employed in [13]. Specifically, we compute:\n\u2013 Percentage of variables in the subproblem; \u2013 Percentage of variables in the objective function of the subproblem; \u2013 Percentage of equality and inequality constraints; \u2013 Statistics (min, max, avg, std) of how many variables are in each constraint; \u2013 Statistics of the number of constraints in which each variable is used; \u2013 Depth in the branch and bound tree.\nWherever a feature has to do with the problem variables, we separately compute the same feature for each type of variable type: eg. continuous, integer, and binary. Therefore, the resulting set is composed of 40 features.\n4 Note that Cplex switches off certain heuristics as soon as branch callbacks, even empty ones, are being used so that the entire search behavior could be different."}, {"heading": "2.2 Branching Heuristics", "text": "In order to realize and test our solving approach, we implemented a portfolio of six branching heuristics.\nMost Fractional Rounding (MF) One of the simplest MIP branching techniques is to select the variable that has a relaxed LP solution whose fractional part is most fractional and to round it first. The driving reasoning behind this is to make decisions on variables that deterministic analysis is least certain about. Therefore, this heuristic strives to find infeasible solutions as quickly as possible.\nLess Fractional Rounding (LF) Alternatively to MF, this technique selects the the variable that has a relaxed LP solution whose fractional part is closest to an integer value and to round it first. This is done to gently nudge the deterministic reasoning in whatever direction it is currently pursuing, with a smallest chance of making a mistake.\nLess Fractional And Highest Objective Rounding (LFHO) This heuristic is based on the same motivation behind the Less Fractional Branching. For each subproblem we branch on the variable for which the pair p=(fr, -obj) is minimized (where fr is the fractionality and obj is the objective value). This means that, if we branch on a variable k in [1,n], the following propriety is guaranteed:\n\u2200i \u2208 [1, n], frk < fri or objk > obji\nMost Fractional And Highest Objective Rounding (MFHO) We use a modification of the previous approach, but this time we focus on the most fractional variables. For each subproblem we branch on the variable for which the pair p=(fr, obj) is maximized. In this case the guaranteed property is:\n\u2200i \u2208 [1, n], frk > fri or objk > obji\nPseudocost Branching Weigthed Score (PW) This heuristic is based on the pseudocosts, numerical values that estimates the variation in objective value for rounding up or rounding down, called respectively up-pseudocost and downpseudocost. The pseudocosts of a variable can be combined in a score function (2.2) that returns a numeric value. This result is used to guide the branching, for which we choose the variable that maximize the score. Further details can be found in [7].\nscore(q\u2212, q+) = (1\u2212 \u00b5) \u2217min(q\u2212, q+) + (\u00b5) \u2217max(q\u2212, q+), \u00b5 = 1/6.\nPseudocost Branching Product Score (P) This approach is based on the same idea as PW. The difference lies in the score function that is now the product of the two pseudocosts."}, {"heading": "2.3 Dataset", "text": "In order to obtain a solver that works well for a generic MIP problem we collected instances from many different datasets: miplib2010 [17], fc [18], lotSizing [19], mik [20], nexp [21], region [22], and pmedcapv, airland, genAssignment, scp, SSCFLP were originally downloaded from [23]. From an initial dataset of about 900 instances we filtered those for which all our solvers timed out in 1,800 seconds. We then removed the easy instances, solved entirely during the Cplex presolving or in less than one second by each solver. We finally obtained a dataset of 341 instances with the desired properties. We randomly selected 180 for the training set and 161 for the testing set.\nIf we cluster our training data the distribution of instances per cluster can be seen in Table 1. Each row is normalized to sum unto 100%. Thus for Cluster 1, 25% of the instances are from the airland dataset. From this table we first observe that there are not enough clusters to perfectly separate the different datasets into unique clusters. This, however, is not what we would want to see. This is because we are more interested in capturing similarities between instances, not splitting benchmarks. And we observe that the region100 and region200 instances are grouped together. We also see that Cluster 4 logically groups the LotSizing and the SSCFLP instances together. Finally, we see that the instances from the miplib, those instances that are supposed to be an overview of all problem types, are spread across all clusters.\nThis clustering therefore demonstrates that we both have a diverse set of instances and that our features are representative enough to automatically notice interesting groupings."}, {"heading": "3 Numerical Results", "text": "With the described methodology, the main question that needs to be addressed is whether switching heuristics can indeed be beneficial to the performance of the solver. To test this, for each of the instances in our test set, we ran each of the implemented heuristics without allowing any switching. We then also ran two versions of a solver that switched between heuristics uniformly at random. The first solver switched between all heuristics, while the second switched only among the top four best heuristics. The results are summarized in Table 2.\nWhat we observe is that neither of the random switching heuristics perform very well by themselves. However, based on the performance of the virtual best solver5 that employs these new solvers, the performance can be further improved beyond what is possible when always sticking to the same heuristic. The question therefore now, becomes, if we can get improved performance just by switching between heuristics randomly, can we do even better if we do so intelligently?\nTo answer this question, we must first set a few parameters of our solver. Particularly, till what depth should we allow our solver to switch heuristics, and at what interval? For this, we cluster the extended dataset that includes both the original training instances and the possible observed subproblems. There are a total of 10 clusters formed. Projecting the feature space into two dimensions using Principal Component Analysis (PCA) [24] we present Figure 1. Here, the cluster boundaries are represented by the solid lines, and the best heuristic for each cluster is represented by a unique symbol at its center. On these figures, we also show the typical way in which features change as the problem is solved with a particular heuristic. The nodes are colored based on the depth of the tree, with (a) showing all the observed subproblems and (b) that of a single branch.\n5 VBS is an oracle solver that for every instance always uses the strategy that results in the shortest runtime\nWhat this figure shows is that the features change gradually. This means that there is no need checking the features at every decision node. We therefore choose to check the subproblem features at every 3rd node. Similarly, the figure and those like it, show that using a depth of 10 is reasonable, as in most cases the nodes don\u2019t span across more than two clusters.\nWe use GGA to tune the parameters of DASH, computing the best heuristic for each cluster. We then present the results in Table 3 where we compare it to a vanilla ISAC approach that for a given instance chooses the single best heuristic and then does not allow any switching. What we observe is that DASH is able to perform much better than its more rigid counterpart. However, we do allow for the possibility that switching heuristics might not be the best strategy for every instance. We therefore also introduce DASH+, which first clusters the original instances using ISAC and then allows each cluster to independently decide if it wants to use dynamic heuristic switching.\nTaking a lesson from [25], which shows that often the features are not equally important, we tried to achieve better overall performance including a feature selection operation. In this paper we utilize the information gain filtering technique, often used in decision trees. In particular, this method is based on the calculation of entropy of the data as a whole and for each class. We apply the feature filtering to ISAC and DASH+ referring to them, respectively, as ISAC filt and DASH+filt, having an improvement in both cases. In particular, the resulting solver DASH+filt performs considerably better than everything else.\nWe finally show the performance of a virtual best solver if allowed to use DASH. And what we observe is that even though the current implementation cannot overtake VBS, future refinements to the portfolio techniques will be able to achieve performances much better than techniques that rely purely on sticking to a single heuristic."}, {"heading": "4 Conclusion", "text": "In this paper we introduce a Dynamic Approach for Switching Heuristics (DASH). Using MIP as the running example, we show how to automatically determine when a subproblem observed during a branch and bound search is significantly different from what has been observed before, and therefore warrants a change of tactics used while solving it. Employing a diverse set of instances we demonstrate that significant performance improvements are possible if a solver does not stick to using a single guiding heuristic."}], "references": [{"title": "Algorithm selection and scheduling", "author": ["S. Kadioglu", "Y. Malitsky", "A. Sabharwal", "H. Samulowitz", "M. Sellmann"], "venue": "CP", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Production Planning by Mixed Integer Programming", "author": ["A.P.Y. Wolsey Laurence"], "venue": "Springer, Berlin", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Network Design", "author": ["T.M.A. Balakrishnan", "P. Mirchandani"], "venue": "Annotated Bibliographies in Combinatorial Optimization. Wiley, New York", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "An efficient approximate allocation algorithm for combinatorial auctions", "author": ["E. Zurel", "N. Nisan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Branch-and-bound methods: General formulation and properties", "author": ["L. Mitten"], "venue": "Operations Research", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1970}, {"title": "A branch-and-cut algorithm for the resolution of large scale traveling salesman problems", "author": ["M. Padberg", "G. Rinaldi"], "venue": "SIAM Review 33", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Branching rules revisited", "author": ["T. Achterberg", "T. Koch", "A. Martin"], "venue": "Operations Research Letters 33", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A computational study of search strategies for mixed integer programming", "author": ["J.T. Linderoth", "M.W.P. Savelsbergh"], "venue": "INFORMS Journal on Computing 11", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Constraint Integer Programming", "author": ["T. Achterberg"], "venue": "PhD thesis, Technische Universit\u00e4t Berlin", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Isac \u2013instance-specific algorithm configuration", "author": ["S. Kadioglu", "Y. Malitsky", "M. Sellmann", "K. Tierney"], "venue": "ECAI", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Using casebased reasoning in an algorithm portfolio for constraint solving", "author": ["E. O\u2019Mahony", "E. Hebrard", "A. Holland", "C. Nugent", "B. O\u2019Sullivan"], "venue": "AICS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Satzilla2012: Improved algorithm selection based on cost-sensitive classification models (2012) SAT Competition", "author": ["L. Xu", "F. Hutter", "J. Shen", "H.H. Hoos", "K. Leyton-Brown"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Non-model-based search guidance for set partitioning problems", "author": ["S. Kadioglu", "Y. Malitsky", "M. Sellmann"], "venue": "AAAI", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning the k in k-means", "author": ["G. Hamerly", "C. Elkan"], "venue": "NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "A gender-based genetic algorithm for the automatic configuration of algorithms", "author": ["C. Ans\u00f3tegui", "M. Sellmann", "K. Tierney"], "venue": "CP", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "MIPLIB 2010 - Mixed Integer Programming Library version 5", "author": ["T. Koch", "T. Achterberg", "E. Andersen", "O. Bastert", "T. Berthold", "R.E. Bixby", "E. Danna", "G. Gamrath", "A.M. Gleixner", "S. Heinz", "A. Lodi", "H. Mittelmann", "T. Ralphs", "D. Salvagnin", "D.E. Steffy", "K. Wolter"], "venue": "Mathematical Programming Computation 3", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Flow pack facets of the single node fixed-charge flow polytope", "author": ["A. Atamt\u00fcrk"], "venue": "Operations Research Letters 29", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "A study of the lot-sizing polytope", "author": ["A. Atamt\u00fcrk", "J.C.M. oz"], "venue": "Mathematical Programming 99", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "On the facets of the mixed\u2013integer knapsack polyhedron", "author": ["A. Atamt\u00fcrk"], "venue": "Mathematical Programming 98", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Valid inequalities for problems with additive variable upper bounds", "author": ["A. Atamt\u00fcrk", "G.L. Nemhauser", "M.W.P. Savelsbergh"], "venue": "Mathematical Programming 91", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Towards a universal test suite for combinatorial auction algorithms", "author": ["K. Leyton-Brown", "M. Pearson", "Y. Shoham"], "venue": "ACM Conference on Electronic Commerce (EC00)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Principal component analysis", "author": ["H. Abdi", "L.J. Williams"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Feature filtering for instance-specific algorithm configuration", "author": ["C. Kroer", "Y. Malitsky"], "venue": "ICTAI", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Through this straight forward formulation it is possible to define a wide variety of problems, ranging from scheduling [1] to production planning [2] to network design [3] to auctions [4] and many others.", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "Through this straight forward formulation it is possible to define a wide variety of problems, ranging from scheduling [1] to production planning [2] to network design [3] to auctions [4] and many others.", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "Through this straight forward formulation it is possible to define a wide variety of problems, ranging from scheduling [1] to production planning [2] to network design [3] to auctions [4] and many others.", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "Through this straight forward formulation it is possible to define a wide variety of problems, ranging from scheduling [1] to production planning [2] to network design [3] to auctions [4] and many others.", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "In practice, these problems are typically approached using branch and bound or branch and cut techniques [5,6].", "startOffset": 105, "endOffset": 110}, {"referenceID": 5, "context": "In practice, these problems are typically approached using branch and bound or branch and cut techniques [5,6].", "startOffset": 105, "endOffset": 110}, {"referenceID": 6, "context": "Due to the critical importance of the selection of the branching variable and value, there have been a number of heuristics presented [7,8,9].", "startOffset": 134, "endOffset": 141}, {"referenceID": 7, "context": "Due to the critical importance of the selection of the branching variable and value, there have been a number of heuristics presented [7,8,9].", "startOffset": 134, "endOffset": 141}, {"referenceID": 8, "context": "Due to the critical importance of the selection of the branching variable and value, there have been a number of heuristics presented [7,8,9].", "startOffset": 134, "endOffset": 141}, {"referenceID": 8, "context": "A good overview of these and other heuristics can be found in [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "Work with portfolios has already shown that there is often no single solver or approach that works optimally on every instance [10,11,12].", "startOffset": 127, "endOffset": 137}, {"referenceID": 10, "context": "Work with portfolios has already shown that there is often no single solver or approach that works optimally on every instance [10,11,12].", "startOffset": 127, "endOffset": 137}, {"referenceID": 11, "context": "Work with portfolios has already shown that there is often no single solver or approach that works optimally on every instance [10,11,12].", "startOffset": 127, "endOffset": 137}, {"referenceID": 12, "context": "While a similar approach was recently introduced in [13], this work expands the research from the set partitioning problem with problem dependent heuristics, to the much more general problem of MIP.", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "Modeled after the ISAC approach [10], DASH assumes that instances that have similar features share the same structure and so will yield to the same algorithm.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "To train this, we follow a similar procedure first described in [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "For this we employ gmeans [14], a general clustering approach that automatically determines the best number of clusters for the dataset in question.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": "Therefore, we employ the parameter tuner GGA [15] to simultaneously assign heuristics to all clusters, using only the original instances for training.", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "To do this, we gather statistics about the problem definition of the remaining subproblem, a process similar to the one employed in [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Further details can be found in [7].", "startOffset": 32, "endOffset": 35}, {"referenceID": 15, "context": "In order to obtain a solver that works well for a generic MIP problem we collected instances from many different datasets: miplib2010 [17], fc [18], lotSizing [19], mik [20], nexp [21], region [22], and pmedcapv, airland, genAssignment, scp, SSCFLP were originally downloaded from [23].", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "In order to obtain a solver that works well for a generic MIP problem we collected instances from many different datasets: miplib2010 [17], fc [18], lotSizing [19], mik [20], nexp [21], region [22], and pmedcapv, airland, genAssignment, scp, SSCFLP were originally downloaded from [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "In order to obtain a solver that works well for a generic MIP problem we collected instances from many different datasets: miplib2010 [17], fc [18], lotSizing [19], mik [20], nexp [21], region [22], and pmedcapv, airland, genAssignment, scp, SSCFLP were originally downloaded from [23].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "In order to obtain a solver that works well for a generic MIP problem we collected instances from many different datasets: miplib2010 [17], fc [18], lotSizing [19], mik [20], nexp [21], region [22], and pmedcapv, airland, genAssignment, scp, SSCFLP were originally downloaded from [23].", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "In order to obtain a solver that works well for a generic MIP problem we collected instances from many different datasets: miplib2010 [17], fc [18], lotSizing [19], mik [20], nexp [21], region [22], and pmedcapv, airland, genAssignment, scp, SSCFLP were originally downloaded from [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 20, "context": "In order to obtain a solver that works well for a generic MIP problem we collected instances from many different datasets: miplib2010 [17], fc [18], lotSizing [19], mik [20], nexp [21], region [22], and pmedcapv, airland, genAssignment, scp, SSCFLP were originally downloaded from [23].", "startOffset": 193, "endOffset": 197}, {"referenceID": 21, "context": "Projecting the feature space into two dimensions using Principal Component Analysis (PCA) [24] we present Figure 1.", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "Taking a lesson from [25], which shows that often the features are not equally important, we tried to achieve better overall performance including a feature selection operation.", "startOffset": 21, "endOffset": 25}], "year": 2013, "abstractText": "Complete tree search is a highly effective method for tackling MIP problems, and over the years, a plethora of branching heuristics have been introduced to further refine the technique for varying problems. Recently, portfolio algorithms have taken the process a step further, trying to predict the best heuristic for each instance at hand. However, the motivation behind algorithm selection can be taken further still, and used to dynamically choose the most appropriate algorithm for each encountered subproblem. In this paper we identify a feature space that captures both the evolution of the problem in the branching tree and the similarity among subproblems of instances from the same MIP models. We show how to exploit these features to decide the best time to switch the branching heuristic and then show how such a system can be trained efficiently. Experiments on a highly heterogeneous collection of MIP instances show significant gains over the pure algorithm selection approach that for a given instance uses only a single heuristic throughout the search. Mixed Integer Programming (MIP) is a powerful problem representation that is ubiquitous in the modern world. The problem is represented as the maximization of an objective function while maintaining the specified linear inequalities and restricting some variables to only take integer values while others are allowed to take on any real value.", "creator": "LaTeX with hyperref package"}}}