{"id": "1307.5101", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2013", "title": "Large-scale Multi-label Learning with Missing Labels", "abstract": "multi - label classification consolidation problems eventually abound in commercial practice ; probably as a result, also many performance methods have recently allegedly been previously proposed specifically for these problems. however, there are several two key challenges that data have not truly been adequately addressed : ( d a ) the intrinsic number of labels can generally be numerous, for that example, lost in searching the millions, and ( b ) ; the minimum test data can be riddled with potentially missing labels. in this pioneering paper, we study a generic framework problem for multi - quadrant label classification that directly addresses the above challenges. somewhat in particular, there we pose the problem as one of mere empirical risk minimization, where the preferred prediction parameter function is parameterized internally by sampling a low - rank matrix. technically we show that doing our approach derives because several existing label - compression theorem based improvement algorithms ( particularly such as excluding the more recently proposed cplst method ( chen song and gao lin, 2012 ) performed in a principled experimental manner. often a distinct key facet of our approach is that we simultaneously handle missing labels in the training set by applying those techniques exclusively from the domain of matrix multiplication completion. to develop simultaneously a scalable reduction algorithm : that can therefore handle a possibly larger number of core classes, this we basically use the alternating total minimization method to find exactly the chosen low - score rank parameter matrix. furthermore, for viewing the hypothetical special ranking case of $ $ l _ 2 2 $ 25 loss, we nonetheless show that adequate special order structure in constructing the test problem can be correctly exploited and finally the explicit alternating minimization operations algorithm can be efficiently implemented. finally, we present empirical evaluation results focusing on a variety thereof of benchmark datasets throughout and consequently show that our methods typically perform typically significantly moderately better than such existing label width compression based methods. moreover, we demonstrate scalability of making our approach by applying it to prove a large wikipedia description based dataset that has exactly 117, 78 564 training group data instances including and altogether 207, 69 386 labels.", "histories": [["v1", "Thu, 18 Jul 2013 23:55:55 GMT  (601kb,D)", "https://arxiv.org/abs/1307.5101v1", null], ["v2", "Mon, 14 Oct 2013 22:33:17 GMT  (718kb,D)", "http://arxiv.org/abs/1307.5101v2", null], ["v3", "Mon, 25 Nov 2013 16:57:43 GMT  (734kb,D)", "http://arxiv.org/abs/1307.5101v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hsiang-fu yu", "prateek jain 0002", "purushottam kar", "inderjit s dhillon"], "accepted": true, "id": "1307.5101"}, "pdf": {"name": "1307.5101.pdf", "metadata": {"source": "CRF", "title": "Large-scale Multi-label Learning with Missing Labels", "authors": ["Hsiang-Fu Yu", "Prateek Jain"], "emails": ["rofuyu@cs.utexas.edu", "prajain@microsoft.com", "t-purkar@microsoft.com", "inderjit@cs.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "Large scale multi-label classification is an important learning problem with several applications to real-world problems such as image and video annotation Carneiro et al. (2007); Wang et al. (2009) and query/keyword suggestions Agrawal et al. (2013). The goal in multi-label classification is to accurately predict a label vector y \u2208 {0, 1}L for a given data point x \u2208 Rd. This problem has been studied extensively in the domain of structured output learning, where the number of labels is assumed to be small and the main focus is thus, on modeling inter-label correlations and using them to accurately predict the label vector Hariharan et al. (2010).\nDue to several motivating real-life applications, recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large, with the key challenge being the design of scalable algorithms that offer real-time predictions and have a small memory footprint. In such situations, simple methods such as 1-vs-all or Binary Relevance (BR), that treat each label as a separate binary classification problem fail miserably. For a problem with (say) 104 labels and 106 features, which is common in several applications, these methods have a memory footprint of around 100 Gigabytes and offer slow predictions.\nar X\niv :1\n30 7.\n51 01\nv3 [\ncs .L\nG ]\n2 5\nN ov\nA common technique that has been used to handle the label proliferation problem in several recent works is \u201clabel space reduction\u201d. The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details.\nIn this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case. However, the flexibility of the framework allows us to use other loss functions and regularizers that are useful for preventing overfitting and increasing scalability.\nMoreover, we can extend our formulation to handle missing labels; in contrast, most dimension reduction formulations (including CPLST) cannot accommodate missing labels. The ability to learn in the presence of missing labels is crucial as for most real-world applications, one cannot expect to accurately obtain (either through manual or automated labeling) all the labels for a given data point. For example, in image annotation, human labelers tag only prominent labels and typically miss out on several objects present in the image. Similarly, in online collections such as Wikipedia, where articles get tagged with categories, human labelers usually tag only with categories they know about. Moreover, there might be considerable noise/disagreement in the labeling.\nIn order to solve for the low-rank linear model that results from our formulation, we use the popular alternating minimization algorithm that works well despite the non-convexity of the rank constraint. For general loss functions and trace-norm regularization, we exploit subtle structures present in the problem to design a fast conjugate gradient based method. For the special case of squared-L2 loss and trace-norm regularization, we further exploit the structure of the loss function to provide a more efficient and scalable algorithm. As compared to direct computation, our algorithm is O(d\u0304) faster, where d\u0304 is the average number of nonzero features in an instance.\nOn the theoretical side, we perform an excess risk analysis for the trace-norm regularized ERM formulation with missing labels, assuming labels are observed uniformly at random. Our proofs do not follow from existing results due to missing labels and require a careful analysis involving results from random matrix theory. Our results show that while in general the low-rank promoting trace-norm regularization does not provide better bounds than learning a full-rank matrix (e.g. using Frobenius norm regularization), for several interesting data distributions, trace-norm regularization does indeed give significantly better bounds. More specifically, for isotropic data distributions, we show that trace-norm based methods have excess risk of O( 1\u221a\nnL ) while full-rank learning can only guarantee O( 1\u221a n ) excess risk, where n is the number of training\npoints and L is the number of labels. Finally, we provide an extensive empirical evaluation of our method on a variety of benchmark datasets. In particular, we compare our method against three recent label compression based methods: CPLST Chen & Lin (2012), Bayesian-CS Kapoor et al. (2012), and WSABIE Weston et al. (2010). On almost all benchmark datasets, our method significantly outperforms these methods, both in the presence and absence of missing\nlabels. Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods.\nRelated Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al. (2012), or reduce the feature dimensionality, such as Sun et al. (2011), or both, such as WSABIE Weston et al. (2010). Most of these existing techniques are tied to a specific loss function (e.g., CPLST and BCS cater only to the squared-L2 loss, and WSABIE works with the weighted approximate ranking loss) and/or cannot accommodate missing labels.\nOur framework models multi-label classification as a general ERM problem with a low-rank constraint, which not only generalizes both label and feature dimensionality reduction but also brings in the ability to support various loss functions and allows for rigorous generalization error analysis. We show that our formulation not only retrieves CPLST, which has been shown to be fairly accurate, as a special case, but in fact substantially enhances it by use of regularization, other loss functions, allowing missing labels etc.\nPaper Organization. We start by studying the generic low-rank empirical risk minimization framework for multi-label learning in Section 2. Next, we propose efficient algorithms for the framework in Section 3 and analyze the generalization performance for the trace-norm regularized ERM in Section 4. We present empirical results in Section 5, and our conclusions in Section 6."}, {"heading": "2 Problem Formulation", "text": "In this section we present a generic ERM-style framework for multi-label classification. For each training point, we shall receive a feature vector xi \u2208 Rd and a corresponding label vector yi \u2208 {0, 1}L with L labels. For any j \u2208 [L], yji = 1 will denote that the lth label is \u201cpresent\u201d or \u201con\u201d whereas y j i = 0 will denote that the label is \u201cabsent\u201d or \u201coff\u201d. Note that although we focus mostly on the binary classification setting in this paper, our methods easily extend to the multi-class setting where yji \u2208 {1, 2, . . . , C}.\nOur predictions for the label vector shall be parametrized as f(x;Z) = ZTx, where Z \u2208 Rd\u00d7L. Although we have adopted a linear parametrization here, our results can easily be extended for non-linear kernels as well. Let `(y, f(x;Z)) \u2208 R be the loss function that computes the discrepancy between the \u201ctrue\u201d label vector and the prediction. We assume that the loss function is decomposable, i.e., `(y, f(x;Z)) =\u2211L\nj=1 `(y j , f j(x;Z)).\nThe motivation for our framework comes from the observation that although the number of labels in a multi-label classification problem might be large, there typically exist significant label correlations, thus reducing the effective number of parameters required to model them to much less than d \u00d7 L. We capture this intuition by restricting the matrix Z to learn only a small number of \u201clatent\u201d factors. This constrains Z to be a low rank matrix which not only controls overfitting but also gives computational benefits.\nGiven n training points our training set will be (X,Y ) whereX = [x1, . . . ,xn]T and Y = [y1 y2 . . . yn]T . Using the loss function `, we propose to learn the parameters Z by using the canonical regularized empirical\nrisk minimization (ERM) method, i.e.,\nZ\u0302 = arg min Z J(Z) = n\u2211 i=1 L\u2211 j=1 `(Yij , f j(xi;Z)) + \u03bb \u00b7 r(Z),\ns.t. rank(Z) \u2264 k, (1)\nwhere r(Z) : Rd\u00d7L \u2192 R is a regularization function. In the presence of missing labels, we compute the loss only over the known labels, i.e.,\nZ\u0302 = arg min Z J\u2126(Z) = \u2211 (i,j)\u2208\u2126 `(Yij , f j(xi;Z)) + \u03bb \u00b7 r(Z),\ns.t. rank(Z) \u2264 k, (2)\nwhere \u2126 \u2286 [n] \u00d7 [L] is the index set that represents \u201cknown\u201d labels. Note that in this work, we assume the standard missing value setting, where each label can be either on, off (i.e., Yij = 1 or 0), or missing (Yij =?); several other works have considered another setting where only positive labels are known and are given as 1 in the label matrix, while negative or missing values are all denoted by 0 Agrawal et al. (2013); Bucak et al. (2011).\nNote that although the above formulation is NP-hard in general due to the non-convex rank constraint, for convex loss functions, one can still utilize the standard alternating minimization method. Moreover, for the special case of L2 loss, we can derive closed form solutions for the full-label case (1) and show connections to several existing methods.\nWe would like to note that while the ERM framework is well known and standard, most existing multilabel methods for large number of labels motivate their work in a relatively ad-hoc manner. By studying this formal framework, we can show that existing methods like CPLST Chen & Lin (2012) are in fact a special case of this generic framework (see next section). Furthermore, having this framework also helps us in studying generalization error bounds for our methods and identifying situations where the methods can be expected to succeed (see Section 4).\n2.1 Special Case: Squared-L2 loss\nIn this section, we study (1) and (2) for the special case of squared L2 loss function, i.e., `(y, f(x;Z)) = \u2016y \u2212 f(x;Z)\u201622. We show that in the absence of missing labels, the formulation in (1) can be solved optimally for the squared L2 loss using SVD. Furthermore, by selecting an appropriate regularizer r(Z) and \u03bb, our solution for L2 loss is exactly the same as that of CPLST Chen & Lin (2012).\nWe first show that the unregularized form of (1) with `(y, f(x;Z)) = \u2016y \u2212 ZTx\u201622 has a closed form solution.\nClaim 1. If `(y, f(x;Z)) = \u2016y \u2212 ZTx\u201622 and \u03bb = 0, then\nVX\u03a3 \u22121 X Mk = arg min Z:rank(Z)\u2264k \u2016Y \u2212XZ\u20162F , (3)\nwhere X = UX\u03a3XV TX is the thin SVD decomposition of X , and Mk is the rank-k truncated SVD of M \u2261 UTXY .\nProof of Claim 1. Let X = UX\u03a3XV TX be the thin SVD decomposition of X , and Mk be the rank-k truncated SVD approximation of UTXY . We have\narg min Z:rank(Z)\u2264k\n\u2016Y \u2212XZ\u2016F\n= arg min Z:rank(Z)\u2264k\n\u2016(UXUTX)(Y \u2212XZ) + (I \u2212 UXUTX)(Y \u2212XZ)\u2016F\n= arg min Z:rank(Z)\u2264k\n\u2016UXUTX(Y \u2212XZ)\u2016F + \u2016(I \u2212 UXUTX)(Y \u2212XZ)\u2016F\n= arg min Z:rank(Z)\u2264k\n\u2016UTX(Y \u2212XZ)\u2016F\n= arg min Z:rank(Z)\u2264k\n\u2016UTXY \u2212 \u03a3XV TXZ)\u2016F\n=VX\u03a3 \u22121 X Mk\nWe now show that this is exactly the solution obtained by Chen & Lin (2012) for their CPLST formulation.\nClaim 2. The solution to Equation 3 is equivalent to ZCPLST = WCPLSTHTCPLST which is the closed form solution for the CPLST scheme Chen & Lin (2012), i.e.,\n(WCPLST , HCPLST )\n= arg min W\u2208Rd\u00d7k H\u2208RL\u00d7k\n\u2016XW \u2212 Y H\u20162F + \u2016Y \u2212 Y HHT \u20162F ,\ns.t. HTH = Ik. (4)\nProof of Claim 2. Let Uk[A]\u03a3k[A]Vk[A] be the rank-k truncated SVD approximation of a matrix A. In Chen & Lin (2012), the authors show that the closed form solution to (4) is\nHC = Vk[Y TXX\u2020Y ], WC = X \u2020Y HC ,\nwhere X\u2020 is the pseudo inverse of X . It follows from X\u2020 = VX\u03a3\u22121X U T X that Y TXX\u2020Y = Y TUXU T XY = MTM and Vk[Y TXX\u2020Y ] = Vk[M ]. Thus, we have\nZCPLST = WCH T C\n= X\u2020Y HCH T C = V TX\u03a3 \u22121 X U T XY Vk[M ]Vk[M ] T = V TX\u03a3 \u22121 X MVk[M ]Vk[M ] T = V TX\u03a3 \u22121 X Mk\nNote that Chen & Lin (2012) derive their method by relaxing a Hamming loss problem and dropping constraints in the canonical correlation analysis in a relatively ad-hoc manner. The above results, on the other hand, show that the same model can be derived in a more principled manner. This helps us in extending the method for several other problem settings in a principled manner and also helps in providing excess risk bounds:\n\u2022 As shown empirically, CPLST tends to overfit significantly whenever d is large. However, we can handle this issue by setting the regularization parameter appropriately. \u2022 The closed form solution in Chen & Lin (2012) cannot directly handle missing labels as it requires\nSVD on fully observed Y . In contrast, our framework can itself handle missing labels without any modifications. \u2022 The formulation in Chen & Lin (2012) is tied to the L2 loss function. In contrast, we can easily handle\nother loss functions; although, the optimization problem might become more difficult to solve."}, {"heading": "3 Algorithms", "text": "In this section, we apply the alternating minimization technique for optimizing (1) and (2). For a matrix Z with a known low rank k, it is inefficient to represent it using d \u00d7 L entries, especially when d and L are large. Hence we consider a low-rank decomposition of the form Z = WHT , where W \u2208 Rd\u00d7k and H \u2208 RL\u00d7k. We further assume that r(Z) can be decomposed into r1(W )+r2(H). In the following sections, we present results with the trace norm regularization, i.e., r(Z) = \u2016Z\u2016tr, which can be decomposed as \u2016Z\u2016tr = 12 ( \u2016W\u20162F + \u2016H\u20162F ) . Thus, minZ J\u2126(Z) under the rank constraint is equivalent to minimizing over W,H:\nJ\u2126(W,H) = \u2211\n(i,j)\u2208\u2126\n`(Yij ,x T i Whj) +\n\u03bb\n2\n( \u2016W\u20162F + \u2016H\u20162F ) (5)\nwhere hTj is the j-th row ofH . Note that when either ofW orH is fixed, J\u2126(W,H) becomes a convex function. This allows us to apply alternating minimization, a standard technique for optimizing functions with such a property, to (5). For a general loss function, after proper initialization, a sequence { ( W (t), H(t) ) } is generated by\nH(t) \u2190 arg min H J\u2126(W (t\u22121), H), W (t) \u2190 arg min W J\u2126(W,H (t)).\nFor a convex loss function, (W (t), H(t)) is guaranteed to converge to a stationary point when the minimum for both minH J\u2126 ( W (t\u22121), H ) and minW J\u2126 ( W,H(t) ) are uniquely defined (see Bertsekas, 1999, Proposition 2.7.1). In fact, when the squared loss is used and Y is fully observed, the case considered in Section 3.2, we can prove that (W (t), H(t)) converges to the global minimum of (5) when either \u03bb = 0 or X is orthogonal.\nOnce W is fixed, updating H is easy as each row hj of H can be independently updated as follows:\nhj \u2190 arg min h\u2208Rk \u2211 i:(i,j)\u2208\u2126 `(Yij ,x T i Wh) + 1 2 \u03bb \u00b7 \u2016h\u201622, (6)\nwhich is easy to solve as k is small in general. Based on the choice of the loss function, (6) is essentially a linear classification or regression problem over k variables with |{i : (i, j) \u2208 \u2126}| instances.\nIf H is fixed, updating W \u2208 Rd\u00d7k is more involved as all variables are mixed up due to the premultiplication withX . Let x\u0303ij = hj\u2297xi. It is not hard to see that updatingW is equivalent to a regularized linear classification/regression problem with |\u2126| data points {(Yij , x\u0303ij) : (i, j) \u2208 \u2126}. Thus if W \u2217 =\nAlgorithm 1 General Loss with Missing Labels To compute\u2207g(w): 1. A\u2190 XW , where vec (W ) = w. 2. Dij \u2190 `\u2032(Yij ,aTi hj), \u2200(i, j) \u2208 \u2126. 3. Return: vec ( XT (DH) ) + \u03bbw\nTo compute: \u22072g(w)s 1. A\u2190 XW , where vec (W ) = w. 2. B \u2190 XS, where vec (S) = s. 3. Uij \u2190 `\u2032\u2032(Yij ,aTi hj)bTi hj , \u2200(i, j) \u2208 \u2126. 4. Return: vec ( XT (UH) ) + \u03bbs.\nAlgorithm 2 Squared Loss with Full Labels To compute\u2207g(w): 1. A\u2190 XW , where vec (W ) = w. 2. B \u2190 Y H . 3. M \u2190 HTH . 4. Return: vec ( XT (AM \u2212B) ) + \u03bbw\nTo compute: \u22072g(w)s 1. A\u2190 XS, where vec (S) = s. 2. M \u2190 HTH . 3. Return: vec ( XT (AM) ) + \u03bbs\narg minW J\u2126(W,H) and we denote w\u2217 := vec (W \u2217), then w\u2217 = arg minw\u2208Rdk g(w),\ng(w) \u2261 \u2211\n(i,j)\u2208\u2126\n` ( Yij ,w T x\u0303ij ) + 1\n2 \u03bb \u00b7 \u2016w\u201622. (7)\nTaking the squared loss as an example, the above is equivalent to a regularized least squares problem with dk variables. When d is large, say 1M, the closed form solution, which requires inverting a dk\u00d7 dk matrix, can hardly be regarded as feasible. As a result, updating W efficiently turns out to be the main challenge for alternating minimization.\nIn large-scale settings where both dk and |\u2126| are large, iterative methods such as Conjugate Gradient (CG), which perform cheap updates and offer a good approximate solution within a few iterations, are more appropriate to solve (7). Several linear classification/regression packages such as LIBLINEAR Fan et al. (2008) can handle such problems if {x\u0303ij : (i, j) \u2208 \u2126} are available. The main operation in such iterative methods is a gradient calculation (\u2207g(w)) or a multiplication of the Hessian matrix and a vector (\u22072g(w)s). Let X\u0303 = [\u00b7 \u00b7 \u00b7 x\u0303ij \u00b7 \u00b7 \u00b7 ]T(i,j)\u2208\u2126 and d\u0304 = \u2211n i=1 \u2016x\u20160/n. Then these operations require at least nnz(X\u0303) = O(|\u2126|d\u0304k) time to compute in general. However, as we show below, we can exploit the structure in X\u0303 to develop efficient techniques such that both the operations mentioned above can be done in O ((|\u2126|+ nnz(X) + d+ L)\u00d7 k) time. As a result, iterative methods, such as CG, can achieve O(d\u0304) speedup. See Appendix A.1 for a detailed CG procedure for (7) with the squared loss. Our techniques, thus, make the alternating minimization method efficient enough to handle large-scale problems."}, {"heading": "3.1 Fast Operations for General Loss Functions with Missing Labels", "text": "We assume that the loss function is a general twice-differentiable function `(a, b), where a and b are scalars. Let `\u2032(a, b) = \u2202\u2202b`(a, b), and ` \u2032\u2032(a, b) = \u2202 2 \u2202b2 `(a, b). The gradient and the Hessian matrix for g(w) are:\n\u2207g(w) = \u2211\n(i,j)\u2208\u2126\n`\u2032(Yij ,w T x\u0303ij)x\u0303ij + \u03bbw, (8)\n\u22072g(w) = \u2211\n(i,j)\u2208\u2126\n`\u2032\u2032(Yij ,w T x\u0303ij)x\u0303ijx\u0303 T ij + \u03bbI. (9)\nA direct computation of\u2207g(w) and\u22072g(w)s using (8) and (9) requires at least O(|\u2126|d\u0304k) time. Below we give faster procedures to perform both operations.\nAssuming that `\u2032(a, b) can be computed in constant time, which holds for most loss functions (e.g. squaredL2 loss, logistic loss), the gradient computation can be done in O ((nnz(X) + |\u2126|+ d)\u00d7 k) time. Algorithm 1 gives the details of computing\u2207g(w) using (10).\nHessian-vector Multiplication. After substituting x\u0303ij = hj \u2297 xi, we have \u22072g(w)s = \u2211\n(i,j)\u2208\u2126\n`\u2032\u2032ij \u00b7 ( (hjh T j )\u2297 (xixTi ) ) s+ \u03bbs,\nwhere `\u2032\u2032ij = ` \u2032\u2032(Yij ,w T x\u0303ij). Let S be the d \u00d7 k matrix such that s = vec (S). Using the identity (BT \u2297A)vec (X) = vec (AXB), we have ( (hjh T j )\u2297 (xixTi ) ) s = vec ( xix T i Shjh T j ) . Thus,\n\u2211 ij `\u2032\u2032ijxix T i Shjh T j = n\u2211 i=1 xi( \u2211 j:(i,j)\u2208\u2126 `\u2032\u2032ij \u00b7 (STxi)ThjhTj )\n= n\u2211 i=1 xi( \u2211 j:(i,j)\u2208\u2126 Uijh T j ) = X TUH,\nwhere U is sparse, and Uij = `\u2032\u2032ij \u00b7 (STxi)Thj , \u2200(i, j) \u2208 \u2126. Thus, we have \u22072g(w)s = vec ( XTUH ) + \u03bbs. (11)\nIn Algorithm 1, we describe a detailed procedure for computing the Hessian-vector multiplication in O ((nnz(X) + |\u2126|+ d)\u00d7 k) time using (11).\nLoss Functions. See Table 1 for expressions of `\u2032(a, b) and `\u2032\u2032(a, b) for three common loss functions: squared loss, logistic loss, and squared hinge loss1. Thus, to solve (7), we can apply CG for squared loss and the trust region Newton method (TRON) Lin et al. (2008) for the other two loss functions.\n1Note that although L2-hinge loss is not twice-differentiable, the sub-differential of \u2202\u2202b `(a, b) still can be used for TRON to solve (7)."}, {"heading": "3.2 Fast Operations for Squared Loss with Full Labels", "text": "For the situation where labels are fully observed, solving (1) efficiently in the large-scale setting remains a challenge. The closed form solution from (3) is not ideal for two reasons: firstly since it involves the SVD of both X and UTXY , the solution becomes infeasible when rank of X is large. Secondly, since it is an unregularized solution, it might overfit. Indeed CPLST has similar scalability and overfitting issues due to absence of regularization and requirement of pseudo inverse calculations for X . When Y is fully observed, Algorithm 1, which aims to handle missing labels with a general loss function, is also not scalable as |\u2126| = nL imposing a O (nLk + nnz(X)k) cost per operation which is prohibitive when n and L are large.\nAlthough, for a general loss, an O(nLk) cost seems to be inevitable, for the L2 loss, we propose fast procedures such that the cost of each operation only depends on nnz(Y ) instead of |\u2126|. In most realworld multi-label problems, nnz(Y ) nL = |\u2126|. As a result, for the squared loss, our technique allows alternating minimization to be performed efficiently even when |\u2126| = nL.\nIf the squared loss is used, the matrix D in Eq. (10) is D = XWHT \u2212 Y when Y is fully observed, where W is the d\u00d7 k matrix such that vec (W ) = w. Then, we have\n\u2207g(w) = vec ( XTXWHTH \u2212XTY H ) + \u03bbw. (12)\nSimilarly, U in Eq. (11) is U = XSHT which gives us\n\u22072g(w)s = vec ( XTXSHTH ) + \u03bbs. (13)\nWith a careful choice of the sequence of the matrix multiplications, we show detailed procedures in Algorithm 2, which use only O(nk + k2) extra space and O ( (nnz(Y ) + nnz(X)) k + (n+ L)k2 ) time to compute both\u2207g(w) and \u22072g(w)s efficiently.\nRemark on parallelization. As we can see, matrix multiplication acts as a crucial subroutine in both Algorithms 1 and 2. Thus, with a highly-optimized parallel BLAS library (such as ATLAS or Intel MKL), our algorithms can easily enjoy speedup brought by the parallel matrix operations provided in the library without any extra efforts. Figure 3 in Appendix D shows that both algorithms do indeed enjoy impressive speedups as the number of cores increases.\nRemark on kernel extension. Given a kernel function K(\u00b7, \u00b7), let f j \u2208 HK be the minimizer of the empirical loss defined in Eq. (2). Then by the Representer Theorem (for example, Scho\u0308lkopf et al., 2001), f j admits a representation of the form: f j(\u00b7; zj) = \u2211n t=1 zjtK(\u00b7,xt), where zj \u2208 Rn. Let the vector function k(x) : Rd \u2192 Rn for K be defined as k(x) = [\u00b7 \u00b7 \u00b7 ,K(x,xt), \u00b7 \u00b7 \u00b7 ]T . Then f(x;Z) can be written as f(x;Z) = ZTk(x), where Z is an n\u00d7L matrix with zj as the j-th column. Once again, we can impose the same trace norm regularization r(Z) and the low rank constraint in Eq. (5). As a result, Z = WHT and f j(xi, zj) = kT (xi)Whj . If K is the kernel Gram matrix for the training set {xi : i = 1, . . . , n} and Ki is its ith column, then the loss in (5) can be replaced by l(Yij ,KTi Whj). Thus, the proposed alternating minimization can be applied to solve Equations (1) and (2) with the kernel extension as well."}, {"heading": "4 Generalization Error Bounds", "text": "In this section we analyze excess risk bounds for our learning model with trace norm regularization. Our analysis demonstrates the superiority of our trace norm regularization-based technique over BR and Frobe-\nnius norm regularization. We require a more careful analysis for our setting since standard results do not apply because of the presence of missing labels.\nOur multi-label learning model is characterized by a distribution D on the space of data points and labels X \u00d7 {0, 1}L where X \u2286 Rd and a distribution that decides the pattern of missing labels. We receive n training points (x1,y1), . . . , (xn,yn) sampled i.i.d from the distribution D, where yi \u2208 {0, 1}L are the ground truth label vectors. However we shall only be able to observe the ground truth label vectors yi at s random locations. More specifically, for each i we only observe yi at locations l1i , . . . , l s i \u2208 [L] where the locations are chosen uniformly from the set [L] and the choices are independent of (xi,yi). Given this training data, we learn a predictor Z\u0302 by performing empirical risk minimization over a constrained set of predictors as follows:\nZ\u0302 = arg inf r(Z)\u2264\u03bb L\u0302(Z) = 1 n n\u2211 i=1 s\u2211 j=1 `(y lji i , f lji (xi;Z)),\nwhere L\u0302(Z) is the empirical risk of a predictor Z. Note that although the method in Equation 2 uses a regularized formulation that is rank-constrained, we analyze just the regularized version without the rank constraints for simplicity. As the class of rank-constrained matrices is smaller than the class of trace-norm constrained matrices, we can in fact expect better generalization performance than that indicated here, if the ERM problem can be solved exactly.\nOur goal would be to show that Z\u0302 has good generalization properties i.e. L(Z\u0302) \u2264 inf r(Z)\u2264\u03bb L(Z) + ,\nwhere L(Z) := E x,y,l\nq `(yl, f l(x;Z)) y is the population risk of a predictor.\nTheorem 3. Suppose we learn a predictor using the formulation Z\u0302 = arg inf \u2016Z\u2016tr\u2264\u03bb L\u0302(Z) over a set of n training points. Then with probability at least 1\u2212 \u03b4, we have\nL(Z\u0302) \u2264 arg inf \u2016Z\u2016tr\u2264\u03bb L(Z) +O\n( s\u03bb \u221a 1\nn\n) +O s \u221a\nlog 1\u03b4 n  , where we assume (w.l.o.g.) that E r \u2016x\u201622 z \u2264 1.\nWe refer to Appendix B for the proof. Interestingly, we can show that our analysis, obtained via uniform convergence bounds, is tight and cannot be improved in general. We refer the reader to Appendix C.1 for the tightness argument. However, it turns out that Frobenius norm regularization is also able to offer the same excess risk bounds and thus, this result does not reveal any advantage for trace norm regularization. Nevertheless, we can still get improved bounds for a general class of distributions over (x,y):\nTheorem 4. Let the data distribution satisfy the following conditions: 1) The top singular value of the covariance matrix X = E\nx\u223cD\nq xx> y is \u2016X\u20162 = \u03c31, 2) tr (X) = \u03a3 and 3) the distribution on X is sub-\nGaussian i.e. for some \u03b7 > 0, for all v \u2208 Rd, E q exp ( x>v )y \u2264 exp ( \u2016v\u201622 \u03b72/2 ) , then with probability at least 1\u2212 \u03b4, we have\nL(Z\u0302) \u2264 arg inf \u2016Z\u2016tr\u2264\u03bb L(Z) +O\ns\u03bb\u221ad(\u03b72 + \u03c31) nL\u03a3 + s \u221a log 1\u03b4 n  .\nIn particular, if the data points are generated from a unit normal distribution, then we have\nL(Z\u0302) \u2264 arg inf \u2016Z\u2016tr\u2264\u03bb L(Z) +O\n( s\u03bb \u221a 1\nnL\n) +O s \u221a\nlog 1\u03b4 n  . The proof of Theorem 4 can be found in Appendix B. Our proofs do not follow either from existing techniques for learning with matrix predictors (for instance Kakade et al. (2012)) or from results on matrix completion with trace norm regularization Shamir & Shalev-Shwartz (2011) due to the complex interplay of feature vectors and missing labels that we encounter in our learning model. Instead, our results utilize a novel form of Rademacher averages, bounding which requires tools from random matrix theory. We note that our results can even handle non-uniform sampling of labels (see Theorem 6 in Appendix B for details).\nWe note that the assumptions on the data distribution are trivially satisfied with finite \u03c31 and \u03b7 by any distribution with support over a compact set. However, for certain distributions, this allows us to give superior bounds for trace norm regularization. We note that Frobenius norm regularization can give no better than a ( \u03bb\u221a n ) style excess error bound even for such distributions (see Appendix C.2 for a proof), whereas\ntrace norm regularization allows us to get superior (\n\u03bb\u221a nL\n) style bounds. This is especially contrasting when,\nfor instance, \u03bb = O( \u221a L), in which case trace norm regularization gives O ( 1\u221a n ) excess error whereas the\nexcess error for Frobenius regularization deteriorates to O (\u221a\nL n\n) . Thus, trace norm seems better suited to\nexploit situations where the data distribution is isotropic. Intuitively, we expect such results due to the following reason: when labels are very sparsely observed, such as when s = O (1), we observe the value of each label onO (n/L) training points. In such a situation, Frobenius norm regularization with say \u03bb = \u221a L essentially allows an independent predictor zl \u2208 Rd to be learned for each label l \u2208 [L]. Since all these predictors are being trained on only O (n/L) training points, the performance accordingly suffers.\nOn the other hand, if we were to train a single predictor for all the labels i.e. Z = z1> for some z \u2208 Rd, such a predictor would be able to observe O(n) points and consequently have much better generalization properties. Note that this predictor also satisfies \u2016z1>\u2016tr \u2264 \u221a L. This seems to indicate that trace norm regularization can capture cross label dependencies, especially in the presence of missing labels, much better than Frobenius norm regularization.\nHaving said that, it is important to note that the two regularizations (trace norm vs. Frobenius norm) might induce different biases in the learning framework. It would be an interesting exercise to study the biasvariance trade-offs offered by these two regularization techniques. However, if one has label correlations then we expect both formulations to suffer similar biases."}, {"heading": "5 Experimental Results", "text": "We now present experimental results in order to assess our proposed algorithms in terms of accuracy and scalability. As we shall see, the results unambiguously demonstrate the superiority of our method over other approaches.\nDatasets. We consider a variety of benchmark datasets including four standard datasets (bibtex, delicious, eurlex, and nus-wide), two datasets with d L (autofood and compphys), and a very large scale\nWikipedia based dataset, which contains about 1M wikipages and 200K labels. See Table 2 for more information about the datasets. We conducted all experiments on an Intel machine with 32 cores.\nCompeting Methods. A list containing details of the competing methods (including ours) is given below. Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al. (2012). 1. LEML (Low rank Empirical risk minimization for Multi-Label Learning): our proposed method. We\nimplemented CG with Algorithms 1 and 2 for squared loss, and TRON Lin et al. (2008) with Algorithm 1 for logistic and squared hinge loss.\n2. CPLST: the method proposed in Chen & Lin (2012). We used the code provided by the author.\n3. BCS: the Bayesian compressed sensing method of Kapoor et al. (2012). We used code provided by the authors to test this method. 4. BR: Binary Relevance with various loss functions. 5. WSABIE: As there is no publicly available code, we implemented this method and hand-tuned the learn-\ning rate and the margin for each dataset as suggested by the authors of WSABIE (Weston, 2013).\nEvaluation Criteria. Three criteria will be used to compare the methods: top-K accuracy, which measures the performance on a few top predictions, Hamming loss, which considers the overall classification performance, and average AUC, which takes the overall ranking into account.\nGiven a test set {xi,yi : i = 1, . . . , n} and an real-valued predictor f(x) : Rd \u2192 R:\n\u2022 Top-K accuracy: for each instance, we select the K labels with the largest decision values for prediction. The average accuracy among all instances are reported as the top-K accuracy.\n\u2022 Hamming-loss: for each pair of instance x and label index j, we round the decision value f j(x) to 0 or 1.\nHamming Loss = 1\nnL n\u2211 i=1 L\u2211 j=1 I[round ( f j(x) ) 6= yj ]\n\u2022 Average AUC: we follow Bucak et al. (2009) to calculate area under ROC curve for each instance and report the average AUC among all test instances."}, {"heading": "5.1 Results with full labels", "text": "We divide datasets into two groups: small datasets (bibtex, autofood, compphys, and delicious) on which BCS and CPLST can be tested without scalability issues and large datasets (eurlex, nus-wide, and wiki) to which only LEML and WSABIE are able to scale.\nSmall datasets. We first compare dimension reduction based approaches to assess their performance with varying dimensionality reduction ratios. Figure 1 presents these results for LEML, CPLST and BCS on the squared L2 loss with BR included for reference. Clearly LEML consistently performs better than other methods for all ratios. Next we compare LEML with three loss functions (squared, logistic, and L2-hinge) to WSABIE, which approximately optimizes a weighted approximate ranking loss. As Table 3 shows, although the best loss for each dataset might vary, LEML is always superior to or competitive with WSABIE.\nBased on Figure 1, Table 3, and further results in Appendix D.2, we make the following observations. 1) LEML can deliver accuracies competitive with BR even with a severe reduction in dimensionality, 2) On bibtex and compphys, LEML is even shown to outperform BR. This is a benefit brought forward by the design of LEML, wherein the relation between labels can be captured by a low rank Z. This enables LEML to better utilize label information than BR and yield better accuracies. 3) On autofood and compphys, CPLST seems to suffer from overfitting and demonstrates a significant dip in performance. In contrast, LEML, which brings regularization into the formulation performs well consistently on all datasets.\nLarger data. Table 4 shows results for LEML and WSABIE on the three larger datasets. We implemented LEML with the squared L2 loss using Algorithm 2 for comparison in the full labels case. Note that Hamming loss is not used here as it is not clear how to convert the label ranking given by WSABIE to a 0/1 encoding. For LEML, we report the time and the accuracies obtained after five alternating iterations. For WSABIE, we ran the method on each dataset with the hand-tuned parameters for about two days, and reported the time and results for the epoch with the highest average AUC. On eurlex and nus-wide, LEML is clearly superior than WSABIE on all evaluation criteria. On wiki, although both methods share a similar performance for k = 250, on increasing k to 500, LEML again outperforms WSABIE. Also clearly noticeable is the stark difference in the running times of the two methods. Whereas LEML takes less than 6 hours to deliver 0.9374 AUC on wiki, WSABIE requires about 1.6 days to achieve 0.9058 AUC. More specifically, WSABIE takes about 7,000s for the first epoch, 16,000s for the second and 36,000s for the third epoch which result in it spending almost two days on just 5 epochs. Although this phenomenon is expected due to the sampling scheme in WSABIE Weston et al. (2010), it becomes more serious as L increases. We leave the issue of designing a better sampling scheme with large L for future work. Figure 2a further illustrates this gap in training times for the nus-wide dataset. All in all, the results clearly demonstrate the scalability and efficiency of LEML."}, {"heading": "5.2 Results with missing labels", "text": "For experiments with missing labels, we compare LEML, BCS, and BR. We implemented BR with missing labels by learning an L2-regularized binary classifier/regressor for each label on observed instances. Thus, the model derived from BR corresponds to the minimizer of (2) with Frobenius norm regularization. Table 5 shows the results when 20% entries were revealed (i.e. 80% missing rate) and squared loss function was used for training. We used k = 0.4L for both LEML and BCS. The results clearly show that LEML outperforms BCS and LEML with respect to all three evaluation criteria. On bibtex, we further present results for various rates of observed labels in Figure 2b and results for various dimension reduction ratios in Figure 2c. LEML clearly shows superior performance over other approaches, which corroborates the theoretical results of Section 4 that indicate better generalization performance for low-rank promoting regularizations. More empirical results for other loss functions, various observed ratios and dimension reduction ratios can\nbe found in Appendix D.3."}, {"heading": "6 Conclusion", "text": "In this paper we studied the multi-label learning problem with missing labels in the standard ERM framework. We modeled our framework with rank constraints and regularizers to increase scalability and efficiency. To solve the obtained non-convex problem, we proposed an alternating minimization based method that critically exploits structure in the loss function to make our method scalable. We showed that our learning framework admits excess risk bounds that indicate better generalization performance for our methods than the existing methods like BR, something which our experiments also confirmed. Our experiments additionally demonstrated that our techniques are much more efficient than other large scale multi-label classifiers and give superior performance than the existing label compression based approaches. For future work, we would like to extend LEML to other (non decomposable) loss functions such as ranking losses and study conditions under which alternating minimization for our problem is guaranteed to converge to the global optimum. Another open question is if our risk bounds can be improved by avoiding the uniform\nconvergence route that we use in the paper."}, {"heading": "A Algorithm Details", "text": ""}, {"heading": "A.1 Conjugate Gradient for Squared Loss", "text": "In Algorithm 3, we show the detailed conjugate gradient procedure used to solve (7) when the squared loss is used. Note that\u22072g(w) is invariant to w as (7) is a quadratic problem due to the squared loss function.\nAlgorithm 3 Conjugate gradient for solving (7) with the squared loss \u2022 Set initial w0, r0 = \u2212\u2207g(w0), d0 = r0. \u2022 For t = 0, 1, 2, . . .\n\u2013 If \u2016rt\u2016 is small enough, then stop the procedure and return wt. \u2013 \u03b1t = rTt rt dTt \u22072g(w0)dt \u2013 wt+1 = wt + \u03b1tdt \u2013 rt+1 = rt \u2212 \u03b1t\u22072g(w0)dt \u2013 \u03b2t = rTt+1rt+1 rTt rt \u2013 dt+1 = rt+1 + \u03b2tdt"}, {"heading": "B Analyzing Trace Norm-bounded Predictors", "text": "In this section, we shall provide a proof of Theorems 3 and 4. Our proof shall proceed by demonstrating a uniform convergence style bound for the empirical losses. More precisely, we shall show, for both trace norm as well as Frobenius regularizations, that with high probability, we have\nL(Z\u0302) \u2264 L\u0302(Z\u0302) + .\nSuppose Z\u2217 \u2208 arg min r(Z)\u2264\u03bb L(Z), then a similar analysis will allow us to show, again with high probability,\nL\u0302(Z\u2217) \u2264 L(Z\u2217) + .\nCombining the two along with the fact that Z\u0302 is the empirical risk minimizer i.e. L\u0302(Z\u0302) \u2264 L\u0302(Z\u2217) will yield the announced claim in the following form:\nL(Z\u0302) \u2264 L(Z\u2217) + 2 .\nThus, in the sequel, we shall only concentrate on proving the aforementioned uniform convergence bound. We shall denote the regularized class of predictors as Z = { Z \u2208 Rd\u00d7L, r(Z) \u2264 \u03bb } , where r(Z) = \u2016Z\u2016tr or r(Z) = \u2016Z\u2016F . We shall also use the following shorthand for the loss incurred by the predictor on a specific label l \u2208 [L]: `(yli, Zl,x) := `(yli, f l(x;Z)), where Zl denotes the lth column of the matrix Z.\nWe shall perform our analysis in several steps outlined below:\n1. Step 1: In this step we shall show, by an application of McDiarmid\u2019s inequality, that with high probability, the excess risk of the learned predictor can be bounded by bounding the expected supre\u0304mus deviation of empirical risks from population risks over the set of predictors in the class Z .\n2. Step 2: In this step we shall show that the expected supre\u0304mus deviation can be bounded by a Rademacher average term.\n3. Step 3: In this step we shall reduce the estimation of the Rademacher average term to the estimation of the spectral norm of a random matrix that we shall describe.\n4. Step 4: Finally, we shall use tools from random matrix theory to bound the spectral norm of the random matrix.\nWe now give details of each of the steps in the following subsections:"}, {"heading": "B.1 Step 1: Bounding Excess Risk by Expected Supre\u0304mus Deviation", "text": "We will first analyze the case s = 1 and will later show how to extend the analysis to s > 1. In this case, we receive n training points (xi,yi) and for each training point xi, we get to see the value of a random label li \u2208 [L] i.e. we get to see the true value of ylii . Thus, for any predictor Z \u2208 Z , the observed training loss is given by\nL\u0302(Z) = 1 n n\u2211 i=1 `(ylii , Zli ,xi).\nThe population risk functional is given by\nL(Z) = E (x,y,l)\nr `l(y l, f l(x;Z)) z\n= E (x,y,l)\nr `l(y l, Zl,x) z\nWe note here that our subsequent analysis shall hold even for non uniform distributions for sampling the labels. The definition of the population risk functional incorporates this. In case we have a uniform distribution over the labels, the above definition reduces to\nL(Z) = E (x,y,l)\nr `l(y l, Zl,x) z\n= E (x\u0303i,y\u0303i,l\u0303i)\nt 1\nn n\u2211 i=1 `(y\u0303 l\u0303ii , Zl\u0303i , x\u0303i) |\nGiven the above, we now analyze the excess risk i.e. the difference between the observed training loss L\u0302(Z\u0302) and the population risk L(Z\u0302).\nL(Z\u0302)\u2212 L\u0302(Z\u0302) \u2264 sup Z\u2208Z\n{ L(Z)\u2212 L\u0302(Z) } = sup\nZ\u2208Z\n{ E\n(x\u0303i,y\u0303i,l\u0303i)\nt 1\nn n\u2211 i=1 `(y\u0303 l\u0303ii , Zl\u0303i , x\u0303i) | \u2212 1 n n\u2211 i=1 `(ylii , Zli ,xi) } \ufe38 \ufe37\ufe37 \ufe38\ng((x1,y1,l1),...,(xn,yn,ln))\nSince all the label-wise loss functions are bounded, an arbitrary change in any (xi,yi) or any li should not perturb the expression g((x1,y1, l1), . . . , (xn,yn, ln)) by more than O ( 1 n ) . Thus, by an application of McDiarmid\u2019s inequality, we have, with probability at least 1\u2212 \u03b4,\nL(Z\u0302)\u2212 L\u0302(Z\u0302) \u2264 E (xi,yi),li Jg((x1,y1, l1), . . . , (xn,yn, ln))K +O \u221a log 1\u03b4 n  Thus, we conclude that the excess risk of the learned predictor can be bounded by calculating the expected supre\u0304mus deviation of empirical risks from population risks."}, {"heading": "B.2 Step 2: Bounding Expected Supre\u0304mus Deviation by a Rademacher Average", "text": "We now analyze the expected supre\u0304mus deviation. We have\nE (xi,yi),li Jg((x1,y1, l1), . . . , (xn,yn, ln))K\n= E (xi,yi),li t sup Z\u2208Z\n{ E\n(x\u0303i,y\u0303i,l\u0303i)\nt 1\nn n\u2211 i=1 `(y\u0303 l\u0303ii , Zl\u0303i , x\u0303i) | \u2212 1 n n\u2211 i=1 `(ylii , Zli ,xi)\n}|\n\u2264 E (xi,yi),li t sup Z\u2208Z\n{ E\n(x\u0303i,y\u0303i,l\u0303i)\nt 1\nn n\u2211 i=1 `(y\u0303 l\u0303ii , Zl\u0303i , x\u0303i) | \u2212 1 n n\u2211 i=1 E (x\u0303i,y\u0303i) r `(y\u0303lii , Zli , x\u0303i)\nz}|\n+ E (xi,yi),li t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 E (x\u0303i,y\u0303i) r `(y\u0303lii , Zli , x\u0303i) z \u2212 1 n n\u2211 i=1 `(ylii , Zli ,xi)\n}|\n= E li t sup Z\u2208Z\n{ E\n(x\u0303i,y\u0303i,l\u0303i)\nt 1\nn n\u2211 i=1 `(y\u0303 l\u0303ii , Zl\u0303i , x\u0303i) | \u2212 1 n n\u2211 i=1 E (x\u0303i,y\u0303i) r `(y\u0303lii , Zli , x\u0303i)\nz}|\n+ E (xi,yi),li t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 E (x\u0303i,y\u0303i) r `(y\u0303lii , Zli , x\u0303i) z \u2212 1 n n\u2211 i=1 `(ylii , Zli ,xi)\n}|\n\u2264 E (li,l\u0303i) t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 E (x\u0303i,y\u0303i) r `(y\u0303 l\u0303ii , Zl\u0303i , x\u0303i) z \u2212 1 n n\u2211 i=1 E (x\u0303i,y\u0303i) r `(y\u0303lii , Zli , x\u0303i)\nz}|\n+ E (xi,yi),li,(x\u0303i,y\u0303i) t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 `(y\u0303lii , Zli , x\u0303i)\u2212 1 n n\u2211 i=1 `(ylii , Zli ,xi)\n}|\n= E (li,l\u0303i), i t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 i ( E (x\u0303i,y\u0303i) r `(y\u0303 l\u0303ii , Zl\u0303i , x\u0303i) z \u2212 E (x\u0303i,y\u0303i) r `(y\u0303lii , Zli , x\u0303i) z)}|\n+ E (xi,yi),li,(x\u0303i,y\u0303i), i t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 i ( `(y\u0303lii , Zli , x\u0303i)\u2212 `(y li i , Zli ,xi)\n)}|\n\u2264 2 E li, i t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 i E (x\u0303i,y\u0303i) r `(y\u0303lii , Zli , x\u0303i)\nz}| + 2 E\n(xi,yi),li, i t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 i`(y li i , Zli ,xi)\n}|\n\u2264 2 E (x\u0303i,y\u0303i),li, i t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 i`(y\u0303 li i , Zli , x\u0303i)\n}| + 2 E\n(xi,yi),li, i t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 i`(y li i , Zli ,xi)\n}|\n\u2264 4 E (xi,yi),li, i t sup Z\u2208Z\n{ 1\nn n\u2211 i=1 i`(y li i , Zli ,xi)\n}| \u2264 4C\nn E\n(xi,yi),li, i t sup Z\u2208Z { n\u2211 i=1 i\u3008Zli ,xi\u3009 }|\n= 4C\nn E X,l, s sup Z\u2208Z \u3008Z,X l \u3009 { ,\nwhere for any x1, . . . ,xn \u2208 X , l \u2208 [L]n and \u2208 {\u22121,+1}n, we define the matrix X l as follows:\nX l := \u2211 i\u2208I1 ixi \u2211 i\u2208I2 ixi . . . \u2211 i\u2208IL ixi \nwhere for any l \u2208 [L], we define Il := {i : li = l}. Note that in the last second inequality we have used the contraction inequality for Rademacher averages (see Ledoux & Talagrand, 2002, proof of Theorem 4.12) We also note that the above analysis also allows for separate label-wise loss functions, so long as they are all bounded and C-Lipschitz. For any matrix predictor class Z , we define its Rademacher complexity as follows:\nRn (Z) := 1\nn E X,l, s sup Z\u2208Z \u3008Z,X l \u3009 {\nWe have thus established that with high probability,\nL(Z\u0302)\u2212 L\u0302(Z\u0302) \u2264 4CRn (Z) +O \u221a log 1\u03b4 n  . We now establish that the same analysis also extends to situations wherein, for each training point we observe values of s labels instead. Thus, for each xi, we observe values for labels l1i , . . . , l s i . In this case the empirical loss is given by\nL\u0302(Z) = 1 n n\u2211 i=1 s\u2211 j=1 `(y lji i , Zlji ,xi)\nThe change in any xi leads to a perturbation of at most O ( s n ) whereas the change in any lji leads to a\nperturbation of O (\n1 n\n) . Thus the sum of squared perturbations is bounded by 2s 2\nn . Thus on application of the McDiarmid\u2019s inequality, we will be able to bound the excess risk by the following expected supre\u0304mus deviation term\nE (xi,yi,l j i ) u vsup Z\u2208Z s E(x,y,l) r`l(yl, Zl,x)z\u2212 1n n\u2211 i=1 s\u2211 j=1 `(y lji i , Zlji ,xi)  } ~\nplus a quantity that behaves like O ( s \u221a log 1\n\u03b4 n\n) . We analyze the expected supre\u0304mus deviation term below:\nE (xi,yi,l j i ) u vsup Z\u2208Z s E(x,y,l) r`l(yl, Zl,x)z\u2212 1n n\u2211 i=1 s\u2211 j=1 `(y lji i , Zlji ,xi)  } ~\n= E (xi,yi,l j i ) u vsup Z\u2208Z  s\u2211 j=1 ( E (x,y,l) r `l(y l, Zl,x) z \u2212 1 n n\u2211 i=1 `(y lji i , Zlji ,xi) ) } ~ \u2264 s\u2211 j=1 E (xi,yi,l j i ) t sup Z\u2208Z { E (x,y,l) r `l(y l, Zl,x) z \u2212 1 n n\u2211 i=1 `(y lji i , Zlji ,xi) }|\n\u2264 s\u2211 j=1 4C n E X,lj , s sup Z\u2208Z \u3008Z,X lj \u3009 { = 4Cs n E X,l, s sup Z\u2208Z \u3008Z,X l \u3009 { = 4CsRn (Z)\nand thus, it just suffices to prove bounds for the case where a single label is observed per point."}, {"heading": "B.3 Step 3: Estimating the Rademacher Average", "text": "We will now bound the following quantity:\nRn(Z) = 1\nn E X,l, s sup Z\u2208Z \u3008Z,X l \u3009 {\nwhere X l is as defined above. Approaches to bounding such Rademacher average terms usually resort to Martingale techniques Kakade et al. (2008) or use of tools from convex analysis Kakade et al. (2012) and decompose the Rademacher average term. However, such decompositions shall yield suboptimal results in our case. Our proposed approach will, instead involve an application of Ho\u0308lder\u2019s inequality followed by an application from results from random matrix theory to bound the spectral norm of a random matrix.\nFor simplicity of notation, for any l \u2208 [L], we denote Vl = \u2211\ni\u2208Il ixi and V := X l = [V1 V2 . . . VL].\nAlso, for any l \u2208 [L], let nl = |Il| denote the number of training points for which values of the lth label was observed i.e. nl = \u2211n i=1 1li=l."}, {"heading": "B.3.1 Distribution Independent Bound", "text": "We apply Ho\u0308lder\u2019s inequality to get the following result:\n1 n E X,l, s sup Z\u2208Z \u3008Z,X l \u3009 { \u2264 1 n E X,l, s sup Z\u2208Z \u2016Z\u2016tr \u2225\u2225\u2225X l \u2225\u2225\u2225 F { \u2264 1 n E X,l, r \u03bb \u2225\u2225\u2225X l \u2225\u2225\u2225 2 z \u2264 \u03bb n \u221a E X,l, r \u2016X l \u2016 2 2 z\nThen the following bound can be derived in a straightforward manner:\nE X,l, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 { \u2264 E X,l, s\u2225\u2225\u2225X l \u2225\u2225\u22252 F { = E X,l, t L\u2211 l=1 \u2016Vl\u201622 | = E X,l, u v L\u2211 l=1 \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208Il ixi \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n} ~\n= E X,l,\nu v L\u2211 l=1 \u2211 i\u2208Il \u2016xi\u201622 + \u2211 i 6=j\u2208Il i j\u3008xi,xj\u3009 } ~\n\u2264 E l t L\u2211 l=1 nlE r \u2016x\u201622 z| \u2264 E l t L\u2211 l=1 nl | = n\nwhere we have assumed, without loss of generality that E x\u223cD\nr \u2016x\u201622 z \u2264 1. This proves\nRn(Z) \u2264 \u03bb\u221a n ,\nwhich establishes Theorem 3. Note that the same analysis holds if Z is Frobenius norm regularized since we can apply the Ho\u0308lder\u2019s inequality for Frobenius norm instead and still get the same Rademacher average bound."}, {"heading": "B.3.2 Tighter Bounds for Trace Norm Regularization", "text": "Notice that in the above analysis, we did not exploit the fact that the top singular value of the matrixX l could be much smaller than its Frobenius norm. However, there exist distributions where trace norm regularization\nenjoys better performance guarantees over Frobenius norm regularization. In order to better present our bounds, we model the data distribution D on X (or rather its marginal) more carefully. Let X := E q xx> y and suppose the distribution D satisfies the following conditions:\n1. The top singular value of X is \u2016X\u20162 = \u03c31\n2. The matrix X has trace tr (X) = \u03a3\n3. The distribution on X is sub-Gaussian i.e. for some \u03b7 > 0, we have, for all v \u2208 Rd,\nE r exp ( x>v )z \u2264 exp ( \u2016v\u201622 \u03b7 2/2 )\nIn order to be consistent with previous results, we shall normalize the vectors x so that they are unit-norm on expectation. Since E r \u2016x\u201622 z = tr (X) = \u03a3, we wish to bound the Rademacher average as\nRn (Z) \u2264 1\nn \u221a \u03a3 E X,l, s sup Z\u2208Z \u3008Z,X l \u3009 {\nIn this case, it is possible to apply the Ho\u0308lder\u2019s inequality as\n1 n \u221a \u03a3 E X,l, s sup Z\u2208Z \u3008Z,X l \u3009 { \u2264 1 n \u221a \u03a3 E X,l, s sup Z\u2208Z \u2016Z\u2016tr \u2225\u2225\u2225X l \u2225\u2225\u2225 2 { \u2264 1 n \u221a \u03a3 E X,l, r \u03bb \u2225\u2225\u2225X l \u2225\u2225\u2225 2 z \u2264 \u03bb n \u221a \u03a3 \u221a E X,l, r \u2016X l \u2016 2 2 z\nThus, in order to boundRn(Z), it suffices to bound E X,l, r\u2225\u2225X l \u2225\u222522z. In this case, since our object of interest is the spectral norm of the matrix X l , we expect to get much better guarantees, for instance, in case the training points x \u2208 X are being sampled from some (near) isotropic distribution. We note that Frobenius norm regularization will not be able to gain any advantage in these situations since it would involve the Frobenius norm of the matrix X l (as shown in the previous subsubsection) and thus, cannot exploit the fact that the spectral norm of this matrix is much smaller than its Frobenius norm."}, {"heading": "B.4 Step 4: Calculating the Spectral norm of a Random Matrix", "text": "To bound E X,l, r\u2225\u2225X l \u2225\u222522z, we first make some simplifications (we will take care of the normalizations later). For any l \u2208 [L], let the probability of the value for label l being observed be pl \u2208 (0, 1] such that \u2211 l pl = 1. Also let P = max l\u2208[L] pl and p = min l\u2208[L] pt. Call the event Emax as the event when nl \u2264 2P \u00b7 n for all l \u2208 [L] i.e. every label will have at most 2P \u00b7 n training points for which its value is seen. The following result shows that this is a high probability event:\nLemma 1. For any \u03b4 > 0, if n \u2265 1 2p2 log L\u03b4 , then with probability 1\u2212 \u03b4, we have\nP [Emax] \u2265 1\u2212 \u03b4\nProof. For any l \u2208 [L], an application of Chernoff\u2019s bound for Boolean random variables tells us that with probability at least 1\u2212exp ( \u22122np2l ) , we have nl \u2264 2pl \u00b7n \u2264 2P \u00b7n. Taking a union bound and using pl \u2265 p finishes the proof.\nConditioning on the event Emax shall allow us to get a control over the spectral norm of the matrix X l by getting a bound on the sub-Gaussian norm of the individual columns of X l . We show below, that conditioning on this event does not affect the Rademacher average calculations. A simple calculation shows that E\nX, r\u2225\u2225X l \u2225\u222522\u2223\u2223\u2223 lz \u2264 n\u03a3. If we have n > 12p2 log L\u03a3Pd(\u03b72+\u03c31) , we have P [\u00acEmax] < Pd(\u03b72+\u03c31)\u03a3 . This gives us the following bound:\nE X,l, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 { = E X, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 \u2223\u2223\u2223\u2223 Emax{P [Emin] + EX, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 \u2223\u2223\u2223\u2223\u00acEmin{ (1\u2212 P [Emax]) = E\nX, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 \u2223\u2223\u2223\u2223 Emax{ (1\u2212 \u03b4) + EX, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 \u2223\u2223\u2223\u2223\u00acEmax{ \u03b4 \u2264 E\nX, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 \u2223\u2223\u2223\u2223 Emax{ + n\u03a3(Pd(\u03b72 + \u03c31)\u03a3 )\n\u2264 O (\nE X, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 \u2223\u2223\u2223\u2223 Emax{) where the last step follows since our subsequent calculations will show that E\nX, r\u2225\u2225X l \u2225\u222522\u2223\u2223\u2223 Emaxz = O (nPd(\u03b72 + \u03c31)). Thus, it suffices to bound E\nX, r\u2225\u2225X l \u2225\u222522\u2223\u2223\u2223 Emaxz = EX, r\u2016V \u201622\u2223\u2223\u2223 Emaxz. For sake of brevity we will omit the conditioning term from now on.\nFor simplicity let Al = Vlc where c = \u03b7 \u00b7 \u221a 2P \u00b7 n and A = [A1A2 . . . AL]. Thus\nE X,l, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 { = c2 \u00b7 E X,l, r \u2016A\u201622 z\nWe first bound the sub-Gaussian norm of the column vectors Al. For any vector v \u2208 Rd, we have:\nE r exp ( A>l v )z = E u vexp 1 c \u2211 i\u2208Il i\u3008xi,v\u3009 }~ = ( E s exp ( \u3008x, 1 c v\u3009 ){)nl\n\u2264 ( exp (\u2225\u2225\u2225\u22251c v \u2225\u2225\u2225\u22252\n2\n\u03b72/2 ))nl = exp ( nl\n2\u03b72P \u00b7 n \u2016v\u201622 \u03b7 2/2 ) \u2264 exp ( \u2016v\u201622 /2\n) where, in the second step, we have used the fact that xi,xj and i, j are independent for i 6= j, in the third step we have used the sub-Gaussian properties of x and in the fourth step, we have use the fact that the event Emax holds. This shows us that the sub-Gaussian norm of the column vector Al is bounded i.e. \u2016Al\u2016\u03c82 \u2264 1.\nWe now proceed to bound E X,\nr \u2016A\u201622 z = E\nX,\nr\u2225\u2225A>\u2225\u22252 2 z . Our proof proceeds by an application of\na Bernstein-type inequality followed by a covering number argument and finishing off by bounding the\nexpectation in terms of the cumulative distribution function. The first two parts of the proof proceed on the lines of the proof of Theorem 5.39 in Vershynin (2012) For any fixed vector v \u2208 Sd\u22121, the set of unit norm vectors in d dimensions, we have:\n\u2016Av\u201622 = L\u2211 l=1 \u3008Al,v\u30092 =: L\u2211 l=1 Z2l\nNow observe that conditioned on l, It \u2229 It\u2032 = \u03d5 if t 6= t\u2032 and thus, conditioned on l, the variables Zt, Zt\u2032 are independent for t 6= t\u2032. This will allow us to apply the following Bernstein-type inequality\nTheorem 5 (Vershynin (2012), Corollary 5.17). Let X1, . . . , XN be independent centered sub-exponential variables with bounded sub-exponential norm i.e. for all i, we have \u2016Xi\u2016\u03c81 \u2264 B for some B > 0. Then for some absolute constant c1 > 0, we have for any > 0,\nP [ N\u2211 i=1 Xi \u2265 N ] \u2264 exp ( \u2212c1 min { 2 B2 , B } N ) .\nTo apply the above result, we will first bound expectation of the random variables Z2l .\nE q Z2l y = E q \u3008Al,v\u30092 y = E u v 1 c \u2211 i\u2208Il i\u3008xi,v\u3009 2}~ = nl c2 E q \u3008x,v\u30092 y \u2264 nl\u03c31 c2 \u2264 \u03c31 \u03b72\nwhere the fourth inequality follows from definition of the top singular norm \u03c31 of X := E q xx> y and the last inequality follows from the event Emax. The above calculation gives us a bound on the expectation of Z2l which will be used to center it. Since we have already established \u2016Al\u2016\u03c82 \u2264 1, we automatically get \u2016Zl\u2016\u03c82 \u2264 1. Using standard inequalities between the sub-exponential norm \u2016\u00b7\u2016\u03c81 and the sub-Gaussian norm \u2016\u00b7\u2016\u03c82 of random variables (for instance, see Vershynin, 2012, Lemma 5.14) we also have\u2225\u2225Z2l \u2212 E qZ2l y\u2225\u2225\u03c81 \u2264 2 \u2225\u2225Z2l \u2225\u2225\u03c81 \u2264 4 \u2016Zl\u20162\u03c82 \u2264 4. Applying Theorem 5 to the variables Xl = Z2l \u2212 E q Z2l y , we get\nP [ L\u2211 l=1 Z2l \u2212 L \u03c31 \u03b72 \u2265 L ] \u2264 exp ( \u2212c1Lmin { 2, }) where c1 > 0 is an absolute constant. Thus with probability at least 1\u2212 exp ( \u2212c1Lmin { 2, }) , for a fixed vector v \u2208 Sd\u22121, we have the inequality\n\u2016Av\u201622 \u2264 ( \u03c31 \u03b72 + ) L\nApplying a union bound over a 14 -net N1/4 over S d\u22121 (which can be of size at most 9d), we get that with probability at most 1\u2212 9d exp ( \u2212c1Lmin { 2, }) , we have the above inequality for every vector v \u2208 N1/4 as well. We note that this implies a bound on the spectral norm of the matrix A (see Vershynin, 2012, Lemma 5.4) and get the following bound\n\u2016A\u201622 \u2264 2 ( \u03c31 \u03b72 + ) L\nPut = c2 \u00b7 dL + \u2032 L where c2 = max { 1, ln 9c1 } and suppose d \u2265 L. Since c2 \u2265 1, we have \u2265 1 which gives\nmin { , 2 } = . This gives us with probability at least 1\u2212 exp (\u2212c1 \u2032),\n\u2016A\u201622 \u2264 2 ( L \u03c31 \u03b72 + c2d+ \u2032 )\nConsider the random variable Y = \u2016A\u2016 2 2\n2 \u2212 L \u03c31 \u03b72 \u2212 c2d. Then we have P [Y > ] \u2264 exp (\u2212c1 ). Thus we\nhave E JY K = \u222b \u221e 0 P [Y > ] d \u2264 \u222b \u221e 0 exp (\u2212c1 ) d = 1\nc1 This gives us\nE r \u2016A\u201622 z \u2264 2 ( L \u03c31 \u03b72 + c2d+ 1\nc1 ) and consequently,\nE X,l, s\u2225\u2225\u2225X l \u2225\u2225\u22252 2 { = c2\u00b7 E X,l, r \u2016A\u201622 z \u2264 4\u03b72P \u00b7n ( L \u03c31 \u03b72 + c2d+ 1\nc1\n) \u2264 O ( n\u03b72P ( d+ L\n\u03c31 \u03b72\n)) \u2264 O ( nPd(\u03b72 + \u03c31) ) where the last step holds when d \u2265 L. Thus, we are able to bound the Rademacher averages, for some absolute constant c3 as\nRn (Z) \u2264 \u03bb\nn \u221a \u03a3 \u221a\u2225\u2225\u2225\u2225 EX,l, r\u2016X l \u201622z \u2225\u2225\u2225\u2225 \u2264 c3\u03bb \u221a Pd(\u03b72 + \u03c31) n\u03a3 ,\nwhich allows us to make the following claim:\nTheorem 6. Suppose we learn a predictor using the trace norm regularized formulation Z\u0302 = arg inf \u2016Z\u2016tr\u2264\u03bb L\u0302(Z) over a set of n training points. Further suppose that, for any l \u2208 [L], the probability of observing the value of label l is given by pl and let P = max\nl\u2208[L] pl. Then with probability at least 1\u2212 \u03b4, we have\nL(Z\u0302) \u2264 arg inf \u2016Z\u2016tr\u2264\u03bb L(Z) +O\n( s\u03bb \u221a dP (\u03b72 + \u03c31)\nn\u03a3\n) +O s \u221a\nlog 1\u03b4 n  , where the terms \u03b7, \u03c31,\u03a3 are defined by the data distribution as before.\nEssentially, the above result indicates that if some label is observed too often, as would be the case when P = \u2126 (1), we get no benefit from trace norm regularization since this is akin to a situation with fully observed labels. However, if the distribution on the labels is close to uniform i.e. P = O ( 1 L ) , the above calculation lets us bound the Rademacher average, and consequently, the excess risk as\nRn (Z) \u2264 c3\u03bb \u221a d(\u03b72 + \u03c31)\nnL\u03a3 ,\nthus proving the first part of Theorem 4. We now notice that However, in case our data distribution is near isotropic, i.e. \u03a3 \u03c31, then this result gives us superior bounds. For instance, if the data points are generated from a standard normal distribution, then we have \u03c31 = 1, \u03a3 = d and \u03b7 = 1 using which we can bound the Rademacher average term as\nRn (Z) \u2264 c3\u03bb \u221a 2\nnL ,\nwhich gives us the second part of Theorem 4."}, {"heading": "C Lower Bounds for Uniform Convergence-based Proofs", "text": "In this section, we show that our analysis for Theorems 3 and 4 are essentially tight. In particular, we show for each case, a data distribution such that the deviation of the empirical losses from the population risks is, up to a constant factor, the same as predicted by the results. We state these lower bounds in two separate subsections below:"}, {"heading": "C.1 Lower Bound for Trace Norm Regularization", "text": "In this section we shall show that for general distribution, Theorem 3 is tight. Recall that Theorem 3 predicts that for a predictor Z\u0302 learned using a trace norm regularized formulation satisfies, with constant probability (i.e. \u03b4 = \u2126 (1)),\nL(Z\u0302) \u2264 L\u0302(Z\u0302) +O ( \u03bb \u221a 1\nn\n) ,\nwhere, for simplicity as well as w.l.o.g., we have assumed s = 1. We shall show that this result is tight by demonstrating the following lower bound:\nClaim 7. There exists a data-label distribution and a loss function such that the empirical risk minimizer learned as Z\u0302 = arg inf\n\u2016Z\u2016tr\u2264\u03bb L\u0302(Z) has, with constant probability, its population risk lower bounded by\nL(Z\u0302) \u2265 L\u0302(Z\u0302) + \u2126 ( \u03bb \u221a 1\nn\n) ,\nthus establishing the tightness claim. Our proof will essentially demonstrate this by considering a nonisotropic data distribution (since, for isotropic distributions, Theorem 4 shows that a tighter upper bound is actually possible). For simplicity, and w.l.o.g., we will prove the result for \u03bb = 1. Let \u00b5 \u2208 Rd be a fixed unit vector and consider the following data distribution\nxi = \u03b6i\u00b5,\nwhere \u03b6i are independent Rademacher variables and a trivial label distribution\nyi = 1,\nwhere 1 \u2208 RL is the all-ones vector. Note that the data distribution satisfies E r \u2016x\u201622 z = 1 and thus, satisfies the assumptions of Theorem 3. Let \u03c9li = 1 iff the label l is observed for the i th training point. Note\nthat for any i, we have \u2211L\nl=1 \u03c9 l i = 1 and that for any l \u2208 [L], \u03c9li = 1 with probability 1/L. Also consider\nthe following loss function `(yl, f l(x;Z)) = \u3008Zl,ylx\u3009\nLet Z\u0302 = arg inf\n\u2016Z\u2016tr\u22641 L\u0302(Z) = arg inf \u2016Z\u2016tr\u22641\n1 n \u3008Z,\u00b5v>\u3009\nwhere v is the vector\nv = [ n\u2211 i=1 \u03b6i\u03c9 1 i n\u2211 i=1 \u03b6i\u03c9 2 i . . . n\u2211 i=1 \u03b6i\u03c9 L i ]\nClearly, since x is a centered distribution and ` is a linear loss function, L(Z\u0302) = 0. However, by Ho\u0308lder\u2019s inequality, we also have\nZ\u0302 = \u2212\u00b5v >\n\u2016v\u20162 ,\nand thus, L\u0302(Z\u0302) = \u2212 1n \u2016v\u20162 since \u2016\u00b5\u20162 = 1. The following lemma shows that with constant probability, \u2016v\u20162 \u2265 \u221a n/2 which shows that L(Z\u0302) \u2265 L\u0302(Z\u0302) + \u2126 (\u221a 1 n ) , thus proving the lower bound.\nLemma 2. With probability at least 3/4, we have \u2016v\u201622 \u2265 n/2. Proof. We have\n\u2016v\u201622 = L\u2211 l=1 ( n\u2211 i=1 \u03b6i\u03c9 l i )2 = L\u2211 l=1 n\u2211 i=1 \u03c9li + L\u2211 l=1 \u2211 i 6=j \u03b6i\u03c9 l i\u03b6j\u03c9 l j\n= n+ \u2211 i 6=j \u03b6i\u03b6j\u3008\u03c9i,\u03c9j\u3009 = n+W,\nwhere \u03c9i = [\u03c91i , \u03c9 2 i , . . . , \u03c9 L i ]. Now clearly E JW K = 0 and as the following calculation shows, E\nq W 2 y \u2264\n2n2/L which, by an application of Tchebysheff\u2019s inequality, gives us, for L > 32, with probability at least 3/4, |W | \u2264 n/2 and consequently \u2016v\u201622 \u2265 n/2. We give an estimation of the variance of Z below.\nE q W 2 y = E u v \u2211 i1 6=j1,i2 6=j2 \u03b6i1\u03b6j1\u3008\u03c9i1 ,\u03c9j1\u3009\u03b6i2\u03b6j2\u3008\u03c9i2 ,\u03c9j2\u3009 } ~\n= 2E u v\u2211 i 6=j \u3008\u03c9i,\u03c9j\u30092 } ~ = 2n(n\u2212 1)E J\u3008\u03c91,\u03c92\u3009K \u2264 2n2 L ,\nwhere we have used the fact that \u3008\u03c9i,\u03c9j\u30092 = \u3008\u03c9i,\u03c9j\u3009 since \u3008\u03c9i,\u03c9j\u3009 = 0 or 1, and that E J\u3008\u03c91,\u03c92\u3009K = 1L since that is the probability of the same label getting observed for x1 and x2."}, {"heading": "C.2 Lower Bound for Frobenius Norm Regularization", "text": "In this section, we shall prove that even for isotropic distributions, Frobenius norm regularization cannot offer O ( 1\u221a nL ) -style bounds as offered by trace norm regularization.\nClaim 8. There exists an isotropic, sub-Gaussian data distribution and a loss function such that the empirical risk minimizer learned as Z\u0302 = arg inf\n\u2016Z\u2016F\u2264\u03bb L\u0302(Z) has, with constant probability, its population risk lower\nbounded by\nL(Z\u0302) \u2265 L\u0302(Z\u0302) + \u2126 ( \u03bb \u221a 1\nn\n) ,\nwhereas an empirical risk minimizer learned as Z\u0302 = arg inf \u2016Z\u2016tr\u2264\u03bb L\u0302(Z) over the same distribution has, with probability at least 1\u2212 \u03b4, its population risk bounded by\nL(Z\u0302) \u2264 L\u0302(Z\u0302) +O ( \u03bb \u221a 1\nnL\n) +O \u221a log 1\u03b4 n  .\nWe shall again prove this result for \u03bb = 1. We shall retain the distribution over labels as well as the loss function from our previous discussion in Appendix C.1. We shall also reuse \u03c9li to denote the label observation pattern. We shall however use Rademacher vectors to define the data distribution i.e. each of the d coordinates of the vector x obeys the law\nr \u223c 1 2 (1{r=1} + 1{r=\u22121}).\nThus we sample xi as\nxi = 1\u221a d\n[ r1i , r 2 i , . . . , r d i ] ,\nwhere each coordinate is independently sampled. We now show that this distribution satisfies the assumptions of Theorem 4. We have E q xx> y = 1d \u00b7 I where I is the d\u00d7d identity matrix. Thus \u03c31 = 1 d and \u03a3 = 1. We also have, for any v \u2208 Rd,\nE r exp ( x>v )z = E u vexp  d\u2211 j=1 xjvj }~ = d\u220f j=1 E q exp ( xjvj\n)y =\nd\u220f j=1 1 2 ( exp ( 1\u221a d vj ) + exp ( \u2212 1\u221a d vj ))\n= d\u220f j=1 cosh ( 1\u221a d vj ) \u2264 d\u220f j=1 exp ( 1 d (vj)2 )\n= exp  d\u2211 j=1 1 d (vj)2  = exp(1 d \u2016v\u201622 ) ,\nwhere the second equality uses the independence of the coordinates of x. Thus we have \u03b72 = 2d . Thus, this distribution fulfills all the preconditions of Theorem 4. Note that had trace norm regularization been applied, then by applying Theorem 4, we would have gotten an excess error of\nO\n(\u221a d(\u03b72 + \u03c31)\nnL\u03a3\n) = O (\u221a d(2/d+ 1/d)\nnL \u00b7 1\n) = O (\u221a 1\nnL ) whereas, as the calculation given below shows, Frobenius norm regularization cannot guarantee an excess\nrisk better than O (\u221a\n1 n\n) . Suppose we do perform Frobenius norm regularization in this case. Then we\nhave Z\u0302 = arg inf\n\u2016Z\u2016F\u22641 L\u0302(Z) = arg inf \u2016Z\u2016F\u22641\n1 n \u3008Z,X\u3009,\nwhere X is the matrix\nX = [ L\u2211 i=1 \u03c91i xi L\u2211 i=1 \u03c92i xi . . . L\u2211 i=1 \u03c9Li xi ] .\nAs before, L(Z\u0302) = 0 since the data distribution is centered and the loss function is linear. By a similar application of Ho\u0308lder\u2019s inequality, we can also get\nZ\u0302 = \u2212 X \u2016X\u2016F ,\nand thus, L\u0302(Z\u0302) = \u2212 1n \u2016X\u2016F . The following lemma shows that with constant probability, \u2016X\u2016F \u2265 \u221a n/2\nwhich shows that L(Z\u0302) \u2265 L\u0302(Z\u0302) + \u2126 (\u221a\n1 n\n) , thus proving the claimed inability of Frobenius norm regular-\nization to give O (\n1\u221a nL\n) -style bounds even for isotropic distributions.\nLemma 3. With probability at least 3/4, we have \u2016X\u20162F \u2265 n/2.\nProof. We have\n\u2016X\u20162F = L\u2211 l=1 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03c9lixi \u2225\u2225\u2225\u2225\u2225 2\n2\n= L\u2211 l=1 n\u2211 i=1 \u03c9li \u2016xi\u2016 2 2 + L\u2211 l=1 \u2211 i 6=j \u03c9li\u03c9 l j\u3008xi,xj\u3009\n= n\u2211 i=1 \u2016xi\u201622 + \u2211 i 6=j \u3008xi,xj\u3009\u3008\u03c9i,\u03c9j\u3009 = n+W\nwhere as before, \u03c9i = [\u03c91i , \u03c9 2 i , . . . , \u03c9 L i ]. We will, in the sequel prove that |W | \u2264 n/2, thus establishing\nthe claim. Clearly E JW K = 0 and as the following calculation shows, E q W 2 y \u2264 2n2/Ld which, by an application of Tchebysheff\u2019s inequality, gives us, for Ld > 32, with probability at least 3/4, |W | \u2264 n/2 and consequently \u2016X\u20162F \u2265 n/2. We give an estimation of the variance of W below.\nE q W 2 y = E u v \u2211 i1 6=j1,i2 6=j2 \u3008xi1 ,xj1\u3009\u3008\u03c9i1 ,\u03c9j1\u3009\u3008xi2 ,xj2\u3009\u3008\u03c9i2 ,\u03c9j2\u3009 } ~\n= 2E u v\u2211 i 6=j \u3008xi,xj\u30092\u3008\u03c9i,\u03c9j\u30092 } ~ = 2n(n\u2212 1)E q \u3008x1,x2\u30092\u3008\u03c91,\u03c92\u3009 y = 2n(n\u2212 1)E q \u3008x1,x2\u30092 y E J\u3008\u03c91,\u03c92\u3009K \u2264 2n2\nLd ,\nwhere we have used the fact that data points and label patterns are sampled independently."}, {"heading": "D More Experimental Results", "text": ""}, {"heading": "D.1 Speedup Results Due to Multi-core Computation", "text": ""}, {"heading": "D.2 Detailed Results with Full Labels", "text": "\u2022 Table 6 shows the top-1 accuracy results for the case with fully observed labels.\n\u2022 Table 7 shows the top-3 accuracy results for the case with fully observed labels.\n\u2022 Table 8 shows the top-5 accuracy results for the case with fully observed labels.\n\u2022 Table 9 shows the Hamming loss results for the case with fully observed labels.\n\u2022 Table 10 shows the average AUC results for the case with fully observed labels."}, {"heading": "D.3 Detailed Results with Missing Labels", "text": "\u2022 Table 11 shows the top-1 accuracy results for the case with various missing ratios and dimension reduction rates.\n\u2022 Table 12 shows the top-3 accuracy results for the case with various missing ratios and dimension reduction rates.\n\u2022 Table 13 shows the top-5 accuracy results for the case with various missing ratios and dimension reduction rates.\n\u2022 Table 14 shows the Hamming loss results for the case with various missing ratios and dimension reduction rates.\n\u2022 Table 15 shows the average AUC results for the case with various missing ratios and dimension reduction rates."}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Agrawal", "Rahul", "Gupta", "Archit", "Prabhu", "Yashoteja", "Varma", "Manik"], "venue": "In Proceedings of the International World Wide Web Conference,", "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Efficient multi-label ranking for multi-class learning: Application to object recognition", "author": ["Bucak", "Serhat Selcuk", "Mallapragada", "Pavan Kumar", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Proceedings of IEEE International Conference on Computer Vision,", "citeRegEx": "Bucak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2009}, {"title": "Multi-label learning with incomplete class assignments", "author": ["Bucak", "Serhat Selcuk", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Bucak et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2011}, {"title": "Supervised learning of semantic classes for image annotation and retrieval", "author": ["Carneiro", "Gustavo", "Chan", "Antoni B", "Moreno", "Pedro J", "Vasconcelos", "Nuno"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Carneiro et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Carneiro et al\\.", "year": 2007}, {"title": "Feature-aware label space dimension reduction for multi-label classification", "author": ["Chen", "Yao-Nan", "Lin", "Hsuan-Tien"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan", "Rong-En", "Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Wang", "Xiang-Rui", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Large scale maxmargin multi-label classification with priors", "author": ["Hariharan", "Bharath", "Zelnik-Manor", "Lihi", "S.V.N. Vishwanathan", "Varma", "Manik"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Hariharan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2010}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Langford", "John", "Zhang", "Tong"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization", "author": ["Kakade", "Sham M", "Sridharan", "Karthik", "Tewari", "Ambuj"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Regularization Techniques for Learning with Matrices", "author": ["Kakade", "Sham M", "Shalev-Shwartz", "Shai", "Tewari", "Ambuj"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kakade et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2012}, {"title": "Multilabel classification using bayesian compressed sensing", "author": ["Kapoor", "Ashish", "Viswanathan", "Raajay", "Jain", "Prateek"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kapoor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2012}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["Ledoux", "Michel", "Talagrand"], "venue": null, "citeRegEx": "Ledoux et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ledoux et al\\.", "year": 2002}, {"title": "Trust region Newton method for large-scale logistic regression", "author": ["Lin", "Chih-Jen", "Weng", "Ruby C", "Keerthi", "S. Sathiya"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "A Generalized Representer Theorem", "author": ["Sch\u00f6lkopf", "Bernhard", "Herbrich", "Ralf", "Smola", "Alex J"], "venue": "In 14th Annual Conference on Computational Learning Theory, pp", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Collaborative Filtering with the Trace Norm: Learning, Bounding, and Transducing", "author": ["Shamir", "Ohad", "Shalev-Shwartz", "Shai"], "venue": "In 24th Annual Conference on Learning Theory,", "citeRegEx": "Shamir et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2011}, {"title": "Canonical correlation analysis for multi-label classification: A least squares formulation, extensions and analysis", "author": ["Sun", "Liang", "Ji", "Shuiwang", "Ye", "Jieping"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Multi-label classification with principal label space transformation", "author": ["Tai", "Farbound", "Lin", "Hsuan-Tien"], "venue": "Neural Computation,", "citeRegEx": "Tai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices, chapter 5 of Compressed Sensing, Theory and Applications, pp. 210\u2013268", "author": ["Vershynin", "Roman"], "venue": null, "citeRegEx": "Vershynin and Roman.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2012}, {"title": "Multi-Label Sparse Coding for Automatic Image Annotation", "author": ["Wang", "Changhu", "Yan", "Shuicheng", "Zhang", "Lei", "Hong-Jiang"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Large scale multi-label classification is an important learning problem with several applications to real-world problems such as image and video annotation Carneiro et al. (2007); Wang et al.", "startOffset": 156, "endOffset": 179}, {"referenceID": 2, "context": "Large scale multi-label classification is an important learning problem with several applications to real-world problems such as image and video annotation Carneiro et al. (2007); Wang et al. (2009) and query/keyword suggestions Agrawal et al.", "startOffset": 156, "endOffset": 199}, {"referenceID": 0, "context": "(2009) and query/keyword suggestions Agrawal et al. (2013). The goal in multi-label classification is to accurately predict a label vector y \u2208 {0, 1}L for a given data point x \u2208 Rd.", "startOffset": 37, "endOffset": 59}, {"referenceID": 0, "context": "(2009) and query/keyword suggestions Agrawal et al. (2013). The goal in multi-label classification is to accurately predict a label vector y \u2208 {0, 1}L for a given data point x \u2208 Rd. This problem has been studied extensively in the domain of structured output learning, where the number of labels is assumed to be small and the main focus is thus, on modeling inter-label correlations and using them to accurately predict the label vector Hariharan et al. (2010). Due to several motivating real-life applications, recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large, with the key challenge being the design of scalable algorithms that offer real-time predictions and have a small memory footprint.", "startOffset": 37, "endOffset": 462}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al.", "startOffset": 194, "endOffset": 212}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al.", "startOffset": 194, "endOffset": 230}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently.", "startOffset": 194, "endOffset": 252}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression.", "startOffset": 194, "endOffset": 467}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case.", "startOffset": 194, "endOffset": 1412}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case. However, the flexibility of the framework allows us to use other loss functions and regularizers that are useful for preventing overfitting and increasing scalability. Moreover, we can extend our formulation to handle missing labels; in contrast, most dimension reduction formulations (including CPLST) cannot accommodate missing labels. The ability to learn in the presence of missing labels is crucial as for most real-world applications, one cannot expect to accurately obtain (either through manual or automated labeling) all the labels for a given data point. For example, in image annotation, human labelers tag only prominent labels and typically miss out on several objects present in the image. Similarly, in online collections such as Wikipedia, where articles get tagged with categories, human labelers usually tag only with categories they know about. Moreover, there might be considerable noise/disagreement in the labeling. In order to solve for the low-rank linear model that results from our formulation, we use the popular alternating minimization algorithm that works well despite the non-convexity of the rank constraint. For general loss functions and trace-norm regularization, we exploit subtle structures present in the problem to design a fast conjugate gradient based method. For the special case of squared-L2 loss and trace-norm regularization, we further exploit the structure of the loss function to provide a more efficient and scalable algorithm. As compared to direct computation, our algorithm is O(d\u0304) faster, where d\u0304 is the average number of nonzero features in an instance. On the theoretical side, we perform an excess risk analysis for the trace-norm regularized ERM formulation with missing labels, assuming labels are observed uniformly at random. Our proofs do not follow from existing results due to missing labels and require a careful analysis involving results from random matrix theory. Our results show that while in general the low-rank promoting trace-norm regularization does not provide better bounds than learning a full-rank matrix (e.g. using Frobenius norm regularization), for several interesting data distributions, trace-norm regularization does indeed give significantly better bounds. More specifically, for isotropic data distributions, we show that trace-norm based methods have excess risk of O( 1 \u221a nL ) while full-rank learning can only guarantee O( 1 \u221a n ) excess risk, where n is the number of training points and L is the number of labels. Finally, we provide an extensive empirical evaluation of our method on a variety of benchmark datasets. In particular, we compare our method against three recent label compression based methods: CPLST Chen & Lin (2012), Bayesian-CS Kapoor et al.", "startOffset": 194, "endOffset": 4174}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case. However, the flexibility of the framework allows us to use other loss functions and regularizers that are useful for preventing overfitting and increasing scalability. Moreover, we can extend our formulation to handle missing labels; in contrast, most dimension reduction formulations (including CPLST) cannot accommodate missing labels. The ability to learn in the presence of missing labels is crucial as for most real-world applications, one cannot expect to accurately obtain (either through manual or automated labeling) all the labels for a given data point. For example, in image annotation, human labelers tag only prominent labels and typically miss out on several objects present in the image. Similarly, in online collections such as Wikipedia, where articles get tagged with categories, human labelers usually tag only with categories they know about. Moreover, there might be considerable noise/disagreement in the labeling. In order to solve for the low-rank linear model that results from our formulation, we use the popular alternating minimization algorithm that works well despite the non-convexity of the rank constraint. For general loss functions and trace-norm regularization, we exploit subtle structures present in the problem to design a fast conjugate gradient based method. For the special case of squared-L2 loss and trace-norm regularization, we further exploit the structure of the loss function to provide a more efficient and scalable algorithm. As compared to direct computation, our algorithm is O(d\u0304) faster, where d\u0304 is the average number of nonzero features in an instance. On the theoretical side, we perform an excess risk analysis for the trace-norm regularized ERM formulation with missing labels, assuming labels are observed uniformly at random. Our proofs do not follow from existing results due to missing labels and require a careful analysis involving results from random matrix theory. Our results show that while in general the low-rank promoting trace-norm regularization does not provide better bounds than learning a full-rank matrix (e.g. using Frobenius norm regularization), for several interesting data distributions, trace-norm regularization does indeed give significantly better bounds. More specifically, for isotropic data distributions, we show that trace-norm based methods have excess risk of O( 1 \u221a nL ) while full-rank learning can only guarantee O( 1 \u221a n ) excess risk, where n is the number of training points and L is the number of labels. Finally, we provide an extensive empirical evaluation of our method on a variety of benchmark datasets. In particular, we compare our method against three recent label compression based methods: CPLST Chen & Lin (2012), Bayesian-CS Kapoor et al. (2012), and WSABIE Weston et al.", "startOffset": 194, "endOffset": 4208}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case. However, the flexibility of the framework allows us to use other loss functions and regularizers that are useful for preventing overfitting and increasing scalability. Moreover, we can extend our formulation to handle missing labels; in contrast, most dimension reduction formulations (including CPLST) cannot accommodate missing labels. The ability to learn in the presence of missing labels is crucial as for most real-world applications, one cannot expect to accurately obtain (either through manual or automated labeling) all the labels for a given data point. For example, in image annotation, human labelers tag only prominent labels and typically miss out on several objects present in the image. Similarly, in online collections such as Wikipedia, where articles get tagged with categories, human labelers usually tag only with categories they know about. Moreover, there might be considerable noise/disagreement in the labeling. In order to solve for the low-rank linear model that results from our formulation, we use the popular alternating minimization algorithm that works well despite the non-convexity of the rank constraint. For general loss functions and trace-norm regularization, we exploit subtle structures present in the problem to design a fast conjugate gradient based method. For the special case of squared-L2 loss and trace-norm regularization, we further exploit the structure of the loss function to provide a more efficient and scalable algorithm. As compared to direct computation, our algorithm is O(d\u0304) faster, where d\u0304 is the average number of nonzero features in an instance. On the theoretical side, we perform an excess risk analysis for the trace-norm regularized ERM formulation with missing labels, assuming labels are observed uniformly at random. Our proofs do not follow from existing results due to missing labels and require a careful analysis involving results from random matrix theory. Our results show that while in general the low-rank promoting trace-norm regularization does not provide better bounds than learning a full-rank matrix (e.g. using Frobenius norm regularization), for several interesting data distributions, trace-norm regularization does indeed give significantly better bounds. More specifically, for isotropic data distributions, we show that trace-norm based methods have excess risk of O( 1 \u221a nL ) while full-rank learning can only guarantee O( 1 \u221a n ) excess risk, where n is the number of training points and L is the number of labels. Finally, we provide an extensive empirical evaluation of our method on a variety of benchmark datasets. In particular, we compare our method against three recent label compression based methods: CPLST Chen & Lin (2012), Bayesian-CS Kapoor et al. (2012), and WSABIE Weston et al. (2010). On almost all benchmark datasets, our method significantly outperforms these methods, both in the presence and absence of missing", "startOffset": 194, "endOffset": 4241}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels.", "startOffset": 109, "endOffset": 131}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al.", "startOffset": 109, "endOffset": 841}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al.", "startOffset": 109, "endOffset": 864}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al.", "startOffset": 109, "endOffset": 889}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al. (2012), or reduce the feature dimensionality, such as Sun et al.", "startOffset": 109, "endOffset": 927}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al. (2012), or reduce the feature dimensionality, such as Sun et al. (2011), or both, such as WSABIE Weston et al.", "startOffset": 109, "endOffset": 992}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al. (2012), or reduce the feature dimensionality, such as Sun et al. (2011), or both, such as WSABIE Weston et al. (2010). Most of these existing techniques are tied to a specific loss function (e.", "startOffset": 109, "endOffset": 1038}, {"referenceID": 0, "context": ", Yij = 1 or 0), or missing (Yij =?); several other works have considered another setting where only positive labels are known and are given as 1 in the label matrix, while negative or missing values are all denoted by 0 Agrawal et al. (2013); Bucak et al.", "startOffset": 221, "endOffset": 243}, {"referenceID": 0, "context": ", Yij = 1 or 0), or missing (Yij =?); several other works have considered another setting where only positive labels are known and are given as 1 in the label matrix, while negative or missing values are all denoted by 0 Agrawal et al. (2013); Bucak et al. (2011). Note that although the above formulation is NP-hard in general due to the non-convex rank constraint, for convex loss functions, one can still utilize the standard alternating minimization method.", "startOffset": 221, "endOffset": 264}, {"referenceID": 0, "context": ", Yij = 1 or 0), or missing (Yij =?); several other works have considered another setting where only positive labels are known and are given as 1 in the label matrix, while negative or missing values are all denoted by 0 Agrawal et al. (2013); Bucak et al. (2011). Note that although the above formulation is NP-hard in general due to the non-convex rank constraint, for convex loss functions, one can still utilize the standard alternating minimization method. Moreover, for the special case of L2 loss, we can derive closed form solutions for the full-label case (1) and show connections to several existing methods. We would like to note that while the ERM framework is well known and standard, most existing multilabel methods for large number of labels motivate their work in a relatively ad-hoc manner. By studying this formal framework, we can show that existing methods like CPLST Chen & Lin (2012) are in fact a special case of this generic framework (see next section).", "startOffset": 221, "endOffset": 907}, {"referenceID": 5, "context": "Several linear classification/regression packages such as LIBLINEAR Fan et al. (2008) can handle such problems if {x\u0303ij : (i, j) \u2208 \u03a9} are available.", "startOffset": 68, "endOffset": 86}, {"referenceID": 12, "context": "Thus, to solve (7), we can apply CG for squared loss and the trust region Newton method (TRON) Lin et al. (2008) for the other two loss functions.", "startOffset": 95, "endOffset": 113}, {"referenceID": 8, "context": "Our proofs do not follow either from existing techniques for learning with matrix predictors (for instance Kakade et al. (2012)) or from results on matrix completion with trace norm regularization Shamir & Shalev-Shwartz (2011) due to the complex interplay of feature vectors and missing labels that we encounter in our learning model.", "startOffset": 107, "endOffset": 128}, {"referenceID": 8, "context": "Our proofs do not follow either from existing techniques for learning with matrix predictors (for instance Kakade et al. (2012)) or from results on matrix completion with trace norm regularization Shamir & Shalev-Shwartz (2011) due to the complex interplay of feature vectors and missing labels that we encounter in our learning model.", "startOffset": 107, "endOffset": 228}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al.", "startOffset": 13, "endOffset": 57}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al.", "startOffset": 13, "endOffset": 147}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al. (2012). 1.", "startOffset": 13, "endOffset": 169}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al. (2012). 1. LEML (Low rank Empirical risk minimization for Multi-Label Learning): our proposed method. We implemented CG with Algorithms 1 and 2 for squared loss, and TRON Lin et al. (2008) with Algorithm 1 for logistic and squared hinge loss.", "startOffset": 13, "endOffset": 351}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al. (2012). 1. LEML (Low rank Empirical risk minimization for Multi-Label Learning): our proposed method. We implemented CG with Algorithms 1 and 2 for squared loss, and TRON Lin et al. (2008) with Algorithm 1 for logistic and squared hinge loss. 2. CPLST: the method proposed in Chen & Lin (2012). We used the code provided by the author.", "startOffset": 13, "endOffset": 456}, {"referenceID": 10, "context": "BCS: the Bayesian compressed sensing method of Kapoor et al. (2012). We used code provided by the authors to test this method.", "startOffset": 47, "endOffset": 68}, {"referenceID": 1, "context": "\u2022 Average AUC: we follow Bucak et al. (2009) to calculate area under ROC curve for each instance and report the average AUC among all test instances.", "startOffset": 25, "endOffset": 45}, {"referenceID": 19, "context": "Although this phenomenon is expected due to the sampling scheme in WSABIE Weston et al. (2010), it becomes more serious as L increases.", "startOffset": 74, "endOffset": 95}, {"referenceID": 8, "context": "Approaches to bounding such Rademacher average terms usually resort to Martingale techniques Kakade et al. (2008) or use of tools from convex analysis Kakade et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 8, "context": "Approaches to bounding such Rademacher average terms usually resort to Martingale techniques Kakade et al. (2008) or use of tools from convex analysis Kakade et al. (2012) and decompose the Rademacher average term.", "startOffset": 93, "endOffset": 172}], "year": 2013, "abstractText": "The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) the ability to tackle problems with a large number (say millions) of labels, and (b) the ability to handle data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions such as the squared loss function to offer efficient algorithms. We further show that our learning framework admits formal excess risk bounds even in the presence of missing labels. Our risk bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as the Wikipedia dataset.", "creator": "LaTeX with hyperref package"}}}