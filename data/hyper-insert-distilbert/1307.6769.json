{"id": "1307.6769", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2013", "title": "Streaming Variational Bayes", "abstract": "we hence present sda - weighted bayes, simply a framework proposed for ( / s ) treaming, ( d ) istributed, ( rarely a ) complete synchronous computation of representing a symmetric bayesian posterior. the approximation framework makes continuous streaming updates to denote the estimated scaled posterior according to integrating a centrally user - specified approximation primitive statistical function. nonetheless we demonstrate the usefulness of our framework, initially with variational bayes ( > vb ) regression as the estimated primitive, providing by successfully fitting itself the latent distribution dirichlet vector allocation model to two dynamic large - document scale document collections. we demonstrate the advantages of our scaling algorithm notation over posterior stochastic vector variational inference ( svi ), allowing both in the single - path pass setting svi also was particularly designed locally for compiling and apply in parallel the variable streaming setting, to which such svi does not mutually apply.", "histories": [["v1", "Thu, 25 Jul 2013 15:03:40 GMT  (214kb,D)", "http://arxiv.org/abs/1307.6769v1", "21 pages"], ["v2", "Wed, 20 Nov 2013 23:29:01 GMT  (76kb,D)", "http://arxiv.org/abs/1307.6769v2", "25 pages, 3 figures, 1 table"]], "COMMENTS": "21 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tamara broderick", "nicholas boyd", "andre wibisono", "ashia c wilson", "michael i jordan"], "accepted": true, "id": "1307.6769"}, "pdf": {"name": "1307.6769.pdf", "metadata": {"source": "CRF", "title": "Streaming Variational Bayes", "authors": ["Tamara Broderick", "Nicholas Boyd", "Andre Wibisono", "Ashia C. Wilson", "Michael I. Jordan"], "emails": [], "sections": [{"heading": null, "text": "(A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation primitive function. We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections. We demonstrate the advantages of our algorithm over stochastic variational inference (SVI), both in the single-pass setting SVI was designed for and in the streaming setting, to which SVI does not apply."}, {"heading": "1 Introduction", "text": "Large, streaming data sets are increasingly the norm in science and technology. While simple descriptive statistics can often be readily computed with a constant number of operations for each data point in the streaming setting, without the need to revisit past data or have advance knowledge of future data, this is generally not the case for the complex, hierarchical models that practitioners often have in mind when they collect such large data sets. Significant progress on scalable learning procedures has been made in recent years [e.g., 1, 2], but the underlying models remain simple and the inferential framework is generally non-Bayesian. The advantages of the Bayesian paradigm (e.g., hierarchical modeling, coherent treatment of uncertainty) currently seem out of reach in the setting of Big Data.\nAn exception to this statement is provided by recent work by [3\u20135], who have shown that a class of mean-field variational inference methods known as variational Bayes (VB) [6] can be usefully deployed for large-scale data sets. They\nar X\niv :1\n30 7.\n67 69\nv1 [\nst at\n.M L\n] 2\n5 Ju\nhave applied their approach to the domain of topic modeling of document collections, an area where there is a major need to develop scalable inference algorithms. The method, referred to as stochastic variational inference (SVI) combines VB with stochastic gradient descent. The objective function for SVI is the variational lower bound on the marginal likelihood that is traditionally used in VB, and the idea is to apply stochastic gradient descent to this objective. The problem is that this objective is based on the conceptual existence of a full data set involving D data points (i.e., documents in the topic model setting) for a fixed value of D. Although the stochastic gradient is computed for a single data point (document) at a time, the posterior being targeted is a posterior for D data points. This value of D must be specified in advance, and is used by the algorithm at each step. Posteriors for D\u2032 data points, for D\u2032 6= D, are not obtained as part of the analysis.\nWe view this lack of a link between the number of documents that have been processed thus far and the posterior that is being targeted as undesirable in many settings involving streaming data, and in this paper we aim at an approximate Bayesian inference algorithm that is scalable like SVI, but is also truly a streaming procedure, in that it yields an approximate posterior for each processed collection of D\u2032 data points\u2014and not just some pre-specified \u201cfinal\u201d number of data points D. We achieve this by returning to the classical perspective of Bayesian updating, where the recursive application of Bayes theorem provides a sequence of posteriors, not a sequence of approximations to a fixed posterior. To this classical recursive perspective we bring the VB framework\u2014our updates are not exact Bayesian updates but rather are variational approximations. This is similar in spirit to assumed density filtering or expectation propagation [7\u20139], but each step of those methods involves a moment-matching step that can be computationally costly for models such as topic models. We are able to avoid the moment-matching step via the use of VB. We also note other related work in this general vein: MCMC approximations have been explored by [10], and VB or VB-like approximations have also been explored by [11, 12].\nAlthough the empirical success of SVI is the main motivation for our work, we also note that a return to the classical framework of recursive Bayesian updating is motivated by recent developments in computer architectures, which permit distributed and asynchronous computations in addition to streaming computations. As we will show, a streaming VB algorithm also naturally lends itself to distributed and asynchronous implementations."}, {"heading": "2 Streaming, distributed, asynchronous Bayesian up-", "text": "dating"}, {"heading": "2.1 Streaming Bayesian updating", "text": "Consider data x1, x2, . . . generated iid according to a distribution p(x | \u0398) and assume that a prior p(\u0398) has also been specified. Then Bayes theorem gives us the posterior distribution of \u0398 given a collection of S data points, C1 := (x1, . . . , xS):\np(\u0398 | C1) = p(C1)\u22121 p(C1 | \u0398) p(\u0398),\nwhere p(C1 |\u0398) = p(x1, . . . , xS |\u0398) = \u220fS\ns=1 p(xs |\u0398). Suppose we have seen and processed b\u22121 collections, sometimes called minibatches, of data. Given the posterior p(\u0398 | C1, . . . , Cb\u22121), we can calculate the posterior after the bth minibatch:\np(\u0398 | C1, . . . , Cb) \u221d p(Cb | \u0398) p(\u0398 | C1, . . . , Cb\u22121). (1)\nThat is, we treat the posterior after b \u2212 1 minibatches as the new prior for the incoming data points. If we can save the posterior from b \u2212 1 minibatches and calculate the normalizing constant for the bth posterior, repeated application of Eq. (1) is streaming; it automatically gives us the new posterior without needing to revisit old data points.\nIn complex models, it is often infeasible to calculate the posterior exactly, and an approximation must be used. Suppose that, given a prior p(\u0398) and data minibatch C, we have an approximation algorithm A that calculates an approximate posterior q: q(\u0398) = A(C, p(\u0398)). Then, setting q0(\u0398) = p(\u0398), one way to recursively calculate an approximation to the posterior is\np(\u0398 | C1, . . . , Cb) \u2248 qb(\u0398) = A (Cb, qb\u22121(\u0398)) . (2)\nWhen A yields the posterior from Bayes theorem, this calculation is exact. This approach already differs from that of [3\u20135], which we will see (Sec. 3.2) directly approximates p(\u0398 | C1, . . . , CB) for fixed B without making intermediate approximations for b strictly between 1 and B."}, {"heading": "2.2 Distributed Bayesian updating", "text": "The sequential updates in Eq. (2) handle streaming data in theory, but in practice, theA calculation might take longer than the time interval between minibatch\narrivals or simply take longer than desired. Parallelizing computations increases algorithm throughput. And posterior calculations need not be sequential. Indeed,\np(\u0398 | C1, . . . , CB) \u221d [ B\u220f b=1 p(Cb | \u0398) ] p(\u0398) \u221d [ B\u220f b=1 p(\u0398 | Cb) p(\u0398)\u22121 ] p(\u0398)\n(3)\nThat is, we can calculate the individual minibatch posteriors p(\u0398 | Cb), perhaps in parallel, and then combine them to find the full posterior p(\u0398 | C1, . . . , CB).\nGiven an approximating algorithmA as above, the corresponding approximate update would be\np(\u0398 | C1, . . . , CB) \u2248 q(\u0398) \u221d [ B\u220f b=1 A(Cb, p(\u0398)) p(\u0398)\u22121 ] p(\u0398), (4)\nfor some approximating distribution q, provided the normalizing constant for the right-hand side of Eq. (4) can be computed.\nVariational inference methods are generally based on the exponential family representations [6], and we will make that assumption here. In particular, we suppose p(\u0398) \u221d exp{\u03be0 \u00b7T (\u0398)}; that is, p(\u0398) is an exponential family distribution for \u0398 with sufficient statistic T (\u0398) and natural parameter \u03be0. We suppose further that A always returns a distribution in the same exponential family; in particular, we suppose that there exists some parameter \u03beb such that\nqb(\u0398) \u221d exp{\u03beb \u00b7 T (\u0398)} for qb(\u0398) = A(Cb, p(\u0398)). (5)\nWhen we make these two assumptions, the update in Eq. (4) becomes\np(\u0398 | C1, . . . , CB) \u2248 q(\u0398) \u221d exp {[ \u03be0 +\nB\u2211 b=1 (\u03beb \u2212 \u03be0)\n] \u00b7 T (\u0398) } , (6)\nwhere the normalizing constant is readily obtained. In what follows we use the shorthand \u03be \u2190 A(C, \u03be0) to denote thatA takes as input a minibatch C and a prior with exponential family parameter \u03be0 and that it returns a distribution in the same exponential family with parameter \u03be.\nSo, to approximate p(\u0398 | C1, . . . , CB), we first calculate \u03beb via the approximation primitive A for each minibatch Cb; note that these calculations may be performed in parallel. Then we sum together the quantities \u03beb\u2212 \u03be0 across b, along\nwith the initial \u03be0 from the prior, to find the final exponential family parameter to the full posterior approximation q. We previously saw that the general Bayes sequential update can be made streaming by iterating with the old posterior as the new prior (Eq. (2)). Similarly, here we see that the full posterior approximation q is in the same exponential family as the prior, so one may iterate these parallel computations to arrive at a parallelized algorithm for streaming posterior computation.\nWe emphasize that while these updates are reminiscent of prior-posterior conjugacy, it is actually the approximate posteriors and single, original prior that we assume belong to the same exponential family. It is not necessary to assume any conjugacy in the generative model itself nor that any true intermediate or final posterior take any particular limited form."}, {"heading": "2.3 Asynchronous Bayesian updating", "text": "Performing B computations in parallel can in theory speed up algorithm running time by a factor of B, but in practice it is often the case that a single computation thread takes longer than the rest. Waiting for this thread to finish diminishes potential gains from distributing the computations. This problem can be ameliorated by making computations asynchronous. In this case, processors known as workers each solve a subproblem. When a worker finishes, it reports its solution to a single master processor. If the master gives the worker a new subproblem without waiting for the other workers to finish, it can decrease downtime in the system.\nOur asynchronous algorithm is in the spirit of Hogwild! [1]. To present the algorithm we first describe an asynchronous computation that we will not use in practice, but which will serve as a conceptual stepping stone. Note in particular that the following scheme makes the computations in Eq. (6) asynchronous. Have each worker continuously iterate between three steps: (1) collect a new minibatch C, (2) compute the local approximate posterior \u03be \u2190 A(C, \u03be0), and (3) return \u2206\u03be := \u03be \u2212 \u03be0 to the master. The master, in turn, starts by assigning the posterior to equal the prior: \u03be(post) \u2190 \u03be0. Each time the master receives a quantity \u2206\u03be from any worker, it updates the posterior synchronously: \u03be(post) \u2190 \u03be(post) + \u2206\u03be. If A returns the exponential family parameter of the true posterior (rather than an approximation), then the posterior at the master is exact by Eq. (4).\nA preferred asynchronous computation works as follows. The master initializes its posterior estimate to the prior: \u03be(post) \u2190 \u03be0. Each worker continuously iterates between four steps: (1) collect a new minibatch C, (2) copy the master posterior value locally \u03be(local) \u2190 \u03be(post). (3) compute the local approximate pos-\nterior \u03be \u2190 A(C, \u03be(local)), and (4) return \u2206\u03be := \u03be \u2212 \u03be(local) to the master. Each time the master receives a quantity \u2206\u03be from any worker, it updates the posterior synchronously: \u03be(post) \u2190 \u03be(post) + \u2206\u03be.\nThe key difference between the first and second frameworks proposed above is that, in the second, the latest posterior is used as a prior. This latter framework is more in line with the streaming update of Eq. (2) but introduces a new layer of approximation. Since \u03be(post) might change at the master while the worker is computing \u2206\u03be, it is no longer the case that the posterior at the master is exact when A returns the exponential family parameter of the true posterior. Nonetheless we find that the latter framework performs better in practice, so we focus on it exclusively in what follows.\nWe refer to our overall framework as SDA-Bayes, which stands for (S)treaming, (D)istributed, (A)synchronous Bayes. The framework is intended to be general enough to allow a variety of local approximations A, but in the current paper our preferred local approximation will be VB."}, {"heading": "3 Case study: latent Dirichlet allocation", "text": "In order to use SDA-Bayes, one needs to specify: (1) a prior on the global parameter(s) \u0398 and (2) a posterior-approximating algorithm A. In what follows, we consider examples of both choices in the context of latent Dirichlet allocation (LDA) [13]. LDA models the content of D documents in a corpus. Themes potentially shared by multiple documents are described by topics. The unsupervised learning problem is to learn the topics as well as discover which topics occur in which documents.\nMore formally, each topic (ofK total topics) is a distribution over the V words in the vocabulary: \u03b2k = (\u03b2kv)Vv=1. Each document is an admixture of topics. The words in document d are assumed to be exchangeable. Each word wdn belongs to a latent topic zdn chosen according to a document-specific distribution of topics \u03b8d = (\u03b8dk) K k=1. The full generative model, with Dirichlet priors for \u03b2k and \u03b8d conditioned on respective parameters \u03b7k and \u03b1, appears in [13]. To see that this model fits our specification in Sec. 2, consider the set of global parameters \u0398 = \u03b2. Each document wd = (wd,n) Nd n=1 is distributed iid conditioned on the global topics. The full collection of data is a corpus C = w = (wd)Dd=1 of documents. The posterior for LDA, p(\u03b2, \u03b8, z | C, \u03b7, \u03b1), is equal to the following\nexpression up to proportionality:\n\u221d [ K\u220f k=1 Dirichlet(\u03b2k | \u03b7k) ] \u00b7 [ D\u220f d=1 Dirichlet(\u03b8d | \u03b1) ] \u00b7 [ D\u220f d=1 Nd\u220f n=1 \u03b8dzdn\u03b2zdn,wdn ] . (7)\nThe posterior for just the global parameters p(\u03b2 | C, \u03b7, \u03b1) can be obtained from p(\u03b2, \u03b8, z |C, \u03b7, \u03b1) by integrating out the local, document-specific parameters \u03b8, z. As is common in complex models, Eq. (7) is intractable to compute and so must be approximated."}, {"heading": "3.1 Posterior-approximation algorithms", "text": "To apply SDA-Bayes to LDA, we use the prior specified by the generative model. It remains to choose a posterior-approximation algorithm A. We consider two possibilities here: variational Bayes (VB) and expectation propagation (EP). Both primitives take Dirichlet distributions as priors for \u03b2 and both return Dirichlet distributions for the approximate posterior of the topic parameters \u03b2; thus the prior and approximate posterior are in the same exponential family. Hence both VB and EP can be utilized as a choice for A in the SDA-Bayes framework.\nMean-field variational Bayes. We use the shorthand pD for Eq. (7), the posterior given D documents. We assume the approximating distribution, written qD for shorthand, takes the form\nqD(\u03b2, \u03b8, z | \u03bb, \u03b3, \u03c6) = [ K\u220f k=1 qD(\u03b2k | \u03bbk) ] \u00b7 [ D\u220f d=1 qD(\u03b8d | \u03b3d) ]\n\u00b7 [ D\u220f d=1 Nd\u220f n=1 qD(zdn | \u03c6dwdn) ] (8)\nfor parameters (\u03bbkv), (\u03b3dk), (\u03c6dv) with k \u2208 {1, . . . , K}, v \u2208 {1, . . . , V }, d \u2208 {1, . . . , D}. Moreover, we set qD(\u03b2k | \u03bbk) = DirichletV (\u03b2k | \u03bbk), qD(\u03b8d | \u03b3d) = DirichletK(\u03b8d | \u03b3d), and qD(zdn | \u03c6dwdn) = CategoricalK(zdn | \u03c6dwdn). The subscripts on Dirichlet and Categorical indicate the dimensions of the distributions (and of the parameters).\nThe problem of VB is to find the best approximating qD, defined as the collection of variational parameters \u03bb, \u03b3, \u03c6 that minimize the KL divergence from the true posterior: KL (qD \u2016 pD). Even finding the minimizing parameters is a difficult optimization problem. Typically the solution is approximated by coordinate\ndescent in each parameter [6, 13] as in Alg. 1. The derivation of VB for LDA can be found in [4, 13].\nExpectation propagation. An EP [7] algorithm for approximating the LDA posterior appears in Alg. 4 of Sup. Mat. A. Alg. 4 differs from [14], which does not provide an approximate posterior for the topic parameters, and is instead our own derivation. Our version of EP, like VB, learns factorized Dirichlet distributions over topics."}, {"heading": "3.2 Other single-pass algorithms for approximate LDA posteriors", "text": "The algorithms in Sec. 3.1 pass through the data multiple times and require storing the data set in memory\u2014but are useful as primitives for SDA-Bayes in the context of the processing of minibatches of data. Next, we consider two algorithms that pass through a data set just one time (single pass) and to which we compare in the evaluations (Sec. 4).\nStochastic variational inference. VB uses coordinate descent to find a value of qD, Eq. (8), that locally minimizes the KL divergence KL (qD \u2016 pD). Stochastic variational inference (SVI) [3, 4] is exactly the application of a particular version of stochastic gradient descent to the same optimization problem. While stochastic gradient descent can often be viewed as a streaming algorithm, the optimization problem itself here depends on D via pD, the posterior on D data points. We see that, as a result, D must be specified in advance, appears in each step of SVI (see Alg. 2), and is independent of the number of data points actually processed by the algorithm. SVI is single-pass, though, and therefore has constant memory requirements. We also note that two new parameters, \u03c40 > 0 and \u03ba \u2208 (0.5, 1], appear in SVI, beyond those in VB, to determine a learning rate \u03c1t as a function of iteration t: \u03c1t := (\u03c40 + t)\u2212\u03ba.\nSufficient statistics. On each round of VB (Alg. 1), we update the local parameters for all documents and then compute \u03bbkv \u2190 \u03b7kv + \u2211D d=1 \u03c6dvkndv. An alternative single-pass (and indeed streaming) option would be to update the local parameters for each minibatch of documents as they arrive and then add the corresponding terms \u03c6dvkndv to the current estimate of \u03bb for each document d in the minibatch. This essential idea has been proposed previously for models other than LDA by [11, 12] and forms the basis of what we call the sufficient statistics update algorithm (SSU): Alg. 3. This algorithm is equivalent to SDA-Bayes with A chosen to be a single iteration over the global variable \u03bb of VB (i.e., updating\n\u03bb exactly once instead of iterating until convergence)."}, {"heading": "4 Evaluation", "text": "We compare SDA-Bayes with VB and EP primitives to SVI and SSU."}, {"heading": "4.1 Performance measure", "text": "We follow [4] (and further [15, 16]) in evaluating our algorithms by computing (approximate) predictive probability. Under this metric, a higher score is better, as a better model will assign a higher probability to the held-out words.\nWe calculate predictive probability by first setting aside held-out testing documentsC(test) from the full corpus and then further setting aside a subset of held-out testing words Wd,test in each testing document d. The remaining (training) documents C(train) are used to estimate the global parameter posterior q(\u03b2), and the remaining (training) words Wd,train within the dth testing document are used to estimate the document-specific parameter posterior q(\u03b8d).1 To calculate predictive probability, an approximation is necessary since we do not know the predictive distribution\u2014just as we seek to learn the posterior distribution. Specifically, we calculate the normalized predictive distribution and report \u201clog predictive probability\u201d as \u2211\nd\u2208C(test) log p(Wd,test | C(train),Wd,train)\u2211 d\u2208C(test) |Wd,test|\n=\n\u2211 d\u2208C(test) \u2211 wtest\u2208Wd,test log p(wtest | C\n(train),Wd,train)\u2211 d\u2208C(test) |Wd,test| ,\nwhere we use the approximation\np(wtest | C(train),Wd,train)\n= \u222b \u03b2 \u222b \u03b8d ( K\u2211 k=1 \u03b8dk\u03b2kwtest ) p(\u03b8d | Wd,train, \u03b2) p(\u03b2 | C(train)) d\u03b8d d\u03b2\n\u2248 \u222b \u03b2 \u222b \u03b8d ( K\u2211 k=1 \u03b8dk\u03b2kwtest ) q(\u03b8d) q(\u03b2) d\u03b8d d\u03b2 = K\u2211 k=1 Eq[\u03b8dk] Eq[\u03b2kwtest ].\n1 In all cases, we estimate q(\u03b8d) for evaluative purposes using VB since direct EP estimation takes prohibitively long."}, {"heading": "4.2 Experiments", "text": "To facilitate comparison with SVI, we use the full Wikipedia corpus of [5] (rather than the subset Wikipedia corpus of [3]) and the Nature corpus of [3] for our experiments. These two corpuses represent a range of sizes (3,611,558 training documents for Wikipedia and 351,525 for Nature) as well as different types of topics. We expect words in Wikipedia to represent an extremely broad range of topics whereas we expect words in Nature to focus more on the sciences. We further use the vocabularies of [3, 5] and SVI code available online at [17]. We hold out 10,000 Wikipedia documents and 1,024 Nature documents (not included in the counts above) for testing. In all cases, we fit an LDA model with K = 100 topics and hyperparameters chosen as: \u2200k, \u03b1k = 1/K, \u2200(k, v), \u03b7kv = 1.\nFor both Wikipedia and Nature, we set the parameters in SVI according to the values of the parameters described in Table 1 of [3] (minibatch size 4,096, number of documents D correctly set in advance, step size parameters \u03ba = 0.5 and \u03c40 = 64). We give single-thread SDA-Bayes and SSU the same minibatch size. Performance and timing results are shown in Table 1, where we can see that SVI and single-thread SDA-Bayes have comparable performance, while SSU performs much worse. SVI is faster than single-thread SDA-Bayes.\nFull SDA-Bayes improves performance and run time. We handicap SDABayes in the above comparisons by utilizing just a single thread. In Table 1, we also report performance of SDA-Bayes with 32 threads and a minibatch size of 256.\nFig. 2 shows the performance of SDA-Bayes when we run with {1, 2, 4, 8, 16, 32} threads. To generate these figures, we use a minibatch size of 512 for Wikipedia and 128 for Nature. We can see from the figures that log predictive probability improves, and run time decreases, as the number of threads increase. We also tried a parallel version of the algorithm (synchronous); Fig. 2 indicates that most of the speedup and performance improvement come from parallelizing\u2014which is theoretically justified by Eq. (3) when A is Bayes rule. Our experiments indicate that our Hogwild!-style asynchrony does not hurt performance. In general, a practitioner might prefer asynchrony since it is more robust to node failures.\nSVI is sensitive to the choice of total data size D. The evaluations above are for a single posterior over D data points. Of greater concern to us in this work is the evaluation of algorithms in the streaming setting. We have seen that SVI is designed to find the posterior for a particular, pre-chosen number of data points D. In practice, when we run SVI on the full data set but change the input value of D in the algorithm, we can see degradations in performance. In particular, we try values of D equal to {0.01, 0.1, 1, 10, 100} times the true D in Fig. 3(a) for the Wikipedia data set and in Fig. 3(d) for the Nature data set.\nA practitioner in the streaming setting will typically not know D in advance, and the figures illustrate that an estimate may not be sufficient. Even in the case where D is known in advance, it is reasonable to imagine a new influx of further data. One might need to run SVI again from the start (and, in so doing, revisit the first data set) to obtain the desired performance.\nSDA-Bayes is less sensitive to minibatch size than SVI is. Like SVI, SDABayes acquires data in minibatches. We see from Figs. 3(b) and 3(e) that performance is affected by minibatch size for both SVI and SDA-Bayes; we try sizes {16, 64, 256, 1024, 4096} for both algorithms. However, performance of SDABayes is roughly the same across all minibatch sizes tried here. By contrast, SVI performance degrades rapidly for smaller minibatch sizes. Not only does this result suggest a sensitivity for SVI that may require careful choices before run time but it is also problematic if learning the model in smaller data increments is desired in a practical setting.\nSVI is sensitive to learning step size. [3, 5] use cross-validation to tune stepsize parameters (\u03c40, \u03ba) in the stochastic gradient descent component of the SVI algorithm. This cross-validation requires multiple runs over the data and thus is not suited to the streaming setting. Figs. 3(c) and 3(f) demonstrate that the parameter choice does indeed affect algorithm performance. In these figures, we keep minibatch size at 4,096 and D at the true training data size.\nWe note that, in the Wikipedia case (Fig. 3(c)), we find better SVI performance\nfor parameters (\u03c40, \u03ba) = (64, 1) than for the reported optimal (\u03c40, \u03ba) = (64, 0.5) in [3]. The former case is comparable to that of 32-SDA in Table 1. The discrepancy may stem from the fact that [3] ran on a subset of 100,000 documents, and here we use the full Wikipedia corpus. Recent work has suggested a way to update (\u03c40, \u03ba) adaptively during an SVI run [18].\nEP is not suited to LDA. Earlier attempts to apply EP to the LDA model in the non-streaming setting have had mixed success, with [19] in particular finding that EP performance can be poor for LDA and, moreover, EP requires \u201cunrealistic intermediate storage requirements.\u201d We found this to also be true in the streaming setting. We were not able to obtain competitive results with EP; based on an 8- thread implementation of SDA-Bayes with an EP primitive2, after over 91 hours on Wikipedia (and 6.7\u00d7 104 data points), log predictive probability had stabilized at around \u22127.95 and, after over 97 hours on Nature (and 9.7 \u00d7 104 data points), log predictive probability had stabilized at around \u22128.02. Although SDA-Bayes with the EP primitive is not effective for LDA, it remains to seen whether this combination may be useful in other domains where EP is known to be effective."}, {"heading": "5 Discussion", "text": "We have introduced SDA-Bayes, a framework for streaming, distributed, asynchronous computation of an approximate Bayesian posterior. Our framework makes streaming updates to the estimated posterior according to a user-specified approximation primitive. We have demonstrated the usefulness of our framework, with variational Bayes as the primitive, by fitting the latent Dirichlet allocation topic model to the Wikipedia and Nature corpora. We have demonstrated the advantages of our algorithm over stochastic variational inference and the sufficient statistics update algorithm, particularly with respect to the key issue of obtaining approximations to posterior probabilities based on the number of documents seen thus far, not posterior probabilities for a fixed number of documents."}, {"heading": "Acknowledgments", "text": "We are grateful to Matt Hoffman for useful discussions and advice. We thank Matt Hoffman, Chong Wang, and John Paisley for generously sharing their code and data. T. Broderick is supported by the Berkeley Fellowship. N. Boyd is supported\n2We chose 8 threads since any fewer was too slow to get results and anything larger created too high of a memory demand on our system.\nby a Hertz Foundation Google Fellowship. A. C. Wilson is supported by the Chancellor\u2019s Fellowship at UC Berkeley. This research is supported in part by NSF CISE Expeditions award CCF-1139158 and DARPA XData Award FA875012-2-0331, and gifts from Amazon Web Services, Google, SAP, Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo!. This material is based upon work supported in part by the Office of Naval Research under contract/grant number N00014-11-1-0688."}, {"heading": "A Expectation Propagation", "text": "Our expectation propagation (EP) algorithm for LDA learns a posterior for both the document-specific topic mixing proportions (\u03b8d)Dd=1 and the topic distributions over words (\u03b2k)Kk=1. By contrast, the algorithm in [14] learns only the former and so is not appropriate to the model in Sec. 3.\nFor consistency, we also follow [14] in making a distinction between token and type word updates, where a token refers to a particular word instance and a type refers to all words with the same vocabulary value. Let C = (wd)Dd=1 denote the set of documents that we observe, and for each word v in the vocabulary, let ndv denote the number of times v appears in document d.\nCollapsed posterior. We begin by collapsing (i.e., integrating out) the word assignments z in the posterior (7) of LDA. We can express the collapsed posterior as\np(\u03b2, \u03b8 | C, \u03b7, \u03b1)\n\u221d [ K\u220f k=1 DirichletV (\u03b2k | \u03b7k) ] \u00b7 D\u220f d=1 [ DirichletK(\u03b8d | \u03b1) \u00b7 V\u220f v=1 ( K\u2211 k=1 \u03b8dk \u03b2kv )ndv] .\nFor each document-word pair (d, v), consider approximating the term \u2211K\nk=1 \u03b8dk\u03b2kv above by [\nK\u220f k=1 DirichletV (\u03b2k | \u03c7kdv + 1V )\n] \u00b7 DirichletK(\u03b8d | \u03b6dv + 1K),\nwhere \u03c7kdv \u2208 RV , \u03b6dv \u2208 RK , and 1M is a vector of all ones of length M . This proposal serves as inspiration for taking the approximating variational distribution for p(\u03b2, \u03b8 | C, \u03b7, \u03b1) to be of the form\nq(\u03b2, \u03b8 | \u03bb, \u03b3) := [ K\u220f k=1 q(\u03b2k | \u03bbk) ] \u00b7 D\u220f d=1 q(\u03b8d | \u03b3d), (9)\nwhere q(\u03b2k | \u03bbk) = Dirichlet(\u03b2k | \u03bbk) and q(\u03b8d | \u03b3d) = Dirichlet(\u03b8d | \u03b3d), with the parameters\n\u03bbk = \u03b7k + D\u2211 d=1 V\u2211 v=1 ndv\u03c7kdv, \u03b3d = \u03b1 + V\u2211 v=1 ndv\u03b6dv, (10)\nand the constraints \u03bbk \u2208 RV+ and \u03b3d \u2208 RK+ for each k and d. We assume this form in the remainder of the analysis and write q(\u03b2, \u03b8 | \u03c7, \u03b6) for q(\u03b2, \u03b8 | \u03bb, \u03b3), where \u03c7 = (\u03c7kdv), \u03b6 = (\u03b6dv).\nOptimization problem. We seek to find the optimal parameters (\u03c7, \u03b6) by minimizing the (reverse) KL divergence:\nmin \u03c7,\u03b6\nKL (p(\u03b2, \u03b8 | C, \u03b7, \u03b1) \u2016 q(\u03b2, \u03b8 | \u03c7, \u03b6)) .\nThis joint minimization problem is not tractable, and the idea of EP is to proceed iteratively by fixing most of the factors in Eq. (9) and minimizing the KL divergence over the parameters related to a single word.\nMore formally, suppose we already have a set of parameters (\u03c7, \u03b6). Consider a document d and word v that occurs in document d (i.e., ndv \u2265 1). We start by removing the component of q related to (d, v) in Eq. (9). Following [7], we subtract out the effect of one occurrence of word v in document d, but at the end of this process we update the distribution on the type level. In doing so, we use the following shorthand for the remaining global parameters:\n\u03bb \\(d,v) k = \u03bbk \u2212 \u03c7kdv = \u03b7k + (ndv \u2212 1)\u03c7kdv + \u2211 (d\u2032,v\u2032):(d\u2032,v\u2032)6=(d,v) nd\u2032v\u2032\u03c7kd\u2032v\u2032\n\u03b3 \\(d,v) d = \u03b3d \u2212 \u03b6dv = \u03b1 + (ndv \u2212 1)\u03b6dv + \u2211 v\u2032:v\u2032 6=v ndv\u2032\u03b6dv\u2032 .\nWe replace this removed part of q by the term \u2211K\nk=1 \u03b8dk\u03b2kv, which corresponds to the contribution of one occurrence of word v in document d to the true posterior p. Call the resulting normalized distribution q\u0303dv, so\nq\u0303dv(\u03b2, \u03b8 | \u03bb\\(d,v), \u03b3\\d, \u03b3\\(d,v)d )\n\u221d [ K\u220f k=1 Dirichlet(\u03b2k | \u03bb\\(d,v)k ) ] \u00b7 [\u220f d\u2032 6=d Dirichlet(\u03b8d\u2032 | \u03b3d\u2032) ]\n\u00b7 Dirichlet(\u03b8d | \u03b3\\(d,v)d ) \u00b7 K\u2211 k=1 \u03b8dk \u03b2kv.\nWe obtain an improved estimate of the posterior q by updating the parameters from (\u03bb, \u03b3) to (\u03bb\u0302, \u03b3\u0302), where\n(\u03bb\u0302, \u03b3\u0302) = arg min \u03bb\u2032,\u03b3\u2032\nKL ( q\u0303dv(\u03b2, \u03b8 | \u03bb\\(d,v), \u03b3\\d, \u03b3\\(d,v)d ) \u2016 q(\u03b2, \u03b8 | \u03bb \u2032, \u03b3\u2032) ) . (11)\nSolution to the optimization problem. First, note that for d\u2032 : d\u2032 6= d, we have \u03b3\u0302d\u2032 = \u03b3d\u2032 .\nNow consider the index d chosen on this iteration. Since \u03b2 and \u03b8 are Dirichletdistributed under q, the minimization problem in Eq. (11) reduces to solving the moment-matching equations [7, 20]\nEq\u0303dv [log \u03b2ku] = E\u03bb\u0302k [log \u03b2ku] for 1 \u2264 k \u2264 K, 1 \u2264 u \u2264 V, Eq\u0303dv [log \u03b8dk] = E\u03b3\u0302d [log \u03b8dk] for 1 \u2264 k \u2264 K.\nThese can be solved via Newton\u2019s method though [7] recommends solving exactly for the first and \u201caverage second\u201d moments of \u03b2ku and \u03b8dk, respectively, instead. We choose the latter approach for consistency with [7]; our own experiments also suggested taking the approach of [7] was faster than Newton\u2019s method with no noticeable performance loss. The resulting moment updates are\n\u03bb\u0302ku =\n\u2211V y=1 ( Eq\u0303dv [\u03b22ky]\u2212 Eq\u0303dv [\u03b2ky] )\u2211V y=1 ( Eq\u0303dv [\u03b2ky]2 \u2212 Eq\u0303dv [\u03b22ky]\n) \u00b7 Eq\u0303dv [\u03b2ku] (12) \u03b3\u0302dk = \u2211K j=1 ( Eq\u0303dv [\u03b82dj]\u2212 Eq\u0303d,n [\u03b8dj]\n)\u2211K j=1 ( Eq\u0303dv [\u03b8dj]2 \u2212 Eq\u0303dv [\u03b82dj]\n) \u00b7 Eq\u0303dv [\u03b8dk]. (13) We then set (\u03c7kdv)Kk=1 and \u03b6dv such that the new global parameters (\u03bbk) K k=1 and \u03b3d are equal to the optimal parameters (\u03bb\u0302k)Kk=1 and \u03b3\u0302d. The resulting algorithm is presented as Alg. 4.\nThe results in the main text (Sec. 4.2) are reported for Alg. 4. We also tried a slightly modified EP algorithm that makes token-level updates to parameter values, rather than type-level updates. This modified version iterates through each word placeholder in document d; that is, through pairs (d, n) rather than pairs (d, v) corresponding to word values. Since there are always at least as many (d, n) pairs as (d, v) pairs with ndv \u2265 1 (and usually many more of the former), the modified algorithm requires many more iterations. In practice, we find better experimental performance for the modified EP algorithm in terms of log predictive probability as a function of number of data points in the training set seen so far: e.g., leveling off at about \u22127.96 for Nature vs. \u22128.02. However, the modified algorithm is also much slower, and still returns much worse results than SDA-Bayes or SVI, so we do not report these results in the main text.\nAlgorithm 4: EP for LDA Input: Data C = (wd)Dd=1; hyperparameters \u03b7, \u03b1 Output: \u03bb Initialize \u2200(k, d, v), \u03c7kdv \u2190 0 and \u03b6dv \u2190 0 while (\u03c7, \u03b6) not converged do\nforeach (d, v) with ndv \u2265 1 do /* Variational distribution without the word\ntoken (d, v) */ \u2200k, \u03bb\\(d,v)k \u2190 \u03b7k + (ndv \u2212 1)\u03c7kdv + \u2211 (d\u2032,v\u2032)6=(d,v) nd\u2032v\u2032\u03c7kd\u2032v\u2032 \u03b3 \\(d,v) d \u2190 \u03b1 + (ndv \u2212 1)\u03b6dv + \u2211 v\u2032 6=v ndv\u2032\u03b6dv\u2032 If any of \u03bb\\(d,v)ku or \u03b3 \\(d,v) dk are non-positive, skip updating this (d, v) /* Variational parameters from moment-matching */ \u2200(k, u), compute \u03bb\u0302ku from Eq. (12) \u2200k, compute \u03b3\u0302dk from Eq. (13) /* Type-level updates to parameter values */\n\u2200k, \u03c7kdv \u2190 n\u22121dv ( \u03bb\u0302k \u2212 \u03bb\\(d,v)k ) + ( 1\u2212 n\u22121dv ) \u03c7kdv\n\u03b6dv \u2190 n\u22121dv ( \u03b3\u0302d \u2212 \u03b3\\(d,v)d ) + ( 1\u2212 n\u22121dv ) \u03b6dv Other \u03c7, \u03b6 remain unchanged\n/* Global variational parameters */ \u2200k, \u03bbk \u2190 \u03b7k + \u2211D\nd=1 \u2211V v=1 ndv\u03c7kdv"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We present SDA-Bayes, a framework for (S)treaming, (D)istributed,<lb>(A)synchronous computation of a Bayesian posterior. The framework makes<lb>streaming updates to the estimated posterior according to a user-specified<lb>approximation primitive function. We demonstrate the usefulness of our<lb>framework, with variational Bayes (VB) as the primitive, by fitting the la-<lb>tent Dirichlet allocation model to two large-scale document collections. We<lb>demonstrate the advantages of our algorithm over stochastic variational in-<lb>ference (SVI), both in the single-pass setting SVI was designed for and in<lb>the streaming setting, to which SVI does not apply.", "creator": "LaTeX with hyperref package"}}}