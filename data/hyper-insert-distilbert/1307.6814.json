{"id": "1307.6814", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2013", "title": "A Propound Method for the Improvement of Cluster Quality", "abstract": "in this paper knockout refinement algorithm ( kb kra ) is proposed to refine original clusters obtained by constantly applying analytical som search and dynamic k - means clustering algorithms. hence kra predict algorithm is based on query contingency table concepts. metrics prices are indirectly computed for the original and refined clusters. precise quality ranges of original and appropriately refined new clusters scores are independently compared measured in suitable terms characteristic of metrics. the proposed prediction algorithm ( kra ) is tested systematically in the educational topic domain and results measurements show consistently that it clearly generates many better query quality clusters yield in specific terms of improved metric values.", "histories": [["v1", "Thu, 25 Jul 2013 17:07:39 GMT  (220kb)", "http://arxiv.org/abs/1307.6814v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shveta kundra bhatia", "v s dixit"], "accepted": false, "id": "1307.6814"}, "pdf": {"name": "1307.6814.pdf", "metadata": {"source": "CRF", "title": "A Propound Method for the Improvement of Cluster Quality", "authors": ["Shveta Kundra Bhatia", "V.S. Dixit"], "emails": ["shvetakundra@gmail.com", "veersaindixit@rediffmail.com"], "sections": [{"heading": "1. Introduction", "text": "Clustering or grouping of users or sessions into logically meaningful clusters is a well studied branch of research. Clustering has been formulated in various ways by using algorithms such as K-Means, SOM etc. K-Means partitions the objects into clusters by minimizing the sum of squared distances between the objects and the centroids of the clusters. Many algorithms have been proposed to improve implementation of the K-Means algorithm.\n1.1 Literature Review\nBentley (1975) suggested kd-trees to improve triangle inequality to enhance K-Means. Bradley and Fayyad (1998) presented a procedure for computing a refined starting condition that is based on a technique for estimating the modes of a distribution to converge to a better local minimum that can be coupled with scalable clustering algorithms. A Kernel-Means algorithm was established by Scholkops in 1998 that maps data points from the input space to a feature space through a non linear transformation minimizing the clustering error in feature space. Samet (1999) devised R-Trees which were not appropriate for problems with high dimensions.\nA partial distance algorithm (PDA) had been proposed in 2000 which allows termination of the distance calculation at an early stage for better results. In 2002 Dhillon suggested refining clusters in high dimensional text data by combining the first variation principal and spherical KMeans. Elkan in 2003 suggested the use of triangle inequality to accelerate K-Means. Refinement of clusters from K-Means with Ant Colony Optimization was suggested by Mary, Raja in 2005. In 2010 refinement of web usage data clustering from K-Means with Genetic Algorithm was suggested by N. Sujatha and K. Iyakutty and in 2011 by Prabha, Saranya. Various initialization methods such as Binary Splitting was reviewed together with several other initialization techniques and compared with KKZ. Multilevel refinement schemes were used for refining and improving the clusters produced by hierarchical agglomerative clustering by Karypis. Self Organizing Feature Maps (SOM) is a type of Artificial Neural Network that is trained using Unsupervised Learning which helps in visualizing high dimensional data into low dimensional data. Researchers have contributed to the improvement of various SOM quality measures such as quantization error, topographic product, topographic error, trustworthiness and neighborhood preservation. The researchers have contributed only to accelerate the algorithms; there is not much contribution in refinement of clusters. In this paper we try to refine the original clusters generated by SOM and K-Means algorithms. We proposed a Knockout Refinement Algorithm (KRA) to refine original clusters using a contingency table to compute dissimilarity among sessions in clusters and eliminating sessions with a higher count of dissimilarity. We then calculate the Davies Bouldin (DB) index, Dunn\u2019s Index, Precision, Recall and F-Measure for the original and refined clusters.\n1.2 Standard K-Means Algorithm\nK-means works using the following steps: 1. Place K objects points into the space that are to\nbe clustered by choosing the initial value of K. Object points represent centroids of initial groups chosen.\n2. Assign each object point to the group that has the closest Centroids.\n3. Re-compute the positions of the K centroids and continue till all object points have been assigned.\n4. Repeat Steps 2 and 3 until the centroids do not change. This produces a separation of the object points into groups from which the metric to be minimized can be calculated.\nThe algorithm aims to minimize an objective function as in Eq. (1)\nJ = \u2211 \u2211 ||xi(j)i 1j 1 \u2212 cj||2 (1) Where, \u2551 ( ) \u2212 \u25512 is a chosen distance measure between a data point and the cluster centre. It indicates the distance of the n data points from their respective cluster centers.\n1.3 Standard SOM Algorithm\n1. Assign random values to the weight vectors of a neuron. 2. Provide an input vector to the network. 3. Traverse each node in the network\na) Find similarity between the input vector and the network\u2019s node\u2019s weight vector using Euclidean Distance.\nb) Find the node that produces the smallest distance which is assigned as the Best Matching Unit (BMU).\n4. Update the nodes in the neighborhood of the BMU by changing the weights using Eq. (2):\n( + ) = ( ) + ( ) ( )( ( ) \u2212 ( )) (2) Where,\n t keeps an account of the iteration number  \u03bb is the iteration range  Wv is the current weight vector  D is the target input  \u0398(t) is the Gaussian neighborhood function  \u03b1(t) is learning rate due to time\n5. Increment t and repeat from step 2 while t< \u03bb.\n1.4 Metrics\n1.4.1 Davies Bouldin Index\nThis index aims to identify sets of clusters that are compact and well separated. The Davies-Bouldin index is defined as in Eq. (3):\n= / \u2211 max [ ( ) ( )( , ) ]1 (3) Were K denotes the number of clusters, , are cluster labels, iam(C ) and iam(C ) are the diameters of the clusters C and C , (C , C ) is the average distance between the clusters. Smaller values of average similarity between each cluster and its most similar one indicate a \u201cbetter\u201d clustering solution.\n1.4.2 Dunn\u2019s Index\nThis index aims at expecting a large distance between the clusters and an expected small diameter of the cluster. The index is defined as in Eq. (4):\n= m n 1\u2026.. m n 1\u2026\u2026. ( , ) max 1\u2026\u2026 ( ) (4) Here ( , ) is to compute the dissimilarity between two clusters defined as in Eq. (5):\n( , ) = m n \u2208 , \u2208 ( , ) (5) And diam(c) is the diameter of the cluster that defines the maximum distance between two points in a cluster. A large value of Dunn\u2019s index indicates compact and well separated clusters.\n1.4.3 F-Measure, Precision And Recall\nF-measure combines the precision and recall concepts from information retrieval. We then calculate the recall and precision of that cluster for each class as: ll( , ) = x / x\nAndon( , ) = x / x Were x is the number of objects of class that are in cluster , x is the number of objects in cluster , and x , is the number of objects in class . Precision and Recall are measures that help to evaluate the quality of a set of retrieved documents. The \u2013 of cluster and class is given by the following Eq. (6):\nF ( , ) = 2 * ll( , ) ( , ) / ( , ) + ( , ) (6)\nThe \u2013 values are within the interval [0, 1] and larger values indicate higher clustering quality."}, {"heading": "2. Process Description and Experiments", "text": "The raw web log file we used for the experiment contained 5999 web requests that can be found at http://www.vtsns.edu.rs/maja/vtsnsNov16 containing information like date/time of request, hit type, page, hostname, referrer, server domain, authenticated user, server response, page views, size etc. For preparing the web log data for the mining process, it needs to be cleared of irrelevant requests; and transformed to a format that can be fed into the clustering algorithm. For Pre-Processing and creation of sessions a tool called Sawmill (Version 7.0), developed by Flower fire Inc. has been used. Sawmill computes session information by tracking the page, date/time, and visitor id for each page view. When a session view is requested, it processes all of these page views at the time of the request. Sawmill groups the hits into initial sessions based on the visitor id by assuming that each visitor contributes to a session. In the next step sorting by time is performed for a click-by-click record of each visitor. A session timeout interval of 30 minutes is considered for generating final sessions and sessions longer than 2 hours are eliminated. Using the Sawmill tool on our web log data led to the creation of sessions and 110\nunique URLs. 72.9% of the total sessions were exported into a .csv format with the help of scripts in tcl language as rest of the sessions had only either one or two page views.\nFurther we optimized our matrix and 59.1% of the sessions and 43 unique URLs were used for experimentation. The optimization was performed on the basis of sessions having less than 3 page views and pages that were viewed 5 or less than 5 times have been removed. The optimized matrix was used for clustering using the Self-Organizing Feature Maps and K-Means algorithms. We used the Spice-SOM tool and SPSS software for implementation of the respective algorithms. Applying the two algorithms we can see that clusters with similarity among sessions have been obtained and can be used for prediction of pages to a user of similar interests. Clusters of sizes 10, 15 and 20 were generated using both the techniques of K-Means and SOM. Apply KRA to refine the original clusters. The quality of obtained clusters is evaluated using the Davies Bouldin and Dunn\u2019s quality measure along with external quality measures such as Precision, Recall and F-Measure. The clusters are listed as Original Clusters (OC) on which we shall apply our proposed Knockout Refinement Algorithm (KRA)."}, {"heading": "3. Proposed Knockout Refinement Algorithm (KRA)", "text": "Where, Dissimilarity can be computed using contingency table which is a 2 by 2 matrix between two sessions.\nWhere q is the number of variables that equal 1 for both i and j sessions, r is the number of variables that equal 1 for session i but that are equal to 0 for session j, s is the number of variables that equal 0 for session i and equal 1 for session j and t is the number of variables that equal 0 for sessions i and j. In this case the factor t is unimportant and is ignored to compute the dissimilarity. The dissimilarity for all sessions is computed as in equation 7:\n, = \u2211 + + +1\u2260 (7) The above computation is applied to every pair of sessions that generates a Symmetric Dissimilarity Matrix (SDM) as follows:\nS1 S2 S3 S4 S1 0 d(S1, S2) d(S1,S3) d(S1,S4) S2 d(S2, S1) 0 d(S2, S3) d(S2,S4) S3 d(S3, S1) d(S3,S2) 0 d(S3,S4) S4 d(S4, S1) d(S4, S2) d(S4, S3) 0\nFor refinement using the above Symmetric Dissimilarity Matrix (SDM) and using a threshold value of 0.3 session pairs are counted and those sessions are removed from the cluster whose count values are greater than 2. Performing the above computation we get refined clusters."}, {"heading": "4. Outcome", "text": "4.1 Results for Davies Bouldin Index\nThe results for comparison of DB index for original and refined clusters are as follows:"}, {"heading": "DAVIES BOULDIN INDEX (K-MEANS)", "text": ""}, {"heading": "DAVIES BOULDIN INDEX (SOM)", "text": "Session i\nSession j 1 0 Sum\n1 q r q + r\n0 s t s + t\nSum q + s r + t\nFig 4: Comparison of DB Index for K-Means Algorithm\n4.2 Results For Dunn\u2019s Index\nThe results for comparison of Dunn\u2019s index for original and refined clusters are as follows:\nFig 6: Comparison of Dunn\u2019s Index for SOM Algorithm\n4.3 Results For Precision, Recall And F-Measure"}, {"heading": "K-MEANS(10 Clusters)", "text": ""}, {"heading": "SOM(10 Clusters)", "text": ""}, {"heading": "K-MEANS(15 Clusters)", "text": "0 0.5\n1 1.5\n2 2.5\n3\n10 15 20\n0\nNumber of Clusters\nORIGINAL CLUSTERS\nREFINED CLUSTERS\n0 0.05\n0.1 0.15\n0.2 0.25\n0.3\n10 15 20\nD un\nn' s\nIn de\nx\nNumber of Clusters\nORIGINAL CLUSTERS\nREFINED CLUSTERS"}, {"heading": "K-MEANS(20 Clusters)", "text": ""}, {"heading": "SOM(20 Clusters)", "text": ""}, {"heading": "5. CONCLUSION", "text": "The proposed algorithm tested on the web log data shows that refined clusters lead to an improved Davies Bouldin and Dunn\u2019s Index values; external quality measures such as Precision, Recall and F-Measure have improved for refined clusters as compared to the original clusters. The proposed algorithm is scalable and can be coupled with clustering algorithms to address any other web log data sets. Note that the performances of clustering algorithms are found to be data dependent.\nREFERENCES\n[1] Arun Prabha K., R.Saranya. Refinement of K-Means Clustering Using Genetic Algorithm; Journal of Computer Applications (JCA)ISSN: 0974-1925, Volume IV, Issue 2, 2011.\n[2] Bei C., and Gray, R., An Improvement of the Minimum Distortion Encoding Algorithm for Vector Quantization. IEEE Transactions on Communications, 33 (10): 1132-1133, 1985.\n[3] Bentley J., Multidimensional Binary Search Trees Used for Associative Searching. ACM, 18 (9): 509-517, 1975.\n[4] Bradley, P.S., U. Fayyad, and C. Reina, \u201cScaling Clustering Algorithms to Large Databases\u201d, Proc. 4th International Conf. on Knowledge Discovery and Data Mining (KDD-98). AAAI Press, Aug. 1998.\n[5] Britos, P., Dami\u00e1n Martinelli, Hernan Merlino, Ram\u00f3n Garc\u00eda-Mart\u00ednez Web Usage Mining Using Self Organized Maps. IJCSNS International Journal of Computer Science and Network Security, VOL.7 No.6, June 2007.\n[6] Cheng D., Gersho B., Ramamurthi Y., and Shoham Y., 1984. Fast Search Algorithms for Vector Quantization and Pattern Recognition. Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing, 1, pp:1-9, 1984.\n[7] Dhillon, I.S., Y. Guan, and J. Kogan. Refining Clusters in High-dimensional Text Data.UTCS Technical Report #TR-0203, January 2002.\n[8] Dhillon,I.S., J. Fan, and Y. Guan. Efficient clustering of very large document collections. In R. Grossman, C. Kamath, P. Kegelmeyer, V. Kumar, and R. Namburu, editors, Data Mining for Scientific and Engineering Applications, pages 357\u2013381. Kluwer Academic Publishers, 2001.\n[9] Elkan, C., Using the Triangle Inequality to Accelerate kMeans. Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003), pp. 609-616, 2003.\n[10] Fayyad, U., D. Haussler, and P. Stolorz. \u201cMining Science Data.\u201d Communications of the ACM 39(11) 1996.\n[11] Greenacre, M., Clustering the Rows and Columns of a Contingency Table. Journal of Classification, 5(1):39\u201351, 1988.\n[12] Hanan Ettaher Dagez &Mhd Sapiyan Baba, \u201cApplying Neural Network Technology in Qualitative Research for Extracting Learning Style to Improve E-Learning Environment, The IEEE International Conference, 978-1-4244-2328-6/08 2008.\n[13] Hjaltason, R. and Samet H., Distance Browsing in Spatial Databases. ACM Transactions on Database Systems, 24 (2): 26- 42, 1999.\n[14] Hossain, M.S., S. Tadepalli, L. T. Watson, I. Davidson, R. F. Helm, and N. Ramakrishnan. Unifying Dependent Clustering and Disparate Clustering for Non homogeneous Data. In KDD \u201910, pages 593\u2013602, 2010.\n[15] Immaculate Mary C., Dr. S.V. Kasmir Raja; Refinement Of Clusters from K-Means With Ant Colony Optimization; Journal of Theoretical and Applied Information Technology 2005 - 2009 JATIT.\n[16] Jain, A. K. and R. C. Dubes. Algorithms for Clustering Data. Prentice-Hall, Englewood Cliffs, NJ, 1988.\n[17] Jain, A.K., Murty, M.N., and Flynn, P.J. 1999. Data clustering: A review. ACM Computing Surveys, 31(3):264\u2013323.\n[18] Karypis George, Eui-Hong (Sam) Han, and Vipin Kumar;Multilevel Refinement for Hierarchical Clustering. Department of Computer Science & Engineering Army HPC Research Center.\n[19] Kate A. Smith and Alan Ng. Web page clustering using a Self-organizing map of user navigation patterns. Decision Support Systems, 35(2):245\u2013256, 2003.\n[20] Katsavounidis, I.; Jay Kuo, C.-C.; Zhen Zhang; A new initialization technique for generalized Lloyd iteration; Signal Processing Letters, IEEE , Oct. 1994,Volume: 1 Issue:10 , 144 \u2013 146.\n[21] Kohonen, T., S. Kaski, K. Lagus, J. Salojarvi, J. Honkela, V. Paatero and A. Saarela. Self-organization of a massive document collection. IEEE Transactions on Neural Networks, 11(3):574\u2013 585, 2000.\n[22] Kosala, R., H. Blockeel, Web Mining Research: A Survey, ACM SIGKKD Explorations, vol. 2(1), July 2000.\n[23] Linde, Y.; Buzo, A.; Gray, R.; An Algorithm for Vector Quantizer Design ; Communications, IEEE Transactions on ;Jan 1980 ;Volume: 28 Issue:1,84 \u2013 95.\n[24] Nadif, M., and G. Govaert. Block Clustering of Contingency Table and Mixture Model. In IDA \u201905, pages 249\u2013259, 2005.\n[25] Pelleg D., and Moore A., Accelerating exact k-means algorithm with geometric reasoning. Proceedings of the fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, pp. 727-734, 1999.\n[26] Proietti, G. and Faloutsos C., Analysis of Range Queries and Self-spatial Join Queries on Real Region Datasets Stored using an R-tree. IEEE Transactions on Knowledge and Data Engineering, 5 (12): 751-762, 2000.\n[27] Scholkopf B., Smola J., and Muller R., Nonlinear component analysis as a kernel eigenvalue problem,\u201d Neural Comput., 10(5):1299\u20131319, 1998.\n[28] Shveta Kundra Bhatia, Harita Mehta and Veer Sain Dixit; Aggregate Profiling for Recommendation of web pages using SOM and K-Means Clustering Techniques; International Journal of Computer Applications. Volume 36 - Number 9 Year of Publication: 2011.\n[29] Sinkkonen, J., S. Kaski, and J. Nikkil\u00a8a. Discriminative Clustering: Optimal Contingency Tables by Learning Metrics. In ECML \u201902, pages 418\u2013430, 2002.\n[30] Sujatha, N., K. Iyakutty; Refinement of Web usage Data Clustering from K-means with Genetic Algorithm; European Journal of Scientific Research ISSN 1450-216X Vol.42 No.3 (2010), pp.478-490.\n[31] Tsuyoshi Murata and Kota Saito \u201cExtracting Users Interests from Web Log Data\u201d, Proceedings of the 2006 IEEE/WIC/ACM International Conference of Web Intelligence (WI 2006 Main Conference Proceedings) (WI\u201906) 2006 IEEE.\n[32] Xu R., and Wunsch D., Survey of clustering algorithms. IEEE Trans. Neural Networks, 16 (3):645-678, 2005.\n[33] Han and Kamber, Concepts of Data Mining; Elsevier Publication.2006\n[34] Written by Cao Thang in Soft Intelligent Laboratory, Ritsumeikan University, 2003\n[35] http://www.sawmill.net\nShveta Kundra Bhatia is working as an Assistant Professor in the Department Of Computer Science, Swami Sharaddhanand College, University of Delhi. Her research area is Web Usage Mining and is currently pursuing PhD under Dr. V.S. Dixit from Department of Computer Science, University of Delhi.\nDr. V. S. Dixit is working in the Department Of Computer Science, Atma Ram Sanatan Dharam College, University of Delhi. His research area is Queuing theory, Peer to Peer systems, Web Usage Mining and Web Recommender Systems. He is currently engaged in doing the research. He is Life member of IETE."}], "references": [{"title": "R.Saranya. Refinement of K-Means Clustering Using Genetic Algorithm; Journal of Computer Applications (JCA)ISSN: 0974-1925", "author": ["K. Arun Prabha"], "venue": "Volume IV, Issue", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "An Improvement of the Minimum Distortion Encoding Algorithm for Vector Quantization", "author": ["Bei C", "R. Gray"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1985}, {"title": "Multidimensional Binary Search Trees Used for Associative Searching", "author": ["J. Bentley"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Scaling Clustering Algorithms to Large Databases", "author": ["P.S. Bradley", "U. Fayyad", "C. Reina"], "venue": "Proc. 4th International Conf. on Knowledge Discovery and Data Mining (KDD-98)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Web Usage Mining Using Self Organized Maps", "author": ["P. Britos", "Dami\u00e1n Martinelli", "Hernan Merlino", "Ram\u00f3n Garc\u00eda-Mart\u00ednez"], "venue": "IJCSNS International Journal of Computer Science and Network Security,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Fast Search Algorithms for Vector Quantization and Pattern Recognition", "author": ["D. Cheng", "B. Gersho", "Y. Ramamurthi", "Y. Shoham"], "venue": "Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1984}, {"title": "Refining Clusters in High-dimensional Text Data.UTCS", "author": ["I.S. Dhillon", "Y. Guan", "J. Kogan"], "venue": "Technical Report #TR-02-", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Efficient clustering of very large document collections", "author": ["I.S. Dhillon", "J. Fan", "Y. Guan"], "venue": "Data Mining for Scientific and Engineering Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Using the Triangle Inequality to Accelerate k- Means", "author": ["C. Elkan"], "venue": "Proceedings of the Twentieth International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Mining Science Data.", "author": ["U. Fayyad", "D. Haussler", "P. Stolorz"], "venue": "Communications of the ACM", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Clustering the Rows and Columns of a Contingency Table", "author": ["M. Greenacre"], "venue": "Journal of Classification,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "Applying Neural Network Technology in Qualitative Research for Extracting Learning Style to Improve E-Learning Environment", "author": ["Hanan Ettaher Dagez", "Mhd Sapiyan Baba"], "venue": "The IEEE International Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Unifying Dependent Clustering and Disparate Clustering for Non homogeneous Data", "author": ["M.S. Hossain", "S. Tadepalli", "L.T. Watson", "I. Davidson", "R.F. Helm", "N. Ramakrishnan"], "venue": "In KDD", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Refinement Of Clusters from K-Means With Ant Colony Optimization; Journal of Theoretical and Applied Information Technology", "author": ["Immaculate Mary C", "Dr. S.V. Kasmir Raja"], "venue": "JATIT", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1988}, {"title": "Data clustering: A review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Web page clustering using a Self-organizing map of user navigation patterns", "author": ["Kate A. Smith", "Alan Ng"], "venue": "Decision Support Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "A new initialization technique for generalized Lloyd iteration", "author": ["I. Katsavounidis", "C.-C. Jay Kuo", "Zhen Zhang"], "venue": "Signal Processing Letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Self-organization of a massive document collection", "author": ["T. Kohonen", "S. Kaski", "K. Lagus", "J. Salojarvi", "J. Honkela", "V. Paatero", "A. Saarela"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Web Mining Research: A Survey", "author": ["R. Kosala", "H. Blockeel"], "venue": "ACM SIGKKD Explorations,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "An Algorithm for Vector Quantizer Design", "author": ["Y. Linde", "A. Buzo", "R. Gray"], "venue": "IEEE Transactions on ;Jan", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1980}, {"title": "Block Clustering of Contingency Table and Mixture Model", "author": ["M. Nadif", "G. Govaert"], "venue": "In IDA", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Accelerating exact k-means algorithm with geometric reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "Proceedings of the fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Analysis of Range Queries and Self-spatial Join Queries on Real Region Datasets Stored using an R-tree", "author": ["G. Proietti", "Faloutsos C"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem,", "author": ["B. Scholkopf", "J. Smola", "R. Muller"], "venue": "Neural Comput.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Aggregate Profiling for Recommendation of web pages using SOM and K-Means Clustering", "author": ["Shveta Kundra Bhatia", "Harita Mehta", "Veer Sain Dixit"], "venue": "Techniques; International Journal of Computer Applications. Volume", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Discriminative Clustering: Optimal Contingency Tables by Learning Metrics", "author": ["J. Sinkkonen", "S. Kaski", "J. Nikkil \u0308a"], "venue": "In ECML", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Iyakutty; Refinement of Web usage Data Clustering from K-means with Genetic Algorithm", "author": ["N. Sujatha"], "venue": "European Journal of Scientific Research ISSN 1450-216X Vol.42", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Extracting Users Interests from Web Log Data", "author": ["Tsuyoshi Murata", "Kota Saito"], "venue": "Proceedings of the 2006 IEEE/WIC/ACM International Conference of Web Intelligence (WI 2006 Main Conference Proceedings)", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "The \u2013 values are within the interval [0, 1] and larger values indicate higher clustering quality.", "startOffset": 37, "endOffset": 43}], "year": 2013, "abstractText": "In this paper Knockout Refinement Algorithm (KRA) is proposed to refine original clusters obtained by applying SOM and K-Means clustering algorithms. KRA Algorithm is based on Contingency Table concepts. Metrics are computed for the Original and Refined Clusters. Quality of Original and Refined Clusters are compared in terms of metrics. The proposed algorithm (KRA) is tested in the educational domain and results show that it generates better quality clusters in terms of improved metric values.", "creator": "easyPDF SDK 7.0"}}}