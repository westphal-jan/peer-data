{"id": "1308.3558", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2013", "title": "Fast Stochastic Alternating Direction Method of Multipliers", "abstract": "in doing this paper, thus we proudly propose a revolutionary new stochastic alternating direction transformation method independent of multipliers ( admm ) algorithm, of which each incrementally approximates under the full function gradient in the newly linearized process admm optimal formulation. now besides generally having a low per - iteration complexity as existing distributed stochastic admm algorithms, the theoretically proposed rt algorithm improves slowly the efficient convergence comparison rate on convex design problems from $ [ o ( \\ frac \u00d7 1 { \\ sqrt { t } } ) $ to $ o ( \\ frac not 1 + t ) $, where $ t $ is twice the base number element of iterations. this matches essentially the convergence rate limit of the batch stationary admm conversion algorithm, considerably but without fulfilling the computing need merely to visit all the empirical samples recorded in once each iteration. experiments modeling on the conditional graph - matching guided fused lasso algorithm demonstrate that the new fusion algorithm development is significantly faster than state - of - / the - magnitude art blended stochastic simulation and batch admm algorithms.", "histories": [["v1", "Fri, 16 Aug 2013 05:48:29 GMT  (1625kb,D)", "http://arxiv.org/abs/1308.3558v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["wenliang zhong", "james tin-yau kwok"], "accepted": true, "id": "1308.3558"}, "pdf": {"name": "1308.3558.pdf", "metadata": {"source": "CRF", "title": "Fast Stochastic Alternating Direction Method of Multipliers", "authors": ["Leon Wenliang Zhong", "James T. Kwok"], "emails": [], "sections": [{"heading": null, "text": "( 1\u221a T ) to O ( 1 T ) , where T is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.\nIndex Terms\u2014multitask learning, feature-wise clustering, regression\nI. INTRODUCTION\nThe alternating direction method of multipliers (ADMM) [1], [2], [3] considers problems of the form\nmin x,y\n\u03a6(x, y) \u2261 \u03c6(x) + \u03c8(y) : Ax+By = c, (1)\nwhere \u03c6, \u03c8 are convex functions, and A,B (resp. c) are constant matrices (resp. vector) of appropriate sizes. Because of the flexibility in splitting the objective into \u03c6(x) and \u03c8(y), it has been a popular optimization tool in many machine learning, computer vision and data mining applications. For example, on large-scale distributed convex optimization, each \u03c6 or \u03c8 can correspond to an optimization subproblem on the local data, and the constraint Ax + By = c is used to ensure all the local variables reach a global consensus; for regularized risk minimization which will be the focus of this paper, \u03c6 can be used for the empirical loss, \u03c8 for the regularizer, and the constraint for encoding the sparsity pattern of the model parameter. In comparison with other state-of-theart optimization methods such as proximal gradient methods [4], [5], the use of ADMM has been shown to have faster convergence in several difficult structured sparse regularization problems [6].\nExisting works on ADMM often assume that \u03a6(x, y) is deterministic. In the context of regularized risk minimization, this corresponds to batch learning and each iteration needs to visit all the samples. With the proliferation of data-intensive applications, it can quickly become computationally expensive. For example, in using ADMM on the overlapping grouplasso, the matrix computations become costly when both the\nLeon Wenliang Zhong and James T. Kwok are with the Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong.\nnumber of features and data set size are large [7]. To alleviate this problem, the use of stochastic and online techniques have recently drawn a lot of interest. Wang and Banerjee [8] first proposed the online ADMM, which learns from only one sample (or a small mini-batch) at a time. However, in general, each round involves nonlinear optimization and is not computationally appealing. Very recently, three stochastic variants of ADMM are independently proposed [9], [6]. Two are based on the stochastic gradient descent (SGD) [10], while one is based on regularized dual averaging (RDA) [5]. In both cases, the difficult nonlinear optimization problem inherent in the online ADMM is circumvented by linearization, which then allows the resultant iterations in these stochastic variants to be efficiently performed.\nHowever, despite their low per-iteration complexities, these stochastic ADMM algorithms converge at a suboptimal rate compared to their batch counterpart. Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u00b5T ) (where 0 < \u00b5 < 1), respectively [11], [12]. This gap in the convergence rates between stochastic and batch ADMM algorithms is indeed not surprising, as it is also observed between SGD and batch gradient descent in the analogous unconstrained optimization setting [13]. Recently, there have been several attempts on bridging this gap [14], [13], [15]. For example, Re Loux et al. [14] proposed an approach whose per-iteration cost is as low as SGD, but can achieve linear convergence for strongly convex functions.\nAlong this line, we propose in the sequel a novel stochastic algorithm that bridges the O(1/ \u221a T ) vs O(1/T ) gap in convergence rates for ADMM. The new algorithm enjoys the same computational simplicity as existing stochastic ADMM algorithms, but with a much faster convergence rate matching that of its batch counterpart. Experimental results demonstrate that it dramatically outperforms existing stochastic and batch ADMM algorithms.\nNotation. In the sequel, we use \u2016 \u00b7 \u2016 to denote a general norm, and \u2016 \u00b7 \u2016\u2217 be its dual norm (which is defined as \u2016z\u2016\u2217 = supx:\u2016x\u2016\u22641 z\nTx). For a function f , we let f \u2032 be a subgradient in the subdifferential set \u2202f(x) = {g | f(y) \u2265 f(x) + gT (y\u2212 x), \u2200y}. When f is differentiable, we use \u2207f for its gradient. A function f is L-Lipschitz if \u2016f(x)\u2212f(y)\u2016 \u2264 L\u2016x\u2212y\u2016 \u2200x, y. It is L-smooth if \u2016\u2207f(x) \u2212 \u2207f(y)\u2016\u2217 \u2264 L\u2016x \u2212 y\u2016, or equivalently,\nf(y) \u2264 f(x) +\u2207f(x)T (y \u2212 x) + L 2 \u2016x\u2212 y\u20162. (2)\nar X\niv :1\n30 8.\n35 58\nv1 [\ncs .L\nG ]\n1 6\nA ug\n2 01\n3\nMoreover, a function f is \u00b5-strongly convex if f(y) \u2265 f(x)+ gT (y \u2212 x) + \u00b52 \u2016y \u2212 x\u2016 2 for g \u2208 \u2202f(x)."}, {"heading": "II. BATCH AND STOCHASTIC ADMM", "text": "As in the method of multipliers, ADMM starts with the augmented Lagrangian of problem (1):\nL(x, y, u) = \u03c6(x)+\u03c8(y)+\u03b2T (Ax+By\u2212c)+\u03c1 2 \u2016Ax+By\u2212c\u20162, (3) where \u03b2 is the vector of Lagrangian multipliers, and \u03c1 > 0 is a penalty parameter. At the tth iteration, the values of x and y (denoted xt, yt) are updated by minimizing L(x, y, \u03b2) w.r.t. x and y. However, unlike the method of multipliers, these are minimized in an alternating manner, which allows the problem to be more easily decomposed when \u03c6 and \u03c8 are separable. Using the scaled dual variable \u03b1t = \u03b2t/\u03c1, the ADMM update can be expressed as [1]:\nxt+1 \u2190 arg min x\n\u03c6(x) + \u03c1\n2 \u2016Ax+Byt \u2212 c+ \u03b1t\u20162, (4)\nyt+1 \u2190 arg min y\n\u03c8(y) + \u03c1\n2 \u2016Axt+1 +By \u2212 c+ \u03b1t\u20162,(5)\n\u03b1t+1 \u2190 \u03b1t +Axt+1 +Byt+1 \u2212 c. (6)\nIn the context of regularized risk minimization, x denotes the model parameter to be learned. Moreover, we assume that\n\u03c6(x) \u2261 1 n n\u2211 i=1 `i(x) + \u2126(x), (7)\nwhere n is the number of samples, `i(x) denotes sample i\u2019s contribution to the empirical loss, and \u2126(x) is a \u201csimple\u201d regularizer commonly encountered in proximal methods. In other words, \u2126 is chosen such that the proximal step minx 1 2\u2016x\u2212a\u2016\n2+\u2126(x), for some constant a, can be computed efficiently. Subproblem (4) then becomes\nxt+1 \u2190 arg min x\n1\nn n\u2211 i=1 `i(x)+\u2126(x)+ \u03c1 2 \u2016Ax+Byt\u2212c+\u03b1t\u20162.\n(8) When the data set is large, solving (8) can be computationally expensive. To alleviate this problem, Wang and Banerjee [8] proposed the online ADMM that uses only one sample in each iteration. Consider the case where \u2126 = 0. Let the index of the sample selected at iteration t be k(t) \u2208 {1, 2, . . . , n}. Instead of using (8), x is updated as\nxt+1 \u2190 arg min x\n`k(t)(x)+ \u03c1\n2 \u2016Ax+Byt\u2212c+\u03b1t\u20162+\nD(x, xt)\n\u03b7t+1 ,\n(9) where D(x, xt) is a Bregman divergence between x and xt, and \u03b7t \u221d 1\u221at is the stepsize. In general, solving (9) requires the use of nonlinear optimization, making it expensive and less appealing. Very recently, several stochastic versions of ADMM have been independently proposed. Ouyang et al. [9]\nproposed the stochastic ADMM1, which updates x as\nxt+1 \u2190 arg min x `\u2032k(t)(xt) T (x\u2212 xt) +\n\u2016x\u2212 xt\u20162\n2\u03b7t+1\n+ \u03c1\n2 \u2016Ax+Byt \u2212 c+ \u03b1t\u20162 (10)\n=\n( 1\n\u03b7t+1 I + \u03c1ATA )\u22121 \u00b7[\nxt \u03b7t+1 \u2212 `\u2032k(t)(xt)\u2212 \u03c1A T (Byt \u2212 c+ \u03b1t)\n] ,(11)\nwhere `\u2032k(t)(xt) is the (sub)gradient of `k(t) at xt, \u03b7t \u221d 1\u221a t is the stepsize, and I is the identity matrix of appropriate size. The updates for y and \u03b1 are the same as in (5) and (6). For the special case where B = \u2212I and c = 0, Suzuki [6] proposed a similar approach called online proximal gradient descent ADMM (OPG-ADMM), which uses the inexact Uzawa method [16] to further linearize the last term in (10), leading to\nxt+1 \u2190 arg min x (`\u2032k(t)(xt) + \u03c1A T (Axt \u2212 yt + \u03b1t))Tx\n+ \u2016x\u2212 xt\u20162\n2\u03b7t+1 (12) = xt \u2212 \u03b7t+1 [ `\u2032k(t)(xt) + \u03c1A T (Axt \u2212 yt + \u03b1t) ] .(13)\nCompared to (11), it avoids the inversion of 1\u03b7t+1 I + \u03c1A TA which can be computationally expensive when ATA is large. Suzuki [6] also proposed another stochastic variant called RDA-ADMM based on the method of regularized dual averaging (RDA) [5] (again for the special case with B = \u2212I and c = 0), in which x is updated as\nxt+1 \u2190 arg min x (g\u0304t + \u03c1A T (Ax\u0304RDAt \u2212 y\u0304RDAt + \u03b1\u0304RDAt )Tx\n+ \u2016x\u20162\n\u03b7t+1 = \u2212\u03b7t+1 [ g\u0304t + \u03c1A T (Ax\u0304RDAt \u2212 y\u0304RDAt + \u03b1\u0304RDAt ) ] .(14)\nHere, \u03b7t \u221d \u221a t, g\u0304t = 1t \u2211t j=1 ` \u2032 k(j)(xj), x\u0304 RDA t = 1 t \u2211t j=1 xj , y\u0304 RDA t = 1 t \u2211t j=1 yj , and \u03b1\u0304 RDA t = 1 t \u2211t j=1 \u03b1j are averages obtained from the past t iterations. For general convex problems, these online/stochastic ADMM approaches all converge at a rate of O(1/ \u221a T ), w.r.t. either the objective value [6] or a weighted combination of the objective value and feasibility violation [9]. When \u03c6 is further required to be strongly convex, the convergence rate can be improved to O(log T/T ) (except for RDA-ADMM whose convergence rate in this situation is not clear). However, in both cases (general and strongly convex), these convergence rates are inferior to their batch ADMM counterparts, which are O(1/T ) and O(\u00b5T ) (with 0 < \u00b5 < 1), respectively [11], [12], [17]."}, {"heading": "III. STOCHASTIC AVERAGE ADMM (SA-ADMM)", "text": "On comparing the update rules on x for the STOC-ADMM and OPG-ADMM ((10) and (12)) with that of the batch\n1To avoid confusion with other stochastic variants of ADMM, this particular algorithm will be called STOC-ADMM in the sequel.\nADMM (8), one can see that the empirical loss on the whole training set (namely, 1n \u2211n i=1 `i(x)) is replaced by the linear approximation based on one single sample plus a proximal term \u2016x\u2212xt\u2016 2\n2\u03b7t+1 . This follows the standard approach taken by\nSGD. However, as is well-known, while the full gradient descent has linear convergence rate, SGD only achieves sublinear convergence [13]. This agrees with the result in Section II that the existing stochastic versions of ADMM all have slower convergence rates than their batch counterpart.\nRecently, by observing that the training set is indeed finite, it is shown that the convergence rates of stochastic algorithms can be improved to match those of the batch learning algorithms. A pioneering approach along this line is the stochastic average gradient (SAG) [14], which considers the optimization of a strongly convex sum of smooth functions (minx 1n \u2211n i=1 `i(x)). By updating an estimate of the full gradient incrementally in each iteration, its per-iteration time complexity is only as low as SGD, yet surprisingly its convergence rate is linear.\nAnother closely related approach is the minimization by incremental surrogate optimization (MISO) [13], which replaces each `i by some \u201csurrogate\u201d function in an incremental manner similar to SAG. In particular, when each `i is smooth and the so-called \u201cLipschitz gradient surrogate\u201d is used, the resultant update rule is very similar to that of SAG.\nMotivated by these recent stochastic optimization results, we will propose in the following a novel stochastic ADMM algorithm that achieves the same convergence rate as batch ADMM on general convex problems. Unlike SAG or MISO, the proposed algorithm is more general and can be applied to optimization problems with equality constraints, which are naturally handled by ADMM."}, {"heading": "A. Algorithm", "text": "In the following, we assume that the `i in (7) is Lsmooth (e.g., the square loss and logistic loss). As in existing stochastic ADMM approaches [8], [9], [6], y and \u03b1 are still updated by (5), (6), and the key difference is on the update of x (Algorithm 1). First, consider the special case where \u2126 = 0. At iteration t, we randomly choose a sample k(t) uniformly from {1, 2, . . . , n}, and then update x as\nxt+1 \u2190 arg min x P `t (x) + r(x, yt, \u03b1t), (15)\nwhere P `t (x) \u2261 1n \u2211n i=1 `i(x\u03c4i(t))+\u2207`i(x\u03c4i(t))T (x\u2212x\u03c4i(t))+ L 2 \u2016x\u2212x\u03c4i(t)\u2016 2, r(x, y, \u03b1) \u2261 \u03c12\u2016Ax+By\u2212c\u2212\u03b1\u2016 2, and \u03c4i(t) ={\nt i = k(t) \u03c4i(t\u2212 1) otherwise . Hence, as in SAG, out of the n gradient terms in (15), only one of them (which corresponds to sample k(t)) is based on the current iterate xt, while all others are previously-stored gradient values. Moreover, note its similarity with the STOC-ADMM update in (10), which only retains terms related to `k(t), while (15) uses the information from all of {`1, `2, . . . , `n}. Another difference with (10) is that the proximal term in (15) involves a constant L, while (11) requires a time-varying stepsize \u03b7t.\nBy setting the derivative of (15) to zero, we have xt+1 \u2190 (\u03c1ATA+LI)\u22121 [ Lx\u0304t \u2212 \u03c1AT (Byt \u2212 c+ \u03b1t)\u2212\u2207`t ] , (16) where x\u0304t \u2261 1n \u2211n i=1 x\u03c4i(t), and \u2207`t \u2261 1 n \u2211n i=1\u2207`i(x\u03c4i(t)). When the dimension of ATA is manageable, (16) is cheaper than the STOC-ADMM update, as (\u03c1ATA + LI)\u22121 can be pre-computed and stored; whereas \u03b7t in (11) is changing and consequently the matrix inverse there has to be re-computed in every iteration.2 When \u03c1ATA+LI is large, even storing its inverse can be expensive. A technique that has been popularly used in the recent ADMM literature is the inexact Uzawa method [16], which uses (2) to approximate r(x, yt, \u03b1t) by its upper bound:\nr(x, yt, \u03b1t) \u2264 P rt (x) \u2261 r(xt, yt, \u03b1t) +\u2207txrT (x\u2212 xt)\n+ LA 2 \u2016x\u2212 xt\u20162, (17)\nwhere \u2207txr \u2261 \u03c1AT (Axt +Byt \u2212 c+ \u03b1t) and LA is an upper bound on the eigenvalues of \u03c1ATA [12]. Hence, (15) becomes\nxt+1 \u2190 arg min x P `t (x) + P r t (x) (18)\n= Lx\u0304t + LAxt \u2212\n[ \u2207`t +\u2207txr) ] LA + L . (19)\nAnalogous to the discussion between (15) and (10) above, our (19) is also similar to the OPG-ADMM update in (13), except that all the information from {`1, `2, . . . , `n} are now used. Moreover, note that although RDA-ADMM also uses an average of gradients (g\u0304t in (14)), its convergence is still slower than the proposed algorithm (as will be seen in Section III-C).\nWhen \u2126 6= 0, it can be added back to (15), leading to\nxt+1 \u2190 arg min x P `t (x) + \u2126(x) + r(x, yt, \u03b1t). (20)\nIn general, it is easier to solve with the inexact Uzawa simplification. The update then becomes\nxt+1 \u2190 arg min x P `t (x) + P r t (x) + \u2126(x) (21)\n= arg min x\n1\n2 \u2225\u2225\u2225\u2225\u2225x\u2212 Lx\u0304t + LAxt \u2212 [ \u2207`t +\u2207txr ] LA + L \u2225\u2225\u2225\u2225\u2225 2\n+ \u2126(x)\nLA + L . (22)\nThis is the standard proximal step popularly used in optimization problems with structured sparsity [18], [19]. As is well-known, it can be efficiently computed as \u2126 is assumed \u201csimple\u201d (e.g., \u2126(x) = \u2016x\u20161, \u2016x\u20162, \u2016x\u2016\u221e and various mixed norms [4])."}, {"heading": "B. Discussion", "text": "In the special case where \u03c8 = 0, (1) reduces to minx \u03c6(x) and the feasibility violation r(x, yt, \u03b1t) can be dropped. The\n2Ouyang et al. [9] claimed that \u03b7t+1 in (11) can be fixed at \u03b7T , though the proof is missing. Moreover, as \u03b7t is decreasing, \u03b7T is the most conservative (smallest) stepsize. Hence, using \u03b7T in all iterations may lead to very slow convergence in practice.\nAlgorithm 1 Stochastic average alternating direction method of multipliers (SA-ADMM).\n1: Initialize: x0, y0, \u03b10 and \u03c4i(\u22121) = 0\u2200i. 2: for t = 0, 1, . . . , T \u2212 1 do 3: randomly choose k \u2208 {1, 2, . . . , n}, and set \u03c4i(t) ={\nt if i = k \u03c4i(t\u2212 1) otherwise ;\n4: update xt+1 using (16) or (19) when \u2126 = 0; and use (22) when \u2126 6= 0; 5: yt+1 \u2190 arg miny \u03c8(y) + \u03c12\u2016Axt+1 +By \u2212 c+ \u03b1t\u2016 2; 6: \u03b1t+1 \u2190 \u03b1t + (Axt+1 +Byt+1 \u2212 c); 7: end for 8: Output: x\u0304T \u2190 1T \u2211T t=1 xt, y\u0304T \u2190 1 T \u2211T t=1 yt.\nupdate rule in (15) then reduces to MISO using the Lipschitz gradient surrogate; and (20) corresponds to the proximal gradient surrogate [13]. SAG, on the other hand, does not have the proximal term \u2016x \u2212 x\u03c4i(t)\u20162 in its update rule, and also cannot handle a nonsmooth \u2126. When \u03c8 6= 0, (17) can be regarded as a quadratic surrogate [13]. Then, (18) (resp. (20)) is a combination of the Lipschitz (resp. proximal) gradient surrogate and quadratic surrogate, which can be easily seen to be another surrogate function in the sense of [13]. However, one should be reminded that MISO only considers unconstrained optimization problems and cannot handle the equality constraints in the ADMM setting (i.e., when \u03c8 6= 0).\nThe stochastic algorithms in Section II only require \u03c6 to be convex, and do not explicitly consider its form in (7). Hence, there are two possibilities in the handling of a nonzero \u2126. The first approach directly takes the nonsmooth \u03c6 in (7), and uses its subgradient in the update equations ((11), (13) and (14)). However, unlike the proximal step in (22), this does not exploit the structure of \u03c6 and subgradient descent often has slow empirical convergence [4]. The second approach folds \u2126 into \u03c8 by rewriting the optimization problem as min x, [ y z ] 1 n \u2211n i=1 `i(x) + [\u03c8(y) + \u2126(z)] : [ A I ] x +[\nB 0 0 \u2212I ] [ y z ] = [ c 0 ] . In the update step for [ y z ] , it is easy to see from (5) that y and z are decoupled and thus can be optimized separately. In comparison with (22), a disadvantage of this reformulation is that an additional variable z (which is of the same size as x) has to be introduced. Hence, it is more computationally expensive empirically. Moreover, the radius of the parameter space is also increased, leading to bigger constants in the big-Oh notation of the convergence rate [9], [6]."}, {"heading": "C. Convergence Analysis", "text": "The proposed ADMM algorithm has comparable periteration complexity as the existing stochastic versions in Section II. In this section, we show that it also has a much faster convergence rate. In the standard convergence analysis of ADMM, equation (4) is used for updating x [12], [8]. In the proposed algorithm, the loss and feasibility violation are linearized, making the analysis more difficult. Moreover,\nthough related to MISO, our analysis is a non-trivial extension because of the presence of equality constraints and additional Lagrangian multipliers in the ADMM formulation.\nLet \u2016x\u2016H \u2261 xTHx for a psd matrix H , Hx \u2261 LAI \u2212 \u03c1ATA, and Hy \u2261 \u03c1BTB. Moreover, denote the optimal solution of (1) by (x\u2217, y\u2217). As in [9], we first establish convergence rates of the (x\u0304T , y\u0304T ) solution in terms of a combination of the objective value and feasibility violation (weighted by \u03b3 > 0). Proof is in Appendix ??.\nTheorem 1: Using update rule (22), we have E [\u03a6(x\u0304T , y\u0304T )\u2212 \u03a6(x\u2217, y\u2217) + \u03b3\u2016Ax\u0304T +By\u0304T \u2212 c\u2016] \u2264 1 2T { \u2016x\u2217 \u2212 x0\u20162Hx + nL\u2016x\n\u2217 \u2212 x0\u20162 + \u2016y\u2217 \u2212 y0\u20162Hy + 2\u03c1 ( \u03b32 \u03c12 + \u2016\u03b10\u2016 2 )}\n. Remark 1: Obviously, (21) reduces to (18) when \u2126 = 0. Hence, Theorem 1 trivially holds when the update rule (19) is used.\nWhen \u2126 = 0 and the inexact Uzawa simplification is not used, a similar convergence rate can be obtained in the following. Proof is in Appendix ??.\nTheorem 2: Using update rule (16), we have E [\u03a6(x\u0304T , y\u0304T )\u2212 \u03a6(x\u2217, y\u2217) + \u03b3\u2016Ax\u0304T +By\u0304T \u2212 c\u2016] \u2264 1 2T { nL\u2016x\u2217 \u2212 x0\u20162 + \u2016y\u2217 \u2212 y0\u20162Hy + 2\u03c1 ( \u03b32 \u03c12 + \u2016\u03b10\u2016 2 )}\n. As in other ADMM algorithms, the (x\u0304T , y\u0304T ) pair obtained from Algorithm 1 may not satisfy the linear constraint Ax+ By = c exactly. As discussed in [6], when B is invertible, this can be alleviated by obtaining y from x\u0304T as y(x\u0304T ) = B\u22121(c\u2212 Ax\u0304T ). The feasibility violation is then zero, and the following corollary shows convergence w.r.t. the objective value. Proof is in Appendix ??. Similarly, when A is invertible. one can also obtain x from y\u0304T as x(y\u0304T ) = A\u22121(c\u2212By\u0304T ). Because of the lack of space, the analogous corollary will not be shown here.\nCorollary 1: Assume that \u03c8 is L\u0303-Lipschitz continuous, and B is invertible. Using the update rule (19) or (22), we have E [\u03a6(x\u0304T , y(x\u0304T )\u2212 \u03a6(x\u2217, y\u2217)] \u2264 12T { \u2016x\u2217 \u2212 x0\u20162Hx +\nnL\u2016x\u2217 \u2212 x0\u20162 + \u2016y\u2217 \u2212 y0\u20162Hy + \u03c1 ( L\u03032LB \u03c12 + \u2016\u03b10\u2016 2 )}\n, where LB is the largest eigenvalue of (B\u22121)TB\u22121; when update rule (16) is used, E [\u03a6(x\u0304T , y(x\u0304T )\u2212 \u03a6(x\u2217, y\u2217)] \u2264 1 2T { nL\u2016x\u2217 \u2212 x0\u20162 + \u2016y\u2217 \u2212 y0\u20162Hy + \u03c1 ( L\u03032LB \u03c12 + \u2016\u03b10\u2016 2 )}\n. Remark 2: In all the above cases, we obtain a convergence rate of O(1/T ), which matches that of the batch ADMM but with a much lower per-iteration complexity."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we perform experiments on the generalized lasso model [20]: minx\u2208Rd 1n \u2211n i=1 `i(x) + \u03bb\u2016Ax\u20161, where A \u2208 Rm\u00d7d (for some m > 0) is a penalty matrix specifying the desired structured sparsity pattern of x. With different settings of A, this can be reduced to models such as the fused lasso, trend filtering, and wavelet smoothing. Here, we will focus on the graph-guided fused lasso [21], whose sparsity pattern is specified by a graph defined on the d variates of x. Following [9], we obtain this graph by sparse inverse covariance selection [22], [1]. Moreover, as classification problems are considered here, we use the logistic loss instead of the square loss commonly used in lasso.\nWhile proximal methods have been used in the optimization of graph-guided fused lasso [23], [24], in general, the underlying proximal step does not have a closed-form because of the presence of A.\nADMM, by splitting the objective as \u03c6(x) = 1 n \u2211n i=1 `i(x), \u03c8(y) = \u03bb\u2016y\u20161 and with constraint Ax = y, has been shown to be more efficient [9], [6]. In this experiment, we compare\n1) two variants of the proposed method: i) SA-ADMM, which uses update rule (16); and ii) SA-IU-ADMM, which uses (19)) based on the inexact Uzawa method; 2) three existing stochastic ADMM algorithms: i) STOCADMM [9]; ii) OPG-ADMM [6]; and iii) RDA-ADMM [6]; 3) two deterministic ADMM variants: i) batch-ADMM, which is the batch version of SA-ADMM using full gradient (i.e., \u03c4i(t) = t \u2200i); and ii) batch-IU-ADMM, which is the batch version of SA-IU-ADMM.\nAll these share the same update rules for y and \u03b1 (i.e., steps 5 and 6 in Algorithm 1), and differ only in the updating of x, which are summarized in Table I. We do not compare with the online ADMM [8] or a direct application of batch ADMM, as they require nonlinear optimization for the update of x. Moreover, it has been shown that the online ADMM is slower than RDA-ADMM [6].\nExperiments are performed on five binary classification data sets:3 a9a, covertype, quantum, rcv1, and sido (Table II), which have been commonly used [14], [6]. For each data set, half of the samples are used for training, while the rest for testing. To reduce statistical variability, results are averaged over 10 repetitions. We fix \u03c1 in (3) to 0.01; and the regularization parameter \u03bb to 10\u22125 for a9a, covertype, quantum, and to 10\u22124 for rcv1 and sido. For selection of stepsize (or its proportionality constant), we run each stochastic algorithm 5 times over a small training subset (with 500 samples) and choose the setting with the smallest training objective value. For each batch algorithm, we run it for 100 iterations on the same training subset. All methods are implemented in MATLAB.\nFigures 1 and 2 show the objective value and testing loss versus the number of effective passes over the data. Overall, SA-IU-ADMM is the fastest, which is followed by SAADMM, and then the other stochastic algorithms. The batch ADMM algorithms are the slowest.\nAs discussed in Section II, STOC-ADMM and OPGADMM has O(log T/T ) convergence when the loss is strongly convex. It is still an open question whether this also holds for the proposed algorithm. In the following, we will compare their performance empirically by adding an extra `2- regularizer on x. Results are shown in Figure 3. As can be seen, the improvement of SA-IU-ADMM over others is even more dramatic."}, {"heading": "V. CONCLUSION", "text": "In this paper, we developed a novel stochastic algorithm that incrementally approximates the full gradient in the linearized ADMM formulation. It enjoys the same computational simplicity as existing stochastic ADMM algorithms, but has a fast convergence rate that matches the batch ADMM. Empirical results on both general convex and strongly convex problems\n3a9a, covertype and rcv1 are from the LIBSVM archive, quantum is from the KDDCup-2004, and sido from the Causality Workbench website.\ndemonstrate its efficiency over batch and stochastic ADMM algorithms. In the future, we will investigate the theoretical convergence rate of the proposed algorithm on strongly convex problems."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, vol. 2, no. 1, pp. 17\u201340, 1976.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Sur l\u2019approximation, par elements finis d\u2019ordre un, et la resolution, par penalisation-dualite, d\u2019une classe de problems de dirichlet non lineares", "author": ["R. Glowinski", "A. Marrocco"], "venue": "Revue Francaise d\u2019Automatique, Informatique, et Recherche Op\u00e9rationelle, vol. 9, pp. 41\u201376, 1975.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1975}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 2873\u20132908, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 2543\u20132596, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Dual averaging and proximal gradient descent for online alternating direction multiplier method", "author": ["T. Suzuki"], "venue": "Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA, USA, 2013, pp. 392\u2013400.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured sparsity via alternating direction methods", "author": ["Z. Qin", "D. Goldfarb"], "venue": "Journal of Machine Learning Research, vol. 13, pp. 1435\u2013 1468, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Online alternating direction method", "author": ["H. Wang", "A. Banerjee"], "venue": "Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, July 2012, pp. 1119\u20131126.  (a) a9a  (b) covertype (c) quantum  (d) rcv1 (e) sido Fig. 3. Objective value versus number of effective passes for the strongly convex problem.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA, USA, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Advanced Lectures on Machine Learning. Springer Verlag, 2004, p. 146168.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["W. Deng", "W. Yin"], "venue": "Rice University, Tech. Rep. TR12-14, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "On the O(1/t) convergence rate of alternating direction method", "author": ["B.S. He", "X.M. Yuan"], "venue": "SIAM Journal on Numerical Analysis, vol. 22, no. 4, pp. 1431\u20131448, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimization with first-order surrogate functions", "author": ["J. Mairal"], "venue": "Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA, USA, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N. Le Roux", "M. Schmidt", "F. Bach"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 2672\u20132680. [Online]. Available: http://books.nips.cc/papers/files/nips25/ NIPS2012 1246.pdf", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Proximal stochastic dual coordinate ascent", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "arXiv:1211.2717, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A unified primal-dual algorithm framework based on Bregman iteration", "author": ["X. Zhang", "M. Burger", "S. Osher"], "venue": "Journal of Scientific Computing, vol. 46, no. 1, pp. 20\u201346, Jan. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "University of Minnesota, Tech. Rep. arXiv:1208.3922, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Convex optimization with sparsity-inducing norms", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Optimization for Machine Learning, S. Sra, S. Nowozin, and S. Wright, Eds. MIT Press, 2011, pp. 19\u201353.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast iterative shrinkage-thresholding algo-  IEEE TRANSACTIONS, AUGUST 2013  7 rithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "The solution path of the generalized lasso", "author": ["R.J. Tibshirani", "J. Taylor"], "venue": "Annals of Statistics, vol. 39(3), pp. 1335\u20131371, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A multivariate regression approach to association analysis of a quantitative trait network", "author": ["S. Kim", "K.-A. Sohn", "E. Xing"], "venue": "Bioinformatics, vol. 25, no. 12, pp. i204\u2013i212, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data", "author": ["O. Banerjee", "L. El Ghaoui", "A. d\u2019Aspremont"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 485\u2013 516, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast Newton-type methods for total variation regularization", "author": ["A. Barbero", "S. Sra"], "venue": "Proceedings of the 28th International Conference on Machine Learning, New York, NY, USA, Jun. 2011, pp. 313\u2013320.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient algorithm for a class of fused lasso problems", "author": ["J. Liu", "L. Yuan", "J. Ye"], "venue": "Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining, 2010, pp. 323\u2013332.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The alternating direction method of multipliers (ADMM) [1], [2], [3] considers problems of the form", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "The alternating direction method of multipliers (ADMM) [1], [2], [3] considers problems of the form", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "The alternating direction method of multipliers (ADMM) [1], [2], [3] considers problems of the form", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "In comparison with other state-of-theart optimization methods such as proximal gradient methods [4], [5], the use of ADMM has been shown to have faster convergence in several difficult structured sparse regularization problems [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "In comparison with other state-of-theart optimization methods such as proximal gradient methods [4], [5], the use of ADMM has been shown to have faster convergence in several difficult structured sparse regularization problems [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "In comparison with other state-of-theart optimization methods such as proximal gradient methods [4], [5], the use of ADMM has been shown to have faster convergence in several difficult structured sparse regularization problems [6].", "startOffset": 227, "endOffset": 230}, {"referenceID": 6, "context": "number of features and data set size are large [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Wang and Banerjee [8] first proposed the online ADMM, which learns from only one sample (or a small mini-batch) at a time.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "Very recently, three stochastic variants of ADMM are independently proposed [9], [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Very recently, three stochastic variants of ADMM are independently proposed [9], [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "Two are based on the stochastic gradient descent (SGD) [10], while one is based on regularized dual averaging (RDA) [5].", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "Two are based on the stochastic gradient descent (SGD) [10], while one is based on regularized dual averaging (RDA) [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 37, "endOffset": 40}, {"referenceID": 10, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 285, "endOffset": 289}, {"referenceID": 11, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 291, "endOffset": 295}, {"referenceID": 12, "context": "This gap in the convergence rates between stochastic and batch ADMM algorithms is indeed not surprising, as it is also observed between SGD and batch gradient descent in the analogous unconstrained optimization setting [13].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "Recently, there have been several attempts on bridging this gap [14], [13], [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Recently, there have been several attempts on bridging this gap [14], [13], [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Recently, there have been several attempts on bridging this gap [14], [13], [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "[14] proposed an approach whose per-iteration cost is as low as SGD, but can achieve linear convergence for strongly convex functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Using the scaled dual variable \u03b1t = \u03b2t/\u03c1, the ADMM update can be expressed as [1]:", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "To alleviate this problem, Wang and Banerjee [8] proposed the online ADMM that uses only one sample in each iteration.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "[9] proposed the stochastic ADMM1, which updates x as", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For the special case where B = \u2212I and c = 0, Suzuki [6] proposed a similar approach called online proximal gradient descent ADMM (OPG-ADMM), which uses the inexact Uzawa method [16] to further linearize the last term in (10), leading to", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "For the special case where B = \u2212I and c = 0, Suzuki [6] proposed a similar approach called online proximal gradient descent ADMM (OPG-ADMM), which uses the inexact Uzawa method [16] to further linearize the last term in (10), leading to", "startOffset": 177, "endOffset": 181}, {"referenceID": 5, "context": "Suzuki [6] also proposed another stochastic variant called RDA-ADMM based on the method of regularized dual averaging (RDA) [5] (again for the special case with B = \u2212I and c = 0), in which x is updated as", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "Suzuki [6] also proposed another stochastic variant called RDA-ADMM based on the method of regularized dual averaging (RDA) [5] (again for the special case with B = \u2212I and c = 0), in which x is updated as", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "either the objective value [6] or a weighted combination of the objective value and feasibility violation [9].", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "either the objective value [6] or a weighted combination of the objective value and feasibility violation [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 10, "context": "However, in both cases (general and strongly convex), these convergence rates are inferior to their batch ADMM counterparts, which are O(1/T ) and O(\u03bc ) (with 0 < \u03bc < 1), respectively [11], [12], [17].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "However, in both cases (general and strongly convex), these convergence rates are inferior to their batch ADMM counterparts, which are O(1/T ) and O(\u03bc ) (with 0 < \u03bc < 1), respectively [11], [12], [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "However, in both cases (general and strongly convex), these convergence rates are inferior to their batch ADMM counterparts, which are O(1/T ) and O(\u03bc ) (with 0 < \u03bc < 1), respectively [11], [12], [17].", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "However, as is well-known, while the full gradient descent has linear convergence rate, SGD only achieves sublinear convergence [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "A pioneering approach along this line is the stochastic average gradient (SAG) [14], which considers the optimization of a strongly convex sum of smooth functions (minx 1 n \u2211n i=1 `i(x)).", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "Another closely related approach is the minimization by incremental surrogate optimization (MISO) [13], which replaces each `i by some \u201csurrogate\u201d function in an incremental manner similar to SAG.", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "As in existing stochastic ADMM approaches [8], [9], [6], y and \u03b1 are still updated by (5), (6), and the key difference is on the update of x (Algorithm 1).", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "As in existing stochastic ADMM approaches [8], [9], [6], y and \u03b1 are still updated by (5), (6), and the key difference is on the update of x (Algorithm 1).", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "As in existing stochastic ADMM approaches [8], [9], [6], y and \u03b1 are still updated by (5), (6), and the key difference is on the update of x (Algorithm 1).", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "A technique that has been popularly used in the recent ADMM literature is the inexact Uzawa method [16], which uses (2) to approximate r(x, yt, \u03b1t) by its upper bound:", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "where \u2207xr \u2261 \u03c1A (Axt +Byt \u2212 c+ \u03b1t) and LA is an upper bound on the eigenvalues of \u03c1AA [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "This is the standard proximal step popularly used in optimization problems with structured sparsity [18], [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "This is the standard proximal step popularly used in optimization problems with structured sparsity [18], [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": ", \u03a9(x) = \u2016x\u20161, \u2016x\u20162, \u2016x\u2016\u221e and various mixed norms [4]).", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "[9] claimed that \u03b7t+1 in (11) can be fixed at \u03b7T , though the proof is missing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "update rule in (15) then reduces to MISO using the Lipschitz gradient surrogate; and (20) corresponds to the proximal gradient surrogate [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "When \u03c8 6= 0, (17) can be regarded as a quadratic surrogate [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "proximal) gradient surrogate and quadratic surrogate, which can be easily seen to be another surrogate function in the sense of [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "However, unlike the proximal step in (22), this does not exploit the structure of \u03c6 and subgradient descent often has slow empirical convergence [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Moreover, the radius of the parameter space is also increased, leading to bigger constants in the big-Oh notation of the convergence rate [9], [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "Moreover, the radius of the parameter space is also increased, leading to bigger constants in the big-Oh notation of the convergence rate [9], [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 11, "context": "In the standard convergence analysis of ADMM, equation (4) is used for updating x [12], [8].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "In the standard convergence analysis of ADMM, equation (4) is used for updating x [12], [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "As in [9], we first establish convergence rates of the (x\u0304T , \u0233T ) solution in terms of a combination of the objective value and feasibility violation (weighted by \u03b3 > 0).", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "As discussed in [6], when B is invertible, this can be alleviated by obtaining y from x\u0304T as y(x\u0304T ) = B\u22121(c\u2212 Ax\u0304T ).", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "In this section, we perform experiments on the generalized lasso model [20]: minx\u2208Rd 1 n \u2211n i=1 `i(x) + \u03bb\u2016Ax\u20161, where A \u2208 Rm\u00d7d (for some m > 0) is a penalty matrix specifying the desired structured sparsity pattern of x.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "Here, we will focus on the graph-guided fused lasso [21], whose sparsity pattern is specified by a graph defined on the d variates of x.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "Following [9], we obtain this graph by sparse inverse covariance selection [22], [1].", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "Following [9], we obtain this graph by sparse inverse covariance selection [22], [1].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "Following [9], we obtain this graph by sparse inverse covariance selection [22], [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 22, "context": "While proximal methods have been used in the optimization of graph-guided fused lasso [23], [24], in general, the underlying proximal step does not have a closed-form because of the presence of A.", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "While proximal methods have been used in the optimization of graph-guided fused lasso [23], [24], in general, the underlying proximal step does not have a closed-form because of the presence of A.", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "n \u2211n i=1 `i(x), \u03c8(y) = \u03bb\u2016y\u20161 and with constraint Ax = y, has been shown to be more efficient [9], [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "n \u2211n i=1 `i(x), \u03c8(y) = \u03bb\u2016y\u20161 and with constraint Ax = y, has been shown to be more efficient [9], [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "1) two variants of the proposed method: i) SA-ADMM, which uses update rule (16); and ii) SA-IU-ADMM, which uses (19)) based on the inexact Uzawa method; 2) three existing stochastic ADMM algorithms: i) STOCADMM [9]; ii) OPG-ADMM [6]; and iii) RDA-ADMM [6]; 3) two deterministic ADMM variants: i) batch-ADMM, which is the batch version of SA-ADMM using full gradient (i.", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "1) two variants of the proposed method: i) SA-ADMM, which uses update rule (16); and ii) SA-IU-ADMM, which uses (19)) based on the inexact Uzawa method; 2) three existing stochastic ADMM algorithms: i) STOCADMM [9]; ii) OPG-ADMM [6]; and iii) RDA-ADMM [6]; 3) two deterministic ADMM variants: i) batch-ADMM, which is the batch version of SA-ADMM using full gradient (i.", "startOffset": 229, "endOffset": 232}, {"referenceID": 5, "context": "1) two variants of the proposed method: i) SA-ADMM, which uses update rule (16); and ii) SA-IU-ADMM, which uses (19)) based on the inexact Uzawa method; 2) three existing stochastic ADMM algorithms: i) STOCADMM [9]; ii) OPG-ADMM [6]; and iii) RDA-ADMM [6]; 3) two deterministic ADMM variants: i) batch-ADMM, which is the batch version of SA-ADMM using full gradient (i.", "startOffset": 252, "endOffset": 255}, {"referenceID": 7, "context": "We do not compare with the online ADMM [8] or a direct application of batch ADMM, as they require nonlinear optimization for the update of x.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Moreover, it has been shown that the online ADMM is slower than RDA-ADMM [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Experiments are performed on five binary classification data sets:3 a9a, covertype, quantum, rcv1, and sido (Table II), which have been commonly used [14], [6].", "startOffset": 150, "endOffset": 154}, {"referenceID": 5, "context": "Experiments are performed on five binary classification data sets:3 a9a, covertype, quantum, rcv1, and sido (Table II), which have been commonly used [14], [6].", "startOffset": 156, "endOffset": 159}], "year": 2013, "abstractText": "In this paper, we propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, the proposed algorithm improves the convergence rate on convex problems from O ( 1 \u221a T ) to O ( 1 T ) , where T is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.", "creator": "LaTeX with hyperref package"}}}