{"id": "1308.4828", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2013", "title": "The Sample-Complexity of General Reinforcement Learning", "abstract": "we present a new algorithm for discussing general binary reinforcement lesson learning behaviour where the true difference environment is known solely to belong to sufficiently a single finite class made of n different arbitrary models. the algorithm is shown moreover to be near - optimal for all but o ( x n log ^ 2 n ) time - steps aligned with high binding probability. infinite difference classes are also considered methods where we clearly show instead that compactness is a key numerical criterion formula for continuously determining possibly the smallest existence dependence of uniform sample - complexity bounds. a constant matching versus lower bound is likewise given for the finite case.", "histories": [["v1", "Thu, 22 Aug 2013 11:39:06 GMT  (18kb)", "http://arxiv.org/abs/1308.4828v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "marcus hutter", "peter sunehag"], "accepted": true, "id": "1308.4828"}, "pdf": {"name": "1308.4828.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Peter Sunehag"], "emails": ["tor.lattimore@anu.edu.au", "marcus.hutter@anu.edu.au", "peter.sunehag@anu.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 8.\n48 28\nv1 [\ncs .L\nG ]\n2 2\nContents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 Notation 3", "text": ""}, {"heading": "3 Finite Case 4", "text": ""}, {"heading": "4 Compact Case 12", "text": ""}, {"heading": "5 Unbounded Environment Classes 12", "text": ""}, {"heading": "6 Lower Bound 13", "text": "7 Conclusions 13 References 14 A Technical Results 15 B Constants 16 C Table of Notation 16\nKeywords\nReinforcement learning; sample-complexity; exploration exploitation."}, {"heading": "1 Introduction", "text": "Reinforcement Learning (RL) is the task of learning policies that lead to nearly-optimal rewards where the environment is unknown. One metric of the efficiency of an RL algorithm is samplecomplexity, which is a high probability upper bound on the number of time-steps when that algorithm is not nearly-optimal that holds for all environment in some class. Such bounds are typically shown for very specific classes of environments, such as (partially observable/factored) Markov Decision Processes (MDP) and bandits. We consider more general classes of environments where at each time-step an agent takes an action a \u2208 A where-upon it receives reward r \u2208 [0, 1] and an observation o \u2208 O, which are generated stochastically by the environment and may depend arbitrarily on the entire history sequence.\nWe present a new reinforcement learning algorithm, named Maximum Exploration Reinforcement Learning (MERL), that accepts as input a finite set M := {\u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdN} of arbitrary environments, an accuracy \u01eb, and a confidence \u03b4. The main result is that MERL has a sample-complexity of\nO\u0303\n(\nN \u01eb2(1\u2212 \u03b3)3 log 2 N \u03b4\u01eb(1\u2212 \u03b3)\n)\n,\nwhere 1/(1\u2212 \u03b3) is the effective horizon determined by discount rate \u03b3. We also consider the case where M is infinite, but compact with respect to a particular topology. In this case, a variant of MERL has the same sample-complexity as above, but where N is replaced by the size of the smallest \u01eb-cover. A lower bound is also given that matches the upper bound except for logarithmic factors. Finally, if M is non-compact then in general no finite sample-complexity bound exists. Related work. Many authors have worked on the sample-complexity of RL in various settings. The simplest case is the multiarmed bandit problem that has been extensively studied with varying assumptions. The typical measure of efficiency in the bandit literature is regret, but samplecomplexity bounds are also known and sometimes used. The next step from bandits is finite state MDPs, of which bandits are an example with only a single state. There are two main settings when MDPs are considered, the discounted case where sample-complexity bounds are proven and the undiscounted (average reward) case where regret bounds are more typical. In the discounted setting the upper and lower bounds on sample-complexity are now extremely refined. See Strehl et al. [2009] for a detailed review of the popular algorithms and theorems. More recent work on closing the gap between upper and lower bounds is by Szita and Szepesva\u0301ri [2010], Lattimore and Hutter [2012], Azar et al. [2012]. In the undiscounted case it is necessary to make some form of ergodicity assumption as without this regret bounds cannot be given. In this work we avoid ergodicity assumptions and discount future rewards. Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in. Even-dar et al. [2005] give essentially unimprovable exponential bounds on the sample-complexity of learning in finite partially observable MDPs. Odalric-Ambrym et al. [2013] show regret bounds\nfor undiscounted RL where the true environment is assumed to be finite, Markov and communicating, but where the state is not directly observable. As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b]. Perhaps the closest related worked is Diuk et al. [2009], which deals with a similar problem in the rather different setting of learning the optimal predictor from a class of N experts. They obtain an O(N logN) bound, which is applied to the problem of structure learning for discounted finite-state factored MDPs. Our work generalises this approach to the non-Markov case and compact model classes."}, {"heading": "2 Notation", "text": "The definition of environments is borrowed from the work of Hutter [2005], although the notation is slightly more formal to ease the application of martingale inequalities.\nGeneral. N = {0, 1, 2, \u00b7 \u00b7 \u00b7 } is the natural numbers. For the indicator function we write [[x = y]] = 1 if x = y and 0 otherwise. We use \u2227 and \u2228 for logical and/or respectively. If A is a set then |A| is its size and A\u2217 is the set of all finite strings (sequences) over A. If x and y are sequences then x \u228f y means that x is a prefix of y. Unless otherwise mentioned, log represents the natural logarithm. For random variable X we write EX for its expectation. For x \u2208 R, \u2308x\u2309 is the ceiling function. Environments and policies. Let A, O and R \u2282 R be finite sets of actions, observations and rewards respectively and H := A \u00d7 O \u00d7 R. H\u221e is the set of infinite history sequences while H\u2217 := (A \u00d7 O \u00d7 R)\u2217 is the set of finite history sequences. If h \u2208 H\u2217 then \u2113(h) is the number of action/observation/reward tuples in h. We write at(h), ot(h), rt(h) for the tth action/observation/reward of history sequence h. For h \u2208 H\u2217, \u0393h := {h\u2032 \u2208 H\u221e : h \u228f h\u2032} is the cylinder set. Let F := \u03c3({\u0393h : h \u2208 H\u2217}) and Ft := \u03c3({\u0393h : h \u2208 H\u2217 \u2227 \u2113(h) = t}) be \u03c3-algebras. An environment \u00b5 is a set of conditional probability distributions over observation/reward pairs given the history so far. A policy \u03c0 is a function \u03c0 : H\u2217 \u2192 A. An environment and policy interact sequentially to induce a measure, P\u00b5,\u03c0, on filtered probability space (H\u221e,F , {Ft}). For convenience, we abuse notation and write P\u00b5,\u03c0(h) := P\u00b5,\u03c0(\u0393h). If h \u228f h \u2032 then conditional probabilities are P\u00b5,\u03c0(h \u2032|h) := P\u00b5,\u03c0(h\u2032)/P\u00b5,\u03c0(h). Rt(h; d) := \u2211t+d k=t \u03b3\nk\u2212trk(h) is the d-step return function and Rt(h) := limd\u2192\u221e Rt(h; d). Given history ht with \u2113(ht) = t, the value function is defined by V \u03c0\u00b5 (ht; d) := E[Rt(h; d)|ht] where the expectation is taken with respect to P\u00b5,\u03c0(\u00b7|ht). V \u03c0\u00b5 (ht) := limd\u2192\u221e V \u03c0 \u00b5 (ht; d). The optimal policy for environment \u00b5 is \u03c0 \u2217 \u00b5 := argmax\u03c0 V \u03c0 \u00b5 , which with our assumptions is known to exist Lattimore and Hutter [2011a]. The value of the optimal policy is V \u2217\u00b5 := V \u03c0\u2217\u00b5 \u00b5 . In general, \u00b5 denotes the true environment while \u03bd is a model. \u03c0 will typically be the policy of the algorithm under consideration. Q\u2217\u00b5(h, a) is the value in history h of following policy \u03c0\u2217\u00b5 except for the first time-step when action a is taken. M is a set of environments (models). Sample-complexity. Policy \u03c0 is \u01eb-optimal in history h and environment \u00b5 if V \u2217\u00b5 (h)\u2212V \u03c0\u00b5 (h) \u2264 \u01eb. The sample-complexity of a policy \u03c0 in environment class M is the smallest \u039b such that, with high probability, \u03c0 is \u01eb-optimal for all but \u039b time-steps for all \u00b5 \u2208 M. Define L\u01eb\u00b5,\u03c0 : H\u221e \u2192 N\u222a {\u221e} to be the number of time-steps when \u03c0 is not \u01eb-optimal.\nL\u01eb\u00b5,\u03c0(h) :=\n\u221e \u2211\nt=1\n[ V \u2217\u00b5 (ht)\u2212 V \u03c0\u00b5 (ht) > \u01eb ] ,\nwhere ht is the length t prefix of h. The sample-complexity of policy \u03c0 is \u039b with respect to accuracy \u01eb and confidence 1\u2212 \u03b4 if P {\nL\u01eb\u00b5,\u03c0(h) > \u039b } < \u03b4, \u2200\u00b5 \u2208 M."}, {"heading": "3 Finite Case", "text": "We start with the finite case where the true environment is known to belong to a finite set of models, M. The Maximum Exploration Reinforcement Learning algorithm is model-based in the sense that it maintains a set, Mt \u2286 M, where models are eliminated once they become implausible. The algorithm operates in phases of exploration and exploitation, choosing to exploit if it knows all plausible environments are reasonably close under all optimal policies and explore otherwise. This method of exploration essentially guarantees that MERL is nearly optimal whenever it is exploiting and the number of exploration phases is limited with high probability. The main difficulty is specifying what it means to be plausible. Previous authors working on finite environments, such as MDPs or bandits, have removed models for which the transition probabilities are not sufficiently close to their empirical estimates. In the more general setting this approach fails because states (histories) are never visited more than once, so sufficient empirical estimates cannot be collected. Instead, we eliminate environments if the reward we actually collect over time is not sufficiently close to the reward we expected given that environment.\nBefore giving the explicit algorithm, we explain the operation of MERL more formally in two parts. First we describe how it chooses to explore and exploit and then how the model class is maintained. See Figure 2 for a diagram of how exploration and exploitation occurs.\nExploring and exploiting. At each time-step t MERL computes the pair of environments \u03bd, \u03bd in the model class Mt and the policy \u03c0 maximising the difference\n\u2206 := V \u03c0\u03bd (h; d)\u2212 V \u03c0\u03bd (h; d), d := 1 1\u2212 \u03b3 log 8 (1\u2212 \u03b3)\u01eb .\nIf \u2206 > \u01eb/4, then MERL follows policy \u03c0 for d time-steps, which we call an exploration phase. Otherwise, for one time-step it follows the optimal policy with respect to the first environment currently in the model class. Therefore, if MERL chooses to exploit, then all policies and environments in the model class lead to similar values, which implies that exploiting is near-optimal. If MERL explores, then either V \u03c0\u03bd (h; d) \u2212 V \u03c0\u00b5 (h; d) > \u01eb/8 or V \u03c0\u00b5 (h; d) \u2212 V \u03c0\u03bd (h; d) > \u01eb/8, which will allow us to apply concentration inequalities to eventually eliminate either \u03bd (the upper bound) or \u03bd (the lower bound). The model class. An exploration phase is a \u03ba-exploration phase if \u2206 \u2208 [\u01eb2\u03ba\u22122, \u01eb2\u03ba\u22121), where\n\u03ba \u2208 K := { 0, 1, 2, \u00b7 \u00b7 \u00b7 , log2 1 \u01eb(1\u2212 \u03b3) + 2 } .\nFor each environment \u03bd \u2208 M and each \u03ba \u2208 K, MERL associates a counter E(\u03bd, \u03ba), which is incremented at the start of a \u03ba-exploration phase if \u03bd \u2208 {\u03bd, \u03bd}. At the end of each \u03ba-exploration phase MERL calculates the discounted return actually received during that exploration phase R \u2208 [0, 1/(1\u2212 \u03b3)] and records the values\nX(\u03bd, \u03ba) := (1\u2212 \u03b3)(V \u03c0\u03bd (h; d)\u2212R) X(\u03bd, \u03ba) := (1\u2212 \u03b3)(R\u2212 V \u03c0\u03bd (h; d)),\nwhere h is the history at the start of the exploration phase. So X(\u03bd, \u03ba) is the difference between the return expected if the true model was \u03bd and the actual return and X(\u03bd, \u03ba) is the difference between the actual return and the expected return if the true model was \u03bd. Since the expected value of R is V \u03c0\u00b5 (h; d), and \u03bd,\u03bd are upper and lower bounds respectively, the expected values of both X(\u03bd, \u03ba) and X(\u03bd, \u03ba) are non-negative and at least one of them has expectation larger than (1\u2212 \u03b3)\u01eb/8.\nMERL eliminates environment \u03bd from the model class if the cumulative sum of X(\u03bd, \u03ba) over all exploration phases where \u03bd \u2208 {\u03bd, \u03bd} is sufficiently large, but it tests this condition only when the counts E(\u03bd, \u03ba) has increased enough since the last test. Let \u03b1j := \u2308 \u03b1j \u2309 for \u03b1 \u2208 (1, 2) as defined in\nthe algorithm. MERL only tests if \u03bd should be removed from the model class when E(\u03bd, \u03ba) = \u03b1j for some j \u2208 N. This restriction ensures that tests are not performed too often, which allows us to apply the union bound without losing too much. Note that if the true environment \u00b5 \u2208 {\u03bd, \u03bd}, then E\u00b5,\u03c0X(\u00b5, \u03ba) = 0, which will ultimately be enough to ensure that \u00b5 remains in the model class with high probability. The reason for using \u03ba to bucket exploration phases will become apparent later in the proof of Lemma 3.\nAlgorithm 1 MERL\n1: Inputs: \u01eb, \u03b4 andM := {\u03bd1, \u03bd2, \u00b7 \u00b7 \u00b7 , \u03bdN}. 2: t = 1 and h empty history 3: d := 1\n1\u2212\u03b3 log 8 (1\u2212\u03b3)\u01eb , \u03b41 := \u03b4\n32|K|N3/2\n4: \u03b1 := 4 \u221a N\n4 \u221a N\u22121 and \u03b1j := \u2308 \u03b1j \u2309\n5: E(\u03bd, \u03ba) := 0, \u2200\u03bd \u2208M and \u03ba \u2208 N 6: loop 7: repeat 8: \u03a0 := {\u03c0\u2217\u03bd : \u03bd \u2208 M} 9: \u03bd, \u03bd, \u03c0 := argmax\n\u03bd,\u03bd\u2208M,\u03c0\u2208\u03a0 V \u03c0\u03bd (h; d)\u2212 V \u03c0 \u03bd (h; d)\n10: if \u2206 := V \u03c0\u03bd (h; d)\u2212 V \u03c0 \u03bd (h; d) > \u01eb/4 then 11: h\u0303 = h and R = 0 12: for j = 0\u2192 d do 13: R = R+ \u03b3jrt(h) 14: Act(\u03c0) 15: end for 16: \u03ba := min { \u03ba \u2208 N : \u2206 > \u01eb2\u03ba\u22122 }\n. 17: E(\u03bd, \u03ba) = E(\u03bd, \u03ba) + 1 and E(\u03bd, \u03ba) = E(\u03bd, \u03ba) + 1 18: X(\u03bd, \u03ba)E(\u03bd,\u03ba) = (1\u2212 \u03b3)(V \u03c0 \u03bd (h\u0303; d)\u2212R) 19: X(\u03bd, \u03ba)E(\u03bd,\u03ba) = (1\u2212 \u03b3)(R \u2212 V \u03c0 \u03bd (h\u0303; d)) 20: else 21: i := min {i : \u03bdi \u2208M} and Act(\u03c0 \u2217 \u03bdi ) 22: end if 23: until \u2203\u03bd \u2208 M, \u03ba, j \u2208 N such that E(\u03bd, \u03ba) = \u03b1j and\nE(\u03bd,\u03ba) \u2211\ni=1\nX(\u03bd, \u03ba)i \u2265\n\u221a\n2E(\u03bd, \u03ba) log E(\u03bd, \u03ba)\n\u03b41 .\n24: M =M\u2212 {\u03bd} 25: end loop 26: function Act(\u03c0) 27: Take action at = \u03c0(h) and receive reward and observation rt, ot from environment 28: t\u2190 t+ 1 and h\u2190 hatotrt 29: end function\nSubscripts. For clarity, we have omitted subscripts in the pseudo-code above. In the analysis we will refer to Et(\u03bd, \u03ba) and Mt for the values of E(\u03bd, \u03ba) and M respectively at time-step t. We write \u03bdt for \u03bdi in line 21 and similarly \u03c0t := \u03c0 \u2217 \u03bdt .\nPhases. An exploration phase is a period of exactly d time-steps, starting at time-step t if\n1. t is not currently in an exploration phase.\n2. \u2206 := V \u03c0\u03bd (ht; d)\u2212 V \u03c0\u03bd (ht; d) > \u01eb/4.\nWe say it is a \u03bd-exploration phase if \u03bd = \u03bd or \u03bd = \u03bd and a \u03ba-exploration phase if \u2206 \u2208 [\u01eb2\u03ba\u22122, \u01eb2\u03ba\u22121) \u2261 [\u01eb\u03ba, 2\u01eb\u03ba) where \u01eb\u03ba := \u01eb2\u03ba\u22122. It is a (\u03bd, \u03ba)-exploration phase if it satisfies both\nof the previous statements. We say that MERL is exploiting at time-step t if t is not in an exploration phase. A failure phase is also a period of d time-steps and starts in time-step t if\n1. t is not in an exploration phase or earlier failure phase\n2. V \u2217\u00b5 (ht)\u2212 V \u03c0\u00b5 (ht) > \u01eb.\nUnlike exploration phases, the algorithm does not depend on the failure phases, which are only used in the analysis, An exploration or failure phase starting at time-step t is proper if \u00b5 \u2208 Mt. The effective horizon d is chosen to ensure that V \u03c0\u00b5 (h; d) \u2265 V \u03c0\u00b5 (h)\u2212 \u01eb/8 for all \u03c0, \u00b5 and h.\nTest statistics. We have previously remarked that most traditional model-based algorithms with sample-complexity guarantees record statistics about the transition probabilities of an environment. Since the environments are assumed to be finite, these statistics eventually become accurate (or irrelevant) and the standard theory on the concentration of measure can be used for hypothesis testing. In the general case, environments can be infinite and so we cannot collect useful statistics about individual transitions. Instead, we use the statistics X(\u03bd, \u03ba), which are dependent on the value function rather than individual transitions. These satisfy E\u00b5,\u03c0[X(\u00b5, \u03ba)i] = 0 while E\u00b5,\u03c0[X(\u03bd, \u03ba)i] \u2265 0 for all \u03bd \u2208 Mt. Testing is then performed on the statistic \u2211\u03b1k i=1 X(\u03bd, \u03ba)i, which will satisfy certain martingale inequalities.\nUpdates. As MERL explores, it updates its model class, Mt \u2286 M, by removing environments that have become implausible. This is comparable to the updating of confidence intervals for algorithms such as MBIE (Strehl and Littman, 2005) or UCRL2 (Auer et al., 2010). In MBIE, the confidence interval about the empirical estimate of a transition probability is updated after every observation. A slight theoretical improvement used by UCRL2 is to only update when the number of samples of a particular statistic doubles. The latter trick allows a cheap application of the union bound over all updates without wasting too many samples. For our purposes, however, we need to update slightly more often than the doubling trick would allow. Instead, we check if an environment should be eliminated if the number of (\u03bd, \u03ba)-exploration phases is exactly \u03b1j for some j where \u03b1j := \u2308 \u03b1j \u2309 and \u03b1 := 4 \u221a N\n4 \u221a N\u22121 \u2208 (1, 2). Since the growth of \u03b1j is still exponential, the\nunion bound will still be applicable.\nProbabilities. For the remainder of this section, unless otherwise mentioned, all probabilities and expectations are with respect to P\u00b5,\u03c0 where \u03c0 is the policy of Algorithm 1 and \u00b5 \u2208 M is the true environment.\nAnalysis. Define Gmax := 216N |K| \u01eb2(1\u2212\u03b3)2 log 2 29N \u01eb2(1\u2212\u03b3)2\u03b41 and Emax := 216N \u01eb2(1\u2212\u03b3)2 log 2 29N \u01eb2(1\u2212\u03b3)2\u03b41 , which are high probability bounds on the number of failure and exploration phases respectively.\nTheorem 1. Let \u00b5 \u2208 M = {\u03bd1, \u03bd2, \u00b7 \u00b7 \u00b7 \u03bdN} be the true environment and \u03c0 be the policy of Algorithm 1. Then\nP { L\u01eb\u00b5,\u03c0(h) \u2265 d \u00b7 (Gmax + Emax) } \u2264 \u03b4.\nIf lower order logarithmic factors are dropped then the sample-complexity bound of MERL\ngiven by Theorem 1 is O\u0303 (\nN \u01eb2(1\u2212\u03b3)3 log 2 N \u03b4\u01eb(1\u2212\u03b3)\n)\n. Theorem 1 follows from three lemmas.\nLemma 2. \u00b5 \u2208 Mt for all t with probability 1\u2212 \u03b4/4.\nLemma 3. The number of proper failure phases is bounded by\nGmax := 216N |K| \u01eb2(1\u2212 \u03b3)2 log 2 2 9N \u01eb2(1\u2212 \u03b3)2\u03b41\nwith probability at least 1\u2212 \u03b42 .\nLemma 4. The number of proper exploration phases is bounded by\nEmax := 216N \u01eb2(1\u2212 \u03b3)2 log 2 2 9N \u01eb2(1\u2212 \u03b3)2\u03b41\nwith probability at least 1\u2212 \u03b44 .\nProof of Theorem 1. Applying the union bound to the results of Lemmas 2, 3 and 4 gives the following with probability at least 1\u2212 \u03b4.\n1. There are no non-proper exploration or failure phases.\n2. The number of proper exploration phases is at most Emax.\n3. The number of proper failure phases is at most Gmax.\nIf \u03c0 is not \u01eb-optimal at time-step t then t is either in an exploration or failure phase. Since both are exactly d time-steps long the total number of time-steps when \u03c0 is sub-optimal is at most d \u00b7 (Gmax + Emax).\nWe now turn our attention to proving Lemmas 2, 3 and 4. Of these, Lemma 4 is more conceptually challenging while Lemma 3 is intuitively unsurprising, but technically difficult. Proof of Lemma 2. If \u00b5 is removed from M, then there exists a \u03ba and j \u2208 N such that \u03b1j \u2211\ni=1\nX(\u00b5, \u03ba)i \u2265 \u221a 2\u03b1j log \u03b1j \u03b41 .\nFix a \u03ba \u2208 K, E\u221e(\u00b5, \u03ba) := limt Et(\u00b5, \u03ba) and Xi := X(\u00b5, \u03ba)i. Define a sequence of random variables\nX\u0303i :=\n{\nXi if i \u2264 E\u221e(\u00b5, \u03ba) 0 otherwise.\nNow we claim that Bn := \u2211n i=1 X\u0303i is a martingale with |Bi+1 \u2212Bi| \u2264 1 and EBi = 0. That it is a martingale with zero expectation follows because if t is the time-step at the start of the exploration phase associated with variable Xi, then E[Xi|Ft] = 0. |Bi+1 \u2212Bi| \u2264 1 because discounted returns are bounded in [0, 1/(1\u2212 \u03b3)] and by the definition of Xi.\nFor all j \u2208 N, we have by Azuma\u2019s inequality that\nP\n{\nB\u03b1j \u2265 \u221a 2\u03b1j log \u03b1j \u03b41\n}\n\u2264 \u03b41 \u03b1j .\nApply the union bound over all j.\nP\n{\n\u2203j \u2208 N : B\u03b1j \u2265 \u221a 2\u03b1j log \u03b1j \u03b41\n} \u2264 \u221e \u2211\nj=1\n\u03b41 \u03b1j .\nComplete the result by the union bound over all \u03ba, applying Lemma 10 (see Appendix) and the definition of \u03b41 to bound \u2211 \u03ba\u2208K \u2211\u221e j=1 \u03b41 \u03b1j \u2264 \u03b4/4.\nWe are now ready to give a high-probability bound on the number of proper exploration phases. If MERL starts a proper exploration phase at time-step t then at least one of the following holds:\n1. E[X(\u03bd, \u03ba)E(\u03bd,\u03ba)|Ft] > (1\u2212 \u03b3)\u01eb/8.\n2. E[X(\u03bd, \u03ba)E(\u03bd,\u03ba)|Ft] > (1\u2212 \u03b3)\u01eb/8. This contrasts with E[X(\u00b5, \u03ba)E(\u00b5,\u03ba)|Ft] = 0, which ensures that \u00b5 remains in M for all time-steps. If one could know which of the above statements were true at each time-step then it would be comparatively easy to show by means of Azuma\u2019s inequality that all environments that are not \u01ebclose are quickly eliminated after O( 1\n\u01eb2(1\u2212\u03b3)2 ) \u03bd-exploration phases, which would lead to the desired\nbound. Unfortunately though, the truth of (1) or (2) above cannot be determined, which greatly increases the complexity of the proof. Proof of Lemma 4. Fix a \u03ba \u2208 K and let Emax,\u03ba be a constant to be chosen later. Let ht be the history at the start of some \u03ba-exploration phase. We say an (\u03bd, \u03ba)-exploration phase is \u03bd-effective if\nE[X(\u03bd, \u03ba)E(\u03bd,\u03ba)|Ft] \u2261 (1\u2212 \u03b3)(V \u03c0\u00b5 (ht; d)\u2212 V \u03c0\u03bd (ht; d)) > (1\u2212 \u03b3)\u01eb\u03ba/2\nand \u03bd-effective if the same condition holds for \u03bd. Now since t is the start of a proper exploration phase we have that \u00b5 \u2208 Mt and so\nV \u03c0\u03bd (ht; d) \u2265 V \u03c0\u00b5 (ht; d) \u2265 V \u03c0\u03bd (ht; d) V \u03c0\u03bd (ht; d)\u2212 V \u03c0\u03bd (ht; d) > \u01eb\u03ba.\nTherefore every proper exploration phase is either \u03bd-effective or \u03bd-effective. Let Et,\u03ba := \u2211\n\u03bd Et(\u03bd, \u03ba), which is twice the number of \u03ba-exploration phases at time t and E\u221e,\u03ba := limt Et,\u03ba, which is twice the total number of \u03ba-exploration phases.1 Let Ft(\u03bd, \u03ba) be the number of \u03bd-effective (\u03bd, \u03ba)-exploration phases up to time-step t. Since each proper \u03ba-exploration phase is either \u03bdeffective or \u03bd-effective or both, \u2211\n\u03bd Ft(\u03bd, \u03ba) \u2265 Et,\u03ba/2. Applying Lemma 8 to y\u03bd := Et(\u03bd, \u03ba)/Et,\u03ba and x\u03bd := Ft(\u03bd, \u03ba)/Et(\u03bd, \u03ba) shows that if E\u221e,\u03ba > Emax,\u03ba then there exists a t\u2032 and \u03bd such that Et\u2032,\u03ba = Emax,\u03ba and\nFt\u2032(\u03bd, \u03ba) 2\nEmax,\u03baEt\u2032(\u03bd, \u03ba) \u2265 1 4N , (1)\nwhich implies that\nFt\u2032(\u03bd, \u03ba) \u2265 \u221a Emax,\u03baEt\u2032(\u03bd, \u03ba)\n4N\n(a) \u2265 Et\u2032(\u03bd, \u03ba)\u221a 4N , (2)\n1Note that it is never the case that \u03bd = \u03bd at the start of an exploration phase, since in this case \u2206 = 0.\nwhere (a) follows because Emax,\u03ba = Et\u2032,\u03ba \u2265 Et\u2032(\u03bd, \u03ba). Let Z(\u03bd) be the event that there exists a t\u2032 satisfying (1). We will shortly show that P {Z(\u03bd)} < \u03b4/(4N |K|). Therefore\nP {E\u221e,\u03ba > Emax,\u03ba} \u2264 P {\u2203\u03bd : Z(\u03bd)} \u2264 \u2211\n\u03bd\u2208M P {Z(\u03bd)}\n\u2264 \u03b4/(4|K|)\nFinally take the union bound over all \u03ba and let\nEmax := \u2211\n\u03ba\u2208K\n1 2 Emax,\u03ba,\nwhere we used 12Emax,\u03ba because Emax,\u03ba is a high-probability upper bound on E\u221e,\u03ba, which is twice the number of \u03ba-exploration phases.\nBounding P {Z(\u03bd)} < \u03b4/(4N |K|). Fix a \u03bd \u2208 M and let X1, X2, \u00b7 \u00b7 \u00b7 , XE\u221e(\u03bd,\u03ba) be the sequence with Xi := X(\u03bd, \u03ba)i and let ti be the corresponding time-step at the start of the ith (\u03bd, \u03ba)exploration phase. Define a sequence\nYi :=\n{\nXi \u2212E[Xi|Fti ] if i \u2264 E\u221e(\u03bd, \u03ba) 0 otherwise\nLet \u03bb(E) := \u221a\n2E log E \u03b41 . Now if Z(\u03bd), then the largest time-step t \u2264 t\u2032 with Et(\u03bd, t) = \u03b1j for some\nj \u2208 N is\nt := max {t \u2264 t\u2032 : \u2203j \u2208 N s.t. \u03b1j = Et(\u03bd, t)} ,\nwhich exists and satisfies\n1. Et(\u03bd, \u03ba) = \u03b1j for some j.\n2. E\u221e(\u03bd, \u03ba) > Et(\u03bd, \u03ba).\n3. Ft(\u03bd, \u03ba) \u2265 \u221a Et(\u03bd, \u03ba)Emax,\u03ba/(16N).\n4. Et(\u03bd, \u03ba) \u2265 Emax,\u03ba/(16N).\nwhere parts 1 and 2 are straightforward and parts 3 and 4 follow by the definition of {\u03b1j}, which was chosen specifically for this part of the proof. Since E\u221e(\u03bd, \u03ba) > Et(\u03bd, \u03ba), at the end of the exploration phase starting at time-step t, \u03bd must remain in M. Therefore\n\u03bb(\u03b1j) (a) \u2265 \u03b1j \u2211\ni=1\nXi (b) \u2265 \u03b1j \u2211\ni=1\nYi + \u01eb\u03ba(1\u2212 \u03b3)Ft(\u03bd, \u03ba)\n2\n(c) \u2265 \u03b1j \u2211\ni=1\nYi + \u01eb\u03ba(1\u2212 \u03b3)\n8\n\u221a\n\u03b1jEmax,\u03ba N , (3)\nwhere in (a) we used the definition of the confidence interval of MERL. In (b) we used the definition of Yi and the fact that EXi \u2265 0 for all i and EXi \u2265 \u01eb\u03ba(1 \u2212 \u03b3)/2 if Xi is effective. Finally we used the lower bound on the number of effective \u03bd-exploration phases, Ft(\u03bd, \u03ba) (part 3 above). If Emax,\u03ba := 211N\n\u01eb2\u03ba(1\u2212\u03b3)2 log2 2 9N \u01eb2(1\u2212\u03b3)2\u03b41 , then by applying Lemma 9 with a = 29N \u01eb2\u03ba(1\u2212\u03b3)2 and b = 1/\u03b41 we\nobtain\nEmax,\u03ba \u2265 29N\n\u01eb2\u03ba(1\u2212 \u03b3)2 log Emax,\u03ba \u03b41 \u2265 2 9N \u01eb2\u03ba(1\u2212 \u03b3)2 log \u03b1j \u03b41\nMultiplying both sides by \u03b1j and rearranging and using the definition of \u03bb(\u03b1j) leads to\n\u01eb\u03ba(1\u2212 \u03b3) 8\n\u221a\n\u03b1jEmax,\u03ba N \u2265 2\u03bb(\u03b1j).\nInserting this into Equation (3) shows that Z(\u03bd) implies that there exists an \u03b1j such that \u2211\u03b1j i=1 Yi \u2264 \u2212\u03bb(\u03b1j). Now by the same argument as in the proof of Lemma 2, Bn := \u2211n i=1 Yi is a martingale with |Bi+1 \u2212Bi| \u2264 1. Therefore by Azuma\u2019s inequality\nP\n{\n\u03b1j \u2211\ni=1\nYi \u2264 \u2212\u03bb(\u03b1j) }\n\u2264 \u03b41 \u03b1j .\nFinally apply the union bound over all j.\nRecall that if MERL is exploiting at time-step t, then \u03c0t is the optimal policy with respect to the first environment in the model class. To prove Lemma 3 we start by showing that in this case \u03c0t is nearly-optimal.\nLemma 5. Let t be a time-step and ht be the corresponding history. If \u00b5 \u2208 Mt and MERL is exploiting (not exploring), then V \u2217\u00b5 (ht)\u2212 V \u03c0t\u00b5 (ht) \u2264 5\u01eb/8.\nProof of Lemma 5. Since MERL is not exploring\nV \u2217\u00b5 (ht)\u2212 V \u03c0t\u00b5 (ht) (a) \u2264 V \u2217\u00b5 (ht; d)\u2212 V \u03c0t\u00b5 (ht; d) + \u01eb\n8 (b) \u2264 V \u03c0 \u2217\n\u00b5 \u03bdt (ht; d)\u2212 V \u03c0t\u03bdt (ht; d) + 5\u01eb/8\n(c) \u2264 5\u01eb/8,\n(a) follows by truncating the value function. (b) follows because \u00b5 \u2208 Mt and MERL is exploiting. (c) is true since \u03c0t is the optimal policy in \u03bdt.\nLemma 5 is almost sufficient to prove Lemma 3. The only problem is that MERL only follows \u03c0t = \u03c0 \u2217 \u03bdt until there is an exploration phase. The idea to prove Lemma 3 is as follows:\n1. If there is a low probability of entering an exploration phase within the next d time-steps following policy \u03c0t, then \u03c0 is nearly as good as \u03c0t, which itself is nearly optimal by Lemma 5.\n2. The number of time-steps when the probability of entering an exploration phase within the next d time-steps is high is unlikely to be too large before an exploration phase is triggered. Since there are not many exploration phases with high probability, there are also unlikely to be too many time-steps when \u03c0 expects to enter one with high probability.\nBefore the proof of Lemma 3 we remark on an easier to prove (but weaker) version of Theorem 1. If MERL is exploiting then Lemma 5 shows that V \u2217\u00b5 (h)\u2212Q\u2217\u00b5(h, \u03c0(h)) \u2264 5\u01eb/8 < \u01eb. Therefore if we cared about the number of time-steps when this is not the case (rather than V \u2217\u00b5 \u2212 V \u03c0\u00b5 ), then we would already be done by combining Lemmas 4 and 5.\nProof of Lemma 3. Let t be the start of a proper failure phase with corresponding history, h. Therefore V \u2217\u00b5 (h)\u2212V \u03c0\u00b5 (h) > \u01eb. By Lemma 5, V \u2217\u00b5 (h)\u2212V \u03c0\u00b5 (h) = V \u2217\u00b5 (h)\u2212V \u03c0t\u00b5 (h)+V \u03c0t\u00b5 (h)\u2212V \u03c0\u00b5 (h) \u2264 5\u01eb/8 + V \u03c0t\u00b5 \u2212 V \u03c0\u00b5 (h) and so\nV \u03c0t\u00b5 (h)\u2212 V \u03c0\u00b5 (h) \u2265 3\u01eb\n8 . (4)\nWe define set H\u03ba \u2282 H\u2217 to be the set of extensions of h that trigger \u03ba-exploration phases. Formally H\u03ba \u2282 H\u2217 is the prefix free set such that h\u2032 in H\u03ba if h \u228f h\u2032 and h\u2032 triggers a \u03ba-exploration phase for the first time since t. Let H\u03ba,d := {h\u2032 : h\u2032 \u2208 H\u03ba \u2227 \u2113(h\u2032) \u2264 t+ d}, which is the set of extensions of h that are at most d long and trigger \u03ba-exploration phases. Therefore\n3\u01eb\n8\n(a) \u2264 V \u03c0t\u00b5 (h)\u2212 V \u03c0\u00b5 (h) (b) = \u2211\n\u03ba\u2208K\n\u2211\nh\u2032\u2208H\u03ba P (h\u2032|h)\u03b3\u2113(h\u2032)\u2212t ( V \u03c0t\u00b5 (h \u2032)\u2212 V \u03c0\u00b5 (h\u2032) )\n(c) \u2264 \u2211\n\u03ba\u2208K\n\u2211\nh\u2032\u2208H\u03ba,d\nP (h\u2032|h) ( V \u03c0t\u00b5 (h \u2032)\u2212 V \u03c0\u00b5 (h\u2032) )\n+ \u01eb\n8\n(d) \u2264 \u2211\n\u03ba\u2208K\n\u2211\nh\u2032\u2208H\u03ba,d\nP (h\u2032|h) ( V \u2217\u00b5 (h \u2032; d)\u2212 V \u03c0\u00b5 (h\u2032; d) )\n+ \u01eb\n4\n(e) \u2264 \u2211\n\u03ba\u2208K\n\u2211\nh\u2032\u2208H\u03ba,d\nP (h\u2032|h)4\u01eb\u03ba + \u01eb\n4 ,\n(a) follows from Equation (4). (b) by noting that that \u03c0 = \u03c0t until an exploration phase is triggered. (c) by replacing H\u03ba with H\u03ba,d and noting that if h\u2032 \u2208 H\u03ba\u2212H\u03ba,d, then \u03b3\u2113(h\n\u2032)\u2212t \u2264 (1\u2212\u03b3)\u01eb/8. (d) by substituting V \u2217\u00b5 (h\n\u2032) \u2265 V \u03c0t\u00b5 (h\u2032) and by using the effective horizon to truncate the value functions. (e) by the definition of a \u03ba-exploration phase.\nSince the maximum of a set is greater than the average, there exists a \u03ba \u2208 K such that \u2211\nh\u2032\u2208H\u03ba,d P (h \u2032|h) \u2265 2\u2212\u03ba\u22123/|K|, which is the probability that MERL encounters a \u03ba-exploration phase within d time-steps from h. Now fix a \u03ba and let t1, t2, \u00b7 \u00b7 \u00b7 , \u00b7 \u00b7 \u00b7 , tG\u03ba be the sequence of timesteps such that ti is the start of a failure phase and the probability of a \u03ba-exploration phase within the next d time-steps is at least 2\u2212\u03ba\u22123/|K|. Let Yi \u2208 {0, 1} be the event that a \u03ba-exploration phase does occur within d time-steps of ti and define an auxiliary infinite sequence Y\u03031, Y\u03032, \u00b7 \u00b7 \u00b7 by Y\u0303i := Yi if i \u2264 G\u03ba and 1 otherwise. Let E\u03ba be the number of \u03ba-exploration phases and Gmax,\u03ba be a constant to be chosen later and suppose G\u03ba > Gmax,\u03ba, then \u2211Gmax,\u03ba i=1 Y\u0303i = \u2211Gmax,\u03ba i=1 Yi and either \u2211Gmax,\u03ba\ni=1 Y\u0303i \u2264 Emax,\u03ba or E\u03ba > Emax,\u03ba, where the latter follows because Yi = 1 implies a \u03ba-exploration phase occurred. Therefore\nP {G\u03ba > Gmax,\u03ba}\n\u2264 P\n\n\n\nGmax,\u03ba \u2211\ni=1\nY\u0303i < Emax,\u03ba\n\n\n\n+ P {E\u03ba > Emax,\u03ba}\n\u2264 P\n\n\n\nGmax,\u03ba \u2211\ni=1\nY\u0303i < Emax,\u03ba\n\n\n\n+ \u03b4\n4|K| .\nWe now choose Gmax,\u03ba sufficiently large to bound the first term in the display above by \u03b4/(4|K|). By the definition of Y\u0303i and Yi, if i \u2264 G\u03ba then E[Y\u0303i|Fti ] \u2265 2\u2212\u03ba\u22123/|K| and for i > G\u03ba, Y\u0303i is always 1. Setting\nGmax,\u03ba := 2 \u03ba+4|K|Emax,\u03ba\n= 217N |K|\n\u01eb\u01eb\u03ba(1\u2212 \u03b3)2 log2\n29N\n\u01eb2(1\u2212 \u03b3)2\u03b41 is sufficient to guarantee E[\n\u2211Gmax,\u03ba i=1 Y\u0303i] > 2Emax,\u03ba and an application of Azuma\u2019s inequality to the\nmartingale difference sequence completes the result. Finally we apply the union bound over all \u03ba and set Gmax := \u2211 \u03ba\u2208N Gmax,\u03ba > \u2211 \u03ba\u2208KGmax,\u03ba."}, {"heading": "4 Compact Case", "text": "In the last section we presented MERL and proved a sample-complexity bound for the case when the environment class is finite. In this section we show that if the number of environments is infinite, but compact with respect to the topology generated by a natural metric, then samplecomplexity bounds are still possible with a minor modification of MERL. The key idea is to use compactness to cover the space of environments with \u01eb-balls and compute statistics on these balls rather than individual environments. Since all environments in the same \u01eb-ball are sufficiently close, the resulting statistics cannot be significantly different and all analysis goes through identically to the finite case. Define a topology on the space of all environments induced by the pseudo-metric\nd(\u03bd1, \u03bd2) := sup h,\u03c0\n|V \u03c0\u03bd1(h)\u2212 V \u03c0\u03bd2(h)|.\nTheorem 6. Let M be compact and coverable by N \u01eb-balls then a modification of Algorithm 1 satisfies\nP { L2\u01eb\u00b5,\u03c0(h) \u2265 d \u00b7 (Gmax + Emax) } \u2264 \u03b4.\nThe main modification is to define statistics on elements of the cover, rather than specific environments.\n1. Let U1, \u00b7 \u00b7 \u00b7 , UN be an \u01eb-cover of M.\n2. At each time-step choose U and U such that \u03bd \u2208 U and \u03bd \u2208 U .\n3. Define statistics {X} on elements of the cover, rather than environments, by\nX(U, \u03ba)E(U,\u03ba) := inf \u03bd\u2208U\n(1\u2212 \u03b3)(R\u2212 V \u03c0\u03bd (h))\nX(U, \u03ba)E(U,\u03ba) := inf \u03bd\u2208U\n(1\u2212 \u03b3)(V \u03c0\u03bd (h)\u2212R)\n4. If there exists a U where the test fails then eliminate all environments in that cover.\nThe proof requires only small modifications to show that with high probability the U containing the true environment is never discarded, while those not containing the true environment are if tested sufficiently often."}, {"heading": "5 Unbounded Environment Classes", "text": "If the environment class is non-compact then we cannot in general expect finite sample-complexity bounds. Indeed, even asymptotic results are usually not possible.\nTheorem 7. There exist non-compact M for which no agent has a finite PAC bound.\nThe obvious example is when M is the set of all environments. Then for any policy M includes an environment that is tuned to ensure the policy acts sub-optimally infinitely often. A more interesting example is the class of all computable environments, which is non-compact and also does not admit algorithms with uniform finite sample-complexity. See negative results by Lattimore and Hutter [2011b] for counter-examples."}, {"heading": "6 Lower Bound", "text": "We now turn our attention to the lower bound. In specific cases, the bound in Theorem 1 is very weak. For example, if M is the class of finite MDPs with |S| states then a natural covering leads to a PAC bound with exponential dependence on the state-space while it is known that the true dependence is at most quadratic. This should not be surprising since information about the transitions for one state gives information about a large subset ofM, not just a single environment. We show that the bound in Theorem 1 is unimprovable for general environment classes except for logarithmic factors. That is, there exists a class of environments where Theorem 1 is nearly tight.\nThe simplest counter-example is a set of MDPs with four states, S = {0, 1,\u2295,\u2296} and N actions, A = {a1, \u00b7 \u00b7 \u00b7 , aN}. The rewards and transitions are depicted in Figure 3 where the transition probabilities depend on the action. Let M := {\u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdN} where for \u03bdk we set \u01eb(ai) = [[i = k]]\u01eb(1 \u2212 \u03b3). Therefore in environment \u03bdk, ak is the optimal action in state 1. M can be viewed as a set of bandits with rewards in (0, 1/(1\u2212 \u03b3)). In the bandit domain tight lower bounds on sample-complexity are known and given in Mannor and Tsitsiklis [2004]. These results can be applied as in Strehl et al. [2009] and Lattimore and Hutter [2012] to show that no algorithm has sample-complexity less than O( N\n\u01eb2(1\u2212\u03b3)3 log 1 \u03b4 )."}, {"heading": "7 Conclusions", "text": "Summary. The Maximum Exploration Reinforcement Learning algorithm was presented. For finite classes of arbitrary environments a sample-complexity bound was given that is linear in the number of environments. We also presented lower bounds that show that in general this cannot be improved except for logarithmic factors. Learning is also possible for compact classes with the sample complexity depending on the size of the smallest \u01eb-cover where the distance between two environments is the difference in value functions over all policies and history sequences. Finally, for non-compact classes of environments sample-complexity bounds are typically not possible.\nRunning time. The running time of MERL can be arbitrary large since computing the policy maximising \u2206 depends on the environment class used. Even assuming the distribution of observation/rewards given the history can be computed in constant time, the values of optimal policies can still only be computed in time exponential in the horizon.\nFuture work. MERL is close to unimprovable in the sense that there exists a class of environments where the upper bound is nearly tight. On the other hand, there are classes of environments where the bound of Theorem 1 scales badly compared to the bounds of tuned algorithms (for example, finite state MDPs). It would be interesting to show that MERL, or a variant thereof, actually performs comparably to the optimal sample-complexity even in these cases. This question is likely to be subtle since there are unrealistic classes of environments where the algorithm minimising sample-complexity should take actions leading directly to a trap where it receives low reward eternally, but is never (again) sub-optimal. Since MERL will not behave this way it will tend to have poor sample-complexity bounds in this type of environment class. This is really a failure of the sample-complexity optimality criterion rather than MERL, since jumping into non-rewarding traps is clearly sub-optimal by any realistic measure.\nAcknowledgements. This work was supported by ARC grant DP120100950."}, {"heading": "M. Azar, R. Munos, and B. Kappen. On the sample complexity of reinforcement learning with a generative", "text": "model. In Proceedings of the 29th international conference on machine learning, New York, NY, USA, 2012. ACM."}, {"heading": "D. Chakraborty and P. Stone. Structure learning in ergodic factored mdps without knowledge of the", "text": "transition function\u2019s in-degree. In Proceedings of the Twenty Eighth International Conference on Machine Learning (ICML\u201911), 2011.\nC. Diuk, L. Li, and B. Leffler. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Andrea Pohoreckyj Danyluk, Le\u0301on Bottou, and Michael L. Littman, editors, Proceedings of the 26th Annual International Conference on Machine Learning (ICML 2009), pages 249\u2013256. ACM, 2009."}, {"heading": "E. Even-dar, S. Kakade, and Y. Mansour. Reinforcement learning in POMDPs without resets. In In", "text": "IJCAI, pages 690\u2013695, 2005."}, {"heading": "M. Hutter. Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures.", "text": "In Proc. 15th Annual Conf. on Computational Learning Theory (COLT\u201902), volume 2375 of LNAI, pages 364\u2013379, Sydney, 2002. Springer, Berlin. URL http://arxiv.org/abs/cs.AI/0204040 .\nM. Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin, 2005. URL http://www.hutter1.net/ai/uaibook.htm."}, {"heading": "T. Lattimore and M. Hutter. Time consistent discounting. In Jyrki Kivinen, Csaba Szepesva\u0301ri, Esko", "text": "Ukkonen, and Thomas Zeugmann, editors, Algorithmic Learning Theory, volume 6925 of Lecture Notes in Computer Science. Springer Berlin / Heidelberg, 2011a."}, {"heading": "T. Lattimore and M. Hutter. Asymptotically optimal agents. In Jyrki Kivinen, Csaba Szepesva\u0301ri, Esko", "text": "Ukkonen, and Thomas Zeugmann, editors, Algorithmic Learning Theory, volume 6925 of Lecture Notes in Computer Science. Springer Berlin / Heidelberg, 2011b."}, {"heading": "T. Lattimore and M. Hutter. PAC bounds for discounted MDPs. Technical report, 2012. http://torlattimore.com/pubs/pac-tech.pdf.", "text": ""}, {"heading": "S. Mannor and J. Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. J.", "text": "Mach. Learn. Res., 5:623\u2013648, December 2004. ISSN 1532-4435."}, {"heading": "M. Odalric-Ambrym, P. Nguyen, R. Ortner, and D. Ryabko. Optimal regret bounds for selecting the state", "text": "representation in reinforcement learning. In Proceedings of the Thirtieth International Conference on Machine Learning (ICML\u201913), 2013.\nD. Ryabko and M. Hutter. On the possibility of learning in reactive environments with arbitrary dependence. Theoretical Computer Science, 405(3):274\u2013284, 2008.\nA. Strehl and M. Littman. A theoretical analysis of model-based interval estimation. In Proceedings of the 22nd international conference on Machine learning, ICML \u201905, pages 856\u2013863, 2005.\nA. Strehl, L. Li, and M. Littman. Reinforcement learning in finite MDPs: PAC analysis. J. Mach. Learn. Res., 10:2413\u20132444, December 2009.\nP. Sunehag and M. Hutter. Optimistic agents are asymptotically optimal. In Proceedings of the 25th Australasian AI conference, 2012."}, {"heading": "I. Szita and C. Szepesva\u0301ri. Model-based reinforcement learning with nearly tight exploration complexity", "text": "bounds. In Proceedings of the 27th international conference on Machine learning, pages 1031\u20131038, New York, NY, USA, 2010. ACM."}, {"heading": "A Technical Results", "text": "Lemma 8. Let x, y \u2208 [0, 1]N satisfy \u2211Ni=1 yi = 1 and \u2211N i=1 xiyi \u2265 1/2. Then maxi x2i yi > 1/(4N).\nProof. The result essentially follows from the fact that a maximum is greater than an average.\nN \u2211\ni=1\nx2i yi =\nN \u2211\ni=1\nxiyi \u2212 N \u2211\ni=1\nxiyi(1\u2212 xi)\n\u2265 1 2 \u2212\nN \u2211\ni=1\nxiyi(1 \u2212 xi) \u2265 1\n2 \u2212\nN \u2211\ni=1\nyi 4 = 1 4\nTherefore there exists an i such that x2i yi \u2265 1/(4N) as required.\nLemma 9. Let a, b > 2 and x := 4a(log ab)2. Then x \u2265 a log bx.\nLemma 10. Let \u03b1j := \u2308 \u03b1j \u2309 where \u03b1 := 4 \u221a N\n4 \u221a N\u22121 . Then \u2211\u221e j=1 \u03b1 \u22121 j \u2264 4\n\u221a N .\nProof. We have 1 \u03b1j \u2264 ( 1 \u03b1 )j < 1. Therefore by the geometric series,\n\u221e \u2211\nj=1\n1 \u03b1j \u2264 1\n1\u2212 1 \u03b1 \u2261 1 1\u2212 4 \u221a N\u22121\n4 \u221a N\n= 4 \u221a N\nas required.\nB Constants\nd 11\u2212\u03b3 log 8 (1\u2212\u03b3)\u01eb \u01eb\u03ba 2 \u03ba\u22122\u01eb\nGmax 216N |K| \u01eb2(1\u2212\u03b3)2 log 2 29N \u01eb2(1\u2212\u03b3)2\u03b41 Gmax,\u03ba 217N |K| \u01eb\u01eb\u03ba(1\u2212\u03b3)2 log 2 29N \u01eb2(1\u2212\u03b3)2\u03b41 Emax 216N \u01eb2(1\u2212\u03b3)2 log 2 29N \u01eb2(1\u2212\u03b3)2\u03b41 Emax,\u03ba 211N\n\u01eb2\u03ba(1\u2212\u03b3)2 log2 2 9N \u01eb2(1\u2212\u03b3)2\u03b41\n\u03b1 4 \u221a N\n4 \u221a N\u22121\n\u03b41 \u03b4\n32|K|N3/2\n|K| log2 1\u01eb(1\u2212\u03b3) + 2"}, {"heading": "C Table of Notation", "text": "N number of candidate models\n\u01eb required accuracy\n\u03b4 probability that an algorithm makes more mistakes than its sample-complexity\nt time-step\nht history at time-step t\nV \u03c0\u00b5 (h) value of policy \u03c0 in environment \u00b5 given history h\nd effective horizon\n\u00b5 true environment\n\u03bd an environment\n\u03bd, \u03bd models achieving upper and lower bounds on the value of the exploration policy\n\u03b3 discount factor Satisfies \u03b3 \u2208 (0, 1) Emax,\u03ba high probability bound on the number of \u03ba-exploration phases\nEmax high probability bound on the number of exploration phases\nE\u221e number of exploration phases\nE\u221e(\u03bd, \u03ba) number of \u03bd-exploration phases\nEt(\u03bd, \u03ba) number of (\u03bd, \u03ba)-exploration phases at time-step t\nFt(\u03bd, \u03ba) number of effective (\u03bd, \u03ba)-exploration phases at time-step t\nX(\u03bd, \u03ba)i ith test statistic for (\u03bd, \u03ba) pair"}], "references": [{"title": "Near-optimal regret bounds for reinforcement learning", "author": ["P. Auer", "T. Jaksch", "R. Ortner"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2010}, {"title": "On the sample complexity of reinforcement learning with a generative", "author": ["Learn. Res"], "venue": null, "citeRegEx": "Res.,? \\Q2010\\E", "shortCiteRegEx": "Res.", "year": 2010}, {"title": "Reinforcement learning in POMDPs without resets", "author": ["E. Even-dar", "S. Kakade", "Y. Mansour"], "venue": null, "citeRegEx": "Even.dar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Even.dar et al\\.", "year": 2009}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "Time consistent discounting", "author": ["Springer", "Berlin"], "venue": null, "citeRegEx": "Springer and Berlin,? \\Q2005\\E", "shortCiteRegEx": "Springer and Berlin", "year": 2005}, {"title": "Optimal regret bounds for selecting the state", "author": ["Mach. Learn"], "venue": null, "citeRegEx": "Learn.,? \\Q2004\\E", "shortCiteRegEx": "Learn.", "year": 2004}, {"title": "Optimistic agents are asymptotically optimal", "author": ["P. Sunehag", "M. Hutter"], "venue": null, "citeRegEx": "Sunehag and Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "More recent work on closing the gap between upper and lower bounds is by Szita and Szepesv\u00e1ri [2010], Lattimore and Hutter [2012], Azar et al.", "startOffset": 116, "endOffset": 130}, {"referenceID": 1, "context": "More recent work on closing the gap between upper and lower bounds is by Szita and Szepesv\u00e1ri [2010], Lattimore and Hutter [2012], Azar et al. [2012]. In the undiscounted case it is necessary to make some form of ergodicity assumption as without this regret bounds cannot be given.", "startOffset": 116, "endOffset": 150}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited.", "startOffset": 62, "endOffset": 81}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in.", "startOffset": 62, "endOffset": 229}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in. Even-dar et al. [2005] give essentially unimprovable exponential bounds on the sample-complexity of learning in finite partially observable MDPs.", "startOffset": 62, "endOffset": 277}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in. Even-dar et al. [2005] give essentially unimprovable exponential bounds on the sample-complexity of learning in finite partially observable MDPs. Odalric-Ambrym et al. [2013] show regret bounds", "startOffset": 62, "endOffset": 429}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b].", "startOffset": 191, "endOffset": 205}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b].", "startOffset": 191, "endOffset": 231}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b].", "startOffset": 191, "endOffset": 258}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b]. Perhaps the closest related worked is Diuk et al.", "startOffset": 191, "endOffset": 317}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b]. Perhaps the closest related worked is Diuk et al. [2009], which deals with a similar problem in the rather different setting of learning the optimal predictor from a class of N experts.", "startOffset": 191, "endOffset": 375}, {"referenceID": 3, "context": "2 Notation The definition of environments is borrowed from the work of Hutter [2005], although the notation is slightly more formal to ease the application of martingale inequalities.", "startOffset": 71, "endOffset": 85}, {"referenceID": 3, "context": "2 Notation The definition of environments is borrowed from the work of Hutter [2005], although the notation is slightly more formal to ease the application of martingale inequalities. General. N = {0, 1, 2, \u00b7 \u00b7 \u00b7 } is the natural numbers. For the indicator function we write [[x = y]] = 1 if x = y and 0 otherwise. We use \u2227 and \u2228 for logical and/or respectively. If A is a set then |A| is its size and A\u2217 is the set of all finite strings (sequences) over A. If x and y are sequences then x \u228f y means that x is a prefix of y. Unless otherwise mentioned, log represents the natural logarithm. For random variable X we write EX for its expectation. For x \u2208 R, \u2308x\u2309 is the ceiling function. Environments and policies. Let A, O and R \u2282 R be finite sets of actions, observations and rewards respectively and H := A \u00d7 O \u00d7 R. H\u221e is the set of infinite history sequences while H\u2217 := (A \u00d7 O \u00d7 R)\u2217 is the set of finite history sequences. If h \u2208 H\u2217 then l(h) is the number of action/observation/reward tuples in h. We write at(h), ot(h), rt(h) for the tth action/observation/reward of history sequence h. For h \u2208 H\u2217, \u0393h := {h\u2032 \u2208 H\u221e : h \u228f h\u2032} is the cylinder set. Let F := \u03c3({\u0393h : h \u2208 H\u2217}) and Ft := \u03c3({\u0393h : h \u2208 H\u2217 \u2227 l(h) = t}) be \u03c3-algebras. An environment \u03bc is a set of conditional probability distributions over observation/reward pairs given the history so far. A policy \u03c0 is a function \u03c0 : H\u2217 \u2192 A. An environment and policy interact sequentially to induce a measure, P\u03bc,\u03c0, on filtered probability space (H\u221e,F , {Ft}). For convenience, we abuse notation and write P\u03bc,\u03c0(h) := P\u03bc,\u03c0(\u0393h). If h \u228f h \u2032 then conditional probabilities are P\u03bc,\u03c0(h \u2032|h) := P\u03bc,\u03c0(h\u2032)/P\u03bc,\u03c0(h). Rt(h; d) := \u2211t+d k=t \u03b3 k\u2212trk(h) is the d-step return function and Rt(h) := limd\u2192\u221e Rt(h; d). Given history ht with l(ht) = t, the value function is defined by V \u03c0 \u03bc (ht; d) := E[Rt(h; d)|ht] where the expectation is taken with respect to P\u03bc,\u03c0(\u00b7|ht). V \u03c0 \u03bc (ht) := limd\u2192\u221e V \u03c0 \u03bc (ht; d). The optimal policy for environment \u03bc is \u03c0 \u2217 \u03bc := argmax\u03c0 V \u03c0 \u03bc , which with our assumptions is known to exist Lattimore and Hutter [2011a]. The value of the optimal policy is V \u2217 \u03bc := V \u03c0\u2217 \u03bc \u03bc .", "startOffset": 71, "endOffset": 2078}, {"referenceID": 0, "context": "This is comparable to the updating of confidence intervals for algorithms such as MBIE (Strehl and Littman, 2005) or UCRL2 (Auer et al., 2010).", "startOffset": 123, "endOffset": 142}, {"referenceID": 3, "context": "See negative results by Lattimore and Hutter [2011b] for counter-examples.", "startOffset": 38, "endOffset": 53}, {"referenceID": 3, "context": "[2009] and Lattimore and Hutter [2012] to show that no algorithm has sample-complexity less than O( N \u01eb2(1\u2212\u03b3)3 log 1 \u03b4 ).", "startOffset": 25, "endOffset": 39}], "year": 2013, "abstractText": "We present a new algorithm for general reinforcement learning where the true environment is known to belong to a finite class of N arbitrary models. The algorithm is shown to be near-optimal for all but O(N log N) time-steps with high probability. Infinite classes are also considered where we show that compactness is a key criterion for determining the existence of uniform sample-complexity bounds. A matching lower bound is given for the finite case.", "creator": "LaTeX with hyperref package"}}}