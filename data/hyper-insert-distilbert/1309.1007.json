{"id": "1309.1007", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2013", "title": "Concentration in unbounded metric spaces and algorithmic stability", "abstract": "we prove an alternative extension problem of the mcdiarmid'earlier s exclusion inequality for metric spaces with unbounded diameter. to maximize this end, we should introduce the quantitative notion of the { \\ em subgaussian diameter }, at which simply is termed a strongly distribution - distance dependent optimal refinement of the metric diameter. further our technique provides an alternative programming approach : to analyze that problem of lee kutin and niyogi'm s explicit method of weakly labeled difference - bounded estimation functions, demonstrating and proving yields sufficiently nontrivial, dimension - limitation free results in essentially some interesting limited cases where the derived former does believe not. writing as an implementation application, we give apparently the first general generalization bound in the algorithmic convergence stability model setting that holds in for unbounded range loss functions. we furthermore extend our random concentration depth inequality to strongly mixing processes.", "histories": [["v1", "Wed, 4 Sep 2013 12:40:31 GMT  (17kb)", "https://arxiv.org/abs/1309.1007v1", null], ["v2", "Wed, 11 Sep 2013 16:24:52 GMT  (19kb)", "http://arxiv.org/abs/1309.1007v2", null]], "reviews": [], "SUBJECTS": "math.PR cs.LG math.FA", "authors": ["aryeh kontorovich"], "accepted": true, "id": "1309.1007"}, "pdf": {"name": "1309.1007.pdf", "metadata": {"source": "CRF", "title": "Concentration in unbounded metric spaces and algorithmic stability", "authors": ["Aryeh Kontorovich"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 9.\n10 07\nv2 [\nm at\nh. PR\n] 1"}, {"heading": "1 Introduction", "text": "Concentration of measure inequalities are at the heart of statistical learning theory. Roughly speaking, concentration allows one to conclude that the performance of a (sufficiently \u201cstable\u201d) algorithm on a (sufficiently \u201cclose to iid\u201d) sample is indicative of the algorithm\u2019s performance on future data. Quantifying what it means for an algorithm to be stable and for the sampling process to be close to iid is by no means straightforward and much recent work has been motivated by these questions. It turns out that the various notions of stability are naturally expressed in terms of the Lipschitz continuity of the algorithm in question (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al., 2009; Zou et al.; Mohri and Rostamizadeh, 2010; London et al., 2012, 2013; Shalizi and Kontorovich, 2013).\nAn elegant and powerful work-horse driving many of the aforementioned results is McDiarmid\u2019s inequality (McDiarmid, 1989):\nP(|\u03d5\u2212 E\u03d5| > t) \u2264 2 exp ( \u2212 2t 2\n\u2211n i=1 w 2 i\n)\n, (1)\nwhere \u03d5 is a real-valued function of the sequence of independent random variables X = (X1, . . . , Xn), such that\n|\u03d5(x) \u2212 \u03d5(x\u2032)| \u2264 wi (2)\nwhenever x and x\u2032 differ only in the ith coordinate. Aside from being instrumental in proving PAC bounds (Boucheron et al., 2005), McDiarmid\u2019s inequality has also found use in algorithmic stability results (Bousquet and Elisseeff, 2002). Non-iid extensions of (1) have also been considered (Marton, 1996; Rio, 2000; Chazottes et al., 2007; Kontorovich and Ramanan, 2008).\nThe distribution-free nature of McDiarmid\u2019s inequality makes it an attractive tool in learning theory, but also imposes inherent limitations on its applicability. Chief among these limitations is the inability of (1) to provide risk bounds for unbounded loss functions. Even in the bounded case, if the Lipschitz condition (2) holds not everywhere but only with high probability \u2014 say, with a much larger constant on a small set of exceptions \u2014 the bound in (1) still charges the full cost of the worst-case constant. To counter this difficulty, Kutin (2002); Kutin and Niyogi (2002) introduced an extension of McDiarmid\u2019s inequality to weakly difference-bounded functions and used it to analyze the risk of \u201calmost-everywhere\u201d stable algorithms. This influential result has been invoked in a number of recent papers (El-Yaniv and Pechyony, 2006; Mukherjee et al., 2006; Hush et al., 2007; Agarwal and Niyogi, 2009; Shalev-Shwartz et al., 2010; Rubinstein and Simma, 2012).\nHowever, the approach of Kutin and Niyogi entails some difficulties as well. These come in two flavors: analytical (complex statement and proof) and practical (conditions are still too restrictive in some cases); we will elaborate upon this in Section 3. In this paper, we propose an alternative approach to the concentration of \u201calmost-everywhere\u201d or \u201caverage-case\u201d Lipschitz functions. To this end, we introduce the notion of the subgaussian diameter of a metric probability space. The latter may be finite even when the metric diameter is infinite, and we show that this notion generalizes the more restrictive property of bounded differences.\nMain results. This paper\u2019s principal contributions include defining the subgaussian diameter of a metric probability space and identifying its role in relaxing the bounded differences condition. In Theorem 1, we show that the subgaussian diameter can essentially replace the far more restrictive metric diameter in concentration bounds. This result has direct ramifications for algorithmic stability (Theorem 2). We furthermore extend our concentration inequality to non-independent processes (Theorem 3) and to other Orlicz norms (Theorem 4).\nOutline of paper. In Section 2 we define the subgaussian diameter and relate it to (weakly) bounded differences in Section 3. We state and prove the concentration inequality based on this notion in Section 4 and give an application to algorithmic stability in Section 5. We then give an extension to non-independent\ndata in Section 6 and discuss other Orlicz norms in Section 7. Conclusions and some open problems are presented in Section 8."}, {"heading": "2 Preliminaries", "text": "A metric probability space (X , \u03c1, \u00b5) is a measurable space X whose Borel \u03c3algebra is induced by the metric \u03c1, endowed with the probability measure \u00b5. Our results are most cleanly presented when X is a discrete set but they continue to hold verbatim for Borel probability measures on Polish spaces. It will be convenient to write E\u03d5 = \u2211\nx\u2208X P(x)\u03d5(x) even when the latter is an integral. Random variables are capitalized (X), specified sequences are written in lowercase, the notation Xji = (Xi, . . . , Xj) is used for all sequences, and sequence concatenation is denoted multiplicatively: xjix k j+1 = x k i . We will frequently use the shorthand P(xji ) = \u220fj k=i P(Xk = xk). Standard order of magnitude notation such as O(\u00b7) and \u2126(\u00b7) will be used. A function \u03d5 : X \u2192 R is L-Lipschitz if\n|\u03d5(x) \u2212 \u03d5(x\u2032)| \u2264 L\u03c1(x, x\u2032), x, x\u2032 \u2208 X .\nLet (Xi, \u03c1i, \u00b5i), i = 1, . . . , n be a sequence of metric probability spaces. We define the product probability space\nXn = X1 \u00d7X2 . . .\u00d7Xn\nwith the product measure\n\u00b5n = \u00b51 \u00d7 \u00b52 \u00d7 . . .\u00d7 \u00b5n\nand \u21131 product metric\n\u03c1n(x, y) =\nn \u2211\ni=1\n\u03c1i(xi, yi), x, y \u2208 Xn. (3)\nWe will denote partial products by\nX ji = Xi \u00d7Xi+1 \u00d7 . . .\u00d7Xj .\nWe write Xi \u223c \u00b5i to mean that Xi is an Xi-valued random variable with law \u00b5i \u2014 i.e., P(Xi \u2208 A) = \u00b5i(A) for all Borel A \u2282 Xi. This notation extends naturally to sequences: Xn1 \u223c \u00b5n. We will associate to each (Xi, \u03c1i, \u00b5i) the symmetrized distance random variable \u039e(Xi) defined by\n\u039e(Xi) = \u01ebi\u03c1i(Xi, x\u2032i), (4)\nwhere Xi, x \u2032 i \u223c \u00b5i are independent and \u01ebi = \u00b11 with probability 1/2, independent of Xi, x \u2032 i. We note right away that \u039e(Xi) is a centered random variable:\nE[\u039e(Xi)] = 0. (5)\nA real-valued random variable X is said to be subgaussian if it admits a \u03c3 > 0 such that\nEe\u03bbX \u2264 e\u03c32\u03bb2/2, \u03bb \u2208 R. (6)\nThe smallest \u03c3 for which (6) holds will be denoted by \u03c3\u2217(X). We define the subgaussian diameter \u2206SG(Xi) of the metric probability space (Xi, \u03c1i, \u00b5i) in terms of its symmetrized distance \u039e(Xi):\n\u2206SG(Xi) = \u03c3\u2217(\u039e(Xi)). (7)\nIf a metric probability space (X , \u03c1, \u00b5) has finite diameter,\ndiam(X ) := sup x,x\u2032\u2208X \u03c1(x, x\u2032) < \u221e,\nthen its subgaussian diameter is also finite:\nLemma 1.\n\u2206SG(X ) \u2264 diam(X ).\nProof. Let \u039e = \u039e(X ) be the symmetrized distance. By (5), we have E[\u039e] = 0 and certainly |\u039e| \u2264 diam(X ). Hence,\nEe\u03bb\u039e \u2264 exp((2 diam(X )\u03bb)2/8) = exp(diam(X )2\u03bb2/2),\nwhere the inequality follows from Hoeffding\u2019s Lemma.\nThe bound in Lemma 1 is nearly tight in the sense that for every \u03b5 > 0 there is a metric probability space (X , \u03c1, \u00b5) for which\ndiam(X ) < \u2206SG(X ) + \u03b5. (8)\nTo see this, take X to be an N -point space with the uniform distribution and \u03c1(x, x\u2032) = 1 for all distinct x, x\u2032 \u2208 X . Taking N sufficiently large makes \u2206SG(X ) arbitrarily close to diam(X ) = 1. We do not know whether diam(X ) = \u2206SG(X ) can be achieved.\nOn the other hand, there exist unbounded metric probability spaces with finite subgaussian diameter. A simple example is (X , \u03c1, \u00b5) with X = R, \u03c1(x, x\u2032) = |x\u2212 x\u2032| and \u00b5 the standard Gaussian probability measure d\u00b5 = (2\u03c0)\u22121/2e\u2212x2/2dx. Obviously, diam(X ) = \u221e. Now the symmetrized distance \u039e = \u039e(X ) is distributed as the difference (=sum) of two standard Gaussians: \u039e \u223c N(0, 2). Since Ee\u03bb\u039e = e\u03bb 2 , we have\n\u2206SG(X ) = \u221a 2. (9)\nMore generally, the subgaussian distributions on R are precisely those for which \u2206SG(R) < \u221e."}, {"heading": "3 Related work", "text": "McDiarmid\u2019s inequality (1) suffers from the limitations mentioned above: it completely ignores the distribution and is vacuous if even one of the wi is infinite.1 In order to address some of these issues, Kutin (2002); Kutin and Niyogi (2002) proposed an extension of McDiarmid\u2019s inequality to \u201calmost everywhere\u201d Lipschitz functions \u03d5 : Xn \u2192 R. To formalize this, fix an i \u2208 [n] and let Xn1 \u223c \u00b5n and x\u2032i \u223c \u00b5i be independent. Define X\u0303n1 = X\u0303n1 (i) by\nX\u0303j(i) =\n{\nXj , j 6= i x\u2032i, j = i.\n(10)\nKutin and Niyogi define \u03d5 to be weakly difference-bounded by (b, c, \u03b4) if\nP\n( |\u03d5(X)\u2212 \u03d5(X\u0303(i))| > b ) = 0 (11)\nand\nP\n( |\u03d5(X)\u2212 \u03d5(X\u0303(i))| > c ) < \u03b4 (12)\nfor all 1 \u2264 i \u2264 n. The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state \u2014 indeed, the present work was motivated in part by a desire for simpler tools. Assuming that \u03d5 is weakly difference-bounded by (b, c, \u03b4) with\n\u03b4 = exp(\u2212\u2126(n)) (13) and c = O(1/n), their bound states that\nP(|\u03d5\u2212 E\u03d5| \u2265 t) \u2264 exp(\u2212\u2126(nt2)) (14) for a certain range of t and n. As noted by Rakhlin et al. (2005), the exponential decay assumption (13) is necessary in order for the Kutin-Niyogi method to yield exponential concentration. In contrast, the bounds we prove here\n(i) do not require |\u03d5(X)\u2212 \u03d5(X\u0303)| to be everywhere bounded as in (11) (ii) have a simple statement and proof, and generalize to non-iid processes\nwith relative ease.\nWe defer the quantitative comparisons between (14) and our results until the latter are formally stated in Section 4.\nIn a different line of work, Bentkus (2008) considered an extension of Hoeffding\u2019s inequality to unbounded random variables. His bound only holds for sums (as opposed to general Lipschitz functions) and the summands must be non-negative (i.e., unbounded only in the positive direction). An earlier notion of \u201ceffective\u201d metric diameter in the context of concentration is that of metric space length (Schechtman, 1982). Another distribution-dependent refinement of diameter is the spread constant (Alon et al., 1998). Lecue\u0301 and Mendelson (2013) gave minimax bounds for empirical risk minimization over subgaussian classes.\n1Note, though, that McDiarmid\u2019s inequality is sharp in the sense that the constants in (1) cannot be improved in a distribution-free fashion."}, {"heading": "4 Concentration via subgaussian diameter", "text": "McDiarmid\u2019s inequality (1) may be stated in the notation of Section 2 as follows. Let (Xi, \u03c1i, \u00b5i), i = 1, . . . , n be a sequence of metric probability spaces and \u03d5 : Xn \u2192 R a 1-Lipschitz function. Then\nP(|\u03d5\u2212 E\u03d5| > t) \u2264 2 exp ( \u2212 2t 2\n\u2211n i=1 diam(Xi)2\n)\n. (15)\nWe defined the subgaussian diameter \u2206SG(Xi) in Section 2, showing in Lemma 1 that it never exceeds the metric diameter. We also showed by example that the former can be finite when the latter is infinite. The main result of this section is that diam(Xi) in (15) can essentially be replaced by \u2206SG(Xi): Theorem 1. If \u03d5 : Xn \u2192 R is 1-Lipschitz then E\u03d5 < \u221e and\nP(|\u03d5\u2212 E\u03d5| > t) \u2264 2 exp ( \u2212 t 2\n2 \u2211n i=1 \u2206 2 SG (Xi)\n)\n.\nOur constant in the exponent is worse than that of (15) by a factor of 4; this appears to be an inherent artifact of our method.\nProof. The strong integrability of \u03d5 \u2014 and in particular, finiteness of E\u03d5 \u2014 follow from exponential concentration (Ledoux, 2001). The rest of the proof will proceed via the Azuma-Hoeffding-McDiarmid method of martingale differences. Define Vi = E[\u03d5 |X i1]\u2212 E[\u03d5 |X i\u221211 ] and expand\nE[\u03d5 |X i1] = \u2211\nxn i+1\u2208X n i+1\nP(xni+1)\u03d5(X i 1x n i+1)\nE[\u03d5 |X i\u221211 ] = \u2211\nxn i \u2208Xn i\nP(xni )\u03d5(X i\u22121 1 x n i ).\nLet V\u0303i be Vi conditioned on X i\u22121 1 ; thus,\nV\u0303i = \u2211\nxn i+1\nP(xni+1) \u2211\nxi,x\u2032i\nP(xi)P(x \u2032 i) ( \u03d5(X i\u221211 xix n i+1)\u2212 \u03d5(X i\u221211 x\u2032ixni+1) ) .\nHence, by Jensen\u2019s inequality, we have\nE[e\u03bbVi |X i\u221211 ] \u2264 \u2211\nxn i+1\nP(xni+1) \u2211\ny,y\u2032\nP(y)P(y\u2032)e\u03bb(\u03d5(X i\u22121 1 yx n i+1)\u2212\u03d5(X i\u22121 1 y \u2032xni+1)).\nFor fixed X i\u221211 \u2208 X i\u221211 and xni+1 \u2208 Xni+1, define F : Xi \u2192 R by F (y) = \u03d5(X i\u221211 yx n i+1), and observe that F is 1-Lipschitz with respect to \u03c1i. Since et + e\u2212t = 2 cosh(t) and cosh(t) \u2264 cosh(s) for all |t| \u2264 s, we have2\ne\u03bb(F (y)\u2212F (y \u2032)) + e\u03bb(F (y \u2032)\u2212F (y)) \u2264 e\u03bb\u03c1i(y,y\u2032) + e\u2212\u03bb\u03c1i(y,y\u2032), 2An analogous symmetrization technique is employed in http://terrytao.wordpress.com/ 2009/06/09/talagrands-concentration-inequality as a variant of the \u201csquare and rearrange\u201d trick.\nand hence\n\u2211\ny,y\u2032\u2208Xi\nP(y)P(y\u2032)e\u03bb(F (y)\u2212F (y \u2032)) (16)\n\u2264 12\n\n\n\u2211\ny,y\u2032\nP(y)P(y\u2032)e\u03bb\u03c1i(y,y \u2032) +\n\u2211\ny,y\u2032\nP(y)P(y\u2032)e\u2212\u03bb\u03c1i(y,y \u2032)\n\n\n= Ee\u03bb\u039e(Xi) \u2264 exp(\u03bb2\u22062 SG (Xi)/2),\nwhere \u039e(Xi) is the symmetrized distance (4) and the last inequality holds by definition of subgaussian diameter (6,7). It follows that\nE[e\u03bbVi |X i\u221211 ] \u2264 exp(\u03bb2\u22062SG(Xi)/2). (17)\nApplying the standard Markov\u2019s inequality and exponential bounding argument, we have\nP(\u03d5\u2212 E\u03d5 > t) = P ( n \u2211\ni=1\nVi > t\n)\n\u2264 e\u2212\u03bbtE [ n \u220f\ni=1\ne\u03bbVi\n]\n= e\u2212\u03bbtE\n[\nn \u220f\ni=1\nE[e\u03bbVi |X i\u221211 ] ]\n\u2264 e\u2212\u03bbtE [ n \u220f\ni=1\nexp(\u03bb2\u22062 SG (Xi)/2)\n]\n= exp\n(\n1 2 \u03bb2\nn \u2211\ni=1\n\u22062 SG (Xi)\u2212 \u03bbt\n)\n. (18)\nOptimizing over \u03bb and applying the same argument to E\u03d5\u2212\u03d5 yields our claim.\nLet us see how Theorem 1 compares to previous results on some examples. Consider Rn equipped with the \u21131 metric \u03c1 n(x, x\u2032) = \u2211\ni\u2208[n] |xi \u2212 x\u2032i| and the standard Gaussian product measure \u00b5n = N(0, In). Let \u03d5 : R\nn \u2192 R be 1/nLipschitz. Then Theorem 1 yields (recalling the calculation in (9))\nP(|\u03d5\u2212 E\u03d5| > \u03b5) \u2264 2 exp(\u2212n\u03b52/4), \u03b5 > 0, (19)\nwhereas the inequalities of McDiarmid (1) and Kutin-Niuyogi (14) are both uninformative since the metric diameter is infinite.\nFor our next example, fix an n \u2208 N and put Xi = {\u00b11,\u00b1n} with the metric \u03c1i(x, x \u2032) = |x \u2212 x\u2032| and the distribution \u00b5i(x) \u221d e\u2212x 2\n. One may verify via a calculation analogous to (9) that \u2206SG(Xi) \u2264 \u221a 2. For independent Xi \u223c \u00b5i,\ni = 1, . . . , n, put \u03d5(Xn1 ) = n \u22121 \u2211n i=1 Xi. Then Theorem 1 implies that in this case the bound in (19) holds verbatim. On the other hand, \u03d5 is easily seen to be weakly difference-bounded by (1, 1/n, e\u2212\u2126(n)) and thus (14) also yields subgaussian concentration, albeit with worse constants. Applying (1) yields the much cruder estimate\nP(|\u03d5\u2212 E\u03d5| > \u03b5) \u2264 2 exp(\u22122\u03b52)."}, {"heading": "5 Application to algorithmic stability", "text": "We refer the reader to (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005) for background on algorithmic stability and supervised learning. Our metric probability space (Zi, \u03c1i, \u00b5i) will now have the structure Zi = Xi \u00d7 Yi where Xi and Yi are, respectively, the instance and label space of the ith example. Under the iid assumption, the (Zi, \u03c1i, \u00b5i) are identical for all i \u2208 N (and so we will henceforth drop the subscript i from these). A training sample is S = Zn1 \u223c \u00b5n is drawn and a learning algorithm A inputs S and outputs a hypothesis f : X \u2192 Y. The hypothesis f = A(S) will be denoted by AS . In line with the previous literature, we assume that A is symmetric (i.e., invariant under permutations of S). The loss of a hypothesis f on an example z = (x, y) is defined by\nL(f, z) = \u2113(f(x), y),\nwhere \u2113 : Y \u00d7 Y \u2192 [0,\u221e) is the cost function. To our knowledge, all previous work required the loss to be bounded by some constant M < \u221e, which figures explicitly in the bounds; we make no such restriction.\nIn the algorithmic stability setting, the empirical risk R\u0302n(A, S) is typically defined as\nR\u0302n(A, S) = 1\nn\nn \u2211\ni=1\nL(AS , zi) (20)\nand the true risk R(A, S) as\nR(A, S) = Ez\u223c\u00b5[L(AS , z)]. (21)\nThe goal is to bound the excess risk R(A, S)\u2212 R\u0302n(A, S). To this end, a myriad of notions of hypothesis stability have been proposed. A variant of uniform stability in the sense of Rakhlin et al. (2005) \u2014 which is slightly more general than the homonymous notion in Bousquet and Elisseeff (2002)\u2014 may be defined as follows. The algorithm A is said to be \u03b2-uniform stable if for all z\u0303 \u2208 Z, the function \u03d5z\u0303 : Zn \u2192 R given by \u03d5z\u0303(z) = L(Az, z\u0303) is \u03b2-Lipschitz with respect to the Hamming metric on Zn:\n\u2200z\u0303 \u2208 Z, \u2200z, z\u2032 \u2208 Zn : |\u03d5z\u0303(z)\u2212 \u03d5z\u0303(z\u2032)| \u2264 \u03b2 n \u2211\ni=1\n1{zi 6=z\u2032i}.\nWe define the algorithm A to be \u03b2-totally Lipschitz stable if the function \u03d5 : Zn+1 \u2192 R given by \u03d5(zn+11 ) = L(Azn1 , zn+1) is \u03b2-Lipschitz with respect to the \u21131 product metric on Zn+1:\n\u2200z, z\u2032 \u2208 Zn+1 : |\u03d5(z)\u2212 \u03d5(z\u2032)| \u2264 \u03b2 n+1 \u2211\ni=1\n\u03c1(zi, z \u2032 i). (22)\nNote that total Lipschitz stability is stronger than uniform stability since it requires the algorithm to respect the metric of Z.\nLet us bound the bias of stable algorithms.\nLemma 2. Suppose A is a symmetric, \u03b2-totally Lipschitz stable learning algorithm over the metric probability space (Z, \u03c1, \u00b5) with \u2206SG(Z) < \u221e. Then\nE[R(A, S)\u2212 R\u0302n(A, S)] \u2264 12\u03b22\u22062SG(Z).\nProof. Observe, as in the proof of (Bousquet and Elisseeff, 2002, Lemma 7), that for all i \u2208 [n],\nE[R(A, S)\u2212 R\u0302n(A, S)] = EZn1 ,Z\u0303n1 [L(AZn1 , Z\u0303i)\u2212 L(AZ\u0303n1 , Z\u0303i)], (23)\nwhere Zn1 \u223c \u00b5n and Z\u0303 is generated from Z via the process defined in (10). For fixed i \u2208 [n] and Zi\u221211 ,Zni+1, define\nWi(Zi, Z \u2032 i) = L(AZn1 , Z \u2032 i)\u2212 L(AZi\u22121\n1 Z\u2032 i Zn i+1 , Z \u2032i)\nand note that (22) implies that |Wi(Zi, Z \u2032i)| \u2264 \u03b2\u03c1(Zi, Z \u2032i). Now rewrite (23) as\nE[R(A, S)\u2212 R\u0302n(A, S)] = \u2211\nzi\u221211 ,z n i+1\nP(zi\u221211 )P(z n i+1)\n\u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)Wi(zi, z \u2032 i). (24)\nInvoking Jensen\u2019s inequality and the argument in (16),\nexp\n\n\n\u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)Wi(zi, z \u2032 i)\n\n \u2264 \u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)e Wi(zi,z \u2032 i)\n= 12\n\n\n\u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)e Wi(zi,z \u2032 i) + \u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)e \u2212Wi(zi,z \u2032 i)\n\n\n\u2264 12\n\n\n\u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)e \u03b2\u03c1(zi,z \u2032 i) + \u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)e \u2212\u03b2\u03c1(zi,z \u2032 i)\n\n\n\u2264 exp(12\u03b2 2\u22062 SG (Z)).\nTaking logarithms yields the estimate \u2211\nzi,z \u2032\ni\nP(zi)P(z \u2032 i)Wi(zi, z \u2032 i) \u2264 12\u03b2 2\u22062 SG (Z), (25)\nwhich, after substituting (25) into (24), proves the claim.\nWe now turn to the Lipschitz continuity of the excess risk.\nLemma 3. Suppose A is a symmetric, \u03b2-totally Lipschitz stable learning algorithm and define the excess risk function \u03d5 : Zn \u2192 R by \u03d5(z) = R(A, z) \u2212 R\u0302n(A, z). Then \u03d5 is 3\u03b2-Lipschitz. Proof. We examine the two summands separately. The definition (21) of R(A, \u00b7) implies that the latter is \u03b2-Lipchitz since it is a convex combination of \u03b2Lipschitz functions. Now R\u0302n(A, \u00b7) defined in (20) is also a convex combination of \u03b2-Lipschitz functions, but because zi appears twice in L(Azn\n1 , zi), changing zi to\nz\u2032i could incur a difference of up to 2\u03b2\u03c1(zi, z \u2032 i). Hence, R\u0302n(A, \u00b7) is 2\u03b2-Lipschitz. As Lipschitz constants are subadditive, the claim is proved.\nCombining Lemmas 2 and 3 with our concentration inequality in Theorem 1 yields the main result of this section:\nTheorem 2. Suppose A is a symmetric, \u03b2-totally Lipschitz stable learning algorithm over the metric probability space (Z, \u03c1, \u00b5) with \u2206SG(Z) < \u221e. Then, for training samples S \u223c \u00b5n and \u03b5 > 0, we have\nP\n(\nR(A, S)\u2212 R\u0302n(A, S) > 12\u03b2 2\u22062 SG (Z) + \u03b5\n) \u2264 exp ( \u2212 \u03b5 2\n18\u03b22\u22062 SG (Z)n\n)\n.\nAs in Bousquet and Elisseeff (2002) and related results on algorithmic stability, we require \u03b2 = O(1/n) for exponential decay. Bousquet and Elisseeff showed that this is indeed the case for some popular learning algorithms, albeit in their less restrictive definition of stability. We conjecture that many of these algorithms continue to be stable in our stronger sense and plan to explore this in future work."}, {"heading": "6 Relaxing the independence assumption", "text": "In this section we generalize Theorem 1 to strongly mixing processes. To this end, we require some standard facts concerning the probability-theoretic notions of coupling and transportation (Lindvall, 2002; Villani, 2003, 2009). Given the probability measures \u00b5, \u00b5\u2032 on a measurable space X , a coupling \u03c0 of \u00b5, \u00b5\u2032 is any probability measure on X \u00d7X with marginals \u00b5 and \u00b5\u2032, respectively. Denoting by \u03a0 = \u03a0(\u00b5, \u00b5\u2032) the set of all couplings, we have\ninf \u03c0\u2208\u03a0\n\u03c0( { (x, y) \u2208 X 2 : x 6= y } ) = 12\n\u2211\nx\u2208X\n|\u00b5(x) \u2212 \u00b5\u2032(x)|\n= \u2016\u00b5\u2212 \u00b5\u2032\u2016 TV\n(26)\nwhere \u2016\u00b7\u2016 TV\nis the total variation norm. An optimal coupling is one that achieves the infimum in (26); one always exists, though it may not be unique. Another elementary property of couplings is that for any two f, g : X \u2192 R and any coupling \u03c0 \u2208 \u03a0(\u00b5, \u00b5\u2032), we have\nE\u00b5f \u2212 E\u00b5\u2032g = E(X,X\u2032)\u223c\u03c0[f(X)\u2212 g(X \u2032)]. (27)\nIt is possible to refine the total variation distance between \u00b5 and \u00b5\u2032 so as to respect the metric of X . Given a space equipped with probability measures \u00b5, \u00b5\u2032 and metric \u03c1, define the transportation cost3 distance T\u03c1(\u00b5, \u00b5 \u2032) by\nT\u03c1(\u00b5, \u00b5 \u2032) = inf\n\u03c0\u2208\u03a0(\u00b5,\u00b5\u2032) E(X,X\u2032)\u223c\u03c0\u03c1(X,X\n\u2032).\nIt is easy to verify that T\u03c1 is a valid metric on probability measures and that for \u03c1(x, x\u2032) = 1{x 6=x\u2032}, we have T\u03c1(\u00b5, \u00b5 \u2032) = \u2016\u00b5\u2212 \u00b5\u2032\u2016 TV\n. As in Section 4, we consider a sequence of metric spaces (Xi, \u03c1i), i = 1, . . . , n and their \u21131 product (Xn, \u03c1n). Unlike the independent case, we will allow nonproduct probability measures \u03bd on (Xn, \u03c1n). We will write Xn1 \u223c \u03bd to mean that P(Xn1 \u2208 A) = \u03bd(A) for all Borel A \u2282 Xn. For 1 \u2264 i \u2264 j < k \u2264 l \u2264 n, we will use the shorthand\nP(xlk |xji ) = P ( X lk = x l k |Xji = x j i ) .\nThe notation P(Xji ) means the marginal distribution ofX j i . Similarly, P(X l k |X j i = xji ) will denote the conditional distribution. For 1 \u2264 i < n, and xi1 \u2208 X i1 , x\u2032i \u2208 Xi define\n\u03c4i(x i 1, x \u2032 i) = T\u03c1ni+1(P(X n i+1 |X i1 = xi1),P(Xni+1 |X i1 = xi\u221211 x\u2032i)),\nwhere \u03c1ni+1 is the \u21131 product of \u03c1i+1, . . . \u03c1n as in (3), and\n\u03c4\u0304i = sup xi1\u2208X i 1 ,x \u2032 i \u2208Xi\n\u03c4i(x i 1, x \u2032 i),\nwith \u03c4\u0304n \u2261 0. In words, \u03c4i(xi1, x\u2032i) measures the transportation cost distance between the conditional distributions induced on the \u201ctail\u201d Xni+1 given two prefixes that differ in the ith coordinate, and \u03c4\u0304i is the maximal value of this quantity. Kontorovich (2007); Kontorovich and Ramanan (2008) discuss how to handle conditioning on measure-zero sets and other technicalities. Note that for product measures the conditional distributions are identical and hence \u03c4\u0304i = 0.\nWe need one more definition before stating our main result. For the prefix xi\u221211 , define the conditional distribution\n\u03bdi(x i\u22121 1 ) = P\n( Xi |X i\u221211 = xi\u221211 )\nand consider the corresponding metric probability space (Xi, \u03c1i, \u03bdi(xi\u221211 )). Define its conditional subgaussian diameter by\n\u2206SG(Xi |xi\u221211 ) = \u2206SG(Xi, \u03c1i, \u03bdi(xi\u221211 )) 3This fundamental notion is also known as the Wasserstein, Monge-Kantorovich, or earthmover distance; see Villani (2003, 2009) for an encyclopedic treatment. The use of coupling and transportation techniques to obtain concentration for dependent random variables goes back to Marton (1996); Samson (2000); Chazottes et al. (2007).\nand the maximal subgaussian diameter by\n\u2206\u0304SG(Xi) = sup xi\u221211 \u2208X i\u22121 1 \u2206SG(Xi |xi\u221211 ). (28)\nNote that for product measures, (28) reduces to the former definition (7). With these definitions, we may state the main result of this section.\nTheorem 3. If \u03d5 : Xn \u2192 R is 1-Lipschitz with respect to \u03c1n, then\nP(|\u03d5\u2212 E\u03d5| > t) \u2264 2 exp ( \u2212 (t\u2212\u2211i\u2264n \u03c4\u0304i)2\n2 \u2211 i\u2264n \u2206\u0304 2 SG (Xi)\n)\n, t > 0.\nObserve that we recover Theorem 1 as a special case. Since typically we will take t = \u03b5n, it suffices that \u2211\ni\u2264n \u03c4\u0304i = o(n) and \u2211 i\u2264n \u2206\u0304 2 SG (Xi) = O(n) to\nensure a exponential bound with decay rate exp(\u2212\u2126(n\u03b52)).\nProof. We begin by considering the martingale difference\nVi = E[\u03d5 |X i1 = xi1]\u2212 E[\u03d5 |X i\u221211 = xi\u221211 ]\nas in the proof of Theorem 1. More explicitly,\nVi = \u2211\nxni+1\nP(xni+1 |xi1)\u03d5(xi1xni+1)\u2212 \u2211\nxni\nP(xni |xi\u221211 )\u03d5(xi\u221211 xni )\n= \u2211\nx\u2032i\nP(x\u2032i |xi\u221211 ) \u2211\nxni+1\n[ P(xni+1 |xi1)\u03d5(xi1xni+1)\u2212 P(xni+1 |xi\u221211 x\u2032i)\u03d5(xi\u221211 x\u2032ixni+1) ] .\nDefine V\u0303i to be Vi conditioned on X i\u22121 1 . Then\nV\u0303i = \u2211\nxi,x\u2032i\nP(xi |X i\u221211 )P(x\u2032i |X i\u221211 )\u00b7 (29)\n\u2211\nxni+1\n[P(xni+1 |X i\u221211 xi)\u03d5(X i\u221211 x\u2032ixni+1)\u2212 P(xni+1 |X i\u221211 x\u2032i)\u03d5(X i\u221211 xixni+1)].\nLet \u03c0 be an optimal coupling realizing the infimum in the transportation cost distance T\u03c1n\ni+1 used to define \u03c4i(x i 1, x \u2032 i). Recalling (27), we have\n\u2211\nxni+1\n[ P(xni+1 |X i\u221211 xi)\u03d5(X i\u221211 xixni+1)\u2212 P(xni+1 |X i\u221211 x\u2032i)\u03d5(X i\u221211 x\u2032ixni+1) ]\n= E(X\u0307n i+1 ,X\u0308n i+1 )\u223c\u03c0\n[\n\u03d5(X i\u221211 xiX\u0307 n i+1)\u2212 \u03d5(X i\u221211 x\u2032iX\u0308ni+1)\n]\n\u2264 E(X\u0307n i+1 ,X\u0308n i+1 )\u223c\u03c0\n\n\u03d5(X i\u221211 xiX\u0307 n i+1)\u2212 \u03d5(X i\u221211 x\u2032iX\u0307ni+1) +\nn \u2211\nj=i+1\n\u03c1j(X\u0307j , X\u0308j)\n\n\n\u2264 EX\u0307n i+1 \u223cP(\u00b7 |Xi\u221211 xi)\n[\n\u03d5(X i\u221211 xiX\u0307 n i+1)\u2212 \u03d5(X i\u221211 x\u2032iX\u0307ni+1)\n]\n+ \u03c4\u0304i\n= F (xi)\u2212 F (x\u2032i) + \u03c4\u0304i, (30)\nwhere the first inequality holds by the Lipschitz property and the second by definition of \u03c4\u0304i, and F : Xi \u2192 R is defined by\nF (y) = \u2211\nxni+1\nP(xni+1 |X i\u221211 xi)\u03d5(X i\u221211 yxni+1).\nLet us substitute (30) into (29):\nV\u0303i \u2264 \u03c4\u0304i + \u2211\nxi,x\u2032i\nP(xi |X i\u221211 )P(x\u2032i |X i\u221211 )(F (xi)\u2212 F (x\u2032i)).\nObserve that F is 1-Lipschitz under \u03c1i and apply Jensen\u2019s inequality:\nE[e\u03bbVi |X i\u221211 ] \u2264 e\u03bb\u03c4\u0304i \u2211\nxi,x\u2032i\nP(xi |X i\u221211 )P(x\u2032i |X i\u221211 )e\u03bb(F (xi)\u2212F (x \u2032 i))\n\u2264 e\u03bb\u03c4\u0304i \u2211\nxi,x\u2032i\nP(xi |X i\u221211 )P(x\u2032i |X i\u221211 ) cosh(\u03bb\u03c1(xi, x\u2032i))\n\u2264 exp ( \u03bb\u03c4\u0304i + 1\n2 \u2206\u03042 SG (Xi)\u03bb2\n)\n,\nwhere the second inequality follows from the argument in (16) and the third from the definition of \u2206\u0304SG(Xi). Repeating the standard martingale argument in (18) yields\nP(\u03d5\u2212 E\u03d5 > t) = P ( n \u2211\ni=1\nVi > t\n)\n\u2264 exp ( 1\n2 \u03bb2\nn \u2211\ni=1\n\u2206\u03042 SG (Xi)\u2212 \u03bbt+ \u03bb\nn \u2211\ni=1\n\u03c4\u0304i\n)\n.\nOptimizing over \u03bb yields the claim."}, {"heading": "7 Other Orlicz diameters", "text": "Let us recall the notion of an Orlicz norm \u2016X\u2016\u03a8 of a real random variable X (see, e.g., Rao and Ren (1991)):\n\u2016X\u2016\u03a8 = inf {t > 0 : E[\u03a8(X/t)] \u2264 1} ,\nwhere \u03a8 : R \u2192 R is a Young function \u2014 i.e., nonnegative, even, convex and vanishing at 0. In this section, we will consider the Young functions\n\u03c8p(x) = e |x|p \u2212 1, p > 1,\nand their induced Orlicz norms. A random variable X is subgaussian if and only if \u2016X\u2016\u03c82 < \u221e. For p 6= 2, \u2016X\u2016\u03c8p < \u221e implies that\nEe\u03bbX \u2264 e(a|\u03bb|)p/p, \u03bb \u2208 R, (31)\nfor some a > 0, but the converse implication need not hold. An immediate consequence of Markov\u2019s inequality is that any X for which (31) holds also satisfies\nP(|X | \u2265 t) \u2264 2 exp ( \u2212p\u2212 1 p ( t a )p/(p\u22121) ) . (32)\nWe define the p-Orlicz diameter of a metric probability space (X , \u03c1, \u00b5), denoted \u2206OR(p)(X ), as the smallest a > 0 that verifies (31) for the symmetrized distance \u039e(X ). In light of (32), Theorem 1 extends straightforwardly to finite p-Orlicz metric diameters:\nTheorem 4. Let (Xi, \u03c1i, \u00b5i), i = 1, . . . , n be a sequence of metric probability spaces and equip Xn with the usual product measure \u00b5n and \u21131 product metric \u03c1n. Suppose that for some p > 1 and all i \u2208 [n] we have \u2206OR(p)(Xi) < \u221e, and define the vector \u2206 \u2208 Rn by \u2206i = \u2206OR(p)(Xi). If \u03d5 : Xn \u2192 R is 1-Lipschitz then for all t > 0,\nP(|\u03d5\u2212 E\u03d5| > t) \u2264 2 exp\n \u2212p\u2212 1 p ( t\n\u2016\u2206\u2016p\n)p/(p\u22121) \n ."}, {"heading": "8 Discussion", "text": "We have given a concentration inequality for metric spaces with unbounded diameter, showed its applicability to algorithmic stability with unbounded losses, and gave an extension to non-independent sampling processes. Some fascinating questions remain:\n(i) How tight is Theorem 1? First there is the vexing matter of having a worse constant in the exponent (i.e., 1/2) than McDiarmid\u2019s (optimal) constant 2. Although this gap is not of critical importance, one would like a bound that recovers McDiarmid\u2019s in the finite-diameter case. More importantly, is it the case that finite subgaussian diameter is necessary for subgaussian concentration of all Lipschitz functions? That is, given the metric probability spaces (Xi, \u03c1i, \u00b5i), i \u2208 [n], can one always exhibit a 1-Lipschitz \u03d5 : Xn \u2192 R that achieves a nearly matching lower bound?\n(ii) We would like to better understand how Theorem 1 compares to the KutinNiyogi bound (14). We conjecture that for any (Xn, \u00b5n) and \u03d5 : Xn \u2192 R that satisfies (11) and (12), one can construct a product metric \u03c1n for which \u2211\ni\u2208[n] \u2206 2 SG (Xi) < \u221e and \u03d5 is 1-Lipschitz. This would imply that\nwhenever the Kutin-Niyogi bound is nontrivial, so is Theorem 1. We have already shown by example (19) that the reverse does not hold.\n(iii) The quantity \u03c4\u0304i defined in Section 6 is a rather complicated object; one desires a better handle on it in terms of the given distribution and metric.\n(iv) Perhaps the most pressing question is that of showing that some common learning algorithms such as k-nearest neighbor, kernel SVM, regularized regression are totally Lipschitz stable under our definition (22)."}, {"heading": "Acknowledgements", "text": "John Lafferty encouraged me to seek a distribution-dependent refinement of McDiarmid\u2019s inequality. Thanks also to Gideon Schechtman, Shahar Mendelson, Assaf Naor, Iosif Pinelis and Csaba Szepesva\u0301ri for helpful correspondence, and to Roi Weiss for carefully proofreading the manuscript."}], "references": [{"title": "Stable transductive learning", "author": ["Ran El-Yaniv", "Dmitry Pechyony"], "venue": "In Learning theory,", "citeRegEx": "El.Yaniv and Pechyony.,? \\Q2007\\E", "shortCiteRegEx": "El.Yaniv and Pechyony.", "year": 2007}, {"title": "Measure Concentration of Strongly Mixing Processes with Applications", "author": ["Aryeh (Leonid) Kontorovich"], "venue": "PhD thesis,", "citeRegEx": "Kontorovich.,? \\Q2007\\E", "shortCiteRegEx": "Kontorovich.", "year": 2007}, {"title": "Concentration Inequalities for Dependent Random Variables via the Martingale Method", "author": ["Leonid (Aryeh) Kontorovich", "Kavita Ramanan"], "venue": "Ann. Probab.,", "citeRegEx": "Kontorovich and Ramanan.,? \\Q2008\\E", "shortCiteRegEx": "Kontorovich and Ramanan.", "year": 2008}, {"title": "Extensions to McDiarmid\u2019s inequality when differences are bounded with high probability", "author": ["Samuel Kutin"], "venue": "Technical Report TR-2002-04,", "citeRegEx": "Kutin.,? \\Q2002\\E", "shortCiteRegEx": "Kutin.", "year": 2002}, {"title": "Almost-everywhere algorithmic stability and generalization error", "author": ["Samuel Kutin", "Partha Niyogi"], "venue": "In UAI,", "citeRegEx": "Kutin and Niyogi.,? \\Q2002\\E", "shortCiteRegEx": "Kutin and Niyogi.", "year": 2002}, {"title": "Learning subgaussian classes: Upper and minimax bounds, arxiv:1305.4825", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": null, "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2013\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2013}, {"title": "The Concentration of Measure Phenomenon", "author": ["Michel Ledoux"], "venue": "Mathematical Surveys and Monographs", "citeRegEx": "Ledoux.,? \\Q2001\\E", "shortCiteRegEx": "Ledoux.", "year": 2001}, {"title": "Improved generalization bounds for largescale structured prediction", "author": ["Ben London", "Bert Huang", "Lise Getoor"], "venue": "In NIPS Workshop on Algorithmic and Statistical Approaches for Large Social Networks,", "citeRegEx": "London et al\\.,? \\Q2012\\E", "shortCiteRegEx": "London et al\\.", "year": 2012}, {"title": "Collective stability in structured prediction: Generalization from one example", "author": ["Ben London", "Bert Huang", "Benjamin Taskar", "Lise Getoor"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "London et al\\.,? \\Q2013\\E", "shortCiteRegEx": "London et al\\.", "year": 2013}, {"title": "Bounding d\u0304-distance by informational divergence: a method to prove measure concentration", "author": ["Katalin Marton"], "venue": "Ann. Probab.,", "citeRegEx": "Marton.,? \\Q1996\\E", "shortCiteRegEx": "Marton.", "year": 1996}, {"title": "On the method of bounded differences", "author": ["Colin McDiarmid"], "venue": "In J. Siemons, editor, Surveys in Combinatorics,", "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Stability bounds for stationary phi-mixing and beta-mixing processes", "author": ["Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mohri and Rostamizadeh.,? \\Q2010\\E", "shortCiteRegEx": "Mohri and Rostamizadeh.", "year": 2010}, {"title": "Rademacher complexity bounds for noni.i.d", "author": ["Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Mohri and Rostamizadeh.,? \\Q2008\\E", "shortCiteRegEx": "Mohri and Rostamizadeh.", "year": 2008}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Rifkin"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Mukherjee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2006}, {"title": "Stability results in learning theory", "author": ["Alexander Rakhlin", "Sayan Mukherjee", "Tomaso Poggio"], "venue": "Anal. Appl. (Singap.),", "citeRegEx": "Rakhlin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2005}, {"title": "In\u00e9galit\u00e9s de Hoeffding pour les fonctions lipschitziennes de suites", "author": ["Emmanuel Rio"], "venue": null, "citeRegEx": "Rio.,? \\Q1991\\E", "shortCiteRegEx": "Rio.", "year": 1991}, {"title": "Generalization bounds of erm algorithm with", "author": ["Bin Zou", "Zong-ben Xu", "Jie Xu"], "venue": "Springer-Verlag, Berlin,", "citeRegEx": "Zou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "It turns out that the various notions of stability are naturally expressed in terms of the Lipschitz continuity of the algorithm in question (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al.", "startOffset": 141, "endOffset": 246}, {"referenceID": 14, "context": "It turns out that the various notions of stability are naturally expressed in terms of the Lipschitz continuity of the algorithm in question (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al.", "startOffset": 141, "endOffset": 246}, {"referenceID": 12, "context": ", 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al., 2009; Zou et al.; Mohri and Rostamizadeh, 2010; London et al., 2012, 2013; Shalizi and Kontorovich, 2013).", "startOffset": 111, "endOffset": 376}, {"referenceID": 11, "context": ", 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al., 2009; Zou et al.; Mohri and Rostamizadeh, 2010; London et al., 2012, 2013; Shalizi and Kontorovich, 2013).", "startOffset": 111, "endOffset": 376}, {"referenceID": 10, "context": "An elegant and powerful work-horse driving many of the aforementioned results is McDiarmid\u2019s inequality (McDiarmid, 1989): P(|\u03c6\u2212 E\u03c6| > t) \u2264 2 exp ( \u2212 2t 2 \u2211n i=1 w 2 i )", "startOffset": 104, "endOffset": 121}, {"referenceID": 9, "context": "Non-iid extensions of (1) have also been considered (Marton, 1996; Rio, 2000; Chazottes et al., 2007; Kontorovich and Ramanan, 2008).", "startOffset": 52, "endOffset": 132}, {"referenceID": 2, "context": "Non-iid extensions of (1) have also been considered (Marton, 1996; Rio, 2000; Chazottes et al., 2007; Kontorovich and Ramanan, 2008).", "startOffset": 52, "endOffset": 132}, {"referenceID": 13, "context": "This influential result has been invoked in a number of recent papers (El-Yaniv and Pechyony, 2006; Mukherjee et al., 2006; Hush et al., 2007; Agarwal and Niyogi, 2009; Shalev-Shwartz et al., 2010; Rubinstein and Simma, 2012).", "startOffset": 70, "endOffset": 225}, {"referenceID": 0, "context": ", 2007; Kontorovich and Ramanan, 2008). The distribution-free nature of McDiarmid\u2019s inequality makes it an attractive tool in learning theory, but also imposes inherent limitations on its applicability. Chief among these limitations is the inability of (1) to provide risk bounds for unbounded loss functions. Even in the bounded case, if the Lipschitz condition (2) holds not everywhere but only with high probability \u2014 say, with a much larger constant on a small set of exceptions \u2014 the bound in (1) still charges the full cost of the worst-case constant. To counter this difficulty, Kutin (2002); Kutin and Niyogi (2002) introduced an extension of McDiarmid\u2019s inequality to weakly difference-bounded functions and used it to analyze the risk of \u201calmost-everywhere\u201d stable algorithms.", "startOffset": 8, "endOffset": 599}, {"referenceID": 0, "context": ", 2007; Kontorovich and Ramanan, 2008). The distribution-free nature of McDiarmid\u2019s inequality makes it an attractive tool in learning theory, but also imposes inherent limitations on its applicability. Chief among these limitations is the inability of (1) to provide risk bounds for unbounded loss functions. Even in the bounded case, if the Lipschitz condition (2) holds not everywhere but only with high probability \u2014 say, with a much larger constant on a small set of exceptions \u2014 the bound in (1) still charges the full cost of the worst-case constant. To counter this difficulty, Kutin (2002); Kutin and Niyogi (2002) introduced an extension of McDiarmid\u2019s inequality to weakly difference-bounded functions and used it to analyze the risk of \u201calmost-everywhere\u201d stable algorithms.", "startOffset": 8, "endOffset": 624}, {"referenceID": 3, "context": "In order to address some of these issues, Kutin (2002); Kutin and Niyogi (2002) proposed an extension of McDiarmid\u2019s inequality to \u201calmost everywhere\u201d Lipschitz functions \u03c6 : Xn \u2192 R.", "startOffset": 42, "endOffset": 55}, {"referenceID": 3, "context": "In order to address some of these issues, Kutin (2002); Kutin and Niyogi (2002) proposed an extension of McDiarmid\u2019s inequality to \u201calmost everywhere\u201d Lipschitz functions \u03c6 : Xn \u2192 R.", "startOffset": 42, "endOffset": 80}, {"referenceID": 3, "context": "The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state \u2014 indeed, the present work was motivated in part by a desire for simpler tools. Assuming that \u03c6 is weakly difference-bounded by (b, c, \u03b4) with \u03b4 = exp(\u2212\u03a9(n)) (13) and c = O(1/n), their bound states that P(|\u03c6\u2212 E\u03c6| \u2265 t) \u2264 exp(\u2212\u03a9(nt)) (14) for a certain range of t and n. As noted by Rakhlin et al. (2005), the exponential decay assumption (13) is necessary in order for the Kutin-Niyogi method to yield exponential concentration.", "startOffset": 22, "endOffset": 382}, {"referenceID": 3, "context": "The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state \u2014 indeed, the present work was motivated in part by a desire for simpler tools. Assuming that \u03c6 is weakly difference-bounded by (b, c, \u03b4) with \u03b4 = exp(\u2212\u03a9(n)) (13) and c = O(1/n), their bound states that P(|\u03c6\u2212 E\u03c6| \u2265 t) \u2264 exp(\u2212\u03a9(nt)) (14) for a certain range of t and n. As noted by Rakhlin et al. (2005), the exponential decay assumption (13) is necessary in order for the Kutin-Niyogi method to yield exponential concentration. In contrast, the bounds we prove here (i) do not require |\u03c6(X)\u2212 \u03c6(X\u0303)| to be everywhere bounded as in (11) (ii) have a simple statement and proof, and generalize to non-iid processes with relative ease. We defer the quantitative comparisons between (14) and our results until the latter are formally stated in Section 4. In a different line of work, Bentkus (2008) considered an extension of Hoeffding\u2019s inequality to unbounded random variables.", "startOffset": 22, "endOffset": 872}, {"referenceID": 3, "context": "The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state \u2014 indeed, the present work was motivated in part by a desire for simpler tools. Assuming that \u03c6 is weakly difference-bounded by (b, c, \u03b4) with \u03b4 = exp(\u2212\u03a9(n)) (13) and c = O(1/n), their bound states that P(|\u03c6\u2212 E\u03c6| \u2265 t) \u2264 exp(\u2212\u03a9(nt)) (14) for a certain range of t and n. As noted by Rakhlin et al. (2005), the exponential decay assumption (13) is necessary in order for the Kutin-Niyogi method to yield exponential concentration. In contrast, the bounds we prove here (i) do not require |\u03c6(X)\u2212 \u03c6(X\u0303)| to be everywhere bounded as in (11) (ii) have a simple statement and proof, and generalize to non-iid processes with relative ease. We defer the quantitative comparisons between (14) and our results until the latter are formally stated in Section 4. In a different line of work, Bentkus (2008) considered an extension of Hoeffding\u2019s inequality to unbounded random variables. His bound only holds for sums (as opposed to general Lipschitz functions) and the summands must be non-negative (i.e., unbounded only in the positive direction). An earlier notion of \u201ceffective\u201d metric diameter in the context of concentration is that of metric space length (Schechtman, 1982). Another distribution-dependent refinement of diameter is the spread constant (Alon et al., 1998). Lecu\u00e9 and Mendelson (2013) gave minimax bounds for empirical risk minimization over subgaussian classes.", "startOffset": 22, "endOffset": 1372}, {"referenceID": 6, "context": "The strong integrability of \u03c6 \u2014 and in particular, finiteness of E\u03c6 \u2014 follow from exponential concentration (Ledoux, 2001).", "startOffset": 108, "endOffset": 122}, {"referenceID": 4, "context": "5 Application to algorithmic stability We refer the reader to (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005) for background on algorithmic stability and supervised learning.", "startOffset": 62, "endOffset": 138}, {"referenceID": 14, "context": "5 Application to algorithmic stability We refer the reader to (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005) for background on algorithmic stability and supervised learning.", "startOffset": 62, "endOffset": 138}, {"referenceID": 14, "context": "A variant of uniform stability in the sense of Rakhlin et al. (2005) \u2014 which is slightly more general than the homonymous notion in Bousquet and Elisseeff (2002)\u2014 may be defined as follows.", "startOffset": 47, "endOffset": 69}, {"referenceID": 14, "context": "A variant of uniform stability in the sense of Rakhlin et al. (2005) \u2014 which is slightly more general than the homonymous notion in Bousquet and Elisseeff (2002)\u2014 may be defined as follows.", "startOffset": 47, "endOffset": 162}, {"referenceID": 1, "context": "Kontorovich (2007); Kontorovich and Ramanan (2008) discuss how to handle conditioning on measure-zero sets and other technicalities.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Kontorovich (2007); Kontorovich and Ramanan (2008) discuss how to handle conditioning on measure-zero sets and other technicalities.", "startOffset": 0, "endOffset": 51}, {"referenceID": 9, "context": "The use of coupling and transportation techniques to obtain concentration for dependent random variables goes back to Marton (1996); Samson (2000); Chazottes et al.", "startOffset": 118, "endOffset": 132}, {"referenceID": 9, "context": "The use of coupling and transportation techniques to obtain concentration for dependent random variables goes back to Marton (1996); Samson (2000); Chazottes et al.", "startOffset": 118, "endOffset": 147}, {"referenceID": 9, "context": "The use of coupling and transportation techniques to obtain concentration for dependent random variables goes back to Marton (1996); Samson (2000); Chazottes et al. (2007).", "startOffset": 118, "endOffset": 172}], "year": 2013, "abstractText": "We prove an extension of McDiarmid\u2019s inequality for metric spaces with unbounded diameter. To this end, we introduce the notion of the subgaussian diameter, which is a distribution-dependent refinement of the metric diameter. Our technique provides an alternative approach to that of Kutin and Niyogi\u2019s method of weakly difference-bounded functions, and yields nontrivial, dimension-free results in some interesting cases where the former does not. As an application, we give apparently the first generalization bound in the algorithmic stability setting that holds for unbounded loss functions. We give two extensions of the basic concentration result: to strongly mixing processes and to other Orlicz norms.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}