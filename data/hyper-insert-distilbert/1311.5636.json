{"id": "1311.5636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2013", "title": "Learning Non-Linear Feature Maps", "abstract": "feature selection plays again a pivotal quantitative role in learning, since particularly in critical areas were parsimonious features profiles can thereby provide insight ways into the underlying analysis process, skills such as biology. existing recent approaches for promising non - transparent linear feature values selection algorithm employing greedy optimisation of centred kernel target resource alignment ( kta ), while accurately exhibiting strong results primarily in exact terms mostly of selective generalisation accuracy and sparsity, can collectively become computationally prohibitive proposals for high - spectral dimensional quantitative datasets. we propose randsel, a randomised persistent feature value selection optimization algorithm, with attractive scaling properties. understanding our nonlinear theoretical resource analysis of randsel profiles provides optimal strong matching probabilistic truth guarantees for the correct identification of relevant geographic features. sufficient experimental results on either real and composite artificial data, show that the exact method methodology successfully identifies physically effective features, performing better than typically a number of competitive adaptation approaches.", "histories": [["v1", "Fri, 22 Nov 2013 01:49:26 GMT  (140kb,D)", "http://arxiv.org/abs/1311.5636v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dimitrios athanasakis", "john shawe-taylor", "delmiro fernandez-reyes"], "accepted": false, "id": "1311.5636"}, "pdf": {"name": "1311.5636.pdf", "metadata": {"source": "CRF", "title": "Learning Non-Linear Feature Maps", "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "emails": ["dathanasakis@cs.ucl.ac.uk", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"], "sections": [{"heading": null, "text": "Feature selection plays a pivotal role in learning, particularly in areas were parsimonious features can provide insight into the underlying process, such as biology. Recent approaches for non-linear feature selection employing greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting strong results in terms of generalisation accuracy and sparsity, can become computationally prohibitive for high-dimensional datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for the correct identification of relevant features. Experimental results on real and artificial data, show that the method successfully identifies effective features, performing better than a number of competitive approaches.\nFeature Selection, Kernels"}, {"heading": "1 Introduction", "text": "Feature selection is an important aspect in the implementation of machine learning methods. The appropriate selection of informative features can reduce generalisation error as well as the storage and processing requirements for large datasets. In addition, parsimonious models can provide valuable insight into the relations underlying elements of the process under examination. There is a wealth of literature on the subject of feature selection when the relationship between variables is linear. Unfortunately when the relation is non-linear feature selection becomes substantially more nuanced.\nKernel methods excel in modelling non-linear relations. Unsurprisingly, a number of kernel-based feature selection algorithms have been proposed. Early propositions, such as Recursive Feature Elimination(RFE) [1] can be computationally prohibitive, while attempts to learn a convex combination of low-rank kernels may fail to encapsulate nonlinearities in the underlying relation. Recent approaches using explicit kernel approximations can capture non-linear relations, but increase the storage and computational requirements. The successful use of a kernel-based feature selection methods is a matter of balance.\n\u2217Use footnote for providing further information about author (webpage, alternative address)\u2014not for acknowledging funding agencies.\nar X\niv :1\n31 1.\n56 36\nv1 [\ncs .L\nG ]\n2 2"}, {"heading": "1.1 Related Work", "text": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3]. Work in [4] provides the foundation of using the alignment of centred kernel matrices as the basis for measuring statistical dependence. The Hilbert-Schmidt Independence criterion is the basis for further work in [5], where greedy optimisation of centred alignment is employed for feature selection. Additionally, [5] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework.\nStability selection [6] is a general framework for variable selection and structure estimation of high dimensional data. The core principle of stability selection is to combine subsampling with a sparse variable selection algorithm. By repeated estimation over a number of different subsamples, the framework keeps track of the number of times each variable was used, thus maintaining an estimate for the importance of each feature. More importantly, stability selection provides finite sample control for some error rates of false discoveries and hence a principled approach for parameter selection. In this work, we propose a synthesis of the two aforementioned approaches through a randomised feature selection algorithm based on estimating the statistical dependence between bootstrapped random subspaces of the dataset in RKHS. The dependence estimation of random subsets of variables is similar to the approach of [13], which is extended through bootstrapping and carefully controlled feature set sizes.\nThis approach is simple to implement and compares favourably with other methods in terms of scalability. The rest of the paper is structured as follows: Section 2 presents the necessary background on feature selection for kernel-based learning. Section 3 introduces a basic randomised algorithm for nonlinear feature selection, along with some simple examples, while Section 4 provides some analysis. Extensive experimentation on real and artificial data in section 5 concludes this paper."}, {"heading": "2 Preliminaries", "text": "We consider the supervised learning problem of modelling the relationship between a m \u00d7 n input matrix X and a corresponding m \u00d7 n\u2032 output matrix Y . The simplest instance of such a problem is binary classification where the objective is the learning problem is to learn a function f : x \u2192 y mapping input vectors x to the desired outputs y. In the binary case we are presented with a m\u00d7 n matrix X and a vector of outputs y, yi \u2208 {+1,\u22121} Limiting the class of discrimination functions to linear classifiers we wish to find a classifier\nf(x) = \u2211 i wixi = \u3008w, x\u3009\nThe linear learning formulation can be generalised to the nonlinear setting through the use of a nonlinear feature map \u03c6(x), leading to the kernelized formulation:\nf(x) = \u3008w, \u03c6(x)\u3009 = \u3008 \u2211 i aiyi\u03c6(xi), \u03c6(x)\u3009 = \u2211 i aiyik(xi, x)\nThe key quantities of interest in our approach is the centred kernel target alignment which is defined as:\na(Cx, Cy) = \u3008Cx, Cy\u3009F \u2016Cx\u2016F \u2016Cy\u2016F =\n\u2211 i,j cxijcyij\u2211\ni,j \u2016cxij\u2016 \u2211 i,j \u2016kyij\u2016\nThe matrices Cx and Cy correspond to centred kernels on the features X and outputs Y and are computed as:\nC = [ I \u2212 11 T\nm\n] K [ I \u2212 11 T\nm ] where 1, in the above equation denotes the m-dimensional vector with all entries set equal to one."}, {"heading": "3 Development of key ideas", "text": "The approach we will take will be based on the following well-known observation that links kernel target alignment with the degree to which an input space contains a linear projection that correlates with the target.\nProposition 3.1 Let P be a probability distribution on the product space X \u00d7 R, where X has a projection \u03c6 into a Hilbert space F defined by a kernel \u03ba. We have that\u221a\nE(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03ba(x,x\u2032)] =\n= sup w:\u2016w\u2016\u22641\nE(x,y)\u223cP [y\u3008w, \u03c6(x)\u3009]\nProof: sup\nw:\u2016w\u2016\u22641 E(x,y)\u223cP [y\u3008w, \u03c6(x)\u3009] =\n= sup w:\u2016w\u2016\u22641\n\u2329 w,E(x,y)\u223cP [\u03c6(x)y] \u232a = \u2225\u2225E(x,y)\u223cP [\u03c6(x)y]\u2225\u2225\n= \u221a\u222b \u222b dP (x, y)dP (x\u2032, y\u2032)\u3008\u03c6(x), \u03c6(x\u2032)\u3009yy\u2032 = \u221a E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03ba(x,x\u2032)]\nThe proposition suggests that we can detect useful representations by measuring kernel target alignment. For non-linear functions the difficulty is to identify which combination of features creates a useful representation. We tackle this problem by sampling subsets S of features and assessing whether on average the presence of a particular feature i contributes to an increase ci in the average kernel target alignment. In this way we derive an empirical estimate of a quantity we will term the contribution.\nDefinition 3.2 The contribution ci of feature i is defined as ci = ES\u223cSi [ E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)] ] \u2212 ES\u2032\u223cS\\i [ E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS\u2032(x,x\u2032)] ] ,\nwhere \u03baS denotes the (non-linear) kernel using features in the set S (in our case this will be a Gaussian kernel with equal width), Si the uniform distribution over sets of features of size bn/2c+1 that include the feature i, S\\i the uniform distribution over sets of features of size bn/2c that do not contain the feature i, and n is the number of features.\nNote that the two distributions over features Si and S\\i are matched in the sense that for each S with non-zero probability in S\\i, S \u222a {i} has equal probability in Si. This approach is a straightforward extension of the idea of BaHsic [5].\nWe will show that for variables that are independent of the target this contribution will be negative. On the other hand, provided there are combinations of variables including the given variable that can generate significant correlations then the contribution of the variable will be positive.\nDefinition 3.3 We will define an irrelevant feature to be one whose value is statistically independent of the label and of the other features.\nWe would like an assurance that irrelevant features do not increase alignment. This is guaranteed for the Gaussian kernel by the following result.\nProposition 3.4 Let P be a probability distribution on the product space X \u00d7 R, where X has a projection \u03c6Si into a Hilbert space F defined by the Gaussian kernel \u03baS on a set of features S. Suppose a feature i 6\u2208 S is irrelevant. We have that\nE(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS\u222a{i}(x,x\u2032)] \u2264 E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)]\nProof (sketch): Since the feature is independent of the target and the other features, functions of these features are also independent. Hence, E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS\u222a{i}(x,x\u2032)] = E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032) exp(\u2212\u03b3(xi \u2212 x\u2032i)2)]\n= E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)]E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [exp(\u2212\u03b3(xi \u2212 x\u2032i)2)] = E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)]\u03b1\nfor \u03b1 = E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [exp(\u2212\u03b3(xi \u2212 x\u2032i)2)] \u2264 1. In fact the quantity \u03b1 is typically less than 1 so that adding irrelevant features decreases the alignment. Our approach will be to progressively remove sets of features that are deemed to be irrelevant, hence increasing the alignment together with the signal to noise ratio for the relevant features. Figure 1 shows how progressively removing features from a learning problem whose output is the XOR function of the first two features both increases the alignment contributions and helps to highlight the two relevant features.\nWe now introduce our definition of a relevant feature.\nDefinition 3.5 A feature i will be termed \u03b7-influential when its contribution ci \u2265 \u03b7 > 0.\nSo far have only considered expected alignment. In practice we must estimate this expectation from a finite sample. We will omit this part of the analysis as it is a straigthforward application of Ustatistics that ensures that with high probability for a sufficiently large sample from Si and S\\i and of samples from P (whose sizes depend on \u03b7, \u03b4, the number k of \u03b7-influential variables and the number T of iterations) an empirical estimate of the contribution of an \u03b7-influential variable will with probability at least 1 \u2212 \u03b4 be greater than 0 for all of the fixed number T of iterations of the algorithm.\nOur final ingredient is a method of removing irrelevant features that we will term culling. At each iteration of the algorithm the contributions of all of the features are estimated using the required sample size and the contributions are sorted. We then remove the bottom 25% of the features in this ordering. Our main result assures us that culling will work under the assumption that the irrelevant variables are independent.\nTheorem 3.6 Fix \u03b7 > 0. Suppose that there are k eta-influential variables and all other variables are irrelevant. Fix \u03b4 > 0 and number T of iterations. Given sufficiently many samples as described above the culling algorithm will with probability at least 1\u2212 \u03b4 remove only irrelevant variables will be removed.\nProof (sketch): Each irrelevant variable has expected contribution less than the contributions of the all the influential features. Hence, with high probability at least 30% of these features will have lower contributions than all the influential features. Hence, the bottom 25% will all be irrelevant."}, {"heading": "4 Properties of the algorithm", "text": "We now define our algorithm for randomised selection (randSel). Given a m \u00d7 n input matrix X and corresponding output matrix Y , randSel proceeds by estimating the individual contribution of features by estimating the alignment of a number of random subsamples that include n2 and n 2 + 1 randomly selected features. This leads to an estimate for the expected alignment contribution of including a feature. The algorithm is parametrized by the number of bootstraps N , a bootstrap sizenb and a percentage z% of features that are dropped after N bootstraps. The algorithm proceeds iteratively until only two features remain. Optionally the algorithm can be further parametrized by permanently including features which were ranked in the top percentile a% on at least a number t occasions. This option enhances the probability of detecting non-linear dependencies between variables, should they be present.\nThere are a number of benefits to this approach, aside from the tangible probabilistic guarantees. RandSel scales gracefully. Considering the computation of a kernel k(x, x\u2032) for samples x, x\u2032 atomic, the number of kernel computations for a single iteration are n2bN , which for a sensible choice of N can be substantially smaller than the m2n complexity of HSIC variants. For example setting nb = \u221a m and N = n an iteration would require mn kernel element computations, and in addition this process is trivial to parallelize.\nAlgorithm 1 randSel Input: input data X , labels Y , number of iterations r subsample size s, number of features n, drop percentile proportion z, top percentile proportion a, number of occasions t repeat\nfor i = 1 to r do (Xi, Yi) = Random subsample of size s over n2 randomly selected variables ai= alignment(Xi, Yi) (X\n(+) i , Y (+) i ) = Random subsample of size s over n 2 + 1 randomly selected variables\na (+) i = alignment(X (+) i , Y (+) i )\nend for for j = 1 to n do\nmean contribution ci = mean( a (+) i ) - mean(ai), where j \u2208 X (+) i and j /\u2208 Xi\nend for drop the z% bottom-contributing features save the a% top-contributing features if fixing features then\nif j top-contributor for t consecutive times then fix feature j\nend if end if\nuntil no features left to fix, or only 2 features remain Return Sequence of estimated contributions and Fixed Variables"}, {"heading": "5 Results", "text": "In this section, we present several experiments comparing our feature selection approach to a number of competing methodologies. We have used three synthetic datasets in order to better illustrate the performance characteristics of these algorithms before proceeding to experiments on real data arising in infectious disease profiling."}, {"heading": "5.1 Experimental Setup", "text": "In both synthetic and real datasets we used nested 10-fold cross validation to perform feature selection, and repeated the simulations on three different reshuffles of the dataset to account for variance. For every iteration we estimate the validation error after feature selection before proceeding to test on the held out test-set. The inner cross-validation loop determines the number of features to use in classifying the test-set for optimal accuracy. If two or more models are tied in terms of performance, the more parsimonious model is preferred.\nWe compare our proposed approach to kernel based algorithms like RFE, FoHsic and BaHsic, as well as a filtering approach relying on Correlation Coefficients and Stability Selection using the Lasso as the underlying sparse selection algorithm. The same range of gaussian kernel bandwidths was explored in all algorithms and the resulting final classifiers employed a regularisation parameter of c = 1."}, {"heading": "5.2 Synthetic Data", "text": "We generated three synthetic datasets in order to carefully illustrate the properties of the different feature selection algorithms. All three synthetic datasets contain 300 samples with a dimensionality of 100 features. The linear and non-linear weston datasets were generated according to [7], and consist of 5 relevant and 95 noise features. Neither the linear or non-linear Weston datasets exhibit a nonlinear interdependence between features. We produced a simple XOR pattern dataset in order to simulate this scenario. Along with the accuracy on the test set and the sparsity we also record the precision and recall of the selection algorithms. Analogously to information retrieval, we define the precision as the number of the relevant features that were selected from the feature selection procedure over the total number of features selected and recall as the number of relevant features selected over the total number of relevant features.\nFor the synthetic benchmarks we used randSel using 3000 bootstraps of size m/4 of the dataset, culling the bottom 25% of variables in terms of expected contribution after the end of each iteration. We did not employ fixing of variables, and the algorithm would iterate until only two variables remained. In the Linear dataset all methods perform fairly well in terms of accuracy with randSel being marginally better. Stability selection is the only method that consistently selects only relevant features, while correlation filtering is marginally better in terms of recall. As is to be expected owing to their linear nature, both correlation filtering and stability selection fail on the Non-linear Weston benchmark. BahSic and FohSic perform better in terms of accuracy, achieving a nearly identical performance on measured variables, while RFE is the only method to achieve perfect recall throughout all folds of the data. Finally in the XOR problem randomised selection and BahSic achieve identical performance across the board. The greedy forward selection employed in FohSic completely fails to identify the two relevant features."}, {"heading": "5.3 Real Data", "text": "We conducted experiments in real datasets arising in the computational profiling of tuberculosis (TB), an application where feature selection plays a pivotal role both in terms of improving accuracy but also providing insight into the underlying mechanisms. We conducted experiments on two different datasets. The first TB dataset consists of 523-dimensional mass-spectrometry proteomic profiles of blood plasma [8], and consists of 100 active TB samples, 40 symptomatic controls, and 49 samples of patients with TB-Like symptoms with a co-existing latent TB infection (LTBI). We performed pairwise comparisons between active TB and Unhealthy Controls (Task 1), Active TB and symptomatic LTBI (Task 2), and Active TB with symptomatic patients without LTBI (Task 3),which correspond to scenarios in real clinical applications. The second dataset comprises of the transcriptomic profiles of 69 healthy individuals with LTBI and 133 healthy controls from [9]. Preprocessing removed probes with low acquisition precision as well as factors with missing values, resulting in a set of 6247 variables. Table 2 summarises the experiments.\nFor the mass spectrometry tasks we used randSel using 5000 bootstraps of size m/3 of the dataset, culling the bottom 25% of variables in terms of expected contribution after the end of each iteration.\nWe used similar parameters for the Micro-array dataset but with an increased number of bootstraps of 10000 in order to account for the substantially higher dimensionality of the data. Again, no variables were fixed and the algorithm iterated until only two variables remained. In Task 1, Randomized selection is tied with stability selection in terms of accuracy, however on average the randomised recovered feature set is significantly sparser. Interestingly, simple filtering based on correlation coefficients performs strongly in the Mass Spectrometry tasks, often beating the HSIC Variants and in fact gives the highest accuracy for Task 2. The only test where all the HSIC variants outperformed correlation filtering is the MicroArray task, which has a substantially increased dimensionality in comparison to the mass spectrometry datasets.\nThe results indicate that the HSIC-based variants( randSel,FoHsic & BaHsic) often recover sparser solutions compared to competing algorithms. Given their mutual reliance on HSIC optimisation, the fact that randSel outperforms the other methods in terms of accuracy can be surprising at first glance. It is instructive at this point to acknowledge that these methods rely on heuristics to solve an NP-hard problem. The synthetic XOR dataset already underlines one scenario where randSel outperforms forward greedy selection. The results on real data, combined with our theoretical guarantees, suggest the possibility of arriving at an improved global solution through randSel\u2019s incorporation of stochastic information, in opposition to the strategy of obliviously eliminating the locally optimal variable employed in BaHsic."}, {"heading": "5.4 Learning Deep Representations with LPBoostMKL", "text": "A final experimental application where we employed randomised selection was in the recent Black Box Learning challenge [10][13]. After performing an initial unsupervised feature learning step on the original dataset using Sparse Filtering [11], we performed randomised selection in the resulting representation, creating kernels corresponding to the remaining features after each iteration of the feature selection algorithm. Treating each kernel as defining a class of weak learners, we used LPBoost to perform multiple kernel learning (LPBoostMKL [12]). The resulting classifier beat many of our other approaches and was one of the strongest performers in the challenge."}, {"heading": "6 Conclussions", "text": "In this paper we propose randSel, a new algorithm for non-linear feature selection based on randomised estimates of HSIC. RandSel, stochastically estimates the expected importance of features at each iteration, proceeding to cull features uninformative features at the end of each iteration. Our theoretical analysis gives strong guarantees for the expected performance of this procedure which is further demonstrated by testing on a number of real and artificial datasets. This, combined with the algorithm\u2019s attractive scaling properties make randSel a strong proposition for use in application areas such as quantitative biology, where the volume of data increases at a frantic pace."}], "references": [{"title": "Gene selection for cancer classification using support vector machines", "author": ["Guyon", "Isabelle", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning 46,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "On kernel target alignment. Advances in neural information processing systems", "author": ["N. Shawe-Taylor", "A. Kandola"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["Cortes", "Corinna", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Feature selection via dependence maximization", "author": ["Song", "Le", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Feature selection for SVMs. Advances in neural information processing systems: 668-674", "author": ["Weston", "Jason", "Sayan Mukherjee", "Olivier Chapelle", "Massimiliano Pontil", "Tomaso Poggio", "Vladimir Vapnik"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Discriminating Active from Latent Tuberculosis in Patients Presenting to Community Clinics, PLOS ONE", "author": ["Gurjinder Sandhu", "Francesca Battaglia", "Barry K. Ely", "Dimitrios Athanasakis", "Rosario Montoya", "Teresa Valencia", "Robert H. Gilman", "Carlton A. Evans", "Jon S. Friedland", "Delmiro Fernandez-Reyes", "Daniel D"], "venue": "Agranoff", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Multiple Kernel Learning on the Limit Order Book", "author": ["Tristan Fletcher", "Zakria Hussain", "John Shawe-Taylor"], "venue": "Proceedings of the First Workshop on Applications of Pattern Analysis", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Fast dependency-aware feature selection in very-highdimensional pattern recognition", "author": ["Somol", "Petr", "Jiri Grim", "Pavel Pudil"], "venue": "In Systems, Man, and Cybernetics (SMC) 2011 IEEE International Conference on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Challenges in Representation Learning: A report on three machine learning contests", "author": ["I.J. Goodfellow", "D. Erhan", "P.L. Carrier", "A. Courville", "M. Mirza", "B. Hamner", "Y. Bengio"], "venue": "Proceedings of the 20th International Conference on Neural Information Processing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Early propositions, such as Recursive Feature Elimination(RFE) [1] can be computationally prohibitive, while attempts to learn a convex combination of low-rank kernels may fail to encapsulate nonlinearities in the underlying relation.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3].", "startOffset": 66, "endOffset": 71}, {"referenceID": 2, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3].", "startOffset": 66, "endOffset": 71}, {"referenceID": 3, "context": "The Hilbert-Schmidt Independence criterion is the basis for further work in [5], where greedy optimisation of centred alignment is employed for feature selection.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Additionally, [5] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "The dependence estimation of random subsets of variables is similar to the approach of [13], which is extended through bootstrapping and carefully controlled feature set sizes.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "This approach is a straightforward extension of the idea of BaHsic [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "The linear and non-linear weston datasets were generated according to [7], and consist of 5 relevant and 95 noise features.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "The first TB dataset consists of 523-dimensional mass-spectrometry proteomic profiles of blood plasma [8], and consists of 100 active TB samples, 40 symptomatic controls, and 49 samples of patients with TB-Like symptoms with a co-existing latent TB infection (LTBI).", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "A final experimental application where we employed randomised selection was in the recent Black Box Learning challenge [10][13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "Treating each kernel as defining a class of weak learners, we used LPBoost to perform multiple kernel learning (LPBoostMKL [12]).", "startOffset": 123, "endOffset": 127}], "year": 2013, "abstractText": "Feature selection plays a pivotal role in learning, particularly in areas were parsimonious features can provide insight into the underlying process, such as biology. Recent approaches for non-linear feature selection employing greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting strong results in terms of generalisation accuracy and sparsity, can become computationally prohibitive for high-dimensional datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for the correct identification of relevant features. Experimental results on real and artificial data, show that the method successfully identifies effective features, performing better than a number of competitive approaches. Feature Selection, Kernels", "creator": "LaTeX with hyperref package"}}}