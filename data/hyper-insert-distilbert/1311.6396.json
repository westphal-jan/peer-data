{"id": "1311.6396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2013", "title": "A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds", "abstract": "we study sequential prediction simulation of real - valued, arbitrary and unknown sequences under the limited squared error loss as well as the mathematical best inference parametric predictor theorem out of a super large, infinitely continuous comparison class of of predictors. inspired by recent results from initial computational control learning theory, we consciously refrain from any statistical assumptions and define the performance with respect parallel to the class value of general parametric system predictors. in particular, whilst we often present generic estimated lower and theoretical upper parameter bounds on this relative performance by transforming completely the prediction task into a parameter driven learning problem. we first introduce the lower bounds on confirming this singular relative performance : in the typical mixture case of q experts framework, from where and we show however that partially for any sequential algorithm, when there always longer exists a sequence for which \" the performance of the regular sequential algorithm is lower bounded by parameter zero. ) we significantly then introduce a sequential local learning algorithm to predict at such arbitrary and unknown coding sequences, respectively and ultimately calculate sequential upper bounds on its generalized total squared prediction error bounds for near every bounded sampling sequence. we further extensively show that in some scenarios indeed we achieve trajectory matching within lower and upper bounds just demonstrating that our algorithms are optimal in a extremely strong fixed minimax conditional sense such becoming that their desired performances potentially cannot be improved further. as potentially an interesting engineering result ) we directly also prove moreover that solely for the usual worst case modeling scenario, the performance of randomized algorithms but can similarly be achieved by sequential algorithms so that randomized network algorithms does not easily improve the performance.", "histories": [["v1", "Mon, 25 Nov 2013 18:36:26 GMT  (12kb)", "https://arxiv.org/abs/1311.6396v1", null], ["v2", "Wed, 22 Jan 2014 21:00:52 GMT  (12kb)", "http://arxiv.org/abs/1311.6396v2", "Submitted to IEEE Transactions on Neural Networks and Learning Systems"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["n denizcan vanli", "suleyman s kozat"], "accepted": false, "id": "1311.6396"}, "pdf": {"name": "1311.6396.pdf", "metadata": {"source": "CRF", "title": "A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds", "authors": ["N. Denizcan Vanli"], "emails": ["vanli@ee.bilkent.edu.tr,", "kozat@ee.bilkent.edu.tr)."], "sections": [{"heading": null, "text": "ar X\niv :1\n31 1.\n63 96\nv2 [\ncs .L\nG ]\n2 2\nJa n\n20 14\nIndex Terms\u2014Sequential prediction, online learning, worstcase performance.\nI. INTRODUCTION\nIn this brief paper, we investigate the generic sequential (online) prediction problem from an individual sequence perspective using tools of computational learning theory, where we refrain from any statistical assumptions either in modeling or on signals [1]\u2013[4]. In this approach we have an arbitrary, deterministic, bounded and unknown signal {x[t]}t\u22651, where |x[t]| < A < \u221e, and x[t] \u2208 R. Since we do not impose any statistical assumptions on the underlying data, we, motivated by recent results from sequential learning [1]\u2013[4], define the performance of a sequential algorithm with respect to a comparison class, where the predictors of the comparison class are formed by observing the the entire sequence in hindsight, under the squared error loss, i.e.,\nn\u2211\nt=1\n(x[t] \u2212 x\u0302s[t]) 2 \u2212 inf\nc\u2208C\nn\u2211\nt=1\n(x[t]\u2212 x\u0302c[t]) 2 ,\nfor an arbitrary length of data n, and for any possible sequence {x[t]}t\u22651, where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t\u2212 1] for prediction, and x\u0302c[t] is the prediction at time t of\nThis work is supported in part by IBM Faculty Award and TUBITAK, Contract no: 112E161.\nThe authors are with the Department of Electrical and Electronics Engineering, Bilkent University, Bilkent, Ankara 06800, Turkey (e-mail: vanli@ee.bilkent.edu.tr, kozat@ee.bilkent.edu.tr).\nthe predictor c such that c \u2208 C, where C represents the class of predictors we \u201ccompete\u201d against. We emphasize that since the predictors x\u0302c[t], c \u2208 C have the access to the entire sequence before the processing starts, the minimum squared prediction error that can be achieved with a sequential predictor x\u0302s[t] is equal to the squared prediction error of the optimal batch predictor x\u0302c[t], c \u2208 C. Here, we call the difference in the squared prediction error of the sequential algorithm x\u0302s[t] and the optimal batch predictor x\u0302c[t], c \u2208 C as the \u201cregret\u201d of not using the optimal predictor (or equivalently, not knowing the future). Therefore, we seek for sequential algorithms x\u0302s[t] that minimize this \u201cregret\u201d or loss for any possible {x[t]}t\u22651. We emphasize that this regret definition is for the accumulated sequential cost, instead of the batch cost.\nInstead of fixing a comparison class of predictors, we parameterize the comparison classes such that the parameter set and functional form of these classes can be chosen as desired. In this sense, in this paper, we consider the most general class of parametric predictors as our class of predictors C such that the \u201cregret\u201d for an arbitrary length of data n is given by\nn\u2211\nt=1\n(x[t]\u2212 x\u0302s[t]) 2 \u2212 inf\nw\u2208Rm\nn\u2211\nt=1\n( x[t]\u2212 f(w, xt\u22121t\u2212a) )2 , (1)\nwhere f(w, xt\u22121t\u2212a) is a parametric function whose parameters w = [w1, . . . , wm]\nT can be set prior to prediction, and this function uses the data xt\u22121t\u2212a, t \u2212 a \u2265 1 for prediction for some arbitrary integer a, which can be viewed as the tap size of the predictor.1 Although the parameters of the parametric prediction function f(w, xt\u22121t\u2212a) can be set arbitrarily, even by observing all the data {x[t]}t\u22651 a priori, the function is naturally restricted to use only the sequential data xt\u221211 in prediction [5]\u2013[7].\nSince we have no statistical assumptions on the underlying data, the corresponding lower and upper bounds on the regret in (1) in this sense provide the \u201cultimate\u201d measure of the learning performance for any sequential predictor. We emphasize that lower bounds not only provide the worst-case performance of an algorithm, but also quantify the prediction power of the parametric class. As such, a positive lower bound guarantees the existence of a data sequence having an arbitrary length such that no matter how \u201csmart\u201d the learning algorithm is, the performance of this smart algorithm on this sequence will be worse than the class of parametric predictors by at least an order of the \u201clower bound\u201d. Hence if an algorithm is\n1All vectors are column vectors and denoted by boldface lower case letters. For a vector u, uT is the ordinary transpose. We denote xba , {x[t]} b t=a .\n2 found such that the upper bound of the regret of that algorithm matches with the lower bound, then that algorithm is optimal in a strong minimax sense such that the actual convergence performance cannot be further improved [7]. To this end, the minimax sense optimality of different parametric learning algorithms such as the well-known prediction algorithms, least mean squares (LMS) [8], recursive least squares (RLS) [8], and online sequential extreme learning machine (OS-ELM) of [1] can be determined using the lower bounds provided in this paper. In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].\nThe mixture of experts framework is previously used in order to derive such upper and lower bounds for the performance of an algorithm. As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied. These results are then extended to the filtering problems [13], [14]. In this paper on the other hand, we consider a holistic approach and provide upper and lower bounds for the general framework, which was previously missing in the literature.\nOur main contribution in this paper is to obtain the generalized lower bounds for a variety of prediction frameworks by transforming the prediction problem to a well-known and studied statistical parameter learning problem [1], [4]\u2013[7]. By doing so, we prove that for any sequential algorithm there always exist some data sequence over any length such that the regret of the sequential algorithm is lower bounded by zero. We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15]. We also provide a universal sequential prediction algorithm and calculate upper bounds on the regret of this algorithm, and show that we obtain matching lower and upper bounds in some scenarios. As an interesting result we also show that given the regret in (1) as the performance measure, there is no additional gain achieved by using randomized algorithms in the worst-case scenario.\nIn Section II, we first present general lower bounds, and then analyze couple of specific scenarios. We then introduce a universal prediction algorithm and calculate the upper bounds on its regret in Section III. In Section IV, we show that in the worst-case scenario, the performance of randomized algorithms can be achieved by sequential algorithms. We finalize our paper by pointing out several concluding remarks."}, {"heading": "II. LOWER BOUNDS", "text": "In this section, we investigate the worst case performance of sequential algorithms to obtain guaranteed lower bounds on the regret. Hence for any arbitrary length of data n, {x[t]}t\u22651, we are trying to find a lower bound on the following\nsup xn 1\n{ n\u2211\nt=1\n(x[t] \u2212 x\u0302s[t]) 2 \u2212 inf\nw\u2208Rm\nn\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n}\n.\n(2) For this regret, we have the following theorem which relates the performance of any sequential algorithm to the general\nclass of parametric predictors. While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].\nTheorem 1: There is no \u201cbest\u201d sequential algorithm for all sequences for any class in the parametric form f(w, xt\u22121t\u2212a), where w \u2208 Rm. Given a parametric class there exist always a sequence such that the regret in (2) is always lower bounded by some nonnegative value.\nThis theorem implies that no matter how smart a sequential algorithm is or how naive the competition class is, it is not possible to outperform the competition class for all sequences. As an example, this result demonstrates that even competing against the class of constant predictors, i.e., the most naive competition class, where x\u0302c[t] always predicts a constant value, any sequential algorithm, no matter how smart, cannot outperform this class of constant predictors for all sequences. We emphasize that in this sense, the lower bounds provide the prediction and modeling power of the parametric class.\nProof of Theorem 1: We begin our proof by pointing out that finding the \u201cbest\u201d sequential predictor for an arbitrary and unknown sequence of xn1 is not straightforward. Yet, for a specific distribution on xn1 , the best predictor is the conditional mean on xn1 under the squared error [16]. Therefore, by this clever transformation, we are able to calculate the regret in (2) in the expectation sense and prove this theorem.\nSince the supremum in (2) is taken over all xn1 , for any distribution xn1 , the regret is lower bounded by\nsup xn 1\n( n\u2211\nt=1\n(x[t] \u2212 x\u0302s[t]) 2 \u2212 inf\nw\u2208Rm\nn\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n)\n\u2265 Exn 1\n[ n\u2211\nt=1\n(x[t]\u2212 x\u0302s[t]) 2 \u2212 inf\nw\u2208Rm\nn\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n]\n\ufe38 \ufe37\ufe37 \ufe38\n,L(n)\n,\nwhere expectation is taken with respect to this particular distribution. Hence it is enough to lower bound L(n) to get a final lower bound. By the linearity of the expectation\nL(n) = Exn 1\n[ n\u2211\nt=1\n(x[t]\u2212 x\u0302s[t]) 2\n]\n\u2212 Exn 1\n[\ninf w\u2208Rm\nn\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n]\n. (3)\nThe squared-error loss E [ (x[t]\u2212x\u0302s[t]) 2 ]\nis minimized with the well-known minimum mean squared error (MMSE) predictor given by [16]\nx\u0302s[t] = E [ x[t] \u2223 \u2223x[t\u2212 1], . . . , x[1] ] = E [ x[t] \u2223 \u2223xt\u221211 ] , (4)\nwhere we drop the explicit xn1 -dependence of the expectation to simplify the presentation.\nSuppose we select a parametric distribution for xn1 with parameter vector \u03b8 = [\u03b81, . . . , \u03b8m]. Then for\nExn 1\n[\ninf w\u2208Rm\nn\u2211\nt=1\n(x[t] \u2212 f(w, xt\u22121t\u2212a)) 2\n]\n3 in (3), we use the following inequality\nE\u03b8\n[\nE xn 1 \u2223 \u2223\u03b8\n[\ninf w\u2208Rm\nn\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n]]\n\u2264 E\u03b8\n[\ninf w\u2208Rm E xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n]]\n.(5)\nBy using (4)-(5), and expanding the expectation we can lower bound L(n) as\nL(n) \u2265 E\u03b8\n[\nE xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n( x[t]\u2212 E [ x[t] \u2223 \u2223xt\u221211 ])2\n]]\n\u2212 E\u03b8\n[\ninf w\u2208Rm E xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n]]\n.\n(6)\nThe inequality in (6) is true for any distribution on xn1 . Hence for a distribution on xn1 such that\nE [ x[t] \u2223 \u2223xt\u221211 , \u03b8 ] = h(\u03b8, xt\u22121t\u2212a), (7)\nwith some function h, if we can find a vector function g(\u03b8) satisfying f(g(\u03b8), xt\u22121t\u2212a) = h(\u03b8, x t\u22121 t\u2212a) then the last term in (6) yields\nE\u03b8\n[\ninf w\u2208Rm E xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n(x[t]\u2212 f(w, xt\u22121t\u2212a)) 2\n]]\n= E\u03b8\n[\nE xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n(x[t]\u2212 h(\u03b8, xt\u22121t\u2212a)) 2\n]]\n.\nThus (6) can be written as\nL(n) \u2265 E\u03b8\n[\nE xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n( x[t]\u2212 E [ x[t] \u2223 \u2223xt\u221211 ])2\n]]\n\u2212E\u03b8\n[\nE xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n( x[t]\u2212 E [ x[t] \u2223 \u2223xt\u221211 , \u03b8 ])2\n]]\n,\nwhich is by definition of the MMSE estimator is always lower bounded by zero, i.e.,\nL(n) \u2265 0.\nBy this inequality we conclude that for predictors of the form f(w, xt\u22121t\u2212a) for which this special parametric distribution, i.e., w = g(\u03b8) exists, the best sequential predictor will be always outperformed by some predictor in this class for some sequence xn1 . Hence there is no \u201cbest\u201d algorithm for all sequences for any class in this parametric form. The question arises if a suitable distribution on xn1 can be found for a given f(w, xt\u22121t\u2212a) such that f(g(\u03b8), x t\u22121 t\u2212a) = h(\u03b8, x t\u22121 t\u2212a) with a suitable transformation g(\u03b8). Suppose f(w, xt\u22121t\u2212a) is bounded by some M \u2208 R\n+ with M < \u221e for all |x[t]| \u2264 A, i.e., |f(w, xt\u22121t\u2212a)| \u2264 M . Then, given \u03b8 from a beta distribution with parameters (C,C), C \u2208 R+, we generate a sequence xn1 such that x[t] = A M f(w, xt\u22121t\u2212a) with probability \u03b8 and x[t] = \u2212 A M f(w, xt\u22121t\u2212a) with probability (1\u2212 \u03b8). Then\nE [ x[t] \u2223 \u2223xt\u221211 , \u03b8 ] = A M (2\u03b8 \u2212 1)f(w, xt\u22121t\u2212a).\nHence, this concludes the proof of the Theorem 1. As an important special case, if we use the restricted functional form f(w, xt\u22121t\u2212a) so that f(w, x t\u22121 t\u2212a) is separable,\nthen the prediction problem is transformed to a parameter estimation problem. The separable form is given by\nf(w, xt\u22121t\u2212a) = fw(w) T fx(x t\u22121 t\u2212a),\nwhere fw(w) and fx(x t\u22121 t\u2212a) are vector functions of size m\u00d71 for some integer m. Then (7) can be written as\nE [ x[t] \u2223 \u2223xt\u221211 , \u03b8 ] = fw(g(\u03b8)) T fx(x t\u22121 t\u2212a),\nwhere fw(g(\u03b8)) = A M (2\u03b8 \u2212 1)fw(w). Denoting fn(w) , A M fw(w) as the normalized prediction function, and after some algebra (6) is obtained as\nL(n) \u2265\nE\u03b8\n[\nE xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n(\nx[t]\u2212E [ (2\u03b8\u22121) \u2223 \u2223xt\u221211 ]T fn(w) Tfx(x t\u22121 t\u2212a)\n)2 ]]\n\u2212 E\u03b8\n[\nE xn 1 \u2223 \u2223\u03b8\n[ n\u2211\nt=1\n( x[t]\u2212 (2\u03b8 \u2212 1)fn(w) Tfx(x t\u22121 t\u2212a) )2\n]]\n,\nso that the regret of the sequential algorithm over the best prediction function is due to the regret attained by the sequential algorithm while learning the parameters of the prediction function, i.e, the parameters of the underlying distribution. To illustrate this procedure, we investigate the regret given in (2) for three candidate function classes that are widely studied in computational learning theory.\nA. mth-order Univariate Polynomial Prediction\nFor a mth order polynomial in x[t\u2212 1] the regret is given by\nsup xn 1\n \n\nn\u2211\nt=1\n(x[t]\u2212 x\u0302s[t]) 2\u2212 inf w\u2208Rm\nn\u2211\nt=1\n(\nx[t]\u2212\np \u2211\ni=1\nwix i[t\u2212 1]\n)2 \n  ,\n(8) where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t \u2212 1] for prediction, w = [w1, . . . , wm]T is the parameter vector, xi[t\u2212 1] is the ith power of x[t\u2212 1].\nSince \u2211m\ni=1 wix i[t \u2212 1] = w1x[t \u2212 1] with appropriate\nselection of w, considering the following distribution on xn1 , we can lower bound the regret in (8). Given \u03b8 from a beta distribution with parameters (C,C), C \u2208 R+, we generate a sequence xn1 having only two values, A and \u2212A such that x[t] = x[t \u2212 1] with probability \u03b8 and x[t] = \u2212x[t \u2212 1] with probability (1\u2212 \u03b8). Then\nE [ x[t] \u2223 \u2223xt\u221211 , \u03b8 ] = (2\u03b8 \u2212 1)x[t\u2212 1],\ngiving h(\u03b8, xt\u22121t\u2212a) = (2\u03b8\u2212 1)x[t\u2212 1]. Since the MMSE given \u03b8 is linear in x[t\u22121], the optimum w that minimizes the accumulated error for this distribution is w = [(2\u03b8\u22121), 0, . . . , 0]T . After following the lines in [5], we obtain a lower bound of the form O(ln(n))."}, {"heading": "B. Multivariate Polynomial Prediction", "text": "Suppose the prediction function is given by wTfx(x t\u22121 t\u2212a) = \u2211m k=1 wkfk(x t\u22121 t\u2212r), where each fk(x t\u22121 t\u2212r) is a multivariate polynomial function (as an example fk(x t\u22121 t\u2212r) = x[t\u22121]x2[t\u22122] x[t\u22123] ), and regret is taken over all w = [w1, . . . , wm] T \u2208 Rm, i.e.,\nsup xn 1\n{ n\u2211\nt=1\n(x[t] \u2212 x\u0302s[t]) 2 \u2212 inf\nw\u2208Rm\nn\u2211\nt=1\n( x[t]\u2212wTfx(x t\u22121 t\u2212a) )2\n}\n,\n4 where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t \u2212 1] for prediction, and w is the parameter for prediction.\nWe emphasize that this class of predictors are not only the super set of univariate polynomial predictors, but also widely used in many signal processing applications to model nonlinearity such as Volterra filters [15]. This filtering technique is attractive when linear filtering techniques do not provide satisfactory results, and includes cross products of the input signals.\nSince \u2211m k=1 wkfk(x t\u22121 t\u2212r) = w1f1(x t\u22121 t\u2212r) with an appropriate selection of w and redefinition of f1(x t\u22121 t\u2212r), we define the following parametric distribution on xn1 to obtain a lower bound. Given \u03b8 from a beta distribution with parameters (C,C), C \u2208 R+, we generate a sequence xn1 having only two values, A and \u2212A, such that x[t] = fn(x t\u22121 t\u2212a) with probability \u03b8 and x[t] = \u2212fn(x t\u22121 t\u2212a) with probability (1 \u2212 \u03b8), where fn(x t\u22121 t\u2212a) = Af1(x t\u22121 t\u2212r ) M , i.e. normalized version of f1(x t\u22121 t\u2212r). Thus, given \u03b8, xn1 forms a two-state Markov chain with transition probability (1\u2212 \u03b8). Then\nE [ x[t] \u2223 \u2223xt\u221211 , \u03b8 ] = (2\u03b8 \u2212 1)fn(x t\u22121 t\u2212a).\nThe lower bound for the regret is given by\nL(n) = E [\n(x[t] \u2212 (2\u03b8\u0302 \u2212 1)fn(x t\u22121 t\u2212a))\n2 ]\n\u2212 E [ (x[t]\u2212 (2\u03b8 \u2212 1)fn(x t\u22121 t\u2212a)) 2 ] ,\nwhere \u03b8\u0302 = E[\u03b8|xt\u221211 ]. After some algebra we achieve\nL(n) = \u22124E[\u03b8\u0302x[t]fn(x t\u22121 t\u2212a)] + 4E[\u03b8x[t]fn(x t\u22121 t\u2212a)]\n+ E[(2\u03b8\u0302 \u2212 1)2]\u2212 E[(2\u03b8 \u2212 1)2].\nIt can be deduced that\n\u03b8\u0302 = E[\u03b8|xt\u221211 ] = t\u2212 2\u2212 Ft\u22122 + C\nt\u2212 2 + 2C ,\nwhere Ft\u22122 is the total number of transitions between the two states in a sequence of length (t\u22121), i.e., \u03b8\u0302 is ratio of number of transitions to time period. Hence,\nE[\u03b8\u0302x[t]fn(x t\u22121 t\u2212a)] = E\n[ t\u2212 2\u2212 Ft\u22122 + C\nt\u2212 2 + 2C x[t]fn(x\nt\u22121 t\u2212a)\n]\n= (t\u2212 2 + C)E[x[t]fn(x\nt\u22121 t\u2212a)]\u2212 E[Ft\u22122x[t]fn(x t\u22121 t\u2212a)]\nt\u2212 2 + 2C\n= \u2212 1\nt\u2212 2 + 2C E[(1\u2212 \u03b8)(t\u2212 2)x[t]fn(x\nt\u22121 t\u2212a)]\n= t\u2212 2\nt\u2212 2 + 2C E[\u03b8x[t]fn(x\nt\u22121 t\u2212a)],\nwhere the third line follows from\nE[x[t]fn(x t\u22121 t\u2212a)] = E[(2\u03b8 \u2212 1)A 2] = 0,\nand E[Ft\u22122|x[t]fn(x t\u22121 t\u2212a)] = (t \u2212 2)(1 \u2212 \u03b8) since Ft\u22122 is a binomial random variable with parameters (1 \u2212 \u03b8) and size (t\u2212 2). Thus we obtain\nL(n)=\u22124 t\u2212 2\nt\u2212 2 + 2C E[\u03b8x(t)fn(x\nt\u22121 t\u2212a)]+4E[\u03b8x(t)fn(x t\u22121 t\u2212a)]\n+ E[(2\u03b8\u0302 \u2212 1)2]\u2212 E[(2\u03b8 \u2212 1)2].\nAfter this line the derivation follows similar lines to [7], giving a lower bound of the form O(ln(n)) for the regret.\nC. k-ahead mth-order Linear Prediction\nThe regret in (2) for k-ahead mth-order linear prediction is given by\nsup xn 1\n{ n\u2211\nt=1\n(x[t] \u2212 x\u0302s[t]) 2 \u2212 inf\nw\u2208Rm\nn\u2211\nt=1\n( x[t]\u2212wTx[t\u2212 k] )2\n}\n,\n(9) where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t\u2212k] for prediction for some integer k, w = [w1, . . . , wm]T is the parameter vector, and x[t\u2212 k] = [x[t\u2212 k], . . . , x[t\u2212 k \u2212m+ 1]]T .\nWe first find a lower bound for k-ahead first-order prediction where wTx[t \u2212 k] = wx[t \u2212 k]. For this purpose we define the following parametric distribution on xn1 as in [5]. Given \u03b8 from a beta distribution with parameters (C,C), C \u2208 R+, we generate a sequence xn1 having only two values, A and \u2212A, such that x[t] = x[t\u2212k] with probability \u03b8 and x[t] = \u2212x[t\u2212k] with probability (1\u2212 \u03b8). Thus, given \u03b8, xn1 forms a two-state Markov chain with transition probability (1 \u2212 \u03b8). Then,\nE [ x[t] \u2223 \u2223xt\u2212k1 , \u03b8 ] = (2\u03b8 \u2212 1)x[t\u2212 k]\ngiving h(\u03b8, xt\u22121t\u2212a) = (2\u03b8 \u2212 1)x[t \u2212 k] and g(\u03b8) = (2\u03b8 \u2212 1). After this point the derivation exactly follows the lines in [5] resulting a lower bound of the form O(ln(n)).\nFor k-ahead mth-order prediction, we generalize the lower bound obtained for k-ahead first-order prediction and following the lines in [5], we obtain a lower bound of the form O(m ln(n)).\nWe next derive upper bounds for a universal sequential prediction algorithm."}, {"heading": "III. A COMPREHENSIVE APPROACH TO REGRET MINIMIZATION", "text": "In this section, we introduce a method which can be used to predict a bounded, arbitrary, and unknown sequence. We derive the upper bounds of this algorithm such that for any sequence xn1 , our algorithm will not perform worse than the presented upper bounds. In some cases, by achieving matching upper and lower bounds, we prove that this algorithm is optimal in a strong minimax sense such that the worst-case performance cannot be further improved.\nWe restrict the prediction functions to be separable, i.e., f(w, xt\u22121t\u2212a) = fw(w) T fx(x t\u22121 t\u2212a), where fw(w) and fx(x t\u22121 t\u2212a) are vector functions of size m \u00d7 1 for some m integer. To avoid any confusion we simply denote \u03b2 , fw(w), where \u03b2 \u2208 Rm. Hence, the same prediction function can be written as f(w, xt\u22121t\u2212a) = \u03b2 Tfx(x t\u22121 t\u2212a).\nIf the parameter vector \u03b2 is selected such that the total squared prediction error is minimized over a batch of data of length n, then the coefficients are given by\n\u03b2\u2217[n] = argmin \u03b2\u2208Rm\nn\u2211\nt=1\n(\nx[t]\u2212 \u03b2Tfx(x t\u22121 t\u2212a)\n)2\n.\nThe well-known least-squares solution to this problem is given by \u03b2\u2217[n] = (Rn\nff )\u22121rn xf , where\nRnff ,\nn\u2211\nt=1\nfx(x t\u22121 t\u2212a)fx(x t\u22121 t\u2212a) T\n5 is invertible and\nrn xf ,\nn\u2211\nt=1\nx[t]fx(x t\u22121 t\u2212a).\nWhen Rn ff is singular, the solution is no longer unique, however a suitable choice can be made using, e.g. pseudoinverses.\nWe also consider the more general least-squares (ridge regression) problem that arises in many signal processing problems, and whose total squared prediction error is minimized over a batch of data of length n with\n\u03b2\u2217[n] = argmin \u03b2\u2208Rm\n{ n\u2211\nt=1\n(\nx[t]\u2212 \u03b2Tfx(x t\u22121 t\u2212a)\n)2 + \u03b4 ||\u03b2||2 } ,\n= [ Rnff + \u03b4I ]\u22121 rn xf .\nWe define a universal predictor x\u0303u[n], as\nx\u0303u[n] = \u03b2u[n\u2212 1] Tf(xn\u22121n\u2212a),\nwhere \u03b2u[n] = \u03b2 \u2217[n] = [ Rnff + \u03b4I ]\u22121 rn xf , and \u03b4 > 0 is a positive constant. Theorem 2: The total squared prediction error of the mthorder universal predictor for any bounded arbitrary sequence of {x[t]}t\u22651, |x[t]| \u2264 A, having an arbitrary length of n satisfies n\u2211\nt=1\n(x[t]\u2212 x\u0303u[t]) 2\u2264 min\n\u03b2\u2208Rm\n{ n\u2211\nt=1\n(x[t]\u2212\u03b2Tfx(x t\u22121 t\u2212a)) 2+\u03b4 ||\u03b2|| 2\n}\n+A2 ln \u2223 \u2223I +Rnff \u03b4 \u22121\u2223\u2223.\nTheorem 2 indicates that the total squared prediction error of the mth-order universal predictor is within O(m ln(n)) of the best batch mth-order parametric predictor for any individual sequence of {x[t]}t\u22651. This result implies that in order to learn m parameters, the universal algorithm pays a regret of O(m ln(n)), which can be viewed as the parameter regret. After we prove Theorem 2, we apply Theorem 2 to the competition classes discussed in Section II.\nProof of Theorem 2: We prove this result for a scalar prediction function such that fx(x t\u22121 t\u2212a) = fx(x t\u22121 t\u2212a) to avoid any confusions. Yet for a vector prediction function of fx(x t\u22121 t\u2212a), one can follow the exact same steps in this proof with vector extensions of the Gaussian mixture.\nThe derivations follow similar lines to [5], [10], hence only main points are presented. We first define a function of the loss, namely the \u201cprobability\u201d for a predictor having parameter \u03b2 as follows\nP\u03b2(x n 1 ) = exp\n(\n\u2212 1\n2h\nn\u2211\nk=1\n(x[k] \u2212 \u03b2fx(x t\u22121 t\u2212a)) 2\n)\n,\nwhich can be viewed as a probability assignment of the predictor with parameter \u03b2 to the data x[t], for 1 \u2264 t \u2264 n, induced by performance of \u03b2 on the sequence xn1 . We then construct a universal estimate of the probability of the sequence xn1 , as an a-priori weighted mixture among all of the probabilities, i.e., Pu(xn1 ) = \u222b\u221e \u2212\u221e p(\u03b2)P\u03b2(x n 1 )d\u03b2, where p(\u03b2) is an apriori weight assigned to the parameter \u03b2, and is selected as Gaussian in order to obtain a closed form bounds, i.e., p(\u03b2) = 1\u221a\n2\u03c0\u03c3 exp\n{\n\u2212 \u03b2 2\n2\u03c32\n}\n.\nFollowing similar lines to [7] with a predictor of \u03b2fx(x t\u22121 t\u2212a)\nwe obtain\nPu(xn|x n\u22121) = \u03b3 exp\n{ \u22121\n2h \u03b32\n( x[n]\u2212 \u03b2[n\u2212 1]f(xn\u22121n\u2212a)\n)2 }\n,\nwhere \u03b3 , \u221a\n(Rn\u22122ff + \u03b4)/(R n\u22121 ff + \u03b4). If we could find\nanother Gaussian satisfying P\u0303u(xn) \u2265 Pu(xn), then it would complete the proof of the theorem.\nAfter some algebra we find that the universal predictor is given by\nx\u0303u[n] = \u03b3 2\u03b2\u2217[n\u2212 1]f(xn\u22121n\u2212a) =\nrn\u22121xf\nRn\u22121ff + \u03b4 f(xn\u22121n\u2212a).\nNow we can select the smallest value of h over the region [\u2212A,A], P\u0303u(xn|xn\u22121) is larger than Pu(xn|xn\u22121), i.e.,\nA \u2264\n\u221a\n2h ln(\u03b3)(\u03b32 \u2212 1) + \u03b32x\u0302u[n]2(1 \u2212 \u03b32)\n(1\u2212 \u03b32)\nh \u2265 A2(1\u2212 \u03b32)\u2212 \u03b32x\u0302u[n] 2\n\u22122 ln(\u03b3) ,\nwhich must hold for all values of x\u0302u[n] \u2208 [\u2212A,A]. Therefore h \u2265 A2 (1\u2212\u03b3\n2) \u22122 ln(\u03b3) , where \u03b3 < 1. Note that for 0 < \u03b3 < 1\nwe have 0 < (1\u2212\u03b3 2)\n\u22122 ln \u03b3 < 1, which implies that we must have\nh \u2265 A2 to ensure that P\u0303u \u2265 Pu. In fact, since this bound on the value of h depends upon the value of \u03b3 and x\u0302u[n], and is only tight for \u03b3 \u2192 1, and x\u0302u[n] = 0, then the restriction that |x[n]| < A can actually be occasionally violated, as long as P\u0303u \u2265 Pu still holds.\nTo illustrate this procedure, we investigate the upper bound for the regret in (2) for the same candidate function classes as we also investigated in Section II.\nA. mth-order Univariate Polynomial Predictor\nFor a mth order polynomial in x[t\u22121], the prediction function is given by f(w, xt\u22121t\u2212a) = \u03b2 Tfx(x t\u22121 t\u2212a) = \u03b2\nTm[t \u2212 1] where m[t \u2212 1] = [x[t \u2212 1], . . . , xm[t \u2212 1]]T , i.e. the vector of powers of x[t \u2212 1]. After replacing Rn\nff = Rnmm =\n\u2211n t=1 m[t\u22121]m[t\u22121] T and rn xf = rnxm = \u2211n t=1 x[t]m[t\u2212 1], we obtain an upper bound n\u2211\nt=1\n(x[t]\u2212 x\u0303u[t]) 2\u2264 min\n\u03b2\u2208Rm\n{ n\u2211\nt=1\n(x[t]\u2212\u03b2Tm[t\u2212 1])2+\u03b4 ||\u03b2|| 2\n}\n+A2 ln \u2223 \u2223I +Rnmm\u03b4 \u22121\u2223\u2223,\nwhich implies that n\u2211\nt=1\n(x[t]\u2212 x\u0303u[t]) 2\u2264 min\n\u03b2\u2208Rm\n{ n\u2211\nt=1\n(x[t]\u2212\u03b2Tm[t\u2212 1])2+\u03b4 ||\u03b2||2 }\n+A2m ln\n(\n1 + A2n\n\u03b4\n)\n."}, {"heading": "B. Multivariate Polynomial Prediction", "text": "The upper bound for a multivariate polynomial prediction function fx(x t\u22121 t\u2212a) exactly follows the upper bound derivation of mth order univariate polynomial predictor giving an upper bound n\u2211\nt=1\n(x[t]\u2212 x\u0303u[t]) 2\u2264 min\n\u03b2\u2208Rm\n{ n\u2211\nt=1\n(x[t]\u2212\u03b2Tfx(x t\u22121 t\u2212a)) 2+\u03b4 ||\u03b2|| 2\n}\n+A2m ln\n(\n1 + A2n\n\u03b4\n)\n.\n6 C. k-ahead mth-order Linear Prediction\nFor k-ahead mth-order prediction, the prediction class is given by f(w, xt\u22121t\u2212a) = \u03b2 Tfx(x t\u22121 t\u2212a) = \u03b2\nTx[t \u2212 k] where x[t\u2212 k] = [x[t\u2212 k], . . . , x[t\u2212 k \u2212m+ 1]]T as before. After replacing Rn\nff = Rnxx = \u2211n t=1 x[t\u2212k]x[t\u2212k] T and rn xf =\nrnxx = \u2211n\nt=1 x[t]x[t \u2212 k] with suitable limits we obtain an upper bound n\u2211\nt=1\n(x[t]\u2212 x\u0303u[t]) 2\u2264 min\n\u03b2\u2208Rm\n{ n\u2211\nt=1\n(x[t]\u2212\u03b2Tx[t\u2212 k])2+\u03b4 ||\u03b2|| 2\n}\n+ A2m ln\n(\n1 + A2n\n\u03b4\n)\n."}, {"heading": "IV. RANDOMIZED OUTPUT PREDICTIONS", "text": "In this section, we investigate the performance of randomized output algorithms for the worst-case scenario with respect to linear predictors with using the same regret measure in (2). We emphasize that the randomized output algorithms are a super set of the deterministic sequential predictors and the derivations here can be readily generalized to include any prediction class. Particularly, we consider randomized output algorithms f ( \u03b8(xt\u221211 ), x t\u22121 1 ) such that the randomization parameters \u03b8 \u2208 Rm can be a function of the whole past. Hence, a randomized sequential algorithm introduce randomization or uncertainty in its output such that the output also depends on a random element. Note that such methods are widely used in applications involving security considerations. As an example, suppose there are m prediction algorithms running in parallel to predict the observation sequence {x[t]}t\u22651 sequentially. At each time t, the randomized output algorithm selects one of the constituent algorithms randomly such that the algorithm k is selected with probability pk[t]. By definition \u2211m k=1 pk[t] = 1 and pk[t] may be generated as the combination of the past observation samples xt\u221211 and a seed independent from the observations.\nFor such randomized output prediction algorithms we consider the following time-accumulated prediction error over a deterministic sequence {x[t]}t\u22651 as the prediction error,\nPrand(n) = n\u2211\nt=1\nE\u03b8\n[( x[t]\u2212 f ( \u03b8(xt\u221211 ), x t\u22121 1 ))2 ] . (10)\nThis expectation is taken over all the randomization due to independent or dependent seeds. Hence our general regret can be extended to include this performance measure\nsup xn 1\n{\nPrand(n)\u2212 min w\u2208Rm\nn\u2211\nt=1\n( x[t]\u2212wTx[t\u2212 1] )2\n}\n. (11)\nExpanding (10) we obtain\nPrand(n) = n\u2211\nt=1\n{ ( x[t]\u2212 E\u03b8 [ f ( \u03b8(xt\u221211 ), x t\u22121 1 )])2\n+ Var\u03b8 ( f ( \u03b8(xt\u221211 ), x t\u22121 1\n)) }\n,\nnoting that x[t] is independent of the randomization. Since E\u03b8 [ f ( \u03b8(xt\u221211 ), x t\u22121 1 )] is a sequential function of xt\u221211 and Var\u03b8 ( f ( \u03b8(xt\u221211 ), x t\u22121 1 )) is always nonnegative, the performance of a randomized output algorithm can be reached by a deterministic sequential algorithm.\nSince deterministic algorithms are subclass of randomized output algorithms, upper bounds we derived for k-ahead mthorder prediction in (9) also hold for (11). Since we also proved that the lower bound for such linear predictions of mth order are in the form of O(m ln(n)), the lower and upper bounds are tight and of the form O(m ln(n)).\nV. CONCLUDING REMARKS In this paper, we consider the problem of sequential prediction from a mixture of experts perspective. We have introduced comprehensive lower bounds on the sequential learning framework by proving that for any sequential algorithm, there always exists a sequence for which the sequential predictor cannot outperform the class of parametric predictors, whose parameters are set non-casually. The lower bounds for important parametric classes such as univariate polynomial, multivariate polynomial, and linear predictor classes are derived in detail. We then introduced a universal sequential prediction algorithm and investigated the upper bound on the regret of this algorithm. We also derived the upper bounds in detail for the same important classes that we discussed for lower bounds, where we further showed that this algorithm is optimal in a strong minimax sense for some scenarios. Finally, we have proven that for the worst-case scenario, randomized algorithms cannot provide any improvement in the performance compared to the sequential algorithms.\nREFERENCES [1] N.-Y. Liang, G.-B. Huang, P. Saratchandran, and N. Sundararajan, \u201cA\nfast and accurate online sequential learning algorithm for feedforward networks,\u201d IEEE Transactions on Neural Networks, vol. 17, no. 6, pp. 1411\u20131423, 2006. [2] L. Devroye, T. Linder, and G. Lugosi, \u201cNonparametric estimation and classification using radial basis function nets and empirical risk minimization,\u201d IEEE Transactions on Neural Networks, vol. 7, no. 2, pp. 475\u2013487, 1996. [3] A. Krzyzak and T. Linder, \u201cRadial basis function networks and complexity regularization in function learning,\u201d IEEE Transactions on Neural Networks, vol. 9, no. 2, pp. 247\u2013256, 1998. [4] N. Cesa-Bianchi, P. Long, and M. Warmuth, \u201cWorst-case quadratic loss bounds for prediction using linear functions and gradient descent,\u201d IEEE Transactions on Neural Networks, vol. 7, no. 3, pp. 604\u2013619, 1996. [5] A. Singer and M. Feder, \u201cUniversal linear prediction by model order weighting,\u201d IEEE Transactions on Signal Processing, vol. 47, no. 10, pp. 2685\u20132699, 1999. [6] G. C. Zeitler and A. Singer, \u201cUniversal linear least-squares prediction in the presence of noise,\u201d in IEEE/SP 14th Workshop on Statistical Signal Processing, 2007. SSP \u201907, 2007, pp. 611\u2013614. [7] A. Singer, S. Kozat, and M. Feder, \u201cUniversal linear least squares prediction: upper and lower bounds,\u201d IEEE Transactions on Information Theory, vol. 48, no. 8, pp. 2354\u20132362, 2002. [8] T. Kailath, A. H. Sayed, and B. Hassibi, Linear Estimation. Prentice Hall, 2000. [9] V. Cherkassky, X. Shao, F. Mulier, and V. Vapnik, \u201cModel complexity control for regression using VC generalization bounds,\u201d IEEE Transactions on Neural Networks, vol. 10, no. 5, pp. 1075\u20131089, 1999. [10] J. Kivinen and M. K. Warmuth, \u201cExponentiated gradient versus gradient descent for linear predictors,\u201d Journal of Information and Computation, vol. 132, no. 1, pp. 1\u201362, 1997. [11] V. Vovk, \u201cCompetitive on-line statistics,\u201d International Statistical Review, vol. 69, pp. 213\u2013248, 2001. [12] T. Weissman and N. Merhav, \u201cUniversal prediction of individual binary sequences in the presence of noise,\u201d IEEE Transactions on Information Theory, vol. 47, no. 6, pp. 2151\u20132173, 2001. [13] T. Moon and T. Weissman, \u201cUniversal FIR MMSE filtering,\u201d IEEE Transactions on Signal Processing, vol. 57, no. 3, pp. 1068\u20131083, 2009. [14] \u2014\u2014, \u201cCompetitive on-line linear FIR MMSE filtering,\u201d in IEEE International Symposium on Information Theory, 2007. ISIT 2007., 2007, pp. 1126\u20131130. [15] V. Mathews, \u201cAdaptive polynomial filters,\u201d Signal Processing Magazine, IEEE, vol. 8, no. 3, pp. 10\u201326, 1991. [16] H. Stark and J. Woods, Probability, Random Processes, and Estimation Theory for Engineers. Upper Saddle River, NJ: Prentice-Hall, 1994."}], "references": [{"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17, no. 6, pp. 1411\u20131423, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonparametric estimation and classification using radial basis function nets and empirical risk minimization", "author": ["L. Devroye", "T. Linder", "G. Lugosi"], "venue": "IEEE Transactions on Neural Networks, vol. 7, no. 2, pp. 475\u2013487, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Radial basis function networks and complexity regularization in function learning", "author": ["A. Krzyzak", "T. Linder"], "venue": "IEEE Transactions on Neural Networks, vol. 9, no. 2, pp. 247\u2013256, 1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Worst-case quadratic loss bounds for prediction using linear functions and gradient descent", "author": ["N. Cesa-Bianchi", "P. Long", "M. Warmuth"], "venue": "IEEE Transactions on Neural Networks, vol. 7, no. 3, pp. 604\u2013619, 1996.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Universal linear prediction by model order weighting", "author": ["A. Singer", "M. Feder"], "venue": "IEEE Transactions on Signal Processing, vol. 47, no. 10, pp. 2685\u20132699, 1999.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Universal linear least-squares prediction in the presence of noise", "author": ["G.C. Zeitler", "A. Singer"], "venue": "IEEE/SP 14th Workshop on Statistical Signal Processing, 2007. SSP \u201907, 2007, pp. 611\u2013614.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal linear least squares prediction: upper and lower bounds", "author": ["A. Singer", "S. Kozat", "M. Feder"], "venue": "IEEE Transactions on Information Theory, vol. 48, no. 8, pp. 2354\u20132362, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Model complexity control for regression using VC generalization bounds", "author": ["V. Cherkassky", "X. Shao", "F. Mulier", "V. Vapnik"], "venue": "IEEE Transactions on Neural Networks, vol. 10, no. 5, pp. 1075\u20131089, 1999.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Journal of Information and Computation, vol. 132, no. 1, pp. 1\u201362, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Competitive on-line statistics", "author": ["V. Vovk"], "venue": "International Statistical Review, vol. 69, pp. 213\u2013248, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Universal prediction of individual binary sequences in the presence of noise", "author": ["T. Weissman", "N. Merhav"], "venue": "IEEE Transactions on Information Theory, vol. 47, no. 6, pp. 2151\u20132173, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Universal FIR MMSE filtering", "author": ["T. Moon", "T. Weissman"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 3, pp. 1068\u20131083, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Competitive on-line linear FIR MMSE filtering", "author": ["\u2014\u2014"], "venue": "IEEE International Symposium on Information Theory, 2007. ISIT 2007., 2007, pp. 1126\u20131130.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive polynomial filters", "author": ["V. Mathews"], "venue": "Signal Processing Magazine, IEEE, vol. 8, no. 3, pp. 10\u201326, 1991.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1991}, {"title": "Probability, Random Processes, and Estimation Theory for Engineers", "author": ["H. Stark", "J. Woods"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION In this brief paper, we investigate the generic sequential (online) prediction problem from an individual sequence perspective using tools of computational learning theory, where we refrain from any statistical assumptions either in modeling or on signals [1]\u2013[4].", "startOffset": 269, "endOffset": 272}, {"referenceID": 3, "context": "INTRODUCTION In this brief paper, we investigate the generic sequential (online) prediction problem from an individual sequence perspective using tools of computational learning theory, where we refrain from any statistical assumptions either in modeling or on signals [1]\u2013[4].", "startOffset": 273, "endOffset": 276}, {"referenceID": 0, "context": "Since we do not impose any statistical assumptions on the underlying data, we, motivated by recent results from sequential learning [1]\u2013[4], define the performance of a sequential algorithm with respect to a comparison class, where the predictors of the comparison class are formed by observing the the entire sequence in hindsight, under the squared error loss, i.", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "Since we do not impose any statistical assumptions on the underlying data, we, motivated by recent results from sequential learning [1]\u2013[4], define the performance of a sequential algorithm with respect to a comparison class, where the predictors of the comparison class are formed by observing the the entire sequence in hindsight, under the squared error loss, i.", "startOffset": 136, "endOffset": 139}, {"referenceID": 0, "context": "for an arbitrary length of data n, and for any possible sequence {x[t]}t\u22651, where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t\u2212 1] for prediction, and x\u0302c[t] is the prediction at time t of", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "1 Although the parameters of the parametric prediction function f(w, xt\u22121 t\u2212a) can be set arbitrarily, even by observing all the data {x[t]}t\u22651 a priori, the function is naturally restricted to use only the sequential data xt\u22121 1 in prediction [5]\u2013[7].", "startOffset": 244, "endOffset": 247}, {"referenceID": 6, "context": "1 Although the parameters of the parametric prediction function f(w, xt\u22121 t\u2212a) can be set arbitrarily, even by observing all the data {x[t]}t\u22651 a priori, the function is naturally restricted to use only the sequential data xt\u22121 1 in prediction [5]\u2013[7].", "startOffset": 248, "endOffset": 251}, {"referenceID": 6, "context": "found such that the upper bound of the regret of that algorithm matches with the lower bound, then that algorithm is optimal in a strong minimax sense such that the actual convergence performance cannot be further improved [7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 0, "context": "To this end, the minimax sense optimality of different parametric learning algorithms such as the well-known prediction algorithms, least mean squares (LMS) [8], recursive least squares (RLS) [8], and online sequential extreme learning machine (OS-ELM) of [1] can be determined using the lower bounds provided in this paper.", "startOffset": 256, "endOffset": 259}, {"referenceID": 7, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 177, "endOffset": 180}, {"referenceID": 2, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "In this sense, the \u201crates\u201d of the corresponding upper and lower bounds are analogous to the VC dimension [9] of classifiers and can be used to quantify the learning performance [1]\u2013[3], [10].", "startOffset": 186, "endOffset": 190}, {"referenceID": 4, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "As an example, linear prediction [5], [7], [11], nonlinear models based on locally linear approximations [6], and the learning of an individual noise-corrupted deterministic sequence [12] is studied.", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "These results are then extended to the filtering problems [13], [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "These results are then extended to the filtering problems [13], [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Our main contribution in this paper is to obtain the generalized lower bounds for a variety of prediction frameworks by transforming the prediction problem to a well-known and studied statistical parameter learning problem [1], [4]\u2013[7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 3, "context": "Our main contribution in this paper is to obtain the generalized lower bounds for a variety of prediction frameworks by transforming the prediction problem to a well-known and studied statistical parameter learning problem [1], [4]\u2013[7].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": "Our main contribution in this paper is to obtain the generalized lower bounds for a variety of prediction frameworks by transforming the prediction problem to a well-known and studied statistical parameter learning problem [1], [4]\u2013[7].", "startOffset": 232, "endOffset": 235}, {"referenceID": 3, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 203, "endOffset": 206}, {"referenceID": 8, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 208, "endOffset": 212}, {"referenceID": 10, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 213, "endOffset": 217}, {"referenceID": 13, "context": "We further derive lower bounds for important classes of predictors heavily investigated in machine learning literature including univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 3, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 272, "endOffset": 275}, {"referenceID": 6, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 276, "endOffset": 279}, {"referenceID": 8, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 281, "endOffset": 285}, {"referenceID": 10, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 286, "endOffset": 290}, {"referenceID": 13, "context": "While proving this theorem we also provide a generic procedure to find lower bounds on the regret in (2) and later use this method to derive lower bounds for parametric classes including the classes of univariate polynomial, multivariate polynomial, and linear predictors [4]\u2013[7], [10]\u2013[12], [15].", "startOffset": 292, "endOffset": 296}, {"referenceID": 14, "context": "Yet, for a specific distribution on x1 , the best predictor is the conditional mean on x1 under the squared error [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "The squared-error loss E [ (x[t]\u2212x\u0302s[t]) 2 ] is minimized with the well-known minimum mean squared error (MMSE) predictor given by [16]", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": ", x[1] ] = E [ x[t] \u2223 \u2223xt\u22121 1 ] , (4)", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "(8) where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t \u2212 1] for prediction, w = [w1, .", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "After following the lines in [5], we obtain a lower bound of the form O(ln(n)).", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t \u2212 1] for prediction, and w is the parameter for prediction.", "startOffset": 96, "endOffset": 99}, {"referenceID": 13, "context": "We emphasize that this class of predictors are not only the super set of univariate polynomial predictors, but also widely used in many signal processing applications to model nonlinearity such as Volterra filters [15].", "startOffset": 214, "endOffset": 218}, {"referenceID": 6, "context": "After this line the derivation follows similar lines to [7], giving a lower bound of the form O(ln(n)) for the regret.", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "(9) where x\u0302s[t] is the prediction at time t of any sequential algorithm that has access data from x[1] up to x[t\u2212k] for prediction for some integer k, w = [w1, .", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "For this purpose we define the following parametric distribution on x1 as in [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "After this point the derivation exactly follows the lines in [5] resulting a lower bound of the form O(ln(n)).", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "For k-ahead mth-order prediction, we generalize the lower bound obtained for k-ahead first-order prediction and following the lines in [5], we obtain a lower bound of the form O(m ln(n)).", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "The derivations follow similar lines to [5], [10], hence only main points are presented.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "The derivations follow similar lines to [5], [10], hence only main points are presented.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "Following similar lines to [7] with a predictor of \u03b2fx(x t\u22121 t\u2212a) we obtain", "startOffset": 27, "endOffset": 30}], "year": 2014, "abstractText": "We study sequential prediction of real-valued, arbitrary and unknown sequences under the squared error loss as well as the best parametric predictor out of a large, continuous class of predictors. Inspired by recent results from computational learning theory, we refrain from any statistical assumptions and define the performance with respect to the class of general parametric predictors. In particular, we present generic lower and upper bounds on this relative performance by transforming the prediction task into a parameter learning problem. We first introduce the lower bounds on this relative performance in the mixture of experts framework, where we show that for any sequential algorithm, there always exists a sequence for which the performance of the sequential algorithm is lower bounded by zero. We then introduce a sequential learning algorithm to predict such arbitrary and unknown sequences, and calculate upper bounds on its total squared prediction error for every bounded sequence. We further show that in some scenarios we achieve matching lower and upper bounds demonstrating that our algorithms are optimal in a strong minimax sense such that their performances cannot be improved further. As an interesting result we also prove that for the worst case scenario, the performance of randomized algorithms can be achieved by sequential algorithms so that randomized algorithms does not improve the performance.", "creator": "LaTeX with hyperref package"}}}