{"id": "1311.6802", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2013", "title": "Recommending with an Agenda: Active Learning of Private Attributes using Matrix Factorization", "abstract": "recommender systems purposely leverage user complex social interactions and demographic information, e. g., drinking age, gender discrimination and political relative affiliation, to personalize content internally and periodically monetize on their users. oftentimes, filtering users that do currently not volunteer this information due to privacy sensitivity concerns collectively or to test the lack strength of initiative in filling out their proprietary profile protection information. in building this simplified work, theoretically we illustrate then a new integrated threat layer in monitoring which the system can nevertheless learn the private attribute for those users who choose don't voluntarily disclose them. we design an internally active attack objective that generally solicit sensitivity ratings primarily for strategically selects selected identifiable items, significantly and could similarly thus then be used by using a potentially recommender system to pursue its already hidden agenda. our method is based therefore on a novel developed usage of synthetic bayesian gradient matrix driven factorization algorithms in seeking an active learning setting. evaluations, on multiple distinct datasets, thereby illustrate that such providing an active attack is clearly indeed feasible simultaneously and can instead be carried simply out using significantly fewer highly rated items than the carefully previously been proposed static inference methods. importantly, this second threat can indirectly succeed without accurately sacrificing the quality of the regular recommendations made to the user.", "histories": [["v1", "Tue, 26 Nov 2013 20:48:59 GMT  (7261kb,D)", "https://arxiv.org/abs/1311.6802v1", null], ["v2", "Wed, 30 Jul 2014 23:08:54 GMT  (2817kb,D)", "http://arxiv.org/abs/1311.6802v2", "This is the extended version of a paper that appeared in ACM RecSys 2014"]], "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["smriti bhagat", "udi weinsberg", "stratis ioannidis", "nina taft"], "accepted": false, "id": "1311.6802"}, "pdf": {"name": "1311.6802.pdf", "metadata": {"source": "CRF", "title": "Recommending with an Agenda: Active Learning of Private Attributes using Matrix Factorization", "authors": ["Smriti Bhagat", "Udi Weinsberg", "Stratis Ioannidis", "Nina Taft"], "emails": ["nina.taft}@technicolor.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "Recommender systems rely on knowing their users \u2013 not just their preferences (i.e., ratings on items), but also their social and demographic information, e.g., age, gender, political affiliation, and ethnicity [13, 18]. A rich user profile allows a recommender system to better personalize its services, and at the same time enables additional monetization opportunities, such as targeted advertising.\nUsers of a recommender system know they are disclosing their preferences (or ratings) for movies, books, or other items (we use movies as our running example). A recommender may obtain additional social and demographic information about its users by explicitly soliciting it [13, 18]. While some users may willingly disclose such information, others may be more privacy-sensitive and elect to only reveal their ratings. Privacy research has shown that some users are uncomfortable revealing their demographic data to personalization systems [1, 14]. Even when such services provide transparency about their data collection and use practices [1], some users are unwilling to disclose personal data despite the allure of personalized services. In [13] the authors conduct a small scale user study on Amazon Turk that examines how to motivate users to disclose their demographic data.\nFor users who wish to withhold some demographic information, a recommender can still undermine their attempt at privacy. In previous work [29], we show that users\u2019 movie ratings can be used to predict their gender with 80% accuracy. Other studies also show the potential to infer demographics from a range of online user activities [2, 3, 17, 19].\nIn this work, we consider a recommender system that offers a legitimate service, yet is simultaneously malicious: it purposefully attempts to extract certain attributes from users who choose to withhold them. Unlike previous work that studies static attacks on the complete data, we consider an active learning setting, in which the recommender system aims to efficiently (quickly and accurately) infer a user\u2019s private attribute via interactive questioning. Recommender systems routinely ask users to rate a few items, as a means to address a \u201ccold start\u201d setting, or to improve the quality of recommendations. We leverage these instances of interactions with the user, alongside with the observation that item selection is at the recommender\u2019s discretion, to propose a new attack. We hypothesize that if the sequence of questions (items to rate) is carefully selected, the recommender system can quickly (so as not to be detected by the user) determine a user\u2019s private attribute with high confidence, thereby violating her privacy. A key idea in the design of this attack is to leverage matrix factorization (MF) as the basis for inference. Most recommender systems use matrix factorization (MF) models as a building block for providing recommendations [16]. While MF is well understood for rating prediction, it has generally not been applied for inference. To the best of our knowledge, this paper is the first to leverage MF as the basis for building both (a) an inference method of private attributes using item ratings and (b) an active learning method that selects items in a way that maximizes inference confidence in the smallest number of questions.\nOur contributions are as follows: \u2022 First, we propose a novel classification method for deter-\nmining a user\u2019s binary private attribute \u2013 her type \u2013 based upon ratings alone. In particular, we use matrix factorization to learn item profiles and type-dependent biases, and show how to incorporate this information into a classification algorithm. This classification is consistent with Bayesian matrix factorization. \u2022 Second, we demonstrate that the resulting classification\nmethod is well suited for learning of a user\u2019s type. A simple passive approach, ordering items based on a set of weights computed off-line, works quite well in many cases. Beyond this, we design an active learning algorithm for selecting the next item to ask a user to rate: each selection maximizes the expected confidence of the private attribute\u2019s inference. Equivalently, the selections of the active learning algorithm minimize the expected risk of misclassifying the user\u2019s private attribute. \u2022 Third, we show that our active learning method is very\nar X\niv :1\n31 1.\n68 02\nv2 [\ncs .L\nG ]\n3 0\nJu l 2\n01 4\nefficient, as item selection can reuse computations made during previous selections. We show that this reduces the na\u0308\u0131ve solution that is cubic in the number of ratings, to one that is quadratic in the number of ratings. \u2022 Fourth, we extensively evaluate the above classifier and\nselection methods on three real-world datasets: Movielens, Flixster and Politics-and-TV. We show that our methods consistently outperform other baselines; with only 10 questions, we achieve between 3-20% higher classification accuracy on different datasets. Importantly, such an attack can be carried out without any sacrifice to the recommendations made to the user.\nIn the remainder of the paper, we review related work (Section 2) and formulate our problem (Section 3). We then present our classifier (Section 4), our active learning method (Section 5), and our empirical results (Section 6)."}, {"heading": "2. RELATED WORK", "text": "A number of studies have shown that user demographics can be inferred from various types of online user activity. For example, Bhagat et al. [2] show that it is possible to learn age and gender information from blogs. Mislove et al. [19] study data from online social networks and illustrate that even when only a fraction of users provide profile attributes (such as location, interests, etc.), it is possible to infer these attributes among users who do not disclose them. Bi et al. [3] show how demographics can be inferred from search queries, and Kosinski et al. [17] show that several personality traits, including political views, sexual orientation, and drug use can be accurately predicted from Facebook \u201clikes\u201d.\nRecommender systems were shown to be exploitable by several works utilizing off-line attacks [4, 21, 29]. Closest to our setting, Weinsberg et al. [29] empirically studied how to infer a user\u2019s gender from her movie ratings using a variety of different classifiers, showing that logistic regression and SVMs succeed with an accuracy close to 80%. We depart from [29] in multiple ways. First, we introduce a novel factor-based classifier, that relies on the Bayesian assumptions behind MF. Second, we study a recommender system in an adversarial setting that actively adapts item selection to quickly learn the private attributes. Finally, we establish that our classifier is very well suited for this task.\nThe Bayesian model underlying MF (discussed in detail in Section 3.2) was recently employed by Silva and Carin [27] to actively learn the actual factors (i.e., the user and item profiles) in MF. More specifically, the authors consider a recommender system that adaptively selects which items to ask its users to rate in order to diminish the entropy of its user and item profiles as quickly as possible. The entropy estimation is based on the Gaussian noise and prior assumptions underlying MF, which we also employ in our work. A variety of active learing objective were also studied by Sutherland et al. [28], including minimizing the prediction error on unrated items, reducing the profile uncertainty, and identifying highly rated items. We depart from the above works as the goal of our learning task is to discover a user\u2019s demographic information, captured by a categorical type, rather the above objectives motivated by rating prediction. Biases have been used extensively to improve prediction performance in MF [15, 16]. Our introduction of demographicspecific biases is not for improving prediction per se\u2013though this does happen; rather, incorporating such biases allows\nus to use the classic MF model as a basis for classification and, subsequently, active learning.\nIn the classic active learning setting [7,8], a learner wishes to disambiguate among a set of several possible hypotheses, each represented as a function over a set of inputs. Only one of the hypotheses is valid; to discover it, the learner has access to an oracle that returns the evaluation of the valid hypothesis on a given input. In the case of a noiseless oracle, that always returns the correct evaluation on a query, Generalized Binary Search (GBS) discovers the valid hypothesis in a number of queries within a polylogarithmic factor from the optimal [7, 8]. Our setup can be cast into the above framework in the context of a noisy oracle, whose evaluations may not necessarily be exact. GBS is known to yield arbitrarily suboptimal results in the presence of noise [9]. Though algorithms for restricted noise models exist (see, e.g., [22] and [9]), no algorithm with provable performance guarantees is known in the presence of an oracle with arbitrary noise. Unfortunately, none of the existing models apply to the noisy setting we encounter here."}, {"heading": "3. SYSTEM DESCRIPTION", "text": ""}, {"heading": "3.1 Problem Statement", "text": "We consider a recommender system, depicted in Figure 1, that provides a legitimate item recommendation service, but at the same time maliciously seeks to infer a private user attribute. The system has access to a dataset, provided by non-privacy-sensitive users, that contains item ratings as well as a categorical variable, which we refer to as the user type. The type is a private attribute such as gender, age, political affiliation, etc. A new user, who is privacy sensitive (i.e., her type is unknown) interacts with the system. The recommender system actively presents items for the user to rate, masquerading it as a way to improve recommendations in the cold-start setting. In this context, our goal is twofold:\n1. We wish to design a type classifier that discovers the type of the user based on her ratings. We seek to leverage the latent factor model prevalent in matrix-factorization, a technique successfully used for rating prediction by recommender systems. 2. We wish to address the problem of actively learning a user\u2019s type. We aim to design an item selection method, that determines the order in which items are shown to a user for her to rate. The best order finds the user\u2019s type as quickly as possible.\nFor the attack to be considered successful, the recommender system needs to obtain high confidence in the value of the\ninferred type, with a minimum number of questions posed to the user. As our classifier and item selection methods rely heavily on matrix factorization, we review this as well as the latent factor model that underlies it below."}, {"heading": "3.2 Data Model & Matrix Factorization", "text": "We use the following notation to describe the training dataset of the recommender. The dataset comprises of ratings to m items in set M \u2261 {1, . . . ,m} given by n users in set N \u2261 {1, . . . , n}. We denote by rij the rating of user i \u2208 N to item j \u2208M, and by E \u2282 N\u00d7M the set of user-item pairs (i, j), for which a rating rij is present in the dataset. Each user is characterized by a categorical type, which captures demographic information such as gender, occupation, income category, etc. Focusing on binary types, we denote by ti \u2208 T \u2261 {+1,\u22121} the type of user i \u2208M.\nWe assume that the ratings are generated from the standard generative model used in matrix factorization, augmented with type-dependent biases. More specifically, there exist latent factors ui \u2208 Rd, i \u2208 N , and vj \u2208 Rd, j \u2208 M (the user and item profiles, resp.) such that ratings are:\nrij = u T i vj + zjti + ij , (i, j) \u2208 E (1)\nwhere ij \u223c N(0, \u03c320) are independent Gaussian noise variables and zjt is a type bias, capturing the effect of a type on the item rating. Our model is thus parametrized by U = [uTi ]i\u2208N \u2208 Rn\u00d7d,V = [vTj ]j\u2208M \u2208 Rm\u00d7d, and Z = [zj,t]j\u2208M,t\u2208T \u2208 Rm\u00d7|T |. We further assume a prior on user and item profiles: for all i \u2208 N , j \u2208M,\nui \u223c N(0, \u03c32uI), and vj \u223c N(0, \u03c32vI), (2)\ni.e., profiles are sampled from independent zero-mean multivariate Gaussian priors.\nThe Gaussian priors (2) are used in many works on socalled Bayesian matrix factorization (see, e.g., [20, 24, 27]). Under (1) and (2), the maximum likelihood estimation of the model parameters reduces to the standard [15,16] minimization of the (non-convex) regularized error:1\nmin U,V,Z \u2211 (i,j)\u2208E (rij\u2212uTi vj\u2212zjti) 2+\u03bb \u2211 i\u2208N \u2016ui\u201622+\u00b5 \u2211 i\u2208M \u2016vj\u201622 (3)\nwith \u03bb = \u03c320 \u03c32u and \u00b5 = \u03c320 \u03c32v . Given a dataset of ratings rij , (i, j) \u2208 E and types ti, i \u2208 N , the parameters U, V, Z can be computed as local minima to (3) through standard methods [16], such as gradient descent or alternating minimization, while \u03bb and \u00b5 are computed through cross-validation."}, {"heading": "4. A FACTOR-BASED CLASSIFIER", "text": "We now turn our attention to the following classification problem. Suppose that the recommender system, with access to the dataset of ratings and types, has computed a set of item profiles V as well as a set of biases Z, e.g., by minimizing (3) through gradient descent. A new user arrives in the system and submits ratings for items in some set A \u2286 M, but does not submit her type. In order to bypass the user\u2019s attempt at privacy, we need to construct a classifier to infer the type of this new user.\n1Note that, as is common practice, to ensure that the profiles U, V obtained by (3) are invariant to a translation (shift) of the ratings, we do not regularize the category biases (or, equivalently, we assume no prior on Z).\nIn this section, we present a classifier that uses the item profiles and biases (i.e., the latent factors obtained through matrix factorization) to accomplish this task. We refer to this classifier as a Factor-Based Classifier (FBC). Crucially, FBC is consistent with the Bayesian model of matrix factorization presented in the previous section. In particular, it amounts to the maximum a-posteriori estimation of the type under the bi-linear noise model (1) and the priors (2)."}, {"heading": "4.1 Type Posterior", "text": "For A \u2282 M the set of items for which the user submits ratings, we introduce the following notation. We denote by rA \u2261 [rj ]j\u2208A \u2208 R|A| the vector of ratings provided by the user, by VA \u2261 [vTj ]j\u2208A \u2208 R|A|\u00d7d the matrix of profiles for items rated, and by zAt \u2261 [zjt]j\u2208A \u2208 R|A| the vector of type-t biases for items rated.\nAs in the previous section, we assume the new user has an unknown profile u \u2208 Rd and a type t \u2208 {\u22121,+1}, such that the ratings she submits follow (1), i.e.,\nrj = u T vj + zjt + j , j \u2208 A, (4)\nwhere j \u223c N(0, \u03c320). That is, conditioned on u and t, the ratings rA = [rj ]j\u2208A \u2208 R|A| given to items in A \u2282 [M ] are distributed as follows:\nPr(rA | u, t) = e\u2212\u2016rA\u2212VAu\u2212zAt\u2016 2 2/2\u03c3 2 0/ ( \u03c30 \u221a 2\u03c0 )|A|\n(5)\nwhere \u03c320 is the noise variance. Moreover, we assume as in the previous section that profile u follows a zero-mean Gaussian prior with covariance \u03c32uI, and that the type follows a uniform prior (i.e., each of the two types is equally likely), i.e.:\nPr(u, t) = 0.5e\u2212\u2016u\u2016 2 2/2\u03c3 2 u/ ( \u03c3u \u221a 2\u03c0 )d\n(6)"}, {"heading": "4.2 Classification", "text": "Under the above assumptions, it is natural to classify the incoming user using maximum a posteriori estimation of the type t. In particular, FBC amounts to\nt\u0302(rA) = arg maxt\u2208T Pr(t | rA). (7)\nUnder this notation, FBC can be determined as follows:\nTheorem 1. Under noise model (5) and prior (6), the FBC classifier is given by\nt\u0302(rA) = sgn(\u03b4 T AMAr\u0304A) (8)\nwhere r\u0304A \u2261 rA \u2212 zA++zA\u22122 , \u03b4A \u2261 zA+\u2212zA\u2212 2 , MA \u2261 I \u2212 VA\u03a3 \u22121 A V T A , and \u03a3A \u2261 \u03bbI + V TA VA, for \u03bb = \u03c320 \u03c32u .\nProof. Under model (5) and prior (6), conditioned on type t, the ratings rA a user gives items in a set A \u2286 [M ] are distributed according to:\nPr(rA | t) = e\n(rA\u2212zAt) T (VA\u03a3\u22121A V T A \u2212I)(rA\u2212zAt)\n2\u03c320\n(\u03c30 \u221a 2\u03c0)|A|(\u03c3u/\u03c30)d \u221a det(\u03a3A) (9)\nwhere \u03a3A \u2261 \u03bbI + V TA VA and \u03bb \u2261 \u03c320 \u03c32u . Hence, the posterior probability of the user\u2019s type is given by:\nPr(t | rA) = e(rA\u2212zAt)\nT (VA\u03a3\u22121A V T A\u2212I)(rA\u2212zAt)/2\u03c3 2 0\u2211\nt\u2032\u2208T\ne(rA\u2212zAt\u2032 ) T (VA\u03a3\u22121A V T A\u2212I)(rA\u2212zAt\u2032 )/2\u03c3 2 0\n(10)\nType t = +1 is thus most likely if (rA \u2212 zA+)T ( VA\u03a3 \u22121 A V T A \u2212 I ) (rA \u2212 zA+)\u2212\n(rA \u2212 zA\u2212)T ( VA\u03a3 \u22121 A V T A \u2212 I ) (rA \u2212 zA\u2212) \u2265 0\nand it is easy to verify that (8) follows.\nThere are several important observations to be made regarding FBC, as defined by Theorem 1.\nSet of Classifiers. We first note that FBC in fact defines a set of classifiers, each parametrized by set A \u2286 M: each such classifier t\u0302 : R|A| \u2192 {\u22121,+1} takes as input any possible set of ratings rA \u2208 R|A| as input. Note however that all classifiers are trained jointly from the ratings dataset: this \u201ctraining\u201d amounts to determining the item profiles V and the item biases Z through matrix factorization. With V and Z learned, when presented with ratings rA, the classifier can compute the vectors r\u0304A, \u03b4A and the matrix MA needed to determine the type. Indeed, the fact that training the classifier amounts to computing the latent factors/item profiles is consistent with the observation that FBC shares the same underlying Bayesian assumptions as matrix factorization.\nRelationship to LDA. Second, for a given set of items A, the classifier defined over ratings rA is a linear classifier. In particular, (8) defines a hyperplane in R|A| above which the user type is classified as +1 and below which the type is classified as \u22121. In fact, when restricted to a specific set of items A, (8) can be seen as classification through Linear Discriminant Analysis (LDA) [11]. More formally, the proof of Theorem 1 uses the fact that the ratings rA \u2208 R|A| are normally distributed with a mean that depends on the user type and a covariance MA = (I \u2212 VA\u03a3\u22121A VA), as defined in Theorem 1. As such, given a uniform prior on the types, the most likely type can indeed be determined through LDA, which yields a decision boundary precisely as in (8) (see, e.g., eq. (4.9) pp. 108 of [11]). Nevertheless, FBC significantly departs from classical LDA in that all classifiers across all sets A \u2286M are trained jointly. Type Priors and Multi-Category Types. A similar analysis to the one we discussed below allows us to extend our results to non-uniform type priors, as well as to the non-binary case. In the case of non-uniform type priors, the decision boundary of the classifier has an additional term, yielding:\n\u03b4TAMAr\u0304A + log \u03c0+ \u03c0\u2212 \u2265 0\nwhere \u03c0t the prior probability of each type. Finally, multicategory classification can be reduced to binary classification, by comparing all possible pairs of types through (8), and selecting the type that dominates all other types. Equivalently, the decision boundaries of each type tesselate R|A|, uniquely mapping each vector rA to the most likely type."}, {"heading": "5. LEARNING STRATEGIES", "text": "The second task in designing this threat is to find a user\u2019s type quickly. In what follows, we present two strategies for addressing this problem. The first is a passive strategy: the recommender presents items to the user in a predetermined order, computed off-line. The second is an active strategy: the recommender selects which item to present to the user next based on the answers she has given so far. Both strategies are extensively evaluated in Section 6.\nAlgorithm 1 FBC-Selection Input: Item profiles V , item biases Z, confidence \u03c4 1: A\u2190 \u2205 2: rA \u2190 \u2205 3: repeat 4: for all j \u2208 M \\ A do 5: Compute Lj through (11) 6: j\u2217 \u2190 arg min\nj\u2208M\\A Lj\n7: Query user to obtain rj\u2217 8: A\u2190 A \u222a {j\u2217}, rA \u2190 rA \u222a rj\u2217 9: until Pr(t\u0302(rA) | rA) > \u03c4\n5.1 MaxGap: A Passive Strategy A simple, passive method for presenting items to the user is to (a) sort items j \u2208M with respect to |\u03b4j |, the absolute value of the gap between the type biases, and (b) present items to the user in decreasing order. We call this strategy MaxGap: intuitively, this method identifies the most discriminative items in the dataset, and solicits responses to these items first. Clearly, MaxGap does not take into account (or adapt to) how the user rates the items presented so far. Despite this limitation, as we will see in Section 6, this simple strategy actually performs surprisingly well in many cases, especially when there exist many highly discriminative items. When this is not the case, however, an active strategy is needed, motivating our second method.\n5.2 FBC-Selection: An Active Strategy Our active method, FBC-Selection, is summarized in Algorithm 1. Let t\u0302 be the FBC classifier defined by (8). Given observed ratings rA \u2261 [rj ]j\u2208A \u2208 R|A|, for some A \u2282 M, we define the risk L(t\u0302(rA)) of the classifier to be 0 if the prediction is correct, and 1 otherwise. Conditioned on rA, the expected risk is E[L(t\u0302(rA)) | rA] = 1 \u2212 Pr(t\u0302(rA) | rA), i.e., it equals the 1 minus the confidence of the classifier, the posterior probability of the predicted type, conditioned on the observed ratings. Since, by (7), FBC selects the type that has the maximum posterior probability, the expected risk is at most (and the confidence at least) 0.5.\nFBC-Selection proceeds greedily, showing the item that minimizes the classifier\u2019s expected risk at each step. More specifically, let A be the set of items whose ratings have been observed so far. To select the next item to present to the user, the algorithm computes for each item j \u2208 M \\ A, the expected risk E[L(t\u0302(rA \u222a rj)) | rA] if rating rj is revealed:\u222b\nrj\u2208R (1\u2212 Pr(t\u0302(rA \u222a rj)) | rA \u222a rj) Pr(rA \u222a rj | rA)drj .\nThis expected risk depends on the distribution of the unseen rating rj conditioned on the ratings observed so far.\nUnder noise model (5) and prior (6), the expected risk for each item j can be computed in a closed form. In particular, let MA, r\u0304A, \u03b4A be as defined in Theorem 1. Then, the expected risk when revealing the rating of item j is proportional to the following quantity, derived in the appendix:\nLj = \u222b rj e \u2212 r\u0304TAj MAj r\u0304Aj +2|\u03b4TAj MAj r\u0304Aj |+\u03b4TAj MAj \u03b4Aj 2\u03c320 drj/ \u221a det\u03a3Aj (11)\nwhere Aj = A\u222a{j}. The integration above is w.r.t. rj , i.e., the predicted rating for item j. The outcome of the above integration can be computed in closed form in terms of the error function erf (i.e., no numerical integration is neces-\nsary). The formula can be found in the appendix. Each iteration amounts to computing the \u201cscores\u201dLj for each item j not selected so far, and picking the item with the lowest score (corresponding to minimum expected risk). Once the item is presented to the user, the user rates it, adding one more rating to the set of observed ratings. The process is repeated until the confidence of the classifier (or, equivalently, the expected risk) reaches a satisfactory level.\n5.3 IncFBC: An Efficient Implementation FBC-Selection requires the computation of the scores Lj after each interaction with the user. Each such calculation involves computing the determinant det(\u03a3Aj ), as well as the matrix MAj = (I \u2212 VAj\u03a3 \u22121 Aj V TAj ), both of which appear in (11). Though having a closed form formula for (11) avoids the need for integration, computing each of these matrices directly from their definition involves a considerable computational cost. In particular, the cost of computing \u03a3A = \u03bbI + V T A VA is O(d\n2|A|). Computing \u03a3\u22121A and det(\u03a3Aj ) have a cost O(d 3) multiplications using, e.g., LU-decomposition, which can be dropped to O(d2.807) using Strassen\u2019s algorithm for multiplication [6]. Finally, the computation of MA requires O(|A|\u00d7d2 + |A|2\u00d7d) multiplications. As a result, the overall complexity of computing Lj directly is O(|A|\u00d7d2+|A|2\u00d7d+d2.807). However, the performance of these computations can be significantly reduced by constructing these matrices incrementally: MAj , \u03a3 \u22121 Aj and det(\u03a3Aj ) can be computed efficiently from MA, \u03a3 \u22121 A , and det(\u03a3A), exploiting the fact that \u03a3Aj = \u03a3A + vjv T j , i.e., it results from \u03a3i through a rank-one update. We discuss this below.\nIncremental computation of det(\u03a3Aj ). The determinant can be computed incrementally using only O(d2) multiplications through the Matrix Determinant Lemma [10], namely:\ndet(\u03a3Aj ) = (1 + v T j \u03a3Avj) det(\u03a3A). (12)\nIncremental computation of \u03a3\u22121Aj . The inverse of a rankone update of a matrix can be computed through the ShermanMorisson formula [26], which gives:\n\u03a3\u22121Aj = \u03a3 \u22121 A \u2212 \u03a3 \u22121 A vjv T j \u03a3 \u22121 A /(1 + v T j \u03a3 \u22121 A vj), (13)\nand again reduces the number of multiplications to O(d2).\nIncremental computation of MAj . Finally, using (13), we can also reduce the cost of computing MAj , as:\nMAj =\n[ MA+ \u03c6\u03c6T\n1+vT j \u03a3 \u22121 A vj\n\u2212\u03be\n\u2212\u03beT 1\u2212vTj \u03be\n] (14)\nwhere \u03be = VA(\u03a3 \u22121 Aj vj) and \u03c6 = VA(\u03a3 \u22121 A vj), which reduces the computation cost to O(|A|2 + d2) multiplications.\nIn conclusion, using the above adaptive operations reduces the cost of computing Lj by one order of magnitude to O(|A|2 + d2), which is optimal (as MA is an |A| \u00d7 |A| matrix, and \u03a3A is d \u00d7 d). The rank-one adaptations yield such performance without sophisticated matrix inversion or multiplication algorithms, such as Strassen\u2019s algorithm. The we refer to resulting algorithm as IncFBC; we empirically compare the two implementations in Section 6.\nAlgorithm 2 PointEst Active Learning Input: Item profiles V , item biases Z, classifier C, confidence \u03c4 1: A\u2190 \u2205, rA \u2190 \u2205 2: repeat 3: t\u0302\u2190 arg maxt\u2208T PrC(t | rA) 4: u\u0302\u2190 (\u03bbI + V TA VA)\n\u22121V TA (rA \u2212 zAt\u0302) 5: for all j \u2208 M \\ A do 6: r\u0302j \u2190 u\u0302T vj + zjt\u0302 7: Lj \u2190 mint\u2208T PrC(t | rA \u222a r\u0302j) 8: j\u2217 \u2190 arg minj Lj 9: Query user to obtain rj\u2217 10: A\u2190 A \u222a {j\u2217}, rA \u2190 rA \u222a rj\u2217 11: until 1\u2212 Lj\u2217 > \u03c4"}, {"heading": "5.4 Selection Through Point Estimation", "text": "An alternative method for selection can be constructed by replacing the exact estimation of the expected risk with a \u201cpoint estimate\u201d (see also [27]). In fact, such a selection method can be easily combined with an arbitrary classifier that operates on user-provided ratings as input. This makes such an approach especially useful when the expected risk is hard to estimate in a closed form. We therefore outline this method below, noting however that several problems arise when the risk is computed through such a point estimation.\nWe describe the method for a general classifier C, also summarized in Algorithm 2. Given a set of ratings rA over a set A \u2286M, the classifier C returns a probability PrC(t | rA), for each type t \u2208 T . This is the probability that the user\u2019s type is t, conditioned on the observed ratings rA. Given a set of observed ratings rA, we can estimate the type of the user using the classifier C though maximum likelihood a-posteriori estimation, as t\u0302(rA) = arg maxt\u2208T PrC(t | rA). Using this estimate, we can further estimate the most likely profile u\u0302 \u2208 Rd through ridge regression [11] over the observed ratings rA and the corresponding profiles VA (see Algorithm 2 for details). Using the estimated profile u\u0302 and the estimated type t\u0302, we can predict the rating of every item j \u2208 M \\ A as r\u0302j = u\u0302T vj + zjt\u0302, and subsequently estimate the expected risk if the rating for item j is revealed as mint\u2208T PrC(t | rA \u222a r\u0302j). We refer to this as a \u201cpoint estimate\u201d, as it replaces the integration that the expected risk corresponds to with the value at a single point, namely, the predicted rating r\u0302j .\nUsing such estimates, selection can proceed as follows. Given the set of observed ratings A, we can estimate the risk of the classifier C for every item j inM\\A through the above estimation process, and pick the item with the minimum estimated risk. The rating of this item is subsequently revealed, and new estimates t\u0302 and u\u0302 can thusly be obtained, repeating the process until a desired confidence is reached.\nClearly, point estimation avoids computing the expected risk exactly, which can be advantageous when the corresponding expectation under a given classifier C can only be computed by numerical integration. This is not the case for FBC, as we have seen, but this can be the only tractable option for an arbitrary classifier. Unfortunately, this estimation can be quite inaccurate in practice, consequently leading to poor performance in selections; we observe such a performance degradation in our evaluations (Section 6). Put differently, a point estimate of the risk takes into account what the predicted rating of an item j is in expectation, and how this rating can potentially affect the risk; however, it does not account for how variable this prediction\nis. A highly variable prediction might have a very different expected risk; the exact computation of the expectation does take this into account whereas point estimation does not."}, {"heading": "6. EVALUATION", "text": "In this section we evaluate the performance of our methods using real datasets. We begin by describing the datasets and experiments, then perform a comparative analysis of both passive and active methods."}, {"heading": "6.1 Experimental Setup", "text": "Datasets. We evaluate our method using three datasets: Movielens, Flixster [12], and Politics-and-TV (PTV) [25]. The Movielens dataset includes users\u2019 ratings for movies alongside with the users\u2019 gender and age. For simplicity, we categorize the age group of users as young adults (ages 18\u201335), or adults (ages 36\u201365). Flixster is a similar movie ratings dataset, and contains user gender information. PTV includes ratings by US users on 50 different TV-shows, along with each user\u2019s gender and political affiliation (Democrat or Republican). We preprocessed Movielens and Flixster to consider only users with at least 20 ratings, and items that were rated by at least 20 users. Since PTV includes only 50 TV-shows, we preprocessed the data to ensure that each user has at least 10 ratings. Table 1 summarizes the datasets used for evaluation. For each user type, the table shows the ratio between the number of users of one type versus the other type (as labeled in the table). Figure 2 shows the cumulative distribution function (CDF) of the number of ratings per user across the three preprocessed datasets. We see that for the Movielens and Flixster datasets, there are many users with hundreds of items rated in their profile.\nEvaluation Method. In our setting, the recommender system infers user attributes from a set of strategically selected items. To understand the effectiveness of FBC compared\nto other classification methods in an adversarial setting, we perform the following evaluation. We first split the dataset into a training set (e.g., users that are willing to disclose the private attribute) and evaluation set (e.g., users that are privacy-sensitive), and train different classifiers on the training set \u2013 e.g., in the case of FBC we learn the item profiles and biases. Then, for each user in the evaluation set, we incrementally select items for the user to rate. In the passive methods, the selection of next item does not depend on the previous ratings provided by the user, whereas in the active methods it does.\nAfter the user rates an item, we use the classifier to infer the private type. For any user, since we only have the rating information for the set of movies that she has rated, we limit the selection process to this set. Users may have rated different number of movies, for instance, roughly 50% of the users of Movielens have rated less than 100 movies out of 3000 (see Figure 2). Therefore, we limit the number of questions asked to 100 for Movielens and Flixster and all 50 for PTV. Unless specified, all evaluations of FBC were done using the efficient incremental implementation IncFBC.\nEvaluation Metrics. We evaluate classification performance through the area under the curve (AUC) metric, and prediction performance through the root mean squared error (RMSE). If a recommender system uses our method to maliciously learn user features, it is important that such a mechanism for strategic solicitation of ratings has a minimal impact on the quality of recommendations, otherwise the user may detect its hidden agenda. We measure the quality of recommendations by holding out an evaluation set of 10 items for each user. After every 10 questions (solicited ratings) we predict the ratings on the evaluation set by applying ridge regression using the provided ratings and item profiles to learn a user profile. We predict the ratings on the evaluation set and compute the RMSE over all users.\nParameter settings. We split each dataset into training and testing and perform MF with 10-fold cross validation. We learn the item latent factors required by FBC using the training set, with type biases for age, gender and political affiliation as applicable to the three datasets. For MF, we use 20 iterations of stochastic gradient descent [16] to minimize (3), using the same regularization parameter for users and movies. Through 10-fold cross validation we determined the optimal dimension to be d = 20, and the optimal regularization parameter to be 0.1, for each of the biases. We also compute the optimal \u03bb used in the classifier (8) through 10-fold cross validation to maximize the AUC, resulting in \u03bb = 100 for gender and \u03bb = 200 for age for the Movielens dataset, \u03bb = 10 for gender and political views for the PTV dataset, and \u03bb = 200 for gender for the Flixster dataset."}, {"heading": "6.2 Passive Learning", "text": "Figure 4 shows the AUC and RMSE obtained using MaxGap, and two other passive methods \u2013 Random and Entropy. For reference, we also show the performance of our active method IncFBC. Random selection is a natural baseline as users may rate items in any arbitrary order. The second method, Entropy, presents items to the user in descending order of their rating entropy, i.e., start with items that have polarized ratings. This method was shown to be efficient in a cold-start setting in [23] as it can quickly build user profiles in a matrix factorization based recommender system.\nAUC. Figure 4a shows that MaxGap performs significantly better than the other passive methods on both Movielens and PTV datasets. For the first 10 questions, which are critical when considering the need for quick inference, it is the best passive method on all datasets. Interestingly, on Movielens-Gender and PTV-Politics, MaxGap performs very similar to the adaptive, more complex IncFBC. As a result of the greedy nature of MaxGap, we expect it to perform well on datasets that have items with large biases. To better understand MaxGap\u2019s performance, Figure 3 shows the top 20% of the cumulative distributions of the type biases over the set of items in the different datasets. The plot clearly shows that the items in PTV-Gender have the lowest biases, resulting in the poorest performance of MaxGap. Conversely, in Movielens-Gender, the biases are the largest, thus MaxGap performs well, in par with the adaptive IncFBC. In PTV-Politics the biases are not overly high, but since most users rate all items, a few discriminating items are sufficient to enable MaxGap to perform well. This observation is supported by the findings of [5] that identified 5 TV-shows that immediately reveal a user\u2019s political views.\nRMSE. Figure 4b plots the RMSE over increasing number of rated items, for MaxGap, Random, and Entropy, along with IncFBC. Even though IncFBC and MaxGap are designed to explore a specific attribute of the user\u2019s profile,\nthey perform very well. Their RMSE is very close to that of Random and Entropy, with the MaxGap visibly worse only in one case, the PTV-Gender dataset. Since IncFBC and MaxBias focus on quickly learning a singe attribute of the user\u2019s profile, it is expected that they perform worse than the other methods, that aim to explore attributes more broadly. However, the figures show that IncFBC and MaxGap are almost identical to the other methods, and MaxGap only perform worse in the PTV-Gender dataset. Moreover, in all datasets, IncFBC performs close to a random selection, indicating that IncFBC does not incur significant impact on the RMSE relative to an arbitrary order in which a user may rate items. Finally, IncFBC has an RMSE similar to the entropy method, which is designed to improve the RMSE in a cold-start setting.\nThese results show that a malicious recommender system that uses IncFBC to infer a private attribute of its users can also use the solicited ratings to provide recommendations, making it difficult for users to detect the hidden agenda."}, {"heading": "6.3 Active Learning", "text": "We compare our selection method to the logistic and multinomial classifiers by adapting them to an active learning setting. These classifiers were the top performing among those studied in previous work [29] for gender prediction. Following [29], we train both of these classifiers over rating vectors padded with zeros: an item not rated by a user is marked with a rating value of 0. In order to use logistic and multinomial classifiers in an active learning setting we use the point-estimate (PointEst) method as described in Section 5.4 (see Algorithm 2). For the remainder of this section we refer to PointEst with a logistic and multinomial classifiers as Logistic and Multinomial, respectively.\nAUC. Figure 5a plots the AUC of classification for a given number of question using PointEst and IncFBC selection, for all datasets. PointEst selector enables us to compare FBC with the other classifiers for which we do not have a\nclosed-form solution for selection. In all datasets, the plots show that IncFBC significantly outperforms both logistic and multinomial within a few questions, and reaches an improvement in AUC of 10\u201330% in the Movielens and Flixster datasets. PointEst with the other classifiers is unable to achieve a good classification accuracy.\nTo put this in perspective, Table 2 shows the performance of these classifiers, and that of a non-linear classifier SVM with RBF kernel, using all user ratings. Note that this table considers all ratings performed by all users in each dataset, whereas the plots in Figure 5a show the average AUC computed over the users that have rated the indicated number of questions. Logistic and in some cases multinomial classifiers perform significantly better than FBC, when classifying over the entire dataset. This shows that although any of these classifiers could be used for a static attack [29], FBC is better suited to adaptive attacks with fewer available ratings. For instance, using IncFBC with just 20 movies per user we obtain a classification accuracy that is reasonably close to that obtained by static inference techniques which use the complete dataset.\nRMSE. For completeness, Figure 5b provides the RMSE using the different active methods, showing that IncFBC has a lower RMSE on almost all datasets.\nRunning Time. Finally, we seek to quantify the improvement in execution time obtained by the incremental computations of IncFBC. We ran both FBC and IncFBC on a commodity server with a RAM size of 128GB and a CPU\nspeed of 2.6GHz. Figure 6 shows the average time per movie selection for both FBC and IncFBC for increasing number of questions (movies presented to the user). The error bars depict the 95% confidence interval surrounding the mean. The plot shows that when the number of questions is small the time per question is relatively constant, and increases with the number of questions. As discussed in Section 5.3, when the number of questions is smaller than the dimension of the factor vectors (in our case d = 20), the complexity of the efficient algorithm is dominated by d. In the first few questions FBC is slightly faster than IncFBC as a result of the efficient implementation of inversion for small matrices. However, as the matrix becomes larger, the size of the matrix dominates the complexity and the incremental computations performed in IncFBC are significantly faster than FBC, reaching a speedup of 30%."}, {"heading": "7. CONCLUSION AND FUTURE WORK", "text": "We presented a new attack that a recommender system could use to pursue a hidden agenda of inferring private attributes for users that do not voluntarily disclose them. Our solution, that includes a mechanism to select which question to ask a user, as well as a classifier, is efficient both in terms of the number of questions asked, and the runtime to generate each question. Moving beyond binary attributes to\nmulti-category attributes, using the relationship of our classifier to LDA, is an interesting open question. Exploring the attack from the user\u2019s perspective, to better advise users on ways to identify and potentially mitigate such attacks is also an important future direction."}, {"heading": "8. REFERENCES", "text": "[1] N. Awad and M. Krishnan. The personalization\nprivacy paradox: An empirical evaluation of information transparency and the willingness to be profiled online for personalization. In MIS Quarterly, March 2006.\n[2] S. Bhagat, I. Rozenbaum, and G. Cormode. Applying link-based classification to label blogs. In WebKDD, 2007.\n[3] B. Bi, M. Shokouhi, M. Kosinski, and T. Graepel. Inferring the demographics of search users. WWW, 2013.\n[4] J. A. Calandrino, A. Kilzer, A. Narayanan, E. W. Felten, and V. Shmatikov. \u201cYou Might Also Like:\u201d Privacy Risks of Collaborative Filtering. IEEE SSP, 2011.\n[5] F. Calmon and N. Fawaz. Privacy against statistical inference. In Allerton, 2012.\n[6] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to algorithms. MIT press, 2001.\n[7] S. Dasgupta. Analysis of a greedy active learning strategy. In NIPS, 2005.\n[8] D. Golovin and A. Krause. Adaptive submodularity: A new approach to active learning and stochastic optimization. In COLT, 2010.\n[9] D. Golovin, A. Krause, and D. Ray. Near-optimal Bayesian active learning with noisy observations. In NIPS, 2010.\n[10] D. A. Harville. Matrix Algebra From a Statistician\u2019s Perspective. Springer-Verlag, 1997.\n[11] T. Hastie, R. Tibshirani, and J. H. Friedman. The elements of statistical learning. Springer-Verlag, 2001.\n[12] M. Jamali and M. Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In RecSys, 2010.\n[13] B. Knijnenburg and A. Kobsa. Making decisions about privacy: Information disclosure in context-aware recommender systems. ACM Trans on Intelligent Interactive Systems, 2013.\n[14] A. Kobsa, B. Knijnenburg, and B. Livshits. Let\u2019s do it at my place instead? attitudinal and behavioral study of privacy in client-side personalization. In CHI, 2014.\n[15] Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In KDD, 2008.\n[16] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 2009.\n[17] M. Kosinski, D. Stillwell, and T. Graepel. Private traits and attributes are predictable from digital records of human behavior. PNAS, 2013.\n[18] A. Levi, O. Mokryn, C. Diot, and N. Taft. Finding a needle in a haystack of reviews: Cold-start context-based recommender system. In ACM RecSys, 2012.\n[19] A. Mislove, B. Viswanath, K. P. Gummadi, and P. Druschel. You are who you know: Inferring user profiles in Online Social Networks. In WSDM, 2010.\n[20] S. Nakajima and M. Sugiyama. Implicit regularization in variational bayesian matrix factorization. ICML, 2010.\n[21] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. IEEE SSP, 2008.\n[22] R. Nowak. The geometry of generalized binary search. Transactions on Information Theory, 2012.\n[23] A. M. Rashid, I. Albert, D. Cosley, S. K. Lam, S. M. Mcnee, J. A. Konstan, and J. Riedl. Getting to know you: Learning new user preferences in recommender systems. In IUI, 2002.\n[24] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In NIPS, 2008.\n[25] S. Salamatian, A. Zhang, F. du Pin Calmon, S. Bhamidipati, N. Fawaz, B. Kveton, P. Oliveira, and N. Taft. How to hide the elephant-or the donkey-in the room: Practical privacy against statistical inference for large data. In GlobalSIP, 2013.\n[26] J. Sherman and W. J. Morrison. Adjustment of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix. Annals of Mathematical Statistics, 20:621, 1949.\n[27] J. Silva and L. Carin. Active learning for online bayesian matrix factorization. KDD, 2012.\n[28] D. J. Sutherland, B. Po\u0301czos, and J. Schneider. Active learning and search on low-rank matrices. In KDD, 2013.\n[29] U. Weinsberg, S. Bhagat, S. Ioannidis, and N. Taft. Blurme: Inferring and obfuscating user gender based on ratings. In ACM RecSys, 2012.\nAPPENDIX Derivation of Equation (11) . For Aj = A\u222a{j} and tc the binary complement of t, the expected risk E[L(t\u0302(rAj )) | rA], if the rating for movie j is revealed is\u222b\nrj\u2208R Pr(t\u0302c(rAj ) | rAj ) Pr(rAj | rA)drj\n= \u222b rj\u2208R Pr(b\u0304(rAj ) | rAj ) Pr(rAj ) Pr(rA) drj\n(9),(10) = C\n\u222b rj\u2208R e yT t\u0302c(rAj ) ( VAj\u03a3 \u22121 Aj V TAj \u2212I ) y b\u0302c(rAj ) /2\u03c320\ndrj\u221a det(\u03a3Aj )\nwhere yt\u0302(rAj ) = rAj \u2212 zAj t\u0302c(rAj ), \u03a3Aj = \u03bbI + V T Aj VAj , and C a term that does not depend on j. The expected risk is thus proportional to:\nLj =\n\u222b rj\u2208R e \u2212(rAj\u2212zAjt\u0302c(rAj ) )TMAj (rAj\u2212zAjt\u0302c(rAj ) )/2\u03c321\ndrj\u221a |\u03a3Aj |\nwhere |\u03a3Aj | = det(\u03a3Aj ) MAj = I\u2212VAj\u03a3 \u22121 Aj V TAj and t\u0302 c(rAj ) the complement of prediction under rAj . Under Theorem 1,\nas t\u0302 is given by (8), Lj simplifies to (11). Closed Form of (11). Let \u03be = VA(\u03a3 \u22121 Aj vj), \u03c6 = VA(\u03a3 \u22121 A vj), \u00b51 = MA + \u03c6\u03c6T\n1+vTj \u03a3 \u22121 A vj\nand \u00b52 = 1\u2212 vTj \u03a3\u22121Aj vj .\nThen, from (14) we get that:\nr\u0304TAjMAj r\u0304Aj + 2|\u03b4 T AjMAj r\u0304Aj |+ \u03b4 T AjMAj \u03b4Aj =\n\u00b52r\u0304 2 j \u2212 2r\u0304TA\u03ber\u0304j + |(\u03b4j\u00b52 \u2212 \u03b4TA\u03be)r\u0304j + \u03b4TA\u00b51r\u0304A \u2212 \u03b4j\u03beT r\u0304A|\n+ r\u0304TA\u00b51r\u0304A + \u03b4 T AjMAj \u03b4Aj\nFor simplicity of exposition, let \u03b11 = \u00b52/\u03c3 2 1 , \u03b12 = \u22122rTA\u03be/\u03c321 , \u03b13 = (zj\u00b52 \u2212 zTA\u03be)/\u03c321 , \u03b14 = (zTA\u00b51 \u2212 zj\u03be T )rA/\u03c3 2 1 , \u03b15 = (r T A\u00b51rA + z T Aj MAj zAj )/\u03c3 2 1 If \u03b13 > 0, substituting these in the risk, we have,\nLj = 1\u221a |\u03a3Aj |\n(\u222b \u2212\u03b14 \u03b13\nrj=\u2212\u221e e\u2212(\u03b11r 2 j+(\u03b12\u2212\u03b13)rj\u2212\u03b14+\u03b15)/2drj\n+ \u222b \u221e rj=\u2212\n\u03b14 \u03b13\ne\u2212(\u03b11r 2 j+(\u03b12+\u03b13)rj+\u03b14+\u03b15)/2drj ) Letting x = \u221a \u03b11rj +\n\u03b12\u2212\u03b13 2 \u221a \u03b11\nand y = \u221a \u03b11rj +\n\u03b12+\u03b13 2 \u221a \u03b11 , and\nsubstituting dx = dy = \u221a \u03b11drj we can rewrite the above as,\nLj = 1\u221a\n\u03b11|\u03a3Aj |\n( e (\u03b12\u2212\u03b13) 2 4\u03b11 \u2212\u03b15+\u03b14 2 h ( \u03b12 \u2212 \u03b13 2 \u221a \u03b11 \u2212 \u03b14 \u221a \u03b11 \u03b13 ) +\ne\n(\u03b12+\u03b13) 2\n4\u03b11 \u2212\u03b15\u2212\u03b14\n2 h ( \u2212\u03b12 + \u03b13\n2 \u221a \u03b11\n+ \u03b14 \u221a \u03b11\n\u03b13 )) where h(x) = \u222b x \u2212\u221e e \u2212x2/2dx, which can be expressed in terms of the error function (erf). A similar derivation applies if \u03b13 \u2264 0."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Recommender systems leverage user demographic informa-<lb>tion, such as age, gender, etc., to personalize recommenda-<lb>tions and better place their targeted ads. Oftentimes, users<lb>do not volunteer this information due to privacy concerns, or<lb>due to a lack of initiative in filling out their online profiles.<lb>We illustrate a new threat in which a recommender learns<lb>private attributes of users who do not voluntarily disclose<lb>them. We design both passive and active attacks that so-<lb>licit ratings for strategically selected items, and could thus<lb>be used by a recommender system to pursue this hidden<lb>agenda. Our methods are based on a novel usage of Bayesian<lb>matrix factorization in an active learning setting. Evalua-<lb>tions on multiple datasets illustrate that such attacks are<lb>indeed feasible and use significantly fewer rated items than<lb>static inference methods. Importantly, they succeed without<lb>sacrificing the quality of recommendations to users.", "creator": "LaTeX with hyperref package"}}}