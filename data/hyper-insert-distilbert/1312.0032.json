{"id": "1312.0032", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2013", "title": "Top-k Query Answering in Datalog+/- Ontologies under Subjective Reports (Technical Report)", "abstract": "the primary use theory of individual preferences in query letter answering, both in traditional databases and in ontology - based data access, has additionally recently received much historical attention, due more to its many real - end world storage applications. in challenging this paper, we tackle the problem of top - k relational query answering answering scores in datalog + / - ontologies ideally subject to inspecting the underlying querying user'associated s preferences structures and a collection of ( subjective ) reports of other databases users. here, each each report type consists freely of scores designed for a bucket list weighted of features, its author'its s preferences among the best features, hence as many well as other information. theses pieces of complementary information collected of every report are then combined, assembled along with examine the querying machine user'possible s preferences patterns and check his / her trust level into revealing each report, to securely rank the query results. furthermore we all present two alternative uniquely such rankings, along with complementary algorithms looking for top - k ( atomic ) query answering under these internal rankings. we also just show generally that, under suitable assumptions, these algorithms nonetheless run nearly in sufficiently polynomial \u2010 time in computing the optimal data storage complexity. we finally present more general information reports, which too are associated loosely with sets up of atoms assigned rather than any single atoms.", "histories": [["v1", "Fri, 29 Nov 2013 22:06:09 GMT  (160kb,D)", "http://arxiv.org/abs/1312.0032v1", "arXiv admin note: text overlap witharXiv:1106.3767by other authors"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1106.3767by other authors", "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["thomas lukasiewicz", "maria vanina martinez", "cristian molinaro", "livia predoiu", "gerardo i simari"], "accepted": false, "id": "1312.0032"}, "pdf": {"name": "1312.0032.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Thomas Lukasiewicz", "Maria Vanina Martinez", "Cristian Molinaro", "Livia Predoiu", "Gerardo I. Simari", "G.I. Simari"], "emails": ["gerardo.simari}@cs.ox.ac.uk", "cmolinaro@dimes.unical.it"], "sections": [{"heading": "1 Introduction", "text": "The use of preferences in query answering, both in traditional databases and in ontologybased data access, has recently received much attention due to its many real-world applications. In particular, in recent times, there has been a huge change in the way data is created and consumed, and users have largely moved to the Social Web, a system of platforms used to socially interact by sharing data and collaborating on tasks.\nIn this paper, we tackle the problem of preference-based query answering in Datalog+/\u2013 ontologies assuming that the user must rely on subjective reports to get a complete picture and make a decision. This kind of situation arises all the time on the Web; for instance, when searching for a hotel, users provide some basic information and receive a list of answers to choose from, each associated with a set of subjective reports (often called reviews) written by other users to tell everyone about their experience. The main problem with this setup, however, is that users are often overwhelmed and frustrated, because they cannot decide which reviews to focus on and which ones to ignore, since it is likely that, for instance, a very negative (or positive) review may have been produced on the basis of a feature that is completely irrelevant to the querying user. ar X\niv :1\n31 2.\n00 32\nv1 [\ncs .A\nI] 2\n9 N\nov 2\n01 3\nWe study a formalization of this process and its incorporation into preference-based query answering in Datalog+/\u2013 ontologies, proposing the use of trust and relevance measures to select the best reports to focus on, given the user\u2019s initial preferences, as well as novel ranking algorithms to obtain a user-tailored answer. The main contributions of this paper can be briefly summarized as follows.\n\u2013 We present an approach to preference-based top-k query answering in Datalog+/\u2013 ontologies, given a collection of subjective reports. Here, each report contains scores for a list of features, its author\u2019s preferences among the features, as well as additional information. Theses pieces of information of every report are then aggregated, along with the querying user\u2019s trust into each report, to a ranking of the query results relative to the preferences of the querying user. \u2013 We present a basic approach to ranking the query results, where each atom is associated with the average of the scores of all reports, and every report is ranked with the average of the scores of each feature, weighted by the report\u2019s trust values and the relevance of the feature and of the report for the querying user. \u2013 We then present an alternative approach to ranking the query results, where we first select the most relevant reports for the querying user, adjust the scores by the trust measure, and compute a single score for each atom by combining the scores computed in the previous step, weighted by the relevance of the features. \u2013 We present algorithms for preference-based top-k (atomic) query answering in Datalog+/\u2013 ontologies under both rankings. We also prove that, under suitable assumptions, the two algorithms run in polynomial time in the data complexity. \u2013 Finally, we also propose and discuss a more general form of reports, which are associated with sets of atoms rather than single atoms.\nThe rest of this paper is organized as follows. In Section 2, we provide some preliminaries on Datalog+/\u2013 and the used preference models. Section 3 then defines subjective reports, along with their trust measures and their relevance. In Section 4, we introduce the two rankings of query results, along with top-k query answering algorithms under these rankings and data tractability results. Section 5 then presents more general subjective reports. In Section 6, we discuss related work. Finally, the concluding Section 7 summarizes the main results of this paper and gives an outlook on future research."}, {"heading": "2 Preliminaries", "text": "First, we briefly recall some basics on Datalog+/\u2013 [7], namely, on relational databases and (Boolean) conjunctive queries ((B)CQs) (along with tuple- and equality-generating dependencies (TGDs and EGDs, respectively) and negative constraints), the chase procedure, and ontologies in Datalog+/\u2013. We also define the used preference models.\nDatabases and Queries. We assume (i) an infinite universe of (data) constants \u2206 (which constitute the \u201cnormal\u201d domain of a database), (ii) an infinite set of (labeled) nulls \u2206N (used as \u201cfresh\u201d Skolem terms, which are placeholders for unknown values, and can thus be seen as variables), and (iii) an infinite set of variables V (used in queries, dependencies, and constraints). Different constants represent different values (unique\nname assumption), while different nulls may represent the same value. We assume a lexicographic order on \u2206\u222a\u2206N , with every symbol in \u2206N following all symbols in \u2206. We denote by X sequences of variablesX1, . . . , Xk with k> 0. We assume a relational schemaR, which is a finite set of predicate symbols (or simply predicates). A term t is a constant, null, or variable. An atomic formula (or atom) a has the form P (t1, ..., tn), where P is an n-ary predicate, and t1, ..., tn are terms. We say that a is ground iff every ti belongs to \u2206.\nA database (instance) D for a relational schema R is a (possibly infinite) set of atoms with predicates fromR and arguments from\u2206. A conjunctive query (CQ) overR has the formQ(X) = \u2203Y\u03a6(X,Y), where\u03a6(X,Y) is a conjunction of atoms (possibly equalities, but not inequalities) with the variables X and Y, and possibly constants, but no nulls. A CQ is atomic iff \u03a6(X,Y) is a single atom and Y= \u2205 (i.e., there are no existentially quantified variables). A Boolean CQ (BCQ) over R is a CQ of the form Q(), i.e., all variables are existentially quantified, often written as the set of all its atoms without quantifiers, when there is no danger of confusion. Answers to CQs and BCQs are defined via homomorphisms, which are mappings \u00b5 : \u2206 \u222a \u2206N \u222a V \u2192 \u2206 \u222a\u2206N \u222a V such that (i) c\u2208\u2206 implies \u00b5(c)= c, (ii) c\u2208\u2206N implies \u00b5(c)\u2208\u2206\u222a\u2206N , and (iii) \u00b5 is naturally extended to atoms, sets of atoms, and conjunctions of atoms. The set of all answers to a CQQ(X)=\u2203Y\u03a6(X,Y) overD, denotedQ(D), is the set of all tuples t over\u2206 for which there exists a homomorphism \u00b5 : X\u222aY\u2192\u2206\u222a\u2206N such that \u00b5(\u03a6(X,Y))\u2286D and \u00b5(X)= t. The answer to a BCQ Q() over a database D is Yes, denoted D |=Q, iff Q(D) 6= \u2205.\nGiven a relational schema R, a tuple-generating dependency (TGD) \u03c3 is a firstorder formula of the form \u2200X\u2200Y\u03a6(X,Y)\u2192 \u2203Z\u03a8(X,Z), where \u03a6(X,Y) and \u03a8(X, Z) are conjunctions of atoms overR (without nulls), called the body and the head of \u03c3, denoted body(\u03c3) and head(\u03c3), respectively. Such \u03c3 is satisfied in a database D for R iff, whenever there exists a homomorphism h that maps the atoms of \u03a6(X,Y) to atoms of D, there exists an extension h\u2032 of h that maps the atoms of \u03a8(X,Z) to atoms of D. All sets of TGDs are finite here. Since TGDs can be reduced to TGDs with only single atoms in their heads, in the sequel, every TGD has w.l.o.g. a single atom in its head. A TGD \u03c3 is guarded iff it contains an atom in its body that contains all universally quantified variables of \u03c3. The leftmost such atom is the guard atom (or guard) of \u03c3. A TGD \u03c3 is linear iff it contains only a single atom in its body. As set of TGDs is guarded (resp., linear) iff all its TGDs are guarded (resp., linear).\nQuery answering under TGDs, i.e., the evaluation of CQs and BCQs on databases under a set of TGDs is defined as follows. For a database D for R, and a set of TGDs \u03a3 onR, the set of models of D and \u03a3, denoted mods(D,\u03a3), is the set of all (possibly infinite) databases B such that (i) D\u2286B and (ii) every \u03c3 \u2208\u03a3 is satisfied in B. The set of answers for a CQ Q to D and \u03a3, denoted ans(Q,D,\u03a3) (or, for KB =(D,\u03a3), ans(Q,KB)), is the set of all tuples t such that t \u2208 Q(B) for all B \u2208mods(D,\u03a3). The answer for a BCQQ toD and\u03a3 is Yes, denotedD\u222a\u03a3 |=Q, iff ans(Q,D,\u03a3) 6= \u2205. Note that query answering under general TGDs is undecidable [2], even when the schema and TGDs are fixed [6]. Decidability and tractability in the data complexity of query answering for the guarded case follows from a bounded tree-width property.\nA negative constraint (or simply constraint) \u03b3 is a first-order formula of the form \u2200X\u03a6(X)\u2192\u22a5, where \u03a6(X) (called the body of \u03b3) is a conjunction of atoms over R (without nulls). Under the standard semantics of query answering of BCQs in Datalog+/\u2013 with TGDs, adding negative constraints is computationally easy, as for each constraint \u2200X\u03a6(X)\u2192\u22a5, we only have to check that the BCQ \u2203X\u03a6(X) evaluates to false in D under \u03a3; if one of these checks fails, then the answer to the original BCQ Q is true, otherwise the constraints can simply be ignored when answering the BCQ Q.\nAn equality-generating dependency (EGD) \u03c3 is a first-order formula of the form \u2200X\u03a6(X) \u2192Xi=Xj , where \u03a6(X), called the body of \u03c3 and denoted body(\u03c3), is a conjunction of atoms over R (without nulls), and Xi and Xj are variables from X. Such \u03c3 is satisfied in a databaseD forR iff, whenever there is a homomorphism h such that h(\u03a6(X,Y))\u2286D, it holds that h(Xi)=h(Xj). Adding EGDs over databases with TGDs along with negative constraints does not increase the complexity of BCQ query answering as long as they are non-conflicting [7]. Intuitively, this ensures that, if the chase (see below) fails (due to strong violations of EGDs), then it already fails on the database, and if it does not fail, then whenever \u201cnew\u201d atoms are created in the chase by the application of the EGD chase rule, atoms that are logically equivalent to the new ones are guaranteed to be generated also in the absence of the EGDs, guaranteeing that EGDs do not influence the chase with respect to query answering.\nWe usually omit the universal quantifiers in TGDs, negative constraints, and EGDs, and we implicitly assume that all sets of dependencies and/or constraints are finite.\nThe Chase. The chase was first introduced to enable checking implication of dependencies, and later also for checking query containment. By \u201cchase\u201d, we refer both to the chase procedure and to its output. The TGD chase works on a database via so-called TGD chase rules (see [7] for an extended chase with also EGD chase rules).\nTGD Chase Rule. Let D be a database, and \u03c3 a TGD of the form \u03a6(X,Y) \u2192 \u2203Z\u03a8(X, Z). Then, \u03c3 is applicable to D iff there exists a homomorphism h that maps the atoms of \u03a6(X,Y) to atoms of D. Let \u03c3 be applicable to D, and h1 be a homomorphism that extends h as follows: for each Xi \u2208 X, h1(Xi) = h(Xi); for each Zj \u2208 Z, h1(Zj) = zj , where zj is a \u201cfresh\u201d null, i.e., zj \u2208 \u2206N , zj does not occur in D, and zj lexicographically follows all other nulls already introduced. The application of \u03c3 on D adds to D the atom h1(\u03a8(X,Z)) if not already in D.\nThe chase algorithm for a database D and a set of TGDs \u03a3 consists of an exhaustive application of the TGD chase rule in a breadth-first (level-saturating) fashion, which outputs a (possibly infinite) chase for D and \u03a3. Formally, the chase of level up to 0 of D relative to \u03a3, denoted chase0(D,\u03a3), is defined as D, assigning to every atom in D the (derivation) level 0. For every k> 1, the chase of level up to k of D relative to \u03a3, denoted chasek(D,\u03a3), is constructed as follows: let I1, . . . , In be all possible images of bodies of TGDs in \u03a3 relative to some homomorphism such that (i) I1, . . . , In\u2286 chasek\u22121(D,\u03a3) and (ii) the highest level of an atom in every Ii is k\u2212 1; then, perform every corresponding TGD application on chasek\u22121(D,\u03a3), choosing the applied TGDs and homomorphisms in a (fixed) linear and lexicographic order, respectively, and assigning to every new atom the (derivation) level k. The chase of D relative to\u03a3, denoted chase(D,\u03a3), is defined as the limit of chasek(D,\u03a3) for k\u2192\u221e.\nThe (possibly infinite) chase relative to TGDs is a universal model, i.e., there exists a homomorphism from chase(D,\u03a3) onto everyB \u2208mods(D,\u03a3) [7]. This implies that BCQs Q over D and \u03a3 can be evaluated on the chase for D and \u03a3, i.e., D\u222a\u03a3 |= Q is equivalent to chase(D,\u03a3) |= Q. For guarded TGDs\u03a3, such BCQsQ can be evaluated on an initial fragment of chase(D,\u03a3) of constant depth k \u00b7 |Q|, which is possible in polynomial time in the data complexity.\nDatalog+/\u2013 Ontologies. A Datalog+/\u2013 ontology KB =(D,\u03a3), where\u03a3=\u03a3T \u222a\u03a3E\u222a \u03a3NC, consists of a database D, a set of TGDs \u03a3T , a set of non-conflicting EGDs \u03a3E , and a set of negative constraints \u03a3NC. We say that KB is guarded (resp., linear) iff \u03a3T is guarded (resp., linear). The following example illustrates a simple Datalog+/\u2013 ontology, which is used in the sequel as a running example.\nExample 1. Consider the following simple ontology KB = (D,\u03a3), where:\n\u03a3 = {\u03c31 : hotel(H)\u2192 accom(H), \u03c32 : apartment(A)\u2192 accom(A), \u03c33 : bb(B)\u2192 accom(B), \u03c34 : apthotel(A)\u2192 hotel(A), \u03c35 : hostel(H)\u2192 \u2203B bed(B,H), \u03c36 : hotel(H)\u2192 \u2203R room(R,H), \u03c37 : bb(B)\u2192 \u2203R room(R,B)}\nand D= {hotel(h1), hotel(h2), locatedIn(h1, oxford), locatedIn(h2, oxfordCenter), hostel(hs1), bb(bb1), apartment(a1), apthotel(a2), locatedIn(a2, oxfordCenter)}.\nThis ontology models a very simple accommodation booking domain, which could be used as the underlying model in an online system. Accommodations can be either hotels, bed and breakfasts, hostels, apartments, or aparthotel. The database D provides some instances for each kind of accommodation, as well as some location facts.\nPreference Models. We now briefly recall some basic concepts regarding the representation of preferences. We assume the following sets, giving rise to the logical language used for this purpose: \u2206Pref \u2286 \u2206 is a finite set of constants, RPref \u2286 R is finite set of predicates, and VPref \u2286 V is an infinite sets of variables. These sets give rise to a corresponding Herbrand base consisting of all possible ground atoms that can be formed, which we denote with HPref, while H is the Herbrand base for the ontology. Clearly, we have HPref \u2286 H, meaning that preference relations are defined over a subset of the possible ground atoms.\nA preference relation over set S is any binary relation \u2286 S \u00d7 S. Here, we are interested in strict partial orders (SPOs), which are irreflexive and transitive relations\u2014 we consider these to be the minimal requirements for a useful preference relation. One possible way of specifying such a relation is the preference formula framework of [9]. We use SPOs(S) to denote the set of all possible strict partial orders over a set S.\nFinally, the rank of an element in a preference relation is defined inductively as follows: (i) rank(a, ) = 1 iff there is no b such that b a; and (ii) rank(a, ) = k+1 iff rank(a, ) = 1 after eliminating from all elements of rank at most k."}, {"heading": "3 Subjective Reports", "text": "Let KB be a Datalog+/\u2013 ontology, a = p(c1, ..., cm) be a ground atom such that KB |= a, and F = (f1, ..., fn) be a tuple of features associated with the predicate p, each of which has a domain dom(fi) = [0, 1]\u222a{\u2212}. We sometimes slightly abuse notation and use F to also denote the set of features {f1, ..., fn}.\nDefinition 1. A report for a ground atom a is a triple (E, P , I), whereE \u2208 dom(f1)\u00d7 ...\u00d7dom(fn), P is an SPO over the elements of F , and I is a set of pairs (key, value).\nIntuitively, reports are evaluations of an entity of interest (atom a) provided by observers. In a report (E, P , I), E specifies a \u201cscore\u201d for each feature, P indicates the relative importance of the features to the report\u2019s observer, and I (called information register) contains general information about the report itself and who provided it. Reports will be analyzed by a user, who has his own strict partial order, denoted PU , over the set of features. The following is a simple example involving hotel ratings.\nExample 2. Consider again the accommodation domain from Example 1, and let the features for predicate hotel be F = (location, cleanliness, price, breakfast, internet); in the following, we abbreviate these features as loc, cl, pri, br, and net, respectively.\nAn example of a report for hotel(h1) is r1 =(\u30081, 0, 0.4, 0.1, 1\u3009, P1 , I1), where P1 is given by the graph in Fig. 1 (left side); PU (the user\u2019s SPO) is shown in the same figure (right side). Finally, let I1 be a register with fields age, nationality, and type of traveler, with data I1.age = 34, I1.nationality = Italian, and I1.type = Business.\nThe set of all reports available is denoted with Reports. In the following, we use Reports(a) to denote the set of all reports that are associated with a ground atom a. Given a tuple of features F , we use SPOs(F) to denote the set of all SPOs over F ."}, {"heading": "3.1 Trust Measures over Reports", "text": "A user analyzing a set of reports may decide that certain opinions within a given report may be more trustworthy than others. For instance, returning to our running example, the score given for the location feature of hotel(h1) might be considered more trustworthy than the ones given for price or breakfast, e.g., because the report declared the\nformer to be among the most preferred features, while the latter are among the least preferred ones, cf. Figure 1 (left). Another example could be a user that is generally untrustworthy of reports on feature cleanliness, because he has learned that people are in general much more critical than he is when it comes to evaluating that aspect of a hotel, or of reports on feature price by business travelers because they do not use their own money to pay. Formally, we have the following definition of trust measure.\nDefinition 2. A trust measure is any function \u03c4 : Reports\u2192 [0, 1]n.\nNote that trust measures do not depend on the user\u2019s own preferences over F (in PU ); rather, for each report (E, P , I), they give a measure of trust to each of the n scores in E depending on P and I . The following shows an example of a trust measure.\nExample 3. Consider again our running example and suppose that the user defines a trust measure \u03c4 , which assigns trust values to a report r = (E, P , I) as follows:\n\u03c4(r) =\n{ 0.25 \u00b7 ( 2\u2212(rank(f1, P )\u22121), ..., 2\u2212(rank(fn, P )\u22121) ) if I.nationality 6= Italian;(\n2\u2212(rank(f1, P )\u22121), ..., 2\u2212(rank(fn, P )\u22121) )\notherwise.\nFor r1 from Example 2 and the SPO in Fig. 1 (left side), we get 2\u2212(rank(loc, P1 )\u22121) = 2\u2212(rank(net, P1 )\u22121) = 1, 2\u2212(rank(cl, P1 )\u22121) = 0.5, and 2\u2212(rank(pri, P1 )\u22121) = 0.25."}, {"heading": "3.2 Relevance of Reports", "text": "The other aspect of importance that a user must consider when analyzing reports is how relevant they are to his/her own preferences. For instance, a report given by someone who has preferences that are completely opposite to those of the user should be considered less relevant than one given by someone whose preferences only differ in a trivial aspect. This is inherently different from the trust measure described above, since trust is computed without taking into account the preference relation given by the user issuing the query. Formally, we define relevance measures as follows.\nDefinition 3. A relevance measure is any function \u03c1 : Reports\u00d7 SPOs(F)\u2192 [0, 1].\nThus, a relevance measure takes as input a report (E, P , I) and an SPO P \u2032 and gives a measure of how relevant the report is relative to P \u2032 ; this is determined on the basis of P and P \u2032 , and can also take I into account.\nExample 4. Consider again the running example, and suppose that the user assigns relevance to a report r = (E, P , I) according to the function\n\u03c1(r, PU ) = 2 \u2212\n\u2211 fi\u2208F |rank(fi, P )\u2212rank(fi, PU )|.\nFrom Fig. 1, e.g., we have that \u03c1(r1, PU ) = 2\u22121\u2217(0+1+1+0+1) = 0.125. Alternatively, a relevance measure comparing the SPO P of a report (E, P , I) with the user\u2019s SPO PU (thus, in this case, information in I is ignored by the relevance measure) might be defined as follows. The relevance measure checks to what extent the\ntwo SPOs agree on the relative importance of the features in F . Formally, let P1 and P2 be SPOs over F . We define a measure of similarity of P1 and P2 as follows:\nsim(P1, P2) =\n\u2211 16i<j6n sim(fi, fj , P1, P2)\nn(n\u2212 1)/2 ,\nwhere\nsim(fi, fj , P1, P2) =  1 if (fi, fj) \u2208 P1 \u2229 P2 or (fj , fi) \u2208 P1 \u2229 P2 1 if (fi, fj) 6\u2208 P1 \u222a P2 and (fj , fi) 6\u2208 P1 \u222a P2 0.5 if ((fi, fj) \u2208 P1\u2206P2 and (fj , fi) 6\u2208 P1 \u222a P2) or\n((fj , fi) \u2208 P1\u2206P2 and (fi, fj) 6\u2208 P1 \u222a P2) 0 if (fi, fj) \u2208 P1 \u222a P2 and (fj , fi) \u2208 P1 \u222a P2 .\nHere, \u2206 is used to denote the symmetric difference (i.e., A\u2206B = A \u222aB \u2212A \u2229B). In the definition of sim(fi, fj , P1, P2),\n\u2013 the first condition refers to the case where P1 and P2 are expressing the same order between fi and fj , \u2013 the second condition refers to the case where both P1 and P2 are not expressing any order between fi and fj , \u2013 the third condition refers to the case where one of P1 and P2 is expressing an order between fi and fj and the other is not expressing any order, \u2013 the last condition refers to the case where P1 and P2 are expressing opposite orders between fi and fj .\nClearly, sim(P1, P2) is 1 when P1 and P2 agree on everything, and 0 when P1 and P2 agree on nothing. Finally, we define a relevance measure by \u03c1((E, P , I), P \u2032) = sim( P , P \u2032) for every report (E, P , I) \u2208 Reports and SPO P \u2032\u2208 SPOs(F)."}, {"heading": "4 Query Answering based on Subjective Reports", "text": "To produce a ranking based on the basic components presented in Section 3, we must first develop a way to combine them in a principled manner. More specifically, the problem that we address is the following. The user is given a Datalog+/\u2013 ontology KB and has an atomic query Q(X) of interest. The user also supplies an SPO PU over the set of features F . The answers to an atomic query Q(X) = p(X) over KB in atom form are defined as {p(t) | t \u2208 ans(Q(X),KB)}; we still use ans(Q(X),KB) to denote the set of answers in atom form. Recall that in our setting, each ground atom b such that KB |= b is associated with a (possibly empty) set of reports. As we consider atomic queries, then each ground atom a \u2208 ans(Q(X),KB) is an atom entailed by KB and thus it is associated with a set of reports Reports(a). Furthermore, each report r \u2208 Reports(a) is associated with a trust score \u03c4(r). We want to rank the ground atoms in Ans(Q(X),KB); that is, we want to obtain a set {\u3008ai, scorei\u3009 | ai \u2208 ans(Q(X),KB)} where scorei for ground atom ai takes into account:\n\u2013 the set of reports Reports(ai) associated with ai; \u2013 the trust score \u03c4(r) associated with each report r \u2208 Reports(ai); and \u2013 the SPO PU over F provided by the user issuing the query."}, {"heading": "4.1 A Basic Approach", "text": "A first approach to solving this problem is Algorithm RepRank-Basic in Fig. 2. A score for each atom is computed as the average of the scores of the reports associated with the atom, where the score of a report r = (E, P , I) is computed as follows:\n\u2013 we first compute the average of the scores E[i] weighted by the trust value for E[i] and a value measuring how important feature fi is for the user issuing the query (this value is given by rank(fi, PU )); \u2013 then, we multiply the value computed in the previous step by \u03c1(r, PU ), which gives a measure of how relevant r is w.r.t. PU .\nThe following is an example of how Algorithm RepRank-Basic works.\nExample 5. Consider again the setup from the running example, where we have the Datalog+/\u2013 ontology from Example 1, the set Reports of the reports depicted in Fig. 3, the SPO PU from Fig. 1 (right), the trust measure \u03c4 defined in Example 3, and the relevance measure \u03c1 introduced in Example 4. Finally, let Q(X) = hotel(X).\nAlgorithm RepRank-Basic iterates through the set of answers (in atom form) to the query, which in this case consists of {hotel(h1), hotel(h2)}. For atom hotel(h1), the algorithm iterates through the set of corresponding reports, which is Reports(hotel(h1)) = {r1, r2, r3}, and maintains the accumulated score after processing each report. For r1, the score is computed as (cf. line 6):\n0.125 \u2217 1 5 \u2217 ( 1 \u2217 1 1 + 0 \u2217 0.5 1 + 0.4 \u2217 0.25 2 + 0.1 \u2217 0.25 3 + 1 \u2217 1 2 ) = 0.03895 .\nThe score for hotel(h1) after processing the three reports is approximately 0.05746. Analogously, assuming Reports(hotel(h2))= {r4, r5, r6}, the score for hotel(h2) is approximately 0.0589. Therefore, the top-2 answer to Q is \u3008hotel(h2), hotel(h1)\u3009.\nThe following result states the time complexity of Algorithm RepRank-Basic. As long as both query answering and the computation of the trust and relevance measures can be done in polynomial time, RepRank-Basic can also be done in polynomial time.\nProposition 1. The worst-case time complexity of Algorithm RepRank-Basic is O(m\u2217 log m+(n+ | PU |)+m\u2217Reportsmax \u2217(f\u03c4 +f\u03c1+n)+fans(Q(X),KB)), where m = |ans(Q(X),KB)|, Reportsmax = max{|Reports(a)| : a \u2208 ans(Q(X),KB)}, f\u03c4 (resp. f\u03c1) is the worst-case time complexity of \u03c4 (resp. \u03c1), and fans(Q(X),KB) is the data complexity of computing ans(Q(X),KB).\nIn the next section, we explore an alternative approach to applying the trust and relevance measures to top-k query answering."}, {"heading": "4.2 A Different Approach to using Trust and Relevance", "text": "A more complex approach consists of using the trust and relevance scores provided by the respective measures in a more fine-grained manner. One way of doing this is via the following steps (more details on each of them are given shortly):\n1. Keep only those reports that are most relevant to the user issuing the query, that is, those reports that are relevant enough to PU according to a relevance measure \u03c1; 2. consider the most relevant reports obtained in the previous step and use the trust measure given by the user to produce scores adjusted by the trust measure; and 3. for each atom, compute a single score by combining the scores computed in the previous step with PU .\nThe first step can simply be carried out by checking, for each report r, if \u03c1(r, PU ) is above a certain given threshold. One way of doing the second step is described in Algorithm SummarizeReports (Fig. 4), which takes a trust measure \u03c4 , a set of reports Reports (for a certain atom), and a function collFunc. The algorithm processes each report in the input sets by building a histogram of average (trust-adjusted) reported values for each of the n features with ten possible \u201cbuckets\u201d (of course, this can be easily generalized to any number of buckets); for each report, the algorithm applies the trust measure to update each feature\u2019s histogram. Once all of the reports are processed, the last step is to collapse the histograms into a single value\u2014this is done by applying the collFunc function, which could simply be defined as the computation of a weighted average for each feature. This single value is finally used to produce the output, which is a tuple of n scores. The following example illustrates how SummarizeReports works.\nExample 6. Let us adopt again the setup from Example 5. Suppose we want to keep only those reports for which the relevance score is above 0.1 (as per the first step of our more complex approach). Recall that the set of answers to Q is {hotel(h1), hotel(h2)} and there are six associated reports. Among them, we keep only reports r1, r2, r4, and r5. Algorithm SummarizeReports will have Reports = {r1, r2} when called for hotel(h1). The histograms built during this call are as follows:\n\u2013 loc: value 0.95 in bucket [0.9, 1]; \u2013 cl: value 1 in bucket [0.5, 0.6] and value 0.3 in bucket [0.9, 1]; \u2013 pri: value 0.8 in bucket [0.2, 0.3) and value 0.2 in bucket [0.5, 0.6); \u2013 br: value 0.1 in bucket [0.2, 0.3) and value 0.5 in bucket [0.5, 0.6); and \u2013 net: value 0.6 in bucket [0.6, 0.7) and value 1 in bucket [0.9, 1].\nAssuming that function collFunc disregards the values in the bucket corresponding to the lowest trust value (if more than one bucket is non-empty), and takes the average of the rest, we have the following result tuple as the output of SummarizeReports: (0.95, 0.3, 0.2, 0.5, 1). Analogously, we have tuple (0.85, 0.1, 0.1, 0.4, 1) for tuple hotel(h2) after calling SummarizeReports with Reports = {r4, r5}.\nThe following proposition states the time complexity of Algorithm SummarizeReports. As long as the trust measure and the collFunc function can be computed in polynomial time, Algorithm SummarizeReports is polynomial time too.\nProposition 2. The worst-case time complexity of Algorithm SummarizeReports is O(|Reports| \u2217 (f\u03c4 + n) + n \u2217 fcollFunc), where f\u03c4 (resp. fcollFunc) is the worst-case time complexity of \u03c4 (resp. collFunc).\nThe following example explores a few different ways in which function collFunc used in Algorithm SummarizeReports might be defined.\nExample 7. One way of computing collFunc is shown in Example 6. There can be other reasonable ways of collapsing the histogram for a feature into a single value. E.g., collFunc might compute the average across all buckets ignoring the trust measure so that no distinction is made among buckets, i.e., collFunc(hists[i]) = \u221110 b=1 hists[i](b)\n10 . Alternatively, the trust measure might be taken into account by giving a weight wb to each bucket b (e.g., the weights might be set in such a way that buckets corresponding to higher trust scores have a higher weight, that is, weighti < weightj for i < j). In this case, the histogram might be collapsed as follows collFunc(hists[i]) =\u221110\nb=1 wb\u2217hists[i](b) 10 . We may also want to apply the above strategies but ignoring the first k buckets (for which the trust score is lower). Function collFunc can also be extended so that the number of elements associated with a bucket is taken into account.\nThus, the second step discussed above gives n scores (adjusted by the trust measure) for each ground atom. Recall that the third (and last) step of the approach adopted in this section is to compute a score for each atom by combining the scores computed in the previous step with PU . One simple way of doing this is to compute the weighted average of such scores where the weight of the i-th score is the inverse of the rank of feature fi in PU .\nAlgorithm RepRank-Hist (Figure 5) is the complete algorithm that combines the three steps discussed thus far. The following continues the running example to show the result of applying this algorithm.\nExample 8. Let us adopt once again the setup from Example 5, but this time applying Algorithm RepRank-Hist. Suppose collFunc is the one discussed in Example 6 and thus Algorithm SummarizeReports returns the scores (0.95, 0.3, 0.2, 0.5, 1) for hotel(h1) and the scores (0.85, 0.1, 0.1, 0.4, 1) for hotel(h2). Algorithm RepRankHist computes a score for each atom by performing a weighted average of the scores in these tuples, which results in:\n\u3008hotel(h1), 2.0166\u3009 and \u3008hotel(h2), 1.6333\u3009.\nTherefore, the top-2 answer to query Q is \u3008hotel(h1), hotel(h2)\u3009.\nNote that the results from Examples 5 and 8 differ in the way they order the two tuples; this is due to the way in which relevance and trust scores are used in each algorithm\u2014the more fine-grained approach adopted by Algorithm RepRank-Hist allows it to selectively use both kinds of values to generate a more informed result.\nProposition 3. The worst-case time complexity of Algorithm RepRank-Hist is: O(m\u2217log m+(n+| PU |)+m\u2217(Reportsmax\u2217f\u03c1+fsum+n)+fans(Q(X),KB)), where\nm = |ans(Q(X),KB)|, Reportsmax = max{|Reports(a)| : a \u2208 ans(Q(X),KB)}, f\u03c1 is the worst-case time complexity of \u03c1, fsum is the worst-case time complexity of Algorithm SummarizeReports as per Proposition 2, and fans(Q(X),KB) is the data complexity of computing ans(Q(X),KB).\nAs a corollary to Propositions 1 and 3, we have the following result.\nTheorem 1. If the input ontology belongs to the guarded fragment of Datalog+/\u2013, then Algorithms RepRank-Basic and RepRank-Hist run in polynomial time in the data complexity.\nThus far, we have considered atomic queries. As each ground atom a such that KB |= a is associated with a set of reports and every ground atom b in ans(Q(X),KB) is such that KB |= b, then reports can be associated with query answers in a natural way. We now introduce a class of queries more general than the class of atomic queries for which the same property holds. A simple query is a conjunctive query Q(X) = \u2203Y\u03a6(X,Y) where \u03a6(X,Y) contains exactly one atom of the form p(X), called distinguished atom (i.e., an atom whose variables are the query\u2019s free variables). For instance, Q(X) = hotel(X)\u2227 locatedIn(X, oxford) is a simple query where hotel(X) is the distinguished atom. The answers to a simple query Q(X) over KB in atom form are defined as {p(t) | t \u2208 ans(Q(X),KB)} where the distinguished atom is of the form p(X); we still use ans(Q(X),KB) to denote the set of answers in atom form. Clearly, for each atom a in ans(Q(X),KB), it is the case that KB |= a."}, {"heading": "5 Towards more General Reports", "text": "In the previous section we considered the setting where reports are associated with ground atoms a such that KB |= a. This setup is limited, since it does not allow to express the fact that certain reports may apply to whole sets of atoms\u2014this is necessary to model certain kinds of opinions often found in reviews, such as \u201caccommodations in Oxford are expensive\u201d. We now generalize the framework presented in Sections 3 and 4 to contemplate this kind of reports.\nDefinition 4. A generalized report (g-report, for short) is a pair gr = (r,Q(X)), where r is a report and Q(X) is a simple query, called the descriptor of gr.\nWe denote with g-Reports the universe of g-reports. Intuitively, given an ontology KB , a g-report (r,Q(X)) is used to associate report r with every atom a in ans(Q(X),KB)\u2014 recall that KB |= a and thus general reports allow us to assign a report to a set of atoms entailed by KB .\nClearly, a report for a ground atom a as defined in Definition 1 is a special case of a g-report in which the only answer to the descriptor is a.\nExample 9. Consider our running example from the accommodations domain and suppose we want to associate a certain report r with all accommodations in the city of Oxford. This can be expressed with a g-report (r,Q(X)) where Q(X) = accom(X) \u2227 locatedIn(X, oxford) with descriptor accom(X).\nIntuitively, a g-report gr = (r,Q(X)) is a report associated with a set of atoms, i.e., the set of atoms in ans(Q(X),KB)). A simple way of handling this generalization would be to associate report r with every atom in this set. Note that, as in the nongeneralized case, it might be the case that two or more g-reports assign two distinct reports to the same ground atom. E.g., we may have a g-report (r,Q(X)), whereQ(X) =\naccom(X) \u2227 locatedIn(X, oxford), expressing that r applies to all accommodations in Oxford, and another g-report (r\u2032, Q\u2032(X)), where Q\u2032(X) = accom(X) \u2227 hotel(X), expressing that r\u2032 applies to all accommodations that are hotels. In our running example, we would simply associate both r and r\u2032 to accom(hi), accom(h2), and accom(a2).\nIn the approach just described, the reports coming from different g-reports are treated in the same way\u2014they all have the same impact on the common atoms. Another possibility is to determine when a g-report is in some sense more specific than another and take such a relationship into account (e.g., more specific g-reports should a greater impact when computing the ranking over atoms). We consider this kind of scenario in the following section.\nLeveraging the Structural Properties of Ontologies\nWe now study two kinds of structure that can be leveraged from knowledge contained in the ontology. The first is based on the notion of hierarchies, which are useful in capturing the influence of reports in \u201cis-a\u201d type relationships. As an example, given a query requesting a ranking over hotels in Oxfordshire, a report for all hotels in Oxford should have a higher impact on the calculation of the ranking than a report for all accommodations in the UK\u2014in particular, the latter might be ignored altogether since it is too general. The second kind of structure is based on identifying subset relationships between the atoms associated with the descriptors in g-reports. For instance, a report for all hotels in Oxford is more general than a report for all hotels in Oxford city center, since the former is a superset of the latter.\nIn the following, we define a partial order among reports based on these notions. We begin by defining hierarchical TGDs.\nDefinition 5. A set of linear TGDs \u03a3T is said to be hierarchical iff for every p(X)\u2192 \u2203Yq(X,Y) \u2208 \u03a3T we have that features(p) \u2286 features(q) and there does not exist database D over R and TGD in \u03a3T of the form p\u2032(X) \u2192 \u2203Yr(X,Y) such that p(X) and p\u2032(X) share ground instances relative to D.\nIn the rest of this section, we assume that all ontologies contain a (possibly empty) subset of hierarchical TGDs. Furthermore, given ontology KB = (D,\u03a3) where \u03a3H \u2286 \u03a3 is a set of hierarchical TGDs, and two ground atoms a, b, we say that a is-a b iff chase({a}, \u03a3H) |= b. For instance, in Example 1, set {\u03c31, \u03c32, \u03c33, \u03c34} \u2286 \u03a3 is a hierarchical set of TGDs (assuming that the conditions over the features hold).\nGiven tuples of features F and F \u2032 such that F \u2286 F \u2032 and vectors E and E\u2032 over the domains of F and F \u2032, respectively, we say that E\u2032 is a particularization of E, denoted E\u2032 = part(E) iff E\u2032[f ] = E[f ] if f \u2208 F \u2229 F \u2032 and E\u2032[f ] = \u2212 otherwise.\nDefinition 6. Let KB = (D,\u03a3) be a Datalog+/\u2013 ontology, a be a ground atom such that KB |= a, and gr = (r,Q(X)) be a g-report with r = (E, P , I). If there exists a ground atom b \u2208 Ans(Q(X),KB) such that a is-a b then we say that g-report gr\u2032 = ((E\u2032, P , I), a), with E\u2032 = part(E), is a specialization of gr for a.\nClearly, a g-report is always a specialization of itself for every atom in the answers to its descriptor.\nExample 10. Let F1 be the set of features for predicate hotel presented in Example 2, and let F2 = \u3008loc, cl, pri, br, net, kfac\u3009 be the set of features for predicate apthotel, where kfac denotes \u201ckitchen facilities\u201d.\nLet gr=(r1, Q(X)) be a g-report, where r1 is the report from Figure 3 andQ(X) = hotel(X) \u2227 locatedIn(X, oxford). If we consider a = apthotel(a2) and b = hotel(a2), clearly we have that b \u2208 Ans(Q(X),KB) and a is-a b. Therefore, a specialization of gr for a is gr\u2032 = ((E\u2032, P1 , I1), a), where E\u2032 = \u30081, 0, 0.4, 0.1, 1,\u2212\u3009.\nDefinition 7. Given g-reports gr1 = (r1, Q1(X1)) and gr2 = (r2, Q2(X2)), we say that gr1 is more general than gr2, denoted gr2 v gr1, iff either (i) Ans(Q2(X2),KB) \u2286 Ans(Q1(X1),KB); or (ii) for each a \u2208 Ans(Q2(X2),KB) there exists b \u2208 Ans(Q1(X1), KB) such that a is-a b. If gr1 v gr2 and gr2 v gr1, we say that gr1 and gr2 are equivalent, denoted gr1 \u2261 gr2.\nExample 11. Consider the g-reports in Figure 6 and the database in the running example with the addition of atoms hotel(h3) and locatedIn(h3, cambridge). We then have: \u2013 gr1 v gr4 since {hotel(h2), hotel(a2)} \u2286 {hotel(h1), hotel(h2), hotel(a2)}; \u2013 gr1 v gr3 since for atom apthotel(a2) (the only answer for the descriptor in gr3) there exists atom hotel(a2) in the answer to descriptor in gr1 and apthotel(a2) is-a hotel(a2); and \u2013 gr4 is incomparable to all other reports, since neither condition from Definition 7 is satisfied.\nThe \u201cmore general than\u201d relationship between g-reports is useful for defining a partial order for the set of reports associated with a given ground atom. This partial order can be defined as follows: gr1 \u223c gr2 iff gr1 \u2261 gr2 and gr1 gr2 iff gr1 v gr2. Here, a \u223c b denotes the equivalence between a and b.\nDefinition 8. A weighting function for g-reports is any function \u03c9 : g-Reports\u2192 [0, 1] such that: (i) if gr1 gr2 then \u03c9(gr1) > \u03c9(gr2); and (i) if gr1 \u223c gr2 then \u03c9(gr1) = \u03c9(gr2).\nFor example, one possible weighting function is defined as \u03c9(gr) = 2\u2212rank(gr, )+1."}, {"heading": "6 Related Work", "text": "The study of preferences has been carried out in many disciplines; in computer science, the developments that are most relevant to our work is in the incorporation of preferences into query answering mechanisms. To date (and to our knowledge), the state\nof the art in this respect is centered around relational databases and, recently, in ontological languages for the Semantic Web [13]. The seminal work in preference-based query answering was that of [12], in which the authors extend the SQL language to incorporate user preferences. The preference formula formalism was introduced in [9] as a way to embed a user\u2019s preferences into SQL. An important development in this line of research is the well-known skyline operator, which was first introduced in [3]. A recent survey of preference-based query answering formalisms is provided in [15]. Studies of preferences related to our approach have also been done in classical logic programming [10,11] as well as answer set programming frameworks [4].\nThe present work can be considered as a further development of the PrefDatalog+/\u2013 framework presented in [13], where we develop algorithms to answer skyline queries, and their generalization to k-rank queries, over classical Datalog+/\u2013 ontologies. The main difference between PrefDatalog+/\u2013 and the work presented here is that PrefDatalog+/\u2013 assumes that a model of the user\u2019s preferences are given at the time the query is issued. On the other hand, we make no such assumption here; instead, we assume that the user only provides some very basic information regarding their preferences over certain features, and that they have access to a set of reports provided by other users in the past. In a sense, this approach is akin to building an ad hoc model on the fly at query time and using it to provide a ranked list of results.\nFinally, this work is closely connected to the study and use of provenance in information systems and, in particular, the Semantic Web and social media [14,1]. Provenance information describes the history of data and information in its life cycle. Research in provenance distinguishes between data and workflow provenance [5]. The former explores the data flow within (typically, database) applications in a fine-grained way, while the latter is coarse-grained and does not consider the flow of data within the involved applications. In this work, we propose a new kind of provenance that is closely related to data provenance, but does not fit into the why, how, and where provenance framework typically considered in data provenance research [8]. We take into account (in a fine-grained way) where evaluations and reports within a social media system are coming from (i.e., information about who has issued the report and what his/her preferences were) and use this information to allow users to make informed and provenance-based decisions. To our knowledge, this is the first study of a direct application of provenance of reports of this kind found in online reviews to query answering."}, {"heading": "7 Summary and Outlook", "text": "In this paper, we have studied the problem of preference-based query answering in Datalog+/\u2013 ontologies under the assumption that the user\u2019s preferences are informed by a set of subjective reports representing opinions of others\u2014such reports model the kind of information found, e.g., in online reviews of products, places, and services. We have first introduced a basic approach, in which reports are assigned to ground atoms. We have proposed two ranking algorithms using trust and relevance functions in order to model the different impact that reports should have on the user-specific ranking by taking into account the differences and similarities between the user\u2019s preferences over basic features and those of the person writing the report, as well as the person\u2019s self-\nreported characteristics (such as age, gender, etc.). As a generalization, we have then extended reports to apply to entire sets of atoms so that they can model more general opinions. Apart from the naive approach of simply replicating the general report for each individual atom that it pertains to, we have proposed a way to use the information in the knowledge base to assign greater weights to more specific reports.\nMuch work remains to be done in this line of research, for instance, exploring conditions over the trust and relevance functions to allow pruning of reports, applying more sophisticated techniques to judging the impact of generalized reports, and the application of existing techniques to allow the obtention of reports from the actual information available in reviews on the Web. We also plan to implement our algorithms and evaluate them over synthetic and real-world data. Finally, another topic for future research is to formally investigate the relationship between well-known data provenance frameworks and the preference-based provenance framework presented in this paper. Acknowledgments. This work was supported by the UK EPSRC grant EP/J008346/1 (\u201cPrOQAW\u201d), an EU (FP7/2007-2013) Marie-Curie Intra-European Fellowship, the ERC grant 246858 (\u201cDIADEM\u201d), and a Yahoo! Research Fellowship."}], "references": [{"title": "Provenance Data in Social Media", "author": ["G. Barbier", "Z. Feng", "P. Gundecha", "H. Liu"], "venue": "Morgan and Claypool", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "The implication problem for data dependencies", "author": ["C. Beeri", "M.Y. Vardi"], "venue": "Proc. of ICALP.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1981}, {"title": "The skyline operator", "author": ["S. B\u00f6rzs\u00f6nyi", "D. Kossmann", "K. Stocker"], "venue": "Proc. of ICDE.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Preferences, contexts and answer sets", "author": ["G. Brewka"], "venue": "Proc. of ICLP.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "The providence of provenance", "author": ["P. Buneman"], "venue": "Proc. of BNCOD.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Taming the infinite chase: Query answering under expressive relational constraints", "author": ["A. Cal\u0131\u0300", "G. Gottlob", "M. Kifer"], "venue": "Proc. of KR.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "A general Datalog-based framework for tractable query answering over ontologies", "author": ["A. Cal\u0131\u0300", "G. Gottlob", "T. Lukasiewicz"], "venue": "J. Web Sem. 14", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Provenance in databases: Why, how and where", "author": ["J. Cheney", "L. Chiticariu", "Tan", "W.-C."], "venue": "Foundation and Trends in Databases 1(4)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Preference formulas in relational queries", "author": ["J. Chomicki"], "venue": "TODS 28(4)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Preference logic programming", "author": ["K. Govindarajan", "B. Jayaraman", "S. Mantha"], "venue": "Proc. of ICLP.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Preference queries in deductive databases", "author": ["K. Govindarajan", "B. Jayaraman", "S. Mantha"], "venue": "New Generation Computing 19(1)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Preferences: Putting more knowledge into queries", "author": ["M. Lacroix", "P. Lavency"], "venue": "Proc. of VLDB. Volume 87.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1987}, {"title": "Preference-based query answering in Datalog+/\u2013 ontologies", "author": ["T. Lukasiewicz", "M.V. Martinez", "G.I. Simari"], "venue": "Proc. of IJCAI.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The foundations for provenance on the Web", "author": ["L. Moreau"], "venue": "Found. Trends Web Sci. 2(2/3)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey on representation, composition and application of preferences in database systems", "author": ["K. Stefanidis", "G. Koutrika", "E. Pitoura"], "venue": "TODS 36(3)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "First, we briefly recall some basics on Datalog+/\u2013 [7], namely, on relational databases and (Boolean) conjunctive queries ((B)CQs) (along with tuple- and equality-generating dependencies (TGDs and EGDs, respectively) and negative constraints), the chase procedure, and ontologies in Datalog+/\u2013.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Note that query answering under general TGDs is undecidable [2], even when the schema and TGDs are fixed [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Note that query answering under general TGDs is undecidable [2], even when the schema and TGDs are fixed [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "Adding EGDs over databases with TGDs along with negative constraints does not increase the complexity of BCQ query answering as long as they are non-conflicting [7].", "startOffset": 161, "endOffset": 164}, {"referenceID": 6, "context": "The TGD chase works on a database via so-called TGD chase rules (see [7] for an extended chase with also EGD chase rules).", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": ", there exists a homomorphism from chase(D,\u03a3) onto everyB \u2208mods(D,\u03a3) [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "One possible way of specifying such a relation is the preference formula framework of [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": ", fn) be a tuple of features associated with the predicate p, each of which has a domain dom(fi) = [0, 1]\u222a{\u2212}.", "startOffset": 99, "endOffset": 105}, {"referenceID": 0, "context": "A trust measure is any function \u03c4 : Reports\u2192 [0, 1].", "startOffset": 45, "endOffset": 51}, {"referenceID": 0, "context": "A relevance measure is any function \u03c1 : Reports\u00d7 SPOs(F)\u2192 [0, 1].", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Algorithm SummarizeReports(\u03c4,Reports, collFunc) Input: Trust measure \u03c4 , set of reports Reports, and function collFunc that collapses histograms to values in [0, 1].", "startOffset": 158, "endOffset": 164}, {"referenceID": 0, "context": "9, 1]} and values of type [0, 1], where n = |F| (we use values 1, .", "startOffset": 26, "endOffset": 32}, {"referenceID": 0, "context": ", fn}, user preferences PU , trust measure \u03c4 , relevance measure \u03c1, relThresh \u2208 [0, 1], function collFunc that collapses histograms to values in [0, 1], set of reports Reports, k > 1.", "startOffset": 80, "endOffset": 86}, {"referenceID": 0, "context": ", fn}, user preferences PU , trust measure \u03c4 , relevance measure \u03c1, relThresh \u2208 [0, 1], function collFunc that collapses histograms to values in [0, 1], set of reports Reports, k > 1.", "startOffset": 145, "endOffset": 151}, {"referenceID": 0, "context": "A weighting function for g-reports is any function \u03c9 : g-Reports\u2192 [0, 1] such that: (i) if gr1 gr2 then \u03c9(gr1) > \u03c9(gr2); and (i) if gr1 \u223c gr2 then \u03c9(gr1) = \u03c9(gr2).", "startOffset": 66, "endOffset": 72}, {"referenceID": 12, "context": "of the art in this respect is centered around relational databases and, recently, in ontological languages for the Semantic Web [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "The seminal work in preference-based query answering was that of [12], in which the authors extend the SQL language to incorporate user preferences.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "The preference formula formalism was introduced in [9] as a way to embed a user\u2019s preferences into SQL.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "An important development in this line of research is the well-known skyline operator, which was first introduced in [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 14, "context": "A recent survey of preference-based query answering formalisms is provided in [15].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "Studies of preferences related to our approach have also been done in classical logic programming [10,11] as well as answer set programming frameworks [4].", "startOffset": 98, "endOffset": 105}, {"referenceID": 10, "context": "Studies of preferences related to our approach have also been done in classical logic programming [10,11] as well as answer set programming frameworks [4].", "startOffset": 98, "endOffset": 105}, {"referenceID": 3, "context": "Studies of preferences related to our approach have also been done in classical logic programming [10,11] as well as answer set programming frameworks [4].", "startOffset": 151, "endOffset": 154}, {"referenceID": 12, "context": "The present work can be considered as a further development of the PrefDatalog+/\u2013 framework presented in [13], where we develop algorithms to answer skyline queries, and their generalization to k-rank queries, over classical Datalog+/\u2013 ontologies.", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "Finally, this work is closely connected to the study and use of provenance in information systems and, in particular, the Semantic Web and social media [14,1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 0, "context": "Finally, this work is closely connected to the study and use of provenance in information systems and, in particular, the Semantic Web and social media [14,1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 4, "context": "Research in provenance distinguishes between data and workflow provenance [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "In this work, we propose a new kind of provenance that is closely related to data provenance, but does not fit into the why, how, and where provenance framework typically considered in data provenance research [8].", "startOffset": 210, "endOffset": 213}], "year": 2013, "abstractText": "The use of preferences in query answering, both in traditional databases and in ontology-based data access, has recently received much attention, due to its many real-world applications. In this paper, we tackle the problem of top-k query answering in Datalog+/\u2013 ontologies subject to the querying user\u2019s preferences and a collection of (subjective) reports of other users. Here, each report consists of scores for a list of features, its author\u2019s preferences among the features, as well as other information. Theses pieces of information of every report are then combined, along with the querying user\u2019s preferences and his/her trust into each report, to rank the query results. We present two alternative such rankings, along with algorithms for top-k (atomic) query answering under these rankings. We also show that, under suitable assumptions, these algorithms run in polynomial time in the data complexity. We finally present more general reports, which are associated with sets of atoms rather than single atoms.", "creator": "LaTeX with hyperref package"}}}