{"id": "1312.0048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2013", "title": "Stochastic Optimization of Smooth Loss", "abstract": "in this paper, we further first prove suppose a specified high probability bound rather than an expectation of bound for stochastic optimization with optimal smooth limit loss. \u2013 furthermore, the existing equilibrium analysis requires computing the knowledge of only optimal classifier for numerical tuning as the step size in order to achieve the technically desired bound. however, this latter information is apparently usually completely not accessible in sufficiently advanced. sometimes we also ourselves propose a strategy outlined to publicly address the limitation.", "histories": [["v1", "Sat, 30 Nov 2013 01:07:25 GMT  (13kb)", "http://arxiv.org/abs/1312.0048v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rong jin"], "accepted": false, "id": "1312.0048"}, "pdf": {"name": "1312.0048.pdf", "metadata": {"source": "CRF", "title": "Stochastic Optimization of Smooth Loss", "authors": ["Rong Jin"], "emails": ["rongjin@cse.msu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n00 48\nv1 [\ncs .L\nG ]\n3 0\nN ov\n2 01\n3 JMLR: Workshop and Conference Proceedings vol 30 (2013) 1\u20136 Stochastic Optimization of Smooth Loss Rong Jin rongjin@cse.msu.edu Department of Computer Science and Engineering Michigan State University East Lansing, MI 48824, USA\nLet \u03c6(z) be a smooth loss function, with |\u2113\u2032(z)| \u2264 L and |\u2113\u2032(z)\u2212 \u2113\u2032(z\u2032)| \u2264 \u03b3|z \u2212 z\u2032|. Let \u2126 = { w \u2208 Rd : |w| \u2264 R } be the solution domain. Let (xi, yi), i = 1, . . . , n be the sequence of i.i.d samples used for training, where xi \u2208 Rd and yi \u2208 {\u22121,+1}. Our goal is to find a solution w\u0302 with a good generalization performance. More specifically, let \u2113(w) be the expected loss for any solution w, i.e. \u2113(w) = E[\u2113(yw\u22a4x)]. Our goal is to minimize \u2113(w).\nA straightforward approach is to optimize \u2113(w) by stochastic optimization. Let w1 = 0 be the initial. At each iteration t, we receive a training example (xi, yi), and update the current solution wt by wt+1 = argmin\u03c0\u2126 (wt \u2212 \u03b7\u2207\u2113t(wt)) where \u03b7 > 0 is the stepsize and \u2113t(w) = \u03c6(ytw\n\u22a4xt). The final solution w\u0302 will be the average of all the solutions, i.e. w\u0302 = \u2211T t=1 wt/T . In (Srebro et al., 2010), the authors were able to show that a simple stochastic optimization method, with an appropriate choice of step size \u03b7, can achieves the following generalization error bound in expectation, i.e.\nE[\u2113(w\u0302)] \u2264 \u2113(w\u2217) +K ( t\nn +\n\u221a \u2113(w\u2217) t\nn\n)\nwhere t = \u03b3|w\u2217|2. There are two limitations with the analysis in (Srebro et al., 2010). First, it shows a bound in expectation, not a high probability bound. Second, it requires the knowledge of \u2113(w\u2217) for tuning the step size in order to achieve the desired bound. In the draft presented in this work, we improve the analysis in (Srebro et al., 2010) by addressing these two limitations.\nFirst, let\u2019s address the first limitation by showing a high probability bound. At each iteration, we have\n\u2113t(wt)\u2212 \u2113t(w\u2217) \u2264 |wt \u2212w\u2217|2 2\u03b7 \u2212 |wt+1 \u2212w\u2217| 2 2\u03b7 + \u03b7 2 |\u2207\u2113t(wt)|2\n\u2264 |wt \u2212w\u2217| 2 2\u03b7 \u2212 |wt+1 \u2212w\u2217| 2 2\u03b7 + 2\u03b7\u03b3\u2113t(wt)\nc\u00a9 2013 R. Jin.\nJin\nwhere in the last step, we use the property |\u03c6\u2032(ytw\u22a4t xt)|2 \u2264 4\u03b3\u03c6(ytw\u22a4t xt). By adding the inequalities of all iterations and using the assumption \u03b7 \u2264 1/[2\u03b3], we have\nT\u2211\nt=1\n\u2113(wt)\u2212 \u2113(w\u2217)) \u2264\nR2 2\u03b7 + 2\u03b7\u03b3\nT\u2211\nt=1\n\u2113(wt) + (\u22122\u03b7\u03b3 + 1) T\u2211\nt=1 \u2113(wt)\u2212 \u2113t(wt) \ufe38 \ufe37\ufe37 \ufe38\n:=AT\n+ T\u2211\nt=1 \u2113t(w\u2217)\u2212 \u2113t(w\u2217) \ufe38 \ufe37\ufe37 \ufe38\n:=BT\nTo bound AT and BT , we need the following bound for martingales.\nTheorem 1 (Bernsteins inequality for martingales). Let X1, . . . ,Xn be a bounded martingale difference sequence with respect to the filtration F = (Fi)1\u2264i\u2264n and with \u2016Xi\u2016 \u2264 K. Let\nSi =\ni\u2211\nj=1\nXj\nbe the associated martingale. Denote the sum of the conditional variances by\n\u03a32n =\nn\u2211\nt=1\nE [ X2t |Ft\u22121 ]\nThen for all constants t, \u03bd > 0,\nPr [ max\ni=1,...,n Si > t and \u03a3\n2 n \u2264 \u03bd\n] \u2264 exp ( \u2212 t 2\n2(\u03bd +Kt/3)\n)\nand therefore,\nPr [ max\ni=1,...,n Si >\n\u221a 2\u03bdt+\n\u221a 2\n3 Kt and \u03a32n \u2264 \u03bd\n] \u2264 e\u2212t\nUsing the above theorem, with a probability 1\u2212 e\u2212t, we can bound BT by\nBT \u2264 \u221a 2t\n3 C +\n\u221a 2tC\u2113(w\u2217)T\nwhere C = LR + \u03c6(0) and t = log(1/\u03b4). To bound AT , we define martingale difference Xt = \u2113(wt)\u2212 \u2113t(wt). Define the conditional variance \u03a32T as\n\u03a32T =\nT\u2211\nt=1\nEt [ X2t ] \u2264 T\u2211\nt=1\nEt[\u2113 2 t (wt)] \u2264 C\nT\u2211\nt=1\n\u2113(wt) = CDT\nwhere DT := \u2211T t=1 \u2113(wt). Using the Berstein inequality for martingale sum, we have\nPr ( AT \u2265 2 \u221a CDT \u03c4 + \u221a 2C\u03c4/3 )\n= Pr ( AT \u2265 2 \u221a CDT \u03c4 + \u221a 2C\u03c4/3,\u03a32T \u2264 CDT ) = Pr ( AT \u2265 2 \u221a CDT \u03c4 + \u221a 2C\u03c4/3,\u03a32T \u2264 CDT ,DT \u2264 C )\n+\nm\u2211\ni=1\nPr ( AT \u2265 2 \u221a CDT \u03c4 + \u221a 2C\u03c4/3,\u03a32T \u2264 CDT , 2i\u22121C < DT \u2264 2iC )\n\u2264 Pr (DT \u2264 C) + m\u2211\ni=1\nPr ( AT \u2265 C \u221a 2i+1\u03c4 + \u221a 2C\u03c4/3,\u03a32T \u2264 C2i )\n\u2264 Pr (DT \u2264 C) +me\u2212\u03c4\nwhere m = \u2308log2 T \u2309. As a result, we have\nPr ( AT \u2264 2 \u221a CDT t+ \u221a 2\n3 Ct\n) + Pr(DT \u2264 C) \u2265 1\u2212 e\u2212t\nwhere t = log(1/\u03b4) + logm. Using the bounds for AT and BT , we have, with a probability 1\u2212 2e\u2212t,\n(1\u2212 2\u03b7\u03b3)DT \u2212 \u2113(w\u2217)T \u2264\nC + R2\n2\u03b7 + (1\u2212 2\u03b7\u03b3)\n( 2 \u221a\nCDT t+\n\u221a 2\n3 Ct\n) + \u221a 2\n3 Ct+\n\u221a 2tC\u2113(w\u2217)T\nwhere t = log(1/\u03b4) + logm. Reorganizing the terms in the above inequality, we have\n(1\u2212 2\u03b7\u03b3) ( DT \u2212 2 \u221a CDT t ) \u2212 \u2113(w\u2217)T \u2264 R2\n2\u03b7 + Ct+\n\u221a 2tC\u2113(w\u2217)T\nwhere t = log(1/\u03b4) + logm + 1. It is easy to verify that DT \u2212 2 \u221a CDT t is monotonically increasing when DT \u2265 Ct. Hence, we have, with a probability 1\u2212 2\u03b4,\n(1\u2212 2\u03b7\u03b3) ( \u2113(w\u0302)\u2212 2 \u221a Ct\nT \u2113(w\u0302)\n) \u2264 \u2113(w\u2217) + R2\n2\u03b7T +\nCt\nT +\n\u221a 2Ct\nT \u2113(w\u2217)\nor\n\u2113(w\u0302)\u2212 \u2113(w\u2217) \u2264 R2\n2\u03b7T + 2\u03b7\u03b3\u2113(w\u0302) +\n\u221a 2Ct\nT \u2113(w\u2217) + 2\n\u221a Ct\nT \u2113(w\u0302) +\nCt\nT (1)\nBy setting \u03b7 = R/2 \u221a \u03b3T\u2113(w\u0302), we have, with a probability 1\u2212 \u03b4,\n\u2113(w\u0302)\u2212 \u2113(w\u2217) \u2264 \u221a 2Ct\nT \u2113(w\u2217) + 2\n\u221a Ct\nT \u2113(w\u0302) +\nCt\nT\nJin\nwhere t = log(1/\u03b4) + logm + 1 + R2\u03b3/C. Since \u2113(w\u2217) \u2264 C and \u2113(w\u0302) \u2264 C, under the assumption T \u2265 t, we have\n\u2113(w\u0302)\u2212 \u2113(w\u2217) \u2264 Ct\nT + 4C\n\u221a t\nT \u2264 5C\n\u221a t\nT\nand therefore, with a probability 1\u2212 2\u03b4\n\u2113(w\u0302)\u2212 \u2113(w\u2217) \u2264 4 \u221a Ct\nT \u2113(w\u2217) + (2\n\u221a 5 + 1) Ct\nT\nThe above analysis allows us to derive a high probability bound for the proposed algorithm. It however does not resolve the problem of determining the appropriate step size \u03b7. We address this limitation by exploring the doubling trick. We divide the learning process intom epoches where the kth epoch is comprised of Tk training examples, with Tk = T12\nk\u22121. Let w1k, . . . ,w Tk k be the sequence of solutions generated by the kth epoch. Define\nDk = 1\nTk\nTk\u2211\ni=1\n\u2113(wik)\nWe assume that, with a probability 1\u2212 \u03b4, we have\nDk \u2212 \u2113(w\u2217) \u2264 K ( Ct\nTk +\n\u221a Ct\nTk \u2113(w\u2217)\n)\nwhere t = log(1/\u03b4) + logm+ 1 +R2\u03b3/C. Define D\u0302k as\nD\u0302k = 1\nTk\nTk\u2211\ni=1\n\u2113ik(w i k)\nwhere \u2113ik(w) = \u03c6(y i kw \u22a4xik). We note that D\u0302k can be computed from the kth epoch. We would like to bound D\u0302k \u2212Dk as\n|D\u0302k \u2212Dk| = 1\nTk\nT\u2211\ni=1\n\u2113ik(w i k)\u2212 \u2113(wik)\nUsing the bound for AT , we have, with a probability 1\u2212 T 2\n|D\u0302k \u2212Dk| \u2264 2 \u221a Ct\nTk Dk +\n\u221a 2\n3\nCt Tk\nor\nDk \u2264 C\nT 3\nIn the second case, since E[D\u0302k] = Dk \u2264 C/T 3, using the Markov inequality, we have, with a probability 1\u2212 T\u22122,\n|D\u0302k \u2212Dk| \u2264 C\nT\nCombining the above two statements, we have, with a probability 1\u2212 2T\u22122\n|D\u0302k \u2212Dk| \u2264 2 \u221a Ct\nTk Dk + 2\nCt Tk\nand consequentially,\n|D\u0302k \u2212Dk| \u2264 6 (\u221a Ct\nTk D\u0302k +\nCt Tk\n)\nWe thus will use the following expression as the surrogate for \u2113(w\u2217)\n\u2113\u0302k = D\u0302k + 6 (\u221a Ct\nTk D\u0302k +\nCt Tk\n)\nUsing \u2113\u0302k, we define the step size \u03b7k+1 as\n\u03b7k+1 = R\n2 \u221a \u03b3Tk+1\u2113\u0302k\nIt is easy to verify that with a probability 1\u22122T\u22122 (i) \u2113\u0302k \u2265 Dk \u2265 \u2113(w\u2217) and (ii) \u2113\u0302k\u2212\u2113(w\u2217) \u2264 (K + 6) (\u221a Ct Tk D\u0302k + CtTk ) . Using the bound in (1), we have\n(1\u22122\u03b7\u03b3)(Dk+1\u2212\u2113(w\u2217)) \u2264 R2\n2\u03b7k+1Tk+1 +2\u03b7k+1\u03b3\u2113(w\u2217)+2\n\u221a Ct\nTk+1 \u2113(w\u2217)+2\n\u221a Ct\nTk+1 (Dk+1 \u2212 \u2113(w\u2217))+\nCt\nTk+1\nUsing the property \u2113\u0302k \u2265 \u2113(w\u2217), we have\n2\u03b7k+1\u03b3\u2113(w\u2217) \u2264 R \u221a \u03b3\nTk+1 \u2113(w\u2217)\nWe also have\nR2\n2\u03b7k+1Tk+1 = R\n\u221a \u03b3\nTk+1 \u2113\u0302k \u2264 R \u221a\u221a\u221a\u221a \u03b3 Tk+1 (K + 6) [ Ct Tk + \u221a Ct Tk D\u0302k ]\nSince\nD\u0302k \u2212Dk \u2264 2 \u221a Ct\nTk Dk + 2\nCt Tk\nand\nDk \u2212 \u2113(w\u2217) \u2264 K ( Ct\nTk +\n\u221a Ct\nTk \u2113(w\u2217)\n)\nwe have\nD\u0302k \u2264 \u2113(w\u2217) +K ( Ct\nTk +\n\u221a Ct\nTk \u2113(w\u2217)\n) + 2 Ct\nTk + 2\n\u221a K Ct\nTk + 4\n\u221a K Ct\nTk + 4\n\u221a K\n\u221a Ct\nTk \u2113(w\u2217)\nJin\nBy choosing sufficiently large K, we have\nD\u0302k \u2264 \u2113(w\u2217) + 2K ( Ct\nTk +\n\u221a Ct\nTk \u2113(w\u2217)\n)\nHence,\nR2\n2\u03b7k+1Tk+1 \u2264 R\n\u221a 2\u03b3(K + 6)Ct\nTk+1 + 2R2\u03b3 Tk+1 +2\n\u221a 2Ct\nTk+1 \u2113(w\u2217)+6\n\u221a 2K 2Ct\nTk+1 +4\n\u221a 2K\n\u221a 2Ct\nTk+1\u2113(w\u2217)\nBy choosing sufficiently large K, we have\nR2\n2\u03b7k+1Tk+1 \u2264 K 3\n( Ct\nTk+1 +\n\u221a Ct\nTk+1 \u2113(w\u2217)\n)\nWe thus have\n(1\u2212 2\u03b7\u03b3)(Dk+1 \u2212 \u2113(w\u2217)) \u2264 2K\n3\n( Ct\nTk+1 +\n\u221a Ct\nTk+1 \u2113(w\u2217)\n)\nBy choosing \u03b7 \u2264 1/[6\u03b3], we have\nDk+1 \u2212 \u2113(w\u2217) \u2264 K ( Ct\nTk+1 +\n\u221a Ct\nTk+1 \u2113(w\u2217)\n)\nReferences\nNathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In NIPS, pages 2199\u20132207, 2010."}], "references": [{"title": "Smoothness, low noise and fast rates", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In (Srebro et al., 2010), the authors were able to show that a simple stochastic optimization method, with an appropriate choice of step size \u03b7, can achieves the following generalization error bound in expectation, i.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "There are two limitations with the analysis in (Srebro et al., 2010).", "startOffset": 47, "endOffset": 68}, {"referenceID": 0, "context": "In the draft presented in this work, we improve the analysis in (Srebro et al., 2010) by addressing these two limitations.", "startOffset": 64, "endOffset": 85}], "year": 2013, "abstractText": "Let \u03c6(z) be a smooth loss function, with |l\u2032(z)| \u2264 L and |l\u2032(z)\u2212 l\u2032(z\u2032)| \u2264 \u03b3|z \u2212 z\u2032|. Let \u03a9 = { w \u2208 R : |w| \u2264 R } be the solution domain. Let (xi, yi), i = 1, . . . , n be the sequence of i.i.d samples used for training, where xi \u2208 R and yi \u2208 {\u22121,+1}. Our goal is to find a solution \u0175 with a good generalization performance. More specifically, let l(w) be the expected loss for any solution w, i.e. l(w) = E[l(ywx)]. Our goal is to minimize l(w). A straightforward approach is to optimize l(w) by stochastic optimization. Let w1 = 0 be the initial. At each iteration t, we receive a training example (xi, yi), and update the current solution wt by wt+1 = argmin\u03c0\u03a9 (wt \u2212 \u03b7\u2207lt(wt)) where \u03b7 > 0 is the stepsize and lt(w) = \u03c6(ytw xt). The final solution \u0175 will be the average of all the solutions, i.e. \u0175 = \u2211T t=1 wt/T . In (Srebro et al., 2010), the authors were able to show that a simple stochastic optimization method, with an appropriate choice of step size \u03b7, can achieves the following generalization error bound in expectation, i.e.", "creator": "LaTeX with hyperref package"}}}