{"id": "1312.1752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2013", "title": "Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation", "abstract": "bio - inspired optimization algorithms has have noticeably been gaining more popularity albeit recently. one version of the most particularly important methods of these algorithms is particle database swarm optimization ( pso ). reconstructed pso is based on the collective intelligence of researching a whole swam of particles. each particle explores up a certain part of the search space looking broadly for the optimal scale position distribution and adjusts its sample position manually according to identifying two factors ; basically the theoretical first is its own experience information and the second is primarily the virtual collective experience of the whole swarm. pso has been successfully used to repeatedly solve exceptionally many optimization problems. \u2026 in this exciting work we use downloaded pso to improve the performance of a well - well known proportional representation computed method of time series data and which essentially is labeled the symbolic aggregate approximation ( sax ). as with other data time reversed series aggregate representation methods, sax spectral results increase in loss of information structures when applied to represent recovered time linear series. in conducting this simulation paper we use yamaha pso to efficiently propose creating a novel new term minimum degree distance encoding wmd for sax measurement to remedy this problem. unlike the two original project minimum distance, hence the new distance reconstruction sets different weights trying to find different segments of the reconstructed time lobe series solution according to determining their information content. providing this weighted minimum distance enhances the computing performance of sax efficiency as we earlier show through experiments using different time series datasets.", "histories": [["v1", "Fri, 6 Dec 2013 02:22:59 GMT  (106kb)", "http://arxiv.org/abs/1312.1752v1", "The 8th International Conference on Advanced Data Mining and Applications (ADMA 2012)"]], "COMMENTS": "The 8th International Conference on Advanced Data Mining and Applications (ADMA 2012)", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["muhammad marwan muhammad fuad"], "accepted": false, "id": "1312.1752"}, "pdf": {"name": "1312.1752.pdf", "metadata": {"source": "CRF", "title": "Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation", "authors": ["Muhammad Marwan", "Muhammad Fuad"], "emails": ["marwan.fuad@iet.ntnu.no"], "sections": [{"heading": null, "text": "Keywords: Particle Swarm Optimization, Bio-inspired Optimization, Time Series Data Mining, Information Loss, Information Content, Symbolic Aggregate Approximation."}, {"heading": "1 Introduction", "text": "A time series is a sequence of real numbers over a period of time. Each of these numbers represents the value of the observed phenomenon at a certain time point. Time series data have been used in many applications such as science, medicine, and engineering. Time series data mining handles several tasks such as similarity search, classification, clustering, and others. ____________________________________ This work was carried out during the tenure of an ERCIM \u201cAlain Bensoussan\u201d Fellowship Programme. This Programme is supported by the Marie-Curie Co-funding of Regional, National and International Programmes (COFUND) of the European Commission.\nTime series datasets are usually very large so direct search or sequential scanning of these datasets is inefficient. In order to overcome this problem transformations can be applied to reduce the dimensionality of the original time series and to represent them in a space with manageable dimensionality. Such transformations are called dimensionality reduction techniques, or representation methods. The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].\nOther methods of time series data mining use multi-resolution approaches. In [16] and [27] a method of multi resolution representation of time series is presented. This symbolic method uses a multi-resolution vector quantized approximation of the time series together with a multi-resolution similarity distance. Using this representation the method keeps both local and global information of the time series data. In [19] and [20] other multi-resolution method are proposed. These methods are based on a fastand-dirty filtering scheme that iteratively reduces the search space using several resolution levels. The technique presented in [22] couples and fast-and-dirty filter with a multi-resolution representation of the time series.\nIndexing time series data usually includes establishing a lower bounding distance on time series in the transformed space to guarantee that the representation method will not cause false dismissals. This is achieved by defining a distance, on the transformed space, that underestimates the distance in the original space. This condition is known as the lower-bounding lemma. [2].\nAmong representation methods of time series data, symbolic representation of time series has several advantages which interested researchers in this field of computer science. One of its main advantages is that symbolic representation permits researchers to benefit from the ample symbolic algorithms known in the text-retrieval and bioinformatics communities [14].\nThere have been many suggestions to represent time series symbolically. But in general, most of these symbolic representation methods suffered from two main inconveniences [15]; the first is that the dimensionality of the symbolic representation method is the same as that of the original space, so there is no virtual dimensionality reduction. The second drawback is that although distance measures have been defined on the reduced symbolic spaces, these distance measures are poorly correlated with the original distance measures defined on the original spaces.\nOne of the most widely-known symbolic representation methods of time series is SAX. SAX uses pre-computed distances obtained from lookup tables. This makes SAX fast to compute.\nIn this work we show how the performance of SAX can be improved by substituting the original similarity measure used with SAX by a new one which assigns different weights to different segments of the time series according to their information content. These weights are set using the particle swarm optimization; a widely used population-based optimization method which has been successful in solving many optimization problems. We show through experiments conducted on\ndifferent time series dataset how the new similarity measure can give better results than the original one.\nThe work presented in this paper is organized as follows: in Section 2 we present related background. In Section 3 we introduce the new scheme and we evaluate its performance in Section 4. In Section 5 we give concluding remarks."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 The Symbolic Aggregate Approximation (SAX)", "text": "The Symbolic Aggregate approXimation method (SAX) [14] is one of the most important symbolic representation methods of time series. The main advantage of SAX is that the similarity measure it uses, called MINDIST, uses statistical lookup tables, which makes it is easy to compute with an overall complexity of ( )NO .\nSAX is based on an assumption that normalized time series have Gaussian distribution, so by determining the breakpoints that correspond to a particular alphabet size, one can obtain equal-sized areas under the Gaussian curve. SAX is applied as follows: 1-The time series are normalized. 2-The dimensionality of the time series is reduced using PAA [9], [30] 3-The PAA representation of the time series is discretized by determining the number and location of the breakpoints (The number of the breakpoints is chosen by the user). Their locations are determined, as mentioned above, using Gaussian lookup tables. The interval between two successive breakpoints is assigned to a symbol of the alphabet, and each segment of PAA that lies within that interval is discretized by that symbol.\nThe last step of SAX is using the following similarity measure:\n( ) ( )( )\u2211 =\n\u2261 N\ni ii r\u0302,s\u0302distN nR\u0302,S\u0302MINDIST 1 2 (1)\nWhere n is the length of the original time series, N is the length of the strings (the number of the segments), S\u0302 and R\u0302 are the symbolic representations of the two time series S and R , respectively, and where the function )(dist is implemented by using the appropriate lookup table. For instance, the lookup table of MINDIST for an alphabet size of 3 is the one shown in Table 1.\nWe also need to mention that the similarity measure used in PAA is:\n( ) ( )\u2211 = \u2212= N i ii rs N nR,Sd 1 2 (2)\nWhere n is the length of the time series, N is the number of segments.\nIt is proven in [9], [30] that the above similarity distance is a lower bound of the Euclidean distance applied in the original space of time series. This means that MINDIST is also a lower bound of the Euclidean distance, because it is a lower bound of the similarity measure used in PAA. This guarantees no false dismissals.\nThere are other versions and extensions of SAX [11], [25], [26], [28]. These versions use it for other applications or apply it to index massive datasets, or compute MINDIST differently [18]. However, the version of SAX that we presented earlier, which is called classic SAX, is the basis of all these versions and extensions and it is actually the most widely-known one."}, {"heading": "2.2 Information Content", "text": "Quantifying the content of information a vector carries was first introduced by Shannon in [24]. This is measured by what is known as entropy and is defined for a discrete probabilistic system by:\n\u2211\u2212= i ii plogpH (3)\nwhere the base of the logarithm is 2.\nThis concept has many applications in cryptography, data transmission, natural language processing, data compression, and others.\nIn time series mining, the concept of information content was implicitly or explicitly present in different representation methods of time series data. DFT [2], [3] and DWT [5], for instance are based on the fact that the first coefficients are the most meaningful ones; i.e. they contain most of the information in the time series, so the other coefficients can be truncated without much loss of information. APCA [10] segments the time series into segments of varying lengths such that their individual reconstruction errors are minimal. The intuition behind this idea is that different regions of the time series contain different amounts of information. So while regions of high activity contain high fluctuations, other regions of low activity show a flat behavior, so a representation method with high fidelity should reflect this difference in behavior."}, {"heading": "3 Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation (PSOWSAX)", "text": "In time series mining the distance that is widely used to compute the similarity between the two time series { }nsssS ,...,, 21= and { }nr,...,r,rR 21= is the Euclidean distance which is defined as follows:\n( ) 2 2\n1 2 \u2211 =\n\u2212= n\ni ii rsR,SL (4)\nOne of the variations of the Euclidean distance, which is much related to the topic of this paper, is the Weighted Euclidean Distance. This distance is defined as:\n( ) ( )\u2211 =\n\u2212= n\ni iii rswW,R,Sd 1\n2 (5)\nwhere W is the weight vector. Fig. 1 shows the Euclidean distance and the weighted Euclidean distance between two time series.\nThe intuition behind the weighted Euclidean distance is, again, that some parts of the time series may have more importance than other parts so the distance should reflect these regions differently compared with regions that contain less information.\nAlthough weighing different regions differently seems to be a good solution to distinguish parts with high information content from others with less information, the question remains on how to set the weights. In [29] the authors proposed setting the weights using relevance feedback provided by the user. We can easily see that this solution is highly subjective and also inefficient.\nIt is important to mention that the weighted distance defined in (5) is applied to the raw time series and not to the reduced, lower-dimensional time series used in representation methods.\nIn this paper we present a modification of SAX based on the concept of information content.\nIn Section 2.1, we presented MINDIST ; the similarity measure used with SAX. From relation (1) we can derive the following similarity measure which we call the Weighted Minimum Distance (WMD):\n( ) ( )( ) [ ]10 1 2 ,w;r\u0302,s\u0302distw N nR\u0302,S\u0302WMD i N i iii \u2208= \u2211 = (6)\nNotice that if we set i,wi \u2200=1 in (6) we obtain MINDIST defined in relation (1), so MINDIST is in fact a special case of WMD . Notice also, which is very important, that from (1) and (6) we can easily see that:\n( ) ( )R\u0302,S\u0302MINDISTR\u0302,S\u0302WMD \u2264 (7) Since MINDIST is a lower bound of the Euclidean distance in the original space this implies that WMD is also a lower bound of the Euclidean distance, so our proposed distance guarantees no false dismissals. .\nThe weights in (6) will be set for the whole time series in the dataset using the particle swarm optimization. Particle Swarm Optimization: Particle Swarm Optimization (PSO) is a member of a family of naturally-inspired optimization algorithms called Swarm Intelligence which are population-based optimization algorithms. PSO was inspired by the social behavior of some animals, such as bird flocking or fish schooling [8]. [23] proposed a model that simulates a swarm. In this model individuals, also called agents or particles, follow these rules (Fig. 2): Separation: Each particle avoids getting too close to its neighbors. Alignment: Each particle steers towards the general heading of its neighbors. Cohesion: Each particle moves towards the average position of its neighbors. Separation Alignment Cohesion\nFig. 2. Simulating swarm\u2019s behavior\nThe above model was elaborated by adding another rule which is obstacle avoidance which uses steer-to-avoid concept.\nAs is the case with many other optimization algorithms, there are quite a large number of variations of PSO. In the following we present a standard PSO [7]. PSO starts by initializing a swarm of sSize particles at random positions 0iX r and velocities\n0 iV r\nwhere { }sSize,..,i 1\u2208 . In the next step each position is evaluated, and for each iteration, using a fitness\nfunction, also called objective function or cost function. The positions 1+kiX r and velocities 1+kiV r are updated at time step ( 1+k ) according to the following formulae:\n( ) ( )kikiLkikGkiki XLXGV.V rrrrrr \u2212+\u2212+=+ \u03d5\u03d5\u03c91 (8)\nki k i k i VXX\nrrr +=+1 (9)\nwhere GGG a.r=\u03d5 , LLL a.r=\u03d5 , ( )10,Ur,r LG \u2192 , R\u2208GL a,a,\u03c9 , kiL\nr is the best position\nfound by particle i , kG r\nis the global best position found by the whole swarm, \u03c9 is called the inertia , La is called the local acceleration , and Ga is called the global acceleration. These last three parameters are control parameters which are chosen by the algorithm designer.\nAlgorithm1 Particle Swarm Optimization (PSO) Require Number of parameters (nPar ),swarm size (sSize ), number of iterations (nItr ),local acceleration ( La ), global acceleration ( Ga ), inertia\u03c9 .\nThe algorithm continues until a stopping criterion terminates it. Fig. 3 presents a pseudo code of PSO.\nAs in the case with other evolutionary algorithms, PSO should keep a balance between exploitation, and exploration. Exploration is defined as the act of searching for the purpose of discovery, and exploitation is defined as the act of utilizing something for any purpose [1].\nDiversity in PSO comes from two sources [31]; the first is the difference between the particle\u2019s current position and that of its best neighbor, and the other is the difference between the particle\u2019s current position and its best historical position. Variation, although provides exploration, can only be sustained for a limited number of generations because convergence of the swarm to the best position is necessary to refine the solution (exploitation)."}, {"heading": "4 Experimental Validation", "text": "We tested our distance on a time series classification task on the datasets available at [12], which is the same repository on which the original SAX was tested. This repository makes up between 90% and 100% of all publicly available, labeled time series data sets in the world, and it represents the interest of the data mining/database community, and not just one group [6].\nWe tested our method in a classification task based on the first nearest-neighbor (1- NN) rule using leaving-one-out cross validation. This means that every time series is compared to the other time series in the dataset. If the 1-NN does not belong to the same class, the error counter is incremented by 1.\nThe purpose of the experiments is to compare PSOWSAX (our new method which uses WMD as a similarity measure) with the original method (which uses MINDIST as a similarity measure) on the classification task and see which one gives the smallest error. This means that for each value of the alphabet size tested we perform the three steps of SAX presented in Section 2.1 to obtain the symbolic representation of the time series, and then we apply MINDIST when we test the original method or we apply WMD when we want to test ours, which, as indicated earlier, is based on PSO. The weights iw in relation (6) are obtained during a training phase. This means for each value of the alphabet size tested, we formulate a PSObased optimization problem where the fitness function is the classification error of the time series (we opt to minimize the classification error) and the outcome of this optimization problem is the weights iw that yield this minimum value of the classification error, then we use these values on the corresponding testing datasets to obtain the final classification error. As for MINDIST , there is no training phase and it is applied directly to the testing datasets.\nFor PSOWSAX, the swarm size we used in the experiments is 16. Each particle is a vector of nPar components representing potential weights of the segments of the time series.\nWe used a standard PSO, the local acceleration La was set to 2, the global acceleration Ga was set to 2. The dimension of the problem nPar is the dimension of the weight vector, which is the number of segments of the times series in the reduced\nspace; i.e. ratio ncompressio\nseriestime original the of lengthnPar = . This dimension differs from one\ndataset to another. The value of inertia \u03c9 depends on the current iteration according to the formula ( ) nItr/itrnItr \u2212=\u03c9 where itr is the current iteration. This means that the influence of\u03c9 decreases as the algorithm progresses.\nThe number of iterations nItr was set to 20. This is a rather small number of iterations and the algorithm could have been left to evolve more. However, the objective of our experiments was to validate our method rather than to show its best performance which could be achieved using more sophisticated PSO algorithms.\nWe also have to mention that we know from experience that for some datasets the classification error is always high, or always low, for all methods of time series representation, so a threshold of a classification error, related to the dataset tested, could be set as a stopping criterion.\nTable 2 summarizes the symbols used in the experiments together with their corresponding values.\nIn Fig. 4. we present some of the results we obtained for alphabet size equal to 3, 10, and 20, respectively. We chose to report these values because the first version of SAX used alphabet size that varied between 3 and 10. Then in a later version the alphabet size varied between 3 and 20. So these values are benchmark values.\nAs we can see from Fig. 4, the classification error of WMD is smaller than that of MINDIST for all values of the alphabet size and for all the datasets, except for\ndataset (Beef), alphabet size=20 and dataset (Fish), alphabet size=3 where both WMD and MINDIST gave the same classification error.\nFinally, we present in Table 3, for reproducibility purposes, the weights of datasets (CBF) and (ECG) (Because of space restrictions, we present these datasets only). As indicated earlier, these weights are obtained by applying PSOWSAX to the training datasets. The final classification errors of WMD (those shown in Fig. 4) are obtained by using those weights, with WMD , on the corresponding testing datasets."}, {"heading": "5 Conclusion", "text": "In this paper we showed through a new scheme PSOWSAX, based on particle swam optimization, how the performance of SAX; one of the most important symbolic representation methods of time series data, can be improved by using a new similarity measure WMD which assigns different weights to different segments of the time series according to their information content. The information loss caused by time series representation methods can be better recovered by setting different weights to different regions of the time series according to their information content.\nThe optimization process takes place at indexing time so the new scheme has the same low complexity as that of the original SAX.\nWe validated the new scheme by conducting classification task experiments on different datasets. The experiments showed that our new scheme gives better results than the original one.\nA possible future work will be to associate the work presented in this paper with the work presented in [20]. This can be achieved in two ways; the first is to use the optimization scheme presented in [20] to locate the breakpoints then to use the optimization scheme presented in this work to set different weights to different segments according to their information content. The second way is to use a one-step optimization problem to locate the breakpoints together with the corresponding weights of segments."}], "references": [{"title": "Webster's New World College Dictionary, Webster's New World, ISBN 0764571257", "author": ["M. Agnes"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Efficient Similarity Search in Sequence Databases", "author": ["R. Agrawal", "C. Faloutsos", "A. Swami"], "venue": "Proceedings of the 4th Conf. on Foundations of Data Organization and Algorithms.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Fast Similarity Search in the Presence of Noise, Scaling, and Translation in Time-Series Databases", "author": ["R. Agrawal", "K.I. Lin", "H.S. Sawhney", "K. Shim"], "venue": "In Proceedings of the 21st Int'l Conference on Very Large Databases. Zurich,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Indexing Spatio-temporal Trajectories with Chebyshev Polynomials", "author": ["Y. Cai", "R. Ng"], "venue": "In SIGMOD", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient Time Series Matching by Wavelets", "author": ["K. Chan", "A.W. Fu"], "venue": "In proc. of the 15th IEEE Int'l Conf. on Data Engineering. Sydney, Australia, Mar 23-26. pp 126-133.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Querying and Mining of Time Series Data: Experimental Comparison of Representations and Distance Measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "In Proc of the 34th VLDB", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "What Makes Particle Swarm Optimization a Very Interesting and Powerful Algorithm", "author": ["J.L. Fern\u00e1ndez-Mart\u00ednez", "E. Garc\u00eda-Gonzalo"], "venue": "Handbook of Swarm Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Practical Genetic Algorithms with CD-ROM", "author": ["R.L. Haupt", "S.E. Haupt"], "venue": "Wiley-Interscience", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases", "author": ["E Keogh", "K Chakrabarti", "M. Pazzani", "Mehrotra"], "venue": "J. of Know. and Inform. Sys.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Locally Adaptive Dimensionality Reduction for Similarity Search in Large Time Series Databases", "author": ["E Keogh", "K Chakrabarti", "M. Pazzani", "S. Mehrotra"], "venue": "SIGMOD pp 151-162", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "HOT SAX: Efficiently Finding the Most Unusual Time Series Subsequence", "author": ["E. Keogh", "J. Lin", "A. Fu"], "venue": "In Proc. of the 5th IEEE International Conference on Data Mining (ICDM 2005), Houston, Texas, Nov 27-30", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficiently Supporting Ad Hoc Queries in Large Datasets of Time Sequences", "author": ["F. Korn", "H. Jagadish", "C. Faloutsos"], "venue": "Proceedings of SIGMOD '97, Tucson, AZ, pp 289-300", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "A Symbolic Representation of Time Series, with Implications for Streaming Algorithms", "author": ["J. Lin", "E. Keogh", "S. Lonardi", "B.Y. Chiu"], "venue": "DMKD", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Experiencing SAX: a Novel Symbolic Representation of Time Series", "author": ["J. Lin", "E. Keogh", "L. Wei", "S. Lonardi"], "venue": "DMKD Journal", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiresolution Symbolic Representation of Time Series", "author": ["C. Megalooikonomou"], "venue": "In proceedings of the 21st IEEE International Conference on Data Engineering (ICDE). Tokyo, Japan.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "The L-index: An Indexing Structure for Efficient Subsequence Matching in Time Sequence Databases", "author": ["Y. Morinaka", "M. Yoshikawa", "T. Amagasa", "S. Uemura"], "venue": "In Proc. 5th PacificAisa Conf. on Knowledge Discovery and Data Mining, pages 51-60", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Enhancing the Symbolic Aggregate Approximation Method Using Updated Lookup Tables", "author": ["M.M. Muhammad Fuad", "P.F. Marteau"], "venue": "14th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems \u2013 KES 2010. Cardiff, Wales, UK. September 8-10", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast Retrieval of Time Series by Combining a Multiresolution Filter with a Representation Technique", "author": ["M.M. Muhammad Fuad", "P.F. Marteau"], "venue": "The International Conference on Advanced Data Mining and Applications\u2013ADMA2010, ChongQing, China, 21 November", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Genetic Algorithms-Based Symbolic Aggregate Approximation", "author": ["M.M. Muhammad Fuad"], "venue": "14th International Conference on Data Warehousing and Knowledge Discovery - DaWaK 2012 \u2013 Vienna, Austria, September 3 \u2013 7", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-resolution Approach to Time Series Retrieval", "author": ["M.M. Muhammad Fuad", "P.F. Marteau"], "venue": "Fourteenth International Database Engineering and Applications Symposium\u2013 IDEAS 2010 , 16-18 August, 2010, Montreal, QC, CANADA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Speeding-up the Similarity Search in Time Series Databases by Coupling Dimensionality Reduction Techniques with a Fast-and-dirty Filter", "author": ["M.M. Muhammad Fuad", "P.F. Marteau"], "venue": "Fourth IEEE International Conference on Semantic Computing\u2013 ICSC 2010, 22-24 September 2010, Carnegie Mellon University, Pittsburgh, PA, USA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Flocks, Herds and Schools: A Distributed Behavioral Model", "author": ["C.W. Reynolds"], "venue": "SIGGRAPH Comput. Graph. 21, 4", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1987}, {"title": "A Mathematical Theory of Communication", "author": ["C. Shannon"], "venue": "The Bell Systems Technical Journal 27, 379\u2013423, 623\u2013656", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1948}, {"title": "iSAX: Disk-Aware Mining and Indexing of Massive Time Series Datasets", "author": ["J. Shieh", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "iSAX: Indexing and Mining Terabyte Sized Time Series", "author": ["J. Shieh", "E. Keogh"], "venue": "In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24 \u2013 27", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Time Series Analysis with Multiple Resolutions", "author": ["Q. Wang", "V. Megalooikonomou", "C. Faloutsos"], "venue": "Inf. Syst. 35, 1", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "SAXually Explict Images: Finding Unusual Shapes", "author": ["L. Wei", "E. Keogh", "X. Xi"], "venue": "ICDM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "FALCON: Feedback Adaptive Loop for Content-Based Retrieval VLDB", "author": ["L. Wu", "C. Faloutsos", "K. Sycara", "T. Payne"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Fast Time Sequence Indexing for Arbitrary Lp Norms", "author": ["B.K. Yi", "C. Faloutsos"], "venue": "Proceedings of the 26th International Conference on Very Large Databases, Cairo, Egypt", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "Robust PSO-based Constrained Optimization by Perturbing the Particle\u2019s Memory", "author": ["A.M. Zavala", "A.H. Aguirre", "E.V. Diharce"], "venue": "Swarm Intelligence: Focus on ant and particle swarm optimization, Felix T. S. Chan and Manoj Kumar Tiwari,Ed. I-Tech Education and Publishing", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 153, "endOffset": 157}, {"referenceID": 9, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 208, "endOffset": 212}, {"referenceID": 8, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 254, "endOffset": 257}, {"referenceID": 28, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 262, "endOffset": 266}, {"referenceID": 15, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 305, "endOffset": 309}, {"referenceID": 3, "context": "The most widely known methods are Discrete Fourier Transform (DFT) [2] and [3], Discrete Wavelet Transform (DWT) [5], Singular Value Decomposition (SVD) [13], Adaptive Piecewise Constant Approximation (APCA) [10], Piecewise Aggregate Approximation (PAA) [9] and [30], Piecewise Linear Approximation (PLA) [17], and Chebyshev Polynomials (CP) [4].", "startOffset": 342, "endOffset": 345}, {"referenceID": 14, "context": "In [16] and [27] a method of multi resolution representation of time series is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "In [16] and [27] a method of multi resolution representation of time series is presented.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "In [19] and [20] other multi-resolution method are proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [19] and [20] other multi-resolution method are proposed.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "The technique presented in [22] couples and fast-and-dirty filter with a multi-resolution representation of the time series.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "One of its main advantages is that symbolic representation permits researchers to benefit from the ample symbolic algorithms known in the text-retrieval and bioinformatics communities [14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "But in general, most of these symbolic representation methods suffered from two main inconveniences [15]; the first is that the dimensionality of the symbolic representation method is the same as that of the original space, so there is no virtual dimensionality reduction.", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "The Symbolic Aggregate approXimation method (SAX) [14] is one of the most important symbolic representation methods of time series.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "2-The dimensionality of the time series is reduced using PAA [9], [30] 3-The PAA representation of the time series is discretized by determining the number and location of the breakpoints (The number of the breakpoints is chosen by the user).", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "2-The dimensionality of the time series is reduced using PAA [9], [30] 3-The PAA representation of the time series is discretized by determining the number and location of the breakpoints (The number of the breakpoints is chosen by the user).", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "It is proven in [9], [30] that the above similarity distance is a lower bound of the Euclidean distance applied in the original space of time series.", "startOffset": 16, "endOffset": 19}, {"referenceID": 28, "context": "It is proven in [9], [30] that the above similarity distance is a lower bound of the Euclidean distance applied in the original space of time series.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "There are other versions and extensions of SAX [11], [25], [26], [28].", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "There are other versions and extensions of SAX [11], [25], [26], [28].", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "There are other versions and extensions of SAX [11], [25], [26], [28].", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "There are other versions and extensions of SAX [11], [25], [26], [28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "These versions use it for other applications or apply it to index massive datasets, or compute MINDIST differently [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "Quantifying the content of information a vector carries was first introduced by Shannon in [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "DFT [2], [3] and DWT [5], for instance are based on the fact that the first coefficients are the most meaningful ones; i.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "DFT [2], [3] and DWT [5], for instance are based on the fact that the first coefficients are the most meaningful ones; i.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "DFT [2], [3] and DWT [5], for instance are based on the fact that the first coefficients are the most meaningful ones; i.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "APCA [10] segments the time series into segments of varying lengths such that their individual reconstruction errors are minimal.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "In [29] the authors proposed setting the weights using relevance feedback provided by the user.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "PSO was inspired by the social behavior of some animals, such as bird flocking or fish schooling [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "[23] proposed a model that simulates a swarm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "In the following we present a standard PSO [7].", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "Exploration is defined as the act of searching for the purpose of discovery, and exploitation is defined as the act of utilizing something for any purpose [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 29, "context": "Diversity in PSO comes from two sources [31]; the first is the difference between the particle\u2019s current position and that of its best neighbor, and the other is the difference between the particle\u2019s current position and its best historical position.", "startOffset": 40, "endOffset": 44}, {"referenceID": 5, "context": "This repository makes up between 90% and 100% of all publicly available, labeled time series data sets in the world, and it represents the interest of the data mining/database community, and not just one group [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 18, "context": "A possible future work will be to associate the work presented in this paper with the work presented in [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "This can be achieved in two ways; the first is to use the optimization scheme presented in [20] to locate the breakpoints then to use the optimization scheme presented in this work to set different weights to different segments according to their information content.", "startOffset": 91, "endOffset": 95}], "year": 2012, "abstractText": "Bio-inspired optimization algorithms have been gaining more popularity recently. One of the most important of these algorithms is particle swarm optimization (PSO). PSO is based on the collective intelligence of a swam of particles. Each particle explores a part of the search space looking for the optimal position and adjusts its position according to two factors; the first is its own experience and the second is the collective experience of the whole swarm. PSO has been successfully used to solve many optimization problems. In this work we use PSO to improve the performance of a well-known representation method of time series data which is the symbolic aggregate approximation (SAX). As with other time series representation methods, SAX results in loss of information when applied to represent time series. In this paper we use PSO to propose a new minimum distance WMD for SAX to remedy this problem. Unlike the original minimum distance, the new distance sets different weights to different segments of the time series according to their information content. This weighted minimum distance enhances the performance of SAX as we show through experiments using different time series datasets.", "creator": "PScript5.dll Version 5.2.2"}}}