{"id": "1312.4314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Learning Factored Representations in a Deep Mixture of Experts", "abstract": "mixtures configurations of experts combine the distinct outputs software of several \" expert \" instructional networks, each of which specializes in testing a different part factor of the input space. this is achieved approximately by training a \" standard gating \" network that maps throughout each intermediate input to a distribution consistent over the experts. both such models show the promise algorithms for building larger networks that are still without cheap instruments to compute at test setting time, and more more parallelizable at training school time. just in this instance this work, that we extend under the mixture of specific experts tuning to a common stacked single model, the deep mixture of experts, alternating with seemingly multiple mixture sets of gating problems and combining experts. this exponentially progressively increases along the number of effective combinations experts by associating virtually each differing input with a combination list of experts at matching each layer, yet arguably maintains a modest model size. on realizing a randomly translated overlapping version of the mnist dataset, we find shows that understanding the deep mixture group of experts automatically learns skills to jointly develop location - relationship dependent ( \" we where \" ) expertise experts at having the first layer, inexperienced and new class - specific ( \" what \" ) competent experts at the original second matching layer. sometimes in addition, we see that realizing the different combinations results are in short use readily when the model software is applied through to a multiple dataset of discrete speech monophones. these coefficients demonstrate effective use mechanisms of all expert combinations.", "histories": [["v1", "Mon, 16 Dec 2013 11:15:10 GMT  (1495kb,D)", "http://arxiv.org/abs/1312.4314v1", null], ["v2", "Wed, 19 Feb 2014 17:57:53 GMT  (1496kb,D)", "http://arxiv.org/abs/1312.4314v2", null], ["v3", "Sun, 9 Mar 2014 20:15:03 GMT  (1496kb,D)", "http://arxiv.org/abs/1312.4314v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david eigen", "marc'aurelio ranzato", "ilya sutskever"], "accepted": false, "id": "1312.4314"}, "pdf": {"name": "1312.4314.pdf", "metadata": {"source": "CRF", "title": "Learning Factored Representations in a Deep Mixture of Experts", "authors": ["David Eigen", "Ilya Sutskever"], "emails": ["deigen@cs.nyu.edu", "ranzato@fb.com", "ilyasu@google.com"], "sections": [{"heading": null, "text": "Mixtures of Experts combine the outputs of several \u201cexpert\u201d networks, each of which specializes in a different part of the input space. This is achieved by training a \u201cgating\u201d network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (\u201cwhere\u201d) experts at the first layer, and class-specific (\u201cwhat\u201d) experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations."}, {"heading": "1 Introduction", "text": "Deep networks have achieved very good performance in a variety of tasks, e.g. [7, 2, 1]. However, a fundamental limitation of these architectures is that the entire network must be executed for all inputs. This computational burden imposes limits network size. One way to scale these networks up while keeping the computational cost low is to increase the overall number of parameters and hidden units, but use only a small portion of the network for each given input. Then, learn a computationally cheap mapping function from input to the appropriate portions of the network.\nThe Mixture of Experts model [4] is a continuous version of this: A learned gating network mixes the outputs of N \u201cexpert\u201d networks to produce a final output. In this work, we extend the Mixture of Experts to use a different gating network at each layer in a multilayer network, forming a Deep Mixture of Experts (DMoE). This increases the number of effective experts by introducing an exponential number of paths through different combinations of experts at each layer. By associating each input with one such combination, our model uses different subsets of its units for different inputs. Thus it can be both large and efficient at the same time.\nWe demonstrate the promise of this approach by evaluating it on two datasets. Using a jittered MNIST dataset, we show that the DMoE learns to factor different aspects of the data representation at each layer (specifically, location and class), making effective use of all paths. We also find that all combinations are used when applying our model to a dataset of speech monophones.\n\u2217Marc\u2019Aurelio Ranzato currently works at the Facebook AI Group.\nar X\niv :1\n31 2.\n43 14\nv1 [\ncs .L\nG ]\n1 6\nD ec"}, {"heading": "2 Related Work", "text": "A standard Mixture of Experts (MoE) [4] learns a set of expert networks fi along with a gating network g. Each fi maps the input x to C outputs (one for each class c = 1, . . . , C), while g(x) is a distribution over experts i = 1, . . . , N that sums to 1. The final output is then given by Eqn. 1\nFMoE(x) = N\u2211 i=1 gi(x)softmax(fi(x)) (1)\n= N\u2211 i=1 p(ei|x)p(c|ei, x) = p(c|x) (2)\nThis can also be seen as a probability model, where the final probability over classes is marginalized over the selection of expert: setting p(ei|x) = gi(x) and p(c|ei, x) = softmax(fi(x)), we have Eqn. 2.\nA product of experts (PoE) [3] is similar, but instead combines the log probabilities to form a product:\nFPoE(x) \u221d N\u220f i=1 softmax(fi(x)) = N\u220f i=1 pi(c|x) (3)\nAlso closely related to our work is the Hierarchical Mixture of Experts [6], which learns a hierarchy of gating networks in a tree structure. Each expert network\u2019s output corresponds to a leaf in the tree; the outputs are then mixed according to the gating weights at each node.\nOur model differs from each of these three models because it dynamically assembles a suitable expert combination for each input. By conditioning our gating and expert networks on the output of the previous layer, our model can express an exponentially large number of effective experts."}, {"heading": "3 Approach", "text": "To extend MoE to a DMoE, we introduce two sets of experts with gating networks (g1, f1i ) and (g2, f2j ), along with a final linear layer f\n3 (see Fig. 1). The final output is produced by composing the mixtures at each layer:\nz1 = N\u2211 i=1 g1i (x)f 1 i (x)\nz2 = M\u2211 j=1 g2j (z 1)f2j (z 1)\nF (x) = z3 = softmax(f3(z2))\nWe set each f li to a single linear map with rectification, and each g l i to two layers of linear maps with rectification (but with few hidden units); f3 is a single linear layer. See Section 4 for details.\nWe train the network using stochastic gradient descent (SGD) with an additional constraint on gating assignments (described below). SGD by itself results in a degenerate local minimum: The experts at each layer that perform best for the first few examples end up overpowering the remaining experts. This happens because the first examples increase the gating weights of these experts, which in turn causes them to be selected with high gating weights more frequently. This causes them to train more, and their gating weights to increase again, ad infinitum.\nTo combat this, we place a constraint on the relative gating assignments to each expert during training. Let Gli(t) = \u2211t t\u2032=1 g l i(xt\u2032) be the running total assignment to expert i of layer l at step t, and\nlet G\u0304l(t) = 1N \u2211N i=1 G l i(t) be their mean (here, xt\u2032 is the training example at step t\n\u2032). Then for each expert i, we set gli(xt) = 0 if G l i(t) \u2212 G\u0304l(t) > m for a margin threshold m, and renormalize the distribution gl(xt) to sum to 1 over experts i. This prevents experts from being overused initially, resulting in balanced assignments. After training with the constraint in place, we lift it and further train in a second fine-tuning phase."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Jittered MNIST", "text": "We trained and tested our model on MNIST with random uniform translations of\u00b14 pixels, resulting in grayscale images of size 36 \u00d7 36. As explained above, the model was trained to classify digits into ten classes.\nFor this task, we set all f1i and f 2 j to one-layer linear models with rectification, f 1 i (x) = max(0,W 1i x + b 1 i ), and similarly for f 2 j . We set f\n3 to a linear layer, f3(z2) = W 3z2 + b3. We varied the number of output hidden units of f1i and f 2 j between 20 and 100. The final output from f3 has 10 units (one for each class).\nThe gating networks g1 and g2 are each composed of two linear+rectification layers with either 50 or 20 hidden units, and 4 output units (one for each expert), i.e. g1(x) = softmax(B \u00b7max(0, Ax+ a) + b), and similarly for g2.\nWe evaluate the effect of using a mixture at the second layer by comparing against using only a single fixed expert at the second layer, or concatenating the output of all experts. Note that for a mixture with h hidden units, the corresponding concatenated model has N \u00b7h hidden units. Thus we expect the concatenated model to perform better than the mixture, and the mixture to perform better than the single network. In each case, we keep the first layer architecture the same (a mixture). We also compare the two-layer model against a one-layer model in which the hidden layer z1 is mapped to the output units through linear layer and softmax."}, {"heading": "4.2 Monophone Speech", "text": "In addition, we ran our model on a dataset of monophone speech samples. This dataset is a random subset of approximately one million samples from a larger proprietary database of several hundred hours of US English data collected using Voice Search, Voice Typing and read data [5]. For our experiments, each sample was limited to 11 frames spaced 10ms apart, and had 40 frequency bins. Each input was fed to the network as a 440-dimensional vector. There were 40 possible output phoneme classes.\nWe trained a model with 4 experts at the first layer and 16 at the second layer. Both layers had 128 hidden units. The gating networks were each two layers, with 64 units in the hidden layer. As before, we evaluate the effect of using a mixture at the second layer by comparing against using only a single expert at the second layer, or concatenating the output of all experts."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Jittered MNIST", "text": "Table 1 shows the error on the training and test sets for each model size (the test set is the MNIST test set with a single random translation per image). In most cases, the deeply stacked experts performs between the two baselines on the training set, as expected. However, the deep models often suffer from overfitting: the mixture\u2019s error on the test set is worse than that of the single expert for two of the four model sizes.\nIn Fig. 2, we show the mean assignment to each expert (i.e. the mean gating output), both by input translation and by class. The first layer assigns experts according to translation, while assignment is uniform by class. Conversely, the second layer assigns experts by class, but is uniform according to translation. This shows that the two layers of experts are indeed being used in complementary ways, so that all combinations of experts are effective. The first layer experts become selective to where the digit appears, regardless of its membership class, while the second layer experts are selective to what the digit class is, irrespective of the digit\u2019s location.\nFinally, Fig. 3 shows the nine test examples with highest gating value for each expert combination. First-layer assignments run over the rows, while the second-layer runs over columns. Note the translation of each digit varies by rows but is constant over columns, while the opposite is true for the class of the digit. Furthermore, easily confused classes tend to be grouped together, e.g. 3 and 5."}, {"heading": "Training Set Error: Jittered MNIST Model Single Expert Mixed Experts Concatenated Experts", "text": ""}, {"heading": "Test Set Error: Jittered MNIST Model Single Expert Mixed Experts Concatenated Experts", "text": ""}, {"heading": "Jittered MNIST: Two-Layer Deep Model", "text": ""}, {"heading": "5.2 Monophone Speech", "text": "Table 2 shows the error on the training and test sets. As was the case for MNIST, the mixture\u2019s error on the training set falls between the two baselines. In this case, however, test set performance is about the same for both baselines as well as the mixture.\nFig. 4 shows the 16 test examples with highest gating value for each expert combination (we show only 4 experts at the second layer due to space considerations). As before, first-layer assignments run over the rows, while the second-layer runs over columns. While not as interpretable as for MNIST, each expert combination appears to handle a distinct portion of the input. This is further bolstered by Fig. 5, where we plot the average number of assignments to each expert combination. Here, the choice of second-layer expert depends little on the choice of first-layer expert."}, {"heading": "Training Set Error: Monophone Speech Model Single Expert Mixed Experts Concatenated Experts", "text": ""}, {"heading": "Test Set Error: Monophone Speech Model Single Expert Mixed Experts Concatenated Experts", "text": ""}, {"heading": "6 Conclusion", "text": "The Deep Mixture of Experts model we examine is a promising step towards developing large, sparse models that compute only a subset of themselves for any given input. We see precisely the gating assignments required to make effective use of all expert combinations: for jittered MNIST, a factorization into translation and class, and distinctive use of each combination for monophone speech data. However, we still use a continuous mixture of the experts\u2019 outputs rather than restricting to the top few. Such an extension is necessary to fulfill our goal of using only a small part of the model for each input; we hope to address this in future work."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Matthiew Zeiler for his contributions on enforcing balancing constraints during training."}, {"heading": "Monophone Speech: Conditional Assignments", "text": ""}], "references": [{"title": "Flexible", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "high performance convolutional neural networks for image classification. In IJCAI", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "ICASSP", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Products of experts", "author": ["G.E. Hinton"], "venue": "ICANN, 1:1\u20136", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S. Nowlan", "G.E. Hinton"], "venue": "Neural Computation, 3:1\u201312", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Application of pretrained deep neural networks to large vocabulary speech recognition", "author": ["N. Jaitly", "P. Nguyen", "A. Senior", "V. Vanhoucke"], "venue": "Interspeech", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural Computation, 6:181\u2013214", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "[7, 2, 1].", "startOffset": 0, "endOffset": 9}, {"referenceID": 1, "context": "[7, 2, 1].", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[7, 2, 1].", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "The Mixture of Experts model [4] is a continuous version of this: A learned gating network mixes the outputs of N \u201cexpert\u201d networks to produce a final output.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "A standard Mixture of Experts (MoE) [4] learns a set of expert networks fi along with a gating network g.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "A product of experts (PoE) [3] is similar, but instead combines the log probabilities to form a product:", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "Also closely related to our work is the Hierarchical Mixture of Experts [6], which learns a hierarchy of gating networks in a tree structure.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "This dataset is a random subset of approximately one million samples from a larger proprietary database of several hundred hours of US English data collected using Voice Search, Voice Typing and read data [5].", "startOffset": 205, "endOffset": 208}], "year": 2017, "abstractText": "Mixtures of Experts combine the outputs of several \u201cexpert\u201d networks, each of which specializes in a different part of the input space. This is achieved by training a \u201cgating\u201d network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent (\u201cwhere\u201d) experts at the first layer, and class-specific (\u201cwhat\u201d) experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.", "creator": "LaTeX with hyperref package"}}}