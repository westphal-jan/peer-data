{"id": "1312.4706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2013", "title": "Designing Spontaneous Speech Search Interface for Historical Archives", "abstract": "producing spontaneous speech in the the form of conversations, meetings, voice - driven mail, archival interviews, oral history, etc. is one of the most very ubiquitous forms of human forensic communication. search engines exist providing access to such speech collections that have the several potential applications to electronically better inform entire intelligence data and make relevant data representations over vast audio / video archives strategically available visually to end users. this project presents practically a distinctive search user interface interface design towards supporting basic search oriented tasks over simultaneously a traditional speech collection presentation consisting actually of holding an historical news archive composed with nearly total 52, 000 audiovisual testimonies of deportation survivors dragged and murdered witnesses of the 1989 holocaust and seven other genocides. the design which incorporates faceted acoustic search, features along with other ui elements like distinguishing highlighted search items, tags, snippets, etc., to fully promote linguistic discovery and reliable exploratory search. here two different designs have been created adapted to fully support both manual and automated transcripts. evaluation techniques was performed using older human search subjects to measure accuracy in retrieving incorrect results, deeper understanding user - driven perspective on choosing the design elements, highlight and reduce ease delays of parsing information.", "histories": [["v1", "Tue, 17 Dec 2013 10:18:44 GMT  (851kb)", "http://arxiv.org/abs/1312.4706v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.CL", "authors": ["donna vakharia", "rachel gibbs"], "accepted": false, "id": "1312.4706"}, "pdf": {"name": "1312.4706.pdf", "metadata": {"source": "CRF", "title": "Designing Spontaneous Speech Search Interface for Historical Archives", "authors": ["Donna Vakharia", "Rachel Gibbs"], "emails": ["donna@cs.utexas.edu", "rachel.gibbs@utexas.edu"], "sections": [{"heading": null, "text": "Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding userperspective on the design elements, and ease of parsing information.\nKeywords Speech Search, Search Interfaces, Faceted Metadata, Automatic Speech Recognition, Oral History"}, {"heading": "1. INTRODUCTION", "text": "The ease of capturing and encoding of audiovisual data has resulted in the production of a massive speech collection with limited tools to analyze the data effectively. As the magnitude and use of such content grows, efficient ways to automatically find relevant segments becomes necessary. While there are efficient search engines for text documents present today, there are no satisfactory systems for performing search over audiovisual data as these systems do not perform any detailed analysis for them [11].\nWe know that core audio-video dimension of oral history is notoriously underutilized [3]. Historical archives provide rich information through the vast set of data. Even for the properly catalogued audiovisual collection, all the efforts in collecting this dataset would be of little value if they fail to be easily accessible by the target audience. These collections would be rendered unlistened and unwatched in such a scenario. The content is rarely organized, indexed or searchable/browsable in any useful way. Because of which the considerable potential of\naudio and video documents to support high-impact, vivid, thematic, and analytic engagement with meaningful issues, personalities, and contexts, is largely untapped [3]. These archives can be made usable by representing them in text by either manual transcription which is quite expensive. Or automated methods or Automatic speech recognition (ASR) tools can be used to convert to text. Text is easier to read, scan, browse, search, publish, display, and distribute. Audio or video documents, in contrast, inevitably have to be experienced in real time.\nThe Survivors of the Shoah Visual History Foundation (VHF) has created an archive that contains interviews of nearly 52,000 survivors and other witnesses of the Holocaust. The institute interviewed Jewish, homosexual, and Jehovah\u2019s Witness, and Roma and Sinti (Gypsy) survivors in addition to liberators and liberation witnesses, political prisoners, rescuers and aid providers, survivors of Eugenics policies, and war crimes trials participants [8]. Approximately 25000 of the collected interviews are in English, 7000 are in Russian, and 575 are in Czech. Presently USC\u2019s Visual History Archive Online enables researchers to perform detailed searches for relevant testimonies and segments of testimonies but with no access to the transcripts.\nThis study presents a search system using the VHF collection that enables users to perform content-based search and retrieve the transcript of the audio relevant to the query. Since manual transcription can be very expensive and time consuming, we try to evaluate the alternative solution by using auto-generated transcripts. However, since transcripts generated by ASR are usually of bad quality and disregards disfluencies, misspellings, sentence end annotations, speaker change annotations, etc. we harness the use of various design elements to enhance readability of the transcripts. We further evaluate user efficiency in finding relevant results using them as opposed to manual transcripts."}, {"heading": "1.1 Proposed Solution", "text": " Design a search system that would enable users to\nperform content-based search and to retrieve the transcript of the interview relevant to the query.\n Support search tasks over the content vs. the tags, keywords associated with the file.\n Support faceted search to help users perform exploratory search and enable discovery.\n Utilize elements such as keyword highlighting, snippets, category search, tags, etc., to enhance user\nexperience."}, {"heading": "1.2 Intended users", "text": "The intended users for the system are majorly historians, academicians, researchers, genealogist, students, and historyenthusiasts. This intended user group includes people of various levels of domain and technical expertise, from experts to novices. Since the collection focuses on a topic of global interest, users will vary in terms of native language, knowledge of terminology, and education levels."}, {"heading": "1.3 Supported search tasks", "text": "Exploratory and Iterative Search, Re-finding, and Known Item Search will be supported, but Known Item Search will be less emphasized as it will be a less-common use case.\nThe remainder of the paper is organized as follows. Section 2 explains the motivation behind this study. Prior work on data evaluation of design of search systems is briefly reviewed in Section 3. Interface design elements are described in Section 4. Section 5 enlists our research hypothesis that we test using pre-defined methodology, presented in Section 6. Evaluation is performed on human subjects and results obtained from them in terms of self-reported feedback, and calculated measures is presented in Section 7. Finally, Section 8 discusses the research areas that need further investigation in future, followed by conclusion in Section 9."}, {"heading": "2. MOTIVATION", "text": "The researchers were motivated to design for this data set for a number of reasons. Both researchers had previously studied the subject extensively and had personal connections to the subject through either cultural exposure or familial relation to survivors and victims. One researcher was already involved in a research activity using this same data set, and a relative\u2019s interview of the other was included in this survivor-testimony collection, conducted by the USC Shoah Foundation Institute. Therefore, both researchers had an interested in increasing access to the audiovisual interviews and transcripts. This collection is extensive and was collected as quickly as possible to preserve the first-hand accounts of witnesses and survivors before they were lost forever.\nThe ease of capture and encoding of audiovisual data has caused a massive amount of speech collection to be produced with limited tools to analyze them effectively. While there are efficient search engines for text documents present today, there are no satisfactory systems for performing search over audiovisual data. This project sought to design a search system that would enable users to perform content-based search and to retrieve the transcript of the interview relevant to the query. Due to the vastness of the collection, human transcription is laborious, time-consuming, and expensive. For this reason, alternate methods of making the texts of the collection available are explored, such as automatic transcription using a variety of algorithms. In addition to the elements of the user interface improving accessibility to the collection, the readability of transcriptions must also be considered and tested. The design of this interface and testing of user comprehension of automatic transcription against human\ntranscription seeks to accommodate any user with access to the collection, from expert researchers to survivors or their families to students seeking to learn and discover about the tragic event upon which this collection focuses."}, {"heading": "3. RELATED WORK", "text": "Various studies and experiments have been performed to address the problem of finding relevant information from vast oral/visual archives. Jones et al. [2] performed a psycholinguistic study measuring readability of several types of speech transcripts. They gauged readability by measuring accuracy of answers to comprehension questions, reactiontime for passage reading, reaction-time for question answering and a subjective rating of passage difficulty. However, the data source for their study included texts from conversational telephone speech, and broadcast news speech and was based completely on automatic transcripts. A study comparing the effects of manual vs. automatic transcripts still remains unexplored.\nThe Informedia Digital Video Library Project at Carnegie Mellon University is creating a digital library of text, images, videos and audio data available for full content retrieval [4]. Hauptmann et al. built a system by integrating technologies involved in creating a digital video library suitable for fullcontent search and retrieval [6]. They use image processing to analyze scenes, speech processing to transcribe audio signal, and natural language processing to determine word relevance. Doing so, they try to overcome limitations of each technology [5].\nCoden et al. also try to address the problem of finding pertinent information for visual data [7]. However their problem includes performing this in real time. To tackle the issue with having transcripts with low accuracy they develop algorithms to extract the essence of video and use distanceranking to select relevant information from the search results. Their evaluation involved data analysis using quantitative methods alone. Also, their system is designed to handle broadcast news data. Such automated techniques to search broadcast news are difficult to apply for larger collections of spontaneous speech.\nAn experimental speech-based search engine, SpeechBot, uses automatic transcripts for indexing and in turn uses searchable index to provide an ability to play the segments of interest within the audio file [11]. Many other studies [4, 13, and 14] have been carried out to work towards similar goals."}, {"heading": "4. INTERFACE DESIGN", "text": "When ideating for the search user interface (SUI) design, the researchers considered likely user types and the types of information needs and levels of domain expertise (or lackthere-of) of each group. Since the purpose of the interface is to increase access to the transcripts of audiovisual interviews with survivors and witnesses of the Holocaust and other genocides, a widely studied topic, the user types varied greatly and included: historians/scholars, teachers (any level), students (any level), genealogists, and anyone who has an interest in studying primary source material for this topic. To accommodate users with different levels of domain expertise, the SUI utilized various elements to support information\nretrieval and discovery. The desired elements were identified and explored during an iterative design process.\nIn addition to a summary about the searchable collection, the SUI landing page presents several ways in which users can instigate search. For the search bar, an autosuggest feature (figure 1) was especially useful as the data set includes many non-English terms and terms with numerous spellings.\nWhile the search bar serves to instigate search for users with a known information need, also introduced on the landing page are a several search elements which persist throughout the interface to support exploratory search where the information need is unknown or unclear. These include: Popular Searches (determined by what other users search), Suggested Searches (curated search list), Featured Interview that contained dynamic excerpt from transcription text (figure 2), and a Browse By category filter (also used as faceted search).\nArriving on a search engine results page (SERP), users could take several options:\n(1) select/explore the search results presented on the page, (2) narrow their search using Time Period (1930-1938, 1939- 1945, 1946-present), Location (Western Europe, Central Europe, Eastern Europe, Southern Europe, Scandinavia), or Subject filters (Kristallnacht, Resettlement & deportation, Concentration & labor camps, Ghettos, Pogroms, Persecuted groups, Death squads, Extermination camps, Jewish resistance, Escapes, Death marches, Liberation) from the left hand facets, (3) reformulate the existing query which persisted in the search bar, or (4) start a new search by selecting the featured interview, a Popular Searches topic, or a topic from Suggested Searches.\nThe search results listings presented several different elements to assist users in evaluating the perceived relevancy of results to their information needs. These included: a hyperlinked title (with descriptiveness dependent upon whether the associated result was human or machine-transcribed), descriptive text (either human-created summary or excerpt from the interview transcript) with highlighted relevant words (either from search query terms or category filters), a snippet discoverable upon mouseover (figure 4) that included a longer excerpt from the transcript with more keyword highlights, search result listings also offered users a hyperlink to go directly to audio or video recording of the interview. All of these features sought to help users evaluate whether or not a particular result was likely to match the information need as much as possible before clicking on and evaluating the result itself.\nOnce clicked, the result presented either a human or automatic transcription of a witness or survivor testimony [please see Appendix for complete screenshots]. Certain elements were presented on both versions of transcripts such as back navigation to the search results, a list of frequency tags, and links to the audio and video recordings which could be utilized as a companion to the transcriptions. The formatting and readability of the transcription text themselves varied depending on the method of transcription. Human-transcribed texts were more readable, presented clear indications of speaker changes. Machine transcripts presented color-coded spontaneous speech markup such as disfluency, sentence, and speaker markers (figure 5, 6)."}, {"heading": "5. RESEARCH HYPOTHESES", "text": "The study sought to evaluate the proposed designs of the SUI via a user study. This study was conducted at the University of Texas at Austin, School of Information with a convenience sample of 8 users of mixed ethnographic backgrounds, levels of domain expertise, and familiarity with database/archival search interfaces. The study sought to test 3 hypotheses regarding the users\u2019 interactions and perceptions of the presentation of information in the SUI and search result documents:\nHypothesis 1 (H1): Users will prefer human-transcribed search results, as opposed to those generated automatically by machine.\nHypothesis 2 (H2): As the level of disfluency is reduced and sentence markers are added, users will be able to judge perceived relevance of presented search results more quickly and with greater confidence.\nHypothesis 3 (H3): Marking speaker changes within the text of the transcripts will improve user understanding and search efficiency, leading users to find desired results in a shorter amount of time (contrasted with transcriptions without marked speaker changes)."}, {"heading": "6. METHODOLOGY", "text": "In this study, users were guided through a short series of tasks designed to gain feedback regarding search user interfaces and clarity of information presentation. Tasks focused on describing impressions, understanding of information architecture, entering pre-defined search queries, refining search queries, and evaluating the ease of parsing the\npresented search artifacts. Morae1 software was used to capture user interactions during the evaluation phase via screen, and audio recording for Tasks 1, 2, and 3. The evaluation phase was divided into following tasks (figure 7).\nTask 0 \u2013 Signing pre-consent form and completing pretask questionnaire: All users signed an informed consent form and were given a copy for their records. Users completed a digital questionnaire collecting information concerning ethnographic details and educational background before beginning the actual tasks. No personally identifiable information was recorded during the study.\nObjectives: To gather information regarding user\u2019s ages, educational backgrounds, ethnographic details, and gauge level of familiarity with search engines, archives, and prior experience researching on Holocaust (if any).\nTask 1 \u2013 Search by keyword using SERP A: Users were assigned a search task to find a relevant result for the topic \"Pre-war religious festival observance\u201d. Although not informed of the data source for the transcriptions of this search task, Task 1 presented users with the SERP from \u201cGood\u201d or human-transcribed search results. Users were then given the following scenario for their information need:\nYou are trying to find information about how religious festivals in pre-war Europe were observed. You are particularly interested in observance of three main festivals:\n1. Passover\n2. Shavuot\n3. Sukkot/Shemini Arzeret/Simchat Torah\nYou are NOT interested in the weekly Sabbath (Shabbos/Shabbat) or in the High Holy Days (Rosh Hashanah/Yom Kippur).\nTo enable this task, they were presented with a search engine results page (SERP) that included tags, snippets, and links to search results documents that were constructed using manual transcripts in this case.\nTask 2 \u2013 Search by keyword using SERP B: In this task, this time users were presented with a different SERP that was\n1 http://www.techsmith.com/morae.html\ncreated using automatic transcripts. In both cases, information regarding data sources (whether transcripts are manual or auto generated) was not revealed. In this case users had to search for information on the topic \u201cLife in the concentration camp\u201d amongst the displayed results. For Task 2, users were shown the SERP from \u201cBad\u201d or machine-transcribed search results and given the following scenario for their information need:\nYou are looking for a description regarding life in the concentration camps that may also report arrival, selection, work, the cold weather, the famine, etc.\nObjectives for Task 1 & 2:\n Perform A/B testing to evaluate and compare efficiency for two SERPs.\n Specifically, to analyze the results based on various criteria such as accuracy, time taken to complete the tasks, and number of clicks required before reaching\nthe final answer.\nTask 3 \u2013 Exploring Search Engine Design: Users were asked to go through the search interface from the homepage and to provide feedback via think-aloud observation. First, they were brought to the landing page and told to describe their overall impressions of the presentation of information architecture and state what actions they understood could be performed from the landing page. They were also asked to describe their observations of various search elements such as Suggested Searches, Popular Searches, Featured Interviews, filters based on faceted metadata, descriptions about the collection, etc. Thereafter, they were asked to perform a simple search and later asked to narrow it down based on Time Period and Location. Also, user feedback on SERP was also noted.\nObjectives:\n Understand whether the search actions were obvious to the users\n Understand whether the users correctly interpreted the purpose of \u201cPopular Searches\u201d and \u201cSuggested\nSearches\u201d and could distinguish between them\n Analyze the ease with which users can complete a simple search task and check discoverability and use\nof filters\n Gauge effectiveness of tags and snippets while browsing through the results\n Understand if the users want to use to the search bar or the side filters to perform a search task\nTask 4 \u2013 Post-task Questionnaire: After performing the above tasks, users completed a questionnaire rating their experience and ease of use overall and individual tasks. Feedback regarding design elements and suggestions to improve the design were captured in this digital questionnaire.\nObjectives:\n To record user feedback on ease of performing tasks, rating helpfulness of design elements like sentence\nmarkers, speaker change markers, disfluency markers, hover maps, and tags.\n To gain insight into user reasoning while choosing either tasks as easier to perform by use of open ended\nquestions.\n Capture suggestions to improve interface design to enable faster search of relevant results."}, {"heading": "7. RESULTS", "text": "User Population:\nSeveral representative user groups with different levels of domain expertise were identified and a total of 8 users participated in this study. All these users were students living in US. Out of them, 5 were pursuing graduate degrees, and 3 pursuing bachelors. 4 users had completed high school in India, 3 in the U.S., and 1 in China. Only 3 users had prior experience researching on Holocaust but did not highly rate their familiarity with the topic. The rest of the users reported little familiarity.\nA/B Testing:\nIn this study, we test two variations of the interface to perform similar task (i.e. finding relevant result). However, the users were given different query each time so that they do not use the information gained from task 1 in task 2.\nInformation regarding time taken to complete the task, number of clicks, and other behavior were recorded for Task 1 and Task 2. Our original design included brief summary for each search results and also the snippet in Task 2. However, while performing evaluation with the first user, we observed that user was making judgment based on summary itself while making relevance judgment and avoided going through the interview transcript. So we modified the design to remove summary from search results and modified the snippet to include sentences that appear nearby the search keywords. Hence, this evaluation includes data from the remaining users\nto measure accuracy and analyzing ease of performing the tasks. But, we do incorporate feedback obtained from that user during Task 3, and 4.\nAll 7 users were able to complete Task 1 correctly. However, for task 2, only 3 users were able to do it identify the relevant result. Figure 9 presents the percentage of accuracy achieved by comparing correct and incorrect responses obtained from users during tasks 1 and 2.\nFigure 10 presents a distribution comparing time taken by the users to complete tasks 1 and 2.\nIt can be observed that task 1 was relatively easier to complete considering 100% accuracy and relatively lesser time taken to find relevant result with the exception of 2 users. Although this perception may have been influenced by Task 1 providing key words (Passover, Sukkot, Shavuot, etc.) while Task 2 did not provide users with explicit keywords, users found the transcripts from Task 1 to be more readable and completed Task 1 with more confidence that they had selected a relevant result.\nFeedback obtained from the post-task questionnaire showed that four users reported that they were able to make decisions using the snippet itself for Task 1. Whereas, for task 2 only three users reported so. All the users felt that readability of the transcript affected the time taken to find relevant search results. Two users rated Task 1 as \u201cQuite difficult\u201d, while the rest found it of average to easy level. In case of Task 2, two\nusers rated it as \u201cQuite difficult\u201d, three rated it as \u201cAverage difficulty\u201d and two found it be easy. Following features were considered helpful in determining results:\n Keyword highlighting\n Presence of tags with keyword frequency\n Readability of the transcript\nAccording to the post-task questionnaire feedback, participants looked to keywords and tags to provide them with contextual clues about the relevancy of results. In Task 1, seeing one of the given subjects as a highlighted keyword led users to decided on the correct result. However, in Task 2, users had to infer what keywords might be from the described information need. This may led them to focus on less relevant highlighted keywords.\nMarkers in Task 2 (Figure 11)\nSince transcripts used for task 2 were generated using ASR (Automatic Speech Recognition), several markers such as end of sentence markers, speaker change markers, and disfluency markers were inserted in the transcript body to enhance readability.\nEnd of sentence change markers : End of sentences were marked using <s> tag in blue. Two users found these markers to be very useful, three of average usefulness, while two users did not find them helpful.\nSpeaker change markers : To distinguish between speakers (interviewer and interviewee) <spk#> tags were used in green, where # was replaced by 1 in case of interviewer and by 2 in case of interviewee. Two users found these markers to be useful, three of average usefulness, while two users did not find them helpful.\nDisfluency markers : To highlight misspelled, repeated and other disfluencies in the text, <<text>> tags were used in dark red, for e.g. <<uhhuh>>, <<of of>>. Two users found these markers to be useful, two of average usefulness, while three users did not find them helpful.\nExploration:\nDuring Exploration, participants were given a scenario stating that they were writing a research paper on the subject of \u201cchild survivors of the Holocaust.\u201d Participants were then introduced to the landing page of the prototype and asked how they would begin to find information that satisfied the specified information need. Users explored various design elements across the search engine. The purpose of \u201cAbout the Collection\u201d was deemed useful by many users, with one user mentioning it as a \u201cgreat starting point for users who are new and want to know more about the collection\u201d. On asking users how they would begin to find information that satisfied their given need, their approaches diverged concerning how to begin the search process. Some users started by entering a keyword query into the search bar, while others wanted to start by selecting one of the Suggested or Popular Searches proffered on the right-hand side of the screen. Users did not tend to start with Browse By categories, although they did note these categories.\nIn the task to narrow down results for World War II, while most of the users immediately went to narrow their search by Time Period in the Browse By filters, some users stated that they would want to reformulate their queries in the search bar to include the keywords \u201cWorld War II\u201d and \u201cHolocaust.\u201d When prompted for a reason, one user cited uncertainty about if it would keep the results or start an entirely new search. After expanding the Time Period filter, most users were uncertain as to which of the three year categories would yield results only for World War II, needing more information than just the numerical years to act. Users who were more familiar with World War II felt comfortable selecting the year category, 1939 - 1945, although this may have been influenced by the change-in-cursor-state when mousing over this selection, as it was the only hyperlinked text. Some also suggested providing filters such as \u201cPre-war\u201d, \u201cWW II\u201d, and \u201cPost-War\u201d instead of year ranges. On checking the SERP with these filters, some users noticed that the number of search results had changed and confirmed that seeing fewer results was what they expected. Users also noticed the addition of the Time-Period tag filter under the search bar.\nUsers were then instructed to narrow their search results again, this time to include results from Poland and Germany. Users immediately looked to Location to narrow their results, including those who stated previously that they would reformulate their query. However, when they saw the options under the Location category (Eastern Europe, Central Europe, Western Europe, Scandinavia), most users were unsure of which filter would include Germany and Poland. Upon discovering the hover map, some users felt that they had enough information to choose Central Europe, while others were still uncertain and needed more explicit information. On arriving on final SERP, all users understood that they could remove one of the filters but keep the other by clicking the \u201cx\u201d on the tag under the search bar. When asked to comment on the presentation of search results, some users noted that the titles of the results were unhelpful and would be significantly more useful if they had a descriptive name, rather than a file name. Users also noticed the tags under each result. Some found this very helpful, while others stated that they would\nhave to narrow their results more because the tags were very similar.\nIn the post-task questionnaire, users suggested several ways to improve search interface design so that the users can find relevant results in a shorter amount of time:\n Adding functionality to pin favorite results that stays on SERP while adding/removing filters\n Emphasize purpose of markers\n Including specific tags\n Having titles describing the transcripts\n Country description in the filters"}, {"heading": "8. FUTURE WORK", "text": "In the future, a more comprehensive representative sample would assist in accurate evaluation of the SUI designs, supported search methods, and presentation of transcription information. Future samples would include domain experts (familiar both with the subject and with using advanced search systems) such as historians and professors; subject-matter experts such as survivors, witnesses, relatives of the formers, and teachers (who may have less experience using search systems); and with domain novices with or without previous experience conducting research using advanced search systems.\nTests with such users would seek to evaluate newly incorporated elements in the SUI (resulting from the current evaluative study) such as more specific labeling and categories and more intuitive interactions such as for facet selection and results refinement. Additionally, researchers would utilize more traditional A/B testing to evaluate different search elements. In the current study, performing separate study for task 1 and 2 may have better addressed the question of efficiency. Though it would have required more number of users and randomizing interface presentation, and since this study involved only 7 users, it was not possible to complete it in present scope of study.\nAdditionally, future studies with Morae will benefit from a more comprehensive set of metrics by which to evaluate efficiency, the presentation of information, and search behaviors of users. Utilize Morae\u2019s native tools will provide excellent means of comparison between user results.\nPossible hypotheses for the future might center around testing different in-text automatic transcription markers, whether placing more visual emphasis on the providing a key to speech markers with the transcripts would affect how users perceived the markers, and if changing the tag presentation in search results from just frequency to something more specific may improve users\u2019 selection of relevant results."}, {"heading": "9. CONCLUSION", "text": "We were able to verify and test our research hypotheses in this study. It was hypothesized in H1 that users would prefer human-transcribed search results, as opposed to automatically generated ones. From our A/B testing, we obtained 100% accuracy in Task 1 vs. 42.8% in Task 2 that helped us conclude that H1 holds true and readability is an important\nfactor aiding users in decision making. Obtaining feedback from users on different markers (end-of-sentence, disfluency, speaker changes) we observed that disfluency markers were quite helpful with users reporting that they would prefer if these highlighted disfluencies can be removed altogether to enhance readability and would not lead to loss of information. But feedback on end-of-sentence markers showed ambiguous results; hence H2 remains inconclusive with the present evaluation. Against our expectations and hypothesis H3, users reported that marking speaker changes did not improve understanding of the underlying text nor did it increase efficiency to perform the search tasks. We were also able to have clear understanding on users\u2019 perspectives on the design elements and ease of parsing information. Overall user experience can be improved by adequate use of UI elements, like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. These interface elements well supported the search task, and helped in query reformulation and narrowing search results. For novice users, presenting popular and suggested search along with brief description about the data collection can serve as a starting point for future searches."}, {"heading": "10. REFERENCES", "text": "[1] Kelly, Diane. \"Methods for evaluating interactive information retrieval systems with users.\" Foundations\nand Trends in Information Retrieval 3.1\u20142 (2009): 1- 224.\n[2] Jones, Douglas A., et al. \"Measuring the readability of\nautomatic speech-to-text transcripts.\" INTERSPEECH. 2003.\n[3] Frisch, Michael. \"Oral history and the digital\nrevolution.\" The Oral History Reader, (2006): 102-114.\n[4] Stevens, Scott, Michael Christen, and Howard Wactlar. \"Informedia: improving access to digital\nvideo.\" Interactions 1.4 (1994): 67-71.\n[5] Hauptmann, Alexander G., and Howard D. Wactlar. \"Indexing and search of multimodal\ninformation.\" Acoustics, Speech, and Signal Processing, 1997. ICASSP-97., 1997 IEEE International Conference on. Vol. 1. IEEE, 1997.\n[6] Hauptmann, A., and M. Smith. \"Text, Speech, and Vision for Video Segmentation: The Informedia TM\nProject.\" AAAI fall symposium, computational models for integrating language and vision. 1995.\n[7] Coden, Anni R., and Eric W. Brown. \"Speech transcript\nanalysis for automatic search.\" System Sciences, 2001.\nProceedings of the 34th Annual Hawaii International Conference on. IEEE, 2001.\n[8] Survivors of the Shoah Visual History Foundation.\n[Online] Available: http://www.vhf.org\n[9] Byrne, William, et al. \"Automatic recognition of spontaneous speech for access to multilingual oral history\narchives.\" Speech and Audio Processing, IEEE Transactions on 12.4 (2004): 420-435.\n[10] Gustman, Samuel, et al. \"Supporting access to large digital oral history archives.\" Proceedings of the 2nd\nACM/IEEE-CS joint conference on Digital libraries. ACM, 2002.\n[11] Van Thong, J-M., et al. \"Speechbot: an experimental\nspeech-based search engine for multimedia content on the web.\" Multimedia, IEEE Transactions on4.1 (2002): 88- 96.\n[12] Lecouteux, Benjamin, et al. \"Imperfect transcript driven speech recognition.\"InterSpeech. 2006.\n[13] Johnson, S. E., et al. \"The Cambridge University spoken\ndocument retrieval system.\" Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on. Vol. 1. IEEE, 1999.\n[14] Woodland, P. C., et al. \"Experiments in broadcast news transcription.\"Acoustics, Speech and Signal Processing,\n1998. Proceedings of the 1998 IEEE International Conference on. Vol. 2. IEEE, 1998.\n[15] Russell-Rose, Tony, and Tyler Tate. Designing the search\nexperience: The information architecture of discovery. Access Online via Elsevier, 2012.\n[16] Yee, Ka-Ping, et al. \"Faceted metadata for image search\nand browsing.\"Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 2003.\n[17] Brin, Sergey, and Lawrence Page. \"The anatomy of a\nlarge-scale hypertextual Web search engine.\" Computer networks and ISDN systems 30.1 (1998): 107-117.\n[18] Tunkelang, Daniel. \"Faceted Search (Synthesis Lectures\non Information Concepts, Retrieval, and Services).\" Morgan and Claypool Publishers (2009).\n[19] White, Ryen W., and Resa A. Roth. \"Exploratory search:\nBeyond the query-response paradigm.\" Synthesis Lectures on Information Concepts, Retrieval, and Services 1.1 (2009): 1-98.\n[20] Wilson, Max L., and Ryen W. White. \"Evaluating\nadvanced search interfaces using established information\u2010seeking models.\" Journal of the American Society for Information Science and Technology 60.7 (2009): 1407-1422."}, {"heading": "11. APPENDIX", "text": "This section includes screenshots of our search engine showing different design elements."}], "references": [{"title": "Methods for evaluating interactive information retrieval systems with users.", "author": ["Kelly", "Diane"], "venue": "Foundations and Trends in Information Retrieval", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Measuring the readability of automatic speech-to-text transcripts.\" INTERSPEECH", "author": ["Jones", "Douglas A"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Oral history and the digital revolution.", "author": ["Frisch", "Michael"], "venue": "The Oral History Reader,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Informedia: improving access to digital video.\" Interactions", "author": ["Stevens", "Scott", "Michael Christen", "Howard Wactlar"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Indexing and search of multimodal information.", "author": ["Hauptmann", "Alexander G", "Howard D. Wactlar"], "venue": "Acoustics, Speech, and Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Text, Speech, and Vision for Video Segmentation: The Informedia TM Project.\" AAAI fall symposium, computational models for integrating language and vision", "author": ["A. Hauptmann", "M. Smith"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Speech transcript analysis for automatic search.", "author": ["Coden", "Anni R", "Eric W. Brown"], "venue": "System Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Automatic recognition of spontaneous speech for access to multilingual oral history archives.", "author": ["Byrne", "William"], "venue": "Speech and Audio Processing, IEEE Transactions on 12.4", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Supporting access to large digital oral history archives.", "author": ["Gustman", "Samuel"], "venue": "Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Speechbot: an experimental speech-based search engine for multimedia content on the web.\" Multimedia", "author": ["Van Thong", "J-M"], "venue": "IEEE Transactions", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Imperfect transcript driven speech recognition.\"InterSpeech", "author": ["Lecouteux", "Benjamin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "The Cambridge University spoken document retrieval system.\" Acoustics", "author": ["Johnson", "S. E"], "venue": "Speech, and Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Experiments in broadcast news transcription.\"Acoustics", "author": ["Woodland", "P. C"], "venue": "Speech and Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Designing the search experience: The information architecture of discovery", "author": ["Russell-Rose", "Tony", "Tyler Tate"], "venue": "Access Online via Elsevier,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Faceted metadata for image search and browsing.\"Proceedings of the SIGCHI conference on Human factors in computing systems", "author": ["Yee", "Ka-Ping"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "The anatomy of a large-scale hypertextual Web search engine.\" Computer networks and ISDN systems", "author": ["Brin", "Sergey", "Lawrence Page"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Faceted Search (Synthesis Lectures on Information Concepts, Retrieval, and Services).", "author": ["Tunkelang", "Daniel"], "venue": "Morgan and Claypool Publishers", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Exploratory search: Beyond the query-response paradigm.\" Synthesis Lectures on Information Concepts, Retrieval, and Services", "author": ["White", "Ryen W", "Resa A. Roth"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Evaluating advanced search interfaces using established information\u2010seeking models.\" Journal of the American Society for Information", "author": ["Wilson", "Max L", "Ryen W. White"], "venue": "Science and Technology", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "While there are efficient search engines for text documents present today, there are no satisfactory systems for performing search over audiovisual data as these systems do not perform any detailed analysis for them [11].", "startOffset": 216, "endOffset": 220}, {"referenceID": 2, "context": "We know that core audio-video dimension of oral history is notoriously underutilized [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Because of which the considerable potential of audio and video documents to support high-impact, vivid, thematic, and analytic engagement with meaningful issues, personalities, and contexts, is largely untapped [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 1, "context": "[2] performed a psycholinguistic study measuring readability of several types of speech transcripts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The Informedia Digital Video Library Project at Carnegie Mellon University is creating a digital library of text, images, videos and audio data available for full content retrieval [4].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "built a system by integrating technologies involved in creating a digital video library suitable for fullcontent search and retrieval [6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Doing so, they try to overcome limitations of each technology [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "also try to address the problem of finding pertinent information for visual data [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "An experimental speech-based search engine, SpeechBot, uses automatic transcripts for indexing and in turn uses searchable index to provide an ability to play the segments of interest within the audio file [11].", "startOffset": 206, "endOffset": 210}], "year": 2013, "abstractText": "Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding userperspective on the design elements, and ease of parsing information.", "creator": "Microsoft\u00ae Office Word 2007"}}}