{"id": "1312.5663", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "k-Sparse Autoencoders", "abstract": "right recently, however it has been unanimously observed that when representations are learnt in a way that ideally encourages spontaneous sparsity, improved computing performance is invariably obtained on classification tasks. these relaxation methods probably involve combinations encoding of activation functions, sampling steps frequently and different decreasing kinds of penalties. to investigate the effectiveness factors of sparsity by filtering itself, maybe we specifically propose the k - sparse autoencoder, which is a strict linear dynamic model, but even where in hidden sub layers only half the k % highest activities are usually kept. when applied theoretically to handling the 2d mnist and binary norb datasets, we find that now this method achieves generally better quality classification recovery results faster than denoising autoencoders, specialized networks trained largely with arbitrary dropout, and moderately restricted sparse boltzmann elimination machines. k - efficiently sparse maximal autoencoders are thus simple to train and the integer encoding speed stage allocation is basically very limited fast, making them well - suited to handling large problem sizes, where surprisingly conventional sparse coding training algorithms specifically cannot be applied.", "histories": [["v1", "Thu, 19 Dec 2013 17:46:46 GMT  (1073kb)", "http://arxiv.org/abs/1312.5663v1", null], ["v2", "Sat, 22 Mar 2014 17:12:07 GMT  (1155kb)", "http://arxiv.org/abs/1312.5663v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alireza makhzani", "brendan frey"], "accepted": true, "id": "1312.5663"}, "pdf": {"name": "1312.5663.pdf", "metadata": {"source": "META", "title": "k-Sparse Autoencoders", "authors": ["Alireza Makhzani"], "emails": ["makhzani@psi.utoronto.ca", "frey@psi.utoronto.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n56 63\nv1 [\ncs .L\nG ]\n1 9\nD ec\n2 01"}, {"heading": "1. Introduction", "text": "Sparse feature learning algorithms range from sparse coding approaches (Olshausen & Field, 1997) to training neural networks with sparsity penalties (Nair & Hinton, 2009; Lee et al., 2007). These methods typically comprise two steps: a learning algorithm that produces a dictionary W that sparsely represents the data {xi}Ni=1, and an encoding algorithm that, given the dictionary, defines a mapping from a new input vector x to a feature vector.\nA practical problem with sparse coding is that both the dictionary learning and the sparse encoding steps are computationally expensive. Dictionaries are usually learnt offline by iteratively recovering sparse codes\nPreliminary work. Under review by the International Conference on Learning Representations. Do not distribute.\nand updating the dictionary. Sparse codes are computed using the current dictionary W and a pursuit algorithm to solve\nz\u0302i = argmin z \u2225xi \u2212Wz\u222522 s.t. \u2225z\u22250 < k (1)\nwhere zi, i = 1, ..,N are the columns of Z. Convex relaxation methods such as \u21131 minimization or greedy methods such as OMP (Tropp & Gilbert, 2007) are used to solve the above optimization. While greedy algorithms are faster, they are still slow in practice. The current sparse codes are then used to update the dictionary, using techniques such as the method of optimal directions (MOD) (Engan et al., 1999) or K-SVD (Aharon et al., 2005). These methods are computationally expensive; MOD requires inverting the data matrix at each step and K-SVD needs to compute a SVD in order to update every column of the dictionary.\nTo achieve speedups, in (Gregor & LeCun, 2010; Kavukcuoglu et al., 2010), a parameterized non-linear encoder function is trained to explicitly predict sparse codes using a soft thresholding operator. However, they assume that the dictionary is already given and do not address the offline phase..\nAnother approach that has been taken recently is to train autoencoders in a way that encourages sparsity. However, these methods usually involve combinations of activation functions, sampling steps and different kinds of penalties, and are sometimes not guaranteed to produce sparse representations for each input. For example, in (Lee et al., 2007; Nair & Hinton, 2009), a \u201clifetime sparsity\u201d penalty function proportional to the negative of the KL divergence between the hidden unit marginals and the target sparsity probability is added to the cost function. This results in sparse activation of hidden units across training points, but does not guarantee that each input has a sparse representation.\nThe contributions of this paper are as follows. (i) We describe \u201ck -sparse autoencoders\u201d and show that they can be efficiently learnt and used for sparse coding. (ii) We explore how different sparsity levels\n(k) impact representations and classification performance. (iii) We show that by solely relying on sparsity as the regularizer and as the only nonlinearity, we can achieve much better results than the other methods, including RBMs (Hinton & Salakhutdinov, 2006), denoising autoencoders (Vincent et al., 2008) and dropout (Hinton et al., 2012). (iv) We demonstrate that k -sparse autoencoders are suitable for pretraining and achieve results comparable to state-ofthe-art on MNIST and NORB datasets.\nIn this paper, \u0393 is an estimated support set and \u0393c is its complement. W \u2020 is the pseudo-inverse of W and suppk(x) is an operator that returns the indices of the k largest coefficients of its input vector. z\u0393 is the vector obtained by restricting the elements of z to the indices of \u0393 and W\u0393 is the matrix obtained by restricting the columns of W to the indices of \u0393."}, {"heading": "2. Description of the Algorithm", "text": ""}, {"heading": "2.1. The Basic Autoencoder", "text": "A shallow autoencoder maps an input vector x to a hidden representation using the function z = f(Px+b), parameterized by {P,b}. f is the activation function, e.g., linear, sigmoidal or ReLU. The hidden representation is then mapped linearly to the output using x\u0302 = Wz + b\u2032. The parameters are optimized to minimize the mean square error of \u2225x\u0302\u2212x\u22252\n2 over all training\npoints. Often, tied weights are used, so that P =W \u22ba.\n2.2. The k-Sparse Autoencoder\nThe k-sparse autoencoder is based on an autoencoder with linear activation functions and tied weights. In the feedforward phase, after computing the hidden code z =W \u22bax+b, rather than reconstructing the input from all of the hidden units, we identify the k largest hidden units and set the others to zero. This can be done by sorting the activities or by using ReLU hidden units with thresholds that are adaptively adjusted until the k larges activities are identified. This results in a vector of activities, suppk{W\n\u22bax + b}. Note that once the k largest activities are selected, the function computed by the network is linear. So the only nonlinearity comes from the selection of the k largest activities. This selection step acts as a regularizer that prevents the use of an overly large number of hidden units when reconstructing the input.\nOnce the weights are trained, the resulting sparse representations may be used for learning to perform downstream classification tasks. However, it has been observed that often, better results are obtained when the sparse encoding stage used for classification does not\nexactly match the encoding used for dictionary training (Coates & Ng, 2011). For example, while in k - means, it is natural to have a hard-assignment of the points to the nearest cluster in the encoding stage, it has been shown in (Van Gemert et al., 2008) that soft assignments result in better classification performance. Similarly, for the k -sparse autoencoder, instead of using the k largest elements ofW \u22bax+b as the features, we have observed that slightly better performance is obtained by using the \u03b1k largest hidden units where \u03b1 \u2265 1 is selected using validation data: h = supp\u03b1k(W\n\u22bax+b). The algorithm is summarized as follows.\nk -Sparse Autoencoders: Training:\n1) Perform the feedforward phase and compute z =W \u22bax + b x\u0302 =Wz + b\u2032 2) Find the k largest activations of z and ignore the rest. \u0393 = suppk(W\n\u22bax + b) 3) Backpropagate the error through the k largest activations defined by \u0393 and iterate.\nSparse Encoding:\nDefine the features as h = supp\u03b1k(W \u22bax + b)\n3. Analysis of the k-Sparse\nAutoencoder\nIn this section, we explain how the k -sparse autoencoder can be viewed in the context of sparse coding with incoherent matrices. This perspective sheds light on why the k -sparse autoencoders work and why they achieve invariant features and consequently good classification results. We first explain a sparse recovery algorithm and then show that the k -sparse autoencoder iterates between an approximation of this algorithm and a dictionary update stage.\n3.1. Iterative Thresholding with Inversion (ITI)\nIterative hard thresholding (Blumensath & Davies, 2009) is a class of low complexity algorithms, which has recently been proposed for the reconstruction of sparse signals. In this work, we use a variant called \u201citerative thresholding with inversion\u201d (Maleki, 2009). Given a fixed x and W , starting from z0 = 0, ITI iteratively finds the sparsest solution of x =Wz using the following steps.\n1. Support Estimation Step:\n\u0393 = suppk{z n +W \u22ba(x \u2212Wzn)} (2)\n2. Inversion Step:\nzn+1\u0393 =W \u2020 \u0393 x = (W \u22ba \u0393 W\u0393)\u22121W \u22ba\u0393x\nzn+1(\u0393)c = 0 (3)\nAssume H = W \u22baW \u2212 I and z0 is the true sparse solution. The first step of ITI estimates the support set as \u0393 = suppk{W\n\u22bax} = suppk{z0 + Hz0}. If W was orthogonal, we would have Hz0 = 0 and the algorithm would succeed in the first iteration. But if W is overcomplete, Hz0 behaves as a noise vector whose variance decreases after each iteration. After estimating the support set of z as \u0393, we restrict W to the indices included in \u0393 and form W\u0393. We then use the pseudo-inverse of W\u0393 to estimate the non-zero values minimizing \u2225x\u2212W\u0393z\u0393\u222522. Lastly, we refine the support estimation and repeat the whole process until convergence.\n3.2. Sparse Coding with the k-Sparse\nAutoencoder\nHere, we show that we can derive the k -sparse autoencoder tarining algorithm by approximating a sparse coding algorithm that uses the ITI algorithm jointly with a dictionary update stage.\nThe conventional approach of sparse coding is to fix the sparse code matrix Z, while updating the dictionary. However, here, after estimating the support set in the first step of the ITI algorithm, we jointly perform the inversion step of ITI and the dictionary update step, while fixing just the support set of the sparse code Z. In other words, we update the atoms of the dictionary and allow the corresponding non-zero values to change at the same time to minimize \u2225X \u2212W\u0393Z\u0393\u222522 over both W\u0393 and Z\u0393.\nWhen we are performing sparse recovery with the ITI algorithm using a fixed dictionary, we should perform a fixed number of iterations to get the perfect reconstruction of the signal. But, in sparse coding, since we learnt a dictionary that is adapted to the signals, as shown in Section 3.3, we can find the support set just with the first iteration of ITI:\n\u0393z = suppk{W \u22bax} (4)\nIn the inversion step of the ITI algorithm, once we estimate the support set, we use the pseudo-inverse of W\u0393 to find the non-zero values of the support set. The pseudo-inverse of the matrix W\u0393 is a matrix, such as P\u0393, that minimizes the following cost function.\nW \u2020 \u0393 = arg min\nP\u0393\n\u2225x \u2212W\u0393z\u0393\u222522\n= arg min P\u0393\n\u2225x \u2212W\u0393P\u0393x\u222522 (5)\nFinding the exact pseudo-inverse of W\u0393 is computationally expensive, so instead, we perform a single step of gradient descent. The gradient of P\u0393 is found as follows:\n\u2202\u2225x \u2212W\u0393z\u0393\u222522 \u2202P\u0393 = \u2202\u2225x \u2212W\u0393z\u0393\u222522 \u2202z\u0393 x (6)\nThe first term of the right hand side of the Equation (6) is the dictionary update stage, which is computed as follows:\n\u2202\u2225x \u2212W\u0393z\u0393\u222522 \u2202z\u0393 = (W\u0393z\u0393 \u2212 x)z\u22ba\u0393 (7)\nTherefore, in order to approximate the pseudo-inverse, we first find the dictionary derivative and then \u201cbackpropagate\u201d it to find the update of the pseudo-inverse.\nWe can view these operations in the context of an autoencoder with linear activations where P is the encoder weight matrix and W is the decoder weight matrix. At each iteration, instead of back-propagating through all the hidden units, we just back-propagate through the units with the k largest activities, defined by supp\nk {W \u22bax}, which is the first iteration of ITI.\nKeeping the k largest hidden activities and ignoring the others is the same as forming W\u0393 by restricting W to the estimated support set. Back-propagation on the decoder weights is the same as gradient descent on the dictionary and back-propagation on the encoder weights is the same as approximating the pseudoinverse of the corresponding W\u0393.\nWe can perform support estimation in the feedforward phase by assuming P = W \u22ba (i.e., the autoencoder has tied weights). In this case, support estimation can be done by computing z = W \u22bax + b and picking the k largest activations; the bias just accounts for the mean and subtracts its contribution. Then the \u201cinversion\u201d and \u201cdictionary update\u201d steps are done at the same time by back-propagation through just the units with the k largest activities.\nIn summary, we can view k -sparse autoencoders as the approximation of a sparse coding algorithm which uses ITI in the sparse recovery stage."}, {"heading": "3.3. Importance of Incoherence", "text": "The coherence of a dictionary indicates the degree of similarity between different atoms or different collections of atoms. Since the dictionary is overcomplete, we can represent each column of the dictionary as a linear combination of other columns. But what incoherence implies is that we should not be able to represent a column as a sparse linear combination of other columns and the coefficients of the linear combination should be dense. For example, if two columns are exactly the same, then the dictionary is highly coherent since we can represent one of those columns as the sparse linear combination of the rest of the columns. A naive measure of coherence that has been proposed in the literature is the mutual coherence \u00b5(W )which is defined as the maximum absolute inner product across all the possible pairs of the atoms of the dictionary.\n\u00b5(W ) =max i\u2260j \u2223\u27e8wi,wj\u27e9\u2223 (8)\nThere is a close relationship between the coherency of the dictionary and the uniqueness of the sparse solution of x = Wz. In (Donoho & Elad, 2003), it has been proven that if k \u2264 (1 + \u00b5\u22121), then the sparsest solution is unique.\nWe can show that if the dictionary is incoherent enough, there is going to be an attraction ball around the signal x and there is only one unique sparse linear combination of the columns that can get into this attraction ball. So even if we perturb the input with a small amount of noise, translation, rotation, etc., we can still achieve perfect reconstruction of the original signal and the sparse features are always roughly conserved. Therefore, incoherency of the dictionary is a measure of invariance and stability of the features. This is related to the denoising autoencoder (Vincent et al., 2008) in which we achieve invariant features by trying to reconstruct the original signal from its noisy versions.\nHere we show that if the dictionary is incoherent enough, the first step of the ITI algorithm is sufficient for perfect sparse recovery.\nTheorem 3.1. Assume x = Wz and the columns of the dictionary have unit \u21132-norm. Also without loss of generality, assume that the non-zero elements of z are its first k elements and are sorted as z1 \u2265 z2 \u2265 ... \u2265 zk . Then, if k\u00b5 \u2264 zk\n2z1 , we can recover the support set of z\nusing suppk(W \u22bax).\nProof : Let us assume 0 \u2264 i \u2264 k and y = W \u22bax. Then, we can write:\nyi = zi + k\n\u2211 j=1,j\u2260i\n\u27e8wi,wj\u27e9zj \u2265 zi \u2212 \u00b5 k\n\u2211 j=1,j\u2260i zj \u2265 zk \u2212 k\u00b5z1\n(9) On the other hand:\nmax i>k {yi} =max i>k \u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 k \u2211 j=1 \u27e8wi,wj\u27e9zj \u23ab\u23aa\u23aa \u23ac \u23aa\u23aa\u23ad \u2264 k\u00b5z1 (10)\nSo if k\u00b5 \u2264 zk 2z1\n, all the first k elements of y are guaranteed to be greater than the rest of its elements.\nAs we can see from Theorem 3.1, the chances of finding the true support set with the encoder part of the k - sparse autoencoder depends on the incoherency of the learnt dictionary. As the k -sparse autoencoder converges (i.e., the reconstruction error goes to zero), the algorithm learns a dictionary that satisfies x \u2248Wz, so the support set of z can be estimated using the first step of ITI. Since suppk(W\n\u22bax) succeeds in finding the support set when the algorithm converges, the learnt dictionary must be sufficiently incoherent."}, {"heading": "4. Experiments", "text": "In this section, we evaluate the performance of k - sparse autoencoders in both unsupervised learning and in shallow and deep discriminative learning tasks."}, {"heading": "4.1. Datasets", "text": "We use the MNIST handwritten digit dataset, which consists of 60,000 training images and 10,000 test images. We randomly separate the training set into 50,000 training cases and 10,000 cases for validation.\nWe also use the small NORB normalized-uniform dataset (LeCun et al., 2004), which contains 24,300 training examples and 24,300 test examples. This database contains images of 50 toys from 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Each image consists of two channels, each of size 96 \u00d7 96 pixels. We take the inner 64 \u00d7 64 pixels of each channel and resize it using bicubic interpolation to the size of 32\u00d7 32 pixels from which we form a vector with 2048 dimensions as the input. Data points are subtracted by the mean and divided by the standard deviation along each input dimension across the whole training set to normalize the contrast. The training set is separated into 20,000 for training and 4,300 for validation.\nWe also test our method on natural image patches extracted from CIFAR-10 dataset. We randomly extract 1000000 patches of size 8\u00d78 from the 50000 32\u00d732 im-\nages of CIFAR-10. Each patch is then locally contrastnormalized and ZCA whitened. This preprocessing pipeline is the same as the one used in (Coates et al., 2011) for feature extraction.\n4.2. Training of k-Sparse Autoencoders"}, {"heading": "4.2.1. Scheduling of the Sparsity Level", "text": "When we are enforcing low sparsity levels in k -sparse autoencoders (e.g., k=15 on MNIST), one issue that might arise is that in the first few epochs, the algorithm greedily assigns individual hidden units to groups of training cases, in a manner similar to kmeans clustering. In subsequent epochs, these hidden units will be picked and re-enforced and other hidden units will not be adjusted. That is, too much sparsity can prevent gradient back-propagation from adjusting the weights of these other \u2018dead\u2019 hidden units. We can address this problem by scheduling the sparsity level over epochs as follows.\nSuppose we are aiming for a sparsity level of k = 15. Then, we start off with a large sparsity level (e.g. k = 100) for which the k -sparse autoencoder can train all the hidden units. We then linearly decrease the sparsity level from k = 100 to k = 15 over the first half of the epochs. This initializes the autoencoder in a good regime, for which all of the hidden units have a significant chance of being picked. Then, we keep k = 15 for the second half of the epochs. With this scheduling, we can train all of the filters, even for low sparsity levels."}, {"heading": "4.2.2. Training Hyper-parameters", "text": "We optimized the model parameters using stochastic gradient descent with momentum as follows.\nvk+1 =mkvk \u2212 \u03b7k\u2207f(xk) xk+1 = xk + vk (11)\nHere, vk is the velocity vector, mk is the momentum and \u03b7k is the learning rate at the k -th iteration. We also use a Gaussian distribution with a standard deviation of \u03c3 for initialization of the weights. We use different momentum values, learning rates and initializations based on the task and the dataset, and validation is used to select hyperparameters. In the unsupervised MNIST task, the values were \u03c3 = 0.01 , mk = 0.9 and \u03b7k = 0.01, for 5000 epochs. In the supervised MNIST task, training started with mk = 0.25 and \u03b7k = 1, and then the learning rate was linearly decreased to 0.001 over 200 epochs. In the unsupervised NORB task, the values were \u03c3 = 0.01, mk = 0.9 and \u03b7k = 0.0001, for 5000 epochs. In the supervised NORB task, training\nstarted with mk = 0.9 and \u03b7k = 0.01, and then the learning rate was linearly decreased to 0.001 over 200 epochs."}, {"heading": "4.2.3. Implementations", "text": "While most of the conventional sparse coding algorithms require complex matrix operations such as matrix inversion or SVD decomposition, the k -sparse autoencoders only need matrix multiplications and sorting operations in both dictionary learning stage and the sparse encoding stage. (For a parallel, distributed implementation, the sorting operation can be replaced by a method that recursively applies a threshold until k values remain.) We used an efficient GPU implementation obtained using the publicly available gnumpy library (Tieleman, 2010) on a single Nvidia GTX 680 GPU."}, {"heading": "4.3. Effect of Sparsity Level", "text": "In k -sparse autoencoders, we are able to tune the value of k to obtain the desirable sparsity level which makes the algorithm suitable for a wide variety of datasets. For example, one application could be pre-training a shallow or deep discriminative neural network. For large values of k (e.g., k = 100 on MNIST), the algorithm tends to learn very local features as is shown in Figure 1a and 2a. These features are too primitive to be used for classification using a shallow architecture since a naive linear classifier does not have enough capacity to combine these features and achieve a good classification rate. However, these features could be used for pre-training deep neural nets.\nAs we decrease the the sparsity level (e.g., k = 40 on MNIST), the output is reconstructed using a smaller number of hidden units and thus the features tend to be more global, as can be seen in Figure 1b,1c and 2b. For example, in the MNIST dataset, the lengths of the strokes increase when the sparsity level is decreased. These less local features are suitable for classification using a shallow architecture. Nevertheless, forcing too much sparsity (e.g., k = 10 on MNIST), results in features that are too global and do not factor the input into parts, as depicted Figure 1d and 2c.\nFig. 3 shows the visualization of filters of the k -sparse autoencoder with 1000 hidden units and sparsity level of k = 50 learnt from random image patches extracted from CIFAR-10 dataset. We can see that the k -sparse autoencoder has learnt localized Gabor filters from natural image patches.\nFig. 4 plots histograms of the hidden unit activities for various unsupervised learning algorithms, includ-\ning the k -sparse autoencoder (k=70 and k=15), applied to the MNIST data. This figure contrasts the sparsity achieved by the k -sparse autoencoder with that of other algorithms."}, {"heading": "4.4. Unsupervised Feature Learning Results", "text": "In order to compare the quality of the features learnt by our algorithm with those learnt by other unsupervised learning methods, we first extracted features using each unsupervised learning algorithm. Then we fixed the features and trained a logistic regression classifier using those features. The usefulness of the features is then evaluated by examining the error rate of the classifier.\nWe trained a number of architectures on the MNIST and NORB datasets, including RBM, dropout autoencoder and denoising autoencoder. In dropout, after finding the features using dropout regularization with a dropout rate of 50%, we used all of the hidden units as the features (this worked best). For the denoising autoencoder, after training the network by dropping the input pixels with a rate of 20%, we used\nall of the uncorrupted input pixels to find the features for classification (this worked best). In the k - sparse autoencoder, after training the dictionary, we used h = supp\u03b1k(W\n\u22bax + b) to find the features as explained in Section 2.2, where \u03b1 was determined using validation data. Results for different architectures are compared in Tables 1, 2. We can see that the performance of our k -sparse autoencoder is better than the rest of the algorithms. In our algorithm, the best result is achieved by k = 25, \u03b1 = 3 with 1000 hidden units on MNIST dataset and by k = 150, \u03b1 = 2 with 4000 hidden units on NORB dataset."}, {"heading": "4.5. Shallow Supervised Learning Results", "text": "In supervised learning, it is a common practice to use the encoder weights learnt by an unsupervised learning method to initialize the early layers of a multilayer discriminative model (Erhan et al., 2010). The back-propagation algorithm is then used\nto adjust the weights of the last hidden layer and also to fine-tune the weights in the previous layers. This procedure is often referred to as discriminative fine-tuning. In this section, we report results using unsupervised learning algorithms such as RBMs, DBNs (Salakhutdinov & Larochelle, 2010), DBMs (Salakhutdinov & Larochelle, 2010), third-order RBM (Nair & Hinton, 2009), dropout autoencoders, denoising autoencoders and k -sparse autoencoders to initialize a shallow discriminative neural network for the MNIST and NORB datasets. We used backpropagation to fine-tune the weights. The regularization method used in the fine-tuning stage of different algorithms is the same as the one used in the training of the corresponding unsupervised learning task. For instance, we fine-tuned the weights obtained from dropout autoencoder with dropout regularization or in denoising autoencoder, we fine-tuned the discriminative neural net by adding noise to the input. In a similar manner, in the fine-tuning stage of the k -sparse autoencoder, we used the \u03b1k largest hidden units in the corresponding discriminative neural network, as explained in Section 2.2. Tables 3 and 4 reports the error rates obtained by different methods."}, {"heading": "4.6. Deep Supervised Learning Results", "text": "The k -sparse autoencoder can be used as a building block of a deep neural network, using greedy layerwise pre-training (Bengio et al., 2007). We first train a shallow k -sparse autoencoder and obtain the hidden codes. We then fix the features and train another k -\nsparse autoencoder on top of them to obtain another set of hidden codes. Then we use the parameters of these autoencoders to initialize a discriminative neural network with two hidden layers.\nIn the fine-tuning stage of the deep neural net, we first fix the parameters of the first and second layers and train a softmax classifier on top of the second layer. We then hold the weights of the first layer fixed and train the second layer and softmax jointly using the initialization of the softmax that we found in the previous step. Finally, we jointly fine-tune all of the layers with the previous initialization. We have observed that this method of layer-wise fine-tuning can improve the classification performance compared to the case where we fine-tune all the layers at the same time.\nIn all of the fine-tuning steps, we keep the \u03b1k largest hidden codes, where k = 25, \u03b1 = 3 in MNIST and k = 150, \u03b1 = 2 in NORB in both hidden layers. Tables 3 and 4 report the classification results of different deep supervised learning methods."}, {"heading": "5. Conclusion", "text": "In this work, we proposed a very fast sparse coding method called k -sparse autoencoder, which achieves exact sparsity in the hidden representation. The main message of this paper is that we can use the resulting representations to achieve state-of-the-art classification results, solely by enforcing sparsity in the hidden units and without using any other nonlinearity or regularization. We also discussed how the k -sparse autoencoder could be used for pre-training shallow and\ndeep supervised architectures."}], "references": [{"title": "K-svd: Design of dictionaries for sparse representation", "author": ["Aharon", "Michal", "Elad", "Michael", "Bruckstein", "Alfred"], "venue": "Proceedings of SPARS,", "citeRegEx": "Aharon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2005}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Blumensath", "Thomas", "Davies", "Mike E"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath et al\\.", "year": 2009}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Coates", "Adam", "Ng", "Andrew"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization", "author": ["Donoho", "David L", "Elad", "Michael"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Donoho et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Donoho et al\\.", "year": 2003}, {"title": "Method of optimal directions for frame design", "author": ["Engan", "Kjersti", "Aase", "Sven Ole", "J. Hakon Husoy"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Engan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Engan et al\\.", "year": 1999}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Learning fast approximations of sparse coding", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Gregor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1010.3467,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun", "Yann", "Huang", "Fu Jie", "Bottou", "Leon"], "venue": "In Computer Vision and Pattern Recognition, CVPR,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Sparse deep belief net model for visual area v2", "author": ["Lee", "Honglak", "Ekanadham", "Chaitanya", "Ng", "Andrew"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Coherence analysis of iterative thresholding algorithms", "author": ["Maleki", "Arian"], "venue": "47th Annual Allerton Conference on,", "citeRegEx": "Maleki and Arian.,? \\Q2009\\E", "shortCiteRegEx": "Maleki and Arian.", "year": 2009}, {"title": "3d object recognition with deep belief nets", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nair et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2009}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Vision research,", "citeRegEx": "Olshausen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1997}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Larochelle", "Hugo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2010}, {"title": "Gnumpy: an easy way to use gpu boards in python", "author": ["Tieleman", "Tijmen"], "venue": "Department of Computer Science, University of Toronto,", "citeRegEx": "Tieleman and Tijmen.,? \\Q2010\\E", "shortCiteRegEx": "Tieleman and Tijmen.", "year": 2010}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["Tropp", "Joel A", "Gilbert", "Anna C"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Tropp et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tropp et al\\.", "year": 2007}, {"title": "Kernel codebooks for scene categorization", "author": ["Van Gemert", "Jan C", "Geusebroek", "Jan-Mark", "Veenman", "Cor J", "Smeulders", "Arnold WM"], "venue": "In Computer Vision\u2013 ECCV", "citeRegEx": "Gemert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gemert et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 12, "context": "Sparse feature learning algorithms range from sparse coding approaches (Olshausen & Field, 1997) to training neural networks with sparsity penalties (Nair & Hinton, 2009; Lee et al., 2007).", "startOffset": 149, "endOffset": 188}, {"referenceID": 6, "context": "The current sparse codes are then used to update the dictionary, using techniques such as the method of optimal directions (MOD) (Engan et al., 1999) or K-SVD (Aharon et al.", "startOffset": 129, "endOffset": 149}, {"referenceID": 0, "context": ", 1999) or K-SVD (Aharon et al., 2005).", "startOffset": 17, "endOffset": 38}, {"referenceID": 10, "context": "To achieve speedups, in (Gregor & LeCun, 2010; Kavukcuoglu et al., 2010), a parameterized non-linear encoder function is trained to explicitly predict sparse codes using a soft thresholding operator.", "startOffset": 24, "endOffset": 72}, {"referenceID": 12, "context": "For example, in (Lee et al., 2007; Nair & Hinton, 2009), a \u201clifetime sparsity\u201d penalty function proportional to the negative of the KL divergence between the hidden unit marginals and the target sparsity probability is added to the cost function.", "startOffset": 16, "endOffset": 55}, {"referenceID": 20, "context": "(iii) We show that by solely relying on sparsity as the regularizer and as the only nonlinearity, we can achieve much better results than the other methods, including RBMs (Hinton & Salakhutdinov, 2006), denoising autoencoders (Vincent et al., 2008) and dropout (Hinton et al.", "startOffset": 227, "endOffset": 249}, {"referenceID": 20, "context": "This is related to the denoising autoencoder (Vincent et al., 2008) in which we achieve invariant features by trying to reconstruct the original signal from its noisy versions.", "startOffset": 45, "endOffset": 67}, {"referenceID": 11, "context": "We also use the small NORB normalized-uniform dataset (LeCun et al., 2004), which contains 24,300 training examples and 24,300 test examples.", "startOffset": 54, "endOffset": 74}, {"referenceID": 3, "context": "This preprocessing pipeline is the same as the one used in (Coates et al., 2011) for feature extraction.", "startOffset": 59, "endOffset": 80}, {"referenceID": 7, "context": "In supervised learning, it is a common practice to use the encoder weights learnt by an unsupervised learning method to initialize the early layers of a multilayer discriminative model (Erhan et al., 2010).", "startOffset": 185, "endOffset": 205}, {"referenceID": 1, "context": "The k -sparse autoencoder can be used as a building block of a deep neural network, using greedy layerwise pre-training (Bengio et al., 2007).", "startOffset": 120, "endOffset": 141}], "year": 2013, "abstractText": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the \u201ck -sparse autoencoder\u201d, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k -sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "creator": "LaTeX with hyperref package"}}}