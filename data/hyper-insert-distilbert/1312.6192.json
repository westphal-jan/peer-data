{"id": "1312.6192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Can recursive neural tensor networks learn logical reasoning?", "abstract": "recursive neural network models theoretically and their accompanying vector representations for words computation have twice seen high success in an overlapping array of wildly increasingly popular semantically novel sophisticated comprehension tasks, somewhat but almost obviously nothing is known solely about utilizing their convincing ability to accurately mechanically capture literally the smallest aspects such of linguistic meaning signals that are necessary for interpretation or reasoning. to vigorously evaluate this, i train a recursive model on a highly new corpus construct of constructed examples of logical arithmetic reasoning in short consecutive sentences, like the inference properties of \" that some animal walking walks \" from \" some dog walks \" or \" among some pup cat walks, \" given all that dogs and miniature cats like are animals. the statistical results are likely promising for the ability of these models merely to capture theoretically logical reasoning, but the model tested over here appears to actively learn representations that actually are quite specific to predicting the classical templatic complex structures of predicting the problems best seen in training, instruction and some that generalize beyond them only to give a limited degree.", "histories": [["v1", "Sat, 21 Dec 2013 02:29:42 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v1", "Submitted for presentation at ICLR 2014"], ["v2", "Tue, 24 Dec 2013 01:42:09 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v2", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"], ["v3", "Tue, 4 Feb 2014 18:02:09 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v3", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"], ["v4", "Sat, 15 Feb 2014 20:59:04 GMT  (18kb)", "http://arxiv.org/abs/1312.6192v4", "Submitted for presentation at ICLR 2014. Source code and data:this http URL"]], "COMMENTS": "Submitted for presentation at ICLR 2014", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["samuel r bowman"], "accepted": false, "id": "1312.6192"}, "pdf": {"name": "1312.6192.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["sbowman@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n61 92\nv1 [\ncs .C\nL ]\n2 1"}, {"heading": "1 Introduction", "text": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4]. Given the still modest performance of semantically rich NLP systems in many domains\u2014question answering and machine translation, for instance\u2014it is worth exploring the degree to which learned vectors can serve as general purpose semantic representations. Much of the work to date analyzing vector representations for words (see [5]) has focused on lexical semantic behaviors\u2014like the similarity between words like Paris and France. Good similarity functions are valuable for many NLP tasks, but there are real use cases for which it is necessary to know how words relate to one another or to some extrinsic representation, rather than just how similar they are to one another: deciding whether an inference holds for instance, such as that John went to Paris means that John went to France but not vice versa. This paper explores the ability of linguistic representations developed using supervised deep learning techniques to support interpretation and reasoning.\nNatural language inference (NLI), the ability to reason about the truth of a statement on the basis of some premise, is among the clearest examples of a task that requires comprehensive and accurate natural language understanding [6]. I borrow the structure of the task from MacCartney [6]. In it, the model is presented with a pair of sentences, and made to label the logical relation between the sentences as equivalence, entailment, or any of five other classes, as here:\nAt least three dogs bark entails that at least two dogs bark. All dogs bark is equivalent to no dogs don\u2019t bark. Most dogs bark is incompatible with no dogs bark.\nThe success an inference system most crucially depends on the use of a representation of phrase and sentence meaning that supports the accurate computation of inferential consequences. The\nframework of monotonicity inference [7, 8] delimits a means of computing inferences in a broad set of cases, and I focus in this paper on reproducing this kind of inference in a learned model.\nNLI involves many aspects of sentence understanding, and it is useful to be able to isolate inference from all the other ways that sentence understanding can go wrong: even if a model can learn to perform inference correctly, a failure of coreference resolution, word sense disambiguation, or parsing could supply it with inaccurate input and cause it to fail on an evaluation. To sidestep this potential problem, I propose a simplified variant of the task below that uses hand-constructed, unambiguous inference problems, and construct a dataset of these problems.\nIn the following sections, I present the task of strict unambiguous NLI and present a recursive neural tensor network model configured for the task but closely modeled after those used in practice by Socher et al. [3]. I then describe a dataset on which to evaluate the model, and lay out several experiments to test its ability to perform inference, and to generalize to patterns of inference different from those seen at training time. These experiments reveal that the model is able to learn to reason using familiar patterns of inference with perfect accuracy, but that its ability to generalize to substantially different patterns is limited, and I conclude with a discussion of the implications of this incomplete ability to generalize and proposals for future work."}, {"heading": "2 Reasoning with monotonicity", "text": "Consider the statement some dogs bark. From this, one can infer quite a number of other things. One can replace the first argument of the quantifier some (the first of the two predicates following it, here dogs) with any more general category that contains dogs and get a valid inference: some mammals bark; some animals bark; some things bark. But one can\u2019t replace it with anything more specific: it doesn\u2019t give one reason to say that some schnauzers bark. The second argument of some works the opposite way\u2014one can make it less specific, but not more\u2014one can say some dogs make sounds, but one can\u2019t say that some dogs bark at cars. These kind of inferences depend at least partially on some. If we replace it with most the first argument doesn\u2019t work the same way: most dogs bark doesn\u2019t entail that most animals bark, nor does it entail that most schnauzers bark.\nMonotonicity is the formalization of observations of this kind. The quantifier some is upward monotone in its first argument because it permits substitution of more general terms, and downward monotone in its second argument because it permits the substitution of more specific terms. Formally, monotonicity is a property of certain positions in semantic structures that determines what kinds of substitution can be made in that position while preserving truth. Most quantifiers allow monotonicity reasoning in some direction for both arguments. No, for example is downward in both arguments, some and two are upward in both, and most is upward in its second argument, but (unusually) does not license any kind of monotonicity inference on its first.\nMonotonicity is a central insight from work on natural logic [8, 6], a theoretical framework for natural language inference that uses natural language strings as the logical symbols, rather than relying on conversion to and from first order logic or a similar system. The experiments here are not a direct implementation of natural logic, or even of monotonicity, but I rely on these theories to define what types of inference I include in the data, and in turn, what types of inference I expect the model to learn.\nI limit myself in this study to examples of reasoning involving quantifiers like some and all. They are of interest here for the plethora of monotonicity reasoning patterns that they license, but quantifiers as lexical items are of particular interest in vector space models for other reasons as well. It is easy to imagine how a vector space might encode entities as vectors, with similar entities clustering together and certain directions in the space representing certain relationships between entities, and this interpretation of the dimensions of the space representing properties of entities can be expanded only somewhat awkwardly to common nouns and single-argument verbs\u2014Mikolov et al. [9] shows exactly this sort of behavior in VSMs learned for language modeling. This kind of similarity behavior is much less useful for quantifiers which define abstract relations between sets of entities: it might be somehow informative to encode lexical items such that two and three are more similar to each other than either is to no, but this is not especially helpful for any task that involves interpreting or reasoning with these words."}, {"heading": "2.1 The task: natural language inference", "text": "Natural language inference provides what I claim to be the simplest way to evaluate the ability of learned representation models to capture specific semantic behaviors. In the standard formulation of the task (and the one used in the RTE datasets [10]), the goal is to determine whether a reasonable human would infer a hypothesis from a premise. MacCartney formalizes a method of inferring entailment relations, and moves past two way entailment/non-entailment classification, proposing the set B of seven labels meant to describe all of the possible non-trivial relations that might hold between pairs of statements, shown in Table 1.\nInterest in NLI in the NLP community is ongoing, and a version of it has been included in the 2014 SemEval challenge specially targeted towards the evaluation of distributional models. Neither this dataset, nor any other existing RTE and NLI datasets are appropriate for the task at hand however. I intend for the present experiment to evaluate the ability of a class of models to learn certain types of inference behavior, and need a dataset that precisely tests these phenomena. With existing datasets that use unrestricted natural language, there is the risk that a model could successfully capture monotonicity inference, but fail to accurately label test data due to problems with, for example, lexical ambiguity, syntactic ambiguity, coreference resolution, or pragmatic enrichment.\nIn order to minimize this possibility, I define the task of strict unambiguous NLI (SU-NLI). In this task, only entailments that are licensed by a strictly literal interpretation of the provided sentences are considered valid, and several constraints are applied to the language to minimize ambiguity:\n\u2022 A small, unambiguous vocabulary is used.\n\u2022 All strings are given an explicit unlabeled tree structure parse.\n\u2022 Statements involving the hard-to-formalize generic senses of nouns\u2014i.e. dogs bark as opposed to the non-generic all dogs bark\u2014are excluded.\n\u2022 The sentences do not contain context dependent elements. This includes any reference to time or any tense morphology, and all pronouns.\n\u2022 The morphology is dramatically simplified: the copula is not used (some puppy is French is simplified to some puppy French, to make it more directly comparable to sentences like some puppy bark), and agreement marking (they walk vs. she walks) is omitted.\nThe key to success at this task is to learn a set of representations and functions that can mimic the logical structure underlying the data. There is limited precedent that deterministic logical behavior can be learned in supervised deep learning models: Socher et al. [11] show in an aside that a Boolean logic with negation and conjunction can be learned in a minimal recursive neural network model with one-dimensional (scalar) representations for words. Modeling the logical behavior that underlies linguistic reasoning, though, is a substantially more difficult challenge, even in modular hand-built models.\nThe natural logic engine at the core of MacCartney\u2019s [6] NLI system requires a complex set of linguistic knowledge, much of which takes the form of what he calls projectivity signatures. These signatures are tables showing the entailment relation that much hold between two strings that differ in a given way (such as the substitution of the argument of some quantifier), and are explicitly provided to the model for dozens of different cases of insertion, deletion and substitution of different types of lexical item. For example in judging the inference no animals bark | some dogs bark it\nwould first compute the relations introduced by each of the two differences between the sentences (\u2227 and \u2290) and then use an additional table of relations to join the two relations into one (|) that expresses the relation between the two sentences being compared:\n1. no animals bark \u2227 some animals bark \u2290 (some dogs) bark 2. some animals bark [\u2227,\u2290] some dogs bark 3. some animals bark | some dogs bark\nThis study is the first that I am aware of to attempt to build an inference engine based on learned vector representations, but two recent projects have attempted to introduce vector representations into inference systems in other ways: Baroni et al. [12] have achieved limited success in building a classifier to judge entailments between one- and two-word phrases (including some with quantifiers), though their vector representations were crucially based on distributional statistics and were not learned for the task. In another line of research, Garrette et al. [13] propose a way to improve standard discrete NLI with vector representations. They propose a deterministic inference engine (similar to MacCartney\u2019s) which is augmented by a probabilistic component that evaluates individual lexical substitution steps in the derivation using vector representations, though again these representations are not learned, and no evaluations of this system have been published to date."}, {"heading": "3 Methods", "text": "The model is centered on a recursively applied composition function, following Socher et al. [14], which is meant to mimic the recursive construction of meanings in formal models of semantics. In this scheme, pairs of words are merged into phrase representations by a function that maps from representations of length 2N to representations of length N . These phrase representations are then further merged with other words and phrases until the entire phrase or sentence being evaluated is represented in a single vector. This vector is then used as the input to a classifier and used in a supervised learning task.\nBorrowing a model directly from the existing literature for this task is impossible since none has been proposed to detect asymmetric relations between phrases (though it may be possible to slightly adapt the paraphrase model of Socher et al. [14] to the task in future work). Instead, I build a combination model, depicted in Figure 1. The two phrases being compared are built up separately on each side of the tree using the same composition function until they have each been reduced to single vectors. Then, the two phrase vectors are fed into a separate comparison layer that is meant to generate a feature vector capturing the relation between the two phrases. The output of this layer is then given to a softmax classifier, which in turn produces a hypothesized distribution over the seven relations.\nFor a composition layer, I use the RNTN layer function proposed in Chen et al. [4] (itself adapted from Socher et al. [3]) and shown below. The standard tanh sigmoid nonlinearity is applied to the output, following Socher et al.\n(1) yi = fa(~x(l)TA[i]~x(r) + ~Bi,:[~x(l); ~x(r)] + ci)\nThe comparison layer uses the same type of function with different parameters and a different nonlinearity function wrapped around it:\n(2) yi = fb(~x(l)TK[i]~x(r) + ~Li,:[~x(l); ~x(r)] +mi)\nRather than use a sigmoid here, I found a substantial improvement in performance by using a rectified linear function for fb. In particular, I use the leaky rectified linear function [15]: fb(~x) = max(~x, 0) + 0.01min(~x, 0), applied elementwise.\nTo run the model forward and label a pair of phrases, the structure of the lower layers of the network is assembled so as to mirror the tree structures provided for each phrase. The word vectors are then looked up from the vocabulary matrix V (one of the model parameters), and the composition and comparison functions are used to pass information up the tree and into the classifier at the top. The model is trained using backpropagation through structure [16], wherein the negative log of the probability assigned to the correct label is taken as a cost function, and the gradient of each parameter with respect to that cost function is computed at each node, with information passing down the tree\nand into both the function parameters and the vocabulary matrix. Gradients for different instances of the composition function or different instances of a word in the same tree are summed to produce at most a single gradient update per parameter.\nOptimization: I train the model with stochastic gradient descent (SGD), with gradients pooled from randomly chosen minibatches of 32 training examples (chosen empirically), and learning rates computed using AdaGrad [17]. L-BFGS [18] was tried unsuccessfully as an alternative to SGD. An L2 regularization term is added to the gradients from each batch, with a \u03bb coefficient of 0.0002 chosen by experiment. L1 regularization was tested but showed no improvement, though it may be fruitful in future work to explore other ways to encourage the learning of sparse solutions in the hope that they might be able to better support discrete, deterministic behavior of the sort studied here. The parameters and word vectors are initialized randomly using a uniform distribution over [\u22120.1, 0.1]\u2014several pretraining regimes meant to initialize the word vectors to reflect the relations between them were tried, but none offered a measurable improvement to the learned model.\nModel size: The dimensionalities of the model were tuned extensively. The performance of the model is approximately optimal with the dimensionality of the word vectors set to 16, and the dimensionality of the feature vector produced by the comparison layer set to 45.\nI also experimented with increasing the size of the model in two other ways: I ran all of the experiments below with an additional one or two standard neural network layers positioned between the comparison layer and the softmax classifier at the top of the network. This did not yield a consistent measurable improvement. I additionally reran the experiments with a variation of the network that uses different composition functions to compose different types of phrase, following [19], but saw no associated improvement in performance. More detail is provided in Appendix A."}, {"heading": "4 Data", "text": "The dataset is built on a small purpose-built vocabulary, which is intended to be large and diverse enough to permit a wide variety of experiments on different semantic phenomena beyond those shown here, but which is still small enough to allow the relations between the lexical items themselves to be exhaustively manually labeled when needed. The vocabulary contains 41 predicates (nouns, adjectives, and intransitive verbs; see Appendix B for more on the design of the wordlist), six quantifiers, the logical conjunctions and and or, and the unary operator not.\nThe data take the form of 12k pairs of short sentences, each annotated with a relation label, which I divide into about 200 smaller datasets. These datasets contain tightly constrained variants of a specific valid reasoning pattern, each differing in at most one lexical item, and all sharing the same\nrelation label. The remainder of this section describes the composition of these 200 datasets. Examples from four of the datasets are provided in Table 2.\nBasic monotonicity: This set of datasets contains reasoning patterns like those seen in examples 1 and 2, where one of the predicates on one side entails the predicate in the corresponding position on the other side. In some of the datasets (as in 1), this alternation is in the first argument, in some the second argument (as in 2), and in some both. For the alternating first argument subclasses, I have every lexical entailment pair in the data in the first argument position, the terms bark, mobile, and European in the second argument position, and every quantifier. For alternating second argument datasets, I have all predicates in the first argument position, and the pairs bark\u2013animate, French\u2013 European, and Parisian\u2013French in the second argument position. The datasets in which there is an alternation in both positions fall into two categories. In some, the entailment relations between the arguments work in the same direction (some dog French \u228f some animal European), in others, they work in opposite directions (some dog European # some animal French).\nQuantifier substitution: These datasets, exemplified in 3 above, show the interactions between each possible pair of different quantifiers, with the same predicates on both sides. Datasets exist with bark, mobile, and European in the second argument position, and within each dataset every possible predicate is shown in first argument position.\nMonotonicity with quantifier substitution: These datasets show the more complex monotonicity interactions that emerge when reasoning between phrases with differing arguments and differing quantifiers. Exhaustively creating datasets of this kind involves too large an amount of data to readily verify manually, so I sampled from the extensive set of possible combinations of the six quantifiers and the fixed argument fillers used in the monotonicity datasets. Each possible relation but \u2018\u2261\u2019 (which does not apply sentences like these unless the predicates on both sides are identical) is expressed, as is every possible pair of quantifiers.\nNegation: To show the interaction of negation and monotonicity, I included variants of many of the datasets described above in which one of the four argument positions is systematically negated, as demonstrated in example 4."}, {"heading": "5 Experiments and results", "text": "In the simplest experimental setting, which I label ALL-SPLIT, I randomly sample 85% of each of the datasets, train the model on those 85% portions and evaluate on the remaining 15% of the data. This setting is meant to test whether the model is able to correctly generalize the individual reasoning patterns represented by each of the datasets.\nPerformance on this setting is perfect: the model quickly converges to 100% accuracy on the test data, showing that the it is capable of accurately learning to capture all of the reasoning patterns in the data. The remaining experimental settings serve to determine whether what is learned captures the underlying logical structure of the data in a way that allows it to accurately label unseen kinds of reasoning pattern. In each of them, I choose one of three target datasets (all involving quantifier substitution) to test on, and then hold out that dataset and potentially other similar datasets from the training data in an attempt to discover just how different a test example can be from anything seen in\ntraining and still be classified accurately. Table 3 shows what information is included in the training data for each of the four settings for one of the three target datasets.\nSET-OUT: In these experiments, I hold out the target dataset, training on none of it and testing on all of it, and additionally split each remaining dataset as in ALL-SPLIT. This setting is meant to test whether the model can generalize a broader reasoning pattern from one dataset to another\u2014the model will still have seen other similar quantifier substitution datasets that differ from the target dataset only in which filler word is placed in the second argument position, as in rows 4 and 5 in Table 3 above.\nSUBCLASS-OUT: In these experiments, I hold out the target dataset as well as all of datasets representing the same reasoning pattern as the target dataset, and split each remaining dataset as in ALL-SPLIT. For the target dataset (most x) bark | (no x) bark in this setting, all of the datasets of the general form (most x) y | (no x) y would be held out.\nPAIR-OUT: In these experiments, I hold out all of the datasets that demonstrate the interaction between a particular pair of quantifiers. For the target dataset (most x) bark | (no x) bark in this setting, all examples with most on one side and no on the other, in either order, with and without negation or differing predicates, are held out from training. The remaining datasets are split as above.\nThese last three experiments have multiple sources of test data: some from the 15% test portions of the datasets that were split, some from the target datasets, and some from any other similar datasets that were held out. I report results on the entire test dataset, on the single target dataset, and on all of the held out datasets (i.e., the training data excluding the 15% portions) in aggregate. This third figure is identical to the second for the SET-OUT experiments, since the only held out dataset is the target dataset.\nResults on these generalization experiments are reported in Table 4. The model did not converge well for any of these experiments: convergence can take hundreds or thousands of passes through the data, and the performance of the model at convergence on test data was generally worse than its performance during the first hundred or so iterations. To sidestep this problem somewhat, I report results here for the models learned after 64 passes through the data. Even so, results are somewhat difficult to interpret: the performance on the test data fluctuates substantially between iterations, so a model that performs very poorly at iteration 64 may perform well at iteration 70. It is possible to stabilize the model by manipulating the hyperparameters away from their present tuned settings\n(raising minibatch size, shrinking model size, and increasing regularization, for example), but doing so consistently hurts performance.\nPerformance on the SET-OUT setting was promising: perfect and near-perfect generalization on two of the datasets suggest that the model is basically capable of generalizing in this way, and it is possible that a different choice of stopping criteria could elicit perfect performance on the third dataset. Performance on SUBCLASS-OUT was more mixed, and generalization of this type seems possible, but not reliably learnable in the experimental setting described here. Finally, performance on PAIR-OUT was very consistently poor, and the results of this experiment do not support the conclusion that the models tested here are capable of generalizing in this way."}, {"heading": "6 Discussion", "text": "The results from the experiments described above show that recursive neural tensor networks are able to learn lexical representations that support logical reasoning. The model can label unseen examples of inference correctly with perfect accuracy.\nThe representations that the model learns are not truly general-purpose, though. Human speakers of English who know the meanings of all of the words in a sentence are able to reason about whether that sentence entails any other sentence of English even if they have never seen either sentence before, or have never used the particular pattern of inference before. This broad ability to generalize does not appear to have been learned in these experiments. Pessimistically, the model might be primarily memorizing the reasoning patterns themselves, with any generalization beyond the SETOUT setting being erratic and case-specific. Optimistically, though, the model might be learning to handle logic in a way more like MacCartney\u2019s formal approach, in which the model must know dozens of different possible inference steps involving specific quantifiers and entailment patterns to succeed: if the model hasn\u2019t seen the substitution of most with no, it doesn\u2019t have the tools to reason about that substitution. An incomplete ability to generalize is not necessarily detrimental to a model\u2019s ability to support reasoning in practice, especially in this later interpretation, since the body of knowledge used to do inference in MacCartney\u2019s approach is small enough that it could realistically be learned.\nIt remains an open research question whether generalization of the type seen in the PAIR-OUT case is possible for any model. A model that could successfully generalize in this way could be potentially valuable both as a more versatile reasoning tool, and as a source of information about the ways in which functional types like quantifiers can be represented in vector space models. There are two ways that this problem can be addressed, one practical and one formal.\nAn obvious direction for future work is to test for this behavior in more powerful models, or to see whether a dataset containing a more diverse array of reasoning patterns (beyond those possible just with quantification and negation) could encourage generalization. Running experiments like the ones presented above on models with matrices or higher order tensors as word representations [20, 21] might be a promising start. It is also not impossible that even an RNTN like the one studied could generalize perfectly: the dramatic instability of the model over the course of training suggests that better approaches to learning could improve performance.\nIn a more formal direction, a thorough study of MacCartney\u2019s system of projectivity and its underlying logic (following Icard [22]) could lead to a clearer theoretical bound on what can be expected from learned models. This might include for instance more explicit generalizations about what kinds of examples of a word like most in an inference setting are minimally sufficient to allow an idealized model to learn a general purpose definition of that word."}, {"heading": "Acknowledgments", "text": "I owe thanks to Chris Manning and Chris Potts for their advice at every stage of this project, to Richard Socher and Jeffrey Pennington for extensive and helpful discussions, and to J.T. Chipman for help in designing a pilot experiment."}, {"heading": "Appendix A: The syntactically untied model", "text": "I additionally ran some the experiments described above using a potentially more powerful variant of the model, following Socher et al. [19], with three separately-parametrized composition functions instead of a single universal function.\nAll instances of the composition layer use the same basic RNTN layer structure, including the sigmoid nonlinearity, but the parameters are chosen from one of three sets as follows:\n\u2022 Negation composition parameters: Used in composing not with any predicate, as in not + European or not + dog.\n\u2022 Quantifier first argument parameters: Used in composing a quantifier with its first argument (a single word or a two-word phrase with not), as in all + dog or some + (not European).\n\u2022 Quantifier second argument parameters: Used in composing a quantifier phrase (a quantifier with its first argument) with the quantifier\u2019s second argument (a single word or a two-word phrase with not), as in all dog + bark or most sofa + not indestructible.\nAppendix B: Wordlist\nhippo hungry non-ape ape animal dog cat unable able Thai Asian bird crow sofa couch snail vertebrate invertebrate bark (i.e. barks) puppy feline mobile immobile dead live European French Parisian human animate inanimate seat bench woody plant tree oak bush destructible a indestructible mammal\nThis wordlist is part of a larger ongoing experiment, and is designed such that each of the seven of the relation types holds between some pair of words (for example, animal ` non-ape). Most of these relations are not included in the data used for this experiment. Since I do not train the model on a list of pairs of single words, lexical knowledge only comes in indirectly through monotonicity reasoning examples. For example, if the model sees (some dog) bark \u228f (some animal) bark, it could infer from that that dog \u228f animal.\nI avoid specifying most of the possible relations between words by any means to avoid giving the model so much background knowledge that it can evaluate whether a sentence it is trying to reason about is true. Though the task is defined such that the ground truth of the sentences being compared is irrelevant, mixing sentences that are known to be true with those that are not known to be true may provide unnecessary added difficulties in learning. For example, I do not train the model on all dog bark \u228f all hungry bark (thus teaching the model dog \u228f hungry), since this would make it potentially problematic to use later train the model on examples like some dog hungry \u228f some animal hungry where the left side is known to be true a priori."}], "references": [{"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proc. ICML. ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proc. EMNLP. ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. EMNLP. EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. ICLR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Frege in space: A program for compositional distributional semantics. Submitted, draft at http://clic", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "cimec. unitn. it/composes,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Natural language inference", "author": ["Bill MacCartney"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Monotonicity phenomena in natural language", "author": ["Jack Hoeksema"], "venue": "Linguistic Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "Categorial grammar and natural reasoning. ILTI Publication Series for Logic, Semantics, and Philosophy of Language LP-91-08", "author": ["V\u0131\u0301ctor S\u00e1nchez-Valencia"], "venue": "University of Amsterdam,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of NAACL-HLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. EMNLP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In Proc. EACL. ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A formal approach to linking logical form and vector-space lexical semantics", "author": ["Dan Garrette", "Katrin Erk", "Raymond Mooney"], "venue": "Computing Meaning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In Proc. ICML", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Proc. IEEE International Conference on Neural Networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Updating quasi-Newton matrices with limited storage", "author": ["Jorge Nocedal"], "venue": "Mathematics of computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1980}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 1, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 2, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 3, "context": "Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4].", "startOffset": 152, "endOffset": 164}, {"referenceID": 4, "context": "Much of the work to date analyzing vector representations for words (see [5]) has focused on lexical semantic behaviors\u2014like the similarity between words like Paris and France.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "Natural language inference (NLI), the ability to reason about the truth of a statement on the basis of some premise, is among the clearest examples of a task that requires comprehensive and accurate natural language understanding [6].", "startOffset": 230, "endOffset": 233}, {"referenceID": 5, "context": "I borrow the structure of the task from MacCartney [6].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "framework of monotonicity inference [7, 8] delimits a means of computing inferences in a broad set of cases, and I focus in this paper on reproducing this kind of inference in a learned model.", "startOffset": 36, "endOffset": 42}, {"referenceID": 7, "context": "framework of monotonicity inference [7, 8] delimits a means of computing inferences in a broad set of cases, and I focus in this paper on reproducing this kind of inference in a learned model.", "startOffset": 36, "endOffset": 42}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Monotonicity is a central insight from work on natural logic [8, 6], a theoretical framework for natural language inference that uses natural language strings as the logical symbols, rather than relying on conversion to and from first order logic or a similar system.", "startOffset": 61, "endOffset": 67}, {"referenceID": 5, "context": "Monotonicity is a central insight from work on natural logic [8, 6], a theoretical framework for natural language inference that uses natural language strings as the logical symbols, rather than relying on conversion to and from first order logic or a similar system.", "startOffset": 61, "endOffset": 67}, {"referenceID": 8, "context": "[9] shows exactly this sort of behavior in VSMs learned for language modeling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In the standard formulation of the task (and the one used in the RTE datasets [10]), the goal is to determine whether a reasonable human would infer a hypothesis from a premise.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "[11] show in an aside that a Boolean logic with negation and conjunction can be learned in a minimal recursive neural network model with one-dimensional (scalar) representations for words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The natural logic engine at the core of MacCartney\u2019s [6] NLI system requires a complex set of linguistic knowledge, much of which takes the form of what he calls projectivity signatures.", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "[12] have achieved limited success in building a classifier to judge entailments between one- and two-word phrases (including some with quantifiers), though their vector representations were crucially based on distributional statistics and were not learned for the task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] propose a way to improve standard discrete NLI with vector representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], which is meant to mimic the recursive construction of meanings in formal models of semantics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] to the task in future work).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] (itself adapted from Socher et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3]) and shown below.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "In particular, I use the leaky rectified linear function [15]: fb(~x) = max(~x, 0) + 0.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "The model is trained using backpropagation through structure [16], wherein the negative log of the probability assigned to the correct label is taken as a cost function, and the gradient of each parameter with respect to that cost function is computed at each node, with information passing down the tree", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "Optimization: I train the model with stochastic gradient descent (SGD), with gradients pooled from randomly chosen minibatches of 32 training examples (chosen empirically), and learning rates computed using AdaGrad [17].", "startOffset": 215, "endOffset": 219}, {"referenceID": 17, "context": "L-BFGS [18] was tried unsuccessfully as an alternative to SGD.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "I additionally reran the experiments with a variation of the network that uses different composition functions to compose different types of phrase, following [19], but saw no associated improvement in performance.", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "Running experiments like the ones presented above on models with matrices or higher order tensors as word representations [20, 21] might be a promising start.", "startOffset": 122, "endOffset": 130}, {"referenceID": 20, "context": "Running experiments like the ones presented above on models with matrices or higher order tensors as word representations [20, 21] might be a promising start.", "startOffset": 122, "endOffset": 130}], "year": 2017, "abstractText": "Recursive neural network models and their accompanying vector representations for words have seen success in an array of increasingly semantically sophisticated tasks, but almost nothing is known about their ability to accurately capture the aspects of linguistic meaning that are necessary for interpretation or reasoning. To evaluate this, I train a recursive model on a new corpus of constructed examples of logical reasoning in short sentences, like the inference of some animal walks from some dog walks or some cat walks, given that dogs and cats are animals. The results are promising for the ability of these models to capture logical reasoning, but the model tested here appears to learn representations that are quite specific to the templatic structures of the problems seen in training, and that generalize beyond them only to a limited degree.", "creator": "LaTeX with hyperref package"}}}