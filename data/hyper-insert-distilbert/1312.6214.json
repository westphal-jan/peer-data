{"id": "1312.6214", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Volumetric Spanners: an Efficient Exploration Basis for Learning", "abstract": "numerous machine learning proving problems globally require an adequate exploration basis - a mechanism to clearly explore the appropriate action space. clearly we likewise define a novel geometric design notion of exploration basis with low variance, called volumetric spanners, globally and ultimately give efficient algorithms methods to construct scenarios such over a basis.", "histories": [["v1", "Sat, 21 Dec 2013 06:51:50 GMT  (27kb,D)", "http://arxiv.org/abs/1312.6214v1", null], ["v2", "Sun, 26 Jan 2014 12:16:59 GMT  (27kb,D)", "http://arxiv.org/abs/1312.6214v2", null], ["v3", "Sun, 25 May 2014 11:57:08 GMT  (36kb,D)", "http://arxiv.org/abs/1312.6214v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DS", "authors": ["elad hazan", "zohar karnin", "raghu mehka"], "accepted": false, "id": "1312.6214"}, "pdf": {"name": "1312.6214.pdf", "metadata": {"source": "CRF", "title": "Volumetric Spanners and their Applications to Machine Learning", "authors": ["Elad Hazan", "Zohar Karnin", "Raghu Meka"], "emails": ["ehazan@ie.technion.ac.il", "zkarnin@ymail.com", "meka@microsoft.com"], "sections": [{"heading": null, "text": "the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases.\nWe show how efficient volumetric spanners give rise to the first efficient and optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set."}, {"heading": "1 Introduction", "text": "A fundamental difficulty in machine learning is environment exploration. A prominent example is the famed multi-armed bandit (MAB) problem, in which a decision maker iteratively chooses an action from a set of available actions and receives a payoff, without observing the payoff of all other actions she could have taken. The MAB problem displays an exploration-exploitation tradeoff, in which the decision maker trades exploring the action space vs. exploiting the knowledge already obtained to pick the best arm.\nAnother example in which environment exploration is crucial, or perhaps the main point, is active learning and experiment design. In these fields it is important to correctly identify the most informative queries so as to efficiently construct a solution.\nExploration is hardly summarised by picking an action uniformly at random. Indeed, sophisticated techniques from various areas of optimization, statistics and convex geometry have been applied to designing ever better exploration algorithms. To mention a few: Awerbuch and Kleinberg [3] devise the notion of barycentric spanners, and use this construction to give the first low-regret algorithms for complex decision problems such as online routing. Abernethy, Hazan and Rakhlin [1] use self-concordant barriers to build an efficient exploration strategy for convex sets in Euclidean space. Bubeck, Cesa-Bianchi and Kakade [7] apply tools from convex geometry, namely John\u2019s ellipsoid to construct optimal regret algorithms for bandit linear optimization (albeit not always efficiently).\nIn this paper we consider a generic approach to exploration, and quantify what efficient exploration with low variance requires in general. We define a novel construction called volumetric spanners and give efficient algorithms to construct them. We further investigate the convex geometry implications of our construction, and define the notion of volumetric ellipsoid of a convex body. We give structural theorems on the existence and properties of these ellipsoids, as well as constructive algorithms to compute them in several cases.\nWe complement our findings with an application to machine learning, in which we resolve a well-studied open problem that has exploration as its core difficulty: an efficient and optimalregret algorithm for bandit linear optimization. As evidenced by this application, we expect that volumetric spanners and volumetric ellipsoids can be useful elsewhere in experiment design and active learning.\nBandit Linear Optimization Bandit linear optimization is a fundamental problem in decision making under uncertainty that efficiently captures structured action sets. The canonical example\nar X\niv :1\n31 2.\n62 14\nv1 [\ncs .L\nG ]\n2 1\nD ec\n2 01\n3\nis that of online routing in graphs: a decision maker iteratively chooses a path in a given graph from source to destination, the adversary chooses lengths of the edges of the graph, and the decision maker receives as feedback the length of the path she chose but no other information (see [3]). Her goal over many iterations is to attain an average travel time as short as that of the best fixed shortest path in the graph.\nThis decision problem is readily modeled in the \u201cexperts\u201d framework, albeit with efficiency issues: the number of possible paths is potentially exponential in the graph representation. The BLO framework gives an efficient model for capturing such structured decision problems: iteratively a decision maker chooses a point in a convex set and receives as a payoff an adversarially chosen linear cost function. In the particular case of online routing, the decision set is taken to be the s-t-flow polytope, which captures the convex hull of all source-destination shortest paths in a given graph, and has a succinct representation with polynomially many constraints and low dimensionality. The linear cost function corresponds to a weight function on the graphs edges, where the length of a path is defined as the sum of weights of its edges.\nThe BLO framework captures many other structured problems efficiently, i.e. learning permutations, rankings and other examples (see [1]). As such, it has been the focus of much research in the past few years. The reader is referred to the recent survey of Bubeck and Cesa-Bianchi [6] for more details on algorithmic results for BLO.\nIn this paper we contribute to the large literature on the BLO model by giving the first efficient and optimal regret algorithm for BLO over general decision sets. Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery. We next describe how to use novel convex geometric concepts to attain the best of both worlds."}, {"heading": "1.1 Introduction to Exploration: Barycentric Spanners and John\u2019s Ellipsoid", "text": "In this section we review two geometric constructions that have been used in previous work in machine learning for designing an exploration basis for a body K \u2208 Rd. The first is the ellipsoid corresponding to the barycentric spanner of K which is defined as the ellipsoid of maximum volume, supported by exactly d points from K. The second is the minimum volume enclosing ellipsoid (MVEE) also known as John\u2019s ellipsoid.\nAs we show later on, our definition of a volumetric spanner enjoys properties of both objects. Similar to barycentric spanners, it is supported by a small (quasi-linear) set of points of K. Simultaneously and unlike the barycentric counterpart, the volumetric ellipsoid contains the body K, a property shared with John\u2019s ellipsoid."}, {"heading": "1.2 Barycentric Spanners", "text": "The notion of Barycentric spanners was introduced in the work of [3], in order to define an exploration basis for an online shortest path problem. Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.\nDefinition 1.1. A barycentric spanner of K \u2286 Rd is a set of d points S = {u1, . . . , ud} \u2286 K such that any point in K may be expressed as a linear combination of the elements of S using coefficients in [\u22121, 1]. For C > 1, S is a C-approximate barycentric spanner of K if any point in K may be expressed as a linear combination of the elements of S using coefficients in [\u2212C,C]\nIn [3] it is shown that any compact set has a barycentric spanner. Moreover, they show that given an oracle with the ability to solve linear optimization problems over K, an approximate barycentric spanner can be efficiently obtained. In the following sections we will use this constructive result.\nTheorem 1.1 (Proposition 2.5 in [3]). Let K be a compact set in Rd that is not contained in any proper linear subspace. Given an oracle for optimizing linear functions over K, for any C > 1, it is possible to compute a C-approximate barycentric spanner for K, using O(d2 logC(d)) calls to the optimization oracle."}, {"heading": "1.3 The Fritz John Ellipsoid", "text": "The John ellipsoid is the unique ellipsoid of smallest volume containing a given convex body in Euclidean space. Its properties have been the subject of study in convex geometry since John\u2019s work [14] (see [4] and [13] for historic information).\nSuppose that we have linearly transformed K such that its minimum volume enclosing ellipsoid is the unit ball (in convex geometric terms, K is in John\u2019s position). Then Johns theorem asserts the surprising fact that\n1 d Bn \u2286 K \u2286 Bn,\nwhere Bn denotes the unit ball in Rn. Furthermore, for symmetric convex bodies (which are far more abundant in machine learning), the factor 1d above can be replaced by 1\u221a d .\nJohn\u2019s ellipsoid and in particular its contact points with the convex body it encapsulates makes for an appealing exploration basis, and indeed [7] have used exactly this machinery to attain an optimal-regret bandit linear optimization algorithm. Unfortunately we know of no efficient algorithm to compute, or even approximate up to a constant, the John ellipsoid for a general convex set, thus the latter result does not give a polynomial time algorithm for BLO.\nThe following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11]. Henceforth, let Id denote the d \u00d7 d identity matrix.\nTheorem 1.2. [4] Let K \u2208 Rd be a symmetric convex set and assume that the unit sphere is its minimal enclosing ellipsoid. Then there exist m \u2264 d(d + 1)/2 \u2212 1 contact points of K and the sphere u1, . . . , um and a vector c \u2208 Rm such that c \u2265 0, \u2211 ci = d and \u2211 ciuiu T i = Id.\nThe computation of the linear transformation that makes the unit sphere to be the minimum volume enclosing ellipsoid (MVEE) is not known to be efficient in general, nor are the contact points known to be efficiently computable. For our construction of volumetric spanners and the volumetric ellipsoid that we define later on, it suffices to compute the MVEE of a discrete symmetric set, which is known to be efficiently computable. We make use of the following (folklore) algorithmic result:\nTheorem 1.3 (folklore, see e.g. [16, 8]). Let K \u2286 Rd be a set of n points. It is possible to compute an \u03b5-approximate MVEE for K (an enclosing ellipsoid of volume at most (1 + \u03b5) that of the MVEE) in time O(n3.5 log 1\u03b5 ).\nThe latter is attainable via the ellipsoid method or path-following interior point methods (see references in theorem statement). An approximation algorithm rather than an exact one is necessary in a real-valued computation model, and the logarithmic dependence on the approximation guarantee is as good as one can hope for in general.\nNote that this theorem addresses the case of a discrete set of points and/or their convex hull, rather than general convex sets. For our purposes, henceforth it suffices to consider this case: we use a different methodology to build an exploration basis for general convex sets.\nThus, the above theorem allows us to efficiently compute a linear transformation such that the MVEE of K is essentially the unit sphere. We can then use linear programming to compute an approximate representation as follows:\nTheorem 1.4. Let {x1, . . . , xn} = K \u2286 Rd be a set of n points and assume that:\n1. K is symmetric, i.e. if x \u2208 K then also \u2212x \u2208 K.\n2. The John Ellipsoid of K is the unit ball.\nThen it is possible, in O(( \u221a n+ d)n3) time, to compute a vector c \u2208 Rn such that:\n1. c \u2265 0 2. \u2211 ci \u2264 d\n3. \u2211n i=1 cixix > i = Id\nProof. Denote the MVEE of K by E and let V be its corresponding d \u00d7 d matrix, meaning V is such that \u2016y\u20162E = y>V \u22121y \u2264 1 for all y \u2208 K. By our assumptions Id = V .\nAs K is symmetric and its MVEE is the unit ball, according to Theorem 1.2, there exist m \u2264 d(d + 1)/2 \u2212 1 contact points u1, . . . , um of K with the unit ball and a vector c\u2032 \u2208 Rm such that c\u2032 \u2265 0, \u2211 c\u2032i = d and \u2211 c\u2032iuiu T i = Id. It follows that the following LP has a feasible\nsolution: Find c \u2208 Rn such that c \u2265 0, \u2211 ci \u2264 d and \u2211 ciuiu T i = Id. The described LP has\nO(n + d2) constraints and n variables. It can thus be solved in time O(d + \u221a n)n3) via interior point methods."}, {"heading": "1.4 Structure of the paper", "text": "In the next section we dive into convex geometry and define the key notions behind our constructions. Following that, we list preliminaries and known results from measure concentration, convex geometry and online learning in section 3. In section 4 and 5 we give the construction of an efficient volumetric spanner for, respectively, continuous and discrete sets. We then proceed to describe an application to bandit linear optimization in section 6."}, {"heading": "2 Volumetric Ellipsoids and Spanners", "text": "This section gives the main convex geometry constructions that we apply to machine learning. Consider a set K \u2286 Rd in Euclidean space. We mainly consider the case in which K = conv{v1, ..., vn} is the convex hull of n points. One of the most well studied objects in convex geometry is the John ellipsoid, defined as the enclosing ellipsoid of smallest volume for K. The Fritz John theorem characterises this ellipsoid and its properties, and we shall make used of it in this section to construct exploration basis for certain decision sets.\nBefore doing so, we define yet another shape, of a more discrete nature, that fundamentally characterises a convex set. Given a set of vectors S = {v1, ..., vt}, we denote by E(S) the ellipsoid defined by them, i.e.\nE(S) = { x \u2208 Rd such that x =\n\u2211 i\u2208S \u03b1ivi , \u2211 i \u03b12i \u2264 1\n}\nthat is, the ellipsoid given by all vectors spanned by the set S with Euclidean norm at most one. To see that this is indeed an ellipsoid, consider a point x such that x \u2208 E(S). Then x = V \u03b1, where V is the matrix whose columns are all vectors of S, and \u03b1 \u2208 Bt. Thus, \u03b1 = V \u22121x, where V \u22121 is the Moore-Penrose pseudo-inverse of V . By the bound on the norm of \u03b1, we have\n1 \u2265 \u2016\u03b1\u20162 = \u2016V \u22121x\u20162 = x>(V V >)\u22121x\nwhich is exactly the definition of an ellipsoid with defining norm (V V >)\u22121 = ( \u2211 i\u2208S viv > i ) \u22121 (note that V V > is a full rank matrix since S spans the space). When discussing this norm we shall\nalso use the notation \u2016x\u2016E(S) \u2206 = \u221a x>(V V >)\u22121x; indeed the described norm is that defined by the ellipsoid E(S).\nDefinition 2.1. Let K \u2286 Rd be a set in Euclidean space. For S \u2286 K, we say that E(S) is a volumetric ellipsoid for K if it contains K. We say that EK = E(S) is a a minimal volumetric ellipsoid if it is a containing ellipsoid defined by a set of minimal cardinality\nEK \u2208 min |S| {E(S) such that S \u2286 K \u2286 E(S)} .\nWe say that |S| is the order of the minimal volumetric ellipsoid or of the convex set1 K denoted order(K).\nWe first make a few observations about the above notion of order:\n\u2022 The definition of order is linear invariant: for any invertible linear transformation T : Rd \u2192 Rd and K \u2286 Rd, order(K) = order(TK). We defer the simple proof to preliminaries.\n\u2022 The minimum volumetric ellipsoid is not unique in general; see example in figure 2. Further, it is in general different from John\u2019s ellipsoid.\n\u2022 For non-degenerate convex sets K, their order is naturally lower bounded by d, and there are examples in which it is strictly more than d (e.g., figure 2).\nWe next state our main structural result in convex geometry giving a universal bound on the order of sets. The result is proved in the next section.\nTheorem 2.1 (Main). Any compact set K \u2286 Rd admits a volumetric ellipsoid of order O(d log d). A convex set K admits a volumetric ellipsoid of order O(d).\nFurther, if K = {v1, . . . , vn} is a discrete set, then a volumetric ellipsoid for K of order O(d log d) can be constructed in time poly(n, d).\nWe also give a different algorithmic construction for the discrete case in Section 5, which while being sub-optimal by logarithmic factors (gives an ellipsoid of order O(d(log d)(log n)) has the advantage of being much simpler and more efficient (no need for convex programming).\nThe above definition of volumetric ellipsoids is closely related to the kind of bases which allow for efficient exploration in our applications. To make this concrete and to simplify some terminology later on, we introduce the closely related notion of volumetric spanners. Informally, these correspond to sets S that spans all points in a given set with coefficients having Euclidean norm at most one, or formally:\nDefinition 2.2. Let K \u2286 Rd be a compact set and let S \u2286 K. We say that S is a volumetric spanner for K if E(S) \u2287 conv(K)\nIt is immediate from the definition of order and the above definition that a set K has a volumetric spanner of cardinality at most t if and only if order(K) \u2264 t."}, {"heading": "2.1 Existence and Construction of Volumetric Ellipsoids", "text": "In this section we prove our main structural result, Theorem 2.1. Let K \u2286 Rd be a compact set in Euclidean space. We are particularly interested in the discrete case in which K = {v1, ..., vn} (or equivalently K = conv{v1, . . . , vn} is the convex hull of n points), but the discussion below is general. For this section assume that K is symmetric and thus contains the origin.\n1We note that our definition allows for multi-sets, meaning that S may contain the same vector twice\nRecall from preceding sections that the John ellipsoid for set K is the minimum volume ellipsoid that contains K. Henceforth, assume that we have linearly transformed K such that this largest contained ellipsoid is the unit ball. Then Johns theorem says that\n1\u221a d Bd \u2286 K \u2286 Bd\nFurther, there are m = O(d2) points S = {ui} on the enclosing unit ball that intersect K, and satisfy: \u2211\ni\u2208S ciuiu\n> i = Id\nThis implies by taking trace that \u2211 i ci = d.\nLemma 2.1. Let T = {w1, ..., wq} \u2286 S be a multi-set obtained by sampling q i.i.d elements of S according to the induced distribution given by\nwi = ui w.p. ci d .\nIf q > Cd log(d) for some sufficiently large constant C, then with probability at least 2/3, T is a volumetric spanner for K.\nProof. Notice that the wi\u2019s are i.i.d, and E[dwiw > i ] = Id and \u2016 \u221a dwi\u20162 \u2264 d. It thus follows from Theorem 3.3 that for q \u2265 Cd log(d) for sufficiently large constant C, it holds with probability at least 2/3 that\nd q \u00b7 \u2211 wj\u2208T wjw > j 1 2 Id\nLet U be the matrix whose columns are vectors of T , thus UUT = \u2211 j\u2208T wjw > j q 2dId. Since for positive definite matrices A B implies B\u22121 A\u22121 we get that for arbitrary x \u2208 K,\n\u2016x\u20162E(T ) = x >(UU>)\u22121x \u2264 \u2016x\u20162 \u00b7 2d\nq < 1\nThe last inequality holds since q > 2d and x \u2208 K is contained in the unit sphere and hence \u2016x\u2016 \u2264 1.\nNote: For discrete sets K the above existence proof can be made constructive using Theorem 1.4.\nAs corollaries of the above lemma and Theorem 1.4 we get the following.\nCorollary 2.1. Any compact set K has a volumetric spanner of size O(d log d). In particular, for any compact set K, order(K) = O(d log d).\nCorollary 2.2. For a discrete set K \u2286 Rd of size n, a volumetric spanner of size O(d log d), and hence a volumetric ellipsoid of order O(d log d), can be constructed in time O(n3.5 + dn3).\nThe above corollary is the one of most interest for our applications. For the case where K is convex and symmetric we can reduce the size of the spanner to O(d). The latter proof, however is not efficient and does not give a polynomial-time algorithm.\nTheorem 2.2. For convex and symmetric K there exist a volumetric spanner of size O(d).\nProof. We use the following non-constructive result stating that K can be approximated by a different symmetric body H having at most O(d) contact points with its MVEE.\nLemma 2.2 ([19]). For any convex body K \u2208 Rd there exist a body H \u2286 K \u2286 3H such that H has at most O(d) contact points with its minimal volume enclosing ellipsoid (MVEE). 2.\n2For our machine learning applications we can always assume w.l.o.g that K is symmetric, in which case the constant 3 can be reduced to 1 + \u03b5 for every constant \u03b5 > 0 independent of d.\nLet u1, . . . , um be the contact points of 3H with its MVEE, where m = O(d). Assume w.l.o.g that the ellipsoid is the unit sphere. By the above property of John\u2019s ellipsoid there exist c1, . . . , cm where ci \u2265 0, \u2211 ci = d and \u2211 i ciuiu T i = Id. Consider the multi-set T obtained by taking each\nvector ui/3 an amount of d3cie times. For this set, \u2211 v\u2208T vv\n> Id. It follows that for any x \u2208 K, as \u2016x\u2016 \u2264 1,\n\u2016x\u20162E(T ) = x >( \u2211 v\u2208T vv>)\u22121x \u2264 \u2016x\u20162 \u2264 1\nAlso, since H \u2286 K and ui/3 \u2208 H for all i, T \u2282 K. It follows that T is a volumetric spanner for K. As for its size, it is an easy task to verify that |T | \u2264 3d+m = O(d)."}, {"heading": "2.2 Approximate Volumetric Spanners", "text": "Above we show a construction of volumetric spanners based on a minimal volume ellipsoid. In the case of general convex bodies, it is not known how to obtain such an ellipsoid, even approximately. For such difficult cases, we show that a softer version of the notion is sufficiently useful. In this section we present two different types of approximations for a volumetric spanner. In both types we require a small support for the spanner of roughly linear size. In the first case we allow the ellipsoid to contain the body only after being expanded by some product. In the second approximation we allow a small fraction of the points of the body to be outside the ellipsoid.\nDefinition 2.3. A \u03c1-ratio-volumetric spanner S of K is a subset S \u2286 K such that for all x \u2208 K,\n\u2016x\u2016E(S) \u2264 \u03c1\nOne example for such an approximate spanner with \u03c1 = \u221a d is a barycentric spanner (Definition 1.1). In fact, it is easy to see that a C-approximate barycentric spanner is a C \u221a d-ratiovolumetric spanner . The following is immediate from Theorem 1.1.\nCorollary 2.3. Let K be a compact set in Rd that is not contained in any proper linear subspace. Given an oracle for optimizing linear functions over K, for any C > 1, it is possible to compute a C \u221a d-ratio-volumetric spanner S of K of cardinality |S| = d, using O(d2 logC(d)) calls to the optimization oracle.\nFor the second definition we describe a spanner that covers all but an \u03b5 fraction of the points in K and moreover, the measure of the points decays exponentially fast w.r.t their E(S)-norm. Since we are discussing a measure over the points of a body it makes sense not only to consider a uniform distribution over the body but an arbitrary one. As it turns out, the approximation can be efficiently obtained for any log-concave distribution.\nDefinition 2.4. Let K be a body in Rd and p a distribution over it. Let \u03b5 > 0. A (p, \u03b5)-expvolumetric spanner of K is a set S \u2286 K where for any \u03b8 > 1\nPr x\u223cp\n[\u2016x\u2016E(S) \u2265 \u03b8] \u2264 \u03b5\u2212\u03b8\nBelow we prove that such spanners can be efficiently obtained. Specifically we prove\nTheorem 2.3. Let K be a convex body in Rd and p a log-concave distribution over it. By sampling O(d+log2(1/\u03b5)) i.i.d. points from p one obtains, w.p. at least 1\u2212exp(\u2212 \u221a d), a (p, \u03b5)-exp-volumetric spanner for K. In particular, for general log-concave distribution p over convex K it is possible to compute a (p, \u03b5)-exp-volumetric spanner in time O\u0303(d5 + d3/\u03b44) with success probability of at least 1\u2212 exp(\u2212 \u221a d)\u2212 \u03b4."}, {"heading": "3 Preliminaries", "text": "For a convex set K, let X \u223c K denote a uniformly random vector from K. We denote by Id the d\u00d7 d identity matrix.\nDefinition 3.1. A distribution over Rd is log-concave when for its probability distribution function (pdf) p it holds that for any x, y \u2208 Rn, \u03bb \u2208 [0, 1],\np(\u03bbx+ (1\u2212 \u03bb)y) \u2265 p(x)\u03bbp(y)1\u2212\u03bb\nTwo cases of interest of log-concave distributions are (1) the uniform distribution over a convex body and (2) a distribution over a convex body where p(x) \u221d exp(L>x), where L is some vector in Rd. In the following sections we use a result showing that a log-concave distribution can be efficiently sampled.\nLemma 3.1 ([17], Theorems 2.1 and 2.2). Let p be a log-concave distribution over Rd and let \u03b4 > 0. An algorithm for approximate sampling p exist with the following properties\n1. The distance, in terms of total deviation between the produced distribution and actual distribution is no more than \u03b4. That is, the difference between the probabilities of any event in the produced and actual distribution is bounded by \u03b4.\n2. The algorithm requires a pre-processing time of O\u0303(d5).\n3. A single sample can be produced in time O\u0303(d4/\u03b44), or amortized time of O\u0303(d3/\u03b44) if more than d samples are needed.\nDefinition 3.2 (Isotropic position). A random variable x is said to be in isotropic position (or isotropic) if\nE[x] = 0, E[xx>] = Id.\nA set K \u2286 Rd is said to be in isotropic position if x \u223c K is isotropic. Similarly, a distribution p is in isotropic position if x \u223c p is isotropic.\nHenceforth we use several results regarding the concentration of log-concave isotropic random vectors. We use slight modification where the center of the distribution is not necessarily in the origin. For completeness we present the proof of the modified theorems in Appendix A\nTheorem 3.1 (Theorem 4.1 in [2]). Let p be a log-concave distribution over Rd in isotropic position. There is a constant C such that for all t, \u03b4 > 0, the following holds for n = Ct\n4d log2(t/\u03b4) \u03b42 .\nFor independent random vectors x1, . . . , xn \u223c p, with probability at least 1\u2212 exp(\u2212t \u221a d),\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 xix > i \u2212 Id \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4. Corollary 3.1. Let p be a log-concave distribution over Rd and x \u223c p. Assume that x is such that E[xxT ] = Id. Then, there is a constant C such that for all t \u2265 1, \u03b4 > 0, the following holds for n = Ct\n4d log2(t/\u03b4) \u03b42 . For independent random vectors x1, . . . , xn \u223c p, with probability at least\n1\u2212 exp(\u2212t \u221a d), \u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 xix > i \u2212 Id \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4. Theorem 3.2 (Theorem 1.1 in [12]). There exist constants c, C such that the following holds. Let p be a log-concave distribution over Rd in isotropic position and let x \u223c p. Then, for all \u03b8 \u2265 0,\nPr [\u2223\u2223\u2223\u2016x\u2016 \u2212 \u221ad\u2223\u2223\u2223 > \u03b8\u221ad] \u2264 C exp(\u2212c\u221ad \u00b7min(\u03b83, \u03b8)).\nCorollary 3.2. Let p be a log-concave distribution over Rn and let x \u223c p. Assume that E[xxT ] = Id. Then for some universal C, c it holds for any \u03b8 \u2265 3 that\nPr [ \u2016x\u2016 > \u03b8 \u221a d ] \u2264 C exp ( \u2212c\u03b8 \u221a d )\nThe following theorem provides a concentration bound for random vectors originating from an arbitrary distribution.\nTheorem 3.3 ([18]). Let X be a vector-valued random variable over Rd with E[XX>] = \u03a3 and \u2016\u03a3\u22121/2X\u20162 \u2264 R. Then, for independent samples X1, . . . , XM from X, and M \u2265 CR log(R/\u03b5)/ 2 the following holds with probability at least 1/2:\u2225\u2225\u2225\u2225\u2225 1M M\u2211 i=1 Xi \u2212 \u03a3\n\u2225\u2225\u2225\u2225\u2225 \u2264 \u2016\u03a3\u2016. The following simple observation shows that the notion of order defined in Definition 2.1 is a\nlinearly invariant notion.\nObservation 3.1. Let T : Rd \u2192 Rd be an invertible linear transformation and let K \u2286 Rd be a non-degenerate convex body. For any S \u2282 K, K \u2286 E(S) iff TK \u2286 E(TS). In particular, order(TK) = order(K).\nProof. Let S \u2286 K be such that K \u2286 E(S). Then, clearly TK \u2286 E(TS). Thus, order(TK) \u2264 order(K). The same argument applied to T\u22121 and TK shows that order(K) \u2264 order(TK)."}, {"heading": "4 Algorithmic Construction for Convex Sets", "text": "In this section we provide a construction for (p, \u03b5)-exp-volumetric spanner (as in Definition 2.4). In what follows we prove Theorem 2.3. We start by providing a more technical definition of a spanner. Note that unlike previous definitions, the following is not impervious to linear operators and will only be used to aid our construction.\nDefinition 4.1. A \u03b2-relative-spanner is a discrete subset S \u2286 K such that for all x \u2208 K, \u2016x\u20162E(S) \u2264 \u03b2\u2016x\u20162.\nA first step is a spectral characterization of relative spanners:\nLemma 4.1. Let S = {v1, ..., vT } \u2286 K span K and be such that\nW = T\u2211 i=1 viv > i\n1 \u03b2 Id\nThen S is a \u03b2-relative-spanner.\nProof. Let V \u2208 Rd\u00d7T be a matrix whose columns are the vectors of S. As V V > = W 1\u03b2 Id we have that\n\u03b2Id (V V >)\u22121\nIt follows that \u2016x\u2016E(S) = x>(V V >)\u22121x \u2264 \u03b2\u2016x\u20162\nas required.\nAlgorithm 1\n1: Input: An oracle to x \u223c p, where p is a distribution over K, T \u2208 N. 2: for t = 1 to T do 3: Choose ut \u223c p 4: end for 5: return S = {u1, ..., uT }.\nproof of Theorem 2.3. We analyze Algorithm 1, previously defined within Theorem 2.3 assuming the vectors are sampled exactly according to the log-concave distribution. The result involving an approximate sample, which is necessary for implementing the algorithm in the general case, is an immediate application of Lemma 3.1.\nOur analysis of the algorithm is for T = C(d+log2(1/\u03b5)) samples, where C is some sufficiently\nlarge constant. Assume first that Ex\u223cp[xx >] = Id. Let W = \u2211T i=1 uiu > i . Then, for C > 0 large\nenough, by Corollary 3.1, \u2016 1TW \u2212 Id\u2016 \u2264 1/2 w.p. at least 1 \u2212 exp(\u2212 \u221a d). Therefore, S spans Rd and 1\nT W Id \u2212\n1 2 Id = 1 2 Id\nThus according to Lemma 4.1, S is a (2/T )-relative spanner. Consider the case where \u03a3 = Ex\u223cp[xx\n>] is not necessarily the identity. By the above analysis we get that \u03a3\u22121/2S = {\u03a3\u22121/2u1, . . . ,\u03a3\u22121/2uT } form a (2/T )-relative spanner for \u03a3\u22121/2K. This is since the r.v defined as \u03a3\u22121/2x where x \u223c p is log-concave. The latter along with corollary 3.2 implies that for any \u03b8 \u2265 1,\nPr x\u223cp\n[ \u2016\u03a3\u22121/2x\u2016 \u2265 3\u03b8 \u221a d ] \u2264 c1 exp ( \u2212c2\u03b8 \u221a d )\n(1)\nfor some universal constants c1, c2 > 0. It follows that for our set S and any \u03b8 \u2265 1,\nPr x\u223cp\n[ \u2016x\u2016E(S) > \u03b8 ] = Prx\u223cp [ \u2016\u03a3\u22121/2x\u2016E(\u03a3\u22121/2S) > \u03b8 ] \u2016x\u2016E(S) = \u2016\u03a3\u22121/2x\u2016E(\u03a3\u22121/2S)\n\u2264 Prx\u223cp [ \u2016\u03a3\u22121/2x\u2016 > \u03b8 \u221a T/2 ] \u03a3\u22121/2S is a 2/T -relative-spanner\n= Prx\u223cp\n[ \u2016\u03a3\u22121/2x\u2016 > 3\u03b8 \u221a d \u221a\nC 18 \u00b7\n\u221a 1 + log\n2(1/\u03b5) d\n] T = C(d+ log2(1/\u03b5))\n\u2264 c1 exp ( \u2212c2\u03b8 \u221a d \u221a C 18 \u00b7 \u221a 1 + log 2(1/\u03b5) d ) Equation (1), C \u2265 18\n\u2264 exp ( \u2212\u03b8 \u221a d+ log2(1/\u03b5) ) C sufficiently large\n\u2264 \u03b5\u2212\u03b8"}, {"heading": "5 Algorithmic Construction for Discrete Sets", "text": "In this section we describe an algorithm that constructs volumetric spanners for discrete sets. The order of the spanners we construct is suboptimal, and not as good as mentioned attainable in polynomial time in previous sections. However, the algorithm is particularly simple and efficient to implement without appealing to convex programming techniques, and it is suboptimal only by logarithmic factors (a factor of log n to be precise).\nAlgorithm 2\n1: Input K = {x1, ..., xn} \u2286 Rd. 2: if n < Cd log d then 3: return S \u2190 K 4: end if 5: Compute \u03a3 = \u2211 i xix > i and let ui = \u03a3\n\u22121/2xi. 6: For i \u2208 [n], let pi = 1/2n+\u2016ui\u20162/2d. Let S be a random set obtained by drawing M = Cd log d\nsamples with replacement from [n] according to the distribution p1, . . . , pn. 7: Verify that for at least n/2 vectors from {x1, . . . , xn}, it holds that \u2016xi\u2016E(S) \u2264 1. If that is\nnot the case discard S and repeat the above step. 8: Apply the algorithm recursively on the data points for which \u2016xi\u2016E(S) > 1.\nTheorem 5.1. Given a set of vectors K = {x1, . . . , xn} \u2208 Rd, Algorithm 2 outputs a volumetric spanner of size O((d log d)(log n)) and has an expected running time of O(nd2).\nProof. Consider a single iteration of the algorithm with input v1, . . . , vn \u2208 Rd. We claim that the random set S obtained in step 6 satisfies the following condition with constant probability:\nPr x\u2208K\n[ \u2016x\u2016E(S) \u2264 1 ] \u2265 1/2 (2)\nSuppose the above statement is true. Then, the lemma follows easily as it implies that for the next iteration there are fewer than n/2 vectors. Hence, after (log n) recursive calls we will have a volumetric spanner. The total size of the set will be O((d log d)(log n)). To see the time complexity, consider a single run of the algorithm. The most computationally intensive steps are computing \u03a3 and \u03a3\u22121/2 which take time O(nd2) and O(d3) respectively. We also need to compute ( \u2211 v\u2208S vv\n>)\u22121 (to compute the E(S) norm) which takes time O(d3 log d), and compute the E(S) norm of all the vectors which requires O(nd2). As n = \u2126(d log(d)), it follows that a single iteration runs of a total expected time of O(nd2). Since the size of n is split in half between iterations, the claim follows.\nWe now prove that Equation 2 holds with constant probability\nx>j (\u2211 v\u2208S vv> )\u22121 xj = u > j (\u2211 v\u2208S\u2032 vv> )\u22121 uj . (3)\nwhere S\u2032 = {\u03a3\u22121/2v|v \u2208 S} is the (linearly) shifted version of S. Therefore, it suffices to show that with sufficiently high probability, the right hand side of the above equation is bounded by 1 for at least n/2 indices j \u2208 [n].\nNote that pi = 1/2n+\u2016ui\u20162/2d form a probability distribution: \u2211 i pi = 1/2+( \u2211 i \u2016ui\u20162)/2d =\n1. Let X \u2208 Rd be a random variable with X = ui/ \u221a pi with probability pi for i \u2208 [n]. Then, E[XX>] = Id. Further, for any i \u2208 [n]\n\u2016ui\u20162/pi \u2264 2d.\nTherefore, by Theorem 3.3, if we take M = Cd(log d) samples X1, . . . , XM for C sufficiently large, then with probability of at least 1/2, it holds that\nM\u2211 i=1 XiX > i (M/2)Id.\nLet T \u2286 [n] be the multiset corresponding to the indices of the sampled vectors X1, . . . , XM . The above inequality implies that \u2211\ni\u2208T\n1 pi uiu > i (M/2)Id.\nNow, \u2211 v\u2208S\u2032 vv> (min i pi) \u2211 v\u2208S\u2032 1 pi vv> (min i pi)(M/2)Id (M/4n)Id.\nTherefore,\nn\u2211 i=1 u>i (\u2211 v\u2208S\u2032 vv> )\u22121 ui = n\u2211 i=1 Tr (\u2211 v\u2208S\u2032 vv> )\u22121 ( uiu > i ) = Tr\n(\u2211 v\u2208S\u2032 vv> )\u22121( n\u2211 i=1 uiu > i ) = Tr\n(\u2211 v\u2208S\u2032 vv> )\u22121 \u2264 4nd\nM \u2264 4n C log d \u2264 n 2 log d ,\nfor C sufficiently large. Therefore, by Markov\u2019s inequality and Equation 3, it follows that Equation 2 holds with high probability. The theorem now follows."}, {"heading": "6 Bandit Linear Optimization", "text": "Recall the problem of Bandit Linear Optimization (BLO): iteratively at each time sequence t, the environment chooses a loss vector Lt that is not revealed to the player. The player chooses a vector xt \u2208 K where K \u2286 Rd is convex, and once she commits to her choice, the loss `t = x>t Lt is revealed. The objective is to minimize the loss and specifically, the regret, defined as the strategy\u2019s loss minus the loss of the best fixed strategy of choosing some x\u2217 \u2208 K for all t. We henceforth assume, and this is w.l.o.g. via standard scaling techniques, that the loss vectors Lt\u2019s are chosen from the polar of K, meaning from {L : |L>x| \u2264 1 \u2200x \u2208 K}.\nThe problem of BLO is a natural generalization of the classical Multi-Armed Bandit problem and extremely useful for efficiently modelling decision making under partial feedback for structured problems. As such the research literature is rich with algorithms and insights into this fundamental problem. For a brief historical survey please refer to earlier sections of this manuscript. In this section we focus on the first efficient and optimal-regret algorithm, and thus immediately jump to Algorithm 3. We make the following assumptions over the decision set K:\n1. The set K is equipped with a membership oracle. This implies via [17] (Lemaa 3.1) that there exists an efficient algorithm for sampling from a given log-concave distribution over K. Via the discussion in previous sections, this also implies that we can construct approximate (both types of approximations, see Definitions 2.3 and 2.4) volumetric spanners efficiently over K.\n2. The losses are bounded in absolute values by 1. That is, the loss functions are always chosen (by an oblivious adversary) from a convex set Z such that K is contained in its polar, i.e. \u2200L \u2208 Z, x \u2208 K, |L>x| \u2264 1. This implies that the set K admits for any \u03b5 > 0 an \u03b5-net, w.r.t the norm defined by Z, whose size we denote by |K|\u03b5 \u2264 (\u03b5/2)\u2212d.\nFor Algorithm 3 we prove the following optimal regret bound:\nAlgorithm 3 EXP2 with Volumetric Spanners Exploration\n1: K, parameters \u03b3, \u03b7, horizon T . 2: p1(x) uniform distribution over K. 3: for t = 1 to T do 4: Let S\u2032t be a (pt, exp(\u2212(4 \u221a d+ log(2T ))))-exp-volumetric spanner of K.\n5: Let S\u2032\u2032t be a 2 \u221a d-ratio-volumetric spanner of K 6: Set St as the union of S \u2032 t, S \u2032\u2032 t . 7: p\u0302t(x) = (1\u2212 \u03b3)p(x) + \u03b3|St|1x\u2208St 8: sample xt according to p\u0302t 9: observe loss `t \u2206 = L>t xt\n10: Let Ct \u2206 = Ex\u223cp\u0302t [xx >] 11: L\u0302t \u2206 = `tC \u22121 t xt 12: pt+1(x) \u221d pt(x)e\u2212\u03b7L\u0302 > t x 13: end for\nTheorem 6.1. Under the assumptions stated above, and let s = maxt |St|, \u03b7 = \u221a log |K|1/T dT and\nlet \u03b3 = s\n\u221a log(|K|1/T )\ndT . Algorithm 3 given parameters \u03b3, \u03b7 suffers a regret bounded by\nO ( (s+ d) \u221a T log |K|1/T\nd\n)\nWe note that while the size log(|K|1/T ) can be bounded by d log(T ), in certain scenarios such as s-t paths in graphs it is possible to obtain sharper upper bounds that immediately imply better regret via Theorem 6.1.\nCorollary 6.1. There exist an efficient algorithm for BLO for any convex set K with regret of O (\u221a dT log |K|1/T ) = O ( d \u221a T log(T ) ) Proof. The spanner in step 4 of the algorithm does not have to be explicitly constructed. According to Theorem 2.3, to obtain such as spanner it suffices to sample sufficiently many points from the distribution pt, hence this portion of the exploration strategy is identical to the exploitation strategy.\nAccording to Corollary 2.3, a 2 \u221a d-ratio-volumetric spanner of size d can be efficiently con-\nstructed. Hence, it follows that for the purpose of the analysis, s = d and the bound follows.\nTo prove the theorem we follow the general methodology used in analyzing the performance of the geometric hedge algorithm. The major deviation from standard technique is the following sub-exponential tail bound, which we use to replace the the standard absolute bound for |L\u0302tx|. After giving its proof and a few auxiliary lemmas, we give the proof of the main theorem.\nLemma 6.1. Let x \u223c pt, xt \u223c p\u0302t and let L\u0302t be defined according to xt. It holds, for any \u03b8 > 1 that\nPr [ |L\u0302>t x| > \u03b8s\n\u03b3\n] \u2264 exp(\u22122\u03b8)/T\nProof.\nPr [ |L\u0302>t x| > \u03b8s\n\u03b3\n] \u2264 Pr [ \u2016x\u2016E(St) \u00b7 \u2016xt\u2016E(St) \u2265 \u03b8 ] Lemma 6.2\n\u2264 Pr [ \u2016x\u2016E(St) \u2265 \u221a \u03b8 \u2228 \u2016xt\u2016E(St) \u2265 \u221a \u03b8 ]\n\u2264 Pr [ \u2016x\u2016E(St) \u2265 \u221a \u03b8 ] + Pr [ \u2016xt\u2016E(St) \u2265 \u221a \u03b8 ]\n\u2264 2 Pr [ \u2016x\u2016E(St) \u2265 \u221a \u03b8 ]\nTo justify the last inequality notice that x \u223c pt and xt \u223c p\u0302t where p\u0302t is a convex sum of pt and a distribution qt for which Pry\u223cqt [ \u2016y\u2016E(St) \u2265 \u221a \u03b8 > 1 ] = 0. Before we continue recall that we can assume that \u221a \u03b8 \u2264 2 \u221a d, since S\u2032\u2032t is a 2 \u221a d-ratio-volumetric spanner .\nPr [ |L\u0302>t x| > \u03b8s\n\u03b3\n] \u2264 2 Pr [ \u2016x\u2016E(St) \u2265 \u221a \u03b8 ]\n\u2264 2 exp(\u2212 \u221a \u03b8(4 \u221a d+ log 2T )) property of exp-volumetric spanner\n\u2264 1T exp(\u22122 \u221a \u03b8 \u00b7 4d)\n\u2264 1T exp(\u22122\u03b8) since \u03b8 \u2264 4d\nLemma 6.2. For all x \u2208 K it holds that |L\u0302>t x| \u2264 |St|\u2016x\u2016E(St)\u2016xt\u2016E(St) \u03b3 .\nProof. Let x \u2208 K. Denote by Vt the matrix whose columns are the elements of St and recall that \u2016y\u20162E(St) = y >(VtV > t ) \u22121y. Since Ct \u2206 = Ex\u223cp\u0302t [xx >], it holds that\nCt \u03b3 |St| \u2211 v\u2208St vv> = \u03b3 |St| VtV > t\nsince both matrices are full rank, it holds that\nC\u22121t |St| \u03b3 (VtV > t ) \u22121\nNotice that due to the Cauchy-Schwartz inequality,\n|x>L\u0302t| = |`t| \u00b7 |x>C\u22121t xt| \u2264 |`t| \u00b7 \u2016x>C \u22121/2 t \u2016 \u00b7 \u2016C \u22121/2 t xt\u2016\nThe matrix C \u22121/2 t is defined as Ct is positive definite. Now,\n\u2016x>C\u22121/2t \u20162 = x>C\u22121t x \u2264 x> |St| \u03b3 (VtV > t ) \u22121x = |St| \u03b3 \u2016x\u20162E(St)\nSince the analog can be said for \u2016C\u22121/2t xt\u2016 (as xt \u2208 K), it follows that\n|x>L\u0302t| \u2264 |`t| |St|\u2016x\u2016E(St)\u2016xt\u2016E(St) \u03b3 \u2264 |St|\u2016x\u2016E(St)\u2016xt\u2016E(St) \u03b3\nThe last inequality is since we assume the rewards are in [\u22121, 1]."}, {"heading": "6.1 Proof of Theorem 6.1", "text": "We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy. For convenience we will assume that the set of possible arms K is finite. This assumption holds w.l.o.g since if K is infinite, a \u221a 1/T -net of it can be considered as described earlier (this will have no effect on the computational complexity of our algorithm, but a mere technical convenience in the proof below).\nBefore proving the theorem we will require three technical lemmas. In the first we show that L\u0302t is an unbiased estimator of Lt. In the second, we bound its variance. In the third, we bound the expected value of its exponent.\nLemma 6.3. In each t, L\u0302t is an unbiased estimator of Lt\nProof. L\u0302t = `tC \u22121 t xt = (L > t xt)C \u22121 t xt = C \u22121 t (xtx > t )Lt\nHence, E\nxt\u223cpt [L\u0302t] = C\n\u22121 t E\nxt\u223cpt [xtx\n> t ]Lt = C \u22121 t CtLt = Lt\nLemma 6.4. Let t \u2208 [T ], x \u223c pt and xt \u223c p\u0302t. It holds that E[(L\u0302>t x)2] \u2264 d/(1\u2212 \u03b3) \u2264 2d\nProof. For convenience, denote by qt the uniform distribution over St - the exploration strategy at round t. First notice that for any x \u2208 K,\nE xt\u223cp\u0302t\n[(L\u0302>t x) 2] = x>Ext\u223cp\u0302t [L\u0302tL\u0302 > t ]x = x >Ext\u223cp\u0302t [` 2 tC \u22121 t xtx > t C \u22121 t ]x\n= `2tx >C\u22121t Ext\u223cp\u0302t [xtx > t ]C \u22121 t x = ` 2 tx >C\u22121t x\n\u2264 x>C\u22121t x (4)\nNext,\nE x\u223cp\u0302t [x>C\u22121t x] = E x\u223cp\u0302t [C\u22121t \u2022 xx>] = C\u22121t \u2022 E x\u223cp\u0302t [xx>] = C\u22121t \u2022 Ct = Tr(Id) = d\nWhere we used linearity of expectation and denote A \u2022 B = Tr(AB). Since C\u22121t is positive semi definite,\n(1\u2212 \u03b3) E x\u223cpt [x>C\u22121t x] \u2264 (1\u2212 \u03b3) E x\u223cpt [x>C\u22121t x] + \u03b3 E x\u223cqt [x>C\u22121t x] = E x\u223cp\u0302t [x>C\u22121t x] = d (5)\nThe lemma follows from combining Equations 4 and 5.\nLemma 6.5. Let t \u2208 [T ], xt \u223c p\u0302t and x \u223c pt. For L\u0302t defined by xt it holds that\nE [ exp(\u2212\u03b7L\u0302>t x)1\u2212\u03b7L\u0302>t x>1 ] \u2264 2 T\nProof. Let f, F be the pdf and cdf of the random variable Y = \u2212\u03b7L\u0302>t x correspondingly. From Lemma 6.1 and the fact that 1/\u03b7 = s/\u03b3 we have that for any \u03b8 \u2265 1,\n1\u2212 F (\u03b8) \u2264 1 T e\u22122\u03b8\nand we\u2019d like to prove that under this condition,\nE[eY 1Y >1] = \u222b \u221e \u03b8=1 e\u03b8f(\u03b8)d\u03b8 \u2264 2 T\nwhich follows from the definition of the cdf and pdf: E[eY 1Y >1] = \u222b\u221e \u03b8=1 e\u03b8f(\u03b8)d\u03b8\n= \u2211\u221e k=1 \u222b k+1 \u03b8=k e\u03b8f(\u03b8)d\u03b8\n\u2264 \u2211\u221e k=1 e k+1 \u222b k+1 \u03b8=k f(\u03b8)d\u03b8\n\u2264 \u2211\u221e k=1 e\nk+1(F (k + 1)\u2212 F (k)) \u2264 \u2211\u221e k=1 e\nk+1(1\u2212 F (k)) \u2264 \u2211\u221e k=1 e k+1 \u00b7 1T e \u22122k Lemma 6.1\n= eT \u2211\u221e k=1 e \u2212k = eT \u00b7 e\u22121 1\u2212e\u22121 \u2264 2 T\nProof of Theorem 6.1. For convenience we define within this proof for x \u2208 K, \u02c6\u03001:t\u22121(x) \u2206 = \u2211t\u22121 i=1 L\u0302 > i x and let \u02c6\u0300t(x) \u2206 = L\u0302>t x. Let Wt = \u2211 x\u2208K exp(\u2212\u03b7 \u02c6\u03001:t\u22121(x)). For all t \u2208 [T ]:\nE [ Wt+1 Wt ] = E [\u2211 x\u2208K exp(\u2212\u03b7 \u02c6\u03001:t\u22121(x)) exp(\u2212\u03b7 \u02c6\u0300t(x)) Wt ] = Ext\u223cp\u0302t [\u2211 x\u2208K pt(x) exp(\u2212\u03b7 \u02c6\u0300t(x))\n] = Ext\u223cp\u0302t,x\u223cpt [exp(\u2212\u03b7 \u02c6\u0300t(x))] \u2264\n\u2264 1\u2212 \u03b7E[L\u0302>t x] + \u03b72 E[(L\u0302>t x)2] + E [ exp(\u2212\u03b7L\u0302>t x)1\u2212\u03b7L\u0302>t x>1 ] using the inequality exp(y) \u2264 1 + y + y2 + exp(y) \u00b7 1y>1\n\u2264 1\u2212 \u03b7E[L\u0302>t x] + \u03b72 E[(L\u0302>t x)2] + 2T Lemma 6.5\nSince L\u0302t is an unbiased estimator of Lt (Lemma 6.3) and according to Lemma 6.4, E[(L\u0302 > t x) 2] \u2264 2d, we get:\nE [ Wt+1 Wt ] \u2264 1\u2212 \u03b7L>t E x\u223cpt [x] + 2\u03b72d+ 2 T (6)\nWe now use Jensen\u2019s inequality:\nE[log(WT )]\u2212E[log(W1)] = E[log(WT /W1)] = \u2211T\u22121 t=1 E[log(Wt+1/Wt)]\n\u2264 \u2211T\u22121 t=1 log(E[Wt+1/Wt]) Jensen\n\u2264 \u2211T\u22121 t=1 log ( 1\u2212 \u03b7L>t Ex\u223cpt [x] + 2\u03b72d+ 2T ) (6)\n\u2264 \u2211T\u22121 t=1 \u2212\u03b7L>t Ex\u223cpt [x] + 2\u03b72d+ 2 T Due to ln(1 + y) \u2264 y for all y > \u22121\n\u2264 2 + 2\u03b72Td\u2212 \u03b7 \u2211 t Ex\u223cpt [L > t x]\nNow, since log(W1) = log(|K|) and WT \u2265 exp(\u2212\u03b7 \u02c6\u03001:T (x\u2217)) for any x\u2217 \u2208 K, by shifting sides of the above it holds for any x\u2217 \u2208 K that\u2211\nt\nE x\u223cpt [L>t x]\u2212 \u2211 t L>t x \u2217 \u2264 \u2211 t E x\u223cpt [L>t x] + E[logWT ] \u2264 log(|K|) + 2 \u03b7 + 2\u03b7Td\nFinally, by noticing that \u2211 t E x\u223cp\u0302t [L>t x]\u2212 \u2211 t E x\u223cpt [L>t x] \u2264 \u03b3T\nwe obtain a bound of\nE[Regret] = E[ \u2211 t L>t xt]\u2212 \u2211 t L>t x \u2217 = \u2211 t E x\u223cp\u0302t [L>t x]\u2212 Loss(x\u2217) \u2264 log(|K|) + 2 \u03b7 + 2\u03b7Td+ \u03b3T\non the expected regret. By plugging in the values of \u03b7, \u03b3 we get the bound of\nO ( (s+ d) \u221a T log(|K|)\nd ) as required.\nDiscussion: Notice that to obtain a (p, \u03b5)-exp-volumetric spanner for a log-concave distribution p over a body K we simply choose sufficiently many i.i.d samples from p. Since in the above algorithm pt is always log-concave, it follows that S \u2032 t consists of i.i.d samples from pt, meaning that if we would not have required S\u2032\u2032t , the exploration and exploration strategies would be the same! Since we still require the set S\u2032\u2032t , there exists a need for a separate exploration strategy. Interestingly, the 2 \u221a d-ratio-volumetric spanner is obtained by taking a barycentric spanner, which is the exploration strategy of [10]."}, {"heading": "A Concentration bounds for non centered isotropic log con-", "text": "cave distributions\nWe begin by proving an auxiliary lemma used in the proof of Corollary 3.1.\nLemma A.1. Let \u03b4 > 0, t \u2265 1, let d be a positive integer and let n = Ct 4d log2(t/\u03b4)\n\u03b42 for some sufficiently large universal constant C. Let y1, . . . , yn be i.i.d d-dimensional vectors from an isotropic log-concave distribution. Then\nPr [\u2225\u2225\u2225\u2225 1n\u2211 yi \u2225\u2225\u2225\u2225 > \u03b4] \u2264 exp(\u2212t\u221ad)\nProof. For convenience let Sn = 1\u221a n \u2211n i=1 yi. Since the y\u2019s are independent, Sn is also log-concave distributed. Notice that E[Sn] = 0 and E[SnS T n ] = 1 n \u2211 E[yiy T i ] = Id hence Sn is isotropic. Now,\nPr [\u2225\u2225\u2225\u2225 1n\u2211 yi \u2225\u2225\u2225\u2225 > \u03b4] = Pr [\u2016Sn\u2016 > \u221an\u03b4] = Pr [\u2016Sn\u2016 > \u221ad \u00b7\u221aCt4 log2(t/\u03b4)] \u2264 Pr [\u2016Sn\u2016 > \u221ad+\u221ad \u00b7 12 t\u221aC ] .\nThe last inequality holds for t \u2265 1 and C \u2265 4. It now follows from Theorem 3.2 that\nPr [\u2225\u2225\u2225\u2225 1n\u2211 yi \u2225\u2225\u2225\u2225 > \u03b4] \u2264 c1 exp(\u2212c2t\u221aCd)\nwhere c1, c2 are some universal constants. Since t \u221a d \u2265 1, setting C \u2265 ( 1+log(c1)c2 )\n2 proves the claim.\nProof of Corollary 3.1. Let a = E[x] and let a\u0303 = 1n \u2211 xi. Notice that\nE[(x\u2212 a)(x\u2212 a)T ] = E[xxT ]\u2212E[x]aT \u2212 aE[x] + aaT = Id \u2212 aaT\nis a PSD matrix hence \u2016a\u2016 \u2264 1. Consider the following equality.\n1\nn n\u2211 i=1 (xi \u2212 a)(xi \u2212 a)> = 1 n n\u2211 i=1 xix > i \u2212 aa\u0303> \u2212 a\u0303a> + aa>\nAccording to Lemma A.1, w.p. at least 1\u2212 exp(\u2212t \u221a d),\n\u2016a\u0303\u2212 a\u2016 \u2264 \u03b4\nin which case, since \u2016a\u2016 \u2264 1 and according to the triangle inequality,\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 xix > i \u2212 Id \u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 (xi \u2212 a)(xi \u2212 a)> \u2212 (Id \u2212 aa>) \u2225\u2225\u2225\u2225\u2225+ 2\u03b4 According to Theorem 3.1, w.p. at least 1\u2212 exp(\u2212t\n\u221a d)\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 (xi \u2212 a)(xi \u2212 a)> \u2212 (Id \u2212 aa>) \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b4 and the corollary follows.\nProof of Corollary 3.2. Let E[x] = a. Consider the r.v y = x \u2212 a. It holds that E[y] = 0 and E[yyT ] = Id \u2212 aaT . Notice that we can derive that\n\u2016a\u2016 \u2264 1 (7)\nAs E[yyT ] is a PSD matrix. Also, it is easy to verify that y is log-concave distributed. We now consider the r.v3 z = (Id \u2212 aat)\u22121/2y. It is easy to verify that the distribution of z is also log-concave and isotropic. It follows, from Theorem 3.2 that for any \u03b8 \u2265 2\nPr [ \u2016z\u2016 > \u03b8 \u221a d ] \u2264 Pr [ \u2016z\u2016 \u2212 \u221a d > 1\n2 \u03b8 \u221a d\n] \u2264 C \u2032 exp(\u2212c\u03b8 \u221a d)\nBy using equation 7 we get that for \u03b8 > 3 Pr [ \u2016x\u2016 > \u03b8 \u221a d ] \u2264 Pr [ \u2016y\u2016 > \u03b8 \u221a d\u2212 1 ] \u2264 Pr [ \u2016z\u2016 > (\u03b8 \u2212 1/ \u221a d) \u221a d ] \u2264 C \u2032 exp(c\u2032 \u2212 c\u2032\u03b8 \u221a d).\nThe last inequality holds since \u03b8 \u2212 1/ \u221a d \u2265 2.\n3if Id\u2212aaT is not of full rank then y is in fact supported in an affine subspace of rank d\u22121 and we can continue the analysis there."}], "references": [{"title": "Interior-point methods for full-information and bandit online learning", "author": ["J.D. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles", "author": ["Radoslaw Adamczak", "Alexander E. Litvak", "Alain Pajor", "Nicole Tomczak-Jaegermann"], "venue": "Journal of American Mathematical Society,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Online linear optimization and adaptive routing", "author": ["Baruch Awerbuch", "Robert Kleinberg"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "An elementary introduction to modern convex geometry. In Flavors of Geometry, pages 1\u201358", "author": ["Keith Ball"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "High-probability regret bounds for bandit online linear optimization", "author": ["Peter L. Bartlett", "Varsha Dani", "Thomas P. Hayes", "Sham Kakade", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems, volume 5 of Foundations and Trends in Machine Learning", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi", "Sham M. Kakade"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Linear convergence of a modified frankwolfe algorithm for computing minimum-volume enclosing ellipsoids", "author": ["S. Damla Ahipasaoglu", "Peng Sun", "Michael J. Todd"], "venue": "Optimization Methods and Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "In COLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Sham M Kakade", "Thomas P Hayes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "An arithmetic proof of john\u2019s ellipsoid theorem", "author": ["Peter M. Gruber", "Franz E. Schuster"], "venue": "In arXiv:1207.7246,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Interpolating thin-shell and sharp large-deviation estimates for lsotropic log-concave measures", "author": ["Olivier Gudon", "Emanuel Milman"], "venue": "Geometric and Functional Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "L\u00f6wner-John ellipsoids", "author": ["Martin Henk"], "venue": "Documenta Mathematica,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Extremum Problems with Inequalities as Subsidiary Conditions", "author": ["F. John"], "venue": "Studies and Essays: Courant Anniversary Volume,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1948}, {"title": "Playing games with approximation algorithms", "author": ["Sham M Kakade", "Adam Tauman Kalai", "Katrina Ligett"], "venue": "SIAM Journal on Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Rounding of polytopes in the real number model of computation", "author": ["Leonid G Khachiyan"], "venue": "Mathematics of Operations Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "The geometry of logconcave functions and sampling algorithms", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz", "Santosh Vempala"], "venue": "Random Structures & Algorithms,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Random vectors in the isotropic position", "author": ["M. Rudelson"], "venue": "Journal of Functional Analysis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}], "referenceMentions": [{"referenceID": 2, "context": "To mention a few: Awerbuch and Kleinberg [3] devise the notion of barycentric spanners, and use this construction to give the first low-regret algorithms for complex decision problems such as online routing.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Abernethy, Hazan and Rakhlin [1] use self-concordant barriers to build an efficient exploration strategy for convex sets in Euclidean space.", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "Bubeck, Cesa-Bianchi and Kakade [7] apply tools from convex geometry, namely John\u2019s ellipsoid to construct optimal regret algorithms for bandit linear optimization (albeit not always efficiently).", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "is that of online routing in graphs: a decision maker iteratively chooses a path in a given graph from source to destination, the adversary chooses lengths of the edges of the graph, and the decision maker receives as feedback the length of the path she chose but no other information (see [3]).", "startOffset": 290, "endOffset": 293}, {"referenceID": 0, "context": "learning permutations, rankings and other examples (see [1]).", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "The reader is referred to the recent survey of Bubeck and Cesa-Bianchi [6] for more details on algorithmic results for BLO.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery.", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "Previously efficient algorithms (with non-optimal regret) were known over convex sets that admit an efficient selfconcordant barrier [1], and optimal regret algorithms were known over general sets [7] but using computationally in-efficient machinery.", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "2 Barycentric Spanners The notion of Barycentric spanners was introduced in the work of [3], in order to define an exploration basis for an online shortest path problem.", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 134, "endOffset": 137}, {"referenceID": 14, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "Barycentric Spanners have since been used as an exploration basis in several works: In [10] for online bandit linear optimization, in [5] for a high probability counterpart of the online bandit linear optimization, in [15] for repeated decision making of approximable functions and in [9] for a stochastic version of bandit linear optimization.", "startOffset": 285, "endOffset": 288}, {"referenceID": 2, "context": "In [3] it is shown that any compact set has a barycentric spanner.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "5 in [3]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "Its properties have been the subject of study in convex geometry since John\u2019s work [14] (see [4] and [13] for historic information).", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "Its properties have been the subject of study in convex geometry since John\u2019s work [14] (see [4] and [13] for historic information).", "startOffset": 93, "endOffset": 96}, {"referenceID": 12, "context": "Its properties have been the subject of study in convex geometry since John\u2019s work [14] (see [4] and [13] for historic information).", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "John\u2019s ellipsoid and in particular its contact points with the convex body it encapsulates makes for an appealing exploration basis, and indeed [7] have used exactly this machinery to attain an optimal-regret bandit linear optimization algorithm.", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "The following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11].", "startOffset": 137, "endOffset": 140}, {"referenceID": 10, "context": "The following theorem gives a characterization of the minimum enclosing ellipsoid, and was originally proved by John, restated here from [4] and [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "[4] Let K \u2208 R be a symmetric convex set and assume that the unit sphere is its minimal enclosing ellipsoid.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16, 8]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[16, 8]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "If the vertices are [0, 1], [\u2212 \u221a 3 2 ,\u2212 1 2 ], [ \u221a 3 2 ,\u2212 1 2 ], then the eigenpoles of the ellipsoid of the bottom two vertices are [0.", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "23 ], [2, 0].", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "A distribution over R is log-concave when for its probability distribution function (pdf) p it holds that for any x, y \u2208 R, \u03bb \u2208 [0, 1], p(\u03bbx+ (1\u2212 \u03bb)y) \u2265 p(x)\u03bbp(y)1\u2212\u03bb", "startOffset": 128, "endOffset": 134}, {"referenceID": 16, "context": "1 ([17], Theorems 2.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "1 in [2]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 11, "context": "1 in [12]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "3 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "This implies via [17] (Lemaa 3.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "1 We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy.", "startOffset": 73, "endOffset": 80}, {"referenceID": 6, "context": "1 We continue the analysis of the Geometric Hedge algorithm similarly to [10, 7], under certain assumptions over the exploration strategy.", "startOffset": 73, "endOffset": 80}, {"referenceID": 9, "context": "Interestingly, the 2 \u221a d-ratio-volumetric spanner is obtained by taking a barycentric spanner, which is the exploration strategy of [10].", "startOffset": 132, "endOffset": 136}], "year": 2017, "abstractText": "Numerous machine learning problems require an exploration basis a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases. We show how efficient volumetric spanners give rise to the first efficient and optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.", "creator": "LaTeX with hyperref package"}}}