{"id": "1401.2482", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "STIMONT: A core ontology for multimedia stimuli description", "abstract": "affective multimedia documents encompass such elements as static images, radio sounds or online videos simultaneously elicit emotional interaction responses in exposed for human interacting subjects. coincidentally these stimuli too are nevertheless stored heavily in affective multimedia databases systematically and successfully used exclusively for a wide variety of extensive research in meta psychology and neuroscience in areas extensively related to attention and emotion processing. although important all physical affective multimedia databases have numerous deficiencies issues which radically impair their operational applicability. these problems, which basically are brought forward in the technical paper, result markedly in historically low recall and precision of multimedia graphics stimuli retrieval experiments which so makes creating emotion signal elicitation procedures algorithms difficult and labor - intensive. to address alleviate these issues a new major core ontology stimont generation is introduced. thus the stimont is written automatically in owl - dl formalism and quickly extends conventional w3c semantic emotionml format starting with an expressive paradigm and appropriate formal information representation of affective concepts, use high - specific level semantics, stimuli also document document metadata items and includes the words elicited physiology. investigating the advantages arising of ontology models in description of meaningful affective multimedia descriptive stimuli are demonstrated significantly in a descriptive document retrieval experiment identified and compared against contemporary keyword - location based hierarchical querying methods. also, a hybrid software tool integrated intelligent stimulus generator allowed for retrieval of affective multimedia and construction optimization of stimuli between sequences is presented.", "histories": [["v1", "Fri, 10 Jan 2014 23:36:51 GMT  (837kb)", "http://arxiv.org/abs/1401.2482v1", "27 pages, 13 figures"]], "COMMENTS": "27 pages, 13 figures", "reviews": [], "SUBJECTS": "cs.MM cs.AI", "authors": ["marko horvat", "nikola bogunovi\\'c", "kre\\v{s}imir \\'cosi\\'c"], "accepted": false, "id": "1401.2482"}, "pdf": {"name": "1401.2482.pdf", "metadata": {"source": "CRF", "title": "STIMONT: A core ontology for multimedia stimuli description", "authors": ["Marko Horvat", "Nikola Bogunovi\u0107", "Kre\u0161imir \u0106osi\u0107"], "emails": ["Marko.Horvat2@fer.hr,"], "sections": [{"heading": null, "text": "elicit emotional responses in exposed human subjects. These stimuli are stored in affective multimedia databases and successfully used for a wide variety of research in psychology and neuroscience in areas related to attention and emotion processing. Although important all affective multimedia databases have numerous deficiencies which impair their applicability. These problems, which are brought forward in the paper, result in low recall and precision of multimedia stimuli retrieval which makes creating emotion elicitation procedures difficult and labor-intensive. To address these issues a new core ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and extends W3C EmotionML format with an expressive and formal representation of affective concepts, high-level semantics, stimuli document metadata and the elicited physiology. The advantages of ontology in description of affective multimedia stimuli are demonstrated in a document retrieval experiment and compared against contemporary keyword-based querying methods. Also, a software tool Intelligent Stimulus Generator for retrieval of affective multimedia and construction of stimuli sequences is presented."}, {"heading": "1 Introduction", "text": "Multimedia documents with annotated semantic and emotion content are stored in affective multimedia databases. Apart from digital objects these databases contain metadata about their high-level semantics and expected emotion that will be induced in a subject when exposed to a contained document. Two important features distinguish affective multimedia databases from other multimedia repositories: i) the purpose of the multimedia documents and ii) the emotion representation of the multimedia documents. Multimedia documents in affective multimedia databases are aimed at inducing or stimulating emotions in exposed subjects. As such they are usually referred to as stimuli. By being exposed to multimedia stimuli individuals\u2019 emotional states may be modulated [1, 2]. This spontaneous cognitive process can be utilized in various domains like Affective Computing and Human-Computer Interaction (HCI) but also in research of human emotions, attention and development of stress-related mental disorders. Affective multimedia databases are standardized which allows them to be used in a controllable and predictable manner and, subsequently, the emotion\nelicitation results can be measured, replicated and validated by different research teams.\nThis paper addresses multiple drawbacks of contemporary affective multimedia databases [3] and proposes an ontology-based approach for formal description of stimuli metadata which aims to optimize both the annotation and retrieval processes from these databases. Therefore, a new STIMulus ONTology (STIMONT) for a formal and comprehensive description of multimedia stimuli has been developed along with a software tool Intelligent Stimulus Generator for database searching, stimuli sequence construction and subject exposure. The ontology is validated and the retrieval results are compared with the existing methods for querying of affective multimedia databases. Motivation for this work was supported by an online survey on usage patterns of multimedia stimuli databases [4]. The survey indicated that domain experts unequivocally recognize the need for an intelligent stimuli retrieval application that would assist them in experimentation. Also, almost all experts agreed that such applications would be useful in their work.\nThe remainder of this paper is organized as follows; Section 2 gives an overview of the related work including most important contemporary stimuli databases, employed emotion theories and existing methods for affective multimedia annotation. In Section 2.1 a number of databases\u2019 deficiencies, which could be amended with an ontologybased formal knowledge presentation, are brought forward. Section 2.2 explains how ontology-based reasoning techniques can be utilized to achieve formal interpretation of high-level semantics in multimedia stimuli. Section 3 introduces the STIMONT model. All aspects of the proposed ontology are thoroughly described, in particular knowledge models of emotion, semantics, context and physiology domains. Usage of the ontology is demonstrated in several real-world examples. Implementation of the ontology is explained in Section 4, and an experimental validation using database querying and affective multimedia document retrieval is described in Section 4.1. Finally, Section 5 discusses various aspects of the proposed ontology and provides insight into future work."}, {"heading": "2 Related work", "text": "This section gives a brief introduction to affective multimedia databases, multimedia emotion annotation models and the ontology-based methods for high-level metadata representation and document retrieval. The development of the ontology specifically tailored for stimuli description is directly motivated by the need to improve querying and retrieval from affective multimedia databases."}, {"heading": "2.1 Affective multimedia databases", "text": "The International Affective Picture System (IAPS) [5] and the International Affective Digital Sounds system (IADS) [6] are two of the most cited databases in the area of affective stimulation. The latest versions of IAPS and IADS contain 1182 and 167 semantically and emotionally annotated stimuli, respectively. These databases cover a\nwide range of semantic categories characterized along the affective dimensions of pleasure, arousal and dominance. They were created with three goals in mind [7]:\n1. Better experimental control of emotional stimuli; 2. Increasing the ability of cross-study comparisons of results; 3. Facilitating direct replication of undertaken studies. The same standardization principles are shared among other similar affective multimedia databases. Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15]. Additional audio-visual affective multimedia databases with category or dimensional emotion annotations are listed here [16]. Facial expression databases are by far the most numerous modality among the affective multimedia databases and, although they are employed in emotion elicitation, facial expression databases are primarily used for face recognition and face detection. A more detailed overview of these databases is given in [17].\nTwo predominant theories used to describe emotion are the discrete category model and the dimensional model of affect (also sometimes called Circumplex model of affect [18] or PAD [19]). All affective multimedia databases have been characterized according to at least one of these models [20]. The dimensional theories of emotion propose that affective meaning can be well characterized by a small number of dimensions. Dimensions are chosen on their ability to statistically characterize subjective emotional ratings with the least number of dimensions possible [21]. These dimensions generally include one bipolar or two unipolar dimensions that represent positivity and negativity and have been labeled in various ways, such as valence or pleasure. Also usually included is a dimension that captures intensity, arousal, or energy level. In contrast to the dimensional theories, categorical theories claim that the dimensional models, particularly those using only two or three dimensions, do not accurately reflect the neural systems underlying emotional responses. Instead, supporters of these theories propose that there are a number of emotions that are universal across cultures and have an evolutionary and biological basis [22]. Which discrete emotions are included in these theories is a point of contention, as is the choice of which dimensions to include in the dimensional models. Most supporters of discrete emotion theories agree that at least the five primary emotions of happiness, sadness, anger, fear and disgust should be included.\nDimensional and categorical theories of affect can both effectively describe emotion in digital systems but are not mutually exclusive. Stimuli previously only characterized according to a single theory have also been characterized according to the complimentary emotion theory, as for example in IAPS [23], IADS [24] and ANEW [25]. Annotations according to both theories of affect are useful for several reasons, predominantly because they providing a more complete characterization of stimuli affect.\nIn terms of semantic content a single multimedia stimulus is described with a single tag from an unsupervised glossary. Semantic relations between different concepts are undefined and multiple different keywords could be used for description of the\nsame concept. For example, a picture stimulus portraying an attack dog may be tagged as \u201edog\u201c, \u201eattack\u201c, \u201eattackdog\u201c, \u201eattack_dog\u201c etc. Synonyms like \u201ecanine\u201c or \u201ehound\u201c would be interpreted as different concepts. Semantic lexicon has no semantic similarity measures and there are no criterions to estimate relatedness between concepts. In such multimedia stimuli databases it is impossible to establish that \u201edog\u201c and \u201ecat\u201c are more closely related than \u201edog\u201c and \u201eSpace Shuttle\u201c. This represents a huge defect in the stimuli retrieval process because a search query has to lexically match the database's keywords and no higher or more semantically meaningful interpretation of the query and annotating tags is attainable. The inadequate semantic descriptors result in three negative effects which impair stimuli retrieval: 1) low recall, 2) low precision and high recall or 3) vocabulary mismatch. Furthermore, affective multimedia databases contain only data about the stimuli themselves and do not describe semantics implied by the stimuli content. An example of the inadequacy of semantic descriptors in contemporary affective multimedia databases is illustrated in the figure below.\nPictures p1 and p2 are taken from IAPS (p1=5635.jpg, p2=7039.jpg) and have PAD emotion values. Both pictures have closely similar neutral valence and sub neutral arousal : , = 6.25, 3.97 , : , = 5.93, 3.29 . As mentioned before these values have been methodically acquired and verified in a manual picture annotation experiment. Although both pictures have similar emotion values they have different semantics and would be appraised differently by subjects. High-level semantics of p1 and p2 \u2013 generally an object or a scene \u2013 are described with just one keyword \u201cWinterStreet\u201d and \u201cTrain\u201d, respectively. While this may be enough for representation of an isolated object, e.g. in a close-up picture with minimal context, this is clearly insufficient for rich meaning of objects within a scene even just for the most relevant and evident high-level semantics. For example, picture p1 is tagged as \u201cWinterStreet\u201d but any of the following tags could substitute the original tag: \u201cSnowedIn\u201d, \u201cWinterCity\u201d, \u201cSnowCoveredCars\u201d or even \u201cDesolation\u201d. Equally so, picture p2 could be described as \u201cFreightTrain\u201d, \u201cRailroad\u201d, \u201cScenery\u201d, \u201cAutumn\u201d or \u201cBeauty\u201d. Reliance on emotion values alone is not enough to discriminate stimuli but at the same time IAPS tagging system is inadequate in describing complete stimuli semantics.\nIn summary, available standardized affective multimedia databases offer a variety of quality audio-visual stimuli to researchers in the field. They were meticulously built and enable valid comparative measurements. But also they are mutually noncompliant, non-contiguous and structurally overly diversified. Their content is described loosely and informally. The databases are domain dependant and have arbitrarily structures."}, {"heading": "2.2 Ontology-based representation of high-level stimuli content", "text": "By definition ontologies are a representation of a shared understanding about a specific domain and enable the derivation of implicit knowledge from the existing explicit knowledge and automated inference with reasoning engines [26]. Ontologies have been successfully applied for description of high-level image content, concept semantics, object labels and relationships defined in the upper levels of the image representation hierarchy [27]. This top-down approach in document representation and retrieval has three main benefits over the opposite approach (i.e. bottom-up) which relies on media features and other low level image descriptors. Firstly, database users prefer to articulate their search queries in a natural language, or in a constructed language similar to their preferred natural language, that are inherently capable for expression of a complex semantics. Secondly, the information one can infer from raw media information cannot be automatically transformed to high-level semantics that the stimuli convey. Thirdly, only rich high level full semantic representation of an image can express the full range of relationships, explicitly observable, implicitly inferable and the variety of purported connotations, actions and the broader context.\nExcept the W3C EmotionML format [28] much work has been done to enable describing information about emotions in multimedia, especially in the video. MPEG-7 multimedia standard, which is based on XML and can be expanded with additional tools, provides a method for describing emotions with its Affective Description Scheme [29]. Researchers have proposed new description tools that rely on MPEG-7 to provide a broader description of affective terms that can be used in video annotation [30]. Also, several multimedia ontologies which are potentially applicable in emotion description have been proposed in the last decade [31] including ontologies specially designed for high-level description of cognitive-emotional related concepts [32]. However, EmotionML currently offers the most sophisticated emotion annotation glossary and, since it is also XML compliant, can be used in MPEG-7 and other compatible standards.\nThe retrieval of multimedia stimuli is in many respects similar to retrieval of general multimedia, except for the added dimension of emotion that is usually disregarded in the document retrieval. However, the usage emotion is not trivial and requires representation of additional information like stimulus context and eliciting physiology which are necessary for a complete understanding of stimulus\u2019 impact. Also, stimuli are delivered to subjects in a sequence through one or more elicitation sessions. These queues or sequences may include temporally overlapping stimuli of different modalities (e.g. visual and auditory) with possible interludes and arranged in a specific order determined by an overseeing expert. Therefore, formal description methods also have to include representation of dynamic stimuli, i.e. time series in one or more multime-\ndia formats with pauses or breaks between consecutive sequences within the same elicitation session. Since stimuli are used in health related research ethical concerns are of the paramount importance and delivery of improper semantic and emotion content to subjects should be averted. Deficient or incomplete stimuli description is a significant potential cause of elicitation defects because it may lead to retrieval and delivery of an unwanted content. With regard to different modalities, methods for description of other multimedia formats like sound or text can be extended from those applied to visual stimuli. In essence demands on content representation remain the same \u2013 high-level semantics with the expected eliciting emotion and the related states should be in the focus of all stimuli description formalisms.\nTherefore, the ontology-based paradigm for stimuli annotation and retrieval proposed in the paper consists of terminological and assertional knowledge about highlevel multimedia stimuli content and a reasoning engine. These two types of knowledge are the basic components of a knowledge-based system based on Description Logics (DLs) [33] as a set of structured knowledge-representation formalisms with decidable-reasoning algorithms. A variety of tools for knowledge engineering exist [34] which allow construction, management, reuse and reasoning with OWLbased ontologies.\nThe knowledge base for ontological representation of stimuli has two main components as in the figure below. The terminological component (TBox) describes the relevant notions of the application domain by stating properties of concepts and roles and their interrelations. TBox contains an ontological representation of the knowledge in audio-visual stimuli content. The assertional component (ABox) is a formal set of assertions describing specific semantics or emotion in terms of the terminological knowledge. ABox describes a concrete world by stating individuals and their specific properties and interrelations.\nThe annotation process of a multimedia stimulus begins with identification of concepts in its content that can be observed by subjects and deemed important (by experts) for assessment of stimulus meaning. After a concept is recognized an equivalent concept must also be identified in the ontology used for stimulus representation.\nTBox must define all concepts that exist in the stimulus content. After an equivalent concept has been found a new individual is created, associated with the stimulus and stored in ABox. This process is repeated for all stimuli in consideration.\nMultimedia content recognition may be performed automatically by an intelligent image recognition algorithm or manually by a group of experts. From the aspect of the STIMONT the image recognition is viewed upon as a black box that yields ground truth object labels. It is of lesser importance how annotations are generated as long as they represent true object labels and relationships present in a multimedia stimulus. However, because of the ethical concerns it would seem highly advisable that an independent team of experts unconditionally verifies quality of stimuli annotation process."}, {"heading": "3 The STIMONT model", "text": "The STIMONT is an upper core ontology designed to provide an integrated and formal description of emotion, high-level semantics, context and physiology content of a multimedia stimulus. The most important feature of the STIMONT is that it provides a formal framework for supporting explicit, human and machine-processable definition of affective multimedia content. This ontology also facilitates storage of stimuli in emotionally-annotated databases, stimuli querying and retrieval and construction of stimuli sequences. The STIMONT enables a common understanding about the perceived meaning of multimedia stimuli, their affective dimensions and context. Using an appropriate interference engine this knowledge allows to derive new facts about the stimuli indexed in affective multimedia databases.\nThe role of the proposed STIMONT has four aspects: 1. To enable development of intelligent tools for multimedia stimuli selection and retrieval, including improving structure and usability of affective multimedia\ndatabases;\n2. To serve as a semantic glue or a bridge allowing the integration, i.e. by providing common attachment points, of the different ontologies employed for repre-\nsentation of multimedia stimuli relevant for their application;\n3. To serve as a starting point for engineering of other ontologies for description of affectively annotated media, and 4. To provide a reference point for evaluation different ontological approaches in these areas. The STIMONT is formalism for representation of all relevant knowledge about any type of multimedia stimuli such as images, sounds, video and text. This is achieved by identifying the key aspects of multimedia documents used for stimulation of emotion responses and separately formalizing them as OWL constructs: high-level semantics interpretation, induced emotion and related emotion states, document metadata and emotion-related physiology. This is illustrated in the figure below."}, {"heading": "3.1 STIMONT concepts", "text": "The core ontology concept is Stimulus which is specified with four other concepts: Emotion, Semantics, Context and Physiology. Each of these specifying components defines one independent information domain that represents particular knowledge about content of a multimedia stimulus. This reflects previously explained approach were a multimedia stimulus elicits some emotion or emotion related states, has a highlevel semantics which is articulated by a subject exposed to a stimulus, contains implicit meta-information relevant for representation and provokes a specific physiological response in exposed subject.\nEach multimedia stimulus is represented in the ABox as exactly one instance of Stimulus concept. Similarly, Emotion, Semantics, Context and Physiology classes must also have individuals in ABox which are associated with individuals of Stimulus class:\n./01 \u2261 \u22034 . 1 /0 \u2293 61 /0 \u2293 7 / 8/ \u2293 9\u210e; 0 <;\nTherefore, for a given stimulus at least one of four components must exist to consider the stimulus annotated. Preferably all concepts should be filled for an optimum description because more available information will facilitate multimedia retrieval and construction of powerful and personalized stimuli sequences, which will support a more efficient emotion elicitation process.\nThe associations of Stimulus class are defined as OWL object properties \u210e 61 /0 . 61 /0 \u2291 ./01 , \u210e . 1 /0 . . 1 /0 \u2291 ./01 , \u210e 7 / 8/. 7 / 8/ \u2291 ./01 and \u210e 9\u210e; 0 <;. 9\u210e; 0 <; \u2291 ./01 . Additionally, the ontology differentiates between three types of high-level semantics >?@ / \u2291 . 1 /0 , . \u2291 . 1 /0 and 6 / \u2291 . 1 /0 . With the STIMONT it is possible to express aggregate semantics with any combination of these three concepts. Furthermore, individuals of Object, Scene and Event can be\nexplicitly retrieved with relations \u210e >?@ / \u2291 \u210e . 1 /0 , \u210e . \u2291 \u210e . 1 /0 and \u210e 6 / \u2291 \u210e . 1 /0 . The STIMONT supports stimuli retrieval based on objects, scenes and events, emotions, context and physiology objects. Schema of the most important concepts and connections in the STIMONT is given in the figure below.\nAs can be seen in Figure 4 the STIMONT reuses a singular upper ontology or a combination of upper and domain ontologies for formal representation of aggregate stimuli semantics. This is done intentionally because a number of amicable upper ontologies already exist. Each of them is highly elaborate with several thousands of concepts. It would be wrong, and indeed contrary to the most important ontology goals (such as knowledge reuse), to develop new common-sense ontology just for multimedia stimuli representation. The STIMONT is currently using Suggested Merged Upper Ontology (SUMO) [35] in OWL DL format as foundation ontology for formal representation of stimuli high-level content. SUMO was chosen because of its comparative advantages as the largest freely available common-sense ontology. Other top-level ontologies could also be used for representation of audio-visual content in stimuli semantics if they satisfy criterions of necessary and sufficient expressiveness and decidability. Furthermore, every domain ontology used in the framework has to be aligned to the chosen core ontology thereby ensuring interoperability between different domain ontologies possibly used by the reasoning engine and the retrieval\nmodule. But regardless of the actual choice all ontologies must be subclassed to Semantics concept. This preserves the structure of the STIMONT and ensures portability of its stimuli annotations."}, {"heading": "3.2 Modeling stimuli emotion", "text": "The entire emotion taxonomy is in the STIMONT subsumed under the umbrella concept Emotion. For the sake of simplicity and easier understanding the term \u201caffect\u201d was intentionally avoided. Typically, affect is a more general concept than emotion and encompasses a wide range of emotion and related states like feelings, moods, sentiments and attitudes. The term \u201cemotion\u201d is used for description of any type of emotion phenomena: strong and weak, focused and vague, long and short-term. Therefore, in the STIMONT model terms \u201cemotion\u201d and \u201cemotion state\u201d can be used interchangeably with \u201caffect\u201d and \u201caffective state\u201d, respectively.\nSecond-level emotion concepts are Category, Dimension, Appraisal, Action Tendency and Sentiment which constitute domains of categorical and dimensional emotion models, theories of appraisal, action tendencies and sentiment analysis, respectively. Category concepts are further divided into six classes: EveryDay, BigSix, FrijdaCategory, OCCCategory, FSRECategory and General. Each of first five classes formally represents one of the category vocabularies in EmotionML [28]: emotions that frequently occur in everyday life, six primary emotions universal in all cultures, categories related to Frijda\u2019s proposal of action tendencies, categories compromising Ortony, Clore and Collins appraisal model, and finally categories used in a study by Fontaine, Scherer, Roesch and Ellsworth, respectively. To provide an even richer expressivity than the EveryDay set, class General was implemented with 66 commonplace emotion concepts like \u201cEuphoria\u201d, \u201cGratitude\u201d, \u201cHorror\u201d, \u201cJealousy\u201d, \u201cWonder\u201d, etc.\nIn the stimulus retrieval process, to achieve higher accuracy and recall, it is necessary to assess all emotions assigned to a stimulus. This is accomplished by using roles hasCategory, hasDimension, hasAppraisal, hasActionTendency and hasSentiment which associate Stimulus and classes subsumed by Emotion. Inverse functions are isCategoryOf, isDimensionOf, isAppraisalOf, isActionTendencyOf and isSentimentOf, respectively. Numerical values of emotional dimensions valence, arousal and dominance are very important because they can be found in many stimuli databases annotated with the Russells\u2019 Circumplex model such as IAPS, IADS, ANEW, ANET and NAPS. In the STIMONT these values are represented as float datatype properties with domain Dimension and range Stimulus: . ./01 \u2291 Q01 0 , . ./01 \u2291 Q01 0 and R 10 . ./01 \u2291 Q01 0 . Float datatatype properties potency, unpredictability and intensity for other dimensional emotion theories are defined in the ontology. Also, datatype properties arousalSD, valenceSD and dominanceSD represent standard deviations for arousal, valence and dominance.\nSimultaneous annotation of a single stimulus with different category and dimensional models is permissible in the STIMONT. For example, if a stimulus /01 from Fig. 2 is annotated with a categorical emotion Happines from the BigSix discrete emotion model and also with dimensional values of the PED emotion model (valence,\narousal) = (7.14, 6.53), than the STIMONT\u2019s TBox and ABox contain the following axioms:\nTo express expert confidence in stimuli potential of eliciting a specific emotion state a string property confidenceLevel with enumerated 5-point Likert scale and a numerical datatype property confidenceValue were implemented in the ontology. Property confidenceLevel indicates the most probable discrete level of agreement in any subsumed class under Emotion with which a particular stimulus instance is connected. The confidence scale ratings are defined using OWL DL and OWL Full construct owl:oneOf. The allowed values under confidenceLevel are string literals VeryHigh, High, Average, Low and VeryLow, where the first represents the highest and the latter the least possible confidence. If a stimulus instance /01 from the previous example elicits emotion 1 with an average and 1 with a very high confidence then the STIMONT\u2019s ABox should be extended with the following axioms:\nThe second implemented construct for expression of confidence with a real number in the closed interval [0, 1]. This feature is similar to EmotionML attribute \u201cconfidence\u201d. The STIMONT supports the same expressivity with the datatype property confidenceValue. If confidence in an annotation is unknown or undefined then ABox does not contain confidenceLevel or confidenceValue axioms."}, {"heading": "3.3 Representation of stimuli semantics", "text": "For formal representation of complex stimuli semantics the STIMONT framework relies on Suggested Upper Merged Ontology (SUMO) which was explicitly designed as an upper, core and common-sense ontology. SUMO is currently the largest freely accessible formal upper ontology. Its large knowledge base contains over 25,000 terms and 80,000 axioms. Available SUMO to WordNet mappings help to express concepts in natural language terms [36] which facilitates extension of the framework towards existing tools for informal representation of multimedia (particularly images) with semantic networks and lexical ontologies. As will be discussed further, SUMO possess comparative advantages over other candidate upper ontologies even those who were specifically constructed for formal representation of multimedia. For all afore-mentioned reasons SUMO constitutes particularly adequate choice for multimedia stimuli annotation.\nAs mentioned previously, to achieve a more detail representation the ontology differentiates between three types of high-level semantics: 1) objects, 2) events, and 3) scenes. The segmentation of high-level semantic components reflects research trends in computer science within areas of concept based image retrieval, automated scene classification and event recognition. The partition of aggregate semantic components should be regarded as mandatory. It helps to create a more explicit and semantically rich representation of multimedia content that leads to more efficient stimuli retrieval. For the purpose of the STIMONT objects can be defined as physical entities represented by stimuli with well-defined and distinguishing features. As such objects may be items or named items such as persons, objects, animals or plants. Events are semantically meaningful human activities (e.g. talking, running, walking, driving, kayaking etc.), taking place within a specific environment and containing a number of necessary objects. Scenes are compound entities which are jointly and implicitly depicted with objects and events in a stimulus.\nTypically, an image stimulus will contain a number of physical objects, one scene and at least one event. Based on the current distribution of keywords in affective multimedia databases Object individuals will be the most represented semantic category and the most often used semantics class in search queries. Retrieving multimedia assets in the proposed architecture can be achieved by using semantic query languages such as the SPARQL query language [37]. The next figure illustrates a SPARQL 1.0 query that might be posed by an expert system using STIMONT.\nPREFIX stimont: http://www.owl-ontologies.com/ stimont.owl#\nPREFIX sumo: <http://www.owl-ontologies.com/ sumo.owl#>\nSELECT ?stim ?emo ?val ?ar\nWHERE\n{\n?stim rdf:type stimont:Stimulus.\n?stim stimont:hasSemantics ?sem.\n?sem rdf:subClassOf sumo:GroupOfPeople.\n?stim stimont:hasDimension ?emo.\nIn the example above stimuli in the lower right corner of the valence-arousal 2D dimensional emotion space defined as \u2208 a6.5,9b, \u2208 a1,3.5b and with semantics containing some type of groups of people are retrieved from the knowledge base. The query may be executed in the Prot\u00e9g\u00e9 ontology editor extended with the Jess rule engine [38]. Forward chaining search strategy should be used to maximize the number of returned tuples and the associated multimedia documents.\nIn another example, IADS stimulus 311.wav is described with a single keyboard \u201cCrowd2\u201d. Looking at description of concepts in SUMO1 the most appropriate (i.e. semantically similar) class for this keyword is \u201cGroupOfPeople\u201d (\u201cAny Group whose members are exclusively Humans.\u201d). The selected class needs to be in TBox, while ABox contains its instance and an instance of Stimulus concept for representation of the IADS sound file. The Stimulus instance and SUMO\u2019s concept GroupOfPeople instance are connected with hasObject property. Therefore, TBox and ABox for representation of IADS stimulus 311.wav would be:\nBy reusing definitions in the previous example semantically complex multimedia like IAPS pictures 5635.jpg and 7039.jpg in Fig. 1, represented as /01d and /01e in ABox, can be annotated with several different concept individuals as:\n/01d, /01e : ./01 1 : f0 / . 1d : . g 1e : ./ / 1h : 70/; 1i : ^ / 1 ?0 1j : 7 0 < 1k : l / /0 Q 0\n1 http://www.ontologyportal.org/SUMO.owl\nAs mentioned previously, contemporary affective multimedia databases lack expressivity in description of stimuli. Pictures 5635.jpg and 7039.jpg are annotated in IAPS with their respective keywords \u201cWinterStreet\u201d and \u201cTrain\u201d that convey only scenes in global terms while individual objects in the pictures are neglected. With STIMONT it is possible to assign an arbitrary number of descriptors for each stimulus. However, there is no uniform standard which determines the optimal level of expressivity. The required level of annotation expressiveness is determined by experts and database owners. It may include only one semantically predominant concept (as in IAPS, IADS, GAPED, ANEW etc.), or a concept and its subsuming category (as in NAPS), or \u2013 ultimately \u2013 the description may include all scenes, objects, events and even affective terms that contribute to meaning of a picture. The example in Fig. 9 follows the last suggestion and describes IAPS pictures 5635.jpg and 7039.jpg with SUMO concepts \u201cWinterSeason\u201d, \u201cSnow\u201d, \u201cStreet\u201d, \u201cCity\u201d, \u201cAutomobile\u201d, \u201cCovering\u201d and \u201cTransportationDevice\u201d, \u201cGeographicArea\u201d, \u201cFallSeason\u201d, \u201cForest\u201d, respectively. If some upper ontology other than SUMO is used for high-level annotation, the concept corpora would be different.\nPictures without a semantic context are much simpler to define than pictures with complex meaning. Retrieval of documents with narrow context is less prone to errors since their meaning is less noisy and the semantics can be described more accurately. IAPS picture 8163.jpg in Fig. 2 is a clear example of such a document. This picture is originally described with only one IAPS keyword \u201cParachute\u201d, but in Fig. 2 this is expanded to 5 different unsupervised keywords \u201cGround\u201d, \u201cParachute\u201d, \u201cPerson\u201d, \u201cParagliding\u201d, \u201cSky\u201d that were translated to subsuming SUMO concepts \u201cLandArea\u201d, \u201cDevice\u201d, \u201cHuman\u201d, \u201cTransportation\u201d, \u201cAtmosphericRegion\u201d as in the next figure. The description requires 5 concepts in TBox and 6 individuals in ABox since the picture displays two persons, i.e. instances of \u201cHuman\u201d concept.\n1 : \\ R^ 1 d : Q 0 1 e : T 1 1 h : l / /0 1 i : ^/1 \u210e 0 p <0\nFig. 10. A part of the STIMONT\u2019s TBox and ABox representing high-level content of narrow semantic context IAPS stimulus 8163.jpg. All Semantics individuals sem12 \u2013 sem16 are connected to stim1 with hasObject property. Together with ontological annotations in Fig. 2 this example comprehensively describes semantics and emotion of this stimulus."}, {"heading": "3.4 Stimuli context and elicited physiology", "text": "Emotion and semantics are not enough to reach a complete understating of a multimedia stimulus. Knowledge about emotion and semantic can be derived by observation, i.e. empirically, of a multimedia document. Additional information about the expected impact of stimulus must be stated by other sources. This metadata is represented by the concept Context in the STIMONT.\nTherefore, Context class can be regarded as a container for storage of all data relevant for understanding of a stimulus that does not belong to Semantics or Emotion classes. Using STIMONT it is possible to annotate stimuli with a wide range of additional metadata such as: stimulus unique identifier (id), stimuli database name, document dimensions (i.e. width and height), document size, color depth, stimulus length (in seconds; applicable to dynamic media such as sounds and films), author name, legal owner, creation date and time, depicted location, area or region, stimulus media format etc. Also, name of stimuli database is very important because it enables functional integration of different affective multimedia databases. Context class can be used to express this versatile knowledge domain and ensure a more complete stimuli description.\nFor example, the IADS stimulus 311.wav from Fig. 8 is 6 seconds long. This metainformation can be expressed with STIMONT to achieve a better understanding of the stimulus. The extension of representation of the stimulus 311.wav using Context class is demonstrated below.\nSimple level of Dublin Core metadata element set [39] is a standard for crossdomain information resource description and can ensure simple and standardized set of conventions for description of these additional context data. Stimuli metadata can be encoded with Dublin Core and integrated into STIMONT using the following pattern: dc:type describes the type of the resource, dc:creator refers to a person or body responsible for the content of the resource, dc:contributor is a person or entity responsible for making contributions to the content of the resource, dc:date is associ-\nated with an event in the lifecycle of the resource and finally dc:format describes the multimedia format of the resource. However, if Dublin Core is used for representation of stimuli context it must be strongly constrained. Although an OWL version of the Dublin Core is suitable for integration with enterprise ontologies its properties are loosely annotated. For instance, the preferred usage of the dc:creator tag is a \u201cLastName, FirstName\u201d literal. This is potentially ambiguous because different persons can have the same name (i.e. John Smith). Within the STIMONT this problem can be overcome if an instance of a Person class from a common upper ontology is used instead of a literal. However, such ad hoc solution negates Dublin Core interoperability and actually replaces loose confinement with another limitation. Currently this is an open problem and may be solved in the future by adding all contextual properties directly in STIMONT and completely avoiding domain ontologies.\nTime and location metadata are important because stimuli with pronounced personal meaning (i.e. ego relevance) are better at affect elicitation. Stimuli semantics aligned with personal experiences results in a stronger appraisal, intense cognitive associations and generates more pronounced behavioral reaction in exposed subjects. For these reasons time and location metadata can help to reject unimportant or meaningless semantics and retrieve only those stimuli that are spatially and temporarily associated with a subject\u2019s anxiety, phobia or mental trauma. For example, in a stimulation of a victim of the September 11 terrorist attack notions like \u201cLocation = New York, USA\u201d and \u201cTime = September 11, 2001\u201d will confine the search to the most relevant stimuli.\nIf the SUMO is used as the upper ontology then the notion of time is represented with the SUMO concept \u201cTimeMeasure\u201d and its subsumed classes, while geographic location can be represented with suitable domain ontology. FAO Ontology and Terminology System2 is a geopolitical ontology written in OWL DL that represents a hierarchy of the world\u2019s countries. This ontology also includes a multitude of useful information about geopolitical concepts like international names, nationality, latitude, longitude, list of countries bordering states and also properties \u201cisPredecessorOf\u201d and \u201cvalidUntil\u201d that together define political history of a concept.\nDefinition of Context class should not be regarded as static. If needed new metadata important for emotion elicitation may be added to this container concept. Context is important for complete description of pictures, but critical for correct understanding of pictures as stimuli.\nSince affective multimedia stimulates generation of physiology in exposed subjects and physiology has been shown to be an important and objective channel for automated estimation of emotion and related states [40], it is useful to connect physiology data with Stimulus class. In a typical emotion stimulation experiment physiology channels such as heart and breathing rate, skin temperature and conductance, ECG and EEG are acquired with a suite of sensors and the data is stored in formatted files. The files are usually large (tens or hundreds of megabytes), have diversified structures and contain series of numerical values recorded at a specific sampling rate. Ontology is not an appropriate medium for direct representation of this type of data, but ontological constructs can be successfully used to reference physiology files stored at some suitable and accessible location.\n2 http://www.fao.org/docrep/008/af243e/af243e00.htm\nIn the STIMONT Physiology concept has only one parameter path that references an URI with a physiology data file acquired during emotion elicitation experiment. Attribute path is defined as owl:DatatypeProperty. It is associated with Stimulus with domain constraints and additionally with owl:ObjectProperty hasPhysiology with domain Stimulus and range Physiology (\u210e 9\u210e; 0 <;. 9\u210e; 0 <; \u2291 ./01 ). The attribute hasPhysiology is non-functional since one stimulus can provoke an unrestricted number of physiology responses (\u22a5\u2291\u2264 1\u210e 9\u210e; 0 <;). The STIMONT does not make any restrictions to the format of physiology files, acquisition procedures or methods for their distribution. In a typical scenario a set of physiology channels would be recorded in a psychophysiology laboratory environment using acquisition equipment and data loggers. Afterwards the data can be processed and a set of acquired physiology objects with annotations attached to each stimulus object can be made available online. It is assumed that each resource has unique URI.\nFor example, ABox with acquired physiology for two subjects that were exposed to the IAPS picture 8163.jpg in Fig. 2 is shown below. Three distinct physiologies were acquired (phy1, phy2, phy3). Heart rate (HR) channel was recorded for both subjects and skin resistance (SR) only for the first one. Subjects were stimulated using the same image stimulus ( /01 ). The image\u2019s unique identifier (id) is \u201c8163\u201d."}, {"heading": "4 Ontology implementation and validation", "text": "The STIMONT was written in OWL DL using the Prot\u00e9g\u00e9 ontology editor (version 3.4.5). Currently the ontology has 178 classes and 38 properties. By far the most complex part of the ontology is related to the Emotion class with 173 subsumed classes which represent individual emotion theories and related concepts such as ActionTendency, Appraisal, Category, Dimension and Sentiment. Representations of Appraisal, Dimension and Sentiment are expressed as float values in range [0.0, 1.0]\nand defined in domain datatype properties, while ActionTendency and Category are represented as individuals in ABox. Object properties like hasActionTendency, hasAppraisal, hasContext, hasEmotion, hasPhysiology and hasSemantics connect individuals of Stimulus class to individuals of pertaining classes.\nTo facilitate intuitive retrieval of affective multimedia documents, a software tool Intelligent Stimuli Generator (intStimGen) was developed. The intStimGen was designed as n-layered Desktop application with an efficient user-friendly graphical interface.\nThis tool enables searching of affective multimedia databases and construction of stimuli sequences using semantic and emotion descriptors. It follows previous work in developing an online tool WNtags for collaborative annotation and retrieval of IAPS images using WordNet [41]. In intStimGen stimuli semantics can be described with unrestricted keywords, tag clouds, WordNet synsets and SUMO concepts while emotion descriptors are based on the dimensional model. Relatedness between descriptors can be calculated with lexical and semantic similarity measures. The implemented measures are: Levenshtein, path length, Wu Palmer, Leacock Chodorow and Li algorithm [42]. Other measures can be modularly added later if needed. Document retrieval and ranking can be performed using any combination of descriptors and similarity measures. Integrated exploration of IAPS and IADS is supported, and the application\u2019s architecture enables other stimuli databases to be modularly added in the future. The constructed sequences can be displayed to a participant on a separate screen and his physiological and behavioral responses may be acquired using specialized hardware. During exposure the application sends system messages which can be used for synchronization of physiology acquisition equipment and other systems.\nThe tool is written in .NET 3.5 framework and optionally may use Jena.NET toolkit3 for querying and integration with the Prot\u00e9g\u00e9. In this environment retrieval can be performed with the Pellet 2.2.2 reasoner over XML-based DIG 1.1 query interface. SPARQL queries as in Fig. 7 may be entered using Jess system shell.\nThe STIMONT is distributed as a single OWL DL file \u201cstimont.owl\u201d and is available for research purposes together with stimuli metadata used in the experiment and installation of the intStimGen tool [43]."}, {"heading": "4.1 Multimedia stimuli retrieval", "text": "In order to functionally validate the proposed concept it was necessary to evaluate the retrieval performance of STIMONT and compare it to other methods for annotation of multimedia stimuli. The retrieval process from the existing affective multimedia databases is based entirely on unmanaged keywords and affective values. Additional metadata like context and physiology are not supported. Therefore, to objectively compare the ontology-based retrieval with the existing procedures it is necessary to confine the validation on the shared search modalities, i.e. semantics and emotion. Furthermore, since emotions in affective multimedia databases are described as vectors in Euclidian space [18, 19] with well known distribution while high-level multimedia meaning is more uncertain, semantics is a more important parameter in evaluation.\nFor experimentation in multimedia retrieval a subset of 772 pictures were first extracted from IAPS. The selected pictures have clear meaning and do not provoke intense negative emotions. In the next step high-level content of the selected pictures was hand annotated with WordNet terms. The keywords were extended with the most appropriate WordNet synsets by considering content of each picture and the meaning of its keyword. If an identical synset does not exist for a given keyword it was extended with more than one synset. Then WordNet to SUMO mappings [44] were used to obtain semantically equivalent SUMO annotations.\nFor this experiment 500 pictures were selected that show animals, nature, people, household objects and food. Each picture in document collection x was annotated with an original IAPS keyword wy \u2208 z and the transformed SUMO concept cy \u2208 {. In total, the selected pictures were described with 318 individual keywords and 162 SUMO concepts. The selected images are emotionally either neutral or highly positive and arousing. Picture affective values were inherited from IAPS. The query collection | consisted of different individual queries from both annotation glossaries. T he retrieved pictures were ranked using string inclusion, Levenshtein, path length and Wu Palmer algorithms. For each query }~ \u2208 | a subset of documents x \u2282 x with x = 100 pictures were randomly selected and classified using the aforementioned glossaries and relatedness measures. In total 120 such document retrieval tasks were executed and 3000 pictures were ranked and classified. The queries always contained one keyword or ontology concept. Only terms that exist in the annotations of x were used thereby ensuring that the retrieved set should be nonempty.\n3 http://www.linkeddatatools.com/downloads/jena-net\nEach of the ranking algorithms provided a measure of relatedness (similarity score) between two labels rel x, y \u2208 a0,1b, where x, y = z \u222a {, that was used to rank the retrieved images, with properties:\nrel x, y = 1, x = y rel x, y < 1, 8 \u2260 ;\nSince unsupervised keywords are mutually semantically unrelated only lexical similarity algorithms (string inclusion and Levenshtein) were used to establish a relatedness measure between them. Path length and Wu Palmer algorithms were used exclusively on SUMO knowledge taxonomy with ontology concepts as nodes and concept inheritance (i.e. IS-A functional relationship) representing connections between nodes. The inclusion represents the simplest measure that only checks if a string is included or exists in another string, while Wu Palmer is the most complex. The idea behind this choice of algorithms was to test how more simple description methods such as keywords and lexical similarity measures compare with more complex annotations (i.e. ontologies) and compatible retrieval methods.\nSince each query }~ always returned all x = 100 images (there was no cut-off) in descending order, with picture most relevant to the search query on top of the list and the least relevant at the end, it was necessary to determine the threshold value t that determines the classification boundary and how the retrieved pictures will be categorized. Given a retrieved picture ~ \u2208 x and its rank in the set of retrieved pictures ~ \u2208 a1, 100b, where = 1 indicates the most relevant picture , the picture ~ was classified in category /~ = l , o as:\n/~ = True, ~ \u2264 / /~ = False, ~ > /\nTherefore, in each query the system partitioned the retrieved corpus into two subsets of pictures: those it considered relevant to the search query (category True), and those it did not (category False). The threshold value was determined for each query using the lift curve. For the maximum precision in retrieval the threshold was set to the rank with the highest lift factor. This adaptive approach enables more objective evaluation of different ranking retrieval algorithms than a constant classification threshold. For example, the result set 1 in [43] containing pictures queried with SUMO concept \u201cMan\u201d and ranked with Wu Palmer algorithm has the maximum lift factor for = 5 implying that to achieve the highest precision in this set only pictures with \u2264 5 have to be classified as True and all other pictures with > 5 as False. The order in which the returned documents were presented was disregarded because it was not possible to exactly establish the optimal order of documents, but only the partition of documents into the two classes: relevant (True) and non relevant (False). The aggregated experiment results are shown in the next table.\nAs can be seen in the table, ontology annotations are better than keywords in all aspects of ranked retrieval of IAPS pictures, albeit the difference is small in some parameters. Accuracy is 4.4557 \u2013 9.8657% higher and precision even more 21.872 \u2013 24.9369%. Recall is 4.7381 \u2013 13.0802% better and F-measure 16.0447 \u2013 21.9814%. Since Fall-out is the proportion of non-relevant documents that are retrieved, out of all non-relevant documents, lower result is better. F-measure combines precision and recall as their weighed harmonic mean. Taken together the results could be interpreted that the ontology retrieval is superior in discrimination of non-relevant documents while a bit less capable in identification of relevant documents.\nThe highest gain with ontology-based retrieval is in precision (24.9369%) and Fmeasure (21.9814%) because it benefits from knowing the semantic relatedness between different concepts which is beyond the capabilities of lexical algorithms. A single concept may encapsulate meaning of several different keywords which must be individually identified and used in retrieval.\nString inclusion algorithm has the worst aggregated performance and represents the baseline algorithm that can be used on contemporary semantic descriptions in affective multimedia databases. However, it has high true positive rate, resulting in above average accuracy and recall, only if query keyword is used for annotation of semantically identical images. This algorithm requires that user has some conception about the keywords before using the database otherwise the retrieval performance will be very poor. All other algorithms are an upgrade to affective multimedia databases and can be used only if they are extended with additional retrieval tools as was done in this experiment. In this experiment the Levenshtein algorithm represented the best retrieval method based purely on lexical properties of picture descriptions. Since IAPS keywords are only lexically related more meaningful description of picture semantics should be implemented with ontologies. Using Levenshtein algorithm the difference between keywords and ontology is somewhat reduced, especially in accuracy, but ontology still gives better results, especially in the percentage of relevant documents that are returned. As can be seen in Table 1 there is a smaller difference in retrieval performance between path length and Wu Palmer algorithms as is between ontology and keywords. This indicates that the relatedness algorithm is much less important than the method for annotation of affective pictures.\nThe results also indicate that IAPS pictures are weakly annotated. Only one keyword or ontology concept per picture is insufficient to relate the full higher meaning which results in higher false positive and false negative count in some data sets. Furthermore, statements about multimedia content \u2013 without rules about the domain \u2013 are not enough to encapsulate all knowledge important for quality retrieval.\nThis experiment should be regarded as an initial validation of the ontology presented in the paper. The tested application of the ontology is a minimum that an ontology-\nbased description of affective multimedia may provide. More complex retrieval procedures involving procedural knowledge and more expressive ontological annotations of stimuli content should be evaluated in the future."}, {"heading": "5 Discussion and future work", "text": "In practical terms the STIMONT should be regarded as a method for annotation of emotion eliciting multimedia to enable better and more efficient stimuli knowledge representation and retrieval that lead to the development of tools for improvement of contemporary affective multimedia databases. The STIMONT builds upon the work of W3C in designing the EmotionML and extends it with additional vocabularies and expressivity towards describing multimedia documents with eliciting emotion values. In this regard the STIMONT\u2019s multimedia annotations are akin to EmotionML references \"triggeredBy\" or \u201ctargetedAt\u201d. The ontology does not differentiate between semantics of these two references. Which specific reference of these two is implied depends weather ABox contains individuals of emotion and appraisals or action tendencies. Semantics of EmotionML references \u201cexpressedBy\u201d and \u201cexperiencedBy\u201d is not currently supported because the STIMONT is primarily designed for representation of stimuli (i.e. anticipated emotion) and not of emotion experienced by subjects. If in the future the proposed ontology could be expanded to formalize the entire spectrum of emotion-related phenomena then it may also be possible to represent observable behavior expressing the emotion and the subject experiencing the emotion.\nIn this application the ontology could deduce various relationships, in particular equivalence, between seemingly implacable emotion terms. For example, using axioms it is possible to state that basic emotion anger in the Ekman\u2019s BigSix and the OCC model are the same as < . S0<.08 \u2261 < . >777 / < ;. Another emotion theory FSRE with the same emotion category anger may also be considered. If BigSix and FSRE anger emotion categories are identical than it is possible to state\n< . S0<.08 \u2261 < . o.p67 / < ;. Then by using a reasoning engine it can be immediately inferred that < . o.p67 / < ; \u2261 < . >777 / < ; is also true. Such equivalencies or other relationships (e.g. subsumption) are possible for other emotion concepts as well. However, cross-equalization of different emotion vocabularies within the same knowledge base should be accomplished by a knowledge engineer working in tandem with a qualified domain expert. Different categorical emotion theories should not be conjoined just because they have lexically similar terms. More thorough analysis of vocabulary semantics is warranted before these identities are axiomatized and stored in TBox. Integration of WordNet-Affect [45] taxonomy of emotion related terms in the STIMONT would be beneficial for establishment of relationships and transformations between different emotion concepts. However, the problem of correct identification of concept domain and range still remains.\nAs mentioned in Section 3.3 the top stimuli semantics is described with SUMO because of its many useful and practical features. Other upper ontologies like DOLCE [46] and ConceptNet [47] could be used instead of SUMO for representation of highlevel objects, events and scenes. Furthermore, dedicated multimedia-representation\nontologies like Large-Scale Concept Ontology for Multimedia (LSCOM) [48] and Core Ontology for MultiMedia (COMM) [49] could also be a natural choice for annotation of multimedia stimuli. High accuracy concept-based video retrieval in some closed domains has been reported for these top ontologies [50]. However, compared to SUMO other ontologies have the following disadvantages when used for high-level stimuli annotation:\n1. Fewer concepts in TBox (i.e. smaller vocabulary); 2. Lesser expressivity; 3. Proprietary or limited access for ontology reuse requiring individual permis-\nsions, and\n4. Lack of integration with other knowledge representations used in image annotation frameworks based on semantic networks such as WordNet [51]. The presented ontology captures high-level information in semantics and emotion but it has been shown that low-level features like color, hue, brightness and texture influence perception and can be positively correlated to arousal and discrete emotion categories [52]. Therefore, it future version of the STIMONT it would be useful to represent perceptual and low-level meaning of stimuli. This would certainly add to the richness of representation and enhance multidimensional stimuli retrieval.\nWith regards to the terminology, throughout the paper term \u201csubject\u201d was used to denote a person who is exposed to an emotionally annotated multimedia document. There was no intention to limit the scope of this term only to participants of an emotion or attention related experiments. The STIMONT can be used in HumanComputer Interaction (HCI) or any other interaction between humans and computer that involves emotion and emotion related phenomena such as affective interfaces or video games. Also, the STIMONT can be applied for representation of multimedia and Virtual Reality (VR) stimuli in psychotherapy as in exposure therapy (ET) or stress inoculation training (SIT) [53]. In this regard terms \u201csubject\u201d, \u201cexposed person\u201d, \u201cparticipant\u201d and even \u201ctrainee\u201d could be used as synonyms.\nThe STIMONT should not be seen as a singular or exclusive solution to the problem of formal representation of emotions and related states, but rather as a representative of knowledge representation methods based on DLs specifically intended for construction of practical software tools for affective multimedia databases.\nDue to pleura of different emotion models, and the ongoing active research on the origin and nature of emotions, it is technically impossible to create a single ontology that will equally and sufficiently represent all theories. For this reason it is important to allow for future expansion and enlargement of the ontology while retaining backward compatibility with previous versions. Other solutions in this domain are possible, but the STIMONT offers numerous advantages like logic-based reasoning, formal presentation of concepts, inclusion of leading emotion theories, standardized representation vocabularies and hierarchies of affective terms with ready-to-use WordNet mappings.\nIn the future STIMONT should allow detailed representation of dynamic stimuli i.e. individual frames of videos and movies. Films, video-clips and related dynamic presentation technologies are often used for induction of emotion in the laboratory because of their relatively high attention capture and intensity [54]. Compared to other multimedia formats personalized video clips, in particular real-life footage, represent the most powerful multimedia elicitation tool because they provide a high\ndegree of personal significance and immersion that that results in a more intensive stimulation than with any individual audio or visual stimuli. Therefore, integration of the STIMONT with MPEG-7 multimedia content description standard for affective annotation of video would be beneficial. Finally, besides annotation of affective multimedia, which is the STIMONT\u2019s immediate application, this ontology can also be used to facilitate integration of different emotion theories and for general reasoning about emotion-related phenomena. This line of research is primarily oriented towards creating tools in areas of psychology and neuroscience."}, {"heading": "6 Conclusion", "text": "The proposed ontology offers a number of advantages over the existing methods for knowledge representation in the emotionally annotated multimedia. In the evaluation experiment the ontology-based approach achieved up to 24.9369% better precision in ranked retrieval of IAPS pictures. Compared to the existing method of retrieval with keywords this represents an increase in all performance and correctness measures. Although ontologies require more complex prerequisites and reasoning infrastructure than the markup or keyword based annotations, they are much more advantageous than the simpler methods and provide for a better multimedia retrieval.\nThe STIMONT provides a practical model for formal representation of high-level semantics, emotion and related states, document context and stimulated physiology that collectively define a multimedia stimulus. The presented ontology builds upon W3C EmotionML format and extends it with additional emotion vocabularies. The STIMONT reuses SUMO common sense ontology and SUMO to WordNet mappings to provide a rich high-level semantic expressivity with interface to commonplace models based on informal knowledge and probabilistic reasoning. The STIMONT facilitates knowledge reuse, interoperability and formalization of stimuli information which are superior to the contemporary methods for representation of emotionally annotated documents. All these features enable formal, consistent and systematic annotation of the affective multimedia and DL-based reasoning about their aggregated content and document properties."}], "references": [{"title": "The Handbook of Emotion Elicitation and Assessment", "author": ["J.A. Coan", "Allen", "J.J.B. (eds."], "venue": "Oxford University Press Series in Affective Science. Oxford University Press, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Conscious emotional experience emerges as a function of multilevel, appraisal-driven response synchronization", "author": ["D. Grandjean", "D. Sander", "K.R. Scherer"], "venue": "Consciousness and Cognition, Vol. 17.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Tagging multimedia stimuli with ontologies", "author": ["M. Horvat", "S. Popovi\u0107", "N. Bogunovi\u0107", "K. \u0106osi\u0107"], "venue": "Proceedings of the 32nd International Convention MIPRO 2009, Croatian Society for Information and Communication Technology, Electronics and Microelectronics \u2013 MIPRO, Opatija, Croatia", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimedia stimuli databases usage patterns: a survey report", "author": ["M. Horvat", "S. Popovi\u0107", "K. \u0106osi\u0107"], "venue": "Proceedings of the 36th International Convention MIPRO 2013, Croatian Society for  25  Information and Communication Technology, Electronics and Microelectronics \u2013 MIPRO, Opatija, Croatia", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual", "author": ["P.J. Lang", "M.M. Bradley", "B.N. Cuthbert"], "venue": "Technical Report A-8. University of Florida, Gainesville, FL", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "The International Affective Digitized Sounds (2nd Edition; IADS-2): affective ratings of sounds and instruction manual", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Technical report B-3. University of Florida, Gainesville, FL", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Measuring emotion: Behavior, feeling and physiology", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Lane, R., Nadel, L. (eds.): Cognitive neuroscience of emotion. Oxford University Press, New York", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "The Geneva Affective PicturE Database (GAPED): A new 730 picture database focusing on valence and normative significance", "author": ["E.S. Dan-Glauser", "K.R. Scherer"], "venue": "Behavior Research Methods, Vol. 43(2).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The Nencki Affective Picture System (NAPS)", "author": ["A. Marchewka", "\u0141. \u017burawski", "K. Jednor\u00f3g", "A. Grabowska"], "venue": "Introduction to a novel standardized wide range high quality realistic pictures database", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "The NimStim set of facial expressions: Judgments from untrained research participants", "author": ["N. Tottenham", "J.W. Tanaka", "T A.C. Leon", "McCarry", "M. Nurse", "T.A. Hare", "D.J. Marcus", "A. Westerlund", "B.J. Casey", "C. Nelson"], "venue": "Psychiatry Research, Vol. 168(3).", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Technical report C-1. Gainesville, FL. The Center for Research in Psychophysiology, University of Florida", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Affective Norms for English Text (ANET): Affective ratings of text and instruction manual", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "(Tech. Rep. No. D-1). University of Florida, Gainesville, FL", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining", "author": ["S. Baccianella", "A. Esuli", "F. Sebastiani"], "venue": "Proceedings of LREC-10, 7th Conference on Language Resources and Evaluation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A survey of affect recognition methods: audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31(1).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Face Databases", "author": ["R. Gross"], "venue": "Li, S., Jain, A. (eds): Handbook of Face Recognition. Springer-Verlag, The Robotics Inistitute, Carnegie Mellon University Forbes Avenue, Pittsburgh", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "A circumplex model of affect", "author": ["J.A. Russell"], "venue": "Journal of Personality and Social Psychology, Vol. 39.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1980}, {"title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament", "author": ["A. Mehrabian"], "venue": "Current Psychology, Vol. 14(4).", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "Emotion representation and physiology assignments in digital systems", "author": ["C Peter", "A. Herbon"], "venue": "Interacting with Computers, Vol. 18.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Measuring emotion: The Self-Assessment Manikin and the semantic differential", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Journal of Behavior Therapy & Experimental Psychiatry, Vol. 25.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Are there basic emotions? Psychological Review, Vol", "author": ["P. Ekman"], "venue": "99.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Multidimensional normative ratings for the international affective picture system, Behavior research", "author": ["T.M. Libkuman", "H. Otani", "R. Kern", "S.G. Viger", "N. Novak"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Affective auditory stimuli: Characterization of the International Affective Digitized Sounds (IADS) by discrete emotional categories", "author": ["R.A. Stevenson", "T.W. James"], "venue": "Behavior Research Methods. Vol. 40(1)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Characterization of affective norms for English words by discrete emotional categories", "author": ["R.A. Stevenson", "J.A. Mikels", "T.W. James"], "venue": "Behavior Research Methods, Vol. 39.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Handbook on ontologies", "author": ["S. Staab", "Studer", "R. (Eds.)."], "venue": "Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Ontology-based reasoning techniques for multimedia interpretation and retrieval", "author": ["B. Neumann", "R. M\u04e7ller"], "venue": "Semantic Multimedia and Ontologies: Theory and Applications, Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "EmotionML \u2013 An Upcoming Standard for Representing Emotions and Related States", "author": ["M. Schr\u00f6der", "P. Baggia", "F. Burkhardt", "C. Pelachaud", "C. Peter", "E. Zovato"], "venue": "Affective Computing and Intelligent Interaction. Lecture Notes in Computer Science Volume 6974.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "The MPEG-7 visual standard for content description-an overview", "author": ["T. Sikora"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, 11(6), 696\u2013702.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Emotion Description with MPEG-7", "author": ["H. Agius"], "venue": "Affective Computing and Intelligent Interaction. Emotion in HCI\u2013Designing for People, 13.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "The landscape of multimedia ontologies in the last decade", "author": ["M.C. Su\u00e1rez-Figueroa", "G.A. Atemezing", "O. Corcho."], "venue": "Multimedia Tools and Applications, Vol. 55(3).", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards an ontology for describing emotions", "author": ["J.M. L\u00f3pez", "R. Gil", "R. Garc\u00eda", "I. Cearreta", "N. Garay"], "venue": "Lecture Notes in Computer Science, 5288, 96\u2013104.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Basic Description Logics (Description Logic Handbook)", "author": ["F. Baader", "W. Nutt"], "venue": "Edited by F. Baader, D. Calvanese, D.L. McGuinness, D. Nardi, P.F. Patel-Schneider, Cambridge University Press", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Tool Support for Ontology Engineering", "author": ["I. Horrocks"], "venue": "Fensel, D. (ed.): Foundations for the Web of Information and Services, Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "The Suggested Upper Merged Ontology: A Large Ontology for the Semantic Web and its Applications", "author": ["A. Pease", "I. Niles", "J. Li"], "venue": "Working Notes of the AAAI-2002 Workshop on Ontologies and the Semantic Web", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Linking Lexicons and Ontologies: Mapping WordNet to the Suggested Upper Merged Ontology", "author": ["I. Niles", "A. Pease"], "venue": "Proceedings of the IEEE International Conference on Information and Knowledge Engineering", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2003}, {"title": "SPARQL Query Language for RDF", "author": ["E. Prud\u2019hommeaux", "A. Seaborne"], "venue": "W3C Candidate Recommendation,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Writing Rules for the Semantic Web Using SWRL and Jess", "author": ["M. O\u2019Connor"], "venue": "8th International Protege Conference, Prot\u00e9g\u00e9 with Rules Workshop. Madrid", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Using Dublin core", "author": ["D. Hillmann"], "venue": "Recommendation. Dublin Core Metadata Initiative", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2001}, {"title": "Toward Recognizing Individual's Subjective Emotion from Physiological Signals in Practical Application", "author": ["O. Villon", "S. Antipolis", "C. Lisetti"], "venue": "Twentieth IEEE International Symposium on Computer-Based Medical Systems.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies", "author": ["M. Horvat", "A. Grbin", "G. Gledec"], "venue": "Frontiers in artificial intelligence and applications, Vol. 243.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Information retrieval by semantic similarity", "author": ["A. Hliaoutakis", "G. Varelas", "E. Voutsakis", "E.G. Petrakis", "E. Milios"], "venue": "International Journal on Semantic Web and Information Systems (IJSWIS), 2(3).", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "STIMONT: A core ontology for multimedia stimuli description \u2013 Supplementary Online Data", "author": ["M. Horvat"], "venue": "http://goo.gl/PocuK.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Linking Lexicons and Ontologies: Mapping WordNet to the Suggested Upper Merged Ontology", "author": ["I. Niles", "A. Pease"], "venue": "Proceedings of the IEEE International Conference on In-formation and Knowledge Engineering", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}, {"title": "WordNet-Affect: an Affective Extension of WordNet", "author": ["C. Strapparava", "A. Valitutti"], "venue": "Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2004}, {"title": "Sweetening Ontologies with DOLCE", "author": ["A. Gangemi", "N. Guarino", "C. Masolo", "A. Oltramari", "L. Schneider"], "venue": "Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web, Lecture Notes in Computer Science, Vol. 2473. Springer Berlin Heidelberg", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "ConceptNet: A practical commonsense reasoning toolkit", "author": ["H. Liu", "Singh. P."], "venue": "BT Technology Journal, Vol. 22.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Largescale concept ontology for multimedia", "author": ["M. Naphade", "J.R. Smith", "J. Tesic", "S.F. Chang", "W. Hsu", "A. Hauptmann", "J. Curtis"], "venue": "IEEE Multimedia, Vol. 13(3).", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "COMM: Designing a WellFounded Multimedia Ontology for the Web", "author": ["R. Arndt", "R. Troncy", "S. Staab", "L. Hardman", "M. Vacuram"], "venue": "The Semantic Web, Lecture Notes in Computer Science, Vol. 4825. Springer Berlin Heidelberg", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Can High-Level Concepts Fill the Semantic Gap in Video Retrieval? A Case Study With Broadcast News", "author": ["A. Hauptmann", "R. Yan", "Lin", "W.-H.", "M. Christel", "H. Wactlar"], "venue": "IEEE Transactions on Multimedia, Vol. 9(5).", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": "MIT Press", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Affective Image Classification using Features Inspired by Psychology and Art Theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "ACM Multimedia 2010 - Multimedia Content Track Full Paper, Florence, Italy", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Stress inoculation training supported by physiology-driven adaptive virtual reality stimulation", "author": ["S. Popovi\u0107", "M. Horvat", "D. Kukolja", "B. Dropulji\u0107", "K. \u0106osi\u0107"], "venue": "Studies in Health Technology and Informatics, Vol. 144.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2009}, {"title": "Emotion elicitation using films", "author": ["Rottenberg", "R.D.J. Ray", "J.J. Gross"], "venue": "The Handbook of Emotion Elicitation and Assessment, Oxford University Press, USA", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "By being exposed to multimedia stimuli individuals\u2019 emotional states may be modulated [1, 2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 1, "context": "By being exposed to multimedia stimuli individuals\u2019 emotional states may be modulated [1, 2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 2, "context": "This paper addresses multiple drawbacks of contemporary affective multimedia databases [3] and proposes an ontology-based approach for formal description of stimuli metadata which aims to optimize both the annotation and retrieval processes from these databases.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Motivation for this work was supported by an online survey on usage patterns of multimedia stimuli databases [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "The International Affective Picture System (IAPS) [5] and the International Affective Digital Sounds system (IADS) [6] are two of the most cited databases in the area of affective stimulation.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "The International Affective Picture System (IAPS) [5] and the International Affective Digital Sounds system (IADS) [6] are two of the most cited databases in the area of affective stimulation.", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "They were created with three goals in mind [7]: 1.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 198, "endOffset": 201}, {"referenceID": 9, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 314, "endOffset": 318}, {"referenceID": 10, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 400, "endOffset": 404}, {"referenceID": 11, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 447, "endOffset": 451}, {"referenceID": 12, "context": "Apart from the IAPS and IADS the most frequently used and readily available affective multimedia databases are Geneva Affective PicturE Database (GAPED) [8], Nencki Affective Pictures System (NAPS) [9], Dataset for Emotion Analysis using eeg, Physiological and video signals (DEAP) [10], NimStim Face Stimulus Set [11], Pictures of Facial Affect (POFA) [12], Affective Norms for English Words (ANEW) [13], Affective Norms for English Texts (ANET) [14] and SentiWordNet [15].", "startOffset": 469, "endOffset": 473}, {"referenceID": 13, "context": "Additional audio-visual affective multimedia databases with category or dimensional emotion annotations are listed here [16].", "startOffset": 120, "endOffset": 124}, {"referenceID": 14, "context": "A more detailed overview of these databases is given in [17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "Two predominant theories used to describe emotion are the discrete category model and the dimensional model of affect (also sometimes called Circumplex model of affect [18] or PAD [19]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Two predominant theories used to describe emotion are the discrete category model and the dimensional model of affect (also sometimes called Circumplex model of affect [18] or PAD [19]).", "startOffset": 180, "endOffset": 184}, {"referenceID": 17, "context": "All affective multimedia databases have been characterized according to at least one of these models [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "Dimensions are chosen on their ability to statistically characterize subjective emotional ratings with the least number of dimensions possible [21].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "Instead, supporters of these theories propose that there are a number of emotions that are universal across cultures and have an evolutionary and biological basis [22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "Stimuli previously only characterized according to a single theory have also been characterized according to the complimentary emotion theory, as for example in IAPS [23], IADS [24] and ANEW [25].", "startOffset": 166, "endOffset": 170}, {"referenceID": 21, "context": "Stimuli previously only characterized according to a single theory have also been characterized according to the complimentary emotion theory, as for example in IAPS [23], IADS [24] and ANEW [25].", "startOffset": 177, "endOffset": 181}, {"referenceID": 22, "context": "Stimuli previously only characterized according to a single theory have also been characterized according to the complimentary emotion theory, as for example in IAPS [23], IADS [24] and ANEW [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": "By definition ontologies are a representation of a shared understanding about a specific domain and enable the derivation of implicit knowledge from the existing explicit knowledge and automated inference with reasoning engines [26].", "startOffset": 228, "endOffset": 232}, {"referenceID": 24, "context": "Ontologies have been successfully applied for description of high-level image content, concept semantics, object labels and relationships defined in the upper levels of the image representation hierarchy [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 25, "context": "Except the W3C EmotionML format [28] much work has been done to enable describing information about emotions in multimedia, especially in the video.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "MPEG-7 multimedia standard, which is based on XML and can be expanded with additional tools, provides a method for describing emotions with its Affective Description Scheme [29].", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "Researchers have proposed new description tools that rely on MPEG-7 to provide a broader description of affective terms that can be used in video annotation [30].", "startOffset": 157, "endOffset": 161}, {"referenceID": 28, "context": "Also, several multimedia ontologies which are potentially applicable in emotion description have been proposed in the last decade [31] including ontologies specially designed for high-level description of cognitive-emotional related concepts [32].", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "Also, several multimedia ontologies which are potentially applicable in emotion description have been proposed in the last decade [31] including ontologies specially designed for high-level description of cognitive-emotional related concepts [32].", "startOffset": 242, "endOffset": 246}, {"referenceID": 30, "context": "These two types of knowledge are the basic components of a knowledge-based system based on Description Logics (DLs) [33] as a set of structured knowledge-representation formalisms with decidable-reasoning algorithms.", "startOffset": 116, "endOffset": 120}, {"referenceID": 31, "context": "A variety of tools for knowledge engineering exist [34] which allow construction, management, reuse and reasoning with OWLbased ontologies.", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "The STIMONT is currently using Suggested Merged Upper Ontology (SUMO) [35] in OWL DL format as foundation ontology for formal representation of stimuli high-level content.", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "Each of first five classes formally represents one of the category vocabularies in EmotionML [28]: emotions that frequently occur in everyday life, six primary emotions universal in all cultures, categories related to Frijda\u2019s proposal of action tendencies, categories compromising Ortony, Clore and Collins appraisal model, and finally categories used in a study by Fontaine, Scherer, Roesch and Ellsworth, respectively.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "The second implemented construct for expression of confidence with a real number in the closed interval [0, 1].", "startOffset": 104, "endOffset": 110}, {"referenceID": 33, "context": "Available SUMO to WordNet mappings help to express concepts in natural language terms [36] which facilitates extension of the framework towards existing tools for informal representation of multimedia (particularly images) with semantic networks and lexical ontologies.", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "Retrieving multimedia assets in the proposed architecture can be achieved by using semantic query languages such as the SPARQL query language [37].", "startOffset": 142, "endOffset": 146}, {"referenceID": 35, "context": "The query may be executed in the Prot\u00e9g\u00e9 ontology editor extended with the Jess rule engine [38].", "startOffset": 92, "endOffset": 96}, {"referenceID": 36, "context": "Simple level of Dublin Core metadata element set [39] is a standard for crossdomain information resource description and can ensure simple and standardized set of conventions for description of these additional context data.", "startOffset": 49, "endOffset": 53}, {"referenceID": 37, "context": "Since affective multimedia stimulates generation of physiology in exposed subjects and physiology has been shown to be an important and objective channel for automated estimation of emotion and related states [40], it is useful to connect physiology data with Stimulus class.", "startOffset": 209, "endOffset": 213}, {"referenceID": 38, "context": "It follows previous work in developing an online tool WNtags for collaborative annotation and retrieval of IAPS images using WordNet [41].", "startOffset": 133, "endOffset": 137}, {"referenceID": 39, "context": "The implemented measures are: Levenshtein, path length, Wu Palmer, Leacock Chodorow and Li algorithm [42].", "startOffset": 101, "endOffset": 105}, {"referenceID": 40, "context": "owl\u201d and is available for research purposes together with stimuli metadata used in the experiment and installation of the intStimGen tool [43].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "Furthermore, since emotions in affective multimedia databases are described as vectors in Euclidian space [18, 19] with well known distribution while high-level multimedia meaning is more uncertain, semantics is a more important parameter in evaluation.", "startOffset": 106, "endOffset": 114}, {"referenceID": 16, "context": "Furthermore, since emotions in affective multimedia databases are described as vectors in Euclidian space [18, 19] with well known distribution while high-level multimedia meaning is more uncertain, semantics is a more important parameter in evaluation.", "startOffset": 106, "endOffset": 114}, {"referenceID": 41, "context": "Then WordNet to SUMO mappings [44] were used to obtain semantically equivalent SUMO annotations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 40, "context": "For example, the result set 1 in [43] containing pictures queried with SUMO concept \u201cMan\u201d and ranked with Wu Palmer algorithm has the maximum lift factor for = 5 implying that to achieve the highest precision in this set only pictures with \u2264 5 have to be classified as True and all other pictures with > 5 as False.", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "Integration of WordNet-Affect [45] taxonomy of emotion related terms in the STIMONT would be beneficial for establishment of relationships and transformations between different emotion concepts.", "startOffset": 30, "endOffset": 34}, {"referenceID": 43, "context": "Other upper ontologies like DOLCE [46] and ConceptNet [47] could be used instead of SUMO for representation of highlevel objects, events and scenes.", "startOffset": 34, "endOffset": 38}, {"referenceID": 44, "context": "Other upper ontologies like DOLCE [46] and ConceptNet [47] could be used instead of SUMO for representation of highlevel objects, events and scenes.", "startOffset": 54, "endOffset": 58}, {"referenceID": 45, "context": "23 ontologies like Large-Scale Concept Ontology for Multimedia (LSCOM) [48] and Core Ontology for MultiMedia (COMM) [49] could also be a natural choice for annotation of multimedia stimuli.", "startOffset": 71, "endOffset": 75}, {"referenceID": 46, "context": "23 ontologies like Large-Scale Concept Ontology for Multimedia (LSCOM) [48] and Core Ontology for MultiMedia (COMM) [49] could also be a natural choice for annotation of multimedia stimuli.", "startOffset": 116, "endOffset": 120}, {"referenceID": 47, "context": "High accuracy concept-based video retrieval in some closed domains has been reported for these top ontologies [50].", "startOffset": 110, "endOffset": 114}, {"referenceID": 48, "context": "Lack of integration with other knowledge representations used in image annotation frameworks based on semantic networks such as WordNet [51].", "startOffset": 136, "endOffset": 140}, {"referenceID": 49, "context": "The presented ontology captures high-level information in semantics and emotion but it has been shown that low-level features like color, hue, brightness and texture influence perception and can be positively correlated to arousal and discrete emotion categories [52].", "startOffset": 263, "endOffset": 267}, {"referenceID": 50, "context": "Also, the STIMONT can be applied for representation of multimedia and Virtual Reality (VR) stimuli in psychotherapy as in exposure therapy (ET) or stress inoculation training (SIT) [53].", "startOffset": 181, "endOffset": 185}, {"referenceID": 51, "context": "Films, video-clips and related dynamic presentation technologies are often used for induction of emotion in the laboratory because of their relatively high attention capture and intensity [54].", "startOffset": 188, "endOffset": 192}], "year": 2013, "abstractText": "Affective multimedia documents such as images, sounds or videos elicit emotional responses in exposed human subjects. These stimuli are stored in affective multimedia databases and successfully used for a wide variety of research in psychology and neuroscience in areas related to attention and emotion processing. Although important all affective multimedia databases have numerous deficiencies which impair their applicability. These problems, which are brought forward in the paper, result in low recall and precision of multimedia stimuli retrieval which makes creating emotion elicitation procedures difficult and labor-intensive. To address these issues a new core ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and extends W3C EmotionML format with an expressive and formal representation of affective concepts, high-level semantics, stimuli document metadata and the elicited physiology. The advantages of ontology in description of affective multimedia stimuli are demonstrated in a document retrieval experiment and compared against contemporary keyword-based querying methods. Also, a software tool Intelligent Stimulus Generator for retrieval of affective multimedia and construction of stimuli sequences is presented.", "creator": "PScript5.dll Version 5.2.2"}}}