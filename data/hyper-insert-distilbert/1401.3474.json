{"id": "1401.3474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Optimal Value of Information in Graphical Models", "abstract": "many real - in world decision / making testing tasks often require encouraging us to swiftly choose choosing among several historically expensive observations. in designing a computer sensor network, for scientific example, it is becoming important to select the subset of sensors observed that is expected thus to best provide the strongest reduction likelihood in uncertainty. in medical decision medicine making tasks, one needs to select deciding which quantitative tests to administer responses before deciding depends on the equally most likely effective appropriate treatment. it has only been customary general legal practice to use experimental heuristic - guided selection procedures for selecting observations. in compiling this paper, first we explicitly present even the principle first efficient optimal algorithms equipped for selecting observations sufficient for a class m of probabilistic graphical models. for example, our algorithms allow each to optimally label minimum hidden variables in hidden parameter markov models ( hmms ). we provide excellent results for observing both : selecting the optimal subset of observations, and for obtaining perhaps an underlying optimal conditional observation plan.", "histories": [["v1", "Wed, 15 Jan 2014 05:30:52 GMT  (382kb)", "http://arxiv.org/abs/1401.3474v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andreas krause", "carlos guestrin"], "accepted": false, "id": "1401.3474"}, "pdf": {"name": "1401.3474.pdf", "metadata": {"source": "CRF", "title": "Optimal Value of Information in Graphical Models", "authors": ["Andreas Krause", "Carlos Guestrin"], "emails": ["KRAUSEA@CALTECH.EDU", "GUESTRIN@CS.CMU.EDU"], "sections": [{"heading": null, "text": "vations. In a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. In medical decision making tasks, one needs to select which tests to administer before deciding on the most effective treatment. It has been general practice to use heuristic-guided procedures for selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. For example, our algorithms allow to optimally label hidden variables in Hidden Markov Models (HMMs). We provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan.\nFurthermore we prove a surprising result: In most graphical models tasks, if one designs an efficient algorithm for chain graphs, such as HMMs, this procedure can be generalized to polytree graphical models. We prove that the optimizing value of information is NPPP-hard even for polytrees. It also follows from our results that just computing decision theoretic value of information objective functions, which are commonly used in practice, is a #P-complete problem even on Naive Bayes models (a simple special case of polytrees).\nIn addition, we consider several extensions, such as using our algorithms for scheduling observation selection for multiple sensors. We demonstrate the effectiveness of our approach on several real-world datasets, including a prototype sensor network deployment for energy conservation in buildings."}, {"heading": "1. Introduction", "text": "In probabilistic reasoning, where one can choose among several possible but expensive observations, it is often a central issue to decide which variables to observe in order to most effectively increase the expected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997; Lindley, 1956). In a medical expert system, for example, multiple tests are available, and each test has a different cost (Turney, 1995; Heckerman, Horvitz, & Middleton, 1993). In such systems, it is thus important to decide which tests to perform in order to become most certain about the patient\u2019s condition, at a minimum cost. Occasionally, the cost of testing can even exceed the value of information for any possible outcome, suggesting to discontinue any further testing.\nThe following running example motivates our research and is empirically evaluated in Section 6. Consider a temperature monitoring task, where wireless temperature sensors are distributed across a\nc\u00a92009 AI Access Foundation. All rights reserved.\nbuilding. The task is to become most certain about the temperature distribution, whilst minimizing energy expenditure, a critically constrained resource (Deshpande, Guestrin, Madden, Hellerstein, & Hong, 2004). Such fine-grained building monitoring is required to obtain significant energy savings (Singhvi, Krause, Guestrin, Garrett, & Matthews, 2005).\nMany researchers have suggested the use of myopic (greedy) approaches to select observations (Scheffer, Decomain, & Wrobel, 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Bayer-Zubek, 2004; Kapoor, Horvitz, & Basu, 2007). Unfortunately, in general, this heuristic does not provide any performance guarantees. In this paper, we present efficient algorithms, which guarantee optimal nonmyopic value of information in chain graphical models. For example, our algorithms can be used for optimal active labeling of hidden states in Hidden Markov Models (HMMs, Baum & Petrie, 1966). We address two settings: subset selection, where the optimal subset of observations is obtained in an open-loop fashion, and conditional plans, a sequential, closed-loop plan where the observation strategy depends on the actual value of the observed variables (c.f., Figure 1). To our knowledge, these are the first optimal and efficient algorithms for observation selection and diagnostic planning based on value of information for this class of graphical models. For both settings, we address the filtering and the smoothing versions: Filtering is important in online decision making, where our decisions can only utilize observations made in the past. Smoothing arises for example in structured classification tasks, where there is no temporal dimension in the data, and hence all observations can be taken into account. We call our approach VOIDP as the algorithms use Dynamic Programming to optimize Value of Information. We evaluate our VOIDP algorithms empirically on three real-world datasets, and also show that they are well-suited for interactive classification of sequential data.\nMost inference problems in graphical models, such as computing marginal distributions and finding the most probable explanation, that can be solved efficiently for chain-structured graphs, can also be solved efficiently for polytrees. We prove that the problem of selecting the best k observations for maximizing decision theoretic value of information is NPPP-complete even for discrete polytree graphical models, giving a complexity theoretic classification of a core artificial intelligence problem. NPPP-complete problems are believed to be significantly harder than NPcomplete or even #P-complete problems commonly arising in the context of graphical models. We furthermore prove that just evaluating decision-theoretic value of information objective functions is #P-complete even in the case of Naive Bayes models, a simple special case of polytree graphical models that is frequently used in practice (c.f., Domingos & Pazzani, 1997).\nUnfortunately, these hardness results show that, while the problem of scheduling a single sensor can be optimally solved using our algorithms, the problem of scheduling multiple, correlated sensors is wildly intractable. Nevertheless, we show how our VOIDP algorithms for single sensor scheduling can be used to approximately optimize a multi-sensor schedule. We demonstrate the effectiveness of this approach on a real sensor network testbed for building management.\nIn summary, we provide the following contributions:\n\u2022 We present the first optimal algorithms for nonmyopically computing and optimizing value of information on chain graphical models.\n\u2022 We show that optimizing decision theoretic value of information is NPPP-hard for discrete polytree graphical models. Just computing decision theoretic value of information is #Phard even for Naive Bayes models.\n\u2022 We present several extensions of our algorithms, e.g., to tree graphical models with few leaves, and to multiple correlated chains (for multi-sensor scheduling).\n\u2022 We extensively evaluate our algorithms on several real-world problems, including sensor scheduling on a real sensor testbed and active labeling in bioinformatics and Natural Language Processing."}, {"heading": "2. Problem Statement", "text": "We will assume that the state of the world is described by a collection of random variables XV = (X1, . . . ,Xn), where V is an index set. For example, V could denote a set of locations, andXi models the temperature reading of a sensor placed at location i \u2208 V . For a subset A = {i1, . . . , ik} \u2286 V , we use the notation XA to refer to the random vector XA = (Xi1 , . . . ,Xik). While some of our algorithms extend to continuous distributions, we generally assume that the variables XV are discrete. We take a Bayesian approach, and assume a prior probability distribution P (XV) over the outcomes of the variables. Suppose we select a subset of the variables, XA (for A \u2286 V), and observe XA = xA. For example, A is the set of locations where we place sensors, or a set of medical tests we decide to perform. After observing the realization of these variables XA = xA, we can compute the posterior distribution over all variables P (XV | XA = xA). Based on this posterior probability we obtain a reward R(P (XV | XA = xA)). For example, this reward function could depend on the uncertainty (e.g., measured by the entropy) of the distribution P (XV | XA = xA). We will describe several examples in more detail below.\nIn general, when selecting observation, we will not know ahead of time what observations we will make. Instead, we only have a distribution over the possible observations. Hence, we will be interested in the expected reward, where we take the expectation over the possible observations.\nWhen optimizing the selection of variables, we can consider different settings: In subset selection, our goal is to pick a subset A\u2217 \u2286 V of the variables, maximizing\nA\u2217 = argmax A \u2211 xA P (XA = xA)R(P (XV | XA = xA)), (1)\nwhere we impose some constraints on the set A we are allowed to pick (e.g., on the number of variables that can be selected, etc.). In the subset selection setting, we commit to the selection of the variables before we get to see their realization.\nInstead, we can also sequentially select one variable after the other, letting our choice depend on the observations made in the past. In this setting, we would like to find a conditional plan \u03c0\u2217\nthat maximizes\n\u03c0\u2217 = argmax \u03c0 \u2211 xV P (xV)R(P (XV | X\u03c0(xV ) = x\u03c0(xV ))). (2)\nHereby, \u03c0 is a conditional plan that can select a different set of variables for each possible state of the world xV . We use the notation \u03c0(xV) \u2286 V to refer to the subset of variables selected by the conditional plan \u03c0 in state XV = xV . Figure 1 presents an example of a conditional plan for the temperature monitoring example. We will define the notion of conditional planning more formally in Section 4.2.\nThis general setup of selecting observations goes back in the decision analysis literature to the notion of value of information by Howard (1966) and in the statistical literature to the notion of Bayesian Experimental Design by Lindley (1956). In this paper, we refer to the Problems (1) and (2) as the problems of optimizing value of information.\nIn this paper, we show how the complexity of solving these value of information problems depend on the properties of the probability distribution P . We give the first algorithms for optimally solving value of information for an interesting and challenging class of distributions including Hidden Markov Models. We also present hardness results showing that optimizing value of information is wildly intractable (NPPP-complete) even for probability distributions for which efficient inference is possible (even for Naive Bayes models and discrete polytrees)."}, {"heading": "2.1 Optimization Criteria", "text": "In this paper, we will consider a class of local reward1 functions Ri, which are defined on the marginal probability distributions of the variables Xi. This class has the computational advantage that local rewards can be evaluated using probabilistic inference techniques. The total reward will then be the sum of all local rewards.\nLet A be a subset of V . Then P (Xj | XA = xA) denotes the marginal distribution of variable Xj conditioned on observations XA = xA. For example, in our temperature monitoring application, Xj models the temperature at location j \u2208 V . The conditional marginal distribution P (Xj = xj | XA = xA) then models the conditional distribution of the temperature at location j after observing the temperature at locations A \u2286 V .\nFor classification purposes, it can be more appropriate to consider the max-marginals\nPmax(Xj = xj | XA = xA) = max xV P (XV = xV ,Xj = xj | XA = xA),\nthat is, for Xj set to value xj , the probability of the most probable assignment XV = xV to all other random variables (including Xj for simplicity of notation) conditioned on the observations XA = xA.\nThe local reward Rj is a functional on the probability distribution P or Pmax over Xj . That is, Rj takes an entire distribution over the variable Xj and maps it to a reward value. Typically, the reward functions will be chosen such that \u201ccertain\u201d or \u201cpeaked\u201d distributions obtain higher reward.\nTo simplify notation, we write\nRj(Xj | xA) , Rj(P (Xj | XA = xA))\n1. Local reward functions are also widely used in additively independent utility models, (c.f., Keeney & Raiffa, 1976).\nto denote the reward for variable Xj upon observing XA = xA, and\nRj(Xj | XA) , \u2211 xA P (XA = xA)Rj(Xj | xA)\nto refer to expected local rewards, where the expectation is taken over all assignments xA to the observations A. Important local reward functions include:\nResidual entropy. If we set\nRj(Xj | xA) = \u2212H(Xj | xA) = \u2211 xj P (xj | xA) log2 P (xj | xA),\nthe objective in the optimization problem becomes to minimize the sum of residual entropies. Optimizing this reward function attempts to reduce the uncertainty in predicting the marginals Xi. We choose this reward function in our running example to measure the uncertainty about the temperature distribution.\nJoint entropy. Instead of minimizing the sum of residual entropies \u2211\niH(Xi), we can also attempt to minimize the joint entropy of the entire distribution,\nH(XV) = \u2212 \u2211 xV P (xV) log2 P (xV).\nNote, that the joint entropy depends on the full probability distribution P (XV), rather than on the marginals P (Xi), and hence it is not local. Nevertheless, we can exploit the chain rule for the joint entropy H(XB) of a set of random variables B = {1, . . . ,m} (c.f., Cover & Thomas, 1991),\nH(XB) = H(X1) +H(X2 | X1) +H(X3 | X1,X2) + \u00b7 \u00b7 \u00b7+H(Xm | X1, . . . ,Xm\u22121).\nHence, if we choose the local reward functions Rj(Xj | XA) = \u2212H(Xj | X1, . . . ,Xj\u22121,XA), we can optimize a non-local reward function (the joint entropy) using only local reward functions.\nDecision-theoretic value of information. The concept of local reward functions also includes the concept of decision theoretic value of information. The notion of value of information is widely used (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and is formalized, e.g., in the context of influence diagrams (Howard & Matheson, 1984) and Partially Observable Markov Decision Processes (POMDPs, Smallwood & Sondik, 1973). For each variable Xj , let Aj be a finite set of actions. Also, let Uj : Aj \u00d7 domXj \u2192 R be a utility function mapping an action a \u2208 Aj and an outcome x \u2208 domXj to a real number. The maximum expected utility principle states that actions should be selected as to maximize the expected utility,\nEUj(a | XA = xA) = \u2211 xj P (xj | xA)Uj(a, xj).\nThe more certain we are about Xj , the more economically we can choose our action. This idea is captured by the notion of value of information, where we choose our local reward function\nRj(Xj | xA) = max a EUj(a | xA).\nMargin for structured prediction. We can also consider the margin of confidence:\nRj(Xj | xA) = Pmax(x\u2217j | xA)\u2212 Pmax(x\u0304j | xA),\nwhere x\u2217j = argmax\nxj Pmax(xj | xA) and x\u0304j = argmax xj 6=x\u2217j Pmax(xj | xA),\nwhich describes the margin between the most likely outcome and the closest runner up. This reward function is very useful for structured classification purposes, as shown in Section 6.\nWeighted mean-squared error. If the variables are continuous, we might want to minimize the mean squared error in our prediction. We can do this by choosing\nRj(Xj | xA) = \u2212wj Var(Xj | xA),\nwhere\nVar(Xj | xA) = \u222b P (xj | xA) [ xj \u2212 \u222b x\u2032jP (x \u2032 j | xA)dx\u2032j ]2 dxj\nis the conditional variance of Xj given XA = xA, and wj is a weight indicating the importance of variable Xj .\nMonitoring for critical regions (Hotspot sampling). Suppose we want to use sensors for detecting fire. More generally, we want to detect, for each j, whether Xj \u2208 Cj , where Cj \u2286 domXj is a \u201ccritical region\u201d for variable Xj . Then the local reward function\nRj(Xj | xA) = P (Xj \u2208 Cj | xA)\nfavors observations A that maximize the probability of detecting critical regions.\nFunction optimization (Correlated bandits). Consider a setting where we have a collection of random variables XV taking numerical values in some interval [\u2212m,m], and, after selecting some of the variables, we get the reward \u2211 i xi. This setting arises if we want to optimize an unknown (random) function, where evaluating the function is expensive. In this setting, we are encouraged to only evaluate the function where it is likely to obtain high values. We can maximize our expected total reward if we choose the local reward function\nRj(Xj | xA) = \u222b xjP (xj | xA)dxj ,\ni.e., the expectation of variable Xj given observations xA. This setting of optimizing a random function can also be considered a version of the classical k-armed bandit problem with correlated arms. More details about the relationship with bandit problems are given in Section 8.\nThese examples demonstrate the generality of our notion of local reward. Note that most examples apply to continuous distributions just as well as for discrete distributions."}, {"heading": "2.2 Cost of Selecting Observations", "text": "We also want to capture the constraint that observations are expensive. This can mean that each observation Xj has an associated positive penalty Cj that effectively decreases the reward. In our example, we might be interested in trading off accuracy with sensing energy expenditure. Alternatively, it is also possible to define a budgetB for selecting observations, where each one is associated with an integer cost \u03b2j . Here, we want to select observations whose sum cost is within the budget, but these costs do not decrease the reward. In our running example, the sensors could be powered by solar power, and regain a certain amount of energy per day, which allows a certain amount of sensing. Our formulation of the optimization problems allows both for penalties and budgets. To simplify notation we also write C(A) = \u2211 j\u2208ACj and \u03b2(A) = \u2211 j\u2208A \u03b2j to extend C and \u03b2 to sets.\nInstead of fixed penalties and costs per observation, both can also depend on the state of the world. For example, in the medical domain, applying a particular diagnostic test can bear different risks for the health of the patient, depending on the patient\u2019s illness. The algorithms we will develop below can be adapted to accommodate such dependencies in a straight-forward manner. We will present details only for the conditional planning algorithm in Section 4.2."}, {"heading": "3. Decomposing Rewards", "text": "In this section, we will present the key observation that allows us to develop efficient algorithms for nonmyopically optimizing value of information in the class of chain graphical models. The algorithms will be presented in Section 4.\nThe set of random variables XV = {X1, . . . ,Xn} forms a chain graphical model (a chain), if Xi is conditionally independent of XV\\{i\u22121,i,i+1} given Xi\u22121 and Xi+1. Without loss of generality we can assume that the joint distribution is specified by the prior P (X1) of variable X1 and the conditional probability distributions P (Xi+1 | Xi). The time series model for the temperature measured by one sensor in our example can be formulated as a chain graphical model. Note that the transition probabilities P (Xi+1 | Xi) are allowed to depend on the index i (i.e., the chain models are allowed to be nonstationary). Chain graphical models have been extensively used in machine learning and signal processing.\nConsider for example a Hidden Markov Model unrolled for n time steps, i.e., V can be partitioned into the hidden variables {X1, . . . ,Xn} and the emission variables {Y1, . . . ,Yn}. In HMMs, the Yi are always observed and the variables Xi form a chain. In many applications, some of which are discussed in Section 6, we can observe some of the hidden variables Xi as well, e.g., by asking an expert, in addition to observing the emission variables. In these cases, the problem of selecting expert labels also belongs to the class of chain graphical models addressed by this paper, since the variables Xi form a chain conditional on the observed values of the emission variables Yi. This idea can be generalized to the class of Dynamic Bayesian Networks where the separators between time slices have size one, and only these separators can be selected for observation. This formulation also includes certain conditional random fields (Lafferty, McCallum, & Pereira, 2001) which form chains, conditional on the emission variables (the features).\nChain graphical models originating from time series have additional, specific properties: In a system for online decision making, only observations from the past and present time steps can be taken into account, not observations which will be made in the future. This is generally referred to as the filtering problem. In this setting, the notation P (Xi | XA) will refer to the distribution of Xi conditional on observations in XA prior to and including time i. For structured classification\nproblems as discussed in Section 6, in general observations made anywhere in the chain must be taken into account. This situation is usually referred to as the smoothing problem. We will provide algorithms both for filtering and smoothing.\nWe will now describe the key insight, which allows for efficient optimization in chains. Consider a set of observations A \u2286 V . If the j variable is observed, i.e., j \u2208 A, then the local reward is simply R(Xj | XA) = R(Xj | Xj). Now consider j /\u2208 A, and let Aj be the subset of A containing the closest ancestor (and for the smoothing problem also the closest descendant) of Xj in XA. The conditional independence property of the graphical model implies that, given XAj , Xj is independent of the rest of the observed variables, i.e., P (Xj | XA) = P (Xj | XAj ). Thus, it follows that R(Xj | XA) = R(Xj | XAj ).\nThese observations imply that the expected reward of some set of observations decomposes along the chain. For simplicity of notation, we add two independent dummy variables X0 and Xn+1, where R0 = C0 = \u03b20 = Rn+1 = Cn+1 = \u03b2n+1 = 0. Let A = {i0, . . . , im+1} where il < il+1, i0 = 0 and im+1 = n + 1. Using this notation, the total reward R(A) = \u2211 j Rj(Xj | XA) for the smoothing case is given by:\nm\u2211 v=0 Riv(Xiv | Xiv)\u2212 Civ + iv+1\u22121\u2211 j=iv+1 Rj(Xj | Xiv ,Xiv+1)  . In filtering settings, we simply replace Rj(Xj | Xiv ,Xiv+1) by Rj(Xj | Xiv). Figure 2 illustrates this decomposition."}, {"heading": "4. Efficient Algorithms for Optimizing Value of Information", "text": "In this section, we present algorithms for efficiently and nonmyopically optimizing value of information in chain graphical models."}, {"heading": "4.1 Efficient Algorithms for Optimal Subset Selection in Chain Models", "text": "In the subset selection problem, we want to find a most informative subset of the variables to observe in advance, i.e., before any observations are made. In our running example, we would, before deploying the sensors, identify k time points that are expected to provide the most informative sensor readings according to our model.\nFirst, define the objective function L on subsets of V by\nL(A) = n\u2211 j=1 Rj(Xj | XA)\u2212 C(A). (3)\nThe subset selection problem is to find the optimal subset\nA\u2217 = argmax A\u2286V,\u03b2(A)\u2264B L(A)\nmaximizing the sum of expected local rewards minus the penalties, subject to the constraint that the total cost must not exceed the budget B.\nWe solve this optimization problem using a dynamic programming algorithm, where the chain is broken into sub-chains using the insight from Section 3. Consider a sub-chain from variable Xa to Xb. We define Lsma:b (k) to represent the expected total reward for the sub-chain Xa, . . . ,Xb, in the smoothing setting where Xa and Xb are observed, and with a budget level of k. Lflta:b(k) represents the expected reward in the filtering setting where only Xa is observed. More formally:\nLflta:b(k) = maxA\u2282{a+1...b\u22121} \u03b2(A)\u2264k b\u22121\u2211 j=a+1 Rj(Xj | XA,Xa)\u2212 C(A),\nfor the filtering version, and\nLsma:b (k) = maxA\u2282{a+1...b\u22121} \u03b2(A)\u2264k\nb\u22121\u2211 j=a+1 Rj(Xj | XA,Xa,Xb)\u2212 C(A),\nfor the smoothing version. Note that in both cases, L0:n+1(B) = maxA:\u03b2(A)\u2264B L(A), as in Equation (3), i.e., by computing the values for La:b(k), we compute the maximum expected total reward for the entire chain. We can compute Lsma:b (k) and L flt a:b(k) using dynamic programming. The base case is simply:\nLflta:b(0) = b\u22121\u2211\nj=a+1\nRj(Xj | Xa),\nfor filtering, and\nLsma:b (0) = b\u22121\u2211\nj=a+1\nRj(Xj | Xa,Xb),\nfor smoothing. The recursion for La:b(k) has two cases: we can choose not to spend any more of the budget, reaching the base case, or we can break the chain into two sub-chains, selecting the optimal observation Xj , where a < j < b. In both filtering and smoothing we have\nLa:b(k) = max { La:b(0), max\nj:a<j<b,\u03b2j\u2264k {Rj(Xj | Xj)\u2212 Cj + La:j(0) + Lj:b(k \u2212 \u03b2j)}\n} .\nInput: Budget B, rewards Rj , costs \u03b2j and penalties Cj Output: Optimal selection A of observation times begin\nfor 0 \u2264 a < b \u2264 n+ 1 do compute La:b(0); for k = 1 to B do\nfor 0 \u2264 a < b \u2264 n+ 1 do sel(\u22121) := La:b(0); for j = a+ 1 to b\u2212 1 do sel(j) := Rj(Xj | Xj)\u2212 Cj + La:j(0) + Lj:b(k \u2212 \u03b2j); La:b(k) = maxj\u2208{a+1,...,b\u22121,\u22121} sel(j); \u039ba:b(k) = argmaxj\u2208{a+1,...,b\u22121,\u22121} sel(j);\nend end a := 0; b := n+ 1; k := B; A := \u2205; repeat\nj := \u039ba:b(k); if j \u2265 0 then A := A \u222a {j}; k := k \u2212 \u03b2j ;\nuntil j = \u22121 ; end\nAlgorithm 1: VOIDP algorithm for optimal subset selection (for both filtering and smoothing).\nAt first, it may seem that this recursion should consider the optimal split of the budget between the two sub-chains. However, since the subset problem is open-loop and the order of the observations is irrelevant, we only need to consider split points where the first sub-chain receives zero budget.\nA pseudo code implementation for this dynamic programming approach, which we call VOIDP for subset selection is given in Algorithm 1. The algorithm fills the dynamic programming tables in two loops, the inner loop ranging over all pairs (a, b), a < b, and the outer loop increasing k. Within the inner loop, when computing the best reward for the sub-chain from a to b, it fills out a table sel, where sel(j) is the reward that could be obtained by making an observation at j, and sel(\u22121) is the reward if no observation is made.\nIn addition to computing the optimal rewards La:b(k) that could be achieved for sub-chain a : b and budget k, the algorithm also stores the choices \u039ba:b(k) that realize this maximum score. Here, \u039ba:b(k) is the index of the next variable that should be selected for sub-chain a : b with budget k, or \u22121 if no variable should be selected. In order to recover the optimal subset for budget k, Algorithm 1 uses the quantities \u039ba:b to recover the optimal subset by tracing the maximal values occurring in the dynamic programming equations. Using an induction proof, we obtain:\nTheorem 1 (Subset Selection). The dynamic programming algorithm described above computes the optimal subset with budget B in (16n 3 +O(n2))B evaluations of expected local rewards.\nNote that if we do not consider different costs \u03b2 for each variable, we would simply choose \u03b2j = 1 for all variables and compute La:b(N). Further note that if the variables Xi are continuous, our algorithm is still applicable when the integrations and inferences necessary for computing the expected rewards can be performed efficiently. This is the case, for example, in a Gaussian linear model (i.e., the variables Xi are normally distributed) and the local reward functions are the residual entropies or the residual variances for each variable."}, {"heading": "4.2 Efficient Algorithms for Optimal Conditional Planning in Chain Models", "text": "In the conditional plan problem, we want to compute an optimal sequential querying policy \u03c0: We observe a variable, pay the penalty, and depending on all values observed in the past, select the next query, proceeding as long as our budget suffices. The objective is to find the plan with the highest expected reward, where, for each possible sequence of observations, the budget B is not exceeded. For filtering, we can only select observations in the future, whereas in the smoothing case, the next observation can be anywhere in the chain. In our running example, the filtering algorithm would be most appropriate: The sensors would sequentially follow the conditional plan, deciding on the most informative times to sense based on the previous observations. Figure 1 shows an example of such a conditional plan."}, {"heading": "4.2.1 FROM SUBSET SELECTION TO CONDITIONAL PLANNING", "text": "Note that in contrast to the subset selection setting that we considered in Section 4.1, in conditional planning, the set of variables depends on the state of the world XV = xV . Hence, for each such state, the conditional plan \u03c0 could select a different set of variables, \u03c0(xV) \u2286 V . As an example, consider Figure 1, where the set of possible observations is V = {morn, noon, eve}, and XV = {Tmorn, Tnoon, Teve}. If the world is in state xV = (high, low, high), then the conditional plan \u03c0 presented in Figure 1 would select \u03c0(xV) = {morn, eve}, whereas, if xV = (low, low, high), it would select \u03c0(xV) = {morn, noon}. Since the conditional plan is a function of the (random) state of the world, it is a set-valued random variable. In order to optimize Problem (2), we define the objective function2\nJ(\u03c0) = \u2211 xV P (xV) n\u2211 j=1 [ Rj(Xj | x\u03c0(xV ))\u2212 C(\u03c0(xV)) ] ,\ni.e., the expected sum of local rewards given the observations made by plan \u03c0(xV) in state XV = xV minus the penalties of the selected variables, where the expectation is taken with respect to the distribution P (XV). In addition to defining the value of a policy J(\u03c0), we also define the cost \u03b2(\u03c0)\n\u03b2(\u03c0) = max xV \u03b2(\u03c0(xV)),\nas maximum cost \u03b2(A) (as defined in Section 2.2) of any set A = \u03c0(xV) that could be selected by the policy \u03c0, in any state of the world XV = xV .\nBased on this notation, our goal is to find a policy \u03c0\u2217 such that\n\u03c0\u2217 = argmax \u03c0\u2208\u03a0 J(\u03c0) such that \u03b2(\u03c0) \u2264 B,\ni.e., a policy that has maximum value, and is guaranteed to never have cost exceeding our budget B. Hereby \u03a0 is the class of sequential policies (i.e., those, where the observations are chosen sequentially, only based on observations that have been previously made).\nIt will be useful to introduce the following notation:\nJ(xA; k) = max \u03c0\u2208\u03a0 J(\u03c0 | XA = xA) such that \u03b2(\u03c0) \u2264 k, (4)\n2. Recall that, in the filtering setting, R(Xj | x\u03c0(xV )) , R(Xj | xA\u2032), where A \u2032 = {t \u2208 \u03c0(xV) s.t. t \u2264 j}, i.e., only\nobservations from the past are taken into account.\nwhere\nJ(\u03c0 | XA = xA) = \u2211 xV P (xV | XA = xA) n\u2211 j=1 [ Rj(Xj | x\u03c0(xV ))\u2212 C(\u03c0(xV)) ] .\nHence, J(xA; k) is the best possible reward that can be achieved by any sequential policy with cost at most k, after observing XA = xA. Using this notation, our goal is to find the optimal plan with reward J(\u2205;B).\nThe value function J satisfies the following recursion. The base case considers the exhausted budget:\nJ(xA; 0) = \u2211 j\u2208V Rj(Xj | xA)\u2212 C(A).\nFor the recursion, it holds that\nJ(xA; k) = max J(xA; 0),maxj /\u2208A \u2211\nxj\nP (xj | xA)J(xA, xj ; k \u2212 \u03b2j)   , (5)\ni.e., the best one can do in state XA = xA with budget k is to either stop selecting variables, or chose the best next variable and act optimally thereupon.\nNote that we can easily allow the cost \u03b2j depend on the state xj of variable Xj . In this case, we would simply replace \u03b2j by \u03b2j(xj), and define J(XA, r) = \u2212\u221e whenever r < 0. Equivalently, we can let the penalty C(A) depend on the state by replacing C(A) by C(xA).\nRelationship to finite-horizon Markov Decision Processes (MDPs). Note that the function J(xA; k) defined in (4) is analogous to the concept of a value function in Markov Decision Processes (c.f., Bellman, 1957): In finite-horizon MDPs, the value function V (s; k) models the maximum expected reward obtainable when starting in state s and performing k actions. For this value function it holds that\nV (s; k) = R(s, k) + max a \u2211 s\u2032 P (s\u2032 | s, a)V (s\u2032; k \u2212 1),\nwhere P (s\u2032 | s, a) is the probability of transiting to state s\u2032 when performing action a in state s, and R(s, k) is the immediate reward obtained in state s if k steps are still left. This recursion, which is similar to Eq. (5), is exploited by the value iteration algorithm for solving MDPs. The conditional planning problem with unit observation cost (i.e., \u03b2(A) = |A|) could be modeled as a finite-horizon MDP, where states correspond to observed evidence XA = xA, actions correspond to observing variables (or not making any observation) and transition probabilities are given by the probability of observing a particular instantiation of the selected variable. The immediate reward is R(s, k) = 0 for k > 0, and R(s, 0) is the expected reward (in the value of information problem) of observing assignment s (i.e., R(P (XV | s))\u2212C(s)). If all observations have unit cost, then for this MDP, it holds that V (xA; k) = J(xA; k). Unfortunately, in the conditional planning problem, since the state of the MDP is uniquely determined by the observed evidence XA = xA, the state space is exponentially large. Hence, existing algorithms for solving MDPs exactly (such as value iteration) cannot be applied to solve large value of information problems. In Section 4.2.2, we develop an efficient dynamic programming algorithm for conditional planning in chain graphical models that avoids this exponential increase in complexity."}, {"heading": "4.2.2 DYNAMIC PROGRAMMING FOR OPTIMAL CONDITIONAL PLANNING IN CHAINS", "text": "We propose a dynamic programming algorithm for obtaining the optimal conditional plan that is similar to the subset algorithm presented in Section 4.1. Again, we utilize the decomposition of rewards described in Section 3. The difference here is that the observation selection and budget allocation now depend on the actual values of the observations. In order to compute the value function J(xA; k) for the entire chain, we will compute the value functions Ja:b(xA; k) for subchains Xa, . . . ,Xb.\nThe base case of our dynamic programming approach deals with the zero budget setting:\nJflta:b (xa; 0) = b\u22121\u2211\nj=a+1\nRj(Xj | Xa = xa),\nfor filtering, and\nJsma:b (xa, xb; 0) = b\u22121\u2211\nj=a+1\nRj(Xj | Xa = xa,Xb = xb),\nfor smoothing. The recursion defines Ja:b(xa; k) (or Ja:b(xa, xb; k) for smoothing), the expected reward for the problem restricted to the sub-chain Xa, . . . ,Xb conditioned on the values of Xa = xa (and Xb = xb for smoothing), and with budget limited by k. To compute this quantity, we again iterate through possible split points j, such that a < j < b. Here we observe a notable difference between the filtering and the smoothing case. For smoothing, we now must consider all possible splits of the budget between the two resulting sub-chains, since an observation at time j might require us to make an additional, earlier observation:\nJsma:b (xa,xb; k) = max { Jsma:b (xa, xb; 0), max\na<j<b {\u2211 xj P (Xj = xj | Xa = xa,Xb = xb) {\nRj(Xj | xj)\u2212 Cj(xj) + max 0\u2264l\u2264k\u2212\u03b2j(xj)\n[ Jsma:j (xa, xj ; l) + J sm j:b (xj , xb; k \u2212 l \u2212 \u03b2j(xj)) ]}}} .\nLooking back in time is not possible in the filtering case, hence the recursion simplifies to Jflta:b (xa; k) = max { Jflta:b (xa; 0), max\na<j<b:\u03b2j(xj)\u2264k {\u2211 xj P (Xj = xj | Xa = xa) {\nRj(Xj | xj)\u2212 Cj(xj) + Jflta:j (xa; 0) + J flt j:b (xj ; k \u2212 \u03b2j(xj))\n}}} .\nFor both Jflt and Jsm, the optimal reward is obtained by J0:n+1(\u2205;B) = J(\u2205;B) = J(\u03c0\u2217). Algorithm 2 presents a pseudo code implementation for the smoothing version \u2013 the filtering case is a straight-forward modification. We call Algorithm 2 the VOIDP algorithm for conditional planning. The algorithm will fill the dynamic programming tables using three loops, the inner loop ranging over all assignments xa, xb, the middle loop ranging over all pairs (a, b) where a < b, and the outer loop covers increasing values of k \u2264 B. Within the innermost loop, the algorithm again computes a table sel such that sel(j) is the optimal reward achievable by selecting variable j next.\nThis value is now an expectation over any possible observation that variable Xj can make. Note that for every possible instantiation Xj = xj a different allocation of the remaining budget k \u2212 \u03b2j(xj) to the left and right sub-chain (a : j and j : b respectively) can be chosen. The quantity \u03c3(j, xj) tracks this optimal budget allocation.\nInput: Budget B, rewards Rj , costs \u03b2j and penalties Cj Output: Optimal conditional plan (\u03c0a:b, \u03c3a:b) begin\nfor 0 \u2264a < b\u2264 n+ 1, xa \u2208 domXa, xb \u2208 domXb do compute Jsma:b (xa, xb; 0); for k = 1 to B do\nfor 0\u2264a<b\u2264n+1, xa\u2208domXa, xb\u2208domXb do sel(\u22121) := Jsma:b (0); for a < j < b do\nsel(j) := 0; for xj \u2208 domXj do\nfor 0 \u2264 l \u2264 k \u2212 \u03b2j(xj) do bd(l) := Jsma:j (xa, xj ; l) + J sm j:b (xj , xb; k \u2212 l \u2212 \u03b2j(xj)); end sel(j) := sel(j) + P (xj | xa, xb) \u00b7 [Rj(Xj | xj)\u2212 Cj(xj) + maxl bd(j)]; \u03c3(j, xj) = argmaxl bd(j);\nend end Jsma:b (k) = maxj\u2208{a+1,...,b\u22121,\u22121} sel(j); \u03c0a:b(xa, xb; k) = argmaxj\u2208{a+1,...,b\u22121,\u22121} sel(j); for xj \u2208 domX\u03c0a:b(k) do \u03c3a:b(xa, xb, xj ; k) = \u03c3(\u03c0a:b(k), xj);\nend end\nend Algorithm 2: VOIDP algorithm for computating an optimal conditional plan (for the smoothing setting).\nInput: Budget k, observations Xa = xa, Xb = xb, \u03c3, \u03c0 begin\nj := \u03c0a:b(xa, xb; k); if j \u2265 0 then\nObserve Xj = xj ; l := \u03c3a:b(xa, xb, xj ; k); Recurse with k := l, Xa = xa and Xj = xj instead of Xb = xb; Recurse with k := k \u2212 l \u2212 \u03b2j , Xj = xj instead of Xa = xa, and Xb = xb;\nend end\nAlgorithm 3: Observation selection using conditional planning.\nThe plan itself is compactly encoded in the quantities \u03c0a:b and \u03c3a:b. Hereby, \u03c0a:b(xa, xb; k) determines the next variable to query after observing Xa = xa and Xb = xb, and with remaining budget k. \u03c3a:b(xa, xb, xj ; k) determines the allocation of the budget after the new observation Xj = xj has been made. Considering the exponential number of possible sequences of observations,\nit is remarkable that the optimal plan can be represented using only polynomial space. Algorithm 3 indicates how the computed plan can be executed. The procedure is recursive, requiring the parameters a := 0, xa := 1, b := n + 1, xb := 1 and k := B for the initial call. In our temperature monitoring example, we could first collect some temperature timeseries as training data, and then learn the chain model from this data. Offline, we would then compute the conditional plan (for the filtering setting), and encode it in the quantities \u03c0a:b and \u03c3a:b. We would then deploy the computed plan on the actual sensor node, together with an implementation of Algorithm 3. While computation of the optimal plan (Algorithm 2) is fairly computationally expensive, the execution of the plan (Algorithm 3) is very efficient (selecting the next timestep for observation requires a single lookup in the \u03c0a:b and \u03c3a:b tables) and hence well-suited for deployment on a small, embedded device.\nWe summarize our analysis in the following Theorem:\nTheorem 2 (Conditional Planning). The algorithm for smoothing presented above computes an optimal conditional plan in d3 \u00b7 B2 \u00b7 (16n\n3 + O(n2)) evaluations of local rewards, where d is the maximum domain size of the random variables X1, . . . , Xn. In the filtering case, the optimal plan can be computed using d3 \u00b7 B \u00b7 (16n\n3 + O(n2)) evaluations, or, if no budget is used, in d3 \u00b7 (16n 4 +O(n3)) evaluations.\nThe faster computation for the filtering / no-budget case is obtained by observing that we do not require the third maximum computation, which distributes the budget into the sub-chains.\nAlso, note that contrary to the algorithm for computing optimal subsets in Section 4.1, Algorithm 2 only requires evaluations of the form R(Xj | XA = xA), which can in general be computed d2 times faster than the expectations R(Xj | XA). Under this consideration, the subset selection algorithm is in general only a factor d \u00b7B faster, even though the conditional planning algorithm has more nested loops."}, {"heading": "4.3 Efficient Algorithms for Trees with Few Leaves", "text": "In Sections 4.1 and 4.2 we have presented dynamic programming-based algorithms that can optimize value of information on chain graphical models. In fact, the key observations of Section 3 that local rewards decompose along chains holds not just in chain graphical models, but also in trees.\nMore formally, a tree graphical model is a joint probability distribution P (XV) over a collection of random variables XV if P (XV) factors as\nP (XV) = 1 Z \u220f (i,j)\u2208E \u03c8i,j(Xi,Xj),\nwhere \u03c8i,j is a nonnegative potential function, mapping assignments to xi and xj to the nonnegative real numbers, E \u2286 V \u00d7 V is a set of edges that form an undirected tree over the index set V , and Z is a normalization constant enforcing a valid probability distribution.\nThe dynamic programming algorithms presented in the previous sections can be extended to such tree models in a straightforward manner. Instead of identifying optimal subsets and conditional plans for sub-chains, the algorithms would then select optimal subsets and plans for sub-trees of increasing size. Note however that the number of sub-trees can grow exponentially in the number of leaves of the tree: A star on n leaves for example has a number of subtrees that is exponential in n. In fact, counting the number of subtrees of an arbitrary tree with n vertices is believed to be intractable (#P-complete, Goldberg & Jerrum, 2000). However, for trees that contain only a\nsmall (constant) number of leaves, the number of subtrees is polynomial, and the optimal subset and conditional plans can be computed in polynomial time."}, {"heading": "5. Theoretical Limits", "text": "Many problems that can be solved efficiently for discrete chain graphical models can also be efficiently solved for discrete polytrees3. Examples include probabilistic inference and the most probable explanation (MPE).\nIn Section 4.3 however we have seen that the complexity of the dynamic programming algorithms for chains increases dramatically when extended to trees: The complexity increases exponentially in the number of leafs of the tree.\nWe prove that, perhaps surprisingly, for the problem of optimizing value of information, this exponential increase in complexity cannot be avoided, under reasonable complexity theoretic assumptions. Before making this statement more formal, we briefly review the complexity classes used in our results."}, {"heading": "5.1 Brief Review of Relevant Computational Complexity Classes", "text": "We briefly review the complexity classes used in the following statements by presenting a complete problem for each of the class. For more details see, e.g., the references by Papadimitriou (1995) or Littman, Goldsmith, and Mundhenk (1998). The class NP contains decision problems which have polynomial-time verifiable proofs. A well-known complete problem is 3SAT for which the instances are Boolean formulas \u03c6 in conjunctive normal form containing at most three literals per clause (3CNF form). The complexity class #P contains counting problems. A complete problem for the class #P is #3SAT which counts the number of satisfying instances to a 3CNF formula. PP is a decision version of the class #P: A complete problem is MAJSAT , which decides whether a given 3CNF formula \u03c6 is satisfied by the majority, i.e., by more than half of all its possible assignments. If A and B are Turing machine based complexity classes, then AB is the complexity class derived by allowing the Turing machines deciding instances of A oracle calls to Turing machines in B. We can intuitively think of the problems in class AB as those that can be solved by a Turing Machine for classA, that has a special command which solves any problem inB. PP is similar to #P in that PPP = P#P, i.e., if we allow a deterministic polynomial time Turing machine to have access to a counting oracle, we cannot solve more complex problems than if we give it access to a majority oracle. Combining these ideas, the class NPPP is the class of problems that can be solved by nondeterministic polynomial time Turing machines that have access to a majority (or a counting) oracle. A complete problem for NPPP is EMAJSAT which, given a 3CNF on variables X1, . . . , X2n, it decides whether there exists an assignment to X1, . . . , Xn such that \u03c6 is satisfied for the majority of assignments to Xn+1, . . . , X2n. NPPP has been introduced and found to be a natural class for modeling AI planning problems in the seminal work by Littman et al. (1998). As an example, the MAP assignment problem is NPPP-complete for general graphical models, as shown by Park and Darwiche (2004).\nThe complexity classes satisfy the following set of inclusions (where the inclusions are assumed, but not known to be strict):\nP \u2286 NP \u2286 PP \u2286 PPP = P#P \u2286 NPPP. 3. Polytrees are Bayesian Networks that form trees if the edge directions are dropped."}, {"heading": "5.2 Complexity of Computing and Optimizing Value of Information", "text": "In order to solve the optimization problems, we will most likely have to evaluate the objective function, i.e., the expected local rewards. Our first result states that, even if we specialize to decision theoretic value of information objective functions as defined in Section 2.1, this problem is intractable even for Naive Bayes models, a special case of discrete polytrees. Naive Bayes models are often used in classification tasks (c.f., Domingos & Pazzani, 1997), where the class variable is predicted from noisy observations (features), that are assumed to be conditionally independent given the class variable. In a sense, Naive Bayes models are the \u201cnext simplest\u201d (from the perspective of inference) class of Bayesian networks after chains. Note that Naive Bayes models correspond to the \u201cstars\u201d referred to in Section 4.3, that have a number of subtrees that is exponential in the number of variables.\nTheorem 3 (Hardness of computation for Naive Bayes models). The computation of decision theoretic value of information functions is #P-complete even for Naive Bayes models. It is also hard to approximate to any factor unless P = NP.\nWe have the immediate corollary that the subset selection problem is PP-hard for Naive Bayes models:\nCorollary 4 (Hardness of subset selection for Naive Bayes models). The problem of determining, given a Naive Bayes model, constants c and B, cost function \u03b2 and a set of decision-theoretic value of information objective functions Ri, whether there is a subset of variables A \u2286 V such that L(A) \u2265 c and \u03b2(A) \u2264 B is PP-hard.\nIn fact, we can show that subset selection for arbitrary discrete polytrees (that are more general than Naive Bayes models, but inference is still tractable) is even NPPP-complete, a complexity class containing problems that are believed to be significantly harder than NP or #P complete problems. This result provides a complexity theoretic classification of value of information, a core AI problem.\nTheorem 5 (Hardness of subset selection computation for polytrees). The problem of determining, given a discrete polytree, constants c and B, cost function \u03b2 and a set of decision-theoretic value of information objective functions Ri, whether there is a subset of variables A \u2286 V such that L(A) \u2265 c and \u03b2(A) \u2264 B is NPPP-complete.\nFor our running example, this implies that the generalized problem of optimally selecting k sensors from a network of correlated sensors is most likely computationally intractable without resorting to heuristics. A corollary extends the hardness of subset selection to the hardness of conditional plans.\nCorollary 6 (Hardness of conditional planning computation for polytrees). Computing conditional plans is PP-hard for Naive Bayes models and NPPP-hard for discrete polytrees.\nAll proofs of results in this section are stated in the Appendix. They rely on reductions of complete problems in NP, #P and NPPP involving boolean formulae to problems of computing / optimizing value of information. The reductions are inspired by the works of Littman et al. (1998) and Park and Darwiche (2004), but require the development of novel techniques, such as new reductions of Boolean formulae to Naive Bayes and polytree graphical models associated with appropriate reward functions, ensuring that observation selections lead to feasible assignments to the Boolean formulae."}, {"heading": "6. Experiments", "text": "In this section, we evaluate our algorithms on several real world data sets. A special focus is on the comparison of the optimal methods with the greedy heuristic and other heuristic methods for selecting observations, and on how the algorithms can be used for interactive structured classification."}, {"heading": "6.1 Temperature Time Series", "text": "The first data set consists of temperature time series collected from a sensor network deployed at Intel Research Berkeley (Deshpande et al., 2004) as described in our running example. Data was continuously collected for 19 days, linear interpolation was used in case of missing samples. The temperature was measured once every 60 minutes, and it was discretized into 10 bins of 2 degrees Kelvin. To avoid overfitting, we used pseudo counts \u03b1 = 0.5 when learning the model. Using parameter sharing, we learned four sets of transition probabilities: from 12 am - 7am, 7 am - 12 pm, 12 pm - 7 pm and 7 pm - 12 am. Combining the data from three adjacent sensors, we got 53 sample time series.\nThe goal of this task was to select k out of 24 time points during the day, during which sensor readings are most informative. The experiment was designed to compare the performance of the optimal algorithms, the greedy heuristic, and a uniform spacing heuristic, which distributed the k observations uniformly over the day. Figure 3(a) shows the relative improvement of the optimal algorithms and the greedy heuristic over the uniform spacing heuristic. The performance is measured in decrease of expected entropy, with zero observations as the baseline. It can be seen that if k is less than about the half of all possible observations, the optimal algorithms decreased the expected uncertainty by several percent over both heuristics. The improvement gained by the optimal plan over the subset selection algorithms appears to become more drastic if a large number of observations (over half of all possible observations) is allowed. Furthermore, for a large number of observations, the optimal subset and the subset selected by the greedy heuristic were almost identical."}, {"heading": "6.2 CpG-Island Detection", "text": "We then studied the bioinformatics problem of finding CpG islands in DNA sequences. CpG islands are regions in the genome with a high concentration of the cytosine-guanine sequence. These areas are believed to be mainly located around the promoters of genes, which are frequently expressed in the cell. In our experiment, we considered the gene loci HS381K22, AF047825 and AL133174, for which the GenBank annotation listed three, two and one CpG islands each. We ran our algorithm on a 50 base window at the beginning and end of each island, using the transition and emission probabilities from Durbin, Eddy, Krogh, and Mitchison (1999) for our Hidden Markov Model, and we used the sum of margins as reward function.\nThe goal of this experiment was to locate the beginning and ending of the CpG islands more precisely by asking experts, whether or not certain bases belong to the CpG region or not. Figure 3(b) shows the mean classification accuracy and mean margin scores for an increasing number of observations. The results indicate that, although the expected margin scores are similar for the optimal algorithm and the greedy heuristic, the mean classification performance of the optimal algorithm was still better than the performance of the greedy heuristic. For example, when making 6 observations, the mean classification error obtained by the optimal algorithm is 25% lower than the error obtained by the greedy heuristic."}, {"heading": "6.3 Part-of-Speech Tagging", "text": "In our third experiment, we investigated the structured classification task of part-of-speech (POS) tagging (CoNLL, 2003). Problem instances are sequences of words (sentences), where each word is part of an entity (e.g., \u201cEuropean Union\u201d), and each entity belongs to one of five categories: Location, Miscellaneous, Organization, Person or Other. Imagine an application, where automatic information extraction is guided by an expert: Our algorithms compute an optimal conditional plan for asking the expert, trying to optimize classification performance while requiring as little expert interaction as possible.\nWe used a conditional random field for the structured classification task, where each node corresponds to a word, and the joint distribution is described by node potentials and edge potentials. The sum of margins was used as reward function. Measure of classification performance was the F1 score, the geometric mean of precision and recall. The goal of this experiment was to analyze how the addition of expert labels increases the classification performance, and how the indirect, decomposing reward function used in our algorithms corresponds to real world classification performance.\nFigure 3(c) shows the increase of the mean expected margin and F1 score for an increasing number of observations, summarized over ten 50 word sequences. It can be seen that the classification performance can be effectively enhanced by optimally incorporating expert labels. Requesting only three out of 50 labels increased the mean F1 score from by more than five percent. The following example illustrates this effect: In one scenario both words of an entity, the sportsman \u2018P. Simmons\u2019, were classified incorrectly \u2013 \u2018P.\u2019 as Other and \u2018Simmons\u2019 as Miscellaneous. The first request of the optimal conditional plan was to label \u2018Simmons\u2019. Upon labeling this word correctly, the word \u2018P.\u2019 was automatically labeled correctly also, resulting in an F1 score of 100 percent."}, {"heading": "7. Applying Chain Algorithms for More General Graphical Models", "text": "In Section 4 we have seen algorithms that can be used to schedule a single sensor, assuming the time series of sensor readings (e.g., temperature) form a Markov chain. This is a very natural assumption for sensor networks (Deshpande et al., 2004). When deploying sensor networks however, multiple sensors need to be scheduled. If the time series for all the sensors were independent, we could use our algorithms to schedule all the sensors independently of each other. However, in practice, the measurements will be correlated across the different sensors \u2013 in fact, this dependence is essential to allow generalization of measurements to locations where no sensor has been placed. In the following, we will describe an approach for using our single-sensor scheduling algorithm to coordinate multiple sensors.\nMore formally, we are interested in monitoring a spatiotemporal phenomenon at a set of locations S = {1, . . . ,m}, and time steps T = {1, . . . , T}. With each location\u2013time pair s, t, we associate a random variable Xs,t that describes the state of the phenomenon at that location and time. The random vector XS,T fully describes the relevant state of the world and the vector XS,t describes the state at a particular time step t. As before, we make the Markov assumption, assuming conditional independence of XS,t from XS,t\u2032 given XS,t\u22121 for all t\u2032 < t\u2212 1.\nSimilarly as in the single-chain case, we consider reward functions Rs,t that are associated with each variable Xs,t. Our goal is then to select, for each timestep, a set At \u2286 S of sensors to activate, in order to maximize the sum of expected rewards. LettingA1:t = A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAt, the expected total reward is then given as \u2211\ns,t\nRs,t(Xs,t | XA1:t)\nfor the filtering setting (i.e., only observations in the past are taken into account for evaluating the rewards), and \u2211\ns,t\nRs,t(Xs,t | XA1:T )\nfor the smoothing setting (where all observations are taken into account). The generalization to conditional planning is done as described in Section 2.\nNote that in the case of a single sensor (` = 1), the problem of optimal sensor scheduling can be solved using Algorithm 1. Unfortunately, the optimization problem is wildly intractable even for the case of two sensors, ` = 2:\nCorollary 7 (Hardness of sensor selection for two chains). Given a model with two dependent chains, constants c and B, a cost function \u03b2 and a set of decision theoretic value of information functions Rs,t, it is NPPP-complete to determine whether there is a subset A1:T of variables such that L(A1:T ) \u2265 c and \u03b2(A1:T ) \u2264 B.\nIn the following, we will develop an approximate algorithm that uses our optimal single-chain algorithms and performs well in practice."}, {"heading": "7.1 Approximate Sensor Scheduling by Lower Bound Maximization", "text": "The reason for the sudden increase in complexity in the case of multiple chains is that the decomposition of rewards along sub-chains (as described in Section 3) does not extend to the case of multiple\nsensors, since influence can flow across chains. Figure 4 visualizes this problem \u2013 there, the distribution for sensor (2) depends on all three observations S(1)1 and S (1) 4 from sensor (1) and S (2) 2 from sensor (2). We address this complexity issue using an (approximate) extension of the decomposition approach used for single chains. We will focus on the decision-theoretic value of information objective (as described in Section 2.1), but other local reward functions, such as residual entropy, can be used as well.\nConsidering only recent observations. As a first approximation, we only allow a sensor to take into account the most recent observations. Intuitively, this appears to be a reasonable approximation, especially if the potential scheduling times in T are reasonably far apart. Formally, when evaluating the local rewards at time t, we replace the set of observations up to time t, A1:t \u2286 T by a subset A\u20321:t \u2286 A1:t such that\nA\u20321:t = { (s, t) \u2208 A1:t : t \u2265 t\u2032 for all (s, t\u2032) \u2208 A1:t } ,\ni.e, for each sensor s, only the last observation (with largest time index t) is kept. We then approximate Rs,t(Xs,t | A1:t) by Rs,t(Xs,t | A\u20321:t). In Figure 4 for example, where A1:5 = {(s1, 1), (s2, 2), (s1, 4)}, the total expected utility at time t5 would be computed using only observations A\u20321:5 = {(s2, 2), (s1, 4)}, i.e., using time t4 for sensor one, and time t2 for sensor two, ignoring influence originating from observation S(1)1 and flowing through the chains as indicated by the dashed arrow. The following proposition proves that this approximation is a lower bound to the true value of information:\nProposition 8 (Monotonicity of value of information). The decision-theoretic value of information Rs,t(A) of a set A of sensors is monotonic in A,\nRs,t(A\u2032) \u2264 Rs,t(A)\nfor all A\u2032 \u2286 A.\nProposition 8 proves that conditioning only on the most recent observations can only decrease our objective function, hence maximizing this approximate objective implies maximizing a lower bound on the true objective.\nA coordinate ascent approach. We propose the following heuristic for maximizing the lower bound on the expected utility. Instead of jointly optimizing over all schedules (timesteps selected for each sensor), the algorithm will repeatedly iterate over all sensors. For all sensors s, it will optimize the selected observations As1:T , holding the schedules for all other sensors fixed. This\nprocedure resembles a coordinate ascent approach, where each \u201ccoordinate\u201d ranges over all possible schedules for a fixed sensor s.\nWhen optimizing for sensor s, the algorithm finds a schedule As1:T such that\nAs1:T = argmax A1:T \u2211 s,t Rs,t ( Xs,t | XA\u20321:t \u22c3 s\u2032 6=s XA\u2032s\u20321:t ) such that \u03b2(As1:T ) \u2264 B, (6)\ni.e., that maximizes, over all schedules A1:T , the sum of expected rewards for all time steps and sensors, given the schedules As\u20321:T for all non-selected sensors s\u2032.\nSolving the single-chain optimization problem. In order to solve the maximization problem (6) for the individual sensors, we use the same dynamic programming approach as introduced in Section 4. The recursive case Lflta:b(k) for k > 0 is exactly the same. However, the base case is computed as\nLflta:b(0) = b\u22121\u2211\nj=a+1 \u2211 s Rs,j ( Xs,j | Xa \u22c3 s\u2032 6=s XA\u2032s\u20321:j ) ,\ni.e., it takes into account the most recent observation for all non-selected sensors s\u2032. Several remarks need to be made about the computation of the base case Lflta:b(0). First of all, in a naive implementation, the computation of the expected utility\nRs,j ( Xs,j | Xa \u22c3 s\u2032 6=s XA\u2032s\u20321:j ) requires time exponential in the number of chains. This is the case since, in order to compute the reward Rs,t, for each chain, all possible observations XA\u2032s1:t = xA\u2032s1:t that could be made need to be taken into account. This computation requires computing the expectation over the joint distribution P (XA\u20321:t), which is exponential in size. This increase in complexity can be avoided using a sampling approximation: Hoeffding\u2019s inequality can be used to derive polynomial bounds on sample complexity for approximating the value of information up to arbitrarily small additive error \u03b5, similarly as done in the approach of Krause and Guestrin (2005a)4. In practice, a small number of samples appears to provide reasonable performance. Secondly, inference itself becomes intractable with an increasing number of sensors. Approximate inference algorithms such as the algorithm proposed by Boyen and Koller (1998) provide a viable way around this problem.\nAnalysis. Since all sensors maximize the same global objective L(A1:T ), the coordinated ascent approach is guaranteed to monotonically increase the global objective with every iteration (ignoring possible errors due to sampling or approximate inference). Hence it must converge (to a local optimum) after a finite number of steps. The procedure is formalized in Algorithm 4.\nAlthough we cannot in general provide performance guarantees for the procedure, we are building on an algorithm that provides an optimal schedule for each sensor in isolation, which should benefit from observations provided by the remaining sensors. Also, note that if the sensors are all independent, Algorithm 4 will obtain the optimal solution. Even if the sensors are correlated, the obtained solution will be at least as good as the solution obtained when scheduling all sensors independently of each other. Algorithm 4 will always converge, and always compute a lower bound on\n4. An absolute error of at most \u03b5 when evaluating each reward Rs,t can accumulate to a total error of at most |T ||S|\u03b5 for all variables and hence to the error of the optimal schedule.\nInput: Budget B Output: Selection A1, . . . ,A` of observation times for each sensor begin\nSelect Ai, 1 \u2264 i \u2264 ` at random; repeat\nfor i = 1 to ` do Use Algorithm 1 to select observations Ai for sensor i, but conditioning on current sensor scheduling Aj , j 6= i, for remaining sensors; end Compute improvement \u03b4 in total expected utility;\nuntil \u03b4 small enough ; end\nAlgorithm 4: Multi-Sensor scheduling.\nthe expected total utility. Considering the intractability of the general problem even for two chains (c.f., , Corollary 7), these properties are reassuring. In our experiments, the coordinated sensor scheduling performed very well, as discussed in Section 7.2."}, {"heading": "7.2 Proof of Concept Study on Real Deployment", "text": "In the work by Singhvi et al. (2005), we presented an approach for optimizing light control in buildings, with the purpose of satisfying building occupants\u2019 preferences about lighting conditions, and simultaneously minimizing energy consumption. In our approach, a wireless sensor network is deployed that monitors the building for environmental conditions (such as the sunlight intensity etc.). The sensors feed their measurements to a building controller that actuates the lighting system (lamps, blinds, etc.) accordingly. At every timestep t \u2208 T , the building controller can choose an action that affects the lighting conditions at all locations S in the building. Utility functions Ut(a,xS,t) are specified that map the chosen actions and the current lighting levels to a utility value. This utility is chosen to capture both users\u2019 preferences about light levels, as well as the energy consumption of the lighting system. Details on the utility functions are described in detail by Singhvi et al..\nWe evaluated our multi-sensor scheduling approach in a real building controller testbed, as described in detail by Singhvi et al.. In our experiments, we used Algorithm 4 to schedule three sensors, allowing each sensor to choose a subset out of ten time steps (in one-hour intervals during daytime). We varied the number of timesteps during which each sensor is activated, and computed the total energy consumption and total user utility (as defined by Singhvi et al.). Figure 5(a) shows the mean user utility and energy savings achieved, for a number of observations varying from no observations to continuous sensing (10 observations in our discretization)5. These results imply that using the predictive model and our active sensing strategy, even a very small number of observations achieves results approximately as good as the results achieved by continuous sensing.\nFigure 5(b) presents the mean total utility achieved using no observations, one observation or ten observations per sensor each day. It can be seen that even a single observation per sensor increases the total utility close to the level achieved by continuous sensing. Figure 5(c) shows the mean energy\n5. Note that in Figure 5(a), energy cost and utility are plotted in different units and should not be directly compared.\nconsumption required for the same experiment. Here, the single sensor observation strategy comes even closer to the power savings achieved for continuous sensing.\nSince the sensor network battery lifetime is in general inversely proportional to the amount of power expended for sensing and communication, we conclude that our sensor scheduling strategy promises to lead to drastic increases in sensor network lifetime, deployment permanence and reduced maintenance cost. In our testbed, the network lifetime could be increased by a factor of 3 without significant reduction in user utility and increase in energy cost."}, {"heading": "8. Related Work", "text": "In this section, we review related work in a number of different areas."}, {"heading": "8.1 Optimal Experimental Design", "text": "Optimal experimental design is a general methodology for selecting informative experiments to infer about aspects of the state of the world (such as the parameters of a particular nonlinear function, etc.). There is a large literature about different approaches to experimental design (c.f., Chaloner & Verdinelli, 1995; Krause, Singh, & Guestrin, 2007).\nIn Bayesian experimental design, a prior distribution over possible states of the world is assumed, and experiments are chosen, e.g., to reduce the uncertainty in the posterior distribution. In its general form, Bayesian experimental design was pioneered by Lindley (1956). The users encode their preferences in a utility function U(P (\u0398), \u03b8?), where the first argument, P (\u0398), is a distribution over states of the world (i.e., the parameters) and the second argument, \u03b8?, is the true state of the world. Observations xA are collected, and the change in expected utility under the prior P (\u0398) and posterior P (\u0398 | XA = xA) can be used as a design criterion. In this sense, the value of observation problems considered in this paper can be considered instances of Bayesian experimental design problems. Typically, Bayesian Experimental Design is employed for continuous distributions, often the multivariate normal distribution. By choosing different utility functions, different notions of optimality are defined, including A- and D- optimality can be developed (Chaloner & Verdinelli, 1995). If we have the posterior covariance matrix \u03a3\u03b8|A, whose maximum eigenvalue is \u03bbmax, then Bayesian A-, D-, and E- optimality minimizes tr ( \u03a3\u03b8|A ) , det ( \u03a3\u03b8|A ) , and \u03bbmax ( \u03a3\u03b8|A ) , respectively. In the terminology of Section 2.1, D-optimality corresponds to choosing the total entropy, and A-optimality corresponds to the (weighted) mean-squared error criteria.\nEven for multivariate normal distributions, optimal Bayesian Experimental design is NP-hard (Ko, Lee, & Queyranne, 1995). In some applications of experimental design, the number of experiments to be selected is often large compared to the number of design choices. In these cases, one can find a fractional design (i.e., a non-integral solution defining the proportions by which experiments should be performed), and round the fractional solutions. In the fractional formulation, A-, D-, and E-optimality criteria can be solved exactly using a semi-definite program (Boyd & Vandenberghe, 2004). There are however no known bounds on the integrality gap, i.e., the loss incurred by this rounding process.\nThe algorithms presented in Section 4.1 can be used to optimally solve non-fractional Bayesian Experimental Design problems for chain graphical models, even for continuous distributions, as long as inference in these distributions is tractable (such as normal distributions). This paper hence provides a new class of combinatorial algorithms for an interesting class of Bayesian experimental design problems."}, {"heading": "8.2 Value of Information in Graphical Models", "text": "Decision-theoretic value of information has been frequently used for principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984). In a sense, value of information problems are special cases of Bayesian experimental design problems, where the prior distribution has a particular structure, typically given by a graphical model as considered in this paper.\nSeveral researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) suggested myopic, i.e., greedy approaches for selectively gathering evidence in graphical models, as considered in this paper, which, unlike the algorithms presented in this paper. While these algorithms are applicable to much more general graphical models, they do not have theoretical guarantees. Heckerman et al. (1993) propose a method to compute the maximum expected utility for specific sets of observations. While their work considers more general graphical models than this paper (Naive Bayes models and certain extensions), they provide only large sample guarantees for the evaluation of a given sequence of observations, and use a heuristic without guarantees to select such sequences. Bilgic and Getoor (2007) present a branch and bound approach towards exactly optimizing value of information in more complex probabilistic models. In contrast to the algorithms described in this paper however, their approach has running time that is worst-case exponential. Munie and Shoham (2008) present algorithms and hardness results for optimizing a special class of value of information objective functions that are motivated by optimal educational testing problems. Their algorithms apply to a different class of graphical models than chains, and only apply for specific objective functions, rather than general local reward functions as considered in this paper. Radovilsky, Shattah, and Shimony (2006) extended the previous version of our paper (Krause & Guestrin, 2005a) to obtain approximation algorithms with guarantees in the case of noisy observations (i.e., selecting a subset of the emission variables to observe, rather than selecting among the hidden variables as considered in this paper)."}, {"heading": "8.3 Bandit Problems and Exploration / Exploitation", "text": "An important class of sequential value of information problems is the class of Bandit problems. In the classical k-armed bandit problem, as formalized by Robbins (1952), a slot machine is given\nwith k arms. A draw from arm i results in a reward with success probability pi that is fixed for each arm, but different (and independent) across each arm. When selecting arms to pull, an important problem is to trade off exploration (i.e., estimation of the success probabilities of the arms) and exploitation (i.e., repeatedly pulling the best arm known so far). A celebrated result by Gittins and Jones (1979) shows that for a fixed number of draws, an optimal strategy can be computed in polynomial time, using a dynamic programming based algorithm. While similar in the sense that an optimal sequential strategy can be computed in polynomial time, Gittins algorithm however has different structure from the dynamic programming algorithms presented in this paper.\nNote that using the \u201cfunction optimization\u201d objective function described in Section 2.1, our approach can be used to solve a particular instance of bandit problems, where the arms are not required to be independent, but, in contrary to the classical notion of bandit problems, can not be chosen repeatedly."}, {"heading": "8.4 Probabilistic Planning", "text": "Optimized information gathering has been also extensively studied in the planning community. Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees.\nThe problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees.\nIn the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM). None of these approaches however provide guarantees about the quality of the obtained solutions. Singh, Krause, Guestrin, Kaiser, and Batalin (2007) present an approximation algorithm with theoretical guarantees for the problem of planning an informative path for environmental monitoring using Gaussian Process models. In contrast to the algorithms presented in this paper, while dealing with more complex probabilistic models and more complex cost functions arising from path planning, their approach requires submodular objective functions (a property that does not hold for value of information as we show in Proposition 9)."}, {"heading": "8.5 Sensor Selection and Scheduling", "text": "In the context of wireless sensor networks, where sensor nodes have limited battery and can hence only enable a small number of measurements, optimizing the value of information from the selected sensors plays a key role. The problem of deciding when to selectively turn on sensors in order to conserve power was first discussed by Slijepcevic and Potkonjak (2001) and Zhao, Shin, and Reich (2002). Typically, it is assumed that sensors are associated with a fixed sensing region, and a spatial\ndomain needs to be covered by the regions associated with the selected sensors. Abrams, Goel, and Plotkin (2004) present an efficient approximation algorithm with theoretical guarantees for this problem. Deshpande, Khuller, Malekian, and Toossi (2008) present an approach for this problem based on semidefinite programming (SDP), handling more general constraints and providing tighter approximations. The approaches described above do not apply to the problem of optimizing sensor schedules for more complex utility functions such as, e.g., the increase in prediction accuracy and other objectives considered in this paper. To address these shortcomings, Koushanfary, Taft, and Potkonjak (2006) developed an approach for sensor scheduling that guarantees a specified prediction accuracy based on a regression model. However, their approach relies on the solution of a Mixed Integer Program, which is intractable in general. Zhao et al. (2002) proposed heuristics for selectively querying nodes in a sensor network in order to reduce the entropy of the prediction. Unlike the algorithms presented in this paper, their approaches do not have any performance guarantees."}, {"heading": "8.6 Relationship to Machine Learning", "text": "Decision Trees (Quinlan, 1986) popularized the value of information as a criterion for creating conditional plans. Unfortunately, there are no guarantees on the performance of this greedy method.\nThe subset selection problem as an instance of feature selection is a central issue in machine learning, with a vast amount of literature (see Molina, Belanche, & Nebot, 2002 for a survey). However, we are not aware of any work providing similarly strong performance guarantees than the algorithms considered in this paper.\nThe problem of choosing observations also has a strong connection to the field of active learning (c.f., Cohn, Gharamani, & Jordan, 1996; Tong & Koller, 2001) in which the learning system designs experiments based on its observations. While sample complexity bounds have been derived for some active learning problems (c.f., Dasgupta, 2005; Balcan, Beygelzimer, & Langford, 2006), we are not aware of any active learning algorithms that perform provably optimal (even for restricted classes of problem instances)."}, {"heading": "8.7 Previous Work by the Authors", "text": "A previous version of this paper appeared in the work by Krause and Guestrin (2005b). Some of the contents of Section 7 appeared as part of the work by Singhvi et al. (2005). The present version is much extended, with new algorithmic and hardness results and more detailed discussions.\nIn light of the negative results presented in Section 5, we cannot expect to be able to optimize value of information in more complex models than chains. However, instead of attempting to solve for the optimal solution, one might wonder whether it is possible to obtain good approximations. The authors showed (Krause & Guestrin, 2005a; Krause et al., 2007; Krause, Leskovec, Guestrin, VanBriesen, & Faloutsos, 2008) that a large number of practical objective functions satisfy an intuitive diminishing returns property: Adding a new observation helps more if we have few observations so far, and less if we have already made many observations. This intuition can be formalized using the combinatorial concept called submodularity. A fundamental result by Nemhauser et al. proves that when optimizing a submodular utility function, the myopic greedy algorithm in fact provides a near-optimal solution, that is within a constant factor of (1\u22121/e) \u2248 63% of optimal. Unfortunately, decision theoretic value of information does not satisfy submodularity.\nProposition 9 (Non-submodularity of value of information). Decision-theoretic value of information is not submodular, even in Naive Bayes models.\nIntuitively, value of information can be non-submodular, if we need to make several observations in order to \u201cconvince\u201d ourselves that we need to change our action."}, {"heading": "9. Conclusions", "text": "We have described novel efficient algorithms for optimal subset selection and conditional plan computation in chain graphical models (and trees with few leaves), including HMMs. Our empirical evaluation indicates that these algorithms can improve upon commonly used heuristics for decreasing expected uncertainty. Our algorithms can also effectively enhance performance in interactive structured classification tasks.\nUnfortunately, the optimization problems become wildly intractable for even a slight generalization of chains. We presented surprising theoretical limits, which indicate that even the class of decision theoretic value of information functions (as widely used, e.g., in influence diagrams and POMDPs) cannot be efficiently computed even in Naive Bayes models. We also identified optimization of value of information as a new class of problems that are intractable (NPPP-complete) for polytrees.\nOur hardness results, along with other recent results for polytree graphical models, the NPcompleteness of maximum a posteriori assignment (Park & Darwiche, 2004) and NP-hardness of inference in conditional linear Gaussian models (Lerner & Parr, 2001), suggest the possibility of developing a generalized complexity characterization of problems that are hard in polytree graphical models.\nIn light of these theoretical limits for computing optimal solutions, it is a natural question to ask whether approximation algorithms with non-trivial performance guarantees can be found. Recent results by Krause and Guestrin (2005a), Radovilsky et al. (2006) and Krause et al. (2007) show that this is the case for interesting classes of value of information problems."}, {"heading": "Acknowledgments", "text": "We would like to thank Ben Taskar for providing the part-of-speech tagging model, and Reuters for making their news archive available. We would also like to thank Brigham Anderson and Andrew Moore for helpful comments and discussions. This work was partially supported by NSF Grants No. CNS-0509383, CNS-0625518, ARO MURI W911NF0710287 and a gift from Intel. Carlos Guestrin was partly supported by an Alfred P. Sloan Fellowship, an IBM Faculty Fellowship and an ONR Young Investigator Award N00014-08-1-0752 (2008-2011). Andreas Krause was partially supported by a Microsoft Research Graduate Fellowship."}, {"heading": "Appendix A", "text": "Proof of Theorem 3. Membership in #P for arbitrary discrete polytrees is straightforward since inference in such models is in P. Let \u03c6 be an instance of #3SAT , where we have to count the number of assignments to X1, . . . , Xn satisfying \u03c6. Let C = {C1, . . . , Cm} be the set of clauses. Now create a Bayesian network with 2n + 1 variables, X1, . . . ,Xn, U1, . . . ,Un and Y , where the Xi are conditionally independent given Y . Let Y be uniformly distributed over the values\n{\u2212n,\u2212(n\u2212 1), . . . ,\u22121, 1, . . . ,m\u2212 1,m}, and each Ui have Bernoulli prior with p = 0.5. Let the observed variables Xi have CPTs defined the following way:\nXi | [Y = +j,Ui = u] \u223c {\n1, if Xi = u satisfies clause Cj ; 0, otherwise.\nXi | [Y = \u2212j,Ui = u] \u223c {\n0, if i = j; u, otherwise.\nIn this model, which is presented in Figure 6, it holds that X1 = X2 = \u00b7 \u00b7 \u00b7 = Xn = 1 iff U1, . . . ,Un encode a satisfying assignment of \u03c6, and Y > 0. Hence, if we observe X1 = X2 = \u00b7 \u00b7 \u00b7 = Xn = 1, we know that Y > 0 with certainty. Furthermore, if at least one Xi = 0, we know that P (Y > 0 | X = x) < 1. Let all nodes have zero reward, except for Y , which is assigned a reward function with the following properties (we will show below how we can model such a local reward function using the decision-theoretic value of information):\nR(Y | XA = xA) = { (n+m)2n\nm , if P (Y > 0 | XA = xA) = 1; 0, otherwise.\nBy the above argument, the expected reward\nR(Y | X1, . . . ,Xn) = \u2211 u,y,x P (Y = y)P (U = u)P (x|u)R(Y | X = x)\n= \u2211\nu sat \u03c6\nP (Y > 0)P (u)(n+m)2 n m = \u2211 u sat \u03c6 1\nis exactly the number of satisfying assignments to \u03c6. Note that the model defined above is not yet a Naive Bayes model. However, it can easily be turned into one by marginalizing out U .\nWe will now show how we can realize a reward function with the above properties in the maximum expected utility sense. Let D = {d1, d2} be a set of two decisions. Define a utility function with the property:\nu(y, d) =  (n+m)2n m , if d = d1 and y > 0; (n+m)22n+1\nn , if d = d1 and y < 0; 0, otherwise.\nThe reward R(Y | XA) is then given as the decision-theoretic value of information:\nR(Y | XA) = \u2211 xA P (xA) max d \u2211 y P (y | xA)u(y, d).\nThe utility function u is based on the following consideration. Upon observing a particular instantiation of the variables X1, . . . ,Xn we make a decision d about variable Y . Our goal is to achieve that the number of times action d1 is chosen exactly corresponds to the number of satisfying assignments to \u03c6. This is accomplished in the following way. If all Xi are 1, then we know that the Ui had encoded a satisfying assignment, and Y > 0 with probability 1. In this case, action d1 is chosen. Now we need to make sure that whenever at least one Xi = 0 (which indicates either that Y < 0 or U is not a satisfying assignment) decision d2 is chosen. Now, if at least one Xi = 0, then either Y = j > 0 and clause j was not satisfied, or Y < 0. The utilities are designed such that unless P (Y > 0 | XA = xA) \u2265 1 \u2212 n2 \u2212n\n2m , the action d2 gives the higher expected reward of 0. Hereby, n2\u2212n\n2m is a lower bound on the probability of \u201cmisclassification\u201d P (Y < 0 | XA = xA). Note that the above construction immediately proves the hardness of approximation: Suppose there were a polynomial time algorithm which computes an approximation R\u0302 that is within any factor \u03b1 > 1 (which can depend on the problem instance) of R = R(Y | X1, . . . ,Xn). Then R\u0302 > 0 implies that R > 0, and R\u0302 = 0 implies that R = 0. Hence, the approximation R\u0302 can be used to decide whether \u03c6 is satisfiable or not, implying that P = NP.\nProof of Corollary 4. Let \u03c6 be a 3CNF formula. We convert it into a Naive Bayes model over variables X1, . . . ,Xn and Y as in the construction of Theorem 3. The function L(V) where V = {1, . . . , n} is the set of all variables Xi counts the number of satisfying assignments to \u03c6. Note that the function L(A) for A \u2286 V = {1, . . . , n} is monotonic, i.e., L(A) \u2264 L(V) for all A \u2286 V , as shown in Proposition 8. Hence the majority of assignments satisfies \u03c6 if and only if L(V) > 2n\u22121.\nProof of Theorem 5. Membership follows from the fact that inference in polytrees is in P for discrete polytrees: A nondeterministic Turing machine with #P oracle can first \u201cguess\u201d the selection of variables, then compute the value of information using Theorem 3 (since such computation is #P-complete for arbitrary discrete polytrees), and compare against constant c.\nTo show hardness, let \u03c6 be an instance of EMAJSAT , where we have to find an instantiation of X1, . . . , Xn such that \u03c6(X1, . . . , X2n) is true for the majority of assignments to Xn+1, . . . , X2n. LetC = {C1, . . . , Cm} be the set of 3CNF clauses. Create the Bayesian network shown in Figure 7, with nodes Ui, each having a uniform Bernoulli prior. Add bivariate variables Yi = (seli, pari), 0 \u2264 i \u2264 2n, where seli takes values in {0, . . . ,m} and pari is a parity bit. The CPTs for Yi are\ndefined as: sel0 uniformly varies over {1, . . . ,m}, par0 = 0, and for Y1, . . . ,Y2n: seli | [seli\u22121 = j,Ui = ui] \u223c {\n0, if j = 0, or ui satisfies Cj ; j, otherwise;\npari | [pari\u22121 = bi\u22121,Ui] \u223c bi\u22121 \u2295 Ui,\nwhere \u2295 denotes the parity (XOR) operator. We now add variables ZTi and ZFi for 1 \u2264 i \u2264 n and let ZTi | [Ui = ui] \u223c { Uniform({0, 1}), if ui = 1; 0, otherwise;\nwhere Uniform denotes the uniform distribution. Similarly, let ZFi | [Ui = ui] \u223c { Uniform({0, 1}), if ui = 0; 0, otherwise.\nIntuitively, ZTi = 1 guarantees us that Ui = 1, whereas ZTi = 0 leaves us uncertain about Ui. The case of ZFi is symmetric.\nWe use the subset selection algorithm to choose theZis that encode the solution toEMAJSAT . If ZTi is chosen, it will indicate that Xi should set to true, similarly ZFi indicates a false assignment to Xi. The parity function is going to be used to ensure that exactly one of {ZTi ,ZFi } is observed for each i.\nWe first assign penalties \u221e to all nodes except ZTi ,ZFi for 1 \u2264 i \u2264 n, and Uj for n + 1 \u2264 j \u2264 2n, which are assigned zero penalty. Let all nodes have zero reward, except for Y2n, which is assigned the following reward:\nR(Y2n | XA = xA) =  4n, if P (sel2n = 0 | XA = xA) = 1 and\n[P (par2n = 1 | XA = xA) = 1 or P (par2n = 0 | XA = xA) = 1]; 0, otherwise.\nNote that sel2n = 0 with probability 1 iff U1, . . . ,U2n encode a satisfying assignment of \u03c6. Furthermore, we get positive reward only if we are both certain that sel2n = 0, i.e., the chosen observation set must contain a proof that \u03c6 is satisfied, and we are certain about par2n. The parity certainty will only occur if we are certain about the assignment U1, . . . ,U2n. It is only possible to infer the value of each Ui with certainty by observing one of Ui,ZTi or ZFi . Since, for i = 1, . . . , n, the cost of observing Ui is \u221e, to receive any reward we must observe at least one of ZTi or ZFi . Assume that we compute the optimal subset O\u0302 for budget 2n, then we can only receive positive reward by observing exactly one of ZTi or ZFi .\nWe interpret the selection ofZTi andZFi as an assignment to the first n variables ofEMAJSAT . Let R\u0302 = R(Y2n | O\u0302). We claim that \u03c6 \u2208 EMAJSAT if and only if R\u0302 > 0.5. First let \u03c6 \u2208 EMAJSAT , with assignment x1, . . . , xn to the first n variables. Now add Un+1, . . . ,U2n to O and add ZTi to O iff xi = 1 and ZFi to O iff xi = 0. This selection guarantees R\u0302 > 0.5.\nNow assume R\u0302 > 0.5. We call an assignment to U1, . . . ,U2n consistent if for any 1 \u2264 i \u2264 n, if ZTi \u2208 O\u0302, then Ui = 1 and if ZFi \u2208 O\u0302 then Ui = 0. For any consistent assignment, the chance that the observations Zi prove the consistency is 2\u2212n. Hence R\u0302 > 0.5 implies that the majority of all provably consistent assignments satisfy \u03c6 and hence \u03c6 \u2208 EMAJSAT . This proves that subset selection is NPPP complete.\nNote that we can realize the local reward function R in the sense of maximum expected utility similarly as described in the Proof of Theorem 3.\nProof of Corollary 6. The constructions in the proof of Theorem 4 and Theorem 5 also prove that computing conditional plans is PP-hard and NPPP-hard respectively, since, in these instances, any plan with positive reward must observe variables corresponding to valid instantiations (i.e., all X1, . . . ,Xn in Corollary 4, and all Un+1, . . . ,U2n and one each of the Z1, . . . ,Zn to satisfy the parity condition in Theorem 5). In these cases, the order of selection is irrelevant, and, hence, the conditional plan effectively performs subset selection.\nProof of Corollary 7. The proof follows from the observation that polytree construction from the proof of Theorem 5 can be arranged into two dependent chains. For this transformation, we revert the arc between ZTi and Ui by applying Bayes\u2019 rule. To make sure there are the same number of nodes for each sensor in each timeslice, we triple the variables Yi, calling the copies Y \u2032i and Y \u2032\u2032i . The conditional probability tables are given as equality constraints, Y \u2032i = Yi and Y \u2032\u2032i = Y \u2032i. After this transformation, the variables associated with timesteps 3i \u2212 2 (for i \u2265 1) are given by the sets {Y \u2032\u2032i\u22121,ZTi }. timesteps 3i\u2212 1 are associated with the sets {Ui,Yi}, and timesteps 3i are associated with {ZFi ,Y \u2032i}.\nProof of Proposition 8. This bound follows from the fact that maximization over a is convex, and an application of Jensen\u2019s inequality. Using an induction argument, we simply need to show that L(A) \u2265 L(\u2205).\nL(A) = \u2211 xA P (XA = xA) (\u2211 t\u2208V max a EU(a, t,x | XA1:t = xA1:t) )\n\u2265 \u2211 t\u2208V max a (\u2211 xA P (XA = xA)EU(a, t,x | XA1:t = xA1:t) ) = \u2211 t\u2208V max a EU(a, t,x) = L(\u2205)\nwhere EU(a, t,x | XA1:t = xA1:t) = \u2211 xt P (xt | XA1:t = xA1:t)Ut(a, xt)\nis the expected utility of action a at time t after observing XA1:t = xA1:t .\nProof of Proposition 9. Consider the following binary classification problem with assymetric cost. We have one Bernoulli random variable Y (the class label) with P (Y = 1) = 0.5 and P (Y = \u22121) = 0.5. We also have two noisy observations X1,X2, which are conditionally independent given Y . Let P (Xi = Y) = 3/4 (i.e., the observations agree with the class label with probability 3/4, and disagree with probability 1/4. We have three actions, a1 (classifying Y as 1), a\u22121 (classifying Y as -1) and a0 (not assigning any label). We define our utility functon U such that we gain utility 1 if we assign the label correctly (U(a1, 1) = U(a\u22121,\u22121) = 1), \u22123 is we misassign the label (U(a1,\u22121) = U(a\u22121, 1) = \u22123), and 0 if we choose a0, i.e., not assign any label. Now, we can verify that L(\u2205) = L({X1}) = L({X2}) = 0, but L({X1,X2}) = ( 3 4\n)2\u2212 3 (14)2 = 616 > 0. Hence, adding X2 to X1 increases the utility more than adding X2 to the empty set, contradicting submodularity."}], "references": [{"title": "Set k-cover algorithms for energy efficient monitoring in wireless sensor networks", "author": ["Z. Abrams", "A. Goel", "S. Plotkin"], "venue": "In IPSN", "citeRegEx": "Abrams et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abrams et al\\.", "year": 2004}, {"title": "Agnostic active learning", "author": ["N. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In ICML", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Ann. Math. Stat,", "citeRegEx": "Baum and Petrie,? \\Q1966\\E", "shortCiteRegEx": "Baum and Petrie", "year": 1966}, {"title": "Learning diagnostic policies from examples by systematic search", "author": ["V. Bayer-Zubek"], "venue": "UAI.", "citeRegEx": "Bayer.Zubek,? 2004", "shortCiteRegEx": "Bayer.Zubek", "year": 2004}, {"title": "A Markovian decision process", "author": ["R. Bellman"], "venue": "Journal of Mathematics and Mechanics, 6.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Voila: Efficient feature-value acquisition for classification", "author": ["M. Bilgic", "L. Getoor"], "venue": "In Twenty-Second Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Bilgic and Getoor,? \\Q2007\\E", "shortCiteRegEx": "Bilgic and Getoor", "year": 2007}, {"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "In Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Boyen and Koller,? \\Q1998\\E", "shortCiteRegEx": "Boyen and Koller", "year": 1998}, {"title": "Bayesian experimental design: A review", "author": ["K. Chaloner", "I. Verdinelli"], "venue": "Statistical Science,", "citeRegEx": "Chaloner and Verdinelli,? \\Q1995\\E", "shortCiteRegEx": "Chaloner and Verdinelli", "year": 1995}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Gharamani", "M.I. Jordan"], "venue": "J AI Research,", "citeRegEx": "Cohn et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1996}, {"title": "Conference on computational natural language learning shared task", "author": ["CoNLL"], "venue": "http://cnts.uia.ac.be/conll2003/ner/.", "citeRegEx": "CoNLL,? 2003", "shortCiteRegEx": "CoNLL", "year": 2003}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "NIPS.", "citeRegEx": "Dasgupta,? 2005", "shortCiteRegEx": "Dasgupta", "year": 2005}, {"title": "Model-driven data acquisition in sensor networks. In VLDB", "author": ["A. Deshpande", "C. Guestrin", "S. Madden", "J. Hellerstein", "W. Hong"], "venue": null, "citeRegEx": "Deshpande et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2004}, {"title": "Energy efficient monitoring in sensor networks. In LATIN", "author": ["A. Deshpande", "S. Khuller", "A. Malekian", "M. Toossi"], "venue": null, "citeRegEx": "Deshpande et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2008}, {"title": "Myopic value of information in influence diagrams", "author": ["S. Dittmer", "F. Jensen"], "venue": "In UAI,", "citeRegEx": "Dittmer and Jensen,? \\Q1997\\E", "shortCiteRegEx": "Dittmer and Jensen", "year": 1997}, {"title": "On the optimality of the simple Bayesian classifier under zero-one loss", "author": ["P. Domingos", "M. Pazzani"], "venue": "Machine Learning,", "citeRegEx": "Domingos and Pazzani,? \\Q1997\\E", "shortCiteRegEx": "Domingos and Pazzani", "year": 1997}, {"title": "Biological Sequence Analysis : Probabilistic Models of Proteins and Nucleic Acids", "author": ["R. Durbin", "S.R. Eddy", "A. Krogh", "G. Mitchison"], "venue": null, "citeRegEx": "Durbin et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Durbin et al\\.", "year": 1999}, {"title": "A dynamic allocation index for the discounted multiarmed bandit problem", "author": ["J.C. Gittins", "D.M. Jones"], "venue": null, "citeRegEx": "Gittins and Jones,? \\Q1979\\E", "shortCiteRegEx": "Gittins and Jones", "year": 1979}, {"title": "Counting unlabelled subtrees of a tree is #p-complete", "author": ["L.A. Goldberg", "M. Jerrum"], "venue": "LMS J Comput. Math.,", "citeRegEx": "Goldberg and Jerrum,? \\Q2000\\E", "shortCiteRegEx": "Goldberg and Jerrum", "year": 2000}, {"title": "An approximate nonmyopic computation for value of information", "author": ["D. Heckerman", "E. Horvitz", "B. Middleton"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Heckerman et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1993}, {"title": "Information value theory", "author": ["R.A. Howard"], "venue": "IEEE Transactions on Systems Science and Cybernetics (SSC-2).", "citeRegEx": "Howard,? 1966", "shortCiteRegEx": "Howard", "year": 1966}, {"title": "Readings on the Principles and Applications of Decision Analysis II, chap. Influence Diagrams, pp. 719\u2013762", "author": ["R.A. Howard", "J. Matheson"], "venue": "Strategic Decision Group, Menlo Park. Reprinted 2005 in Decision Analysis", "citeRegEx": "Howard and Matheson,? \\Q1984\\E", "shortCiteRegEx": "Howard and Matheson", "year": 1984}, {"title": "Non-myopic multi-aspect sensing with partially observable Markov decision processes", "author": ["S. Ji", "R. Parr", "L. Carin"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Ji et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2007}, {"title": "Selective supervision: Guiding supervised learning with decision-theoretic active learning", "author": ["A. Kapoor", "E. Horvitz", "S. Basu"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Kapoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Decisions with Multiple Objectives: Preferences and Value Trade-offs", "author": ["R.L. Keeney", "H. Raiffa"], "venue": null, "citeRegEx": "Keeney and Raiffa,? \\Q1976\\E", "shortCiteRegEx": "Keeney and Raiffa", "year": 1976}, {"title": "An exact algorithm for maximum entropy sampling", "author": ["C. Ko", "J. Lee", "M. Queyranne"], "venue": "Operations Research,", "citeRegEx": "Ko et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ko et al\\.", "year": 1995}, {"title": "Efficient optimization of information-theoretic exploration in slam", "author": ["T. Kollar", "N. Roy"], "venue": "In AAAI", "citeRegEx": "Kollar and Roy,? \\Q2008\\E", "shortCiteRegEx": "Kollar and Roy", "year": 2008}, {"title": "Sleeping coordination for comprehensive sensing using isotonic regression and domatic partitions", "author": ["F. Koushanfary", "N. Taft", "M. Potkonjak"], "venue": "In Infocom", "citeRegEx": "Koushanfary et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Koushanfary et al\\.", "year": 2006}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proc. of Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Krause and Guestrin,? \\Q2005\\E", "shortCiteRegEx": "Krause and Guestrin", "year": 2005}, {"title": "Optimal nonmyopic value of information in graphical models - efficient algorithms and theoretical limits", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proc. of IJCAI", "citeRegEx": "Krause and Guestrin,? \\Q2005\\E", "shortCiteRegEx": "Krause and Guestrin", "year": 2005}, {"title": "Efficient sensor placement optimization for securing large water distribution networks", "author": ["A. Krause", "J. Leskovec", "C. Guestrin", "J. VanBriesen", "C. Faloutsos"], "venue": "Journal of Water Resources Planning and Management,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "In JMLR", "citeRegEx": "Krause et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Inference in hybrid networks: Theoretical limits and practical algorithms. In UAI", "author": ["U. Lerner", "R. Parr"], "venue": null, "citeRegEx": "Lerner and Parr,? \\Q2001\\E", "shortCiteRegEx": "Lerner and Parr", "year": 2001}, {"title": "On a measure of the information provided by an experiment", "author": ["D.V. Lindley"], "venue": "Annals of Mathematical Statistics, 27, 986\u20131005.", "citeRegEx": "Lindley,? 1956", "shortCiteRegEx": "Lindley", "year": 1956}, {"title": "The computational complexity of probabilistic planning", "author": ["M. Littman", "J. Goldsmith", "M. Mundhenk"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Littman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1998}, {"title": "Feature selection algorithms: A survey and experimental evaluation", "author": ["L. Molina", "L. Belanche", "A. Nebot"], "venue": "In ICDM", "citeRegEx": "Molina et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Molina et al\\.", "year": 2002}, {"title": "Sequential decision models for expert system optimization", "author": ["V.S. Mookerjee", "M.V. Mannino"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "Mookerjee and Mannino,? \\Q1997\\E", "shortCiteRegEx": "Mookerjee and Mannino", "year": 1997}, {"title": "Optimal testing of structured knowledge", "author": ["M. Munie", "Y. Shoham"], "venue": "In Twenty-Third Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Munie and Shoham,? \\Q2008\\E", "shortCiteRegEx": "Munie and Shoham", "year": 2008}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison-Wesley.", "citeRegEx": "Papadimitriou,? 1995", "shortCiteRegEx": "Papadimitriou", "year": 1995}, {"title": "Complexity results and approximation strategies for map explanations", "author": ["J.D. Park", "A. Darwiche"], "venue": "Journal of Aritificial Intelligence Research,", "citeRegEx": "Park and Darwiche,? \\Q2004\\E", "shortCiteRegEx": "Park and Darwiche", "year": 2004}, {"title": "Anytime point-based approximations for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": null, "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning, 1, 81\u2013106.", "citeRegEx": "Quinlan,? 1986", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Efficient deterministic approximation algorithms for non-myopic value of information in graphical models", "author": ["Y. Radovilsky", "G. Shattah", "S.E. Shimony"], "venue": "In IEEE International Conference on Systems, Man and Cybernetics (SMC),", "citeRegEx": "Radovilsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Radovilsky et al\\.", "year": 2006}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society, 58, 527\u2013535.", "citeRegEx": "Robbins,? 1952", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Active learning of partially hidden Markov models for information extraction", "author": ["T. Scheffer", "C. Decomain", "S. Wrobel"], "venue": "In ECML/PKDD Workshop on Instance Selection", "citeRegEx": "Scheffer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Scheffer et al\\.", "year": 2001}, {"title": "Global a-optimal robot exploration in slam", "author": ["R. Sim", "N. Roy"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Sim and Roy,? \\Q2005\\E", "shortCiteRegEx": "Sim and Roy", "year": 2005}, {"title": "Efficient planning of informative paths for multiple robots", "author": ["A. Singh", "A. Krause", "C. Guestrin", "W.J. Kaiser", "M.A. Batalin"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Singh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2007}, {"title": "Intelligent light control using sensor networks", "author": ["V. Singhvi", "A. Krause", "C. Guestrin", "J. Garrett", "H. Matthews"], "venue": "In Proc. of the 3rd ACM Conference on Embedded Networked Sensor Systems (SenSys)", "citeRegEx": "Singhvi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Singhvi et al\\.", "year": 2005}, {"title": "Power efficient organization of wireless sensor networks", "author": ["S. Slijepcevic", "M. Potkonjak"], "venue": "In ICC", "citeRegEx": "Slijepcevic and Potkonjak,? \\Q2001\\E", "shortCiteRegEx": "Slijepcevic and Potkonjak", "year": 2001}, {"title": "The optimal control of partially observable Markov decision processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Information gain-based exploration using raoblackwellized particle filters", "author": ["C. Stachniss", "G. Grisetti", "W. Burgard"], "venue": "In Robotics Science and Systems (RSS)", "citeRegEx": "Stachniss et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Stachniss et al\\.", "year": 2005}, {"title": "Active learning for parameter estimation in Bayesian networks", "author": ["S. Tong", "D. Koller"], "venue": null, "citeRegEx": "Tong and Koller,? \\Q2001\\E", "shortCiteRegEx": "Tong and Koller", "year": 2001}, {"title": "Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm", "author": ["P.D. Turney"], "venue": "Journal of Artificial Intelligence Research, 2, 369\u2013409.", "citeRegEx": "Turney,? 1995", "shortCiteRegEx": "Turney", "year": 1995}, {"title": "Selective evidence gathering for diagnostic belief networks", "author": ["L. van der Gaag", "M. Wessels"], "venue": "AISB Quart.,", "citeRegEx": "Gaag and Wessels,? \\Q1993\\E", "shortCiteRegEx": "Gaag and Wessels", "year": 1993}, {"title": "Information-driven dynamic sensor collaboration for tracking applications", "author": ["F. Zhao", "J. Shin", "J. Reich"], "venue": "IEEE Signal Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 19, "context": "In probabilistic reasoning, where one can choose among several possible but expensive observations, it is often a central issue to decide which variables to observe in order to most effectively increase the expected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997; Lindley, 1956).", "startOffset": 224, "endOffset": 305}, {"referenceID": 33, "context": "In probabilistic reasoning, where one can choose among several possible but expensive observations, it is often a central issue to decide which variables to observe in order to most effectively increase the expected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997; Lindley, 1956).", "startOffset": 224, "endOffset": 305}, {"referenceID": 52, "context": "In a medical expert system, for example, multiple tests are available, and each test has a different cost (Turney, 1995; Heckerman, Horvitz, & Middleton, 1993).", "startOffset": 106, "endOffset": 159}, {"referenceID": 3, "context": "Many researchers have suggested the use of myopic (greedy) approaches to select observations (Scheffer, Decomain, & Wrobel, 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Bayer-Zubek, 2004; Kapoor, Horvitz, & Basu, 2007).", "startOffset": 93, "endOffset": 233}, {"referenceID": 19, "context": "This general setup of selecting observations goes back in the decision analysis literature to the notion of value of information by Howard (1966) and in the statistical literature to the notion of Bayesian Experimental Design by Lindley (1956).", "startOffset": 132, "endOffset": 146}, {"referenceID": 19, "context": "This general setup of selecting observations goes back in the decision analysis literature to the notion of value of information by Howard (1966) and in the statistical literature to the notion of Bayesian Experimental Design by Lindley (1956). In this paper, we refer to the Problems (1) and (2) as the problems of optimizing value of information.", "startOffset": 132, "endOffset": 244}, {"referenceID": 33, "context": "The notion of value of information is widely used (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and is formalized, e.", "startOffset": 50, "endOffset": 109}, {"referenceID": 18, "context": "The notion of value of information is widely used (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and is formalized, e.", "startOffset": 50, "endOffset": 109}, {"referenceID": 37, "context": ", the references by Papadimitriou (1995) or Littman, Goldsmith, and Mundhenk (1998).", "startOffset": 20, "endOffset": 41}, {"referenceID": 37, "context": ", the references by Papadimitriou (1995) or Littman, Goldsmith, and Mundhenk (1998). The class NP contains decision problems which have polynomial-time verifiable proofs.", "startOffset": 20, "endOffset": 84}, {"referenceID": 34, "context": "NP has been introduced and found to be a natural class for modeling AI planning problems in the seminal work by Littman et al. (1998). As an example, the MAP assignment problem is NP-complete for general graphical models, as shown by Park and Darwiche (2004).", "startOffset": 112, "endOffset": 134}, {"referenceID": 34, "context": "NP has been introduced and found to be a natural class for modeling AI planning problems in the seminal work by Littman et al. (1998). As an example, the MAP assignment problem is NP-complete for general graphical models, as shown by Park and Darwiche (2004). The complexity classes satisfy the following set of inclusions (where the inclusions are assumed, but not known to be strict):", "startOffset": 112, "endOffset": 259}, {"referenceID": 34, "context": "The reductions are inspired by the works of Littman et al. (1998) and Park and Darwiche (2004), but require the development of novel techniques, such as new reductions of Boolean formulae to Naive Bayes and polytree graphical models associated with appropriate reward functions, ensuring that observation selections lead to feasible assignments to the Boolean formulae.", "startOffset": 44, "endOffset": 66}, {"referenceID": 34, "context": "The reductions are inspired by the works of Littman et al. (1998) and Park and Darwiche (2004), but require the development of novel techniques, such as new reductions of Boolean formulae to Naive Bayes and polytree graphical models associated with appropriate reward functions, ensuring that observation selections lead to feasible assignments to the Boolean formulae.", "startOffset": 44, "endOffset": 95}, {"referenceID": 11, "context": "1 Temperature Time Series The first data set consists of temperature time series collected from a sensor network deployed at Intel Research Berkeley (Deshpande et al., 2004) as described in our running example.", "startOffset": 149, "endOffset": 173}, {"referenceID": 9, "context": "3 Part-of-Speech Tagging In our third experiment, we investigated the structured classification task of part-of-speech (POS) tagging (CoNLL, 2003).", "startOffset": 133, "endOffset": 146}, {"referenceID": 11, "context": "This is a very natural assumption for sensor networks (Deshpande et al., 2004).", "startOffset": 54, "endOffset": 78}, {"referenceID": 26, "context": "This increase in complexity can be avoided using a sampling approximation: Hoeffding\u2019s inequality can be used to derive polynomial bounds on sample complexity for approximating the value of information up to arbitrarily small additive error \u03b5, similarly as done in the approach of Krause and Guestrin (2005a)4.", "startOffset": 281, "endOffset": 309}, {"referenceID": 6, "context": "Approximate inference algorithms such as the algorithm proposed by Boyen and Koller (1998) provide a viable way around this problem.", "startOffset": 67, "endOffset": 91}, {"referenceID": 47, "context": "2 Proof of Concept Study on Real Deployment In the work by Singhvi et al. (2005), we presented an approach for optimizing light control in buildings, with the purpose of satisfying building occupants\u2019 preferences about lighting conditions, and simultaneously minimizing energy consumption.", "startOffset": 59, "endOffset": 81}, {"referenceID": 33, "context": "In its general form, Bayesian experimental design was pioneered by Lindley (1956). The users encode their preferences in a utility function U(P (\u0398), \u03b8?), where the first argument, P (\u0398), is a distribution over states of the world (i.", "startOffset": 67, "endOffset": 82}, {"referenceID": 33, "context": "2 Value of Information in Graphical Models Decision-theoretic value of information has been frequently used for principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984).", "startOffset": 145, "endOffset": 204}, {"referenceID": 18, "context": "2 Value of Information in Graphical Models Decision-theoretic value of information has been frequently used for principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984).", "startOffset": 145, "endOffset": 204}, {"referenceID": 44, "context": "Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) suggested myopic, i.", "startOffset": 20, "endOffset": 118}, {"referenceID": 22, "context": "Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) suggested myopic, i.", "startOffset": 20, "endOffset": 118}, {"referenceID": 17, "context": ", Howard, 1966; Lindley, 1956; Heckerman et al., 1993), and popularized in decision analysis in the context of influence diagrams (Howard & Matheson, 1984). In a sense, value of information problems are special cases of Bayesian experimental design problems, where the prior distribution has a particular structure, typically given by a graphical model as considered in this paper. Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen, 1997; Kapoor et al., 2007) suggested myopic, i.e., greedy approaches for selectively gathering evidence in graphical models, as considered in this paper, which, unlike the algorithms presented in this paper. While these algorithms are applicable to much more general graphical models, they do not have theoretical guarantees. Heckerman et al. (1993) propose a method to compute the maximum expected utility for specific sets of observations.", "startOffset": 31, "endOffset": 824}, {"referenceID": 5, "context": "Bilgic and Getoor (2007) present a branch and bound approach towards exactly optimizing value of information in more complex probabilistic models.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Bilgic and Getoor (2007) present a branch and bound approach towards exactly optimizing value of information in more complex probabilistic models. In contrast to the algorithms described in this paper however, their approach has running time that is worst-case exponential. Munie and Shoham (2008) present algorithms and hardness results for optimizing a special class of value of information objective functions that are motivated by optimal educational testing problems.", "startOffset": 0, "endOffset": 298}, {"referenceID": 5, "context": "Bilgic and Getoor (2007) present a branch and bound approach towards exactly optimizing value of information in more complex probabilistic models. In contrast to the algorithms described in this paper however, their approach has running time that is worst-case exponential. Munie and Shoham (2008) present algorithms and hardness results for optimizing a special class of value of information objective functions that are motivated by optimal educational testing problems. Their algorithms apply to a different class of graphical models than chains, and only apply for specific objective functions, rather than general local reward functions as considered in this paper. Radovilsky, Shattah, and Shimony (2006) extended the previous version of our paper (Krause & Guestrin, 2005a) to obtain approximation algorithms with guarantees in the case of noisy observations (i.", "startOffset": 0, "endOffset": 711}, {"referenceID": 43, "context": "In the classical k-armed bandit problem, as formalized by Robbins (1952), a slot machine is given", "startOffset": 58, "endOffset": 73}, {"referenceID": 16, "context": "A celebrated result by Gittins and Jones (1979) shows that for a fixed number of draws, an optimal strategy can be computed in polynomial time, using a dynamic programming based algorithm.", "startOffset": 23, "endOffset": 48}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information.", "startOffset": 0, "endOffset": 500}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem.", "startOffset": 0, "endOffset": 986}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM).", "startOffset": 0, "endOffset": 1275}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM).", "startOffset": 0, "endOffset": 1295}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM).", "startOffset": 0, "endOffset": 1321}, {"referenceID": 3, "context": "Bayer-Zubek (2004) for example proposed a heuristic method based on the Markov Decision Process framework. However, her approach makes approximations without theoretical guarantees. The problem of optimizing decision theoretic value of information can be naturally formalized as a (finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik, 1973). Hence, in principle, algorithms for planning in POMDPs, such as the anytime algorithm by Pineau, Gordon, and Thrun (2006), can be employed for optimizing value of information. Unfortunately, the state space grows exponentially with the number of variables that are considered in the selection problem. In addition, the complexity of planning in POMDPs grows exponentially in the cardinality of the state space, hence doubly-exponentially in the number of variables for selection. This steep increase in complexity makes application of black-box POMDP solvers infeasible. Recently, Ji, Parr, and Carin (2007) demonstrated the use of POMDP planning on a multi-sensor scheduling problem. While presenting promising empirical results, their approach however uses approximate POMDP planning techniques without theoretical guarantees. In the robotics literature, Stachniss, Grisetti, and Burgard (2005), Sim and Roy (2005) and Kollar and Roy (2008) have presented approaches to information gathering in the context of Simultaneous Localization and Mapping (SLAM). None of these approaches however provide guarantees about the quality of the obtained solutions. Singh, Krause, Guestrin, Kaiser, and Batalin (2007) present an approximation algorithm with theoretical guarantees for the problem of planning an informative path for environmental monitoring using Gaussian Process models.", "startOffset": 0, "endOffset": 1585}, {"referenceID": 48, "context": "The problem of deciding when to selectively turn on sensors in order to conserve power was first discussed by Slijepcevic and Potkonjak (2001) and Zhao, Shin, and Reich (2002).", "startOffset": 110, "endOffset": 143}, {"referenceID": 48, "context": "The problem of deciding when to selectively turn on sensors in order to conserve power was first discussed by Slijepcevic and Potkonjak (2001) and Zhao, Shin, and Reich (2002). Typically, it is assumed that sensors are associated with a fixed sensing region, and a spatial", "startOffset": 110, "endOffset": 176}, {"referenceID": 54, "context": "Zhao et al. (2002) proposed heuristics for selectively querying nodes in a sensor network in order to reduce the entropy of the prediction.", "startOffset": 0, "endOffset": 19}, {"referenceID": 41, "context": "6 Relationship to Machine Learning Decision Trees (Quinlan, 1986) popularized the value of information as a criterion for creating conditional plans.", "startOffset": 50, "endOffset": 65}, {"referenceID": 30, "context": "The authors showed (Krause & Guestrin, 2005a; Krause et al., 2007; Krause, Leskovec, Guestrin, VanBriesen, & Faloutsos, 2008) that a large number of practical objective functions satisfy an intuitive diminishing returns property: Adding a new observation helps more if we have few observations so far, and less if we have already made many observations.", "startOffset": 19, "endOffset": 125}, {"referenceID": 27, "context": "7 Previous Work by the Authors A previous version of this paper appeared in the work by Krause and Guestrin (2005b). Some of the contents of Section 7 appeared as part of the work by Singhvi et al.", "startOffset": 88, "endOffset": 116}, {"referenceID": 27, "context": "7 Previous Work by the Authors A previous version of this paper appeared in the work by Krause and Guestrin (2005b). Some of the contents of Section 7 appeared as part of the work by Singhvi et al. (2005). The present version is much extended, with new algorithmic and hardness results and more detailed discussions.", "startOffset": 88, "endOffset": 205}, {"referenceID": 27, "context": "Recent results by Krause and Guestrin (2005a), Radovilsky et al.", "startOffset": 18, "endOffset": 46}, {"referenceID": 27, "context": "Recent results by Krause and Guestrin (2005a), Radovilsky et al. (2006) and Krause et al.", "startOffset": 18, "endOffset": 72}, {"referenceID": 27, "context": "Recent results by Krause and Guestrin (2005a), Radovilsky et al. (2006) and Krause et al. (2007) show that this is the case for interesting classes of value of information problems.", "startOffset": 18, "endOffset": 97}], "year": 2009, "abstractText": "Many real-world decision making tasks require us to choose among several expensive observations. In a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. In medical decision making tasks, one needs to select which tests to administer before deciding on the most effective treatment. It has been general practice to use heuristic-guided procedures for selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. For example, our algorithms allow to optimally label hidden variables in Hidden Markov Models (HMMs). We provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan. Furthermore we prove a surprising result: In most graphical models tasks, if one designs an efficient algorithm for chain graphs, such as HMMs, this procedure can be generalized to polytree graphical models. We prove that the optimizing value of information is NP-hard even for polytrees. It also follows from our results that just computing decision theoretic value of information objective functions, which are commonly used in practice, is a #P-complete problem even on Naive Bayes models (a simple special case of polytrees). In addition, we consider several extensions, such as using our algorithms for scheduling observation selection for multiple sensors. We demonstrate the effectiveness of our approach on several real-world datasets, including a prototype sensor network deployment for energy conservation in buildings.", "creator": "TeX"}}}