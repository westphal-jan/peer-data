{"id": "1401.3493", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Predicting the Performance of IDA* using Conditional Distributions", "abstract": "korf, jeremy reid, and max edelkamp intentionally introduced this a mathematical formula to predict otherwise the number of independent nodes ida * will expand on a single consistent iteration for a given consistent heuristic, and experimentally demonstrated that it always could simply make very unusually accurate predictions. in combining this paper subsequently we show shown that, more in addition primarily to requiring the gibbs heuristic to be very consistent, proving their computational formulas such predictions are accurate only if at predicted levels high of the corresponding brute - force minimum search feature tree where the actual heuristic sampling values obey precisely the unconditional distribution that was they repeatedly defined and sometimes then used unsuccessfully in running their formula. we then automatically propose itself a new generic formula which that works well regularly without these requirements, i. e., : it can make increasingly accurate predictions of string ida * \u00d7 s performance guidelines for efficiently inconsistent reliable heuristics simulations and if evaluating the expected heuristic values in any valid level consistency don't simply obey the unconditional distribution. clearly in order hence to achieve this we please introduce together the conditional gibbs distribution of heuristic values which is a generalization of their unconditional heuristic distribution. we also provide extensions of our constraint formula elements that handle individual confidence start states and the binary augmentation of network ida * with local bidirectional pathmax ( then bpmx ), creating a technique for rapidly propagating heuristic distribution values when inconsistent heuristics are instead used. consistent experimental results continuously demonstrate confirm the accuracy of using our new method design and show all its variations.", "histories": [["v1", "Wed, 15 Jan 2014 05:41:44 GMT  (345kb)", "http://arxiv.org/abs/1401.3493v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["uzi zahavi", "ariel felner", "neil burch", "robert c holte"], "accepted": false, "id": "1401.3493"}, "pdf": {"name": "1401.3493.pdf", "metadata": {"source": "CRF", "title": "Predicting the Performance of IDA* using Conditional Distributions", "authors": ["Uzi Zahavi", "Ariel Felner", "Neil Burch", "Robert C. Holte"], "emails": ["zahaviu@cs.biu.ac.il", "felner@bgu.ac.il", "burch@cs.ualberta.ca", "holte@cs.ualberta.ca"], "sections": [{"heading": "1. Introduction and Overview", "text": "Heuristic search algorithms such as A* (Hart, Nilsson, & Raphael, 1968) and IDA* (Korf, 1985) are guided by the cost function f(n) = g(n)+h(n), where g(n) is the actual distance from the start state to state n and h(n) is a heuristic function estimating the cost from n to the nearest goal state. A heuristic h is admissible if h(n) \u2264 dist(n, goal) for every state n and goal state goal, where dist(n,m) is the cost of a least-cost path from n to m. If h(n) is admissible, i.e. always returns a lower bound estimate of the optimal cost, these algorithms are guaranteed to find an optimal path from the start state to a goal state if one exists.\nc\u00a92010 AI Access Foundation. All rights reserved.\nAn important question to ask is how many nodes will be expanded by these algorithms to solve a given problem. A major advance in answering this question was the work done by Korf, Reid, and Edelkamp which introduced a formula to predict the number of nodes IDA* will expand (Korf & Reid, 1998; Korf, Reid, & Edelkamp, 2001). These papers, the formula they present, and the predictions it makes, will all be referred to as KRE in this paper. Prior to KRE, the standard method for comparing two heuristic functions was to compare their average values, with preference being given to the heuristic with the larger average (Korf, 1997; Korf & Felner, 2002; Felner, Korf, Meshulam, & Holte, 2007). KRE made a substantial improvement on this by characterizing the quality of a heuristic function by the distribution of its values. They then developed the KRE formula based on the heuristic distribution to predict the number of nodes expanded by IDA* when it is searching with a specific heuristic and cost threshold. Finally, they compared the predictions of their formula to the actual number of nodes expanded by IDA* for different thresholds on several benchmark search spaces and showed that it gave virtually perfect predictions. This was a major advance in the analysis of search algorithms and heuristics.\nDespite its impressive results, the KRE formula has two main shortcomings. The first is that KRE assumes that in addition to being admissible the given heuristic is also consistent. A heuristic h is consistent if for every pair of states, m and n, h(m) \u2212 h(n) \u2264 dist(m,n).1 When the heuristic is consistent, the heuristic values of a node\u2019s children are thus constrained to be similar to the heuristic value of the node. A heuristic is inconsistent if it is not consistent, i.e. if for some pair of nodes m and n, h(m)\u2212 h(n) > dist(m,n). Inconsistency allows a node\u2019s children to have heuristic values that are arbitrarily larger or smaller than the node\u2019s own heuristic value. While the term inconsistency has a negative connotation as something to be avoided, recent studies have shown that inconsistent heuristics are easy to define in many search applications and can produce substantial performance improvements (Felner, Zahavi, Schaeffer, & Holte, 2005; Zahavi, Felner, Schaeffer, & Sturtevant, 2007; Zahavi, Felner, Holte, & Schaeffer, 2008). For this reason, it is important to extend the KRE formula to accurately predict IDA*\u2019s performance on inconsistent heuristics, as such heuristics are likely to become increasingly important in future applications.\nThe second shortcoming of the KRE formula is that it works well only at levels of the search tree where the heuristic distribution follows the equilibrium distribution (defined below in Section 3.1.2). This always holds at sufficiently deep levels of the search tree, where the heuristic values converge to the equilibrium distribution. In addition, it will hold at all levels when the heuristic values of the set of start states is distributed according to the equilibrium distribution. However, as will be shown below (in Section 3.2.2) the KRE formula can be very inaccurate at depths of practical interest on single start states and on large sets of start states whose values are not distributed according to the equilibrium distribution. In such cases, the heuristic values at the levels of the search tree that are actually examined by IDA* will not obey the equilibrium distribution and applying KRE to these cases will result in inaccurate predictions.\nThe main objective of this paper is to develop a formula to accurately predict the number of nodes IDA* will expand, for a given cost threshold, for any given heuristic and set of start states, including those not currently covered by KRE. To do this we first extend KRE\u2019s idea\n1. This is a general definition for any graph. In the case of undirected graphs we can write the consistency definition as |h(m)\u2212 h(n)| \u2264 dist(m,n).\nof a heuristic distribution, which is unconditional, to a conditional distribution, in which the probability of a specific heuristic value is not constant, as in KRE, but is conditioned on certain local properties of the search space. Our conditional distribution provides more insights about the behavior of the heuristic values during search because it is informed about when (in what context in the search tree) a specific heuristic value will be produced. This allows for a better study of heuristic behavior.\nBased on the conditional distribution we develop a new formula, CDP (Conditional Distribution Prediction), that predicts IDA*\u2019s performance on any set of start states (regardless of how their heuristic values are distributed) and for any desired depth (not necessarily large) whether the heuristic is consistent or not. CDP has a recursive structure and information about the number of nodes is propagated from the root to the leaves of the search tree. In all of our experiments CDP\u2019s predictions are at least as accurate as KRE\u2019s, and CDP is much more accurate for inconsistent heuristics or sets of start states that have non-equilibrium heuristic distributions. In its basic form, CDP is not particularly accurate on single start states. We describe a simple extension that improves its accuracy in this setting. Finally, we adapt CDP to make predictions when IDA* is augmented with the bidirectional pathmax method (BPMX) (Felner et al., 2005). When inconsistent heuristics are being used, BPMX is a useful addition to IDA*. It prunes many subtrees that would otherwise be explored, thereby substantially reducing the number of nodes IDA* expands.\nThroughout the paper we provide experimental results demonstrating the accuracy of CDP in all of the above scenarios using the same two benchmark domains used in KRE \u2013 the sliding-tile puzzle and Rubik\u2019s Cube.\nFor simplicity of discussion, we assume in this paper that all edges cost 1. This is true for many problem domains. The generalization of the ideas to the case of variable edge costs is straightforward, although their practical implementation introduces some additional challenges (briefly described in Section 11.2).\nThe paper is organized as follows. Section 2 presents background material. Section 3 derives the KRE formula from first principles and discusses its limitations. In Section 4, our notion of the conditional distribution of heuristic values is presented. Our new formula, CDP, is presented in Section 4.2. Section 5 discusses a subtle but important way in which our experiments differ from KRE\u2019s. Experimental results are presented in Sections 6 and 7. The extension of the CDP formula to better handle single start states is presented in Section 8. Section 9 proposes a technique, based on CDP, for estimating upper and lower bounds on the number of nodes IDA* can expand for a given unconditional distribution. Section 10 presents an extension of CDP for predicting the performance of IDA* when BPMX is applied. Related work is discussed in Section 11, and conclusions and suggestions for future work are given in Section 12. A preliminary version of this paper appeared (Zahavi, Felner, Burch, & Holte, 2008)."}, {"heading": "2. Background", "text": "Two application domains were used by KRE to demonstrate the accuracy of their formula. In our experiments we use exactly the same domains. In this section we describe them as well as the search algorithm and the different heuristic functions that are used in our experiments."}, {"heading": "2.1 Problem Domains", "text": "Two of the classic examples in the AI literature of a single-agent pathfinding problems are Rubik\u2019s Cube and the sliding-tile puzzle."}, {"heading": "2.1.1 Rubik\u2019s Cube", "text": "Rubik\u2019s Cube was invented in 1974 by Erno Rubik of Hungary. The standard version consists of a 3\u00d73\u00d73 cube (Figure 1), with different colored stickers on each of the exposed squares of the sub-cubes, or cubies. There are 20 movable cubies and 6 stable cubies in the center of each face. The movable cubies can be divided into eight corner cubies, with three faces each, and twelve edge cubies, with two faces each. Corner cubies can only move among corner positions, and edge cubies can only move among edge positions.\nEach one of the 6 faces of the cube can be rotated 90, 180, or 270 degrees relative to the rest of the cube. This results in 18 possible moves for each state. Since twisting the same face twice in a row is redundant, the branching factor after the first move can be reduced to 15. In addition, movements of opposite faces are independent. For example, twisting the left face and then the right face leads to the same state as performing the same moves in the opposite order. Pruning redundant moves results in a search tree with an asymptotic branching factor of about 13.34847 (Korf, 1997).\nIn the goal state, all the squares on each side of the cube are the same color. The puzzle is scrambled by making a number of random moves, and the task is to restore the cube to its original unscrambled state. There are about 4\u00d7 1019 different reachable states."}, {"heading": "2.1.2 The Sliding-tile Puzzles", "text": "The sliding-tile puzzle consists of a square frame containing a set of numbered square tiles, and an empty position called the blank. The legal operators are to slide any tile that is horizontally or vertically adjacent to the blank into the blank position. The problem is to rearrange the tiles from some random initial configuration into a particular desired goal configuration. The state space grows exponentially in size as the number of tiles increases, and it has been shown that finding optimal solutions to the sliding-tile problem is NPcomplete (Ratner & Warmuth, 1986). The two most common versions of the sliding-tile puzzle are the 3\u00d7 3 8-puzzle, and the 4\u00d7 4 15-puzzle. The 8-puzzle contains 9!/2 (181,440)\nreachable states, and the 15-puzzle contains about 1013 reachable states. The goal states of these puzzles are shown in Figure 2.\nThe classic heuristic function for the sliding-tile puzzles is called the Manhattan Distance. It is computed by counting the number of grid units that each tile is displaced from its goal position, and summing these values over all tiles, excluding the blank. Since each tile must move at least its Manhattan Distance to its goal position, and each move changes the location of one tile by just one grid unit, the Manhattan Distance is a lower bound of the minimum number of moves needed to solve a problem instance."}, {"heading": "2.2 Iterative Deepening A*", "text": "Iterative deepening A* (IDA*) (Korf, 1985) performs a series of depth-first searches, increasing a cost threshold d each time. In the depth-first search, all nodes n with f(n) \u2264 d are expanded. Threshold d is initially set to h(s), where s is the start node. If a goal is found using the current threshold, the search ends successfully. Otherwise, IDA* proceeds to the next iteration by increasing d to the minimum f value that exceeded d in the previous iteration."}, {"heading": "2.3 Pattern Databases (PDBs)", "text": "A powerful approach for obtaining admissible heuristics is to create a simplified version, or abstraction, of the given state space and then to use exact distances in the abstract space as estimates of the distances in the original state space. The type of abstractions we use in this paper for the sliding-tile puzzles are illustrated in Figure 3. The left side of the figure shows a 15-puzzle state S and the goal state. The right side shows the corresponding abstract states, which are defined by erasing the numbers on all the tiles except for 2, 3, 6 and 7. To estimate the distance from S to the goal state in the 15-puzzle, we calculate the exact distance from the abstract state corresponding to S to the abstract goal state.\nA pattern database (PDB) is a lookup table that stores the distance to the abstract goal of every abstract state (or \u201cpattern\u201d) (Culberson & Schaeffer, 1994, 1998). A PDB is built by running a breadth-first search2 backwards from the abstract goal until the whole abstract space is spanned. To compute h(s) for a state s in the original space, s is mapped to the corresponding abstract state p and the distance-to-goal for p is looked up in the PDB.\n2. This description assumes all operators have the same cost. This technique can be easily extended to cases where operators have different costs.\nFor example, a PDB for the 15-puzzle based on tiles 2, 3, 6, and 7 would contain an entry for every possible way of placing those four tiles and the blank in the 16 puzzle positions. Such a PDB could be implemented as a 5-dimensional array, PDB, with the array indexes being the locations of the blank and tiles 2, 3, 6, and 7 respectively. The lookup for state S shown in Figure 3 would then be PDB[0][8][12][13][14] (the blank is in position 0, tile 2 is in position 8, tile 3 is in position 12, etc.). In this paper, accessing the PDB for a state S as just described will be referred to as a regular lookup, and the heuristic value returned by a regular lookup will be referred to as a regular heuristic value.\nPattern databases have proven very useful for finding lower bounds for combinatorial puzzles (Korf, 1997; Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner, Korf, & Hanan, 2004; Felner et al., 2007). Furthermore, they have also proven to be useful for other search problems, e.g., multiple sequence alignment (McNaughton, Lu, Schaeffer, & Szafron, 2002; Zhou & Hansen, 2004) and planning (Edelkamp, 2001a)."}, {"heading": "2.4 Geometric Symmetries", "text": "It is common practice to exploit special properties of a state space to enable additional heuristic evaluations. In particular, additional PDB lookups can be performed given a single PDB. For example, consider Rubik\u2019s Cube and suppose we had a PDB based on the positions of the cubies that have a yellow face (the positions of the other cubies don\u2019t matter). Reflecting and rotating the puzzle will enable similar lookups for cubies with a different color (e.g., green, red, etc.) since the puzzle is perfectly symmetric with respect to color. Thus, there are 24 symmetric lookups for such a PDB and different heuristic values are obtained for each of these lookups in the same PDB. All these heuristic values are admissible for any given state of the puzzle.\nAs another example, consider the sliding-tile puzzle. A line of symmetry is the main diagonal (assuming the goal location of the blank is in the upper left corner). Any configuration of tiles can be reflected about the main diagonal and the reflected configuration shares the same attributes as the original one. Such reflections are usually used when using PDBs for the sliding-tile puzzle (Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner et al., 2004, 2007) and can be looked up from the same PDB."}, {"heading": "2.5 Methods for Creating Inconsistent Heuristics", "text": "With consistent heuristics, the difference between the heuristic value of neighboring nodes is constrained to be less than or equal to the cost of the connecting edge. For inconsistent heuristics, there is no constraint on the difference between heuristic values of neighboring nodes and it can be much larger than the cost of the edge connecting them.\nThe KRE formula is designed to work with consistent heuristics and therefore the KRE papers report on experiments done with consistent heuristics only. By contrast, our new formula, CDP, works for all types of heuristics including inconsistent heuristics. Therefore, in this paper, in addition to the usual consistent heuristics such as regular PDB lookups or Manhattan Distance we also experiment with inconsistent heuristics. We have previously described several methods for producing inconsistent heuristics (Zahavi et al., 2007). Two inconsistent heuristics that are used in the experiments below are the Random selection of heuristics and Dual evaluations.\n\u2022 Random selection of heuristics: A well-known method for overcoming the pitfalls of a given heuristic is to employ several heuristics and use their maximum value (Holte, Felner, Newton, Meshulam, & Furcy, 2006). For example, multiple heuristics can be based on domain-specific geometric symmetries such as the ones described above. When using geometric symmetries there are no additional storage costs associated with these extra evaluations, even when these evaluations are based on PDBs.\nAlthough using multiple heuristics results in an improved heuristic value, and therefore is likely to reduce the number of nodes expanded in finding a solution, it also increases the time required to calculate the heuristic values of the nodes, which might increase the overall running time of the search. Instead of using all the available heuristics for every heuristic calculation, one could instead choose to consult only one of them, with the selection being made either randomly or systematically. Because only one heuristic is consulted at each node, the time-per-node is virtually the same as if only one heuristic was available. Even if each of the individual heuristics is consistent, the heuristic values that are actually used are inconsistent because different heuristics are consulted at different nodes. We showed (Zahavi et al., 2007) that this inconsistency generally reduces the number of expanded nodes compared to using the same heuristic for all the nodes and it is almost as low as if the maximum over all the heuristics had been computed at every node. For Rubik\u2019s Cube, we randomly chose one of the 24 different lookups of the same PDB that arise from the 24 lines of symmetry of this cube.\n\u2022 Dual evaluation: In permutation state spaces such as Rubik\u2019s Cube, for each state s there exists a dual state sd located the same distance from the goal as s (Felner\net al., 2005; Zahavi, Felner, Holte, & Schaeffer, 2006; Zahavi et al., 2008). Therefore, any admissible heuristic applied to sd is also admissible for s. The puzzles studied in this paper are permutation state spaces, and the dual of a state in these puzzles is calculated by reversing the role of locations and objects: the \u201cregular\u201d state uses a set of objects indexed by their current location, while the \u201cdual\u201d state has a set of locations indexed by the objects they contain. When using PDBs, a dual lookup is to look up sd in the PDB. Performing only regular PDB lookups for the states generated during the search produces consistent values. However, the values produced by performing the dual lookup can be inconsistent because the identity of the objects being queried can change dramatically between two consecutive lookups. Due to its diversity, the dual heuristic was shown to be preferable to a regular heuristic (Zahavi et al., 2007). An exact definition and explanations about the dual lookup is provided in the original papers (Felner et al., 2005; Zahavi et al., 2006, 2008).\nIt is important to note that all three PDB lookups (regular, dual, and random) consult the same PDB. Thus, they need the same amount of memory and share the same overall distribution of heuristic values (Zahavi et al., 2007).\n3. The KRE Formula and its Limitations\nThis section begins with a short derivation of the KRE formula for state spaces in which all state transitions have a cost of 1. KRE describe how this can be generalized to account for variable edge costs (Korf et al., 2001).\n3.1 The KRE formula\nFor a given state s and IDA* threshold d, KRE aims to predict N(s, d), the number of nodes that IDA* will expand if it uses s as its start state and does a complete search with an IDA* threshold of d (i.e., searches to depth d and does not terminate its search if the goal is encountered). This can be written as\nN(s, d) =\nd \u2211\ni=0\nNi(s, d) (1)\nwhere Ni(s, d) is the number of nodes expanded by IDA* at level i when its threshold is d. One way to decompose Ni(s, d) is as the product of two terms\nNi(s, d) = Ni(s) \u00b7 Pex(s, d, i) (2)\nwhere Ni(s) is the number of nodes in level i of BFS d s , the brute-force search tree (i.e., the tree created by breadth first search without any heuristic pruning) of depth d rooted at start state s, and Pex(s, d, i) is the percentage of the nodes in level i of BFS d s that are expanded by IDA* when its threshold is d. In KRE, Ni(s) is written as Ni, i.e., without the dependence on the start state s. This is perfectly correct for state spaces with a uniform branching factor b, because Ni(s) in such cases is simply bi. For state spaces with a non-uniform but \u201cregular\u201d branching structure,\nKRE showed how Ni could be computed exactly using recurrence equations that are independent of s. However, the base cases of the recurrences in KRE do depend on s so their using Ni instead of Ni(s) is reasonable but not strictly correct."}, {"heading": "3.1.1 Conditions for Node Expansion in IDA*", "text": "To understand how Pex(s, d, i) is treated in KRE, it is necessary to reflect on the conditions required for node expansion. A node n in level i of BFSds will be expanded by IDA* if it satisfies two conditions:\n1. f(n) = g(n) + h(n) must be less than or equal to d. When all edges have a unit cost, g(n) = i and this condition is equivalent to h(n) \u2264 d \u2212 i. We call nodes that satisfy this condition potential nodes because they have the potential to be expanded.\n2. n must be generated by IDA*, i.e., its parent (at level i \u2212 1) must be expanded by IDA*.\nKRE restricted its analysis to heuristics that are consistent and proved that in this case the second condition is implied by the first condition. In other words, when the given heuristic is consistent, the nodes expanded by IDA* at level i of BFSds for threshold d are exactly the set of potential nodes at level i.3 This observation allows Equation 2 to be rewritten as\nNi(s, d) = Ni(s) \u00b7 PPOTENTIAL(s, i, d\u2212 i) (3)\nwhere PPOTENTIAL(s, i, v) is defined as the percentage of nodes at level i of BFS d s whose heuristic value is less than or equal to v.\nNote that although PPOTENTIAL(s, i, d\u2212i) = Pex(s, d, i) when the given heuristic is consistent, PPOTENTIAL(s, i, d\u2212 i) overestimates Pex(s, d, i) when the heuristic is inconsistent, sometimes by a very large amount (see Section 3.2.1).\n3.1.2 Approximating PPOTENTIAL(s, i, v)\nKRE use three different approximations to PPOTENTIAL(s, i, v). In KRE\u2019s theoretical analysis PPOTENTIAL(s, i, v) is approximated by the \u201cequilibrium\u201d distribution, which we denote as PEQ(v). It is defined to be \u201cthe probability that a node chosen randomly and uniformly among all nodes at a given depth of the brute-force search tree has heuristic value less than or equal to v, in the limit of large depth\u201d (Korf et al. 2001, p. 208). KRE proved that, in the limit of large d,\nd \u2211\ni=0\nNi(s) \u00b7 PEQ(d\u2212 i)\nwould converge to N(s, d) if the given heuristic is consistent. Their final formula (the KRE formula) is therefore:\n3. See section 3.2.1 below for more discussion of the KRE formula with consistent heuristics.\nN(s, d) = d \u2211\ni=0\nNi(s) \u00b7 PEQ(d\u2212 i) (4)\nKRE contrasted the equilibrium distribution with the \u201coverall\u201d distribution, which is defined as \u201cthe probability that a state chosen randomly and uniformly from all states in the problem has heuristic value less than or equal to v\u201d (p. 207). Unlike the equilibrium distribution, which is defined over a search tree, the overall distribution is a property of the state space. The overall distribution can be directly computed from a pattern database, if just one pattern database is used and each of its entries corresponds to the same number of states in the original state space, or can be approximated, in more complex settings, by computing the heuristic values of a large random sample of states. KRE argued that in Rubik\u2019s Cube the overall distribution for a heuristic defined by a single pattern database is the same as the equilibrium distribution, but that for the sliding-tile puzzles, the two distributions are different.\nThe heuristic used in KRE\u2019s experiments with Rubik\u2019s Cube was defined as the maximum over three pattern databases. For each individual pattern database, the overall distribution was computed exactly. In KRE\u2019s experiments these distributions were combined to approximate PPOTENTIAL(s, i, v) by assuming that the values from the three pattern databases were independent.\nFor the experiments with the sliding-tile puzzles, KRE defined three types of states based on the whether the blank was located in a corner position, an edge position, or an interior position, and approximated PPOTENTIAL(s, i, v) by a weighted combination of the overall distributions of the states of each type. The weights used at level i were the exact percentages of states of the different types at that level.\nIn our experiments we followed KRE precisely and use the overall distribution for individual Rubik\u2019s Cube pattern databases and the weighted overall distribution just described for the sliding-tile puzzles. For simplicity, in the reminder of this paper we use the phrase unconditional heuristic distribution4 and the notation P (v) to refer to the probability that a node has a heuristic less than or equal to v. We let the exact context determine which distribution P (v) actually denotes, whether it is the equilibrium distribution, the overall distribution, or any other approximation of PPOTENTIAL and Pex. Likewise we will use p(v) (lower case p) to denote P (v) \u2212 P (v \u2212 1) (with p(0) = P (0)). p(v) is the probability that a state will have a heuristic value of exactly v according to the distribution P .\n3.2 Limitations of the KRE Formula\nThe KRE formula (Equation 4) has two main shortcomings: (1) its predictions are not accurate if the given heuristic is inconsistent, and (2) even with consistent heuristics its predictions can be inaccurate for individual start states or sets of start states whose heuristic values are not distributed according to the unconditional heuristic distribution, P (v). We now turn to examine each of these in detail."}, {"heading": "3.2.1 Inconsistent Heuristics", "text": "As specifically mentioned in the KRE papers one property required for the KRE analysis is that the heuristic be consistent. This is necessary because the KRE formula aims to count the number of potential nodes at each level in BFSds . With consistent heuristics, the heuristic value of neighboring states never changes by more than the change in the g-value, as illustrated in the left side of Figure 4 (where the number inside a node is its heuristic value). This implies that the f -value of a node\u2019s ancestor is always less than or equal to the f -value of the node (i.e., f is monotone non-decreasing along a path in the search tree). Therefore, it is easy to prove that with consistent heuristics all the ancestors of a potential node are also potential nodes (Korf et al., 2001). Consequently IDA* will expand all and only the potential nodes in BFSds . Hence, a formula such as KRE that aims to count the number of potential nodes in BFSds can be used to predict the number of nodes IDA* will expand when given a consistent heuristic.\nFor inconsistent heuristics this reasoning does not apply. The heuristic values of neighboring states can differ by much more than the cost of the edge that connects them, and thus the f -values along a path in the search tree are not guaranteed to be monotonically non-decreasing. Therefore, the ancestors of a potential node are not guaranteed to be potential nodes themselves, with the consequence that a potential node might never be generated. For example, consider the search tree in the right side of Figure 4. The numbers inside each node show the node\u2019s heuristic value. Assume that the start node is R and that the IDA* threshold is 5 (a node is a potential node if its f -value is less than or equal to 5). There are 3 potential nodes at depth 2 (all with heuristic value 3). Consider the potential node n. The path to it is through node m but node m is not a potential node (f(m) = 1 + 5 = 6 > 5), so it will be generated but not expanded. Therefore, node n will never be generated, preventing IDA* from expanding it. Since the KRE formula counts the number of potential nodes, it will count node n and thus overestimate the number of expanded nodes when an inconsistent heuristic is used.\nThe amount by which KRE overestimates the number of nodes expanded by IDA* with an inconsistent heuristic can be very large. To illustrate this, consider the state space for Rubik\u2019s Cube and a PDB heuristic defined by the locations of 6 (out of 12) of the edge cubies. The regular method for looking up a heuristic value in a PDB produces a consistent heuristic. As discussed in Section 2.5 two alternative PDB lookups that produce inconsistent\n4. \u201cunconditional\u201d to distinguish it from the conditional distribution we introduce in Section 4.1\nheuristics are the dual evaluation and the random selection from multiple heuristics. In Rubik\u2019s Cube there are 24 symmetries and each can be applied to any state to create a new way to perform a PDB lookup for it. Thus, there are 24 heuristics for Rubik\u2019s Cube based on the same PDB and the random-symmetry lookup chooses one of them randomly.\nBecause all three lookups (regular, dual, and random-symmetry) consult the same PDB they have the same distribution of heuristic values, P (v), and therefore KRE will predict that IDA* will expand the same number of nodes regardless of whether a regular, dual, or random-symmetry lookup is done. The experimental results in Table 1 show that a substantially different number of nodes are actually expanded in practice for each of these methods.\nEach row of Table 1 presents results for a specific IDA* threshold (d). Each result is an average over 1, 000 random initial states, each of which is generated by making 180 random moves from the goal state. The \u201cKRE\u201d column shows the KRE prediction based on the unconditional heuristic distribution. The last three columns of Table 1 show the number of nodes IDA* expands when it performs either a regular, dual, or random-symmetry lookup in the PDB. The KRE prediction is within 8% of the actual number of nodes expanded when IDA* uses the regular (consistent) PDB lookup (third column) but it substantially overestimates the number of nodes expanded when IDA* uses the dual or random-symmetry inconsistent lookups in the same PDB (fourth and fifth columns)."}, {"heading": "3.2.2 Sets of Start States Whose Heuristics Values do not Obey the unconditional heuristic distribution", "text": "As explained above, KRE used the unconditional heuristic distribution P (v) and, in their theoretical analysis, proved that its use in the KRE formula would give accurate predictions in the limit of large depth. In fact, accurate predictions will occur as soon as the heuristic distribution at the depth of interest d closely approximates P (v). This happens at large depths by definition but this can happen even at very shallow levels under certain circumstances. The reason that KRE was able to produce extremely accurate predictions in its experiments using the unconditional heuristic distribution P (v) for all depths and all start states is that its experiments report average predictions and performances over\na large number of randomly drawn start states. In the spaces used in KRE\u2019s experiments, the heuristic distribution of a large random set of start states very closely approximated the P (v) distribution they used. This caused heuristic distributions at all levels to closely approximate P (v).\nHowever, if the set of start states does not have its heuristic values distributed according to P (v), as is the case for most non-random sets of start states or a single start state, KRE should not be expected to make good predictions for small depths. In other words, in such cases the unconditional heuristic distribution P (v) is not expected to be a good approximation of Pex(s, d, i).\nConsider the case of a single start state and a consistent heuristic. The distribution of heuristic values in the search tree close to the start state will be highly correlated with the heuristic value of the start state, and therefore will not be the same in search trees with start states having different heuristic values. For example, a great deal of pruning is likely to occur near the top of the search tree for a start state with a large heuristic value, resulting in fewer nodes expanded than for a start state with a small heuristic value. Applying KRE to these two states will produce the same prediction, and therefore be inaccurate for at least one of them, because it uses the same unconditional heuristic distribution P (v) in both cases.\nTable 2 demonstrates this phenomenon on Rubik\u2019s Cube with one regular 6-edge PDB lookup for IDA* threshold d = 12. The \u201cIDA*\u201d column shows the average number of nodes expanded for 1, 000 start states, all with the same heuristic value h for any given row. KRE ignores the heuristic values of the start states and predicts that 8,161,064 nodes will be expanded by IDA* for every start state. The row for d = 12 in Table 1 shows that this is an accurate prediction when performance is averaged over a large random sample of start states, but in Table 2 we see that it is too low for start states with small heuristic values and too high for ones with large heuristic values."}, {"heading": "3.2.3 Convergence of the Heuristic Distributions at Large Depths", "text": "As described above, KRE will make accurate predictions for level i if the nodes at that level actually obey the unconditional heuristic distribution P (v). As i increases, the distribution of heuristic values will start to converge to P (v). The rate of convergence depends upon the state space. It is believed to be fairly slow for the sliding-tile puzzles, but faster for\nRubik\u2019s Cube. If the convergence occurs before the IDA* threshold is reached KRE will provide accurate predictions for any set of start states (including single start states).\nIn order to experimentally test this we repeated the KRE Rubik\u2019s Cube experiment but, in addition to using a large set of random start states, we also looked at the individual performance of two start states, s6, which has a low heuristic value (6), and s11, which has the maximum value for the heuristic used in this experiment (11). As in KRE we used the 8-6-6 heuristic which takes the maximum of 3 different PDBs (one based on all 8 corner cubies and two based on 6 edge cubies each). This heuristic is admissible and consistent. Over the billion random states we sampled to estimate P (v) the maximum value was 11 and the average value was 8.898.\nTable 3 presents the results. The KRE column presents the KRE prediction and the Multiple start states columns presents the actual number of states generated (averaged over a set of random start states) for each IDA* threshold. Both columns are copied from the KRE journal paper (Korf et al., 2001). The Ratio columns of Table 3 shows the value predicted by the KRE formula divided by the actual number of nodes generated. This ratio was found to be very close 1.0 for multiple start states, indicating that KRE\u2019s predictions were very accurate.\nThe results for the two individual start states we tested are shown in the \u201cSingle start state\u201d part of the table. Note that both states are optimally solved at depth 17, but, as in KRE, the search at that depth was run to completion. In both cases the KRE formula was not accurate for small thresholds but the accuracy of the prediction increased as the threshold increased. At threshold d = 17 the KRE prediction was roughly a factor of 2 too small for s6 and about 10% too large for s11. This is a large improvement over the smaller thresholds. These predictions will become even more accurate as depth continues to increase.\nThe reason the predictions improve for larger values of d is that at deeper depths the heuristic distribution within a single level converges to the unconditional heuristic distribution. Using dashed and dotted lines of various types, Figure 5(a) shows the distribution of heuristic values seen in states that are 0, 1, 2 and 4 moves away from s6. The solid line in Figure 5(a) is the unconditional heuristic distribution. The x-axis corresponds to different heuristic values and the y-axis shows the percentage of states at the specified depth with heuristic values less than or equal to each x value. For example for depth 0 (which includes\nthe start state only) only a heuristic value of 6 was seen (leftmost curve). For depth 1, heuristic values of 5, 6 and 7 were seen (second curve from the left), and so on. The figure shows that the heuristic distribution at successive depths converges to the unconditional heuristic distribution (rightmost curve in Figure 5(a)). At depth 17 (not shown), the heuristic distribution is probably quite close to the unconditional heuristic distribution, making the KRE prediction quite accurate even for this single start state.\nFigure 5(b) shows the heuristic distributions for nodes that are 0, 1, 2, and 4 moves away from s11. In this case the unconditional heuristic distribution is to the left of the heuristic distributions for the shallow depths, with the heuristic distribution for depth 0 being the rightmost curve in this figure. Comparing parts (a) and (b) of Figure 5 we see that the convergence to the unconditional heuristic distribution is faster for s11 than for s6, which explains why the KRE prediction in Table 3 is more accurate for s11.\n4. Conditional Distribution and the CDP Formula\nWe now present our new formula CDP (Conditional Distribution Prediction), which overcomes the two shortcomings of KRE described in the previous section. An important feature of CDP is that it extends the unconditional heuristic distribution of heuristic values P (v) used in KRE to be a conditional distribution."}, {"heading": "4.1 Conditional Distribution of Heuristic Values", "text": "The conditional distribution of heuristic values is denoted P (v|context), where the context represents local properties of the search tree in the neighborhood of a node that influence the distribution of heuristic values in the node\u2019s children. Specifically, if Pn(v) is the percentage of node n\u2019s children that have a heuristic value less than or equal to v, then we define P (v|context) to be the average of Pn(v) over all nodes n that satisfy the conditions defined\nby the context. P (v|context) can be interpreted as the probability that a node with heuristic value less than or equal to v will be produced when a node satisfying the conditions specified by context is expanded. When the context is empty it is denoted P (v) as in Section 3. We use p(v|context) (lower case p) to denote the probability that a node with heuristic value equal to v will be produced when a node satisfying the conditions specified by context is expanded. Obviously, P (v|context) =\n\u2211v i=0 p(i|context)."}, {"heading": "4.1.1 The Basic 1-Step Model", "text": "The conditioning context can be any combination of local properties of the search tree, including properties of the node itself (e.g. its heuristic value), the operator that was applied to generate the node, properties of the node\u2019s ancestors in the search tree, etc. The simplest conditional distribution is p(v|vp), the probability of a node with a heuristic value equal to v being produced when a node with value vp is expanded. We call this a 1-step model because each value is conditioned by nodes that are one step away only. In special circumstances, p(v|vp) can be determined exactly by analysis of the state space and the heuristic, but in general it must be approximated empirically by sampling the state space.\nIn our sampling method p(v|vp) is represented by the entryM [v][vp] in a two-dimensional matrix M [0..hmax][0..hmax], where hmax is the maximum possible heuristic value. To build the matrix we first set all values in the matrix to 0. We then randomly generate a state and calculate its heuristic value vp. After that, we generate each child of this state one at a time, calculate the child\u2019s heuristic value (v), and increment M [v][vp]. We repeat this process a large number of times in order to generate a large sample. Finally, we divide the value of each cell of the matrix by the sum of the column the cell belongs to, so that entry M [v][vp] represents the percentage of children generated with value v when a state with value vp is expanded.\nFigure 6 shows the bottom right corner of two such matrices for the 6-edge PDB of Rubik\u2019s Cube. The left matrix (a) shows p(v|vp) for the regular (consistent) lookup in this PDB and the right matrix (b) shows p(v|vp) for the inconsistent heuristic created by the dual lookup in this PDB. The matrix in (a) is tridiagonal because neighboring values cannot differ by more than 1. For example, states with a heuristic value of 8 can only have children with heuristics of 7, 8 and 9; these occur with probabilities of 0.21, 0.70 and 0.09 respectively (see column 8). By contrast, the matrix in (b) is not tridiagonal. In column 8, for example, we see that 6% of the time states with heuristic value of 8 have children with heuristic values of 6."}, {"heading": "4.1.2 Richer Models", "text": "When IDA* expands a node, it eliminates some children because of operator pruning. For example, in state spaces with undirected operators, such as we are using in our studies, the parent of a node would be generated among the node\u2019s children but IDA* would immediately prune it away. Distribution p(v|vp) does not take this into account. In order to take this into consideration it is necessary to extend the context of the conditional probability to include the heuristic value of the parent of the node being expanded (we refer to the parent node as gp). We denote this by p(v|vp,vgp) and call this a \u201c2-step\u201d model because it conditions on information from ancestors up to two steps away. p(v|vp,vgp) gives the probability of a node with a heuristic value equal to v being generated when the node being expanded has a heuristic value of vp and the parent of the node being expanded has a heuristic value of vgp. It is estimated by sampling in the same way as was done to estimate p(v|vp), except that each sample generates a random state, gp, then all its neighbors, and then all of their neighbors except those eliminated by operator pruning. Naturally, the results of the sampling for this 2-step model are stored in a three-dimensional array.\nThe context of the conditional distribution can be extended in other ways as well. For the sliding-tile puzzles, KRE conditions the overall distribution on the \u201ctype\u201d of the state being expanded, where the type indicates if the blank is in a corner, edge, or interior location. In our experiments with the sliding-tile puzzle below, we extend p(v|vp,vgp) with this type information: p(v, t|vp, tp,vgp, tgp) gives the probability of a node of type t with heuristic value equal to v being generated when the node being expanded has heuristic value vp and type tp and the expanded node\u2019s parent has heuristic value vgp and type tgp.\n4.2 A New Prediction Formula, CDP (Conditional Distribution Prediction)\nIn this section we use the conditional distributions just described to develop CDP, an alternative to the KRE formula for predicting the number of nodes IDA* will expand for a given heuristic, IDA* threshold, and set of start states. As will be shown below experimentally, the new formula CDP overcomes the limitations of KRE and works well for inconsistent heuristics and for any set of start states with arbitrary IDA* threshold.\nOur overall approach is as follows. Define Ni(s, d, v) to be the number of nodes that IDA* will generate at level i with a heuristic value equal to v when s is the start state and d is the IDA* threshold. Given Ni(s, d, v), the number of nodes IDA* will expand at level i for threshold d is\n\u2211d\u2212i v=0 Ni(s, d, v), and, N(s, d), the total number of nodes expanded in a\ncomplete iteration of IDA* with threshold d over all levels, the quantity we are ultimately\ninterested in, is \u2211d\ni=0 \u2211d\u2212i v=0 Ni(s, d, v). In these summations v only runs up to d\u2212 i because\nonly nodes with heuristic values in the range [0 . . . d\u2212 i] will be expanded at level i.\nIf Ni(s, d, v) could be calculated exactly, this formula would calculate N(s, d) exactly whether the given heuristic is consistent or not. However, there is no general method for efficiently calculating Ni(s, d, v) exactly. Instead, Ni(s, d, v) will be estimated recursively from Ni\u22121(s, d, v) and the conditional distribution; the exact details depend on the conditional model being used and are given in the subsections that follow. We will use N\u0303i(s, d, v) to denote an approximation of Ni(s, d, v). In Section 4.5.1 we will describe conditions in which this calculation is, in fact, exact, and therefore produces perfect predictions of N(s, d). But in the general case these predictions may not be perfect and are only estimates. At the present time we have no analytical tools for estimating their accuracy but as we show experimentally, these estimates are often very accurate."}, {"heading": "4.3 Prediction Using the Basic 1-Step Model", "text": "If the basic 1-step conditional distribution p(v|vp) is being used, Ni(s, d, v) can be estimated recursively as follows:\nNi(s, d, v) \u2248 N\u0303i(s, d, v) =\nd\u2212(i\u22121) \u2211\nvp=0\nN\u0303i\u22121(s, d, vp) \u00b7 bvp \u00b7 p(v|vp) (5)\nwhere bvp is the average branching factor of nodes with heuristic value vp, which is estimated during the sampling process that estimates the conditional distribution.5 The reasoning behind this equation is that Ni\u22121(s, d, vp)\u00b7bvp is the total number of children IDA* generates via the nodes it expands at level i\u22121 with heuristic value equal to vp. This is multiplied by p(v|vp) to get the expected number of these children that have heuristic value v. Nodes at level i\u22121 are expanded if and only if their heuristic value is less than or equal to d\u2212 (i\u2212 1), hence the summation only includes vp values in the range of [0 . . . d\u2212 (i\u22121)]. By restricting vp to be less than or equal to d \u2212 (i \u2212 1) in every recursive application of this formula, we ensure (even for inconsistent heuristics) that a node is only counted at level i if all its ancestors are expanded by IDA*. The base case of this recursion, N0(s, d, v), is 1 for v = h(s) and 0 for all other values of v.\nBased on this, the number of nodes expanded by IDA* given start state s, threshold d, and a particular heuristic can be predicted as follows:\nCDP1(s, d) =\nd \u2211\ni=0\nd\u2212i \u2211\nv=0\nN\u0303i(s, d, v) (6)\nIf a set, S, of start states is given instead of just one start state, the calculation is identical except that the base case of the recursion is defined using all the start states in S. That is, we define N0(S, d, v) to be equal to k if there are k states in S with a heuristic value of v. The rest of the formula remains the same (with S substituted for s everywhere).\n5. In the general case of our equation the branching factor depends on the context that defines the conditional distribution. Since in the 1-step model, the context is just the heuristic value v, we formally allow the branching factor to depend on it. In practice, the branching factor is usually the same for all heuristic values."}, {"heading": "4.4 Prediction Using Richer Models", "text": "If the 2-step conditional distribution p(v|vp, vgp) is being used, we define Ni(s, d, v, vp) to be the number of nodes that IDA* will generate at level i with a heuristic value equal to v from nodes at level i\u2212 1 with heuristic value vp when s is the start state and d is the IDA* threshold. Ni(s, d, v, vp) can be estimated recursively as follows:\nNi(s, d, v, vp) \u2248 N\u0303i(s, d, v, vp) =\nd\u2212(i\u22122) \u2211\nvgp=0\nN\u0303i\u22121(s, d, vp, vgp) \u00b7 bvp,vgp \u00b7 p(v|vp, vgp) (7)\nwhere bvp,vgp is the average branching factor of nodes with heuristic value vp and a parent with heuristic value vgp. The base case for this 2-step model is at level 1, not level 0. N1(s, d, v, vp) is 0 for vp 6= h(s), and is the number of children of the start state s with heuristic value v for vp = h(s). Based on this 2-step model the number of nodes expanded by IDA* given start state s, threshold d, and a particular heuristic can be predicted as follows:\nCDP2(s, d) =\nd \u2211\ni=0\nd\u2212i \u2211\nv=0\nd\u2212(i\u22121) \u2211\nvp=0\nN\u0303i(s, d, v, vp) (8)\nIf there is a set S of start states instead of just one, the base case is N1(S, d, v, vp), the number of children with heuristic value v of the states in S with heuristic value vp.\nAnalogous definitions of Ni and CDP can be used with any definition of context. For example, if using a 1-step model with a set T of state types, one would define Ni(s, d, v, t) as the number of nodes of type t that IDA* will generate at level i with a heuristic value equal to v, and estimate it recursively as follows:\nNi(s, d, v, t) \u2248 N\u0303i(s, d, v, t) =\nd\u2212(i\u22121) \u2211\nvp=0\n\u2211\ntp\u2208T\nN\u0303i\u22121(s, d, vp, tp) \u00b7 bvp,tp \u00b7 p(v, t|vp, tp) (9)\nBased on this model the number of nodes expanded by IDA* given start state s, threshold d, and a particular heuristic can be predicted as follows:\nCDP(s, d) =\nd \u2211\ni=0\nd\u2212i \u2211\nv=0\n\u2211\nt\u2208T\nN\u0303i(s, d, v, t) (10)"}, {"heading": "4.5 Prediction Accuracy", "text": "The accuracy of our predictions can be arbitrarily good or arbitrarily bad depending on the accuracy of the conditional model being used. In the following subsections we examine each of these extreme cases.\nIn principle, extending the context should never decrease the accuracy of the predictions because additional information is taken into account. However, when the conditional model is being estimated by sampling, an extended context can result in poorer predictions because there are fewer samples in each context. This is our explanation of why the 1-step model is more accurate than the 2-step model in rows h = 6 and h = 9 in Table 7 in Section 6.2 below."}, {"heading": "4.5.1 Perfect Predictions", "text": "Consider any definition of context that includes the heuristic value of the node being expanded (vp in the contexts defined above) and contains sufficient information to allow operator pruning to be correctly accounted for. We will use the notation (v, x) to refer to a specific instance of such a context, where v is the heuristic value of the node being expanded and x is an instantiation of the other information in the the context (e.g., the state type information in the last model above). The general form of our predictive model with such a context is\nCDP(s, d) = d \u2211\ni=0\nd\u2212i \u2211\nv=0\n\u2211\nall x such that (v, x) is an instance of the context\nN\u0303i(s, d, v, x) (11)\nwith\nN\u0303i(s, d, v, x) =\nd\u2212(i\u22121) \u2211\nvp=0\n\u2211\nall xp such that (vp, xp) is an instance of the context\nN\u0303i\u22121(s, d, vp, xp) \u00b7 bvp,xp \u00b7 p(v, x|vp, xp) (12)\nwhere bvp,xp is the average branching factor, after operator pruning, of all nodes satisfying the conditions of context (vp, xp), and p(v, x|vp, xp) is the average over all nodes n satisfying the conditions of context (vp, xp) of pn(v, x), the percentage of n\u2019s children, after operator pruning, that satisfy the conditions of context (v, x).\nIf, for every context (vp, xp), all nodes n satisfying the conditions defined by (vp, xp) have exactly the same branching factor bvp,xp and exactly the same value of pn(v, x) for all contexts (v, x), a simple proof by induction starting from the correctness of the base cases, N1(s, d, v, x), shows that Ni(s, d, v, x) = N\u0303i(s, d, v, x) for all i, i.e., that our prediction method correctly calculates exactly how many nodes will satisfy the conditions of each context at every level of the search tree. From this it follows that CDP(s, d) will be exactly the number of nodes IDA* will expand given start state s and IDA* threshold d.\nA practical setting in which the predictions of our 2-step model are guaranteed to be perfect by this reasoning is when the following conditions hold:\n1. the heuristic is defined to be the exact distance to the goal in an abstract state space, as is the case when a single pattern database is used.\n2. any two states, s1, s2, that map to the same abstract state x have the same set of operators {op1, ..., opk} that apply to them, and\n3. if states s1 and s2 map to abstract state x, then for all operators op \u2208 {op1, ..., opk} that apply to s1 and s2, s1\u2019s child op(s1) and s2\u2019s child op(s2) map to the same abstract state, op(x).\nDefine the context of a node to be its heuristic value and the abstract state to which it maps. Condition (2) guarantees that for every context (v, x), all nodes satisfying the conditions of (v, x) have exactly the same branching factor bv,x. This is true because if nodes n1 and n2 satisfy the conditions of context (v, x), they both map to the same abstract state x, and condition (2) then requires that exactly the same set of operators apply to them both. Conditions (2) and (3) together guarantee that for every context (vp, xp), all nodes satisfying the conditions of (vp, xp) have exactly the same value of pn(v, x) for all v and x. This is true because if nodes n1 and n2 satisfy the conditions of context (vp, xp), they both map to the abstract state xp, the same set of operators applies to both, and each operator op creates a child that, in both cases, maps to a specific abstract state, op(xp). Therefore the percentage of children that map to any particular abstract state is the same for both n1 and n2.\nA straightforward implementation of the prediction method in this setting associates a counter with each abstract state, which is initialized to the number of start states that map to the abstract state. The counter for abstract state x is updated once for each value of i (1 \u2264 i \u2264 d) by adding to it, for each operator op, the current value of the counter of each abstract state y such that op(y) = x. This algorithm has a computational complexity that is O(d\u00d7|A|\u00d7\u03b22) where |A| is the number of abstract states and \u03b2 is the effective branching factor in the abstract space. Because the complexity depends only linearly on d, in contrast to the typically exponential dependency on d for the number of nodes IDA* will expand, for sufficiently large d the prediction will be arbitrarily faster to compute than the search itself. For example, for a PDB for the 15-puzzle based on the positions of 8 tiles and the blank (roughly 4 billion abstract states), the prediction for 1000 start states with d = 52 takes only 6% of the time required to execute the search.\nAn exact prediction for this setting has two potential uses. The first is to determine if searching with a single PDB is feasible or not. For example, the calculation might show that even the first iteration of IDA* (with a threshold of h(start)) will take more than a year to complete. The second is to use the prediction to compare the actual performance of an alternative method executed on a set of start states (e.g. taking the maximum over a set of PDBs) to the performance using a single PDB without actually having to execute the IDA* search with the single PDB."}, {"heading": "4.5.2 Very Poor Predictions", "text": "The predictions made by a conditional model will be extremely inaccurate if the distribution of heuristic values is independent of the information supplied by the context. We illustrate this with an example based on the 4x3 sliding-tile puzzle and two heuristics, a PDB based on the locations of tiles 1-7 and the blank, and the heuristic that returns 0 for every state. If the given state has the blank in its goal position, or in a position that is an even number of moves from the goal position, the heuristic value for that state is taken from the PDB. The other states have a heuristic value of 0. In the search tree, the heuristic used at level i will therefore be the opposite of the one used at level i\u2212 1.\nA 1-step model in this situation will clearly be hopeless for predicting the heuristic distribution in levels where the PDB is being used until i is sufficiently large that the distribution at level i converges to the unconditional distribution.\nThere is some hope that a 2-step model could make reasonably accurate predictions because the PDB, considered by itself, defines a consistent heuristic and therefore the distribution of heuristic values of a node\u2019s children are somewhat correlated with the heuristic value of the node\u2019s parent.\nWe tested this using the 4x3 sliding-tile puzzle, which is small enough that we could build our 2-step model using all the states in the state space so that no error is introduced through a sampling process. To test the prediction accuracy of the model we generated 50,000 solvable states at random and, as will be explained in detail in the next section, used state s as a start state in combination with IDA* threshold d if IDA* would actually have executed an iteration with threshold d when given state s as a start state. This means that a different number of start states might be used for each value of d. The \u201cNum\u201d column in Table 4 indicates how many start states were used for each value of d (first column) and we have only included in this table results for which more than 5,000 start states were used. The \u201cIDA*\u201d column shows the average number of nodes expanded by IDA* on the start states used for each d and the \u201cPrediction\u201d column shows the number predicted by our 2-step model. The \u201cRatio\u201d column is \u201cPrediction\u201d divided by \u201cIDA*\u201d. One can clearly see the improvement of the predictions as d increases. But even at the deepest depth at which our sample provided more than 5,000 start states, the prediction is a factor of 6 smaller than the true value. Of course, using the constant heuristic value of 0 in alternate levels is not something one would do in practice, but we obtained similar results, for essentially the same reason, with the 15-puzzle when switching, from one level to the next, between a pattern database based on tiles 1-7 and a pattern database based on tiles 9-15 (see Section 7.1)."}, {"heading": "5. Experimental Setup", "text": "The next two sections describe the experimental results that we obtained by running IDA* and comparing the number of nodes it expanded to the number predicted by KRE and by\nCDP. We experimented on the same two application domains used by KRE, namely, Rubik\u2019s Cube (Section 6) and the sliding-tile puzzle (Section 7). In each domain we evaluated the accuracy of the two formulas, for both consistent and inconsistent heuristics, on a set of solvable start states that were generated at random.\nIn all the experiments reported here, the start states used for a given IDA* threshold d were subject to a special condition. State s is only used as a start state in combination with threshold d if IDA* actually performs an iteration with threshold d when s is the start state. For example, we would not use s as a start state for d = 17 if s is only distance 11 from the goal or if h(s) > 17. In addition, for the sliding-tile puzzle, start state s would not be used with IDA* threshold d if h(s) and d were of different parity. By contrast, the experiments in the KRE paper did not restrict the choice of start states in this way, the same start states were used with every IDA* threshold .\nThis difference in how start states are chosen can have a large impact on the number of nodes IDA* expands. Table 5 illustrates this for the 15-Puzzle using the Manhattan Distance heuristic for IDA* threshold d (first column) between 43 and 50. The \u201cnodes\u201d column under \u201cUnrestricted\u201d shows the number of nodes IDA* expanded on average for 50,000 randomly generated solvable start states. The values in this column are in close agreement with the corresponding results of Table 5 in the KRE paper (Korf et al., 2001). The \u201cnumber\u201d column shows how many of these start states satisfy our additional condition. If we remove the start states that violate our condition, IDA* expands substantially fewer nodes on average, as shown in the \u201cnodes\u201d column under \u201cRestricted\u201d, and the difference increases as d increases. At d = 50 there is almost an order of magnitude difference between the number of nodes expanded in the two settings. This difference needs to be kept in mind when making comparisons with the experimental results reported here and in the KRE papers."}, {"heading": "50 91,329,281 18,758 9,904,973", "text": ""}, {"heading": "49 40,479,725 20,389 6,037,064", "text": ""}, {"heading": "48 20,355,110 21,028 3,508,482", "text": ""}, {"heading": "47 8,963,747 22,243 2,108,766", "text": ""}, {"heading": "46 4,542,249 22,266 1,182,522", "text": ""}, {"heading": "45 1,985,565 22,937 688,119", "text": ""}, {"heading": "44 1,014,941 22,484 393,406", "text": ""}, {"heading": "43 439,942 22,525 219,001", "text": ""}, {"heading": "6. Experimental Results for Rubik\u2019s Cube", "text": "We begin with Rubik\u2019s Cube experiments. The heuristic used here is the 6-edge PDB heuristic described above (Section 3.2.1). We experimented with the (consistent) regular lookup and the (inconsistent) random-symmetry and dual lookups on this PDB. For the CDP formula, two models were used, CDP1 and CDP2, which denote the 1-step and 2-step models, respectively.\nAs outlined in Section 4.1.1, the conditional distribution tables were built by generating one billion states (each is generated by applying 180 random moves to the goal state), computing all their neighbors, and incorporating their heuristic information into the matrix representing the one-step model. For the two-step model we also generated all the grandchildren and used their heuristic information.\nIn addition, in order to get reliable samples we added the following two procedures:\n\u2022 While generating children and grandchildren for sampling we used the same pruning techniques based on operator ordering that were used in the main search (see the description in Section 2.1.1). That is, we did not use a sequence of operators that would not be generated by the main search. This is done by looking at the random walk that led to the initial state and using the last operator in the random walk as the basis for operator pruning.\n\u2022 In order to get a reliable sample we need each entry in the table to be sufficiently sampled. Some entries in the table have very low frequency. For example, states with heuristic value of 0 are very rare even in a sample of a billion states causing our table for the 0 row to be generated by a very small sample. Therefore, we enriched such entries by artificially creating random states with heuristic value of 0. Other under-sampled entries were sampled in a similar way. One technique, for example, for creating (with high probability) a random state with a heuristic value of x, is to perform a random walk of length x on a random state with heuristic value of 0."}, {"heading": "6.1 Rubik\u2019s Cube with Consistent Heuristics", "text": "Table 6 compares KRE to CDP1 and to CDP2. The accuracy of the three prediction methods was compared while using regular lookups on the 6-edge PDB. Results in each row are averages over a set of 1000 random states. Each row presents the results of an IDA* iteration\nwith different threshold (d), given in the first column. The second column (IDA*) presents the actual number of nodes expanded for each IDA* threshold. The next columns report the predictions and the accuracy (\u201cRatio\u201d) of each prediction defined as the ratio between the predicted number and the actual number of expanded nodes. As was reported in the KRE paper, the KRE formula was found to be very accurate for a consistent heuristic when averaged over a large set of random start states. The table shows that CDP1 is reasonably accurate but systematically underestimates because the one-step model does not consider that a node\u2019s parent will not be included among its children. We elaborate on this below. CDP2\u2019s predictions are very accurate, slightly more accurate than KRE\u2019s."}, {"heading": "6.2 Rubik\u2019s Cube with Start States Having Specific Heuristic Values", "text": "Table 2, presented above (Section 3.2.2), and the related discussion, show that KRE might not make accurate predictions when start states are restricted to have a specific heuristic value h. For the particular example shown (IDA* threshold 12) KRE will always predict a value of 8, 161, 064, but the exact value depends on the specific set of start states used because the IDA* threshold of 12 is not sufficiently large for the number of nodes to be independent of the start states. Table 7 extends Table 2 to include the predictions of CDP. It shows that both versions of CDP substantially outperform KRE on any particular set of start states."}, {"heading": "6.3 Rubik\u2019s Cube with Inconsistent Heuristics", "text": "The same experiments were repeated for inconsistent heuristics. The dual and randomsymmetry lookups were performed on the 6-edge PDB instead of the regular lookup, thereby creating an inconsistent heuristic. As discussed in Section 3.2.1, KRE produces the same prediction for all heuristics (consistent and inconsistent) derived from a single PDB and overestimates for the inconsistent heuristics. Table 8 shows that CDP2 is extremely accurate. Its prediction is always within 2% of the actual number of nodes expanded.\nThe 1-step model used by CDP1 systematically underestimates the actual number of nodes expanded for regular and dual lookups (see the regular lookup in Table 6 and the dual lookup in Table 8). To understand why, consider what happens when the node m in the right side of Figure 7 is expanded. It generates two children, node n and (assuming\noperators have inverses as is the case in Rubik\u2019s Cube) a copy of its parent R (shown as m\u2019s left child in Figure 7). This child is 2 levels deeper than R and therefore has an f -value that is 2 greater than R\u2019s. With an IDA* threshold of 5, this child will not be a potential node and the 1-step model will conclude that m will generate a potential child with a probability of 0.5, whereas in fact all of the children that remain after operator pruning are potential nodes.\nThe reason the 1-step model does not underestimate the number of nodes expanded when random-symmetry lookups are done is because the child copy of R is not constrained to have the same heuristic value as R itself \u2013 different symmetries could be chosen for different occurrences R. The child\u2019s f -value has no correlation with the f -value of R and the above explanation of why CDP1 underestimates does not apply.\nIn fact, if different copies of a state have uncorrelated h-values the only effect of operator pruning that needs to be taken into account is that it reduces the number of children, and this can be done as well within a 1-step model when calculating the branching factor. There may be other advantages of using the wider context of the 2-step model but the results for the random-symmetry heuristic show that they are minor in this case."}, {"heading": "7. Experimental Results - Sliding-Tile Puzzle", "text": "In the KRE experiments on the sliding-tile puzzle, three state types are used, based on whether the blank is in a corner, edge, or interior location. We used the same state types in our experiments and used exact recurrence equations for N(s, v, d, t) in the type-dependent version of the KRE formula. The heuristic used was Manhattan Distance (MD). We experimented with the 2-step CDP that includes the type system in the recurrence equations. Results for the 1-step CDP are not included here because it performed poorly in early versions of these experiments.\nFor the 8-puzzle the conditional distribution P (v, t|vp, tp,vgp, tgp) needed by CDP2 and the typed unconditional distribution P (v, t) needed by the type-dependent KRE formula were computed by enumerating all the states in the 8-puzzle reachable from the goal.\nFor the 15-puzzle, it was not possible to do exhaustive enumeration of the entire state space so the conditional distributions were estimated by generating ten billion reachable states at random. This uniform random sample was used to estimate P (v, t) for KRE, and each state in the sample was used as gp in the sampling method described in Section 4.1.2 for P (v, t|vp, tp,vgp, tgp). For the latter, however, the basic sampling method had to be extended because even after processing ten billion gp states some of the entries in the 6-dimensional matrix were missing or were not sampled sufficiently. To correct this, after we generate gp, its children, and its grandchildren and update the matrix accordingly, we check if the matrix already contains data for gp\u2019s great-grandchildren. If it does not then we generate gp\u2019s great-grandchildren and update the corresponding entries in the matrix. This continues as long as we encounter contexts that have never been seen before. This introduces a small statistical bias into the sample, but it guarantees that the sample contains the required data.\nPrediction results for KRE and CDP2 for the 8- and 15-puzzles are shown in Table 9 in the same format as above. For the 8-puzzle the predictions were made for an IDA* threshold\nof 22 and each row corresponds to the group of all 8-puzzle states with the same heuristic value h (shown in the first column) for which IDA* would actually have used threshold 22. The second column gives the number of states in each group. Clearly, as shown in the \u201cIDA*\u201d column, for states with higher initial heuristic values IDA* expanded a smaller number of nodes. This trend is not reflected in the KRE predictions since KRE does not take h into account. For KRE the only difference between the attributes of different rows is the different type distribution for the given group. Thus, the predicted number of expanded nodes of KRE is very similar for all rows (around 1,400). The CDP formula takes the heuristic value of the start state into account and was able to predict the number of expanded nodes much better than KRE. The bottom part of Table 9 show results for the 15-puzzle for an IDA* threshold of 52. Similar tendencies are observed."}, {"heading": "7.1 Inconsistent Heuristics for the Sliding-tile Puzzle", "text": "Our next experiment is for an inconsistent heuristic on the 8-puzzle. We defined two PDBs, one based on the location of the blank and tiles 1\u20134, the other based on the location of the blank and tiles 5\u20138. To create an inconsistent heuristic, only one of the PDBs was consulted by a regular lookup. The choice of PDB was made systematically, not randomly, based on the position of the blank. Different occurrences of the same state were guaranteed to do the same lookup but neighboring states were guaranteed to consult different PDBs and this causes inconsistency. The results are presented in Table 10 for a variety of IDA* thresholds. For each threshold the \u201cNum\u201d column indicates how many start states were used. The results show that CDP\u2019s predictions are reasonably accurate, and very much more accurate than KRE\u2019s which overestimate by up to a factor of 26.\nSimilar experiments were conducted for the 15-puzzle. Here, the first PDB was based on the location of the blank and tiles 1\u20137, the other was based on the location of the blank and tiles 9\u201315. Table 11 shows the results for IDA* thresholds from 48 to 55 (recall that the median solution length for this puzzle is 52). The numbers shown are averages\nover 50,000 start states. The CDP predictions for the 15-puzzle are considerably worse than for the 8-puzzle, but the KRE predictions have degraded much more. The reason for the inaccuracy of these predictions was discussed in Section 4.5.2. Much more accurate predictions are produced if the context is extended to include the heuristic value of both pattern databases, not just the one that the search algorithm actually consults."}, {"heading": "8. Accurate Predictions for Single Start States", "text": "We have seen that CDP works well when the base cases of the recursive calculation of Ni(s, d, v) is seeded by a large set of start states, no matter how their heuristic values are distributed. However, the actual number of expanded nodes for a specific single start state can deviate from the number predicted by CDP. The conditional distribution reflects the expected values over all nodes that share the same context, and the single start state of interest might behave differently than the \u201caverage\u201d state that has the same context. Consider a Rubik\u2019s Cube state with a heuristic value of 8. CDP2 predicts that IDA* will expand 6, 743, 686 for such a state with IDA* threshold 12. Table 2 shows that on the average (over 1, 000 start states with a heuristic value of 8) 6, 002, 025 states are expanded. Examining the results for the individual start states showed that the actual number of expanded nodes ranged between 2, 398, 072 to 15, 290, 697 nodes.\nIn order to predict the number of expanded nodes for a single start state we propose the following enhancement to CDP. Suppose that we want to predict the number of expanded nodes for IDA* threshold d and start state s. First, we perform a small initial search from s to depth r. We then use all the states at depth r to seed the base cases of the CDP formula and compute the formula with IDA* threshold d\u2212 r. This will cause a larger set of nodes to be used in calculating Ni(s, d, v), thereby improving the accuracy of CDP\u2019s predictions."}, {"heading": "8.1 Rubik\u2019s Cube, 6-edge PDB Heuristic", "text": "Table 12 shows results for four specific Rubik\u2019s Cube states with a heuristic value of 8 (of the regular 6-edge PDB lookup) when the IDA* threshold was set to 12. We chose the states with the least and greatest number of expanded nodes and two states around the median. The first column shows the actual number of nodes IDA* expands for each state.\nThe next columns show the number of expanded nodes predicted by our enhanced CDP2 formula where the initial search was performed to depths (r) of 0, 2, 5 and 6. Clearly, these initial searches give much better predictions than the original CDP2 (with r = 0), which predicts 6, 743, 686 for all these states. With an initial search to depth 6, the predictions are very accurate."}, {"heading": "8.2 Rubik\u2019s Cube, 8-6-6 Heuristic", "text": "Section 3.2.3 presented KRE predictions for two start states, s6, with a heuristic value of 6, and s11, with a heuristic value of 11, for Rubik\u2019s Cube with the 8-6-6 heuristic. Here we repeat these experiments with CDP1. Tables 13 and 14 show the results with an initial search of depth (r) 0 and 4. The tables show that CDP1 was able to achieve substantially better predictions than KRE in most cases, and that an initial search to depth 4 usually improved CDP1\u2019s predictions."}, {"heading": "8.3 Experiments on the 8-Puzzle - Single Start States", "text": "We performed experiments with the enhanced CDP2 formula on all the states of the 8-puzzle with the (consistent) MD heuristic. We use the term \u201ctrial\u201d to refer to each pair of a single start state and a given IDA* threshold d. The trials included all possible values of d and for each d all start states for which IDA* would actually perform a search with IDA* threshold d. Predictions were made for each trial separately, and the relative error, predicted/actual, for the trial was calculated. The results are shown in Figure 8. There are four curves in the figure, for KRE, for CDP, and for the enhanced CDP with initial search depths (r) of 5\nand 10. The x-axis is relative error. The y-axis is the percentage of trials for which the prediction had a relative error of x or less. For example, the y-value of 20% for the KRE curve at x = 0.5 means that KRE underestimated by a factor of 2 or more on 20% of the trials. The rightmost point of the KRE plot (x = 10, y = 94%) indicates that on 6% of the trials KRE\u2019s prediction was more than 10 times the actual number of nodes expanded. By contrast CDP has a much larger percentage of highly accurate predictions, with over 99% of its predictions within a factor of two of the actual number of nodes expanded. The figure clearly shows the advantage of using the enhanced CDP. With an initial search to a depth of 10, 90% of the trials had predictions within 10% of the correct number."}, {"heading": "9. Performance Range for a Given Unconditional Distribution", "text": "The experiments in this paper that have used the 6-edge PDB for Rubik\u2019s Cube have illustrated the fact that the number of nodes IDA* expands given a PDB can vary tremendously depending on how the PDB is used (Zahavi et al., 2007). To see this clearly, the middle three columns of Table 15 show data that has already been seen in Tables 6 and 8, namely, the number of nodes IDA* expands when the 6-edge PDB is used in the regular manner,\nwith dual lookups, and with random-symmetry lookups. IDA* expands ten times fewer nodes when the 6-edge PDB is consulted with random-symmetry lookups than when it is consulted in the normal way.\nThis raises the intriguing question of what range of performance can be achieved by varying the conditional distribution when the unconditional distribution is fixed."}, {"heading": "9.1 Upper Limit", "text": "The upper extreme, which results in the most nodes expanded, occurs when a consistent heuristic is used. This is because IDA* only expands potential nodes, so the maximum number of nodes is expanded when the conditional distribution is such that the parent of every potential node at level i is a potential node at level i \u2212 1. An exact calculation of the number of potential nodes in the brute-force tree is therefore a theoretical upper bound on the number of nodes IDA* will expand for a given unconditional distribution. As we have already discussed, one way to estimate the number of potential nodes is to use the KRE formula. This estimate of the upper bound of the number of nodes that IDA* could expand is denoted as CDP in Table 15.\nAlternatively, the number of potential nodes can be approximated with the CDP formula given the conditional distribution. Consider Equation 6. In the summation we consider all possible vp values in [0, d\u2212(i\u22121)] as only these nodes are potential nodes at level i\u22121. Thus only these nodes are expanded by IDA* at level i \u2212 1 and only these nodes can generate children at level i.6 Now, let\u2019s substitute this with vp \u2208 [0, hmax]. We now consider all the nodes at level i \u2212 1, even the ones that are not potential nodes. Using this in the summation will calculate the number of all nodes with heuristic v at level i even ones that are not actually generated be IDA* (because their parents were not potential nodes, i.e. with vp > d\u2212 (i\u2212 1). This is shown in Equation 13.\nNi(s, v) =\nhmax \u2211\nvp=0\nNi\u22121(s, vp) \u00b7 bvp \u00b7 p(v|vp) (13)\n6. Note that if the heuristic is consistent then only vp values in {v \u2212 1, v,v + 1} need to be considered in the summation because nodes with other values of vp (smaller than v \u2212 1 or larger than v + 1) cannot generate children with a heuristic value of v.\nUsing this in the general prediction equation we get:\nCDP =\nd \u2211\ni=0\nd\u2212i \u2211\nv=0\nNi(s, v) (14)\nThis gives an alternative method to approximate the number of potential nodes. Both these methods approximate this upper bound. In practice, however, it is possible that the number of expanded nodes will slightly exceed this approximate bound due to noise and small errors in the sampling or the calculations."}, {"heading": "9.2 Lower Limit", "text": "With consistent heuristics values of neighboring states are highly correlated. At the other extreme are cases where there is no correlation between heuristic values of neighboring nodes. That is, the heuristic value of a child node is statistically independent of the heuristic value of its parent. This means that regardless of the parent\u2019s heuristic value vp, the heuristic values of the children are distributed according to the unconditional heuristic distribution, i.e., p(v|vp) = p(v).\nOur motivation for using this as an estimated lower bound on the number of nodes IDA* could expand for a given unconditional distribution is the empirical observation that the number of nodes IDA* expands decreases as the correlation between a parent\u2019s heuristic value and its children\u2019s heuristic values decreases.\nThis is illustrated in the last row of the three middle columns of Table 15, which shows the correlation between the heuristic values of neighboring states when different types of lookups are done in the 6-edge PDB. It was calculated using Pearson\u2019s correlation coefficient, defined over n pairs of x, y values according to the following equation\nCorrelationxy = n \u2211n i=1 xiyi \u2212 \u2211n i=1 xi \u2211n i=1 yi \u221a\nn \u2211n i=1 x 2 i \u2212 ( \u2211n i=1 xi) 2 \u221a n \u2211n i=1 y 2 i \u2212 ( \u2211n i=1 yi) 2\n(15)\nIn order to calculate the correlation, 60, 000 random pairs of (xi,yi) neighboring states were generated. Their heuristic values were computed and used in Equation 15. The bottom row of Table 15 shows that the number of nodes expanded decreases as the correlation between neighboring heuristic values decreases. This leads us to suggest that the number of nodes expanded will reach a minimum when the correlation is zero.7\nThis estimated lower bound can be calculated using the CDP formula with p(v|vp) = p(v). We denote this by CDP. For the 1-step model this would be calculated using the following equations:\n7. In theory, it is possible for a heuristic to have a negative correlation between the parent\u2019s heuristic value and its children\u2019s heuristic values, i.e., parents with low heuristic values could tend to have children with large heuristic values and vice versa. We believe this is unlikely to occur in practice.\nNi(s, d, v) =\nd\u2212(i\u22121) \u2211\nvp=0\nNi\u22121(s, d, vp) \u00b7 bvp \u00b7 p(v) (16)\nCDP = d \u2211\ni=0\nd\u2212i \u2211\nv=0\nNi(s, d, v) (17)\nAs can be seen by comparing the rightmost two columns in Table 15, the randomsymmetry use of the 6-edge PDB is within a factor of two of our estimated minimum possible number of nodes expanded with this PDB, which suggests that to substantially improve upon its performance one would have to use a different PDB.\nTable 16 shows the estimated upper and lower bounds of IDA*\u2019s performance, for a range of IDA* thresholds, for three different PDBs for Rubik\u2019s Cube. The bounds are calculated using 1, 000 random start states. The table shows that, according to these estimates, inconsistent heuristics based on the 5-edge PDB can outperform consistent heuristics based on the 6-edge PDB but probably cannot outperform consistent heuristics based on the 7-edge PDB since the estimated lower bound of the 5-edge PDB is larger than the estimated upper bound of the 7-edge PDB."}, {"heading": "10. Predicting the Performance of IDA* with BPMX", "text": "With an inconsistent heuristic, the heuristic value of a child can be much larger than that of the parent. If this happens in a state space with undirected edges, the child\u2019s heuristic value can be propagated back to the parent. If this causes the parent\u2019s f -value to exceed the IDA* threshold the entire search subtree rooted at the parent can be pruned without generating any of the remaining children. This propagation technique is called bidirectional pathmax (BPMX) (Felner et al., 2005; Zahavi et al., 2007). It was shown to be very effective in reducing the search effort by pruning subtrees that would otherwise be explored. We now show how to modify CDP to handle BPMX propagation. Since BPMX only applies to state spaces with undirected edges, the discussion in this section is limited to such spaces."}, {"heading": "10.1 Bidirectional Pathmax (BPMX)", "text": "Traditional pathmax (Me\u0301ro, 1984) propagates heuristic values from a parent to its children, and can be applied in any state space. Admissibility is preserved by subtracting the cost of the connecting edge from the heuristic value. The basic insight of bidirectional pathmax (BPMX) is that when edges are undirected heuristic values can propagate to all neighbors, which includes from a child node to its parent. This process can continue to any distance in any direction. BPMX is illustrated in Figure 9. The left side of the figure shows the inconsistent heuristic values for a node and its two children. Consider the left child with a heuristic value of 5. Since this value is admissible and all edges in this example have a cost of one, all its immediate neighbors are at least 4 moves away from the goal, their neighbors are at least 3 moves away, and so on. When the left child is generated, its heuristic value (h = 5) can propagate up to the parent and then down again to the right child. To preserve admissibility, each propagation along a path reduces h by the cost of traversing the path. This results in h = 4 for the root and h = 3 for the right child. When using IDA*, this bidirectional propagation may cause many nodes to be pruned that would otherwise be expanded. For example, suppose the current IDA* threshold is 2. Without the propagation of h from the left child, both the root node (f = g + h = 0 + 2 = 2) and the right child (f = g + h = 1 + 1 = 2) would be expanded. Using the above propagation, the left child will increase the parent\u2019s h value to 4, resulting in search at this node being abandoned without even generating the right child.\n10.2 CDP Overestimates When BPMX is Applied\nWhen an inconsistent heuristic is being used and BPMX is applied, CDP will overestimate the number of expanded nodes because it will count the nodes in subtrees that BPMX prunes. In Section 4.2, we defined Ni(s, d, v) to be the number of nodes that IDA* will generate at level i with a heuristic value exactly equal to v when s is the start state and d is the IDA* threshold. The formula given for estimating Ni(s, d, v) (Equation 5) was:\nN\u0303i(s, d, v) =\nd\u2212(i\u22121) \u2211\nvp=0\nN\u0303i\u22121(s, d, vp) \u00b7 bvp \u00b7 p(v|vp)\nIn calculating N\u0303i(s, d, v) from N\u0303i\u22121(s, d, vp) this formula assumes that when a node is expanded all its children are generated. This is why N\u0303i\u22121(s, d, vp) is multiplied by the branching factor bvp . When BPMX is applied, a child may prune the parent before the rest of the children are generated. If this happens, the assumption that all the children of expanded nodes are generated would be wrong. For example, without BPMX, while\nexpanding the root of the left tree in Figure 9 both children are generated and the child on the right is also expanded. Indeed CDP will count two nodes in this case. When BPMX is applied the root is expanded but the child on the right will not be generated (and therefore not expanded). Thus, CDP, which counts the two nodes, is overestimating the number of nodes expanded. In the following section we modify our equation to correct this.\n10.3 New Formula for Estimating Ni(s, d, v)\nLet n be the node that is currently being expanded. Assume that n has b children and consider the order in which they are generated. We call this order the generation order. Note that when BPMX is applied, the probability that a child will be generated decreases as we move through the generation order. Children that appear late in the order will have a larger chance of not being generated since there are more previous children that might cause a BPMX cutoff. Let pbx(l) be the probability that the child in location l in the order will be generated even if BPMX is applied. With this definition we can extend Equation 5 as follows:\nN\u0303i(s, d, v) =\nd\u2212(i\u22121) \u2211\nvp=0\nbvp \u2211\nl=1\n{N\u0303i\u22121(s, d, vp) \u00b7 pbx(l) \u00b7 p(v|vp)} (18)\nN\u0303i(s, d, v) is being calculated in a similar way to Equation 5, except for the way we count the total number of children IDA* generates via the nodes it expands at level i \u2212 1 with heuristic value equal to vp. The idea here is to iterate over all the possible locations in the generation order and calculate the probability that a node in location l will be generated. In practice, however, the actual context for pbx has other variables besides the location l. It also includes the IDA* threshold (d), the depth of the parent (i \u2212 1) and the heuristic value of the parent (vp), We thus get our final formula:\nN\u0303i(s, d, v) =\nd\u2212(i\u22121) \u2211\nvp=0\nbvp \u2211\nl=1\n{N\u0303i\u22121(s, d, vp) \u00b7 pbx(l, d, i\u2212 1, vp) \u00b7 p(v|vp)} (19)\nThis is exactly equal to Equation 5 in the special case when pbx(l) = 1 for all l, which happens when BPMX is not used or when it is used with a consistent heuristic.\n10.4 Calculating pbx\nFor simplicity, our model assumes that a heuristic value can only be propagated by BPMX one level up the tree. This means that a state can be pruned only from its immediate children and not by descendants at deeper levels. We make this assumption for another reason besides simplicity of description. Our experiments with Rubik\u2019s Cube and other domains showed that indeed almost all the pruning of BPMX was caused by a 1-level BPMX propagation. A generalized formula with deeper BPMX propagations can be similarly developed but it will include complicated recursive terms with very low practical value, at least for the state spaces and heuristics we have studied.\nAssume that c is a child of n in location l of the generation order. Child c will be generated only if n is not pruned by any of its l \u2212 1 children that appear before c in the\ngeneration order. Assume that n is at level i and that the threshold is d. Since n is expanded, h(n) \u2264 d \u2212 i. With BPMX h(n) can be increased (and cause BPMX pruning) from a child k if h(k) > d \u2212 i + 1. In this case, h(k) \u2212 1 is larger than d \u2212 i, so when it is used instead of h(n) IDA* will decide not to expand n and no additional children will be generated. Therefore, in order for a child c in location l of the generation order to be generated, all its l \u2212 1 predecessors in the generation order must have heuristics less than or equal to d\u2212 i+1. Assuming that the heuristic value of the parent is v the probability of this will be\npbx(l, d, i, v) = {\nd\u2212i+1 \u2211\nh=0\np(h|v)}l\u22121 (20)\nWe sum up the probability of each relevant heuristic value and raise the sum to the power of l \u2212 1 since l \u2212 1 children appear before c."}, {"heading": "10.5 Experiments on Rubik\u2019s Cube with BPMX", "text": "We repeated the experiments on Rubik\u2019s Cube with the 6-edge PDB but with BPMX activated. Since BPMX affects only inconsistent heuristics, only the \u201cDual\u201d and \u201cRandom Symmetry\u201d heuristics were tested. Each heuristic was tested for IDA* thresholds 8 through 13. The results, averaged over the same set of 1, 000 random states, are presented in Table 17. The \u201cNo BPMX\u201d columns are repeated from Table 8. The additional columns show our results with BPMX. The column \u201cIDA* + BPMX\u201d presents the actual number of expanded nodes when using BPMX. BPMX reduces the number of nodes expanded by more than 30% for the Dual and by more than 25% reduction for the Random Symmetry, making the unmodified CDP2\u2019s predictions high by about the same amount. The \u201cCDP bx 2 \u201d column shows that the modifications introduced in this section greatly improve the accuracy."}, {"heading": "11. Related Work", "text": "Previous work on predicting A* or IDA*\u2019s performance from properties of a heuristic falls into two main camps. The first bases its analysis on the accuracy of the heuristic, while the second bases its analysis, as we have done, on the distribution of heuristic values. The next two subsections survey these approaches."}, {"heading": "11.1 Analysis Based on a Heuristic\u2019s Accuracy", "text": "One common approach is to characterize a heuristic by focusing on the error in the heuristic value (deviation from the optimal cost). The first analysis in this line, focusing on the effect of errors on the performance of search algorithms, was done by Pohl (1970). Many other papers in this line have appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, & Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid & Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & Ro\u0308ger, 2008).\nThese works usually assume an abstract model space of a tree where every node has exactly b children and aim to provide the asymptotic estimation for the number of expanded nodes. They mainly differ by the model assumptions (e.g. binary or non-binary trees) and for what case the results are derived (worst case or average case). Worst case analysis showed that there is a correlation between the heuristic errors and the search complexity. They found that if the relative error, |h(n)\u2212h\n\u2217(n)| h\u2217(n) , is constant, the search complexity will\nbe exponential (in the length of solution path) but if the absolute error, |h(n) \u2212 h\u2217(n)|, is bounded by a constant the search complexity is linear (Pohl, 1977; Gaschnig, 1979). Three main assumptions used by Pohl (1977) are that the branching factor is assumed to be constant across inputs, that there is a single goal state and that there are no transpositions in the search space. When these assumptions do not hold, as is the case for many standard benchmark domains in planning, general search algorithms such as A* explore exponential number of states even under the assumption of an almost perfect heuristic (i.e., a heuristic whose error is bounded by a small additive constant) (Helmert & Ro\u0308ger, 2008).\nSince it is difficult to guarantee precise bounds on the magnitude of errors produced by a given heuristic, a probabilistic characterization of these magnitudes was suggested (Huyn et al., 1980; Pearl, 1984). Heuristics are modeled as random variables (RVs), and the relative errors are assumed to be independent and identically distributed (IID model). In this model, attaining an average polynomial A* complexity was proved to be essentially equivalent to requiring that values of h(n) be clustered near h\u2217(n) where the allowed deviation is a logarithmic function of h\u2217(n) itself.\nAdditional research in this line was conducted by Chenoweth and Davis (1991). Instead of using the IID model, they suggested using the \u201cNC model\u201d, which places no constraints on the errors of h. With this model the heuristic is defined according to how the heuristic values grow with respect to the distance to the goal, and not according to the error. They predicted that A* complexity will be polynomial whenever the values of h(n) are logarithmicaly clustered near h\u2217(n) + \u03b7(h\u2217(n)), where \u03b7 is an arbitrary, non-negative, and non-decreasing function. Heuristics whose values grow slower than the distance to the goal cause exponential complexity. Studies with the \u201cNC model\u201d showed that replacing a\nheuristic h with wh for some w \u2265 0 can often change A* complexity from exponential to polynomial.\nMost of these works focused on tree searches. By contrast, Sen et al. (2004) presented a general technique for extending the analysis of the average case performance of A* from search spaces that are trees to search spaces that are directed acyclic graphs. Their analytical results show that the expected complexity can change from exponential to polynomial as the heuristic estimates of nodes become more accurate and restrictions are placed on the cost matrix. Recent research in this line, analyzing the complexity of the A* algorithm was presented by Dinh et al. (2007). This research presented both worst and average case analysis for the performance of A* for approximately accurate heuristics8 for search problems with multiple solutions. Bounds presented in that paper have been proved to be dependent on the heuristic accuracy and distribution of solutions."}, {"heading": "11.2 Analysis Based on the Heuristic Distribution", "text": "As discussed at the outset of this paper, KRE suggested an alternative approach for calculating the time complexity of IDA* on multiple-goal spaces (Korf & Reid, 1998; Korf et al., 2001). Arguing that the heuristic accuracy is very difficult to obtain, they suggested deriving the analysis from the unconditional distribution of heuristic values, which is easy to determine at least approximately. They also came up with a method for deriving a closed-form formula for Ni, the number of nodes at level i of the brute-force search tree. That method was later formalized (Edelkamp, 2001b). Unlike the work described in the previous subsection, which provides a \u201cbig-O\u201d complexity analysis, KRE\u2019s aim (and ours) is to exactly predict the number of nodes IDA* will expand.\nKRE correctly point out that, when operators do not all have the same cost, Ni must be defined as the number of nodes that can be reached by a path of cost i, as opposed to the number of nodes that are i edges from the start state. The calculation of Ni in this more general setting has been studied in detail by Ruml, in a slightly different context (Ruml, 2002). His solution involves using a conditional distribution for edge costs that bears a strong resemblance to our conditional distribution on heuristic values.\nBased on the work of KRE and on the insight that for PDB heuristics there is a correlation between the size of the PDB and its heuristic value distribution, a new analysis limited to PDB heuristics has been done (Korf, 2007; Breyer & Korf, 2008). The prediction is achieved based on the branching factor of the problem and the size of the PDB without knowing the actual heuristic distribution. In order to derive the heuristic distribution from the size of the PDB it was assumed that the forward and backward branching factors of the abstract space are equal and that the abstract space has a negligible number of cycles. Since the second assumption is usually not realistic this model underestimates the number of expanded nodes.\nThe KRE formula was developed to predict the performance of the IDA* algorithm. The general approach can also be applied to A* as long as appropriate modifications are made to the computations of Ni and P (v) (Korf et al., 2001; Holte & Herna\u0301dvo\u0308lgyi, 2004; Breyer & Korf, 2008). The challenge is accounting for the effect of A*\u2019s pruning of the search tree when it generates a state that it has previously reached by a path of smaller or equal\n8. A heuristic is an \u01eb-approximation if (1\u2212 \u01eb)h\u2217(s) \u2264 h(s) \u2264 (1+ \u01eb)h\u2217(s) for all states in the search space.\ncost. This is particularly challenging when the heuristic is inconsistent, because in that case the first time A* generates a state it is not guaranteed to have reached it via a least-cost path, so the state will occur more than once in A*\u2019s search tree. Indeed, in the worst case, for every state A* will enumerate all the paths to the state in decreasing order of cost, thereby generating exactly the same search tree as IDA* (Martelli, 1977). But in general, A*\u2019s pruning will reduce Ni, especially for large i, in ways that may be hard to capture in a small set of recurrence equations. The heuristic distribution over A*\u2019s entire search tree, taken to its maximum depth, is, for consistent heuristics, the overall distribution (Korf et al., 2001) since each state occurs exactly once in A*\u2019s search tree (as just observed, this is not true for inconsistent heuristics). This does not imply that the overall distribution can be used to good effect on a level-by-level basis, but its use in the KRE formula did result in accurate predictions of A*\u2019s performance on the 15-puzzle for two different consistent heuristics when used together with an exact calculation of Ni for A*\u2019s search tree (Breyer & Korf, 2008)."}, {"heading": "12. Conclusions and Future Work", "text": "Historically, heuristics were characterized by their average. KRE introduced the idea of characterizing heuristics by their unconditional heuristic distribution and presented their formula to predict the number of nodes expanded on one iteration of IDA* based on the unconditional heuristic distribution. The work we have presented in this paper takes another step along this line. The conditional distribution we have introduced, and the prediction formula CDP based on it, advance our understanding of how properties of a heuristic affect the performance of IDA*.\nOur CDP method advances KRE by improving its predictions at shallow depths, on a wider range of sets of start states, and for inconsistent heuristics. We have also shown how to use it to make an accurate prediction for a single start state and for an IDA* search that uses BPMX heuristic value propagation.\nOf course, with the more sophisticated methods, more preprocessing is needed and special care must be taken when gathering the data in order to get a reliable sample. It is much easier to calculate the average of the heuristic than to calculate a 3-dimensional matrix. On the other hand, the latter approach better characterizes the heuristic and enables generating accurate predictions for a larger variety of circumstances.\nFuture work will address a number of issues. It is not yet clear what attributes make the best context for prediction, and how this is influenced by the choice of the heuristic and by the attributes of the specific domain. Larger contexts (more parameters) will probably provide better prediction at a cost of more pre-processing. This tradeoff needs to be further studied. Another direction will aim to extend this analysis approach to predict the performance of other search algorithms such as A*."}, {"heading": "13. Acknowledgments", "text": "This research was supported by grant number 728/06 and 305/09 from the Israeli Science Foundation (ISF) to Ariel Felner. Robert Holte and Neil Burch gratefully acknowledge the ongoing support for this work from Canada\u2019s Natural Sciences and Engineering Research\nCouncil (NSERC) and Alberta\u2019s Informatics Circle of Research Excellence (iCORE). The code for Rubik\u2019s Cube in this paper is based on the implementation of Richard E. Korf used in his seminal work on this domain(Korf, 1997). We thank the anonymous reviewer who encouraged us to widen our experimental results and to better explain the results of KRE and their relation to our results. His/her comments clearly improved the strength of this paper. Thanks also to Sandra Zilles for her careful checking of the details in Section 4."}], "references": [{"title": "Recent results in analyzing the performance of heuristic search", "author": ["T. Breyer", "R. Korf"], "venue": "In Proceedings of the First International Workshop on Search in Artificial Intelligence and Robotics (held in conjunction with AAAI),", "citeRegEx": "Breyer and Korf,? \\Q2008\\E", "shortCiteRegEx": "Breyer and Korf", "year": 2008}, {"title": "High-performance A* search using rapidly growing heuristics", "author": ["S.V. Chenoweth", "H.W. Davis"], "venue": "In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence", "citeRegEx": "Chenoweth and Davis,? \\Q1991\\E", "shortCiteRegEx": "Chenoweth and Davis", "year": 1991}, {"title": "Efficiently searching the 15-puzzle", "author": ["J.C. Culberson", "J. Schaeffer"], "venue": "Tech. rep. 94-08,", "citeRegEx": "Culberson and Schaeffer,? \\Q1994\\E", "shortCiteRegEx": "Culberson and Schaeffer", "year": 1994}, {"title": "On the value of good advice: The complexity of A* search with accurate heuristics", "author": ["H.T. Dinh", "A. Russell", "Y. Su"], "venue": "In Proceedings of the Twenty-Second Conference on Artificial Intelligence", "citeRegEx": "Dinh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2007}, {"title": "Planning with pattern databases", "author": ["S. Edelkamp"], "venue": "In Proceedings of the 6th European Conference on Planning (ECP-01),", "citeRegEx": "Edelkamp,? \\Q2001\\E", "shortCiteRegEx": "Edelkamp", "year": 2001}, {"title": "Prediction of regular search tree growth by spectral analysis", "author": ["S. Edelkamp"], "venue": "In Advances in Artificial Intelligence, Joint German/Austrian Conference on AI,", "citeRegEx": "Edelkamp,? \\Q2001\\E", "shortCiteRegEx": "Edelkamp", "year": 2001}, {"title": "Additive pattern database heuristics", "author": ["A. Felner", "R.E. Korf", "S. Hanan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Felner et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Felner et al\\.", "year": 2004}, {"title": "Compressed pattern databases", "author": ["A. Felner", "R.E. Korf", "R. Meshulam", "R.C. Holte"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Felner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Felner et al\\.", "year": 2007}, {"title": "Dual lookups in pattern databases", "author": ["A. Felner", "U. Zahavi", "J. Schaeffer", "R.C. Holte"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Felner et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Felner et al\\.", "year": 2005}, {"title": "Performance Measurement and Analysis of Certain Search Algorithms", "author": ["J. Gaschnig"], "venue": "Ph.D. thesis,", "citeRegEx": "Gaschnig,? \\Q1979\\E", "shortCiteRegEx": "Gaschnig", "year": 1979}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P.E. Hart", "N.J. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "How good is almost perfect", "author": ["M. Helmert", "G. R\u00f6ger"], "venue": "In Proceedings of the Twenty-Third Conference on Artificial Intelligence", "citeRegEx": "Helmert and R\u00f6ger,? \\Q2008\\E", "shortCiteRegEx": "Helmert and R\u00f6ger", "year": 2008}, {"title": "Maximizing over multiple pattern databases speeds up heuristic search", "author": ["R.C. Holte", "A. Felner", "J. Newton", "R. Meshulam", "D. Furcy"], "venue": "Artificial Intelligence,", "citeRegEx": "Holte et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Holte et al\\.", "year": 2006}, {"title": "Steps towards the automatic creation of search heuristics", "author": ["R.C. Holte", "I.T. Hern\u00e1dv\u00f6lgyi"], "venue": "Tech. rep. TR04-02,", "citeRegEx": "Holte and Hern\u00e1dv\u00f6lgyi,? \\Q2004\\E", "shortCiteRegEx": "Holte and Hern\u00e1dv\u00f6lgyi", "year": 2004}, {"title": "Probabilistic analysis of the complexity of A", "author": ["N. Huyn", "R. Dechter", "J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "Huyn et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Huyn et al\\.", "year": 1980}, {"title": "Searching for an optimal path in a tree with random costs", "author": ["R.M. Karp", "J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "Karp and Pearl,? \\Q1983\\E", "shortCiteRegEx": "Karp and Pearl", "year": 1983}, {"title": "Depth-first iterative-deepening: An optimal admissible tree search", "author": ["R.E. Korf"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf,? \\Q1985\\E", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Finding optimal solutions to Rubik\u2019s Cube using pattern databases", "author": ["R.E. Korf"], "venue": "In Proceedings of the Fourteenth Conference on Artificial Intelligence", "citeRegEx": "Korf,? \\Q1997\\E", "shortCiteRegEx": "Korf", "year": 1997}, {"title": "Analyzing the performance of pattern database heuristics", "author": ["R.E. Korf"], "venue": "In Proceedings of the Twenty-Second Conference on Artificial Intelligence", "citeRegEx": "Korf,? \\Q2007\\E", "shortCiteRegEx": "Korf", "year": 2007}, {"title": "Disjoint pattern database heuristics", "author": ["R.E. Korf", "A. Felner"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf and Felner,? \\Q2002\\E", "shortCiteRegEx": "Korf and Felner", "year": 2002}, {"title": "Complexity analysis of admissible heuristic search", "author": ["R.E. Korf", "M. Reid"], "venue": "In Proceedings of the Fifteenth Conference on Artificial Intelligence", "citeRegEx": "Korf and Reid,? \\Q1998\\E", "shortCiteRegEx": "Korf and Reid", "year": 1998}, {"title": "On the complexity of admissible search algorithms", "author": ["A. Martelli"], "venue": "Artificial Intelligence,", "citeRegEx": "Martelli,? \\Q1977\\E", "shortCiteRegEx": "Martelli", "year": 1977}, {"title": "An expected-cost analysis of backtracking and non-backtracking algorithms", "author": ["C.J.H. McDiarmid", "G.M. Provan"], "venue": "In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence", "citeRegEx": "McDiarmid and Provan,? \\Q1991\\E", "shortCiteRegEx": "McDiarmid and Provan", "year": 1991}, {"title": "Memory-efficient A* heuristics for multiple sequence alignment", "author": ["M. McNaughton", "P. Lu", "J. Schaeffer", "D. Szafron"], "venue": "In Proceedings of the Eighteenth Conference on Artificial Intelligence", "citeRegEx": "McNaughton et al\\.,? \\Q2002\\E", "shortCiteRegEx": "McNaughton et al\\.", "year": 2002}, {"title": "A heuristic search algorithm with modifiable estimate", "author": ["L. M\u00e9ro"], "venue": "Artificial Intelligence,", "citeRegEx": "M\u00e9ro,? \\Q1984\\E", "shortCiteRegEx": "M\u00e9ro", "year": 1984}, {"title": "Heuristics: Intelligent Search Strategies for Computer Problem Solving", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1984\\E", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "Heuristic search viewed as path finding in a graph", "author": ["I. Pohl"], "venue": "Artificial Intelligence,", "citeRegEx": "Pohl,? \\Q1970\\E", "shortCiteRegEx": "Pohl", "year": 1970}, {"title": "Practical and theoretical considerations in heuristic search algorithms", "author": ["I. Pohl"], "venue": "Machine Intelligence,", "citeRegEx": "Pohl,? \\Q1977\\E", "shortCiteRegEx": "Pohl", "year": 1977}, {"title": "Finding a shortest solution for the n \u00d7 n extension of the 15-puzzle is intractable", "author": ["D. Ratner", "M.K. Warmuth"], "venue": "In Proceedings of the Fifth Conference on Artificial Intelligence", "citeRegEx": "Ratner and Warmuth,? \\Q1986\\E", "shortCiteRegEx": "Ratner and Warmuth", "year": 1986}, {"title": "Adaptive Tree Search", "author": ["W. Ruml"], "venue": "Ph.D. thesis,", "citeRegEx": "Ruml,? \\Q2002\\E", "shortCiteRegEx": "Ruml", "year": 2002}, {"title": "Average-case analysis of best-first search in two representative directed acyclic graphs", "author": ["A.K. Sen", "A. Bagchi", "W. Zhang"], "venue": "Artificial Intelligence,", "citeRegEx": "Sen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2004}, {"title": "Predicting the performance of IDA* with conditional distributions", "author": ["U. Zahavi", "A. Felner", "N. Burch", "R.C. Holte"], "venue": "In Proceedings of the Twenty-Third Conference on Artificial Intelligence", "citeRegEx": "Zahavi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zahavi et al\\.", "year": 2008}, {"title": "Dual search in permutation state spaces", "author": ["U. Zahavi", "A. Felner", "R. Holte", "J. Schaeffer"], "venue": "In Proceedings of the Twenty-First Conference on Artificial Intelligence", "citeRegEx": "Zahavi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zahavi et al\\.", "year": 2006}, {"title": "Duality in permutation state spaces and the dual search algorithm", "author": ["U. Zahavi", "A. Felner", "R.C. Holte", "J. Schaeffer"], "venue": "Artificial Intelligence,", "citeRegEx": "Zahavi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zahavi et al\\.", "year": 2008}, {"title": "Space-efficient memory-based heuristics", "author": ["R. Zhou", "E.A. Hansen"], "venue": "In Proceedings of the Nineteenth Conference on Artificial Intelligence", "citeRegEx": "Zhou and Hansen,? \\Q2004\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Heuristic search algorithms such as A* (Hart, Nilsson, & Raphael, 1968) and IDA* (Korf, 1985) are guided by the cost function f(n) = g(n)+h(n), where g(n) is the actual distance from the start state to state n and h(n) is a heuristic function estimating the cost from n to the nearest goal state.", "startOffset": 81, "endOffset": 93}, {"referenceID": 17, "context": "Prior to KRE, the standard method for comparing two heuristic functions was to compare their average values, with preference being given to the heuristic with the larger average (Korf, 1997; Korf & Felner, 2002; Felner, Korf, Meshulam, & Holte, 2007).", "startOffset": 178, "endOffset": 250}, {"referenceID": 8, "context": "Finally, we adapt CDP to make predictions when IDA* is augmented with the bidirectional pathmax method (BPMX) (Felner et al., 2005).", "startOffset": 110, "endOffset": 131}, {"referenceID": 17, "context": "34847 (Korf, 1997).", "startOffset": 6, "endOffset": 18}, {"referenceID": 16, "context": "2 Iterative Deepening A* Iterative deepening A* (IDA*) (Korf, 1985) performs a series of depth-first searches, increasing a cost threshold d each time.", "startOffset": 55, "endOffset": 67}, {"referenceID": 17, "context": "Pattern databases have proven very useful for finding lower bounds for combinatorial puzzles (Korf, 1997; Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner, Korf, & Hanan, 2004; Felner et al., 2007).", "startOffset": 93, "endOffset": 205}, {"referenceID": 7, "context": "Pattern databases have proven very useful for finding lower bounds for combinatorial puzzles (Korf, 1997; Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner, Korf, & Hanan, 2004; Felner et al., 2007).", "startOffset": 93, "endOffset": 205}, {"referenceID": 8, "context": "An exact definition and explanations about the dual lookup is provided in the original papers (Felner et al., 2005; Zahavi et al., 2006, 2008).", "startOffset": 94, "endOffset": 142}, {"referenceID": 8, "context": "This propagation technique is called bidirectional pathmax (BPMX) (Felner et al., 2005; Zahavi et al., 2007).", "startOffset": 66, "endOffset": 108}, {"referenceID": 24, "context": "1 Bidirectional Pathmax (BPMX) Traditional pathmax (M\u00e9ro, 1984) propagates heuristic values from a parent to its children, and can be applied in any state space.", "startOffset": 51, "endOffset": 63}, {"referenceID": 27, "context": "Many other papers in this line have appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, & Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid & Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & R\u00f6ger, 2008).", "startOffset": 51, "endOffset": 271}, {"referenceID": 9, "context": "Many other papers in this line have appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, & Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid & Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & R\u00f6ger, 2008).", "startOffset": 51, "endOffset": 271}, {"referenceID": 25, "context": "Many other papers in this line have appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, & Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid & Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & R\u00f6ger, 2008).", "startOffset": 51, "endOffset": 271}, {"referenceID": 27, "context": "They found that if the relative error, |h(n)\u2212h \u2217(n)| h(n) , is constant, the search complexity will be exponential (in the length of solution path) but if the absolute error, |h(n) \u2212 h\u2217(n)|, is bounded by a constant the search complexity is linear (Pohl, 1977; Gaschnig, 1979).", "startOffset": 248, "endOffset": 276}, {"referenceID": 9, "context": "They found that if the relative error, |h(n)\u2212h \u2217(n)| h(n) , is constant, the search complexity will be exponential (in the length of solution path) but if the absolute error, |h(n) \u2212 h\u2217(n)|, is bounded by a constant the search complexity is linear (Pohl, 1977; Gaschnig, 1979).", "startOffset": 248, "endOffset": 276}, {"referenceID": 14, "context": "Since it is difficult to guarantee precise bounds on the magnitude of errors produced by a given heuristic, a probabilistic characterization of these magnitudes was suggested (Huyn et al., 1980; Pearl, 1984).", "startOffset": 175, "endOffset": 207}, {"referenceID": 25, "context": "Since it is difficult to guarantee precise bounds on the magnitude of errors produced by a given heuristic, a probabilistic characterization of these magnitudes was suggested (Huyn et al., 1980; Pearl, 1984).", "startOffset": 175, "endOffset": 207}, {"referenceID": 22, "context": "The first analysis in this line, focusing on the effect of errors on the performance of search algorithms, was done by Pohl (1970). Many other papers in this line have appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, & Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid & Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & R\u00f6ger, 2008).", "startOffset": 119, "endOffset": 131}, {"referenceID": 8, "context": "Many other papers in this line have appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, & Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid & Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & R\u00f6ger, 2008). These works usually assume an abstract model space of a tree where every node has exactly b children and aim to provide the asymptotic estimation for the number of expanded nodes. They mainly differ by the model assumptions (e.g. binary or non-binary trees) and for what case the results are derived (worst case or average case). Worst case analysis showed that there is a correlation between the heuristic errors and the search complexity. They found that if the relative error, |h(n)\u2212h \u2217(n)| h(n) , is constant, the search complexity will be exponential (in the length of solution path) but if the absolute error, |h(n) \u2212 h\u2217(n)|, is bounded by a constant the search complexity is linear (Pohl, 1977; Gaschnig, 1979). Three main assumptions used by Pohl (1977) are that the branching factor is assumed to be constant across inputs, that there is a single goal state and that there are no transpositions in the search space.", "startOffset": 64, "endOffset": 1035}, {"referenceID": 1, "context": "Additional research in this line was conducted by Chenoweth and Davis (1991). Instead of using the IID model, they suggested using the \u201cNC model\u201d, which places no constraints on the errors of h.", "startOffset": 50, "endOffset": 77}, {"referenceID": 29, "context": "By contrast, Sen et al. (2004) presented a general technique for extending the analysis of the average case performance of A* from search spaces that are trees to search spaces that are directed acyclic graphs.", "startOffset": 13, "endOffset": 31}, {"referenceID": 3, "context": "Recent research in this line, analyzing the complexity of the A* algorithm was presented by Dinh et al. (2007). This research presented both worst and average case analysis for the performance of A* for approximately accurate heuristics8 for search problems with multiple solutions.", "startOffset": 92, "endOffset": 111}, {"referenceID": 29, "context": "The calculation of Ni in this more general setting has been studied in detail by Ruml, in a slightly different context (Ruml, 2002).", "startOffset": 119, "endOffset": 131}, {"referenceID": 18, "context": "Based on the work of KRE and on the insight that for PDB heuristics there is a correlation between the size of the PDB and its heuristic value distribution, a new analysis limited to PDB heuristics has been done (Korf, 2007; Breyer & Korf, 2008).", "startOffset": 212, "endOffset": 245}, {"referenceID": 21, "context": "Indeed, in the worst case, for every state A* will enumerate all the paths to the state in decreasing order of cost, thereby generating exactly the same search tree as IDA* (Martelli, 1977).", "startOffset": 173, "endOffset": 189}, {"referenceID": 17, "context": "Korf used in his seminal work on this domain(Korf, 1997).", "startOffset": 44, "endOffset": 56}], "year": 2010, "abstractText": "Korf, Reid, and Edelkamp introduced a formula to predict the number of nodes IDA* will expand on a single iteration for a given consistent heuristic, and experimentally demonstrated that it could make very accurate predictions. In this paper we show that, in addition to requiring the heuristic to be consistent, their formula\u2019s predictions are accurate only at levels of the brute-force search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula. We then propose a new formula that works well without these requirements, i.e., it can make accurate predictions of IDA*\u2019s performance for inconsistent heuristics and if the heuristic values in any level do not obey the unconditional distribution. In order to achieve this we introduce the conditional distribution of heuristic values which is a generalization of their unconditional heuristic distribution. We also provide extensions of our formula that handle individual start states and the augmentation of IDA* with bidirectional pathmax (BPMX), a technique for propagating heuristic values when inconsistent heuristics are used. Experimental results demonstrate the accuracy of our new method and all its variations.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}