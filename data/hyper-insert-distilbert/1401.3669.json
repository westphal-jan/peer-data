{"id": "1401.3669", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Hrebs and Cohesion Chains as similar tools for semantic text properties research", "abstract": "in this unified study topic it is often proven that before the descriptive hrebs being used in computational denotation analysis formulation of texts classification and cohesion chains ( formerly defined appropriately as a fusion transformation between lexical chains and coreference chains ) literally represent approximately similar synthetic linguistic mapping tools. such this dynamic result gives brings us the possibility to extend to spatial cohesion chains ( ccs ) some important directional indicators as, for example the linear kernel of empirical ccs, the topicality of giving a cc, target text concentration, temporal cc - diffuseness and mean diffuseness indicative of the text. what let us even mention that seemingly nowhere falls in concerning the lexical chains linking or coreference chains literature these three kinds of indicators are merely introduced broadly and used progressively since now. similarly, theoretically some complicated applications of mathematical ccs in modelling the parallel study work of a text ( as for example xml segmentation or summarization methodology of a text ) could be briefly realized starting again from ordinary hrebs. as an an illustration reconstruction of the similarity between academic hrebs and ccs a detailed analyze out of within the poem \" lacul \" discovered by mihai eminescu it is still given.", "histories": [["v1", "Wed, 15 Jan 2014 17:01:36 GMT  (13kb)", "http://arxiv.org/abs/1401.3669v1", "13 pages, KNOWLEDGE ENGINEERING: PRINCIPLES AND TECHNIQUES Proceedings of the International Conference on Knowledge Engineering, Principles and Techniques, KEPT 2013 Cluj-Napoca (Romania)"]], "COMMENTS": "13 pages, KNOWLEDGE ENGINEERING: PRINCIPLES AND TECHNIQUES Proceedings of the International Conference on Knowledge Engineering, Principles and Techniques, KEPT 2013 Cluj-Napoca (Romania)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["d tatar", "m lupea", "e kapetanios"], "accepted": false, "id": "1401.3669"}, "pdf": {"name": "1401.3669.pdf", "metadata": {"source": "CRF", "title": "HREBS AND COHESION CHAINS AS SIMILAR TOOLS FOR SEMANTIC TEXT PROPERTIES RESEARCH", "authors": ["DOINA TATAR", "MIHAIELA LUPEA"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\n36 69\nv1 [\ncs .C\nL ]\n1 5\nJa n"}, {"heading": "1. Introduction", "text": "Denotation analysis is a complex discipline concerned with the mutual relationships of sentences. An important tool used in Denotation analysis is the concept of hreb defined in [10] as a discontinuous text unit that can be presented in a set form or a list form, when the order is important. A hreb contains all entities denoting the same real entity or referring to one another in the text. This basic concept is baptized in this way in honor of L. Hebek ([3]) who introduced measurement in the domain of Denotation analysis, as it is known in Quantitative Linguistics. As we will show, the concepts as Lexical Chain or Coreference Chain (as in Computational Linguistics) subsume the notion of hrebs in the variant of word-hrebs. In fact, we are interested in this paper only in the notion of word-hrebs (for other kinds of hrebs: morpheme-hrebs, phrase-hrebs and sentence-hrebs see [10], [15]).\nWe will operate with the concept of Cohesion Chain (CC), defined as a Lexical Chain or a Coreference Chain, and will show the relationship between CCs and hrebs (more exactly a slow modified kind of word-hrebs, quasi-hrebs). Due to this relation, some denotational properties of a text defined using hrebs could be translated to CCs, in the benefit of the last ones. Similarly, some applications of CCs in the study of\n2000 Mathematics Subject Classification. 68T50,03H65. Key words and phrases. Lexical Chains, Coreference Chains, Hrebs, Text segmentation, Text\nsummarization.\nc\u00a92011 Babes\u0327-Bolyai University, Cluj-Napoca\n1\n2 DOINA TATAR(1), MIHAIELA LUPEA(2), AND EPAMINONDAS KAPETANIOS(3)\na text (as for example segmentation or summarization of a text) could be realized starting from quasi-hrebs.\nThe structure of the paper is as follows: Section 2 presents the concept of hreb and some indicators of a text connected with it. In Section 3 the Lexical Chains, the Coreference Chains, and their use in segmentation and summarization are introduced. In Section 4 we analyze a poem by Eminescu from the point of view of word-hrebs (as in [10]) and CCs. The paper ends with some conclusions and further work proposal."}, {"heading": "2. Hrebs", "text": "A word-hreb contains all the words which are synonyms or refer to one of the synonyms. The hrebs usually are constructed using some rules such that a word belongs to one or more hrebs [10]. For example a verb with personal ending (1st and 2nd person) belongs both to the given verb and to the person (subject) it overtly refer to. We will slightly modify the definition of a hreb eliminating the above syntactical constraint and will denote the new concept by quasi-hreb. Namely, for us verbs with personal ending (1st and 2nd person) belong to the given verb and don\u2019t have any connection with the hreb representing the subject of these verbs. In this way, a word belongs to only one quasi-hreb, similarly with the property that a word belongs to only one Lexical Chain or Reference Chain (Coherence Chain). The rest of the properties of hrebs mentioned in [10] are unmodified for quasi-hreb: references belong to the quasi-hreb of the word they refer to, e.g. pronouns and Named Entities belong to the basic word; synonyms constitute a common quasi-hreb; articles and prepositions are not considered; adverbs may coincide with adjectives, and may belong to the same quasi-hreb.\nAccording to the information and ordering of entities, [15] defines five kinds of hrebs:\n(1) Data-hreb containing the raw data, e.g. words, and the position of each unit in text.\n(2) List-hreb containing the data but without the positions of the units in the text.\n(3) Set-hreb being the set containing only the lemmas (for word-hrebs). (4) Ordered set-hreb is identical with (3) but the units are ordered according to a certain principle, e.g. alphabetically, or according to length, frequency, etc. (5) Ordered position-hreb containing only the positions of units in the given text. In our example in Section 4 we will use only the cases 1, 2 and 3. Complete word-hreb analyses of several texts can be found in [15].\n2.1. Denotational analysis with hrebs. Creating hrebs means a reduction of the text to its fundamental semantic components. Having defined them one can make statements both about the text and the hrebs themselves and obtain new indicators. A short introduction in these indicators is given below (for a complete presentation see [10]):\n1. By lemmatizing the words occurring in a List-hreb, and eliminating the duplicates, the corresponding Set-hreb is obtained. If in a Set-hreb there are at least two"}, {"heading": "HREBS AND COHESION CHAINS AS SIMILAR TOOLS FOR SEMANTIC TEXT PROPERTIES RESEARCH3", "text": "words (different lemmas), then the hreb belongs to the Kernel (core) of the text, i.e. if |hrebi| \u2265 2 then hrebi \u2208 Kernel. The hrebs of aKernel will be called kernel hrebs.\n2. An important indicator of a text is the size of theKernel, denoted by |Kernel|. 3. Topicality of a set-kernel hreb Hi, is calculated as:\nT (Hi) = |Hi|\n|Kernel| 4. Kernel concentration is defined as the size of the kernel divided by the total\nnumber n of hrebs in the text:\nKC = |Kernel|\nn 5. Text concentration is calculated based on the List-hrebs. If Hi is a List-hreb (containing all word-forms, not only lemmas) and L is the number of tokens in the text, then pi = |Hi|/L is the relative frequency of the List-hrebHi. Text concentration TC is given as:\nTC =\nn\u2211\ni=1\np2i\nRelative text concentration, TCrel is defined as:\nTCrel = 1\u2212\n\u221a TC\n1\u2212 1/\u221an 6. Hreb diffuseness The diffuseness DH of a given hreb H with nH elements, where the positions of tokens are (in an ascending order) P = {pos1, ..., posnH}, is defined using the maximal and minimal position of tokens occurring in it:\nDH = posnH \u2212 pos1\nnH i.e. the difference of the last and the first position divided by the cardinal number of the hreb.\n7. Mean diffuseness of the text is:\nDText = 1\nK\nK\u2211\nj=1\nDHj\nwhere K is the number of kernel-hrebs (|Kernel|) in Text. 8. Finally, text compactness is defined as:\nC = 1\u2212 n/L 1\u2212 1/L\nwhere n is the number of hrebs in the text and L is the number of (word-)tokens.\n4 DOINA TATAR(1), MIHAIELA LUPEA(2), AND EPAMINONDAS KAPETANIOS(3)"}, {"heading": "3. Cohesion Chains", "text": "3.1. Lexical Chains. Lexical Chains (LCs) are sequences of words which are in a lexical cohesion relation with each other and they tend to indicate portions of a text that form semantic units ([8], [11], [5]). The most frequent lexical cohesion relations are the synonymy and the repetition, but could be also hypernyms, hyponyms, etc.. Lexical cohesion relationships between the words of LCs are established using an auxiliary knowledge source such as a dictionary or a thesaurus.\nA Lexical Chain could be formalized as:\nLCi : [LC 1 i (Tokenj), \u00b7 \u00b7 \u00b7 , LCmi (Tokenk)]\nwhere the first element of the chain LCi is the word LC 1 i , representing the token with the number j in the text, the last element of the chain LCi is the word LC m i , representing the token with the number k in the text (where j < k), the length of the chain LCi is m. Because the analyze is made on the level of sentences, usually the sentences where the words occur are indicated. The representation in this case is:\nLCi : [LC 1 i (Sj), \u00b7 \u00b7 \u00b7 , LCmi (Sk)]\nThe first element of the chain LCi is the word LC 1 i , and occurs in the sentence Sj , the last element of the chain LCi is the word LC m i , and occurs in the sentence Sk of the text (where j < k). LCs could further serve as a basis for Text segmentation and Text summarization (see [4]). The first paper which used LCs (manually built) to indicate the structure of a text was that of Morris and Hirst ([7]), and it relies on the hierarchical structure of Roget\u2019s thesaurus to find semantic relations between words. Since the chains are used to structure the text according to the attentional/intentional theory of Grosz and Sidner theory, ([1]), their algorithm divides texts into segments which form hierarchical structures (each segment is represented by the span of a LC). Some algorithms for linear segmentation (as opposite to hierarchical segmentation) are given in [12], [13], [14]. In all these algorithms it is applied the following remark of Hearst 1997 [2]: There are certain points at which there may be radical changes in space, time, character configuration, event structure(...). At points where all of these change in a maximal way, an episode boundary is strongly present. The algorithms are based on different ways of scoring the sentences of a text and then observing the graph of the score function. In this paper we introduce two new scoring functions for sentences (in the next subsection).\nLet us remark that linear segmentation and the (extractive) summarization are two interdependent goals: good segmentation of a text could improve the summarization ([4]). Moreover, the rule of extracting sentences from the segments is decisive for the quality of the summary. Some largely applied strategies (rules) are ([12]):\n1. The first sentence of a segment is selected. 2. For each segment the sentence with a maximal score is considered the most important for this segment, and hence it is selected (for example, the minima in the"}, {"heading": "HREBS AND COHESION CHAINS AS SIMILAR TOOLS FOR SEMANTIC TEXT PROPERTIES RESEARCH5", "text": "graph of the below Score1 and Score2 functions represent the sentences candidates for boundaries between segments of a text).\n3. From each segment the most informative sentence (the least similar) relative to the previously selected sentences is picked up.\nThus, one can say that determining a segmentation of a text and selecting a strategy (1, 2 or 3), a summary of the text can be obtained, as well.\n3.2. Coreference Chains. Coreference Chains are chains of antecedents-anaphors of a text. A complete study of Coreference Chains is the textbook [6]. A Coreference Chain contains the occurrences of the entities identified as antecedents for a given anaphor and also the occurrences of this anaphor.\nThe formalization of a Coreference Chain is as follows:\nCRi : [CR 1 i (Tokenj), \u00b7 \u00b7 \u00b7 , CRmi (Tokenk)], (where j < k )\nor\nCRi : [CR 1 i (Sj), \u00b7 \u00b7 \u00b7 , CRmi (Sk)], (where j < k )\ndepending on the marks (tokens or sentences) picked out. In the same way as the Lexical Chains, Coreference Chains express the cohesion of a text. The algorithms of segmentation (and summarization) of a text based on Lexical Chains could be adapted for Coreference Chains. In this paper we refer to both Lexical Chains and Coreference Chains by the name of Cohesion Chains.\n3.3. Scoring the sentences by Cohesion Chains. Cohesion Chains (CCs) defined as in the above sections could be used to score the sentences such that when this score is low, cohesion is low, and thus the sentence is a candidate for a boundary between segments; similarly for a high score (a high cohesion) and the non-boundary feature of a sentence. In this paper we propose the following two new functions of score for sentences:\nScore1(Si) = the number of tokens in Si contained in at least oneCC\nthe number of tokens in Si.\nLet us remark that 0 \u2264 Score1(Si) \u2264 1. When Score1(Si) = 0 (or close to 0), Si is a candidate for a boundary between segments because Si has a low connection with other sentences. When Score1(Si) = 1 (or close to 1), Si is \u201dvery\u201d internal for a segment. So, observing the graph of function Score1(Si) = we could determine the segments of a text.\nThe second proposed scoring function is:\nScore2(Si) = the number of CCswhich traverse Si the total number of CCs in the text\nAgain 0 \u2264 Score2(Si) \u2264 1 and the above remarks remain valid: when Score2(Si) = is 0 (or close to 0), Si is a candidate for a boundary between segments because Si has a low connection with the others sentences. When Score2(Si) = is 1 (or close to 1), Si is \u201dvery\u201d internal for a segment.\n6 DOINA TATAR(1), MIHAIELA LUPEA(2), AND EPAMINONDAS KAPETANIOS(3)\nAs a final remark, let us observe that the hrebs (quasi-hrebs) could be used exactly in the same way to score the sentences: it is enough to put quasi-hrebs instead of CCs in the definitions for Score1(Si) and Score\n2(Si). Thus, hrebs (quasi-hrebs) could serve to segment and/or summarize texts.\nIn the same way, the indicators 1-8 used in Denotational analysis with hrebs could be extended to CCs. Let us remark that quasi-hrebs (and thus CCs) are defined in the Data-hrebs format. This is accordingly with the definition of Lexical Chains where the most important (frequent) lexical relation which is present in a Lexical Chain is the repetition [11]. The more frequently a word is repeated in a Lexical Chain, the more important this Lexical Chain is. Obtaining CCs from Data-hrebs (duplicates are not eliminated), we will impose the condition to a kernel CC to have at least a given number of elements. In other words, a kernel CC must contain a size bigger than a minimal one. Further, the topicality of a kernel CC, text concentration, CC-diffuseness and mean diffuseness of the text could be defined.\nLet us mention that nowhere in the Lexical Chains or Coreference Chains literature these kinds of indicators are introduced up to now."}, {"heading": "4. Example in Romanian", "text": "For the Eminescu\u2019s poem \u201dLacul\u201d we will exemplify hrebs, quasi-hrebs and CCs, and the relationships between them. We will begin with the Rules for hreb formation in Romanian language [10].\nRules of hrebs formation for the Romanian language\nThe Rules for hrebs are of the form: a \u2208 B. Here a is an expression containing a special element called pos indicator which is written in italic (pos is for part of speech). Particularly, a could be formed only from the pos indicator. B is a (name for a) given hreb written with capital letters. More exactly, the Rule a \u2208 B means: a (or pos indicator of a) is an element of the hreb B . The connection between a and B will result from the word used for pos indicator. As a word-form could be contained in more then one hreb, in the application of rules it is possible to obtain a result as: a \u2208 B,C, \u00b7 \u00b7 \u00b7 meaning: a is an element of hreb B and hreb C and \u00b7 \u00b7 \u00b7 . The rules are valid only for the pos of a being noun, verb, adjective, adverb, pronoun.\nRULES: R1. verb \u2208 V ERB R2. personal ending of a verb, which could be a noun or a pronoun, \u2208 NOUN or PRONOUN R3. synonym of a verb \u2208 V ERB R4. pronoun referring to a noun \u2208 NOUN R5. pronoun referring to a Named Entity \u2208 NAMED ENTITY . R6. synonym of a Named Entity \u2208 NAMED ENTITY . R7. non-referring pronoun \u2208 PRONOUN R8. noun \u2208 NOUN R9. synonym of a noun \u2208 NOUN R10. adjective \u2208 ADJECTIV E"}, {"heading": "HREBS AND COHESION CHAINS AS SIMILAR TOOLS FOR SEMANTIC TEXT PROPERTIES RESEARCH7", "text": "R11. synonym of an adjective \u2208 ADJECTIV E R12. adverb \u2208 ADV ERB R13.synonym of an adverb \u2208 ADV ERB\nThe Rules 1-13 could be summarized as follows: a noun, its synonyms, referring pronouns and personal endings in a verb belong all to the given noun; a Named Entity, its synonyms, referring pronouns and personal endings in a verb belong all to the given Named Entity; a verb in all its forms, its synonyms, belong to the given verb, however, the personal endings belong also to the respective noun; an adjective (adverb) and its synonyms belong all to the given adjective (adverb).\nWe illustrate the rules as applied to the poem \u201dLacul\u201d. Namely, we will make a denotation of tokens in the poem, then will extract:\n\u2022 A. Hrebs ( Table 1), \u2022 B. Quasi-hrebs (Table 2) \u2022 C. Cohesion Chains (Table 3).\nThe tokens numbered are only nouns, verbs, adjectives, adverbs, and pronouns (in this poem do not exist Named Entities).\nLACUL (denotation of tokens)\n(S1) Lacul (1) codrilor (2) albastru (3) Nuferi (4) galbeni (5) \u0131\u0302l (6) \u0131\u0302ncarca\u0306 (7). (S2)Tresa\u0306rind (8) \u0131\u0302n cercuri (9) albe (10) El (11) cutremura\u0306 (12) o barca\u0306 (13). (S3) S\u0327i eu (14) trec (15) de-a lung (16) de maluri (17), Parc-ascult (18) s\u0327i parc-as\u0327tept (19) Ea (20) din trestii (21) sa\u0306 ra\u0306sara\u0306 (22) S\u0327i sa\u0306-mi (23) cada\u0306 (24) lin (25) pe piept (26). (S4) Sa\u0306 sa\u0306rim (27) \u0131\u0302n luntrea (28) mica\u0306 (29) ,\nI\u0302ng\u0302\u0131nat\u0327i (30) de glas (31) de ape (32), S\u0327i sa\u0306 scap (33) din ma\u0302na\u0306 (34) ca\u0302rma (35), S\u0327i lopet\u0327ile (36) sa\u0306-mi (37) scape (38). (S5) Sa\u0306 plutim (39) cuprins\u0327i (40) de farmec (41) Sub lumina (42) bl\u0302\u0131ndei (43) lune (44). (S6)V\u0131\u0302ntu-n (45) trestii (46) lin (47) fos\u0327neasca\u0306 (48), Unduioasa (49) apa\u0306 (50) sune (51)! (S7)Dar nu vine (52)... (S8)Singuratic (53)\nI\u0302n zadar (54) suspin (55) s\u0327i sufa\u0306r (56) L\u0302\u0131nga\u0306 lacul (57) cel albastru (58)\nI\u0302nca\u0306rcat (59) cu flori (60) de nufa\u0306r (61).\n8 DOINA TATAR(1), MIHAIELA LUPEA(2), AND EPAMINONDAS KAPETANIOS(3)\n4.1. From Hrebs to Cohesion Chains. By the application of the above mentioned rules a total number of 51 hrebs are obtained. From all these, only 12 hrebs presented in Table 1 contain more than one element. In Table 1 the hrebs are constituted as Data-hrebs, where SDH means \u201dSize of Data-hreb\u201d and SSH means \u201dSize of Set-hreb\u201d.\nThe names of all 51 hrebs are as follows: A ASCULTA, A AS\u0327TEPTA, A I\u0302NCA\u0306RCA, A CA\u0306DEA, A CUTREMURA, A FOS\u0327NI, A PA\u0306REA, , A PLUTI, A RA\u0306SA\u0306RI, , A SA\u0306RI, A SCA\u0306PA, A SUFERI, A SUNA, A SUSPINA, A TRECE, A TRESA\u0306RI, A VENI, ALB, ALBASTRU, APA\u0306, BARCA\u0306, BLA\u0302NDA\u0306, CA\u0302RMA\u0306, CERC, CODRU, CUPRINS, EU, EA, FARMEC, FLOARE, GALBEN, GLAS, I\u0302NCA\u0306RCAT, I\u0302NGA\u0302NAT, LAC, LIN, LOPATA\u0306, LUMINA\u0306, LUNA\u0306, LUNG, MAL, MA\u0302NA\u0306, MIC, NOI, NUFA\u0306R, PIEPT, SINGURATIC, TRESTIE, UNDUIOASA\u0306, VA\u0302NT, ZADAR.\nFrom the set of Rules R1-R13, the Rule R2 makes the difference when the quasihrebs are calculated. This rule is reproduced here:\nR2. personal ending of a verb, which could be a noun or a pronoun, \u2208 NOUN or PRONOUN\nIn Table 1 are bold marked all the verbs which are contained in a NOUN or PRONOUN hreb due to the Rule R2. All these verbs are not present in Table 2, the table of quasi-hrebs. As a remark, the hreb \u201dNOI\u201d is not a quasi-hreb, because both elements ( sa\u0306rim 27, plutim 39) are obtained by Rule R2.\nLet us remember that Lexical Chains are sequences of words which are in a lexical cohesion relation (synonymy, repetition, hypernymy, hyponymy, etc) with each other. Coreference Chains are chains of antecedents-anaphors of a text. Examining Table 2 of quasi-hrebs, we observe that: the quasi-hreb EU corresponds to a Coreference Chain (eu 14, -mi 23, -mi 37), the quasi-hreb LAC to a Coreference Chain (lacul 1, il"}, {"heading": "HREBS AND COHESION CHAINS AS SIMILAR TOOLS FOR SEMANTIC TEXT PROPERTIES RESEARCH9", "text": ""}, {"heading": "A PA\u0306REA (parc- 18, parc- 19) 2 1", "text": ""}, {"heading": "A SCA\u0306PA (scap 33, scape 38) 2 1", "text": ""}, {"heading": "Denotation of CC Elements of CC Length of CC", "text": "6, el 11, lacul 57). The quasi-hreb EA is not a chain (it has only one element). The rest of quasi-hrebs represents Lexical Chains: (nuferi 4, nufa\u0306r 61), (ape 32, apa\u0306 50), (barca 13, luntrea 28), (trestii 21, trestii 46), (albastru 3, albastru 58), (parc- 18, parc- 19), (lin 25, lin 47), (scap 33, scape 38). Table 3 contains the Cohesion Chains denoted as we will use further. We obtained CCs from the Datahrebs, and the length of a Cohesion Chain is given by the SDH column, because the duplicates are not eliminated (as in SSH column).\nCalculating the scores Score1 for each sentence are obtained the following results:\nScore1(S1) = 4/7 = 0.57 Score1(S2) = 2/6 = 0.33 Score1(S3) = 6/13 = 0.46 Score1(S4) = 5/12 = 0.42 Score1(S5) = 0/6 = 0.\n10 DOINA TATAR(1), MIHAIELA LUPEA(2), AND EPAMINONDAS KAPETANIOS(3)\nScore1(S6) = 3/7 = 0.43 Score1(S7) = 0/1 = 0. Score1(S8) = 3/9 = 0.33\nTaking as segment boundaries the sentences with minimal score, the text is divided in 4 segments: Seg1 = [S1, S2];Seg2 = [S3, S5];Seg3 = [S6, S7];Seg4 = [S8] or 3 segments: Seg1 = [S1, S2];Seg2 = [S3, S5];Seg3 = [S6, S8] if mono-sentence segments are not permitted.\nScoring with Score2 formula, the results are as following:\nScore2(S1) = 3/10 = 0.30 Score2(S2) = 4/10 = 0.40 Score2(S3) = 8/10 = 0.80 Score2(S4) = 9/10 = 0.90 Score2(S5) = 6/10 = 0.60 Score2(S6) = 6/10 = 0.60 Score2(S7) = 3/10 = 0.30 Score2(S8) = 3/10 = 0.30\nThe text has only one segment [S1, S8], with the most \u201dinternal\u201d sentence S4. A summary of the poem using Score1 is formed by the sentences: S1, S3, S6 and using Score2, by the sentence S1. In both cases the rule one (Section 3.1) has been applied.\n4.2. Indicators of Cohesion Chains. Let us suggest how the indicators in Section 2.1 could be defined for the Cohesion Chains CC1 to CC10.\n\u2022 Kernel CCs : Considering the minimal size of a kernel CC being 2, all CCs are in Kernel. Considering the minimal size of a kernel CC being 3, only CC1 and CC2 are in Kernel. The last supposition is more realistic, since a CC has always at least 2 elements; \u2022 The size of the Kernel is 2, in the last above case; \u2022 Topicality of the kernel CC denoted by CC1 is 3/2 = 1.5 and topicality of CC2 is 4/2 = 2; \u2022 Kernel concentration is KC = 2/10 = 0.2; \u2022 p1 = 3/61; p2 = 4/61; pi = 2/61, i = 3 to 10. Text concentration is TC = 0.0151 and Relative Text concentration is TCRel = 1.2830; \u2022 Diffuseness for each CC is as follows: DCC1 = (37\u221214)/3 = 7.66;DCC2 = (57\u22121)/4 = 14;DCC3 = (61\u22124)/2 =\n28.5;DCC4 = (50 \u2212 32)/2 = 9;DCC5 = (28 \u2212 13)/2 = 7.5;DCC6 = (46 \u2212 21)/2 = 12.5;DCC7 = (58\u2212 3)/2 = 27.5;DCC8 = (19\u2212 18)/2 = 0.5;DCC9 = (47\u2212 25)/2 = 11;DCC10 = (38\u2212 33)/2 = 2.5\n\u2022 Mean diffuseness of the text is DText = 10.75; \u2022 Text compactness is C = (1 \u2212 10/61)/(1\u2212 1/61) = 0.8505."}, {"heading": "HREBS AND COHESION CHAINS AS SIMILAR TOOLS FOR SEMANTIC TEXT PROPERTIES RESEARCH11", "text": "The above indicators could make differences between CCs, such that some of them are kernel CCs, or have a higher topicality and/or diffuseness."}, {"heading": "5. Conclusions and Further work", "text": "Lexical Chains and Coreference Chains (CCs) are intensively studied, but few indicators are standard for them. The indicators inspired from the hrebs must be studied and adopted for CCs. These indicators, in the context of some applications using CCs, could become instruments for the evaluation of these applications and for improving them. For example, there is a large debate about how to select CCs to construct the summaries of a text: selecting long or short CCs is one of the questions. Using only kernel CCs, or kernel CCs with a high topicality and /or high diffuseness could be a solution.\nAs a general remark, Quantitative Linguistics and Computational Linguistics are considered two distinct fields with their own journals, techniques and specialists. It is important to identify those parts they have in common, and to try to extract the advantage from this commonality. This paper is a step toward this desirable aim."}], "references": [{"title": "Attention, Intentions and the Structure of Discourse", "author": ["B. Grosz", "C. Sidner"], "venue": "Computational Linguistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages", "author": ["M. Hearst"], "venue": "Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Lectures on Text Theory", "author": ["L. Hebek"], "venue": "Prague: Oriental Institute", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Natural Language Processing: semantic aspects", "author": ["E. Kapetanios", "D. Tatar", "C. Sacarea"], "venue": "Science Publishers,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Finding text boundaries and finding topic boundaries: two different tasks?", "author": ["A. Labadie", "V. Prince"], "venue": "Proceedings of GoTAL08", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Anaphora Resolution\u201d, Pearson Education, Longman", "author": ["R. Mitkov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text", "author": ["J. Morris", "G. Hirst"], "venue": "Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "WSD and text segmentation based on lexical cohesion\u201d, 755761", "author": ["M. Okumura", "T. Honda"], "venue": "Proceedings of COLING-94", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Vectors and Codes of Text", "author": ["I.I. Popescu", "J. Macutek", "E. Kelih", "R. Cech", "K.H. Best", "G. Altmann"], "venue": "Studies in Quantitative Linguistics", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Quantitative analysis of poetry", "author": ["I.I. Popescu", "M. Lupea", "D. Tatar", "G. Altmann"], "venue": "Ed. Mouton de Gruyter,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Select: a lexical cohesion based news story segmentation system", "author": ["N. Stokes", "J. Carthy", "A.F. Smeaton"], "venue": "AI Communications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Text Entailment for Logical Segmentation and Summarization\u201d, 233244", "author": ["D. Tatar", "A. Mihis", "D. Lupsa"], "venue": "Proceedings of 13th International Conference on Applications of Natural Language to Information Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Segmenting text by lexical chains distribution\u201d, 4144", "author": ["D. Tatar", "E. Tamaianu-Morita", "G. Serban-Czibula"], "venue": "Proceedings of Knowledge Engineering Principles and Techniques (KEPT),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Text summarization by Formal Concept Analysis approach", "author": ["D. Tatar", "M. Lupea", "Z. Marian"], "venue": "Proceedings of KEPT", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Denotative Textanalyse\u201d, Wien, Praesens. 1 University \u201dBabes-Bolyai\u201d, Romania E-mail address: dtatar@cs.ubbcluj.ro 2 University \u201dBabes-Bolyai\u201d, Romania E-mail address: lupea@cs.ubbcluj.ro 3 University Westminster, UK E-mail address: E.Kapetanios@westminster.ac.uk", "author": ["A. Ziegler", "G. Altmann"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}], "referenceMentions": [{"referenceID": 9, "context": "An important tool used in Denotation analysis is the concept of hreb defined in [10] as a discontinuous text unit that can be presented in a set form or a list form, when the order is important.", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "Hebek ([3]) who introduced measurement in the domain of Denotation analysis, as it is known in Quantitative Linguistics.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "In fact, we are interested in this paper only in the notion of word-hrebs (for other kinds of hrebs: morpheme-hrebs, phrase-hrebs and sentence-hrebs see [10], [15]).", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "In fact, we are interested in this paper only in the notion of word-hrebs (for other kinds of hrebs: morpheme-hrebs, phrase-hrebs and sentence-hrebs see [10], [15]).", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "In Section 4 we analyze a poem by Eminescu from the point of view of word-hrebs (as in [10]) and CCs.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "The hrebs usually are constructed using some rules such that a word belongs to one or more hrebs [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 9, "context": "The rest of the properties of hrebs mentioned in [10] are unmodified for quasi-hreb: references belong to the quasi-hreb of the word they refer to, e.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "According to the information and ordering of entities, [15] defines five kinds of hrebs: (1) Data-hreb containing the raw data, e.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Complete word-hreb analyses of several texts can be found in [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "A short introduction in these indicators is given below (for a complete presentation see [10]): 1.", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "Lexical Chains (LCs) are sequences of words which are in a lexical cohesion relation with each other and they tend to indicate portions of a text that form semantic units ([8], [11], [5]).", "startOffset": 172, "endOffset": 175}, {"referenceID": 10, "context": "Lexical Chains (LCs) are sequences of words which are in a lexical cohesion relation with each other and they tend to indicate portions of a text that form semantic units ([8], [11], [5]).", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "Lexical Chains (LCs) are sequences of words which are in a lexical cohesion relation with each other and they tend to indicate portions of a text that form semantic units ([8], [11], [5]).", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "LCs could further serve as a basis for Text segmentation and Text summarization (see [4]).", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "The first paper which used LCs (manually built) to indicate the structure of a text was that of Morris and Hirst ([7]), and it relies on the hierarchical structure of Roget\u2019s thesaurus to find semantic relations between words.", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "Since the chains are used to structure the text according to the attentional/intentional theory of Grosz and Sidner theory, ([1]), their algorithm divides texts into segments which form hierarchical structures (each segment is represented by the span of a LC).", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "Some algorithms for linear segmentation (as opposite to hierarchical segmentation) are given in [12], [13], [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "Some algorithms for linear segmentation (as opposite to hierarchical segmentation) are given in [12], [13], [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Some algorithms for linear segmentation (as opposite to hierarchical segmentation) are given in [12], [13], [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 1, "context": "In all these algorithms it is applied the following remark of Hearst 1997 [2]: There are certain points at which there may be radical changes in space, time, character configuration, event structure(.", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "Let us remark that linear segmentation and the (extractive) summarization are two interdependent goals: good segmentation of a text could improve the summarization ([4]).", "startOffset": 165, "endOffset": 168}, {"referenceID": 11, "context": "Some largely applied strategies (rules) are ([12]): 1.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "A complete study of Coreference Chains is the textbook [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "This is accordingly with the definition of Lexical Chains where the most important (frequent) lexical relation which is present in a Lexical Chain is the repetition [11].", "startOffset": 165, "endOffset": 169}, {"referenceID": 9, "context": "We will begin with the Rules for hreb formation in Romanian language [10].", "startOffset": 69, "endOffset": 73}], "year": 2016, "abstractText": "In this study it is proven that the Hrebs used in Denotation analysis of texts and Cohesion Chains (defined as a fusion between Lexical Chains and Coreference Chains) represent similar linguistic tools. This result gives us the possibility to extend to Cohesion Chains (CCs) some important indicators as, for example the Kernel of CCs, the topicality of a CC, text concentration, CCdiffuseness and mean diffuseness of the text. Let us mention that nowhere in the Lexical Chains or Coreference Chains literature these kinds of indicators are introduced and used since now. Similarly, some applications of CCs in the study of a text (as for example segmentation or summarization of a text) could be realized starting from hrebs. As an illustration of the similarity between Hrebs and CCs a detailed analyze of the poem \u201dLacul\u201d by Mihai Eminescu is given.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}