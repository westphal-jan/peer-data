{"id": "1401.3839", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks", "abstract": "lama is a classical level planning system based on heuristic forward search. its strong core feature is typically the best use consisting of a pseudo - heuristic derived extensively from landmarks, propositional rational formulas that equally must be absolutely true in assessing every general solution of a planning task. lama programming builds on replacing the fast weighted downward planning system, constructed using finite - domain rather larger than binary state variables and multi - boundary heuristic search. removing the larger latter is employed to consistently combine the landmark heuristic with having a variant meaning of compiling the old well - known ff - heuristic. both heuristics models are cost - sensitive, focusing on the high - minimum quality solutions expressed in the critical case medium where actions often have non - arbitrary uniform time cost. because a weighted a * system search implementing is used always with iteratively decreasing initial weights, proceeds so slowly that the planner driver continues efforts to search, for shared plans containing of better quality \u2026 until the search is terminated. lama showed second best performance rate among all planner planners, in the shortest sequential satisficing ahead track of building the 17th international planning workshop competition 2008. in this following paper too we present the system in detail and investigate which multiple features of lama are crucial for its performance. first we present individual results meant for resolving some of the domains used at the competition, demonstrating good behaviors and ultimately bad cases for the particular techniques widely implemented in mr lama. overall, we critically find that using meaningful landmarks improves performance, whereas the incorporation effect of bounded action costs squared into the discrete heuristic estimators proves it not to be beneficial. we show that while in some domains a small search that necessarily ignores cost systematically solves far more complicated problems, raising the complicated question of how to deal with action costs more quickly effectively in the future. evaluating the aforementioned iterated weighted a * based search implementation greatly extensively improves results, simultaneously and simultaneously shows synergy response effects interfering with the sequential use of common landmarks.", "histories": [["v1", "Thu, 16 Jan 2014 04:52:55 GMT  (341kb)", "http://arxiv.org/abs/1401.3839v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["silvia richter", "matthias westphal"], "accepted": false, "id": "1401.3839"}, "pdf": {"name": "1401.3839.pdf", "metadata": {"source": "META", "title": "The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks", "authors": ["Silvia Richter", "Matthias Westphal"], "emails": ["silvia.richter@nicta.com.au", "westpham@informatik.uni-freiburg.de"], "sections": [{"heading": null, "text": "the use of a pseudo-heuristic derived from landmarks, propositional formulas that must be true in every solution of a planning task. LAMA builds on the Fast Downward planning system, using finite-domain rather than binary state variables and multi-heuristic search. The latter is employed to combine the landmark heuristic with a variant of the well-known FF heuristic. Both heuristics are cost-sensitive, focusing on high-quality solutions in the case where actions have non-uniform cost. A weighted A\u2217 search is used with iteratively decreasing weights, so that the planner continues to search for plans of better quality until the search is terminated.\nLAMA showed best performance among all planners in the sequential satisficing track of the International Planning Competition 2008. In this paper we present the system in detail and investigate which features of LAMA are crucial for its performance. We present individual results for some of the domains used at the competition, demonstrating good and bad cases for the techniques implemented in LAMA. Overall, we find that using landmarks improves performance, whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial. We show that in some domains a search that ignores cost solves far more problems, raising the question of how to deal with action costs more effectively in the future. The iterated weighted A\u2217 search greatly improves results, and shows synergy effects with the use of landmarks."}, {"heading": "1. Introduction", "text": "In the last decade, heuristic search has become the dominant approach to domain-independent satisficing planning. Starting with the additive heuristic by Bonet and Geffner (2001), implemented in the HSP planning system, much research has been conducted in search of heuristic estimators that are efficient to calculate yet powerful in guiding the search towards a goal state. The FF planning system by Hoffmann and Nebel (2001), using a heuristic estimator based on relaxed planning graphs, broke ground by showing best performance among all fully automated systems at the International Planning Competition in 2000, and continues to be state of the art today. Ever since, heuristic-search approaches have played a prominent role in the classical or sequential satisficing tracks of the biennial competition, with Fast Downward (Helmert, 2006) winning in 2004 and SGPlan (Chen, Wah, & Hsu, 2006) placing first in 2006.\nThe LAMA planning system is the youngest member in this line, winning the sequential satisficing track at the International Planning Competition (IPC) in 2008. LAMA is a classical planning\nc\u00a92010 AI Access Foundation. All rights reserved.\nsystem based on heuristic search. It follows in the footsteps of HSP, FF, and Fast Downward and uses their earlier work in many respects. In particular, it builds on Fast Downward by extending it in three major ways:\n1. Landmarks. In LAMA, Fast Downward\u2019s causal graph heuristic is replaced with a variant of the FF heuristic (Hoffmann & Nebel, 2001) and heuristic estimates derived from landmarks. Landmarks are propositional formulas that have to become true at some point in every plan for the task at hand (Porteous, Sebastia, & Hoffmann, 2001). LAMA uses landmarks to direct search towards those states where many landmarks have already been achieved. Via preferred operators, landmarks are also used as an additional source of search control which complements the heuristic estimates. In recent work, we have shown this use of landmarks in addition to the FF heuristic to improve performance, by leading to more problems being solved and shorter solution paths (Richter, Helmert, & Westphal, 2008).\n2. Action costs. Both the landmark heuristic we proposed earlier (Richter et al., 2008) and the FF heuristic have been adapted to use action costs. However, LAMA does not focus purely on the cost-to-go, i. e., the estimated cost of reaching the goal from a given search node. There is a danger that a cost-sensitive planner may concentrate too much on finding a cheap plan, at the expense of finding a plan at all within a given time limit. LAMA weighs the estimated cost-to-go (as a measure of plan quality) against the estimated goal distance (as a measure of remaining search effort) by combining the values for the two estimates.\n3. Anytime search. LAMA continues to search for better solutions until it has exhausted the search space or is interrupted. After finding an initial solution with a greedy best-first search, it conducts a series of weighted A\u2217 searches with decreasing weights, restarting the search each time from the initial state when an improved solution is found. In recent work, we have shown this approach to be very efficient on planning benchmarks compared to other anytime methods (Richter, Thayer, & Ruml, 2010).\nAt the International Planning Competition 2008, LAMA outperformed its competitors by a substantial margin. This result was not expected by its authors, as their previous work concerning LAMA\u2019s putative core feature, the landmark heuristic (Richter et al., 2008), showed some, but not tremendous improvement over the base configuration without landmarks. This paper aims to provide a reference description of LAMA as well as an extensive evaluation of its performance in the competition.\n\u2022 Detailed description of LAMA. We present all distinguishing components of the planner in detail, describing how landmarks are generated and used in LAMA, how action costs are incorporated into the heuristic estimators and how the anytime search proceeds. Some aspects of LAMA have been presented in previous publications (Richter et al., 2008, 2010; Helmert, 2006). However, aspects that have not been adequately covered in those publications, in particular the procedure for finding landmarks, are described here in detail. Other relevant aspects described in previous work, like the landmark heuristic, are summarised for the convenience of the reader. Our aim is that this paper, together with previous ones, form a comprehensive picture of the LAMA system.\n\u2022 Experimental evaluation of LAMA. Building on this, we conduct an experimental evaluation focusing on the aspects that differentiate LAMA from predecessor systems like FF and\nFast Downward. We do not repeat comparisons published in earlier work, like the comparison between LAMA\u2019s anytime method and other anytime algorithms (Richter et al., 2010), or the comparison of LAMA\u2019s methods for handling landmarks to alternative landmark approaches (Richter et al., 2008). Instead, we aim to elicit how much the performance of the LAMA system as a whole is enhanced by each of the three distinguishing features described above (landmarks, action costs and anytime search). To answer this question, we contrast several variations of our planner using various subsets of these features.\nWe find that using cost-sensitive heuristics did not pay off on the IPC 2008 benchmark tasks. Our results show that the cost-sensitive variant of the FF heuristic used in LAMA performs significantly worse than the traditional unit-cost version of the same heuristic. Similarly, all other cost-sensitive planners in the competition fared worse than the baseline planner FF that ignored action costs, demonstrating that cost-based planning presents a considerable challenge. While we do not conduct a full analysis of the reasons for this, we showcase the problems of the cost-sensitive FF heuristic in some example domains and provide informed hypotheses for the encountered effects. Landmarks prove to be particularly helpful in this context. While in the unit-cost case landmarks only lead to a moderate increase in performance, in the case of planning with action costs they substantially improve coverage (the number of problems solved), thus effectively mitigating the problems of the cost-sensitive FF heuristic in LAMA. The anytime search significantly improves the quality of solutions throughout and even acts in synergy with landmarks in one domain."}, {"heading": "2. Preliminaries", "text": "We use a planning formalism with state variables of finite (rather than binary) range, similar to the one employed by Helmert (2009). It is based on the SAS+ planning model (Ba\u0308ckstro\u0308m & Nebel, 1995), but extends it with conditional effects. While LAMA also handles axioms in the same way as Fast Downward (Helmert, 2006), we do not formalise axioms here, since they are not important for our purposes.\nDefinition 1. Planning tasks in finite-domain representation (FDR tasks) A planning task in finite-domain representation (FDR task) is given by a 5-tuple \u3008V, s0, s?,O,C\u3009 with the following components:\n\u2022 V is a finite set of state variables, each with an associated finite domainDv. A fact is a pair \u3008v, d\u3009 (also written v 7\u2192 d), where v \u2208 V and d \u2208 Dv. A partial variable assignment s is a set of facts, each with a different variable. (We use set notation such as \u3008v, d\u3009 \u2208 s and function notation such as s(v) = d interchangeably.) A state is a variable assignment defined on all variablesV.\n\u2022 s0 is a state called the initial state.\n\u2022 s? is a partial variable assignment called the goal.\n\u2022 O is a finite set of operators. An operator \u3008pre, eff\u3009 consists of a partial variable assignment pre called its precondition, and a finite set of effects eff. Effects are triplets \u3008cond, v, d\u3009, where cond is a (possibly empty) partial variable assignment called the effect condition, v is the affected variable and d \u2208 Dv is called the new value for v.\n\u2022 C : O \u2192 N+0 is an integer-valued non-negative action cost function.\nAn operator o = \u3008pre, eff\u3009 \u2208 O is applicable in a state s if pre \u2286 s, and its effects are consistent, i. e., there is a state s\u2032 such that s\u2032(v) = d for all \u3008cond, v, d\u3009 \u2208 eff where cond \u2286 s, and s\u2032(v) = s(v) otherwise. In this case, we say that the operator o can be applied to s resulting in the state s\u2032 and write s[o] for s\u2032. For operator sequences \u03c0 = \u3008o1, . . . , on\u3009, we write s[\u03c0] for s[o1] . . . [on] (only defined if each operator is applicable in the respective state). The operator sequence \u03c0 is called a plan if s? \u2286 s0[\u03c0]. The cost of \u03c0 is the sum of the action costs of its operators, \u2211n i=1 C(oi).\nEach state variable v of a planning task in finite-domain representation has an associated directed graph called the domain transition graph, which captures the ways in which the value of v may change (Jonsson & Ba\u0308ckstro\u0308m, 1998; Helmert, 2006). The vertex set of this graph is Dv, and it contains an arc between two nodes d and d\u2032 if there exists an operator that can change the value of v from d to d\u2032. Formally:\nDefinition 2. Domain transition graph The domain transition graph (DTG) of a state variable v \u2208 V of an FDR task \u3008V, s0, s?,O,C\u3009 is the digraph \u3008Dv, A\u3009 which includes an arc \u3008d, d\u2032\u3009 iff d , d\u2032, there is an operator \u3008pre, eff\u3009 \u2208 O with \u3008cond, v, d\u2032\u3009 \u2208 eff, and for the union of conditions pre\u222a cond it holds that either it contains v = d or it does not contain v = d\u0303 for any d\u0303 \u2208 Dv."}, {"heading": "3. System Architecture", "text": "LAMA builds on the Fast Downward system (Helmert, 2006), inheriting the overall structure and large parts of the functionality from that planner. Like Fast Downward, LAMA accepts input in the PDDL2.2 Level 1 format (Fox & Long, 2003; Edelkamp & Hoffmann, 2004), including ADL conditions and effects and derived predicates (axioms). Furthermore, LAMA has been extended to handle the action costs introduced for IPC 2008 (Helmert, Do, & Refanidis, 2008). Like Fast Downward, LAMA consists of three separate components:\n\u2022 The translation module\n\u2022 The knowledge compilation module\n\u2022 The search module\nThese components are implemented as separate programs that are invoked in sequence. In the following, we provide a brief description of the translation and knowledge compilation modules. The main changes in LAMA, compared to Fast Downward, are implemented in the search module, which we discuss in detail."}, {"heading": "3.1 Translation", "text": "The translation module, short translator, transforms the PDDL input into a planning task in finitedomain representation as specified in Definition 1. The main components of the translator are an efficient grounding algorithm for instantiating schematic operators and axioms, and an invariant\nsynthesis algorithm for determining groups of mutually exclusive facts. Such fact groups are consequently replaced by a single state variable, encoding which fact (if any) from the group is satisfied in a given world state. Details on this component can be found in a recent article by Helmert (2009).\nThe groups of mutually exclusive facts (mutexes) found during translation are later used to determine orderings between landmarks. For this reason, LAMA does not use the finite-domain representations offered at IPC 2008 (object fluents), but instead performs its own translation from binary to finite-domain variables. While not all mutexes computed by the translation module are needed for the new encoding of the planning task, the module has been extended in LAMA to retain all found mutexes for their later use with landmarks.\nFurther changes we made, compared to the translation module described by Helmert, were to add the capability of handling action costs, implement an extension concerning the parsing of complex operator effect formulas, and limit the runtime of the invariant synthesis algorithm. As invariant synthesis may be time critical, in particular on large (grounded) PDDL input, we limit the maximum number of considered mutex candidates in the algorithm, and abort it, if necessary, after five minutes. Note that finding few or no mutexes does not change the way the translation module works; if no mutexes are found, the resulting encoding of the planning task contains simply the same (binary-domain) state variables as the PDDL input. When analysing the competition results, we found that the synthesis algorithm had aborted only in some of the tasks of one domain (Cyber Security)."}, {"heading": "3.2 Knowledge Compilation", "text": "Using the finite-domain representation generated by the translator, the knowledge compilation module is responsible for building a number of data structures that play a central role in the subsequent landmark generation and search. Firstly, domain transition graphs (see Definition 2) are produced which encode the ways in which each state variable may change its value through operator applications and axioms. Furthermore, data structures are constructed for efficiently determining the set of applicable operators in a state and for evaluating the values of derived state variables. We refer to Helmert (2006) for more detail on the knowledge compilation component, which LAMA inherits unchanged from Fast Downward."}, {"heading": "3.3 Search", "text": "The search module is responsible for the actual planning. Two algorithms for heuristic search are implemented in LAMA: (a) a greedy best-first search, aimed at finding a solution as quickly as possible, and (b) a weighted A\u2217 search that allows balancing speed against solution quality. Both algorithms are variations of the standard textbook methods, using open and closed lists. The greedy best-first search always expands a state with minimal heuristic value h among all open states and never expands a state more than once. In order to encourage cost-efficient plans without incurring much overhead, it breaks ties between equally promising states by preferring those states that are reached by cheaper operators, i. e., taking into account the last operator on the path to the considered state in the search space. (The cost of the entire path could only be used at the expense of increased time or space requirements, so that we do not consider this.) Weighted A\u2217 search (Pohl, 1970) associates costs with states and expands a state with minimal f \u2032-value, where f \u2032 = w \u00b7 h + g, the weight w is an integer \u2265 1, and g is the best known cost of reaching the considered state from the\ninitial state. In contrast to the greedy search, weighted A\u2217 search re-expands states whenever it finds cheaper paths to them.\nIn addition, both search algorithms use three types of search enhancements inherited from Fast Downward (Helmert, 2006; Richter & Helmert, 2009). Firstly, multiple heuristics are employed within a multi-queue approach to guide the search. Secondly, preferred operators \u2014 similar to the helpful actions in FF \u2014 allow giving precedence to operators that are deemed more helpful than others in a state. Thirdly, deferred heuristic evaluation mitigates the impact of large branching factors assuming that heuristic estimates are fairly accurate. In the following, we discuss these techniques and the resulting algorithms in more detail and give pseudo code for the greedy best-first search. The weighted A\u2217 search is very similar, so we point out the differences between the two algorithms along the way.\nMulti-queue heuristic search. LAMA uses two heuristic functions to guide its search: the namegiving landmark heuristic (see Section 5), and a variant of the well-known FF heuristic (see Section 6). The two heuristics are used with separate queues, thus exploiting strengths of the utilised heuristics in an orthogonal way (Helmert, 2006; Ro\u0308ger & Helmert, 2010). To this end, separate open lists are maintained for each of the two heuristics. States are always evaluated with respect to both heuristics, and their successors are added to all open lists (in each case with the value corresponding to the heuristic of that open list). When choosing which state to evaluate and expand next, the search algorithm alternates between the different queues based on numerical priorities assigned to each queue. These priorities are discussed later.\nDeferred heuristic evaluation. The use of deferred heuristic evaluation means that states are not heuristically evaluated upon generation, but upon expansion, i. e., when states are generated in greedy best-first search, they are put into the open list not with their own heuristic value, but with that of their parent. Only after being removed from the open list are they evaluated heuristically, and their heuristic estimate is in turn used for their successors. The use of deferred evaluation in weighted A\u2217 search is analogous, using f \u2032 instead of h as the sorting criterion of the open lists. If many more states are generated than expanded, deferred evaluation leads to a substantial reduction in the number of heuristic estimates computed. However, deferred evaluation incurs a loss of heuristic accuracy, as the search can no longer use h-values or f \u2032-values to differentiate between the successors of a state (all successors are associated with the parent\u2019s value in the open list). Preferred operators are very helpful in this context as they provide an alternative way to determine promising successors.\nPreferred operators. Operators that are deemed particularly useful in a given state are marked as preferred. They are computed by the heuristic estimators along with the heuristic value of a state (see Sections 6 and 5). To use preferred operators, in the greedy best-first search as well as in the weighted A\u2217 search, the planner maintains an additional preferred-operator queue for each heuristic. When a state is evaluated and expanded, those successor states that are reached via a preferred operator (the preferred states) are put into the preferred-operator queues, in addition to being put into the regular queues like the non-preferred states. (Analogously to regular states, any state preferred by at least one heuristic is added to all preferred-operator queues. This allows for cross-fertilisation through information exchange between the different heuristics.) States in the preferred-operator queues are evaluated earlier on average, as they form part of more queues and have a higher chance of being selected at any point in time than the non-preferred states. In addition,\nLAMA (like the IPC 2004 version of Fast Downward) gives even higher precedence to preferred successors via the following mechanism. The planner keeps a priority counter for each queue, initialised to 0. At each iteration, the next state is removed from the queue that has the highest priority. Whenever a state is removed from a queue, the priority of that queue is decreased by 1. If the priorities are not changed outside of this routine, this method will alternate between all queues, thus expanding states from preferred queues and regular queues equally often. To increase the use of preferred operators, LAMA increases the priorities of the preferred-operator queues by a large number boost of value 1000 whenever progress is made, i. e., whenever a state is discovered that has a better heuristic estimate than previously expanded states. Subsequently, the next 1000 states will be removed from preferred-operator queues. If another improving state is found within the 1000 states, the boosts accumulate and, accordingly, it takes longer until states from the regular queues are expanded again.\nAlternative methods for using preferred operators include the one employed in the YAHSP system (Vidal, 2004), where preferred operators are always used over non-preferred ones. By contrast, our scheme does not necessarily empty the preferred queues before switching back to regular queues. In the FF planner (Hoffmann & Nebel, 2001), the emphasis on preferred operators is even stronger than in YAHPS: the search in FF is restricted to preferred operators until either a goal is found or the restricted search space has been exhausted (in which case a new search is started without preferred operators). Compared to these approaches, the method for using preferred operators in LAMA, in conjunction with deferred heuristic evaluation, has been shown to result in substantial performance improvement and deliver best results in the classical setting of operators with unit costs (Richter & Helmert, 2009). The choice of 1000 as the boost value is not critical here, as we found various values between 100 and 50000 to give similarly good results. Only outside this range does performance drop noticeably.\nNote that when using action costs, the use of preferred operators may be even more helpful than in the classical setting. For example, if all operators have a cost of 0, a heuristic using pure cost estimates might assign the same heuristic value of 0 to all states in the state space, giving no guidance to search at all. Preferred operators, however, still provide the same heuristic guidance in this case as in the case with unit action costs. While this is an extreme example, similar cases appear in practice, e. g. in the IPC 2008 domain Openstacks, where all operators except for the one opening a new stack have an associated cost of 0.\nPseudo code. Algorithm 1 shows pseudo code for the greedy best-first search. The main loop (lines 25\u201336) runs until either a goal has been found (lines 27\u201329) or the search space has been exhausted (lines 32\u201333). The closed list contains all seen states and also keeps track of the links between states and their parents, so that a plan can be efficiently extracted once a goal state has been found (line 28). In each iteration of the loop, the search adds the current state (initially the start state) to the closed list and processes it (lines 30\u201331), unless the state has been processed before, in which case it is ignored (line 26). By contrast, weighted A\u2217 search processes states again whenever they are reached via a path with lower cost than before, and updates their parent links in the closed list accordingly. Then the search selects the next open list to be used (the one with highest priority, line 34), decreases its priority and extracts the next state to be processed (lines 35\u201336). The processing of a state includes calculating its heuristic values and preferred operators with both heuristics (lines 3\u20134), expanding it, and inserting the successors into the appropriate open\nGlobal variables: \u03a0 = \u3008V, s0, s?,O,C\u3009 . Planning task to solve regFF, pref FF, regLM, pref LM . Regular and preferred open lists for each heuristic best seen value . Best heuristic value seen so far for each heuristic priority . Numerical priority for each queue\n1: function expand state(s) 2: progress\u2190 False 3: for h \u2208 {FF,LM} do 4: h(s), preferred ops(h, s)\u2190 heuristic value of s and preferred operators given h 5: if h(s) < best seen value[h] then 6: progress\u2190 True 7: best seen value[h]\u2190 h(s) 8: if progress then . Boost preferred-operator queues 9: priority[pref FF]\u2190 priority[pref FF] + 1000\n10: priority[pref LM]\u2190 priority[pref LM] + 1000 11: succesor states\u2190 { s[o] | o \u2208 O and o applicable in s } 12: for s\u2032 \u2208 succesor states do 13: for h \u2208 {FF,LM} do 14: add s\u2032 to queue regh with value h(s) . Deferred evaluation 15: if s\u2032 reached by operator o \u2208 preferred ops(h, s) then 16: add s\u2032 to queue pref FF with value FF(s), and to queue pref LM with value LM(s)\n17: function greedy bfs lama 18: closed list \u2190 \u2205 19: for h \u2208 {FF,LM} do . Initialize FF and landmark heuristics 20: best seen value[h]\u2190 \u221e 21: for l \u2208 {reg, pref } do . Regular and preferred open lists for each heuristic 22: lh \u2190 \u2205 23: priority[lh]\u2190 0 24: current state\u2190 s0 25: loop 26: if current state < closed list then 27: if s = s? then 28: extract plan \u03c0 by tracing current state back to initial state in closed list 29: return \u03c0 30: closed list \u2190 closed list \u222a {current state} 31: expand state(current state) 32: if all queues are empty then 33: return failure . No plan exists 34: q\u2190 non-empty queue with highest priority 35: priority[q]\u2190 priority[q] \u2212 1 36: current state\u2190 pop state(q) . Get lowest-valued state from queue q\nAlgorithm 1: The greedy best-first-search with search enhancements used in LAMA.\nlists (lines 11\u201316). If it is determined that a new best state has been found (lines 5-7), the preferredoperator queues are boosted by 1000 (lines 8-10).\n3.3.1 Restarting Anytime Search\nLAMA was developed for the International Planning Competition 2008 and is tailored to the conditions of this competition in several ways. In detail, those conditions were as follows. While in previous competitions coverage, plan quality and runtime were all used to varying degrees in order to determine the effectiveness of a classical planning system, IPC 2008 introduced a new integrated performance criterion. Each operator in the PDDL input had an associated non-negative integer action cost, and the aim was to find a plan of lowest-possible total cost within a given time limit of 30 minutes per task. Given that a planner solves a task at all within the time limit, this new performance measure depends only on plan quality, not on runtime, and thus suggests guiding the search towards a cheapest goal rather than a closest goal as well as using all of the available time to find the best plan possible.\nGuiding the search towards cheap goals may be achieved in two ways, both of which LAMA implements: firstly, the heuristics should estimate the cost-to-go, i. e., the cost of reaching the goal from a given state, rather than the distance-to-go, i. e., the number of operators required to reach the goal. Both the landmark heuristic and the FF heuristic employed in LAMA are therefore capable of using action costs. Secondly, the search algorithm should not only take the cost-to-go from a given state into account, but also the cost necessary for reaching that state. This is the case for weighted A\u2217 search as used in LAMA. To make the most of the available time, LAMA employs an anytime approach: it first runs a greedy best-first search, aimed at finding a solution as quickly as possible. Once a plan is found, it searches for progressively better solutions by running a series of weighted A\u2217 searches with decreasing weight. The cost of the best known solution is used for pruning the search, while decreasing the weight over time makes the search progressively less greedy, trading speed for solution quality.\nSeveral anytime algorithms based on weighted A\u2217 have been proposed (Hansen & Zhou, 2007; Likhachev, Ferguson, Gordon, Stentz, & Thrun, 2008). Their underlying idea is to continue the weighted A\u2217 search past the first solution, possibly adjusting search parameters like the weight or pruning bound, and thus progressively find better solutions. The anytime approach used in LAMA differs from these existing algorithms in that we do not continue the weighted A\u2217 search once it finds a solution. Instead, we start a new weighted A\u2217 search, i. e., we discard the open lists of the previous search and re-start from the initial state. While resulting in some duplicate effort, these restarts can help overcome bad decisions made by the early (comparatively greedy) search iterations with high weight (Richter et al., 2010). This can be explained as follows: After finding a goal state sg, the open lists will usually contain many states that are close to sg in the search space, because the ancestors of sg have been expanded; furthermore, those states are likely to have low heuristic values because of their proximity to sg. Hence, if the search is continued (even after updating the open lists with lower weights), it is likely to expand most of the states around sg before considering states that are close to the initial state. This can be critical, as it means that the search is concentrating on improving the end of the current plan, as opposed to its beginning. A bad beginning of a plan, however, may have severe negative influence on its quality, as it may be impossible to improve the quality of the plan substantially without changing its early operators.\nConsider the example of a search problem shown in Figure 1. The task is to reach a goal state (g1 or g2) from the start state s in a gridworld, where the agent can move with cost 1 to each of the 8 neighbours of a cell if they are not blocked. The heuristic values are inaccurate estimates of the straight-line goal distances of cells. In particular, the heuristic values underestimate distances in the left half of the grid. We conduct a weighted A\u2217 search with weight 2 in Figure 1a (assuming for simplicity a standard textbook search, i. e., no preferred operators and no deferred evaluation). Because the heuristic values to the left of s happen to be lower than to the right of s, the search expands states to the left and finds goal g1 with cost 6. The grey cells are generated, but not expanded in this search phase, i. e., they are in the open list. In Figure 1b, the search continues with a reduced weight of 1.5. A solution with cost 5 consists in turning right from s and going to g2. However, the search will first expand all states in the open list that have an f \u2032-value smaller than 7. After expanding a substantial number of states, the second solution it finds is a path which starts off left of s and takes the long way around the obstacle to g2, again with cost 6. If we instead restart with an empty open list after the first solution (Figure 1c), fewer states are expanded. The critical state to the right of s is expanded quickly and the optimal path is found.\nNote that in the above example, it is in particular the systematic errors of the heuristic values that leads the greedy search astray and makes restarts useful. In planning, especially when using deferred evaluation, heuristic values may also be fairly inaccurate, and restarts can be useful. In an experimental comparison on all tasks from IPC 1998 to IPC 2006 (Richter et al., 2010) this restarting approach performed notably better than all other tested methods, dominating similar algorithms based on weighted A\u2217 (Hansen, Zilberstein, & Danilchenko, 1997; Hansen & Zhou, 2007; Likhachev, Gordon, & Thrun, 2004; Likhachev et al., 2008), as well as other anytime approaches (Zhou & Hansen, 2005; Aine, Chakrabarti, & Kumar, 2007).\n3.3.2 Using cost and distance estimates\nBoth heuristic estimators used in LAMA are cost-sensitive, aiming to guide the search towards high-quality solutions. Focusing a planner purely on action costs, however, may be dangerous, as cheap plans may be longer and more difficult to find, which in the worst case could mean that the planner fails to find a plan at all within the given time limit. Zero-cost operators present a particular challenge: since zero-cost operators can always be added to a search path \u201cfor free\u201d, even a costsensitive search algorithm like weighted A\u2217 may explore very long search paths without getting closer to a goal. Methods have been suggested that allow a trade-off between the putative cost-to-go and the estimated goal distance (Gerevini & Serina, 2002; Ruml & Do, 2007). However, they require the user to specify the relative importance of cost versus distance up-front, a choice that was not obvious in the context of IPC 2008. LAMA gives equal weight to the cost and distance estimates by adding the two values during the computation of its heuristic functions (for more details, see Sections 5 and 6). This measure is a very simple one, and its effect changes depending on the magnitude and variation of action costs in a problem: the smaller action costs are, the more this method favours short plans over cheap plans. For example, 5 zero-cost operators result in an estimated cost of 5, whereas 2 operators of cost 1 result in an estimated cost of 4. LAMA would thus prefer the 2 operators of cost 1 over the 5 zero-cost operators. By contrast, when the action costs in a planning task are larger than the length of typical plans, the cost estimates dominate the distance estimates and LAMA is completely guided by costs. Nevertheless this simple measure proves useful on the IPC 2008 benchmarks, outperforming pure cost search in our experiments. More so-\nphisticated methods for automatically balancing cost against distance (for example by normalising the action costs in a given task with respect to their mean or median) are a topic of future work."}, {"heading": "4. Landmarks", "text": "Landmarks are subgoals that must be achieved in every plan. They were first introduced by Porteous, Sebastia and Hoffmann (2001) and were later studied in more depth by the same authors (Hoffmann, Porteous, & Sebastia, 2004). Using landmarks to guide the search for a solution in planning is an intuitive approach that humans might use. Consider the well-known benchmark domain Logistics, where the goal is to deliver objects (e. g. boxes) between various locations using a fleet of vehicles. Cities consist of sets of locations, where trucks may transport boxes within the city, whereas planes have to be used between cities. An example Logistics task is shown in Figure 2. Arguably the first mental step a human would perform, when trying to solve the task in Figure 2, is to realise that the box must be transported between the two cities, from the left city (locations A\u2013D) to the right city (location E), and that therefore, the box will have to be transported in the plane. This in turn means that the box will have to be at the airport location C, so it can be loaded into a plane. This partitions the task into two subproblems, one of transporting the box to the airport at location C, and one of delivering it from there to the other city. Both subproblems are smaller and easier to solve than the original task.\nLandmarks capture precisely these intermediate conditions that can be used to direct search: the facts L1 = \u201cbox is at C\u201d and L2 = \u201cbox is in plane\u201d are landmarks in the task shown in Figure 2. This knowledge, as well as the knowledge that L1 must become true before L2, can be automatically extracted from the task in a preprocessing step (Hoffmann et al., 2004).\nLAMA uses landmarks to derive goal-distance estimates for a heuristic search. It measures the goal distance of a state by the number of landmarks that still need to be achieved on the path from this state to a goal. Orderings between landmarks are used to infer which landmarks should be achieved next, and whether certain landmarks have to be achieved more than once. In addition, preferred operators (Helmert, 2006) are used to suggest operators that achieve those landmarks that need to become true next. As we have recently shown, this method for using landmarks leads to substantially better performance than the previous use of landmarks by Hoffmann et al., both in terms of coverage and in terms of plan quality (Richter et al., 2008). We discuss the differences between their approach and ours in more detail in Section 4.3. In the following section we define\nlandmarks and their orderings formally, including some useful special cases that can be detected efficiently."}, {"heading": "4.1 Definitions", "text": "Hoffmann et al. (2004) define landmarks as facts that are true at some point in every plan for a given planning task. They also introduce disjunctive landmarks, defined as sets of facts of which at least one needs to be true at some point. We subsume their landmark definitions into a more general definition based on propositional formulas, as we believe this to be useful for future work on the topic of landmarks. It should be noted, however, that LAMA currently only supports fact landmarks and disjunctions of facts (for more details, see Section 4.2). Hoffmann et al. show that it is PSPACE-hard to determine whether a given fact is a landmark, and whether an ordering holds between two landmarks. Their complexity results carry over in a straight-forward way to the more general case of propositional formulas, so we do not repeat the proofs.\nDefinition 3. Landmark Let \u03a0 = \u3008V, s0, s?,O,C\u3009 be a planning task in finite-domain representation, let \u03c0 = \u3008o1, . . . , on\u3009 be an operator sequence applicable in s0, and let i, j \u2208 {0, . . . , n}.\n\u2022 A propositional formula \u03d5 over the facts of \u03a0 is called a fact formula.\n\u2022 A fact F is true at time i in \u03c0 iff F \u2208 s0[\u3008o1, . . . , oi\u3009].\n\u2022 A fact formula \u03d5 is true at time i in \u03c0 iff \u03d5 holds given the truth value of all facts of \u03a0 at time i. At any time i < 0, \u03d5 is not considered true.\n\u2022 A fact formula \u03d5 is a landmark of \u03a0 iff in each plan for \u03a0, \u03d5 is true at some time.\n\u2022 A propositional formula \u03d5 over the facts of \u03a0 is added at time i in \u03c0 iff \u03d5 is true at time i in \u03c0, but not at time i \u2212 1 (it is considered added at time 0 if it is true in s0).\n\u2022 A fact formula \u03d5 is first added at time i in \u03c0 iff \u03d5 is true at time i in \u03c0, but not at any time j < i.\nNote that facts in the initial state and facts in the goal are always landmarks by definition. The landmarks we discussed earlier for the example task in Figure 2 were all facts. However, more complex landmarks may be required in larger tasks. Consider an extended version of the\nexample, where the city on the right has two airports, and there are multiple planes and trucks, as depicted in Figure 3. The previous landmark L1 = \u201cbox is at C\u201d is still a landmark in our extended example. However, L2 = \u201cbox is in plane\u201d has no corresponding fact landmark in this task, since neither \u201cbox is in plane1\u201d nor \u201cbox is in plane2\u201d is a landmark. The disjunction \u201cbox is in plane1 \u2228 box is in plane2\u201d, however, is a landmark. In the following we refer to landmarks that are facts as fact landmarks, and to disjunctions of facts as disjunctive landmarks. While the use of disjunctive landmarks has been shown to improve performance, compared to using only fact landmarks (Richter et al., 2008), more complex landmarks introduce additional difficulty both with regard to their detection and their handling during planning. As mentioned before, LAMA currently only uses fact landmarks and disjunctive landmarks, rather than general propositional formulas. The extension to more complex types of landmarks is an interesting topic of future work. (See Keyder, Richter and Helmert, 2010, for a discussion of conjunctive landmarks).\nVarious kinds of orderings between landmarks can be defined and exploited during the planning phase. We define three types of orderings for landmarks, which are equivalent formulations of the definitions by Hoffmann et al. (2004) adapted to the FDR setting:\nDefinition 4. Orderings between landmarks Let \u03d5 and \u03c8 be landmarks in an FDR planning task \u03a0.\n\u2022 We say that there is a natural ordering between \u03d5 and \u03c8, written \u03d5 \u2192 \u03c8, if in each plan where \u03c8 is true at time i, \u03d5 is true at some time j < i.\n\u2022 We say that there is a necessary ordering between \u03d5 and \u03c8, written \u03d5 \u2192n \u03c8, if in each plan where \u03c8 is added at time i, \u03d5 is true at time i \u2212 1.\n\u2022 We say that there is a greedy-necessary ordering between \u03d5 and \u03c8, written \u03d5 \u2192gn \u03c8, if in each plan where \u03c8 is first added at time i, \u03d5 is true at time i \u2212 1.\nNatural orderings are the most general; every necessary or greedy-necessary ordering is natural, but not vice versa. Similarly, every necessary ordering is greedy-necessary, but not vice versa. Knowing that a natural ordering is also necessary or greedy-necessary allows deducing additional information about plausible temporal relationships between landmarks, as described later in this section. Also, the landmark heuristic in LAMA uses this knowledge to deduce whether a landmark needs to be achieved more than once. As a theoretical concept, necessary orderings (\u03d5 is always true in the step before \u03c8) are more straightforward and appealing than greedy-necessary orderings (\u03d5 is true in the step before \u03c8 becomes true for the first time). However, methods that find landmarks in conjunction with orderings can often find many more landmarks when using the more general concept of greedy-necessary orderings (Hoffmann et al., 2004). LAMA follows this paradigm and finds greedy-necessary (as well as natural) orderings, but not necessary orderings. In our example in Figure 3, \u201cbox is in truck1\u201d must be true before \u201cbox is at C\u201d and also before \u201cbox is at F\u201d. The first of these orderings is greedy-necessary, but not necessary, and the second is neither greedy-necessary nor necessary, but natural.\nHoffmann et al. (2004) propose further kinds of orderings between landmarks that can be usefully exploited. For example, reasonable orderings, which were first introduced in the context of top-level goals (Koehler & Hoffmann, 2000), are orderings that do not necessarily hold in a given planning task. However, adhering to these orderings may save effort when solving the task. In our example task, it is \u201creasonable\u201d to load the box onto truck1 before driving the truck to the airport at\nC. However, this order is not guaranteed to hold in every plan, as it is possible, though not \u201creasonable\u201d, to drive the truck to C first, then drive to B and collect the box, and then return to C. The idea is that if a landmark \u03c8 must become false in order to achieve a landmark \u03d5, but \u03c8 is needed after \u03d5, then it is reasonable to achieve \u03d5 before \u03c8 (as otherwise, we would have to achieve \u03c8 twice). The idea may be applied iteratively, as we are sometimes able to find new, induced reasonable orderings if we restrict our focus to plans that obey a first set of reasonable orderings. Hoffmann et al. call the reasonable orderings found in such a second pass obedient-reasonable orderings. The authors note that conducting more than two iterations of this process is not worthwhile, as it typically does not result in notable additional benefit. The following definition characterises these two types of orderings formally.\nDefinition 5. Reasonable orderings between landmarks Let \u03d5 and \u03c8 be landmarks in an FDR planning task \u03a0.\n\u2022 We say that there is a reasonable ordering between \u03d5 and \u03c8, written \u03d5\u2192r \u03c8, if for every plan \u03c0 where \u03c8 is added at time i and \u03d5 is first added at time j with i < j, it holds that \u03c8 is not true at time m with m \u2208 {i + 1, . . . , j} and \u03c8 is true at some time k with j \u2264 k.\n\u2022 We say that a plan \u03c0 obeys a set of orderings O, if for all orderings \u03d5 \u2192x \u03c8 \u2208 O, regardless of their type, it holds that \u03d5 is first added at time i in \u03c0 and \u03c8 is not true at any time j \u2264 i.\n\u2022 We say that there is an obedient-reasonable ordering between \u03d5 and \u03c8 with regard to a set of orderings O, written \u03d5 \u2192Or \u03c8, if for every plan \u03c0 obeying O where \u03c8 is added at time i and \u03d5 is first added at time j with i < j, it holds that \u03c8 is not true at time m with m \u2208 {i + 1, . . . , j} and \u03c8 is true at some time k with j \u2264 k.\nOur definitions are equivalent to those of Hoffmann et al. (2004), except that we care only about plans rather than arbitrary operator sequences, allowing us to (theoretically) identify more reasonable orderings. In practice, we use the same approximation techniques as Hoffmann et al., thus generating the same orderings.\nA problem with reasonable and obedient-reasonable orderings is that they may be cyclic, i. e., chains of orderings \u03d5\u2192r \u03c8\u2192x . . .\u2192r \u03d5 for landmarks \u03d5 and \u03c8 may exist (Hoffmann et al., 2004). This is not the case for natural orderings, as their definition implies that they cannot be cyclic in solvable tasks.\nIn addition, the definitions as given above are problematic in special cases. Note that the definition of a reasonable ordering \u03d5 \u2192r \u03c8 includes the case where there exist no i < j such that \u03c8 is added at time i and \u03d5 is first added at time j, i. e., the case where it holds that in all plans \u03d5 is first added (a) before or (b) at the same time as \u03c8.1 While (a) implies that reasonable orderings are a generalisation of natural orderings, which might be regarded as a desirable property, (b) may lead to undesirable orderings. For example, it holds that \u03d5 \u2192r \u03c8 and \u03c8 \u2192r \u03d5 for all pairs \u03d5, \u03c8 that are first added at the same time in all plans, for instance if \u03d5 and \u03c8 are both true in the initial state. Similarly, it holds that \u03d5 \u2192r \u03d5 for all \u03d5. We use these definitions despite their weaknesses here, and simply note that our planner does not create all such contentious orderings. LAMA does not create reflexive orderings \u03d5 \u2192r \u03d5; and \u03d5 \u2192r \u03c8 with \u03d5, \u03c8 true in the initial state is only created if it is assumed or proven that \u03c8 must be true strictly after \u03c6 at some point in any plan (see also Section\n1. According to personal communication with the authors, this case was overlooked by Hoffmann et al.\n4.2.5). A re-definition of reasonable orderings, addressing the problems of the definition by Hoffmann et al. and identifying precisely the wanted/unwanted cases, is a topic of future work. Closely connected is the question whether reasonable orderings should be interpreted as strict orderings, where \u03d5 should be achieved before \u03c8 (as in the definition of obedience above), or whether we allow achieving \u03d5 and \u03c8 simultaneously. We use the strict sense of obedience for reasons of consistency with the previous work by Hoffmann et al., and because it aligns better with our intended meaning of reasonable orderings, even though this strict interpretation of obedience does not fit the contentious cases discussed above.\nLandmarks and orderings may be represented using a directed graph called the landmark graph. A partial landmark graph for our extended example is depicted in Figure 4. The following section 4.2 contains an extensive description of how landmarks and their orderings are discovered in LAMA. Readers not interested in the exact details of this process may skip this description, as it is not central to the rest of this paper. Section 4.3 discusses how our approach for finding and using landmarks relates to previous work. Section 5 describes how landmarks are used as a heuristic estimator in LAMA."}, {"heading": "4.2 Extracting Landmarks and Orderings", "text": "As mentioned before, deciding whether a given formula is a landmark and deciding orderings between landmarks are PSPACE-hard problems. Thus, practical methods for finding landmarks are incomplete (they may fail to find a given landmark or ordering) or unsound (they may falsely declare a formula to be a landmark, or determine a false ordering). Several polynomial methods have been proposed for finding fact landmarks and disjunctive landmarks, such as back-chaining from the goals of the task, using criteria based on the relaxed planning graph (Porteous et al., 2001; Hoffmann et al., 2004; Porteous & Cresswell, 2002), and forward propagation in the planning graph (Zhu & Givan, 2003).\nThe algorithm used in LAMA for finding landmarks and orderings between them is partly based on the previous back-chaining methods mentioned above, adapting them to the finite-domain representation including conditional effects. In addition, our algorithm exploits the finite-domain representation by using domain transition graphs to find further landmarks. We discuss the differences between our method and the previous ones in detail in Section 4.3. The idea of back-chaining is to start from a set of known landmarks and to find new fact landmarks or disjunctive landmarks that must be true in any plan before an already known landmark may become true. This procedure starts from the set of all goal facts, and stops when no more new landmarks can be found. Our method identifies new landmarks and orderings by considering, for any given fact landmark or disjunctive landmark \u03c8 that is not true in the initial state:\n\u2022 The shared preconditions of its possible first achievers. These are the operator preconditions and effect conditions shared by all effects that can potentially first achieve \u03c8. This method has been adapted from previous work (see Section 4.3).\n\u2022 For fact landmarks v 7\u2192 d, the domain transition graph (DTG) of v. Here, we identify nodes in the DTG (i. e., values d\u2032 of v) that must necessarily be traversed in order to reach d.\n\u2022 A restricted relaxed planning graph lacking all operators that could possibly achieve \u03c8. (There are some subtleties involving conditional effects that will be explained later.) Every landmark which does not occur in the last level of this graph can only be achieved after \u03c8.\nAs in previous work (Porteous et al., 2001; Hoffmann et al., 2004), we subsequently use the discovered landmarks and orderings to derive reasonable and obedient-reasonable orderings in a postprocessing step. In the following, we give a detailed description of each step of the procedure for finding landmarks and orderings in LAMA. High-level pseudo code for our algorithm, containing the steps described in the following sections 4.2.1\u20134.2.4, is shown in Algorithm 2.\n4.2.1 Back-Chaining: Landmarks via Shared Preconditions of Possible First Achievers\nFirst achievers of a fact landmark or disjunctive landmark \u03c8 are those operators that potentially make \u03c8 true and can be applied at the end of a partial plan that has never made \u03c8 true before. We call any fact A that is a precondition for each of the first achievers a shared precondition. As at least one of the first achievers must be applied to make \u03c8 true, A must be true before \u03c8 can be achieved, and A is thus a landmark, with the ordering A \u2192gn \u03c8. Any effect condition for \u03c8 in an operator can be treated like a precondition in this context, as we are interested in finding the conditions that must hold for \u03c8 to become true. We will in the following use the term extended preconditions of an operator o for \u03c8 to denote the union of the preconditions of o and its effect conditions for \u03c8. The extended preconditions shared by all achievers of a fact are calculated in line 19 of Algorithm 2. In addition, we can create disjunctive landmarks \u03d5 by selecting, from the precondition facts of the first achievers, sets of facts such that each set contains one extended precondition fact from each first achiever (line 22). As one of the first achievers must be applied to make \u03c8 true, one of the facts in \u03d5 must be true before \u03c8, and the disjunction \u03d5 is thus a landmark, with the ordering \u03d5\u2192gn \u03c8. Since the number of such disjunctive landmarks is exponential in the number of achievers of \u03c8, we restrict ourselves to disjunctions where all facts stem from the same predicate symbol, which are deemed most helpful (Hoffmann et al., 2004). Furthermore, we discard any fact sets of size greater than four, though we found this restriction to have little impact compared to the predicate restriction.\nGlobal variables: \u03a0 = \u3008V, s0, s?,O,C\u3009 . Planning task to solve LG = \u3008L,O\u3009 . Landmark graph of \u03a0 queue . Landmarks to be back-chained from\n1: function add landmark and ordering(\u03d5, \u03d5\u2192x \u03c8) 2: if \u03d5 is a fact and \u2203\u03c7 \u2208 L : \u03c7 . \u03d5 and \u03d5 |= \u03c7 then . Prefer fact landmarks 3: L\u2190 L \\ {\u03c7} . Remove disjunctive landmark 4: O\u2190 O \\ { (\u03d1\u2192x \u03c7), (\u03c7\u2192x \u03d1) | \u03d1 \u2208 L } . Remove obsolete orderings 5: if \u2203\u03c7 \u2208 L : \u03c7 . \u03d5 and var(\u03d5) \u2229 var(\u03c7) , \u2205 then . Abort on overlap with existing landmark 6: return 7: if \u03d5 < L then . Add new landmark to graph 8: L\u2190 L \u222a {\u03d5} 9: queue\u2190 queue \u222a {\u03d5}\n10: O\u2190 O \u222a {\u03d5\u2192x \u03c8} . Add new ordering to graph\n11: function identify landmarks 12: LG\u2190 \u3008s?, \u2205\u3009 . Landmark graph starts with all goals, no orderings 13: queue\u2190 s? 14: further orderings\u2190 \u2205 . Additional orderings (see Section 4.2.3) 15: while queue , \u2205 do 16: \u03c8\u2190 pop(queue) 17: if s0 6|= \u03c8 then 18: RRPG\u2190 the restricted relaxed plan graph for \u03c8 19: preshared \u2190 shared extended preconditions for \u03c8 extracted from RRPG 20: for \u03d5 \u2208 preshared do 21: add landmark and ordering(\u03d5, \u03d5\u2192gn \u03c8) 22: predisj \u2190 sets of facts covering shared extended preconditions for \u03c8 given RRPG 23: for \u03d5 \u2208 predisj do 24: if s0 6|= \u03d5 then 25: add landmark and ordering(\u03d5, \u03d5\u2192gn \u03c8) 26: if \u03c8 is a fact then 27: prelookahead \u2190 extract landmarks from DTG of the variable in \u03c8 using RRPG 28: for \u03d5 \u2208 prelookahead do 29: add landmark and ordering(\u03d5, \u03d5\u2192 \u03c8) 30: potential orderings\u2190 potential orderings \u222a {\u03c8\u2192 F | F never true in RRPG } 31: add further orderings between landmarks from potential orderings\nAlgorithm 2: Identifying landmarks and orderings via back-chaining, domain transition graphs and restricted relaxed planning graphs.\nSince it is PSPACE-hard to determine the set of first achievers of a landmark \u03c8 (Hoffmann et al., 2004), we use an over-approximation containing every operator that can possibly be a first achiever (Porteous & Cresswell, 2002). By intersecting over the extended preconditions of (possibly) more operators we do not lose correctness, though we may miss out on some landmarks. The approximation of first achievers of \u03c8 is done with the help of a restricted relaxed planning graph. During construction of the graph we leave out any operators that would add \u03c8 unconditionally, and we also ignore any conditional effects which could potentially add \u03c8. When the relaxed planning graph levels out, its last set of facts is an over-approximation of the facts that can be achieved before \u03c8 in the planning task. Any operator that is applicable given this over-approximating set and achieves \u03c8 is a possible first achiever of \u03c8.\n4.2.2 Landmarks via Domain Transition Graphs\nGiven a fact landmark L = {v 7\u2192 l}, we can use the domain transition graph of v to find further fact landmarks v 7\u2192 l\u2032 (line 27) as follows. If the DTG contains a node that occurs on every path from the initial state value s0(v) of a variable to the landmark value l, then that node corresponds to a landmark value l\u2032 of v: We know that every plan achieving L requires that v takes on the value l\u2032, hence the fact L\u2032 = {v 7\u2192 l\u2032} can be introduced as a new landmark and ordered naturally before L. To find these kinds of landmarks, we iteratively remove one node from the DTG and test with a simple graph algorithm whether s0(v) and l are still connected \u2013 if not, the removed node corresponds to a landmark. We further improve this procedure by removing, as a preprocessing step, all nodes for which we know that they cannot be true before achieving L. These are the nodes that correspond to facts other than L and do not appear in the restricted RPG that never adds L. Removing these nodes may decrease the number of paths reaching L and may thus allow us to find more landmarks.\nConsider again the landmark graph of our extended example, shown in Figure 4. Most of its landmarks and orderings can be found via the back-chaining procedure described in the previous section, because the landmarks are direct preconditions for achieving their successors in the graph. There are two exceptions: \u201cbox in truck1\u201d and \u201cbox at C\u201d. These two landmarks are however found with the DTG method. The DTG in Figure 5 immediately shows that the box location must take on both the value t1 and the value C on any path from its initial value B to its goal value F.\n4.2.3 Additional Orderings from Restricted Relaxed Planning Graphs\nThe restricted relaxed planning graph (RRPG) described in Section 4.2.1, which for a given landmark \u03c8 leaves out all operators that could possibly achieve \u03c8, can be used to extract additional orderings between landmarks. Any landmark \u03c7 that does not appear in this graph cannot be reached before \u03c8, and we can thus introduce a natural ordering \u03c8\u2192 \u03c7. For efficiency reasons, we construct the RRPG for \u03c8 only once (line 18), i. e., when needed to find possible first achievers of \u03c8 during the back-chaining procedure. We then extract all orderings between \u03c8 and facts that can only be reached after \u03c8 (line 30). For all such facts F that are later recognised to be landmarks, we then introduce the ordering \u03c8\u2192 F (line 31).\n4.2.4 Overlapping Landmarks\nDue to the iterative nature of the algorithm it is possible that we find disjunctive landmarks for which at least one of the facts is already known to be a fact landmark. In such cases, we let fact landmarks take precedence over disjunctive ones, i. e., when a disjunctive landmark is discovered that includes an already known fact landmark, we do not add the disjunctive landmark. Conversely, as soon as a fact landmark is found that is part of an already known disjunctive landmark, we discard the disjunctive landmark including its orderings2, and add the fact landmark instead. To keep the procedure and the resulting landmark graph simple, we furthermore do not allow landmarks to overlap. Whenever some fact from a newly discovered disjunctive landmark is also part of some already known landmark, we do not add the newly discovered landmark. All these cases are handled in the function add landmark and ordering (lines 1\u2013 10).\n4.2.5 Generating Reasonable and Obedient-Reasonable Orderings\nWe want to introduce a reasonable ordering L \u2192r L\u2032 between two (distinct) fact landmarks L and L\u2032 if it holds that (a) L\u2032 must be true at the same time or after first achieving L, and (b) achieving L\u2032 before L would require making L\u2032 false again to achieve L. We approximate both (a) and (b) as proposed by Hoffmann et al. (2004) with sufficient conditions. In the case of (a), we test if L\u2032 \u2208 s? or if we have a chain of natural or greedy-necessary orderings between landmarks L = L1 \u2192 . . .\u2192 Ln, with n > 1, Ln\u22121 , L\u2032 and a greedy-necessary ordering L\u2032 \u2192gn Ln. For (b) we check whether (i) L and L\u2032 are inconsistent, i. e., mutually exclusive, or (ii) all operators achieving L have an effect that is inconsistent with L\u2032, or (iii) there is a landmark L\u2032\u2032 inconsistent with L\u2032 with the ordering L\u2032\u2032 \u2192gn L.\nInconsistencies between facts can be easily identified in the finite-domain representation if the facts are of the form v 7\u2192 d and v 7\u2192 d\u2032, i. e., if they map the same variable to different values. In addition, LAMA uses the groups of inconsistent facts computed by its translator component.\nIn a second pass, obedient-reasonable orderings are added. This is done with the same method as above, except that now reasonable orderings are used in addition to natural and greedy-necessary orderings to derive the fact that a landmark L\u2032 must be true after a landmark L. Finally, we use a simple greedy algorithm to break possible cycles due to reasonable and obedient-reasonable orderings in the landmark graph, where every time a cycle is identified, one of the involved reasonable or\n2. Note that an ordering {F,G} \u2192 \u03c8 neither implies F \u2192 \u03c8 nor G \u2192 \u03c8 in general. Conversely, \u03d5 \u2192 {F,G} neither implies \u03d5\u2192 F nor \u03d5\u2192 G.\nobedient-reasonable orderings is removed. The algorithm removes obedient-reasonable orderings rather than reasonable orderings whenever possible."}, {"heading": "4.3 Related Work", "text": "Orderings between landmarks are a generalisation of goal orderings, which have been frequently exploited in planning and search in the past. In particular, the approach by Irani and Cheng (Irani & Cheng, 1987; Cheng & Irani, 1989) is a preprocessing procedure like ours that analyses the planning task to extract necessary orderings between goals, which are then imposed on the search algorithm. A goal A is ordered before a goal B in this approach if in any plan A is necessarily true before B. Koehler and Hoffmann (2000) introduce reasonable orderings for goals.\nHoffmann et al. (2004), in an article detailing earlier work by Porteous et al. (2001), introduce the idea of landmarks, generalise necessary and reasonable orderings from goals to landmarks, and propose methods for finding and using landmarks for planning. The proposed method for finding landmarks, which was subsequently extended by Porteous and Cresswell (2002), is very closely related to ours. Hoffmann et al. propose a method for finding fact landmarks that proceeds in three stages. First, potential landmarks and orderings are suggested by a fast candidate generation procedure. Second, a filtering procedure evaluates a sufficient condition for landmarks on each candidate fact, removing those which fail the test. Third, reasonable and obedient-reasonable orderings between the landmarks are approximated. This step is largely identical in their approach and ours, except that we use different methods to recognise inconsistencies between facts.\nThe generation of landmark candidates is done via back-chaining from the goal much like in our approach, and intersecting preconditions over all operators which can first achieve a fact F and appear before F in the relaxed planning graph. Note that even if all these operators share a common precondition L, there might be other first achievers of F (appearing after F in the relaxed planning graph) that do not have L as a precondition, and hence L is not a landmark. To test whether a landmark candidate L found via back-chaining is indeed a landmark, Hoffmann et al. (2004) build a restricted relaxed planning task leaving out all operators which could add L. If this task is unsolvable, then L is a landmark. This is a sufficient, but not necessary condition: if L is necessary for solving the relaxed task it is also necessary for solving the original task, while the converse is not true. This verification procedure guarantees that the method by Hoffmann et al. only generates true landmarks; however, unsound orderings may be established due to unsound landmark candidates. While the unsound landmarks are pruned after failing the verification test, unsound orderings may remain.\nPorteous and Cresswell (2002) propose the alternative approximation for first achievers of a fact F that we use. They consider all first achievers that are possibly applicable before F and thus guarantee the correctness of the found landmarks and orderings. They also find disjunctive landmarks. Our method for landmark detection differs from theirs by adding detection of landmarks via domain transition graphs, and detection of additional orderings via restricted relaxed planning graphs. Porteous and Cresswell additionally reason about multiple occurrences of landmarks (if the same landmark has to be achieved, made false again and re-achieved several times during all plans), which we do not.\nThe approach by Hoffmann et al. (2004) exploits landmarks by decomposing the planning task into smaller subtasks, making the landmarks intermediary goals. Instead of searching for the goal of the task, it iteratively aims to achieve a landmark that is minimal with respect to the orderings. In\ndetail, it first builds a landmark graph (with landmarks as vertices and orderings as arcs). Possible cycles are broken by removing some arcs. The sources S of the resulting directed acyclic graph are handed over to a base planner as a disjunctive goal, and a plan is generated to achieve one of the landmarks in S . This landmark, along with its incident arcs, is then removed from the landmark graph, and the process repeats from the end state of the generated plan. Once the landmark graph becomes empty, the base planner is asked to generate a plan to the original goal. (Note that even though all goal facts are landmarks and were thus achieved previously, they may have been violated again.)\nAs a base planner for solving the subtasks any planner can be used; Hoffmann et al. (2004) experimented with FF. They found that the decomposition into subtasks can lead to a more directed search, solving larger instances than plain FF in many domains. However, we found that their method leads to worse average performance on the IPC benchmarks from 1998 to 2006 when using Fast Downward as a base planner (Richter et al., 2008). Furthermore, the method by Hoffmann et al. often produces solutions that are longer than those produced by the base planner, as the disjunctive search control frequently switches between different parts of the task which may have destructive interactions. Sometimes this even leads to dead ends, so that this approach fails on solvable tasks. By contrast, our approach incorporates landmark information while searching for the original goal of the planning task via a heuristic function derived from the landmarks (see next section). As we have recently shown, this avoids the possibility of dead-ends and usually generates better-quality solutions (Richter et al., 2008).\nSebastia et al. (2006) extend the work by Hoffmann et al. by employing a refined preprocessing technique that groups landmarks into consistent sets, minimising the destructive interactions between the sets. Taking these sets as intermediary goals, they avoid the increased plan length experienced by Hoffmann et al. (2004). However, according to the authors this preprocessing is computationally expensive and may take longer than solving the original problem.\nZhu and Givan (2003) propose a technique for finding landmarks by propagating \u201cnecessary predecessor\u201d information in a planning graph. Their definition of landmarks encompasses operators that are necessary in any plan (called action landmarks), and they furthermore introduce the notion of a causal landmark for fact landmarks that are required as a precondition for some operators in every plan. They argue that fact landmarks which are not causal are \u201caccidental\u201d effects and do not warrant being sought explicitly. Their algorithm computes action landmarks and causal fact landmarks at the same time by propagating information during the construction of a relaxed planning graph. An extended variant of their algorithm is also able to infer multiple occurrences of landmarks. Gregory et al. (2004) build on their work to find disjunctive landmarks through symmetry breaking.\nSimilar to our work, Zhu and Givan (2003) use the causal fact landmarks and action landmarks to estimate the goal distance of a given state. To this end, they treat each fact landmark as a virtual action (sets of operators that can achieve the fact landmark) and obtain a distance estimate by bin packing. The items to be packed into bins are the real landmark actions (singletons) and virtual actions, where each bin may only contain elements such that a pairwise intersection of the elements is non-empty. Zhu and Givan employ a greedy algorithm to estimate the minimum number of bins and use this value as distance estimate. Their experimental results are preliminary, however, and do not demonstrate a significant advantage of their method over the FF planner."}, {"heading": "5. The Landmark Heuristic", "text": "The LAMA planning system uses landmarks to calculate heuristic estimates. Since we know that all landmarks must be achieved in order to reach a goal, we can approximate the goal distance of a state s reached by a path (i. e., a sequence of states) \u03c0 as the estimated number of landmarks that still need to be achieved from s onwards. These landmarks are given by\nL(s, \u03c0) B ( L \\ Accepted(s, \u03c0)) \u222a ReqAgain(s, \u03c0)\nwhere L is the set of all discovered landmarks, Accepted(s, \u03c0) is the set of accepted landmarks, and ReqAgain(s, \u03c0) is the set of accepted landmarks that are required again, with the following definitions based on a given landmarks graph (L,O) :\nAccepted(s, \u03c0) B  { \u03c8 \u2208 L | s |= \u03c8 and @(\u03d5\u2192x \u03c8) \u2208 O } \u03c0 = \u3008\u3009 Accepted(s0[\u03c0\u2032], \u03c0\u2032) \u222a { \u03c8 \u2208 L | s |= \u03c8 \u03c0 = \u03c0\u2032; \u3008o\u3009 and \u2200(\u03d5\u2192x \u03c8) \u2208 O : \u03d5 \u2208 Accepted(s0[\u03c0\u2032], \u03c0\u2032) }\nReqAgain(s, \u03c0) B { \u03d5 \u2208 Accepted(s, \u03c0) | s 6|= \u03d5 and ( s? |= \u03d5 or \u2203(\u03d5\u2192gn \u03c8) \u2208 O : \u03c8 < Accepted(s, \u03c0)\n) } A landmark \u03d5 is first accepted in a state s if it is true in that state, and all landmarks ordered before \u03d5 are accepted in the predecessor state from which s was generated. Once a landmark has been accepted, it remains accepted in all successor states. For the initial state, accepted landmarks are all those that are true in the initial state and do not have any predecessors in the landmark graph. An accepted landmark \u03d5 is required again if it is not true in s and (a) it forms part of the goal or (b) it must be true directly before some landmark \u03c8 (i. e., \u03d5 \u2192gn \u03c8) where \u03c8 is not accepted in s. In the latter case, since we know that \u03c8 must still be achieved and \u03d5 must be true in the time step before \u03c8, it holds that \u03d5 must be achieved again. The number |L(s, \u03c0)| is then the heuristic value assigned to state s. Pseudo code for the heuristic is given in Algorithm 3.\nThe landmark heuristic will assign a non-zero value to any state that is not a goal state, since goals are landmarks that are always counted as required again per condition (a) above. However, the heuristic may also assign a non-zero value to a goal state. This happens if plans are found that do not obey the reasonable orderings in the landmark graph, in which case a goal state may be reached without all landmarks being accepted.3 Hence, we need to explicitly test states for the goal condition in order to identify goal states during search.\nNote that this heuristic is path-dependent, i. e., it depends on the sequence of states by which s is reached from the initial state. This raises the question of what happens if a state s can be reached via several paths. In LAMA, the heuristic for a state is calculated only once, when it is first reached. An alternative option would be to re-evaluate s each time a new path to s is discovered, taking into account the information of all paths to s known at the time. As Karpas and Domshlak (2009) note, we can calculate the landmarks that are accepted in s given a set of paths P to s as Accepted(s,P) B\u22c2 \u03c0\u2208P Accepted(s, \u03c0), since it holds that any landmark that is not achieved along all paths \u03c0 \u2208 Pmust\n3. In the special case where \u03d5 \u2192r \u03c8 and \u03d5 and \u03c8 can become true simultaneously, we could avoid this by accepting both \u03d5 and \u03c8 at once (Buffet & Hoffmann, 2010), or we could modify our definition of reasonable orderings such that \u03d5 \u2192r \u03c8 does not hold unless \u03c8 must become true strictly after \u03d5. The general problem that goal states may be assigned a non-zero value, however, still persists even with these modifications.\nGlobal variables: \u03a0 = \u3008V, s0, s?,O,C\u3009 . Planning task to solve LG = \u3008L,O\u3009 . Landmark graph of \u03a0 Accepted . Landmarks accepted in states evaluated so far\nfunction lm count heuristic(s, \u03c0) if \u03c0 = \u3008\u3009 then . Initial state\nAccepted(s, \u03c0)\u2190 {\u03c8 \u2208 L | s0 |= \u03c8 and @(\u03d5\u2192x \u03c8) \u2208 O } else\n\u03c0\u2032 \u2190 \u3008o1, . . . , on\u22121\u3009 for \u03c0 = \u3008o1, . . . , on\u3009 parent \u2190 s0[\u03c0\u2032] . Accepted(parent, \u03c0\u2032) has been calculated before Reached \u2190 {\u03c8 \u2208 L | s |= \u03c8 and \u2200(\u03d5\u2192x \u03c8) \u2208 O : \u03d5 \u2208 Accepted(parent, \u03c0\u2032) } Accepted(s, \u03c0)\u2190 Accepted(parent, \u03c0\u2032) \u222a Reached\nNotAccepted \u2190 L \\ Accepted(s, \u03c0) ReqGoal\u2190 {\u03d5 \u2208 Accepted(s, \u03c0) | s 6|= \u03d5 and s? |= \u03d5 } ReqPrecon\u2190 { \u03d5 \u2208 Accepted(s, \u03c0) | s 6|= \u03d5 and \u2203\u03c8 : (\u03d5\u2192gn \u03c8) \u2208 O \u2227 \u03c8 < Accepted(s, \u03c0) } return |NotAccepted \u222a ReqGoal \u222a ReqPrecon|\nAlgorithm 3: The landmark count heuristic.\nbe achieved from s onwards. The heuristic value of s can then be derived from this in an analogous way as before.\nThe landmark heuristic as outlined above estimates the goal distance of states, i. e., the number of operator applications needed to reach the goal state from a given state. To participate in IPC 2008, we made this function cost-sensitive by weighting landmarks with an estimate of their minimum cost. Apart from estimating goal distance by counting the number of landmarks that still need to be achieved from a state, we estimate the cost-to-go from a state by the sum of all minimum costs of those landmarks. The cost counted for each landmark is the minimum action cost of any of its first achievers. (Alternative, more sophisticated methods for computing the costs of landmarks are conceivable and are a potential topic of future work.) The heuristic value LAMA assigns to a state is however not its pure cost-to-go estimate, but rather the sum of its cost estimate and its distance estimate. By thus accounting for both the costs-to-go and the goal distances of states, this measure aims to balance speed of the search and quality of the plans, and in particular counter-act the problems that may arise from zero-cost operators (see Section 3.3).\nWe also generate preferred operators along with the landmark heuristic. An operator is preferred in a state if applying it achieves an acceptable landmark in the next step, i. e., a landmark whose predecessors have already been accepted. If no acceptable landmark can be achieved within one step, the preferred operators are those which occur in a relaxed plan to a nearest acceptable landmark. A nearest landmark in the cost-unaware setting is one that is relaxed reachable with a minimal number of operators, while in the cost-sensitive setting it is a landmark reachable with the cheapest hadd cost (see Section 6), where again both cost and distance estimates are taken into account. This nearest landmark can be computed by building a relaxed planning graph or, equivalently, performing a relaxed exploration (which is what LAMA does, see Section 6), and determining the earliest or least costly occurrence of an acceptable landmark in this structure. A relaxed plan to this landmark is\nthen extracted, and the operators in this plan form preferred operators if they are applicable in the current state.\n6. The Cost-Sensitive FF/add Heuristic\nWhen we first introduced the landmark heuristic (Richter et al., 2008), it proved not to be competitive on its own, compared to established heuristics like the FF heuristic (Hoffmann & Nebel, 2001). However, the joint use of the FF heuristic and the landmark heuristic in a multi-heuristic search improved the performance of a planning system, compared with only using the FF heuristic. This is thus the path LAMA follows. The FF heuristic is based on a relaxation of the planning task that ignores delete effects, which in FDR tasks translates to allowing state variables to hold several values simultaneously.\nThe FF heuristic for a state s is computed in two phases: the first phase, or forward phase, calculates an estimate for each fact in the planning task of how costly it is to achieve the fact from s in a relaxed task. Concurrently, it selects an operator called best support for each fact F, which is a greedy approximation of a cheapest achiever (an achiever a of F where the costs of making a applicable and applying it are minimal among all achievers of F, when starting in s). In the second phase, a plan for the relaxed task is constructed based on the best supports for each fact. This is done by chaining backwards from the goals, selecting the best supports of the goals, and then recursively selecting the best supports for the preconditions of already selected operators. The union of these best supports then constitutes the relaxed plan (i. e., for each fact its best support is only added to the relaxed plan once, even if the fact is needed several times as a precondition). The length of the resulting relaxed plan is the heuristic estimate reported for s.\nThe forward phase can be viewed as propagating cost information for operators and facts in a relaxed planning graph (Hoffmann & Nebel, 2001). However, this graph does not need to be explicitly constructed to compute the heuristic. Instead, a form of generalised Dijkstra cheapestpath algorithm as described by Liu, Koenig and Furcy (2002) is used in LAMA, which propagates costs from preconditions to applicable operators and from operators to effects. In this method, each operator and fact is represented only once, reducing the time and space requirements from O(NK), where N is the size of the relaxed planning task and K the depth of the relaxed planning graph, to O(N). In order to deal with conditional effects, operators with n effects are split into n operators with one effect each, and the corresponding effect conditions are moved into the preconditions of those operators. If any of those n operators is selected for inclusion in the relaxed plan, the original operator is included instead (again, each operator is included in the relaxed plan only once).\nThe cost estimate for an operator in the original FF heuristic is its depth in the relaxed planning graph, which in the case of planning with unit-cost operators is equivalent (Fuentetaja, Borrajo, & Linares Lo\u0301pez, 2009) to propagating costs via the hmax criterion (Bonet & Geffner, 2001). The hmax criterion estimates the cost of an operator by the maximum over the costs of its preconditions, plus the action cost of the operator itself (1 when planning without action costs). The cost of a fact is estimated as the cost of its cheapest achiever, or zero if the fact is true in the current state s. While originally proposed for unit-cost planning, this heuristic can be adapted to cost-based planning in a straightforward way by using action costs in the cost propagation phase, and reporting the total cost of the resulting relaxed plan, rather than its length, as the heuristic estimate.\nUsing other criteria for cost propagation results in variations of the FF heuristic (Bryce & Kambhampati, 2007; Fuentetaja et al., 2009). One variant that has been previously proposed in the litera-\nture (Do & Kambhampati, 2003) is to use the hadd criterion (Bonet & Geffner, 2001). It is similar to the hmax criterion except for estimating the cost of operators via the sum, rather than the maximum, of the costs for their preconditions. We will in the following use the term FF/add for this variant of the FF heuristic. Independently of us, Keyder and Geffner (2008) implemented the FF/add heuristic which they call ha in their planner FF(ha) at IPC 2008. A formal specification of the FF/add heuristic can be found in their paper. The heuristic function in LAMA is similar to this cost-sensitive FF/add heuristic. However, as with the landmark heuristic, LAMA is not purely guided by action costs, but rather uses both cost and distance estimates equally. This means that during cost propagation, each operator contributes its action cost plus 1 for its distance, rather than just its action cost, to the propagated cost estimates."}, {"heading": "7. Experiments", "text": "To evaluate how much each of the central features of LAMA contributes to its performance, we have conducted a number of experiments comparing different configurations of these features. We focus our detailed evaluation on the benchmark tasks from the International Planning Competition (IPC) 2008, as we are interested in the setting of planning with action costs. The effect of landmarks in classical planning tasks without actions costs has been studied in previous work (Richter et al., 2008), but we provide summarising results for this case, using the domains of IPCs 1998\u20132006, in Section 7.6. The benchmark set of IPC 2008 comprises 9 domains with 30 tasks each, resulting in a total of 270 tasks. For one of the domains (Openstacks), two different formulations were available (STRIPS and ADL). As in the competition, we report the better result of those two formulations for each planner.\nAs described in Section 1, LAMA builds on the platform provided by Fast Downward in three major ways: (1) through the use of landmarks, (2) by using cost-sensitive heuristics to guide search for cheap plans, and (3) by employing anytime search to continue to search for better solutions while time remains. To examine the usefulness of landmarks, we conduct experiments with and without them, while keeping all other planner features fixed. The use of action costs in LAMA is the result of a number of design decisions. Both the landmark heuristic and the FF/add heuristic have been made cost-sensitive. However, rather than focusing purely on action costs, LAMA uses both distance estimates and cost estimates in combination (see Section 3.3) to balance speed and quality of the search. To measure the benefit of this combining approach, we test three different approaches to dealing with costs: (a) using the traditional cost-unaware heuristics (distance estimates), (b) using purely cost-sensitive heuristics (though using distance estimates for tie-breaking), and (c) using the combination of the distance and cost estimates, as in LAMA. The different choices regarding landmarks and approaches to action costs thus result in the following six planner configurations:\n\u2022 F: Use the cost-unaware FF/add heuristic (estimating goal distance).\n\u2022 Fc: Use the purely cost-sensitive FF/add heuristic (estimating cost-to-go).\n\u2022 F+c : Use the FF/add heuristic that combines action costs and distances.\n\u2022 FL: Use the cost-unaware variants of both the FF/add heuristic and the landmark heuristic.\n\u2022 FLc: Use the purely cost-sensitive variants of both heuristics.\n\u2022 FL+c : Use the variants that combine action costs and distances for both heuristics.\nNote that in contrast to the setting of optimal planning (Karpas & Domshlak, 2009), the landmark heuristic by itself is not competitive in our case, and landmarks in LAMA are used only to provide additional information to an already guided search. As such, we do not include any configurations using only landmarks as heuristic estimators in our detailed results. However, we provide summarising results supporting our claim that they are not competitive.\nEach configuration is run with iterated (anytime) search. When highlighting the contribution of the iterated search, we report first solutions vs. final solutions, where the final solution of a configuration is the last, and best, solution it finds within the 30-minute timeout. (Note that the quality of a solution is always determined by its cost, irrespective of whether the heuristic used to calculate it is cost-sensitive or not.) When discussing the three possible approaches to costs (costunaware search, purely cost-sensitive search, or LAMA\u2019s combination of distances and costs) we write X, Xc, and X+c to denote the three cost approaches independently of the heuristics used.\nWe measure performance using the same criterion that was employed at IPC 2008 (Helmert et al., 2008). Each planner configuration is run for 30 minutes per task. After the timeout, a planner aggregates the ratio c\u2217/c to its total score if c is the cost of the plan it has found, and c\u2217 is the cost of the best known solution (e. g., a reference solution calculated by the competition organisers, or the best solution found by any of the participating planners).\nExperiments were run on the hardware used in the competition, a cluster of machines with Intel Xeon CPUs of 2.66GHz clock speed. The time and memory limits were set to the same values as in the competition, using a timeout of 30 minutes and a memory limit of 2 GB. In the following, we first provide a general overview of the results. Then we discuss special cases, i. e., domains where the results for certain configurations deviate from the overall trend, and try to give plausible explanations for why this may happen."}, {"heading": "7.1 Overview of Results", "text": "In this section, we show that the purely cost-based FF/add configuration Fc solves significantly fewer tasks than its cost-unaware counterpart F. While Fc finds higher-quality solutions, this does not make up for its low coverage (number of solved tasks) when measuring performance with the IPC criterion. Using landmarks improves quality slightly, so that cost-unaware search using landmarks (FL) achieves the highest IPC performance score amongst our configurations. When using the cost-sensitive FF/add heuristic, adding landmarks (resulting in the configurations FLc and FL+c ) increases coverage substantially, while incurring only a small loss in quality. Iterated search improves the scores of all configurations significantly. Lastly, using the combination of cost and distance estimates in the heuristics (X+c ) is superior to pure cost-based search when using iterated search. Together, using landmarks and the combination of cost and distance estimates (FL+c ) achieves nearly the same performance as the FL configuration.\nIn the following, we support these findings with experimental data. In Section 7.1.1 (Performance in Terms of the IPC Score), we show that the cost-sensitive FF/add heuristic by itself scores lowly in terms of the IPC criterion, but that landmarks and the combination of cost and distance estimates together make up for this bad performance. Furthermore, our results demonstrate the magnitude of the impact that iterated search has on the performance scores. In Section 7.1.2 (Coverage), we show that the bad performance of the cost-sensitive FF/add heuristic is due to solving fewer tasks, and that the use of landmarks mitigates this problem. In Section 7.1.3 (Quality), we present data showing that the purely cost-sensitive FF/add heuristic finds higher-quality plans than the cost-\nunaware FF/add heuristic in the first search, but that with iterated search, this difference all but disappears. Furthermore, after iterated search the intermediate approach of using cost and distance estimates scores higher than the purely cost-based search. LAMA\u2019s approach of using landmarks and the combination of cost and distance estimates (FL+c ) thus effectively mitigates the bad performance of the cost-sensitive FF/add heuristic.\n7.1.1 Performance in Terms of the IPC Score\nThe scores of all planners scoring more than 100 points at IPC 2008 are shown in the top part of Table 1. Apart from LAMA, this includes a base planner run by the competition organisers (FF with a preprocessing step that compiles away action costs), the FF(ha) and FF(hsa) planners by Keyder\nand Geffner (2008) and the C3 planner by Lipovetzky and Geffner (2009). The plans found by these planners have been obtained from the competition website (Helmert et al., 2008). However, the scores for those plans depend on the best known solutions for the tasks. The scores we show here thus differ from the ones published at IPC 2008, as we have re-calculated them to reflect new best solutions found in our experiments. To illustrate the magnitude of the change, the original total scores of the IPC planners are shown in parentheses in the last table row.\nWhile the configuration FL+c results in essentially the same planner as (the IPC version of) LAMA, we report its results again, as some minor corrections have been implemented in LAMA since the competition. In addition, the planner makes arbitrary decisions at some points during execution due to underlying programming library methods, leading to varying results. However, as Table 1 shows these differences between FL+c and LAMA are very small. We have furthermore added columns to the table showing the hypothetical results for LAMA that would be obtained if its search were slowed down by the constant factors 10 and 100, respectively (i. e., the results obtained when cutting of the search after 3 minutes, or 18 seconds, respectively). The numbers show that LAMA still outperforms the other IPC planners even with a severe handicap, demonstrating that the good performance of LAMA is not mainly due to an efficient implementation.\nThe bottom part of Table 1 contains the results for our six experimental configurations after the first search iteration (left) and after the 30-minute timeout (right). As can be seen, both the use of landmarks and iterated search lead to significant improvements in performance. Even with just one of those two features our planner performs notably better than any of its competitors from IPC 2008. (Note however that the baseline planner performed very badly in Cyber Security due to problems with reading very large task descriptions.) In combination, the benefits of landmarks and iterated search grow further: in cost-unaware search the use of landmarks results in 2 additional score points for the first solutions, but in 9 additional points for the final solutions. Similar results hold for the cost-sensitive configurations. This is mainly due to the Openstacks domain, where using landmarks is highly detrimental to solution quality for the first solutions. Iterated search mitigates the problem by improving quality to similar levels with and without landmarks. Overall, there is thus a slight synergy effect between landmarks and iterated search, making the joint benefit of the two features larger than the sum of their individual contributions. The effect of landmarks in the Openstacks domain is discussed in more detail later.\nThe use of cost-sensitive search did not pay off in our experiments. Cost-unaware search is always at least roughly equal, and often substantially better than the cost-sensitive configurations. Cost-sensitive planning seems to be not only a problem for LAMA, but also for the other participating planners at IPC 2008: notably, all cost-sensitive competitors of LAMA fare worse than the cost-ignoring baseline. In LAMA, best performance is achieved by using cost-unaware search with landmarks and iterated search. However, using the combination of cost and distance estimates instead (FL+c ) leads to performance that is almost equally good. In particular, FL+c is substantially better than the pure cost search FLc if iterated search is used.\nA more detailed view on the same data is provided in Figure 6, where we show the performance over time for our six experimental configurations. A data point at 100 seconds, for example, shows the score the corresponding planner would have achieved if the timeout had been 100 seconds. As the top panel shows, cost-sensitive search is consistently worse than cost-unaware search when using only the FF/add heuristic. Using landmarks (see centre panel), the two settings FL and FL+c achieve better performance than F, though FL+c needs 2 minutes to surpass F, while FL does so within 5 seconds. Pure cost search, even with landmarks (FLc), performs worse than F at all times. The\nbottom panel of Figure 6 shows that when not using iterated search, the performance of the 4 best configurations FL, F, FL+c , and FLc is fairly similar eventually, but the cost-sensitive approaches need more time than the cost-unaware configurations to reach the same performance levels.\n7.1.2 Coverage\nThe bad performance of cost-sensitive search is surprising, given that our performance criterion awards higher scores to cheaper plans. One explanation could be that this is mainly due to different coverage. If finding plans of high quality is substantially harder than finding plans of low quality, then focusing on nearest goals rather than cheapest goals may solve more tasks within a given time limit. In Table 2 we show the coverage for all considered planners and configurations. The numbers confirm that when not using landmarks, the coverage of cost-unaware search is indeed substantially higher than the coverage of cost-sensitive search. However, with landmarks, the differences in coverage between the various cost approaches are small. In particular, landmarks do not improve coverage further for the cost-unaware search, but bring the cost-sensitive configurations up\nto the same coverage level as the cost-unaware search. Landmarks thus seem to be very helpful in overcoming the coverage problems of cost-sensitive search.\nAs mentioned before, the landmark heuristic by itself is however not competitive. Using only the landmark heuristic and not the FF/add heuristic results in IPC 2008 performance scores between 164 and 167 with iterated search, and coverage points between 185 and 189 for the three possible cost settings. This is substantially worse then the performance scores greater than 194 and coverage points greater than 223 achieved by any of the other LAMA configurations.\n7.1.3 Quality\nAs a next step, we look purely at solution quality. Firstly, we want to answer the question whether the improvement in coverage achieved by landmarks in the cost-sensitive search comes at a price in solution quality, i. e., whether using landmarks directs the search to close goals rather than cheap goals. Secondly, we would like to know how the solution quality differs between the cost-sensitive and the cost-unaware configurations. In particular, how much quality do we lose by combining\ndistance and cost estimates (X+c ) as opposed to using pure cost search (Xc)? The score used at IPC 2008 and in Table 1 incorporates both coverage and quality information by counting unsolved tasks as 0 \u2013 a method that allows ranking several planners solving different subsets of the total benchmark set. When we are interested in examining quality independent of coverage, we must restrict our focus on those tasks solved by all compared planners. Table 3 contains quality information comparing the solution costs of several configurations, where we compare configurations pair-wise in order to maximise the number of commonly solved tasks. The top part of Table 3 contains comparisons involving the first solutions found by each configuration, while the bottom part of the table concerns the best solutions found after iterative search. For each pair of configurations we show the number of tasks solved by both, and the geometric mean of the cost ratio for the plans they find.\nAs expected, the cost-sensitive configurations Fc and F+c find cheaper plans than the costunaware configuration F on average, where in particular the pure cost search Fc finds high-quality first plans (see the first column in the top part of the table). For both Fc and F+c , however, the difference to F is not very large. In some domains, most notably in Elevators, the plans found by the cost-sensitive heuristics are actually worse than the plans found by cost-unaware search.\nLandmarks deteriorate quality for the first plans of Fc; but F+c , which starts out with a worse quality than Fc, is not noticeably further deteriorated by landmarks. For both configurations, however, the main negative impact through landmarks is in the Openstacks domain, where plans become nearly twice as expensive for Fc, and 50% more expensive for F+c . By contrast, in the remaining 8 domains average plan quality for both configurations with landmarks is even slightly better on average than without landmarks.\nWe note that iterative search has a remarkable impact on the relative performance of the different configurations. When looking at the solutions found after iterative search, Fc actually performs worse than F+c , whereas it is the other way round for the first solutions (compare the first two columns in the top row versus the bottom row of the table). This can be explained to some extent by the fact that the same reasons that cause Fc to have low coverage also prevent it from improving much over time. As we will show in selected domains later, the cost-sensitive heuristic often expands many more nodes than the cost-unaware search, leading to the observed behaviour. This is most likely due to the fact that finding plans of high quality is hard and thus unsuccessful in many of the benchmark tasks. For example, in some domains cost-sensitive search leads to large local minima that do not exist for cost-unaware search. More generally, good plans are often longer than bad plans, which may lead to increased complexity in particular in domains where the heuristic values are inaccurate. We will showcase the problems of cost-sensitive search in more detail in the Elevators and PARC Printer domains later on.\nWith iterative search, landmarks do not deteriorate quality for either Fc nor F+c on average, as the negative impact of the Openstacks domain is no longer present. (This effect in the Openstacks domain will be discussed in more detail later.)\nSummarising our findings, we can say that landmarks effectively support the cost-sensitive FF/add heuristic in finding solutions, without steering the search away from good solutions. Similarly, combining distance and cost estimates as in X+c leads the search to finding solutions quickly without overly sacrificing quality, as is demonstrated by its superior anytime performance compared to pure cost search.\nBy way of example, we now present detailed results for four of the nine competition domains. We choose domains that we deem to be of particular interest because the results in them either exaggerate or contradict the general trends discussed so far. The domains Elevators and PARC Printer\nhighlight the problems of cost-sensitive search; in Cyber Security cost-sensitive search performs uncharacteristically well; and Openstacks is a domain where landmarks do not lead to the usual improvement, but rather to a deterioration of performance."}, {"heading": "7.2 Elevators", "text": "The Elevators domain models the transportation of passengers in a building via fast and slow elevators, where each elevator has a certain passenger capacity and can access certain floors. Passengers may have to change elevators to get to their final destination, and furthermore the two different types of elevators have different associated cost functions. This is in contrast to the Miconic domain, used in an earlier international planning competition (Bacchus, 2001), which also models the transporting of passengers via elevators, but where there is only one elevator that can access all floors with just one (unit-cost) operator. In Elevators, the floors in the building are grouped into blocks, overlapping by one floor. Slow elevators only operate within a block and can access all floors within their block. Fast elevators can access all blocks, but only certain floors within each block (in the first 10 IPC tasks every second floor, and in the other 20 tasks every fourth floor). Fast elevators are usually more expensive than slow elevators except for a distance of two floors, where both elevator types cost the same. However, fast elevators may sometimes be advantageous when transporting passengers between blocks (as they avoid the need for passengers to switch elevators on the shared floor between blocks), and they usually have a higher capacity.\nAn example task with eight floors, grouped into two blocks, is shown in Figure 7. There are a total of four elevators, two slow ones and two fast ones. The cost function used in the 30 IPC tasks for moving an elevator from its current location to a target floor is 6+n for slow elevators and 2+3n for fast elevators, where n is the distance travelled (the number of floors between the current location of the elevator and its target). Operators concerning passengers boarding and leaving elevators are free of cost. Assuming this cost function, it is cheaper in this example to transport the passenger located at floor 0 using the two slow elevators (changing at floor 4) than using a direct fast elevator.\nElevators is one of the domains where configurations using the cost-sensitive FF/add heuristic solve far fewer problems than their cost-unaware counterparts. Using landmarks increases coverage, but does not solve the problem completely. Furthermore, it is notable that on the problems that the cost-sensitive configurations do solve, their solutions often have worse quality than the solutions of the cost-unaware configurations. Table 4 illustrates this fact for the first solutions found when using\nonly the FF/add heuristic. With iterative search (not shown), the solution quality for F+c improves to a similar level as that of F, whereas Fc remains substantially worse.\nWhile we do not have a full explanation for why the configurations involving the cost-sensitive FF/add heuristic perform so badly in this domain, several factors seem to play a role. Firstly, in its attempt to optimise costs, the cost-sensitive FF/add heuristic focuses on relatively complex solutions involving mainly slow elevators and many transfers of passengers between elevators, where the relaxed plans are less accurate (i. e., translate less well to actual plans), than in the case for the cost-unaware heuristic. Secondly, the costs associated with the movements of elevators dominate the heuristic values, causing local minima for the cost-sensitive heuristic. Thirdly, the capacity constraints associated with elevators may lead to plateaus and bad-quality plans in particular for the cost-sensitive heuristic. In the following sections, we describe each of these factors in some detail.\nLastly, we found that the deferred heuristic evaluation technique used in LAMA (see Section 3.3) did not perform well in this domain. When not using deferred evaluation, the Fc configuration solves 3 additional tasks (though the quality of solutions remains worse than with the F configuration). This partly explains why the FF(ha) planner by Keyder and Geffner (2008) has a substantially higher coverage than our Fc configuration in this domain. While the two planners use the same heuristic, they differ in several aspects. Apart from deferred evaluation these aspects include the search algorithm used (greedy best-first search vs. enhanced hill-climbing) and the method for using preferred operators (maintaining additional queues for preferred states vs. pruning all nonpreferred successor states).\n7.2.1 Slow vs. Fast Elevators\nWhen examining the results, we found that the Fc and F+c configurations tend to produce plans where slow elevators are used for most or all of the passengers, while the F configuration uses fast elevators more often (cf. Table 5). This is not surprising, as for each individual passenger, travelling from their starting point to their destination tends to be cheaper in a slow elevator (unless the distance is very short), whereas fewer operators are typically required when travelling in a fast elevator. The independence assumptions inherent in the FF/add heuristic (see Section 6) lead to constructing relaxed plans that aim to optimise the transportation of each passenger individually, rather than taking synergy effects into account.\nThe plans produced by Fc and F+c are also longer, on average, than the plans produced by F (see Table 4), one reason for this being that the predominant use of slow elevators requires passengers to change between elevators more often. As plans become longer and involve more passengers travelling in each of the slow elevators, heuristic estimates may become worse. For example, the relaxed plans extracted for computation of the heuristic are likely to abstract away more details if more passengers travel in the same elevator (e. g., since once a passenger has been picked up from or delivered to a certain location, the elevator may \u201cteleport\u201d back to this location with no extra cost in a relaxed plan to pick up or deliver subsequent passengers). Generally, we found that the relaxed plans for the initial state produced by Fc and F+c tend to be similar in length and cost to those produced by F, but the final solutions produced by Fc and F+c are worse than those of F. One reason for this is probably that the increased complexity of planning for more passenger change-overs between elevators in combination with worse relaxed plans poses a problem to the cost-sensitive FF/add heuristic.\n7.2.2 LocalMinima Due to Elevator-Movement Costs\nSince action costs model distances, the total cost of a relaxed plan depends on the target floors relative to the current position of an elevator. For both Fc and F+c , the action costs of moving the elevator usually dominate the estimates of the FF/add heuristic. Consider the two example tasks in Figure 8, which differ only in the initial state of the elevators. The elevators need to travel to all three floors in a solution plan, but due to abstracted delete effects a relaxed plan for the initial state will only include operators that travel to the two floors other than the starting floor of the elevator (i. e., the elevator can be \u201cteleported\u201d back to its starting floor without cost). In the left task, the relaxed cost of visiting all three floors is lower than in the right task, as the cost is in the left task is the sum of going from floor 4 to floor 8, and going from floor 4 to floor 0, resulting in a total cost of 10 + 10 = 20. In the right task, the relaxed cost for visiting all floors is the cost of going from floor 0 to floor 4, and from floor 0 to floor 8, resulting in a total cost of 10 + 14 = 24. In the left task, once the passenger has boarded the elevator on floor 4, all immediate successor states have a\nworse heuristic estimate due to the movement costs of the elevator. In particular, the correct action of moving the elevator up to floor 8 (to deliver the passenger) results in a state of worse heuristic value. If we increased the number of waiting passengers at floor 4, the planning system would therefore try boarding all possible subsets of passengers before moving the elevator. And even once the elevator is moved up to floor 8, the heuristic estimate will only improve after the passenger has been dropped off and either (a) the elevator has moved back to floor 4, or (b) the second passenger has boarded and the elevator has moved down to floor 0.\nConsequently, movement costs may dominate any progress obtained by transporting passengers for a number of successive states. In other words, the planner often has to \u201cblindly\u201d achieve some progress and move the elevators towards a middle position given the remaining target floors, in order for the cost-sensitive heuristic to report progress. For the cost-unaware heuristic, the situation is less severe, as the number of elevator movements in the relaxed plan does not increase, and hence the planner encounters a plateau in the search space rather than a local minimum. The use of preferred operators may help to escape from the plateau relatively quickly, whereas a local minimum is much harder to escape from. Two approaches exist that may circumvent this problem. Firstly, the use of enforced hill-climbing (Hoffmann & Nebel, 2001) rather than greedy best-first search is likely to avoid exploration of the entire local minima: in this approach, a breadth-first search is conducted from the first state of a minima/plateau until an improving state is found. Secondly, an improved heuristic could be used that approximates the optimal relaxed cost h+ more exactly. The cost minima shown in Figure 8 is brought about by the independence assumptions inherent in the FF/add heuristic, which estimate the relaxed cost for each goal fact individually in the cheapest possible way. An optimal relaxed plan, however, costs the same in the left task as in the right task. A more accurate approximation of the optimal relaxed cost h+ could therefore mitigate the described cost minima. Keyder and Geffner (2009) have recently proposed such an improvement of the FF/add heuristic4 and shown it to be particularly useful in the Elevators and PARC Printer domains.\n4. In Keyder & Geffner\u2019s approach, the relaxed plan extracted by the FF/add heuristic is improved by iteratively (1) selecting a fact F, (2) fixing all operators that are not related to F (because they do not contribute to achieving F nor rely on its achievement), and (3) computing a cheaper way of achieving F given the operators that were fixed in the previous step.\n7.2.3 Plateaus Due to Capacity Constraints\nIn general, the relaxed plans in the Elevators domain are often of bad quality. One of the reasons is the way the capacity of elevators is encoded in the operators for passengers boarding and leaving elevators. For any passenger p transported in an elevator e, one of the preconditions for p leaving e is that n passengers be boarded in e, where n is a number greater than 0. When constructing a relaxed plan, the FF/add heuristic recursively selects operators that achieve each necessary precondition in the cheapest way. This results in boarding the passenger that is closest to e in the initial state, even if this passenger p\u2032 is different from p, to achieve the condition that some passenger is boarded. The relaxed plan will then contain operators for both boarding p and p\u2032 into e, and may furthermore contain other operators for boarding p\u2032 into whatever elevator e\u2032 is deemed best for transporting p\u2032. Hence, the relaxed plans often contain many unnecessary boarding operators.\nAs mentioned in Section 3.3, the greedy best-first search in LAMA breaks ties between equally promising operators by trying the cheaper operator first. Consequently, the zero-cost operators for passengers boarding and leaving elevators are tried first in any state. We found that as soon as one passenger is boarded into a certain elevator, the relaxed plans in the next state are often substantially different, in that more passengers are being assigned to that same elevator. This can be explained by the fact that as soon as one passenger is in an elevator, the precondition for leaving that elevator which is having at least one person boarded, is fulfilled (rather than incurring additional cost). In some example tasks we examined, we found that this effect results in committing to bad boarding operators: LAMA may initially try some bad boarding operator, e. g. boarding the nearest passenger into an elevator to satisfy a capacity precondition for another passenger, as described above. The relaxed plan in the successor state then assigns more passengers to this elevator, at lower cost. Due to the improved heuristic value of the successor state, LAMA retains this plan prefix, even though the first operator was a bad one. It is plausible (though we did not explore it experimentally) that this effect is stronger for the configurations involving the cost-sensitive heuristic, as the costs of their relaxed plans vary more strongly from one state to the next.\nMore importantly, the capacity constraints lead to plateaus in the search space, as correct boarding and leaving operators are often not recognised as good operators. For example, if the capacity of an elevator is c, then boarding the first c\u22121 passengers that need to be transported with this elevator usually leads to improved heuristic values. However, boarding the c-th passenger does not result in a state of better heuristic value if there are any further passengers that need to be transported via the same elevator, because the c-th passenger boarding destroys the precondition that there must be room in the elevator for other passengers to board. Similarly, the correct leaving of a passenger may not lead to an improved heuristic value if it makes the elevator empty and other passengers need to be transported with that elevator later (because the last passenger leaving destroys the precondition for leaving that there must be at least one passenger boarded).\nThese effects exist for both the cost-sensitive and the cost-unaware heuristic. However, they typically occur within the plateaus (F) or local minima (Fc, F+c ) created by the elevator positions, as described in the previous section, which means they affect the cost-sensitive configurations more severely. The plateaus become particularly large when several passengers are waiting on the same floor, e. g. when passengers are accumulating on the floor shared by two blocks in order to switch elevators. The planner then tries to board all possible subsets of people into all available elevators (as the zero-cost boarding and leaving operators are always tried first), moving the elevators and even dropping off passengers at other floors, and may still fail to find a state of better heuristic value.\nWhen examining the number of states in local minima for each of the configurations, we found that Fc and F+c indeed encounter many more such states than F. For example, the percentage of cases in which a state is worse than the best known state is typically around 10% (in rare cases 25%) for F. For Fc and F+c , on the other hand, the numbers are usually more than 35%, often more than 50%, and in large problems even up to 80%.\nTo verify that the capacity constraints indeed contribute to the bad performance of the costsensitive heuristic in this domain, we removed these constraints from the IPC tasks and ran the resulting problems with the F, Fc and F+c configurations. Not surprisingly, the tasks become much easier to solve, as elevators can now transport all passengers at once. More interestingly though, the bad plan qualities produced by the cost-sensitive configurations (relative to the cost-unaware configuration) indeed become much less frequent, as Table 6 shows.\nIn summary, our findings suggest that the bad performance of the cost-sensitive FF/add heuristic in the Elevators domain is due to bad-quality relaxed plans (brought about by the focus on slow elevators and the capacity constraints) and plateaus and local minima in the search space (resulting from the movement costs of elevators and the capacity constraints)."}, {"heading": "7.3 PARC Printer", "text": "The PARC Printer domain (Do, Ruml, & Zhou, 2008) models the operation of a multi-engine printer capable of processing several printing jobs at a time. Each sheet that must be printed needs to pass through several printer components starting in a feeder and then travelling through transporters, printing engines and possibly inverters before ending up in a finishing tray. The various sheets belonging to a print job must arrive in the correct order at the same finisher tray, but may travel along different paths using various printing engines. There are colour printing engines and ones that print in black and white, where colour printing is more expensive. The action costs of operators are comparatively large, ranging from 2000 to more than 200,000. Colour-printing is the most expensive operator, while operators for printing in black and white cost roughly half as much, and operators for transporting sheets are relatively cheap.\nLike in the Elevators domain, the cost-sensitive FF/add heuristic did not perform well here, with Fc and F+c failing to solve many of the tasks that the cost-unaware configuration F is able to solve. (Note that Fc and F+c perform very similarly in this domain, as the large action costs outweigh the distance estimates in F+c .) However, in contrast to the Elevators domain, the Fc and F+c configurations result in notably improved plan quality compared to F. An overview of the number\nof problems solved and the average quality of first solutions is shown in Table 7. When using landmarks, the differences between cost-sensitive and cost-unaware configurations are strongly reduced, with all three landmark configurations achieving a better performance than the F configuration.\nLike in Elevators, we found the quality of relaxed plans to be poor. In the cost-unaware case, a relaxed plan transports sheets from a feeder to the finishing tray via a shortest path, irrespective of whether a suitable printing engine lies on this path. As any path from feeder to finishing tray passes through some printing engine, this frequently involves printing a wrong image on a paper, while additional operators in the relaxed plan handle the transportation from a feeder to a suitable printing engine to print the correct image on the sheet as well. When the cost-sensitive heuristic is used, relaxed plans furthermore become substantially longer, using many transportation operators to reach a cheap printing engine. Analogously to the Elevators domain, the increased complexity associated with longer plans (in combination with the bad quality of the relaxed plans) is thus likely to be the reason for the bad performance of the cost-sensitive heuristic. However, landmarks mitigate the problem, as the numbers of solved tasks in Table 7 clearly show. Landmarks found in this domain encompass those for printing a correct image on each sheet, where a disjunctive landmark denotes the possible printers for each sheet. This helps to counteract the tendencies of the cost-sensitive FF/add heuristic to transport sheets to the wrong printers.\nIn summary, PARC Printer is like Elevators a domain where the cost-sensitive FF/add heuristic performs badly, though in contrast to Elevators the problem here is purely one of coverage, not of solution quality. Even more than in Elevators, landmarks overcome the problems of the cost-sensitive configurations, improving them to a similar performance levels as the cost-unaware configurations."}, {"heading": "7.4 Cyber Security", "text": "The Cyber Security domain stands out as a domain where the cost-sensitive configurations perform significantly better than their cost-unaware counterparts, especially when looking at first solutions. (Iterative search reduces the gap, but does not close it completely.) The domain models the vulnerabilities of computer networks to insider attacks (Boddy, Gohde, Haigh, & Harp, 2005). The task consists in gaining access to sensitive information by using various malware programs or physically accessing computers in offices. Action costs model the likelihood of the attack to fail, i. e., the risk of being exposed. For example, many actions in the office of the attacker, like using the computer, do not involve any cost, whereas entering other offices is moderately costly, and directly instructing people to install specific software has a very high associated cost. In particular, action costs are used to model the desire of finding different methods of attack for the same setting. For example, several tasks in the domain differ only in the costs they associate with certain operators.\nIn the Cyber Security domain, taking action costs into account pays off notably: while the Fc and F+c configurations solve 2 and 1 problems less, respectively, than the F configuration (see Table 2),\nthey nevertheless result in a better total score. Using landmarks, both cost-sensitive configurations are improved such that they solve all problems while maintaining the high quality of solutions, resulting in an even larger performance gap between FLc (27.59 points) and FL+c (26.60 points) on the one side, and FL (20.43 points) on the other side.\nThe plans found by the cost-unaware search often involve physically accessing computers in other offices or sending viruses by email, and as such result in large cost. Lower costs can be achieved by more complex plans making sophisticated use of software. As opposed to the Elevators and PARC Printer domains, the relaxed plans in Cyber Security are of very good quality. This explains why the performance of the cost-sensitive heuristic is not negatively impacted by longer plans. Using iterative search improves the performance of FL and F to nearly the same levels as their cost-sensitive counterparts (see Table 8)."}, {"heading": "7.5 Openstacks", "text": "The Openstacks domain models the combinatorial optimisation problem minimum maximum simultaneous open stacks (Fink & Vo\u00df, 1999; Gerevini, Haslum, Long, Saetti, & Dimopoulos, 2009), where the task is to minimise the storage space needed in a manufacturing facility. The manufacturer receives a number of orders, each comprising a number of products. Only one product can be made at a time, and the manufacturer will always produce the total required quantity of a product (over all orders) before beginning the production of a different product. From the time the first product in an order has been produced to the time when all products in the order have been produced, the order is said to be open and requires a stack (a temporary storage space). The problem consists in ordering the products such that the maximum number of stacks open at any time is minimised. While it is easy to find a solution for this problem (any product order is a solution, requiring n stacks in the worst case where n is the number of orders), finding an optimal solution is NP-hard. The minimisation aspect is modelled in the planning tasks via action costs, in that only the operator for opening new stacks has a cost of 1, while all other operators have zero cost. This domain was previously used at IPC 2006 (Gerevini et al., 2009). While that earlier formulation of the domain has unit costs, it is equivalent to the cost formulation described above. Since the number of operators that do not open stacks is the same in every plan for a given task, minimising plan length is equivalent to minimising action costs.\nWe noticed that in this domain using landmarks resulted in plans of substantially worse quality, compared to not using landmarks. In particular, this is true for the first plans found, whereas the use of anytime search improves the results for both configurations to similar levels. Across all cost settings, using the landmark heuristic in combination with the FF/add heuristic typically produces plans where the majority of orders is started very early, resulting in a large number of simultaneously open stacks, whereas using only the FF/add heuristic leads to plans in which the products corresponding to open orders are manufactured earlier, and the starting of new orders is delayed until earlier orders have been shipped. This is mainly due to the fact that no landmarks are found by\nLAMA regarding the opening of stacks, which means that due to the choice of action costs in this domain, all landmarks have cost zero and the landmark heuristic is not able to distinguish between plans of different cost. The landmarks found by LAMA relate to the starting and shipping of orders as well as the making of products.5 However, even if landmarks regarding the opening of stacks were found, they would not be helpful: landmarks state that certain things must be achieved, not that certain things need not be achieved. Landmarks can thus not be used to limit the number of open stacks. The landmark orderings are furthermore not helpful for deciding an order between products, as all product orders are possible\u2014which means that no natural orderings exist between the corresponding landmarks\u2014and no product order results in the form of \u201cwasted effort\u201d captured by reasonable landmark orderings.\nAs mentioned above, all landmarks found by LAMA have a minimal cost of zero. Therefore, the landmark heuristic fails to estimate the cost to the goal, and distinguishes states only via the number of missing started or shipped orders and products. (These goal distance estimates are used directly in FL, combined with the all-zero landmark heuristic cost estimates in FL+c , and as tiebreakers amongst the zero-cost estimates in FLc, resulting in the same relative ranking of states by the landmark heuristic in all three cases.) As soon as one stack is open, for each order o the operator that starts o achieves a landmark that is minimal with respect to landmark orderings (namely the landmark stating that o must be started), and the planner thus tends to start orders as soon as possible. The landmark heuristic is not able to take into account future costs that arise through bad product orderings. This is also a problem for the FF/add heuristic, albeit a less severe one: the FF/add heuristic accounts for the cost of opening (exactly) one new stack whenever at least one more stack is needed, and the heuristic will thus prefer states that do not require any further stacks.\nThe landmark heuristic does, however, provide a good estimate of the goal distance. Since the landmark heuristic prefers states closer to a goal state with no regard for costs, its use results\n5. If the size of disjunctions were not limited in LAMA, it would always find a landmark stacks avail(1) \u2228 stacks avail(2) \u2228 \u00b7 \u00b7 \u00b7 \u2228 stacks avail(n) stating that at least one of the n stacks must be open at some point. However, any landmark stating that two or more stacks need to be open would require a more complex form of landmarks involving conjunction, which LAMA cannot handle.\nin plans where stacks are opened as needed. This is reflected in our empirical results, where the additional use of the landmark heuristic drastically reduces the number of expanded search nodes (see Figure 9), but leads to higher-cost plans (see Figure 10). Without iterative search, the LAMA configuration FL+c only achieves 13.85 points for this domain, compared to 19.77 points when not using landmarks (configuration F+c ).\nUsing iterative search, the negative effect of the landmarks on quality is mitigated, as can be seen in Figure 11. FL+c generates up to 21 distinct, and each time improved, plans per problem. In the end, the difference in points is merely 27.40 for FL+c vs. 28.30 for F+c . This score is reached after less than 5 minutes of iterated search per task.\nThus, Openstacks is an example for a domain where landmarks are detrimental to solution quality. However, using landmarks provides the benefit of speeding up planning by reducing the\nnumber of expanded nodes. This allows iterative search to effectively improve solution quality in the given time limit such that the final results of using landmarks are similar to those of not using landmarks."}, {"heading": "7.6 Domains from Previous Competitions", "text": "Tables 9 and 10 show results on the IPC domains from previous years (1998\u20132006). As these domains do not contain action costs, the cost-sensitive configurations of LAMA are not applicable and LAMA runs with the FL configuration. The configurations examined for LAMA are thus FL and F, both with iterated search and without, where FL with iterated search is shown as LAMA. Also given are the results for two IPC-winning systems of previous years, FF and Fast Downward. For both FF and Fast Downward, we ran current versions. In particular Fast Downward has evolved substantially since its 2004 competition version, the original causal graph heuristic having been replaced with the better context-enhanced additive heuristic (Helmert & Geffner, 2008). After correspondence with the authors, the version of Fast Downward used here is the one featuring in recent work by Richter and Helmert (2009).\nAs Table 9 shows, LAMA performs better than both FF and Fast Downward in terms of the IPC 2008 criterion. This is true even if we turn off landmarks or iterated search in LAMA, but not if we turn off both options simultaneously. When viewing the large difference between the scores of iterated versus non-iterated search in LAMA, note that on these domains no \u201cbest known\u201d reference results were used in the score calculation (in contrast to the 2008 tasks, for most of which such reference results were generated manually or with domain-specific solvers by the competition organisers). This means that the planner producing the best solution for a task is awarded the highest-possible score of 1, even though better solutions might exist. This may skew results in favour of the planner that delivers cheaper solutions, i. e., exaggerate the differences between planners.\nTable 10 shows that LAMA\u2019s edge over Fast Downward is due to higher-quality solutions rather than coverage, as Fast Downward solves more problems. Compared to FF, LAMA has better coverage, with the gap between LAMA and FF being substantially larger than the gap between LAMA and Fast Downward. Note that the F and LAMA configurations roughly correspond to the results published as \u201cbase\u201d and \u201cheur\u201d in earlier work (Richter et al., 2008). However, subsequent changes to the code to support action costs negatively affect in particular the Philosophers domain, where we observe a significant decrease in coverage. This is also one of the reasons for the difference in coverage between LAMA and the closely related Fast Downward system.\nComparing the various experimental configurations for LAMA, we note that the use of landmarks leads to moderate improvements in both coverage and solution quality. As mentioned above, iterative search significantly improves performance in terms of the IPC 2008 score."}, {"heading": "8. Conclusion and Outlook", "text": "In this article, we have given a detailed account of the LAMA planning system. The system uses two heuristic functions in a multi-heuristic state-space search: a cost-sensitive version of the FF heuristic, and a landmark heuristic guiding the search towards states where many subgoals have already been achieved. Action costs are employed by the heuristic functions to guide the search to cheap goals rather than close goals, and iterative search improves solution quality while time remains.\nWe have conducted an extensive experimental study on the set of benchmark tasks from the last international planning competition, in order to identify how much each of the features of our planner contributes to its performance in the setting of planning with action costs. We discussed overall results and provided plausible explanations for deviating behaviour in some special cases.\nThe most noticeable outcome of our experiments is that using cost-sensitive heuristics did not produce the desired outcome. In particular, the cost-sensitive FF/add heuristic performs significantly worse than the FF/add heuristic that ignores costs. This is due to the cost-sensitive heuristic solving far fewer tasks while leading to little improvement in solution quality on the tasks that it does solve, especially when using iterated search. When investigating the reasons for this effect, we found that the cost-sensitive FF/add heuristic reacts strongly to bad relaxed plans, i. e., it is in particular in those domains where the relaxed plans computed by the heuristic have low quality that the costsensitive heuristic is likely to perform worse than the cost-unaware heuristic. As we showed for the Elevators domain, action costs may also introduce local minima into the search space where without action costs the search space of the FF/add heuristic would have plateaus. Moreover, the increased complexity of planning for a cheaper goal that is potentially further away from the initial state may lead to worse performance.\nLandmarks prove to be very helpful in this context, as they mitigate the problems of the costsensitive FF/add heuristic. Using landmarks, the coverage of cost-sensitive search is improved to nearly the same level as that of cost-unaware search, while not deteriorating solution quality. Despite the mitigating effect of landmarks, however, LAMA would still have achieved a slightly higher score at IPC 2008 if it had simply ignored costs, rather than using cost-sensitive heuristics. For cost-unaware search, we found landmarks to improve coverage and solution quality in the domains from the IPCs 1998\u20132006. On the domains from IPC 2008, landmarks improved solution quality for the cost-unaware search, but did not further increase the (already very high) coverage.\nIterative search improves results notably for all of our experimental configurations, raising the score of LAMA by a quarter on the IPC 2008 domains. In the Openstacks domain, we could furthermore observe a synergy effect between the iterative search and landmarks. While landmarks usually improve quality, in this domain they lead to bad plans by not accounting for action costs. However, they speed up planning so that the planner evaluates substantially fewer states. Iterative search then effectively improves on the initial bad plans while benefiting from the speed-up provided by the landmarks. In general, we can use landmarks as a means to quickly find good solutions, while using iterative search as a way to improve plan quality over time. Overall, we found that the domains used at IPC 2008 constitute a varied benchmark set that reveals various strengths and weaknesses in our planning system.\nBuilding on the results presented in this article, we identify several directions for future work. Firstly, our results suggest that more research into cost-sensitive heuristics is needed. We would like to conduct a more thorough analysis of the short-comings of the cost-sensitive FF/add heuristic, to answer the question whether and how they might be overcome. Keyder and Geffner (2009) propose a method for extracting better relaxed plans from the best supports computed by the costsensitive FF/add heuristic, resulting in improved coverage. However, the large ledge of the costunaware heuristic in our experiments suggests that the cost-unaware FF/add heuristic is still better than the improved cost-sensitive heuristic by Keyder and Geffner. It would be interesting to examine to what degree the problems we experienced with the FF/add heuristic extend to other delete-relaxation heuristics, and whether heuristics not based on the delete relaxation could be more effectively adapted to action costs. In addition, future work could explore the benefit of combin-\ning traditional distance estimators and cost-sensitive heuristics in more sophisticated ways than the mechanism currently used in LAMA (see the discussion in Section 3.3.2).\nSecondly, we believe it to be useful for future research to improve the definition of reasonable orderings, eliminating the problems of the definition by Hoffmann et al. mentioned in Section 4.1.\nThirdly, we would like to extend the use of landmarks in our system in several ways. For one, our current approach does not take into account whether the same landmark must be achieved several times. Supporting such multiple occurrences of landmarks would be beneficial in the Openstacks domain, for example, as it could help to minimise the creation of stacks by accounting for their costs. While methods exist for detecting the multiplicity of landmarks (Porteous & Cresswell, 2002; Zhu & Givan, 2003), it will be crucial to develop techniques for deriving orderings between the individual occurrences of such landmarks. Furthermore, we would like to extend LAMA to support more complex landmarks like conjunctions or other simple formulas. In addition to representing and using such landmarks in the landmark heuristic this involves the development of new methods for detecting them along with their corresponding orderings."}, {"heading": "Acknowledgments", "text": "The authors thank Malte Helmert, Charles Gretton, Sylvie Thiebaux and Patrik Haslum as well as the anonymous reviewers for helpful feedback on earlier drafts of this paper.\nThe computing resources for our experiments were graciously provided by Pompeu Fabra University. We thank He\u0301ctor Palacios for his support in conducting the experiments.\nNICTA is funded by the Australian Government, as represented by the Department of Broadband, Communications and the Digital Economy, and the Australian Research Council, through the ICT Centre of Excellence program.\nThis work was partially supported by Deutsche Forschungsgemeinschaft as part of the Transregional Collaborative Research Center SFB/TR 8 Spatial Cognition, project R4-[LogoSpace]."}], "references": [{"title": "AWA* \u2013 A window constrained anytime heuristic search algorithm", "author": ["S. Aine", "P.P. Chakrabarti", "R. Kumar"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Aine et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Aine et al\\.", "year": 2007}, {"title": "The AIPS\u201900 planning competition", "author": ["F. Bacchus"], "venue": "AI Magazine,", "citeRegEx": "Bacchus,? \\Q2001\\E", "shortCiteRegEx": "Bacchus", "year": 2001}, {"title": "Course of action generation for cyber security using classical planning", "author": ["M. Boddy", "J. Gohde", "T. Haigh", "S. Harp"], "venue": "Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Boddy et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boddy et al\\.", "year": 2005}, {"title": "Planning as heuristic search", "author": ["B. Bonet", "H. Geffner"], "venue": "Artificial Intelligence,", "citeRegEx": "Bonet and Geffner,? \\Q2001\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2001}, {"title": "A tutorial on planning graph based reachability heuristics", "author": ["D. Bryce", "S. Kambhampati"], "venue": "AI Magazine,", "citeRegEx": "Bryce and Kambhampati,? \\Q2007\\E", "shortCiteRegEx": "Bryce and Kambhampati", "year": 2007}, {"title": "All that glitters is not gold: Using landmarks for reward shaping in FPG", "author": ["O. Buffet", "J. Hoffmann"], "venue": "In Proceedings of the ICAPS 2010 Workshop on Planning and Scheduling Under Uncertainty", "citeRegEx": "Buffet and Hoffmann,? \\Q2010\\E", "shortCiteRegEx": "Buffet and Hoffmann", "year": 2010}, {"title": "Temporal planning using subgoal partitioning and resolution in SGPlan", "author": ["Y. Chen", "B.W. Wah", "Hsu", "C.-W"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Chen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "Ordering problem subgoals", "author": ["J. Cheng", "K.B. Irani"], "venue": "Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Cheng and Irani,? \\Q1989\\E", "shortCiteRegEx": "Cheng and Irani", "year": 1989}, {"title": "Sapa: A scalable multi-objective heuristic metric temporal planner", "author": ["M.B. Do", "S. Kambhampati"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Do and Kambhampati,? \\Q2003\\E", "shortCiteRegEx": "Do and Kambhampati", "year": 2003}, {"title": "On-line planning and scheduling: An application to controlling modular printers", "author": ["M.B. Do", "W. Ruml", "R. Zhou"], "venue": "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Do et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Do et al\\.", "year": 2008}, {"title": "PDDL2.2: The language for the classical part of the 4th International Planning Competition", "author": ["S. Edelkamp", "J. Hoffmann"], "venue": "Tech. rep. 195,", "citeRegEx": "Edelkamp and Hoffmann,? \\Q2004\\E", "shortCiteRegEx": "Edelkamp and Hoffmann", "year": 2004}, {"title": "Applications of modern heuristic search methods to pattern sequencing problems", "author": ["A. Fink", "S. Vo\u00df"], "venue": "Computers and Operations Research,", "citeRegEx": "Fink and Vo\u00df,? \\Q1999\\E", "shortCiteRegEx": "Fink and Vo\u00df", "year": 1999}, {"title": "PDDL2.1: An extension to PDDL for expressing temporal planning domains", "author": ["M. Fox", "D. Long"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fox and Long,? \\Q2003\\E", "shortCiteRegEx": "Fox and Long", "year": 2003}, {"title": "A unified view of cost-based heuristics", "author": ["R. Fuentetaja", "D. Borrajo", "C. Linares L\u00f3pez"], "venue": "In ICAPS 2009 Workshop on Heuristics for Domain-Independent Planning,", "citeRegEx": "Fuentetaja et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fuentetaja et al\\.", "year": 2009}, {"title": "Deterministic planning in the fifth international planning competition: PDDL3 and experimental evaluation of the planners", "author": ["A. Gerevini", "P. Haslum", "D. Long", "A. Saetti", "Y. Dimopoulos"], "venue": "Artificial Intelligence,", "citeRegEx": "Gerevini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gerevini et al\\.", "year": 2009}, {"title": "LPG: A planner based on local search for planning graphs with action costs", "author": ["A. Gerevini", "I. Serina"], "venue": "Proceedings of the Sixth International Conference on Artificial Intelligence Planning and Scheduling (AIPS", "citeRegEx": "Gerevini and Serina,? \\Q2002\\E", "shortCiteRegEx": "Gerevini and Serina", "year": 2002}, {"title": "On the extraction of disjunctive landmarks from planning problems via symmetry reduction", "author": ["P. Gregory", "S. Cresswell", "D. Long", "J. Porteous"], "venue": "In Proceedings of the Fourth International Workshop on Symmetry and Constraint Satisfaction Problems,", "citeRegEx": "Gregory et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gregory et al\\.", "year": 2004}, {"title": "Anytime heuristic search", "author": ["E.A. Hansen", "R. Zhou"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hansen and Zhou,? \\Q2007\\E", "shortCiteRegEx": "Hansen and Zhou", "year": 2007}, {"title": "Anytime heuristic search: First results", "author": ["E.A. Hansen", "S. Zilberstein", "V.A. Danilchenko"], "venue": "Technical report cmpsci 97-50,", "citeRegEx": "Hansen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1997}, {"title": "The Fast Downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Helmert,? \\Q2006\\E", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "Concise finite-domain representations for PDDL planning tasks", "author": ["M. Helmert"], "venue": "Artificial Intelligence,", "citeRegEx": "Helmert,? \\Q2009\\E", "shortCiteRegEx": "Helmert", "year": 2009}, {"title": "Unifying the causal graph and additive heuristics", "author": ["M. Helmert", "H. Geffner"], "venue": "Proceedings of the Eighteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Helmert and Geffner,? \\Q2008\\E", "shortCiteRegEx": "Helmert and Geffner", "year": 2008}, {"title": "The FF planning system: Fast plan generation through heuristic search", "author": ["J. Hoffmann", "B. Nebel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hoffmann and Nebel,? \\Q2001\\E", "shortCiteRegEx": "Hoffmann and Nebel", "year": 2001}, {"title": "Ordered landmarks in planning", "author": ["J. Hoffmann", "J. Porteous", "L. Sebastia"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hoffmann et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2004}, {"title": "Subgoal ordering and goal augmentation for heuristic problem solving", "author": ["K.B. Irani", "J. Cheng"], "venue": "Proceedings of the 10th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Irani and Cheng,? \\Q1987\\E", "shortCiteRegEx": "Irani and Cheng", "year": 1987}, {"title": "State-variable planning under structural restrictions: Algorithms and complexity", "author": ["P. Jonsson", "C. B\u00e4ckstr\u00f6m"], "venue": "Artificial Intelligence,", "citeRegEx": "Jonsson and B\u00e4ckstr\u00f6m,? \\Q1998\\E", "shortCiteRegEx": "Jonsson and B\u00e4ckstr\u00f6m", "year": 1998}, {"title": "Cost-optimal planning with landmarks", "author": ["E. Karpas", "C. Domshlak"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Karpas and Domshlak,? \\Q2009\\E", "shortCiteRegEx": "Karpas and Domshlak", "year": 2009}, {"title": "Heuristics for planning with action costs revisited", "author": ["E. Keyder", "H. Geffner"], "venue": "In Proceedings of the 18th European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Keyder and Geffner,? \\Q2008\\E", "shortCiteRegEx": "Keyder and Geffner", "year": 2008}, {"title": "Trees of shortest paths vs. Steiner trees: Understanding and improving delete relaxation heuristics", "author": ["E. Keyder", "H. Geffner"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Keyder and Geffner,? \\Q2009\\E", "shortCiteRegEx": "Keyder and Geffner", "year": 2009}, {"title": "Sound and complete landmarks for and/or graphs", "author": ["E. Keyder", "S. Richter", "M. Helmert"], "venue": "Proceedings of the 19th European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Keyder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keyder et al\\.", "year": 2010}, {"title": "On reasonable and forced goal orderings and their use in an agenda-driven planning algorithm", "author": ["J. Koehler", "J. Hoffmann"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Koehler and Hoffmann,? \\Q2000\\E", "shortCiteRegEx": "Koehler and Hoffmann", "year": 2000}, {"title": "Anytime search in dynamic graphs", "author": ["M. Likhachev", "D. Ferguson", "G.J. Gordon", "A. Stentz", "S. Thrun"], "venue": "Artificial Intelligence,", "citeRegEx": "Likhachev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Likhachev et al\\.", "year": 2008}, {"title": "ARA*: Anytime A* with provable bounds on sub-optimality", "author": ["M. Likhachev", "G.J. Gordon", "S. Thrun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Likhachev et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Likhachev et al\\.", "year": 2004}, {"title": "Inference and decomposition in planning using causal consistent chains", "author": ["N. Lipovetzky", "H. Geffner"], "venue": "Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Lipovetzky and Geffner,? \\Q2009\\E", "shortCiteRegEx": "Lipovetzky and Geffner", "year": 2009}, {"title": "Speeding up the calculation of heuristics for heuristic search-based planning", "author": ["Y. Liu", "S. Koenig", "D. Furcy"], "venue": "In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI", "citeRegEx": "Liu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2002}, {"title": "Heuristic search viewed as path finding in a graph", "author": ["I. Pohl"], "venue": "Artificial Intelligence,", "citeRegEx": "Pohl,? \\Q1970\\E", "shortCiteRegEx": "Pohl", "year": 1970}, {"title": "Extending landmarks analysis to reason about resources and repetition", "author": ["J. Porteous", "S. Cresswell"], "venue": "In Proceedings of the 21st Workshop of the UK Planning and Scheduling Special Interest Group (PLANSIG", "citeRegEx": "Porteous and Cresswell,? \\Q2002\\E", "shortCiteRegEx": "Porteous and Cresswell", "year": 2002}, {"title": "On the extraction, ordering, and usage of landmarks in planning", "author": ["J. Porteous", "L. Sebastia", "J. Hoffmann"], "venue": "Pre-proceedings of the Sixth European Conference on Planning (ECP", "citeRegEx": "Porteous et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Porteous et al\\.", "year": 2001}, {"title": "Preferred operators and deferred evaluation in satisficing planning", "author": ["S. Richter", "M. Helmert"], "venue": "Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Richter and Helmert,? \\Q2009\\E", "shortCiteRegEx": "Richter and Helmert", "year": 2009}, {"title": "Landmarks revisited", "author": ["S. Richter", "M. Helmert", "M. Westphal"], "venue": "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Richter et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Richter et al\\.", "year": 2008}, {"title": "The joy of forgetting: Faster anytime search via restarting", "author": ["S. Richter", "J.T. Thayer", "W. Ruml"], "venue": "Proceedings of the Twentieth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Richter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Richter et al\\.", "year": 2010}, {"title": "The more, the merrier: Combining heuristic estimators for satisficing planning", "author": ["G. R\u00f6ger", "M. Helmert"], "venue": "Proceedings of the Twentieth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "R\u00f6ger and Helmert,? \\Q2010\\E", "shortCiteRegEx": "R\u00f6ger and Helmert", "year": 2010}, {"title": "Best-first utility-guided search", "author": ["W. Ruml", "M.B. Do"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Ruml and Do,? \\Q2007\\E", "shortCiteRegEx": "Ruml and Do", "year": 2007}, {"title": "Decomposition of planning problems", "author": ["L. Sebastia", "E. Onaindia", "E. Marzal"], "venue": "AI Communications,", "citeRegEx": "Sebastia et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sebastia et al\\.", "year": 2006}, {"title": "A lookahead strategy for heuristic search planning", "author": ["V. Vidal"], "venue": "Proceedings of the Fourteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Vidal,? \\Q2004\\E", "shortCiteRegEx": "Vidal", "year": 2004}, {"title": "Beam-stack search: Integrating backtracking with beam search", "author": ["R. Zhou", "E.A. Hansen"], "venue": "Proceedings of the Fifteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Zhou and Hansen,? \\Q2005\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2005}, {"title": "Landmark extraction via planning graph propagation", "author": ["L. Zhu", "R. Givan"], "venue": "ICAPS", "citeRegEx": "Zhu and Givan,? \\Q2003\\E", "shortCiteRegEx": "Zhu and Givan", "year": 2003}], "referenceMentions": [{"referenceID": 19, "context": "Ever since, heuristic-search approaches have played a prominent role in the classical or sequential satisficing tracks of the biennial competition, with Fast Downward (Helmert, 2006) winning in 2004 and SGPlan (Chen, Wah, & Hsu, 2006) placing first in 2006.", "startOffset": 167, "endOffset": 182}, {"referenceID": 3, "context": "Starting with the additive heuristic by Bonet and Geffner (2001), implemented in the HSP planning system, much research has been conducted in search of heuristic estimators that are efficient to calculate yet powerful in guiding the search towards a goal state.", "startOffset": 40, "endOffset": 65}, {"referenceID": 3, "context": "Starting with the additive heuristic by Bonet and Geffner (2001), implemented in the HSP planning system, much research has been conducted in search of heuristic estimators that are efficient to calculate yet powerful in guiding the search towards a goal state. The FF planning system by Hoffmann and Nebel (2001), using a heuristic estimator based on relaxed planning graphs, broke ground by showing best performance among all fully automated systems at the International Planning Competition in 2000, and continues to be state of the art today.", "startOffset": 40, "endOffset": 314}, {"referenceID": 39, "context": "Both the landmark heuristic we proposed earlier (Richter et al., 2008) and the FF heuristic have been adapted to use action costs.", "startOffset": 48, "endOffset": 70}, {"referenceID": 39, "context": "This result was not expected by its authors, as their previous work concerning LAMA\u2019s putative core feature, the landmark heuristic (Richter et al., 2008), showed some, but not tremendous improvement over the base configuration without landmarks.", "startOffset": 132, "endOffset": 154}, {"referenceID": 19, "context": "Some aspects of LAMA have been presented in previous publications (Richter et al., 2008, 2010; Helmert, 2006).", "startOffset": 66, "endOffset": 109}, {"referenceID": 40, "context": "We do not repeat comparisons published in earlier work, like the comparison between LAMA\u2019s anytime method and other anytime algorithms (Richter et al., 2010), or the comparison of LAMA\u2019s methods for handling landmarks to alternative landmark approaches (Richter et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 39, "context": ", 2010), or the comparison of LAMA\u2019s methods for handling landmarks to alternative landmark approaches (Richter et al., 2008).", "startOffset": 103, "endOffset": 125}, {"referenceID": 19, "context": "While LAMA also handles axioms in the same way as Fast Downward (Helmert, 2006), we do not formalise axioms here, since they are not important for our purposes.", "startOffset": 64, "endOffset": 79}, {"referenceID": 19, "context": "We use a planning formalism with state variables of finite (rather than binary) range, similar to the one employed by Helmert (2009). It is based on the SAS planning model (B\u00e4ckstr\u00f6m & Nebel, 1995), but extends it with conditional effects.", "startOffset": 118, "endOffset": 133}, {"referenceID": 19, "context": "Each state variable v of a planning task in finite-domain representation has an associated directed graph called the domain transition graph, which captures the ways in which the value of v may change (Jonsson & B\u00e4ckstr\u00f6m, 1998; Helmert, 2006).", "startOffset": 201, "endOffset": 243}, {"referenceID": 19, "context": "LAMA builds on the Fast Downward system (Helmert, 2006), inheriting the overall structure and large parts of the functionality from that planner.", "startOffset": 40, "endOffset": 55}, {"referenceID": 19, "context": "Details on this component can be found in a recent article by Helmert (2009).", "startOffset": 62, "endOffset": 77}, {"referenceID": 19, "context": "We refer to Helmert (2006) for more detail on the knowledge compilation component, which LAMA inherits unchanged from Fast Downward.", "startOffset": 12, "endOffset": 27}, {"referenceID": 35, "context": ") Weighted A\u2217 search (Pohl, 1970) associates costs with states and expands a state with minimal f \u2032-value, where f \u2032 = w \u00b7 h + g, the weight w is an integer \u2265 1, and g is the best known cost of reaching the considered state from the", "startOffset": 21, "endOffset": 33}, {"referenceID": 19, "context": "In addition, both search algorithms use three types of search enhancements inherited from Fast Downward (Helmert, 2006; Richter & Helmert, 2009).", "startOffset": 104, "endOffset": 144}, {"referenceID": 19, "context": "The two heuristics are used with separate queues, thus exploiting strengths of the utilised heuristics in an orthogonal way (Helmert, 2006; R\u00f6ger & Helmert, 2010).", "startOffset": 124, "endOffset": 162}, {"referenceID": 44, "context": "Alternative methods for using preferred operators include the one employed in the YAHSP system (Vidal, 2004), where preferred operators are always used over non-preferred ones.", "startOffset": 95, "endOffset": 108}, {"referenceID": 40, "context": "While resulting in some duplicate effort, these restarts can help overcome bad decisions made by the early (comparatively greedy) search iterations with high weight (Richter et al., 2010).", "startOffset": 165, "endOffset": 187}, {"referenceID": 40, "context": "In an experimental comparison on all tasks from IPC 1998 to IPC 2006 (Richter et al., 2010) this restarting approach performed notably better than all other tested methods, dominating similar algorithms based on weighted A\u2217 (Hansen, Zilberstein, & Danilchenko, 1997; Hansen & Zhou, 2007; Likhachev, Gordon, & Thrun, 2004; Likhachev et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 31, "context": ", 2010) this restarting approach performed notably better than all other tested methods, dominating similar algorithms based on weighted A\u2217 (Hansen, Zilberstein, & Danilchenko, 1997; Hansen & Zhou, 2007; Likhachev, Gordon, & Thrun, 2004; Likhachev et al., 2008), as well as other anytime approaches (Zhou & Hansen, 2005; Aine, Chakrabarti, & Kumar, 2007).", "startOffset": 140, "endOffset": 261}, {"referenceID": 23, "context": "This knowledge, as well as the knowledge that L1 must become true before L2, can be automatically extracted from the task in a preprocessing step (Hoffmann et al., 2004).", "startOffset": 146, "endOffset": 169}, {"referenceID": 19, "context": "In addition, preferred operators (Helmert, 2006) are used to suggest operators that achieve those landmarks that need to become true next.", "startOffset": 33, "endOffset": 48}, {"referenceID": 39, "context": ", both in terms of coverage and in terms of plan quality (Richter et al., 2008).", "startOffset": 57, "endOffset": 79}, {"referenceID": 39, "context": "While the use of disjunctive landmarks has been shown to improve performance, compared to using only fact landmarks (Richter et al., 2008), more complex landmarks introduce additional difficulty both with regard to their detection and their handling during planning.", "startOffset": 116, "endOffset": 138}, {"referenceID": 19, "context": "(See Keyder, Richter and Helmert, 2010, for a discussion of conjunctive landmarks). Various kinds of orderings between landmarks can be defined and exploited during the planning phase. We define three types of orderings for landmarks, which are equivalent formulations of the definitions by Hoffmann et al. (2004) adapted to the FDR setting:", "startOffset": 25, "endOffset": 314}, {"referenceID": 23, "context": "However, methods that find landmarks in conjunction with orderings can often find many more landmarks when using the more general concept of greedy-necessary orderings (Hoffmann et al., 2004).", "startOffset": 168, "endOffset": 191}, {"referenceID": 23, "context": "However, methods that find landmarks in conjunction with orderings can often find many more landmarks when using the more general concept of greedy-necessary orderings (Hoffmann et al., 2004). LAMA follows this paradigm and finds greedy-necessary (as well as natural) orderings, but not necessary orderings. In our example in Figure 3, \u201cbox is in truck1\u201d must be true before \u201cbox is at C\u201d and also before \u201cbox is at F\u201d. The first of these orderings is greedy-necessary, but not necessary, and the second is neither greedy-necessary nor necessary, but natural. Hoffmann et al. (2004) propose further kinds of orderings between landmarks that can be usefully exploited.", "startOffset": 169, "endOffset": 583}, {"referenceID": 23, "context": "\u2192r \u03c6 for landmarks \u03c6 and \u03c8 may exist (Hoffmann et al., 2004).", "startOffset": 37, "endOffset": 60}, {"referenceID": 23, "context": "Our definitions are equivalent to those of Hoffmann et al. (2004), except that we care only about plans rather than arbitrary operator sequences, allowing us to (theoretically) identify more reasonable orderings.", "startOffset": 43, "endOffset": 66}, {"referenceID": 37, "context": "Several polynomial methods have been proposed for finding fact landmarks and disjunctive landmarks, such as back-chaining from the goals of the task, using criteria based on the relaxed planning graph (Porteous et al., 2001; Hoffmann et al., 2004; Porteous & Cresswell, 2002), and forward propagation in the planning graph (Zhu & Givan, 2003).", "startOffset": 201, "endOffset": 275}, {"referenceID": 23, "context": "Several polynomial methods have been proposed for finding fact landmarks and disjunctive landmarks, such as back-chaining from the goals of the task, using criteria based on the relaxed planning graph (Porteous et al., 2001; Hoffmann et al., 2004; Porteous & Cresswell, 2002), and forward propagation in the planning graph (Zhu & Givan, 2003).", "startOffset": 201, "endOffset": 275}, {"referenceID": 37, "context": "As in previous work (Porteous et al., 2001; Hoffmann et al., 2004), we subsequently use the discovered landmarks and orderings to derive reasonable and obedient-reasonable orderings in a postprocessing step.", "startOffset": 20, "endOffset": 66}, {"referenceID": 23, "context": "As in previous work (Porteous et al., 2001; Hoffmann et al., 2004), we subsequently use the discovered landmarks and orderings to derive reasonable and obedient-reasonable orderings in a postprocessing step.", "startOffset": 20, "endOffset": 66}, {"referenceID": 23, "context": "Since the number of such disjunctive landmarks is exponential in the number of achievers of \u03c8, we restrict ourselves to disjunctions where all facts stem from the same predicate symbol, which are deemed most helpful (Hoffmann et al., 2004).", "startOffset": 216, "endOffset": 239}, {"referenceID": 23, "context": "Since it is PSPACE-hard to determine the set of first achievers of a landmark \u03c8 (Hoffmann et al., 2004), we use an over-approximation containing every operator that can possibly be a first achiever (Porteous & Cresswell, 2002).", "startOffset": 80, "endOffset": 103}, {"referenceID": 23, "context": "We approximate both (a) and (b) as proposed by Hoffmann et al. (2004) with sufficient conditions.", "startOffset": 47, "endOffset": 70}, {"referenceID": 24, "context": "In particular, the approach by Irani and Cheng (Irani & Cheng, 1987; Cheng & Irani, 1989) is a preprocessing procedure like ours that analyses the planning task to extract necessary orderings between goals, which are then imposed on the search algorithm. A goal A is ordered before a goal B in this approach if in any plan A is necessarily true before B. Koehler and Hoffmann (2000) introduce reasonable orderings for goals.", "startOffset": 31, "endOffset": 383}, {"referenceID": 23, "context": "To test whether a landmark candidate L found via back-chaining is indeed a landmark, Hoffmann et al. (2004) build a restricted relaxed planning task leaving out all operators which could add L.", "startOffset": 85, "endOffset": 108}, {"referenceID": 23, "context": "The approach by Hoffmann et al. (2004) exploits landmarks by decomposing the planning task into smaller subtasks, making the landmarks intermediary goals.", "startOffset": 16, "endOffset": 39}, {"referenceID": 39, "context": "However, we found that their method leads to worse average performance on the IPC benchmarks from 1998 to 2006 when using Fast Downward as a base planner (Richter et al., 2008).", "startOffset": 154, "endOffset": 176}, {"referenceID": 39, "context": "As we have recently shown, this avoids the possibility of dead-ends and usually generates better-quality solutions (Richter et al., 2008).", "startOffset": 115, "endOffset": 137}, {"referenceID": 23, "context": "As a base planner for solving the subtasks any planner can be used; Hoffmann et al. (2004) experimented with FF.", "startOffset": 68, "endOffset": 91}, {"referenceID": 23, "context": "(2006) extend the work by Hoffmann et al. by employing a refined preprocessing technique that groups landmarks into consistent sets, minimising the destructive interactions between the sets. Taking these sets as intermediary goals, they avoid the increased plan length experienced by Hoffmann et al. (2004). However, according to the authors this preprocessing is computationally expensive and may take longer than solving the original problem.", "startOffset": 26, "endOffset": 307}, {"referenceID": 16, "context": "Gregory et al. (2004) build on their work to find disjunctive landmarks through symmetry breaking.", "startOffset": 0, "endOffset": 22}, {"referenceID": 46, "context": "Similar to our work, Zhu and Givan (2003) use the causal fact landmarks and action landmarks to estimate the goal distance of a given state.", "startOffset": 21, "endOffset": 42}, {"referenceID": 26, "context": "As Karpas and Domshlak (2009) note, we can calculate the landmarks that are accepted in s given a set of paths P to s as Accepted(s,P) B \u22c2 \u03c0\u2208P Accepted(s, \u03c0), since it holds that any landmark that is not achieved along all paths \u03c0 \u2208 Pmust", "startOffset": 3, "endOffset": 30}, {"referenceID": 39, "context": "When we first introduced the landmark heuristic (Richter et al., 2008), it proved not to be competitive on its own, compared to established heuristics like the FF heuristic (Hoffmann & Nebel, 2001).", "startOffset": 48, "endOffset": 70}, {"referenceID": 13, "context": "Using other criteria for cost propagation results in variations of the FF heuristic (Bryce & Kambhampati, 2007; Fuentetaja et al., 2009).", "startOffset": 84, "endOffset": 136}, {"referenceID": 38, "context": "When we first introduced the landmark heuristic (Richter et al., 2008), it proved not to be competitive on its own, compared to established heuristics like the FF heuristic (Hoffmann & Nebel, 2001). However, the joint use of the FF heuristic and the landmark heuristic in a multi-heuristic search improved the performance of a planning system, compared with only using the FF heuristic. This is thus the path LAMA follows. The FF heuristic is based on a relaxation of the planning task that ignores delete effects, which in FDR tasks translates to allowing state variables to hold several values simultaneously. The FF heuristic for a state s is computed in two phases: the first phase, or forward phase, calculates an estimate for each fact in the planning task of how costly it is to achieve the fact from s in a relaxed task. Concurrently, it selects an operator called best support for each fact F, which is a greedy approximation of a cheapest achiever (an achiever a of F where the costs of making a applicable and applying it are minimal among all achievers of F, when starting in s). In the second phase, a plan for the relaxed task is constructed based on the best supports for each fact. This is done by chaining backwards from the goals, selecting the best supports of the goals, and then recursively selecting the best supports for the preconditions of already selected operators. The union of these best supports then constitutes the relaxed plan (i. e., for each fact its best support is only added to the relaxed plan once, even if the fact is needed several times as a precondition). The length of the resulting relaxed plan is the heuristic estimate reported for s. The forward phase can be viewed as propagating cost information for operators and facts in a relaxed planning graph (Hoffmann & Nebel, 2001). However, this graph does not need to be explicitly constructed to compute the heuristic. Instead, a form of generalised Dijkstra cheapestpath algorithm as described by Liu, Koenig and Furcy (2002) is used in LAMA, which propagates costs from preconditions to applicable operators and from operators to effects.", "startOffset": 49, "endOffset": 2022}, {"referenceID": 27, "context": "Independently of us, Keyder and Geffner (2008) implemented the FF/add heuristic which they call ha in their planner FF(ha) at IPC 2008.", "startOffset": 21, "endOffset": 47}, {"referenceID": 39, "context": "The effect of landmarks in classical planning tasks without actions costs has been studied in previous work (Richter et al., 2008), but we provide summarising results for this case, using the domains of IPCs 1998\u20132006, in Section 7.", "startOffset": 108, "endOffset": 130}, {"referenceID": 31, "context": "and Geffner (2008) and the C3 planner by Lipovetzky and Geffner (2009). The plans found by these planners have been obtained from the competition website (Helmert et al.", "startOffset": 41, "endOffset": 71}, {"referenceID": 1, "context": "This is in contrast to the Miconic domain, used in an earlier international planning competition (Bacchus, 2001), which also models the transporting of passengers via elevators, but where there is only one elevator that can access all floors with just one (unit-cost) operator.", "startOffset": 97, "endOffset": 112}, {"referenceID": 27, "context": "This partly explains why the FF(ha) planner by Keyder and Geffner (2008) has a substantially higher coverage than our Fc configuration in this domain.", "startOffset": 47, "endOffset": 73}, {"referenceID": 27, "context": "Keyder and Geffner (2009) have recently proposed such an improvement of the FF/add heuristic4 and shown it to be particularly useful in the Elevators and PARC Printer domains.", "startOffset": 0, "endOffset": 26}, {"referenceID": 14, "context": "This domain was previously used at IPC 2006 (Gerevini et al., 2009).", "startOffset": 44, "endOffset": 67}, {"referenceID": 39, "context": "Note that the F and LAMA configurations roughly correspond to the results published as \u201cbase\u201d and \u201cheur\u201d in earlier work (Richter et al., 2008).", "startOffset": 121, "endOffset": 143}, {"referenceID": 19, "context": "In particular Fast Downward has evolved substantially since its 2004 competition version, the original causal graph heuristic having been replaced with the better context-enhanced additive heuristic (Helmert & Geffner, 2008). After correspondence with the authors, the version of Fast Downward used here is the one featuring in recent work by Richter and Helmert (2009). As Table 9 shows, LAMA performs better than both FF and Fast Downward in terms of the IPC 2008 criterion.", "startOffset": 200, "endOffset": 370}, {"referenceID": 27, "context": "Keyder and Geffner (2009) propose a method for extracting better relaxed plans from the best supports computed by the costsensitive FF/add heuristic, resulting in improved coverage.", "startOffset": 0, "endOffset": 26}], "year": 2010, "abstractText": "LAMA is a classical planning system based on heuristic forward search. Its core feature is the use of a pseudo-heuristic derived from landmarks, propositional formulas that must be true in every solution of a planning task. LAMA builds on the Fast Downward planning system, using finite-domain rather than binary state variables and multi-heuristic search. The latter is employed to combine the landmark heuristic with a variant of the well-known FF heuristic. Both heuristics are cost-sensitive, focusing on high-quality solutions in the case where actions have non-uniform cost. A weighted A\u2217 search is used with iteratively decreasing weights, so that the planner continues to search for plans of better quality until the search is terminated. LAMA showed best performance among all planners in the sequential satisficing track of the International Planning Competition 2008. In this paper we present the system in detail and investigate which features of LAMA are crucial for its performance. We present individual results for some of the domains used at the competition, demonstrating good and bad cases for the techniques implemented in LAMA. Overall, we find that using landmarks improves performance, whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial. We show that in some domains a search that ignores cost solves far more problems, raising the question of how to deal with action costs more effectively in the future. The iterated weighted A\u2217 search greatly improves results, and shows synergy effects with the use of landmarks.", "creator": "TeX"}}}