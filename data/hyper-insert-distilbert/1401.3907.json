{"id": "1401.3907", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Policy Invariance under Reward Transformations for General-Sum Stochastic Games", "abstract": "we easily extend the descriptive potential - utility based shaping method valid from markov decision processes to multi - goal player general - hypothesis sum stochastic probability games. we prove positively that the nash equilibria in scoring a stochastic marko game remains permanently unchanged after potential - based equilibrium shaping calculation is securely applied to the environment. reading the property of policy theory invariance provides a potential possible way of speeding smooth convergence dynamics when making learning scenarios to play a realistic stochastic gambling game.", "histories": [["v1", "Thu, 16 Jan 2014 05:22:56 GMT  (89kb)", "http://arxiv.org/abs/1401.3907v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["xiaosong lu", "howard m schwartz", "sidney n givigi jr"], "accepted": false, "id": "1401.3907"}, "pdf": {"name": "1401.3907.pdf", "metadata": {"source": "CRF", "title": "Policy Invariance under Reward Transformations for General-Sum Stochastic Games", "authors": ["Xiaosong Lu", "Howard M. Schwartz"], "emails": ["LUXIAOS@SCE.CARLETON.CA", "SCHWARTZ@SCE.CARLETON.CA", "SIDNEY.GIVIGI@RMC.CA"], "sections": [{"heading": "1. Introduction", "text": "In reinforcement learning, one may suffer from the temporal credit assignment problem (Sutton & Barto, 1998) where a reward is received after a sequence of actions. The delayed reward will lead to difficulty in distributing credit or punishment to each action from a long sequence of actions and this will cause the algorithm to learn slowly. An example of this problem can be found in some episodic tasks such as a soccer game where the player is only given credit or punishment after a goal is scored. If the number of states in the soccer game is large, it will take a long time for a player to learn its equilibrium policy.\nReward shaping is a technique to improve the learning performance of a reinforcement learner by introducing shaping rewards to the environment (Gullapalli & Barto, 1992; Mataric, 1994). When the state space is large, the delayed reward will slow down the learning dramatically. To speed up the learning, the learner may apply shaping rewards to the environment as a supplement to the delayed reward. In this way, a reinforcement learning algorithm can improve its learning performance by combining a \"good\" shaping reward function with the original delayed reward.\nThe applications of reward shaping can be found in the literature (Gullapalli & Barto, 1992; Dorigo & Colombetti, 1994; Mataric, 1994; Randl\u00f8v & Alstr\u00f8m, 1998). Gullapalli and Barto (1992) demonstrated the application of shaping to a key-press task where a robot was trained to press keys on a keyboard. Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a predefined animate-like behavior. Mataric (1994) presented an intermediate reinforcement function for a group of mobile robots to learn a foraging task. Randl\u00f8v and Alstr\u00f8m (1998) combined reinforcement learning with shaping to make an agent learn to drive a bicycle to a goal. The theoretical\nc\u00a92011 AI Access Foundation. All rights reserved.\nanalysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora, 2003; Asmuth, Littman, & Zinkov, 2008). Ng et al. (1999) presented a potential-based shaping reward that can guarantee the policy invariance for a single agent in a Markov decision process (MDP). Ng et al. proved that the optimal policy keeps unchanged after adding the potential-based shaping reward to an MDP environment. Following Ng et al., Wiewiora (2003) showed that the effects of potential-based shaping can be achieved by a particular initialization of Q-values for agents using Q-learning. Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach.\nThe above articles focus on applications of reward shaping to a single agent in an MDP. For the applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008) introduced a social shaping reward for players to learn their equilibrium policies in the iterated prisoner\u2019s dilemma game. But there is no theoretical proof of policy invariance under the reward transformation. In our research, we prove that the Nash equilibria under the potential-based shaping reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under the framework of general-sum stochastic games. Note that the similar work of Devlin and Kudenko (2011) was published while this article was under review. But Devlin and Kudenko only proved sufficiency based on a proof technique introduced by Asmuth et al. (2008), while we prove both sufficiency and necessity using a different proof technique in this article."}, {"heading": "2. Framework of Stochastic Games", "text": "Stochastic games were first introduced by Shapley (1953). In a stochastic game, players choose the joint action and move from one state to another state based on the joint action they choose. In this section, under the framework of stochastic games, we introduce Markov decision processes, matrix games and stochastic games respectively."}, {"heading": "2.1 Markov Decision Processes", "text": "A Markov decision process is a tuple (S,A,T,\u03b3 ,R) where S is the state space, A is the action space, T : S\u00d7A\u00d7S \u2192 [0,1] is the transition function, \u03b3 \u2208 [0,1] is the discount factor and R : S\u00d7A\u00d7S \u2192R is the reward function. The transition function denotes a probability distribution over next states given the current state and action. The reward function denotes the received reward at the next state given the current action and the current state. A Markov decision process has the following Markov property: the player\u2019s next state and reward only depend on the player\u2019s current state and action. A player\u2019s policy \u03c0 : S \u2192 A is defined as a probability distribution over the player\u2019s actions given a state. An optimal policy \u03c0\u2217 will maximize the player\u2019s discounted future reward. For any MDP, there exists a deterministic optimal policy for the player (Bertsekas, 1987).\nStarting in the current state s and following the optimal policy thereafter, we can get the optimal state-value function as the expected sum of discounted rewards (Sutton & Barto, 1998)\nV \u03c0 \u2217 (s) = E\n{\nT\n\u2211 j=0\n\u03b3 jrk+ j+1|sk = s,\u03c0\u2217 }\n(1)\nwhere k is the current time step, rk+ j+1 is the received immediate reward at the time step k+ j+1, \u03b3 \u2208 [0,1] is a discount factor, and T is a final time step. In (1), we have T \u2192 \u221e if the task is an infinite-horizon task such that the task will run over infinite period. If the task is episodic, T is\ndefined as the terminal time when each episode is terminated at the time step T . Then we call the state where each episode ends as the terminal state sT . In a terminal state, the state-value function is always zero such that V (sT ) = 0 for all sT \u2208 S. Given the current state s and action a, and following the optimal policy thereafter, we can define an optimal action-value function (Sutton & Barto, 1998)\nQ\u03c0 \u2217 (s,a) = \u2211\ns\u2032\u2208S\nT (s,a,s\u2032) [ R(s,a,s\u2032)+ \u03b3V \u03c0 \u2217 (s\u2032) ]\n(2)\nwhere T (s,a,s\u2032) = Pr{sk+1 = s\u2032|sk = s,ak = a} is the probability of the next state being sk+1 = s\u2032 given the current state sk = s and action ak = a at time step k, and R(s,a,s\u2032) = E{rk+1|sk = s,ak = a, sk+1 = s\u2032} is the expected immediate reward received at state s\u2032 given the current state s and action a. In a terminal state, the action-value function is always zero such that Q(sT ,a) = 0 for all sT \u2208 S."}, {"heading": "2.2 Matrix Games", "text": "A matrix game is a tuple (n,A1, . . . ,An,R1, . . . ,Rn) where n is the number of players, Ai(i = 1, . . . ,n) is the action set for the player i and Ri : A1 \u00d7\u00b7\u00b7 \u00b7\u00d7An \u2192 R is the payoff function for the player i. A matrix game is a game involving multiple players and a single state. Each player i(i = 1, . . . ,n) selects an action from its action set Ai and receives a payoff. The player i\u2019s payoff function Ri is determined by all players\u2019 joint action from joint action space A1\u00d7\u00b7\u00b7 \u00b7\u00d7An. For a two-player matrix game, we can set up a matrix with each element containing a payoff for each joint action pair. Then the payoff function Ri for player i(i = 1,2) becomes a matrix. If the two players in the game are fully competitive, we will have a two-player zero-sum matrix game with R1 =\u2212R2.\nIn a matrix game, each player tries to maximize its own payoff based on the player\u2019s strategy. A player\u2019s strategy in a matrix game is a probability distribution over the player\u2019s action set. To evaluate a player\u2019s strategy, we introduce the following concept of Nash equilibrium. A Nash equilibrium in a matrix game is a collection of all players\u2019 policies (\u03c0\u22171 , \u00b7 \u00b7 \u00b7 ,\u03c0\u2217n ) such that\nVi(\u03c0\u22171 , \u00b7 \u00b7 \u00b7 ,\u03c0\u2217i , \u00b7 \u00b7 \u00b7 ,\u03c0\u2217n ) \u2265 Vi(\u03c0\u22171 , \u00b7 \u00b7 \u00b7 ,\u03c0i, \u00b7 \u00b7 \u00b7 ,\u03c0\u2217n ), \u2200\u03c0i \u2208 \u03a0i, i = 1, \u00b7 \u00b7 \u00b7 ,n (3)\nwhere Vi(\u00b7) is the expected payoff for player i given all players\u2019 current strategies and \u03c0i is any strategy of player i from the strategy space \u03a0i. In other words, a Nash equilibrium is a collection of strategies for all players such that no player can do better by changing its own strategy given that other players continue playing their Nash equilibrium policies (Bas\u0327ar & Olsder, 1999). We define Qi(a1, . . . ,an) as the received payoff of the player i given players\u2019 joint action a1, . . . ,an, and \u03c0i(ai) (i = 1, . . . ,n) as the probability of player i choosing action a1. Then the Nash equilibrium defined in (3) becomes\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Qi(a1, . . . ,an)\u03c0\u22171 (a1) \u00b7 \u00b7 \u00b7\u03c0\u2217i (ai) \u00b7 \u00b7 \u00b7\u03c0\u2217n (an)\u2265\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Qi(a1, . . . ,an)\u03c0\u22171 (a1) \u00b7 \u00b7 \u00b7\u03c0i(ai) \u00b7 \u00b7 \u00b7\u03c0\u2217n (an), \u2200\u03c0i \u2208 \u03a0i, i = 1, \u00b7 \u00b7 \u00b7 ,n (4)\nwhere \u03c0\u2217i (ai) is the probability of player i choosing action ai under the player i\u2019s Nash equilibrium strategy \u03c0\u2217i .\nA two-player matrix game is called a zero-sum game if the two players are fully competitive. In this way, we have R1 = \u2212R2. A zero-sum game has a unique Nash equilibrium in the sense of the expected payoff. It means that, although each player may have multiple Nash equilibrium\nstrategies in a zero-sum game, the value of the expected payoff Vi under these Nash equilibrium strategies will be the same. If the players in the game are not fully competitive or the summation of the players\u2019 payoffs is not zero, the game is called a general-sum game. In a general-sum game, the Nash equilibrium is no longer unique and the game might have multiple Nash equilibria. Unlike the deterministic optimal policy for a single player in an MDP, the equilibrium strategies in a multiplayer matrix game may be stochastic."}, {"heading": "2.3 Stochastic Games", "text": "A Markov decision process contains a single player and multiple states while a matrix game contains multiple players and a single state. For a game with more than one player and multiple states, we define a stochastic game (or Markov game) as the combination of Markov decision processes and matrix games. A stochastic game is a tuple (n,S,A1, . . . ,An,T,\u03b3 ,R1, . . . ,Rn) where n is the number of the players, T : S\u00d7A1 \u00d7\u00b7\u00b7 \u00b7\u00d7An \u00d7 S \u2192 [0,1] is the transition function, Ai(i = 1, . . . ,n) is the action set for the player i, \u03b3 \u2208 [0,1] is the discount factor and Ri : S\u00d7A1 \u00d7\u00b7\u00b7 \u00b7\u00d7An \u00d7 S \u2192 R is the reward function for player i. The transition function in a stochastic game is a probability distribution over next states given the current state and joint action of the players. The reward function Ri(s,a1, . . . ,an,s\u2032) denotes the reward received by player i in state s\u2032 after taking joint action (a1, . . . ,an) in state s. Similar to Markov decision processes, stochastic games also have the Markov property. That is, the player\u2019s next state and reward only depend on the current state and all the players\u2019 current actions.\nTo solve a stochastic game, we need to find a policy \u03c0i : S \u2192 Ai that can maximize player i\u2019s discounted future reward with a discount factor \u03b3 . Similar to matrix games, the player\u2019s policy in a stochastic game is probabilistic. An example is the soccer game introduced by Littman (Littman, 1994) where an agent on the offensive side must use a probabilistic policy to pass an unknown defender. In the literature, a solution to a stochastic game can be described as Nash equilibrium strategies in a set of associated state-specific matrix games (Bowling, 2003; Littman, 1994). In these state-specific matrix games, we define the action-value function Q\u2217i (s,a1, . . . ,an) as the expected reward for player i when all the players take joint action a1, . . . ,an in state s and follow the Nash equilibrium policies thereafter. If the value of Q\u2217i (s,a1, . . . ,an) is known for all the states, we can find player i\u2019s Nash equilibrium policy by solving the associated state-specific matrix game (Bowling, 2003). Therefore, for each state s, we have a matrix game and we can find the Nash equilibrium strategies in this matrix game. Then the Nash equilibrium policies for the game are the collection of Nash equilibrium strategies in each state-specific matrix game for all the states."}, {"heading": "2.4 Multi-Player General-Sum Stochastic Games", "text": "For a multi-player general-sum stochastic game, we want to find the Nash equilibria in the game if we know the reward function and transition function in the game. A Nash equilibrium in a stochastic game can be described as a tuple of n policies (\u03c0\u22171 , . . . ,\u03c0\u2217n ) such that for all s \u2208 S and i = 1, \u00b7 \u00b7 \u00b7 ,n,\nVi(s,\u03c0\u22171 , . . . ,\u03c0\u2217i , . . . ,\u03c0\u2217n )\u2265Vi(s,\u03c0\u22171 , . . . ,\u03c0i, . . . ,\u03c0\u2217n ) for all \u03c0i \u2208 \u03a0i (5)\nwhere \u03a0i is the set of policies available to player i and Vi(s,\u03c0\u22171 , . . . ,\u03c0\u2217n ) is the expected sum of discounted rewards for player i given the current state and all the players\u2019 equilibrium policies. To simplify notation, we use V \u2217i (s) to represent Vi(s,\u03c0\u22171 , \u00b7 \u00b7 \u00b7 ,\u03c0\u2217n ) as the state-value function under Nash equilibrium policies. We can also define the action-value function Q\u2217(s,a1, \u00b7 \u00b7 \u00b7 ,an) as the expected\nsum of discounted rewards for player i given the current state and the current joint action of all the players, and following the Nash equilibrium policies thereafter. Then we can get\nV \u2217i (s) = \u2211 a1,\u00b7\u00b7\u00b7 ,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u2217i (s,a1, \u00b7 \u00b7 \u00b7 ,an)\u03c0\u22171 (s,a1) \u00b7 \u00b7 \u00b7\u03c0\u2217n (s,an), (6)\nQ\u2217i (s,a1, . . . ,an) = \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ Ri(s,a1, . . . ,an,s \u2032)+ \u03b3V \u2217i (s\u2032) ] , (7)\nwhere \u03c0\u2217i (s,ai) \u2208 PD(Ai) is a probability distribution over action ai under player i\u2019s Nash equilibrium policy, T (s,a1, . . . ,an,s\u2032) = Pr{sk+1 = s\u2032|sk = s,a1, . . . ,an} is the probability of the next state being s\u2032 given the current state s and joint action (a1, . . . ,an), and Ri(s,a1, . . . ,an,s\u2032) is the expected immediate reward received in state s\u2032 given the current state s and joint action (a1, . . . ,an). Based on (6) and (7), the Nash equilibrium in (5) can be rewritten as\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u2217i (s,a1, . . . ,an)\u03c0\u22171 (s,a1) \u00b7 \u00b7 \u00b7\u03c0\u2217i (s,ai) \u00b7 \u00b7 \u00b7\u03c0\u2217n (s,an)\u2265\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u2217i (s,a1, . . . ,an)\u03c0\u22171 (s,a1) \u00b7 \u00b7 \u00b7\u03c0i(s,ai) \u00b7 \u00b7 \u00b7\u03c0\u2217n (s,an). (8)"}, {"heading": "3. Potential-Based Shaping in General-Sum Stochastic Games", "text": "Ng et al. (1999) presented a reward shaping method to deal with the credit assignment problem by adding a potential-based shaping reward to the environment. The combination of the shaping reward with the original reward may improve the learning performance of a reinforcement learning algorithm and speed up the convergence to the optimal policy. The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008). In our research, we extend the potential-based shaping method from Markov decision processes to multi-player stochastic games. We prove that the Nash equilibria under the potential-based shaping reward transformation will be the Nash equilibria for the original game under the framework of general-sum stochastic games.\nWe define a potential-based shaping reward Fi(s,s\u2032) for player i as\nFi(s,s \u2032) = \u03b3\u03a6i(s\u2032)\u2212\u03a6i(s), (9)\nwhere \u03a6 : S \u2192 R is a real-valued shaping function and \u03a6(sT ) = 0 for any terminal state sT . We define a multi-player stochastic game as a tuple M = (S,A1, . . . ,An,T,\u03b3 ,R1, . . . ,Rn) where S is a set of states, A1, . . . ,An are players\u2019 action sets, T is the transition function, \u03b3 is the discount factor, and Ri(s,a1, . . . ,an,s\u2032)(i = 1, . . . ,n) is the reward function for player i. After adding the shaping reward function Fi(s,s\u2032) to the reward function Ri(s,a1, . . . ,an,s\u2032), we define a transformed multi-player stochastic game as a tuple M\u2032 = (S,A1, . . . ,An,T,\u03b3 ,R\u20321, . . . ,R\u2032n) where R\u2032i(i = 1, . . . ,n) is the new reward function given by R\u2032i(s,a1, . . . ,an,s\n\u2032) = Fi(s,s\u2032)+Ri(s,a1, . . . ,an,s\u2032). Inspired by Ng et al. (1999)\u2019s proof of policy invariance in an MDP, we prove the policy invariance in a multi-player general-sum stochastic game as follows.\nTheorem 1. Given an n-player discounted stochastic game M = (S,A1, . . . ,An,T,\u03b3 ,R1, . . . ,Rn), we define a transformed n-player discounted stochastic game M\u2032 = (S,A1, . . . ,An,T,\u03b3 ,R1+F1, . . . ,Rn+ Fn) where Fi \u2208 S\u00d7S is a shaping reward function for player i. We call Fi a potential-based shaping function if Fi has the form of (9). Then, the potential-based shaping function Fi is a necessary and sufficient condition to guarantee the Nash equilibrium policy invariance such that\n\u2022 (Sufficiency) If Fi (i = 1, . . . ,n) is a potential-based shaping function, then every Nash equilibrium policy in M\u2032 will also be a Nash equilibrium policy in M (and vice versa).\n\u2022 (Necessity) If Fi (i = 1, . . . ,n) is not a potential-based shaping function, then there may exist a transition function T and reward function R such that the Nash equilibrium policy in M\u2032\nwill not be the Nash equilibrium policy in M.\nProof. (Proof of Sufficiency) Based on (8), a Nash equilibrium in the stochastic game M can be represented as a set of policies such that for all i = 1, . . . ,n,s \u2208 S and \u03c0Mi \u2208 \u03a0\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u2217Mi(s,a1, . . . ,an)\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an)\u2265\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u2217Mi(s,a1, . . . ,an)\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an). (10)\nWe subtract \u03a6i(s) on both sides of (10) and get\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u2217Mi(s,a1, . . . ,an)\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an)\u2212\u03a6i(s)\u2265\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u2217Mi(s,a1, . . . ,an)\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an)\u2212\u03a6i(s). (11)\nSince \u2211a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An \u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an) = 1, we can get\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An [Q\u2217Mi(s,a1, . . . ,an)\u2212\u03a6i(s)]\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an)\u2265\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An [Q\u2217Mi(s,a1, . . . ,an)\u2212\u03a6i(s)]\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an). (12)\nWe define Q\u0302M\u2032i (s,a1, . . . ,an) = Q \u2217 Mi(s,a1, . . . ,an)\u2212\u03a6i(s). (13)\nThen we can get\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u0302M\u2032i (s,a1, . . . ,an)\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an)\u2265\n\u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An Q\u0302M\u2032i (s,a1, . . . ,an)\u03c0 \u2217 M1(s,a1) \u00b7 \u00b7 \u00b7\u03c0Mi(s,ai) \u00b7 \u00b7 \u00b7\u03c0 \u2217 Mn(s,an). (14)\nWe now use some algebraic manipulations to rewrite the action-value function under the Nash equilibrium in (7) for player i in the stochastic game M as\nQ\u2217Mi(s,a1, . . . ,an)\u2212\u03a6i(s) = \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ RMi(s,a1, . . . ,an,s \u2032)+ \u03b3V \u2217Mi(s \u2032)\n+\u03b3\u03a6i(s\u2032)\u2212 \u03b3\u03a6i(s\u2032) ] \u2212\u03a6i(s). (15)\nSince \u2211s\u2032\u2208S T (s,a1, . . . ,an,s\u2032) = 1, the above equation becomes\nQ\u2217Mi(s,a1, . . . ,an)\u2212\u03a6i(s) = \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ RMi(s,a1, . . . ,an,s \u2032)\n+\u03b3\u03a6i(s\u2032)\u2212\u03a6i(s)+ \u03b3V \u2217Mi(s \u2032)\u2212 \u03b3\u03a6i(s\u2032)\n]\n. (16)\nAccording to (6), we can rewrite the above equation as\nQ\u2217Mi(s,a1, . . . ,an)\u2212\u03a6i(s) = \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ RMi(s,a1, . . . ,an,s \u2032)+ \u03b3\u03a6i(s\u2032)\u2212\u03a6i(s)\n+\u03b3 \u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An\nQ\u2217Mi(s \u2032 ,a\u20321, . . . ,a \u2032 n)\u03c0\u2217M1(s \u2032 ,a\u20321) \u00b7 \u00b7 \u00b7\u03c0\u2217Mi(s \u2032 ,a\u2032n)\u2212 \u03b3\u03a6i(s\u2032)\n]\n= \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) { RMi(s,a1, . . . ,an,s \u2032)+ \u03b3\u03a6i(s\u2032)\u2212\u03a6i(s)\n+\u03b3 \u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An\n[\nQ\u2217Mi(s \u2032 ,a\u20321, . . . ,a \u2032 n)\u2212\u03a6i(s \u2032) ] \u03c0\u2217M1(s \u2032 ,a\u20321) \u00b7 \u00b7 \u00b7\u03c0\u2217Mi(s \u2032 ,a\u2032n) } . (17)\nBased on the definitions of Fi(s,s\u2032) in (9) and Q\u0302M\u2032i (s,a1, . . . ,an) in (13), the above equation becomes\nQ\u0302M\u2032i (s,a1, . . . ,an) = \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ RMi(s,a1, . . . ,an,s \u2032)+Fi(s,s \u2032)\n+\u03b3 \u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An\nQ\u0302M\u2032i (s \u2032 ,a\u20321, . . . ,a \u2032 n) \u03c0\u2217M1(s \u2032 ,a\u20321) \u00b7 \u00b7 \u00b7\u03c0\u2217Mi(s \u2032 ,a\u2032n)\n]\n. (18)\nSince equations (14) and (18) have the same form as equations (6)-(8), we can conclude that Q\u0302M\u2032i (s,a1, . . . ,an) is the action-value function under the Nash equilibrium for player i in the stochastic game M\u2032. Therefore, we can obtain\nQ\u0302M\u2032i (s,a1, . . . ,an) = Q \u2217 M\u2032i (s,a1, . . . ,an) = Q \u2217 Mi(s,a1, . . . ,an)\u2212\u03a6i(s). (19)\nIf the state s is the terminal state sT , then we have Q\u0302M\u2032i (sT ,a1, . . . ,an) = Q \u2217 Mi(sT ,a1, . . . ,an)\u2212 \u03a6i(sT ) = 0\u2212 0 = 0. Based on (14) and Q\u0302M\u2032i (s,a1, . . . ,an) = Q \u2217 M\u2032i (s,a1, . . . ,an), we can find that the Nash equilibrium in M is also the Nash equilibrium in M\u2032. Then the state-value function under the Nash equilibrium in the stochastic game M\u2032 can be given as\nV \u2217M\u2032i (s) =V \u2217 Mi(s)\u2212\u03a6i(s). (20)\n(Proof of Necessity) If Fi (i = 1, . . . ,n) is not a potential-based shaping function, we will have Fi(s,s\u2032) 6= \u03b3\u03a6i(s\u2032)\u2212\u03a6i(s). Similar to Ng et al. (1999)\u2019s proof of necessity, we define \u2206 = Fi(s,s\u2032)\u2212 [\u03b3\u03a6i(s\u2032)\u2212\u03a6i(s)]. Then we can build a stochastic game M by giving the following transition function T and player 1\u2019s reward function RM1(\u00b7)\nT (s1,a 1 1,a2, . . . ,an,s3) = 1, T (s1,a 2 1,a2, . . . ,an,s2) = 1,\nT (s2,a1, . . . ,an,s3) = 1,\nT (s3,a1, . . . ,an,s3) = 1,\nRM1(s1,a1, . . . ,an,s3) = \u2206 2 , (21) RM1(s1,a1, . . . ,an,s2) = 0, RM1(s2,a1, . . . ,an,s3) = 0, RM1(s3,a1, . . . ,an,s3) = 0,\nwhere ai(i = 1, . . . ,n) represents any possible action ai \u2208 Ai from player i, and a11 and a 2 1 represent player 1\u2019s action 1 and action 2 respectively. Equation T (s1,a11,a2, . . . ,an,s3) = 1 in (21) denotes that, given the current state s1, player 1\u2019s action a11 will lead to the next state s3 no matter what joint action the other players take. Based on the above transition function and reward function, we can get the game model including states (s1,s2,s3) shown in Figure 1. We now define \u03a61(si) = \u2212F1(si,s3)(i = 1,2,3). Based on (6), (7), (19), (20) and (21), we can obtain player 1\u2019s action-value function at state s1 in M and M\u2032\nQ\u2217M1(s1,a 1 1, . . . ) = \u2206 2 , Q\u2217M1(s1,a 2 1, . . . ) = 0,\nQ\u2217M\u20321(s1,a 1 1, . . . ) = F1(s1,s2)+ \u03b3F1(s2,s3)\u2212 \u2206 2 , Q\u2217M\u20321(s1,a 2 1, . . . ) = F1(s1,s2)+ \u03b3F1(s2,s3).\nThen the Nash equilibrium policy for player 1 at state s1 is\n\u03c0\u2217M1(s1,a1) =\n\n\n\na11 if \u2206 > 0, a21 otherwise , \u03c0\u2217M\u20321(s1,a1) =\n\n\n\na21 if \u2206 > 0, a11 otherwise . (22)\nTherefore, in the above case, the Nash equilibrium policy for player 1 at state s1 in M is not the Nash equilibrium policy in M\u2032.\nThe above analysis shows that the potential-based shaping reward with the form of Fi(s,s\u2032) = \u03b3\u03a6i(s\u2032)\u2212 \u03a6i(s) guarantees the Nash equilibrium policy invariance. Now the question becomes how to select a shaping function \u03a6i(s) to improve the learning performance of the learner. Ng et al. (1999) showed that \u03a6i(s) = V \u2217Mi(s) is a good candidate for improving the player\u2019s learning\nperformance in an MDP. We substitute \u03a6i(s) =V \u2217Mi(s) into (18) and get\nQ\u0302M\u2032i (s,a1, . . . ,an) = Q \u2217 M\u2032i (s,a1, . . . ,an)\n= \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ RMi(s,a1, . . . ,an,s \u2032)+Fi(s,s \u2032)\n+\u03b3 \u2211 a1,...,an\u2208A1\u00d7\u00b7\u00b7\u00b7\u00d7An\nQ\u2217M\u2032i (s \u2032 ,a\u20321, . . . ,a \u2032 n) \u03c0\u2217M1(s \u2032 ,a\u20321) \u00b7 \u00b7 \u00b7\u03c0\u2217Mi(s \u2032 ,a\u2032n)\n]\n= \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ RMi(s,a1, . . . ,an,s \u2032)+Fi(s,s \u2032)\n+ \u03b3(V \u2217Mi(s \u2032)\u2212\u03a6i(s\u2032))\n]\n= \u2211 s\u2032\u2208S T (s,a1, . . . ,an,s \u2032) [ RMi(s,a1, . . . ,an,s \u2032)+Fi(s,s \u2032) ] . (23)\nEquation (23) shows that the action-value function Q\u2217M\u2032i (s,a1, . . . ,an) in state s can be easily obtained by checking the immediate reward RMi(s,a1, . . . ,an,s \u2032)+Fi(s,s\u2032) that player i received in state s\u2032. However, in practical applications, we will not have all the information of the environment such as T (s,a1, . . . ,an,s\u2032) and Ri(s,a1, . . . ,an,s\u2032). This means that we cannot find a shaping function \u03a6i(s) such that \u03a6i(s) = V \u2217Mi(s) without knowing the model of the environment. Therefore, the goal for designing a shaping function is to find a \u03a6i(s) as a \u201cgood\u201d approximation to V \u2217Mi(s)."}, {"heading": "4. Conclusion", "text": "A potential-based shaping method can be used to deal with the temporal credit assignment problem and speed up the learning process in MDPs. In this article, we extend the potential-based shaping method to general-sum stochastic games. We prove that the proposed potential-based shaping reward applied to a general-sum stochastic game will not change the original Nash equilibrium of the game. The analysis result in this article has the potential to improve the learning performance of the players in a stochastic game."}], "references": [{"title": "Potential-based shaping in model-based reinforcement learning", "author": ["J. Asmuth", "M.L. Littman", "R. Zinkov"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,", "citeRegEx": "Asmuth et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2008}, {"title": "Social reward shaping in the prisoner\u2019s dilemma", "author": ["M. Babes", "E. Munoz de Cote", "M.L. Littman"], "venue": "In Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS", "citeRegEx": "Babes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Babes et al\\.", "year": 2008}, {"title": "Dynamic Noncooperative Game Theory", "author": ["T. Ba\u015far", "G.J. Olsder"], "venue": "SIAM Series in Classics in Applied Mathematics 2nd,", "citeRegEx": "Ba\u015far and Olsder,? \\Q1999\\E", "shortCiteRegEx": "Ba\u015far and Olsder", "year": 1999}, {"title": "Dynamic Programming: Deterministic and Stochastic Models", "author": ["D.P. Bertsekas"], "venue": "PrenticeHall, Englewood Cliffs, NJ.", "citeRegEx": "Bertsekas,? 1987", "shortCiteRegEx": "Bertsekas", "year": 1987}, {"title": "Multiagent Learning in the Presence of Agents with Limitations", "author": ["M. Bowling"], "venue": "Ph.D. thesis, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA.", "citeRegEx": "Bowling,? 2003", "shortCiteRegEx": "Bowling", "year": 2003}, {"title": "Theoretical considerations of potential-based reward shaping for multi-agent systems", "author": ["S. Devlin", "D. Kudenko"], "venue": "In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Devlin and Kudenko,? \\Q2011\\E", "shortCiteRegEx": "Devlin and Kudenko", "year": 2011}, {"title": "Robot shaping: developing autonomous agents through learning", "author": ["M. Dorigo", "M. Colombetti"], "venue": "Artificial Intelligence,", "citeRegEx": "Dorigo and Colombetti,? \\Q1994\\E", "shortCiteRegEx": "Dorigo and Colombetti", "year": 1994}, {"title": "Shaping as a method for accelerating reinforcement learning", "author": ["V. Gullapalli", "A. Barto"], "venue": "In Proceedings of the 1992 IEEE International Symposium on Intelligent Control,", "citeRegEx": "Gullapalli and Barto,? \\Q1992\\E", "shortCiteRegEx": "Gullapalli and Barto", "year": 1992}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the 11th International Conference on Machine Learning, pp. 157\u2013163.", "citeRegEx": "Littman,? 1994", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "Reward functions for accelerated learning", "author": ["M.J. Mataric"], "venue": "Proceedings of the 11th International Conference on Machine Learning.", "citeRegEx": "Mataric,? 1994", "shortCiteRegEx": "Mataric", "year": 1994}, {"title": "Policy invariance under reward transformations: theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "In Proceedings of the 16th International Conference on Machine Learning,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Learning to drive a bicycle using reinforcement learning and shaping", "author": ["J. Randl\u00f8v", "P. Alstr\u00f8m"], "venue": "In Proceedings of the 15th International Conference on Machine Learning", "citeRegEx": "Randl\u00f8v and Alstr\u00f8m,? \\Q1998\\E", "shortCiteRegEx": "Randl\u00f8v and Alstr\u00f8m", "year": 1998}, {"title": "Stochastic games", "author": ["L.S. Shapley"], "venue": "Proceedings of the National Academy of Sciences, Vol. 39, pp. 1095\u20131100.", "citeRegEx": "Shapley,? 1953", "shortCiteRegEx": "Shapley", "year": 1953}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Potential-based shaping and Q-value initialization are equivalent", "author": ["E. Wiewiora"], "venue": "Journal of Artificial Intelligence Research, 19, 205\u2013208.", "citeRegEx": "Wiewiora,? 2003", "shortCiteRegEx": "Wiewiora", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "Reward shaping is a technique to improve the learning performance of a reinforcement learner by introducing shaping rewards to the environment (Gullapalli & Barto, 1992; Mataric, 1994).", "startOffset": 143, "endOffset": 184}, {"referenceID": 9, "context": "The applications of reward shaping can be found in the literature (Gullapalli & Barto, 1992; Dorigo & Colombetti, 1994; Mataric, 1994; Randl\u00f8v & Alstr\u00f8m, 1998).", "startOffset": 66, "endOffset": 159}, {"referenceID": 6, "context": "Gullapalli and Barto (1992) demonstrated the application of shaping to a key-press task where a robot was trained to press keys on a keyboard.", "startOffset": 0, "endOffset": 28}, {"referenceID": 6, "context": "Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a predefined animate-like behavior.", "startOffset": 0, "endOffset": 29}, {"referenceID": 6, "context": "Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a predefined animate-like behavior. Mataric (1994) presented an intermediate reinforcement function for a group of mobile robots to learn a foraging task.", "startOffset": 0, "endOffset": 128}, {"referenceID": 6, "context": "Dorigo and Colombetti (1994) applied shaping policies for a robot to perform a predefined animate-like behavior. Mataric (1994) presented an intermediate reinforcement function for a group of mobile robots to learn a foraging task. Randl\u00f8v and Alstr\u00f8m (1998) combined reinforcement learning with shaping to make an agent learn to drive a bicycle to a goal.", "startOffset": 0, "endOffset": 259}, {"referenceID": 14, "context": "analysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora, 2003; Asmuth, Littman, & Zinkov, 2008).", "startOffset": 58, "endOffset": 136}, {"referenceID": 10, "context": "In our research, we prove that the Nash equilibria under the potential-based shaping reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under the framework of general-sum stochastic games.", "startOffset": 107, "endOffset": 124}, {"referenceID": 6, "context": "analysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora, 2003; Asmuth, Littman, & Zinkov, 2008). Ng et al. (1999) presented a potential-based shaping reward that can guarantee the policy invariance for a single agent in a Markov decision process (MDP).", "startOffset": 112, "endOffset": 155}, {"referenceID": 6, "context": "analysis of reward shaping can be found in the literature (Ng, Harada, & Russell, 1999; Wiewiora, 2003; Asmuth, Littman, & Zinkov, 2008). Ng et al. (1999) presented a potential-based shaping reward that can guarantee the policy invariance for a single agent in a Markov decision process (MDP). Ng et al. proved that the optimal policy keeps unchanged after adding the potential-based shaping reward to an MDP environment. Following Ng et al., Wiewiora (2003) showed that the effects of potential-based shaping can be achieved by a particular initialization of Q-values for agents using Q-learning.", "startOffset": 112, "endOffset": 459}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach. The above articles focus on applications of reward shaping to a single agent in an MDP. For the applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008) introduced a social shaping reward for players to learn their equilibrium policies in the iterated prisoner\u2019s dilemma game.", "startOffset": 0, "endOffset": 304}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach. The above articles focus on applications of reward shaping to a single agent in an MDP. For the applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008) introduced a social shaping reward for players to learn their equilibrium policies in the iterated prisoner\u2019s dilemma game. But there is no theoretical proof of policy invariance under the reward transformation. In our research, we prove that the Nash equilibria under the potential-based shaping reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under the framework of general-sum stochastic games. Note that the similar work of Devlin and Kudenko (2011) was published while this article was under review.", "startOffset": 0, "endOffset": 805}, {"referenceID": 0, "context": "Asmuth et al. (2008) applied the potential-based shaping reward to a model-based reinforcement learning approach. The above articles focus on applications of reward shaping to a single agent in an MDP. For the applications of reward shaping in general-sum games, Babes, Munoz de Cote, and Littman (2008) introduced a social shaping reward for players to learn their equilibrium policies in the iterated prisoner\u2019s dilemma game. But there is no theoretical proof of policy invariance under the reward transformation. In our research, we prove that the Nash equilibria under the potential-based shaping reward transformation (Ng et al., 1999) will also be the Nash equilibria for the original game under the framework of general-sum stochastic games. Note that the similar work of Devlin and Kudenko (2011) was published while this article was under review. But Devlin and Kudenko only proved sufficiency based on a proof technique introduced by Asmuth et al. (2008), while we prove both sufficiency and necessity using a different proof technique in this article.", "startOffset": 0, "endOffset": 965}, {"referenceID": 12, "context": "Framework of Stochastic Games Stochastic games were first introduced by Shapley (1953). In a stochastic game, players choose the joint action and move from one state to another state based on the joint action they choose.", "startOffset": 72, "endOffset": 87}, {"referenceID": 3, "context": "For any MDP, there exists a deterministic optimal policy for the player (Bertsekas, 1987).", "startOffset": 72, "endOffset": 89}, {"referenceID": 8, "context": "An example is the soccer game introduced by Littman (Littman, 1994) where an agent on the offensive side must use a probabilistic policy to pass an unknown defender.", "startOffset": 52, "endOffset": 67}, {"referenceID": 4, "context": "In the literature, a solution to a stochastic game can be described as Nash equilibrium strategies in a set of associated state-specific matrix games (Bowling, 2003; Littman, 1994).", "startOffset": 150, "endOffset": 180}, {"referenceID": 8, "context": "In the literature, a solution to a stochastic game can be described as Nash equilibrium strategies in a set of associated state-specific matrix games (Bowling, 2003; Littman, 1994).", "startOffset": 150, "endOffset": 180}, {"referenceID": 4, "context": ",an) is known for all the states, we can find player i\u2019s Nash equilibrium policy by solving the associated state-specific matrix game (Bowling, 2003).", "startOffset": 134, "endOffset": 149}, {"referenceID": 10, "context": "The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008).", "startOffset": 144, "endOffset": 198}, {"referenceID": 14, "context": "The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008).", "startOffset": 144, "endOffset": 198}, {"referenceID": 0, "context": "The theoretical studies on potentialbased shaping methods that appear in the published literature consider the case of a single agent in an MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008).", "startOffset": 144, "endOffset": 198}, {"referenceID": 9, "context": "Potential-Based Shaping in General-Sum Stochastic Games Ng et al. (1999) presented a reward shaping method to deal with the credit assignment problem by adding a potential-based shaping reward to the environment.", "startOffset": 56, "endOffset": 73}, {"referenceID": 0, "context": ", 1999; Wiewiora, 2003; Asmuth et al., 2008). In our research, we extend the potential-based shaping method from Markov decision processes to multi-player stochastic games. We prove that the Nash equilibria under the potential-based shaping reward transformation will be the Nash equilibria for the original game under the framework of general-sum stochastic games. We define a potential-based shaping reward Fi(s,s) for player i as Fi(s,s ) = \u03b3\u03a6i(s)\u2212\u03a6i(s), (9) where \u03a6 : S \u2192 R is a real-valued shaping function and \u03a6(sT ) = 0 for any terminal state sT . We define a multi-player stochastic game as a tuple M = (S,A1, . . . ,An,T,\u03b3 ,R1, . . . ,Rn) where S is a set of states, A1, . . . ,An are players\u2019 action sets, T is the transition function, \u03b3 is the discount factor, and Ri(s,a1, . . . ,an,s)(i = 1, . . . ,n) is the reward function for player i. After adding the shaping reward function Fi(s,s) to the reward function Ri(s,a1, . . . ,an,s), we define a transformed multi-player stochastic game as a tuple M\u2032 = (S,A1, . . . ,An,T,\u03b3 ,R1, . . . ,Rn) where Ri(i = 1, . . . ,n) is the new reward function given by Ri(s,a1, . . . ,an,s \u2032) = Fi(s,s)+Ri(s,a1, . . . ,an,s). Inspired by Ng et al. (1999)\u2019s proof of policy invariance in an MDP, we prove the policy invariance in a multi-player general-sum stochastic game as follows.", "startOffset": 24, "endOffset": 1201}, {"referenceID": 10, "context": "Similar to Ng et al. (1999)\u2019s proof of necessity, we define \u2206 = Fi(s,s)\u2212 [\u03b3\u03a6i(s)\u2212\u03a6i(s)].", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "Ng et al. (1999) showed that \u03a6i(s) = V \u2217 Mi(s) is a good candidate for improving the player\u2019s learning", "startOffset": 0, "endOffset": 17}], "year": 2011, "abstractText": "We extend the potential-based shaping method from Markov decision processes to multi-player general-sum stochastic games. We prove that the Nash equilibria in a stochastic game remains unchanged after potential-based shaping is applied to the environment. The property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}