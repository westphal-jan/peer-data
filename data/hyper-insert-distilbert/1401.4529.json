{"id": "1401.4529", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "General factorization framework for context-aware recommendations", "abstract": "additional general error feature based solutions are emerging in the field of passive recommender operational systems itself with the increased need shown to incorporate having multiple meaningful sources of specific information into coupling a single robust model. the common property product of undertaking these approaches is that they use generally a fixed index factorization model class that upwards can be extended to ideally handle arbitrary number of context dimensions ; the intermediate model is readily then learnt using strictly different optimization techniques. yet in this recent paper lastly we propose a general framework in which arbitrary indirect linear feature evaluation models variables can be learnt efficiently. furthermore moreover, both whereas the active factorization oriented model workshop and the flexible model members process are considered unrestricted only in the framework, thus it is more flexible than state - of - the - richter art macro general feature input based interaction solutions. the generic framework above allows for both intensive implicit feedback system based item ranking management and rating prediction far from explicit evaluation feedback. the paper focuses on reducing the simple implicit feedback based recommendation problems, albeit due to especially its expansive comparative use widely in practical systems compared specifically to explicit passive feedback. using clearly the flexibility curve of explaining the framework we ( 1 ) fundamentally evaluate various factorization models using around 5 implicit feedback data sets and vertically injecting contextual information into the model ; ( 2 ) identify analytical models that can rapidly solve the improved implicit feedback based context - aware implicit recommendation task better than previously published proposed model classes. advantages and optimization drawbacks of various models performance and learning strategies are broadly also typically discussed briefly.", "histories": [["v1", "Sat, 18 Jan 2014 11:13:26 GMT  (316kb)", "http://arxiv.org/abs/1401.4529v1", null], ["v2", "Tue, 19 May 2015 11:50:22 GMT  (593kb,D)", "http://arxiv.org/abs/1401.4529v2", "The final publication is available at Springer viathis http URLData Mining and Knowledge Discovery, 2015"]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["bal\\'azs hidasi", "domonkos tikk"], "accepted": false, "id": "1401.4529"}, "pdf": {"name": "1401.4529.pdf", "metadata": {"source": "CRF", "title": "General factorization framework for context-aware recommendations", "authors": ["Bal\u00e1zs Hidasi", "Domonkos Tikk"], "emails": ["balazs.hidasi@gravityrd.com", "domonkos.tikk@gravityrd.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\n45 29\nv1 [\ncs .I\nR ]\n1 8\nJa n"}, {"heading": "1 Introduction", "text": "Recommender systems are information filtering tools that help users in information overload to find interesting items. For modeling user preferences, classical approaches either use item metadata (content based filtering, CBF; [1]), or user\u2013 item interactions (collaborative filtering, CF; [2]). CF algorithms proved to be more accurate than CBF methods, if sufficient interaction data (or events) is available [3].\nLatent factor based CF methods gained popularity due to their attractive accuracy and scalability [4]. They intend to capture user preferences by uncovering latent features that explain the observed user\u2013item events (ratings). Models are created by the factorization of the partially observed user\u2013item rating matrix, and the user preferences are approximated by the scalar product of the user and item factors. Matrix factorization (MF) methods may differ in the model building, the learning method and the objective function. For learning, MF methods may apply, e.g., alternating least squares (ALS; [5]), stochastic gradient [6], or a probabilistic framework [7].\nII\nThe dualistic user\u2013item based modeling concept can be extended by considering additional information that may influence the user preferences at recommendation; such data are together termed contextual information, or briefly context [8]. The hypothesis of context-aware recommendation systems is that the integration of context into the model may improve the modeling capacity and accuracy.\nHere we propose a novel general modeling framework and present its application for recommendation problems. In this framework arbitrary linear feature models can be learnt efficiently without restriction on (1) the factorization model and (2) the complexity of the model members. Both implicit feedback based item ranking and explicit feedback based rating prediction are supported. We focus here on implicit feedback problems, due to its expansive use in practical systems compared to explicit feedback.\nThe contribution of this paper is threefold: (1) we show that arbitrary linear feature models can be learnt efficiently using an ALS based pointwise preference estimation; (2) we evaluate various factorization models in this flexible framework to identify those that solve the implicit feedback based context aware recommendation task accurately; (3) we compare the pointwise preference estimation, with the Bayesian Personalized Ranking (BPR; [9]); a pairwise learning strategy) and show that while BPR clearly outperforms pointwise preference estimation in the classical 2D MF setting, it seems to fall behind with more complex models on bigger data sets.\nThe rest of the paper is organized as follows. Section 2 describes the basic general framework and the learning process, followed by a thorough comparison of models presented in section 3. The framework is extended in section 4 to incorporate additional data sources; two examples highlight the usefulness of the extension. Section 5 overviews other general frameworks and points out the fundamental differences to our approach."}, {"heading": "2 The basic general framework", "text": "In recommendation problems, the main goal is the modeling of user preferences on items. In the traditional CF approach the preference model is solely learnt from sample events ; an event describes a user interaction with an item. In the context-aware case, the model is extended with contexts that conditions the preference relation. Context can be the location or time of the interaction, the device on which the interaction was performed, or any other parameters that may influence the user preference, including weather, social network, referral\u2019s link, search keyword, etc. Here we present a general modeling framework \u2014 inspired by the latent factor CF approach \u2014 which efficiently integrates context data into the preference model.\nLet dom(A) denote the domain of the categorical attribute A; the values of dom(A) are termed entities. The Descartes product of the domains, A = dom(A(1)) \u00d7 . . . \u00d7 dom(A(D)) denotes the data space, where A(i) is the ith attribute, i \u2208 [1, D]. The size of the data space is: \u220fD\ni=1 Si, where Si = \u2223 \u2223dom(A(i)) \u2223 \u2223,\nIII\nthat is the cardinality of the domain. The training data set T contains data points (termed events) in the data space: T \u2282 A. In the recommendation setting, attributes may represent the set of users, the set of items, or a contextual parameter. In the basic framework, we assume that each attribute takes at most one value in every data point (attribute values are atomic). We waive this assumption in the extension, see section 4. We also assume that attribute values are categorical.\nInspired by factorization methods, we assign a feature vector of length K to each entity in our preference model. A(i) is represented as a feature matrix (M (i) \u2208 RK\u00d7Si), assembled from the feature vectors of entities of A(i). In the general framework, a preference model is any linear model of the feature vectors such that: (1) a model consists of sums of Hadamard (or elementwise) products; (2) each product contains at least two feature vectors; (3) in a product each feature vector belongs to a different attribute (linearity).\nWe now show that standard factorization models are special cases of the general modeling framework. In standard 2DMF, the preference of user u on item i is predicted as product of user and an item features: r\u0302u,i = 1 T ( M (1) u \u25e6M (2) i ) . iTALS, the context-aware tensor factorization model [10] predicts the preference of user u on item i under context-state c as r\u0302u,i,c = 1 T ( M (1) u \u25e6M (2) i \u25e6M (3) c ) , the product of each features."}, {"heading": "2.1 ALS-based training on implicit data", "text": "The presented learning method generalizes the works of [10] and the concept introduced in [11]. Weighted root mean squared error (wRMSE) is used for optimizing feature vectors with ALS.3 A weight is assigned to all possible combinations of attribute values:\nC : ti1,...,iD =< a (1) i1 , . . . , a (D) iD >\u2192 R\nC(ti1,...,iD ) = ci1,...,iD =\n{\nc\u2032i1,...,iD \u226b c0, if ti1,...,iD \u2208 T c0, otherwise\n(1)\nA practical choice for weights is c0 = 1 and c \u2032 i1,...,iD = 100.4\nThe loss function is:5\nL =\nS1,...,SD\u2211\ni1=1,...,iD=1\nci1,...,iD (r\u0302i1,...,iD \u2212 ri1,...,iD ) 2 (2)\n3 A pairwise ranking method, namely BPR based [9] optimization with SGD was also implemented, but the ALS method is preferred for training. The description of the BPR+SGD learning is omitted here, but can be reconstructed from [9,12]. 4 Since an entity combination may be present multiple times in the training data, the actual weight of a combination is proportional to the support of that combination. 5 Without regularization due to clearer presentation. Actual loss function contains \u21132 regularization.\nIV\nwhere r\u0302i1,...,iD is the predicted preference, ri1,...,iD is the actual preference value 6:\nri1,...,iD =\n{\n1, if ti1,...,iD \u2208 T 0, otherwise (3)\nA given general model:\nr\u0302i1,...,iD = 1 T ( M (\u03c31)\u03c01 \u25e6 . . . \u25e6M (\u03c3p1 ) \u03c0p1 + . . .+\n+M (\u03c3pq\u22121+1) \u03c0pq\u22121+1 \u25e6 . . . \u25e6M (\u03c3pq ) \u03c0pq\n) (4)\nwhere \u03c3k \u2208 [1 . . .D] and \u03c0k = ij if \u03c3k = j. Since the computation of each M (i) feature matrix is similar, the steps required for the calculation of M (1) are shown without the loss of generality. For clearer presentation, the members of the model (equation (4)) are grouped into two groups based on whether a column of M (1) is part of them:7\nr\u0302i1,...,iD = ( M (\u03c32)\u03c02 \u25e6 . . . \u25e6M (\u03c3p1) \u03c0p1 + . . .+\n+M (\u03c3pk\u22121+2) \u03c0pk\u22121+2 \u25e6 . . . \u25e6M (\u03c3pk ) \u03c0pk )T \ufe38 \ufe37\ufe37 \ufe38\n(Q1) T\nM (1) i1 +\n+ ( M (\u03c3pk+1) \u03c0pk+1 \u25e6 . . . \u25e6M (\u03c3pk+1 ) \u03c0pk+1 +\n+ . . .+M (\u03c3pq\u22121+1) \u03c0pq\u22121+1 \u25e6 . . . \u25e6M (\u03c3pq ) \u03c0pq )T \ufe38 \ufe37\ufe37 \ufe38\n(Q2) T\n1\n(5)\nWhen recomputing M (1), every other matrix is fixed, thus L is convex in the elements ofM (1). The minimum is reached when \u2202L/\u2202M (1) is zero. The columns of M (1) can be computed separately, because the derivative is linear in them. Each column is computed similarly, therefore only the steps for M (1) 1 (the first\n6 To use the framework with explicit feedback, one only needs to alter the weighting (by setting c0 to 0 and ci1,...,iD to 1) and use the rating values as preferences. 7 To avoid more complex notation we assume that the columns of M (1) are the first members in the products where they are present.\nV column of M (1)) are shown:\n\u2202L\n\u2202M (1) 1\n= \u22122\nS2,...,SD\u2211\ni2=1,...,iD=1\nr1,i2,...,iDc1,i2,...,iDQ1\n\ufe38 \ufe37\ufe37 \ufe38\nO\n+\n+2\nS2,...,SD\u2211\ni2=1,...,iD=1\nc0r\u03021,i2,...,iDQ1\n\ufe38 \ufe37\ufe37 \ufe38\nI2=I+JM (1) 1\n+2\nS2,...,SD\u2211\ni2=1,...,iD=1\n(c1,i2,...,iD \u2212 c0)r\u03021,i2,...,iDQ1\n\ufe38 \ufe37\ufe37 \ufe38\nI1=I\u2032+J \u2032M (1) 1\n(6)\nO, I \u2032 and J \u2032 can be computed efficiently (see section 2.2), however the naive computation of I and J is expensive. Therefore we further transform I2. With the expansion of r\u03021,...,iD (substituting (5) with i1 = 1):\nI2 = 2c0\nS2,...,SD\u2211\ni2=1,...,iD=1\nQ1(Q1) TM (1) 1 +Q1(Q2) T 1 (7)\nExpanding eitherQ1(Q1) T orQ1(Q2) T results in sums of matrix products, where the arguments are the elementwise products of multiple feature vectors:\nS2,...,SD\u2211\ni2=1,...,iD=1\n(\nM (j1) ij1 \u25e6 . . . \u25e6M (jm) ijm\n)(\nM (l1) il1 \u25e6 . . . \u25e6M (lt) ilt\n)T\n(8)\nwhere ji 6= jk if i 6= k, li 6= lk if i 6= k, ji \u2208 [2 . . . n] and lk \u2208 [2 . . . n]. With rearranging this expression, only the following types of quantities are needed to be computed:\nVI\n(a) C(j) =\nSj\u2211\ni=1\nM (j) i\n(\nM (j) i\n)T\n,\n(b) O(l) =\nSl\u2211\ni=1\nM (l) i ,\n(c) Sk,\n(9)\nwhere (a) the covariance matrix of the feature vectors of the jth feature matrix (C(j) \u2208 RK\u00d7K); (b) the sum of the feature vectors of the lth feature matrix (O(l) \u2208 RK); (c) domain size (Sk \u2208 R). (8) can be computed from (a), (b) and (c) using (1) elementwise product of RK\u00d7K matrices; (2) elementwise product of R\nK vectors; (3) matrix product of RK vectors; (4) matrix\u2013scalar multiplication. Note that Sk is a fix value during the training process, and C\n(j) and O(j) only changes after the jth feature matrix is recomputed. Therefore these quantities can be precomputed and should be updated only once per epoch.\nAfter O, I \u2032, J \u2032, I and J from equation (6) are computed, \u2202L \u2202M\n(1) 1\n= 0 can\nbe solved for M (1) 1 . Either a least squares solver (LS) or an approximate least squares solver (e.g. conjugate gradient, [13,14]) can be used to get the new value of the feature vector. Algorithm 2.1 shows the high level pseudocode of the training.\nYet we neglected regularization and biases. Regularization can be done by adding a K\u00d7K sized diagonal matrix to J +J \u2032 (i.e. to the coefficient matrix of M (i) j ) just before computing the feature vector. The model (4) can be extended with biases by adding \u2211D\ni=1 \u2211Si j=1 vi,jbi,j to it, where bi,j is the bias value for the\njth entity of the ith attribute and vi,j is the weight of the bias. The training of this biased model can also be done efficiently (with complexity of the non-biased K + 1-feature model\u2019s)."}, {"heading": "2.2 Complexity of training", "text": "In equation (6), O, I \u2032 and J \u2032 can be computed in O(N+1 K 2|O|) time; here N+1 is the number of training events where the value of the A(1) attribute is a (1) 1 , and |O| is the complexity of the model (i.e. the number of vector operations to compute r\u0302). This is possible due to the definition of c weights and r preferences, as most of the members in the sums of O, I \u2032 and J \u2032 are in fact zeroes. For each column of M (1) this sums up to O(N+K2|O|), where N+ is the number of training examples. I and J are independent of the actually computed column. Thus they are computed once during the recomputation of M (1), assembled from members described in (9). C(j) and O(j) can be computed in O(SjK\n2) and O(SjK) time, respectively. Recall, these quantities should only be recomputed after the jth feature matrix is changed (recomputed). The operations of the assembling take up O(|O|K2) time.\nVII\nAlgorithm 2.1 ALS-based learning of the general framework on implicit data\nInput: T : training data; MODEL: the description of the desired model K: number of features; E: number of epochs; \u03bb: regularization coefficient Output: {M (i)}i=1,...,D K \u00d7 Si sized low rank matrices procedure Train(T , MODEL, K, E, \u03bb)\n1: for i = 1, . . . , D do 2: M (i) \u2190 Random K \u00d7 Si sized matrix 3: C(i) \u2190 \u2211Si\nk=1 M (i) k\n(\nM (i) k\n)T\nand O(i) \u2190 \u2211Si k=1 M (i) k\n4: end for 5: for e = 1, . . . , E do 6: for i = 1, . . . , D do 7: Compute the shared parts I and J 8: for j = 1, . . . , Si do 9: Compute O, I\u2032 and J \u2032\n10: Add regularization 11: Solve \u2202L\n\u2202M (i) j\n= 0 for M (i) j\n12: end for 13: C(i) \u2190 \u2211Si\nk=1 M (i) k and O (i) \u2190 \u2211Si k=1 M (i) k\n(\nM (i) k\n)T\n14: end for 15: end for 16: return {M (i)}i=1,...,D end procedure\nThe computation of M (1) 1 using an LS solver is O(K 3) that sums up to O(S1K\n3) for the whole M (1) matrix. All computations required forM (1), including the recomputation of C(1) and O(1) can be done efficiently in O(N+|O|K2+ S1K 3) (assuming thatN+ \u226b S1). So the total cost of an epoch isO(DN +|O|K2+ \u2211D i=1 SiK\n3). Thus the training scales linearly with the size of the training set. In practice it scales quadratically with the number of factors instead of cubically, because |O|DN+ \u226b\n\u2211D i=1 Si and K is small (K \u2208 [20 . . . 300]), thus the first\nterm dominates.\nSince the computational bottleneck of the algorithm is the O(K3) complexity of the LS solver, we opt for a conjugate gradient (CG) based approximate LS solver. CG was used as an approximation for the naive LS solver in ALS based MF in [13] and for three way tensor factorization by [14]. As shown there, CG greatly improves scalability at none to little loss in recommendation accuracy. We generalize this idea for our framework8. Using CG instead of LS can significantly improve the time required for the training. With careful implementation the complexity of the algorithm can be reduced to O(DN+|O|K+\n\u2211D i=1 SiK\n2) that is linear in K in practice.\n8 Some non-trivial technical steps are required for the adaptation; not shown here.\nVIII"}, {"heading": "3 Model comparison", "text": "The flexibility of the general framework permits to learn arbitrary linear models efficiently. Here we attempt to answer (1) if is there a single model that performs well on many different problems; (2) in which scenarios can one model outperform the other.\nOur experimental setup is the following. We used five implicit data sets from different domains. Two of them (Grocery and VoD) are proprietary, the others (TV1, TV2, [15] and LastFM 1K, [16]) are publicly available; see also Table 1.9 The primary metric is recall@20 that is a good proxy to evaluate top-N recommendations. We estimate that in practice users are exposed in average to 20 recommendations during a visit (4 pageviews, 5 items per recommendation), that dictates a cutoff at 20. Mean Average Precision (MAP) is the secondary evaluation metric. In contrast to recall, MAP considers the order of the recommended items, and prefers methods that ranks relevant items high on the list (also with cutoff at 20, MAP@20).\nTwo context dimension are used: (1) seasonality and (2) sequentiality [10]. For seasonality, we define the season (or periodicity) and time bands therein: each event is assigned to a time band based on its time stamp. This model captures time-based consumption of users and items (e.g. daily routines; seasonal consumption patterns). For sequentiality, the context state of an event is defined as the previously consumed item by the same user. This context enables distinction of item groups with different repetitiveness patterns (e.g. diapers and washing machines). The context states are automatically generated within the context dimensions. We let K = 20 (features) and E = 10 (epochs). Other parameters were optimized using a validation subset of the training data. Finally, the methods were retrained on the whole training data using the optimal parameters.\nTo simplify the notion we omit indices, simplify operators and use capital letters for feature vectors in the models. U stands for user, I for item, S for\n9 The column \u201cMulti\u201d shows the average multiplicity of user\u2013item pairs in the training events. It is 1.0 at TV1 and TV2 due to possible filtering of duplicate events.\nIX\nseasonality and Q for sequentiality features. Thus, UI + USI + UQI denotes:\nr\u0302u,i,s,q = 1 T ( M (U)u \u25e6M (I) i ) + 1T ( M (U)u \u25e6M (S) s \u25e6M (I) i ) +\n+1T (\nM (U)u \u25e6M (Q) q \u25e6M (I) i\n)\nUsing one set of features for each of the 4 dimensions, there exists 79 different linear models. For evaluating the preference modeling, we selected 13 models descendant of the basic CF; organized in a hierarchy (figure 1). These models use 5 conceptually different members that describe the relationship between certain dimensions: (1) UI is the basic CF model; (2) US (or UQ) is a context dependent user bias that does not play role during the ranking but has noise filtering properties during training; (3) IS (or IQ) is a context dependent item bias that helps in ranking as well as in learning; (4) USI (or UQI or USQI) is the context-state dependent reweighting of the user\u2013item relation; (5) SQ is the interaction bias between context dimensions.\nThe results are also shown on figure 1. From the basic CF model, the first step towards context-aware modeling is the adding a single context dimension using either an elementwise model or a pairwise model. As expected, this increases prediction accuracy.10 Note that the left branch of the hierarchy (elementwise) includes models with context-dependent reweighting, while the right branch (pairwise) focuses on modeling with (context dependent) user and item biases. Both models were already suggested for context-aware modeling (see [10,17]).\nThe pairwise model (with a single context) can be further extended with additional contexts: either to a full pairwise model\u2014identical with the model of Factorization Machines (FM; [18])\u2014or to the same model without the SQ member. Note that SQ does not play a role in ranking items, but slows down the learning and may add noise at learning if context dimensions are independent. Also, this independence is a rather desirable property of context dimensions, because one intends to grasp different aspects of the same problem, instead of using overlapping information. Discarding SQ (a) simplifies our model, and also decreases the time of training and predicting; (b) yields similar or better results due to noise reduction. Observe that the removal of SQ greatly increases the performance on the bigger data sets. In 3 out of 5 cases, models with two context dimensions significantly outperform single-context models, and yield similar results in other 2 cases.\nWe also examined the efficiency of submodels of the reduced pairwise model, UI+US+IS+UQ+IQ. We scrutinized submodels where the context affects the user or the item bias only. UI + IS + IQ (item bias pairwise) model is usually similar but sometimes inferior to its parent model, because it lacks the noise filtering capability of the user biases during learning. UI + US + UQ (user bias pairwise) model is clearly worse than its parent, because the lack of item bias\n10 The type of seasonality we use here is not suitable for TV2 which explains the dropped performance.\nX negatively affects the ranking capability. Yet on Grocery the UI+US+UQmodel achieves the overall best scores. We assume that different context dimensions are more useful for UC, IC or UCI members and seasonality is alike for Grocery.\nOn the left (elementwise) side of figure 1, USQI is the natural extension of USI and UQI with multiple context dimensions, but it is inferior to its parents. The reason lies in the reweighting of the user\u2013item relations. We found that the elementwise model is more sensitive than the pairwise, because underlying features can be blurred if K is small. The blurring effect is strengthened if multiple vectors are used to reweight the user\u2013item relation, thus it can only keep its performance on smaller (less complex) data sets, where a few latent features model the entities well.\nAnother extension of the elementwise model is USI + UQI; sum of the two context reweighted user\u2013item relation parts. The performance gain of the model over its parents implies that two context dimensions are better than one with elementwise models as well. Since elementwise parts are sensitive to the quality of the context, it makes sense to further extend the USI +UQI model with the basic CF model, creating the UI+USI+UQI model. The context-independent UI part can compensate for the sensitivity while the USI and UQI parts can effectively modify the prediction according to the current context. This model is the overall best for 3 and second best for 2 data sets.\nThe final model is the combination of the two main branches: i.e. reweighting plus item and user biases. Though the model is fairly good, it does not beat its parents because the same context dimensions are not appropriate for UCI, UC and IC simultaneously.\nXI"}, {"heading": "3.1 Experiments with BPR+SGD learning", "text": "As mentioned, the general framework is capable of learning the model with BPR+SGD learning as well. We ran the same experiments with BPR+SGD as with wRMSE+ALS, and show recall@20 in table 2 for the best models11.\nBPR+SGD clearly outperforms the wRMSE+ALS learning on the basic CF model (UI); stated also in [9]. However, the performance gain of contexts is more significant with wRMSE+ALS. Results suggest that BPR+SGD is unable to learn UCI type members of the model. A complete explanation of BPR+SGD\u2019s failure on 3-member products requires further studies.\nOverall, wRMSE+ALS was better in 3 out of 5 cases, if the best models are compared. We still argue for using wRMSE+ALS because (1) wRMSE+ALS is better than BPR+SGD on larger data sets (Grocery, LastFM, VoD); (2) BPR+SGD yields no generally best model (i.e. the best model depends on the data set); (3) ALS (and CG+ALS) can be easily parallelized (unlike SGD)."}, {"heading": "4 Extension of the framework", "text": "Thus far the framework is constrained to use attributes where at most one entity can belong to an event. This excludes the usage of some important information sources. We introduce an extension to waive this restriction.\nBased on the NSVD1 approach [19] let us use M (j) = X(j)W (j) where the columns of M (j) \u2208 RK\u00d7Sj are the primary features, those of X(j) \u2208 RK\u00d7Zj are the secondary features and W (j) \u2208 RZj\u00d7Si is the sparse mixing matrix that connects the secondary and primary features, i.e. a primary feature vector is the linear combination of some secondary feature vectors.\nThis approach allows us to extend our framework in the desired way and has multiple advantages. We overcome the original problem by attaching a secondary attribute to the entities of the primary attribute. Hence multiple entities of\n11 Note that the full pairwise model with BPR+SGD learning is essentially the same as FM on implicit problems.\nXII\nthe secondary attribute can be assigned to an event. However the constraint (of having at most one entity per event) still applies to the primary attribute, therefore the computation of the standard dimensions does not change: M (j) can be stored and used for the computations.\nSince the derivative of the loss function w.r.t. the secondary features is not linear in the columns of X(j), an approximative solution is required. We chose to update the secondary feature vectors as if they were independent. To ensure convergence, after training some of the secondary feature vectors, the model should be updated before continuing. Since the update is fast, it can be done after the computation of each vector. Moreover, the update of feature vectors can be parallelized.12\nNext we demonstrate the capabilities of the extension on the Grocery data.13\nThe starting model is UI+USI+UQI."}, {"heading": "4.1 Metadata boosted predictions", "text": "CBF is often combined with CF to create hybrid algorithms that outperform both of them. E.g. item metadata helps overcoming the item cold-start problem in CF. Here we use a simple vector space approach to incorporate metadata into the model. Each item was represented as a sparse, \u21132 normalized metadata vector. Thus we obtain a term\u2013item matrix, with terms as attributes of the items, i.e. we dedicated a primary attribute for the items in our model, and assigned metadata terms as secondary attributes to the entities of this primary attribute.\nTwo models were examined: (i) using metadata in the place of items (UM + USM + UQM); (ii) using metadata and traditional item features in the same model (UI+USI+UQI+UM+USM+UQM); whereM is the metadata based primary item feature (results in table 3). In case (i) the performance decreases, but in case (ii) we observed an increase of 2.78% in recall@20 and 11.55% in MAP@20.\n12 This method can be still slow if the average number of secondary entities assigned to the primary entities is high. Then one can apply two-phase learning of the secondary features [3] for efficiency. Two-phase learning was less accurate in our initial experiments. 13 Due to the lack of necessary metadata, other benchmark data sets are excluded here.\nXIII"}, {"heading": "4.2 Incorporating session information", "text": "Different sessions of the same user are usually treated uniformly by recommender systems, assuming that user preference does not change across sessions. Although, this usually holds, there are some exceptional scenarios: a user who prefers horror movies may watch a romantic comedy with his girlfriend. Generally, some external factors may deviate the user preference from the regular one, and the traditional dualistic user\u2013item model, unaware of external factors, fails to capture that. We handle irregular sessions by adding a session dependent part to the model.\nThe domain of the primary attribute in this setting is the set of events, thus each event has its own primary feature vector. The domain of the secondary attribute is the set of the items. A secondary attribute value is assigned to a primary if (a) the item of the event is not the same as the value of the secondary attribute; (b) an event with the item of the secondary attribute was observed in the same session as the primary event.\nThe model was UI + USI + UQI +XI, where X is the session information based feature vector. XI predicts the preference on the item given the other items in the user\u2019s session. Recall@20 is improved by 9.17% and MAP@20 by 25.07% (see table 3), showing that session information can certainly improve recommendation accuracy."}, {"heading": "5 Related work", "text": "There are different ways to model the data space of context-aware recommendations. The most extensive data space model is the multidimensional data model (MD) [20] and most of the other models are restricted MD variants. The MD data space is the Descartes product of dimensions S = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7DN and each dimension is the Descartes product of attributes Di = Ai,1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Ai,ni . Each attribute has an attribute domain containing the possible attribute values. Values are atomic. S describes the \u201cinput\u201d data space that should be coupled with the appropriate \u201coutput\u201d data according to the objective of the modeling, such as e.g. rating prediction 14 For example for rating prediction the data space is to be extended with a rating dimension (R) that consists of a single attribute, the rating value. Training data is the subset of this extended data space (T \u2282 S\u00d7R). This multidimensional data model can readily describe the data space of many recommendation settings. E.g. the \u201cinput\u201d data space of 2D matrix factorization is S = U \u00d7 I where both U and I have one attribute, the user and the item ID. In the 2D MF setting, the preference/rating prediction and the pairwise ranking differs only in the output dimension and training data generation, but S does not change. Our framework (with the extension introduced in section 4) fully leverages the multidimensional data model. The attributes in our framework\n14 For building a complete context-aware model, one also needs a loss function and an optimizer as well.\nXIV\ncorrespond to the dimensions, and the secondary attributes to the attributes of MD.\nThe research of general factorization frameworks is fairly new, but there are some prior work in this direction. Rendle et. al proposed factorization machines (FM; [18]) as a general framework. Its original version is for rating prediction (explicit flavor). Each rating is associated with different attributes, for example the user who rated, the item that was rated, the context of the rating, metadata of the item, etc. It allows all data types that we used in this paper. It is suggested in [18] that the released libFM software is capable of handling implicit feedback through BPR+SGD. There are three main differences between our framework and FM. First, FM uses a restricted MD as its data model, where each dimension must contain exactly one attribute. The unrestricted MD can be simply transformed to this restricted variant by using the attributes as dimensions. For example the model with S = U \u00d7 I and I = Term1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 TermM will become S = U \u00d7 Term1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 TermM . However, this transformation results in many extra dimensions. It also forces modeling interactions between attributes of the same dimension (e.g. interactions between different metadata terms), which is the consequence of fixed model class of FM. However in practice, the attributes of a given dimension are usually either independent (i.e. there is no interaction between them) or one is fully determined by the other (i.e. it is useless to model the interaction). Category hierarchy information is an example for the latter, while partial basket information is for the former. The modeling of insignificant interactions can slow down the learning process drastically[21] and can result in poorer predictive performance. Second, the model class of FM is fixed to full N -wise models (N is set to 2 by default), while our framework is capable of using arbitrary linear models. This can be very important, because (as shown in section 3), some models clearly outperform the pairwise model in the recommendation setting. Third, FM handles the implicit recommendation problems by optimizing for the BPR criteria with an SGD optimizer, while our framework uses a wRMSE based loss function with (approximate) ALS optimizer. And while in the classical 2D setting BPR+SGD clearly outperforms wRMSE+ALS, the latter method seems to be more effective when more complex models and bigger data sets are used (see the quantitative comparison of our framework with FM and BPR+SGD with wRMSE+ALS section 3.1).\nChen et. al proposed another framework, coined SVDFeature, that uses a subset of the FM model [22,23]. Basically it assigns each attribute either to the user or to the item as a property. A feature vector is defined for each property (including the item and the user itself), and the feature vector of the item (or user) is the weighted sum of the feature vectors of its properties. The rating is predicted by the scalar product of these aggregated feature vectors. In other words it uses a partial pairwise model that only keeps the interactions between item and user attributes. The authors claim that doing so the training time decreases drastically compared to FM\u2019s and the interactions dropped are mostly useless (such as interaction between metadata terms of the items). Our experiments in section 3 also show that leaving out useless interactions results in more\nXV\naccurate models. SVDFeature can incorporate either explicit or implicit feedback as it uses a ranking loss function. The model is learned using SGD. The main differences between our framework and SVDFeature are as follows. First, SVDFeature basically uses a 2D data space model S = U \u00d7 I. However there is no restriction on the number of attributes in dimensions U and I. Although some context variables can be assigned to either to the user or to the item part, there are important ones that can not. These contexts are either dependent on both the user and the item or on other properties (such as time) of the transaction. When one considers contexts like these either as a user or as an item context, the model will be consequently restrictive. 15 Since we are using transaction context, a direct quantitative comparison of SVDFeature and our framework is not possible. Second, the model class of SVDFeature is also fixed as consequence of the fixed number of dimensions of the data space model. Third, SVDFeature also uses BPR+SGD for learning."}, {"heading": "6 Discussion & future work", "text": "In this paper we presented a general factorization framework. Compared to prior work, our approach is more flexible, because there is no restriction on the model as long as it is linear. The framework is capable of rating prediction as well as implicit feedback based item ranking through pointwise preference estimation. The framework enables experimentation with different models. The thorough model comparison experiment (Section 3) suggests that there are model classes that usually outperform others: we found that the composite of the basic CF model and additional context dependent user\u2013item prediction models to be the best. We note that this model is not a member of model classes of previously proposed general frameworks.\nWe also proposed an extension to our framework that lifts the restriction on the number of entities in a context dimension corresponding to an event, hence allows us to integrate any context into a single factorization model. Thus the framework fully complies with one of the most extensive data space models for context-aware recommendation problems. We showed that the inclusion of metadata and/or session information can further improve the prediction accuracy significantly.\nWe briefly compared the pointwise preference estimation based learning to a pairwise item ranking approach and found that while the latter clearly outperforms the former in the classic 2D matrix factorization setting; it can fall behind with more complex models on larger data sets. However the two compared methods also differ in the optimization technique, not just in the optimization criteria. This might have some effect on the results. However using SGD for wRMSE based pointwise preference estimation is ineffective due to poor scaling;\n15 E.g. seasonality expresses the knowledge that user A prefers product B in season1 and C in season2; however user-seasonality can only describe that user A likes to buy products in season1 and season2.\nXVI\nand ALS can not be used for BPR, because the nonlinearity in BPR. Therefore future research includes the investigation of ranking based loss functions and whether any of them can be generalized in a way so that it can be used effectively in out framework with ALS optimization."}], "references": [{"title": "Content-based recommender systems: State of the art and trends", "author": ["P. Lops", "M. Gemmis", "G. Semeraro"], "venue": "Recommender Systems Handbook. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Advances in Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Recommending new movies: Even a few ratings are more valuable than metadata", "author": ["I. Pil\u00e1szy", "D. Tikk"], "venue": "Recsys\u201909: ACM Conf. on Recommender Systems.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Advances in collaborative filtering", "author": ["Y. Koren", "R. Bell"], "venue": "In Ricci, F., et al., eds.: Recommender Systems Handbook. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["R. Bell", "Y. Koren"], "venue": "ICDM\u201907: IEEE Int. Conf. on Data Mining.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Major components of the Gravity recommendation system", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "B. N\u00e9meth", "D. Tikk"], "venue": "SIGKDD Explor. Newsl. 9", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Advances in Neural Information Processing Systems 20. MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Context-aware recommender systems", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Recsys\u201908: ACM Conf. on Recommender Systems.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "UAI \u201909: 25 Conf. on Uncertainty in Artificial Intelligence.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback", "author": ["B. Hidasi", "D. Tikk"], "venue": "Proc. of the ECML-PKDD, Part II. Number 7524 in LNCS. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "ICDM-08: IEEE Int. Conf. on Data Mining.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling and learning context-aware recommendation scenarios using tensor decomposition", "author": ["H. Wermser", "A. Rettinger", "V. Tresp"], "venue": "ASONAM\u201911: IEEE Int. Conf. on Advances in Social Networks Analysis and Mining.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Applications of the conjugate gradient method for implicit feedback collaborative filtering", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "D. Tikk"], "venue": "RecSys\u201911: ACM Conf. on Recommender Systems.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-aware recommendations from implicit data via scalable tensor factorization", "author": ["B. Hidasi", "D. Tikk"], "venue": "ArXiv e-prints", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Analysis of cold-start recommendations in iptv systems", "author": ["P. Cremonesi", "R. Turrin"], "venue": "Proc. of the 2009 ACM Conference on Recommender Systems.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Music Recommendation and Discovery in the Long Tail", "author": ["O. Celma"], "venue": "Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["S. Rendle", "L. Schmidt-Thieme"], "venue": "WSDM\u201910: ACM Int. Conf. on Web Search and Data Mining.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 3(3)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving regularized singular value decomposition for collaborative filtering", "author": ["A. Paterek"], "venue": "Proc. of KDD Cup and Workshop. Volume 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Incorporating contextual information in recommender systems using a multidimensional approach", "author": ["G. Adomavicius", "R. Sankaranarayanan", "S. Sen", "A. Tuzhilin"], "venue": "ACM Trans. Inf. Syst. 23(1)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Social network and click-through prediction with factorization machines", "author": ["S. Rendle"], "venue": "Proc. of the KDD Cup and Workshop.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature-based matrix factorization", "author": ["T. Chen", "Z. Zheng", "Q. Lu", "W. Zhang", "Y. Yu"], "venue": "CoRR abs/1109.2271", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "SVDFeature: A toolkit for feature-based collaborative filtering", "author": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yu"], "venue": "Journal of Machine Learning Research 13", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For modeling user preferences, classical approaches either use item metadata (content based filtering, CBF; [1]), or user\u2013 item interactions (collaborative filtering, CF; [2]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "For modeling user preferences, classical approaches either use item metadata (content based filtering, CBF; [1]), or user\u2013 item interactions (collaborative filtering, CF; [2]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "CF algorithms proved to be more accurate than CBF methods, if sufficient interaction data (or events) is available [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Latent factor based CF methods gained popularity due to their attractive accuracy and scalability [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": ", alternating least squares (ALS; [5]), stochastic gradient [6], or a probabilistic framework [7].", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": ", alternating least squares (ALS; [5]), stochastic gradient [6], or a probabilistic framework [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": ", alternating least squares (ALS; [5]), stochastic gradient [6], or a probabilistic framework [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "The dualistic user\u2013item based modeling concept can be extended by considering additional information that may influence the user preferences at recommendation; such data are together termed contextual information, or briefly context [8].", "startOffset": 233, "endOffset": 236}, {"referenceID": 8, "context": "The contribution of this paper is threefold: (1) we show that arbitrary linear feature models can be learnt efficiently using an ALS based pointwise preference estimation; (2) we evaluate various factorization models in this flexible framework to identify those that solve the implicit feedback based context aware recommendation task accurately; (3) we compare the pointwise preference estimation, with the Bayesian Personalized Ranking (BPR; [9]); a pairwise learning strategy) and show that while BPR clearly outperforms pointwise preference estimation in the classical 2D MF setting, it seems to fall behind with more complex models on bigger data sets.", "startOffset": 444, "endOffset": 447}, {"referenceID": 9, "context": "iTALS, the context-aware tensor factorization model [10] predicts the preference", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "The presented learning method generalizes the works of [10] and the concept introduced in [11].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "The presented learning method generalizes the works of [10] and the concept introduced in [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "3 A pairwise ranking method, namely BPR based [9] optimization with SGD was also implemented, but the ALS method is preferred for training.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "The description of the BPR+SGD learning is omitted here, but can be reconstructed from [9,12].", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "The description of the BPR+SGD learning is omitted here, but can be reconstructed from [9,12].", "startOffset": 87, "endOffset": 93}, {"referenceID": 12, "context": "conjugate gradient, [13,14]) can be used to get the new value of the feature vector.", "startOffset": 20, "endOffset": 27}, {"referenceID": 13, "context": "conjugate gradient, [13,14]) can be used to get the new value of the feature vector.", "startOffset": 20, "endOffset": 27}, {"referenceID": 12, "context": "CG was used as an approximation for the naive LS solver in ALS based MF in [13] and for three way tensor factorization by [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "CG was used as an approximation for the naive LS solver in ALS based MF in [13] and for three way tensor factorization by [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "Two of them (Grocery and VoD) are proprietary, the others (TV1, TV2, [15] and LastFM 1K, [16]) are publicly available; see also Table 1.", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "Two of them (Grocery and VoD) are proprietary, the others (TV1, TV2, [15] and LastFM 1K, [16]) are publicly available; see also Table 1.", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "Two context dimension are used: (1) seasonality and (2) sequentiality [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "Both models were already suggested for context-aware modeling (see [10,17]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 16, "context": "Both models were already suggested for context-aware modeling (see [10,17]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 17, "context": "The pairwise model (with a single context) can be further extended with additional contexts: either to a full pairwise model\u2014identical with the model of Factorization Machines (FM; [18])\u2014or to the same model without the SQ member.", "startOffset": 181, "endOffset": 185}, {"referenceID": 8, "context": "BPR+SGD clearly outperforms the wRMSE+ALS learning on the basic CF model (UI); stated also in [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 18, "context": "Based on the NSVD1 approach [19] let us use M (j) = XW (j) where the columns of M (j) \u2208 Rj are the primary features, those of X \u2208 Rj are the secondary features and W (j) \u2208 Rji is the sparse mixing matrix that connects the secondary and primary features, i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "Then one can apply two-phase learning of the secondary features [3] for efficiency.", "startOffset": 64, "endOffset": 67}, {"referenceID": 19, "context": "The most extensive data space model is the multidimensional data model (MD) [20] and most of the other models are restricted MD variants.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "al proposed factorization machines (FM; [18]) as a general framework.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "It is suggested in [18] that the released libFM software is capable of handling implicit feedback through BPR+SGD.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "The modeling of insignificant interactions can slow down the learning process drastically[21] and can result in poorer predictive performance.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "al proposed another framework, coined SVDFeature, that uses a subset of the FM model [22,23].", "startOffset": 85, "endOffset": 92}, {"referenceID": 22, "context": "al proposed another framework, coined SVDFeature, that uses a subset of the FM model [22,23].", "startOffset": 85, "endOffset": 92}], "year": 2017, "abstractText": "General feature based solutions are emerging in the field of recommender systems with the increased need to incorporate multiple sources of information into a single model. The common property of these approaches is that they use a fixed factorization model class that can be extended to handle arbitrary number of context dimensions; the model is then learnt using different optimization techniques. In this paper we propose a general framework in which arbitrary linear feature models can be learnt efficiently. Moreover, both the factorization model and the model members are unrestricted in the framework, thus it is more flexible than state-of-the-art general feature based solutions. The framework allows for both implicit feedback based item ranking and rating prediction from explicit feedback. The paper focuses on the implicit feedback based recommendation problems, due to its expansive use in practical systems compared to explicit feedback. Using the flexibility of the framework we (1) evaluate various factorization models using 5 implicit feedback data sets and injecting contextual information into the model; (2) identify models that can solve the implicit feedback based context-aware recommendation task better than previously proposed model classes. Advantages and drawbacks of various models and learning strategies are also discussed briefly.", "creator": "LaTeX with hyperref package"}}}