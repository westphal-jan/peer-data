{"id": "1401.4609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Computing All-Pairs Shortest Paths by Leveraging Low Treewidth", "abstract": "we present completely two new and efficient complexity algorithms for computing all - pairs by shortest sequence paths. first the algorithms operate on truncated directed regular graphs with real ( possibly negative ) weights. they then make instead use profiles of directed length path tree consistency operators along towards a smaller vertex direction ordering d. both algorithms run concurrent in o ( and n ^ 2 w _ np d ) grid time, specifically where w _ d is the graph cache width only induced indirectly by interpreting this lower vertex ordering. for graphs of constant treewidth, this yields o ( 2 n ^ 0 2 ) time, though which is largely optimal. on chordal graphs, but the algorithms run in o ( nm ) spatial time.. in addition, we also present a variant that exploits graph separators to arrive purely at selecting a reasonable run time of o ( n w _ } d ^ 3d 2 + 9 n ^ 2 s _ d ) computed on general polynomial graphs, where each s _ d reads andlt = 16 w _ d is satisfying the exceptional size capability of imagining the largest globally minimal database separator vertices induced by following the vertex position ordering query d. we show empirically shown that on some both freely constructed and realistic benchmarks, in many rare cases the constrained algorithms outperform floyd - warshalls task as well j as johnsons algorithm, which represent the apparently current technological state of probably the art with a run time of o ( ^ n ^ np 3 ) and o ( q nm + > n ^ 2 log n ), respectively. our algorithms can mainly be used widely for implementing spatial and computed temporal reasoning, such systems as implementations for satisfying the locally simple temporal reconstruction problem, solutions which partly underlines their relevance positively to the planning and scheduling community.", "histories": [["v1", "Sat, 18 Jan 2014 21:23:48 GMT  (2251kb)", "http://arxiv.org/abs/1401.4609v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI", "authors": ["l\\'eon r planken", "mathijs m de weerdt", "roman p j van der krogt"], "accepted": false, "id": "1401.4609"}, "pdf": {"name": "1401.4609.pdf", "metadata": {"source": "META", "title": "Computing All-Pairs Shortest Paths by Leveraging Low Treewidth", "authors": ["L\u00e9on Planken", "Mathijs de Weerdt", "Roman van der Krogt"], "emails": ["l.r.planken@tudelft.nl", "m.m.deweerdt@tudelft.nl", "roman@4c.ucc.ie"], "sections": [{"heading": null, "text": "We present two new and efficient algorithms for computing all-pairs shortest paths. The algorithms operate on directed graphs with real (possibly negative) weights. They make use of directed path consistency along a vertex ordering d. Both algorithms run in O ( n2wd ) time, where wd is the graph width induced by this vertex ordering. For graphs of constant treewidth, this yields O ( n2 )\ntime, which is optimal. On chordal graphs, the algorithms run in O (nm) time. In addition, we present a variant that exploits graph separators to arrive at a run time of O ( nw2d + n 2sd )\non general graphs, where sd \u2264 wd is the size of the largest minimal separator induced by the vertex ordering d. We show empirically that on both constructed and realistic benchmarks, in many cases the algorithms outperform Floyd\u2013Warshall\u2019s as well as Johnson\u2019s algorithm, which represent the current state of the art with a run time of O ( n3 ) and O ( nm+ n2 log n ) , respectively. Our algorithms can be used for spatial and temporal reasoning, such as for the Simple Temporal Problem, which underlines their relevance to the planning and scheduling community."}, {"heading": "1. Introduction", "text": "Finding shortest paths is an important and fundamental problem in communication and transportation networks, circuit design, bioinformatics, Internet node traffic, social networking, and graph analysis in general\u2014e.g. for computing betweenness (Girvan & Newman, 2002)\u2014and is a sub-problem of many combinatorial problems, such as those that can be represented as a network flow problem. In particular, in the context of planning and scheduling, finding shortest paths is important to solve a set of binary linear constraints on events, i.e. the Simple Temporal Problem (STP; Dechter, Meiri, & Pearl, 1991). The STP in turn appears as a sub-problem to the NP-hard Temporal Constraint Satisfaction Problem (TCSP; Dechter et al., 1991) and Disjunctive Temporal Problem (DTP; Stergiou & Koubarakis, 2000), which are powerful enough to model e.g. job-shop scheduling problems. The shortest path computations in these applications can account for a significant part of the total run time of a solver. Thus, it is hardly surprising that these topics have received substantial interest in the planning and scheduling community (Satish Kumar, 2005; Bresina, Jo\u0301nsson, Morris, & Rajan, 2005; Rossi, Venable, & Yorke-Smith, 2006; Shah & Williams, 2008; Conrad, Shah, & Williams, 2009).\nc\u00a92012 AI Access Foundation. All rights reserved.\nInstances of the STP, called Simple Temporal Networks (STNs), have a natural representation as directed graphs with real edge weights. Recently, there has been specific interest in STNs stemming from hierarchical task networks (HTNs; Castillo, Ferna\u0301ndez-Olivares, & Gonza\u0301lez, 2006; Bui & Yorke-Smith, 2010). These graphs have the \u201csibling-restricted\u201d property: each task, represented by a pair of vertices, is connected only to its sibling tasks, its parent or its children. In these graphs the number of children of a task is restricted by a constant branching factor, and therefore the resulting STNs also have a tree-like structure.\nThe canonical way of solving an STP instance (Dechter et al., 1991) is by computing all-pairs shortest paths (APSP) on its STN, thus achieving full path consistency. For graphs with n vertices and m edges, this can be done in O ( n3 )\ntime with the Floyd\u2013Warshall algorithm (Floyd, 1962), based on Warshall\u2019s (1962) formulation of efficiently computing the transitive closure of Boolean matrices. However, the state of the art for computing APSP on sparse graphs is an algorithm based on the technique originally proposed by Johnson (1977), which does some preprocessing to allow n runs of Dijkstra\u2019s (1959) algorithm. Using a Fibonacci heap (Fredman & Tarjan, 1987), the algorithm runs in O ( n2 log n+ nm ) time. In the remainder of this paper, we refer to this algorithm as Johnson.\nIn this paper we present two new algorithms for APSP with real edge weights (in Section 3). One algorithm, dubbed Chleq\u2013APSP, is based on a point-to-point shortest path algorithm by Chleq (1995); the other, named Snowball, is similar to Planken, de Weerdt, and van der Krogt\u2019s (2008) algorithm for enforcing partial (instead of full) path consistency (P3C). These new algorithms advance the state of the art in computing APSP. In graphs of constant treewidth, such as sibling-restricted STNs based on HTNs with a constant branching factor, the run time of both algorithms is bounded by O ( n2 ) , which is\noptimal since the output is \u0398 ( n2 ) . In addition to these STNs, examples of such graphs of constant treewidth are outerplanar graphs, graphs of bounded bandwidth, graphs of bounded cutwidth, and series-parallel graphs (Bodlaender, 1986).\nWhen Chleq\u2013APSP and Snowball are applied to chordal graphs, they have a run time of O (nm), which is a strict improvement over the state of the art (Chaudhuri & Zaroliagis, 2000, with a run time of O ( nmw2d ) ; wd is defined below). Chordal graphs are an important subset of general sparse graphs: interval graphs, trees, k-trees and split graphs are all special cases of chordal graphs (Golumbic, 2004). Moreover, any graph can be made chordal using a so-called triangulation algorithm. Such an algorithm operates by eliminating vertices one by one, connecting the neighbours of each eliminated vertex and thereby inducing cliques in the graph.\nThe induced width wd of the vertex ordering d is defined to be equal to the cardinality of the largest such set of neighbours encountered. The upper bound of the run time of both proposed algorithms on these general graphs, O ( n2wd ) , depends on this induced width. Finding a vertex ordering of minimum induced width, however, is an NP-hard problem (Arnborg, Corneil, & Proskurowski, 1987). This minimum induced width is the tree-likeness property of the graph mentioned above, i.e. the treewidth, denoted w\u2217. In contrast, the induced width is not a direct measure of the input (graph), so the bound of O ( n2wd ) is not quite proper. Still, it is better than the bound on Johnson if wd \u2208 o (log n).1\n1. We prefer to write x \u2208 o (f(n)) instead of the more common x = o (f(n)). Formally, the right-hand side represents the set of all functions that grow strictly slower than f(n), and the traditional equality in fact only works in one direction (see also Graham, Knuth, & Patashnik, 1989, Section 9.2).\nTo see this, note that the bound on Johnson is never better than O ( n2 log n ) , regardless of the value of m.\nIn this paper, we also present a variant of Snowball that exploits graph separators and attains an upper bound on the run time of O ( nw2d + n 2sd ) . This upper bound is even better than the one for the two other new algorithms, since sd \u2264 wd is the size of the largest minimal separator induced by the vertex ordering d. While theoretical bounds on the run time usually give a good indication of the performance of algorithms, we see especially for this last variant that they do not always predict which algorithm is best in which settings. In Section 4, therefore, we experimentally establish the computational efficiency of the proposed algorithms on a wide range of graphs, varying from random scale-free networks and parts of the road network of New York City, to STNs generated from HTNs and job-shop scheduling problems.\nBelow, we first give a more detailed introduction of the required concepts, such as induced width, chordal graphs and triangulation, after which we present the new algorithms and their analysis."}, {"heading": "2. Preliminaries", "text": "In this section, we briefly introduce the algorithm that enforces directed path consistency (DPC) and how to find a vertex ordering required for this algorithm. We then present our algorithms for all-pairs shortest paths, all of which require enforcing DPC (or a stronger property) as a first step. In our treatment, we assume the weights on the edges in the graph are real and possibly negative."}, {"heading": "2.1 Directed Path Consistency", "text": "Dechter et al. (1991) presented DPC, included here as Algorithm 1, as a way check whether an STP instance is consistent.2 This is equivalent to checking that the graph does not contain a negative cycle (a closed path with negative total weight). The algorithm takes as input a weighted directed graph G = \u3008V,E\u3009 and a vertex ordering d, which is a bijection between V and the natural numbers {1, . . . , n}. In this paper, we simply represent the ith vertex in such an ordering as the natural number i. The (possibly negative) weight on the arc from i to j is represented as wi\u2192j \u2208 R. Our shorthand for the existence of an arc between these vertices, in either direction, is {i, j} \u2208 E. Finally, we denote by Gk the graph induced on vertices {1, . . . , k}; likewise, for a set of vertices V \u2032 \u2286 V , GV \u2032 denotes the graph induced on V \u2032. So, in particular, GV = Gn = G.\nIn iteration k, the algorithm adds edges (in line 5) between all pairs of lower-numbered neighbours i, j of k, thus triangulating the graph. Moreover, in lines 3 and 4, it updates the edge between i and j with the weight of the paths i\u2192 k \u2192 j and j \u2192 k \u2192 i, if shorter. Consequently, for i < j, a defining property of DPC is that it ensures that wi\u2192j is no higher than the total weight of any path from i to j that consists only of vertices outside Gj (except for i and j themselves). This implies in particular that after running DPC, w1\u21922 and w2\u21921 are labelled by the shortest paths between vertices 1 and 2.\n2. Note that other algorithms\u2014such as Bellman\u2013Ford\u2014can be used for this purpose as well, and usually perform better in practice.\nAlgorithm 1: DPC (Dechter et al., 1991)\nInput: Weighted directed graph G = \u3008V,E\u3009; vertex ordering d : V \u2192 {1, . . . , n} Output: DPC version of G, or inconsistent if G contains a negative cycle\n1 for k \u2190 n to 1 do 2 forall i < j < k such that {i, k} , {j, k} \u2208 E do 3 wi\u2192j \u2190 min {wi\u2192j , wi\u2192k + wk\u2192j} 4 wj\u2192i \u2190 min {wj\u2192i, wj\u2192k + wk\u2192i} 5 E \u2190 E \u222a {{i, j}} 6 if wi\u2192j + wj\u2192i < 0 then 7 return inconsistent 8 end\n9 end\n10 end\n11 return G = \u3008V,E\u3009\nThe run time of DPC depends on a measure wd called the induced width relative to the ordering d of the vertices. Dechter et al. (1991) define this induced width of a vertex ordering d procedurally to be exactly the highest number of neighbours j of k with j < k encountered during the DPC algorithm. This includes neighbours in the original graph (i.e. {j, k} \u2208 E) as well as vertices that became neighbours through edges added during an earlier iteration of the algorithm. However, the definition can be based on just the original graph and the vertex ordering, by making use of the following result.\nProposition 1. Suppose that G = \u3008V,E\u3009 is an undirected graph and d : V \u2192 {1, . . . , n} (where d is a bijection) is a vertex ordering. Suppose further that we are given n sets of edges E\u2032k for 1 \u2264 k \u2264 n, defined as follows:\nE\u2032k = { {j, k} \u2286 V | j < k \u2227 \u2203path from k to j in G{j}\u222a{k,k+1,...,n} } Then, E\u2032k is exactly the set of edges visited during iteration k of DPC.\nProof. Note that by definition, each set E\u2032k is a superset of the original edges between vertex k and its lower-numbered neighbours. We use this fact to prove the equivalence by induction.\nThe equivalence holds for the first iteration k = n, because E\u2032n is exactly the set of original edges between vertex n and its lower-numbered neighbours, and there are no earlier iterations during which DPC may have added other edges {j, k} with j < k. Now, assuming that the equivalence holds for all sets E\u2032` with ` > k, we show that it also holds for E \u2032 k. For this inductive case, we prove both inclusion relations separately.\n(\u2287) To reach a contradiction, assume that there exists some edge {j, k} 6\u2208 E\u2032k, with j < k, which is visited by DPC during iteration k. Because E\u2032k includes the original edges between k and lower-numbered neighbours, this must be a new edge added during some earlier iteration ` > k, so there must exist edges {j, `} , {k, `} \u2208 E\u2032`. By the induction hypothesis, j and k are therefore connected in the induced subgraph G{j,k}\u222a{`,`+1,...,n}. But then they\nmust also be connected in the larger subgraph G{j}\u222a{k,k+1,...n} and thus by definition be included in E\u2032k: a contradiction.\n(\u2286) Assume, again for reaching a contradiction, that there exists some edge {j, k} \u2208 E\u2032k not part of E during iteration k of DPC and therefore not visited by the algorithm. Clearly, {j, k} cannot have been one of the original edges. By definition of E\u2032k there must therefore exist a path with at least one intermediate vertex from j to k in the induced subgraph G{j}\u222a{k,k+1,...n}. Let ` be the lowest-numbered vertex other than j and k on this path; we have that ` > k > j. Then, by the induction hypothesis, there must exist edges {j, `} , {k, `} \u2208 E\u2032`, both of which were visited by DPC during iteration `. Once more, we reach a contradiction, since DPC must have added {j, k} to E during iteration ` > k.\nWe now formally define the induced width as follows, and conclude with Proposition 1 that this is equivalent to the original procedural definition.\nDefinition 1. Given an undirected graph G = \u3008V,E\u3009, a vertex ordering d, and n sets of edges E\u2032k as in Proposition 1, the induced width wd of G (relative to d) is the following measure:\nwd = max k\u2208V \u2223\u2223E\u2032k\u2223\u2223 It follows that the run time of DPC is not a property of the graph per se; rather, it is dependent on both the graph and the vertex ordering used. With a careful implementation, DPC\u2019s time bound is O ( nw2d ) if this ordering is known beforehand.\nThe edges added by DPC are called fill edges and make the graph chordal (sometimes also called triangulated). Indeed, DPC differs from a triangulation procedure only by its manipulation of the arc weights. In a chordal graph, every cycle of length four or more has an edge joining two vertices that are not adjacent in the cycle. By Definition 1, the number of edges in such a chordal graph, denoted by mc \u2265 m, is O (nwd). We now give the formal definitions of these concepts.\nDefinition 2. Given a graph G = \u3008V,E\u3009 and a set { v1, v2, . . . , vk } \u2286 V of vertices that form a cycle in G, a chord of this cycle is an edge between non-adjacent vertices in this cycle, i.e. an edge { vi, vj } \u2208 E with 1 < j\u2212 i < k\u22121. A graph G = \u3008V,E\u3009 is called chordal if all cycles of size larger than 3 have a chord.\nDefinition 3. Given a graph G = \u3008V,E\u3009, a triangulation T of G, with T \u2229 E = \u2205, is a set of edges such that G\u2032 = \u3008V,E \u222a T \u3009 is chordal. These edges are called fill edges. T is a minimal triangulation of G if there exists no proper subset T \u2032 \u2282 T such that T \u2032 is a triangulation of G."}, {"heading": "2.2 Finding a Vertex Ordering", "text": "In principle, DPC can use any vertex ordering to make the graph both chordal and directionally path-consistent. However, since the vertex ordering defines the induced width, it directly influences the run time and the number of edges mc in the resulting graph. As mentioned in the introduction, finding an ordering d with minimum induced width wd = w\n\u2217, and even just determining the treewidth w\u2217, is an NP-hard problem in general. Still, the class of constant-treewidth graphs can be recognised, and optimally triangulated, in O (n)\ntime (Bodlaender, 1996). If G is already chordal, we can find a perfect ordering (resulting in no fill edges) in O (m) time, using e.g. maximal cardinality search (MCS; Tarjan & Yannakakis, 1984). This perfect ordering is also called a simplicial ordering, because every vertex k together with its lower-numbered neighbours in the ordering induces a clique (simplex) in the subgraph Gk. This implies the following (known) result, relating induced width and treewidth to the size of the largest clique in G.\nProposition 2. If a graph G is chordal, the size of its largest clique is exactly w\u2217+ 1. If a non-chordal graph G is triangulated along a vertex ordering d, yielding a chordal graph G\u2032, the size of the largest clique in G\u2032 is exactly wd + 1. The treewidth of G\n\u2032 equals wd and is an upper bound for the treewidth of the original graph G: w\u2217 \u2264 wd.\nFor general graphs, various heuristics exist that often produce good results. We mention here the minimum degree heuristic (Rose, 1972), which in each iteration chooses a vertex of lowest degree. Since the ordering produced by this heuristic is not fully known before DPC starts but depends on the fill edges added, an adjacency-list-based implementation will require another O (log n) factor in DPC\u2019s time bound. However, for our purposes in this article, we can afford the comfort of maintaining an adjacency matrix, which yields bounds of O ( n2 + nw2d ) time and O ( n2 ) space."}, {"heading": "3. All-Pairs Shortest Paths", "text": "Even though, to the best of our knowledge, a DPC-based APSP algorithm has not yet been proposed, algorithms for computing single-source shortest paths (SSSP) based on DPC can be obtained from known results in a relatively straightforward manner. Chleq (1995) proposed a point-to-point shortest path algorithm that with a trivial adaptation computes SSSP; Planken, de Weerdt, and Yorke-Smith (2010) implicitly also compute SSSP as part of their IPPC algorithm. These algorithms run in O (mc) time and thus can simply be run once for each vertex to yield an APSP algorithm with O (nmc) \u2286 O ( n2wd ) time complexity. Below, we first show how to adapt Chleq\u2019s algorithm to compute APSP; then, we present a new, efficient algorithm named Snowball that relates to Planken et al.\u2019s (2008) P3C."}, {"heading": "3.1 Chleq\u2019s Approach", "text": "Chleq\u2019s (1995) point-to-point shortest path algorithm was simply called Min\u2013path and computes the shortest path between two arbitrary vertices s, t \u2208 V in a directionally pathconsistent graph G. It is reproduced here as Algorithm 2 and can be seen to run in O (mc) time because each edge is considered at most twice. The shortest distance from the source vertex s is maintained in an array D; the algorithm iterates downward from s to 1 and then upward from 1 to t, updating the distance array when a shorter path is found.\nSince the sink vertex t is only used as a bound for the second loop, it is clear that D actually contains shortest distances between all pairs (s, t\u2032) with t\u2032 \u2264 t. Therefore, we can easily adapt this algorithm to compute SSSP within the same O (mc) time bound by setting t = n and returning the entire array D instead of just D[t]. We call the result Chleq\u2013APSP, included as Algorithm 3, which calls this SSSP algorithm (referred to as Min\u2013paths) n times to compute all-pairs shortest paths in O (nmc) \u2286 O ( nw2d ) time.\nAlgorithm 2: Min\u2013path (Chleq, 1995)\nInput: Weighted directed DPC graph G = \u3008V,E\u3009; (arbitrary) source vertex s and destination vertex t Output: Distance from s to t, or inconsistent if G contains a negative cycle\n1 \u2200i \u2208 V : D[i]\u2190\u221e 2 D[s]\u2190 0 3 for k \u2190 s to 1 do 4 forall j < k such that {j, k} \u2208 E do 5 D[j]\u2190 min {D[j], D[k] + wk\u2192j} 6 end\n7 end 8 for k \u2190 1 to t do 9 forall j > k such that {j, k} \u2208 E do\n10 D[j]\u2190 min {D[j], D[k] + wk\u2192j} 11 end\n12 end\n13 return D[t]\nAlgorithm 3: Chleq\u2013APSP\nInput: Weighted directed graph G = \u3008V,E\u3009; vertex ordering d : V \u2192 {1, . . . , n} Output: Distance matrix D, or inconsistent if G contains a negative cycle\n1 G\u2190 DPC(G, d) 2 return inconsistent if DPC did\n3 for i\u2190 1 to n do 4 D[i][\u2217]\u2190 Min\u2013paths(G, i) 5 end\n6 return D\nAlgorithm 4: Snowball\nInput: Weighted directed graph G = \u3008V,E\u3009; vertex ordering d : V \u2192 {1, . . . , n} Output: Distance matrix D, or inconsistent if G contains a negative cycle\n1 G\u2190 DPC(G, d) 2 return inconsistent if DPC did\n3 \u2200i, j \u2208 V : D[i][j]\u2190\u221e 4 \u2200i \u2208 V : D[i][i]\u2190 0 5 for k \u2190 1 to n do 6 forall j < k such that {j, k} \u2208 E do 7 forall i \u2208 {1, . . . , k \u2212 1} do 8 D[i][k]\u2190 min {D[i][k], D[i][j] + wj\u2192k} 9 D[k][i]\u2190 min {D[k][i], wk\u2192j +D[j][i]}\n10 end\n11 end\n12 end\n13 return D"}, {"heading": "3.2 The Snowball Algorithm", "text": "In this section, we present an algorithm that computes APSP (or full path-consistency), dubbed Snowball and included as Algorithm 4, that has the same asymptotic worst-case time bounds as Chleq\u2013APSP but requires strictly less computational work.\nLike Chleq\u2013APSP, this algorithm first ensures that the input graph is directionally pathconsistent. The idea behind the algorithm is then that we grow, during the execution of the outermost loop, a clique {1, . . . , k} of computed (shortest) distances, one vertex at a time, starting with the trivial clique consisting of just vertex 1; while DPC performed a backward sweep along d, Snowball iterates in the other direction. When adding vertex k to the clique, the two inner loops ensure that we compute the distances between k and all vertices i < k. This works because we know by DPC that for any such pair (i, k), there must exist a shortest path from i to k of the form i \u2192 \u00b7 \u00b7 \u00b7 \u2192 j \u2192 k (and vice versa), such that {j, k} \u2208 E with j < k is an edge of the chordal graph. This means that the algorithm only needs to \u201clook down\u201d at vertices i, j < k, and it follows inductively that D[i][j] and D[j][i] are guaranteed to be correct from an earlier iteration.\nThe name of our algorithm derives from its \u201csnowball effect\u201d: the clique of computed distances grows quadratically during the course of its operation. A small example of the operation of Snowball is given in Figure 1. Originally, the graph contained a shortest path 4\u20137\u20136\u20132\u20135\u20131\u20133. Dashed edges have been added by DPC, and the path 4\u20132\u20131\u20133 is now also a shortest path; in particular, w4\u21922 holds the correct value. This snapshot is taken for k = 4; the shaded vertices 1\u20133 have already been visited and shortest distances D[i][j] have been computed for all i, j \u2264 3. Then, during the iteration k = 4, for j = 2 and i = 3, the algorithm sets the correct weight of D[4][3] by taking the sum w4\u21922 +D[2][3]. Theorem 3. Algorithm 4 ( Snowball) correctly computes all-pairs shortest paths in O (nmc) \u2286 O ( n2wd ) time.\nProof. The proof is by induction. After enforcing DPC, w1\u21922 and w2\u21921 are labelled by the shortest distances between vertices 1 and 2. For k = 2 and i = j = 1, the algorithm then sets D[1][2] and D[2][1] to the correct values.\nNow, assume that D[i][j] is set correctly for all vertices i, j < k. Let \u03c0 : i = v0 \u2192 v1 \u2192 \u00b7 \u00b7 \u00b7 \u2192 v`\u22121 \u2192 v` = k be a shortest path from i to k, and further let hmax = arg maxh\u2208{0,1,...,`} {vh \u2208 \u03c0}. By DPC, if 0 < hmax < `, there exists a path of the same weight where a shortcut vhmax\u22121 \u2192 vhmax+1 is taken. This argument can be repeated to conclude that there must exist a shortest path \u03c0\u2032 from i to k that lies completely in Gk and, except for the last arc, in Gk\u22121. Thus, by the induction hypothesis and the observation that the algorithm considers all arcs from the subgraph Gk\u22121 to k, D[i][k] is set to the correct value. An analogous argument holds for D[k][i].\nWith regard to the algorithm\u2019s time complexity, note that the two outermost loops together result in each of the mc edges in the chordal graph being visited exactly once. The inner loop always has fewer than n iterations, yielding a run time of O (nmc) time. From the observation above that mc \u2264 nwd, we can also state a looser time bound of O ( n2wd ) .\nWe now briefly discuss the consequences for two special cases: graphs of constant treewidth and chordal graphs. For chordal graphs, which can be recognised in O (m) time, we can just substitute m for mc in the run-time complexity; further, as described above, a perfect ordering exists and can be found in O (m) time. This gives the total run-time complexity of O (nm). Likewise, we stated above that for a given constant \u03ba, it can be determined in O (n) time whether a graph has treewidth w\u2217 \u2264 \u03ba, and if so, a vertex ordering d with wd = w\n\u2217 can be found within the same time bound. Then, omitting the constant factor wd, the algorithm runs in O ( n2 )\ntime. This also follows from the algorithm\u2019s pseudocode by noting that every vertex k has a constant number (at most w\u2217) of neighbours j < k.\nWe note here the similarity between Snowball and the P3C algorithm (Planken et al., 2008), presented below. Like Snowball, P3C operates by enforcing DPC, followed by a single backward sweep along the vertex ordering. P3C then computes, in O ( nw2d ) time, shortest\nAlgorithm 5: P 3C (Planken et al., 2008)\nInput: Weighted directed graph G = \u3008V,E\u3009; vertex ordering d : V \u2192 {1, . . . , n} Output: PPC version of G, or inconsistent if G contains a negative cycle\n1 G\u2190 DPC(G, d) 2 return inconsistent if DPC did\n3 for k \u2190 1 to n do 4 forall i, j < k such that {i, k} , {j, k} \u2208 E do 5 wi\u2192k \u2190 min {wi\u2192k, wi\u2192j + wj\u2192k} 6 wk\u2192j \u2190 min {wk\u2192j , wk\u2192i + wi\u2192j} 7 end\n8 end\n9 return G\npaths only for the arcs present in the chordal graph. This similarity and a property of chordal graphs in fact prompt us to present a version of Snowball with improved time complexity."}, {"heading": "3.3 Improving Run-Time Complexity Using Separators", "text": "In this section, we present an improvement of Snowball for an O ( nw2d + n 2sd )\nrun time, where sd is the size of the largest minimal separator in the chordal graph obtained by triangulation along d.\nDefinition 4. Given a connected graph G = \u3008V,E\u3009, a separator is a set V \u2032 \u2286 V such that GV \\V \u2032 is no longer connected. A separator V\n\u2032 is minimal if no proper subset of V \u2032 is a separator.\nThis bound is better because, as seen below, it always holds that sd \u2264 wd. The improvement hinges on a property of chordal graphs called partial path consistency (PPC). In a partially path-consistent graph, each arc is labelled by the length of the shortest path between its endpoints.3 P3C, presented as Algorithm 5, depends on DPC and computes PPC in O ( nw2d ) time, which is the current state of the art. Then, we use a clique tree of the PPC graph to compute the shortest path between all vertices. Figure 2 shows an example of a chordal graph and its associated clique tree. Such a clique tree has the following useful properties (Heggernes, 2006, Section 3.2).\nProperty 1. Every chordal graph G = \u3008V,E\u3009 has an associated clique tree T = \u3008C, S\u3009, which can be constructed in linear time O (mc).\nProperty 2. Each clique tree node c \u2208 C is associated with a subset Vc \u2286 V and induces a maximal clique in G. Conversely, every maximal clique in G has an associated clique tree node c \u2208 C.\nProperty 3. T is coherent: for each vertex v \u2208 V , the clique tree nodes whose associated cliques contain v induce a subtree of T .\n3. Full path-consistency (FPC) is achieved if an arc exists for all pairs of vertices u, v \u2208 V .\nProperty 4. If two clique tree nodes ci, cj \u2208 C are connected by an edge {ci, cj} \u2208 S, Vci \u2229 Vcj is a minimal separator in G. Conversely, for each minimal separator V \u2032 in G, there is a clique tree edge {ci, cj} \u2208 S such that V \u2032 = Vci \u2229 Vcj . Property 5. All vertices appear in at least one clique associated with a node in T , so:\u22c3 c\u2208C Vc = V .\nSince we have by Proposition 2 on page 358 that the size of the largest clique in a chordal graph is exactly wd + 1, it follows from Properties 2 and 4 that sd \u2264 wd. Now, the idea behind Snowball\u2013Separators is to first compute PPC in O ( nw2d ) time using P3C, and then traverse the clique tree. PPC ensures that shortest paths within each clique have been computed. Then, when traversing the clique tree from an arbitrary root node out, we grow a set Vvisited of vertices in cliques whose nodes have already been traversed. For each clique node c \u2208 C visited during the traversal, shortest paths between vertices in the clique Vc and vertices in Vvisited must run through the separator Vsep between c and c\u2019s parent. If sd is the size of the largest minimal separator in G, for each pair of vertices it suffices to consider at most sd alternative routes for a total of O ( n2sd ) routes, yielding the\nstated overall time complexity of O ( nw2d + n 2sd ) . We formally present the algorithm based on this idea as Algorithm 6 with its associated recursive procedure Process\u2013clique\u2013tree\u2013node (on the following page).\nNote that because we visit a node\u2019s parent before visiting the node itself, it always holds that Vcparent \u2286 Vvisited. Further note that, for simplicity of presentation, we assume the graph to be connected. If not, we can simply find all connected components in linear time and construct a clique tree for each of them.\nThe improved algorithm has an edge over the original algorithm when separators are small while the treewidth is not. HTN-based sibling-restricted STNs (which are described as part of our experimental validation in Section 4.3.5), for instance, have many separators of size 2. If every task has as many as O ( \u221a n) subtasks and every task with its subtasks induces a clique, we have wd \u2208 O ( \u221a n) and sd = 2, implying that Snowball\u2013Separators still\nhas an optimal O ( n2 ) time complexity for these instances.4\nBefore we proceed to prove that the algorithm is correct and meets the stated run-time bounds, we introduce the following definition.\n4. However, since in general not every task and its subtasks form a clique, this low value of sd will usually not be attained in practice.\nAlgorithm 6: Snowball\u2013separators\nInput: Weighted directed graph G = \u3008V,E\u3009; vertex ordering d : V \u2192 {1, . . . , n} Output: Distance matrix D, or inconsistent if G contains a negative cycle\n1 G\u2190 P3C(G, d) 2 return inconsistent if P3C did\n3 \u2200i, j \u2208 V : D[i][j]\u2190\u221e 4 \u2200i \u2208 V : D[i][i]\u2190 0 5 \u2200 {i, j} \u2208 E : D[i][j]\u2190 wi\u2192j 6 \u2200 {i, j} \u2208 E : D[j][i]\u2190 wj\u2192i 7 build a clique tree T = \u3008C, S\u3009 of G 8 select an arbitrary root node croot \u2208 C of T 9 (D,Vvisited)\u2190 Process\u2013clique\u2013tree\u2013node(croot, nil, D,\u2205)\n10 return D\nProcedure Process\u2013clique\u2013tree\u2013node(c, cparent, D, Vvisited)\nInput: Current clique tree node c, c\u2019s parent cparent, distance matrix D, set of visited vertices Vvisited Output: Updated matrix D and set Vvisited\n1 if cparent 6= nil then 2 Vnew \u2190 Vc \\ Vcparent 3 Vsep \u2190 Vc \u2229 Vcparent 4 Vother \u2190 Vvisited \\ Vc 5 forall (i, j, k) \u2208 Vnew \u00d7 Vsep \u00d7 Vother do 6 D[i][k]\u2190 min {D[i][k], D[i][j] +D[j][k]} 7 D[k][i]\u2190 min {D[k][i], D[k][j] +D[j][i]} 8 end\n9 end 10 Vvisited \u2190 Vvisited \u222a Vc 11 forall children c\u2032 of c do 12 (D,Vvisited)\u2190 Process\u2013clique\u2013tree\u2013node(c\u2032, c,D, Vvisited) // recursive call 13 end\n14 return (D,Vvisited)\nDefinition 5. We define a distance matrix D as valid for a set U of vertices, and (D,U) as a valid pair, if for all pairs of vertices (i, j) \u2208 U \u00d7 U , D[i][j] holds the shortest distance in G from i to j.\nWe split the correctness proof of the algorithm into three parts: Lemmas 4 and 5 culminate in Theorem 6. The first step is to show that if Process\u2013clique\u2013tree\u2013node is called with a valid pair (D,U) and some clique node c, the procedure extends the validity to U \u222a Vc.\nLemma 4. Consider a call to procedure Process\u2013clique\u2013tree\u2013node with, as arguments, a clique node c, c\u2019s parent cparent, a distance matrix D, and the set of visited vertices Vvisited. If D is valid for Vvisited upon calling, then D becomes valid for Vc \u222a Vvisited after running lines 1\u20138 of Process\u2013clique\u2013tree\u2013node.\nProof. First, note that by Property 2, Vc induces a clique in G. Therefore, edges exist between each pair (i, k) of vertices in Vc, and since the graph is PPC, wi\u2192k is labelled with the shortest distance between i and k. Due to lines 5 and 6 of the main algorithm, D also contains these shortest distances, so D is valid for Vc.\nNow, it remains to be shown that for each pair of vertices (i, k) \u2208 Vc \u00d7 Vvisited the shortest distances D[i][k] and D[k][i] are set correctly. We show here the case for D[i][k]; the other case is analogous.\nThe desired result follows trivially if cparent = nil, since the procedure is then called with Vvisited = \u2205. Otherwise, let Vnew = Vc \\ Vcparent , Vsep = Vc \u2229 Vcparent and Vother = Vvisited \\ Vc as set by the procedure in lines 2\u20134. If either i or k lies in Vsep, the correctness of D[i][k]\u2019s value was already proven, so we only need to consider pairs of vertices (i, k) \u2208 Vnew\u00d7Vother.\nFor any such pair (i, k), Vsep is a separator between i and k by Property 4, so any shortest path from i to k is necessarily a concatenation of shortest paths from i to j\u2217 and from j\u2217 to k, for some j\u2217 \u2208 Vsep. Since it follows from the definitions of Vnew, Vsep and Vother that for all (i, j) \u2208 Vnew \u00d7 Vsep and (j, k) \u2208 Vsep \u00d7 Vother, D[i][j] and D[j][k] are correctly set (by the validity of D for Vc and Vvisited, respectively), the loop on lines 5\u20138 yields the desired result.\nOur next step is to prove that through the recursive calls, validity is in fact extended to the entire subtree rooted at c.\nLemma 5. Consider again a call to procedure Process\u2013clique\u2013tree\u2013node with, as arguments, a clique node c, c\u2019s parent Vcparent, a distance matrix D, and the set of visited vertices Vvisited. If D is valid for Vvisited upon calling, then the returned, updated pair (D\n\u2032, V \u2032visited) is also valid.\nProof. First, note that by Lemma 4, D is valid for Vvisited after the update in line 10.\nAssume that the clique tree has a depth of d; the proof is by reverse induction over the depth of the clique tree node. If c is a clique tree node at depth d (i.e. a leaf), the loop in lines 11\u201313 is a no-op, so we immediately obtain the desired result.\nNow assume that the lemma holds for all nodes at depth k and let c be a clique tree node at depth k \u2212 1. For the first call (if any) made for a child node c\u2032 during the loop in lines 11\u201313, this lemma can then be applied. As a consequence, the returned and updated\npair is again valid. This argument can be repeated until the loop ends and the procedure returns a valid pair.\nWith these results at our disposal, we can state and prove the main theorem of this section.\nTheorem 6. Algorithm 6 ( Snowball\u2013Separators) correctly computes all-pairs shortest paths in O ( nw2d + n 2sd ) time.\nProof. Note that Vvisited = \u2205 for the call to Process\u2013clique\u2013tree\u2013node in line 9 of Snowball\u2013 Separators; therefore, the pair (D,Vvisited) is trivially valid. By Lemma 5, this call thus returns a valid updated pair (D,Vvisited). Since Process\u2013clique\u2013tree\u2013node has recursively traversed the entire clique tree, Vvisited contains the union \u22c3 c\u2208C Vc of all cliques in the clique tree T = \u3008C, S\u3009, which by Property 5 equals the set of all vertices in G. Therefore, D contains the correct shortest paths between all pairs of vertices in the graph.\nAs for the time complexity, note that the initialisations in lines 3 and 4 can be carried out in O ( n2 )\ntime, whereas those in lines 5 and 6 require O (mc) time. By Property 1, the clique tree can be built in linear time O (mc). Since the clique tree contains at most n nodes, Process\u2013clique\u2013tree\u2013node is called O (n) times. Line 1 requires O ( w2d )\ntime. To implement lines 2\u20134 and 10 of Process\u2013clique\u2013tree\u2013node, we represent the characteristic function for Vvisited as an array of size n; using Vvisited instead of Vcparent everywhere, we then we simply iterate over all O (wd) members of Vc to perform the required computations.\nNow, only the complexity of the loop in lines 5\u20138 remains to be shown. Note that |Vsep| \u2264 sd by definition, and |Vother| < n always. Further using the observation that each of the n vertices in the graph appears in Vnew for exactly one invocation of Process\u2013clique\u2013tree\u2013 node (after which it becomes a staunch member of Vvisited), we obtain a total time bound of O ( n2sd ) for the loop over all invocations.\nWhile the recursive description above is perhaps easier to grasp and satisfies the claimed time bounds, we found that efficiency benefited in practice from an iterative implementation. It also turns out that a good heuristic is to first visit child nodes connected to the already visited subtree by a large separator, postponing the processing of children connected by a small separator, because the set of visited vertices is then still small. In this way, the sum of terms |Vsep \u00d7 Vvisited| is kept low. In our implementation, we therefore used a priority queue of clique nodes ordered by their separator sizes. Future research must point out whether it is feasible to determine an optimal traversal of the clique tree within the given time bounds.\nHaving presented our new algorithms and proven their correctness and formal complexity, we now move on to an empirical evaluation of their performance."}, {"heading": "4. Experiments", "text": "We evaluate the two algorithms together with efficient implementations of Floyd\u2013Warshall and Johnson with a Fibonacci heap5 across six different benchmark sets.6\n5. For Johnson we used the corrected Fibonacci heap implementation by Fiedler (2008), since the widely used pseudocode of Cormen, Leiserson, Rivest, and Stein (2001) contains mistakes. 6. Available at http://dx.doi.org/10.4121/uuid:49388c35-c7fb-464f-9293-cca1406edccf\nThe properties of the test cases are summarised in Table 1. This table lists the number of test cases, the range of the number of vertices n, edges m, the induced width wd produced by the minimum degree heuristic, as well as the size of the largest minimal separators sd in the graphs. More details on the different sets can be found below, but one thing that stands out immediately is that sd is often equal to or only marginally smaller than wd. However, the median size of the minimum separator is less than 10 for all instances except the constructed chordal graphs.\nAll algorithms were implemented in Java and went through an intensive profiling phase.7 The experiments were run using Java 1.6 (OpenJDK-1.6.0.b09) in server mode, on Intel Xeon E5430 CPUs running 64-bit Linux. The Java processes were allowed a maximum heap size of 4 GB, and used the default stack size. We report the measured CPU times, including the time that was spent running the triangulation heuristic for Chleq\u2013APSP and Snowball. The reported run times are averaged over 10 runs for each unique problem instance. Moreover, we generated 10 unique instances for each parameter setting, obtained by using different random seeds. Thus, each reported statistic represents an average over 100 runs, unless otherwise indicated. Finally, each graph instance was ensured to contain no cycles of negative weight."}, {"heading": "4.1 Triangulation", "text": "As discussed in Section 2.2, finding an optimal vertex ordering (with minimum induced width) is NP-hard, but several efficient triangulation heuristics for this problem exist. We ran our experiments with six different heuristics: the minimum fill and minimum degree heuristics, static variants of both (taking into account only the original graph), an ordering produced by running maximum cardinality search (MCS) on the original graph, and a random ordering. All of these, except minimum fill, have time complexities within the bound on the run time of Chleq\u2013APSP and Snowball. We found that the minimum degree heuristic gave on average induced widths less than 1.5% higher than those found by minimum fill,\n7. Our implementations are available in binary form at http://dx.doi.org/10.4121/uuid:776a266e-81c6-41ee-9d23-8c89d90b6992\nbut with drastically lower run time. The exorbitant time consumption of the minimum fill heuristic can be partially explained by the fact that we used the LibTW package8 to compute this ordering, whose implementation can probably be improved. However, it is also known from the literature that the theoretical bound on the minimum fill heuristic is worse than that of minimum degree (Kj\u00e6rulff, 1990). All other heuristics are not only slower than minimum degree, but also yield an induced width at least 12% higher, resulting in a longer total triangulation time and a longer total run time of Snowball (see the summary of the results over all benchmarks given in Table 2). Again, this confirms Kj\u00e6rulff\u2019s earlier work. In the experimental results included below we therefore only show the results based on the minimum degree heuristic."}, {"heading": "4.2 Chordal Graphs", "text": "To evaluate the performance of the new algorithms on chordal graphs, we construct chordal graphs of a fixed size of 1,000 vertices with a treewidth ranging from 79 up to just less than the number of vertices, thus yielding a nearly complete graph at the high end. The results of this experiment are depicted in Figure 3. In this, and other figures, the error bars represent the standard deviations in the measured run time for the instances of that size. For graphs up to an induced width of about three quarters of the number of vertices, Snowball significantly outperforms Floyd\u2013Warshall (which yields the expected horizontal line), and overall the run time of both new algorithms is well below that of Johnson across the entire range. Figure 4 shows the run times on chordal graphs of a constant treewidth and with increasing number of vertices. Here, the two new algorithms outperform Johnson by nearly an order of magnitude (a factor 9.3 for Snowball around n = 1300), and even more so regarding Floyd\u2013Warshall, confirming the expectations based on the theoretical upper bounds."}, {"heading": "4.3 General Graphs", "text": "For general, non-chordal graphs, we expect from the theoretical analysis that the O ( nw2d ) -\ntime Chleq\u2013APSP and Snowball algorithms are faster than Johnson with its O ( nm+ n2 log n ) 8. Available from http://treewidth.com/.\ntime bound when wd is low, and that Johnson is faster on sparse graphs (where m is low) of a large induced width wd. The main question is at which induced width this changeover occurs. Regarding Floyd\u2013Warshall with its O ( n3 )\nbound, we expect that for larger n it is always outperformed by the other algorithms."}, {"heading": "4.3.1 Scale-Free Graphs", "text": "Scale-free networks are networks whose degree distribution follows a power law. That is, for large values of k, the fraction P (k) of vertices in the network having k connections to other vertices tends to P (k) \u223c ck\u2212\u03b3 , for some constant c and parameter \u03b3. In other words, few vertices have many connections while many vertices have only a few connections. Such a property can be found in many real-world graphs, such as in social networks and in the Internet. Our instances were randomly generated with Albert and Baraba\u0301si\u2019s (2002) preferential attachment method, where in each iteration a new vertex is added to the graph, which is then attached to a number of existing vertices; the higher the degree of an existing vertex, the more likely it is that it will be connected to the newly added vertex. To see at which induced width Johnson is faster, we compare the run times on such generated graphs with 1,000 vertices. By varying the number of attachments for each new vertex from 2 to n/2, we obtain graphs with an induced width ranging from 88 to 866. In these graphs, the induced width is already quite large for small attachment values: for example, for a value of 11, the induced width is already over 500.\nThe results of this experiment can be found in Figure 5. Here we see that up to an induced width of about 350 (attachment value 5), Snowball is the most efficient. For higher induced widths, Johnson becomes the most efficient; for wd around 800, even Floyd\u2013Warshall becomes faster than Snowball. A consistent observation but from a different angle can be made from Figure 6, where the induced width is between 150 and 200, the number of edges\nis between 2,176 and 3,330 and the number of vertices is varied from 250 to 1,000. Here we see that for small graphs up to 350 vertices, Johnson is the fastest; then Snowball overtakes it, and around 750 vertices Chleq\u2013APSP is also faster than Johnson (this holds for all results up to a sparse graph of 1,000 vertices).\nAround the mark of 750 vertices, the results show a decrease in the run time for both Snowball and Chleq\u2013APSP. This is an artifact of the (preferential attachment) benchmark generator. Since we cannot generate scale-free graphs with a specific induced width, we modify the attachment value instead. As it turns out, for graphs of this size only one attachment value yields an induced width within the desired range; for the graph of size 750, this width is at the high end of the interval, whereas for the graph of size 800 it is near the low end. This explains the reduced run time for the larger graph.\nFor these scale-free networks, we conclude that Snowball is the fastest of the four algorithms when the induced width is not too large (at most one third of the number of vertices in our benchmark set). However, we also observe that the structure of scale-free networks is such that they have a particularly high induced width for relatively sparse graphs, exactly because a few vertices have most of the connections. Therefore, Snowball is most efficient only for relatively small attachment values."}, {"heading": "4.3.2 Selections from New York Road Network", "text": "More interesting than the artificially constructed graphs are graphs based on real networks, for which shortest path calculations are relevant. The first of this series is based on the road network of New York City, which we obtained from the DIMACS challenge website.9 This network is very large (with 264,346 vertices and 733,846 edges) so we decided to compute\n9. http://www.dis.uniroma1.it/~challenge9/\nshortest paths for (induced) subgraphs of varying sizes. These were obtained by running a simple breadth-first search from a random starting location until the desired number of vertices had been visited. The extent of the subnetworks thus obtained is illustrated for three different sizes in Figure 7. The results of all algorithms on these subgraphs can be found in Figure 8. Here we observe the same ranking of the algorithms as on the chordal graphs of a fixed treewidth and for diamonds: Floyd\u2013Warshall is slowest with its \u0398 ( n3 ) run time, then each of Johnson, Chleq\u2013APSP, and Snowball is significantly faster than its predecessor. This can be explained by considering the induced width of these graphs. Even for the largest graphs the induced width is around 30, which is considerably smaller than the number of vertices."}, {"heading": "4.3.3 STNs from Diamonds", "text": "This benchmark set is based on problem instances in difference logic proposed by Strichman, Seshia, and Bryant (2002) and also appearing in the smt-lib (Ranise & Tinelli, 2003), where the constraint graph for each instance takes the form of a circular chain of \u201cdiamonds\u201d. Each such diamond consists of two parallel paths of equal length starting from a single vertex and ending in another single vertex. From the latter vertex, two paths start again, to converge on a third vertex. This pattern is repeated for each diamond in the chain; the final vertex is then connected to the very first one. The sizes of each diamond and the total number of diamonds are varied between benchmarks.\nProblems in this class are actually instances of the NP-complete Disjunctive Temporal Problem (DTP): constraints take the form of a disjunction of inequalities. From each DTP instance, we obtain a STP instance (i.e. a graph) by randomly selecting one inequality from each such disjunction. This STP is most probably inconsistent, so its constraint graph contains a negative cycle; we remedy this by modifying the weights on the constraint edges. The idea behind this procedure is that the structure of the graph still conforms to the type of networks that one might encounter when solving the corresponding DTP instance, and that the run time of the algorithms mostly depends on this structure. Moreover, to reduce the influence of the randomized extraction procedure, we repeat it for 10 different seeds.\nFor our benchmark set, we considered problem instances which had the size of the diamonds fixed at 5 and their number varying. The most interesting property of this set is that the graphs generated from it are very sparse. We ran experiments on 130 graphs, ranging in size from 111 to 2751 vertices, all with an induced width of 2. This induced width is clearly extremely small, which translates into Chleq\u2013APSP and Snowball being considerably faster than Johnson and Floyd\u2013Warshall, as evidenced by Figure 9."}, {"heading": "4.3.4 STNs from Job-Shop Scheduling", "text": "We generated each of the 400 graphs in our \u201cjob-shop\u201d set from an instance of a real jobshop problem. These instances were of the type available in smt-lib (Ranise & Tinelli, 2003), but of a larger range than included in that benchmark collection. To obtain these graphs from the job-shop instances, we again used the extraction procedure described in the previous section. The most striking observation that can be taken from Figure 10 is that the difference between Johnson and the two new algorithms is not quite as pronounced, though Snowball is consistently the fastest of the three by a small margin. The fact that this\nmargin is so small is most likely due to the structure of these graphs, which is also reflected in their relatively high induced width. Note also that the run times for Floyd\u2013Warshall are better for graphs of up to 160 vertices, while for larger graphs the other algorithms are significantly faster."}, {"heading": "4.3.5 STNs from HTNs", "text": "Finally, we consider a benchmark set whose instances imitate so-called sibling-restricted STNs originating from Hierarchical Task Networks. This set is therefore particularly interesting from a planning point of view. In these graphs, constraints may occur only between parent tasks and their children, and between sibling tasks (Bui & Yorke-Smith, 2010). We consider an extension that includes landmark variables (Castillo, Ferna\u0301ndez-Olivares, & Gonza\u0301lez, 2002) that mimic synchronisation between tasks in different parts of the network, and thereby cause some deviation from the tree-like HTN structure. We generate HTNs using the following parameters: (i) the number of tasks in the initial HTN tree (fixed at 250; note that tasks have a start and end point), (ii) the branching factor, determining the number of children for each task (between 4 and 7), (iii) the depth of the HTN tree (between 3 and 7), (iv) the ratio of landmark time points to the number of tasks in the HTN, varying from 0 to 0.5 with a step size of 0.05, and (v) the probability of constraints between siblings, varying from 0 to 0.5 with a step size of 0.05.\nThese settings result in graphs of between 500 and 625 vertices, with induced widths varying between 2 and 128. Though the induced width seems high in light of our claim above that it is constant, we verified that wd \u2264 2 \u00d7 branching factor + #landmarks + 1 for all instances. Filling in the maximal values of 7 and 125 respectively, we find an upper bound wd \u2264 140, well above the actual maximum encountered.\nFigure 11 shows the results of these experiments as a function of the induced widths of the graphs. We can see that only for the larger induced widths, Johnson and Chleq\u2013APSP come close. These large induced widths are only found for high landmark ratios of 0.5. The results indicate that for the majority of STNs stemming from HTNs, Snowball is significantly more efficient than Johnson."}, {"heading": "4.4 Snowball\u2013Separators", "text": "In Section 3.3 we presented a version of Snowball that has an improved worst-case run time over vanilla Snowball by taking advantage of the separators in the graph. In this section, we discuss the results of our experiments comparing these two variants. First, we turn our attention to the benchmark problems on regular graphs. Our results are summarised in Figure 12. As one can see, Snowball\u2013Separators actually performs strictly worse on these sets in terms of run-time performance when compared to the original Snowball.\nHowever, as can be seen in Table 1, the largest minimal separator is often equal to or only marginally smaller than the induced width. Even though there may be only few separators this large, and many may be substantially smaller (as noted above, for most instances the median separator size is below 10), this prompts us to run experiments on instances where separator sizes are artificially kept small. Indeed, we found that there are cases where Snowball\u2013Separators shows an improvement over vanilla Snowball when comparing the number of update operations performed\u2014i.e. lines 8 and 9 of Snowball and lines 6 and 7\nin Process\u2013clique\u2013tree\u2013node, along with lines 3 and 4 of DPC and lines 5 and 6 of P3C. One such case is presented in Figure 13. This describes the results on a collection of chordal graphs of 512 vertices, in which the largest minimal separator is fixed at size 2, and the treewidth is varied between 16 and 448. The figure also includes the results of DPC and P3C, as these are the respective subroutines of Snowball and Snowball\u2013Separators. For these graphs, Snowball\u2013Separators performs strictly fewer update operations than Snowball on all instances, although the difference becomes smaller as the induced width increases. While the number of updates shows a distinct improvement over Snowball, the run times of the Snowball\u2013Separators algorithms do not show the same improvement. Instead, as can be seen from Figure 14, the run times of Snowball are strictly better than those of Snowball\u2013Separators on all instances. Snowball can now even be seen to outperform P3C which has a better theoretical bound; the reason is that the adjacency matrix data structure as used by Snowball is very fast, while the adjacency list used by P3C, though staying within the theoretical bound, inflicts a larger constant factor on the run time.\nFrom these experiments, we can conclude that on graphs of these sizes, the additional bookkeeping required by Snowball\u2013Separators outweighs the potential improvement in the number of distance matrix updates."}, {"heading": "4.5 A Proper Upper Bound on the Run Time", "text": "On general graphs, the run time of the proposed algorithms depends on the induced width wd of the ordering produced by the triangulation heuristic. This induced width is not a direct measure of the input (graph), so the given upper bound on the run time is not quite proper. To arrive at a proper bound, in this section we aim to relate the run time to the treewidth, denoted w\u2217, which is a property of the input. However, determining the treewidth, an\nNP-hard problem, is an intractable task for the benchmark problems we used. We therefore compare the measured induced width wd \u2265 w\u2217, an upper bound on the treewidth, to a lower bound x \u2264 w\u2217.10 We are unaware of any guarantee on the quality relative to the treewidth of either the minimum degree triangulation heuristic or the lower bound we used. However, we can calculate the ratio wd/x to get an upper bound on the ratio wd/w\n\u2217. From this measure we can then obtain an upper bound on the run time expressed in the treewidth, at least for the benchmark problems in this paper.\nThe results of these computations can be found in Figure 15, where we plot these ratios for the New York, HTN, scale-free and job-shop benchmarks as a function of the lower bound x. Using a least-squares approach, we then fitted functions wd(x) = cx\nk (showing up as a straight line in this log-log plot) to the plotted data points. For functions found by fitting, we get k = 4.6 for New York, k = 2.3 for HTN, and k = 0.98 for job-shop, all with small multiplicative constants 0.012 < c < 1.62. As one can see from the plotted data points for the scale-free instances, they are not amenable to such a fit and we therefore omit it from the figure.\nThe decreasing trend for the job-shop data indicates that the quality of the triangulation (i.e. of the upper bound represented by the induced width) gradually increases: the lower and upper bound are always less than a factor 2 apart. Indeed, if we plot a line representing a function w\u2032d(x) = 2x (yielding a horizontal line in this figure), we find that it describes a comfortable upper bound on the data points for this benchmark set.\nThe HTN data prompts us to plot a function w\u2032\u2032d(x) = 2 5x 2.5, with an exponent slightly higher than the one we found from the least-squares fit, and further tweaked slightly by a\n10. The lower bound was computed with the LibTW package; see http://treewidth.com/. We used the MMD + Least-c heuristic.\nmultiplicative coefficient to bring it into view. This function as plotted represents an ample upper bound for the HTN benchmarks (as well as the job-shop ones).\nThe fit for the data points for the New York benchmark is not good and the trend of the points themselves is not very clear, because the lower bound only spans an interval from 1 to 4. Therefore, we cannot give an upper bound for this set of benchmarks with any acceptable level of confidence.\nHowever, the scale-free data points we plotted, which could not be fitted with a function yielding a straight line, do mostly follow a clear curving trend. A hypothesis for this behaviour is that the quality of the upper and lower bound deteriorates mostly for the middle sizes of the benchmarks; smaller and larger scale-free graphs are easier to triangulate well.11 To give an upper bound, we could plot any line on the outer hull of these data points; e.g. the horizontal line represented by wd(x) = 8x would work. The most pessimistic assumption would be to choose a function with the highest slope, and we find that the upper bound w\u2032\u2032d(x) = 2 5x 2.5, found for the HTN benchmarks, also works here.\nFrom this discussion, we may conclude that for all benchmarks we ran except for New York, wd(x) is O ( x2.5 ) which in turn is O ( w\u22172.5 ) ; the run time of the algorithms Snowball\nand Chleq\u2013APSP on these instances can therefore be bounded by O ( n2w\u22172.5 ) .\nTo conclude this section, we remark that an alternative to a triangulation heuristic would be to use an approximation algorithm with a bound on the induced width that can be theoretically determined. For example, Bouchitte\u0301, Kratsch, Mu\u0308ller, and Todinca (2004) give a O (logw\u2217) approximation of the treewidth w\u2217. Using such an approximation would give an upper bound on the run time of Snowball of O ( n2w\u2217 logw\u2217 ) . However, the run\n11. This mirrors earlier observations by the authors.\ntime of obtaining this approximate induced width is O ( n3 log4 nw\u22175 logw\u2217 ) and has a high constant as well, so their work is\u2014for now\u2014mainly of theoretical value."}, {"heading": "5. Related Work", "text": "For dense, directed graphs with real weights, the state-of-the-art APSP algorithms run in O ( n3/ log n ) time (Chan, 2005; Han, 2008). These represent a serious improvement over\nthe O ( n3 )\nbound on Floyd\u2013Warshall but do not profit from the fact that in most graphs that occur in practice, the number of edges m is significantly lower than n2.\nThis profit is exactly what algorithms for sparse graphs aim to achieve. Recently, an improvement was published over theO ( nm+ n2 log n ) algorithm based on Johnson\u2019s (1977) and Fredman and Tarjan\u2019s (1987) work: an algorithm for sparse directed graphs running in O ( nm+ n2 log log n ) time (Pettie, 2004). In theory, this algorithm is thus faster than Johnson (in worst cases, for large graphs) when m \u2208 o (n log n).12 However, currently no implementation exists (as confirmed through personal communication with Pettie, June 2011). The upper bound of O ( n2wd ) on the run time of Snowball is smaller than this established upper bound when the induced width is small (i.e. when wd \u2208 o (log log n)), and, of course, for chordal graphs and graphs of constant treewidth.\nWe are familiar with one earlier work to compute shortest paths by leveraging low treewidth. Chaudhuri and Zaroliagis (2000) present an algorithm for answering (point-topoint) shortest path queries with O ( w3dn log n ) preprocessing time and query time O ( w3d ) .\nA direct extension of their results to APSP would imply a run time of O ( n2w3d ) on general\ngraphs and O ( nmw2d ) on chordal graphs. Our result of computing APSP on general graphs\nin O ( n2wd ) and in O (nm) on chordal graphs is thus a strict improvement.\nA large part of the state-of-the-art in point-to-point shortest paths is focused on road networks (with positive edge weights). These studies have a strong focus on heuristics, ranging from goal-directed search and bi-directional search to using or creating some hierarchical structure, see for example (Geisberger, Sanders, Schultes, & Delling, 2008; Bauer, Delling, Sanders, Schieferdecker, Schultes, & Wagner, 2008). One of these hierarchical heuristics has some similarities to the idea of using chordal graphs. This heuristic is called contraction. The idea there is to distinguish important (core) vertices, which may be possible end points, from vertices that are never used as a start or end point. These latter vertices are then removed (bypassed) one-by-one, connecting their neighbours directly.\nOther restrictions on the input graphs for which shortest paths are computed can also be assumed, and sometimes lead to algorithms with tighter bounds. For example, for unweighted chordal graphs, APSP lengths can be determined inO ( n2 )\ntime (Balachandhran & Rangan, 1996; Han, Sekharan, & Sridhar, 1997) if all pairs at distance two are known. See (Dragan, 2005) for an overview and unification of such approaches. Considering only planar graphs, recent work shows that APSP be found in O ( n2 log2 n ) (Klein, Mozes, &\nWeimann, 2010), which is an improvement over Johnson in cases where m \u2208 \u03c9 ( n log2 n ) .\nIn the context of planning and scheduling, a number of similar APSP problems need to be computed sequentially, potentially allowing for a more efficient approach using dynamic algorithms. Even and Gazit (1985) provide a method where addition of a single edge can require O ( n2 ) steps, and deletion O ( n4/m ) on average. Thorup (2004) and Deme-\n12. We explain our use of the notation x \u2208 o (f(n)) in Footnote 1 on page 354.\ntrescu and Italiano (2006) later give an alternative approach with an amortized run time of O ( n2(log n+ log2 n+mn ) ) . Especially in the context of planning and scheduling, it is not essential that the shortest paths between all time points be maintained. Often, it is sufficient when the shortest paths of a selection of pairs are maintained. Above, we already mentioned the P3C algorithm by Planken et al. (2008) for the single-shot case; Planken et al. (2010) describe an algorithm that incrementally maintains the property of partial path consistency on chordal graphs in time linear in the number of edges."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper we give three algorithms for computing all-pairs shortest paths, with a run time bounded by (i) O ( n2 )\nfor graphs of constant treewidth, matching earlier results that also required O ( n2 )\n(Chaudhuri & Zaroliagis, 2000); (ii) O (nm) on chordal graphs, improving over the earlier O ( nmw2d ) ; and (iii) O ( n2wd ) on general graphs, showing again an\nimprovement over previously known tightest bound of O ( n2w3d ) . In these bounds, wd is the induced width of the ordering used; experimentally we have determined this to be bounded by the treewidth to the power 2.5 for most of our benchmarks.\nThese contributions are obtained by applying directed path consistency combined with known graph-theoretic techniques, such as a vertex elimination and tree decomposition, to computing shortest paths. This supports the general idea that such techniques may help in solving graphically-representable combinatorial problems, but the main contribution of this article is more narrow, focusing on improving the state of the art for this single, but important problem of computing APSP.\nFrom the results of our extensive experiments we can make recommendations as to which algorithm is best suited for which type of problems. Only for very small instances, Floyd\u2013Warshall should be used; this is probably mostly thanks to its simplicity, yielding a very straightforward implementation with low overhead. Snowball can exploit the fact that a perfect elimination ordering can be efficiently found for chordal graphs, which makes it the most efficient algorithm for this class of graphs. From all our experiments on different types of general graphs, we conclude that Snowball consistently outperforms Johnson (and Floyd\u2013Warshall), except when the induced width is very high. Our experiments also show that Snowball always outperforms both Chleq\u2013APSP and Snowball\u2013Separators. Although the latter has a better bound on its run time, surprisingly its actual performance is worse than Snowball on all instances of our benchmark sets. This holds even for those instances for which Snowball\u2013Separators performs significantly fewer updates. Thus, we conclude that the additional bookkeeping required by Snowball\u2013Separators does not pay off.\nRegarding these experiments, it must be noted that, although we did the utmost to obtain a fair comparison, a constant factor in the measurements depends in a significant way on the exact implementation details (e.g. whether a lookup-table or a heap is used), as is also put forward in earlier work on experimentally comparing shortest path algorithms (Mondou, Crainic, & Nguyen, 1991; Cherkassky, Goldberg, & Radzik, 1996). In our own implementation a higher constant factor for the Snowball algorithms may be caused by adhering to the object-oriented paradigm, i.e. inheriting from the DPC and P3C superclasses, and choosing to reuse code rather than inlining method calls. Nonetheless, we are confident that the general trends we identified hold independently of such details.\nNote that strictly speaking, the algorithms introduced in this paper compute all-pairs shortest distances. If one wants to actually trace shortest paths, the algorithms can be extended to keep track of the midpoint whenever the distance matrix is updated, just like one does for Floyd\u2013Warshall. Then, for any pair of vertices, the actual shortest path in the graph can be traced in O (n) time.\nIn our current implementation of Snowball\u2013Separators, we used a priority queue to decide heuristically which clique tree node to visit next, giving precedence to nodes connected by a large separator to the part of the clique tree already visited. As noted before, we defer answering the question whether an optimal ordering can be found efficiently to future work. We remark that using the minimum-degree heuristic for triangulation provides Snowball with a natural edge, delaying the processing of vertices where the number of iterations of the middle loop is small until k grows large.\nCherkassky and Goldberg (1999) compared several innovative algorithms for singlesource shortest paths that gave better efficiency than the standard Bellman\u2013Ford algorithm in practice, while having the same worst-case bound of O (nm) on the run time. In future work, we will investigate if any of these clever improvements can also be exploited in Snowball.\nSnowball\u2013Separators can be improved further in a way that does not influence the theoretical complexity but may yield better performance in practice. Iterating over Vother can be seen as a reverse traversal of the part of the clique tree visited before, starting at c\u2019s parent. Then, instead of always using the separator between the current clique node (containing k) and its parent for all previously visited vertices in Vother, we can keep track of the smallest separator encountered during this backwards traversal for no extra asymptotic cost. Since it was shown in Table 1 that the largest minimal separator is often hardly smaller than the induced width, it might well pay off to search for smaller separators. We plan to implement this improvement in the near future.\nAnother possible improvement is suggested by the following observation on DPC. A variant of DPC can be proposed where edge directionality is taken into account: during iteration k, only those neighbours i, j < k are considered for which there is a directed path i\u2192 k \u2192 j, resulting in the addition of the arc i\u2192 j. This set of added arcs would often be much smaller than twice the number of edges added by the standard DPC algorithm, and while the graph produced by the directed variant would not be chordal, the correctness of Snowball would not be impacted.\nFurthermore, we would like to also experimentally compare our algorithms to the recent algorithms by Pettie (2004) and the algorithms for graphs of constant treewidth by Chaudhuri and Zaroliagis (2000) in future work. In addition, we are interested in more efficient triangulation heuristics, or triangulation heuristics with a guaranteed quality, to be able to give a guaranteed theoretical bound on general graphs. Another direction, especially interesting in the context of planning and scheduling, is to use the ideas presented here to design a faster algorithm for dynamic all-pairs shortest paths: maintaining shortest paths under edge deletions (or relaxations) and additions (or tightenings)."}, {"heading": "Acknowledgments", "text": "Roman van der Krogt is supported by Science Foundation Ireland under Grant number 08/RFP/CMS1711.\nWe offer our sincere gratitude to our reviewers for their comments, which helped us improve the clarity of the article and strengthen our empirical results.\nThis article is based on a conference paper with the same title, which has received an honourable mention for best student paper at the International Conference on Automated Planning and Scheduling (Planken, de Weerdt, & van der Krogt, 2011)."}, {"heading": "Appendix A. Johnson\u2019s Heap", "text": "In the experiments in this paper, we presented the results for Johnson using a Fibonacci heap, because only then the theoretical bound of O ( nm+ n2 log n ) time is attained. In practice, using a binary heap for a theoretical bound of O (nm log n) time turns out to be more efficient on some occasions, as we show by the results in this section.\nFigure 16 shows the run times of Johnson with a binary heap and with a Fibonacci heap on all of the benchmark sets listed in Table 1. On the diamonds, HTN, and New York benchmarks the binary heap is a few percent faster than the Fibonacci heap, but the slope of the lines in this doubly logarithmic scale is the same, so we can conclude that the average-case run time has similar asymptotic behavior. However, for larger job-shop problems, a binary heap is a factor 2 slower than a Fibonacci heap, and on our chordal graph benchmark problems even a factor 10. Our benchmark problems on scale-free graphs with a fixed number of vertices help explaining this difference.\nIn Figure 17, the run time of both variants of Johnson can be found for scale-free graphs with 1,000 vertices, with the number of edges varying from about 2,000 to almost 80,000. Here, we see that only for the sparsest scale-free graphs with about 2, 000 edges, the binary heap is slightly faster, but when more edges are considered, using the Fibonacci heap significantly outperforms using the binary heap. In particular, the run time of the Fibonacci heap implementation increases only slowly with the number of edges, while the run time of the binary heap increases much more significantly. This can be explained by the fact that when running Dijkstra\u2019s algorithm as a subroutine in Johnson, each update of a (candidate) shortest path can be done in amortized constant time with a Fibonacci heap, while in a binary heap this has a worst-case cost of O (log n) time per update. The number of updates is bounded by m for each run of Dijkstra\u2019s algorithm, yielding a bound of O (nm) updates for Johnson. For the binary heap this O (nm log n) bound accounts for a significant part of the run time, while with a Fibonacci heap other operations (such as extracting the minimum element from the heap) have a bigger relative contribution to the run time.\nBased on the results over all benchmark sets, we conclude that although Johnson with a binary heap can help reducing the actual run time in sparse graphs, Johnson with a Fibonacci heap is overall the better choice if m can be large."}], "references": [{"title": "Statistical Mechanics of Complex Networks", "author": ["R. Albert", "Barab\u00e1si", "A.-L"], "venue": "Reviews of Modern Physics,", "citeRegEx": "Albert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Albert et al\\.", "year": 2002}, {"title": "Complexity of Finding Embeddings in a k -Tree", "author": ["S. Arnborg", "D.G. Corneil", "A. Proskurowski"], "venue": "SIAM Journal on Algebraic and Discrete Methods,", "citeRegEx": "Arnborg et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Arnborg et al\\.", "year": 1987}, {"title": "All-pairs-shortest-length on strongly chordal graphs", "author": ["V. Balachandhran", "C.P. Rangan"], "venue": "Discrete applied mathematics,", "citeRegEx": "Balachandhran and Rangan,? \\Q1996\\E", "shortCiteRegEx": "Balachandhran and Rangan", "year": 1996}, {"title": "Combining hierarchical and goal-directed speed-up techniques for Dijkstra\u2019s algorithm", "author": ["R. Bauer", "D. Delling", "P. Sanders", "D. Schieferdecker", "D. Schultes", "D. Wagner"], "venue": "In Experimental Algorithms (WEA 2008),", "citeRegEx": "Bauer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bauer et al\\.", "year": 2008}, {"title": "Classes of graphs with bounded tree-width", "author": ["H.L. Bodlaender"], "venue": "Tech. rep. RUU-CS86-22, Utrecht University.", "citeRegEx": "Bodlaender,? 1986", "shortCiteRegEx": "Bodlaender", "year": 1986}, {"title": "A Linear-Time Algorithm for Finding Tree-Decompositions of Small Treewidth", "author": ["H.L. Bodlaender"], "venue": "SIAM Journal on Computing, 25 (6), 1305\u20131317.", "citeRegEx": "Bodlaender,? 1996", "shortCiteRegEx": "Bodlaender", "year": 1996}, {"title": "On Treewidth Approximations", "author": ["V. Bouchitt\u00e9", "D. Kratsch", "H. M\u00fcller", "I. Todinca"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Bouchitt\u00e9 et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bouchitt\u00e9 et al\\.", "year": 2004}, {"title": "Activity Planning for the Mars Exploration Rovers", "author": ["J.L. Bresina", "A.K. J\u00f3nsson", "P.H. Morris", "K. Rajan"], "venue": "In Proc. of the 15th Int. Conf. on Automated Planning and Scheduling,", "citeRegEx": "Bresina et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bresina et al\\.", "year": 2005}, {"title": "Efficient Variable Elimination for Semi-Structured Simple Temporal Networks with Continuous Domains", "author": ["H.H. Bui", "N. Yorke-Smith"], "venue": "Knowledge Engineering Review,", "citeRegEx": "Bui and Yorke.Smith,? \\Q2010\\E", "shortCiteRegEx": "Bui and Yorke.Smith", "year": 2010}, {"title": "A Temporal Constraint Network Based Temporal Planner", "author": ["L. Castillo", "J. Fern\u00e1ndez-Olivares", "A. Gonz\u00e1lez"], "venue": "In Proc. of the 21st Workshop of the UK Planning and Scheduling Special Interest Group,", "citeRegEx": "Castillo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Castillo et al\\.", "year": 2002}, {"title": "Efficiently Handling Temporal Knowledge in an HTN planner", "author": ["L. Castillo", "J. Fern\u00e1ndez-Olivares", "A. Gonz\u00e1lez"], "venue": "In Proc. of the 16th Int. Conf. on Automated Planning and Scheduling,", "citeRegEx": "Castillo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Castillo et al\\.", "year": 2006}, {"title": "Shortest Paths in Digraphs of Small Treewidth", "author": ["S. Chaudhuri", "C.D. Zaroliagis"], "venue": "Part I: Sequential Algorithms. Algorithmica,", "citeRegEx": "Chaudhuri and Zaroliagis,? \\Q2000\\E", "shortCiteRegEx": "Chaudhuri and Zaroliagis", "year": 2000}, {"title": "Shortest paths algorithms: theory and experimental evaluation", "author": ["B.V. Cherkassky", "A.V. Goldberg", "T. Radzik"], "venue": "Mathematical programming,", "citeRegEx": "Cherkassky et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cherkassky et al\\.", "year": 1996}, {"title": "Negative-cycle detection algorithms", "author": ["B.V. Cherkassky", "A.V. Goldberg"], "venue": "Mathematical Programming,", "citeRegEx": "Cherkassky and Goldberg,? \\Q1999\\E", "shortCiteRegEx": "Cherkassky and Goldberg", "year": 1999}, {"title": "Efficient Algorithms for Networks of Quantitative Temporal Constraints", "author": ["N. Chleq"], "venue": "Proc. of the 1st Int. Workshop on Constraint Based Reasoning, pp. 40\u201345.", "citeRegEx": "Chleq,? 1995", "shortCiteRegEx": "Chleq", "year": 1995}, {"title": "Flexible execution of plans with choice", "author": ["P.R. Conrad", "J.A. Shah", "B.C. Williams"], "venue": "In Proc. of the 19th Int. Conf. on Automated Planning and Scheduling", "citeRegEx": "Conrad et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Conrad et al\\.", "year": 2009}, {"title": "Introduction to Algorithms, 2nd edition", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2001}, {"title": "Temporal Constraint Networks", "author": ["R. Dechter", "I. Meiri", "J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 1991}, {"title": "Fully Dynamic All-Pairs Shortest Paths with Real Edge Weights", "author": ["C. Demetrescu", "G.F. Italiano"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Demetrescu and Italiano,? \\Q2006\\E", "shortCiteRegEx": "Demetrescu and Italiano", "year": 2006}, {"title": "A note on two problems in connexion with graphs", "author": ["E.W. Dijkstra"], "venue": "Numerische Mathematik,", "citeRegEx": "Dijkstra,? \\Q1959\\E", "shortCiteRegEx": "Dijkstra", "year": 1959}, {"title": "Estimating all pairs shortest paths in restricted graph families: a unified approach", "author": ["F.F. Dragan"], "venue": "Journal of Algorithms, 57 (1), 1\u201321.", "citeRegEx": "Dragan,? 2005", "shortCiteRegEx": "Dragan", "year": 2005}, {"title": "Updating Distances in Dynamic Graphs", "author": ["S. Even", "H. Gazit"], "venue": "Methods of Operations Research,", "citeRegEx": "Even and Gazit,? \\Q1985\\E", "shortCiteRegEx": "Even and Gazit", "year": 1985}, {"title": "Analysis of Java implementations of Fibonacci Heap", "author": ["N. Fiedler"], "venue": "http://tinyurl. com/fibo-heap.", "citeRegEx": "Fiedler,? 2008", "shortCiteRegEx": "Fiedler", "year": 2008}, {"title": "Algorithm 97: Shortest path", "author": ["R.W. Floyd"], "venue": "Communications of the ACM, 5 (6), 345.", "citeRegEx": "Floyd,? 1962", "shortCiteRegEx": "Floyd", "year": 1962}, {"title": "Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms", "author": ["M. Fredman", "R.E. Tarjan"], "venue": "Journal of the ACM,", "citeRegEx": "Fredman and Tarjan,? \\Q1987\\E", "shortCiteRegEx": "Fredman and Tarjan", "year": 1987}, {"title": "Contraction hierarchies: Faster and simpler hierarchical routing in road networks", "author": ["R. Geisberger", "P. Sanders", "D. Schultes", "D. Delling"], "venue": "In Proc. of the Int. Workshop on Experimental Algorithms,", "citeRegEx": "Geisberger et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Geisberger et al\\.", "year": 2008}, {"title": "Community Structure in Social and Biological Networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "Proc. of the National Academy of Sciences of the USA,", "citeRegEx": "Girvan and Newman,? \\Q2002\\E", "shortCiteRegEx": "Girvan and Newman", "year": 2002}, {"title": "Algorithmic Graph Theory and Perfect Graphs", "author": ["M. Golumbic"], "venue": "Elsevier.", "citeRegEx": "Golumbic,? 2004", "shortCiteRegEx": "Golumbic", "year": 2004}, {"title": "Concrete Mathematics: A Foundation for Computer Science (1st edition)", "author": ["R.L. Graham", "D.E. Knuth", "O. Patashnik"], "venue": null, "citeRegEx": "Graham et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Graham et al\\.", "year": 1989}, {"title": "Unified All-Pairs Shortest Path Algorithms in the Chordal Hierarchy", "author": ["K. Han", "C.N. Sekharan", "R. Sridhar"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Han et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Han et al\\.", "year": 1997}, {"title": "Minimal triangulations of graphs: A survey", "author": ["P. Heggernes"], "venue": "Discrete Mathematics, 306 (3), 297\u2013317. Minimal Separation and Minimal Triangulation.", "citeRegEx": "Heggernes,? 2006", "shortCiteRegEx": "Heggernes", "year": 2006}, {"title": "Efficient Algorithms for Shortest Paths in Sparse Networks", "author": ["D.B. Johnson"], "venue": "Journal of the ACM, 24 (1), 1\u201313.", "citeRegEx": "Johnson,? 1977", "shortCiteRegEx": "Johnson", "year": 1977}, {"title": "Triangulation of Graphs - Algorithms Giving Small Total State Space", "author": ["U. Kj\u00e6rulff"], "venue": "Tech. rep., Aalborg University.", "citeRegEx": "Kj\u00e6rulff,? 1990", "shortCiteRegEx": "Kj\u00e6rulff", "year": 1990}, {"title": "Shortest Paths in Directed Planar Graphs with Negative Lengths: A Linear-space O", "author": ["P.N. Klein", "S. Mozes", "O. Weimann"], "venue": null, "citeRegEx": "Klein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2010}, {"title": "Shortest path algorithms: A computational study with the C programming language", "author": ["J.F. Mondou", "T.G. Crainic", "S. Nguyen"], "venue": "Computers & Operations Research,", "citeRegEx": "Mondou et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Mondou et al\\.", "year": 1991}, {"title": "A New Approach to All-pairs Shortest Paths on Real-weighted Graphs", "author": ["S. Pettie"], "venue": "Theoretical Computer Science, 312 (1), 47\u201374. Planken, L. R., de Weerdt, M. M., & van der Krogt, R. P. J. (2008). P3C: A New Algorithm for the Simple Temporal Problem. In Proc. of the 18th Int. Conf. on Automated Planning and Scheduling, pp. 256\u2013263.", "citeRegEx": "Pettie,? 2004", "shortCiteRegEx": "Pettie", "year": 2004}, {"title": "Computing allpairs shortest paths by leveraging low treewidth", "author": ["L.R. Planken", "M.M. de Weerdt", "R.P.J. van der Krogt"], "venue": "In Proc. of the 21st Int. Conf. on Automated Planning and Scheduling,", "citeRegEx": "Planken et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Planken et al\\.", "year": 2011}, {"title": "Incrementally Solving STNs by Enforcing Partial Path Consistency", "author": ["L.R. Planken", "M.M. de Weerdt", "N. Yorke-Smith"], "venue": "In Proc. of the 20th Int. Conf. on Automated Planning and Scheduling,", "citeRegEx": "Planken et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Planken et al\\.", "year": 2010}, {"title": "The SMT-LIB Format: An Initial Proposal", "author": ["S. Ranise", "C. Tinelli"], "venue": "In Proc. of Pragmatics of Decision Procedures in Automated Reasoning", "citeRegEx": "Ranise and Tinelli,? \\Q2003\\E", "shortCiteRegEx": "Ranise and Tinelli", "year": 2003}, {"title": "A Graph-Theoretic Study of the Numerical Solution of Sparse Positive Definite Systems of Linear Equations", "author": ["D.J. Rose"], "venue": "Read, R. (Ed.), Graph theory and computing, pp. 183\u2013217. Academic Press.", "citeRegEx": "Rose,? 1972", "shortCiteRegEx": "Rose", "year": 1972}, {"title": "Uncertainty in soft temporal constraint problems: A general framework and controllability algorithms for the fuzzy case", "author": ["F. Rossi", "K.B. Venable", "N. Yorke-Smith"], "venue": "Journal of AI Research,", "citeRegEx": "Rossi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rossi et al\\.", "year": 2006}, {"title": "On the Tractability of Restricted Disjunctive Temporal Problems", "author": ["T.K. Satish Kumar"], "venue": "Proc. of the 15th Int. Conf. on Automated Planning and Scheduling, pp. 110\u2013119.", "citeRegEx": "Kumar,? 2005", "shortCiteRegEx": "Kumar", "year": 2005}, {"title": "Fast Dynamic Scheduling of Disjunctive Temporal Constraint Networks through Incremental Compilation", "author": ["J.A. Shah", "B.C. Williams"], "venue": "In Proc. of the 18th Int. Conf. on Automated Planning and Scheduling,", "citeRegEx": "Shah and Williams,? \\Q2008\\E", "shortCiteRegEx": "Shah and Williams", "year": 2008}, {"title": "Backtracking algorithms for disjunctions of temporal constraints", "author": ["K. Stergiou", "M. Koubarakis"], "venue": "Artificial Intelligence,", "citeRegEx": "Stergiou and Koubarakis,? \\Q2000\\E", "shortCiteRegEx": "Stergiou and Koubarakis", "year": 2000}, {"title": "Deciding Separation Formulas with SAT", "author": ["O. Strichman", "S.A. Seshia", "R.E. Bryant"], "venue": "In Proc. of the 14th Int. Conf. on Computer Aided Verification,", "citeRegEx": "Strichman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Strichman et al\\.", "year": 2002}, {"title": "Simple Linear-time Algorithms to Test Chordality of Graphs, Test Acyclicity of Hypergraphs, and Selectively Reduce Acyclic Hypergraphs", "author": ["R.E. Tarjan", "M. Yannakakis"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Tarjan and Yannakakis,? \\Q1984\\E", "shortCiteRegEx": "Tarjan and Yannakakis", "year": 1984}, {"title": "Fully-dynamic All-Pairs Shortest Paths: Faster and Allowing Negative Cycles", "author": ["Planken", "De Weerdt", "M. Van der Krogt Thorup"], "venue": "In Algorithm Theory, Vol. 3111 of LNCS,", "citeRegEx": "Planken et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Planken et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 17, "context": "The STP in turn appears as a sub-problem to the NP-hard Temporal Constraint Satisfaction Problem (TCSP; Dechter et al., 1991) and Disjunctive Temporal Problem (DTP; Stergiou & Koubarakis, 2000), which are powerful enough to model e.", "startOffset": 97, "endOffset": 125}, {"referenceID": 17, "context": "The canonical way of solving an STP instance (Dechter et al., 1991) is by computing all-pairs shortest paths (APSP) on its STN, thus achieving full path consistency.", "startOffset": 45, "endOffset": 67}, {"referenceID": 23, "context": "For graphs with n vertices and m edges, this can be done in O ( n3 ) time with the Floyd\u2013Warshall algorithm (Floyd, 1962), based on Warshall\u2019s (1962) formulation of efficiently computing the transitive closure of Boolean matrices.", "startOffset": 108, "endOffset": 121}, {"referenceID": 4, "context": "In addition to these STNs, examples of such graphs of constant treewidth are outerplanar graphs, graphs of bounded bandwidth, graphs of bounded cutwidth, and series-parallel graphs (Bodlaender, 1986).", "startOffset": 181, "endOffset": 199}, {"referenceID": 27, "context": "Chordal graphs are an important subset of general sparse graphs: interval graphs, trees, k-trees and split graphs are all special cases of chordal graphs (Golumbic, 2004).", "startOffset": 154, "endOffset": 170}, {"referenceID": 14, "context": "The canonical way of solving an STP instance (Dechter et al., 1991) is by computing all-pairs shortest paths (APSP) on its STN, thus achieving full path consistency. For graphs with n vertices and m edges, this can be done in O ( n3 ) time with the Floyd\u2013Warshall algorithm (Floyd, 1962), based on Warshall\u2019s (1962) formulation of efficiently computing the transitive closure of Boolean matrices.", "startOffset": 46, "endOffset": 316}, {"referenceID": 14, "context": "The canonical way of solving an STP instance (Dechter et al., 1991) is by computing all-pairs shortest paths (APSP) on its STN, thus achieving full path consistency. For graphs with n vertices and m edges, this can be done in O ( n3 ) time with the Floyd\u2013Warshall algorithm (Floyd, 1962), based on Warshall\u2019s (1962) formulation of efficiently computing the transitive closure of Boolean matrices. However, the state of the art for computing APSP on sparse graphs is an algorithm based on the technique originally proposed by Johnson (1977), which does some preprocessing to allow n runs of Dijkstra\u2019s (1959) algorithm.", "startOffset": 46, "endOffset": 540}, {"referenceID": 14, "context": "The canonical way of solving an STP instance (Dechter et al., 1991) is by computing all-pairs shortest paths (APSP) on its STN, thus achieving full path consistency. For graphs with n vertices and m edges, this can be done in O ( n3 ) time with the Floyd\u2013Warshall algorithm (Floyd, 1962), based on Warshall\u2019s (1962) formulation of efficiently computing the transitive closure of Boolean matrices. However, the state of the art for computing APSP on sparse graphs is an algorithm based on the technique originally proposed by Johnson (1977), which does some preprocessing to allow n runs of Dijkstra\u2019s (1959) algorithm.", "startOffset": 46, "endOffset": 608}, {"referenceID": 12, "context": "One algorithm, dubbed Chleq\u2013APSP, is based on a point-to-point shortest path algorithm by Chleq (1995); the other, named Snowball, is similar to Planken, de Weerdt, and van der Krogt\u2019s (2008) algorithm for enforcing partial (instead of full) path consistency (PC).", "startOffset": 22, "endOffset": 103}, {"referenceID": 12, "context": "One algorithm, dubbed Chleq\u2013APSP, is based on a point-to-point shortest path algorithm by Chleq (1995); the other, named Snowball, is similar to Planken, de Weerdt, and van der Krogt\u2019s (2008) algorithm for enforcing partial (instead of full) path consistency (PC).", "startOffset": 22, "endOffset": 192}, {"referenceID": 17, "context": "Algorithm 1: DPC (Dechter et al., 1991) Input: Weighted directed graph G = \u3008V,E\u3009; vertex ordering d : V \u2192 {1, .", "startOffset": 17, "endOffset": 39}, {"referenceID": 17, "context": "Dechter et al. (1991) define this induced width of a vertex ordering d procedurally to be exactly the highest number of neighbours j of k with j < k encountered during the DPC algorithm.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "time (Bodlaender, 1996).", "startOffset": 5, "endOffset": 23}, {"referenceID": 39, "context": "We mention here the minimum degree heuristic (Rose, 1972), which in each iteration chooses a vertex of lowest degree.", "startOffset": 45, "endOffset": 57}, {"referenceID": 14, "context": "Chleq (1995) proposed a point-to-point shortest path algorithm that with a trivial adaptation computes SSSP; Planken, de Weerdt, and Yorke-Smith (2010) implicitly also compute SSSP as part of their IPPC algorithm.", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "Chleq (1995) proposed a point-to-point shortest path algorithm that with a trivial adaptation computes SSSP; Planken, de Weerdt, and Yorke-Smith (2010) implicitly also compute SSSP as part of their IPPC algorithm.", "startOffset": 0, "endOffset": 152}, {"referenceID": 14, "context": "Chleq (1995) proposed a point-to-point shortest path algorithm that with a trivial adaptation computes SSSP; Planken, de Weerdt, and Yorke-Smith (2010) implicitly also compute SSSP as part of their IPPC algorithm. These algorithms run in O (mc) time and thus can simply be run once for each vertex to yield an APSP algorithm with O (nmc) \u2286 O ( nwd ) time complexity. Below, we first show how to adapt Chleq\u2019s algorithm to compute APSP; then, we present a new, efficient algorithm named Snowball that relates to Planken et al.\u2019s (2008) PC.", "startOffset": 0, "endOffset": 535}, {"referenceID": 14, "context": "Algorithm 2: Min\u2013path (Chleq, 1995) Input: Weighted directed DPC graph G = \u3008V,E\u3009; (arbitrary) source vertex s and destination vertex t Output: Distance from s to t, or inconsistent if G contains a negative cycle", "startOffset": 22, "endOffset": 35}, {"referenceID": 22, "context": "For Johnson we used the corrected Fibonacci heap implementation by Fiedler (2008), since the widely used pseudocode of Cormen, Leiserson, Rivest, and Stein (2001) contains mistakes.", "startOffset": 67, "endOffset": 82}, {"referenceID": 22, "context": "For Johnson we used the corrected Fibonacci heap implementation by Fiedler (2008), since the widely used pseudocode of Cormen, Leiserson, Rivest, and Stein (2001) contains mistakes.", "startOffset": 67, "endOffset": 163}, {"referenceID": 32, "context": "However, it is also known from the literature that the theoretical bound on the minimum fill heuristic is worse than that of minimum degree (Kj\u00e6rulff, 1990).", "startOffset": 140, "endOffset": 156}, {"referenceID": 14, "context": "5 ) ; the run time of the algorithms Snowball and Chleq\u2013APSP on these instances can therefore be bounded by O ( n2w\u22172.5 ) . To conclude this section, we remark that an alternative to a triangulation heuristic would be to use an approximation algorithm with a bound on the induced width that can be theoretically determined. For example, Bouchitt\u00e9, Kratsch, M\u00fcller, and Todinca (2004) give a O (logw\u2217) approximation of the treewidth w\u2217.", "startOffset": 50, "endOffset": 384}, {"referenceID": 35, "context": "Recently, an improvement was published over theO ( nm+ n2 log n ) algorithm based on Johnson\u2019s (1977) and Fredman and Tarjan\u2019s (1987) work: an algorithm for sparse directed graphs running in O ( nm+ n2 log log n ) time (Pettie, 2004).", "startOffset": 219, "endOffset": 233}, {"referenceID": 20, "context": "See (Dragan, 2005) for an overview and unification of such approaches.", "startOffset": 4, "endOffset": 18}, {"referenceID": 20, "context": "These represent a serious improvement over the O ( n3 ) bound on Floyd\u2013Warshall but do not profit from the fact that in most graphs that occur in practice, the number of edges m is significantly lower than n2. This profit is exactly what algorithms for sparse graphs aim to achieve. Recently, an improvement was published over theO ( nm+ n2 log n ) algorithm based on Johnson\u2019s (1977) and Fredman and Tarjan\u2019s (1987) work: an algorithm for sparse directed graphs running in O ( nm+ n2 log log n ) time (Pettie, 2004).", "startOffset": 65, "endOffset": 385}, {"referenceID": 20, "context": "These represent a serious improvement over the O ( n3 ) bound on Floyd\u2013Warshall but do not profit from the fact that in most graphs that occur in practice, the number of edges m is significantly lower than n2. This profit is exactly what algorithms for sparse graphs aim to achieve. Recently, an improvement was published over theO ( nm+ n2 log n ) algorithm based on Johnson\u2019s (1977) and Fredman and Tarjan\u2019s (1987) work: an algorithm for sparse directed graphs running in O ( nm+ n2 log log n ) time (Pettie, 2004).", "startOffset": 65, "endOffset": 417}, {"referenceID": 11, "context": "Chaudhuri and Zaroliagis (2000) present an algorithm for answering (point-topoint) shortest path queries with O ( w3 dn log n ) preprocessing time and query time O ( w3 d ) .", "startOffset": 0, "endOffset": 32}, {"referenceID": 11, "context": "Chaudhuri and Zaroliagis (2000) present an algorithm for answering (point-topoint) shortest path queries with O ( w3 dn log n ) preprocessing time and query time O ( w3 d ) . A direct extension of their results to APSP would imply a run time of O ( n2w3 d ) on general graphs and O ( nmw2 d ) on chordal graphs. Our result of computing APSP on general graphs in O ( nwd ) and in O (nm) on chordal graphs is thus a strict improvement. A large part of the state-of-the-art in point-to-point shortest paths is focused on road networks (with positive edge weights). These studies have a strong focus on heuristics, ranging from goal-directed search and bi-directional search to using or creating some hierarchical structure, see for example (Geisberger, Sanders, Schultes, & Delling, 2008; Bauer, Delling, Sanders, Schieferdecker, Schultes, & Wagner, 2008). One of these hierarchical heuristics has some similarities to the idea of using chordal graphs. This heuristic is called contraction. The idea there is to distinguish important (core) vertices, which may be possible end points, from vertices that are never used as a start or end point. These latter vertices are then removed (bypassed) one-by-one, connecting their neighbours directly. Other restrictions on the input graphs for which shortest paths are computed can also be assumed, and sometimes lead to algorithms with tighter bounds. For example, for unweighted chordal graphs, APSP lengths can be determined inO ( n2 ) time (Balachandhran & Rangan, 1996; Han, Sekharan, & Sridhar, 1997) if all pairs at distance two are known. See (Dragan, 2005) for an overview and unification of such approaches. Considering only planar graphs, recent work shows that APSP be found in O ( n2 log n ) (Klein, Mozes, & Weimann, 2010), which is an improvement over Johnson in cases where m \u2208 \u03c9 ( n log n ) . In the context of planning and scheduling, a number of similar APSP problems need to be computed sequentially, potentially allowing for a more efficient approach using dynamic algorithms. Even and Gazit (1985) provide a method where addition of a single edge can require O ( n2 ) steps, and deletion O ( n4/m ) on average.", "startOffset": 0, "endOffset": 2060}, {"referenceID": 11, "context": "Chaudhuri and Zaroliagis (2000) present an algorithm for answering (point-topoint) shortest path queries with O ( w3 dn log n ) preprocessing time and query time O ( w3 d ) . A direct extension of their results to APSP would imply a run time of O ( n2w3 d ) on general graphs and O ( nmw2 d ) on chordal graphs. Our result of computing APSP on general graphs in O ( nwd ) and in O (nm) on chordal graphs is thus a strict improvement. A large part of the state-of-the-art in point-to-point shortest paths is focused on road networks (with positive edge weights). These studies have a strong focus on heuristics, ranging from goal-directed search and bi-directional search to using or creating some hierarchical structure, see for example (Geisberger, Sanders, Schultes, & Delling, 2008; Bauer, Delling, Sanders, Schieferdecker, Schultes, & Wagner, 2008). One of these hierarchical heuristics has some similarities to the idea of using chordal graphs. This heuristic is called contraction. The idea there is to distinguish important (core) vertices, which may be possible end points, from vertices that are never used as a start or end point. These latter vertices are then removed (bypassed) one-by-one, connecting their neighbours directly. Other restrictions on the input graphs for which shortest paths are computed can also be assumed, and sometimes lead to algorithms with tighter bounds. For example, for unweighted chordal graphs, APSP lengths can be determined inO ( n2 ) time (Balachandhran & Rangan, 1996; Han, Sekharan, & Sridhar, 1997) if all pairs at distance two are known. See (Dragan, 2005) for an overview and unification of such approaches. Considering only planar graphs, recent work shows that APSP be found in O ( n2 log n ) (Klein, Mozes, & Weimann, 2010), which is an improvement over Johnson in cases where m \u2208 \u03c9 ( n log n ) . In the context of planning and scheduling, a number of similar APSP problems need to be computed sequentially, potentially allowing for a more efficient approach using dynamic algorithms. Even and Gazit (1985) provide a method where addition of a single edge can require O ( n2 ) steps, and deletion O ( n4/m ) on average. Thorup (2004) and Deme-", "startOffset": 0, "endOffset": 2187}, {"referenceID": 36, "context": "Above, we already mentioned the PC algorithm by Planken et al. (2008) for the single-shot case; Planken et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 36, "context": "Above, we already mentioned the PC algorithm by Planken et al. (2008) for the single-shot case; Planken et al. (2010) describe an algorithm that incrementally maintains the property of partial path consistency on chordal graphs in time linear in the number of edges.", "startOffset": 48, "endOffset": 118}, {"referenceID": 12, "context": "Cherkassky and Goldberg (1999) compared several innovative algorithms for singlesource shortest paths that gave better efficiency than the standard Bellman\u2013Ford algorithm in practice, while having the same worst-case bound of O (nm) on the run time.", "startOffset": 0, "endOffset": 31}, {"referenceID": 12, "context": "Cherkassky and Goldberg (1999) compared several innovative algorithms for singlesource shortest paths that gave better efficiency than the standard Bellman\u2013Ford algorithm in practice, while having the same worst-case bound of O (nm) on the run time. In future work, we will investigate if any of these clever improvements can also be exploited in Snowball. Snowball\u2013Separators can be improved further in a way that does not influence the theoretical complexity but may yield better performance in practice. Iterating over Vother can be seen as a reverse traversal of the part of the clique tree visited before, starting at c\u2019s parent. Then, instead of always using the separator between the current clique node (containing k) and its parent for all previously visited vertices in Vother, we can keep track of the smallest separator encountered during this backwards traversal for no extra asymptotic cost. Since it was shown in Table 1 that the largest minimal separator is often hardly smaller than the induced width, it might well pay off to search for smaller separators. We plan to implement this improvement in the near future. Another possible improvement is suggested by the following observation on DPC. A variant of DPC can be proposed where edge directionality is taken into account: during iteration k, only those neighbours i, j < k are considered for which there is a directed path i\u2192 k \u2192 j, resulting in the addition of the arc i\u2192 j. This set of added arcs would often be much smaller than twice the number of edges added by the standard DPC algorithm, and while the graph produced by the directed variant would not be chordal, the correctness of Snowball would not be impacted. Furthermore, we would like to also experimentally compare our algorithms to the recent algorithms by Pettie (2004) and the algorithms for graphs of constant treewidth by Chaudhuri and Zaroliagis (2000) in future work.", "startOffset": 0, "endOffset": 1808}, {"referenceID": 11, "context": "Furthermore, we would like to also experimentally compare our algorithms to the recent algorithms by Pettie (2004) and the algorithms for graphs of constant treewidth by Chaudhuri and Zaroliagis (2000) in future work.", "startOffset": 170, "endOffset": 202}], "year": 2012, "abstractText": "We present two new and efficient algorithms for computing all-pairs shortest paths. The algorithms operate on directed graphs with real (possibly negative) weights. They make use of directed path consistency along a vertex ordering d. Both algorithms run in O ( nwd ) time, where wd is the graph width induced by this vertex ordering. For graphs of constant treewidth, this yields O ( n ) time, which is optimal. On chordal graphs, the algorithms run in O (nm) time. In addition, we present a variant that exploits graph separators to arrive at a run time of O ( nw d + n sd ) on general graphs, where sd \u2264 wd is the size of the largest minimal separator induced by the vertex ordering d. We show empirically that on both constructed and realistic benchmarks, in many cases the algorithms outperform Floyd\u2013Warshall\u2019s as well as Johnson\u2019s algorithm, which represent the current state of the art with a run time of O ( n ) and O ( nm+ n log n ) , respectively. Our algorithms can be used for spatial and temporal reasoning, such as for the Simple Temporal Problem, which underlines their relevance to the planning and scheduling community.", "creator": "TeX"}}}