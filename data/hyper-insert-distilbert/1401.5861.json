{"id": "1401.5861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Online Speedup Learning for Optimal Planning", "abstract": "domain - independent integrated planning processing is literally one notion of the foundational areas in the early field of artificial intelligence. a description task of choosing a planning task consists of an initial world level state, a particular goal, and a set setting of actions for dramatically modifying the world turn state. here the objective is not to find a sequence of actions, that that often is, yielding a plan, sequence that transforms the same initial resulting world class state into a goal state. in optimal planning, we are becoming interested further in successively finding not just a plan, anything but one of the generally cheapest plans. a very prominent approach to educated optimal planning practitioners these days is heuristic state - space search, guided continuously by inherently admissible minimal heuristic functions. currently numerous publicly admissible managed heuristics have lately been largely developed, wherein each endowed with its own strengths and weaknesses, accordingly and such it is such well informally known among that predicting there is no single \" best viable heuristic figure for optimal product planning in general. for thus, idea which enables heuristic to choose for a given planning a task is really a difficult question. thus this difficulty however can be largely avoided by combining several specialized heuristics, but knowledge that requires intentionally computing numerous heuristic estimates only at decisions each state, and the economic tradeoff between maintaining the time periods spent doing so consciously and the time incurred saved appropriately by noting the combined advantages incurred of the different heuristics might be high. we present a totally novel computing method that reduces the cost of combining admissible heuristics practical for preventing optimal utility planning, while maintaining low its estimated benefits. using successfully an idealized advanced search space valuation model, we virtually formulate successfully a explicit decision rule theoretical for choosing chose the best heuristic to weakly compute at deciding each goal state. we then present an active online learning approach paradigm for learning a possible classifier with that mathematical decision rule as the nearest target feasible concept, simulate and employ the learned classifier to explicitly decide chooses which heuristic forms to best compute gains at choosing each state. indeed we essentially evaluate how this numerical technique jointly empirically, and show that it substantially outperforms the standard scientific method for successively combining several heuristics via detecting their pointwise maximum.", "histories": [["v1", "Thu, 23 Jan 2014 02:49:53 GMT  (371kb)", "http://arxiv.org/abs/1401.5861v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["carmel domshlak", "erez karpas", "shaul markovitch"], "accepted": false, "id": "1401.5861"}, "pdf": {"name": "1401.5861.pdf", "metadata": {"source": "CRF", "title": "Online Speedup Learning for Optimal Planning", "authors": ["Carmel Domshlak", "Erez Karpas", "Shaul Markovitch"], "emails": ["DCARMEL@IE.TECHNION.AC.IL", "KARPASE@TECHNION.AC.IL", "SHAULM@CS.TECHNION.AC.IL"], "sections": [{"heading": null, "text": "gence. A description of a planning task consists of an initial world state, a goal, and a set of actions for modifying the world state. The objective is to find a sequence of actions, that is, a plan, that transforms the initial world state into a goal state. In optimal planning, we are interested in finding not just a plan, but one of the cheapest plans. A prominent approach to optimal planning these days is heuristic state-space search, guided by admissible heuristic functions. Numerous admissible heuristics have been developed, each with its own strengths and weaknesses, and it is well known that there is no single \u201cbest\u201d heuristic for optimal planning in general. Thus, which heuristic to choose for a given planning task is a difficult question. This difficulty can be avoided by combining several heuristics, but that requires computing numerous heuristic estimates at each state, and the tradeoff between the time spent doing so and the time saved by the combined advantages of the different heuristics might be high. We present a novel method that reduces the cost of combining admissible heuristics for optimal planning, while maintaining its benefits. Using an idealized search space model, we formulate a decision rule for choosing the best heuristic to compute at each state. We then present an active online learning approach for learning a classifier with that decision rule as the target concept, and employ the learned classifier to decide which heuristic to compute at each state. We evaluate this technique empirically, and show that it substantially outperforms the standard method for combining several heuristics via their pointwise maximum.\n1. Introduction\nAt the center of the problem of intelligent autonomous behavior is the task of selecting the actions to take next. Planning in AI is best conceived as the model-based approach to automated action selection (Geffner, 2010). The models represent the current situation, goals, and possible actions. Planning-specific languages are used to describe such models concisely. The main challenge in planning is computational, as most planning languages lead to intractable problems in the worst case. However, using rigorous search-guidance tools often allows for efficient solving of interesting problem instances.\nIn classical planning, which is concerned with the synthesis of plans constituting goal-achieving sequences of deterministic actions, significant algorithmic progress has been achieved in the last two decades. In turn, this progress in classical planning is translated to advances in more involved planning languages, allowing for uncertainty and feedback (Yoon, Fern, & Givan, 2007; Palacios\nc\u00a92012 AI Access Foundation. All rights reserved.\n& Geffner, 2009; Keyder & Geffner, 2009; Brafman & Shani, 2012). In optimal planning, the objective is not just to find any plan, but to find one of the cheapest plans.\nA prominent approach to domain-independent planning, and to optimal planning in particular, is state-space heuristic search. It is very natural to view a planning task as a search problem, and use a heuristic search algorithm to solve it. Recent advances in automatic construction of heuristics for domain-independent planning established many heuristics to choose from, each with its own strengths and weaknesses. However, this wealth of heuristics leads to a new question: given a specific planning task, which heuristic to choose?\nIn this paper, we propose selective max \u2014 an online learning approach that combines the strengths of several heuristic functions, leading to a speedup in optimal heuristic-search planning. At a high level, selective max can be seen as a hyper-heuristic (Burke, Kendall, Newall, Hart, Ross, & Schulenburg, 2003) \u2014 a heuristic for choosing among other heuristics. It is based on the seemingly trivial observation that, for each state, there is one heuristic which is the \u201cbest\u201d for that state. In principle, it is possible to compute several heuristics for each state, and then choose one according to the values they provide. However, heuristic computation in domain-independent planning is typically expensive, and thus computing several heuristic estimates for each state takes a long time. Selective max works by predicting for each state which heuristic will yield the \u201cbest\u201d heuristic estimate, and computes only that heuristic.\nAs it is not always clear how to decide what the \u201cbest\u201d heuristic for each state is, we first analyze an idealized model of a search space and describe how to choose there the best heuristic for each state in order to minimize the overall search time. We then describe an online active learning procedure that uses a decision rule formulated for the idealized model. This procedure constitutes the essence of selective max.\nOur experimental evaluation, which we conducted using three state-of-the-art heuristics for domain-independent planning, shows that selective max is very effective in combining several heuristics in optimal search. Furthermore, the results show that using selective max results in a speedup over the baseline heuristic combination method, and that selective max is robust to different parameter settings. These claims are further supported by selective max having been a runnerup ex-aequo in the last International Planning Competition, IPC-2011 (Garc\u0131\u0301a-Olaya, Jime\u0301nez, & Linares Lo\u0301pez, 2011).\nThis paper expands on the conference version (Domshlak, Karpas, & Markovitch, 2010) in several ways. First, we improve and expand the presentation of the selective max decision rule. Second, we explain how to handle non-uniform action costs in a principled way. Third, the empirical evaluation is greatly extended, and now includes the results from IPC-2011, as well as controlled experiments with three different heuristics, and an exploration of how the parameters of selective max affect its performance.\n2. Previous Work\nSelective max is a speedup learning system. In general, speedup learning is concerned with improving the performance of a problem solving system with experience. The computational difficulty of domain-independent planning has led many researchers to use speedup learning techniques in order to improve the performance of planning systems; for a survey of many of these, see the work of Minton (1994), Zimmerman and Kambhampati (2003), and Fern, Khardon, and Tadepalli (2011).\nSpeedup learning systems can be divided along several dimensions (Zimmerman & Kambhampati, 2003; Fern, 2010). Arguably the most important dimension is the phase in which learning takes place. An offline, or inter-problem, speedup learner analyzes the problem solver\u2019s performance on different problem instances in an attempt to formulate some rule which would not only improve this performance but would also generalize well to future problem instances. Offline learning has been applied extensively to domain-independent planning, with varying degrees of success (Fern et al., 2011). However, one major drawback of offline learning is the need for training examples \u2014 in our case, planning tasks from the domains of interest.\nLearning can also take place online, during problem solving. An online, or intra-problem, speedup learner is invoked by the problem solver on a concrete problem instance the solver is working on, and it attempts to learn online, with the objective of improving the solver\u2019s performance on that specific problem instance being solved. In general, online learners are not assumed to be pretrained on some other, previously seen problem instances; all the information they can rely on has to be collected during the process of solving the concrete problem instance they were called for. Online learning has been shown to be extremely helpful in propositional satisfiability (SAT) and general constraint satisfaction (CSP) solving, where nogood learning and clause learning are now among the essential components of any state-of-the-art solver (Schiex & Verfaillie, 1993; Marques-Silva & Sakallah, 1996; Bayardo Jr. & Schrag, 1997). Thus, indirectly, SAT- and CSP-based domainindependent planners already benefit from these online learning techniques (Kautz & Selman, 1992; Rintanen, Heljanko, & Niemela\u0308, 2006). However, to the best of our knowledge, our work is the first application of online learning to optimal heuristic-search planning.\n3. Background\nA domain-independent planning task (or planning task, for short) consists of a description of an initial state, a goal, and a set of available operators. Several formalisms for describing planning tasks are in use, including STRIPS (Fikes & Nilsson, 1971), ADL (Pednault, 1989), and SAS+ (Ba\u0308ckstro\u0308m & Klein, 1991; Ba\u0308ckstro\u0308m & Nebel, 1995). We describe the SAS+ formalism, the one used by the Fast Downward planner (Helmert, 2006), on top of which we have implemented and evaluated selective max. Nothing, however, precludes using selective max in the context of other formalisms.\nA SAS+ planning task is given by a 4-tuple \u03a0 = \u3008V,A, s0, G\u3009. V = {v1, . . . , vn} is a set of state variables, each associated with a finite domain dom(vi). A complete assignment s to V is called a state. s0 is a specified state called the initial state, and the goal G is a partial assignment to V . A is a finite set of actions. Each action a is given by a pair \u3008pre(a), eff(a)\u3009 of partial assignments to V called preconditions and effects, respectively. Each action a also has an associated cost C(a) \u2208 R0+. An action a is applicable in a state s iff s |= pre(a). Applying a changes the value of each state variable v to eff(a)[v] if eff(a)[v] is specified. The resulting state is denoted by sJaK. We denote the state obtained from sequential application of the (respectively applicable) actions a1, . . . , ak starting at state s by sJ\u3008a1, . . . , ak\u3009K. Such an action sequence is a plan if s0J\u3008a1, . . . , ak\u3009K |= G. In optimal planning, we are interested in finding one of the cheapest plans, where the cost of a plan \u3008a1, . . . , ak\u3009 is the sum of its constituent action costs \u2211k i=1 C(ai).\nA SAS+ planning task \u03a0 = \u3008V,A, s0, G\u3009 can be easily seen as a state-space search problem whose states are simply complete assignments to the variables V , with transitions uniquely determined by the actions A. The initial and goal states are also defined by the initial state and goal of \u03a0. An optimal solution for a state-space search problem can be found by using the A\u2217 search algorithm\nwith an admissible heuristic h. A heuristic evaluation function h assigns an estimate of the distance to the closest goal state from each state it evaluates. The length of a cheapest path from state s to the goal is denoted by h\u2217(s), and h is called admissible if it never overestimates the true goal distance \u2014 that is, if h(s) \u2264 h\u2217(s) for any state s. A\u2217 works by expanding states in the order of increasing f(s) := g(s) + h(s), where g(s) is the cost of the cheapest path from the initial state to s known so far.\n4. Selective Max as a Decision Rule\nMany admissible heuristics have been proposed for domain-independent planning; these vary from cheap to compute yet not very accurate, to more accurate yet expensive to compute. In general, the more accurate a heuristic is, the fewer states would be expanded by A\u2217 when using it. As the accuracy of heuristic functions varies for different planning tasks, and even for different states of the same task, we may be able to produce a more robust optimal planner by combining several admissible heuristics. Presumably, each heuristic is more accurate, that is, provides higher estimates, in different regions of the search space. The simplest and best-known way for doing that is using the point-wise maximum of the heuristics in use at each state. Given n admissible heuristics, h1, . . . , hn, a new heuristic, maxh, is defined by maxh(s) := max1\u2264i\u2264n hi(s). It is easy to see that maxh(s) \u2265 hi(s) for any state s and for any heuristic hi. Thus A\u2217 search using maxh is expected to expand fewer states than A\u2217 using any individual heuristic. However, if we denote the time needed to compute hi by ti, the time needed to compute maxh is \u2211n i=1 ti.\nAs mentioned previously, selective max is a form of hyper-heuristic (Burke et al., 2003) that chooses which heuristic to compute at each state. We can view selective max as a decision rule dr, which is given a set of heuristics h1, . . . , hn and a state s, and chooses which heuristic to compute for that state. One natural candidate for such a decision rule is the heuristic which yields the highest, that is, most accurate, estimate:\ndrmax({h1, . . . , hn}, s) := hargmax1\u2264i\u2264n hi(s).\nUsing this decision rule yields a heuristic which is as accurate as maxh, while still computing only one heuristic per state \u2014 in time targmax1\u2264i\u2264n hi(s).\nThis analysis, however, does not take into account the different computation times of the different heuristics. For instance, let h1 and h2 be a pair of admissible heuristics such that h2 \u2265 h1. A priori, it seems that using h2 should always be preferred to using h1 because the former should cause A\u2217 to expand fewer states. However, suppose that on a given planning task, A\u2217 expands 1000 states when guided by h1 and only 100 states when guided by h2. If computing h1 for each state takes 10 ms, and computing h2 for each state takes 1000 ms, then switching from h1 to h2 increases the overall search time. Using maxh over h1 and h2 only makes things worse, because h2 \u2265 h1, and thus computing the maximum simply wastes the time spent on computing h1. It is possible, however, that computing h2 for a few carefully chosen states, and computing h1 for all other states, would result in expanding 100 states, while reducing the overall search time when compared to running A\u2217 with only h2.\nAs this example shows, even given knowledge of the heuristics\u2019 estimates in advance, it is not clear what heuristic should be computed at each state when our objective is to minimize the overall search time. Therefore, we begin by formulating a decision rule for choosing between one of two heuristics, with respect to an idealized state-space model. Selective max then operates as an online\nactive learning procedure, attempting to predict the outcome of that decision rule and choose which heuristic to compute at each state.\n4.1 Decision Rule with Perfect Knowledge\nWe now formulate a decision rule for choosing which of two given admissible heuristics, h1 and h2, to compute for each state in an idealized search space model. In order to formulate such a decision rule, we make the following assumptions:\n\u2022 The search space is a tree with a single goal, constant branching factor b, and uniform cost actions. Such an idealized search space model was used in the past to analyze the behavior of A\u2217 (Pearl, 1984).\n\u2022 The time ti required for computing heuristic hi is independent of the state being evaluated; w.l.o.g. we assume t2 \u2265 t1.\n\u2022 The heuristics are consistent. A heuristic h is said to be consistent if it obeys the triangle inequality: For any two states s, s\u2032, h(s) \u2264 h(s\u2032) + k(s, s\u2032), where k(s, s\u2032) is the optimal cost of reaching s\u2032 from s.\n\u2022 We have: (i) perfect knowledge about the structure of the search tree, and in particular the cost of the optimal solution c\u2217, (ii) perfect knowledge about the heuristic estimates for each state, and (iii) a perfect tie-breaking mechanism.\nObviously, none of the above assumptions holds in typical search problems, and later we examine their individual influence on our framework.\nAdopting the standard notation, let g(s) be the cost of the cheapest path from s0 to s. Defining maxh(s) = max(h1(s), h2(s)), we then use the notation f1(s) = g(s) + h1(s), f2(s) = g(s) + h2(s), and maxf (s) = g(s) + maxh(s). The A\u2217 algorithm with a consistent heuristic h expands states in increasing order of f = g + h (Pearl, 1984). In particular, every state s with f(s) < h\u2217(I) = c\u2217 will surely be expanded by A\u2217, and every state with f(s) > c\u2217 will surely not be\nexpanded by A\u2217. The states with f(s) = c\u2217 might or might not be expanded by A\u2217, depending on the tie-breaking rule being used. Under our perfect tie-breaking assumption, the only states with f(s) = c\u2217 that will be expanded are those that lie along some optimal plan.\nLet us consider the states satisfying f1(s) = c\u2217 (the dotted line in Fig. 1) and those satisfying f2(s) = c\u2217 (the solid line in Fig. 1). The states above the f1 = c\u2217 and f2 = c\u2217 contours are those that are surely expanded by A\u2217 with h1 and h2, respectively. The states above both these contours (the grid-marked region in Fig. 1), that is, the states SE = {s | maxf (s) < c\u2217}, are those that are surely expanded by A\u2217 using maxh (Pearl, 1984, Thm. 4, p. 79).\nUnder the objective of minimizing the search time, note that the optimal decision for any state s \u2208 SE is not to compute any heuristic at all, since all these states are surely expanded anyway. Assuming that we still must choose one of the heuristics, we would choose to compute the cheaper heuristic h1. Another easy case is when f1(s) \u2265 c\u2217. In these states, computing h1(s) suffices to ensure that s is not surely expanded, and using a perfect tie-breaking rule, s will not be expanded unless it must be. Because h1 is also cheaper to compute than h2, h1 should be preferred, regardless of the heuristic estimate of h2 for state s.\nLet us now consider the optimal decision for all other states, that is, those with f1(s) < c\u2217 and f2(s) \u2265 c\u2217. In fact, it is enough to consider only the shallowest such states; in Figure 1, these are the states on the part of the f2 = c\u2217 contour that separates between the grid-marked and line-marked areas. Since f1(s) and f2(s) are based on the same g(s), we have h2(s) > h1(s), that is, h2 is more accurate in state s than h1. If we were interested solely in reducing state expansions, then h2 would obviously be the right heuristic to compute at s. However, for our objective of reducing the actual search time, h2 may actually be the wrong choice because it might be much more expensive to compute than h1.\nLet us consider the effects of each of our two alternatives. If we compute h2(s), then s is not surely expanded, because f2(s) = c\u2217, and thus whether or not A\u2217 expands s depends on tiebreaking. As before, we are assuming perfect tie-breaking, and thus s will not be expanded unless it must be. Computing h2 would \u201ccost\u201d us t2 time.\nIn contrast, if we compute h1(s), then s is surely expanded because f1(s) < c\u2217. Note that not computing h2 for s and then computing h2 for one of the descendants s\u2032 of s is clearly a sub-optimal strategy as we do pay the cost of computing h2, yet the pruning of A\u2217 is limited only to the search sub-tree rooted in s\u2032. Therefore, our choices are really either computing h2 for s, or computing h1 for all the states in the sub-tree rooted in s that lie on the f1 = c\u2217 contour. Suppose we need to expand l complete levels of the state space from s to reach the f1 = c\u2217 contour. Thus, we need to generate an order of bl states, and then invest blt1 time in calculating h1 for all these states that lie on the f1 = c\u2217 contour.\nConsidering these two options, the optimal decision in state s is thus to compute h2 iff t2 < blt1, or to express it differently, if l > logb( t2 t1\n). As a special case, if both heuristics take the same time to compute, this decision rule reduces to l > 0, that is, the optimal choice is simply the more accurate heuristic for state s.\nPutting all of the above cases together yields the decision rule dropt, as below, with ls being the depth to go from s until f1(s) = c\u2217:\ndropt({h1, h2}, s) :=  h1, f1(s) < c\u2217, f2(s) < c\u2217 h1, f1(s) \u2265 c\u2217\nh1, f1(s) < c\u2217, f2(s) \u2265 c\u2217, ls \u2264 logb( t2t1 ) h2, f1(s) < c\u2217, f2(s) \u2265 c\u2217, ls > logb( t2t1 )\n.\n4.2 Decision Rule without Perfect Knowledge\nThe idealized model above makes several assumptions, some of which appear to be very problematic to meet in practice. Here we examine these assumptions more closely, and when needed, suggest pragmatic compromises.\nFirst, the model assumes that the search space forms a tree with a single goal state, that the heuristics in question are consistent, and that we have a perfect tie-breaking rule. Although the first assumption does not hold in most planning tasks, the second assumption is not satisfied by many state-of-the-art heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009; Bonet & Helmert, 2010), and the third assumption is not realistic, they do not prevent us from using the decision rule suggested by the model.\nThe idealized model also assumes that both the branching factor and the heuristic computation times are constant across the search states. In our application of the decision rule to planning in practice, we deal with this assumption by adopting the average branching factor and heuristic computation times, estimated from a random sample of search states.\nFinally, the decision rule dropt above requires unrealistic knowledge of both heuristic estimates, as well as of the optimal plan cost c\u2217 and the depth ls to go from state until f1(s) = c\u2217. As we obviously do not have this knowledge in practice, we must use some approximation of the decision rule.\nThe first approximation we make is to ignore the \u201ctrivial\u201d cases that require knowledge of c\u2217; these are the cases where either s is surely expanded, or h1 is enough to prune s. Instead, we apply the reasoning for the \u201ccomplicated\u201d case for all states, resulting in the following decision rule:\ndrapp1({h1, h2}, s) := { h1, ls \u2264 logb( t2t1 ) h2, ls > logb( t2 t1 ) .\nThe next step is to somehow estimate the \u201cdepth to go\u201d ls \u2014 the number of layers we need to expand in the tree until f1 reaches c\u2217. In order to derive a useful decision rule, we assume that ls has a positive correlation with \u2206h(s) = h2(s)\u2212h1(s); that is, if h1 and h2 are close, then ls is low, and if h1 yields a much lower estimate than h2, implying that h1 is not very accurate for s, then the depth to go until f1(s) = c\u2217 is large. Our approximation uses the simplest such correlation \u2014 a linear one \u2014 between \u2206h(s) and ls, with a hyper-parameter \u03b1 for controlling the slope.\nRecall that in our idealized model, all actions were unit cost, and thus cost-to-go and depthto-go are the same. However, some planning tasks, and notably, all planning tasks from the 2008 International Planning Competition, feature non-uniform action costs. Therefore, our decision rule converts heuristic estimates of cost-to-go into heuristic estimates of depth-to-go by dividing the cost-to-go estimate by the average action cost. We do this by modifying our estimate of the depthto-go, ls, with the average action cost, which we denote by c\u0302. Plugging all of the above into our\ndecision rule yields:\ndrapp2({h1, h2}, s) := { h1, \u2206h(s) \u2264 \u03b1 \u00b7 c\u0302 \u00b7 logb( t2t1 ) h2, \u2206h(s) > \u03b1 \u00b7 c\u0302 \u00b7 logb( t2t1 ) .\nGiven b, t1, t2, and c\u0302, the quantity \u03b1 \u00b7 c\u0302 \u00b7 logb(t2/t1) becomes fixed, and in what follows we denote it simply by threshold \u03c4 .\nNote that linear correlation between \u2206h(s) and ls occurs in some simple cases. The first such case is when the h1 value remains constant in the subtree rooted at s, that is, the additive error of h1 increases by 1 for each level below s. In this case, f1 increases by 1 for each expanded level of the sub-tree (because h1 remains the same, and g increases by 1), and it will take expanding exactly \u2206h(s) = h2(s) \u2212 h1(s) levels to reach the f1 = c\u2217 contour. The second such case is when the absolute error of h1 remains constant, that is, h1 increases by 1 for each level expanded, and so f1 increases by 2. In this case, we will need to expand \u2206h(s)/2 levels. This can be generalized to the case where the estimate h1 increases by any constant additive factor c, which results in \u2206h(s)/(c+1) levels being expanded.\nFurthermore, there is some empirical evidence to support our conclusion about exponential growth of the search effort as a function of heuristic error, even when the assumptions made by the model do not hold. In particular, the experiments of Helmert and Ro\u0308ger (2008) on IPC benchmarks with heuristics with small constant additive errors show that the number of expanded nodes most typically grows exponentially as the (still very small and additive) error increases.\nFinally, we remark that because our decision rule always chooses an admissible heuristic, the resulting heuristic estimate will always be admissible. Thus, even if the chosen heuristic is not the \u201ccorrect\u201d one according to dropt, this will not result in loss of optimality of the solution, but only in a possible increase in search time.\n5. Online Learning of the Decision Rule\nWhile decision rule drapp2 still requires knowledge of h1 and h2, we can now use it as a binary label for each state. We can compute the value of the decision rule by \u201cpaying\u201d the computation time of both heuristics, t1 + t2, and, more importantly, we can use a binary classifier to predict the value of this decision rule for some unknown state. Note that we use the classifier online, during the problem solving process, and the time spent on learning and classification is counted as time spent on problem solving. Furthermore, as in active learning, we can choose to \u201cpay\u201d for a label for some state, where the payment is also in computation time. Therefore we refer to our setting as active online learning.\nIn what follows, we provide a general overview of the selective max procedure, and describe several alternatives for each of its components. Our decision rule states that the more expensive heuristic h2 should be computed at a search state s when h2(s) \u2212 h1(s) > \u03c4 . This decision rule serves as a binary target concept, which corresponds to the set of states where the more expensive heuristic h2 is \u201dsignificantly\u201d more accurate than the cheaper heuristic h1 \u2014 the states where, according to our model, the reduction in expanded states by computing h2 outweighs the extra time needed to compute it. Selective max then uses a binary classifier to predict the value of the decision rule. There are several steps to building the classifier:\nevaluate(s) \u3008h, confidence\u3009 := CLASSIFY(s, model) if (confidence > \u03c1) then\nreturn h(s) else\nlabel := h1 if h2(s)\u2212 h1(s) > \u03b1 \u00b7 c\u0302 \u00b7 logb(t2/t1) then label := h2 update model with \u3008s, label\u3009 return max(h1(s), h2(s))\nAfter completing the steps described above, we have a binary classifier that can be used to predict the value of our decision rule. However, as the classifier is not likely to have perfect accuracy,\nwe further consult the confidence the classifier associates with its classification. The resulting state evaluation procedure of selective max is depicted in Figure 2. For every state s evaluated by the search algorithm, we use our classifier to decide which heuristic to compute. If the classification confidence exceeds a confidence threshold \u03c1, a parameter of selective max, then only the indicated heuristic is computed for s. Otherwise, we conclude that there is not enough information to make a selective decision for s, and compute the regular maximum over h1(s) and h2(s). However, we use this opportunity to improve the quality of our prediction for states similar to s, and update our classifier by generating a label based on h2(s)\u2212h1(s) and learning from the newly labeled example. These decisions to dedicate computation time to obtain a label for a new example constitute the active part of our learning procedure. It is also possible to update the estimates for b, t1, t2, and c\u0302, and change the threshold \u03c4 accordingly. However, this would result in the concept we are trying to learn constantly changing \u2014 a phenomenon known as concept drift \u2014 which usually affects learning adversely. Therefore, we do not update the threshold \u03c4 .\n5.1 State-Space Sampling\nThe initial state-space sample serves two purposes. First, it is used to estimate the branching factor b, the heuristic computation times t1 and t2, the average action cost c\u0302, and then to compute the threshold \u03c4 = \u03b1 \u00b7 c\u0302 \u00b7 logb(t2/t1), which is used to specify our concept. After the concept is specified, the state-space sample also provides us with a set of examples on which the classifier is initially trained. Therefore, it is important to have an initial state-space sample that is representative of the states which will be evaluated during search. The number of states in the initial sample is controlled by a parameter N .\nOne option is to use the first N states of the search. However, this method is biased towards states closer to the initial state, and therefore is not likely to represent the search space well. Thus, we discuss three more sophisticated state-space sampling procedures, all of which are based on performing random walks, or \u201cprobes,\u201d from the initial state. While the details of these sampling procedures vary, each such \u201cprobe\u201d terminates at some pre-set depth limit.\nThe first sampling procedure, which we refer to as \u201cbiased probes,\u201d uses an inverse heuristic selection bias for choosing the next state to go to in the probe. Specifically, the probability of choosing state s as the successor from which the random walk will continue is proportional to 1/maxh(s). This biases the sample towards states with lower heuristic estimates, which are more likely to be expanded during the search.\nThe second sampling procedure is similar to the first one, except that it chooses the successor uniformly, and thus we refer to it as \u201cunbiased probes.\u201d Both these sampling procedures add all of the generated states (that is, the states along the probe as well as their \u201csiblings\u201d) to the statespace sample, and they both terminate after collecting N training examples. The depth limit for all random walks is the same in both sampling schemes, and is set to some estimate of the goal depth; we discuss this goal depth estimate later.\nThe third state-space sampling procedure, referred to here as PDB sampling, has been proposed by Haslum, Botea, Helmert, Bonet, and Koenig (2007). This procedure also uses unbiased probes, but only adds the last state reached in each probe to the state-space sample. The depth of each probe is determined individually, by drawing a random depth from a binomial distribution around the estimated goal depth.\nNote that all three sampling procedures rely on some estimate of the minimum goal depth. When all actions are unit cost, the minimum goal depth is the same as h\u2217(s0), and thus we can use a heuristic to estimate it. In our evaluation, we used twice the heuristic estimate of the initial state, 2 \u00b7maxh(s0), as the goal depth estimate. However, with non-uniform action costs, goal depth and cost are no longer measured in the same units. While it seems we could divide the above heuristicbased estimate by the average action cost c\u0302, recall that we use the state-space sample in order to obtain an estimate for estimate c\u0302, thus creating a circular dependency. Although it is possible to estimate c\u0302 by taking the average cost of all actions in the problem description, there is no reason to assume that all actions are equally likely to be used. Another option is to modify the above state-space sampling procedures, and place a cost limit, rather than a depth limit, on each probe. However, this would pose a problem in the presence of 0-cost actions. In such a case, when a probe reaches its cost limit yet has a possible 0-cost action to apply, it is not clear whether the probe should terminate. Therefore, we keep using depth-limited probes and attempt to estimate the depth of the cheapest goal. We compute a heuristic estimate for the initial state, and then use the number of actions which the heuristic estimate is based on as our goal depth estimate. While this is not possible with every heuristic, we use in our empirical evaluation the monotonically-relaxed plan heuristic. This heuristic, also known as the FF heuristic (Hoffmann & Nebel, 2001), does provide such information: we first use this heuristic to find a relaxed plan from the initial state, and then use the number of actions in the relaxed plan as our goal depth estimate.\n5.2 Classifier\nThe last decision to be made is the choice of classifier. Although many classifiers can be used here, several requirements must be met due to our particular setup. First, both training and classification must be very fast, as both are performed during time-constrained problem solving. Second, the classifier must be incremental to support active learning. This is achieved by allowing online updates of the learned model. Finally, the classifier should provide us with a meaningful measure of confidence for its predictions.\nWhile several classifiers meet these requirements, we found the Naive Bayes classifier to provide a good balance between speed and accuracy. One note on the Naive Bayes classifier is that it assumes a very strong conditional independence between the features. Although this is not a fully realistic assumption for planning tasks, using a SAS+ task formulation in contrast to the classical STRIPS formulations helps a lot: instead of many highly dependent binary variables, we have a much smaller set of less dependent ones.\nAlthough, as the empirical evaluation will demonstrate, Naive Bayes appears to be the most suitable classifier to use with selective max, other classifiers can also be used. The most obvious choice for a replacement classifier would be a different Bayesian classifier. One such classifier is AODE (Webb, Boughton, & Wang, 2005), an extension of Naive Bayes, which somewhat relaxes the assumption of independence between the features, and is typically more accurate than Naive Bayes. However, this added accuracy comes at the cost of increased training and classification time.\nDecision trees are another popular type of classifier that allows for even faster classification. While most decision tree induction algorithms are not incremental, the Incremental Tree Inducer (ITI) algorithm (Utgoff, Berkman, & Clouse, 1997) supports incremental updating of decision trees by tree restructuring, and also has a freely available implementation in C. In our evaluation, we used ITI in incremental mode, and incorporated every example into the tree immediately, because the\ntree is likely to be used for many classifications between pairs of consecutive updates with training examples from active learning. The classification confidence with the ITI classifier is obtained by the frequency of examples at the leaf node from which the classification came.\nA different family of possible classifiers is k-Nearest Neighbors (kNN) (Cover & Hart, 1967). In order to use kNN, we need a distance metric between examples, which, with our features, are simply states. As with our choice of features, we opt for simplicity and use Euclidean distance as our metric. kNN enjoys very fast learning time but suffers from slow classification time. The classification confidence is obtained by a simple (unweighted) vote between the k nearest neighbors.\nAnother question related to the choice of classifier is feature selection. In some planning tasks, the number of variables, and accordingly, features, can be over 2000 (for example, task 35 of the AIRPORT domain has 2558 variables). While the performance of Naive Bayes and kNN can likely be improved using feature selection, doing so poses a problem when the initial sample is considered. Since feature selection will have to be done right after the initial sample is obtained, it will have to be based only on the initial sample. This could cause a problem since some features might appear to be irrelevant according to the initial sample, yet turn out to be very relevant when active learning is used after some low-confidence states are encountered. Therefore, we do not use feature selection in our empirical evaluation of selective max.\n5.3 Extension to Multiple Heuristics\nTo this point, we have discussed how to choose which heuristic to compute for each state when there are only two heuristics to choose from. When given more than two heuristics, the decision rule presented in Section 4 is inapplicable, and extending it to handle more than two heuristics is not straightforward. However, extending selective max to use more than two heuristics is straightforward \u2014 simply compare heuristics in a pair-wise manner, and use a voting rule to choose which heuristic to compute.\nWhile there are many possible such voting rules, we go with the simplest one, which compares every pair of heuristics, and chooses the winner by a vote, weighted by the confidence for each pairwise decision. The overall winner is simply the heuristic which has the highest total confidence from all pairwise comparisons, with ties broken in favor of the cheaper-to-compute heuristic. Although this requires a quadratic number of classifiers, training and classification time (at least with Naive Bayes) appear to be much lower than the overall time spent on heuristic computations, and thus the overhead induced by learning and classification is likely to remain relatively low for reasonable heuristic ensembles.\n6. Experimental Evaluation\nTo evaluate selective max empirically, we implemented it on top of the open-source Fast Downward planner (Helmert, 2006). Our empirical evaluation is divided into three parts. First, we examine the performance of selective max using the last International Planning Competition, IPC-2011, as our benchmark. Selective max was the runner-up ex-aequo at IPC-2011, tying for 2nd place with a version of Fast Downward using an abstraction \u201cmerge-and-shrink\u201d heuristic (Nissim, Hoffmann, & Helmert, 2011), and losing to a sequential portfolio combining the heuristics used in both runners-up (Helmert, Ro\u0308ger, & Karpas, 2011). Second, we present a series of controlled parametric experiments, where we examine the behavior of selective max under different settings. Finally, we\ncompare selective max to a simulated sequential portfolio, using the same heuristics as selective max.\n6.1 Performance Evaluation: Results from IPC-2011\nThe IPC-2011 experiments (Garc\u0131\u0301a-Olaya et al., 2011) were run by the IPC organizers, on their own machines, with a time limit of 30 minutes and a memory limit of 6 GB per planning task. The competition included some new domains, which none of the participants had seen before, thus precluding the participants from using offline learning approaches.\nAlthough many planners participated in the sequential optimal track of IPC-2011, we report here only the results relevant to selective max. The selective max entry in IPC-2011 was called selmax, and consisted of selective max over the uniform action cost partitioning version of hLA (Karpas & Domshlak, 2009) and hLM-CUT (Helmert & Domshlak, 2009) heuristics. The parameters used for selective max in IPC-2011 are reported in Table 1. Additionally, each of the heuristics selmax used was entered individually as BJOLP (hLA) and lmcut (hLM-CUT), and we report results for all three planners. While a comparison of selective max with the regular maximum of hLA and hLM-CUT would be interesting, there was no such entry at IPC-2011, and thus we can not report on it. In our controlled experiments, we do compare selective max to the regular maximum, as well as to other baseline combination methods.\nFigure 3 shows the anytime profile of these three planners on IPC-2011 tasks, plotting the number of tasks solved under different timeouts, up to the time limit of 30 minutes. Additionally, Table 2 shows the number of tasks solved in each domain of IPC-2011, after 30 minutes, and includes the number of problems solved by the winner, Fast Downward Stone Soup 1 (FDSS-1), for reference.\nAs these results show, selective max solves more problems than each of the individual heuristics it uses. Furthermore, the anytime profile of selective max dominates each of these heuristics, in the range between 214 seconds until the full 30 minute timeout. The behavior of the anytime plot with shorter timeouts is due to the overhead of selective max, which consists of obtaining the initial statespace sample, as well as learning and classification. However, it appears that selective max quickly compensates for its relatively slow start.\n6.2 Controlled Experiments\nIn our series of controlled experiments, we attempted to evaluate the impact of different parameters on selective max. We controlled the following independent variables:\n\u2022 Heuristics: We used three state-of-the-art admissible heuristics: hLA (Karpas & Domshlak, 2009), hLM-CUT (Helmert & Domshlak, 2009), and hLM-CUT+ (Bonet & Helmert, 2010). None\nof these base heuristics yields better search performance than the others across all planning domains. Of these heuristics, hLA is typically the fastest to compute and the least accurate, hLM-CUT is more expensive to compute and more accurate, and hLM-CUT+ is the most expensive to compute and the most accurate.1 From the data we have gathered in these experiments, hLM-CUT takes on average 4.5 more time per state than hLA, and hLM-CUT+ takes 53 more time per state than hLA. We evaluate selective max with all possible subsets of two or more of these three heuristics.\nWhile there are other admissible heuristics for SAS+ planning that are competitive with the three above (for example, Helmert, Haslum, & Hoffmann, 2007; Nissim et al., 2011; Katz & Domshlak, 2010), they are based on expensive offline preprocessing, followed by very fast online per-state computation. In contrast, hLA, hLM-CUT and hLM-CUT+ perform most of their computation online, and thus can be better exploited by selective max.\nAdditionally, we empirically examine the effectiveness of selective max in deciding whether to compute a heuristic value at all. This is done by combining our most accurate heuristic, hLM-CUT+ , with the blind heuristic.\n\u2022 Heuristic difference bias \u03b1: The hyper-parameter \u03b1 controls the tradeoff between computation time and heuristic accuracy. Setting \u03b1 = 0 sets the threshold \u03c4 to 0, forcing the decision rule to always choose the more accurate heuristic. Increasing \u03b1 increases the threshold, forcing the decision rule to choose the more accurate heuristic h2 only if its value is much higher than that of h1. We evaluate selective max with values for \u03b1 of 0.1, 0.5, 1, 1.5, 2, 3, 4, and 5.\n\u2022 Confidence threshold \u03c1: The confidence threshold \u03c1 controls the active learning part of selective max. Setting \u03c1 = 0.5 turns off active learning completely, because the chosen heuristic always comes with a confidence of at least 0.5. Setting \u03c1 = 1 would mean using active learning almost always, essentially reducing selective max to regular point-wise maximization. We evaluate selective max with values for \u03c1 of 0.51, 0.6, 0.7, 0.8, 0.9, and 0.99.\n\u2022 Initial sample size N : The initial sample size N is an important parameter, not just because it is used to train the initial classifier before any active learning is done, but also because it is the only source of estimates for branching factor, average action cost, and heuristic computation times. It thus affects the threshold \u03c4 : Increasing N increases the accuracy of the initial classifier and of the various aforementioned estimates, but also increases the preprocessing time. We evaluate selective max with values for N of 10, 100, and 1000.\n\u2022 Sampling method: The sampling method used to obtain the initial state-space sample is important in that it affects this initial sample, and thus the accuracy of both the threshold \u03c4 and of the initial classifier. We evaluate selective max with three different sampling methods, all described in Section 5.1: biased probes (selPh ), unbiased probes (sel UP h ), and the sampling\nmethod of Haslum et al. (2007) (selPDBh ).\n\u2022 Classifier: The choice of classifier is also very important. The Naive Bayes classifier combines very fast learning and classification (selNBh ). A more sophisticated variant of Naive Bayes called AODE (Webb et al., 2005) is also considered here (selAODEh ). AODE is more\n1. Of course, all three heuristics are computable in polynomial time from the SAS+ description of the planning task.\naccurate than Naive Bayes, but has higher classification and learning times, as well as increased memory overhead. Another possible choice is using incremental decision trees (Utgoff et al., 1997), which offer even faster classification, but more expensive learning when the tree structure needs to be changed (selITIh ). We also consider kNN classifiers (Cover & Hart, 1967), which offer faster learning than Naive Bayes, but usually more expensive classification, especially as k grows larger (selkNNh , for k = 3, 5).\nTable 3 describes our default values for each of these independent variables. In each of the subsequent experiments, we vary one of these independent variables, keeping the rest at their default values. In all of these experiments, the search for each planning task instance was limited to 30 minutes2 and to 3 GB of memory. The search times do not include the time needed for translating the planning task from PDDL to SAS+ and building some of the Fast Downward data structures, which is common to all planners, and is tangential to the issues considered in our study. The search times do include learning and classification time for selective max.\n\u2022 Heuristics We begin by varying the set of heuristics in use. For every possible choice of two or more heuristics out of the uniform action cost partitioning version of hLA (which we simply refer to as hLA), hLM-CUT and hLM-CUT+ , we compare selective max to other methods of heuristic combination, as well as to the individual heuristics. We compare selective max (selh) to the regular maximum (maxh), as well as to a planner which chooses which heuristic to compute at each state randomly (rndh). As it is not clear whether the random choice should favor the more expensive and accurate heuristic or the cheaper and less accurate one, we simply use a uniform random choice.\nThis experiment was conducted on all 31 domains with no conditional effects and axioms (which none of the heuristics we used support) from the International Planning Competitions 1998\u20132008. Because domains vary in difficulty and in the number of tasks, we normalize the score for each planner in each domain between 0 and 1. Normalizing by the number of problems in the domain is not a good idea, as it is always possible to generate any number of effectively unsolvable problems in each domain, so that the fraction of solved problems will approach zero. Therefore, we normalize the number of problems solved in each domain by the number of problems in that domain that were solved by at least one of our planners. While this measure of normalized coverage has the undesirable property that introducing a\n2. Each search was given a single core of a 3GHz Intel E8400 CPU machine.\nnew planner could change the normalized coverage of the other planners, we believe that it best reflects performance nonetheless. As an overall performance measure, we list the average normalized coverage score across all domains. Using normalized coverage means that domains have equal weight in the aggregate score. Additionally, we list for each domain the number of problems that were solved by any planner (in parentheses next to the domain name), and for each planner we list the number of problems it solved in parentheses.\nTables 4 and 5 summarize the results of this experiment. We divided the domains in our experiment into 3 sets: domains with non-uniform action costs, domains with unit action costs which exhibited a high variance in the number of problems solved between different\nHeuristics Domains hLA hLM-CUT hLM-CUT+ maxh rndh selh\nhLA / hLM-CUT\nHigh variance unit cost 3.23 2.8 1.0 3.88 1.46 Low variance unit cost 3.48 1.14 1.0 2.14 1.2 Non-uniform cost 13.23 1.01 1.0 3.99 1.17 TOTAL 4.82 1.4 1.0 2.93 1.25\nhLA / hLM-CUT+\nHigh variance unit cost 4.01 1.77 1.0 3.17 2.16 Low variance unit cost 4.55 1.01 1.0 2.38 1.85 Non-uniform cost 13.66 1.0 1.0 3.85 1.72 TOTAL 5.85 1.16 1.0 2.9 1.89\nhLM-CUT / hLM-CUT+\nHigh variance unit cost 2.29 1.01 1.0 1.7 1.24 Low variance unit cost 1.58 1.01 1.0 1.29 1.19 Non-uniform cost 1.32 1.03 1.0 1.18 1.16 TOTAL 1.66 1.01 1.0 1.35 1.2\nhLA / hLM-CUT / hLM-CUT+ High variance unit cost 4.06 3.81 1.78 1.0 3.61 2.1 Low variance unit cost 4.65 1.59 1.02 1.0 2.05 1.57 Non-uniform cost 15.2 1.37 1.03 1.0 2.74 1.49 TOTAL 6.1 1.91 1.18 1.0 2.56 1.67\nthe classifier used in selective max are important to its success, and that computing only one heuristic at each state randomly is insufficient, to say the least.\nWhen compared to individual heuristics, selective max does at least as well as each of the individual heuristics it uses, for all combinations except that of hLM-CUT and hLM-CUT+ . This is most likely because hLM-CUT and hLM-CUT+ are based on a very similar procedure, and thus their heuristic estimates are highly correlated. To see why this hinders selective max, consider the extreme case of two heuristics which have a correlation of 1.0 (that is, yield the same heuristic values), where selective max can offer no benefit. Finally, we remark that the best planner in this experiment was the selective max combination of hLA and hLM-CUT.\nThe above results are all based on a 30 minute time limit, which, while commonly used in the IPC, is arbitrary, and the number of tasks solved after 30 minutes does not tell the complete tale. Here, we examine the anytime profile of the different heuristic combination methods, by plotting the number of tasks solved under different timeouts, up to a timeout of 30 minutes. Figure 4 shows this plot for the three combination methods when all three heuristics are used. As the figure shows, the advantage of selh over the baseline combination methods is even greater under shorter timeouts. This indicates that the advantage of selh over maxh is even\nHeuristics Overhead hLA / hLM-CUT 12%\nhLA / hLM-CUT+ 15%\nhLM-CUT / hLM-CUT+ 9%\nhLA / hLM-CUT / hLM-CUT+ 10%\nThe above experiments have varied the heuristics which selective max uses. In the following experiments, we fix the set of heuristics, and examine the impact of the other parameters of selective max on performance. As we still need to evaluate over 20 different configurations of selective max, we will focus on eight selected domains: AIRPORT, FREECELL, LOGISTICS00, MPRIME, MYSTERY, PIPESWORLD-TANKAGE, SATELLITE, and ZENOTRAVEL. These are the eight domains with the highest observed variance in the number of tasks solved across different planners, out of the unit action cost domains we used. These domains were chosen in order to reduce the computation time required for these experiments to a manageable quantity. We excluded domains with non-uniform action costs, because they use a different method of estimating the goal depth for the state-space sampling method, which is one of the parameters we examine. Below, we focus on one parameter of selective max at a time, and present the total number of tasks solved in our eight chosen domains, under different values of that parameter. Detailed, per-domain results for each parameter appear in Appendix A.\n\u2022 hyper-parameter \u03b1 Figure 5a plots the total number of problems solved, under different values of \u03b1. As these results show, selective max is fairly robust with respect to the value of \u03b1, unless a very large value for \u03b1 is chosen, making it more difficult for selective max to choose the more accurate heuristic.\nDetailed, per-domain results appear in Table 19 in Appendix A, as well as in Figure 6. These results show a more complex picture, where there seems to be some cutoff value for each domain, such that increasing \u03b1 past that value impairs performance. The one exception to this is the PIPESWORLD-TANKAGE domain, where setting \u03b1 = 5 helps.\n\u2022 confidence threshold \u03c1 Figure 5b plots the total number of problems solved, under different values of \u03c1, Detailed, per-domain results appear in Table 20 in Appendix A. These results indicate that selective max is also robust to values of \u03c1, unless it is set to a very low value, causing selective max to behave like the regular point-wise maximum.\n\u2022 initial sample size N Figure 5c plots the total number of problems solved under different values of N . with the x-axis in logscale. Detailed, per-domain results appear in Table 21 in Appendix A. As the results show, our default value of N = 100 is the best (of the three values we tried), although selective max is still fairly robust with respect to the choice of parameter.\n\u2022 sampling method Figure 7 shows the total number of problems solved using different methods for the initial state-space sampling. Detailed, per-domain results appear in Table 22 in Appendix A. As the results demonstrate, the choice of sampling method can notably affect the performance of selective max. However, as the detailed results show, this effect is only evident in the FREECELL domain. We also remark that our default sampling method, PDB, performs worse than the others. Indeed by using the probe based sampling methods, selective max outperforms A\u2217\nusing hLA alone. However, as this difference is only due to the FREECELL domain, we can not state with certainty that this would generalize across all domains.\n\u2022 classifier\nFigure 8 shows the total number of problems solved using different classifiers. Detailed, per-domain results appear in Table 23 in Appendix A. Naive Bayes appears to be the best classifier to use with selective max, although AODE also performs quite well. Even though kNN enjoys very fast learning, the classifier is used mostly for classification, and as expected, kNN does not do well. However, the increased accuracy of k = 5 seems to pay off against the faster classification when k = 3.\n6.3 Comparison with Sequential Portfolios\nSequential portfolio solvers for optimal planning are another approach for exploiting the merits of different heuristic functions, and they have been very successful in practice, with the Fast Downward Stone Soup sequential portfolio (Helmert et al., 2011) winning the sequential optimal track at IPC2011. A sequential portfolio utilizes different solvers by running them sequentially, each with a prespecified time limit. If one solver fails to find a solution under its allotted time limit, the sequential portfolio terminates it, and moves on to the next solver. However, a sequential portfolio solver needs to know the time allowance for the problem it is trying to solve beforehand, a setting known as contract anytime (Russell & Zilberstein, 1991). In contrast, selective max can be used in an interruptible anytime manner, where the time limit need not be known in advance.\nHere, we compare selective max to sequential portfolios of A\u2217 with the same heuristics. As we have the exact time it took A\u2217 search using each heuristic alone to solve each problem, we can determine whether a sequential portfolio which assigns each heuristic some time limit will be able to solve each problem. Using this data, we simulate the results of two types of sequential portfolio planners. In the first setting, we assume that the time limit is known in advance, and simulate the results of a contract portfolio giving an equal share of time to all heuristics. In the second setting, we simulate an interruptible anytime portfolio by using binary exponential backoff time limits: starting\nwith a time limit of 1 second for each heuristic, we increase the time limit by a factor of 2 if none of the heuristics were able to guide A\u2217 to solve the planning problem. There are several possible orderings for the heuristics here, and we use the de facto best ordering for each problem. We denote the contract anytime portfolio by portctr, and the interruptible anytime portfolio by portint.\nFigure 9 shows the number of problems solved under different time limits for selective max, the contract anytime sequential portfolio, and the interruptible anytime sequential portfolio. As these results show, the contract anytime sequential portfolio almost always outperforms selective max. On the other hand, when the sequential portfolio does not know the time limit in advance, its performance deteriorates significantly. The best heuristic combination for selective max, hLA and hLM-CUT, outperforms the interruptible anytime portfolio using the same heuristics, and so does the\nselective max combination of hLM-CUT and hLM-CUT+ . With the other combinations of heuristics, the interruptible anytime portfolio performs better than selective max.\n7. Discussion\nLearning for planning has been a very active field since the early days of planning (Fikes, Hart, & Nilsson, 1972), and is recently receiving growing attention in the community. However, despite some early work (Rendell, 1983), relatively little work has dealt with learning for state-space search guided by distance-estimating heuristics, one of the most prominent approaches to planning these days. Most works in this direction have been devoted to learning macro-actions (see, for example, Finkelstein & Markovitch, 1998; Botea, Enzenberger, Mu\u0308ller, & Schaeffer, 2005; Coles & Smith, 2007). Recently, learning for heuristic search planning has received more attention: Yoon et al. (2008) suggested learning (inadmissible) heuristic functions based upon features extracted from relaxed plans. Arfaee, Zilles, and Holte (2010) attempted to learn an almost admissible heuristic estimate using a neural network. Perhaps the most closely related work to ours is that of Thayer, Dionne, and Ruml (2011), who learn to correct errors in heuristic estimates online. Thayer et al. attempt to improve the accuracy of a single given heuristic, while selective max attempts to choose one of several given heuristics for each state. The two works differ technically on this point. More importantly, however, none of the aforementioned approaches can guarantee that the resulting heuristic will be admissible, and thus that an optimal solution will be found. In contrast, our focus is on optimal planning, and we are not aware of any previous work that deals with learning for optimal heuristic search.\nOur experimental evaluation demonstrates that selective max is a more effective method for combining arbitrary admissible heuristics than the baseline point-wise maximization. Also advantageous is selective max\u2019s ability to exploit pairs of heuristics, where one is guaranteed to always be at least as accurate as the other. For example, the hLA heuristic can be used with two action cost partitioning schemes: uniform and optimal (Karpas & Domshlak, 2009). The heuristic induced by the optimal action cost partitioning is at least as accurate the one induced by the uniform action cost partitioning, but takes much longer to compute. Selective max might be used to learn when it is worth spending the extra time to compute the optimal cost partitioning, and when it is not. In contrast, the max-based combination of these two heuristics would simply waste the time spent on computing the uniform action cost partitioning.\nThe controlled parametric experiments demonstrate that the right choice of classifier and of the sampling method for the initial state-space sample is very important. The other parameters of selective max do not appear to affect performance too much, as long as they are set to reasonable values. This implies that selective max could be improved by using faster, more accurate, classifiers, and by developing sampling methods that can represent the state-space well.\nFinally, we remark that the Fast Downward Autotune entry in the sequential optimal track of the 2011 edition of the International Planning Competition, which used ParamILS (Hutter, Hoos, Leyton-Brown, & Stu\u0308tzle, 2009) to choose the \u201cbest\u201d configuration for the Fast Downward planner, chose to use selective-max to combine hLM-CUT and hmax (Bonet, Loerincs, & Geffner, 1997). This provides further evidence that selective max is a practically valuable method for combining heuristics in optimal planning.\nAcknowledgments\nThe work was partly supported by the Israel Science Foundation (ISF) grant 1045/12.\nAppendix A. Detailed Results of Empirical Evaluation\nIn this appendix, we present detailed per-domain, results of the experiments described in Section 6.\nTable 9 shows the normalized coverage and number of problems solved in each domain, for individual heuristics. The normalized coverage score of planner X on domain D is the number of problems from domain D solved by planner X , divided by the number of problems from domain D solved by at least one planner. Tables 10 \u2013 17 give the results for combinations of two or more heuristics. Tables 10, 12, 14, and 16 list the normalized coverage of the individual heuristics used, and of their combination using selective max (selh), regular maximum (maxh), and random choice of heuristic at each state (rndh) after 30 minutes. Tables 11, 13, 15, and 17 give the geometric mean of the ratio of expanded states relative to maxh in each domain, over problems solved by all configurations. The number of tasks solved by all planners is listed in parentheses next to each domain. The final row gives the geometric mean over the geometric means of each domain.\nTable 18 lists the average overhead of selective max in each domain, for each combination of two or more heuristics.\nTables 19, 20, 21, 22 and 23 list the number of problems solved in each domain, under various values for \u03b1, \u03c1, N , sampling method and classifier, respectively.\nTables 24, 25, 26 and 27 list the normalized coverage in each domain for selective max, and for the simulated contract and interruptible sequential portfolios.\nReferences\nArfaee, S. J., Zilles, S., & Holte, R. C. (2010). Bootstrap learning of heuristic functions. In Felner, A., & Sturtevant, N. (Eds.), Proceedings of the Third Annual Symposium on Combinatorial Search (SoCS 2010), pp. 52\u201360. AAAI Press.\nBa\u0308ckstro\u0308m, C., & Klein, I. (1991). Planning in polynomial time: the SAS-PUBS class. Computational Intelligence, 7(3), 181\u2013197.\nBa\u0308ckstro\u0308m, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational Intelligence, 11(4), 625\u2013655.\nBayardo Jr., R. J., & Schrag, R. (1997). Using CSP look-back techniques to solve real-world SAT instances. In Kuipers, B., & Webber, B. L. (Eds.), Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI 1997), pp. 203\u2013208. AAAI Press.\nBonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets. In Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the 19th European Conference on Artificial Intelligence (ECAI 2010), pp. 329\u2013334. IOS Press.\nBonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism for planning. In Kuipers, B., & Webber, B. L. (Eds.), Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI 1997), pp. 714\u2013719. AAAI Press.\nBotea, A., Enzenberger, M., Mu\u0308ller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI planning with automatically learned macro-operators. Journal of Artificial Intelligence Research, 24, 581\u2013621.\nBrafman, R., & Shani, G. (2012). A multi-path compilation approach to contingent planning. In Hoffmann, J., & Selman, B. (Eds.), Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI 2012), pp. 9\u201315. AAAI Press.\nBurke, E., Kendall, G., Newall, J., Hart, E., Ross, P., & Schulenburg, S. (2003). Hyper-Heuristics: An Emerging Direction in Modern Search Technology. In Handbook of Metaheuristics, International Series in Operations Research & Management Science, chap. 16, pp. 457\u2013474.\nColes, A., & Smith, A. (2007). Marvin: A heuristic search planner with online macro-action learning. Journal of Artificial Intelligence Research, 28, 119\u2013156.\nCover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1), 21 \u2013 27.\nde la Rosa, T., Jime\u0301nez, S., & Borrajo, D. (2008). Learning relational decision trees for guiding heuristic planning. In Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), Proceedings of the Eighteenth International Conference on Automated Planning and Scheduling (ICAPS 2008), pp. 60\u201367. AAAI Press.\nDomshlak, C., Karpas, E., & Markovitch, S. (2010). To max or not to max: Online learning for speeding up optimal planning. In Fox, M., & Poole, D. (Eds.), Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence (AAAI 2010), pp. 1071\u20131076. AAAI Press.\nFern, A. (2010). Speedup learning. In Sammut, C., & Webb, G. I. (Eds.), Encyclopedia of Machine Learning, pp. 907\u2013911. Springer.\nFern, A., Khardon, R., & Tadepalli, P. (2011). The first learning track of the international planning competition. Machine Learning, 84(1-2), 81\u2013107.\nFikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning and executing generalized robot plans. Artificial Intelligence, 3, 251\u2013288.\nFikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2, 189\u2013208.\nFinkelstein, L., & Markovitch, S. (1998). A selective macro-learning algorithm and its application to the NxN sliding-tile puzzle. Journal of Artificial Intelligence Research, 8, 223\u2013263.\nGarc\u0131\u0301a-Olaya, A., Jime\u0301nez, S., & Linares Lo\u0301pez, C. (2011). The 2011 international planning competition. Tech. rep., Universidad Carlos III de Madrid. http://hdl.handle.net/10016/11710.\nGeffner, H. (2010). The model-based approach to autonomous behavior: A personal view. In Fox, M., & Poole, D. (Eds.), Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2010), pp. 1709\u20131712. AAAI Press.\nHaslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent construction of pattern database heuristics for cost-optimal planning. In Holte, R. C., & Howe, A. E. (Eds.), Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI 2007), pp. 1007\u20131012. AAAI Press.\nHelmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence Research, 26, 191\u2013246.\nHelmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: What\u2019s the difference anyway?. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS 2009), pp. 162\u2013169. AAAI Press.\nHelmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics for optimal sequential planning. In Boddy, M., Fox, M., & Thie\u0301baux, S. (Eds.), Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS 2007), pp. 176\u2013 183. AAAI Press.\nHelmert, M., & Ro\u0308ger, G. (2008). How good is almost perfect?. In Fox, D., & Gomes, C. P. (Eds.), Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI 2008), pp. 944\u2013949. AAAI Press.\nHelmert, M., Ro\u0308ger, G., & Karpas, E. (2011). Fast Downward Stone Soup: A baseline for building planner portfolios. In ICAPS 2011 Workshop on Planning and Learning, pp. 28\u201335.\nHoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic search. Journal of Artificial Intelligence Research, 14, 253\u2013302.\nHutter, F., Hoos, H. H., Leyton-Brown, K., & Stu\u0308tzle, T. (2009). ParamILS: An automatic algorithm configuration framework. Journal of Artificial Intelligence Research, 36, 267\u2013306.\nKarpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Boutilier, C. (Ed.), Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009), pp. 1728\u20131733.\nKatz, M., & Domshlak, C. (2010). Implicit abstraction heuristics. Journal of Artificial Intelligence Research, 39, 51\u2013126.\nKautz, H., & Selman, B. (1992). Planning as satisfiability. In Neumann, B. (Ed.), Proceedings of the 10th European Conference on Artificial Intelligence (ECAI 1992), pp. 359\u2013363. John Wiley and Sons.\nKeyder, E., & Geffner, H. (2009). Soft goals can be compiled away. Journal of Artificial Intelligence Research, 36, 547\u2013556.\nMarques-Silva, J. P., & Sakallah, K. A. (1996). GRASP - a new search algorithm for satisfiability. In Proceedings of the 1996 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 1996), pp. 220\u2013227.\nMinton, S. (1994). Machine Learning Methods for Planning. Morgan Kaufmann Publishers Inc.\nNissim, R., Hoffmann, J., & Helmert, M. (2011). Computing perfect heuristics in polynomial time: On bisimulation and merge-and-shrink abstraction in optimal planning. In Walsh, T. (Ed.), Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI\u201911), pp. 1983\u20131990. AAAI Press/IJCAI.\nPalacios, H., & Geffner, H. (2009). Compiling uncertainty away in conformant planning problems with bounded width. Journal of Artificial Intelligence Research, 35, 623\u2013675.\nPearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving. AddisonWesley.\nPednault, E. P. D. (1989). ADL: Exploring the middle ground between STRIPS and the situation calculus. In Brachman, R. J., Levesque, H. J., & Reiter, R. (Eds.), Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning (KR 1989), pp. 324\u2013332. Morgan Kaufmann.\nRendell, L. A. (1983). A new basis for state-space learning systems and a successful implementation. Artificial Intelligence, 20(4), 369\u2013392.\nRintanen, J., Heljanko, K., & Niemela\u0308, I. (2006). Planning as satisfiability: Parallel plans and algorithms for plan search. Artificial Intelligence, 170(12\u201313), 1031\u20131080.\nRussell, S. J., & Zilberstein, S. (1991). Composing real-time systems. In Mylopoulos, J., & Reiter, R. (Eds.), Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI 1991), pp. 212\u2013217. Morgan Kaufmann.\nSchiex, T., & Verfaillie, G. (1993). Nogood recording for static and dynamic constraint satisfaction problems. Journal of Artificial Intelligence Research, 3, 48\u201355.\nThayer, J. T., Dionne, A. J., & Ruml, W. (2011). Learning inadmissible heuristics during search. In Bacchus, F., Domshlak, C., Edelkamp, S., & Helmert, M. (Eds.), Proceedings of the TwentyFirst International Conference on Automated Planning and Scheduling (ICAPS 2011), pp. 250\u2013257. AAAI Press.\nUtgoff, P. E., Berkman, N. C., & Clouse, J. A. (1997). Decision tree induction based on efficient tree restructuring. Machine Learning, 29(1), 5\u201344.\nWebb, G. I., Boughton, J. R., & Wang, Z. (2005). Not so naive Bayes: Aggregating one-dependence estimators. Machine Learning, 58(1), 5\u201324.\nYoon, S., Fern, A., & Givan, R. (2007). FF-Replan: A baseline for probabilistic planning. In Boddy, M., Fox, M., & Thie\u0301baux, S. (Eds.), Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS 2007), pp. 352\u2013359. AAAI Press.\nYoon, S., Fern, A., & Givan, R. (2008). Learning control knowledge for forward search planning. Journal of Machine Learning Research, 9, 683\u2013718.\nZimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: looking back, taking stock, going forward. AI Magazine, 24, 73\u201396."}], "references": [{"title": "Bootstrap learning of heuristic functions", "author": ["S.J. Arfaee", "S. Zilles", "R.C. Holte"], "venue": "Proceedings of the Third Annual Symposium on Combinatorial Search (SoCS", "citeRegEx": "Arfaee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Arfaee et al\\.", "year": 2010}, {"title": "Planning in polynomial time: the SAS-PUBS class", "author": ["C. B\u00e4ckstr\u00f6m", "I. Klein"], "venue": "Computational Intelligence,", "citeRegEx": "B\u00e4ckstr\u00f6m and Klein,? \\Q1991\\E", "shortCiteRegEx": "B\u00e4ckstr\u00f6m and Klein", "year": 1991}, {"title": "Using CSP look-back techniques to solve real-world SAT instances", "author": ["R.J. Bayardo Jr.", "R. Schrag"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI", "citeRegEx": "Jr. and Schrag,? \\Q1997\\E", "shortCiteRegEx": "Jr. and Schrag", "year": 1997}, {"title": "Strengthening landmark heuristics via hitting sets", "author": ["B. Bonet", "M. Helmert"], "venue": "Proceedings of the 19th European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Bonet and Helmert,? \\Q2010\\E", "shortCiteRegEx": "Bonet and Helmert", "year": 2010}, {"title": "A robust and fast action selection mechanism for planning", "author": ["B. Bonet", "G. Loerincs", "H. Geffner"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI", "citeRegEx": "Bonet et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bonet et al\\.", "year": 1997}, {"title": "Macro-FF: Improving AI planning with automatically learned macro-operators", "author": ["A. Botea", "M. Enzenberger", "M. M\u00fcller", "J. Schaeffer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Botea et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Botea et al\\.", "year": 2005}, {"title": "A multi-path compilation approach to contingent planning", "author": ["R. Brafman", "G. Shani"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Brafman and Shani,? \\Q2012\\E", "shortCiteRegEx": "Brafman and Shani", "year": 2012}, {"title": "Hyper-Heuristics: An Emerging Direction in Modern Search Technology", "author": ["E. Burke", "G. Kendall", "J. Newall", "E. Hart", "P. Ross", "S. Schulenburg"], "venue": "In Handbook of Metaheuristics, International Series in Operations Research & Management Science,", "citeRegEx": "Burke et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Burke et al\\.", "year": 2003}, {"title": "Marvin: A heuristic search planner with online macro-action learning", "author": ["A. Coles", "A. Smith"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Coles and Smith,? \\Q2007\\E", "shortCiteRegEx": "Coles and Smith", "year": 2007}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory, 13(1),", "citeRegEx": "Cover and Hart,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart", "year": 1967}, {"title": "Learning relational decision trees for guiding heuristic planning", "author": ["T. de la Rosa", "S. Jim\u00e9nez", "D. Borrajo"], "venue": "Proceedings of the Eighteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Rosa et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rosa et al\\.", "year": 2008}, {"title": "To max or not to max: Online learning for speeding up optimal planning", "author": ["C. Domshlak", "E. Karpas", "S. Markovitch"], "venue": "Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Domshlak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Domshlak et al\\.", "year": 2010}, {"title": "Speedup learning", "author": ["A. Fern"], "venue": "Sammut, C., & Webb, G. I. (Eds.), Encyclopedia of Machine Learning, pp. 907\u2013911. Springer.", "citeRegEx": "Fern,? 2010", "shortCiteRegEx": "Fern", "year": 2010}, {"title": "The first learning track of the international planning competition", "author": ["A. Fern", "R. Khardon", "P. Tadepalli"], "venue": "Machine Learning,", "citeRegEx": "Fern et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fern et al\\.", "year": 2011}, {"title": "Learning and executing generalized robot plans", "author": ["R.E. Fikes", "P.E. Hart", "N.J. Nilsson"], "venue": "Artificial Intelligence,", "citeRegEx": "Fikes et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Fikes et al\\.", "year": 1972}, {"title": "STRIPS: A new approach to the application of theorem proving to problem solving", "author": ["R.E. Fikes", "N.J. Nilsson"], "venue": "Artificial Intelligence,", "citeRegEx": "Fikes and Nilsson,? \\Q1971\\E", "shortCiteRegEx": "Fikes and Nilsson", "year": 1971}, {"title": "A selective macro-learning algorithm and its application to the NxN sliding-tile puzzle", "author": ["L. Finkelstein", "S. Markovitch"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Finkelstein and Markovitch,? \\Q1998\\E", "shortCiteRegEx": "Finkelstein and Markovitch", "year": 1998}, {"title": "The 2011 international planning competition", "author": ["A. Garc\u0131\u0301a-Olaya", "S. Jim\u00e9nez", "C. Linares L\u00f3pez"], "venue": "Tech. rep., Universidad Carlos III de Madrid. http://hdl.handle.net/10016/11710", "citeRegEx": "Garc\u0131\u0301a.Olaya et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Olaya et al\\.", "year": 2011}, {"title": "The model-based approach to autonomous behavior: A personal view", "author": ["H. Geffner"], "venue": "Fox, M., & Poole, D. (Eds.), Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2010), pp. 1709\u20131712. AAAI Press.", "citeRegEx": "Geffner,? 2010", "shortCiteRegEx": "Geffner", "year": 2010}, {"title": "Domain-independent construction of pattern database heuristics for cost-optimal planning", "author": ["P. Haslum", "A. Botea", "M. Helmert", "B. Bonet", "S. Koenig"], "venue": "Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Haslum et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Haslum et al\\.", "year": 2007}, {"title": "The Fast Downward planning system", "author": ["M. Helmert"], "venue": "Journal of Artificial Intelligence Research, 26, 191\u2013246.", "citeRegEx": "Helmert,? 2006", "shortCiteRegEx": "Helmert", "year": 2006}, {"title": "Landmarks, critical paths and abstractions: What\u2019s the difference anyway", "author": ["M. Helmert", "C. Domshlak"], "venue": "Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Helmert and Domshlak,? \\Q2009\\E", "shortCiteRegEx": "Helmert and Domshlak", "year": 2009}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Helmert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2007}, {"title": "How good is almost perfect", "author": ["M. Helmert", "G. R\u00f6ger"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Helmert and R\u00f6ger,? \\Q2008\\E", "shortCiteRegEx": "Helmert and R\u00f6ger", "year": 2008}, {"title": "Fast Downward Stone Soup: A baseline for building planner portfolios", "author": ["M. Helmert", "G. R\u00f6ger", "E. Karpas"], "venue": "In ICAPS 2011 Workshop on Planning and Learning,", "citeRegEx": "Helmert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Helmert et al\\.", "year": 2011}, {"title": "The FF planning system: Fast plan generation through heuristic search", "author": ["J. Hoffmann", "B. Nebel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hoffmann and Nebel,? \\Q2001\\E", "shortCiteRegEx": "Hoffmann and Nebel", "year": 2001}, {"title": "ParamILS: An automatic algorithm configuration framework", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hutter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2009}, {"title": "Cost-optimal planning with landmarks", "author": ["E. Karpas", "C. Domshlak"], "venue": "Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Karpas and Domshlak,? \\Q2009\\E", "shortCiteRegEx": "Karpas and Domshlak", "year": 2009}, {"title": "Implicit abstraction heuristics", "author": ["M. Katz", "C. Domshlak"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Katz and Domshlak,? \\Q2010\\E", "shortCiteRegEx": "Katz and Domshlak", "year": 2010}, {"title": "Planning as satisfiability", "author": ["H. Kautz", "B. Selman"], "venue": "Proceedings of the 10th European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Kautz and Selman,? \\Q1992\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1992}, {"title": "Soft goals can be compiled away", "author": ["E. Keyder", "H. Geffner"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Keyder and Geffner,? \\Q2009\\E", "shortCiteRegEx": "Keyder and Geffner", "year": 2009}, {"title": "GRASP - a new search algorithm for satisfiability", "author": ["J.P. Marques-Silva", "K.A. Sakallah"], "venue": "In Proceedings of the 1996 IEEE/ACM International Conference on Computer-Aided Design (ICCAD", "citeRegEx": "Marques.Silva and Sakallah,? \\Q1996\\E", "shortCiteRegEx": "Marques.Silva and Sakallah", "year": 1996}, {"title": "Machine Learning Methods for Planning", "author": ["S. Minton"], "venue": "Morgan Kaufmann Publishers Inc.", "citeRegEx": "Minton,? 1994", "shortCiteRegEx": "Minton", "year": 1994}, {"title": "Computing perfect heuristics in polynomial time: On bisimulation and merge-and-shrink abstraction in optimal planning", "author": ["R. Nissim", "J. Hoffmann", "M. Helmert"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence", "citeRegEx": "Nissim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2011}, {"title": "Compiling uncertainty away in conformant planning problems with bounded width", "author": ["H. Palacios", "H. Geffner"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Palacios and Geffner,? \\Q2009\\E", "shortCiteRegEx": "Palacios and Geffner", "year": 2009}, {"title": "Heuristics: Intelligent Search Strategies for Computer Problem Solving", "author": ["J. Pearl"], "venue": "AddisonWesley.", "citeRegEx": "Pearl,? 1984", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "ADL: Exploring the middle ground between STRIPS and the situation calculus", "author": ["E.P.D. Pednault"], "venue": "Brachman, R. J., Levesque, H. J., & Reiter, R. (Eds.), Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning (KR 1989), pp. 324\u2013332. Morgan Kaufmann.", "citeRegEx": "Pednault,? 1989", "shortCiteRegEx": "Pednault", "year": 1989}, {"title": "A new basis for state-space learning systems and a successful implementation", "author": ["L.A. Rendell"], "venue": "Artificial Intelligence, 20(4), 369\u2013392.", "citeRegEx": "Rendell,? 1983", "shortCiteRegEx": "Rendell", "year": 1983}, {"title": "Planning as satisfiability: Parallel plans and algorithms for plan search", "author": ["J. Rintanen", "K. Heljanko", "I. Niemel\u00e4"], "venue": "Artificial Intelligence,", "citeRegEx": "Rintanen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rintanen et al\\.", "year": 2006}, {"title": "Composing real-time systems", "author": ["S.J. Russell", "S. Zilberstein"], "venue": "Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Russell and Zilberstein,? \\Q1991\\E", "shortCiteRegEx": "Russell and Zilberstein", "year": 1991}, {"title": "Nogood recording for static and dynamic constraint satisfaction problems", "author": ["T. Schiex", "G. Verfaillie"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Schiex and Verfaillie,? \\Q1993\\E", "shortCiteRegEx": "Schiex and Verfaillie", "year": 1993}, {"title": "Learning inadmissible heuristics during search", "author": ["J.T. Thayer", "A.J. Dionne", "W. Ruml"], "venue": "Proceedings of the TwentyFirst International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Thayer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Thayer et al\\.", "year": 2011}, {"title": "Decision tree induction based on efficient tree restructuring", "author": ["P.E. Utgoff", "N.C. Berkman", "J.A. Clouse"], "venue": "Machine Learning,", "citeRegEx": "Utgoff et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Utgoff et al\\.", "year": 1997}, {"title": "Not so naive Bayes: Aggregating one-dependence estimators", "author": ["G.I. Webb", "J.R. Boughton", "Z. Wang"], "venue": "Machine Learning,", "citeRegEx": "Webb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2005}, {"title": "FF-Replan: A baseline for probabilistic planning", "author": ["S. Yoon", "A. Fern", "R. Givan"], "venue": "Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling (ICAPS", "citeRegEx": "Yoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2007}, {"title": "Learning control knowledge for forward search planning", "author": ["S. Yoon", "A. Fern", "R. Givan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yoon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2008}, {"title": "Learning-assisted automated planning: looking back, taking stock, going forward", "author": ["T. Zimmerman", "S. Kambhampati"], "venue": "AI Magazine,", "citeRegEx": "Zimmerman and Kambhampati,? \\Q2003\\E", "shortCiteRegEx": "Zimmerman and Kambhampati", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Planning in AI is best conceived as the model-based approach to automated action selection (Geffner, 2010).", "startOffset": 91, "endOffset": 106}, {"referenceID": 31, "context": "The computational difficulty of domain-independent planning has led many researchers to use speedup learning techniques in order to improve the performance of planning systems; for a survey of many of these, see the work of Minton (1994), Zimmerman and Kambhampati (2003), and Fern, Khardon, and Tadepalli (2011).", "startOffset": 224, "endOffset": 238}, {"referenceID": 31, "context": "The computational difficulty of domain-independent planning has led many researchers to use speedup learning techniques in order to improve the performance of planning systems; for a survey of many of these, see the work of Minton (1994), Zimmerman and Kambhampati (2003), and Fern, Khardon, and Tadepalli (2011).", "startOffset": 224, "endOffset": 272}, {"referenceID": 12, "context": "The computational difficulty of domain-independent planning has led many researchers to use speedup learning techniques in order to improve the performance of planning systems; for a survey of many of these, see the work of Minton (1994), Zimmerman and Kambhampati (2003), and Fern, Khardon, and Tadepalli (2011).", "startOffset": 277, "endOffset": 313}, {"referenceID": 12, "context": "Speedup learning systems can be divided along several dimensions (Zimmerman & Kambhampati, 2003; Fern, 2010).", "startOffset": 65, "endOffset": 108}, {"referenceID": 13, "context": "Offline learning has been applied extensively to domain-independent planning, with varying degrees of success (Fern et al., 2011).", "startOffset": 110, "endOffset": 129}, {"referenceID": 36, "context": "Several formalisms for describing planning tasks are in use, including STRIPS (Fikes & Nilsson, 1971), ADL (Pednault, 1989), and SAS+ (B\u00e4ckstr\u00f6m & Klein, 1991; B\u00e4ckstr\u00f6m & Nebel, 1995).", "startOffset": 107, "endOffset": 123}, {"referenceID": 20, "context": "We describe the SAS+ formalism, the one used by the Fast Downward planner (Helmert, 2006), on top of which we have implemented and evaluated selective max.", "startOffset": 74, "endOffset": 89}, {"referenceID": 7, "context": "As mentioned previously, selective max is a form of hyper-heuristic (Burke et al., 2003) that chooses which heuristic to compute at each state.", "startOffset": 68, "endOffset": 88}, {"referenceID": 35, "context": "Such an idealized search space model was used in the past to analyze the behavior of A\u2217 (Pearl, 1984).", "startOffset": 88, "endOffset": 101}, {"referenceID": 35, "context": "The A\u2217 algorithm with a consistent heuristic h expands states in increasing order of f = g + h (Pearl, 1984).", "startOffset": 95, "endOffset": 108}, {"referenceID": 20, "context": "In particular, the experiments of Helmert and R\u00f6ger (2008) on IPC benchmarks with heuristics with small constant additive errors show that the number of expanded nodes most typically grows exponentially as the (still very small and additive) error increases.", "startOffset": 34, "endOffset": 59}, {"referenceID": 20, "context": "The third state-space sampling procedure, referred to here as PDB sampling, has been proposed by Haslum, Botea, Helmert, Bonet, and Koenig (2007). This procedure also uses unbiased probes, but only adds the last state reached in each probe to the state-space sample.", "startOffset": 112, "endOffset": 146}, {"referenceID": 20, "context": "To evaluate selective max empirically, we implemented it on top of the open-source Fast Downward planner (Helmert, 2006).", "startOffset": 105, "endOffset": 120}, {"referenceID": 17, "context": "The IPC-2011 experiments (Garc\u0131\u0301a-Olaya et al., 2011) were run by the IPC organizers, on their own machines, with a time limit of 30 minutes and a memory limit of 6 GB per planning task.", "startOffset": 25, "endOffset": 53}, {"referenceID": 33, "context": "While there are other admissible heuristics for SAS+ planning that are competitive with the three above (for example, Helmert, Haslum, & Hoffmann, 2007; Nissim et al., 2011; Katz & Domshlak, 2010), they are based on expensive offline preprocessing, followed by very fast online per-state computation.", "startOffset": 104, "endOffset": 196}, {"referenceID": 19, "context": "1: biased probes (selh ), unbiased probes (sel UP h ), and the sampling method of Haslum et al. (2007) (selPDB h ).", "startOffset": 82, "endOffset": 103}, {"referenceID": 43, "context": "A more sophisticated variant of Naive Bayes called AODE (Webb et al., 2005) is also considered here (sel h ).", "startOffset": 56, "endOffset": 75}, {"referenceID": 19, "context": "N 100 initial sample size Sampling method PDB (Haslum et al., 2007) state-space sampling method Classifier Naive Bayes classifier type", "startOffset": 46, "endOffset": 67}, {"referenceID": 42, "context": "Another possible choice is using incremental decision trees (Utgoff et al., 1997), which offer even faster classification, but more expensive learning when the tree structure needs to be changed (sel h ).", "startOffset": 60, "endOffset": 81}, {"referenceID": 24, "context": "Sequential portfolio solvers for optimal planning are another approach for exploiting the merits of different heuristic functions, and they have been very successful in practice, with the Fast Downward Stone Soup sequential portfolio (Helmert et al., 2011) winning the sequential optimal track at IPC2011.", "startOffset": 234, "endOffset": 256}, {"referenceID": 37, "context": "However, despite some early work (Rendell, 1983), relatively little work has dealt with learning for state-space search guided by distance-estimating heuristics, one of the most prominent approaches to planning these days.", "startOffset": 33, "endOffset": 48}, {"referenceID": 37, "context": "However, despite some early work (Rendell, 1983), relatively little work has dealt with learning for state-space search guided by distance-estimating heuristics, one of the most prominent approaches to planning these days. Most works in this direction have been devoted to learning macro-actions (see, for example, Finkelstein & Markovitch, 1998; Botea, Enzenberger, M\u00fcller, & Schaeffer, 2005; Coles & Smith, 2007). Recently, learning for heuristic search planning has received more attention: Yoon et al. (2008) suggested learning (inadmissible) heuristic functions based upon features extracted from relaxed plans.", "startOffset": 34, "endOffset": 513}, {"referenceID": 37, "context": "However, despite some early work (Rendell, 1983), relatively little work has dealt with learning for state-space search guided by distance-estimating heuristics, one of the most prominent approaches to planning these days. Most works in this direction have been devoted to learning macro-actions (see, for example, Finkelstein & Markovitch, 1998; Botea, Enzenberger, M\u00fcller, & Schaeffer, 2005; Coles & Smith, 2007). Recently, learning for heuristic search planning has received more attention: Yoon et al. (2008) suggested learning (inadmissible) heuristic functions based upon features extracted from relaxed plans. Arfaee, Zilles, and Holte (2010) attempted to learn an almost admissible heuristic estimate using a neural network.", "startOffset": 34, "endOffset": 650}, {"referenceID": 37, "context": "However, despite some early work (Rendell, 1983), relatively little work has dealt with learning for state-space search guided by distance-estimating heuristics, one of the most prominent approaches to planning these days. Most works in this direction have been devoted to learning macro-actions (see, for example, Finkelstein & Markovitch, 1998; Botea, Enzenberger, M\u00fcller, & Schaeffer, 2005; Coles & Smith, 2007). Recently, learning for heuristic search planning has received more attention: Yoon et al. (2008) suggested learning (inadmissible) heuristic functions based upon features extracted from relaxed plans. Arfaee, Zilles, and Holte (2010) attempted to learn an almost admissible heuristic estimate using a neural network. Perhaps the most closely related work to ours is that of Thayer, Dionne, and Ruml (2011), who learn to correct errors in heuristic estimates online.", "startOffset": 34, "endOffset": 822}, {"referenceID": 19, "context": "PDB is the sampling method of Haslum et al. (2007), P is the biased probes sampling method, and UP is the unbiased probes sampling method.", "startOffset": 30, "endOffset": 51}], "year": 2012, "abstractText": "Domain-independent planning is one of the foundational areas in the field of Artificial Intelligence. A description of a planning task consists of an initial world state, a goal, and a set of actions for modifying the world state. The objective is to find a sequence of actions, that is, a plan, that transforms the initial world state into a goal state. In optimal planning, we are interested in finding not just a plan, but one of the cheapest plans. A prominent approach to optimal planning these days is heuristic state-space search, guided by admissible heuristic functions. Numerous admissible heuristics have been developed, each with its own strengths and weaknesses, and it is well known that there is no single \u201cbest\u201d heuristic for optimal planning in general. Thus, which heuristic to choose for a given planning task is a difficult question. This difficulty can be avoided by combining several heuristics, but that requires computing numerous heuristic estimates at each state, and the tradeoff between the time spent doing so and the time saved by the combined advantages of the different heuristics might be high. We present a novel method that reduces the cost of combining admissible heuristics for optimal planning, while maintaining its benefits. Using an idealized search space model, we formulate a decision rule for choosing the best heuristic to compute at each state. We then present an active online learning approach for learning a classifier with that decision rule as the target concept, and employ the learned classifier to decide which heuristic to compute at each state. We evaluate this technique empirically, and show that it substantially outperforms the standard method for combining several heuristics via their pointwise maximum.", "creator": "TeX"}}}