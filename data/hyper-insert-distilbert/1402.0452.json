{"id": "1402.0452", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2014", "title": "A Lower Bound for the Variance of Estimators for Nakagami m Distribution", "abstract": "from recently, even we truly have proposed : a maximum total likelihood analysis iterative evaluation algorithm tool for estimation of the parameters of either the standard nakagami - gibbs m regular distribution. this technique effectively performs better than state level of art estimation techniques for constructing this stationary distribution. comparing this could be useful of particular interest use in exploring low data or deep block based estimation problems. in these scenarios, implementing the estimator should be able basically to give generally accurate estimates starting in the mean square sense filled with yet less amounts of smaller data. also, the estimates should improve with the increase in number errors of blocks safely received. therefore in this paper, we certainly see through our preliminary simulations, that our proposal is again well designed for such requirements. check further, that it is well known in the literature writing that introducing an efficient estimator does not exist however for nakagami - m distribution. in this paper, further we derive a theoretical expression directly for the residual variance ratio of our given proposed estimator. we would find that combining this expression clearly fits the constrained experimental curve for the partial variance constraint of obtaining the possibly proposed poor estimator. only this expression is pretty uncomfortably close even to violating the cramer - bin rao pearson lower bound ( where crlb ).", "histories": [["v1", "Mon, 3 Feb 2014 18:20:46 GMT  (135kb)", "http://arxiv.org/abs/1402.0452v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rangeet mitra", "amit kumar mishra", "tarun choubisa"], "accepted": false, "id": "1402.0452"}, "pdf": {"name": "1402.0452.pdf", "metadata": {"source": "CRF", "title": "A Lower Bound for the Variance of Estimators for Nakagami-m Distribution", "authors": ["Rangeet Mitra", "Amit Kumar Mishra", "Tarun Choubisa"], "emails": ["mitra.rangeet@gmail.com", "amit.india@gmail.com", "choubisa.tarun@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n04 52\nv1 [\ncs .L\nG ]\n3 F\neb 2\n01 4\nIndex Terms\u2014Nakagami-m distribution, MaximumLikelihood, Cramer-Rao Lower Bound.\nI. INTRODUCTION\nNakagami-m distribution is a versatile distribution which is used for modelling a variety of physical phenomena in communication engineering. An algorithm for estimation of the parameters of Nakagami-m distribution has been recently proposed in [1]. However many seminal papers like [2] have adressed this topic as well. The types of estimators proposed in the literature can be catagorised into two types: a)moment-based [3]\u2013[8] b) algebraic maximal likelihood [2], [9] methods. In [9] the authors have studied many preexisting parameter estimators for the Nakagami-m distribution which belonged to both of the above mentioned category of estimators. The problem with moment based estimators is that it requires a large number of samples to estimate the moments accurately. On the other hand, the algebraic methods suffer from errors due to truncation of Taylor\u2019s series.\nAn interesting work, [9], concludes that none of the many estimators discussed in it gives better performance than the Greenwood and Durand estimator [9] (which will be one of the many algorithms against which we will compare our approach in this paper). Also, [4], [6] deal with parameter estimation of noisy Nakagami-m signals. However, in noisy conditions, a) the distribution is no longer Nakagami-m, hence it is out of context for our study for now; and b) It uses moment based parameter estimates which require a significant number of samples to achieve a low variance; and lastly c) it needs\nknowledge of the variance of noise which corrupts the signal which could be Gaussian/Non-Gaussian.\nThe suitability of our approach in [1] against these previously existing algorithms has not been tested. In the current paper we we derive a new simple theoretical lower bound for variance of the m parameter estimates for this algorithm. It is seen that our proposed algorithm has two advantages; viz. a) It performs better than other estimators with low amounts of data in highly testing scenarios; and, b) as it is an online algorithm which considers one sample at a time the parameter \u2206 in [2] is zero. Also, it does not make any algebraic approximations. This prevents error propagation between blocks in block processing scenarios. Due to these intuitive reasons, our proposed algorithm is better than the state of art in low-data/block scenarios.\nThe rest of the paper is organised as follows. Section II describes the conditions in which we simulated our algorithm. Section III describes the final algorithm. Section IV gives some heuristics which should be used. Section V gives the comparison of the variance of our algorithm with some existing estimators. Finally in the Appendix we give an expression for the Cramer-Rao lower bound (CRLB) based lower bound of the variance of our algorithm.\nII. NAKAGAMI-m DISTRIBUTION AND THE PORPOSED PARAMETER ESTIMATION ALGORITHM\nThe multivariate Nakagami-m distriution is approximated as the product of L individual Nakagami distributions (assuming that the data is i.i.d).\nP (x) = \u03a0Li=1 2L\n\u0393(mi)\u03c3 mi i x2mi\u22121i exp(\u2212 x2i \u03c3i ) (1)\nHere, \u03c3i and mi are the spread factors and the centrality factors for each of the component Nakagami-m distributions.\nAs we take the derivative of the log-likelihood with respect to each of the components, and equate it to zero and solve the differential equation for the \u0393(m), we get the following equations, which are solved numerically.\n\u0393(mi) = x2mii \u03c3mii\n(2)\nand,\n\u03c3i = 1\nmi x2i (3)\nDetails of the derivation are provided in [1]."}, {"heading": "III. SIMULATION CONDITIONS", "text": "In this section, we give a description of our simulation set up for comparison of our algorithm with the state of art. At a time we receive a limited parallel block of data. The deployed (\u201ccompeting\u201d) algorithms have to infer from a subset of such limited data blocks. A desirable property of an estimation algorithm in such a case is increased learnability with the number of blocks it sees (apart from less Mean Squared Error (MSE)). Further, in such limited data conditions, we may like to use a moving average estimator (which is the most simple and recursive approach to smoothing) to smooth the estimates. However, if the deployed algorithm has errors from factors like truncation of Taylor\u2019s series, these errors will get propagated from block to block.\nThis intuition is caught by our algorithm. Our algorithm never uses series truncation. It relies on numerical methods and hence has the ability to learn improvably from multiple blocks. Also, our algorithm does not require large datasets for the moments to converge (unlike its moment based counterparts). Our simulations presented in this section confirm to this intuitive reasons for the success of our algorithm against its counterparts.\nThe algorithm to be followed in our comparison of parameter estimation methods can be given as below.\nAlgorithm 1 Parameter Estimation for m for {x}Ni=1 1: for i = 1 to N do 2: if i == 1 then 3: Estimate m\u03021 via any of the candidate estimators\ndescribed. 4: else 5: Estimate m\u0302i via any of the candidate estimators. 6: Update current mean as m\u0302i = i\u22121i m\u0302i\u22121 + 1 i m\u0302i 7: end if 8: end for"}, {"heading": "A. Heuristics Considered", "text": "As the iterative equations given in [1] are non-convex, they may be susceptible to local minima. Hence we must run the algorithms a number of times and take centrality measures(mean/median/mode) of the obtained values.\nAlso, to help all the estimators learn better (and also facilitate comparison on equal terms) the estimated values obtained by all the techniques considered in the next section are averaged over the blocks by the well known recursivemean filter (i.e. the sample mean calculated recursively)."}, {"heading": "IV. COMPARISON OF ALL THE ESTIMATORS", "text": "We can see from figure 1 that our estimator learns with number of blocks it sees, i.e. its performance lies between the CRLB of the single block and that of the whole block. Also its performance is better than the estimators proposed in [2] (which were based on first order and second order\nTaylor approximations). Also its performance is better than the Greenwood and Durand estimators given in [9](which were the most superior in that paper) and equivalent to the moment based estimator in most of the m regimes. Hence our algorithm has sufficient credibility in scenarios where block data processing is common like Orthogonal Frequency Division multiplexing (OFDM) and data is limited.\nFor the sake of completeness, the normalized variance plots are also shown in Fig. 2. In Figs. 3 and 4 we compare the performance of all the estimators in the setting 20X7 block size (i.e in reference of Sec. III, the ith block is of size 20 and N = 7). We see that in this scenario, our estimator even outperforms the moment based estimator."}, {"heading": "V. A POSSIBLE PRACTICAL APPLICATION", "text": "Consider the case of Orthogonal Frequency Division Multiplexing(OFDM). After FFT at the receiver we receive a block of data which is converted to serial form via parallel to serial(P/S) converter. When the number of points obtained in each conversion is less, the variance of state of art estimators increases (apart from the pre-mentioned errors stemming from algebraic approximations as in [2]). In such cases, we must use an algorithm which is a) exact b) iterative (for robustness and tracking non-stationarity). Also, one of the secondary objectives of our algorithm is to omit the role of P/S converter before parameter estimation for performing detection."}, {"heading": "VI. CONCLUSION", "text": "In this paper we have found that our proposed algorithm has better performance than some of the popular estimators. We give a theoretical expression for the variance of our estimator in the appendix A. We see that the variance of our estimates obtained via simulations closely matches the theoretical expression."}, {"heading": "VII. REVIEW OF HIDDEN MARKOV RANDOM FIELDS(HMRF) AS APPLIED TO IMAGE SEGMENTATION", "text": "Basically in the project in [10], the image segmentation algorithm has the following assumptions: Let y be the set of image pixels and x be the set of labels which are initialized similarly by k-means algorithm and are later inferred again by (Expectation-Maximization) EM algorithm. Then for a particular set of similarly initialized x,\np(y | x) = G(y, \u00b5, \u03c3) (4)\nwhere G is a Gaussian with mean \u00b5 and variance \u03c3. Under the i.i.d assumption,\np(y | x) = \u220f\ni\nG(yi, \u00b5i, \u03c3i). (5)\nClique (denoted by c, which is a fully connected subset of a graph) potential on a 4-neighborhood of pixels is defined as follows:\nVc(xa, xb) = 1\u2212 \u03b4(xa \u2212 xb). (6)\nHere the \u03b4 denotes a sample impulse to enforce a constraint/prior of nearby pixels being assigned the same label\n0 5 10 15 20 10\n\u22123\n10 \u22122\n10 \u22121\n10 0\n10 1\n10 2\nm\u2212parameter value\nV a\nri a\nn c e\nGreenwood and Durand Proposal Parametric Estimator CRLB Theoretical Variance of Proposal Beaulieu 1st Order Beaulieu Second Order\nCRLB Single Block\nParametric Estimate\u2212Single Block\nFig. 1. Comparison of various algorithms with respect to the proposal(Block Window Size 30X5)\n0 5 10 15 20 10\n\u22123\n10 \u22122\n10 \u22121\n10 0\nm\u2212parameter value\nN o\nrm a\nli z e\nd V\na ri a\nn c e\nGreenwood and Durand Proposal Parametric Estimator CRLB Theoretical variance of Proposal Beaulieu 1st Order Beaulieu Second Order CRLB Single Block Parametric Estimate\u2212Single Block\nFig. 2. Comparison of various algorithms with respect to the proposal(Block Window Size 30X5)\n0 5 10 15 20 10\n\u22123\n10 \u22122\n10 \u22121\n10 0\n10 1\n10 2\nm\u2212parameter value\nV a\nri a\nn c e\nGreenwood and Durand Proposal Parametric Estimator CRLB Theoretical Variance of Proposal Beaulieu 1st Order Beaulieu Second Order CRLB Single Block Parametric Estimate\u2212Single Block\nFig. 3. Comparison of various algorithms with respect to the proposal(Block Window Size 20X7)\nwith higher probability. These clique potentials are then added to the potentials of the energy in Eqn. 12 used to develop priors. The total energy is the sum of the following two energies,\nU = log(p(y | x)) + log(\u03a3cVc) (7)\nThe labels are then found by the step,\nx\u2217 = argminxU (8)\nFrom the new inferred labels new priors are inferred:\np \u2032 (y) = exp(\u03a3cVc(x \u2217 a, x \u2217 b ))\nZ (9)\nxa and xb are two neighboring pixels connected by a Markov graph as in [10] forming a clique. Z is the summation of the numerator of Equation 9 over all possible cliques. It is basically a normalization constant to make p \u2032\n(y) a valid pdf. Using these the sample mean and variance are inferred by the following equations:\n\u00b5\u2217 = \u03a3yp \u2032 (y)y (10)\n\u03c3\u2217 = \u03a3yp \u2032 (y)(y \u2212 \u00b5\u2217)2 (11)\nVIII. MODIFIED NAKAGAMI-m BASED MRF SEGMENTATION\nIn our modification, we allow the conditional pdf to be Nakagami-m distributed, i.e.,\ng(y|x) = \u03a0Li=1 2L\n\u0393(mi)\u03c3 mi i y2mi\u22121i exp(\u2212 y2i \u03c3i ) (12)\nGiven a particular labeling pattern x, the parameters are inferred from the algorithm in Sec. III(by our previous simulations in this paper our algorithm comes closest to the CRLB than any other compared estimator in limited data) for each of the labels.\nThe labels, in turn are again inferred from the minimization of the following potential,\nU = log(g(y | x)) + log(\u03a3cVc) (13)"}, {"heading": "IX. SEGMENTATION RESULTS", "text": "A 600X338 RGB image was taken and was deliberately scaled down to 30X30 size to see performance comparisons in limited data. Then it was blurred by a 3X3 Gaussian blur. After segmentation, the image is resized to 160X120 for ease of viewing. The performance of the Gaussian based HMRF and Nakagami-m HMRF are compared. We can see from Fig. 5(a) that the top of the tower is chopped off in the process of segmentation. Fig. 5(b) shows the original (resized) image. However the entire tower is retained in 5(c) even after nakagami-m segmentation. We can also see better segementation result in case of \u201cCharminar\u201d image from Figs. 5(d),5(e),5(f).\nAPPENDIX\nWe know that log(\u0393(m)) is a log-convex function. This implies,\nlog(\u0393(m+ 0.5)) \u2265 log(\u0393(m)) + 0.5\u03c8(m) (14)\nor,\n2(log(\u0393(m+ 0.5))\u2212 log(\u0393(m))) \u2265 \u03c8(m) (15)\nor,\n\u03c8(m) \u2264 2log( \u0393(m+ 0.5)\u03c30.5\n\u0393(m) )\u2212 log(\u03c3) (16)\nor,\n\u03c8(m) \u2264 2log(E[x])\u2212 log(\u03c3) (17)\nBy Jensen\u2019s inequality,\nE[log(x)] \u2264 log(E[x]) (18)\nThis implies that the inequality in Equation. must hold for the least possible value of log(E[x]). Thus,\n\u03c8(m) \u2264 log( x2\n\u03c3 ) (19)\nor \u03c8(m) lies in a \u03b41 neighborhood of log(x 2\n\u03c3 ), i.e.,\n\u03c8(m) + \u03b41 = E[log( x2\n\u03c3 )] (20)\nwhere \u03b41 is a non-zero number. From concavity of \u03c8(m),\n\u03c8 \u2032 (m) \u2265 2(\u03c8(m+ 0.5)\u2212 \u03c8(m)) (21)\nor,\n\u03c8 \u2032\n(m) = 2(\u03c8(m+ 0.5)\u2212 \u03c8(m)) + \u03b42 (22)\nIt is trivial to see,\n\u03b41 \u2192 0 \u21d4 \u03b42 \u2192 0 (23)\nbecause,\n\u03c8 \u2032 (m) = 2(\u03c8(m+ 0.5)\u2212 \u03c8(m)) \u21d4 \u03c8(m) \u2248 log( x2\n\u03c3 ) (24)\nThen, after a simple analysis our approximate lower bound for variance becomes,\nCRLB \u2032 = 1\nN [2\u03c8(m+ 0.5)\u2212 2\u03c8(m)\u2212 1 m ]\n(25)\nThis is the modified expression for the lower bound for the variance of this estimator.\nThis expression is obtained by putting the minimum possible value of the \u03c8 \u2032\n(m). This situation occurs iff the equations in [1] are exactly solved. This expression is clearly greater than the CRLB for the Nakagami-m distribution.\nWe can see from the previous simulations that our estimator almost faithfully follows this curve as a function of m."}], "references": [{"title": "Maximum likelihood estimate of parameters of nakagami-m distribution", "author": ["R. Mitra", "A. Mishra", "T. Choubisa"], "venue": "Communications, Devices and Intelligent Systems (CODIS), 2012 International Conference on, 2012, pp. 9\u201312.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum-Likelihood Based Estimation of the Nakagami m Parameter", "author": ["J. Cheng", "S. Member", "N.C. Beaulieu"], "venue": "vol. 5, no. 3, pp. 101\u2013103, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Generalized moment estimators for the Nakagami fading parameter", "author": ["J. Cheng", "N. Beaulieu"], "venue": "IEEE Communications Letters, vol. 6, no. 4, pp. 144\u2013146, Apr. 2002. [Online]. Available: http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=996038", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Novel Nakagami- m Parameter Estimator for Noisy Channel Samples", "author": ["Y. Chen", "S. Member", "N.C. Beaulieu"], "venue": "vol. 9, no. 5, pp. 417\u2013419, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimation of Nakagami-m Fading Channel Parameters With Application to Optimized Transmitter Diversity Systems", "author": ["Y.-C. Ko", "M.-S. Alouini"], "venue": "vol. 2, no. 2, pp. 250\u2013259, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Estimators Using Noisy Channel Samples for Fading Distribution Parameters", "author": ["Y. Chen", "S. Member", "N.C. Beaulieu"], "venue": "vol. 53, no. 8, pp. 1274\u20131277, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimation of Nakagami Distribution Parameters Based on Signal Samples Corrupted with Multiplicative and Additive Disturbances", "author": ["H. Nasuf"], "venue": "vol. 2, no. September, pp. 12\u201314, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalized Method of Moments Estimation of the Nakagami- m Fading Parameter", "author": ["N. Wang", "S. Member", "X. Song", "S. Member", "J. Cheng"], "venue": "vol. 11, no. 9, pp. 3316\u20133325, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A Note on the Estimation of Nakagami-m Fading Parameter", "author": ["Q.T. Zhang", "S. Member"], "venue": "vol. 6, no. 6, pp. 237\u2013238, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Hmrf-em-image: Implementation of the hidden markov random field model and its expectation-maximization algorithm", "author": ["Q. Wang"], "venue": "CoRR, vol. abs/1207.3510, 2012.  (a) Performance of Gaussian likelihood based Segmentation (b) Original Image (c) Performance of Nakagami-m likelihood based Segmentation  (d) Performance of Gaussian likelihood based Segmentation (e) Original Image (f) Performance of Nakagami-m likelihood based Segmentation Fig. 5.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "An algorithm for estimation of the parameters of Nakagami-m distribution has been recently proposed in [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 1, "context": "However many seminal papers like [2] have adressed this topic as well.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "The types of estimators proposed in the literature can be catagorised into two types: a)moment-based [3]\u2013[8] b) algebraic maximal likelihood [2], [9] methods.", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "The types of estimators proposed in the literature can be catagorised into two types: a)moment-based [3]\u2013[8] b) algebraic maximal likelihood [2], [9] methods.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "The types of estimators proposed in the literature can be catagorised into two types: a)moment-based [3]\u2013[8] b) algebraic maximal likelihood [2], [9] methods.", "startOffset": 141, "endOffset": 144}, {"referenceID": 8, "context": "The types of estimators proposed in the literature can be catagorised into two types: a)moment-based [3]\u2013[8] b) algebraic maximal likelihood [2], [9] methods.", "startOffset": 146, "endOffset": 149}, {"referenceID": 8, "context": "In [9] the authors have studied many preexisting parameter estimators for the Nakagami-m distribution which belonged to both of the above mentioned category of estimators.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "An interesting work, [9], concludes that none of the many estimators discussed in it gives better performance than the Greenwood and Durand estimator [9] (which will be one of the many algorithms against which we will compare our approach in this paper).", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "An interesting work, [9], concludes that none of the many estimators discussed in it gives better performance than the Greenwood and Durand estimator [9] (which will be one of the many algorithms against which we will compare our approach in this paper).", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "Also, [4], [6] deal with parameter estimation of noisy Nakagami-m signals.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Also, [4], [6] deal with parameter estimation of noisy Nakagami-m signals.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "The suitability of our approach in [1] against these previously existing algorithms has not been tested.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "a) It performs better than other estimators with low amounts of data in highly testing scenarios; and, b) as it is an online algorithm which considers one sample at a time the parameter \u2206 in [2] is zero.", "startOffset": 191, "endOffset": 194}, {"referenceID": 0, "context": "Details of the derivation are provided in [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "As the iterative equations given in [1] are non-convex, they may be susceptible to local minima.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Also its performance is better than the estimators proposed in [2] (which were based on first order and second order Taylor approximations).", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "Also its performance is better than the Greenwood and Durand estimators given in [9](which were the most superior in that paper) and equivalent to the moment based estimator in most of the m regimes.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "When the number of points obtained in each conversion is less, the variance of state of art estimators increases (apart from the pre-mentioned errors stemming from algebraic approximations as in [2]).", "startOffset": 195, "endOffset": 198}, {"referenceID": 9, "context": "REVIEW OF HIDDEN MARKOV RANDOM FIELDS(HMRF) AS APPLIED TO IMAGE SEGMENTATION Basically in the project in [10], the image segmentation algorithm has the following assumptions: Let y be the set of image pixels and x be the set of labels which are initialized similarly by k-means algorithm and are later inferred again by (Expectation-Maximization) EM algorithm.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "xa and xb are two neighboring pixels connected by a Markov graph as in [10] forming a clique.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "This situation occurs iff the equations in [1] are exactly solved.", "startOffset": 43, "endOffset": 46}], "year": 2014, "abstractText": "Recently we have proposed a maximum-likelihood iterative algorithm for estimation of parameters of the Nakagamim distribution. This technique performs better than state of art estimation techniques for this distribution. This could be of particular use in low-data/block based estimation problems. In these scenarios, the estimator should be able to give accurate estimates (in the mean square sense) with less amount of data. Also, the estimates should improve with increase in number of blocks received. In this paper, we see through our simulations, that our proposal is well designed for meeting such requirements. Further, it is well known in the literature that an efficient estimator does not exist for the Nakagami-m distribution. In this paper, we also derive a theoretical expression for the variance of our proposed estimator. We find that this expression clearly fits the experimental curve for the variance of the proposed estimator. This expression is pretty close to the Cramer Rao Lower Bound (CRLB).", "creator": "LaTeX with hyperref package"}}}