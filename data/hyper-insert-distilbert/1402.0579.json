{"id": "1402.0579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk", "abstract": "this paper presents a model - design based planner called achieving the human probabilistic sulu planner or the vector p - sulu planner, which controls stochastic systems implemented in a goal broadly directed manner within user - specified persistent risk awareness bounds. ensuring the objective target of composing the p - eng sulu planner is adequate to allow users'to integrate command efficient continuous, credible stochastic systems, such as unmanned commercial aerial and space vehicles, in engaging a sophisticated manner that is both musically intuitive and safe. specifically to execute this intuitive end, we clearly first extensively develop a robust new plan representation called a chance - constrained qualitative state plan ( ccqsp ), through integrating which users can specify the desired evolution element of targeting the plant state as well successfully as the calculated acceptable yield level criterion of total risk. provide an example of implementing a ccqsp statement is go to achieving a pass through exit b within 30 minutes, with less commonly than 0. 001 % default probability of failure. \" we hence then now develop effectively the p - sulu planner, \u2033 which can further tractably solve creating a ccqsp planning readiness problem. in order largely to enable automatic ccqsp planning, we accordingly develop the following two potential capabilities in this paper : beyond 1 ) geographic risk - guided sensitive decision planning schemes with risk uncertainty bounds, between and above 2 ) strategic goal - directed planning in a highly continuous domain with temporal constraints. the enhanced first capability paradigm is employed to ensures that calculating the probability of avoiding failure worldwide is therefore bounded. the second capability is essential for guiding the planner to solve dynamic problems with a continuous restricted state space such as vehicle path planning. we demonstrate out the psychological capabilities of the p - ping sulu environmental planner by simulations on two real - imaginary world scenarios : whether the rocket path decision planning and scheduling consisted of shipping a large personal aerial vehicle and as often well the as inducing the space rendezvous of loading an appropriate autonomous cargo transportation spacecraft.", "histories": [["v1", "Tue, 4 Feb 2014 01:41:20 GMT  (3050kb)", "http://arxiv.org/abs/1402.0579v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["masahiro ono", "brian c williams", "l blackmore"], "accepted": false, "id": "1402.0579"}, "pdf": {"name": "1402.0579.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk", "authors": ["Masahiro Ono", "Brian C. Williams", "Lars Blackmore"], "emails": ["ONO@APPI.KEIO.AC.JP", "WILLIAMS@MIT.EDU", "LARS.BLACKMORE@SPACEX.COM"], "sections": [{"heading": "1. Introduction", "text": "There is an increasing need for risk-sensitive optimal planning in uncertain environments, while guaranteeing an acceptable probability of success. A motivating example for this article is the Boeing concept of a future aerial personal transportation system (PTS), as shown in Figure 1. The PTS consists of a fleet of small personal aerial vehicles (PAV) that enable the flexible point-to-point transportation of individuals and families.\nc\u20dd2013 AI Access Foundation. All rights reserved.\nIn order to provide safety, PTS should be highly automated. In 2004, in the US, pilot error was listed as the primary cause of 75.5% of fatal general aviation accidents, according to the 2005 Joseph T. Nall Report (Aircraft Owners and Pilots Association Air Safety Foundation, 2005). Automated path planning, scheduling, collision avoidance, and traffic management will significantly improve the safety of PTS, as well as its efficiency. The challenges to operating such a system include adapting to uncertainties in the environment, such as storms and turbulence, while satisfying the complicated needs of users.\nThere is a substantial body of work on planning under uncertainty that is relevant. However, our approach is distinctive in three key respects. First, our planner, the p-Sulu Planner, allows users to explicitly limit the probability of constraint violation. This capability is particularly important for risk-sensitive missions where the impact of failure is significant. Second, the planner is goal-directed, by which we mean that it achieves time-evolved goals within user-specified temporal constraints. Third, the planner works in a continuous state space. A continuous state space representation fits naturally to many real-world applications, such as planning for aerial, space, and underwater vehicles. It is also important for problems with resources.\nFigure 2 shows a sample PTS scenario. A passenger of a PAV starts in Provincetown, MA and wants to go to Bedford within 30 minutes. The passenger also wants to go through a scenic area and remain there between 5 and 10 minutes during the flight. There is a no-fly zone (NFZ) and a storm that must be avoided. However, the storm\u2019s future location is uncertain; the vehicle\u2019s location is uncertain as well, due to control error and exogenous disturbances. Thus there is a risk of penetrating the NFZ or the storm. The passengers want to limit such risk to at most 0.001%.\nIn order to handle such a planning problem, we introduce a novel planner called the Probabilistic Sulu Planner (p-Sulu Planner), building upon prior work on the model-based plan executive called Sulu (Le\u0301aute\u0301 & Williams, 2005). The p-Sulu Planner provides the following three capabilities, in order to meet the needs described in the above scenario: 1) goal-directed planning in a continuous domain, 2) near-optimal planning, and 3) risk-sensitive planning with risk bounds.\n\u2022 Goal-directed planning in a continuous domain The p-Sulu Planner must plan actions with continuous effects that achieve time evolved goals specified by users. In the case of the PTS scenario in Figure 2, the PAVmust sequentially achieve two temporally extended goals, called"}, {"heading": "1.1 Overview of the Planner", "text": "This section describes the inputs and outputs of the p-Sulu Planner informally. They are rigorously defined in Section 2."}, {"heading": "1.1.1 INPUTS", "text": "Initial Condition The p-Sulu Planner plans a control sequence starting from the current state, which is typically estimated from noisy sensor measurements. Therefore, the p-Sulu Planner takes the probability distribution, instead of the point estimate, of the current state as the initial condition.\nStochastic Plant Model In the control community the planning problem is to generate a sequence of control inputs that actuate a physical system, called the plant. The action model for a plant is typically a system of real-valued equations over control, state and observable variables. The pSulu Planner takes as an input a linear stochastic plant model, which specifies probabilistic state transitions in a continuous domain. This is a stochastic extension of the continuous plant model used by Le\u0301aute\u0301 and Williams (2005). In this paper we limit our focus to Gaussian-distributed uncertainty.\nChance-constrained qualitative state plan (CCQSP) In order to provide users with an intuitive way to command stochastic systems, we develop a new plan representation called a chanceconstrained qualitative state plan (CCQSP). It is an extension of qualitative state plan (QSP), developed and used by Le\u0301aute\u0301 and Williams (2005), Hofmann and Williams (2006), and Blackmore, Li, and Williams (2006). CCQSP specifies a desired evolution of the plant state over time, and is defined by a set of discrete events, a set of episodes, which impose constraints on the plant state evolution, a set of temporal constraints between events, and a set of chance constraints that specify reliability constraints on the success of sets of episodes in the plan.\nA CCQSP may be depicted as a directed acyclic graph, as shown in Figure 3. The circles represent events and squares represent episodes. Flexible temporal constraints are represented as a simple temporal network (STN) (Dechter, Meiri, & Pearl, 1991), which specifies upper and lower bounds on the duration between two events (shown as the pairs of numbers in parentheses). The plan in Figure 3 describes the PTS scenario depicted in Figure 2, which can be stated informally as:\n\u201cStart from Provincetown, reach the scenic region within 30 time units, and remain there for between 5 and 10 time units. Then end the flight in Bedford. The probability of failure of these episodes must be less than 1%. At all times, remain in the safe region by avoiding the no-fly zones and the storm. Limit the probability of penetrating such obstacles to 0.0001%. The entire flight must take at most 60 time units.\u201d\nA formal definition of CCQSP is given in Section 2.4.3.\nObjective function The user of the p-Sulu Planner can specify an objective function (e.g., a cost function). We assume that it is a convex function."}, {"heading": "1.1.2 OUTPUT", "text": "Executable control sequence The p-Sulu Planner plans over a finite horizon. One of the two outputs of the p-Sulu Planner is an executable control sequence over the horizon that satisfies all constraints specified by the input CCQSP. In the case of the PTS scenario, the outputs are the vehicle\u2019s actuation inputs, such as acceleration and ladder angle, that result in the nominal paths shown\nin Figure 2. In order for the control sequence to be executable, it must be dynamically feasible. For example, the curvature of the PAV\u2019s path must not exceed the vehicles\u2019 maneuverability.\nOptimal schedule The other output of the p-Sulu Planner is the optimal schedule, a set of execution time steps for events in the input CCQSP that minimizes a given cost function. In the case of the PTS scenario shown in Figure 3, a schedule specifies when to leave the scenic region and when to arrive at Bedford, for example. The p-Sulu Planner finds a schedule that satisfies all the simple temporal constraints specified by the CCQSP, and minimizes the cost function.\nThe two outputs \u2013 the control sequence and the schedule \u2013 must be consistent with each other: the time-evolved goals are achieved on the optimal schedule by applying the control sequence to the given initial conditions."}, {"heading": "1.2 Approach", "text": "The p-Sulu Planner must solve a very difficult problem of generating an executable control sequence for a CCQSP, which involves both combinatorial optimization of a discrete schedule and non-convex optimization of a continuous control sequence. Our approach in this article is to develop the p-Sulu Planner in three technical steps, which we call \u201cspirals\u201d.\nIn the first spiral, described in Section 4, we solve a special case of the CCQSP planning problem, where the feasible state space is convex (e.g., path planning problem without obstacles) and the schedule is fixed, as shown in Figure 4-(a). This problem can be transformed into a convex optimization problem by the risk allocation approach, which is presented in our previous work (Ono & Williams, 2008a). We obtain a feasible, near-optimal solution to the CCQSP planning problem by optimally solving the convex optimization using an interior point method (Blackmore & Ono, 2009).\nIn the second spiral, which is presented in Section 5, we consider a CCQSP problem with a non-convex state space in order to include obstacles, as in Figure 4-(b). We develop a branch and bound-based algorithm, called non-convex iterative risk allocation (NIRA). Subproblems of the branch-and-bound search of NIRA are convex chance-constrained optimal control problems, which are solved in the first spiral. The NIRA algorithm cannot handle a problem with a flexible schedule.\nIn the third spiral, which is described in Section 6, we develop another branch and boundbased algorithm, namely the p-Sulu Planner, which can solve a general CCQSP planning problem with a flexible schedule and obstacles. Subproblems of the branch-and-bound search of the pSulu Planner are non-convex chance-constrained optimal control problems, which are solved by the NIRA algorithm.\n(Ono & Williams 2008b) (Section 4)\nNIRA (Section 5)\np-Sulu (Section 6)"}, {"heading": "1.3 Related Work", "text": "Recall that the CCQSP planning problem is distinguished by its use of time-evolved goals, continuous states and actions, stochastic optimal solutions and chance constraints. While the planning and control disciplines have explored aspects of this problem, its solution in total is novel, and our approach to solving this problem efficiently through risk allocation is novel.\nMore specifically, there is an extensive literature on planning with discrete actions to achieve temporally extended goals (TEGs), such as TLPlan (Bacchus & Kabanza, 1998) and TALPlan (Kvarnstrom & Doherty, 2000), which treat TEGs as temporal domain control knowledge and prune the search space by progressing the temporal formula. However, since these TEG planners assume discrete state spaces, they cannot handle problems with continuous states and effects without discretization. Ignoring chance constraints, the representation of time evolved goals used by TLPlan and the p-Sulu Planner is similar. TLPlan uses a version of metric interval temporal logic (MITL) (Alur, Feder, & Henzinger, 1996) applied to discrete states, while the p-Sulu Planner uses qualitative state plans (QSPs) (Le\u0301aute\u0301 & Williams, 2005; Hofmann & Williams, 2006; Li, 2010) over continuous states. Li (2010) shows that, for a given state space, any QSP can be expressed in MITL. The key difference that defines the p-Sulu Planner is the addition of chance constraints, together with its use of continuous variables.\nSeveral planners, particularly those that are employed as components of model-based executives, command actions in continuous state space. For example, Sulu (Le\u0301aute\u0301 & Williams, 2005) takes as input a deterministic linear model and QSP, which specifies a desired evolution of the plant state as well as flexible temporal constraints, and outputs a continuous control sequence. Chekhov (Hofmann &Williams, 2006) also takes as input a QSP and a nonlinear deterministic system model, and outputs a continuous control sequence. In order to enable fast real-time plan execution, Chekhov precomputes flow tubes, the sets of continuous state trajectories that end in the goal regions specified by the given plan. Kongming (Li, 2010) provides a generative planning capability for hybrid\nsystems, involving both continuous and discrete actions. It employs a compact representation of hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and flow tubes for continuous actions. These planners adapt to the effects of uncertainty, but do not explicitly reason about the effects of uncertainty during planning. For example, Sulu employs a receding horizon approach, which continuously replans the control sequence using the latest measurements. Chekhov\u2019s flow tube representation of feasible policies allows the executive to generate new control sequences in response to disturbances on-line. The p-Sulu Planner is distinct from these continuous planners in that it plans with a model of uncertainty in dynamics, instead of just reacting to it. Its plan guarantees the user-specified probability of success by explicitly reasoning about the effects of uncertainty.\nIn AI planning literatures, a planning domain description language, PDDL+, supports mixed discrete-continuous planning domains (Fox& Long, 2006). Probabilistic PDDL (Younes & Littman, 2004) and the Relational Dynamic influence Diagram Language (RDDL) (Sanner, 2011) can handle stochastic systems. Recently, Coles, Coles, Fox, and Long (2012) developed a forward-chaining heuristic search planner named COLIN, which can deal with continuous linear change and durationdependent effects. However, these planners do not handle chance constraints. We note that the outputs of the p-Sulu Planner is continuous in space but discrete in time. The time-dependent MDP developed by Boyan and Littman (2000) can handle continuous time by encoding time in the state. Extension of the p-Sulu Planner to continuous-time planning would be an interesting future direction.\nMost work within the AI community on probabilistic planning has focused on planning in discrete domains and builds upon the Markov decision process (MDP) framework. A growing subcommunity has focused on extensions of MDPs to the continuous domain. However, tractability is an issue, since they typically require partitioning or approximation of continuous state space. A straightforward partitioning of continuous state and action spaces into discrete states and actions often leads to an exponential blow-up in running time. Furthermore, when the feasible state space is unbounded, it is impossible to partition the space into a finite number of compact subspaces. An alternative approach is the function approximation (Boyan & Moore, 1995), but its convergence is guaranteed only when the approximation error is bounded (Bertsekas & Tsitsiklis, 1996; Lagoudakis & Parr, 2003). Time-dependent MDPs (Boyan & Littman, 2000; Feng, Dearden, Meuleau, & Washington, 2004) can do efficient partitioning of continuous state space, but make an assumption that the set of available states and actions are finite (i.e., discrete). Hence, planning by these MDPs in a continuous state space, such as Rn, requires to approximate the state space by a finite number of discrete states. Our approach is essentially different from the MDP approaches in that the continuous variables are directly optimized through convex optimization without discretization of continuous state space. Hence, the continuity of the state space does not harm the tractability of the p-Sulu Planner.\nA second point of comparison is the treatment of risk. Like the p-Sulu Planner, the MDP framework offers an approach to marrying utility and risk. However, most MDP algorithms balance the utility and risk by assigning a large negative utility to the event of constraint violation. Such an approach cannot guarantee bounds on the probability of constraint violation. The constrained MDP approach (Altman, 1999) can explicitly impose constraints. Dolgov and Durfee (2005) showed that stationary deterministic policies for constrained MDPs can be obtained by solving a mixed integer linear program (MILP). However, the constrained MDP framework can only impose bounds on the expected value of costs, and again, cannot guarantee strict upper bounds on the probability\nof constraint violation. In contrast, the p-Sulu Planner allows users to impose chance constraints, which explicitly restrict the probability of constraint violation. As far as the authors know, the risk-sensitive reinforcement learning approach proposed by Geibel and Wysotzki (2005) is the only work that considers chance constraints in the MDP framework. They developed a reinforcement learning algorithm for MDPs with a constraint on the probability of entering error states. Our work is distinct from theirs in that the p-Sulu Planner is goal-directed, by which we mean that it achieves time-evolved goals within user-specified temporal constraints. To summarize, no prior MDP work supports continuous state and actions in combination with general continuous noise on transitions while ensuring that the probability of failure is bounded.\nRisk-sensitive control methods in a continuous domain have been extensively studied in the discipline of control theory. For example, the celebrated H\u221e control method minimizes the effect of disturbances on the output of a system while guaranteeing the stability of the system (Stoorvogel, 1992). Risk-sensitive control approaches allow users to choose the level of risk averseness through the minimization of an expected exponentiated cost function (Jacobson, 1973; Fleming & McEneaney, 1995). However, these approaches do not address chance constraints and optimal scheduling. Several methods have been proposed for solving stochastic optimal control problems over continuous variables with chance constraints. The method proposed by van Hessem (2004) turns a stochastic problem into a deterministic problem using a very conservative ellipsoidal relaxation. Blackmore (2006) proposes a sampling-based method called Particle Control, which evaluates joint chance constraints by a Monte-Carlo simulation, instead of using a conservative bound. As a result, the stochastic planning problem is reduced to a MILP problem. Although it has a theoretical guarantee that it can obtain the exactly optimal solution when an infinite number of samples are used, computation time is an issue. Blackmore et al. (2006) and Nemirovski and Shapiro (2006) employed Boole\u2019s inequality to decompose a joint chance constraint into individual chance constraints. Although Boole\u2019s inequality is less conservative than the ellipsoidal relaxation, their approach still has non-negligible conservatism since it fixes each individual risk bound to a uniform value. Our approach builds upon this approach, with modifications to allow flexible individual risk bounds.\nTo the best of the authors\u2019 knowledge, the p-Sulu Planner is the first goal-directed planner that is able to plan in a continuous state space with chance constraints."}, {"heading": "1.4 Innovations", "text": "The p-Sulu Planner is enabled by six innovations presented in this article. First, in order to allow users to command stochastic systems intuitively, we develop a new plan representation, CCQSP (Section 2.4.3). Second, in order to decompose a chance constraint over a disjunctive clause into a disjunction of individual chance constraints, we introduce the risk selection approach (Section 5.1.2). Third, in order to obtain lower bounds for the branch-and-bound search in NIRA, we develop the fixed risk relaxation (FRR), a linear program relaxation of the subproblems (Section 5.4.2). Fourth, we minimize the search space for the optimal schedule by introducing a new forward checking method that efficiently prunes infeasible assignment of execution time steps (Section 6.2). Fifth, in order to enhance the computation time of schedule optimization, we introduce a method to obtain a lower bound for the branch-and-bound by solving fixed-schedule planning problems with an partial assignment of a schedule. (Section 6.3)\nSixth, in order to minimize the number of non-convex subproblems solved in the branch-andbound search, we introduce a variable ordering heuristic, namely the convex-episode-first (CEF) heuristic, which explores the episodes with a convex feasible state region before the ones with a non-convex state region (Section 6.2.2).\nThe rest of this article is organized as follows. Section 2 formally defines the CCQSP and states the CCQSP planning problem. Section 3 derives the encoding of the problem as a chanceconstrained optimization problem, as well as the encodings of two limited versions of the CCQSP planning problem: one with a fixed schedule and a convex state space, and another with a fixed schedule and a non-convex state space. Section 4 reviews the solution to a fixed-schedule CCQSP planning problem with a convex state space. Section 5 develops the NIRA algorithm, which solves a fixed-schedule CCQSP planning problem with a non-convex state space, and Section 6 introduces the p-Sulu Planner, which solves a CCQSP planning problem with a flexible schedule and a nonconvex state space. Finally, Section 7 shows simulation results on various scenarios, including the personal transportation system (PTS)."}, {"heading": "2. Problem Statement", "text": "Recall that the p-Sulu Planner takes as input a linear stochastic plant model, which specifies the effects of actions; an initial state description, describing a distribution over initial states; a CCQSP, which specifies desired evolutions of the state variables, as well as acceptable levels of risk; and an objective function. Its output is an executable control sequence and an optimal schedule. Planning is performed over a finite horizon, since the p-Sulu Planner is incorporated with the finite-horizon optimal control. We first define the variables used in the problem formulations. Then we define elements of the inputs and outputs."}, {"heading": "2.1 Definition of Time Step", "text": "We consider a series of discretized finite time steps t = 0, 1, 2, \u00b7 \u00b7 \u00b7N with a fixed time interval \u2206T , where integer N is the size of the planning horizon. Since the time interval \u2206T can take any positive real value, it suffices to consider time steps with only integer indices to approximate the system\u2019s dynamics. We use the term \u201ctime step\u201d to mean an integer index of the discretized time steps, while using the term \u201ctime\u201d to mean a real-valued time. We define sets T and T\u2212 as follows:\nT := {0, 1, 2, \u00b7 \u00b7 \u00b7N}. (1) T\u2212 := {0, 1, 2, \u00b7 \u00b7 \u00b7N \u2212 1}. (2)\nWe limit the scope of this article to a discrete-time stochastic system. This is because optimizing a control sequence for a continuous-time stochastic system requires solving a stochastic differential equation (SDE) repeatedly. Performing such a computation is not tractable except for very simple problems."}, {"heading": "2.2 Definitions of Events", "text": "An event denotes the start or end of an episode of behavior in our plan representation.\nDefinition 1. An event e \u2208 E is a instance that is executed at a certain time step in T.\nWe define two special events, the start event e0 and the end event eE . Without loss of generality, we assume that e0 is executed at t = 0. The end event eE represents the termination of the entire plan."}, {"heading": "2.3 Definitions of Variables", "text": "Variables used in our problem formulation involve a discrete schedule, a continuous state vector, and a continuous control vector.\nWe formally define an event as well as a schedule as follows:\nDefinition 2. An execution time step s(e) \u2208 T is an integer-valued scalar that represents the time step at which an event e \u2208 E is executed. A schedule s := [s(e0), s(e1), \u00b7 \u00b7 \u00b7 s(eE)] is a sequence of execution time steps of all the events e \u2208 E . Finally, a partial schedule \u03c3 := [\u03c3(e) \u2208 s | e \u2208 E\u03c3 \u2286 E ] is an ordered set of execution time steps of a subset of events E\u03c3.\nBy definition, the start event is executed at t = 0 i.e, s(e0) = 0. Following the notation of a schedule, we denote by \u03c3(e) the execution time of an event e \u2208 E\u03c3. See also the definition of a schedule (Definition 2).\nWe consider a continuous state space, where a state vector and a state sequence are defined as follows:\nDefinition 3. A state vector xt \u2208 Rnx is a real-valued vector that represents the state of the plant at time step t. A state sequence x0:N := [x0 \u00b7 \u00b7 \u00b7xN ] is a vector of state variables from time step 0 to N .\nOur actions are assignments to continuous decision variables, which are referred to as a control vector:\nDefinition 4. A control vector ut \u2208 Rnu is a real-valued vector that represents the control input to the system at time step t. A control sequence u0:N\u22121 := [u0 \u00b7 \u00b7 \u00b7uN\u22121] is a vector of control inputs from time 0 to N \u2212 1."}, {"heading": "2.4 Definitions of Inputs", "text": "This subsection defines the four inputs of the p-Sulu Planner: an initial condition, a stochastic plant model, a CCQSP, and an objective function."}, {"heading": "2.4.1 INITIAL CONDITION", "text": "The belief state at the beginning of the plan is represented by an initial state, which is assumed to have a Gaussian distribution with a known mean x\u03040 and a covariance matrix \u03a3x0 :\nx0 \u223c N (x\u03040,\u03a3x0). (3)\nThe parameters in (3) are specified by an initial condition, which is defined as follows:\nDefinition 5. An initial condition I is a pair I = \u27e8x\u03040,\u03a3x0\u27e9, where x\u03040 is the mean initial state and \u03a3x0 is the covariance matrix of the initial state."}, {"heading": "2.4.2 STOCHASTIC PLANT MODEL", "text": "The p-Sulu Planner controls dynamical systems in which actions correspond to the settings of continuous control variables, and whose effects are on continuous state variables. The p-Sulu Planner specifies these actions and their effects through a plant model. A plant model is considered as a state transition model in a continuous space. We employ a variant of a linear plant model with additive Gaussian uncertainty that is commonly used in the context of chance-constrained stochastic optimal control (Charnes & Cooper, 1959; Nemirovski & Shapiro, 2006; Oldewurtel, Jones, & Morari, 2008; van Hessem, 2004), with a modification to consider controller saturation. Specifically, we assume the following plant model:\nxt+1 = Atxt +Bt\u00b5U(ut) +wt (4)\nwhere wt \u2208 Rnx is a state-independent disturbance at t-th time step that has a zero-mean Gaussian distribution with a given covariance matrix denoted by \u03a3wt :\nwt \u223c N (0,\u03a3wt). (5)\nAlthough this model prohibits state-dependent disturbance, most types of noise involved in our target applications are state independent. For example, in the PTS scenario introduced in Section 1, the primary source of uncertainty is a wind turbulence, which is typically not dependent on the state of a vehicle. In the space rendezvous scenario discussed in Section 7.5, the main sources of perturbations for a space craft are the tidal force and unmodeled gravitational effects of Sun, Moon, and other planets (Wertz &Wiley J. Larson, 1999). Such noises can be modeled as a state-dependent noise in practice when the scale of the planned actions is significantly smaller than that of the Solar System.\nnot dependent on the state of the space craft. We note that our problem formulation can encode time-varying noise by specifying different covariance matrices \u03a3wt for each time step.\nThe set U \u2282 Rnu is a compact convex set that represents the continuous domain of the feasible control inputs. If an infeasible control input ut /\u2208 U is given to the plant, its actuators saturate. The function \u00b5U(\u00b7) : Rnu 7\u2192 U in (4) represents the effect of actuator saturation as follows:\n\u00b5U(u) := { u (if u \u2208 U) PU(u) (otherwise) ,\nwhere PU(u) is a projection of u on U. For example, when u is one-dimensional and U = [l, u], PU(u) = max(min(u, u), l). Note that \u00b5U introduces nonlinearity in the plant.\nThe parameters in (4) and (5) are specified by a stochastic plant model, which is defined as follows:\nDefinition 6. A stochastic plant model M is a four-tuple M = \u27e8A0:N\u22121,B0:N\u22121,\u03a3w0:N\u22121 ,U\u27e9, where A0:N\u22121 and B0:N\u22121 are sets of N matrices A0:N\u22121 := {A0,A1, \u00b7 \u00b7 \u00b7AN\u22121}, B0:N\u22121 := {B0,B1, \u00b7 \u00b7 \u00b7BN\u22121},\u03a3w0:N\u22121 is a set ofN covariance matrices\u03a3w0:N\u22121 = {\u03a3w0 ,\u03a3w1 , \u00b7 \u00b7 \u00b7 ,\u03a3wN\u22121}, and U \u2282 Rnu is a compact convex set that represents the domain of the feasible control inputs.\nNote that xt, as well as wt, is a random variable, while ut is a deterministic variable. Figure 5 illustrates our plant model. In a typical plant model, the probability circles grow over time since disturbancewt is added at every time step, as drawn in the figure. This effect represents a commonly observed tendency that the distant future involves more uncertainty than the near future.\nIn order to mitigate the accumulation of uncertainty, we employ a close-loop control approach, which generates the control input ut by incorporating a nominal control input u\u0304t \u2208 Rnu with an error feedback, as follows:\nut = u\u0304t +Kt(xt \u2212 x\u0304t), (6)\nwhere Kt is a matrix representing a constant stabilizing feedback gain at time t and x\u0304t is the nominal state vector. The nominal state x\u0304t is obtained by the following recursion:\nx\u03040 := x0 (7)\nx\u0304t+1 = Atx\u0304t +Btu\u0304t. (8)\nA closed-loop control approach has been employed by Geibel and Wysotzki (2005) and Oldewurtel et al. (2008) in the context of chance-constrained optimal control and shown that it significantly improves performance.\nIn this closed-loop planning method, the nominal control input u\u0304t is planned before the execution. The actual control input ut is computed in real time by using (6). The feedback term in (6) linearly responds to the error xt \u2212 x\u0304t. By choosing the feedback gain Kt appropriately, the growth of the probability circles in Figure 5 can be slowed down. Neglecting the effect of controller saturation (i.e., assuming U = Rnx), it follows from (4) and (6) that xt has a Gaussian distribution with a covariance matrix \u03a3xt , which evolves as follows:\n\u03a3xt+1 = (At +BtKt)\u03a3xt(At +BtKt) T +\u03a3wt . (9)\nIn a typical plant, some of the eigenvalues of A are one. Therefore, when there is no error feedback (i.e., Kt = 0), the \u201csize\u201d of \u03a3xt grows by \u03a3wt at each iteration. By choosing Kt so that the norm of the largest eigenvalue of (At +BtKt) is less than one, the covariance \u03a3xt does not grow continuously. Such a feedback gain can be found by using standard control techniques, such as a linear quadratic regulator (LQR) (Bertsekas, 2005). Since we consider a finite-horizon, discretetime planning problem, the optimal time-varying LQR gain Kt is obtained by solving the finitehorizon, discrete-time Riccati equation. In practice, it often suffices to use the steady-state (i.e., time-invariant) LQR gain, which is obtained by solving the infinite-horizon, discrete-time Riccati equation for simplicity. We note that the feedback gainKt can also be optimized in real time. This approach is often used for robust and stochastic model predictive controls (Goulart, Kerrigan, & Maciejowski, 2006; Oldewurtel et al., 2008; Ono, 2012). However, such an extension is beyond the scope of this paper.\nAn issue is that, if the error xt \u2212 x\u0304t happens to be very large, the control input ut may exceed its feasible domain U, resulting in actuator saturation. Therefore, (9) does not hold due to the nonlinearity of the function \u00b5U(\u00b7). We address this issue through the risk allocation approach. More specifically, we impose chance constraints on control saturation, and allocate risk to both state and control constraints. This approach is discussed more in detail in Section 4.1.5."}, {"heading": "2.4.3 CHANCE-CONSTRAINED QUALITATIVE STATE PLAN (CCQSP)", "text": "A qualitative state plan (QSP) (Le\u0301aute\u0301 & Williams, 2005) is a temporally flexible plan that specifies the desired evolution of the plant state. The activities of a QSP are called episodes and specify constraints on the plant state. CCQSP is an extension of QSPs to stochastic plans that involve chance constraints, defined as follows:\nDefinition 7. A chance-constrained qualitative state plan (CCQSP) is a four-tupleP = \u27e8E ,A, T , C\u27e9, where E is a set of discrete events, A is a set of episodes, T is a set of simple temporal constraints, and C is a set of chance constraints.\nThe four elements of a CCQSP are defined precisely in a moment. Like a QSP, a CCQSP can be illustrated diagrammatically by a directed acyclic graph in which the discrete events in E are represented by vertices, drawn as circles, and the episodes as arcs with ovals. A CCQSP has a start event e0 and an end eE , which corresponds to the beginning and the end of the mission, respectively.\nFor example, Figure 3 shows the CCQSP of the PTS scenario. The state regions and obstacles in the CCQSP are illustrated in Figure 2. It involves four events: E = {e0, e1, e2, eE}. Their meanings are described as follows.\n1. The start event e0 corresponds to the take off of the PAV from Provincetown.\n2. The second event e1 corresponds to the time step when PAV reaches the scenic region.\n3. Event e2 is associated with the time instant when the PAV has just left the scenic region.\n4. The end event eE corresponds to the arrival of the PAV in Bedford.\nThe CCQSP has four episodes A = {a1, a2, a3, a4} and two chance constraints C = {c1, c2}. A natural language expression of the CCQSP is:\n\u201c Start from Provincetown, reach the scenic region within 30 time units, and remain there for between 5 and 10 time units. Then end the flight in Bedford. The probability of failure of these activities must be less than 1%. At all times, remain in the safe region by avoiding the no-fly zones and the storm. Limit the probability of penetrating such obstacles to 0.0001%. The entire flight must take at most 60 time units.\u201d\nBelow we formally define the three types of constraints - episodes, temporal constraints, and chance constraint.\nEpisodes Each episode a \u2208 A specifies the desired state of the system under control over a time interval.\nDefinition 8. An episode a = \u27e8eSa , eEa ,\u03a0a(tS , tE), Ra\u27e9 has an associated start event eSa and an end event eEa . Ra \u2208 RN is a region in a state space. \u03a0a \u2286 T is a set of time steps at which the state xt must be in the region Ra.\nThe feasible region Ra can be any subset of RN . We will approximate Ra with a set of linear constraints later in Section 3.1.1.\n\u03a0a(tS , tE) is a subset of T given as a function of the episode\u2019s start time step tS = s(eSa ) and its end time step tE = s(eEa ). Different forms of \u03a0a(tS , tE) result in various types of episodes. The following three types of episodes are particularly of interest to us:\n1. Start-in episode: \u03a0a(tS , tE) = {tS}\n2. End-in episode: \u03a0a(tS , tE) = {tE}\n3. Remain-in episode: \u03a0a(tS , tE) = {tS , tS + 1, \u00b7 \u00b7 \u00b7 , tE}\nFor a given episode a, the set of time steps at which the plant state must be in the region Ra is obtained by substituting s(eSa ) and s(e E a ), the execution time steps of the start event and the end event of the episode, into tS and tE . In other words, an episode a requires that the plant state is in Ra for all time steps in \u03a0a ( s(eSa ), s(e E a ) ) . For the rest of the article, we use the following abbreviated notation: \u03a0a(s) := \u03a0a ( s(eSa ), s(e E a ) ) .\nUsing this notation, an episode is equivalent to the following state constraint:\u2227 t\u2208\u03a0a(s) xt \u2208 Ra. (10)\nFor example, in the CCQSP shown in Figure 3, there are four episodes: a1 (\u201cStart in [Provincetown]\u201d), a2 (\u201cRemain in [Scenic region]\u201d), a3 (\u201cEnd in Bedford\u201d), and a4 (\u201cRemain in [safe region]\u201d).\nIn Section 6, we solve a relaxed optimization problem with a partial schedule (Definition 2) in order to obtain a lower bound on the optimal objective value. In such relaxed problems, only a subset of the episodes that are relevant to the given partial schedule are imposed. We formally define a partial episode set of a partial schedule \u03c3 as follows:\nDefinition 9. Given a partial schedule \u03c3, A(\u03c3) \u2286 A is its partial episode set, which is a subset of A that involves the episodes whose start event and end event are assigned execution time steps.\nA(\u03c3) = { a \u2208 A | eSa \u2208 E\u03c3 \u2227 eEa \u2208 E\u03c3 } ,\nwhere the definition of E\u03c3 is given in Definition 2.\nChance constraint Recall that a chance constraint is a probabilistic constraint that requires the constraints defining each episode to be satisfied within a user-specified probability. A CCQSP can have multiple chance constraints. A chance constraint is associated with at least one episode.\nA chance constraint is formally defined as follows:\nDefinition 10. A chance constraint c = \u27e8\u03a8c,\u2206c\u27e9 is a constraint requiring that:\nPr  \u2227 a\u2208\u03a8c \u2227 t\u2208\u03a0a(s) xt \u2208 Ra  \u2265 1\u2212\u2206c, (11) where\u2206c is a user-specified risk bound and \u03a8c \u2286 A is a set of episodes associated with the chance constraint c.\nNote that every episode in a CCQSP must be associated with exactly one chance constraint. Any episode inAmust not be involved in more than one chance constraint or unassociated with any chance constraint.\nFor example, the CCQSP shown in Figure 3 has two chance constraints, c1 and c2. Their associated episodes are \u03a8c1 = {a1, a2, a3} and \u03a8c2 = {a4}. Therefore, c1 requires that the probability of satisfying the three episodes a1, a2, and a3 (colored in green) is more than than 99%, while c2 requires that the probability of satisfying the episode a4 is more than 99.99999%.\nWe make the following assumption, which is necessary in order to guarantee the convexity of constraints in Section 4.2.\nAssumption 1. \u2206c \u2264 0.5\nThis assumption requires that the risk bounds are less than 50%. We claim that this assumption does not constrain practical applications, since typically the user of an autonomous system would not accept more than 50% risk.\nTemporal constraint A CCQSP includes simple temporal constraints (STCs) (Dechter et al., 1991), which impose upper and lower bounds on the duration of episodes and on the temporal distances between two events in E .\nDefinition 11. A simple temporal constraint \u03c4 = \u27e8eS\u03c4 , eE\u03c4 , bmin\u03c4 , bmax\u03c4 \u27e9 is a constraint, specifying that the duration from a start event eS\u03c4 to an end event e E \u03c4 be in the real-valued interval [bmin\u03c4 , b max \u03c4 ] \u2286 [0,+\u221e].\nTemporal constraints are represented diagrammatically by arcs between nodes, labeled with the time bounds [bmin\u03c4 , b max \u03c4 ], or by labels over episodes. For example, the CCQSP shown in Figure 3 has four simple temporal constraints. One requires the time between e0 and e1 to be at most 30 time units. One requires the time between e1 and e2 to be at least 5 units and at most 10 units. One requires the time between e2 and eE to be at most 40 time units. One requires the time between e0 and eE to be at most 60 time units.\nA schedule s is feasible if it satisfies all temporal constraints in the CCQSP. The number of feasible schedules is finite, since T is discrete and finite. We denote by SF the domain of feasible schedules, which is formally defined as follows:\nSF = {s \u2208 T|E| | \u2200\u03c4\u2208T bmin\u03c4 \u2264 \u2206T{s(eE\u03c4 )\u2212 s(eS\u03c4 )} \u2264 bmax\u03c4 }, (12)\nwhere |E| is the number of events in the CCQSP. The temporal duration is multiplied by the time interval\u2206T because bmin\u03c4 and b min \u03c4 are real-valued time, while s is a set of discrete time steps in T."}, {"heading": "2.4.4 OBJECTIVE FUNCTION", "text": "In this section, we formally define the objective function.\nDefinition 12. An objective function J : UN \u00d7 XN \u00d7 SF 7\u2192 R is a real-valued function over the nominal control sequence u\u03040:N\u22121, the nominal state sequence x\u03041:N , and the schedule s. We assume that J is a convex function over x\u03041:N and u\u03040:N\u22121.\nA typical example of an objective function is the quadratic sum of control inputs, which requires the total control efforts to be minimized:\nJ(u\u03040:N\u22121, x\u03041:N , s) = N\u22121\u2211 t=0 ||u\u0304t||2.\nAnother example is: J(u\u03040:N\u22121, x\u03041:N , s) = s(eE), (13)\nwhich minimizes the total plan execution time, by requiring that the end event eE of the qualitative state plan be scheduled as soon as possible.\nThere is often a need to minimize the expectation of a cost function. Note that, in our case, the expectation of a function over x1:N and u0:N\u22121 can be reduced to a function over u\u03040:N\u22121 because it follows from (4)-(6) that the probability distributions of x1:N and u0:N\u22121 are uniquely determined by u\u03040:N\u22121 and Kt. In practice, it is often more convenient to express the objective function as a function of u\u03040:N\u22121 and x\u03041:N , rather than as a function of u\u03040:N\u22121. Since x\u03041:N are specified by u\u03040:N\u22121 using (8), the two expressions are equivalent. The conversion from the expectation of a cost function to a function over nominal values can be conducted a priori.\nIf there is no controller saturation, such a conversion can often be obtained in a closed form. The conversion is particularly straight forward when the cost function is polynomial, since the expectation is equivalent to a combination of raw moments, which can be readily derived from the cumulants. Note that the third and higher cumulants of the Gaussian distribution are zero. Below we show examples of the conversion regarding three commonly-used cost functions: linear, quadratic, and the Manhattan norm.\nE[xt] = x\u0304t (14) E[xTt Qxt] = x\u0304Tt Qx\u0304t + tr(Q\u03a3xt) (15)\nE[||xt||1] = nx\u2211 i=1 \u03c3xt,i\n\u221a 2\n\u03c0 1F1 ( \u22121 2 , 1 2 ,\u2212 x\u03042t,i 2\u03c32xt,i ) , (16)\nwhere Q is a positive definite matrix, \u03c3xt,i is the ith diagonal element of \u03a3xt , and 1F1(\u00b7) is a confluent hypergeometric function. All functions above are convex. The expectation of a function of ut can also be transformed to a function of u\u0304t in the same manner. Note that the second term on the right hand side of (15) is a constant. Hence, minimizing x\u0304Tt Qx\u0304t yields the same solution as minimizing E[xTt Qxt].\nWhen there is controller saturation, it is difficult to obtain the conversion in a closed-form due to the nonlinearity of \u00b5U(\u00b7) in (4). In practice, we use an approximation that assumes no saturation. Since our closed-loop control approach explicitly limits the probability of controller saturation to a small probability (see Section 4.1.5 for the detail), the approximation error is trivial. This claim is empirically validated in Section 7.2.4."}, {"heading": "2.5 Definitions of Outputs", "text": "The output of the p-Sulu Planner is an optimal solution, which consists of an optimal control sequence u\u22c60:N\u22121 \u2208 UN and an optimal schedule s\u22c6 \u2208 SF .\nDefinition 13. The optimal solution is a pair \u27e8u\u22c60:N\u22121, s\u22c6\u27e9. The solution satisfies all constraints in the given CCQSP (Definition 7), the initial condition I , and the stochastic plant model M . The solution minimizes the given objective function J(u0:N\u22121, x\u03041:N , s) (Definition 12)."}, {"heading": "2.6 Problem Statement", "text": "We now formally define the CCQSP planning problem.\nProblem 1: CCQSP Planning Problem Given a stochastic plant model M = \u27e8A0:N\u22121,B0:N\u22121,\u03a3w0:N\u22121\u27e9\u27e9, an initial condition I = \u27e8x\u03040,\u03a3x0\u27e9, a CCQSP P = \u27e8E ,A, T , C\u27e9, and an objective function J(u0:N\u22121, x\u03041:N , s), a CCQSP planning problem is to find an optimal solution \u27e8u\u22c60:N\u22121, s\u22c6\u27e9 for M, I, P , and J .\nWe note that the p-Sulu Planner gives a near-optimal solution to Problem 1. The p-Sulu Planner employs two approximations, namely risk allocation (Section 4.1.1) and risk selection (Section 5.1.1), for the sake of computational tractability. As a result, its solution is not strictly optimal in general. However, we empirically show in Section 7 that the suboptimality due to risk allocation and risk selection is significantly smaller than existing approximation methods."}, {"heading": "3. Problem Encoding", "text": "This section encodes the CCQSP planning problem stated in the previous section into a mathematical programming problem. Sections 4 - 6 then address how to solve this form of mathematical problem. Recall that we build our CCQSP planner, the p-Sulu Planner, in three spirals. We first present the problem encoding of a general CCQSP planning problem with a non-convex state space and a flexible schedule (Figure 4-(c)) in Subsection 3.1. Then we present the encodings of the two special cases of the CCQSP planning problem in Subsections 3.2 and 3.3: one with a non-convex state space and a fixed schedule (Figure 4-(b)), and one with a convex state space and a fixed schedule (Figure 4-(a))."}, {"heading": "3.1 Encoding of a CCQSP Planning Problem with a Non-convex State Space and Flexible Schedule", "text": ""}, {"heading": "3.1.1 ENCODING OF FEASIBLE REGIONS", "text": "In order to encode Problem 1 into a mathematical programming problem, the geometric constraint in (11), xt \u2208 Ra, must be represented by algebraic constraints. For that purpose, we approximate the feasible state regions Ra by a set of half-spaces, each of which is represented by a linear state constraint.\nFigure 6 shows two simple examples. The feasible region of (a) is outside of the obstacle, which is approximated by a triangle. The feasible region of (b) is inside of the pickup region, which is again approximated by a triangle. Each feasible region is approximated by a set of linear constraints as follows:\n(a) 3\u2228 i=1 hTi x \u2264 gi, (b) 3\u2227 i=1 hTi x \u2265 gi.\nWe approximate the feasible regions so that the set of linear constraints is a sufficient condition of the original state constraint xt \u2208 Ra.\nWe assume that the set of linear state constraints that approximates a feasible region has been reduced to conjunctive normal form (CNF) as follows:\n\u2227 k\u2208Ka \u2228 j\u2208Ja,k hTa,k,jxt \u2212 ga,k,j \u2264 0, (17)\nwhere Ka = {1, 2, \u00b7 \u00b7 \u00b7 |Ka|} and Jc,i = {1, 2, \u00b7 \u00b7 \u00b7 |Jc,i|} are sets of indices. By replacing xt \u2208 Ra in (11) by (17), a chance constraint c is encoded as follows:\nPr  \u2227 a\u2208\u03a8c \u2227 t\u2208\u03a0a(s) \u2227 k\u2208Ka \u2228 j\u2208Ja,k hTc,a,k,jxt \u2212 gc,a,k,j \u2264 0  \u2265 1\u2212\u2206c. (18) In order to simplify the notation, we merge indices a \u2208 \u03a8c, t \u2208 \u03a0a(s), and k \u2208 Ka into a new index i \u2208 Ic(s), where Ic(s) = {1, 2, \u00b7 \u00b7 \u00b7 |Ic(s)|} and |Ic(s)| = |Ka| \u00b7 \u2211 a\u2208\u03a8c |\u03a0a(s)|. We let ai, ki, and ti the indices that correspond to to the combined index i, and let hc,i,j = hc,ai,ki,j . Using these notations, the three conjunctions of (18) are combined into one, and we obtain the following encoding of a chance constraint:\nPr  \u2227 i\u2208Ic(s) \u2228 j\u2208Jc,i hTc,i,jxti \u2212 gc,i,j \u2264 0  \u2265 1\u2212\u2206c. (19) The specification of chance constraints given in (19) requires that all |Ic(s)| disjunctive clauses of state constraints must be satisfied with a probability 1\u2212\u2206c. The i\u2019th disjunctive clause of the c\u2019th chance constraint is composed of |Jc,i| linear state constraints."}, {"heading": "3.1.2 CCQSP PLANNING PROBLEM ENCODING", "text": "Using (3), (4), (5), (6), and (19), a CCQSP planning problem (Problem 1), which is solved in the third spiral, is encoded as follows:\nProblem 2: General CCQSP Planning Problem\nmin u\u03040:N\u22121,s\nJ(u0:N\u22121, x\u03041:N , s) (20)\ns.t. s \u2208 SF (21) xt+1 = Atxt +Bt\u00b5U(ut) +wt, \u2200t \u2208 T\u2212 (22) ut = u\u0304t +Kt(xt \u2212 x\u0304t), \u2200t \u2208 T\u2212 (23)\u2227 c\u2208C Pr  \u2227 i\u2208Ic(s) \u2228 j\u2208Jc,i hTc,i,jxti \u2212 gc,i,j \u2264 0\n \u2265 1\u2212\u2206c. (24) x0 \u223c N (x\u03040,\u03a3x0), wt \u223c N (0,\u03a3wt), \u2200t \u2208 T\u2212 (25)\nRecall that SF , formally defined in (12), is the set of schedules that satisfy all temporal constraints in the given CCQSP. This CCQSP execution problem is a hybrid optimization problem over both discrete variables s (schedule) and continuous variables u0:N\u22121 (control sequence). Note that the temporal constraints within Problem 2 are solved in Section 6. A similar problem encoding is also employed in the chance-constraint MDP proposed by Geibel and Wysotzki (2005). However, our encoding differs from Geibel and Wysotzki in two respects: 1) we optimize not only the continuous control sequence u0:N\u22121 but also the discrete schedule s with temporal constraints; 2) we allow joint chance constraints, which require the satisfaction of multiple state constraints for a given probability. Problem 2 is solved in Section 6."}, {"heading": "3.2 Encoding of a CCQSP Planning Problem with a Non-convex State Space and Fixed Schedule", "text": "A restricted version of a CCQSP planning problem with a fixed schedule, which is solved in the second spiral, is obtained by fixing s in Problem 2 as follows:\nProblem 3: CCQSP Planning Problem with a Fixed Schedule\nJ\u22c6(s) = min u\u03040:N\u22121\nJ \u2032(u0:N\u22121, x\u03041:N ) (26)\ns.t. xt+1 = Atxt +Bt\u00b5U(ut) +wt, \u2200t \u2208 T\u2212 (27) ut = u\u0304t +Kt(xt \u2212 x\u0304t), \u2200t \u2208 T\u2212 (28)\u2227 c\u2208C Pr  \u2227 i\u2208Ic(s) \u2228 j\u2208Jc,i hTc,i,jxti \u2212 gc,i,j \u2264 0  \u2265 1\u2212\u2206c, (29) x0 \u223c N (x\u03040,\u03a3x0), wt \u223c N (0,\u03a3wt), \u2200t \u2208 T\u2212 (30)\nwhere J\u22c6(s) is the optimal objective value of the CCQSP Planning problem with the schedule fixed to s. Note that the schedule s, which is a decision variable in Problem 2, is treated as a constant in Problem 3. Therefore, the objective function J \u2032 is now a function of only control sequence and mean\nstate, since we have fixed the schedule. Since we assumed that J is a convex function regarding to u0:N\u22121 and x\u03041:N , J \u2032 is also a convex function. Section 5 solves Problem 3."}, {"heading": "3.3 Encoding of a CCQSP Planning Problem with a Convex State Space and Fixed Schedule", "text": "A more restrictive version of a CCQSP planning problem with a fixed schedule and a convex state space, which is solved in the first spiral, is obtained by removing the disjunctions in the chance constraints in Problem 3 as follows:\nProblem 4: CCQSP Planning Problem with a Fixed Schedule and a Convex State Space\nmin u\u03040:N\u22121\nJ \u2032(u0:N\u22121, x\u03041:N ) (31)\nxt+1 = Atxt +Bt\u00b5U(ut) +wt, \u2200t \u2208 T\u2212 (32) ut = u\u0304t +Kt(xt \u2212 x\u0304t), \u2200t \u2208 T\u2212 (33)\u2227 c\u2208C Pr  \u2227 i\u2208Ic(s) hTc,ixti \u2212 gc,i \u2264 0  \u2265 1\u2212\u2206c. (34) x0 \u223c N (x\u03040,\u03a3x0), wt \u223c N (0,\u03a3wt), \u2200t \u2208 T\u2212 (35)\nSection 4 solves Problem 4."}, {"heading": "4. CCQSP Planning with a Convex State Space and a Fixed Schedule", "text": "This section presents the solution methods to Problem 4, which is the CCQSP planning problem with a convex state space and a fixed schedule, as shown in Figure 4-(a). When there are no obstacles in the environment and the execution time steps to achieve time-evolved goals are fixed, the CCQSP planning problem is reduced to a convex chance-constrained finite-horizon optimal control problem.\nIn our past work we presented the risk allocation approach, which conservatively approximates the chance-constrained finite-horizon optimal control problem by a tractable convex optimization problem (Ono & Williams, 2008a, 2008b; Blackmore & Ono, 2009). Although an optimal solution to the approximated convex optimization problem is not an exactly optimal solution to the original convex chance-constrained finite-horizon optimal control problem, its suboptimality is significantly smaller than previous approaches. This section gives a brief overview of the risk allocation approach, as well as the solution to the convex chance-constrained finite-horizon optimal control problem."}, {"heading": "4.1 Deterministic Approximation of Problem 4", "text": "Evaluating whether a joint chance constraint (34) is satisfied requires computing an integral of a multivariate probability distribution over an arbitrary region, since the probability in (34) involves multiple constraints. Such an integral cannot be obtained in a closed form. We address this issue by decomposing the intractable joint chance constraint (34) into a set of individual chance constraints, each of which involves only a univariate probability distribution. The key feature of an individual\nchance constraint is that it can be transformed into an equivalent deterministic constraint that can be evaluated analytically."}, {"heading": "4.1.1 RISK ALLOCATION APPROACH", "text": "The decomposition can be considered as an allocation of risk. Through the decomposition, the risk bound of the joint chance constraint is distributed to the individual chance constraints. There are many feasible risk allocations. The problem is to find a risk allocation that results in the minimum cost. We offer readers an intuitive understanding of the risk allocation approach using the example below.\nRacing Car Example Consider a racing car example, shown in Figure 7. The dynamics of the vehicle have Gaussian-distributed uncertainty. The task is to plan a path that minimizes the time to reach the goal, with the guarantee that the probability of crashing into a wall during the race is less than 0.1% (chance constraint). Planning the control sequence is equivalent to planning the nominal path, which is shown as the solid lines in Figure 7. To limit the probability of crashing into the wall, a good driver would set a safety margin, which is colored in dark gray in Figure 7, and then plan the nominal path outside of the safety margin.\nThe driver wants to set the safety margin as small as possible in order to make the nominal path shorter. However, since the probability of crashing during the race is bounded, there is a certain lower bound on the size of the safety margin. Given this constraint, there are different ways of setting a safety margin; in Figure 7(a) the width of the margin is uniform; in Figure 7(b) the safety margin is narrow around the corner, and wide at the other places.\nAn intelligent driver would take the strategy of (b), since he knows that going closer to the wall at the corner makes the path shorter, while doing so at the straight line does not. A key observation here is that taking a risk (i.e., setting a narrow safety margin) at the corner results in a greater reward (i.e. time saving) than taking the same risk at the straight line. This gives rise to the notion of risk allocation. The good risk allocation strategy is to save risk when the reward is small, while taking it when the reward is great. As is illustrated in this example, the risk allocation must be optimized in order to minimize the objective function of a joint chance-constrained stochastic optimization problem."}, {"heading": "4.1.2 DECOMPOSITION OF CONJUNCTIVE JOINT CHANCE CONSTRAINTS THROUGH RISK ALLOCATION", "text": "We derive the mathematical representation of risk allocation by reformulating each chance constraint over a conjunction of constraints into a conjunction of chance constraints. The reformulation was initially presented by Pre\u0301kopa (1999) and introduced to chance-constrained optimal control by Ono and Williams (2008b). The concept of risk allocation was originally developed by Ono and Williams (2008a). Let Ci be a proposition that is either true or false. Then the following lemma holds:\nLemma 1.\nPr [ N\u2227 i=1 Ci ] \u2265 1\u2212\u2206 \u21d0 \u2203\u03b4i \u2265 0, N\u2227 i=1 Pr [Ci] \u2265 1\u2212 \u03b4i \u2227 N\u2211 i=1 \u03b4i \u2264 \u2206\nProof.\nPr [ N\u2227 i=1 Ci ] \u2265 1\u2212\u2206 \u21d4 Pr [ N\u2228 i=1 Ci ] \u2264 \u2206 (36)\n\u21d0 \u2227 c\u2208C N\u2211 i=1 Pr [ Ci ] \u2264 \u2206 (37)\n\u21d4 \u2203\u03b4i \u2265 0 N\u2227 i=1 Pr [ Ci ] \u2264 \u03b4i \u2227 N\u2211 i=1 \u03b4i \u2264 \u2206\n\u21d4 \u2203\u03b4i \u2265 0 N\u2227 i=1 Pr [Ci] \u2265 1\u2212 \u03b4i \u2227 N\u2211 i=1 \u03b4i \u2264 \u2206. (38)\nThe overline C is the negation of a literal C. We use the following Boole\u2019s inequality to obtain (37) from (36):\nPr [ N\u2228 i=1 Cc,i ] \u2264 N\u2211 i=1 Pr[Cc,i].\nThe following result immediately follows from Lemma 1 by substituting a linear constraint hTc,ixti \u2212 gc,i \u2264 0 for Ci for each chance constraint c.\nCorollary 1. The following set of constraints is a sufficient condition of the joint chance constraint (34) in Problem 4:\n\u2203\u03b4c,i \u2265 0 \u2227 c\u2208C  \u2227 i\u2208Ic(s) Pr [ hTc,ixti \u2212 gc,i \u2264 0 ] \u2265 1\u2212 \u03b4c,i \u2227 \u2211 i\u2208Ic(s) \u03b4c,i \u2264 \u2206c  (39) The newly introduced variables \u03b4c,i represent the upper bounds on the probability of violating each linear state constraint. We refer to them as individual risk bounds. Each individual risk bound,\n\u03b4c,i, can be viewed as the amount of risk allocated to the i\u2019th clause. The fact that \u03b4c,i is a bound on probability implies that 0 \u2264 \u03b4c,i \u2264 1. The second term of (39) requires that the total amount of risk is upper-bounded to the original risk bound\u2206c. Here we find an analogue to the resource allocation problem, where the allocation of a resource is optimized with an upper bound on the total amount of available resource. Likewise, the allocation of risk \u03b4c,i must be optimized in order to minimize the cost. Therefore, we call this decomposition method a risk allocation."}, {"heading": "4.1.3 CONSERVATISM OF RISK ALLOCATION APPROACH", "text": "As mentioned previously, the risk allocation approach gives a conservative approximation of the original chance constraint. This subsection evaluates the level of conservatism of the risk allocation approach.\nLet Pfail be the true probability of failure, defined as the probability that a solution violates the constraints (i.e., the left hand side of (34)). Since (39) is a sufficient but not necessary condition for (34), Pfail is smaller than or equal to the risk bound \u2206 in general: \u2206 \u2265 Pfail. Hence, the conservatism introduced by risk allocation is represented as\n\u2206\u2212 Pfail.\nThe best-case scenario for the risk allocation approach is when the violations of all constraints are mutually exclusive, meaning that a solution that violates one constraint always satisfies all the other constraints. In that case, (39) becomes a necessary and sufficient condition for (34) and hence, risk allocation does not involve any conservatism. Therefore,\n\u2206\u2212 Pfail = 0.\nOn the other hand, the worst-case scenario is when all constraints are equivalent, meaning that a solution that violates one constraint always violates all the other constraints. In such a case,\n\u2206\u2212 Pfail = N \u2212 1 N \u2206,\nwhere N is the number of constraints. Most practical problems lie somewhere between the best-case scenario and the worst-case scenario, but typically closer to the best-case than to the worst-case scenario. For example, if there are two separate obstacles in a path planning problem, collisions with the two obstacles are mutually exclusive events. Collision with an obstacle at one time step does not usually imply collisions at other time steps. A rough approximation of such a real-world situation is to assume that the satisfaction of constraints are probabilistically independent. With such an assumption, the true probability of failure is:\nPfail = \u220f i\u2208Ic Pr [qc,i(u) \u2264 0] \u2264 1\u2212 \u220f i\u2208Ic (1\u2212 \u03b4i),\nwhere Ic is the set of the index of all state constraints. Note that \u03b4i \u2264 \u2206. Therefore, the conservatism introduced by risk allocation is at the second order of \u2206:\n\u2206\u2212 Pfail \u223c O(\u22062).\nFor example, if \u2206 = 1%, the true probability of failure is approximately Pfail \u223c 0.99%. In most practical cases, the users prefer to set very small risk bounds, typically less than 1%. In such cases, the conservatism introduced by risk allocation becomes very small."}, {"heading": "4.1.4 CONVERSION TO DETERMINISTIC CONSTRAINTS", "text": "Each individual chance constraint in (39) only involves a single linear constraint. Furthermore, assuming that there is no actuator saturation, xti has a Gaussian distribution with the covariance matrix given by (9). Hence, hTc,ixti has a univariate Gaussian distribution. The following lemma transforms an individual chance constraint into an equivalent deterministic constraint that involves the mean of state variables, instead of the random state variables:\nLemma 2. The following two conditions are equivalent. Pr [ hTc,ixti \u2212 gc,i \u2264 0 ] \u2265 1\u2212 \u03b4c,i \u21d4 hTc,ix\u0304ti \u2212 gc,i \u2264 \u2212mc,i(\u03b4c,i)\nwhere\nmc,i(\u03b4c,i) = \u2212 \u221a 2hTc,i\u03a3x,tihc,i erf \u22121(2\u03b4c,i \u2212 1). (40)\nNote that erf\u22121 is the inverse of the Gauss error function and \u03a3x,ti is the covariance matrix of xti . This lemma holds because \u2212mc,i(\u00b7) is the inverse of cumulative distribution function of univariate, zero-mean Gaussian distribution with variance hTc,i\u03a3x,tihc,i."}, {"heading": "4.1.5 RISK ALLOCATION APPROACH FOR THE CLOSED-LOOP CONTROL POLICY", "text": "When a close-loop control policy is employed (i.e., Kt \u0338= 0 in (6)), there is a risk of actuator saturation. Since the nonlinearity of the function \u00b5U(\u00b7) in (5) makes the probability distribution of xti non-Gaussian, mc,i(\u00b7) cannot be obtained by (40). Although it is theoretically possible to derive mc,i(\u00b7) for non-Gaussian distributions, it is very difficult in our case since the inverse of the cumulative distribution function of xti cannot be obtained in a closed-form.\nOur solution to this issue is summarized in Lemma 3 below, which allows us to assume that xti is Gaussian-distributed and hence to use (40), even if there is a possibility of actuator saturation. This approach is enabled by imposing additional chance constraints that bound the risk of actuator saturation as follows:\nPr [ut \u2208 U] \u2265 1\u2212 \u03f5t, \u2200t \u2208 T\u2212, (41)\nwhere \u03f5t is the bound on the risk of actuator saturation at time step t. Using the method presented in Section 3.1.2, we approximate U by a polytope as follows:\nut \u2208 U \u21d0\u21d2 \u2227 i\u2208IU hU,iut \u2212 gU,i \u2264 0\nAssuming that xti is Gaussian-distributed, we use Lemma 2 to transform (41) into deterministic constraints on nominal control inputs as follows:\u2227\ni\u2208IU hU,iu\u0304t \u2212 gU,i \u2264 \u2212mU,t,i(\u03f5t,i) \u2227 \u2211 i\u2208IU \u03f5t,i \u2264 \u03f5t, \u2200t \u2208 T\u2212, (42)\nwhere\nmU,t,i(\u03f5c,i) = \u2212 \u221a 2hTU,i\u03a3x,thU,i erf \u22121(2\u03f5c,i \u2212 1). (43)\nThe following lemma holds:\nLemma 3. The following set of constraints is a sufficient condition of the joint chance constraint (34) in Problem 4:\n\u2203\u03b4c,i \u2265 0, \u03f5t \u2265 0 \u2227 c\u2208C  \u2227 i\u2208Ic(s) hTc,ix\u0304ti \u2212 gc,i \u2264 \u2212mc,i(\u03b4c,i)\n\u2227 \u2211\ni\u2208Ic(s)\n\u03b4c,i + Tmaxc\u2211 t=0 \u2211 i\u2208IU \u03f5t,i \u2264 \u2206c  \u2227\n\u2227 t\u2208T\u2212 \u2227 i\u2208IU hU,iu\u0304t \u2212 gU,i \u2264 \u2212mU,t,i(\u03f5t,i), (44)\n(45)\nwhere mc,i(\u00b7) and mU,t,i are given by (40) and (43). Tmaxc is the last time step that the episodes associated with the chance constraint c are executed, given the schedule s:\nTmaxc = max a\u2208\u03a8c s(eEa ).\nIntuitively, the constraint (44) requires that, with probability 1 \u2212 \u2206c, the episode constraints are satisfied and the actuators do not saturate until all episodes associated with c are executed.\nProof. We consider two plants: M = \u27e8A0:N\u22121,B0:N\u22121,\u03a3w0:N\u22121 ,U\u27e9 and M \u2032 = \u27e8A0:N\u22121,B0:N\u22121,\u03a3w0:N\u22121 ,Rnu\u27e9, where U \u2282 Rnu is a compact convex set (see Definition 6). The difference between the two plants is thatM has a possibility of actuator saturation, whileM \u2032 does not. As a result, while the probability distribution of the state variables ofM is non-Gaussian, that of M \u2032 is Gaussian. Note that M and M \u2032 result in different probability distributions of xti and ut. In order to explicitly show which plant model is considered, we use notations such as xMti and uM\n\u2032 t in this proof. We first consider M \u2032. It follows from Lemmas 1 and 2 that:\n(44) =\u21d2 \u2227 c\u2208C Pr  \u2227\ni\u2208Ic(s)\nhTc,ix M \u2032 ti \u2212 gc,i \u2264 0\n \u2227 Tmaxc\u2227\nt=0\nuM \u2032\nt \u2208 U  \u2265 1\u2212\u2206c  .\nLet w0:N\u22121 := [w0 \u00b7 \u00b7 \u00b7wN\u22121]. We define a feasible disturbance set, Wc(v0:N\u22121, s) \u2282 RNnx , as follows:\nWc(v0:N\u22121, s) := w0:N\u22121 \u2208 RNnx \u2223\u2223\u2223\u2223\u2223  \u2227\ni\u2208Ic(s)\nhTc,ix M \u2032 ti \u2212 gc,i \u2264 0\n \u2227 Tmaxc\u2227\nt=0\nuM \u2032\nt \u2208 U  . (46)\nThen, by definition,\nPr  \u2227 i\u2208Ic(s) hTc,ix M \u2032 ti \u2212 gc,i \u2264 0  \u2227 Tmaxc\u2227 t=0 uM \u2032 t \u2208 U  = Pr [w0:N\u22121 \u2208 Wc(v0:N\u22121, s)] .\nNext we considerM . Note thatM andM \u2032 are identical as long as there is no actuator saturations (i.e., uMt \u2208 U). Therefore, for a given w0:N\u22121 \u2208 Wc(v0:N\u22121, s), it follows from (46) that xMt = xM \u2032 t and u M t = u M \u2032 t . Hence,\nw0:N\u22121 \u2208 Wc(v0:N\u22121, s) =\u21d2  \u2227 i\u2208Ic(s) hTc,ix M ti \u2212 gc,i \u2264 0  \u2227 Tmaxc\u2227 t=0 uMt \u2208 U  . Accordingly, for a given c \u2208 C,\nPr  \u2227 i\u2208Ic(s) hTc,ix M ti \u2212 gc,i \u2264 0  \u2265 Pr\n \u2227 i\u2208Ic(s) hTc,ix M ti \u2212 gc,i \u2264 0  \u2227 Tmaxc\u2227 t=0 uMt \u2208 U  \u2265 Pr [w0:N\u22121 \u2208 Wc(v0:N\u22121, s)]\n= Pr  \u2227 i\u2208Ic(s) hTc,ix M \u2032 ti \u2212 gc,i \u2264 0  \u2227 Tmaxc\u2227 t=0 uM \u2032 t \u2208 U  \u2265 1\u2212\u2206c.\nThis completes the proof of Lemma 3\nWe note that Lemma 3 is a probabilistic extension of the closed-loop robust model predictive control (RMPC) methods proposed by Acikmese, Carson III, and Bayard (2011) and Richards and How (2006). These methods avoid the risk of actuator saturation by imposing tightened control constraints on u\u0304t. Since we consider stochastic uncertainty, we replace the constraint tightening by chance constraints."}, {"heading": "4.2 Convex Programming Solution to Problem 4", "text": "Using Lemma 3, we replace the stochastic optimization problem, Problem 4, with the deterministic convex optimization problem:\nProblem 5: Deterministic Approximation of Problem 4\nmin u\u03041:N ,\u03b4c,i\u22650,\u03f5t,i\u22650\nJ \u2032(u1:N , x\u03041:N ) (47)\ns.t. \u2200t \u2208 T\u2212, x\u0304t+1 = Atx\u0304t +Btut (48)\u2227 c\u2208C \u2227 i\u2208Ic(s)\nhTc,ix\u0304ti \u2212 gc,i \u2264 \u2212mc,i(\u03b4c,i) (49)\u2227 t\u2208T\u2212 \u2227 i\u2208IU hU,iu\u0304t \u2212 gU,i \u2264 \u2212mU,t,i(\u03f5t,i) (50)\n\u2227 c\u2208C \u2211 i\u2208Ic(s) \u03b4c,i + Tmaxc\u2211 t=0 \u2211 i\u2208IU \u03f5t,i \u2264 \u2206c. (51)\nIt follows immediately from Corollaries 1 and 2 that a feasible solution to Problem 5 is always a feasible solution to Problem 4. Furthermore, Blackmore and Ono (2009) showed that an optimal solution to Problem 5 is a near-optimal solution to Problem 4. The following lemma guarantees the tractability of Problem 5.\nLemma 4. Problem 5 is a convex optimization problem.\nProof. The inverse error function erf\u22121(x) is concave for x. Since we assume in Section 2.4.3 that \u2206c \u2264 0.5, the feasible ranges of \u03b4 and \u03f5 are upperbounded by 0.5. Since the safety margin function mc,i(\u03b4c,i) and mU,t,i(\u03f5t,i) are convex for 0 < \u03b4c,i \u2264 0.5 and 0 < \u03f5t,i \u2264 0.5, the constraints (49) and (50) are convex within the feasible region. All other constraints are also convex since they are linear. Finally, the objective function is convex by assumption (Section 2.4.4). Therefore, Problem 5 is a convex optimization problem.\nSince Problem 5 is a convex optimization problem, it can be solved by an interior point method optimally and efficiently. This completes our first spiral, planning for CCQSPs with a fixed schedule and convex constraints. In the next section we present a solution method for a non-convex problem through a branch-and-bound algorithm, whose subproblems are convex problems."}, {"heading": "5. CCQSP Planning with a Non-convex State Space", "text": "Next, we consider the second spiral, comprised of Problem 3 in Section 3.2, a variant of the CCQSP planning problem that involves a fixed schedule and non-convex constraints, such as obstacles, as shown in Figure 4-(b). Once again, this is encoded as a chance-constrained optimization problem, but the addition of the obstacle avoidance constraints requires disjunctive state constraints. Hence, the problem results in a non-convex, chance-constrained optimization. This section introduces a novel algorithm, called Non-convex Iterative Risk Allocation (NIRA), that optimally solves a deterministic approximation of Problem 3.\nThe solution to a CCQSP planning problem with a non-convex state space is two-fold. In the first step, described in Section 5.1, we obtain a deterministic approximation of Problem 3. In order to handle disjunctive chance constraints, we develop an additional decomposition approach called risk selection, which reformulates each chance constraint over a disjunction of constraints into a disjunction of individual chance constraints. Once the chance constraints in (29) are decomposed into a set of individual chance constraints through risk allocation and risk selection, the same technique as in Section 4.1.4 is used to obtain equivalent deterministic constraints. As a result, we obtain a disjunctive convex programming problem (Problem 6 in Section 5.1.3).\nThe deterministic disjunctive convex programming problem is solved in the second step, described in Sections 5.2-5.4. We introduce the NIRA algorithm (Algorithm 1) that significantly reduces the computation time without making any compromise in the optimality of the solution. The reduction in computation time is enabled by our new bounding approach, Fixed Risk Relaxation (FRR). FRR relaxes nonlinear constraints in the subproblems of the branch-and-bound algorithm with linear constraints. In many cases, FRR of the nonlinear subproblems is formulated as a linear programming (LP) or approximated by an LP. NIRA obtains a strictly optimal solution of Problem 6 by solving the subproblems exactly without FRR at unpruned leaf nodes of the search tree, while other subproblems are solved approximately with FRR in order to reduce the computation time."}, {"heading": "5.1 Deterministic Approximation", "text": "As in Section 4, we first obtain a deterministic approximation of Problem 3."}, {"heading": "5.1.1 RISK SELECTION APPROACH", "text": "The deterministic approximation is obtained by decomposing the non-convex joint chance constraint (29) into a set of individual chance constraints, through risk allocation and risk selection. We revisit the race car example to explain the concept of risk selection intuitively.\nRacing Car Example We consider the example shown in Figure 8, where a vehicle with uncertain dynamics plans a path that minimizes the time to reach the goal. The vehicle is allowed to choose one of the two routes shown in Figure 8. We impose a chance constraint that limits the probability of crashing into a wall during the mission to 0.1%.\nThe satisfaction of the chance constraint can be guaranteed by the following process. First, for each of the routes, we find a safety margin that limits the probability of crash throughout the route to 0.1% from the start to the goal. Then, we let the vehicle plan a nominal path that operates within the safety margins. Since both routes have a 0.1% safety margin, the chance constraint is satisfied no matter which route the vehicle chooses. Therefore, the vehicle can optimize the path by choosing the route that results in a smaller cost. The optimization process can be considered as a selection of risk; the vehicle is given two options as in Figure 8, routes (a) and (b), both of which involve the same level of risk; then the vehicle selects the one that results in less cost. Hence, we name this decomposition approach as the risk selection."}, {"heading": "5.1.2 DECOMPOSITION OF CONJUNCTIVE JOINT CHANCE CONSTRAINT THROUGH RISK SELECTION", "text": "In this subsection, we derive the mathematical representation of risk selection. Let Ci be a proposition that is either true or false. Then the following lemma holds:\nLemma 5.\nPr [ N\u2228 i=1 Ci ] \u2265 1\u2212\u2206 \u21d0 N\u2228 i=1 Pr [Ci] \u2265 1\u2212\u2206\nProof. The following inequality always holds:\n\u2200i Pr [ N\u2228 i=1 Ci ] \u2265 Pr [Ci] . (52)\nHence,\nPr [ N\u2228 i=1 Ci ] \u2265 1\u2212\u2206 \u21d0 \u2203i Pr [Ci] \u2265 1\u2212\u2206 \u21d4 N\u2228 i=1 Pr [Ci] \u2265 1\u2212\u2206. (53)\nThe following corollary follows immediately from Lemmas 3 and 5.\nCorollary 2. The following set of constraints is a sufficient condition of the disjunctive joint chance constraint (29) in Problem 3:\n\u2203\u03b4c,i \u2265 0, \u03f5t \u2265 0 \u2227 c\u2208C  \u2227 i\u2208Ic(s) \u2228 j\u2208Jc,i hTc,i,jxti \u2212 gc,i,j \u2264 mc,i(\u03b4c,i)\n\u2227 \u2211\ni\u2208Ic(s)\n\u03b4c,i + Tmaxc\u2211 t=0 \u2211 i\u2208IU \u03f5t,i \u2264 \u2206c  5 \u2227\n\u2227 t\u2208T\u2212 \u2227 i\u2208IU hU,iu\u0304t \u2212 gU,i \u2264 \u2212mU,t,i(\u03f5t,i). (54)\nNote that the resulting set of constraints (54) is a sufficient condition for the original chance constraint (29). Therefore, any solution that satisfies (54) is guaranteed to satisfy (29). Furthermore, although (54) is a conservative approximation of (29), the conservatism introduced by risk selection is generally small in many practical applications. This claim is empirically validated in Section 7.2.3."}, {"heading": "5.1.3 DETERMINISTIC APPROXIMATION OF PROBLEM 3", "text": "Using Corollary 2, the non-convex fixed-schedule CCQSP planning problem (Problem 3) is approximated by the following deterministic convex optimization problem. For later convenience, we label each part of the optimization problem as O (objective function), M (plant model), C (chance constraints on states), D (chance constraints on control inputs), and R (risk allocation constraint).\nProblem 6: Deterministic Approximation of Problem 3\nmin u\u03041:N ,\u03b4c,i\u22650,\u03f5t,i\u22650\n(O :) J \u2032(u1:N , x\u03041:N ) (55)\ns.t. (M :) \u2200t \u2208 T\u2212, x\u0304t+1 = Atx\u0304t +Btut (56) (C :) \u2227 c\u2208C \u2227 i\u2208Ic(s) \u2228 j\u2208Jc,i hTc,i,jx\u0304ti \u2212 gc,i,j \u2264 \u2212mc,i,j(\u03b4c,i) (57)\n(D :) \u2227\nt\u2208T\u2212 \u2227 i\u2208IU hU,iu\u0304t \u2212 gU,i \u2264 \u2212mU,t,i(\u03f5t,i) (58)\n(R :) \u2227 c\u2208C \u2211 i\u2208Ic(s) \u03b4c,i + Tmaxc\u2211 t=0 \u2211 i\u2208IU \u03f5t,i \u2264 \u2206c. (59)\nIt follows immediately from Corollary 2 that an optimal solution to Problem 6 is guaranteed to be a feasible solution to the original problem with regard to satisfying the chance constraints (Problem 3). Furthermore, we empirically demonstrate in Section 7.2.3 that it is a near-optimal solution to Problem 3 in our applications."}, {"heading": "5.2 NIRA: Branch and Bound-Based Solution to Problem 6", "text": "We next present the Non-convex Iterative Risk Allocation (NIRA) algorithm. Recall that NIRA optimally solves Problem 6 by a branch-and-bound algorithm. The standard branch-and-bound solution to problems involving disjunctive nonlinear constraints, such as those in Problem 6, is to use a bounding approach in which the nonlinear convex relaxed subproblems are constructed by removing all non-convex constraints below the corresponding disjunction. This approach was used by Balas (1979) and Li andWilliams (2005) for a different problem known as disjunctive linear programming, whose subproblems are LPs instead of convex programmings. However, although the standard branch-and-bound algorithm is guaranteed to find a globally optimal solution to Problem 6, its computation time is slow because the algorithm needs to solve numerous nonlinear subproblems in order to compute relaxed bounds.\nOur new bounding approach, Fixed Risk Relaxation (FRR), addresses this issue by computing lower bounds more efficiently. We observe that the relaxed subproblems are nonlinear convex optimization problems. FRR relaxes the nonlinear constraints to linear constraints. Particularly, when the objective function is linear, an FRR of a subproblem (Problem 8) is an LP, which can be very efficiently solved. The optimal objective value of an FRR of a subproblem is a lower bound of the optimal objective value of the original subproblem.\nNIRA solves the FRRs of the subproblems in order to efficiently obtain the lower bounds, while solving the original subproblems exactlywithout relaxation at unpruned leaf nodes in order to obtain an exact optimal solution. As a result, NIRA achieves significant reduction in computation time, without any loss in optimality."}, {"heading": "5.2.1 THE NIRA ALGORITHM OVERVIEW", "text": "Algorithm 1 shows the pseudocode of the NIRA algorithm. Its input is the deterministic approximation of a non-convex chance-constrained optimal control problem (Problem 6), which is a five-tuple\nAlgorithm 1 Non-convex Iterative Risk Allocation (NIRA) algorithm function NIRA(problem) returns optimal solution to Problem 6 1: Set up queue as a FILO queue 2: Incumbent \u2190 \u221e 3: rootSubproblem \u2190 obtainRootSubproblem(problem) 4: queue \u2190 rootSubproblem 5: while queue is not empty do 6: subproblem \u2190 the last entry in queue 7: Remove subproblem from queue 8: lb \u2190 obtainLowerBound(subproblem) 9: if lb \u2264 Incumbent then 10: if c = |C| \u2227 i = |Ic(s)| then 11: (J, U\u0304) \u2190 Solve(subproblem) 12: if J\u22c6 < Incumbent then 13: Incumbent \u2190 J , U\u0304\u22c6 \u2190 U\u0304 //Update the optimal solution 14: end if 15: else 16: i \u2190 i+ 1 17: if i > |Ic(s)| then 18: c \u2190 c+ 1, i \u2190 1 19: end if 20: for j \u2208 Jc,i do 21: newSubproblems \u2190 Expand(subproblem,problem,c,i,j) 22: Add newSubproblems to queue 23: end for 24: end if 25: end if 26: end while 27: return U\u0304\u22c6\n\u27e8O,M,C,D,R\u27e9, as well as a fixed schedule s. Its output is an optimal nominal control sequence U\u0304\n\u22c6 := [u\u0304\u22c60 \u00b7 \u00b7 \u00b7 u\u0304\u22c6N\u22121].\nEach node of the branch-and-bound search tree corresponds to a subproblem that is a convex chance-constrained optimization problem (Problem 5). We use a FILO queue to store subproblems so that the search is conducted in a depth-first manner (Line 1). At each node, the corresponding subproblem is solved to obtain a lower bound of the objective value of all subsequent subproblems (Line 8). The details of the bounding approaches are explained in Subsection 5.4. If the lower bound is larger than the incumbent, the algorithm prunes the branch. Otherwise, the branch is expanded (Line 21). If a branch is expanded to the leaf without being pruned, subproblems are solved exactly (Line 11). Subsection 5.3 explains our expansion procedure in detail. The NIRA algorithm always results in a globally optimal solution to Problem 6, since the solution U\u0304\u22c6 is obtained by solving the subproblems at leaf nodes exactly. The next two subsections introduces the branching and bounding methods."}, {"heading": "5.3 Branching", "text": "This subsection explains how NIRA constructs the root subproblem (Line 3 of Algorithm 1), as well as how it expands the nodes (Line 21 of Algorithm 1). The root subproblem is a convex optimal CCQSP planning problem without any chance constraints. When a node is expanded, the subproblems of its children nodes are constructed by adding one constraint in a disjunction to the subproblem of the parent node. In order to simplify notations, we let Cc,i,j represent each individual chance constraint (57) in Problem 6:\nCc,i,j := { True (if hTc,i,j \u2212 gc,i,jx\u0304ti \u2264 \u2212mc,i,j(\u03b4c,i)) False (otherwise)."}, {"heading": "5.3.1 WALK-THROUGH EXAMPLE", "text": "We first present a walk-through example to intuitively explain the branching procedure. The example is an instance of Problem 6, which involves four individual chance constraints:\u2227\ni\u2208{1,2} \u2228 j\u2208{1,2} hT1,i,jx\u0304ti \u2212 g1,i,j \u2264 \u2212m1,i,j(\u03b41,i) (60)\nUsing this notation defined above, the set of individual chance constraints (57) is represented as follows:\n(C1,1,1 \u2228 C1,1,2) \u2227 (C1,2,1 \u2228 C1,2,2) (61)\nFigure 9-(a) shows a tree obtained by dividing the original problem into subproblems sequentially. The subproblems corresponding to the tree\u2019s four leaf nodes (Nodes 4-7 in Figure 9-(a)) exhaust all conjunctive (i.e., convex) combinations among the chance constraints (61). On the other hand, the subproblems corresponding to the three branch nodes (Nodes 1-3 in Figure 9-(a)) involve disjunctive (i.e., nonconvex) clauses of chance constraints. We relax such non-convex subproblems to convex subproblems by removing all clauses that contain disjunctions in order to obtain the search tree shown in Figure 9-(b).\nThe non-convex problem (Problem 6) can be optimally solved by repeatedly solving the relaxed convex subproblems using the algorithms presented in Section 4. The following subsections introduce the algorithms that construct a search tree with relaxed convex subproblems, such as the one in Figure 9-(b)."}, {"heading": "5.3.2 RELAXED CONVEX SUBPROBLEM", "text": "The formulation of the relaxed convex subproblems is given in Problem 7. We represent the index j as j(c, i) since the convex relaxation chooses only one disjunct for each disjunction specified by (c, i). Let Ic be a set of indices for i. We denote by J\u22c6SP the optimal objective value of the relaxed subproblem.\nProblem 7: Convex Relaxed Subproblem of NIRA\nJ\u22c6SP = minu\u03041:N ,\u03b4c,i\u22650,\u03f5t,i\u22650 (O :) J \u2032(u1:N , x\u03041:N )\ns.t. (M :) \u2200t \u2208 T\u2212, x\u0304t+1 = Atx\u0304t +Btut (C :) \u2227 c\u2208C \u2227 i\u2208Ic hTc,i,j(c,i)x\u0304ti \u2212 gc,i,j(c,i) \u2264 \u2212mc,i,j(c,i)(\u03b4c,i) (62)\n(D :) \u2227\nt\u2208T\u2212 \u2227 i\u2208IU hU,iu\u0304t \u2212 gU,i \u2264 \u2212mU,t,i(\u03f5t,i) (63)\n(R :) \u2227 c\u2208C \u2211 i\u2208Ic \u03b4c,i + Tmaxc\u2211 t=0 \u2211 i\u2208IU \u03f5t,i \u2264 \u2206c. (64)\nNote that Problem 7 is identical to Problem 5. Hence, the algorithms introduced in Section 4 can be used to solve the relaxed subproblems."}, {"heading": "5.3.3 CONSTRUCTION OF ROOT SUBPROBLEM", "text": "The root subproblem is a special case of Problem 7 above with Ic being an empty set for all c \u2208 C. The function presented in Algorithm 2 is used in Line 3 of the NIRA algorithm (Algorithm 1) to construct the root subproblem of the branch-and-bound tree. Note that, in Algorithm 2, we use an object-oriented notation, such as subproblem.O, to represent the objective function O of subproblem. The resulting root subproblem is as follows:"}, {"heading": "5.3.4 EXPANSION OF SUBPROBLEMS", "text": "In order to create a child subproblem of a subproblem, the function described in Algorithm 3 is used in Line 21 of the NIRA algorithm (Algorithm 1). It adds the individual chance constraint specified by the indices (c, i, j) as a conjunct. Note that the resulting child subproblem is still a convex optimization, because the individual chance constraint is added conjunctively. The NIRA algorithm (Algorithm 1) enumerates children nodes for all disjuncts in Jc,i (Lines 20-23).\nAlgorithm 2 Construction of the root subproblem of NIRA function obtainRootSubproblem(problem) returns root subproblem 1: rootSubproblem.O \u2190 problem.O 2: rootSubproblem.M \u2190 problem.M 3: rootSubproblem.D \u2190 problem.D 4: for c \u2208 C do 5: rootSubproblem.Ic \u2190 \u03d5 6: rootSubproblem.Rc.lhs \u2190 \u2211Tmaxc t=0 \u2211 i\u2208IU \u03f5t,i\n7: rootSubproblem.Rc.rhs \u2190 problem.Rc.rhs 8: end for 9: return rootSubproblem\nAlgorithm 3 Expansion of a subproblem of NIRA function Expand(subproblem, problem, c, i, j) returns a child subproblem 1: subproblem.Ic \u2190 subproblem.Ic \u222a i 2: subproblem.Rc.lhs \u2190 subproblem.Rc.lhs+ \u03b4c,i 3: return subproblem"}, {"heading": "5.4 Bounding", "text": "In this subsection, we present two implementations of the obtainLowerBound function in Line 8 of Algorithm 1. The first one uses the optimal solution of the convex subproblems (Problem 7) as lower bounds. This approach typically results in extensive computation time. The second one solves an LP relaxation of the convex subproblems, called fixed risk relaxation (FRR). FRR dramatically reduces the computation time compared to the first implementation. The NIRA algorithm employs the second implementation."}, {"heading": "5.4.1 SIMPLE BOUNDING", "text": "Algorithm 4 shows the most straightforward way to obtain lower bounds. It simply solves the convex relaxed subproblems (Problem 7) using the methods presented in Section 4.2. The optimal objective value of a relaxed subproblem gives a lower bound of the optimal objective value of all the subproblems below it. For example, the optimal solution of the relaxed subproblem at Node 2\u2032 in Figure 9-(b) gives a lower bound of the objective value of the subproblems at Nodes 4 and 5. This is because the constraints of the relaxed subproblems are always a subset of the constraints of the subproblems below. Note that optimization problems are formulated as minimizations.\nHowever, despite the simplicity of this approach, its computation time is slow because the algorithm needs to solve a myriad of subproblems. For example, a simple path planning problem with\nAlgorithm 4 A simple implementation of the obtainLowerBound function in Line 8 of Algorithm 1 function obtainLowerBound-Naive(subproblem) returns a lower bound 1: Solve subproblem using algorithms presented in Section 4.2 2: return the optimal objective value\nten time steps and one rectangular obstacle requires the solution of 410 = 1, 048, 576 in the worst case, although the branch-and-bound process often significantly reduces the number of subproblems to be solved. Moreover, the subproblems (Problem 7) are nonlinear convex optimization problems due to the nonlinearity ofmc,i,j andmU,t,i in (62) and (63). A general nonlinear optimization problem requires significantly more solution time than more specific classes of optimization problems, such as linear programmings (LPs) and quadratic programmings (QPs)."}, {"heading": "5.4.2 FIXED RISK RELAXATION", "text": "Our new relaxation approach, fixed risk relaxation (FRR), addresses this issue. FRR linearizes the nonlinear constraints (62) and (63) in Problem 7 by fixing all the individual risk allocations, \u03b4c,i and \u03f5t,i, to their upper bound \u2206. When the objective function is linear, an FRR is an LP. An FRR with a convex piecewise linear objective function can also be reformulated as an LP by introducing slack variables (See Section 7.1.1 for an example.). A general convex objective function can be approximated by a convex piecewise linear function. Hence, in many cases, the FRRs of subproblems result in LPs, which can be solved very efficiently. The fixed risk relaxation of Problem 7 is as follows:\nProblem 8: Fixed Risk Relaxation of Problem 7\nJ\u22c6FRR = minu\u03041:N J \u2032(u1:N , x\u03041:N )\ns.t. \u2200t \u2208 T\u2212, x\u0304t+1 = Atx\u0304t +Btut\u2227 c\u2208C \u2227 i\u2208Ic(s)\nhTc,ix\u0304ti \u2212 gc,i \u2264 \u2212mc,i,j(c,i)(\u2206c) (65)\u2227 t\u2208T\u2212 \u2227 i\u2208IU hU,iu\u0304t \u2212 gU,i \u2264 \u2212mU,t,i(\u2206c) (66)\nNote that the nonlinear terms in (62) and (63), mc,i,j and mU,t,i, become constant by fixing \u03b4c,i and \u03f5t,i to\u2206c, which is a constant. The optimal objective value of the FRR provides a tightest lower bound among the linear relaxations of constraints (62) and (63). The following lemmas hold:\nLemma 6. Problem 8 gives a lower bound to the optimal objective value of Problem 7:\nJ\u22c6FRR \u2264 J\u22c6SP\nProof. mc,i,j(\u00b7) and mU,t,i(\u00b7) are monotonically decreasing functions. Since \u03b4c,i \u2264 \u2206c and \u03f5t,i \u2264 \u2206c, all individual chance constraints (65) and (66) of the Fixed Risk Relaxation are less stricter than the first conjunct of (62) and (63). Therefore, the cost of the optimal solution of the Fixed Risk Relaxation is less than or equal to the original subproblem.\nLemma 7. FRR gives the tightest lower bound among the linear relaxations of constraints (62) and (63).\nProof. The linear relaxation of (62) and (63) becomes tighter by fixing \u03b4c,i and \u03f5t,i to a lesser value. However, setting \u03b4c,i and \u03f5t,i to values less than \u2206c may exclude feasible solutions, such as the one\nAlgorithm 5 An FRR implementation of the obtainLowerBound function in Line 8 of Algorithm 1 function obtainLowreBound-FRR(subproblem) returns lower bound 1: for \u2200(c, i, j) in subproblem.C do 2: subproblem.Cc,i,j .rhs \u2190 \u2212mc,i,j(\u2206c) //Apply fixed risk relaxation 3: end for 4: for \u2200(t, i) do 5: subproblem.Dt,i.rhs \u2190 \u2212mU,t,i //Apply fixed risk relaxation 6: end for 7: Remove subproblem.R 8: Solve subproblem using an LP solver 9: return the optimal objective value\nthat sets \u03b4c,i = \u2206c for some (c, i). Hence, FRR is the tightest linear relaxation of (62) and (63), resulting in the tightest lower bound.\nNote that the optimal solution of Fixed Risk Relaxation (Problem 8) is typically an infeasible solution to Problem 7, since setting \u03b4c,i = \u03f5t,i = \u2206c violates the constraint (64).\nAlgorithm 5 implements the fixed risk relaxation. The LP relaxation is solved by an LP solver, and its optimal objective value is returned.\nThis completes our second spiral, planning for CCQSPs with a fixed schedule and nonconvex constraints. In the next section, we turn to our final spiral, which involves flexible temporal constraints."}, {"heading": "6. CCQSP Planning with a Flexible Schedule", "text": "This section presents the complete p-Sulu Planner, which efficiently solves the general CCQSP planning problem with a flexible schedule and a non-convex state space (Problem 2 in Section 3.1.2). The problem is to find a schedule of events s that satisfies simple temporal constraints, as well as a nominal control sequence u\u03040:N\u22121 that satisfies the chance constraints and minimizes cost. Our approach is to first generate a feasible schedule and then to extend it to a control sequence for that schedule, while iteratively improving the candidate schedules using branch-and-bound.\nWe build the p-Sulu Planner upon the NIRA algorithm presented in the previous section. Recall that NIRA optimizes the nominal control sequence u\u03040:N\u22121 given a fixed schedule s. The p-Sulu Planner uses NIRA as a subroutine that takes a schedule s as an input, and outputs the optimal objective value as well as an executable control sequence. We denote the optimal objective value for a given schedule s as J\u22c6(s). Using this notation, the CCQSP planning problem with a flexible schedule (Problem 2) can be rewritten as a schedule optimization problem as follows:\nmin s\u2208SF\nJ\u22c6(s). (67)\nRecall that the domain of feasible schedules SF (Definition 11) is a finite set, since we consider a discretized, finite set of time steps T (see Section 2.1). Hence, the schedule optimization problem (67) is a combinatorial constraint optimization problem, where the constraints are given in the form of simple temporal constraints.\nAlgorithm 6 The the p-Sulu Planner function pSulu(ccqsp) returns optimal schedule and control sequence 1: Incumbent = \u221e 2: Set up queue as a FILO queue 3: E\u03c30 = {e0}, \u03c30(e0) = 0 //initialize the partial schedule 4: queue \u2190 \u27e8E\u03c30 , \u03c30\u27e9 5: while queue is not empty do 6: \u27e8E\u03c3, \u03c3\u27e9 \u2190 the last entry in queue 7: Remove \u27e8E\u03c3, \u03c3\u27e9 from queue 8: [J\u22c6,u0:N\u22121] \u2190 obtainLowerBound(ccqsp, E\u03c3, \u03c3) 9: if J\u22c6 < Incumbent then 10: if E\u03c3 = E then 11: Incumbent \u2190 J\u22c6, OptCtlSequence \u2190 u0:N\u22121, OptSchedule \u2190 \u03c3 12: else 13: expand(ccqsp, queue, e, E\u03c3, \u03c3) 14: end if 15: end if 16: end while 17: return OptCtlSequence,OptSchedule"}, {"heading": "6.1 Algorithm Overview", "text": "Our solution approach is again to use a branch-and-bound algorithm. In the branch-and-bound search, the p-Sulu Planner incrementally assigns an execution time step to each event in order to find the schedule that minimizes J\u22c6(s) in (67). The objective function is evaluated by solving the fixed schedule CCQSP planning problem using the NIRA algorithm. Although the combination of the two branch-and-bound searches in the p-Sulu Planner and NIRA are equivalent to one unified branch-and-bound search in practice, we treat them separately for ease of explanation.\nAs shown in Figure 12, the branch-and-bound algorithm searches for an optimal schedule by incrementally assigning execution time steps to each event in a depth-first manner. Each node of the search tree corresponds to a partial schedule (Definition 2), which assigns execution time steps to a subset of the events included in the CCQSP. The partial schedule at the root node only involves an assignment to the start node e0. The tree is expanded by assigning an execution time step to one new event at a time. For example, the node \u03c3(e1) = 2 in Figure 12-(a) represents a partial schedule that assigns the execution time step t = 0 to the event e0 and t = 2 to e1, while leaving eE unassigned.\nThe the p-Sulu Planner obtains the lower bound of the objective function value J\u22c6(s) by solving a CCQSP planning problem with a partial schedule that can be extended to s. The the p-Sulu Planner minimizes the search space by dynamically pruning the domain through forward checking. More specifically, after an execution time is assigned to an event at each iteration of the branch-andbound search, the the p-Sulu Planner runs a shortest-path algorithm to tighten the real-valued upper and lower bounds of the execution time step of unassigned events according to the newly assigned execution time step.\nAlgorithm 6 shows the pseudocode of the algorithm. At each node of the search tree, a fixedschedule CCQSP planning problem is solved with the given partial schedule. If the node is at the\nleaf of the tree and the optimal objective value is less than the incumbent, the optimal solution is updated (Line 11). If the node is not at the leaf, the optimal objective value of the corresponding subproblem is a lower bound for the optimal objective value of subsequent nodes. If the lower bound is less than the incumbent, the node is expanded by enumerating the feasible execution time assignments to an unassigned event (Line 13). Otherwise, the node is not expanded, and hence pruned. Details of this branch-and-bound process are described in later subsections.\ne (\u03c3) ] = [0.8, 3.9] from Figure 11-(b); (b) the node \u03c3(e1) = 2 is\nexpanded; DeE (\u03c3) = 4, 5 given \u03c3(e0) = 0 and \u03c3(e1) = 2, since\n[\ndmaxe (\u03c3), d min e (\u03c3)\n]\n= [3.6, 5.5] from Figure 11-(c).\nWalk-through example We present a walk-through example to give readers insight into the solution process. We consider a CCQSP shown in Figure 10-(a). The CCQSP specifies a mission to go through a waypoint A and get to the goal region B while avoiding the obstacle C, as shown in Figure 10-(b). We assume that the time interval is \u2206T = 1.0.\nFigures 11 and 12 illustrate the solution process. The the p-Sulu Planner algorithm is initialized by assigning the execution time 0 to the start event e0. Figure 11-(a) is the distance graph representation of the simple temporal constraints (Dechter, 2003) of the CCQSP. Note that a simple chance constraint is equivalently represented as a pair of inequality constraints as follows:\ns(e)\u2212 s(e\u2032) \u2208 [l, u] \u21d0\u21d2 s(e)\u2212 s(e\u2032) \u2264 u \u2227 s(e\u2032)\u2212 s(e) \u2264 \u2212l.\nThe two inequality constraints are represented by two directional edges between each two nodes in the distance graph. The the p-Sulu Planner runs an all-pair shortest-path algorithm on the distance graph to obtain the d-graph shown in Figure 11-(b). A d-graph is a completed distance graph where each edge is labeled by the shortest-path length. The d-graph represents the tightest temporal constraints. Then the algorithm enumerates the feasible execution-time assignments for the event e1 using the d-graph. According to the d-graph, the execution time for the event e1 must be between 0.8 and 3.9. Since we consider discrete time steps with the time interval \u2206T = 1.0, the feasible execution time steps for e1 are {1, 2, 3}. The idea behind enumerating all feasible execution time steps is to assign an event, and thus to tighten the bounds of all unassigned events in order to ensure feasibility.\nAt the node \u03c3(e1) = 1, the the p-Sulu Planner solves the FRR of the fixed-schedule CCQSP planning problem only with the \u201cEnd in A\u201d episode and the execution schedule \u03c3(e1) = 1. In other words, it tries to find the optimal path that goes through A at t = 1, but neglects the goal B and obstacle C. If a solution exists, its optimal cost gives a lower bound on the objective value of all feasible paths that go through A at t = 1. Assume here that such a solution does not exist. Then, the the p-Sulu Planner prunes the node \u03c3(e1) = 1, and goes to the next node \u03c3(e1) = 2. It solves the FRR of the corresponding fixed-schedule subproblem to find the best path that goes through A at t = 2. Assume that the the p-Sulu Planner finds a solution. Then, the the p-Sulu Planner expands the node in the following process. First, it fixes the execution time \u03c3(e1) = 2 in the d-graph, and runs a shortest-path algorithm in order to tighten the temporal constraints (11-(c)). Then the the pSulu Planner uses the updated d-graph to enumerate the feasible execution-time assignments for the event eE , which are {4, 5}. It visits both nodes and solves the fixed-schedule subproblems exactly with all episodes and a fully assigned schedule. For example, at the node \u03c3(eE) = 5, it computes the best path that goes through A at t = 2 and reaches B at t = 5 while avoiding the obstacle C, as shown in Figure 10-(b). Assume that the optimal objective values of the subproblems are 10.0 for \u03c3(eE) = 4 and 8.0 for \u03c3(eE) = 5. The algorithm records the solution with \u03c3(eE) = 5 and its cost 8.0 as the incumbent.\nThe algorithm then backs up and visits the node \u03c3(e1) = 3, where a relaxed subproblem with only the \u201cEnd in A\u201d episode is solved to obtain the lower bound of the objective value of subsequent nodes. The lower bound turns out to be 9.0, which exceeds the incumbent. Therefore, the branch is pruned. Since there are no more nodes to expand, the algorithm is terminated, and the incumbent solution is returned.\nAlgorithm 7 Implementation of expand function in Line 13 of Algorithm 6 function expand(ccqsp, queue, e, E\u03c3, \u03c3) 1: Fix the distance between e0 and e to \u03c3(e)\u2206T on the d-graph of ccqsp 2: Update the d-graph by running a shortest-path algorithm 3: Choose e\u2032 from E\\E\u03c3 //choose an unassigned event 4: E\u03c3\u2032 := E\u03c3 \u222a e\u2032 5: De\u2032(\u03c3) := { t \u2208 T | dmine\u2032 (\u03c3) \u2264 t\u2206T \u2264 dmaxe\u2032 (\u03c3)} 6: for t in De\u2032(\u03c3) do\n7: \u03c3\u2032(e) := { \u03c3(e) (e \u2208 E\u03c3) t (e = e\u2032)\n//update the partial schedule\n8: queue \u2190 \u27e8E\u03c3\u2032 , \u03c3\u2032\u27e9 9: end for"}, {"heading": "6.2 Branching", "text": "Algorithm 7 outlines the implementation of the expand() function in Algorithm 6. It takes a partial schedule \u03c3 as an input, and adds to the queue a set of schedules that assign an execution time step to an additional event e\u2032. In other words, the domain of the newly added schedules E\u03c3\u2032 has one more assigned event than the domain of the input partial schedule E\u03c3. The details of Algorithm 7 are explained in the following parts of this subsection."}, {"heading": "6.2.1 ENUMERATION OF FEASIBLE TIME STEP ASSIGNMENTS USING D-GRAPH", "text": "When enumerating all feasible time steps, the simple temporal constraints must be respected. To accomplish this, we use a d-graph to translate the bounds on the durations between two events into the bounds on the execution time step of each event. It is shown by Dechter et al. (1991) that the set of feasible execution times for an event e is bounded by the distance between e and e0 on the dgraph. A d-graph is a directed graph, where the weights of the edges represent the shortest distances between nodes, as in Figure 11-(b). In order to obtain the d-graph representation, we first translate the simple temporal constraints into a directed distance graph, as in Figure 11-(a). The weight of an edge between two nodes (events) corresponds to the maximum duration of time from the origin node to the destination node, as specified by the corresponding simple temporal constraint. The distance takes a negative value to represent lower bounds. The d-graph (Figure 11-(b)) is obtained from the distance graph (Figure 11-(a)) by running an all-pair shortest-path algorithm (Dechter et al., 1991).\nForward checking over a d-graph The the p-Sulu Planner algorithm incrementally assigns an execution time step to each event, as explained in the walk-through example. The p-Sulu Planner minimizes the search space through forward checking using the d-graph. As in forward checking methods of Constraint Programming, our method prunes all values of unassigned variables (i.e., execution times of an unassigned event) that violate simple temporal constraints. What is different here from normal forward checking is that no back tracking is performed, due to decomposability of d-graph. The forward checking is conducted in the following process. Once an execution time step t is assigned to an event e (i.e., \u03c3(e) = t), the distance from e0 to e is fixed to t\u2206T , and the distance from e to e0 is fixed to \u2212t\u2206T on the distance graph (Line 1 of Algorithm 7). Recall that t is an index of discretized time steps with a fixed interval \u2206T , while the temporal bounds are given as real-valued times (Section 2.1). We then run a shortest-path algorithm to update the d-graph (Line\n2). Given a partial schedule \u03c3, we denote the updated shortest distance from the start event e0 to e\u2032 on the d-graph by dmaxe\u2032 (\u03c3), and the distance from e \u2032 to e0 by dmine\u2032 (\u03c3).\nFor example, the execution time 2 is assigned to the event e1 in Figure 11-(c) (i.e., \u03c3(e1) = 2), so the distance between e0 and e1 is fixed to 2 and the distance in the opposite direction is fixed to \u22122. Then we run a shortest-path algorithm again to update the d-graph. As a result, we obtain updated distances dmaxeE (\u03c3) = 5.5 and d min eE\n(\u03c3) = \u22123.6. Dechter et al. (1991) showed that dmaxe\u2032 (\u03c3) corresponds to the upper bound of the feasible execution time for an unassigned event e\u2032, while dmineE (\u03c3) corresponds to the negative of the lower bound. Hence, after a partial schedule \u03c3 is assigned to events e \u2208 E\u03c3, the updated domain for an unassigned event e\u2032 /\u2208 E\u03c3 is bounded by dmine\u2032 (\u03c3) and dmaxe\u2032 (\u03c3). Note that the domain of the execution time steps e\u2032 is included in, but not equal to [dmine\u2032 (\u03c3), d max e\u2032 (\u03c3)], because we only consider discrete execution time steps in a finite set T. During the forward checking, the p-Sulu Planner only computes the real-valued bounds [dmine\u2032 (\u03c3), d max e\u2032 (\u03c3)]. The feasible values of an unassigned variable e\n\u2032 are not enumerated until the search tree is expanded to e\u2032.\nEnumerating the domain of execution time steps for an unassigned event We can readily extract the feasible execution time steps for any unassigned event e\u2032 /\u2208 E\u03c3 from the updated d-graph with a partial schedule \u03c3. LetDe\u2032(\u03c3) be the domain of execution time steps for an unassigned event e\u2032 /\u2208 E\u03c3, given a partial schedule \u03c3. The finite domain De\u2032(\u03c3) is obtained as follows:\nDe\u2032(\u03c3) := { t \u2208 T | dmine\u2032 (\u03c3) \u2264 t\u2206T \u2264 dmaxe\u2032 (\u03c3)}.\nNote that De(\u03c3) may be empty when the temporal constraints are tight, even though they are feasible. The user of the p-Sulu Planner must make\u2206T small enough so that De is not empty.\nFor example, Figure 11-(b) is the d-graph given the partial schedule {\u03c3(e0) = 0}. According to the d-graph, e1 must be executed between 0.8 and 3.9. Assuming that \u2206T = 1, the set of feasible execution time steps for e1 isDe1(\u03c3) = {1, 2, 3}, as shown in Figure 12-(a). Likewise, Figure 11-(c) is the d-graph given the partial schedule {\u03c3(e0) = 0, \u03c3(e1) = 2}; the feasible execution time of eE is between 3.6 and 5.5. Hence, the set of feasible execution time steps for eE is DeE (\u03c3) = {4, 5}, as shown in Figure 12-(b).\nThe enumeration is conducted in Line 6 in Algorithm 7. Then the algorithm creates extensions of the input partial schedule by assigning each of the time steps to e\u2032 (Line 7), and puts the extended partial schedules in the queue (Line 8)."}, {"heading": "6.2.2 EFFICIENT VARIABLE ORDERING OF BRANCH-AND-BOUND SEARCH", "text": "When choosing the next event to assign a time step in Line 3 of Algorithm 7, two variable ordering heuristics are found to be effective in order to reduce computation time.\nThe first heuristic is our new convex-episode-first (CEF) heuristic, which prioritizes events that are not associated with non-convex constraints. The idea of the CEF heuristic is based on the observation that subproblems of the branch-and-bound algorithm are particularly difficult to solve when the episodes in A(E\u03c3) involve non-convex state constraints. The \u201cRemain in R2\\C\u201d (2D plane minus the obstacle C) episode in the walk-through example in Figures 10 is an example of such non-convex episodes. Therefore, an effective approach to reduce the computation time of the p-Sulu Planner is to minimize the number of non-convex subproblems solved in the branch-andbound process. This idea can be realized by sorting the events so that the episodes with a convex feasible region are always examined in the branch-and-bound process before the episodes with a\nnon-convex feasible region. In the walk-through example, note that we visited the event e1 before the event eE in this example. This is because the \u201cEnd in A\u201d episode only involves a convex state constraint while \u201cRemain in R2\\C\u201d (2D plane minus the obstacle C) is non-convex.\nThe second one is the well-known most constrained variable heuristic. When the p-Sulu Planner expands a node, it counts the number of feasible time steps of all unassigned events, and chooses the one with the least number of feasible time steps. The second heuristic used to break ties in the first heuristic."}, {"heading": "6.3 Bounding", "text": "We next present the implementation of the obtainLowerBound() function in Line 8 of Algorithm 6. The algorithm obtains the lower bound by solving a relaxed CCQSP planning problem with a fixed partial schedule.\nAlgorithm 8 outlines the implementation of the obtainLowerBound() function. It takes a partial schedule \u03c3 as an input, and outputs the lower bound of the objective function, as well as the optimal control sequence, given the partial schedule \u03c3. It constructs a relaxed optimization problem, which only involves episodes whose start and end events are both assigned execution time steps (Line 1). If the optimization problem involves non-convex constraints, the NIRA algorithm is used to obtain the solution to the problem (Line 3). Otherwise we solve the FRR of the convex optimization problem to obtain the lower bound efficiently (Line 5). If the input is a fully assigned schedule (E\u03c3 = E), the corresponding node is a leaf node. In such case we obtain an exact solution to the CCQSP planning problem with the fixed schedule \u03c3 by running the NIRA algorithm (Line 3). The details of Algorithm 8 are explained in the subsequent part of this subsection.\nAlgorithm 8 Implementation of obtainLowerBound function in Line 8 of Algorithm 6 function obtainLowerBound(ccqsp, E\u03c3, \u03c3) returns optimal objective value and control sequence 1: subprblem \u2190 Problem 9 with \u03c3 given ccqsp 2: if E\u03c3 = E or A(\u03c3) has episodes with non-convex state regions, then 3: [J\u22c6,u0:N\u22121] \u2190 NIRA(subprblem) //Algorithm 1 4: else 5: J\u22c6 \u2190obtainLowreBound-FRR(subprblem) //Algorithm 5 6: u0:N\u22121 \u2190 \u03a6 7: end if 8: return [J\u22c6,u0:N\u22121]"}, {"heading": "6.3.1 RELAXED OPTIMIZATION PROBLEM WITH PARTIAL SCHEDULE", "text": "We consider a relaxed optimization problem as follows:\nProblem 9: Relaxed Optimization Problem for a Partial Schedule \u03c3\nJ\u22c6(\u03c3) = min u0:N\u22121\u2208UN\nJ(u0:N\u22121, x\u03041:N , \u03c3) (68)\ns.t. \u2200t \u2208 T\u2212, xt+1 = Atx\u0304t +Btut (69)\u2227 c\u2208C \u2227 a\u2208(\u03a8c\u2229A(\u03c3)) \u2227 t\u2208\u03a0a(\u03c3) \u2227 k\u2208Ka \u2228 j\u2208Ja,k hTc,a,k,jxt \u2212 gc,a,k,j \u2264 \u2212mc,a,k,j(\u03b4c,a,k)\n(70)\u2211 k\u2208Ka,a\u2208(\u03a8c\u2229A(\u03c3)) \u03b4c,a,k \u2265 1\u2212\u2206c, (71)\nwhere J\u22c6(\u03c3) is the optimal objective value of the relaxed subproblem with a partial schedule \u03c3. Recall that A(\u03c3) is the partial episode set of \u03c3, which only involves the episodes whose start and end nodes are both assigned execution time steps by the partial schedule \u03c3 (Definition 9). For notational simplicity, we merge the three conjunctions of (70) and obtain the following:\u2227\nc\u2208C \u2227 i\u2208Ic(\u03c3) \u2228 j\u2208Jc,i hTc,i,jx\u0304ti \u2212 gc,i,j \u2264 \u2212mc,i,j(\u03b4c,i).\nNote that this chance constraint is exactly the same as (57), except that a partial schedule \u03c3 is specified instead of a fully assigned schedule s. Hence, Problem 9 is an instance of a non-convex CCQSP planning problem with a fixed schedule (Problem 6), and can be optimally solved by the NIRA algorithm. Also note that \u03c3 is a fully assigned schedule at the leaf node of the branch-andbound search tree.\nThe optimal objective value of Problem 9 gives a lower bound of the optimal objective value of all the subsequent subproblems in the branch-and-bound tree. This property is formally stated in Lemma 8 below. In order to prove this feature, we first define the concept of an extension of a partial schedule as follows:\nDefinition 14. A schedule s : E 7\u2192 T is an extension of a partial schedule \u03c3 : E\u03c3 7\u2192 T if and only if both assign the same time steps to all the events in the domain of \u03c3:\n\u03c3(e) = s(e) \u2200e \u2208 E\u03c3.\nFor example, in Figure 12-(b), a fully assigned schedule {s(e0) = 0, s(e1) = 2, s(eE) = 4} and {s(e0) = 0, s(e1) = 2, s(eE) = 5} is an extension of a partial schedule {\u03c3(e0) = 0, \u03c3(e1) = 2}.\nThe following lemma always holds:\nLemma 8. If a schedule s is an extension of a partial schedule \u03c3, then the optimal objective value of Problem 9 with \u03c3 is a lower bound of the optimal objective value with s:\nJ\u22c6(\u03c3) \u2264 J\u22c6(s).\nProof. Since \u03c3 is a partial schedule, E\u03c3 \u2282 E , and hence A(\u03c3) \u2286 A. Also, since \u03c3(e) = s(e) for all e \u2208 E\u03c3, all the state constraints in the chance constraint (70) of Problem 9 with a partial schedule \u03c3 are included in the problem with a full schedule s. This means that the feasible state space of the\nproblem with s is a subset of the one with \u03c3. Hence, if the chance constraint (24) of the problem with s is satisfied, the chance constraint (70) of the problem with \u03c3 is also satisfied. Therefore, the problem with \u03c3 always results in a better (less) or equal cost than the problem with \u03c3\u2032, because the former has looser constraints.\nFor example, in Figure 12-(b), e1 has been assigned an execution time step but eE has not. Therefore, at node \u03c3(e1) = 2, the chance-constrained optimization problem with only the \u201cEnd in A\u201d episode is solved with the partial schedule {\u03c3(e0) = 0, \u03c3(e1) = 2} (see Figure 10-(a)). It gives a lower bound of the cost of the problems with the fully assigned schedules {s(e0) = 0, s(e1) = 2, s(eE) = 4} and {s(e0) = 0, s(e1) = 2, s(eE) = 5}.\nAlgorithm 8 obtains a lower bound by solving Problem 9 exactly using the NIRA algorithm, if it involves episodes with non-convex state regions (Line 3). If the function is called on a leaf node, Problem 9 is also solved exactly by NIRA. This is because the solutions of leaf subproblems are candidate solutions of an optimal solution of the overall problem. Hence, by solving them exactly, we can ensure the optimality of the branch-and-bound search."}, {"heading": "6.3.2 FURTHER BOUNDING WITH FRR", "text": "If the relaxed subproblem (Problem 9) is convex, then the p-Sulu Planner solves the FRR of the subproblem, instead of solving it exactly with NIRA, in order to obtain a lower bound more efficiently (Line 5 of Algorithm 8). Many practical CCQSP execution problems have only one episode that has a non-convex feasible region. For example, in the CCQSP planning problem shown in Figures 2 and 3, only the \u201csafe region\u201d (R2 minus the obstacles) is non-convex, while \u201cProvincetown\u201d (start region), \u201cScenic region,\u201d and \u201cBedford\u201d (goal region) are convex. In such a case subproblems are solved exactly only at the leaf nodes, and their lower bounds are always evaluated by approximate solutions of FRRs of the subproblems at the non-leaf nodes."}, {"heading": "7. Results", "text": "In this section we empirically demonstrate that the p-Sulu Planner can efficiently operate various systems within the given risk bound. We first present the simulation settings in Section 7.1. Section 7.2 presents the simulation results of the NIRA algorithm, and validates our claim that it can efficiently compute a feasible and near-optimal solution. Section 7.3 demonstrates the p-Sulu Planner on two different benchmark problems. The simulation results highlight the p-Sulu Planner\u2019s capability to operate within the user-specified risk bound. Section 7.4 deploys the p-Sulu Planner on the PTS scenarios, while Section 7.5 applies the p-Sulu Planner to the space rendezvous of an autonomous cargo spacecraft to the International Space Station."}, {"heading": "7.1 Simulation Settings", "text": "Recall that, as we stated in Section 2.4, the p-Sulu Planner takes four inputs: a stochastic plant model M, an an initial condition I, a CCQSP P , and an objective function J . This section specifies M and J , which are commonly used by all the problems in Sections 7.2-7.4. We specify P and I for each problem in the corresponding section."}, {"heading": "7.1.1 STOCHASTIC PLANT MODEL", "text": "This section explains the plant model used in Sections 7.2 - 7.4. Section 7.5 uses a different plant model that is described in detail in Section 7.5.2. We consider a point-mass double-integrator plant, as shown in (72)-(73). Parameters, such as umax, vmax, \u03c32, and \u2206T are set individually for each problem. This plant model is commonly assumed in literatures on unmanned aerial vehicle (UAV) path planning (Kuwata & How, 2011; Le\u0301aute\u0301, 2005; Wang, Yadav, & Balakrishnan, 2007).\nOur state vector xt consists of positions and velocities in x and y directions, while the control vector consists of the accelerations:\nxt := [x y vx vy] T , ut := [ax ay] T .\nThe plant model is specified by the following matrices:\nA =  1 0 \u2206t 0 0 1 0 \u2206t 0 0 1 0 0 0 1  , B =  \u2206t2/2 0 0 \u2206t2/2 \u2206t 0 0 \u2206t  , \u03a3w =  \u03c32 0 0 0 0 \u03c32 0 0 0 0 0 0 0 0 0 0  (72) \u2200t \u2208 T, ||ut|| \u2264 umax, ||Cxt|| \u2264 vmax, (73)\nwhere\nC = ( 0 0 1 0 0 0 0 1 ) .\nThe first constraint in (73) is imposed in order to limit the acceleration. This nonlinear constraint is approximated by the following set of linear constraints:\n\u2200t \u2208 T, rn \u00b7 ut \u2264 umax (n = 1, 2, \u00b7 \u00b7 \u00b7 , Nr)\nrn =\n[ cos 2\u03c0n\nNr , sin\n2\u03c0n\nNr ] We chooseNr = 16. The second constraint in (73) is imposed in order to limit the velocity. We use the same linear approximation as above."}, {"heading": "7.1.2 OBJECTIVE FUNCTION", "text": "In Sections 7.2.3, 7.3, and 7.4, the cost function is the Manhattan norm of the control input over the planning horizon, as follows:\nJ(x\u0304ti ,U , s) = T\u2211 t=1 (|ux,t|+ |uy,t|) .\nThis cost function represents the total change in momentum, which is roughly proportional to the fuel consumption of an aerial vehicle. Note that a minimization problem with the piece-wise linear cost function above can be equivalently replaced by the following minimization problem with a linear cost function and additional linear constraints by introducing slack variables \u00b5x,t and \u00b5y,t:\nmin\nT\u2211 t=1 (\u00b5x,t + \u00b5y,t)\ns.t. \u2200t \u2208 T, \u00b5x,t \u2265 ux,t \u2227 \u00b5x,t \u2265 \u2212ux,t \u2227 \u00b5y,t \u2265 uy,t \u2227 \u00b5y,t \u2265 \u2212uy,t\nIn Section 7.2.4, we minimize expected quadratic cost as follows:\nJ(x\u0304ti ,U , s) = T\u2211 t=1 E [ u2x,t + u 2 y,t ] . (74)"}, {"heading": "7.1.3 COMPUTING ENVIRONMENT", "text": "All simulations except for the ones in Section 7.2 are conducted on a machine with a dual-core Intel Xeon CPU clocked at 2.40 GHz, and with 16 GB of RAM. The algorithms are implemented in C/C++, and run on Debian 5.0.8 OS. The simulations in Section 7.2 are conducted on a machine with a quad-core Intel Core i7 CPU clocked at 2.67 GHz, and with 8 GB of RAM. The algorithms are implemented in Matlab, and run on Windows 7 OS. We used IBM ILOG CPLEX Optimization Solver Academic Edition version 12.2 as the linear program solver, and SNOPT version 7.2-9 as the convex optimization solver."}, {"heading": "7.2 NIRA Simulation Results", "text": "We first statistically compare the performance of NIRA with the prior art. Recall that NIRA is a solver for CCQSP planning problems with non-convex state constraints and a fixed schedule (Problem 3), and used as a subroutine in the p-Sulu Planner."}, {"heading": "7.2.1 COMPARED ALGORITHMS", "text": "There are two existing algorithms that can solve the same problem:\n1. Fixed risk allocation (Blackmore et al., 2006) - This approach fixes the risk allocation to a uniform value. As a result, with an assumption that the cost function is linear, Problem 6 can be reformulated to a mixed-integer linear programming (MILP) problem, which can be solved efficiently by a MILP solver, such as CPLEX.\n2. Particle Control (Blackmore, 2006) - Particle Control is a sampling-based method, which uses a finite number of samples to approximate the joint chance constraints. The control sequence is optimized so that the number of samples that violate constraints is less than\u2206cNp, whereNp is the total number of samples. The optimization problem is again reformulated into MILP, with an assumption that the cost function is linear.\nWe also compare NIRA with an MDP in Section 7.2.5. Although an MDP does not solve exactly the same problem as NIRA, it can also avoid risk by considering a penalty cost of constraint violations. The purpose of the comparison is to highlight the capabilities of chance-constrained planning to provide a guarantee on the probability of failure."}, {"heading": "7.2.2 PROBLEM SETTINGS", "text": "We compare closed-loop and open-loop NIRAs with the two algorithms on a 2-D path planning problem with a randomized location of an obstacle, as shown in Figure 13. A vehicle starts from [0, 0] and heads to the goal at [1.0, 1.0], while avoiding a rectangular obstacle. The obstacle with edge length 0.6 is placed at a random location within the square region with its corners at [0, 0], [1, 0], [1, 1], and [0, 1]. We consider ten time steps with the time interval \u2206t = 1.0. We require that\nthe mean state at t = 10 is at [1.0, 1.0]. The risk bound is set to \u2206 = 0.01. We set the standard deviation of the disturbance as \u03c3 = 0.01. We use the expected quadratic cost function given in (74). The steady-state LQR gain is used for the closed-loop NIRA withQ = I4 andR = 10000I2, where In is the n \u00d7 n identity matrix and Q and R are cost matrices for the state and control variables, respectively."}, {"heading": "7.2.3 PERFORMANCE COMPARISON", "text": "Recall that the solution of the NIRA algorithm, which is used by the p-Sulu Planner to solve subproblems, is not an exactly optimal solution to Problem 3, since risk allocation (Section 4.1.1) and risk selection (Section 5.1.1) replace the chance constraint (29) with its sufficient condition (57) \u2227 (59). Since the chance constraint (29) is very difficult to evaluate, all the previously proposed methods solve optimization with its approximation. We provide empirical evidence that our risk allocation/selection approach results in a solution that is significantly closer to the optimal solution than the prior art, while satisfaction of the original constraint (29) is guaranteed.\nWe evaluate the suboptimality of the solutions by the difference between the risk bound, \u2206 = 0.001, and the resulting probability of constraint violation, Pfail, estimated by a Monte-Carlo simulation. 1 \u2212 Pfail is equal to the left-hand-side value of (29) in Problem 3. Hence, the chance constraint (29) is equivalent to: Pfail \u2264 \u2206. The strictly optimal solution to this problem should achieve Pfail = \u2206, although such an exact solution is unavailable, since there is no algorithm to solve Problem 3 exactly. A solution is suboptimal if Pfail < \u2206, and their ratio \u2206/Pfail represents the degree of suboptimality. A solution violates the chance constraint if Pfail > \u2206.\nTable 1 compares the performance of the four algorithms. The values in the table are the averages and the standard deviations of 100 runs with random locations for the obstacle. The probability of constraint violation, Pfail, is evaluated by Monte-Carlo simulations with 106 samples.\nComparison of closed-loop and open-loop NIRAs Before comparing NIRA with existing algorithms, we first compare the two variants of NIRA: the closed-loop and open loop NIRAs. Table 1 shows that the closed-loop NIRA results in less cost than the open-loop NIRA. Importantly, the former outperforms the latter in all the 100 test cases. This reduction in cost by the closed-loop approach is explained by Figure 13-(b), which shows the 3\u03c3 ellipses of the probability distribution of the state. Since the closed-loop NIRA assumes a feedback control, the future position is less uncertain. As a result, the plan generated by the closed-loop NIRA is less conservative. In fact, Table 1 shows that Pfail of the closed-loop NIRA is closer to the risk bound than that of the open-loop NIRA. However, the closed-loop planning problem requires about twice as much solution time as the open-loop one since it is more complicated due to additional chance constraints on control input.\nComparison with the fixed risk allocation approach Table 1 shows that closed and open NIRAs result in the average probabilities of failure 0.0096 and 0.0095 respectively, which is within the userspecified risk bound \u2206 = 0.01. On the other hand, the fixed risk allocation approach results in a very conservative probability of failure, Pfail = 0.000219, which is 98% smaller than \u2206. This result indicates that the solution by NIRA is significantly closer to the exactly optimal solution than the fixed risk allocation approach. In fact, the NIRA algorithm results in less cost than the fixed risk allocation approach in all the 100 runs. This is because it optimizes the risk allocation while the fixed risk allocation approach uses the predetermined risk allocation.\nFigure 14 shows the suboptimality measure \u2206/Pfail of the open-loop NIRA with different settings of the risk bound \u2206. For all values of \u2206, the suboptimality of NIRA is significantly smaller than the fixed risk allocation approach. The graph shows a tendency that the suboptimality of NIRA gets smaller for less \u2206, while the suboptimality of the fixed risk allocation approach is approximately constant.\nNIRA achieves the improvement in solution optimality with a cost of computation time; Table 1 shows that NIRA takes longer computation time than the risk allocation approach by the factor\nof two. Hence, NIRA and the fixed risk allocation approach provide users with a trade-off between suboptimality and computation time.\nComparison with the Particle Control Table 1 shows that the average probability of failure of the Particle Control approach is higher than the risk bound \u2206 = 0.01, meaning that the approach tends to generate infeasible solutions. On the other hand, NIRA guarantees the satisfaction of the chance constraint since it employs a conservative approximation of the joint chance constraint.\nParticle Control has a guarantee that its solution converges to an optimal solution when increasing the number of samples to infinity. However, using a large number of samples is impractical, since computation time and memory usage grow exponentially as the number of samples increases. For example, we used only 100 samples in the analysis in Table 1. When using 300 samples, it took 4596 seconds (about 1.5 hours) to solve the same problem with the obstacle\u2019s centered at [0.5, 0.5]. Computation with 1000 samples could not be conducted, because of the shortage of memory. On the other hand, the computation time of NIRA is significantly shorter than PC, while guaranteeing the feasibility of the solution."}, {"heading": "7.2.4 OPTIMAL PLANNING WITH EXPECTED COST", "text": "Next we demonstrate the capability of the p-Sulu Planner to handle expected cost, instead of the cost of the expected trajectory, for the same path planning problem presented above. Specifically, we consider the expected quadratic cost function shown in (74). When conducting open-loop planning, this cost function can be transformed to a function of nominal control inputs with a constant term by using the equality (15). However, when performing closed-loop planning, this equality is not exact, due to controller saturation. Nevertheless, we use (15) as an approximation of the expected cost, as explained in Section 2.4.4. In this subsection we empirically evaluate the error of this approximation.\nTable 2 compares the approximate expected cost function value obtained by the closed-loop NIRA with the actual expected cost estimated by Monte-Carlo simulation with one million samples. The path planning problem is solved 100 times with a randomized location of the obstacle. The risk bound is set to \u2206 = 0.01. As shown in the table, the approximate cost almost exactly agrees with the actual cost. This is because our closed-loop planning approach explicitly bounds the risk of controller saturation."}, {"heading": "7.2.5 COMPARISON WITH MDP", "text": "Next we compare NIRA with an MDP formulation. For the sake of tractability of the MDP, we consider a single integrator dynamics with a two-dimensional state space and a two-dimensional control input, which specifies the velocity of a vehicle. The rest of the problem setting is the same, except that the state space is discretized into a 100-by-100 grid. We implement a finite-horizon MDP-based path planner, which imposes a penalty c on an event of failure and minimizes the expected cost based on explicit state dynamic programming. The MDP-based path planner imposes a cost as follows:\nE [ T\u2211 t=1 ( u2x,t + u 2 y,t + cI(xt) )] ,\nwhere I(xt) is an indicator function that is one if xt is in a obstacle and zero otherwise. The resulting optimization problem is solved via dynamic programming.\nWe ran the MDP-based path planner with three values of penalty c: 1, 10, and 100. For each choice of c, we conducted 100 simulations with a randomized obstacle position. Figure 14 shows a typical output of the MDP-based path planner. Note that, with a small penalty (c = 1), the path planner chooses to take a 100% risk of failure by ignoring the obstacle. This is simply because the penalty of failure is smaller than the expected reduction of cost by going through an obstacle. An issue of utilitarian approaches such as MDPs is that minimization of unconstrained cost can sometimes lead to such impractical solution.\nTable 3 shows the mean and the standard deviation of path lengths, as well as the maximum, minimum, and the mean of the resulting probability of failure among the 100 runs. As expected, by imposing a larger penalty, the MDP-path planner chooses a more risk-averse path, which has a longer nominal path length. In this sense, an MDP can also conduct a trade-off between cost and risk. MDP is particularly useful when the primary concern of the user is the cost of failure instead of the probability of failure. On the other hand, when a user would like to impose a hard bound on the probability of failure, our chance constrained planning approach has an advantage. Observe that, even with the same penalty value, the MDP-based path planner results in a wide range of failure probabilities depending on the location of the obstacle. Most notably, with c = 10, some of the\npaths move directly across the obstacle, and in so doing, accept a 100% probability of failure, while others go around the obstacle. Undesirable behaviors, such as crossing an obstacle, are likely to be suppressed by imposing a greater penalty, but without a guarantee. Moreover, imposing a heavy penalty on failure often results in an overly conservative, risk averse solution. On the other hand, the behavior of NIRA with regarding to risk is predictable, in a sense that the path is guaranteed to go around the obstacle, regardless of its location. This is because the chance constraint requires that there exists a margin between the path and the boundary of the obstacle. The p-Sulu Planner inherits this property from NIRA."}, {"heading": "7.3 The p-Sulu Planner Simulation Results", "text": "Next we present the simulation results of the p-Sulu Planner on two problems, in order to illustrate its capability of planning with schedule constraints. We also empirically evaluate the scalability of p-Sulu."}, {"heading": "7.3.1 PATH PLANNING WITH OBSTACLES", "text": "In this simulation we test the p-Sulu Planner on a path planning problem in the environment shown in Figure 17. The input CCQSP is shown in Figure 16. The CCQSP requires a vehicle to arrive at the goal region within 15 minutes, by going through Waypoint 1 and Waypoint 2 with the temporal constraints specified in Figure 16. It also imposes two chance constraints: one that requires the vehicle to achieve the time-evolved goals with 90% certainty, and another that requires the vehicle to limit the probability of violating the obstacles to \u2206obs. We set\u2206t = 1 and \u03c32 = 0.0025.\nFigure 17 shows the plans generated by the p-Sulu Planner with three different risk bounds: \u2206obs = 10%, 0.1%, and 0.001%. The computation times were 79.9 seconds, 86.4 seconds, and 88.1 seconds, respectively. Figure 17 also shows the plan generated by Sulu, a deterministic planner that does not explicitly consider uncertainty (Le\u0301aute\u0301 &Williams, 2005). Observe that Sulu leaves no margin between the path and obstacles. As a result, the Sulu path results in a 94.1% probability of hitting obstacles, as estimated by aMonte-Carlo simulation with 107 samples. On the other hand, the p-Sulu Planner leaves margins between the path and the obstacles in order to satisfy the risk bound,\nspecified in the chance constraint. The margins are larger for the plans with smaller risk bounds. The probabilities of failure of the three plans generated by the p-Sulu Planner, estimated by Monte-Carlo simulations with 107 samples, are 9.53%, 0.0964%, and 0.00095%, respectively. Hence the chance constraints are satisfied. The schedule optimized by the p-Sulu Planner is {s(e0) = 0, s(e1) = 5, s(e2) = 10, s(eE) = 15}, which satisfies all the temporal constraints in the CCQSP.\nIn Figure 16, it appears that the path cuts across the obstacle. This is due to the discretization of time; the optimization problem only requires that the vehicle locations at each discrete time step satisfy the constraints, and does not consider the state in between. This issue can be addressed by a constraint-tightening method (Kuwata, 2003)."}, {"heading": "7.3.2 PATH PLANNING IN AN INDOOR ENVIRONMENT", "text": "We next give the p-Sulu Planner the CCQSP shown in Figure 18, which simulates a path planning problem in an indoor environment. A vehicle must get to the goal region at the other side of the room in three to five seconds. The \u201cRemain in safe region\u201d episode requires the vehicle to stay\nwithin the room and outside of the obstacle during the five-second planning horizon. The CCQSP imposes two chance constraints shown in Figure 18. We set \u2206t = 0.5 and \u03c32 = 5.0\u00d7 10\u22125.\nGiven this CCQSP, the planner faces a choice: heading straight to the goal by going through the narrow passage between the left wall and the obstacle minimizes the path length, but involves higher risk of constraint violation; making a detour around the right side of the obstacle involves less risk, but results in a longer path.\nFigure 19 shows the p-Sulu Planner\u2019s outputs with \u2206obs = 10%, 1%, and 0.1%. The computation times were 35.1 seconds, 84.5 seconds, and 13.3 seconds, respectively. The result is consistent with our intuition. When the p-Sulu Planner is allowed a 10% risk, the planner chooses to go straight to the goal, resulting in the cost function value of 1.21; when the user gives a 1% or 0.1% risk bound, it chooses the risk-averse path, resulting in the cost function values of 3.64 and 3.84, respectively. This example demonstrates the p-Sulu Planner\u2019s capability to make an intelligent choice in order to minimize the cost, while limiting the risks to user-specified levels."}, {"heading": "7.3.3 SCALABILITY ANALYSIS", "text": "In this subsection we conduct an empirical analysis of the scalability of the p-Sulu Planner, as the environment becomes increasingly constrained.. As shown in Figure 20, we measured the computation time to solve a path planning problem with different numbers of obstacles and waypoints. In all simulations, the path starts at [0, 12] and ends in a square region centered at [24, 12]. Figure 20 shows twenty simulation results, with zero to three obstacles and zero to four waypoints. Obstacles and waypoints are represented by blue and red squares in the figure, respectively. The positions of the center of the obstacles are [6, 12], [12, 12], and [18, 12], while the positions of the center of the waypoints are [9, 9], [9, 15], [15, 15], and [15, 9]. The computation time is shown in the caption of each subfigure in Figure 20.\nBy comparing the results in Figure 20 horizontally, we observe exponential growth in computation time with the number of obstacles. This result is expected since the number of disjunctive clauses in the state constraint of the p-Sulu Planner increases exponentially with the number of obstacles. Building a tractable extension of the p-Sulu Planner for a large number of obstacles is future work. On the other hand, by comparing the results vertically, we find that the computation time with the same number of obstacles and different number of waypoints stays in the same order of magnitude. This is because adding an extra waypoint only increases the number of conjunctive clauses in the state constraints.\nIn the remaining sections we describe the application of psulu to two real world problems, air vehicle and space vehicle control. A third application, building energy management, using a variant of the p-Sulu Planner, is reported by Ono, Graybill, and Williams (2012)."}, {"heading": "7.4 PTS Scenarios", "text": "Next, we deploy the p-Sulu Planner on PTS scenarios, the robotic air taxi system introduced in Section 1."}, {"heading": "7.4.1 SCENARIOS", "text": "We consider three scenarios, specified by the CCQSPs shown in Figure 21. Scenarios 1 and 2 are similar to the scenic flight scenario introduced at the beginning of this paper (see Figure 1). In\nScenario 1, a personal aerial vehicle (PAV) takes off from Runway 71 of Provincetown Municipal Airport (KPVC) in Provincetown, Massachusetts, fly over a scenic region, and lands on Runway 23 of Hanscom Field (KBED) in Bedford, Massachusetts. The vehicle is required to stay within the scenic region at least for 2 minutes and at most for 10 minutes. The entire flight must take more than 13 minutes and less than 15 minutes. Scenario 2 is the same as Scenario 1, except for the runways used for take-off and landing.\nScenario 3 simulates a leisure flight off the coast of Massachusetts. A PAV takes off Runway 7 of Provincetown Municipal Airport, and flies over two regions where whales are often seen. Then the vehicle lands on Runway 11 of Hanscom Field.\nWe place three no-fly zones, as shown in Figure 22. The entire flight must take more than 13 minutes and less than 15 minutes. Each scenario has three chance constraints, {c1, c2, c3}, as shown in Figure 21. The first one, c1, is concerned with the vehicle\u2019s operation; it requires the vehicle to take off from and land on the right runways at the right airports with less than 10 % probability of failure. The second chance constraint, c2, is concerned with the leisure activities; it requires the vehicle to fly over the scenic regions with less than 10 % probability of failure. Finally, c3 is concerned with the passenger\u2019s safety; it requires the vehicle to limit the risk of penetrating the no-fly zones to 0.01 %.\n1. A runway of an airport is specified by a number, which represents the clockwise angle from the north. For example, Runway 7 points 70 degrees away from the north."}, {"heading": "7.4.2 PLANT PARAMETERS", "text": "We set umax = 250 m/s, which approximates the maximum cruise speed of private jet airplanes, such as Gulfstream V. The maximum acceleration is determined from the maximum bank angle. Assuming that an aircraft is flying at a constant speed, the lateral acceleration a is given as a function of the bank angle \u03d5 as follows:\na = g \u00b7 tan\u03d5,\nwhere g is the acceleration of gravity. Typically passenger aircraft limits the bank angle to 25 degrees for passenger comfort, even though the aircraft is capable of turning with a larger bank angle. Hence, we use:\numax = 9.8 m/s2 \u00b7 tan(25\u25e6) = 4.6 m/s2.\nWe set \u03c3 = 100 m and \u2206T = 60 seconds."}, {"heading": "7.4.3 SIMULATION RESULTS", "text": "Figure 22 shows the paths planned by the p-Sulu Planner for the three scenarios. In all the scenarios, all the episode requirements in the CCQSPs in Figure 21 are met within the specified temporal and chance constraints.\nTable 4 compares the performance of Sulu and the p-Sulu Planner. As expected, Sulu\u2019s plans result in excessive probabilities of failure in all scenarios. This is because Sulu does not consider uncertainty in the planning process, although the PAV is subject to disturbance in reality. On the other hand, the p-Sulu Planner successfully limits the probability of failure within the user-specified risk bounds for all three scenarios. Furthermore, although the p-Sulu Planner significantly reduces the risk of failure, its cost is higher than that of Sulu only by 9.5 - 12.8 %. Such a capability of limiting the risk and maximizing the efficiency at the same time is a desirable feature for PTS, which transports passengers.\nAs shown in Table 4, the p-Sulu Planner typically takes several minutes to compute the plan. This length of computation time would be allowed for PTS applications, since we assume that the pSulu Planner is used for preplanning; before take-off, the passengers of a PAV specify requirements, and the p-Sulu Planner creates a risk-sensitive flight plan. We assume that a real-time plan executive executes the plan after take-off.\nWe note that it is more desirable to have a real-time risk-sensitive plan executive, since risk factors, such as the location of storms, change over time. Our future work is to reduce the computation time of the p-Sulu Planner so that it can be used for real-time execution."}, {"heading": "7.5 Space Rendezvous Scenario", "text": "The p-Sulu Planner is a general planner whose application is not limited to a specific plant model. In order to show the generality of the planner, we deployed the p-Sulu Planner on a system whose plant model is significantly different from PTS.\nSpecifically, we chose an autonomous space rendezvous scenario of the H-II Transfer Vehicle (HTV), shown in Figure 23, as our subject. HTV is an unmanned cargo spacecraft developed by the Japanese Aerospace Exploration Agency (JAXA), which is used to resupply the International Space Station (ISS). Collision of the vehicle with the ISS may result in a fatal disaster, even if the collision speed is low. For example, in August 1994, the Russian unmanned resupply vehicle Progress M34 collided with the Mir space station in a failed attempt to automatic rendezvous and docking. As a result, one of the modules of Mir was permanently depressurized. In order to avoid such an accident, HTV is required to follow a specified safety sequence during the automated rendezvous, as described in the following subsection."}, {"heading": "7.5.1 HTV RENDEZVOUS SEQUENCE", "text": "In HTV\u2019s autonomous rendezvous mission, the final approach phase starts from the Approach Initiation (AI) point, which is located 5 km behind the ISS, as shown in Figure 24. First, HTV moves to the R-bar Initiation (RI) point, which is located 500 m below the ISS, guided by the relative GPS\nnavigation. At the RI point, HTV switches the navigation mode to Rendezvous Sensor (RVS) Navigation. In RVS Navigation, HTV measures the distance to ISS precisely by beaming the laser to the reflector placed on the nadir (earth-facing) side of the ISS. Then, HTV proceeds to the Hold Point (HP), located 300 m below the ISS. It is required to hold at HP in order to perform a 180-degree yaw-around maneuver. The new orientation of HTV allows the vehicle to abort the rendezvous quickly in case of emergency. After the yaw-around maneuver, HTV resumes the approach, and holds again at the Parking Point (PP), which is 30 m below the ISS. Finally, HTV approaches at a distance of 10 meters from the ISS, and stops within the Capture Box (CB) of the ISS\u2019s robotic arm. The robotic arm then grabs HTV and docks it to the ISS. Please refer to the report by Japan Aerospace Exploration Agency (2009) for the details of the rendezvous sequence.\nThe rendezvous sequence described above is represented by the CCQSP shown in Figure 25. In addition to the time-evolved goals specified in the actual rendezvous sequence, we specify temporal constraints and chance constraints in the simulation, as shown in the figure. We require HTV to hold at each intermediate goal for at least 240 seconds. The transition between the goals must take at least 600 seconds, in order to make sure that the vehicle moves slowly enough. The entire mission must be completed within 4800 seconds (1 hour 20 minutes). We require HTV to stay within the Safe Zone, a conic area below the ISS, during the RVS navigation phase with 99.5% probability, since otherwise the laser may not be reflected back to HTV properly. We assume that the goals are square regions, with 10 m sides for RI and HP, 2 m sides for PP, and 1 m sides for CB. Finally, we require that HTV achieves all the time-evolved goals with 99.5% success probability."}, {"heading": "7.5.2 ORBITAL DYNAMICS", "text": "The rendezvous can be considered as a two-body problem, where a chaser spacecraft (e.g., HTV) moves in relation to a target spacecraft (e.g., ISS), which is in a circular orbit. In such a problem, it is convenient to describe the motion of the chaser spacecraft using a rotating frame that is fixed to the target space craft, known as a Hill coordinate frame (Schaub & Junkins, 2003). As shown in Figure 24, we set the x-axis pointing away from the center of the earth and the y-axis along the\norbital velocity of the target spacecraft. Since HTV\u2019s path is within the x-y plane, we don\u2019t consider the z-axis.\nIt is known that the relative motion of the chase spacecraft in the Hill coordinate frame is described by the following Clohessy-Wiltshire (CW) equation (Vallado, 2001):\nx\u0308 = 2\u03c9y\u0307 + 3\u03c92x+ Fx\ny\u0308 = 2\u03c9x\u0307+ Fy\nwhere \u03c9 is the angular speed of the target spacecraft\u2019s orbit, and Fx and Fy are the force per unit mass, or the acceleration in x and y directions. The first terms on the right-hand sides represent the Coriolis force.\nAn object that follows the CW equation moves in an unintuitive manner. Its unforced motion is not in a straight line due to the Coriolis effect; in general, an object cannot stay at the same position without external force. For example, Figure 26 shows the fuel-optimal path to visit two waypoints, A and B, and come back to the start. As can be seen in the figure, the optimal path is not typically a straight line. The virtue of the p-Sulu Planner is that it can handle such irregular dynamic systems in the same way as regular systems, just by setting the A and B matrices of our plant model (4) appropriately.\nThe state vector consists of positions and velocity in the x\u2212 y plane:\nx = [x y vx vy] T\nWe obtain the discrete-time CW equation using the impulse-invariant discretization:\nxk+1 = Axk +Buk,\nwhere\nA =  4\u2212 3 cos(\u03c9\u2206T ) 0 sin(\u03c9\u2206T )\u03c9\n2{1\u2212cos(\u03c9\u2206T )} \u03c9\n\u22126{\u03c9\u2206T \u2212 sin(\u03c9\u2206T )} 1 \u22122{1\u2212cos(\u03c9\u2206T )}\u03c9 4 sin(\u03c9\u2206T )\n\u03c9 \u2212 3\u2206T 3\u03c9 sin(\u03c9\u2206T ) 0 cos(\u03c9\u2206T ) 2 sin(\u03c9\u2206T ) \u22126\u03c9{1\u2212 cos(\u03c9\u2206T )} 0 \u22122 sin(\u03c9\u2206T ) 4 cos(\u03c9\u2206T )\u2212 3\n\nB =  sin(\u03c9\u2206T ) \u03c9 2{1\u2212cos(\u03c9\u2206T )} \u03c9 \u22122{1\u2212cos(\u03c9\u2206T )} \u03c9 4 sin(\u03c9\u2206T ) \u03c9 \u2212 3\u2206T cos(\u03c9\u2206T ) 2 sin(\u03c9\u2206T )\n\u22122 sin(\u03c9\u2206T ) 4 cos(\u03c9\u2206T )\u2212 3  We use the ISS\u2019s orbital angular speed, \u03c9 = 0.001164 rad/sec, at which the station goes around the Earth in 90 minutes. We choose the interval \u2206T = 120 seconds. The number of time steps N is set to 40. Hence, the entire plan is 4800 seconds (1 hour and 20 minutes). In the discretization, we assumed impulse inputs as follows:[\nFx Fy\n] = N\u22121\u2211 k=0 \u03b4(t\u2212\u2206T \u00b7 k)uk,\nwhere \u03b4(\u00b7) is the Dirac delta function. Such an assumption is justified because the thrusters of the Reaction Control System (RCS) of the spacecraft, which are used for the final approach maneuver, operate for a very short duration (0.01\u22125.0 seconds) at each burn (Wertz &Wiley J. Larson, 1999).\nWe consider stochastic uncertainty w, added to the discrete-time dynamic equation:\nxk+1 = Axk +Buk + w.\nSuch an assumption of additive uncertainty is commonly used in past research on autonomous rendezvous and formation flight in space (Shields, Sirlin, & Wette, 2002; Smith & Hadaegh, 2007;\nCampbell & Udrea, 2002). We assume that w has a zero-mean Gaussian distribution, with the following covariance matrix:\n\u03a3w =  10\u22126 0 0 0 0 10\u22126 0 0 0 0 0 0 0 0 0 0  ."}, {"heading": "7.5.3 OBJECTIVE FUNCTION", "text": "We employ an objective function J that requires for the p-Sulu Planner to minimize the fuel consumption. It follows from the Tsiolkovsky rocket equation that the fuel consumption of spacecraft is proportional to the total change in velocity, called Delta-V or \u2206V (Wertz &Wiley J. Larson, 1999). The total fuel consumption is the summation of the fuel consumption of reaction jets in x and y directions for all time steps. Hence our objective function is described as follows:\nJ(u0:N ) = \u2206Vx +\u2206Vy\n= \u222b (N\u22121)\u2206T 0 |Fx|+ |Fy|dt\n= k=N\u22121\u2211 k=0 \u2223\u2223\u2223\u2223\u2223 \u222b (N\u22121)\u2206T 0 \u03b4(t\u2212\u2206T \u00b7 k)ux,kdt \u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 \u222b (N\u22121)\u2206T 0 \u03b4(t\u2212\u2206T \u00b7 k)uy,kdt \u2223\u2223\u2223\u2223\u2223 =\nk=N\u22121\u2211 k=0 |ux,k|+ |uy,k|."}, {"heading": "7.5.4 SIMULATION RESULT", "text": "Figure 27 shows the planning result of the p-Sulu Planner. We compare the result with Sulu, as well as a nominal planning approach, in which we assume that HTV moves from AI to RI using a two-impulse transition (called \u201cCW guidance law\u201d) (Matsumoto, Dubowsky, Jacobsen, & Ohkami, 2003; Vallado, 2001). From RI to CB, it follows a predetermined path that goes through the center of the Safe Zone, as shown in Figure 27-(b), with a constant speed.\nAs shown in Figure 27, the optimal paths generated by the p-Sulu Planner and Sulu are not straight. Such curved paths exploit the Coriolis effect to minimize fuel consumption.\nTable 5 compares the performance of the three planning approaches. The two rows regarding the probabilities of failure correspond to the two chance constraints specified in the CCQSP, shown in Figure 25. The probabilities are evaluated by Monte Carlo simulation with one million samples.\nAs expected, the probabilities of failure of the path generated by the p-Sulu Planner are less than the risk bounds specified by the CCQSP, shown in Figure 25. On the other hand, once again, Sulu\u2019s path results in almost 100% probability of failure. This is because Sulu minimizes the fuel consumption without considering uncertainty. The resulting path pushes against the boundaries of feasible regions, as is evident in Figure 27-(c). Also note that, although the p-Sulu Planner significantly reduces the probability of constraint violation compared with Sulu, its cost (Delta V) is higher than Sulu only by 0.2%. The p-Sulu Planner results in a significantly smaller cost (Delta V) than the nominal planning approach. The 1.42 m/sec reduction in Delta V is equivalent to an 11.9 kg saving of fuel, assuming the 16, 500 kg mass of the vehicle and the 200 sec specific impulse\n(ISP ) of the thrusters. Although the p-Sulu Planner takes longer to compute the plan than the other two approaches, the 11.4 second computation time is negligible compared with the 1 hour and 20 minute plan duration."}, {"heading": "8. Conclusions", "text": "This article introduced a model-based planner, the p-Sulu Planner, which operates within userspecified risk bounds. The p-Sulu Planner optimizes a continuous control sequence and a discrete schedule, given as input a continuous stochastic plant model, an objective function, and a newly developed plan representation, a chance-constrained qualitative state plan (CCQSP). A CCQSP involves time-evolved goals, simple temporal constraints, and chance constraints, which specify the user\u2019s acceptable levels of risk on subsets of the plan.\nOur approach to developing the p-Sulu Planner was two-fold. In the first step, we developed an efficient algorithm, called non-convex iterative risk allocation (NIRA), that can plan in a nonconvex state space but for a fixed schedule. We solved the problem based on the key concept of risk allocation and risk selection, which achieves tractability by allocating the specified risk to individual constraints and by mapping the result into an equivalent disjunctive convex program. The NIRA algorithm employs a branch-and-bound algorithm to solve the disjunctive convex program. Its subproblems are fixed-schedule CCQSP problems with a convex state space, which can be solved by our previously developed algorithms (Blackmore & Ono, 2009). We developed a novel relaxation method called fixed risk relaxation (FRR), which provides the tightest linear relaxation of the nonlinear constraints in the convex subproblems.\nIn the second step, we developed the p-Sulu Planner, which can solve a CCQSP planning problem with a flexible schedule. The scheduling problem was formulated as a combinatorial constrained optimization problem (COP), which is again solved by a branch-and-bound algorithm. Each subproblem of the branch-and-bound search is a CCQSP planning problem with a fixed schedule, which is solved by NIRA. The domain of the feasible schedule is pruned by running a shortest-path algorithm on the d-graph representation of the given temporal constraints. The lower bounds of the optimal objective value of the subproblems are obtained by solving fixed-schedule CCQSP planning problems where a subset of the state constraints are imposed. We proposed an efficient variable ordering that prioritizes convex subproblems over non-convex ones. We demonstrated the p-Sulu Planner on various examples, from a personal aerial transportation system to autonomous space rendezvous, and showed that it can efficiently solve CCQSP planning problems with small suboptimality, compared to past algorithms."}, {"heading": "Acknowledgments", "text": "This paper is based upon work supported in part by the Boeing Company under Grant No. MITBA-GTA-1 and by the National Science Foundation under Grant No. IIS-1017992. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the view of the sponsoring agencies. We would like to thank Michael Kerstetter, Scott Smith, Ronald Provine, and Hui Li at Boeing Company for their support. Thanks also to Robert Irwin for advice on the draft."}], "references": [{"title": "A robust model predictive control algorithm for incrementally conic uncertain/nonlinear systems", "author": ["B. Acikmese", "J.M. Carson III", "D.S. Bayard"], "venue": "International Journal of Robust and Nonlinear Control,", "citeRegEx": "Acikmese et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Acikmese et al\\.", "year": 2011}, {"title": "2005 Joseph T", "author": ["Aircraft Owners", "Pilots Association Air Safety Foundation"], "venue": "Nall Report - accident trands and factors for 2004..", "citeRegEx": "Owners and Foundation,? 2005", "shortCiteRegEx": "Owners and Foundation", "year": 2005}, {"title": "Constrained Markov decision processes", "author": ["E. Altman"], "venue": "Stochastic modeling. Chapman & Hall/CRC.", "citeRegEx": "Altman,? 1999", "shortCiteRegEx": "Altman", "year": 1999}, {"title": "The benefits of relaxing punctuality", "author": ["R. Alur", "T. Feder", "T.A. Henzinger"], "venue": "Journal of the ACM,", "citeRegEx": "Alur et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Alur et al\\.", "year": 1996}, {"title": "Planning for temporally extended goals", "author": ["F. Bacchus", "F. Kabanza"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Bacchus and Kabanza,? \\Q1998\\E", "shortCiteRegEx": "Bacchus and Kabanza", "year": 1998}, {"title": "Disjunctive programming", "author": ["E. Balas"], "venue": "Annals of Discrete Mathematics.", "citeRegEx": "Balas,? 1979", "shortCiteRegEx": "Balas", "year": 1979}, {"title": "Dynamic Programming and Optimal Control Volume I (Third Edition)", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas,? 2005", "shortCiteRegEx": "Bertsekas", "year": 2005}, {"title": "Neuro-Dynamic Programming (1st edition)", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "A probabilistic particle control approach to optimal, robust predictive control", "author": ["L. Blackmore"], "venue": "Proceedings of the AIAA Guidance, Navigation and Control Conference.", "citeRegEx": "Blackmore,? 2006", "shortCiteRegEx": "Blackmore", "year": 2006}, {"title": "A probabilistic approach to optimal robust path planning with obstacles", "author": ["L. Blackmore", "H. Li", "B.C. Williams"], "venue": "In Proceedings of American Control Conference", "citeRegEx": "Blackmore et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blackmore et al\\.", "year": 2006}, {"title": "Convex chance constrained predictive control without sampling", "author": ["L. Blackmore", "M. Ono"], "venue": "In Proceedings of the AIAA Guidance, Navigation and Control Conference", "citeRegEx": "Blackmore and Ono,? \\Q2009\\E", "shortCiteRegEx": "Blackmore and Ono", "year": 2009}, {"title": "Exact solutions to time-dependent MDPs", "author": ["J.A. Boyan", "M.L. Littman"], "venue": "In in Advances in Neural Information Processing Systems,", "citeRegEx": "Boyan and Littman,? \\Q2000\\E", "shortCiteRegEx": "Boyan and Littman", "year": 2000}, {"title": "Generalization in reinforcement learning: Safely approximating the value function", "author": ["J.A. Boyan", "A.W. Moore"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Boyan and Moore,? \\Q1995\\E", "shortCiteRegEx": "Boyan and Moore", "year": 1995}, {"title": "Collision avoidance in satellite clusters", "author": ["M.E. Campbell", "B. Udrea"], "venue": "In Proceedings of the American Control Conference", "citeRegEx": "Campbell and Udrea,? \\Q2002\\E", "shortCiteRegEx": "Campbell and Udrea", "year": 2002}, {"title": "Colin: Planning with continuous linear numeric change", "author": ["A.J. Coles", "A. Coles", "M. Fox", "D. Long"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Coles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Coles et al\\.", "year": 2012}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": "Elsevier.", "citeRegEx": "Dechter,? 2003", "shortCiteRegEx": "Dechter", "year": 2003}, {"title": "Temporal constraint networks", "author": ["R. Dechter", "I. Meiri", "J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 1991}, {"title": "Stationary deterministic policies for constrained MDPs with multiple rewards, costs, and discount factors", "author": ["D. Dolgov", "E. Durfee"], "venue": "Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Dolgov and Durfee,? \\Q2005\\E", "shortCiteRegEx": "Dolgov and Durfee", "year": 2005}, {"title": "Dynamic programming for structured continuous markov decision problems", "author": ["Z. Feng", "R. Dearden", "N. Meuleau", "R. Washington"], "venue": "In Proceedings of the Proceedings of the Twentieth Conference Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Feng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2004}, {"title": "Risk-sensitive control on an infinite time horizon", "author": ["W. Fleming", "W. McEneaney"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Fleming and McEneaney,? \\Q1995\\E", "shortCiteRegEx": "Fleming and McEneaney", "year": 1995}, {"title": "Modelling mixed discrete-continuous domains for planning", "author": ["M. Fox", "D. Long"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fox and Long,? \\Q2006\\E", "shortCiteRegEx": "Fox and Long", "year": 2006}, {"title": "Risk-sensitive reinforcement learning applied to control under constraints", "author": ["P. Geibel", "F. Wysotzki"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Geibel and Wysotzki,? \\Q2005\\E", "shortCiteRegEx": "Geibel and Wysotzki", "year": 2005}, {"title": "Optimization over state feedback policies for robust control with constraints", "author": ["P.J. Goulart", "E.C. Kerrigan", "J.M. Maciejowski"], "venue": "Automatica, 42(4),", "citeRegEx": "Goulart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goulart et al\\.", "year": 2006}, {"title": "Robust execution of temporally flexible plans for bipedal walking devices", "author": ["A.G. Hofmann", "B.C. Williams"], "venue": "In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS-06)", "citeRegEx": "Hofmann and Williams,? \\Q2006\\E", "shortCiteRegEx": "Hofmann and Williams", "year": 2006}, {"title": "Optimal stochastic linear systems with exponential performance criteria and their relation to deterministic differential games", "author": ["D. Jacobson"], "venue": "Automatic Control, IEEE Transactions on, 18(2), 124 \u2013 131.", "citeRegEx": "Jacobson,? 1973", "shortCiteRegEx": "Jacobson", "year": 1973}, {"title": "HTV-1 mission press kit", "author": ["Japan Aerospace Exploration Agency"], "venue": "Available on-line at http: //www.jaxa.jp/countdown/h2bf1/pdf/presskit_htv_e.pdf.", "citeRegEx": "Agency,? 2009", "shortCiteRegEx": "Agency", "year": 2009}, {"title": "Cooperative distributed robust trajectory optimization using receding horizon MILP", "author": ["Y. Kuwata", "J.P. How"], "venue": "IEEE Transactions on Control Systems Technology,", "citeRegEx": "Kuwata and How,? \\Q2011\\E", "shortCiteRegEx": "Kuwata and How", "year": 2011}, {"title": "Real-time trajectory design for unmanned aerial vehicles using receding horizon control", "author": ["Y. Kuwata"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology.", "citeRegEx": "Kuwata,? 2003", "shortCiteRegEx": "Kuwata", "year": 2003}, {"title": "Talplanner: A temporal logic based forward chaining planner", "author": ["J. Kvarnstrom", "P. Doherty"], "venue": "Annals of Mathematics and Artificial Intelligence", "citeRegEx": "Kvarnstrom and Doherty,? \\Q2000\\E", "shortCiteRegEx": "Kvarnstrom and Doherty", "year": 2000}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Coordinating agile systems through the model-based execution of temporal plans", "author": ["T. L\u00e9aut\u00e9"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology.", "citeRegEx": "L\u00e9aut\u00e9,? 2005", "shortCiteRegEx": "L\u00e9aut\u00e9", "year": 2005}, {"title": "Coordinating agile systems through the model-based execution of temporal plans", "author": ["T. L\u00e9aut\u00e9", "B.C. Williams"], "venue": "In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "L\u00e9aut\u00e9 and Williams,? \\Q2005\\E", "shortCiteRegEx": "L\u00e9aut\u00e9 and Williams", "year": 2005}, {"title": "Generalized conflict learning for hybrid discrete linear optimization", "author": ["H. Li", "B.C. Williams"], "venue": "In Proc. 11th International Conf. on Principles and Practice of Constraint Programming", "citeRegEx": "Li and Williams,? \\Q2005\\E", "shortCiteRegEx": "Li and Williams", "year": 2005}, {"title": "Kongming: A Generative Planner for Hybrid Systems with Temporally Extended Goals", "author": ["H.X. Li"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Li,? 2010", "shortCiteRegEx": "Li", "year": 2010}, {"title": "Fly-by approach and guidance for uncontrolled rotating satellite capture", "author": ["S. Matsumoto", "S. Dubowsky", "S. Jacobsen", "Y. Ohkami"], "venue": "In Proceedings of AIAA Guidance,", "citeRegEx": "Matsumoto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Matsumoto et al\\.", "year": 2003}, {"title": "Convex approximations of chance constrained programs", "author": ["A. Nemirovski", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski and Shapiro,? \\Q2006\\E", "shortCiteRegEx": "Nemirovski and Shapiro", "year": 2006}, {"title": "A tractable approximation of chance constrained stochastic MPC based on affine disturbance feedback", "author": ["F. Oldewurtel", "C.N. Jones", "M. Morari"], "venue": "In Proceedings of Conference on Decision and Control", "citeRegEx": "Oldewurtel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oldewurtel et al\\.", "year": 2008}, {"title": "Closed-loop chance-constrained MPC with probabilistic resolvability", "author": ["M. Ono"], "venue": "Proceedings of the IEEE Conference on Decision and Control.", "citeRegEx": "Ono,? 2012", "shortCiteRegEx": "Ono", "year": 2012}, {"title": "Risk-sensitive plan execution for connected sustainable home", "author": ["M. Ono", "W. Graybill", "B.C. Williams"], "venue": "In Proceedings of the 4th ACM Workshop On Embedded Systems (BuildSys)", "citeRegEx": "Ono et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ono et al\\.", "year": 2012}, {"title": "An efficient motion planning algorithm for stochastic dynamic systems with constraints on probability of failure", "author": ["M. Ono", "B.C. Williams"], "venue": "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI-08)", "citeRegEx": "Ono and Williams,? \\Q2008\\E", "shortCiteRegEx": "Ono and Williams", "year": 2008}, {"title": "Iterative risk allocation: A new approach to robust model predictive control with a joint chance constraint", "author": ["M. Ono", "B.C. Williams"], "venue": "In Proceedings of 47th IEEE Conference on Decision and Control", "citeRegEx": "Ono and Williams,? \\Q2008\\E", "shortCiteRegEx": "Ono and Williams", "year": 2008}, {"title": "The use of discrete moment bounds in probabilistic constrained stochastic programming models", "author": ["A. Pr\u00e9kopa"], "venue": "Annals of Operations Research, 85, 21\u201338.", "citeRegEx": "Pr\u00e9kopa,? 1999", "shortCiteRegEx": "Pr\u00e9kopa", "year": 1999}, {"title": "Robust stable model predictive control with constraint tightening", "author": ["A. Richards", "J. How"], "venue": "In American Control Conference,", "citeRegEx": "Richards and How,? \\Q2006\\E", "shortCiteRegEx": "Richards and How", "year": 2006}, {"title": "Relational dynamic influence diagram language (RDDL): Language description", "author": ["S. Sanner"], "venue": "Available at http://users.cecs.anu.edu.au/ \u0303ssanner/IPPC_2011/RDDL. pdf.", "citeRegEx": "Sanner,? 2011", "shortCiteRegEx": "Sanner", "year": 2011}, {"title": "Analytical mechanics of space systems", "author": ["H. Schaub", "J.L. Junkins"], "venue": "American Institute of Aeronautics and Astronautics,", "citeRegEx": "Schaub and Junkins,? \\Q2003\\E", "shortCiteRegEx": "Schaub and Junkins", "year": 2003}, {"title": "Metrology sensor characterization and pointing control for the formation interferometer testbed (fit)", "author": ["J. Shields", "S. Sirlin", "M. Wette"], "venue": "In Proceedings of IEEE Aerospace Conference", "citeRegEx": "Shields et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shields et al\\.", "year": 2002}, {"title": "Distributed estimation, communication and control for deep space formations", "author": ["R. Smith", "F. Hadaegh"], "venue": "IET Control Theory and Applications", "citeRegEx": "Smith and Hadaegh,? \\Q2007\\E", "shortCiteRegEx": "Smith and Hadaegh", "year": 2007}, {"title": "The H\u221e Control Problem: A State Space Approach", "author": ["A. Stoorvogel"], "venue": "Prentice Hall. Vallado, D. A. (2001). Fundamentals of Astrodynamics and Applications, Second Edition. Microcosm Press.", "citeRegEx": "Stoorvogel,? 1992", "shortCiteRegEx": "Stoorvogel", "year": 1992}, {"title": "Stochastic inequality constrained closed-loop model predictive control with application to chemical process operation", "author": ["D.H. van Hessem"], "venue": "Ph.D. thesis,", "citeRegEx": "Hessem,? \\Q2004\\E", "shortCiteRegEx": "Hessem", "year": 2004}, {"title": "Cooperative uav formation flying with obstacle/collision avoidance", "author": ["X. Wang", "V. Yadav", "S.N. Balakrishnan"], "venue": "IEEE Transactions on Control Systems", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Space Mission Analysis and Design (Third Edition). Microcosm/Springer", "author": ["J.R. Wertz", "Wiley J. Larson"], "venue": null, "citeRegEx": "Wertz et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wertz et al\\.", "year": 1999}, {"title": "PPDDL1.0: An extension to pddl for expressing planning domains with probabilistic effects", "author": ["H.L.S. Younes", "M.L. Littman"], "venue": null, "citeRegEx": "Younes and Littman,? \\Q2004\\E", "shortCiteRegEx": "Younes and Littman", "year": 2004}], "referenceMentions": [{"referenceID": 33, "context": "Journal of Artificial Intelligence Research 46 (2013) 511-577 Submitted 12/12; published 03/13", "startOffset": 27, "endOffset": 54}, {"referenceID": 27, "context": "This is a stochastic extension of the continuous plant model used by L\u00e9aut\u00e9 and Williams (2005). In this paper we limit our focus to Gaussian-distributed uncertainty.", "startOffset": 69, "endOffset": 96}, {"referenceID": 27, "context": "This is a stochastic extension of the continuous plant model used by L\u00e9aut\u00e9 and Williams (2005). In this paper we limit our focus to Gaussian-distributed uncertainty. Chance-constrained qualitative state plan (CCQSP) In order to provide users with an intuitive way to command stochastic systems, we develop a new plan representation called a chanceconstrained qualitative state plan (CCQSP). It is an extension of qualitative state plan (QSP), developed and used by L\u00e9aut\u00e9 and Williams (2005), Hofmann and Williams (2006), and Blackmore, Li, and Williams (2006).", "startOffset": 69, "endOffset": 493}, {"referenceID": 21, "context": "It is an extension of qualitative state plan (QSP), developed and used by L\u00e9aut\u00e9 and Williams (2005), Hofmann and Williams (2006), and Blackmore, Li, and Williams (2006).", "startOffset": 102, "endOffset": 130}, {"referenceID": 8, "context": "It is an extension of qualitative state plan (QSP), developed and used by L\u00e9aut\u00e9 and Williams (2005), Hofmann and Williams (2006), and Blackmore, Li, and Williams (2006). CCQSP specifies a desired evolution of the plant state over time, and is defined by a set of discrete events, a set of episodes, which impose constraints on the plant state evolution, a set of temporal constraints between events, and a set of chance constraints that specify reliability constraints on the success of sets of episodes in the plan.", "startOffset": 135, "endOffset": 170}, {"referenceID": 33, "context": "TLPlan uses a version of metric interval temporal logic (MITL) (Alur, Feder, & Henzinger, 1996) applied to discrete states, while the p-Sulu Planner uses qualitative state plans (QSPs) (L\u00e9aut\u00e9 & Williams, 2005; Hofmann & Williams, 2006; Li, 2010) over continuous states.", "startOffset": 185, "endOffset": 246}, {"referenceID": 33, "context": "Kongming (Li, 2010) provides a generative planning capability for hybrid", "startOffset": 9, "endOffset": 19}, {"referenceID": 30, "context": "TLPlan uses a version of metric interval temporal logic (MITL) (Alur, Feder, & Henzinger, 1996) applied to discrete states, while the p-Sulu Planner uses qualitative state plans (QSPs) (L\u00e9aut\u00e9 & Williams, 2005; Hofmann & Williams, 2006; Li, 2010) over continuous states. Li (2010) shows that, for a given state space, any QSP can be expressed in MITL.", "startOffset": 186, "endOffset": 281}, {"referenceID": 43, "context": "Probabilistic PDDL (Younes & Littman, 2004) and the Relational Dynamic influence Diagram Language (RDDL) (Sanner, 2011) can handle stochastic systems.", "startOffset": 105, "endOffset": 119}, {"referenceID": 2, "context": "The constrained MDP approach (Altman, 1999) can explicitly impose constraints.", "startOffset": 29, "endOffset": 43}, {"referenceID": 29, "context": "These planners adapt to the effects of uncertainty, but do not explicitly reason about the effects of uncertainty during planning. For example, Sulu employs a receding horizon approach, which continuously replans the control sequence using the latest measurements. Chekhov\u2019s flow tube representation of feasible policies allows the executive to generate new control sequences in response to disturbances on-line. The p-Sulu Planner is distinct from these continuous planners in that it plans with a model of uncertainty in dynamics, instead of just reacting to it. Its plan guarantees the user-specified probability of success by explicitly reasoning about the effects of uncertainty. In AI planning literatures, a planning domain description language, PDDL+, supports mixed discrete-continuous planning domains (Fox& Long, 2006). Probabilistic PDDL (Younes & Littman, 2004) and the Relational Dynamic influence Diagram Language (RDDL) (Sanner, 2011) can handle stochastic systems. Recently, Coles, Coles, Fox, and Long (2012) developed a forward-chaining heuristic search planner named COLIN, which can deal with continuous linear change and durationdependent effects.", "startOffset": 66, "endOffset": 1027}, {"referenceID": 9, "context": "The time-dependent MDP developed by Boyan and Littman (2000) can handle continuous time by encoding time in the state.", "startOffset": 36, "endOffset": 61}, {"referenceID": 2, "context": "The constrained MDP approach (Altman, 1999) can explicitly impose constraints. Dolgov and Durfee (2005) showed that stationary deterministic policies for constrained MDPs can be obtained by solving a mixed integer linear program (MILP).", "startOffset": 30, "endOffset": 104}, {"referenceID": 47, "context": "For example, the celebrated H\u221e control method minimizes the effect of disturbances on the output of a system while guaranteeing the stability of the system (Stoorvogel, 1992).", "startOffset": 156, "endOffset": 174}, {"referenceID": 24, "context": "Risk-sensitive control approaches allow users to choose the level of risk averseness through the minimization of an expected exponentiated cost function (Jacobson, 1973; Fleming & McEneaney, 1995).", "startOffset": 153, "endOffset": 196}, {"referenceID": 19, "context": "As far as the authors know, the risk-sensitive reinforcement learning approach proposed by Geibel and Wysotzki (2005) is the only work that considers chance constraints in the MDP framework.", "startOffset": 91, "endOffset": 118}, {"referenceID": 19, "context": "As far as the authors know, the risk-sensitive reinforcement learning approach proposed by Geibel and Wysotzki (2005) is the only work that considers chance constraints in the MDP framework. They developed a reinforcement learning algorithm for MDPs with a constraint on the probability of entering error states. Our work is distinct from theirs in that the p-Sulu Planner is goal-directed, by which we mean that it achieves time-evolved goals within user-specified temporal constraints. To summarize, no prior MDP work supports continuous state and actions in combination with general continuous noise on transitions while ensuring that the probability of failure is bounded. Risk-sensitive control methods in a continuous domain have been extensively studied in the discipline of control theory. For example, the celebrated H\u221e control method minimizes the effect of disturbances on the output of a system while guaranteeing the stability of the system (Stoorvogel, 1992). Risk-sensitive control approaches allow users to choose the level of risk averseness through the minimization of an expected exponentiated cost function (Jacobson, 1973; Fleming & McEneaney, 1995). However, these approaches do not address chance constraints and optimal scheduling. Several methods have been proposed for solving stochastic optimal control problems over continuous variables with chance constraints. The method proposed by van Hessem (2004) turns a stochastic problem into a deterministic problem using a very conservative ellipsoidal relaxation.", "startOffset": 91, "endOffset": 1431}, {"referenceID": 8, "context": "Blackmore (2006) proposes a sampling-based method called Particle Control, which evaluates joint chance constraints by a Monte-Carlo simulation, instead of using a conservative bound.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Blackmore (2006) proposes a sampling-based method called Particle Control, which evaluates joint chance constraints by a Monte-Carlo simulation, instead of using a conservative bound. As a result, the stochastic planning problem is reduced to a MILP problem. Although it has a theoretical guarantee that it can obtain the exactly optimal solution when an infinite number of samples are used, computation time is an issue. Blackmore et al. (2006) and Nemirovski and Shapiro (2006) employed Boole\u2019s inequality to decompose a joint chance constraint into individual chance constraints.", "startOffset": 0, "endOffset": 446}, {"referenceID": 8, "context": "Blackmore (2006) proposes a sampling-based method called Particle Control, which evaluates joint chance constraints by a Monte-Carlo simulation, instead of using a conservative bound. As a result, the stochastic planning problem is reduced to a MILP problem. Although it has a theoretical guarantee that it can obtain the exactly optimal solution when an infinite number of samples are used, computation time is an issue. Blackmore et al. (2006) and Nemirovski and Shapiro (2006) employed Boole\u2019s inequality to decompose a joint chance constraint into individual chance constraints.", "startOffset": 0, "endOffset": 480}, {"referenceID": 6, "context": "Such a feedback gain can be found by using standard control techniques, such as a linear quadratic regulator (LQR) (Bertsekas, 2005).", "startOffset": 115, "endOffset": 132}, {"referenceID": 36, "context": "This approach is often used for robust and stochastic model predictive controls (Goulart, Kerrigan, & Maciejowski, 2006; Oldewurtel et al., 2008; Ono, 2012).", "startOffset": 80, "endOffset": 156}, {"referenceID": 37, "context": "This approach is often used for robust and stochastic model predictive controls (Goulart, Kerrigan, & Maciejowski, 2006; Oldewurtel et al., 2008; Ono, 2012).", "startOffset": 80, "endOffset": 156}, {"referenceID": 20, "context": "(8) A closed-loop control approach has been employed by Geibel and Wysotzki (2005) and Oldewurtel et al.", "startOffset": 56, "endOffset": 83}, {"referenceID": 20, "context": "(8) A closed-loop control approach has been employed by Geibel and Wysotzki (2005) and Oldewurtel et al. (2008) in the context of chance-constrained optimal control and shown that it significantly improves performance.", "startOffset": 56, "endOffset": 112}, {"referenceID": 16, "context": "Temporal constraint A CCQSP includes simple temporal constraints (STCs) (Dechter et al., 1991), which impose upper and lower bounds on the duration of episodes and on the temporal distances between two events in E .", "startOffset": 72, "endOffset": 94}, {"referenceID": 21, "context": "A similar problem encoding is also employed in the chance-constraint MDP proposed by Geibel and Wysotzki (2005). However, our encoding differs from Geibel and Wysotzki in two respects: 1) we optimize not only the continuous control sequence u0:N\u22121 but also the discrete schedule s with temporal constraints; 2) we allow joint chance constraints, which require the satisfaction of multiple state constraints for a given probability.", "startOffset": 85, "endOffset": 112}, {"referenceID": 37, "context": "The reformulation was initially presented by Pr\u00e9kopa (1999) and introduced to chance-constrained optimal control by Ono and Williams (2008b).", "startOffset": 45, "endOffset": 60}, {"referenceID": 33, "context": "The reformulation was initially presented by Pr\u00e9kopa (1999) and introduced to chance-constrained optimal control by Ono and Williams (2008b). The concept of risk allocation was originally developed by Ono and Williams (2008a).", "startOffset": 127, "endOffset": 141}, {"referenceID": 33, "context": "The reformulation was initially presented by Pr\u00e9kopa (1999) and introduced to chance-constrained optimal control by Ono and Williams (2008b). The concept of risk allocation was originally developed by Ono and Williams (2008a). Let Ci be a proposition that is either true or false.", "startOffset": 127, "endOffset": 226}, {"referenceID": 33, "context": "This completes the proof of Lemma 3 We note that Lemma 3 is a probabilistic extension of the closed-loop robust model predictive control (RMPC) methods proposed by Acikmese, Carson III, and Bayard (2011) and Richards and How (2006).", "startOffset": 69, "endOffset": 204}, {"referenceID": 33, "context": "This completes the proof of Lemma 3 We note that Lemma 3 is a probabilistic extension of the closed-loop robust model predictive control (RMPC) methods proposed by Acikmese, Carson III, and Bayard (2011) and Richards and How (2006). These methods avoid the risk of actuator saturation by imposing tightened control constraints on \u016bt.", "startOffset": 69, "endOffset": 232}, {"referenceID": 8, "context": "Furthermore, Blackmore and Ono (2009) showed that an optimal solution to Problem 5 is a near-optimal solution to Problem 4.", "startOffset": 13, "endOffset": 38}, {"referenceID": 5, "context": "This approach was used by Balas (1979) and Li andWilliams (2005) for a different problem known as disjunctive linear programming, whose subproblems are LPs instead of convex programmings.", "startOffset": 26, "endOffset": 39}, {"referenceID": 5, "context": "This approach was used by Balas (1979) and Li andWilliams (2005) for a different problem known as disjunctive linear programming, whose subproblems are LPs instead of convex programmings.", "startOffset": 26, "endOffset": 65}, {"referenceID": 15, "context": "Figure 11-(a) is the distance graph representation of the simple temporal constraints (Dechter, 2003) of the CCQSP.", "startOffset": 86, "endOffset": 101}, {"referenceID": 16, "context": "The d-graph (Figure 11-(b)) is obtained from the distance graph (Figure 11-(a)) by running an all-pair shortest-path algorithm (Dechter et al., 1991).", "startOffset": 127, "endOffset": 149}, {"referenceID": 15, "context": "It is shown by Dechter et al. (1991) that the set of feasible execution times for an event e is bounded by the distance between e and e0 on the dgraph.", "startOffset": 15, "endOffset": 37}, {"referenceID": 15, "context": "Dechter et al. (1991) showed that dmax e\u2032 (\u03c3) corresponds to the upper bound of the feasible execution time for an unassigned event e\u2032, while dmin eE (\u03c3) corresponds to the negative of the lower bound.", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "This plant model is commonly assumed in literatures on unmanned aerial vehicle (UAV) path planning (Kuwata & How, 2011; L\u00e9aut\u00e9, 2005; Wang, Yadav, & Balakrishnan, 2007).", "startOffset": 99, "endOffset": 168}, {"referenceID": 9, "context": "Fixed risk allocation (Blackmore et al., 2006) - This approach fixes the risk allocation to a uniform value.", "startOffset": 22, "endOffset": 46}, {"referenceID": 8, "context": "Particle Control (Blackmore, 2006) - Particle Control is a sampling-based method, which uses a finite number of samples to approximate the joint chance constraints.", "startOffset": 17, "endOffset": 34}, {"referenceID": 27, "context": "This issue can be addressed by a constraint-tightening method (Kuwata, 2003).", "startOffset": 62, "endOffset": 76}, {"referenceID": 33, "context": "3 SCALABILITY ANALYSIS In this subsection we conduct an empirical analysis of the scalability of the p-Sulu Planner, as the environment becomes increasingly constrained.. As shown in Figure 20, we measured the computation time to solve a path planning problem with different numbers of obstacles and waypoints. In all simulations, the path starts at [0, 12] and ends in a square region centered at [24, 12]. Figure 20 shows twenty simulation results, with zero to three obstacles and zero to four waypoints. Obstacles and waypoints are represented by blue and red squares in the figure, respectively. The positions of the center of the obstacles are [6, 12], [12, 12], and [18, 12], while the positions of the center of the waypoints are [9, 9], [9, 15], [15, 15], and [15, 9]. The computation time is shown in the caption of each subfigure in Figure 20. By comparing the results in Figure 20 horizontally, we observe exponential growth in computation time with the number of obstacles. This result is expected since the number of disjunctive clauses in the state constraint of the p-Sulu Planner increases exponentially with the number of obstacles. Building a tractable extension of the p-Sulu Planner for a large number of obstacles is future work. On the other hand, by comparing the results vertically, we find that the computation time with the same number of obstacles and different number of waypoints stays in the same order of magnitude. This is because adding an extra waypoint only increases the number of conjunctive clauses in the state constraints. In the remaining sections we describe the application of psulu to two real world problems, air vehicle and space vehicle control. A third application, building energy management, using a variant of the p-Sulu Planner, is reported by Ono, Graybill, and Williams (2012).", "startOffset": 9, "endOffset": 1832}, {"referenceID": 25, "context": "Please refer to the report by Japan Aerospace Exploration Agency (2009) for the details of the rendezvous sequence.", "startOffset": 58, "endOffset": 72}], "year": 2013, "abstractText": "This paper presents a model-based planner called the Probabilistic Sulu Planner or the p-Sulu Planner, which controls stochastic systems in a goal directed manner within user-specified risk bounds. The objective of the p-Sulu Planner is to allow users to command continuous, stochastic systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. To this end, we first develop a new plan representation called a chance-constrained qualitative state plan (CCQSP), through which users can specify the desired evolution of the plant state as well as the acceptable level of risk. An example of a CCQSP statement is \u201cgo to A through B within 30 minutes, with less than 0.001% probability of failure.\u201d We then develop the p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to enable CCQSP planning, we develop the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a continuous domain with temporal constraints. The first capability is to ensures that the probability of failure is bounded. The second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning. We demonstrate the capabilities of the p-Sulu Planner by simulations on two real-world scenarios: the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft.", "creator": " TeX output 2013.09.27:1252"}}}