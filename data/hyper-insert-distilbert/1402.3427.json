{"id": "1402.3427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2014", "title": "Indian Buffet Process Deep Generative Models", "abstract": "denoising hierarchical autoencoders ( see das ) extensively are typically better applied broadly to relatively large datasets or for unsupervised hypothesis learning of degraded representative data encodings ; further they rely positively on the idea that of naturally making the residual learned voting representations robust to partial filtered corruption of altering the reconstructed input pattern, and perform learning investigations using stochastic matrix gradient descent optimization with relatively low large selective datasets. in presenting this paper, we quickly present a fully bayesian independent da filter architecture that allows improvement for the application of das even problems when quantitative data is scarce. our largely novel naive approach formulates the corrupted signal encoding problem under a certain nonparametric naive bayesian proportional regard, considering a gaussian process polynomial prior over the latent input results encodings generated alternatively given the ( most corrupt ) filtered input observations. improving subsequently, the decoder problem modules of our new model are largely formulated as large - gradient margin regression gradient models, treated under merely the bayesian feedback inference paradigm, by successfully exploiting the differential maximum entropy discrimination ( med ) framework. nevertheless we exhibit the effectiveness approach of expanding our traditional approach via using several datasets, dealing together with both classification and linear transfer learning applications.", "histories": [["v1", "Fri, 14 Feb 2014 10:44:48 GMT  (189kb)", "http://arxiv.org/abs/1402.3427v1", null], ["v2", "Sun, 30 Aug 2015 18:41:41 GMT  (0kb,I)", "http://arxiv.org/abs/1402.3427v2", "This paper has been withdrawn by the author due to errors in the experiments (software bugs)"], ["v3", "Fri, 8 Jul 2016 19:37:54 GMT  (124kb,D)", "http://arxiv.org/abs/1402.3427v3", null], ["v4", "Sat, 16 Jul 2016 14:11:47 GMT  (124kb,D)", "http://arxiv.org/abs/1402.3427v4", null], ["v5", "Sun, 6 Aug 2017 21:27:48 GMT  (245kb,D)", "http://arxiv.org/abs/1402.3427v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sotirios p chatzis"], "accepted": false, "id": "1402.3427"}, "pdf": {"name": "1402.3427.pdf", "metadata": {"source": "CRF", "title": "Maximum Entropy Discrimination Denoising Autoencoders", "authors": ["Sotirios P. Chatzis"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n34 27\nv1 [\ncs .L\nG ]\n1 4\nFe b\n20 14\nIndex Terms\u2014Autonencoders, maximum entropy discrimination, meanfield, Gaussian process."}, {"heading": "1 INTRODUCTION", "text": "It has recently become obvious that deep architectures allow for obtaining better modeling and representational capacities in the context of challenging pattern recognition applications [1]. Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5]. The stunning performance of these approaches in a multitude of applications is largely attributed to the use of an unsupervised training criterion to perform a layerwise initialization: each layer is at first trained to produce a higher level (hidden) representation of the observed patterns, based on the representation it receives as input from the layer below, by optimizing a local unsupervised criterion. After initialization, global fine-tuning of the model parameters may be performed by means of an appropriate training criterion. This way of training allows these deep architectures to avoid getting stuck to poor local optima, despite their complexity, contrary to most previous deep neural network architectures (e.g., [6]). In this work, we focus on autoencoder models [7]; specifically, we are interested in a recent variation of this method, namely denoising autoencoders [8]. Traditional autoencoders (AEs) are unsupervised feature extractors designed to retain at minimum a certain amount of \u201cinformation\u201d about their input, while at the same time being constrained to a given form for\ntheir output (e.g. a real-valued vector of a given size). Denoising autoencoders (DAs) are based on the observation that humans can recognize objects in partially occluded or corrupted images: this leads to the intuitive concept that an effective extracted representation should be capable of capturing stable patterns in the modeled high-dimensional data from partial observation only. Based on this intuition, DAs are designed to satisfy the additional criterion of robustness of the extracted representation to partial destruction of the input; that is, they are optimized to reconstruct input data from partial and random corruption. DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2]. Existing DA architectures are essentially based on simple feedforward neural network principles. As such, their training algorithms consist in obtaining pointestimates of the model parameters by minimizing a suitable reconstruction error criterion. A repercussion of this design choice is that DAs and their deep learning counterparts, i.e. stacked DAs (SDAs), require large amounts of data to perform learning [10]. This problem is essentially a result of the fact that obtaining point-estimates of the entailed parameters is a frequentist method [11] that works well only under the condition of training using relatively large datasets. However, we know that humans are able to perform inductive reasoning (equivalent to concept generalization) with only a few examples [12]. As such, it is interesting to consider whether DA models can be reformulated in such a way that allows for performing learning using scarce datasets. To address this issue, in this paper we consider a fully Bayesian treatment of DA networks. Contrary to frequentist methods that consider model parameters to be fixed, and the training data to be some random sample from an infinite number of existing but unobserved data points, our Bayesian treatment is designed under the assumption of dealing with fixed scarce data and parameters that are random because they are unknowns. By imposing a suitable prior over model parameters, that encodes prior knowledge or results of a previous model, and obtaining a corresponding posterior distribution based on the given observed data, our Bayesian inference\n2 approach allows for including uncertainty in learning the model and generating much better encodings (when dealing with limited and scarce datasets). Recently, several researchers have attempted to exploit the merits of Bayesian inference in the context of parametric unsupervised feature extraction methodologies. However, in most cases, these approaches consist in merely merging supervised nonparametric Bayesian prediction techniques with existing parametric unsupervised learning methodologies. For example, [13] use restricted Boltzmann machines (RBMs) to extract features in an unsupervised manner, which are subsequently fed as inputs to the covariance kernel of a Gaussian process (GP) regressor or classifier [14]. On this basis, after initial RBM pretraining, they adjust the RBM parameters by backpropagating gradients from the GP through the neural network. As we observe, in this approach the employed nonparametric Bayesian model is only used for the purposes of inference at test time, and is not utilized to perform initial learning of the generated latent representations, which are still learned using a conventional approach yielding point-estimates. Similarly, [15] consider a classification model that imposes GP priors on the discriminative function that maps the latent encodings into class labels. The result of this choice is a Gaussian process latent variable model (GPLVM) [16] for the predicted class labels. While much more efficient compared to the approach of [13], neither does this method utilize the component nonparametric Bayesian model to perform learning of the generated input encodings. Further, [17] combined autoencoder training with neighborhood component analysis [18], which encouraged the model to encode similar latent representations for inputs belonging to the same class. Finally, [19] recently proposed a deep Gaussian process (DGP) model. DGP is essentially a deep belief network (DBN) [2], [3] the component models of each layer of which are Gaussian processes instead of RBMs. Inference is performed using an approximate variational marginalization technique. As such, both latent encodings and classification predictions are obtained in a fully Bayesian fashion. In this work, we follow a different path; motivated by the aforementioned advantages of (hierarchical) Bayesian model formulations, in this paper, for the first time in the literature, we present a fully Bayesian formulation of DA models, designed for performing learning when data is scarce. Specifically, we consider a nonlinear encoder module, obtained by imposing a Gaussian process prior [14] over the latent encodings of the observed (corrupt) data. This formulation of the postulated encoders allows for extracting much more complex underlying patterns from the observed data compared to simpler architectures. Further, we consider a linear decoder module for our model with appropriate priors imposed over its parameters. Contrary to existing approaches, in our model the used decoders leverage the large-margin principle to learn the function mapping the obtained latent representations (encodings) to the original data presented to our model. Application of the large-margin learning principle allows for obtaining a more discriminative learning technique, making more effective use of our training data during estimation of the postulated latent data representations. To introduce the large-margin learning principle in the context of our hierarchical Bayesian model, we build upon the maximum entropy discrimination (MED) framework [20], [21]. The MED framework integrates the large-margin principle with Bayesian posterior inference in an elegant and computationally efficient fashion, allowing to leverage existing high-performance techniques for both hierarchical Bayesian models and support vector machines (SVMs). Adoption of the MED framework in the context of our model is performed by optimizing a composite objective function that takes into consideration both the (negative) log-likelihood of our hierarchical Bayesian model, which measures the goodness of fit to the training data, and a measure of reconstruction error on training data. On this basis, we derive a mean-field inference algorithm, which obtains a regularized posterior distribution over the latent encodings of the observed data in a feasible space defined by a set of expected margin constraints generalized from the familiar SVM-style margin constraints. We dub our model as the maximum-entropy discrimination denoising autoencoder (MED2A) model. The remainder of this paper is organized as follows: In Section 2, we provide a brief review of the main concept behind traditional autoencoders and their denoising variants. In Section 3, we introduce our method, and derive its inference algorithm. In Section 4, we conduct the experimental evaluation of our approach. Finally, in the last section, we summarize and discuss our results, and conclude this work."}, {"heading": "2 AUTOENCODER NEURAL NETWORKS", "text": ""}, {"heading": "2.1 Traditional autoencoders", "text": "A traditional autoencoder comprises two distinct parts: an encoder and a decoder. The encoder is a deterministic mapping of a d-dimensional input vector x to a hidden representation y. Usually, it is taken as an affine transform followed by a squashing non-linearity, of the form y , f(x; \u03b8) = \u03c3(Wx+ b) (1) where W \u2208 Rd \u2032\u00d7d, b \u2208 Rd \u2032\u00d71, and \u03b8 = {W , b}. From the resulting representation y, we can reconstruct its corresponding vector in the considered d-dimensional input space by using the decoder of the AE. Specifically, the obtained input reconstruction z is again derived from an affine mapping, optionally followed by a squashing non-linearity, and reads z , g(y; \u03b8\u2032) = \u03c3(W \u2032y + b\u2032) (2) whereW \u2032 \u2208 Rd\u00d7d \u2032 and b\u2032 \u2208 Rd\u00d71, and \u03b8\u2032 = {W \u2032, b\u2032}. The obtained reconstructions z are generally regarded as an\n3 approximation of the actual corresponding input vectors x. Typically, they are taken as the mode of a considered distribution p(x|z), which is learnt in an unsupervised way through model training. This, in turn, gives rise to an associated reconstruction error minimized through model training, that reads [8]\nL(x, z) \u221d \u2212log p(x|z) (3)\nIn cases of real-valued observations x, the distribution p(x|z) is typically taken to be of Gaussian form, reading:\np(x|z) = N (x|z, \u03c32I) (4)\nwhich, essentially, gives rise to a squared reconstruction error loss objective function L(x, z). Similarly, in cases of binary observations x, or x \u2208 [0, 1]d, it is usually considered that\np(x|z) = Bernoulli(x|z) (5)\nwhere Bernoulli(x|z) = \u220fd i=1 z xi i (1 \u2212 zi) 1\u2212xi . This, in turn, gives rise to a cross-entropy reconstruction error loss L(x, z). As already mentioned, AE training consists in minimizing the reconstruction loss over the whole observations space. In other words, it consists in minimizing the expected reconstruction loss, Eq(X)[L(x, z)], where q(X) is the actual distribution of the observed data. Since, this latter quantity is hardly ever known, an approximate solution is obtained by minimizing the empirical expectation given a set of training data D = {xn}Nn=1. In other words, AE training reduces to the optimization problem\nmin \u03b8,\u03b8\u2032\nN \u2211\nn=1\n[L(xn, g(f(xn; \u03b8); \u03b8 \u2032))] (6)\nwhich, effectively, aims at retaining as much of the information that was present in the input as possible."}, {"heading": "2.2 Denoising autoencoders", "text": "It has been shown that the optimal reconstruction criterion of AE cannot guarantee extraction of useful features in several occasions. Specifically, it has been shown that it may lead to the solution of just copying the input to the output or other trivial ones that yield very low reconstruction error in the training set combined with extremely poor modeling and generalization performance [8]. Utilizing sparse representation strategies that impose dimensionally constraints on the obtained representation is one solution towards the amelioration of this issue (e.g., [7]). Denoising autoencoders address this overfitting problem in a different way [8]. Instead of imposing constraints, DAs modify the employed reconstruction criterion, stipulating that it also denoises partially corrupted input. This approach is based on the assumption that a higher level representation should be rather stable and robust under corruptions of the input, and that such noise-robust features should capture useful structure in\nthe input distribution. This intuition leads to the training criterion of DAs, which comprises reconstructing a clean \u201crepaired\u201d input from a corrupted version of it. For this purpose, initially the inputs x used for DA training get corrupted by means of a stochastic mapping (corruption process) x\u0303 \u223c q(x\u0303|x). Subsequently, the corrupted inputs x\u0303 are mapped to a hidden representation y, as in the case of a traditional AE; that is\ny , f(x\u0303; \u03b8) = \u03c3(Wx\u0303+ b) (7)\nEventually, DAs try to reconstruct the initial, uncorrupted version x of their training inputs x\u0303; these reconstructions z are obtained by means of (2). Therefore, in DAs z becomes a deterministic function of the corrupted signals x\u0303, instead of the original ones x they try to reconstruct.\nCorruption process selection. Some simple corruption process models often used in the literature are [8]: (i) additive isotropic Gaussian noise (GS), i.e., q(x\u0303|x) = N (x\u0303|x, \u03c32\u01ebI); (ii) masking noise (MN), which forces to zero a randomly selected fraction of the elements in x; and, (iii) salt-and-pepper noise (SP), which forces a randomly selected fraction of the elements in x to their minimum or maximum value (0 or 1), according to a fair coin flip.\nUsing DAs to Build Deep Architectures. Initializing deep architectures by stacking DAs is performed in a way fairly similar to traditional AEs [4], [7]. Training proceeds by layerwisely training the stacked DAs, beginning from the lowest layer DA (which is fed with the original observations)."}, {"heading": "3 PROPOSED APPROACH", "text": "Let us assume a DA model with N -dimensional inputs fed to it, and expected to generate C-dimensional outputs. We are seeking a fully Bayesian treatment of this model, capable of obtaining competitive performance when data is scarce. Let us denote as Y = {yd\u00b7} D d=1, where yd\u00b7 = [ydn] N n=1, the set of the (original) N -dimensional observations presented to our model, and as X = {xd\u00b7}Dd=1, where xd\u00b7 = [xdn] N n=1, the set of their corresponding corrupt versions, obtained by transforming the original data by employing a suitable corrupting distribution. Let us begin with the formulation of the encoder of our model: to obtain this module, we elect to impose a GP prior, with input variables {xd\u00b7} D d=1, over the latent encodings generated by our model, say {zd\u00b7}Dd=1, where zd\u00b7 = [zdc] C c=1. A drawback of such a modeling approach consists in the fact that performing inference for GP models entails inversion of the gram matrix of the input data [22], resulting in an O(D3) complexity, which is rather inefficient. To ameliorate this issue, we resort to utilization of a popular sparse pseudo-input Gaussian process (SPGP) modeling approach [23]: We impose a prior over the latent encodings {zd\u00b7}Dd=1 taking the form of a GP predictive distribution parameterized by a small pseudodataset X\u0304 = {x\u0304m\u00b7} M m=1, with corresponding latent encoding values {z\u0304m\u00b7}Mm=1 (pseudo-encodings). In other\n4 words, we begin by considering a set of simple GP priors over the pseudo-encodings\np(z\u0304\u00b7c|X\u0304) = N (z\u0304\u00b7c|0,KM ) \u2200c \u2208 {1, . . . , C} (8)\nwhere z\u0304\u00b7c = [z\u0304mc] M m=1, and KM is the gram matrix of the pseudo-inputs, i.e., KM = [k(x\u0304i, x\u0304j)]i,j . with k(\u00b7, \u00b7) being the employed kernel function. This way, we eventually obtain a prior over the latent encodings {z\u00b7c}Cc=1 parameterized by the introduced pseudo-data, taking the form of a simple GP predictive density, reading [23]\np(z\u00b7c|X, X\u0304, z\u0304\u00b7c) = N (z\u00b7c|KDMK \u22121 M z\u0304\u00b7c,\u039b) (9)\nwhere KDM = [k(xi, x\u0304j)]i,j , \u039b = diag(\u03bb\u0303d),\n\u03bb\u0303d = kdd \u2212 k T dK \u22121 M kd (10)\nand kd = [k(xd, x\u0304i)]i, while kdd = k(xd,xd). Having defined the encoder modules of our model, let us now proceed to the definition of the employed decoder modules. We consider a likelihood function of the form\np(yd\u00b7|zd\u00b7) = N \u220f\nn=1\nN (ydn|\u03b7 T nzd\u00b7, \u03b4 2) (11)\nwhere \u03b42 is the variance of a simple white noise model. Further, we impose a Gaussian prior with axis-aligned elliptical covariance over the parameter vectors \u03b7n, n \u2208 {1, . . . , N}, yielding\np(\u03b7n) = N (\u03b7n|0, diag(\u03d5) \u22121) \u2200n (12)\nwhere \u03d5 = [\u03d5c] C c=1 is a precision hyperparameters vector. We also impose Gamma hyperpriors over the \u03d5c, yielding\np(\u03d5) =\nC \u220f\nc=1\nG(\u03d5c|\u03c9\u03030, \u03c9\u03020)\nThe selected form of the prior imposed over the model parameters \u03b7n allows for obtaining an exponentially tractable solution to the latent encodings dimensionality inference problem, by means of automatic relevance determination (ARD) [24]. The notion of ARD is to continually create new components while detecting when a component starts to overfit. In the case of our model, the ARD mechanism can be implemented by imposing a hierarchical prior over the model parameters \u03b7n, to discourage large values, with the width along each latent dimension controlled by a Gamma-distributed precision hyperparameter, as illustrated in (12). If one of these precisions \u03d5c tends to infinity, then the outgoing weights \u03b7nc \u2200n will have to be very close to zero in order to maintain a high likelihood under this prior, which in turn leads the model to ignore the cth latent dimension, and, hence, the corresponding direction in the postulated latent subspace is effectively \u201cswitched off.\u201d In addition, we stipulate that the postulated linear regression scheme (11) that connects the obtained latent\nencodings zd\u00b7 and the observed data yd\u00b7, is subject to the following large-margin constraints:\n\n \n \nydn \u2212 E[\u03b7 T nzd\u00b7] \u2264 \u03b5+ \u03bedn \u2212ydn + E[\u03b7Tnzd\u00b7] \u2264 \u03b5+ \u03be \u2217 dn \u03bedn, \u03be \u2217 dn \u2265 0 \u2200d, n (13)\nOur imposed constraints are inspired from large-margin approaches, and especially, the literature on MED regression models [20, Chapter 4], as they are based on maximization of an expected margin, that takes into account the Bayesian nature of our model. Note that, in Eq. (13), \u03b5 is a precision parameter, functioning similar to the precision parameter in support vector regression (SVR), and \u03bedn, \u03be \u2217 dn are some slack-variables, used in a way similar to SVR [25].\nThis concludes the formulation of our model."}, {"heading": "3.1 Inference algorithm", "text": "To perform inference for our model, we adopt the MED inference framework, and extend it by introducing an additional (negative) likelihood term into the optimized objective function, which emanates from the assumption (11) of our model.\nSpecifically, conventional MED inference in the context of our model would comprise solution of the following minimization problem:\nmin q(Z,Z\u0304,H,\u03d5),\u03be,\u03be\u2217,X\u0304,\u03b42\nKL ( q(Z, Z\u0304,H,\u03d5)||p(Z, Z\u0304,H,\u03d5) )\n+ \u03b3\nD \u2211\nd=1\nN \u2211\nn=1\n(\u03bedn + \u03be \u2217 dn)\n(14) under the constraints (13), where Z = {z\u00b7c}Cc=1, Z\u0304 = {z\u0304\u00b7c}Cc=1,H = {\u03b7n} N n=1, \u03be = {\u03bedn}d,n, \u03be\n\u2217 = {\u03be\u2217dn}d,n, and KL(q||p) stands for the Kullback-Leibler (KL) divergence between the (approximate) posterior and the prior of our model. However, in our work, we follow a different approach, inspired from variational Bayesian inference [26], and the inference procedure of the nonparametric Bayesian large-margin classifier of [27]: We elect to optimize a composite objective function that takes into consideration both the expected (negative) log-likelihood of our hierarchical Bayesian model, which measures the goodness of fit to the training data, as well as the quality of the reconstruction of our training data, as is also performed in the context of conventional approaches. This way, inference for our model eventually reduces to\n5 solution of the following problem\nmin q(Z,Z\u0304,H,\u03d5),\u03be,\u03be\u2217,X\u0304,\u03b42\n\u2212 D \u2211\nd=1\nE[log p(yd\u00b7|zd\u00b7)]\n+ \u03b3 D \u2211\nd=1\nN \u2211\nn=1\n(\u03bedn + \u03be \u2217 dn)\n+ KL ( q(Z, Z\u0304,H,\u03d5)||p(Z, Z\u0304,H,\u03d5) )\n\u2200d, n, s.t. :\n\n \n \nydn \u2212 E[\u03b7 T nzd\u00b7] \u2264 \u03b5+ \u03bedn \u2212ydn + E[\u03b7Tnzd\u00b7] \u2264 \u03b5+ \u03be \u2217 dn \u03bedn, \u03be \u2217 dn \u2265 0\n(15) Note that, in the above expressions, all the expectations E[\u00b7] are computed w.r.t. the posterior q(Z, Z\u0304,H ,\u03d5). Our inference algorithm proceeds in an iterative fashion. On each iteration, each one of the factors of the sought posterior, i.e., q(Z), q(Z\u0304),q(H), q(\u03d5), as well as the pseudo-inputs X\u0304 , the noise variance \u03b42, the slackvariables \u03be, \u03be\u2217, and the employed kernel hyperparameters (if any) are consecutively optimized, one at a time. It has been shown that such an iterative consecutive updating procedure is guaranteed to monotonically optimize the objective function of our problem [21]."}, {"heading": "3.1.1 Posterior Distribution", "text": "Let us begin with deriving the expression of the posterior distribution of our model. Following the meanfield principle [28], we assume that the sought posterior distribution factorizes over the z\u00b7c, z\u0304\u00b7c, and \u03b7n, similar to the imposed prior. Then, optimizing (15) w.r.t. q(\u03b7n), we obtain\nq(\u03b7n) = N (\u03b7n|\u03bbn,\u03a3) (16)\nwhere\n\u03a3 \u22121 = diag(E[\u03d5]) +\n1\n\u03b42\nD \u2211\nd=1\nE[zd\u00b7z T d\u00b7] (17)\nand the means \u03bbn are obtained by solving the primal problem\nmin \u03bbn,\u03be\u00b7n,\u03be \u2217 \u00b7n\n\u2212 \u03bbTn\nD \u2211\nd=1\nydn \u03b42 E[zd\u00b7] + \u03b3\nD \u2211\nd=1\n(\u03bedn + \u03be \u2217 dn)\n+ 1\n2 \u03bbTn\u03a3 \u22121\u03bbn\n\u2200d, s.t. :\n\n \n \nydn \u2212 \u03bb T nE[zd\u00b7] \u2264 \u03b5+ \u03bedn \u2212ydn + \u03bb T nE[zd\u00b7] \u2264 \u03b5+ \u03be \u2217 dn \u03bedn, \u03be \u2217 dn \u2265 0\n(18)\nwhere \u03be\u00b7n = [\u03bedn] D d=1, \u03be \u2217 \u00b7n = [\u03be \u2217 dn] D d=1.\nEven though the optimization problem (18) may seem computationally cumbersome to solve, it is easy to show that it essentially reduces to a simple SVR problem. Specifically, let us take the Cholesky decomposition of \u03a3:\n\u03a3 \u22121 = UTU (19)\nLet us also introduce the notation\nvn =\nD \u2211\nd=1\nydn \u03b42 E[zd\u00b7] (20)\n\u03bb \u2032\nn = U(\u03bbn \u2212\u03a3vn) (21)\ny \u2032 dn = ydn \u2212 v T n\u03a3E[zd\u00b7] (22)\nrd = ( U\u22121 )T E[zd\u00b7] (23)\nIt is a matter of simple algebra to show that, using (19)- (23), the optimization problem (18) reduces to\nmin \u03bb \u2032\nn ,\u03be \u00b7n ,\u03be\u2217 \u00b7n\n1 2 ||\u03bb \u2032 n|| 2 2 + \u03b3\nD \u2211\nd=1\n(\u03bedn + \u03be \u2217 dn)\n\u2200d, s.t. :\n\n   \n   \ny \u2032 dn \u2212 (\u03bb \u2032 n) T rd \u2264 \u03b5+ \u03bedn \u2190\u2212 \u00b5dn \u2212y \u2032\ndn + (\u03bb \u2032 n) T rd \u2264 \u03b5+ \u03be\u2217dn \u2190\u2212 \u00b5 \u2217 dn \u03bedn \u2265 0 \u2190\u2212 \u03bddn \u03be\u2217dn \u2265 0 \u2190\u2212 \u03bd \u2217 dn\n(24)\nwhere the {\u00b5dn, \u00b5\u2217dn, \u03bddn, \u03bd \u2217 dn} are Lagrange multipliers. The problem (24) is essentially the primal form of the learning problem of a simple SVR model with inputs rd, and targets ydn. Therefore, to compute the \u03bbn, we can exploit all the highly-efficient solvers developed for the standard SVR problem. Indeed, in our work, we use the SVM-light toolbox of [29], which implements a highly-scalable working set selection algorithm, and provides both the primal parameters \u03bb \u2032\nn (and, hence, the \u03bbn), and the dual ones, i.e. the \u00b5\u00b7n = [\u00b5dn] D d=1, and the \u00b5 \u2217 \u00b7n = [\u00b5 \u2217 dn] D d=1. Note also that theN SVR problems implied by (24) can be executed in parallel, by leveraging available parallel processing infrastructure. Further, regarding the posteriors over the encodings of the training data q(z\u00b7c), we have\nq(z\u00b7c) = N (z\u00b7c|mc,Sc) (25)\nwhere\nS\u22121c = \u039b \u22121 +\n1\n\u03b42\nN \u2211\nn=1\nE[\u03b72nc]I (26)\nmc = Sc\n{\n\u039b \u22121KDMK \u22121 M E[z\u0304\u00b7c] +\n1\n\u03b42\nN \u2211\nn=1\ny\u00b7nE[\u03b7nc]\n\u2212 1\n\u03b42\n\u2211\nc\u2032 6=c\nN \u2211\nn=1\nE[\u03b7nc\u03b7nc\u2032z\u00b7c\u2032 ] +\nN \u2211\nn=1\n(\u00b5\u00b7n \u2212 \u00b5 \u2217 \u00b7n)E[\u03b7nc]\n}\n(27) and y\u00b7n = [ydn] D d=1.\nSimilarly, regarding the posteriors over the pseudoencodings z\u0304\u00b7c, we have\nq(z\u0304\u00b7c) = N (z\u0304\u00b7c|m\u0304c, S\u0304c) (28)\nwhere S\u0304c = KMQ \u22121 m KM (29)\n6 QM = KM +K T DM\u039b \u22121KDM (30)\nand m\u0304c = KMQ \u22121 M K T DM\u039b \u22121 E[z\u00b7c] (31)\nFinally, the precision hyperparameters \u03d5 yield the following hyperposterior:\nq(\u03d5) = C \u220f\nc=1\nG(\u03d5c|\u03c9\u0303c, \u03c9\u0302c) (32)\nwhere\n\u03c9\u0303c = \u03c9\u03030 + N\n2 (33)\nand\n\u03c9\u0302c = \u03c9\u03020 + 1\n2\nN \u2211\nn=1\n\u03bb2nc (34)"}, {"heading": "3.1.2 Pseudo-inputs selection", "text": "Further, we need to obtain the pseudo-inputs set X\u0304 . It is easy to show that solving (15) w.r.t. to the set X\u0304 reduces to the optimization problem\nmax X\u0304 E[\nC \u2211\nc=1\nlog p(z\u00b7c|X, X\u0304, z\u0304\u00b7c)] + E[ C \u2211\nc=1\nlog p(z\u0304\u00b7c|X\u0304)] (35)\nAs the maximization problem (35) does not yield closedform solutions, one has to resort to some iterative algorithm to obtain the sought estimates. For this purpose, the limited memory variant of the BFGS algorithm (L-BFGS) [30] is a suitable candidate solution. However, this procedure entails a prohibitive combinatorial search, especially when the observations space is highdimensional, i.e. for large values of N . Alternatively, to be conservative, in this work we avoid learning the pseudo-inputs (which can potentially greatly increase the training algorithm complexity and runtime). Instead, we use as our pseudo-inputs a subset of the actual (corrupt) observations X . Selection of this subset is performed by means of an iterative greedy selection process, which on each iteration selects the data point x : x \u2208 X \u2229 x /\u2208 X\u0304 addition of which in the set X\u0304 yields the maximum value of\nE[\nC \u2211\nc=1\nlog p(z\u00b7c|X, X\u0304, z\u0304\u00b7c)] + E[ C \u2211\nc=1\nlog p(z\u0304\u00b7c|X\u0304)]\nand proceeds until we reach the desired number M of pseudo-inputs."}, {"heading": "3.1.3 Hyperparameter estimation", "text": "Finally, let us now turn to the estimates of the hyperparameters of our model over which we didn\u2019t impose some prior distribution. We begin with the noise variance \u03b42; solving (15) w.r.t. \u03b42 we obtain\n\u03b42 = 1\nND\nD \u2211\nd=1\nN \u2211\nn=1\n(\ny2dn \u2212 2ydnE[\u03b7 T n\u00b7zd\u00b7] + E[\u03b7 T n\u00b7zd\u00b7z T d\u00b7\u03b7n\u00b7]\n)\n(36)\nFurther, we also consider the case the employed kernel functions of the imposed GP priors do also entail some hyperparameters. To obtain their estimates, we optimize (15) over them. This essentially reduces to the optimization problem\nmax \u03b8 E[\nC \u2211\nc=1\nlog p(z\u00b7c|X, X\u0304, z\u0304\u00b7c)] + E[ C \u2211\nc=1\nlog p(z\u0304\u00b7c|X\u0304)] (37)\nwhere \u03b8 are the sought hyperparameters. As the maximization problem (37) does not yield closed-form solutions, we may resort to an iterative algorithm to obtain the sought estimates, namely L-BFGS."}, {"heading": "4 EXPERIMENTS", "text": "Here, we conduct the experimental evaluation of our approach. First, we consider an image classification experiment using benchmark data well-known in the literature of AE neural networks. Our model is used to extract salient features from these images which are subsequently presented to simple linear SVM classifiers to perform the classification task. Further, we consider a video mining application, namely sports video mining in football videos. In this case, we use our model to extract salient features from raw video data, and present them to a simple linear SVM classifier to perform the classification task. Finally, we assess the performance of our method in a transfer learning experiment. In this case, our model is used to extract features from data pertaining to different domains; subsequently, we train a simple binary classifier (linear SVM) with the extracted features pertaining to data from one domain, and evaluate the resulting model using data from different domains. Apart from our method, in our experiments we also evaluate the mSDA model [31], SDAs [8], as well as the related DGP method of [19]. In all our experiments, we use masking noise (MN) with the percentage of zeroed features, \u03b6, selected so as to optimize the performance of each model. In all cases, we evaluate SDA for a set of alternative selections of the number of latent features, ranging from 100 to an overcomplete representation, matching the dimensionality of the observations space. Our approach is initialized considering an overcomplete representation. mSDA generates an overcomplete representation by default, as described in [31]. In all cases, we report results for optimal model configuration, as experimentally determined. Finally, evaluation of our model is performed using RBF kernels for the postulated GPs (omitting kernel hyperparameters optimization). All our source codes were implemented in MATLAB."}, {"heading": "4.1 Image classification benchmarks", "text": "In this experiment, we consider three benchmarks dealing with the problem of content-based image classification, commonly used in the deep learning literature. Specifically, we experiment with:\n7\n(i) The standard USPS handwritten digit classification dataset. From this dataset, we retain only 10 examples per class (digit) to perform training. This way, we make it possible to evaluate whether our approach achieves the goal of obtaining high modeling performance by using scarce data, and how it compares to the competition. (ii) The small-NORB dataset [32], which comprises stereo image pairs of fifty toys belonging to five generic categories. Each toy was imaged under six lighting conditions, nine elevations and eighteen azimuths. Originally, the objects were divided evenly into test and training sets yielding 24, 300 examples each. From this dataset, we use only 10 images from each class to perform training, so as to assess whether our approach can obtain high modeling performance by using scarce data. (iii) The Convex/Nonconvex dataset, adopted from [33]. From this dataset, we retain only 50 examples per class (digit) to perform training, so as to evaluate model performance under the restrictions of limited data availability; we use more data than in the previous two cases from each category, since we are here dealing with only two categories (convex, nonconvex). We repeat our experiments multiple times to alleviate the effect of random data selection on the obtained performances. In all cases, the mSDA, SDA, and DGP models yielded optimal performance when evaluated with 4 layers. For the USPS and small-NORB datasets, optimal performance was obtained for the proportion \u03b6 of zeroed features equal to 10%; in the case of the Convex/Nonconvex dataset, a proportion equal to 75% yielded the optimal performance. In all cases, SDA yielded optimal performance for 100 latent features per layer. Our obtained optimal results (for various numbers of pseudo-inputs M in the case of our method) are illustrated in Tables 1-3. These results comprise both the obtained recognition rates and the entailed computational costs of the evaluated algorithms (means over the conducted experiment repetitions). As we observe, our method obtains the best recognition performance results in all cases. We also observe that the computational costs of our approach are comparable to most of its competitors. mSDA is a notable exception to this rule; although obtaining the worst modeling performance among the considered methods, it takes two orders of magnitude less time than all the other approaches."}, {"heading": "4.2 Sports Video Mining", "text": "In this experiment, we consider the problem of sports video mining in football videos. Specifically, we are interested in detecting four camera view classes: central, left, right, and end-zone, and four play types: long play, short play, kick, and field goal play. To perform evaluations, we use a database comprising 30-min NFL American football games. The frames of the videos were downsampled to 45 \u00d7 36 resolution, and were preprocessed\nso as to remove commercials and replays. We retain only 100 examples from each category (i.e., play type or camera view) to perform training, and use another 1000 to perform evaluations. In Fig. 1, we provide some example frames from our videos.\nApart from the related methods, i.e. SDA, mSDA, and DGP, in these experiments we also compare to a baseline approach, namely the iM2EDM method of [34]. The latter method is not presented with the raw video signals, but with appropriate feature vectors extracted as described in [34]. Specifically, to capture camera view information, we use the color distribution and the yard line angle [35]. For this purpose, we estimate the spatial color distribution, and perform edge detection using the Canny algorithm, which we combine with the Hough transform to detect the yard lines and to compute their angles. Regarding play type information extraction, we utilize for this purpose camera motion information (panning and tilting), as this information is sufficient to characterize different play types: strong panning is usually associated with a long play, while a weak panning effect is usually associated with short\n8\nplays. To compute the two kinds of camera motion, we choose the optical flow-based method of [36]. We repeated our experiments 10 times to alleviate the effect of random data selection on the obtained performances. mSDA, SDA, and DGP yielded their best performance for architectures with 6 layers. Optimal performance was obtained for the proportion \u03b6 of zeroed features equal to 10% in all cases. SDA yielded optimal performance for 300 latent features per layer. In Table 4, we provide the obtained optimal performances of the evaluated algorithms (average results per detected class over the conducted 10 repetitions of our experiments). As we observe, our method works clearly better than the related approaches, yielding performance close to iM2EDM which relies to sophisticated extraction of appropriate descriptors, contrary to our approach."}, {"heading": "4.3 Transfer learning", "text": "In this experiment, we assess the capacity of our model to learn robust feature representations that capture salient high-level structure from data belonging to different domains. Inspired from the recent work of [37], we use the encodings generated by our model as inputs to linear SVM classifiers, and evaluate the resulting classifiers on the basis of their capacity to effectively perform classification of data from a given domain while being trained on data from different domains. Specifically, similar to [37], we consider a sentiment analysis application across different product domains. Our datasets are derived from the Amazon reviews benchmark [38]. This benchmark contains more than 340, 000 reviews from 25 different types of products from Amazon.com. In this dataset, different domains vary substantially in terms of the number of instances and class distribution. Some domains comprise hundreds of thousands of reviews, while others have only a few hundred. To ensure that our results will not be negatively affected from these imbalances, in our experiments we retain only four domains with similar numbers of samples, namely Books (B), DVDs (D), Electronics (E), and Kitchen appliances (K), similar to [38]. This yields a set of twelve transfer learning tasks between (pairs of) domains with well-balanced classes, which is more amenable to learning by means of a general purpose classifier. From these data, we use only 200 samples from each class for training, and the rest for testing (i.e.,\nwe train with scarce data). We repeat our experiment multiple times with different splits of the data into training and test sets to alleviate the effect of random data selection on the obtained performances. Similar to the work of [37], we formulate the recognition task as a binary classification problem: the trained models are required to recognize whether a review is positive (higher than 3 stars) or negative (3 stars or lower). Each review text is treated as a bag-of-words and transformed into binary vectors encoding the presence/absence of unigrams and bigrams, similar to [37]. For computational reasons, only the 500 most frequent terms of the vocabulary of unigrams and bigrams are kept in the feature set, similar to [31].\nEvaluation metrics. Let us denote as e(S, T ) the transfer error between source domain S and transfer domain T ; the transfer error is defined as the test error of a classifier trained on the source domain S and evaluated on the target domain T . Based on this measure, we can define the in-domain error e(T, T ) as the error of a classifier trained and evaluated in the same domain. Let us also denote as eb(T, T ) the in-domain error of a baseline classifier; in our experiments, the used baseline classifiers are linear SVMs on the raw dataset features. Using these definitions, we can introduce a metric suitable for evaluating the performance of a transfer learning algorithm, expressing whether (and how much of) a performance deterioration is incurred when the transfer error is equal to e(S, T ). Specifically, the introduced metric is the relative transfer loss t(S, T ), given by\nt(S, T ) = e(S, T )\u2212 eb(T, T )\neb(T, T ) (38)\nResults. We compare to related state-of-the-art methods, namely SDA and mSDA1. In this experiment, the SDA and mSDA models comprise 5 layers; this configuration was shown in [31] to yield the optimal result (we have confirmed this finding through our experiments). For both SDA and mSDA, and our approach, the proportion \u03b6 of zeroed features of the employed masking noise yields an optimal value equal to 50%. SDA yielded its best performance for 500 latent features. In Fig. 2, we illustrate the obtained relative transfer loss for all the evaluated methods and for all sourcetarget domain pairs, setting the number of pseudoinputs of our method equal to the total number of available data points (\u201cfull MED2A\u201d), as well as equal to 100 and 300. We observe that our method yields significantly better performance than the competition in all the considered transfer scenarios. We also observe that selecting M = 300 resulted in an optimal trade-off between computational complexity and model performance, since increasingM over 300 did not seem to yield performance gains worth the extra complexity. Finally, we would like to emphasize that the computational costs\n1. Application of DGP to a transfer learning scenario is not straightforward, given the model formulation of [19].\n9\nof our method are comparable to SDA: for M = 300, our method took 3.94 h to finish, while SDA took 4.16 h, i.e. slightly more than our approach. Note that mSDA takes two orders of magnitude less time (similar to the previous experiment), as also reported in [31]."}, {"heading": "5 CONCLUSIONS", "text": "DA neural networks are known to yield exceptional results when trained using large datasets; however, their performance deteriorates when data is scarce. This behavior of DAs contrasts to the capacity of humans to perform learning of complex abstract concepts with very limited observations. Under this motivation, in this paper we examined whether DA neural networks can be reformulated in such a way that allows for them to perform better when data is scarce. We assumed that the reason why DA performance deteriorates when decreasing the volume of training data is due to the frequentist nature of their formulation. Based on this assumption, we postulated a novel, fully Bayesian treatment of DAs, utilizing concepts from Bayesian nonparametrics and large-margin principles. Specifically, our approach formulates the encoder modules by considering a Gaussian process prior over the latent input encodings generated given the (corrupt) input observations. Subsequently, the\ndecoder modules of our model are formulated as largemargin regression models, treated under the Bayesian inference paradigm, by exploiting the maximum entropy discrimination (MED) framework. We provided an efficient variational algorithm for model inference, under the mean-field paradigm. We evaluated the efficacy of our method by considering a number of experiments dealing with high-dimensional observations. As we observed, our method yielded a clear improvement over the competition, while imposing comparable computational costs to most alternatives. Our future research goals are aimed at improving the computational complexity of our approach without undermining its modeling effectiveness. Borrowing ideas from the mSDA method of [31] might be a good starting point for this purpose."}], "references": [{"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Large scale kernel machines, L. Bottou, O. Chapelle, D. De- Coste, and J. Weston, Eds. MIT press, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, pp. 504\u2013507, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, vol. 19, 2007, pp. 153\u2013160.  10", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "Advances in neural information processing systems, vol. 20, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Connectionist learning procedures", "author": ["G. Hinton"], "venue": "Artificial Intelligence, vol. 40, pp. 185\u2013234, 1989.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y. Boureau", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems (NIPS\u201907), vol. 20, 2007, pp. 1185\u20131192.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Theory-based Bayesian models of inductive learning and reasoning", "author": ["J.B. Tenenbaum", "C. Kemp", "P. Shafto"], "venue": "Trends in Cognitive Sciences, 2006, pp. 309\u2013318.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Using deep belief nets to learn covariance kernels for Gaussian processes", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proc. Neural Information Processing Systems, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Nonparametric guidance of autoencoder representations using label information", "author": ["J. Snoek", "R.P. Adams", "H. Larochelle"], "venue": "Journal of Machine Learning Research, vol. 13, no. 1-48, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["N.D. Lawrence"], "venue": "Journal of Machine Learning Research, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proc. Artificial Intelligence and Statistics, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Proc. Neural Information Processing Systems, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep Gaussian Processes", "author": ["A.C. Damianou", "N.D. Lawrence"], "venue": "Proc. AISTATS, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative, generative and imitative learning", "author": ["T. Jebara"], "venue": "Ph.D. dissertation, MIT, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Maximum entropy discrimination", "author": ["T. Jaakkola", "M. Meila", "T. Jebara"], "venue": "Proc. NIPS, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Extensions of the informative vector machine", "author": ["N.D. Lawrence", "J.C. Platt", "M.I. Jordan"], "venue": "Deterministic and Statistical Methods in Machine Learning, J. Winkler, N. D. Lawrence, and M. Niranjan, Eds. Berlin: Springer-Verlag, 2005, pp. 56\u201387.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Sparse Gaussian processes using pseudo-inputs", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "Advances in Neural Information Processing Systems, Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, Eds., vol. 18. Cambridge, MA: The MIT Press, 2006, pp. 1259\u20131266.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic determination of the intrinsic structure in Bayesian factor analysis", "author": ["E. Fokoue"], "venue": "Statistical and Applied Mathematical Sciences Institute, Research Triangle Park, NC, Tech. Rep. TR- 2004-17, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "An introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Learning in Graphical Models, M. Jordan, Ed. Dordrecht: Kluwer, 1998, pp. 105\u2013162.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Infinite SVM: a Dirichlet process mixture of large-margin kernel machines", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "Proc. ICML, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian parameter estimation via variational methods", "author": ["T. Jaakkola", "M. Jordan"], "venue": "Statistics and Computing, vol. 10, pp. 25\u201337, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods - Support Vector Learning, B. Sch\u00f6lkopf, C. Burges, and A. Smola, Eds. MIT-Press, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "On the limited memory method for large scale optimization", "author": ["D. Liu", "J. Nocedal"], "venue": "Mathematical Programming B, vol. 45, no. 3, pp. 503\u2013528, 1989.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z.E. Xu", "K.Q. Weinberger", "F. Sha"], "venue": "Proc. ICML, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "Proc. CVPR, 2004.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Proc. ICML, 2007, pp. 473\u2013480.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Infinite Markov-switching maximum entropy discrimination machines", "author": ["S.P. Chatzis"], "venue": "Proc. ICML, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental hidden Markov models for view-based sport video analysis", "author": ["Y. Dingand", "G. Fan"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Qualitative estimation of camera motion parameters from video sequences", "author": ["M. Srinivasan", "S. Venkatesh", "R. Hosie"], "venue": "Pattern Recognit., vol. 30, pp. 593\u2013606, 1997.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proc. ICML, 2011.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "Proc. EMNLP, 2006, pp. 120\u2013128.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "It has recently become obvious that deep architectures allow for obtaining better modeling and representational capacities in the context of challenging pattern recognition applications [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "Deep belief networks and stacked autoencoders are characteristic examples of successful such architectures [2], [3], [4], [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": ", [6]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "In this work, we focus on autoencoder models [7]; specifically, we are interested in a recent variation of this method, namely denoising autoencoders [8].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "In this work, we focus on autoencoder models [7]; specifically, we are interested in a recent variation of this method, namely denoising autoencoders [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 213, "endOffset": 216}, {"referenceID": 6, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 218, "endOffset": 221}, {"referenceID": 8, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 223, "endOffset": 226}, {"referenceID": 4, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 233, "endOffset": 236}, {"referenceID": 1, "context": "DAs can be stacked into deep learning architectures, yielding the so-called stacked DA (SDA) architectures [8], which have been shown to yield excellent data modeling performance in several benchmark applications [8], [7], [9], [5], [3], [2].", "startOffset": 238, "endOffset": 241}, {"referenceID": 9, "context": "stacked DAs (SDAs), require large amounts of data to perform learning [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "However, we know that humans are able to perform inductive reasoning (equivalent to concept generalization) with only a few examples [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "For example, [13] use restricted Boltzmann machines (RBMs) to extract features in an unsupervised manner, which are subsequently fed as inputs to the covariance kernel of a Gaussian process (GP) regressor or classifier [14].", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "For example, [13] use restricted Boltzmann machines (RBMs) to extract features in an unsupervised manner, which are subsequently fed as inputs to the covariance kernel of a Gaussian process (GP) regressor or classifier [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "Similarly, [15] consider a classification model that imposes GP priors on the discriminative function that maps the latent encodings into class labels.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "The result of this choice is a Gaussian process latent variable model (GPLVM) [16] for the predicted class labels.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "While much more efficient compared to the approach of [13], neither does this method utilize the component nonparametric Bayesian model to perform learning of the generated input encodings.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "Further, [17] combined autoencoder training with neighborhood component analysis [18], which encouraged the model to encode similar latent representations for inputs belonging to the same class.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "Further, [17] combined autoencoder training with neighborhood component analysis [18], which encouraged the model to encode similar latent representations for inputs belonging to the same class.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Finally, [19] recently proposed a deep Gaussian process (DGP) model.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "DGP is essentially a deep belief network (DBN) [2], [3] the component models of each layer of which are Gaussian processes instead of RBMs.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "DGP is essentially a deep belief network (DBN) [2], [3] the component models of each layer of which are Gaussian processes instead of RBMs.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "Specifically, we consider a nonlinear encoder module, obtained by imposing a Gaussian process prior [14] over the latent encodings of the observed (corrupt) data.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "To introduce the large-margin learning principle in the context of our hierarchical Bayesian model, we build upon the maximum entropy discrimination (MED) framework [20], [21].", "startOffset": 165, "endOffset": 169}, {"referenceID": 19, "context": "To introduce the large-margin learning principle in the context of our hierarchical Bayesian model, we build upon the maximum entropy discrimination (MED) framework [20], [21].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "This, in turn, gives rise to an associated reconstruction error minimized through model training, that reads [8]", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Similarly, in cases of binary observations x, or x \u2208 [0, 1], it is usually considered that", "startOffset": 53, "endOffset": 59}, {"referenceID": 7, "context": "Specifically, it has been shown that it may lead to the solution of just copying the input to the output or other trivial ones that yield very low reconstruction error in the training set combined with extremely poor modeling and generalization performance [8].", "startOffset": 257, "endOffset": 260}, {"referenceID": 6, "context": ", [7]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "Denoising autoencoders address this overfitting problem in a different way [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Some simple corruption process models often used in the literature are [8]: (i) additive isotropic Gaussian noise (GS), i.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "Initializing deep architectures by stacking DAs is performed in a way fairly similar to traditional AEs [4], [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Initializing deep architectures by stacking DAs is performed in a way fairly similar to traditional AEs [4], [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 20, "context": "A drawback of such a modeling approach consists in the fact that performing inference for GP models entails inversion of the gram matrix of the input data [22], resulting in an O(D) complexity, which is rather inefficient.", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "To ameliorate this issue, we resort to utilization of a popular sparse pseudo-input Gaussian process (SPGP) modeling approach [23]: We impose a prior over the latent encodings {zd\u00b7}Dd=1 taking the form of a GP predictive distribution parameterized by a small pseudodataset X\u0304 = {x\u0304m\u00b7} M m=1, with corresponding latent encoding values {z\u0304m\u00b7}m=1 (pseudo-encodings).", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "This way, we eventually obtain a prior over the latent encodings {z\u00b7c}c=1 parameterized by the introduced pseudo-data, taking the form of a simple GP predictive density, reading [23]", "startOffset": 178, "endOffset": 182}, {"referenceID": 22, "context": "The selected form of the prior imposed over the model parameters \u03b7n allows for obtaining an exponentially tractable solution to the latent encodings dimensionality inference problem, by means of automatic relevance determination (ARD) [24].", "startOffset": 235, "endOffset": 239}, {"referenceID": 23, "context": "However, in our work, we follow a different approach, inspired from variational Bayesian inference [26], and the inference procedure of the nonparametric Bayesian large-margin classifier of [27]: We elect to optimize a composite objective function that takes into consideration both the expected (negative) log-likelihood of our hierarchical Bayesian model, which measures the goodness of fit to the training data, as well as the quality of the reconstruction of our training data, as is also performed in the context of conventional approaches.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "However, in our work, we follow a different approach, inspired from variational Bayesian inference [26], and the inference procedure of the nonparametric Bayesian large-margin classifier of [27]: We elect to optimize a composite objective function that takes into consideration both the expected (negative) log-likelihood of our hierarchical Bayesian model, which measures the goodness of fit to the training data, as well as the quality of the reconstruction of our training data, as is also performed in the context of conventional approaches.", "startOffset": 190, "endOffset": 194}, {"referenceID": 19, "context": "It has been shown that such an iterative consecutive updating procedure is guaranteed to monotonically optimize the objective function of our problem [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Following the meanfield principle [28], we assume that the sought posterior distribution factorizes over the z\u00b7c, z\u0304\u00b7c, and \u03b7n, similar to the imposed prior.", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "Indeed, in our work, we use the SVM-light toolbox of [29], which implements a highly-scalable working set selection algorithm, and provides both the primal", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "For this purpose, the limited memory variant of the BFGS algorithm (L-BFGS) [30] is a suitable candidate solution.", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "Apart from our method, in our experiments we also evaluate the mSDA model [31], SDAs [8], as well as the related DGP method of [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "Apart from our method, in our experiments we also evaluate the mSDA model [31], SDAs [8], as well as the related DGP method of [19].", "startOffset": 85, "endOffset": 88}, {"referenceID": 17, "context": "Apart from our method, in our experiments we also evaluate the mSDA model [31], SDAs [8], as well as the related DGP method of [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 28, "context": "mSDA generates an overcomplete representation by default, as described in [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "(ii) The small-NORB dataset [32], which comprises stereo image pairs of fifty toys belonging to five generic categories.", "startOffset": 28, "endOffset": 32}, {"referenceID": 30, "context": "(iii) The Convex/Nonconvex dataset, adopted from [33].", "startOffset": 49, "endOffset": 53}, {"referenceID": 31, "context": "SDA, mSDA, and DGP, in these experiments we also compare to a baseline approach, namely the iMEDM method of [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 31, "context": "The latter method is not presented with the raw video signals, but with appropriate feature vectors extracted as described in [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 32, "context": "Specifically, to capture camera view information, we use the color distribution and the yard line angle [35].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "To compute the two kinds of camera motion, we choose the optical flow-based method of [36].", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "Inspired from the recent work of [37], we use the encodings generated by our model as inputs to linear SVM classifiers, and evaluate the resulting classifiers on the basis of their capacity to effectively perform classification of data from a given domain while being trained on data from different domains.", "startOffset": 33, "endOffset": 37}, {"referenceID": 34, "context": "Specifically, similar to [37], we consider a sentiment analysis application across different product domains.", "startOffset": 25, "endOffset": 29}, {"referenceID": 35, "context": "Our datasets are derived from the Amazon reviews benchmark [38].", "startOffset": 59, "endOffset": 63}, {"referenceID": 35, "context": "To ensure that our results will not be negatively affected from these imbalances, in our experiments we retain only four domains with similar numbers of samples, namely Books (B), DVDs (D), Electronics (E), and Kitchen appliances (K), similar to [38].", "startOffset": 246, "endOffset": 250}, {"referenceID": 34, "context": "Similar to the work of [37], we formulate the recognition task as a binary classification problem: the trained models are required to recognize whether a review is positive (higher than 3 stars) or negative (3 stars or lower).", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "Each review text is treated as a bag-of-words and transformed into binary vectors encoding the presence/absence of unigrams and bigrams, similar to [37].", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "For computational reasons, only the 500 most frequent terms of the vocabulary of unigrams and bigrams are kept in the feature set, similar to [31].", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": "In this experiment, the SDA and mSDA models comprise 5 layers; this configuration was shown in [31] to yield the optimal result (we have confirmed this finding through our experiments).", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "Application of DGP to a transfer learning scenario is not straightforward, given the model formulation of [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 28, "context": "Note that mSDA takes two orders of magnitude less time (similar to the previous experiment), as also reported in [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 28, "context": "Borrowing ideas from the mSDA method of [31] might be a good starting point for this purpose.", "startOffset": 40, "endOffset": 44}], "year": 2017, "abstractText": "Denoising autoencoders (DAs) are typically applied to relatively large datasets for unsupervised learning of representative data encodings; they rely on the idea of making the learned representations robust to partial corruption of the input pattern, and perform learning using stochastic gradient descent with relatively large datasets. In this paper, we present a fully Bayesian DA architecture that allows for the application of DAs even when data is scarce. Our novel approach formulates the signal encoding problem under a nonparametric Bayesian regard, considering a Gaussian process prior over the latent input encodings generated given the (corrupt) input observations. Subsequently, the decoder modules of our model are formulated as large-margin regression models, treated under the Bayesian inference paradigm, by exploiting the maximum entropy discrimination (MED) framework. We exhibit the effectiveness of our approach using several datasets, dealing with both classification and transfer learning applications.", "creator": "LaTeX with hyperref package"}}}