{"id": "1402.6779", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2014", "title": "Resourceful Contextual Bandits", "abstract": "sometimes we today study contextual feedback bandits with ancillary constraints on resources, which mostly are common in broader real - world applications such when as choosing ads variables or dynamic weighted pricing of items. lastly we design the first objective algorithm theoretically for smooth solving these problems, and prove a continuous regret guarantee with near - optimal residual statistical entropy properties.", "histories": [["v1", "Thu, 27 Feb 2014 03:17:19 GMT  (51kb)", "https://arxiv.org/abs/1402.6779v1", null], ["v2", "Thu, 10 Apr 2014 22:00:13 GMT  (42kb)", "http://arxiv.org/abs/1402.6779v2", "Added some details to one of the proofs"], ["v3", "Mon, 19 May 2014 23:01:03 GMT  (55kb)", "http://arxiv.org/abs/1402.6779v3", "This is the full version of a paper in COLT 2014. Version history: (V2) Added some details to one of the proofs, (v3) a big revision following comments from COLT reviewers (but no new results)"], ["v4", "Tue, 1 Jul 2014 14:55:01 GMT  (56kb)", "http://arxiv.org/abs/1402.6779v4", "This is the full version of a paper in COLT 2014. Version history: (v2) Added some details to one of the proofs, (v3) a big revision following comments from COLT reviewers (but no new results), (v4) edits in related work, minor edits elsewhere"], ["v5", "Mon, 13 Jul 2015 00:12:19 GMT  (57kb)", "http://arxiv.org/abs/1402.6779v5", "This is the full version of a paper in COLT 2014. Version history: (v2) Added some details to one of the proofs, (v3) a big revision following comments from COLT reviewers (but no new results), (v4) edits in related work, minor edits elsewhere. (v5) A correction for Theorem 3, corollary for contextual dynamic pricing with discretization; updated follow-up work &amp; open questions"], ["v6", "Fri, 31 Jul 2015 18:31:27 GMT  (57kb)", "http://arxiv.org/abs/1402.6779v6", "This is the full version of a paper in COLT 2014. Version history: (v2) Added some details to one of the proofs, (v3) a big revision following comments from COLT reviewers (but no new results), (v4) edits in related work, minor edits elsewhere. (v6) A correction for Theorem 3, corollary for contextual dynamic pricing with discretization; updated follow-up work &amp; open questions"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.GT", "authors": ["ashwinkumar badanidiyuru", "john langford", "aleksandrs slivkins"], "accepted": false, "id": "1402.6779"}, "pdf": {"name": "1402.6779.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ashwinkumar Badanidiyuru", "John Langford", "Aleksandrs Slivkins", "A. Slivkins", "BADANIDIYURU LANGFORD SLIVKINS"], "emails": ["ASHWINKUMARBV@GMAIL.COM", "JCL@MICROSOFT.COM", "SLIVKINS@MICROSOFT.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n67 79\nv6 [\ncs .L\nG ]\n3 1"}, {"heading": "1. Introduction", "text": "Contextual bandits is a machine learning framework in which an algorithm makes sequential decisions according to the following protocol: in each round, a context arrives, then the algorithm chooses an action from the fixed and known set of possible actions, and then the reward for this action is revealed; the reward may depend on the context, and can vary over time. Contextual bandits is one of the prominent directions in the literature on online learning with exploration-exploitation tradeoff; many problems in this space are studied under the name multi-armed bandits.\nA canonical example of contextual bandit learning is choosing ads for a search engine. Here, the goal is to choose the most profitable ad to display to a given user based on a search query and the available information about this user, and optimize the ad selection over time based on user feedback such as clicks. This description leaves out many important details, one of which is that every ad is associated with a budget which constrains the maximum amount of revenue which that ad can generate. In fact, this issue is so important that in some formulations it is the primary problem (e.g., Devanur and Vazirani, 2004).\nThe optimal solution with budget constraints fundamentally differs from the optimal solution without constraints. As an example, suppose that one ad has a high expected revenue but a small budget such that it can only be clicked on once. Should this ad be used immediately? From all\n\u2217 This is the full version of a paper in the 26th Conf. on Learning Theory (COLT), 2014. The present version includes a correction for Theorem 3, a corollary for contextual dynamic pricing with discretization, and an updated discussion of related work.\nThe main results have been obtained while A. Badanidiyuru was a research intern at Microsoft Research New York City. A. Badanidiyuru was also partially supported by NSF grant AF-0910940 of Robert Kleinberg.\nc\u00a9 A. Badanidiyuru, J. Langford & A. Slivkins.\nreasonable perspectives, the answer is \u201cno\u201d. From the user\u2019s or advertiser\u2019s perspective, we prefer that this ad be displayed for the user with the strongest interest rather than for a user who simply has more interest than in other options. From a platform\u2019s viewpoint, it is better to have more ads in the system, since they effectively increases the price paid in a second price auction. And from everyone\u2019s viewpoint, it is simply odd to burn out the budget of an ad as soon as it is available. Instead, a small budget should be parceled out over time.\nTo address these issues, we consider a generalization of contextual bandits in which there are one or several resources that are consumed by the algorithm. This formulation has many natural applications. Dynamic ad allocation follows the ad example described above: here, resources correspond to advertisers\u2019 budgets. In dynamic pricing, a store with a limited supply of items to sell can make customized offers to customers. In dynamic procurement, a contractor with a batch of jobs and a limited budget can experiment with prices offered to the workers, e.g. workers in a crowdsourcing market. The above applications have been studied on its own, but never in models that combine contexts and limited resources.\nWe obtain the first known algorithm for contextual bandits with resource constraints (other than time) that improves over a trivial reduction to the non-contextual version of the problem. As such, we merge two lines of work on multi-armed bandits: contextual bandits and bandits with resource constraints. While significant progress has been achieved in each of the two lines of work (in particular, optimal solutions have been worked out for very general models), the specific approaches break down when applied to our model.\nOur model. We define resourceful contextual bandits (in short: RCB), a common generalization of two general models for contextual bandits and bandits with resource constraints: respectively, contextual bandits with arbitrary policy sets (e.g., Langford and Zhang, 2007; Dudik et al., 2011) and bandits with knapsacks (Badanidiyuru et al., 2013a).\nThere are several resources that are consumed by the algorithm, with a separate budget constraint on each. (Time is one of these resources, with deterministic consumption of 1 for every action.) In each round, the algorithm receives a reward and consumes some amount of each resource, in a manner that depends on the context and the chosen action, and may be randomized. We consider a stationary environment: in each round, the context and the mapping from actions to rewards and resource consumption is sampled independently from a fixed joint distribution, called the outcome distribution. Rewards and consumption of various resources can be correlated in an arbitrary way. The algorithm stops as soon as any constraint is violated. Initially the algorithm is given no information about the outcome distribution (except the distribution of context arrivals). In particular, expected rewards and resource consumptions are not known.\nAn algorithm is given a finite set \u03a0 of policies: mappings from contexts to actions. We compete against algorithms that must commit to some policy in \u03a0 before each round. Our benchmark is a hypothetical algorithm that knows the outcome distribution and makes optimal decisions given this knowledge and the restriction to policies in \u03a0. The benchmark\u2019s expected total reward is denoted OPT(\u03a0). Regret of an algorithm is defined as OPT(\u03a0) minus the algorithm\u2019s expected total reward.\nFor normalization, per-round rewards and resource consumptions lie in [0, 1]. We assume that the distribution of context arrivals is known to the algorithm.\nDiscussion of the model. Allowing stochastic resource consumptions and arbitrary correlations between per-round rewards and per-round resource consumptions is essential: this is why our model\nsubsumes diverse applications such as the ones discussed above,1 and many extensions thereof. Further discussion of the application domains can be found in Appendix A.\nIntuitively, the policy set \u03a0 consists of all policies that can possibly be learned by a given learning method, such as linear estimation or decision trees. Restricting to \u03a0 allows meaningful performance guarantees even if competing against all possible policies is intractable. The latter is common in real-life applications, as the set of possible contexts can be very large.\nOur benchmark can change policies from one round to another without restriction. As we prove, this is essentially equivalent in power to the best fixed distribution over policies. However, the best fixed policy may perform substantially worse.2\nOur stopping condition corresponds to hard constraints: an advertiser cannot exceed his budget, a store cannot sell more items than it has in stock, etc. An alternative stopping condition is to restrict the algorithm to actions that cannot possibly violate any constraint if chosen in the current round, and stop if there is no such action. This alternative is essentially equivalent to the original version.3 Moreover, we can w.l.o.g. allow our benchmark to use this alternative.\nOur contributions: main algorithm. We design an algorithm, called MixtureElimination, and prove the following guarantee on its regret.\nTheorem 1 For all RCB problems with K actions, d resources, time horizon T , and for all policy sets \u03a0. Algorithm MixtureElimination achieves expected total reward\nREW \u2265 OPT(\u03a0)\u2212O ( 1 + 1B OPT(\u03a0) )\u221a dKT log (dKT |\u03a0|), (1)\nwhere B = miniBi is the smallest of the resource constraints B1 , . . . , Bd.\nThis regret guarantee is optimal in several regimes. First, we achieve an optimal square-root \u201cscaling\u201d of regret: if all constraints are scaled by the same parameter \u03b1 > 0, then regret scales as \u221a \u03b1. Second, if B = T (i.e., there are no constraints), we recover the optimal O\u0303( \u221a KT ) regret. Third, we achieve O\u0303( \u221a KT ) regret for the important regime when OPT(\u03a0) and B are at least a constant fraction of T . In fact, Badanidiyuru et al. (2013a) provide a complimentary \u2126( \u221a KT ) lower bound for this regime, which holds in a very strong sense: for any given tuple (K,B, OPT(\u03a0), T ). The \u221a log |\u03a0| term in Theorem 1 is unavoidable (Dudik et al., 2011). The dependence on the minimum of the constraints (rather than, say, the maximum or some weighted combination thereof) is also unavoidable (Badanidiyuru et al., 2013a). For strongest results, one can rescale per-round rewards and per-round consumption of each resource so that they can be as high as 1.4\nNote that the regret bound in Theorem 1 does not depend on the number of contexts, only on the number of policies in \u03a0. In particular, it tolerates infinitely many contexts. On the other hand, if the set X of contexts is not too large, we can also obtain a regret bound with respect to the best policy among all possible policies. Formally, take \u03a0 = {all policies} and observe that |\u03a0| \u2264 K |X|.\nFurther, Theorem 1 extends to policy sets \u03a0 that consist of randomized policies: mappings from contexts to distributions over actions. This may significantly reduce |\u03a0|, as a given randomized\n1. For example, in dynamic pricing the algorithm receives a reward and loses an item only if the item is sold. 2. The expected total reward of the best fixed policy can be half as large as that of the best distribution. This holds\nfor several different domains including dynamic pricing / procurement, even without contexts (Badanidiyuru et al., 2013a). Note that without resource constraints, the two benchmarks are equivalent.\n3. Each budget constraint changes by at most one, which does not affect our regret bounds in any significant way. 4. E.g., if per-round consumption of some resource i is deterministically at most 1\n10 , then multiplying it by 10 would\neffectively increase the corresponding budget Bi by a factor of 10, and hence can only improve the regret bound.\npolicy might not be representable as a distribution over a small number of deterministic policies.5 We assume deterministic policies in the rest of the paper.\nComputational issues. This paper is focused on proving the existence of solutions to this problem, and the mathematical properties of such a solution. The algorithm is specified as a mathematically well-defined mapping from histories to actions; we do not provide a computationally efficient implementation. Such \u201cinformation-theoretical\u201d results are common for the first solutions to new, broad problem formulations (e.g. Kleinberg et al., 2008; Kleinberg and Slivkins, 2010; Dudik et al., 2011). In particular, in the prior work for RCB without resource constraints there exists an algorithm with O\u0303( \u221a KT ) regret (Auer et al., 2002; Dudik et al., 2011), but for all known computationally efficient algorithms regret scales with T as T 2/3 (Langford and Zhang, 2007).\nOur contributions: partial lower bound. We derive a partial lower bound: we prove that RCB is essentially hopeless for the regime OPT(\u03a0) \u2264 B \u2264 \u221a KT/2. The condition OPT(\u03a0) \u2264 B is satisfied, for example, in dynamic pricing with limited supply.\nTheorem 2 Any algorithm for RCB incurs regret \u2126(OPT(\u03a0)) in the worst case over all problem instances such that OPT(\u03a0) \u2264 B \u2264 \u221a KT/2 (using the notation from Theorem 1).\nThe above lower bound is specific to the general (\u201ccontextual\u201d) case of RCB. In fact, it points to a stark difference between RCB and the non-contextual version: in the latter, o(OPT) regret is achievable as long as (for example) B \u2265 log T (Badanidiyuru et al., 2013a).\nWhile Theorem 2 is concerned with the regime of small B, note that in the \u201copposite\u201d regime of very large B, namely B \u226b \u221a KT , the regret achieved in Theorem 1 is quite low: it can be expressed as O\u0303( \u221a KT + \u01eb \u00b7 OPT(\u03a0)), where B = 1\u01eb \u221a KT .\nOur contributions: discretization. In some applications of RCB, such as dynamic pricing and dynamic procurement, the action space is a continuous interval of prices. Theorem 1 usefully applies whenever the policy set \u03a0 is chosen so that the number of distinct actions used by policies in \u03a0 is finite and small compared to T . (Because one can w.l.o.g. remove all other actions.) However, one also needs to handle problem instances in which the policies in \u03a0 use prohibitively large or infinite number of actions.\nWe consider a paradigmatic example of RCB with an infinite action space: contextual dynamic pricing with a single product and prices in the [0, 1] interval. We derive a corollary of Theorem 1 that applies to an arbitrary finite policy set \u03a0. To the best of our knowledge, this is the first result on contextual dynamic pricing with infinite price set.\nWe use discretization: we reduce the original problem to one in which actions (i.e., prices) are multiples of some carefully chosen \u01eb > 0. Our approach proceeds as follows. For each \u01eb > 0 and each policy \u03c0 let \u03c0\u01eb be a policy that takes the price computed by \u03c0 and rounds it down to the nearest multiple of \u01eb. We define the \u201cdiscretized\u201d policy set \u03a0\u01eb = {\u03c0\u01eb : \u03c0 \u2208 \u03a0}. We use Theorem 1 to obtain a regret bound relative to \u03a0\u01eb. Here the \u01eb controls the tradeoff between the number of actions in that regret bound and the \u201cdiscretization error\u201d of \u03a0\u01eb. Then we optimize the choice of \u01eb to obtain\n5. We can reduce RCB with randomized policies to RCB with deterministic policies simply by replacing each context x with a vector (a(x, \u03c0) : \u03c0 \u2208 \u03a0) such that a(x, \u03c0) = \u03c0(x), and encoding the randomization in policies through the randomization in the context arrivals. While this blows up the context space, it does not affect our regret bound.\nthe regret bound relative to \u03a0. The technical difficulty here is to bound the discretization error in terms of \u01eb; for this purpose we assume Lipschitz demands.6\nTheorem 3 Consider contextual dynamic pricing with a single product and prices in [0, 1]. Use standard notation: supply B, policy set \u03a0 and time horizon T . Assume Lipschitz demands with Lipschitz constant L. Then algorithm MixtureEliminationwith discretized policy set \u03a0\u01eb (defined as above) and \u01eb suitably chosen as a function of (B,T,L, |\u03a0|) achieves expected total reward\nREW \u2265 OPT(\u03a0)\u2212O(T 3/5 B1/5) \u00b7 (L log (T |\u03a0|))1/5 (2)\nThis regret bound is most interesting for the important regime B \u2265 \u2126(T ) (studied, for example, in Besbes and Zeevi (2009, 2011); Wang et al. (2014)). Then regret is O(T 4/5) (L log (T |\u03a0|))1/5.\nIt is unclear whether this regret bound is optimal. When specialized to the non-contextual case, it is not optimal. The optimal regret is then O(B2/3), even for an arbitrary budget B and even without the Lipscitz assumption (Babaioff et al., 2015). Extending the discretization approach beyond dynamic pricing with a single product is problematic even without contexts, see Section 10 for further discussion.\nDiscussion: main challenges in RCB. The central issue in bandit problems is the tradeoff between exploration: acquiring new information, and exploitation: making seemingly optimal decisions based on this information. In this paper, we resolve the explore-exploit tradeoff in the presence of contexts and resource constraints. Each of the three components (explore-exploit tradeoff, contexts, and resource constraints) presents its own challenges, and we need to deal with all these challenges simultaneously. Below we describe these individual challenges one by one.\nA well-known naive solution for explore-exploit tradeoff, which we call pre-determined exploration, decides in advance to allocate some rounds to exploration, and the remaining rounds to exploitation. The decisions in the exploration rounds do not depend on the observations, whereas the observations from the exploitation rounds do not impact future decisions. While this approach is simple and broadly applicable, it is typically inferior to more advanced solutions based on adaptive exploration \u2013 adapting the exploration schedule to the observations, so that many or all rounds serve both exploration and exploitation.7 Thus, the general challenge in most explore-exploit settings is to design an appropriate adaptive exploration algorithm.\nResource constraints are difficult to handle for the following three reasons. First, an algorithm\u2019s ability to exploit is constrained by resource consumption for the purpose of exploration; the latter is stochastic and therefore difficult to predict in advance. Second, the expected per-round reward is no longer the right objective to optimize, as the action with the highest expected per-round reward could consume too much resources. Instead, one needs to take into account the expected reward over the entire time horizon. Third, with more than one constrained resource (incl. time) the best fixed policy is no longer the right benchmark; instead, the algorithm should search over distributions over policies, which is a much larger search space.\nIn contextual bandit problems, an algorithm effectively chooses a policy \u03c0 \u2208 \u03a0 in each round. Naively, this can be reduced to a non-contextual bandit problem in which \u201cactions\u201d correspond to\n6. Lipschitz demands is a common assumption in some of the prior work on (non-contextual) dynamic pricing, even with a single product (Besbes and Zeevi, 2009; Wang et al., 2014). However, the optimal algorithm for the single-product case (Babaioff et al., 2015) does not need this assumption. 7. For example, the difference in regret between pre-determined and adaptive exploration is O\u0303( \u221a KT ) vs. O(K log T )\nfor stochastic K-armed bandits, and O\u0303(T 3/4) vs. O\u0303(B2/3) for dynamic pricing with limited supply.\npolicies. In particular, the main results in Badanidiyuru et al. (2013a) directly apply to this reduced problem. However, the action space in the reduced problem has size |\u03a0|; accordingly, regret scales as \u221a |\u03a0| in the worst case. The challenge in contextual bandits is to reduce this dependence. In particular, note that we replace \u221a\n|\u03a0| with log |\u03a0|, an exponential improvement. Organization of the paper. We start with a survey of related work and preliminaries (Sections 2-3). We define the main algorithm, prove its correctness, and describe the key steps of regret analysis in Sections 4-6. The remaining details of the regret analysis are in Section 7. We prove the lower bound in Section 8. We conclude with an extensive discussion of the state-of-art for RCB and the directions for further work (Sections 10). Appendix A contains a discussion of the main application domains for RCB."}, {"heading": "2. Related work", "text": "Multi-armed bandits have been studied since Thompson (1933) in Operations Research, Economics, and several branches of Computer Science, see (Gittins et al., 2011; Bubeck and Cesa-Bianchi, 2012) for background. This paper unifies two active lines of work on bandits: contextual bandits and bandits with resource constraints.\nContextual Bandits (Auer, 2002; Langford and Zhang, 2007) add contextual side information which can be used in prediction. This is a necessary complexity for virtually all applications of bandits since it is far more common to have relevant contextual side information than no such information. Several versions have been studied in the literature, see (Bubeck and Cesa-Bianchi, 2012; Dudik et al., 2011; Slivkins, 2014) for a discussion. For contextual bandits with policy sets, there exist two broad families of solutions, based on multiplicative weight algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) or confidence intervals (Dudik et al., 2011; Agarwal et al., 2012). We rework the confidence interval approach, incorporating and extending the ideas from the work on resource-constrained bandits (Badanidiyuru et al., 2013a).\nPrior work on resource-constrained bandits includes dynamic pricing with limited supply (Babaioff et al., 2015; Besbes and Zeevi, 2009, 2012), dynamic procurement on a budget (Badanidiyuru et al., 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al., 2011; Tran-Thanh et al., 2010, 2012). Badanidiyuru et al. (2013a) define and optimally solve a common generalization of all these settings: the non-contextual version of RCB. An extensive discussion of these and other applications, including applications to repeated auctions and network routing, can be found in (Badanidiyuru et al., 2013a).\nTo the best of our knowledge, the only prior work that explicitly considered contextual bandits with resource constraints is (Gyo\u0308rgy et al., 2007). This paper considers a somewhat incomparable setting with arbitrary policy sets and a single constrained resource: time, whose consumption is stochastic and depends on the context and the chosen action. Gyo\u0308rgy et al. (2007) design an algorithm whose regret scales O(f(t) log t) for any time t, where f is any positive diverging function and the constant in O() depends on the problem instance and on f .\nOur setting can be seen as a contextual bandit version of stochastic packing (e.g. Devanur and Hayes, 2009; Devanur et al., 2011). The difference is in the feedback structure: in stochastic packing, full information about each round is revealed before that round.\nWhile we approximate our benchmark OPT(\u03a0) with a linear program optimum, our algorithm and analysis are conceptually very different from the vast literature on approximately solving linear programs, and in particular from LP-based work on bandit problems such as Guha et al. (2010).\nConcurrent and independent work. Agrawal and Devanur (2014) study a model for contextual bandits with resource constraints that is incomparable with ours. The model for contexts is more restrictive: contexts do not change over time,8 and expected outcome of each round is linear in the context. Whereas the model for rewards and resource constraints is more general: the total reward can be an arbitrary concave function of the time-averaged outcome vector v\u0304, and the resource constraint states that v\u0304 must belong to a given convex set (which can be arbitrary)."}, {"heading": "3. Problem formulation and preliminaries", "text": "We consider an online setting where in each round an algorithm observes a context x from a possibly infinite known set of possible contexts X and chooses an action a from a finite known set A. The world then specifies a reward r \u2208 [0, 1] and the resource consumption. There are d resources that can be consumed, and the resource consumption is specified by numbers ci \u2208 [0, 1] for each resource i. Thus, the world specifies the vector (r; c1 , . . . , cd), which we call the outcome vector; this vector can depend on the the chosen action a and the round. There is a known hard constraint Bi \u2208 R+ on the consumption of each resource i; we call it a budget for resource i. The algorithm stops at the earliest time \u03c4 when any budget constraint is violated; its total reward is the sum of the rewards in all rounds strictly preceding \u03c4 . The goal of the algorithm is to maximize the expected total reward.\nWe are only interested in regret at a specific time T (time horizon) which is known to the algorithm. Formally, we model time as a specific resource with budget T and a deterministic consumption of 1 for every action. So d \u2265 2 is the number of all resources, including time. W.l.o.g., Bi \u2264 T for every resource i.\nWe assume that an algorithm can choose to skip a round without doing anything. Formally, we posit a null action: an action with 0 reward and 0 consumption of all resources except the time. This is for technical convenience, so as to enable Lemma 5.\nStochastic assumptions. We assume that there exists an unknown distribution D(x, r, ci), called the outcome distribution, from which each round\u2019s observations are created independently and identically, where the vectors are indexed by individual actions. In particular, context x is drawn from the marginal distribution DX(\u00b7), and the observed reward and resource consumptions for each action a are drawn from the conditional distribution D(ra, cia|x). We assume that the marginal distribution over contexts D(x) is known.\nPolicy sets and the benchmark. An algorithm is given a finite set \u03a0 of policies \u2013 mappings from contexts to actions. Our benchmark is a hypothetical algorithm that knows the outcome distribution D, and makes optimal decisions given this knowledge. The benchmark is restricted to policies in \u03a0: before each round, it must commit to some policy \u03c0 \u2208 \u03a0, and then choose action \u03c0(x) upon arrival of any given context x. The expected total reward of the benchmark is denoted OPT(\u03a0). Regret of an algorithm is OPT(\u03a0) minus the algorithm\u2019s expected total reward.\n8. Agrawal and Devanur (2014) also claimed an extension to contexts that change over time, which has subsequently been retracted (see Footnote 1 in Agrawal and Devanur (2015)). This extension constitutes the main result in Agrawal and Devanur (2015) (which is subsequent work relative to the present paper).\nUniform budgets. We say that the budgets are uniform if Bi = B for each resource i. Any problem instance can be reduced to one with uniform budgets by dividing all consumption values for every resource i by Bi/B, where B = miniBi. (That is tantamount to changing the units in which we measure consumption of resource i.) We assume uniform budgets B from here on.\nNotation. Let r(\u03c0) = E(x,r)\u223cD[r\u03c0(x)] and ci(\u03c0) = E(x,ci)\u223cD[ci\u03c0(x)] be the expected per-round reward and the expected per-round consumption of resource i for policy \u03c0. Similary, define r(P ) = E\u03c0\u223cP [r(\u03c0)] and ci(P ) = E\u03c0\u223cP [ci(\u03c0)] as the natural extension to a distribution P over policies.\nThe tuple \u00b5 = ( (r(\u03c0); c1(\u03c0) , . . . , cd(\u03c0)) : \u03c0 \u2208 \u03a0) is called the expected-outcomes tuple. For a distribution P over policies, let P (\u03c0) is the probability that P places over policy \u03c0. By a slight abuse of notation, let P (a|x) = \u2211\u03c0(x)=a P (\u03c0) be the probability that P places on action a given context x. Thus, each context x induces a distribution P (\u00b7|x) over actions."}, {"heading": "3.1. Linear approximation and the benchmark", "text": "We set up a linear relaxation that will be crucial throughout the paper. As a by-product, we (effectively) reduce our benchmark OPT(\u03a0) to the best fixed distribution over policies.\nA given distribution P over policies defines an algorithm ALGP : in each round a policy \u03c0 is sampled independently from P , and the action a = \u03c0(x) is chosen. The value of P is the total reward of this algorithm, in expectation over the outcome distribution.\nAs the value of P is difficult to characterize exactly, we approximate it (generalizing the approach from (Babaioff et al., 2015; Badanidiyuru et al., 2013a) for the non-contextual version). We use a linear approximation where all rewards and consumptions are deterministic and the time is continuous. Let r(P, \u00b5) and ci(P, \u00b5) be the expected per-round reward and the expected per-round consumption of resource i for policy \u03c0 \u223c P , given expected-outcomes tuple \u00b5. Then the linear approximation corresponds to the solution of a simple linear program:\nMaximise t r(P, \u00b5) in t \u2208 R subject to t ci(P, \u00b5) \u2264 B for each i\nt \u2265 0. (3)\nThe solution to this LP, which we call the LP-value of P , is\nLP(P, \u00b5) = r(P, \u00b5) mini B/ci(P, \u00b5). (4)\nDenote OPTLP = supP LP(P, \u00b5), where the supremum is over all distributions P over \u03a0.\nLemma 4 OPTLP \u2265 OPT(\u03a0).\nTherefore, it suffices to compete against the best fixed distribution over \u03a0, as approximated by OPTLP, even though our benchmark OPT(\u03a0) allows unrestricted changes over time. Note that proving regret bounds relative to OPTLP rather than to OPT(\u03a0) only makes our results stronger.\nA distribution P over \u03a0 that attains the supremum value OPTLP is called LP-optimal. Such P is called LP-perfect if furthermore |support(P )| \u2264 d and ci(P, \u00b5) \u2264 B/T for each resource i. We find it useful to consider LP-perfect distributions throughout the paper.\nLemma 5 An LP-perfect distribution exists for any instance of RCB.\nLemma 4 and Lemma 5 are proved for the non-contextual version of RCB in Badanidiyuru et al. (2013a). The general case can be reduced to the non-contextual version via a standard reduction where actions in the new problem correspond to policies in \u03a0 in the original problem. For Lemma 5, Badanidiyuru et al. (2013a) obtain an LP-perfect distribution by mixing an LP-optimal distribution with the \u201cnull action\u201d; this is why we allow the null action in the setting.\n4. The algorithm: MixtureElimination\nThe algorithm\u2019s goal is to converge on a LP-perfect distribution over policies. The general design principle is to explore as much as possible while avoiding obviously suboptimal decisions.\nOverview of the algorithm. In each round t, the following happens.\n1. Compute estimates. We compute high-confidence estimates for the per-round reward r(\u03c0) and per-round consumption ci(\u03c0), for each policy \u03c0 \u2208 \u03a0 and each resource i. The collection I of all expected-outcomes tuple that are consistent with these high-confidence estimates is called the confidence region.\n2. Avoid obviously suboptimal decisions. We prune away all distributions P over policies in \u03a0 that are not LP-perfect with high confidence. More precisely, we prune all P that are not LP-perfect for any expected-outcomes tuple in the confidence region I; the remaining distributions are called potentially LP-perfect. Let F be the convex hull of the set of all potentially LP-perfect distributions. 3. Explore as much as possible. We choose a distribution P \u2208 F which is balanced, in the sense that no action is starved; see Equation (5) for the precise definition. Note that balanced distributions are typically not LP-perfect.\n4. Select an action. We choose policy \u03c0 \u2208 \u03a0 independently from P . Given context x, the action a is chosen as a = \u03c0(x). The algorithm adds some random noise: with probability q0, the action a is instead chosen uniformly at random, for some parameter q0.\nThe algorithm halts as soon as the time horizon is met, or one of the resources is exhausted. The pseudocode can be found in Algorithm 1.\nSome details. After each round t, we estimate the per-round consumption ci(\u03c0) and the per-round reward r(\u03c0), for each policy \u03c0 \u2208 \u03a0 and each resource i, using the following unbiased estimators:\nc\u0303i(\u03c0) = ci 1{a=\u03c0(x)} P [a = \u03c0(x) |x] and r\u0303(\u03c0) = r 1{a=\u03c0(x)} P [a = \u03c0(x) |x] .\nThe corresponding time-averages up to round t are denoted\nc\u0302t,i(\u03c0) = 1\nt\u22121\nt\u22121\u2211\ns=1\nc\u0303s,i(\u03c0) and r\u0302t(\u03c0) = 1t\u22121\nt\u22121\u2211\ns=1\nr\u0303s(\u03c0).\nWe show that with high probability these time-averages are close to their respective expectations. To express the confidence term in a more lucid way, we use the following shorthand, called confidence radius: radt(\u03bd) = \u221a Crad \u03bd/t, where Crad = \u0398(log(dT |\u03a0|)) is a parameter which we will fix later. We show that w.h.p. the following holds:\n|r(\u03c0)\u2212 r\u0302t(\u03c0)| \u2264 radt (K/\u03b1\u03c0,t) , (6) |ci(\u03c0)\u2212 c\u0302t,i(\u03c0)| \u2264 radt (K/\u03b1\u03c0,t) for all i. (7)\nAlgorithm 1 MixtureElimination 1: Parameters: #actions K , time horizon T , budget B, benchmark set \u03a0, context distribution DX. 2: Data structure: \u201cconfidence region\u201d I \u2190 {all feasible expected-outcomes tuples}. 3: For each round t = 1 . . . T do 4: \u2206t = {distributions P over \u03a0: P is LP-perfect for some \u00b5 \u2208 I}. 5: Let Ft be the convex hull of \u2206t. 6: Let \u03b1\u03c0,t = maxP\u2208Ft P (\u03c0), \u2200\u03c0 \u2208 \u03a0. 7: Choose a \u201cbalanced\u201d distribution Pt \u2208 Ft: any P \u2208 Ft such that \u2200\u03c0 \u2208 \u03a0\nE x\u223cDX\n[ 1\n(1\u2212 q0)P (\u03c0(x)|x) + q0K\n] \u2264 2K\n\u03b1\u03c0,t , where q0 = min\n( 1 2 , \u221a K T log(K T |\u03a0|) ) . (5)\n8: Observe context xt; choose action at to \u201dplay\u201d: 9: with probability q0, draw at u.a.r. in A; else, draw \u03c0 \u223c Pt and let at = \u03c0(xt).\n10: Observe outcome vector (r, c1 , . . . , cd). 11: Halt if one of the resources is exhausted. 12: Eliminate expected-outcomes tuples from I that violate equations (6-7)\n(Here \u03b1\u03c0,t = maxP\u2208Ft P (\u03c0), as in Algorithm 1.)"}, {"heading": "5. Correctness of the algorithm", "text": "We need to prove that in each round t, some P \u2208 Ft satisfies (5), and Equations (6-7) hold for all policies \u03c0 \u2208 \u03a0 with high probability. Notation. Recall that Pt is the distribution over \u03a0 chosen in round t of the algorithm, and q0 is the noise probability. The \u201cnoisy version\u201d of Pt is defined as\nP \u2032t(a|x) = (1\u2212 q0)Pt(a|x) + q0/K (\u2200x \u2208 X, a \u2208 A).\nThen action at in round t is drawn from distribution P \u2032t (\u00b7|xt).\nLemma 6 In each round t, some P \u2208 Ft satisfies (5). Proof First we prove that Ft is compact; here each distribution over \u03a0 is interpreted as a |\u03a0|dimensional vector, and compactness is w.r.t. the Borel topology on R|\u03a0|. This can be proved via standard real analysis arguments; we provide a self-contained proof in Appendix B.\nIn what follows we extend the minimax argument from Dudik et al. (2011). Our proof works for any q0 \u2208 [0, 12 ] and any compact and convex set F \u2282 F\u03a0.\nDenote \u03b1\u03c0 = maxP\u2208F P (\u03c0), for each \u03c0 \u2208 \u03a0. Let F\u03a0 be the set of all distributions over \u03a0. Equation (5) holds for a given P \u2208 F if and only if for every distribution Z \u2208 F\u03a0 we have that\nf(P,Z) , E x\u223cDX E \u03c0\u223cZ\n[ \u03b1\u03c0\nP \u2032(\u03c0(x)|x)\n] \u2264 2K,\nwhere P \u2032 is the noisy version of P . It suffices to show that\nmin P\u2208F max Z\u2208F\u03a0\nf(P,Z) \u2264 2K. (8)\nWe use a min-max argument: noting that f is a convex function of P and a concave function of Z , by the Sion\u2019s minimax theorem (Sion, 1958) we have that\nmin P\u2208F max Z\u2208F\u03a0 f(P,Z) = max Z\u2208F\u03a0 min P\u2208F f(P,Z). (9)\nFor each policy \u03c0 \u2208 \u03a0, let \u03b2\u03c0 \u2208 argmax\u03b2\u2208F \u03b2(\u03c0) be a distribution which maximizes the probability of selecting \u03c0. Such distribution exists because \u03b2 7\u2192 \u03b2(\u03c0) is a continuous function on a compact set F . Recall that \u03b1\u03c0 = \u03b2\u03c0(\u03c0).\nGiven any Z \u2208 F\u03a0, define distribution PZ \u2208 F\u03a0 by PZ(\u03c0) = \u2211\n\u03c6\u2208\u03a0 Z(\u03c6)\u03b2\u03c6(\u03c0). Note that PZ is a convex combination of distributions in F . Since F is convex, it follows that PZ \u2208 F . Also, note that PZ(a|x) \u2265 \u2211 \u03c0\u2208\u03a0: \u03c0(x)=a Z(\u03c0)\u03b1\u03c0. Letting P \u2032 Z be the noisy version of PZ , we have:\nmin P\u2208F f(P,Z) \u2264 f(PZ , Z) = E x\u223cDX\n[ \u2211\n\u03c0\nZ(\u03c0)\u03b1\u03c0 P \u2032Z(\u03c0(x)|x)\n]\n= E x\u223cDX\n  \u2211\na\u2208A\n\u2211\n\u03c0\u2208\u03a0: \u03c0(x)=a\nZ(\u03c0)\u03b1\u03c0 P \u2032Z(a|x)\n  = E\nx\u223cDX\n[ \u2211\na\u2208X\n\u2211 \u03c0\u2208\u03a0: \u03c0(x)=a Z(\u03c0)\u03b1\u03c0\n(1\u2212 q0)PZ(a|x) + q0/K\n]\n\u2264 E x\u223cDX\n[ \u2211\na\u2208X\n1\n1\u2212 q0\n] = K\n1\u2212 q0 \u2264 2K.\nThus, by Equation (9) we obtain Equation (8).\nTo analyze Equations (6-7), we will use Bernstein\u2019s inequality for martingales (Freedman, 1975), via the following formulation from Bubeck and Slivkins (2012):\nLemma 7 Let G0 \u2286 G1 \u2286 . . . \u2286 Gn be a filtration, and X1, . . . ,Xn be real random variables such that Xt is Gt-measurable, E(Xt|Gt\u22121) = 0 and |Xt| \u2264 b for some b > 0. Let Vn = \u2211n t=1 E(X 2 t |Gt\u22121). Then with probability at least 1\u2212 \u03b4 it holds that\n\u2211n t=1 Xt \u2264 \u221a 4Vn log(n\u03b4\u22121) + 5b2 log 2(n\u03b4\u22121).\nLemma 8 With probability at least 1\u2212 1T , Equations (6-7) hold for all rounds t and policies \u03c0 \u2208 \u03a0. Proof Let us prove Equation (6). (The proof of (7) is similar.) Fix round t and policy \u03c0 \u2208 \u03a0. We bound the conditional variance of the estimators r\u0303t(\u03c0). Specifically, let Gt be the \u03c3-algebra induced by all events up to (but not including) round t. Then\nE [ r\u0303t(\u03c0) 2 | Gt ] = E\nx\u223cDX, a\u223cP \u2032t\n[ r2t 1{\u03c0(x)=a}\nP \u2032t(a|x)2\n] \u2264 E\nx\u223cDX\n[ 1\nP \u2032t(\u03c0(x)|x)\n] \u2264 2K\n\u03b1\u03c0,t .\nThe last inequality holds by the algorithm\u2019s choice of distribution Pt. Since the confidence region I in our algorithm is non-increasing over time, it follows that \u03b1\u03c0,t is non-increasing in t, too. We conclude that Var [r\u0303s(\u03c0) | Gs] \u2264 2K/\u03b1\u03c0,t for each round s \u2264 t. Therefore, noting that r\u0303t(\u03c0) \u2264 1/P \u2032(\u03c0(xt)|xt) \u2264 K/q0, we obtain Equation (6) by applying Lemma 7 with Xt = r\u0303t(\u03c0)\u2212 r(\u03c0)."}, {"heading": "6. Regret analysis: proof of Theorem 1", "text": "We provide the key steps of the proof; the details can be found in Section 7. Let It and \u2206t be, resp., the confidence region I and the set \u2206 of potentially LP-perfect distributions computed in round t. Let Conv(\u2206t) be the convex hull of \u2206t. First we bound the deviations within the confidence region.\nLemma 9 For any two expected-outcomes tuples \u00b5\u2032, \u00b5\u2032\u2032 \u2208 It and a distribution P \u2208 Conv(\u2206t):\n|ci(P, \u00b5\u2032)\u2212 ci(P, \u00b5\u2032\u2032)| \u2264 radt (dK) for each resource i (10) |r(P, \u00b5\u2032)\u2212 r(P, \u00b5\u2032\u2032)| \u2264 radt (dK) (11)\nProof Let us prove Equation (11). (Equation (10) is proved similarly.) By definition of It:\n|r(P, \u00b5\u2032)\u2212 r(P, \u00b5\u2032\u2032)| \u2264 \u2211\u03c0\u2208\u03a0 P (\u03c0) |r(\u03c0, \u00b5\u2032)\u2212 r(\u03c0, \u00b5\u2032\u2032)| \u2264 \u2211 \u03c0\u2208\u03a0 P (\u03c0) radt (K/\u03b1\u03c0,t) .\nIt remains to prove that the right-hand side is at most radt(dK). By linearity, it suffices to prove this for P \u2208 \u2206t. So let us assume P \u2208 \u2206t from here on. Recall that |support(P )| \u2264 d since P is LP-perfect, and P (\u03c0) \u2264 \u03b1\u03c0,t for any policy \u03c0 \u2208 \u03a0. Therefore:\n\u2211 \u03c0\u2208\u03a0 P (\u03c0) radt (K/\u03b1\u03c0,t) \u2264 \u2211 \u03c0\u2208\u03a0 radt (KP (\u03c0))\n\u2264 radt ( dK \u2211 \u03c0\u2208\u03a0 P (\u03c0) ) = radt (dK) .\nUsing Lemma 9 and a long computation (fleshed out in Section 7), we prove the following.\nLemma 10 For any two expected-outcomes tuples \u00b5\u2032, \u00b5\u2032\u2032 \u2208 It and a distribution P \u2208 Conv(\u2206t):\nLP(P, \u00b5\u2032)\u2212 LP(P, \u00b5\u2032\u2032) \u2264 ( 1B LP(P, \u00b5 \u2032) + 2) \u00b7 T \u00b7 radt(dK).\nLet REWt and Ct,i be, respectively, the (realized) total reward and average consumption of resource i up to and including round t. Recall that P \u2032t is the noisy version of distribution Pt chosen by the algorithm in round t. Given Pt, the expected revenue and resource-i consumption in round t is, respectively, r(P \u2032t , \u00b5) and ci(P \u2032 t , \u00b5). Denote rt = 1 t \u2211t i=1 r(P \u2032 t , \u00b5) and ci,t = 1 t \u2211t i=1 ci(P \u2032 t , \u00b5).\nAnalysis of a clean execution. Henceforth, without further notice, we assume a clean execution where several high-probability conditions are satisfied. Formally, the algorithm\u2019s execution is clean if in each round t Equations (6-7) are satisfied, and moreover min ( |1t REWt \u2212 rt|, |Ct,i \u2212 ct,i| ) \u2264 radt(1). In particular, the set \u2206t of potentially LP-perfect distributions indeed contains a LP-perfect distribution. By Lemma 8 and Azuma-Hoeffding Inequality, clean execution happens with probability at least 1\u2212 1T . Thus, it suffices to lower-bound the total reward REWT for a clean execution.\nLemma 11 For any distribution P \u2032 \u2208 Conv(\u2206t) and any expected-outcomes tuple \u00b5 \u2208 It,\nmin P\u2208\u2206t LP(P, \u00b5) \u2264 LP(P \u2032, \u00b5) \u2264 max P\u2208\u2206t LP(P, \u00b5). (12)\nProof The proof consists of two parts. The second inequality in Equation (12) follows easily because the distribution which maximizes LP(P, \u00b5) by definition belongs to \u2206t, and so\nLP(P \u2032, \u00b5) \u2264 max P\u2208Conv(\u2206t) LP(P, \u00b5) = max P\u2208\u2206t LP(P, \u00b5).\nTo prove the first inequality in Equation (12), we first argue that LP(P, \u00b5) is a quasi-concave function of P . Denote \u03b7i(P, \u00b5) = B \u00b7 r(P, \u00b5)/ci(P, \u00b5) for each resource i. Then \u03b7i is a quasiconcave function of P since each level set (the set of distributions P that satisfy \u03b7i(P, \u00b5) \u2265 \u03b1 for some \u03b1 \u2208 R) is a convex set. Therefore LP(P, \u00b5) = mini \u03b7i(P, \u00b5) is a quasi-concave function of P as a minimum of quasi-concave functions.\nSince P \u2032 \u2208 Conv(\u2206t), it is a convex combination P \u2032 = \u2211\nQ\u2208\u2206t \u03b1Q Q with \u2211 Q\u2208\u2206t\n\u03b1Q = 1. Therefore:\nLP(P \u2032, \u00b5) = LP\n  \u2211\nQ\u2208\u2206t\n\u03b1Q Q, \u00b5\n \n\u2265 min Q\u2208\u2206t,\u03b1Q>0 LP(Q,\u00b5) By definition of quasi-concave functions \u2265 min Q\u2208\u2206t LP(Q,\u00b5).\nThe following lemma captures a crucial argument. Denote\n\u03a6t = ( 2 + 1B [ max\nP\u2208F\u03a0, \u00b5\u2208It LP(P, \u00b5)\n]) \u00b7 T \u00b7 radt(dK)\nLemma 12 For any expected-outcomes tuple \u00b5\u2217, \u00b5\u2217\u2217 \u2208 It and distributions P \u2032, P \u2032\u2032 \u2208 Conv(\u2206t):\n|LP(P \u2032, \u00b5\u2217)\u2212 LP(P \u2032\u2032, \u00b5\u2217\u2217)| \u2264 3\u03a6t. (13) Proof Assume P \u2032, P \u2032\u2032 \u2208 \u2206t. In particular, P \u2032, P \u2032\u2032 are LP-perfect for some expected-outcomes tuples \u00b5\u2032, \u00b5\u2032\u2032 \u2208 It, resp. Also, some distribution P \u2217 \u2208 \u2206t is LP-perfect for \u00b5\u2217 (by Lemma 5). Therefore:\nLP(P \u2032, \u00b5\u2217) \u2265 LP(P \u2032, \u00b5\u2032)\u2212 \u03a6t (by Lemma 10: P = P \u2032) \u2265 LP(P \u2217, \u00b5\u2032)\u2212 \u03a6t \u2265 LP(P \u2217, \u00b5\u2217)\u2212 2\u03a6t (by Lemma 10: P = P \u2217) \u2265 LP(P \u2032\u2032, \u00b5\u2217)\u2212 2\u03a6t.\nWe proved Equation (13) for P \u2032, P \u2032\u2032 \u2208 \u2206t. Thus:\nmax P\u2208\u2206t LP(P, \u00b5\u2217)\u2212 min P\u2208\u2206t LP(P, \u00b5\u2217) \u2264 2\u03a6t. (14)\nNext we generalize to P \u2032, P \u2032\u2032 \u2208 Conv(\u2206t).\nLP(P \u2032, \u00b5\u2217) \u2265 min P\u2208\u2206t LP(P, \u00b5\u2217) (by Lemma 11)\n\u2265 max P\u2208\u2206t LP(P, \u00b5\u2217)\u2212 2\u03a6t (by Equation (14)) \u2265 LP(P \u2032\u2032, \u00b5\u2217)\u2212 2\u03a6t (by Lemma 11).\nWe proved Equation (13) for \u00b5\u2217 = \u00b5\u2217\u2217. We obtain the general case by plugging in Lemma 10.\nNext, we upper-bound \u03a6t in terms of\n\u03a8t = (2 + 1 B OPTLP) \u00b7 T \u00b7 radt(dK).\nCorollary 13 \u03a6t \u2264 2\u03a8t, assuming that B \u2265 6 \u00b7 T \u00b7 radt(dK). Proof Follows from Lemma 12 via a simple computation, see Section 7.\nCorollary 14 LP(Pt, \u00b5) \u2265 OPTLP \u2212 12\u03a8t, where \u00b5 is the actual expected-outcomes tuple. Proof Follows from Lemma 12 and Corollary 13, observing that Pt \u2208 Conv(\u2206t) and OPTLP = LP(P \u2217, \u00b5) for some P \u2217 \u2208 \u2206t.\nIn the remainder of the proof (which is fleshed out in Section 7) we build on the above lemmas and corollaries to prove the following sequence of claims:\nREWt \u2265 t\nT (OPTLP \u2212O(\u03a8t))\nCt,i \u2264 B/T +O(radt(dK)) (15) REWT \u2265 OPTLP \u2212O(\u03a8T ).\nTo complete the proof of Theorem 1, we re-write the last equation as REWT \u2265 f(OPTLP) for an appropriate function f(), and observe that f(OPTLP) \u2265 f(OPT) because function f() is increasing."}, {"heading": "7. Regret analysis: remaining details for the proof of Theorem 1", "text": ""}, {"heading": "7.1. Proof of Lemma 10", "text": "We restate the lemma for convenience.\nLemma For any two expected-outcomes tuples \u00b5\u2032, \u00b5\u2032\u2032 \u2208 It and a distribution P \u2208 Conv(\u2206t):\nLP(P, \u00b5\u2032)\u2212 LP(P, \u00b5\u2032\u2032) \u2264 ( 1B LP(P, \u00b5 \u2032) + 2) \u00b7 T \u00b7 radt(dK).\nProof For brevity, we will denote:\nLP\u2032 = LP(P, \u00b5\u2032) and LP\u2032\u2032 = LP(P, \u00b5\u2032\u2032)\nr\u2032 = r(P, \u00b5\u2032) and r\u2032\u2032 = r(P, \u00b5\u2032\u2032)\nc\u2032i = ci(P, \u00b5 \u2032) and c\u2032\u2032i = ci(P, \u00b5 \u2032\u2032).\nBy symmetry, it suffices to prove the upper bound for LP\u2032 \u2212 LP\u2032\u2032. Henceforth, assume LP\u2032 > LP\u2032\u2032. We consider two cases, depending on whether\nT \u2264 B/c\u2032\u2032i for all resources i. (16)\nCase 1. Assume Equation (16) holds. Then LP\u2032\u2032 = T r\u2032\u2032. Therefore by Lemma 9\nLP\u2032 \u2212 LP\u2032\u2032 \u2264 T r\u2032 \u2212 T r\u2032\u2032 \u2264 T radt(dK).\nCase 2. Assume Equation (16) fails. Then LP\u2032\u2032 = B r\u2032\u2032/c\u2032\u2032i for some resource i. We consider two subcases, depending on whether\nT \u2264 B/c\u2032j for all resources j. (17)\nSubcase 1. Assume Equation (17) holds. Then:\nLP\u2032 = T r\u2032 (18)\nLP\u2032\u2032 \u2264 T \u00b7min(r\u2032, r\u2032\u2032) \u2264 LP\u2032 (19)\nEquation (19) follows from (18) and LP\u2032 > LP\u2032\u2032. For \u03b4 \u2208 [0, c\u2032\u2032i ), define\nr(\u03b4) = r\u2032\u2032 + \u03b4\nci(\u03b4) = c \u2032\u2032 i \u2212 \u03b4\nf(\u03b4) = B r(\u03b4)/ci(\u03b4).\nThen f() is monotonically and continuously increasing function, with f(\u03b4) \u2192 \u221e as \u03b4 \u2192 c\u2032\u2032i . For convenience, define f(c\u2032\u2032i ) = \u221e.\nLet \u03b40 = min(c\u2032\u2032i , radt(dK)). By Lemma 9, we have f(\u03b40) \u2265 Br\u2032/c\u2032i. Therefore:\nf(0) = LP\u2032\u2032 < LP\u2032 \u2264 Br\u2032/c\u2032i \u2264 f(\u03b40).\nThus, by Equation (19), we can fix \u03b4 \u2208 [0, \u03b40) such that f(\u03b4) = T \u00b7min(r\u2032, r\u2032\u2032).\nLP\u2032\u2032 = B r\u2032\u2032\nc\u2032\u2032i = B r(\u03b4)\u2212 \u03b4 ci(\u03b4) + \u03b4\n\u2265 B r(\u03b4)\u2212 \u03b4 ci(\u03b4)\n( 1\u2212 \u03b4\nci(\u03b4)\n) .\nf(\u03b4)\u2212 LP\u2032\u2032 \u2264 B ci(\u03b4) \u03b4 +B r(\u03b4) ci(\u03b4)2 \u03b4\n= ( 1 + r(\u03b4)\nci(\u03b4)\n) B \u03b4\nci(\u03b4)\n= ( 1 + f(\u03b4)\nB\n) f(\u03b4) \u03b4\nr(\u03b4)\n\u2264 ( 1 + T r\u2032\nB\n) T r\u2032\u2032 \u03b4\nr(\u03b4)\n\u2264 ( 1 + LP\u2032\nB\n) T \u03b4\n\u2264 ( LP\u2032/B + 1 ) \u00b7 T \u00b7 radt(dK).\nLP\u2032 \u2212 f(\u03b4) = T r\u2032 \u2212 T min(r, r\u2032) \u2264 T \u00b7 radt(dK)\nLP\u2032 \u2212 LP\u2032\u2032 = ( LP\u2032 \u2212 f(\u03b4) ) + ( f(\u03b4)\u2212 LP\u2032\u2032 )\n\u2264 ( LP\u2032/B + 2 ) \u00b7 T \u00b7 radt(dK).\nSubcase 2. Assume Equation (17) fails. Then LP\u2032 = B r\u2032/c\u2032j for some resource j. Note that c\u2032i \u2264 c\u2032j and c\u2032\u2032j \u2264 c\u2032\u2032i by the choice of i and j.\nFrom these inequalities and Lemma 9 we obtain c\u2032\u2032i \u2264 c\u2032j + radt(dK). Therefore,\nB r\u2032\u2032 c\u2032\u2032i \u2265 B r \u2032 \u2212 radt(dK) c\u2032j + radt(dK)\n(by Lemma 9)\n\u2265 B r \u2032 \u2212 radt(dK)\nc\u2032j\n( 1\u2212 radt(dK)\nc\u2032j\n) .\nLP\u2032 \u2212 LP\u2032\u2032 = B r \u2032 c\u2032j \u2212Br \u2032\u2032 c\u2032\u2032i\n\u2264 ( B\nc\u2032j +B\nr\u2032\n(c\u2032j) 2\n) radt(dK)\n\u2264 ( T + LP\u2032 T\nB\n) radt(dK)\n\u2264 ( LP\u2032/B + 1 ) \u00b7 T \u00b7 radt(dK)."}, {"heading": "7.2. Remainder of the proof after Lemma 12", "text": "We start with Corollary 13, which we restate here for convenience.\nCorollary \u03a6t \u2264 2\u03a8t, assuming that B \u2265 6 \u00b7 T \u00b7 radt(dK). Proof Let \u03b3 = maxP\u2208F\u03a0,\u00b5\u2208It LP(P, \u00b5). Note that \u03b3 \u2264 T . Then from Lemma 12 we obtain:\n\u03b3 \u2212 OPTLP \u2264 3( \u03b3B + 2) \u00b7 T \u00b7 radt(dK) \u2264 \u03b3 2 + 6 \u00b7 T \u00b7 radt(dK). (20)\nUsing (20) and Lemma 12 we get the desired bound:\n\u03a6t \u2264 ( \u03b3B + 2) \u00b7 T \u00b7 radt(dK)\n\u2264 ( 2 OPTLP + 12 \u00b7 T \u00b7 radt(dK)\nB + 2\n) \u00b7 T \u00b7 radt(dK)\n\u2264 ( 2 OPTLP\nB + 4\n) \u00b7 T \u00b7 radt(dK) = 2\u03a8t.\nIn the remainder of this appendix, we prove the claims in Equation (15) one by one.\nCorollary 15 REWt \u2265 tT (OPTLP \u2212O(\u03a8t)) for each round t \u2264 \u03c4 . Proof From Lemma 14 we obtain\nT r(P \u2032t , \u00b5) \u2265 (1\u2212 q0) LP(Pt, \u00b5) \u2265 (1\u2212 q0) (OPTLP \u2212 12\u03a8t) \u2265 OPTLP \u2212 13\u03a8t.\nSumming up and taking average over rounds, we obtain:\nT rt \u2265 OPTLP \u2212 13t \u2211t\ns=1 \u03a8s \u2265 OPTLP \u2212O(\u03a8t). By definition of clean execution, we obtain:\nREWt \u2265 t(rt \u2212 radt(rt)) \u2265 tT (OPTLP \u2212O(\u03a8t)).\nCorollary 16 Ct,i \u2264 B/T +O(radt(dK)) for each round t \u2264 \u03c4 . Proof Let \u00b5 be the (actual) expected-outcomes tuple, and recall that Pt is LP-optimal for some expected-outcomes tuple \u00b5\u2032 \u2208 \u2206t. Then, by Lemma 9, it follows that ci(Pt, \u00b5) \u2264 ci(Pt, \u00b5\u2032) + radt(dK). Furthermore since Pt is LP-optimal for \u00b5\u2032 we have ci(Pt, \u00b5\u2032) \u2264 BT . Therefore:\nci(Pt, \u00b5) \u2264 BT + radt(dK) ci(P \u2032 t , \u00b5) \u2264 (1\u2212 q0) ci(Pt, \u00b5) + q0\n\u2264 BT +O(radt(dK)).\nNow summing and taking average we obtain ct,i \u2264 BT + O(radt(dK)). Using the definition of clean execution, it follows that\nCt,i \u2264 ct,i + radt(ct,i) \u2264 BT +O(radt(dK)).\nLemma 17 REWT \u2265 OPTLP \u2212O(\u03a8T ). Proof Either \u03c4 = T or some resource i gets exhausted, in which case (using Corollary 16)\n\u03c4 = B C\u03c4,i \u2265 B\nB T + rad\u03c4 (dK)\n\u21d2 \u03c4 BT + \u03c4rad\u03c4 (dK) \u2265 B \u21d2 \u03c4 BT + TradT (dK) \u2265 B\n\u21d2 \u03c4 \u2265 T ( 1\u2212 TB radT (dK) ) . (21)\nUsing this lower bound and Corollary 15, we obtain the desired bound on the total revenue REWT .\nREWT = REW\u03c4 \u2265 \u03c4\nT ( OPTLP \u2212O(\u03a8\u03c4 ) )\n\u2265 OPTLP(1\u2212 TB radT (dK)) \u2212 O(\u03c4 \u03a8\u03c4 )\nT\n\u2265 OPTLP \u2212\u03a8T \u2212 O(\u03c4 \u03a8\u03c4 )\nT .\nIn the above, the first inequality holds by Corollary 15, the second by Equation (21), and the third by definition of \u03a8T .\nFinally, we note that \u03c4 \u03a8\u03c4 is an increasing function of \u03c4 , and substitute \u03c4\u03a8\u03c4 \u2264 T\u03a8T . We complete the proof of Theorem 1 as follows. Re-writing Lemma 17 as REWT \u2265 f(OPTLP),\nfor an appropriate function f(), note that REWT \u2265 f(OPT) because function f() is increasing."}, {"heading": "8. Lower bound: proof of Theorem 2", "text": "In fact, we prove a stronger theorem that implies Theorem 2. Theorem 18 Fix any tuple (K,T,B) such that K \u2208 [2, T ] and B \u2264 \u221a KT/2. Any algorithm for RCB incurs regret \u2126(OPT(\u03a0)) in the worst case over all problem instances with K actions, time horizon T , smallest budget B, and policy sets \u03a0 such that OPT(\u03a0) \u2264 B.\nWe will use the following lemma (which follows from simple probability arguments).\nLemma 19 Consider two collections of n balls I1 and I2, each numbered from 1 to n. Let I1 consists of all red balls, while I2 consist of n \u2212 1 red balls and 1 green ball (with labels chosen uniformly at random). In this setting, let an algorithm is given access to random samples from one of Ii with replacement. The algorithm is allowed to first look at the ball\u2019s number and then decide whether to inspect it\u2019s color. Then any algorithm A which with probability at least 12 can distinguish between I1 and I2 must inspect color of at least n/2 balls in expectation.\nIn the remainder of this section we prove Theorem 18. Let us define a family of problem instances as follows. Let the set of arms be {a1, a2, . . . , aK}. There are T/B different contexts labelled {x1, ..., xT/B} and there is a uniform distribution over contexts. The policy set \u03a0 consists of T (K \u2212 1)/B policies \u03c0i,j , where 2 \u2264 i \u2264 K and 1 \u2264 j \u2264 T/B. Define them as follows: \u03c0i,j(xl) = ai for l = j, and \u03c0i,j(xl) = a1 for l 6= j.\nThere is just one resource constraint B (apart from time). Pulling arm a1 always costs 0 and arm ai, i 6= 1 always costs 1. Now consider the following problem instances: \u2022 Let F0 be the instance in which every arm always gives a reward 0. Note that OPT(F0) = 0. \u2022 Let Fi,j be the instance in which arm ai on context xj gives reward 1, otherwise every arm on every context gives reward 0. Note that in this case the optimal distribution over policies is just to follow \u03c0i,j and gets reward \u2248 B.\nNow consider any algorithm A and let the expected number of times it pulls arm ai be pi on input F0. Let i\u2032, i\u2032 6= 1 be the arm for which this is minimum. Then by simple linearity of expectation we get that B \u2265 (K\u2212 1)pi\u2032 . It is also simple to see that for the algorithm to get a regret better than \u2126(OPT) it should be able to distinguish between F0 and Fi\u2032,. at least with probability 1 2 . From lemma 19 this can be done iff pi\u2032 \u2265 T/(2B). Combining the two equations we get B \u2265 (K \u2212 1)T/(2B). Solving for B we get B \u2265 \u221a KT/2."}, {"heading": "9. Discretization for contextual dynamic pricing (proof of Theorem 3)", "text": "We consider contextual dynamic pricing with B copies of a single product. The action space consists of all prices p \u2208 [0, 1]. We obtain regret bounds relative to an arbitrary policy set \u03a0. Preliminaries. Let S(p|x) be the contextual sales rate: the probability of a sale for price p and context x. Note that S(p|x) is non-increasing in p, for any given x.\nThe assumption of Lipschitz demands is stated as follows:\n|S(p|x)\u2212 S(p\u2032|x)| \u2264 L \u00b7 |p\u2212 p\u2032| for all contexts x, (22)\nfor some constant L called the Lipschitz constant. For simplicity, assume L \u2265 1. For a (possibly randomized) policy \u03c0, define the contextual sales rate S(\u03c0|x) = Ep\u223c\u03c0(x)[S(p|x) ] and the absolute sales rate S(\u03c0) = Ex[S(\u03c0|x) ]. The latter is exactly the expected per-round resource consumption for \u03c0. Let r(\u03c0) be the expected per-round reward for \u03c0.\nAs discussed in the Introduction, we define the discretization with step \u01eb as follows. For each price p, let f\u01eb(p) be p rounded down to the nearest multiple of \u01eb, i.e. the largest price p\u2032 \u2264 p such\nthat p\u2032 \u2208 \u01ebN. For each policy \u03c0 we define a discretized policy \u03c0\u01eb = f\u01eb(\u03c0). The discretized policy set is then \u03a0\u01eb = {\u03c0\u01eb : \u03c0 \u2208 \u03a0}. Note that for all policies \u03c0 and all contexts x we have\n\u03c0(x) \u2265 \u03c0\u01eb(x) \u2265 \u03c0(x)\u2212 \u01eb.\nBy monotonicity of the sales rate and the Lipschitz assumption, resp., it follows that\nS(\u03c0|x) \u2264 S(\u03c0\u01eb|x) \u2264 S(\u03c0|x) + \u01ebL.\nConsequently, S(\u03c0) \u2264 S(\u03c0\u01eb) \u2264 S(\u03c0) + \u01ebL. Discretization error. The key technical step is to bound the discretization error of the discretized policy set \u03a0\u01eb compared to the original policy set \u03a0, as quantified by the difference in OPTLP(\u00b7).\nOur proof will use an intermediate policy class \u03a6\u03b4 = {S(\u03c0) \u2265 \u03b4}, where \u03b4 > 0. First we bound the discretization error relative to \u03a6\u03b4.\nLemma 20 OPTLP(\u03a6\u03b4)\u2212 OPTLP(\u03a0\u01eb) \u2264 2 \u00b7 \u01eb(1 + L\u03b4\u22122) \u00b7 B, for each \u01eb, \u03b4 > 0. Proof Using a trivial reduction to the non-contextual case (when a policy corresponds to an action in the bandits-with-knapsacks problem), one can use a generic discretization result from Badanidiyuru et al. (2013a). According to this result (specialized to contextual dynamic pricing), it suffices to prove that for each policy \u03c0 \u2208 \u03a6\u03b4 the following two properties hold: (P1) S(\u03c0\u01eb) \u2265 S(\u03c0), (P2) r(\u03c0\u01eb)/S(\u03c0\u01eb) \u2265 r(\u03c0)/S(\u03c0) \u2212 \u01eb(1 + L\u03b4\u22122), as long as S(\u03c0\u01eb) > 0. In words: the sales rate of the discretized policy \u03c0\u01eb is at least the same, and the reward-to-consumption ratio is not much worse.\nProperty (P1) holds trivially because \u03c0\u01eb \u2264 \u03c0 (deterministically and for every context), and the contextual sales rate S(p|x) is decreasing in p for any fixed context x.\nr(\u03c0\u01eb) = E x,\u03c0\n[ f\u01eb(\u03c0(x)) \u00b7 S(\u03c0\u01eb|x) ]\n\u2265 E x,\u03c0 [ (\u03c0(x)\u2212 \u01eb) \u00b7 S(\u03c0\u01eb|x) ]\n\u2265 E x,\u03c0 [ \u03c0(x) \u00b7 S(\u03c0|x) ] \u2212 \u01eb E x,\u03c0 [S(\u03c0\u01eb|x) ]\n= r(\u03c0)\u2212 \u01eb S(\u03c0\u01eb). r(\u03c0\u01eb)/S(\u03c0\u01eb) \u2265 r(\u03c0)/S(\u03c0\u01eb)\u2212 \u01eb.\nNow, by the Lipschitz assumption, S(\u03c0\u01eb) \u2264 S(\u03c0) + \u01ebL, so to complete the proof\nr(\u03c0\u01eb) S(\u03c0\u01eb) \u2265 r(\u03c0) S(\u03c0) + \u01ebL \u2212 \u01eb \u2265 r(\u03c0) S(\u03c0) \u2212 \u01ebL (S(\u03c0))2 \u2212 \u01eb \u2265 r(\u03c0) S(\u03c0) \u2212 \u01ebL \u03b42 \u2212 \u01eb.\nNow we bound the loss in OPTLP between \u03a0 and \u03a6\u03b4.\nLemma 21 OPTLP(\u03a0)\u2212 OPTLP(\u03a6\u03b4) \u2264 \u03b4T , for each \u03b4 > 0. Proof If \u03b4 \u2265 B/T , the statement is trivial because OPTLP(\u03a0) \u2264 B. So w.l.o.g. assume \u03b4 < B/T .\nBy Lemma 5, there exists an LP-perfect distribution P over policies in \u03a0. Recall that P is a mixture of (at most) two policies, say \u03c0 and \u03c0\u2032, and c(P ) \u2264 B/T . W.l.o.g. assume S(\u03c0) \u2264 S(\u03c0\u2032).\nIf S(\u03c0) \u2265 \u03b4 then \u03c0, \u03c0\u2032 \u2208 \u03a6\u03b4, so OPTLP(\u03a0) = OPTLP(\u03a6\u03b4).\nThe remaining case is S(\u03c0) < \u03b4. Then S(\u03c0\u2032) \u2265 B/T > \u03b4, so \u03c0\u2032 \u2208 \u03a6\u03b4. Therefore:\nOPTLP(\u03a0) = LP(P ) \u2264 LP(\u03c0) + LP(\u03c0\u2032) \u2264 LP(\u03c0) + OPTLP(\u03a6\u03b4).\nIt remains to prove that LP(\u03c0) \u2264 \u03b4T . Indeed,\nr(\u03c0) = E x,\u03c0 [\u03c0(x) \u00b7 S(\u03c0|x) ] \u2264 E x,\u03c0 [S(\u03c0|x) ] = S(\u03c0) \u2264 \u03b4.\nLP(\u03c0) = r(\u03c0) min(T, B/S(\u03c0)) \u2264 r(\u03c0)T \u2264 \u03b4T.\nPutting Lemma 20 and Lemma 20 together and optimizing \u03b4, we obtain:\nLemma 22 For each \u01eb > 0, letting \u03b4 = (2\u01ebBL/T )1/3, we have\nOPTLP(\u03a0)\u2212 OPTLP(\u03a0\u01eb) \u2264 2\u03b4T + 2\u01ebB.\nPlugging in the general result. Let REW(\u03a0\u2032) be the expected total reward when MixtureElimination is run with policy set \u03a0\u2032 which uses only K distinct actions. Recall that we actually prove a somewhat stronger version of Theorem 1: the same regret bound (1), but with respect to OPTLP(\u03a0\u2032) rather than OPT(\u03a0\u2032). In our setting we have d = 2 resource constraints (incl. time) and OPTLP(\u03a0\u2032) \u2264 B. Therefore:\nREW(\u03a0\u2032) \u2265 OPTLP(\u03a0\u2032)\u2212O (\u221a KT log (KT |\u03a0\u2032|) ) .\nPlugging in \u03a0\u2032 = \u03a0\u01eb and K = 1\u01eb , and using Lemma 22, we obtain\nREW(\u03a0\u01eb) \u2265 OPTLP(\u03a0)\u2212O ( \u01ebB + \u03b4T + \u221a T \u01eb log ( T \u01eb |\u03a0\u01eb| )) , (23)\nfor each \u01eb > 0 and \u03b4 = (2\u01ebBL/T )1/3. We obtain Theorem 3 choosing \u01eb = (BL)\u22122/5 T\u22121/5 (log(T |\u03a0\u01eb|))3/5 and noting |\u03a0\u01eb| \u2264 |\u03a0|."}, {"heading": "10. Conclusions and open questions", "text": "We define a very general setting for contextual bandits with resource constraints (denoted RCB). We design an algorithm for this problem, and derive a regret bound which achieves the optimal root-T scaling in terms of the time horizon T , and the optimal \u221a log |\u03a0| scaling in terms of the policy set \u03a0. Further, we consider discretization issues, and derive a specific corollary for contextual dynamic pricing with a single product; we obtain a regret bound that applies to an arbitrary policy set \u03a0. Finally, we derive a partial lower bound which establishes a stark difference from the non-contextual version. These results set the stage for further study of RCB, as discussed below.\nThe main question left open by this work is to combine provable regret bounds and a computationally efficient (CE) implementation. While we focused on the statistical properties, we believe our techniques are unlikely to lead to CE implementations. Achieving near-optimal regret bounds in a CE way has been a major open question for contextual bandits with policy sets (without resource constraints). This question has been resolved in the positive in a simultaneous and independent work (Agarwal et al., 2014). Very recently, a follow-up paper (Agrawal et al., 2015) has\nachieved the corresponding advance on RCB, by combing the techniques from Agarwal et al. (2014) and Agrawal and Devanur (2014) (which, in turn, builds on Badanidiyuru et al. (2013a)).\nComputational issues aside, several open questions concern our regret bounds. First, it is desirable to achieve the same regret bounds without assuming a known time horizon T (as it is in most bandit problems in the literature). This may be difficult because time is one of the resource constraints in our problem, and our techniques rely on knowing all resource constraints in advance. More generally, one can consider a version of RCB in which some of the resource constraints are not fully revealed to an algorithm; instead, the algorithm receives updated estimates of these constrains over time.\nSecond, while our main regret bound in Theorem 1 is optimal in the important regime when OPT(\u03a0) and B are at least a constant fraction of T , it is not tight for some other regimes. For a concrete comparison, consider problem instances with a constant number of resources (d), a constant number of actions (K), and OPT(\u03a0) \u2265 \u2126(B). Then, ignoring logarithmic factors, we obtain regret OPT(\u03a0) \u221a T/B, whereas the lower bound in Badanidiyuru et al. (2013a) is OPT(\u03a0)/ \u221a B. So there is a gap when B \u226a T . Likewise, for contextual dynamic pricing with a single product, there is a gap between our algorithmic result (Theorem 3) and the B2/3 lower bound for the non-contextual case from Babaioff et al. (2015). In both cases, both upper and lower bounds can potentially be improved.\nThird, for special cases when actions correspond to prices one would like to extend the discretization approach beyond contextual dynamic pricing with a single product. However, this is problematic even without contexts: essentially, nothing is known whenever one has multiple resource constraints, and even with a single resource constraint (besides time) the solutions are very non-trivial; see Badanidiyuru et al. (2013a) for more discussion.\nFourth, if there are no contexts or resource constraints then one can achieve O(log T ) regret with an instance dependent constant; it is not clear whether one can meaningfully extend this result to contextual bandits with resource constraints.\nThe model of RCB can be extended in several directions, two of which we outline below. The most immediate extension is to an unknown distribution of context arrivals. This extension has been addressed, among other results, in the follow-up paper (Agrawal et al., 2015). The most important extension, in our opinion, would be from a stationary environment to one controlled by an adversary (perhaps restricted in some natural way). We are not aware of any prior work in this direction, even for the non-contextual version."}], "references": [{"title": "Contextual bandit learning under the realizability assumption", "author": ["Alekh Agarwal", "Miroslav Dudik", "Satyen Kale", "John Langford"], "venue": "In 15th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert Schapire"], "venue": "In 31st Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Bandits with concave rewards and convex knapsacks", "author": ["Shipra Agrawal", "Nikhil R. Devanur"], "venue": "In 15th ACM Conf. on Economics and Computation (ACM EC),", "citeRegEx": "Agrawal and Devanur.,? \\Q2014\\E", "shortCiteRegEx": "Agrawal and Devanur.", "year": 2014}, {"title": "Linear contextual bandits with global constraints and objective", "author": ["Shipra Agrawal", "Nikhil R. Devanur"], "venue": "Jul 2015. e-report,", "citeRegEx": "Agrawal and Devanur.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal and Devanur.", "year": 2015}, {"title": "Contextual bandits with global constraints and objective", "author": ["Shipra Agrawal", "Nikhil R. Devanur", "Lihong Li"], "venue": "Jun 2015. e-report,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "J. of Machine Learning Research (JMLR),", "citeRegEx": "Auer.,? \\Q2002\\E", "shortCiteRegEx": "Auer.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Dynamic pricing with limited supply", "author": ["Moshe Babaioff", "Shaddin Dughmi", "Robert D. Kleinberg", "Aleksandrs Slivkins"], "venue": "ACM Trans. on Economics and Computation,", "citeRegEx": "Babaioff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Babaioff et al\\.", "year": 2015}, {"title": "Learning on a budget: posted price mechanisms for online procurement", "author": ["Ashwinkumar Badanidiyuru", "Robert Kleinberg", "Yaron Singer"], "venue": "In 13th ACM Conf. on Electronic Commerce (EC),", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2012}, {"title": "Bandits with knapsacks", "author": ["Ashwinkumar Badanidiyuru", "Robert Kleinberg", "Aleksandrs Slivkins"], "venue": "In 54th IEEE Symp. on Foundations of Computer Science (FOCS),", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2013}, {"title": "Bandits with knapsacks", "author": ["Ashwinkumar Badanidiyuru", "Robert Kleinberg", "Aleksandrs Slivkins"], "venue": "A technical report on arxiv.org.,", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2013}, {"title": "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms", "author": ["Omar Besbes", "Assaf Zeevi"], "venue": "Operations Research,", "citeRegEx": "Besbes and Zeevi.,? \\Q2009\\E", "shortCiteRegEx": "Besbes and Zeevi.", "year": 2009}, {"title": "On the minimax complexity of pricing in a changing environment", "author": ["Omar Besbes", "Assaf Zeevi"], "venue": "Operations Reseach,", "citeRegEx": "Besbes and Zeevi.,? \\Q2011\\E", "shortCiteRegEx": "Besbes and Zeevi.", "year": 2011}, {"title": "Blind network revenue management", "author": ["Omar Besbes", "Assaf J. Zeevi"], "venue": "Operations Research,", "citeRegEx": "Besbes and Zeevi.,? \\Q2012\\E", "shortCiteRegEx": "Besbes and Zeevi.", "year": 2012}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E. Schapire"], "venue": "In 14th Intl. Conf. on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "The best of both worlds: stochastic and adversarial bandits", "author": ["S\u00e9bastien Bubeck", "Aleksandrs Slivkins"], "venue": "In 25th Conf. on Learning Theory (COLT),", "citeRegEx": "Bubeck and Slivkins.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Slivkins.", "year": 2012}, {"title": "The spending constraint model for market equilibrium: Algorithmic, existence and uniqueness results", "author": ["Nikhil Devanur", "Vijay Vazirani"], "venue": "In 36th ACM Symp. on Theory of Computing (STOC),", "citeRegEx": "Devanur and Vazirani.,? \\Q2004\\E", "shortCiteRegEx": "Devanur and Vazirani.", "year": 2004}, {"title": "The AdWords problem: Online keyword matching with budgeted bidders under random permutations", "author": ["Nikhil R. Devanur", "Thomas P. Hayes"], "venue": "In 10th ACM Conf. on Electronic Commerce (EC),", "citeRegEx": "Devanur and Hayes.,? \\Q2009\\E", "shortCiteRegEx": "Devanur and Hayes.", "year": 2009}, {"title": "Near optimal online algorithms and fast approximation algorithms for resource allocation problems", "author": ["Nikhil R. Devanur", "Kamal Jain", "Balasubramanian Sivan", "Christopher A. Wilkens"], "venue": "In 12th ACM Conf. on Electronic Commerce (EC),", "citeRegEx": "Devanur et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Devanur et al\\.", "year": 2011}, {"title": "Efficient optimal leanring for contextual bandits", "author": ["Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In 27th Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dudik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2011}, {"title": "On tail probabilities for martingales", "author": ["D.A. Freedman"], "venue": "The Annals of Probability,", "citeRegEx": "Freedman.,? \\Q1975\\E", "shortCiteRegEx": "Freedman.", "year": 1975}, {"title": "Multi-Armed Bandit Allocation Indices", "author": ["John Gittins", "Kevin Glazebrook", "Richard Weber"], "venue": null, "citeRegEx": "Gittins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gittins et al\\.", "year": 2011}, {"title": "Multi-armed Bandits with Metric Switching Costs", "author": ["Sudipta Guha", "Kamesh Munagala"], "venue": "In 36th Intl. Colloquium on Automata, Languages and Programming (ICALP),", "citeRegEx": "Guha and Munagala.,? \\Q2007\\E", "shortCiteRegEx": "Guha and Munagala.", "year": 2007}, {"title": "Approximation algorithms for restless bandit problems", "author": ["Sudipta Guha", "Kamesh Munagala", "Peng Shi"], "venue": "IEEE FOCS 2007 and ACM-SIAM SODA", "citeRegEx": "Guha et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2010}, {"title": "Approximation algorithms for correlated knapsacks and non-martingale bandits", "author": ["Anupam Gupta", "Ravishankar Krishnaswamy", "Marco Molinaro", "R. Ravi"], "venue": "In 52nd IEEE Symp. on Foundations of Computer Science (FOCS),", "citeRegEx": "Gupta et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2011}, {"title": "Continuous time associative bandit problems", "author": ["Andr\u00e1s Gy\u00f6rgy", "Levente Kocsis", "Ivett Szab\u00f3", "Csaba Szepesv\u00e1ri"], "venue": "In 20th Intl. Joint Conf. on Artificial Intelligence (IJCAI),", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Sharp dichotomies for regret minimization in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins"], "venue": "In 21st ACM-SIAM Symp. on Discrete Algorithms (SODA),", "citeRegEx": "Kleinberg and Slivkins.,? \\Q2010\\E", "shortCiteRegEx": "Kleinberg and Slivkins.", "year": 2010}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In 40th ACM Symp. on Theory of Computing (STOC),", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits", "author": ["John Langford", "Tong Zhang"], "venue": "In 21st Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Langford and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "Tighter bounds for multi-armed bandits with expert advice", "author": ["Brendan McMahan", "Matthew Streeter"], "venue": "In 22nd Conf. on Learning Theory (COLT),", "citeRegEx": "McMahan and Streeter.,? \\Q2009\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2009}, {"title": "Truthful incentives in crowdsourcing tasks using regret minimization mechanisms", "author": ["Adish Singla", "Andreas Krause"], "venue": "In 22nd Intl. World Wide Web Conf. (WWW),", "citeRegEx": "Singla and Krause.,? \\Q2013\\E", "shortCiteRegEx": "Singla and Krause.", "year": 2013}, {"title": "On general minimax theorems", "author": ["Maurice Sion"], "venue": "Pac. J. Math.,", "citeRegEx": "Sion.,? \\Q1958\\E", "shortCiteRegEx": "Sion.", "year": 1958}, {"title": "Dynamic ad allocation: Bandits with budgets", "author": ["Aleksandrs Slivkins"], "venue": "A technical report on arxiv.org/abs/1306.0155,", "citeRegEx": "Slivkins.,? \\Q2013\\E", "shortCiteRegEx": "Slivkins.", "year": 2013}, {"title": "markets: Theoretical challenges", "author": ["William R. Thompson"], "venue": "SIGecom Exchanges,", "citeRegEx": "Thompson.,? \\Q2013\\E", "shortCiteRegEx": "Thompson.", "year": 2013}, {"title": "Dynamic pricing with limited supply. The algorithm is a monopolistic seller with a limited inventory. In the basic version, there is a limited supply of identical items. In each round, a new customer arrives, the algorithm picks a price, and offers one item for sale at this price", "author": ["Badanidiyuru"], "venue": null, "citeRegEx": "Badanidiyuru,? \\Q2013\\E", "shortCiteRegEx": "Badanidiyuru", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "We consider very general settings for both contextual bandits (arbitrary policy sets, Dudik et al. (2011)) and bandits with resource constraints (bandits with knapsacks, Badanidiyuru et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 8, "context": "(2011)) and bandits with resource constraints (bandits with knapsacks, Badanidiyuru et al. (2013a)), and prove a regret guarantee with near-optimal statistical properties.", "startOffset": 71, "endOffset": 99}, {"referenceID": 20, "context": "We define resourceful contextual bandits (in short: RCB), a common generalization of two general models for contextual bandits and bandits with resource constraints: respectively, contextual bandits with arbitrary policy sets (e.g., Langford and Zhang, 2007; Dudik et al., 2011) and bandits with knapsacks (Badanidiyuru et al.", "startOffset": 226, "endOffset": 278}, {"referenceID": 20, "context": "The \u221a log |\u03a0| term in Theorem 1 is unavoidable (Dudik et al., 2011).", "startOffset": 47, "endOffset": 67}, {"referenceID": 8, "context": "In fact, Badanidiyuru et al. (2013a) provide a complimentary \u03a9( \u221a KT ) lower bound for this regime, which holds in a very strong sense: for any given tuple (K,B, OPT(\u03a0), T ).", "startOffset": 9, "endOffset": 37}, {"referenceID": 27, "context": "Such \u201cinformation-theoretical\u201d results are common for the first solutions to new, broad problem formulations (e.g. Kleinberg et al., 2008; Kleinberg and Slivkins, 2010; Dudik et al., 2011).", "startOffset": 109, "endOffset": 188}, {"referenceID": 20, "context": "Such \u201cinformation-theoretical\u201d results are common for the first solutions to new, broad problem formulations (e.g. Kleinberg et al., 2008; Kleinberg and Slivkins, 2010; Dudik et al., 2011).", "startOffset": 109, "endOffset": 188}, {"referenceID": 6, "context": "In particular, in the prior work for RCB without resource constraints there exists an algorithm with \u00d5( \u221a KT ) regret (Auer et al., 2002; Dudik et al., 2011), but for all known computationally efficient algorithms regret scales with T as T 2/3 (Langford and Zhang, 2007).", "startOffset": 118, "endOffset": 157}, {"referenceID": 20, "context": "In particular, in the prior work for RCB without resource constraints there exists an algorithm with \u00d5( \u221a KT ) regret (Auer et al., 2002; Dudik et al., 2011), but for all known computationally efficient algorithms regret scales with T as T 2/3 (Langford and Zhang, 2007).", "startOffset": 118, "endOffset": 157}, {"referenceID": 29, "context": ", 2011), but for all known computationally efficient algorithms regret scales with T as T 2/3 (Langford and Zhang, 2007).", "startOffset": 94, "endOffset": 120}, {"referenceID": 7, "context": "The optimal regret is then O(B2/3), even for an arbitrary budget B and even without the Lipscitz assumption (Babaioff et al., 2015).", "startOffset": 108, "endOffset": 131}, {"referenceID": 10, "context": "This regret bound is most interesting for the important regime B \u2265 \u03a9(T ) (studied, for example, in Besbes and Zeevi (2009, 2011); Wang et al. (2014)).", "startOffset": 99, "endOffset": 149}, {"referenceID": 11, "context": "Lipschitz demands is a common assumption in some of the prior work on (non-contextual) dynamic pricing, even with a single product (Besbes and Zeevi, 2009; Wang et al., 2014).", "startOffset": 131, "endOffset": 174}, {"referenceID": 7, "context": "However, the optimal algorithm for the single-product case (Babaioff et al., 2015) does not need this assumption.", "startOffset": 59, "endOffset": 82}, {"referenceID": 8, "context": "In particular, the main results in Badanidiyuru et al. (2013a) directly apply to this reduced problem.", "startOffset": 35, "endOffset": 63}, {"referenceID": 22, "context": "Multi-armed bandits have been studied since Thompson (1933) in Operations Research, Economics, and several branches of Computer Science, see (Gittins et al., 2011; Bubeck and Cesa-Bianchi, 2012) for background.", "startOffset": 141, "endOffset": 194}, {"referenceID": 15, "context": "Multi-armed bandits have been studied since Thompson (1933) in Operations Research, Economics, and several branches of Computer Science, see (Gittins et al., 2011; Bubeck and Cesa-Bianchi, 2012) for background.", "startOffset": 141, "endOffset": 194}, {"referenceID": 5, "context": "Contextual Bandits (Auer, 2002; Langford and Zhang, 2007) add contextual side information which can be used in prediction.", "startOffset": 19, "endOffset": 57}, {"referenceID": 29, "context": "Contextual Bandits (Auer, 2002; Langford and Zhang, 2007) add contextual side information which can be used in prediction.", "startOffset": 19, "endOffset": 57}, {"referenceID": 15, "context": "Several versions have been studied in the literature, see (Bubeck and Cesa-Bianchi, 2012; Dudik et al., 2011; Slivkins, 2014) for a discussion.", "startOffset": 58, "endOffset": 125}, {"referenceID": 20, "context": "Several versions have been studied in the literature, see (Bubeck and Cesa-Bianchi, 2012; Dudik et al., 2011; Slivkins, 2014) for a discussion.", "startOffset": 58, "endOffset": 125}, {"referenceID": 6, "context": "For contextual bandits with policy sets, there exist two broad families of solutions, based on multiplicative weight algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) or confidence intervals (Dudik et al.", "startOffset": 128, "endOffset": 201}, {"referenceID": 30, "context": "For contextual bandits with policy sets, there exist two broad families of solutions, based on multiplicative weight algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) or confidence intervals (Dudik et al.", "startOffset": 128, "endOffset": 201}, {"referenceID": 14, "context": "For contextual bandits with policy sets, there exist two broad families of solutions, based on multiplicative weight algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) or confidence intervals (Dudik et al.", "startOffset": 128, "endOffset": 201}, {"referenceID": 20, "context": ", 2011) or confidence intervals (Dudik et al., 2011; Agarwal et al., 2012).", "startOffset": 32, "endOffset": 74}, {"referenceID": 0, "context": ", 2011) or confidence intervals (Dudik et al., 2011; Agarwal et al., 2012).", "startOffset": 32, "endOffset": 74}, {"referenceID": 7, "context": "Prior work on resource-constrained bandits includes dynamic pricing with limited supply (Babaioff et al., 2015; Besbes and Zeevi, 2009, 2012), dynamic procurement on a budget (Badanidiyuru et al.", "startOffset": 88, "endOffset": 141}, {"referenceID": 8, "context": ", 2015; Besbes and Zeevi, 2009, 2012), dynamic procurement on a budget (Badanidiyuru et al., 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al.", "startOffset": 71, "endOffset": 151}, {"referenceID": 31, "context": ", 2015; Besbes and Zeevi, 2009, 2012), dynamic procurement on a budget (Badanidiyuru et al., 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al.", "startOffset": 71, "endOffset": 151}, {"referenceID": 33, "context": ", 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al.", "startOffset": 110, "endOffset": 126}, {"referenceID": 23, "context": ", 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al., 2011; Tran-Thanh et al., 2010, 2012).", "startOffset": 177, "endOffset": 253}, {"referenceID": 25, "context": ", 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al., 2011; Tran-Thanh et al., 2010, 2012).", "startOffset": 177, "endOffset": 253}, {"referenceID": 26, "context": "To the best of our knowledge, the only prior work that explicitly considered contextual bandits with resource constraints is (Gy\u00f6rgy et al., 2007).", "startOffset": 125, "endOffset": 146}, {"referenceID": 19, "context": "Our setting can be seen as a contextual bandit version of stochastic packing (e.g. Devanur and Hayes, 2009; Devanur et al., 2011).", "startOffset": 77, "endOffset": 129}, {"referenceID": 10, "context": "Multi-armed bandits have been studied since Thompson (1933) in Operations Research, Economics, and several branches of Computer Science, see (Gittins et al.", "startOffset": 44, "endOffset": 60}, {"referenceID": 0, "context": ", 2011; Agarwal et al., 2012). We rework the confidence interval approach, incorporating and extending the ideas from the work on resource-constrained bandits (Badanidiyuru et al., 2013a). Prior work on resource-constrained bandits includes dynamic pricing with limited supply (Babaioff et al., 2015; Besbes and Zeevi, 2009, 2012), dynamic procurement on a budget (Badanidiyuru et al., 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al., 2011; Tran-Thanh et al., 2010, 2012). Badanidiyuru et al. (2013a) define and optimally solve a common generalization of all these settings: the non-contextual version of RCB.", "startOffset": 8, "endOffset": 667}, {"referenceID": 0, "context": ", 2011; Agarwal et al., 2012). We rework the confidence interval approach, incorporating and extending the ideas from the work on resource-constrained bandits (Badanidiyuru et al., 2013a). Prior work on resource-constrained bandits includes dynamic pricing with limited supply (Babaioff et al., 2015; Besbes and Zeevi, 2009, 2012), dynamic procurement on a budget (Badanidiyuru et al., 2012; Singla and Krause, 2013; Slivkins and Vaughan, 2013), dynamic ad allocation with advertisers\u2019 budgets (Slivkins, 2013), and bandits with a single deterministic resource (Guha and Munagala, 2007; Gupta et al., 2011; Tran-Thanh et al., 2010, 2012). Badanidiyuru et al. (2013a) define and optimally solve a common generalization of all these settings: the non-contextual version of RCB. An extensive discussion of these and other applications, including applications to repeated auctions and network routing, can be found in (Badanidiyuru et al., 2013a). To the best of our knowledge, the only prior work that explicitly considered contextual bandits with resource constraints is (Gy\u00f6rgy et al., 2007). This paper considers a somewhat incomparable setting with arbitrary policy sets and a single constrained resource: time, whose consumption is stochastic and depends on the context and the chosen action. Gy\u00f6rgy et al. (2007) design an algorithm whose regret scales O(f(t) log t) for any time t, where f is any positive diverging function and the constant in O() depends on the problem instance and on f .", "startOffset": 8, "endOffset": 1316}, {"referenceID": 24, "context": "While we approximate our benchmark OPT(\u03a0) with a linear program optimum, our algorithm and analysis are conceptually very different from the vast literature on approximately solving linear programs, and in particular from LP-based work on bandit problems such as Guha et al. (2010).", "startOffset": 263, "endOffset": 282}, {"referenceID": 2, "context": "Agrawal and Devanur (2014) study a model for contextual bandits with resource constraints that is incomparable with ours.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "Agrawal and Devanur (2014) also claimed an extension to contexts that change over time, which has subsequently been retracted (see Footnote 1 in Agrawal and Devanur (2015)).", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "Agrawal and Devanur (2014) also claimed an extension to contexts that change over time, which has subsequently been retracted (see Footnote 1 in Agrawal and Devanur (2015)).", "startOffset": 0, "endOffset": 172}, {"referenceID": 2, "context": "Agrawal and Devanur (2014) also claimed an extension to contexts that change over time, which has subsequently been retracted (see Footnote 1 in Agrawal and Devanur (2015)). This extension constitutes the main result in Agrawal and Devanur (2015) (which is subsequent work relative to the present paper).", "startOffset": 0, "endOffset": 247}, {"referenceID": 7, "context": "As the value of P is difficult to characterize exactly, we approximate it (generalizing the approach from (Babaioff et al., 2015; Badanidiyuru et al., 2013a) for the non-contextual version).", "startOffset": 106, "endOffset": 157}, {"referenceID": 8, "context": "Lemma 4 and Lemma 5 are proved for the non-contextual version of RCB in Badanidiyuru et al. (2013a). The general case can be reduced to the non-contextual version via a standard reduction where actions in the new problem correspond to policies in \u03a0 in the original problem.", "startOffset": 72, "endOffset": 100}, {"referenceID": 8, "context": "Lemma 4 and Lemma 5 are proved for the non-contextual version of RCB in Badanidiyuru et al. (2013a). The general case can be reduced to the non-contextual version via a standard reduction where actions in the new problem correspond to policies in \u03a0 in the original problem. For Lemma 5, Badanidiyuru et al. (2013a) obtain an LP-perfect distribution by mixing an LP-optimal distribution with the \u201cnull action\u201d; this is why we allow the null action in the setting.", "startOffset": 72, "endOffset": 315}, {"referenceID": 20, "context": "In what follows we extend the minimax argument from Dudik et al. (2011). Our proof works for any q0 \u2208 [0, 1 2 ] and any compact and convex set F \u2282 F\u03a0.", "startOffset": 52, "endOffset": 72}, {"referenceID": 32, "context": "We use a min-max argument: noting that f is a convex function of P and a concave function of Z , by the Sion\u2019s minimax theorem (Sion, 1958) we have that", "startOffset": 127, "endOffset": 139}, {"referenceID": 21, "context": "To analyze Equations (6-7), we will use Bernstein\u2019s inequality for martingales (Freedman, 1975), via the following formulation from Bubeck and Slivkins (2012):", "startOffset": 79, "endOffset": 95}, {"referenceID": 16, "context": "To analyze Equations (6-7), we will use Bernstein\u2019s inequality for martingales (Freedman, 1975), via the following formulation from Bubeck and Slivkins (2012):", "startOffset": 132, "endOffset": 159}, {"referenceID": 8, "context": "Proof Using a trivial reduction to the non-contextual case (when a policy corresponds to an action in the bandits-with-knapsacks problem), one can use a generic discretization result from Badanidiyuru et al. (2013a). According to this result (specialized to contextual dynamic pricing), it suffices to prove that for each policy \u03c0 \u2208 \u03a6\u03b4 the following two properties hold: (P1) S(\u03c0\u01eb) \u2265 S(\u03c0), (P2) r(\u03c0\u01eb)/S(\u03c0\u01eb) \u2265 r(\u03c0)/S(\u03c0) \u2212 \u01eb(1 + L\u03b4\u22122), as long as S(\u03c0\u01eb) > 0.", "startOffset": 188, "endOffset": 216}, {"referenceID": 1, "context": "This question has been resolved in the positive in a simultaneous and independent work (Agarwal et al., 2014).", "startOffset": 87, "endOffset": 109}, {"referenceID": 4, "context": "Very recently, a follow-up paper (Agrawal et al., 2015) has", "startOffset": 33, "endOffset": 55}, {"referenceID": 0, "context": "achieved the corresponding advance on RCB, by combing the techniques from Agarwal et al. (2014) and Agrawal and Devanur (2014) (which, in turn, builds on Badanidiyuru et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 0, "context": "achieved the corresponding advance on RCB, by combing the techniques from Agarwal et al. (2014) and Agrawal and Devanur (2014) (which, in turn, builds on Badanidiyuru et al.", "startOffset": 74, "endOffset": 127}, {"referenceID": 0, "context": "achieved the corresponding advance on RCB, by combing the techniques from Agarwal et al. (2014) and Agrawal and Devanur (2014) (which, in turn, builds on Badanidiyuru et al. (2013a)).", "startOffset": 74, "endOffset": 182}, {"referenceID": 7, "context": "Then, ignoring logarithmic factors, we obtain regret OPT(\u03a0) \u221a T/B, whereas the lower bound in Badanidiyuru et al. (2013a) is OPT(\u03a0)/ \u221a B.", "startOffset": 94, "endOffset": 122}, {"referenceID": 7, "context": "Likewise, for contextual dynamic pricing with a single product, there is a gap between our algorithmic result (Theorem 3) and the B2/3 lower bound for the non-contextual case from Babaioff et al. (2015). In both cases, both upper and lower bounds can potentially be improved.", "startOffset": 180, "endOffset": 203}, {"referenceID": 7, "context": "Likewise, for contextual dynamic pricing with a single product, there is a gap between our algorithmic result (Theorem 3) and the B2/3 lower bound for the non-contextual case from Babaioff et al. (2015). In both cases, both upper and lower bounds can potentially be improved. Third, for special cases when actions correspond to prices one would like to extend the discretization approach beyond contextual dynamic pricing with a single product. However, this is problematic even without contexts: essentially, nothing is known whenever one has multiple resource constraints, and even with a single resource constraint (besides time) the solutions are very non-trivial; see Badanidiyuru et al. (2013a) for more discussion.", "startOffset": 180, "endOffset": 701}, {"referenceID": 4, "context": "This extension has been addressed, among other results, in the follow-up paper (Agrawal et al., 2015).", "startOffset": 79, "endOffset": 101}], "year": 2015, "abstractText": "We study contextual bandits with ancillary constraints on resources, which are common in realworld applications such as choosing ads or dynamic pricing of items. We design the first algorithm for solving these problems that handles constrained resources other than time, and improves over a trivial reduction to the non-contextual case. We consider very general settings for both contextual bandits (arbitrary policy sets, Dudik et al. (2011)) and bandits with resource constraints (bandits with knapsacks, Badanidiyuru et al. (2013a)), and prove a regret guarantee with near-optimal statistical properties.", "creator": "LaTeX with hyperref package"}}}