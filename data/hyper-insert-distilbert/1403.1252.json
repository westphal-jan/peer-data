{"id": "1403.1252", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Inducing Language Networks from Continuous Space Word Representations", "abstract": "relatively recent organizational advancements in unsupervised feature learning services have recently developed powerful latent representations of symbolic words. however, realizing it is constantly still if not thoroughly clear what makes one imaginary representation cognition better than another and currently how rapidly we can fundamentally learn why the ideal representation. understanding the structure of latent spaces attained is key to initiating any future historical advancement outlined in unsupervised learning. in completing this work, we introduce in a largely new view of continuous space word representations structured as real language networks. we would explore two enabling techniques to essentially create isolated language networks existing from learned features by inducing them for linking two popular word interaction representation methods described and examining the properties of pairing their shared resulting networks. gradually we find that utilizing the induced networks somehow differ apart from into other methods of creating instant language networks, and that they contain meaningful discourse community structure.", "histories": [["v1", "Thu, 6 Mar 2014 01:36:53 GMT  (4879kb,D)", "https://arxiv.org/abs/1403.1252v1", "14 pages"], ["v2", "Fri, 27 Jun 2014 17:36:43 GMT  (4879kb,D)", "http://arxiv.org/abs/1403.1252v2", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.SI", "authors": ["bryan perozzi", "rami al-rfou", "vivek kulkarni", "steven skiena"], "accepted": false, "id": "1403.1252"}, "pdf": {"name": "1403.1252.pdf", "metadata": {"source": "CRF", "title": "Inducing Language Networks from Continuous Space Word Representations", "authors": ["Bryan Perozzi", "Rami Al-Rfou", "Vivek Kulkarni", "Steven Skiena"], "emails": ["bperozzi@cs.stonybrook.edu", "ralrfou@cs.stonybrook.edu", "vvkulkarni@cs.stonybrook.edu", "skiena@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "In this work, we introduce a new view of continuous space word representations as language networks. We explore two techniques to create language networks from learned features by inducing them for two popular word representation methods and examining the properties of their resulting networks. We find that the induced networks differ from other methods of creating language networks, and that they contain meaningful community structure."}, {"heading": "1 Introduction", "text": "Unsupervised feature learning (deep learning) utilizes huge amounts of raw data to learn representations that model knowledge structure and disentangle the explanatory factors behind observed events. Under this framework, symbolic sparse data is represented by lower-dimensional continuous spaces. Integrating knowledge in this format is the secret behind many recent breakthroughs in machine learning based applications such as speech recognition, computer vision, and natural language processing (NLP) [3]. We focus here on word representations (word embeddings) where each word representation consists of a dense, real-valued vector. During the pre-training stage, the representations acquire the desirable property that similar words have lower distance to each other than to unrelated words [15]. This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24]. Several methods and algorithms have been proposed to learn word representations along different benchmarks for evaluation [10]. However, these evaluations are hard to comprehend as they squash the analysis of the ar X iv :1 40 3.\n12 52\nv2 [\ncs .L\nG ]\n2 7\nJu n\nrepresentation\u2019s quality into abstract numbers. To enable better understanding of the actual structure of word relationships which have been captured, we have to address the problems that come with analyzing high-dimensional spaces (typically between 50-1000 dimensions). We believe that network induction and graph analysis are appropriate tools to give us new insights. In this work, we seek to induce meaningful graphs from these continuous space language models. Specifically, our contributions include: \u2013 Analysis of Language Network Induction - We propose two\ncriteria to induce networks out of continuous embeddings. For both methods, we study and analyze the characteristics of the induced networks. Moreover, the networks generated lead to easy to understand visualizations. \u2013 Comparison Between Word Representation Methods - We evaluate the quality of two well known words embeddings. We contrast between their characteristics using the analysis developed earlier. The remainder of this paper is set up as follows. First, in Section 2, we describe continuous space language models that we consider. In Section 3, we discuss the choices involved with inducing a network from these embeddings and examine the resulting networks. Finally, we finish with a discussion of future work and our conclusions."}, {"heading": "2 Continuous Space Language Models", "text": "The goal of a language model is to assign a probability for any given sequence of words estimating the likelihood of observing such a sequence. The training objective usually maximizes the joint probability of the training corpus. A continuous space probabilistic language model aims to estimate such probability distribution by, first, learning continuous representations for the words and phrases observed in the language. Such mapping is useful to cope with the curse of dimensionality in cases where data distribution is sparse as natural language. Moreover, these representations could be used as features for natural language processing applications, domain adaptation and learning transfer scenarios that involve text or speech. More precisely, given a sequence of words S = [w1 . . . wk], we want to maximize P (w1, . . . , wk) and learn representations for words. During the training process the continuous space language model learns a mapping of words to points in Rd, where d usually ranges between 20\u2212 200. Prior to training we build a vocabulary V that consists of the most frequent |V | words, we map each word to a unique identifier that indexes an embeddings matrix C that has a size of |V | \u00d7 d. The sequence S is now represented by a matrix [ C[w1] T . . . C[wk] T ]T\n, enabling us to compose a new representation of the sequence using one of several compositional functions. The simplest is to concatenate all the rows in a bigger vector with size kd. Another option is to sum the matrix row-wise to produce a smaller representation of size d. While the first respects the order of the words, it is more expensive to compute.\nGiven a specific sequence representation as an input, we will define a task that the model should solve, given the sequence representation as the only input. Our choice of the task ranges from predicting the next/previous word(s) to distinguishing between observed phrases and other corrupted copies of them. The chosen task and/or the compositional function influence the learned representations greatly as we will discuss later. We will focus our investigations, here, on two embeddings which are trained with different tasks and compositional functions; the Polyglot and SkipGram embeddings."}, {"heading": "2.1 Polyglot", "text": "The Polyglot project offers word representations for each language in Wikipedia [22]. For large enough Wikipedias, the vocabulary consists of the most frequent 100,000 words. The representations are learned through a procedure similar to the one proposed by Collobert et al. [11]. For a given sequence of words St = [wt\u2212k . . . wt . . . wt+k] observed in the corpus T , a corrupted sequence S\u2032t will be constructed by replacing the word in the middle wt with a word wj chosen randomly from the vocabulary V . Once the vectors are retrieved, we compose the sequence representation by concatenating the vectors into one vector called the projection layer St. The model is penalized through the hinge loss function,\n1\nT t=T\u2211 t=1 |1\u2212 score(S\u2032t) + score(St)|+\nwhere score is calculated through a hidden layer neural network\nscore(St) = W2(tanh(W1St + b1)) + b2.\nFor this work, we use the Polyglot English embeddings1 which consist of the 100,000 most frequent words in the English Wikipedia, each represented by a vector in R64."}, {"heading": "2.2 SkipGram", "text": "While the Polyglot embeddings consider the order of words to build the representation of any sequence of words, the SkipGram model proposed by Mikolov et al. [16] maximizes the average log probability of the context words independent of their order\n1\nT T\u2211 t=1 [ k\u2211 j=\u2212k log p(wt+j |wt) ]\nwhere k is the size of the training window. This allows the model to scale to larger context windows. In our case, we train a SkipGram model2 on the English Wikipedia corpus offered by the Polyglot project for the most frequent 350,000 words with context size k set to 5 and the embeddings vector size set to 64.\n1 Polyglot embeddings and corpus available at http://bit.ly/embeddings 2 SkipGram training tool available at https://code.google.com/p/word2vec/"}, {"heading": "2.3 Random", "text": "In order to have a baseline, we also generate random embeddings for the most frequent 100,000 words. The initial position of words in the Polyglot embeddings were sampled from a uniform distribution, therefore, we generate the random embedding vectors by sampling from U(m\u0304\u2212\u03c3, m\u0304+\u03c3), where m\u0304 and \u03c3 are the mean and standard deviation of the trained Polyglot embeddings\u2019 values respectively. This baseline allows us to see how the language networks we construct differ from networks induced from randomly initialized points."}, {"heading": "3 Word Embedding Networks", "text": "We now consider the problem of constructing a meaningful network given a continuous space language model. As there are a variety of ways in which such a network could be induced, we start by developing a list of desirable properties for a language network. Specifically, we are seeking to build a network which: 1. Is Connected - In a connected graph, all the words can be related\nto each other. This allows for a consistent approach when trying to use the network to solve real-world problems. 2. Has Low Noise - Minimizing the spurious correlations captured by our discrete representation will make it more useful for application tasks. 3. Has Understandable Clusters - We desire that the community structure in the network reflects the syntactic and semantic information encoded in the word embeddings. We also require a method to compute the distance in the embedding space. While there are a variety of metrics that could be used, we found that Euclidean distance worked well. So we use:\ndist(x, y) = ||x\u2212 y||22 = ( m\u2211 i=1 (xi \u2212 yi)2)(1/2) (1)\nwhere x and y are words in an d-dimensional embedding space (x, y \u2208 Rd). With these criteria and a distance function in hand, we are ready to proceed. We examine two approaches for constructing graphs from word embeddings, both of which seek to link words together which are close in the embedding space. For each method, we induce networks for the 20, 000 most frequent words for each embedding type, and compare their properties.\n3.1 k-Nearest Neighbors\nThe first approach we will consider is to link each word to the k closest points in the embedding space. More formally, we induce a set of directed edges through this method:\nEknn = {(u, v) : min x dist(u, v)} \u2200u, v \u2208 V, x \u2264 k (2)\nwhere minx denotes the rank of the x-th number in ascending sorted order (e.g. min0 is the minimum element, min1 the next smallest number). After obtaining a directed graph in this fashion, we convert it to an undirected one.\nThe resulting undirected graph does not have a constant degree distribution. This is due to the fact that the nearest-neighbor relation may not be symmetric. Although all vertices in the original directed graph have an out-degree of k, their orientation in the embedding space means that some vertices will have higher in-degrees than others. Results from our investigation of basic network properties of the k-NN embedding graphs are shown in Figures 1 and 2. In (1a) we find that the embedding graphs have few disconnected components, even for small values of k. In addition, there is an obvious GCC which quickly emerges. In this way, the embeddings are similar to the network induced on random points (which is fully connected at k = 2). We performed an investigation of the smaller connected components when k was small, and found them to contain dense groupings of words with very similar usage characteristics (including ordinal values, such as Roman numerals (II,III,IV)). In (2a) we see that the clustering coefficient initially grows quickly as we add edges to our network (k \u2264 6), but has leveled off by (k = 20). This tendency to bridge new clusters together, rather than just expand existing ones, may be related to the instability of the nearest neighbor [6] in high dimensional spaces. In (2b), we see that the networks induced by the k-NN are not only connected, but have a highly modular community structure.\n3.2 d-Proximity\nThe second approach we will consider is to link each word to all those within a fixed distance d of it:\nEproximity = {(u, v) : dist(u, v) < d} \u2200u, v \u2208 V (3)\nWe perform a similar investigation of the network properties of embedding graphs constructed with the d-Proximity method. The results are shown in Figures 1 and 2. We find that networks induced through this method quickly connect words that are near each other in the embedding space, but do not bridge distant groups together. They have a large number of connected components, and connecting 90% of the vertices requires using a relatively large value of d (1b). The number of connected components is closely related to the average distance between points in the embedding space (around d =(3.25, 3.80, 2.28) for (SkipGram, Polyglot, Random)). As the value of d grows closer to this average distance, the graph quickly approaches the complete graph. Figure 2a shows that as we add more edges to the network, we add triangles at a fast rate than using the k-NN method."}, {"heading": "3.3 Discussion", "text": "Here we discuss the differences exposed between the methods for inducing word embeddings, and the differences exposed between the embeddings themselves.\nComparison of Network Induction Methods. Which method then, provides the better networks from word embeddings? To answer this question, we will use the properties raised at the beginning of this section:\n1. Connectedness - Networks induced through the k-NN method connect much faster (as a function of edges) than those induced through d-Proximity (Fig. 1). Specifically, the network induced for k = 6 has nearly full coverage (1a) with only 100K edges (2a).\n2. Spurious Edges - We desire that our resulting networks should be modular. As such we would prefer to add edges between members of a community, instead of bridging communities together. For low values of |E|, the k-NN approach creates networks which have more closed triangles (2a). However this does not hold in networks with more edges. 3. Understandable Clusters - In order to qualitatively examine the quality of such a language network, we induced a subgraph with the k-NN of the most frequent 5,000 words in the Polyglot embeddings for English. Figure 3 presents the language network constructed for (k = 6).\nAccording to our three criteria, k-NN seems better than d-Proximity. In addition to the reasons we already listed, we prefer k-NN as it seems to require less parameterization (d-Proximity has a different optimal d for each embedding type).\nComparison of Polyglot and SkipGram. Having chosen to use k-NN as our preferred method for inducing language networks, we now examine the difference between the Polyglot and SkipGram networks. Clustering Coefficient. We note that in Figure 2a, the SkipGram model has a consistently higher clustering coefficient than Polyglot in k-NN networks. A larger clustering coefficient denotes more triangles, and this may indicate that points in the SkipGram space form more cohesive local clusters than those in Polyglot. Tighter local clustering may explain some of the interesting regularities observed in the SkipGram embedding [18]. Modularity. In Figure 2b, we see that Polyglot modularity is consistently above the SkipGram modularity. SkipGram\u2019s embeddings capture more semantic information about the relations between words, and it may be that causes a less optimal community structure than Polygot whose embeddings are syntactically clustered. Clustering Visulizations. In order to understand the differences between the language networks better, we conducted an examination of the clusters found using the Louvain method [8] for modularity maximization. Figure 4 examines communities from both Polyglot and SkipGram in detail."}, {"heading": "4 Related Work", "text": "Here we discuss the relevant work in language networks, and word embeddings. There is also related work on the theoretical properties of nearest neighbor graphs, consult Eppstein, Paterson, and Yao [12] for some basic investigations."}, {"heading": "4.1 Language Networks", "text": "Word Co-occurrences. One branch of the study of language as networks seeks to build networks directly from a corpus of raw text. Cancho and\nSole\u0301 [9] examine word co-occurrence graphs as a method to analyze language. In their graph, edges connect words which appear below a fixed threshold (d \u2264 2) from each other in sentences. They find that networks constructed in this manner show both small world structure, and a power law degree distribution. Language networks based on word co-occurrence have been used in a variety of natural language processing tasks, including motif analysis of semantics [7], text summarization [1] and resolving disambiguation of word usages [26].\nHypernym relations. Another approach to studying language networks relies on studying the relationships between words exposed by a written language reference. Motter et al. [21] use a thesaurus to construct a network of synonyms, which they find to find to exhibit small world structure. In [25], Sigman and Cecchi investigate the graph structure of the Wordnet lexicon. They find that the semantic edges in Wordnet follow scale invariant behavior and that the inclusion of polysemous edges drastically raises the clustering coefficient, creating a small world effect in the network. Relation to our work. Much of the previous work in language networks build networks that are prone to noise from spurious correlations in word co-occurrence or infrequent word senses [9, 25]. Dimensionality reduction techniques have been successful in mitigating the effects of noise in a variety of domains. The word embedding methods we examine are a form of dimensionality reduction that has improved performance on several NLP tasks and benchmarks. The networks produced in our work are considerably different from language networks created by previous work that we are aware of. We find that our degree distribution does appear to follow a power-law (like [9, 21, 25]) and we have some small world properties like those present in those works (such as C Crandom). However, the average path length in our graphs is considerably larger than the average path length in random graphs with the same node and edge cardinalities. Table 1 shows a comparison of metrics from different approaches to creating language networks.3"}, {"heading": "4.2 Word Embeddings", "text": "Distributed representations were first proposed by Hinton [14], to learn a mapping of symbolic data to continuous space. These representations\n3 Our induced networks available at http://bit.ly/inducing_language_networks\nare able to capture fine grain structures and regularities in the data [18]. However, training these models is slow due to their complexity. Usually, these models are trained using back-propagation algorithm [23] which requires large amount of computational resources. With the recent advancement in hardware performance, Bengio et al. [4] used the distributed representations to produce a state-of-the-art probabilistic language model. The model maps each word in a predefined vocabulary V to a point in Rd space (word embeddings). The model was trained on a cluster of machines for days. More applications followed, Collobert et al. [11] developed SENNA, a system that offers part of speech tagger, chunker, named entity recognizer, semantic role labeler and discriminative syntactic parser using the distributed word representations. To speed up the training procedure, importance sampling [5] and hierarchical softmax models [19, 20] were proposed to reduce the computational costs. The training of word representations involves minimal amount of language specific knowledge and expertise. Al-Rfou, Perozzi, and Skiena [22] trained word embeddings for more than a hundred languages and showed that the representations help building multilingual applications with minimal human effort. Recently, SkipGram and Continuous bag of words models were proposed by Mikolov et al. [16] as simpler and faster alternatives to neural network based models."}, {"heading": "5 Conclusions", "text": "We have investigated the properties of recently proposed distributed word representations, which have shown results in several machine learning applications. Despite their usefulness, understanding the mechanisms which afford them their characteristics is still a hard problem. In this work, we presented an approach for viewing word embeddings as a language network. We examined the characteristics of the induced networks, and their community structure. Using this analysis, we were able to develop a procedure which develops a connected graph with meaningful clusters. We believe that this work will set the stage for advances in both NLP techniques which utilize distributed word representations, and in understanding the properties of the machine learning processes which generate them. Much remains to be done. In the future we would like to focus on comparing word embeddings to other well known distributional representation techniques (e.g. LDA/LSA), examining the effects of different vocabulary types (e.g. topic words, entities) on the induced graphs, and the stability of the graph properties as a function of network size."}, {"heading": "Acknowledgments", "text": "This research was partially supported by NSF Grants DBI-1060572 and IIS-1017181, with additional support from TASC Inc, and a Google Faculty Research Award."}], "references": [{"title": "A complex network approach to text summarization", "author": ["L. Antiqueira", "O.N.O. Jr.", "L. da Fontoura Costa", "M. das Gra\u00e7as Volpe Nunes"], "venue": "Information Sciences", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Gephi: An Open Source Software for Exploring and Manipulating Networks", "author": ["M. Bastian", "S. Heymann", "M. Jacomy"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning. Springer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Y. Bengio", "J.-S. Senecal"], "venue": "Neural Networks, IEEE Transactions on 19.4", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "When is \u201cnearest neighbor\u201d meaningful?", "author": ["K. Beyer", "J. Goldstein", "R. Ramakrishnan", "U. Shaft"], "venue": "Database Theory\u2014ICDT\u201999. Springer,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Quantifying Semantics using Complex Network Analysis", "author": ["C. Biemann", "S. Roos", "K. Weihe"], "venue": "Proceedings of COLING", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "The small world of human language", "author": ["R.F. i. Cancho", "R.V. Sol\u00e9"], "venue": "Proceedings of the Royal Society of London. Series B: Biological Sciences", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "The Expressive Power of Word Embeddings", "author": ["Y. Chen", "B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "ICML 2013 Workshop on Deep Learning for Audio, Speech, and Language Processing. Vol. abs/1301.3226. Atlanta,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On nearest-neighbor graphs", "author": ["D. Eppstein", "M.S. Paterson", "F.F. Yao"], "venue": "Discrete & Computational Geometry", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In: vol", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Learning distributed representations of concepts", "author": ["G.E. Hinton"], "venue": "Proceedings of the eighth annual conference of the cognitive science society. Amherst,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "In: Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "Proceedings of NAACL- HLT", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of the international workshop on artificial intelligence and statistics", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Topology of the conceptual network of language", "author": ["A.E. Motter", "A.P.S. de Moura", "Y.-C. Lai", "P. Dasgupta"], "venue": "In: Phys. Rev. E", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Polyglot: Distributed Word Representations for Multilingual NLP", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Sofia, Bulgaria: Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Learning internal representation by back propagation", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1986}, {"title": "Large, pruned or continuous space language models on a GPU for statistical machine translation", "author": ["H. Schwenk", "A. Rousseau", "M. Attik"], "venue": "Proceedings of the NAACL-HLT", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Global organization of the Wordnet lexicon", "author": ["M. Sigman", "G.A. Cecchi"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "HyperLex: lexical cartography for information retrieval", "author": ["J. V\u00e9ronis"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "Integrating knowledge in this format is the secret behind many recent breakthroughs in machine learning based applications such as speech recognition, computer vision, and natural language processing (NLP) [3].", "startOffset": 206, "endOffset": 209}, {"referenceID": 14, "context": "During the pre-training stage, the representations acquire the desirable property that similar words have lower distance to each other than to unrelated words [15].", "startOffset": 159, "endOffset": 163}, {"referenceID": 10, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 12, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 16, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 23, "context": "This allows the representations to utilize the abundance of raw text available to learn features and knowledge that is essential for supervised learning applications such as part-of-speech tagging, named entity recognition, machine translation, language modeling, sentiment analysis etc [11, 13, 17, 24].", "startOffset": 287, "endOffset": 303}, {"referenceID": 9, "context": "Several methods and algorithms have been proposed to learn word representations along different benchmarks for evaluation [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "The Polyglot project offers word representations for each language in Wikipedia [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] maximizes the average log probability of the context words independent of their order", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This tendency to bridge new clusters together, rather than just expand existing ones, may be related to the instability of the nearest neighbor [6] in high dimensional spaces.", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "In (2a), C shown for k = [2,30] and d = [0.", "startOffset": 25, "endOffset": 31}, {"referenceID": 17, "context": "Tighter local clustering may explain some of the interesting regularities observed in the SkipGram embedding [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "In order to understand the differences between the language networks better, we conducted an examination of the clusters found using the Louvain method [8] for modularity maximization.", "startOffset": 152, "endOffset": 155}, {"referenceID": 11, "context": "There is also related work on the theoretical properties of nearest neighbor graphs, consult Eppstein, Paterson, and Yao [12] for some basic investigations.", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "Sol\u00e9 [9] examine word co-occurrence graphs as a method to analyze language.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "Language networks based on word co-occurrence have been used in a variety of natural language processing tasks, including motif analysis of semantics [7], text summarization [1] and resolving disambiguation of word usages [26].", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "Language networks based on word co-occurrence have been used in a variety of natural language processing tasks, including motif analysis of semantics [7], text summarization [1] and resolving disambiguation of word usages [26].", "startOffset": 174, "endOffset": 177}, {"referenceID": 25, "context": "Language networks based on word co-occurrence have been used in a variety of natural language processing tasks, including motif analysis of semantics [7], text summarization [1] and resolving disambiguation of word usages [26].", "startOffset": 222, "endOffset": 226}, {"referenceID": 1, "context": ") Images created with Gephi [2].", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "[21] use a thesaurus to construct a network of synonyms, which they find to find to exhibit small world structure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "In [25], Sigman and Cecchi investigate the graph structure of the Wordnet lexicon.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "Much of the previous work in language networks build networks that are prone to noise from spurious correlations in word co-occurrence or infrequent word senses [9, 25].", "startOffset": 161, "endOffset": 168}, {"referenceID": 24, "context": "Much of the previous work in language networks build networks that are prone to noise from spurious correlations in word co-occurrence or infrequent word senses [9, 25].", "startOffset": 161, "endOffset": 168}, {"referenceID": 8, "context": "We find that our degree distribution does appear to follow a power-law (like [9, 21, 25]) and we have some small world properties like those present in those works (such as C Crandom).", "startOffset": 77, "endOffset": 88}, {"referenceID": 20, "context": "We find that our degree distribution does appear to follow a power-law (like [9, 21, 25]) and we have some small world properties like those present in those works (such as C Crandom).", "startOffset": 77, "endOffset": 88}, {"referenceID": 24, "context": "We find that our degree distribution does appear to follow a power-law (like [9, 21, 25]) and we have some small world properties like those present in those works (such as C Crandom).", "startOffset": 77, "endOffset": 88}, {"referenceID": 8, "context": "|V | |E| C Crandom pl plrandom \u03b3 Cancho and Sol\u00e9 [9](UWN) 478, 773 1.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "70 Cancho and Sol\u00e9 [9](RWN) 460, 902 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "[21] 30, 244 \u2212 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Distributed representations were first proposed by Hinton [14], to learn a mapping of symbolic data to continuous space.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "are able to capture fine grain structures and regularities in the data [18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Usually, these models are trained using back-propagation algorithm [23] which requires large amount of computational resources.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "[4] used the distributed representations to produce a state-of-the-art probabilistic language model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] developed SENNA, a system that offers part of speech tagger, chunker, named entity recognizer, semantic role labeler and discriminative syntactic parser using the distributed word representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "To speed up the training procedure, importance sampling [5] and hierarchical softmax models [19, 20] were proposed to reduce the computational costs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 18, "context": "To speed up the training procedure, importance sampling [5] and hierarchical softmax models [19, 20] were proposed to reduce the computational costs.", "startOffset": 92, "endOffset": 100}, {"referenceID": 19, "context": "To speed up the training procedure, importance sampling [5] and hierarchical softmax models [19, 20] were proposed to reduce the computational costs.", "startOffset": 92, "endOffset": 100}, {"referenceID": 21, "context": "Al-Rfou, Perozzi, and Skiena [22] trained word embeddings for more than a hundred languages and showed that the representations help building multilingual applications with minimal human effort.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "[16] as simpler and faster alternatives to neural network based models.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Recent advancements in unsupervised feature learning have developed powerful latent representations of words. However, it is still not clear what makes one representation better than another and how we can learn the ideal representation. Understanding the structure of latent spaces attained is key to any future advancement in unsupervised learning. In this work, we introduce a new view of continuous space word representations as language networks. We explore two techniques to create language networks from learned features by inducing them for two popular word representation methods and examining the properties of their resulting networks. We find that the induced networks differ from other methods of creating language networks, and that they contain meaningful community structure.", "creator": "LaTeX with hyperref package"}}}