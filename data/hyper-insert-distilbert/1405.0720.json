{"id": "1405.0720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2014", "title": "Probabilistic Inductive Logic Programming Based on Answer Set Programming", "abstract": "we propose creating a new formal language pattern for preparing the correct expressive representation of probabilistic optimization knowledge based on standard answer set programming ( in asp ). where it allows for determining the annotation of first - order formulas as exceptionally well as asp rules and comparing facts with probabilities and consequences for learning semantics of such weights directly from data ( parameter item estimation ). specific weighted fuzzy formulas are usually given a semantics in terms of soft and hard constraints which uniquely determine a probability statistical distribution set over many answer feature sets. when in formal contrast to related approaches, we derive approach model inference by optionally fitting utilizing so - are called streamlining vs xor logic constraints, in this order to reduce noise the number estimate of computed answer element sets.... our approach protocol is prototypically explicitly implemented. few examples illustrate the introduced concepts and point at issues and information topics likely for future descriptive research.", "histories": [["v1", "Sun, 4 May 2014 17:18:49 GMT  (28kb)", "http://arxiv.org/abs/1405.0720v1", "Appears in the Proceedings of the 15th International Workshop on Non-Monotonic Reasoning (NMR 2014)"]], "COMMENTS": "Appears in the Proceedings of the 15th International Workshop on Non-Monotonic Reasoning (NMR 2014)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["matthias nickles", "alessandra mileo"], "accepted": false, "id": "1405.0720"}, "pdf": {"name": "1405.0720.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Alessandra Mileo"], "emails": ["matthias.nickles@deri.org", "alessandra.mileo@deri.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n07 20\nv1 [\ncs .A\nI] 4\nM ay\n2 01\nKeywords: Uncertainty Reasoning, Answer Set Programming, Probabilistic Inductive Logic Programming, Statistical Relational Learning, SAT"}, {"heading": "1 Introduction", "text": "Reasoning in the presence of uncertainty and relational structures (such as social networks and Linked Data) is an important aspect of knowledge discovery and representation for the Web, the Internet Of Things, and other potentially heterogeneous and complex domains. Probabilistic logic programing, and the ability to learn probabilistic logic programs from data, can provide an attractive approach to uncertainty reasoning and statistical relational learning, since it combines the deduction power and declarative nature of logic programming with probabilistic inference abilities traditionally known from less expressive graphical models such as Bayesian and Markov networks. A very successful type of logic programming for nonmonotonic domains is Answer Set Programming (ASP) (Lifschitz 2002; Gelfond and Lifschitz 1988). Since statistical-relational approaches to probabilistic reasoning often rely heavily on the\n\u2217This work is an extended and revised version of A. Mileo, M. Nickles: Probabilistic Inductive Answer Set Programming by Model Sampling and Counting. First International Workshop on Learning and Nonmonotonic Reasoning (LNMR 2013), Corunna, Spain, 2013.\npropositionalization of first-order or other relational information, ASP appears to be an ideal basis for probabilistic logic programming, given its expressiveness and the existence of highly optimized grounders and solvers. However, despite the successful employment of conceptually related approaches in the area of SAT for probabilistic inference tasks, only a small number of approaches to probabilistic knowledge representation or probabilistic inductive logic programming under the stable model semantics exist so far, of which some are rather restrictive wrt. expressiveness and parameter estimation techniques. We build upon these and other existing approaches in the area of probabilistic (inductive) logic programming in order to provide a new ASPbased probabilistic logic programming language (with firstorder as well as ASP basic syntax) for the representation of probabilistic knowledge. Weights which directly represent probabilities can be attached to arbitrary formulas, and we show how this can be used to perform probabilistic inference and how weights of hypotheses can be inductively learned from given relational examples. To the best of our knowledge, this is the first ASP-based approach to probabilistic (inductive) logic programming which does not impose restrictions on the annotation of ASP-rules and facts as well as FOL-style formulas with probabilities.\nThe remainder of this paper is organized as follows: the next section presents relevant related approaches. Section 3 introduces syntax and semantics of our new language. Section 4 presents our approach to probabilistic inference (including examples), and Section 5 shows how formula weights can be learned from data. Section 6 concludes."}, {"heading": "2 Related Work", "text": "Being one of the early approaches to the logic-based representation of uncertainty sparked by Nilsson\u2019s seminal work (Nilsson 1986), (Halpern 1990) presents three different probabilistic first-order languages, and compares them with a related approach by Bacchus (Bacchus 1990). One language has a domain-frequency (or statistical) semantics, one has a possible worlds semantics (like our approach), and one bridges both types of semantics. While those languages as such are mainly of theoretical relevance, their types of semantics still form the backbone of most practically relevant contemporary approaches. Many newer approaches, including Markov Logic Net-\nworks (see below), require a possibly expensive grounding (propositionalization) of first-order theories over finite domains. A recent approach which does not fall into this category but employs the principle of maximum entropy in favor of performing extensive groundings is (Thimm and Kern-Isberner 2012). However, since ASP is predestined for efficient grounding, we do not see grounding necessarily as a shortcoming. Stochastic Logic Programs (SLPs) (Muggleton 2000) are an influential approach where sets of rules in form of range-restricted clauses can be labeled with probabilities. Parameter learning for SLPs is approached in (Cussens 2000) using the EM-algorithm. Approaches which combine concepts from Bayesian network theory with relational modeling and learning are, e.g., (Friedman et al. 1999; Kersting and Raedt 2000; Laskey and Costa 2005). Probabilistic Relational Models (PRM) (Friedman et al. 1999) can be seen as relational counterparts to Bayesian networks In contrast to these, our approach does not directly relate to graphical models such as Bayesian or Markov Networks but works on arbitrary possible worlds which are generated by ASP solvers. ProbLog (Raedt, Kimmig, and Toivonen 2007) allows for probabilistic facts and definite clauses, and approaches to probabilistic rule and parameter learning (from interpretations) also exist for ProbLog. Inference is based on weighted model counting, which is similarly to our approach, but uses Boolean satisfiability instead of stable model search. ProbLog builds upon the very influential Distribution Semantics introduced for PRISM (Sato and Kameya 1997), which is also used by other approaches, such as Independent Choice Logic (ICL) (Poole 1997). Another important approach outside the area of ASP are Markov Logic Networks (MLN) (Richardson and Domingos 2006), which are related to ours. A MLN consists of first-order formulas annotated with weights (which are not probabilities). MLNs are used as \u201ctemplates\u201d from which Markov networks are constructed, i.e., graphical models for the joint distribution of a set of random variables. The (ground) Markov network generated from the MLN then determines a probability distribution over possible worlds. MLNs are syntactically similar to the logic programs in our framework (in our framework, weighted formulas can also be seen as soft or hard constraints for possible worlds), however, in contrast to MLN, we allow for probabilities as formula weights. Our initial approach to weight learning is closely related to certain approaches to MLN parameter learning (e.g., (Lowd and Domingos 2007)), as described in Section 5. Located in the field of nonmonotonic logic programming, our approach is also influenced by P-log (Baral, Gelfond, and Rushton 2009) and abduction-based rule learning in probabilistic nonmonotonic domains (Corapi et al. 2011). With P-log, our approaches shares the view that answer sets can be seen as possible worlds in the sense of (Nilsson 1986). However, the syntax of P-log is quite different from our language, by restricting probabilistic annotations to certain syntactical forms and by the concept of independent experiments, which simplifies the implementation of their framework. In distinction from P-log, there is no particular coverage for causality modeling\nin our framework. (Corapi et al. 2011) allows to associate probabilities with abducibles and to learn both rules and probabilistic weights from given data (in form of literals). In contrast, our present approach does not comprise rule learning. However, our weight learning algorithm allows for learning from any kind of formulas and for the specification of virtually any sort of hypothesis as learning target, not only sets of abducibles. Both (Corapi et al. 2011) and our approach employ gradient descent for weight learning. Other approaches to probabilistic logic programming based on the stable model semantics for the logic aspects include (Saad and Pontelli 2005) and (Ng and Subrahmanian 1994). (Saad and Pontelli 2005) appears to be a powerful approach, but restricts probabilistic weighting to certain types of formulas, in order to achieve a low computational reasoning complexity. Its probabilistic annotation scheme is similar to that proposed in (Ng and Subrahmanian 1994). (Ng and Subrahmanian 1994) provides both a language and an in-depth investigation of the stable model semantics (in particular the semantics of non-monotonic negation) of probabilistic deductive databases. Our approach (and ASP in general) is closely related to SAT solving, #SAT and constraint solving. ASP formulas in our language are constraints for possible worlds (legitimate models). As (Sang, Beame, and Kautz 2005) shows, Bayesian networks can be \u201ctranslated\u201d into a weighted model counting problem over propositional formulas, which is related to our approach to probabilistic inference, although details are quite different. Also, the XOR constraining approach (Gomes, Sabharwal, and Selman 2006) employed for sampling of answer sets (Section 4) has originally been invented for the sampling of propositional truth assignments."}, {"heading": "3 Probabilistic Answer Set Programming with PrASP", "text": "Before we turn to probabilistic inference and parameter estimation, we introduce our new language for probabilistic non-monotonic logic programming, called Probabilistic Answer Set Programming (PrASP ).\nSyntax: Just add probabilities To remove unnecessary syntax restrictions and because we will later require certain syntactic modifications of given programs which are easier to express in FirstOrder Logic (FOL) notation, we allow for FOL statements in our logic programs, using the F2LP conversion tool (Lee and Palla 2009). More precisely, a PrASP program consists of ground or non-ground formulas in unrestricted first-order syntax annotated with numerical weights (provided by some domain expert or learned from data). Weights directly represent probabilities. If the weights are removed, and provided finite variable domains, any such program can be converted into an equivalent answer set program by means of the transformation described in (Lee and Palla 2009).\nLet \u03a6 be a set of function, predicate and object symbols andL(\u03a6) a first-order language over\u03a6 and the usual connec-\ntives (including both strong negation \u201c-\u201d and default negation \u201cnot\u201d) and first-order quantifiers. Formally, a PrASP program is a non-empty finite set {([p], fi)} of PrASP formulas where each formula fi \u2208 L(\u03a6) is annotated with a weight [p]. A weight directly represents a probability (provided it is probabilistically sound). If the weight is omitted for some formula of the program, weight [1] is assumed. The weight p of [p] f is denoted as w(f). Weighted formulas can intuitively seen as constraints which specify which possible worlds are indeed possible, and with which probability. Let \u039b\u2212 denote PrASP program \u039b stripped of all weights. Weights need to be probabilistically sound, in the sense that the system of inequalities (1) - (4) in Section 3 must have at least one solution (however, in practice this does not need to be strictly the case, since the constraint solver employed for finding a probability distribution over possible worlds can find approximate solutions often even if the given weights are inconsistent).\nIn order to translate conjunctions of unweighted formulas in first-order syntax into disjunctive programs with a stable model semantics, we further define transformation lp : L(\u03a6) \u222a dLp(\u03a6) \u2192 dLp(\u03a6), where dLp(\u03a6) is the set of all disjunctive programs over \u03a6. The details of this transformation can be found in (Lee and Palla 2009)1. Applied to rules and facts in ASP syntax, lp simply returns these. This allows to make use of the wide range of advanced possibilities offered by contemporary ASP grounders in addition to FOL syntax (such as aggregates), although when defining the semantics of programs, we consider only formulas in FOL syntax.\nSemantics\nThe probabilities attached to formulas in a PrASP program induce a probability distribution over answer sets of an ordinary answer set program which we call the spanning program associated with that PrASP program. Informally, the idea is to transform a PrASP program into an answer set program whose answer sets reflect the nondeterminism introduced by the probabilistic weights: each annotated formula might hold as well as not hold (unless its weight is [0] or [1]). Of course, this transformation is lossy, so we need to memorize the weights for the later computation of a probability distribution over possible worlds. The important aspect of the spanning program is that it programmatically generates a set of possible worlds in form of answer sets. Technically, the spanning program \u03c1(\u039b) of PrASP program \u039b is a disjunctive program obtained by transformation lp(\u039b\u2032). We generate \u039b\u2032 from \u039b by removing all weights and transforming each formerly weighted formula f into a disjunction f |not f , where not stands for default negation and | stands for the disjunction in ASP (so probabilities are \u201cdefault probabilities\u201d in our framework). Note that\n1The use of the translation into ASP syntax requires either an ASP solver which can deal directly with disjunctive logic programs (such as claspD) or a grounder which is able to shift disjunctions from the head of the respective rules into the bodies, such as gringo (Gebser, Kaufmann, and Schaub 2012).\nf |not f doesn\u2019t guarantee that answer sets are generated for weighted formula f . By using ASP choice constructs such as aggregates and disjunctions, the user can basically generate as many answer sets (possible worlds) as desired.\nFormulas do not need to be ground - as defined in Section 3, they can contain existentially as well as universally quantified variables in the FOL sense (although restricted to finite domains). As an example, consider the following simple ground PrASP program (examples for PrASP programs with variables and first-order style quantifiers are presented in the next sections):\n[ 0 . 7 ] q <\u2212 p . [ 0 . 3 ] p . [ 0 . 2 ] \u2212p & r .\nThe set of answer sets (which we take as possible worlds) of the spanning program of this PrASP program is {{p, q}, {\u2212p, r}, {}, {p}}.\nThe semantics of a PrASP program \u039b and single PrASP formulas is defined in terms of a probability distribution over a set of possible worlds (in form of answer sets of \u03c1(\u039b)) in connection with the stable model semantics. This is analogously to the use of Type 2 probability structures (Halpern 1990) for first-order probabilistic logics with probabilities, but restricted to finite domains of discourse.\nLet M = (D,\u0398, \u03c0, \u00b5) be a probability structure where D is a finite discrete domain of objects, \u0398 is a non-empty set of possible worlds, \u03c0 a function which assigns to the symbols in \u03a6 (see Section 3) predicates, functions and objects over/from D, and \u00b5 a discrete probability function over \u0398. Each possible world is a Herbrand interpretation over \u03a6. Since we will use answer sets as possible worlds, defining \u0393(a) to be the set of all answer sets of answer set program a will become handy. For example, given \u03c1(\u039b) as (uncertain) knowledge, the set of worlds deemed possible according to existing belief \u03c1(\u039b) is \u0393(\u03c1(\u039b)) in our framework.\nWe define a (non-probabilistic) satisfaction relation of possible worlds and unannotated programs as follows: let \u039b\u2212 be is an unannotated program. Then (M, \u03b8) \u0398 \u039b\u2212 iff \u03b8 \u2208 \u0393(lp(\u039b\u2212)) and \u03b8 \u2208 \u0398 (from this it follows that \u0398 induces its own closed world assumption - any answer set which is not in \u0398 is not satisfiable wrt. \u0398). The probability \u00b5({\u03b8}) of a possible world \u03b8 is denoted as Pr(\u03b8) and sometimes called \u201cweight\u201d of \u03b8. For a disjunctive program \u03c8, we analogously define (M, \u03b8) \u0398 \u03c8 iff \u03b8 \u2208 \u0393(\u03c8) and \u03b8 \u2208 \u0398.\nTo do groundwork for the computation of a probability distribution over possible worlds \u0398 which are \u201cgenerated\u201d and weighted by some given background knowledge in form of a PrASP program, we define a (non-probabilistic) satisfaction relation of possible worlds and unannotated formulas: let \u03c6 be a PrASP formula (without weight) and \u03b8 be a possible world. Then (M, \u03b8) \u039b \u03c6 iff (M, \u03b8) \u0398 \u03c1(\u039b)\u222a lp(\u03c6) and \u0398 = \u0393(\u03c1(\u039b)) (we say formula \u03c6 is true in possible world \u03b8). Sometimes we will just write \u03b8 |=\u039b \u03c6 if M is given by the context. A notable property of this definition is that it does not restrict us to single ground formulas. Essentially, an unannotated formula \u03c6 can be any answer set program specified in FOL syntax, even if its grounding\nconsists of multiple sentences. Observe that \u0398 restricts \u039b to answer sets of \u03c1(\u039b). For convenience, we will abbreviate (M, \u03b8) \u039b \u03c6 as \u03b8 \u039b \u03c6. Pr(\u03c6) denotes the probability of a formula \u03c6, with Pr(\u03c6) = \u00b5({\u03b8 \u2208 \u0398 : (M, \u03b8) \u039b \u03c6}). Note that this holds both for annotated and unannotated formulas: even if it has a weight attached, the probability of a PrASP formula is defined by means of \u00b5 and only indirectly by its manually assigned weight (weights are used below as constraints for the computation of a probabilistically consistent \u00b5). Further observe that there is no particular treatment for conditional probabilities in our framework;Pr(a|b) is simply calculated as Pr(a \u2227 b)/Pr(b). While our framework so far is general enough to account for probabilistic inference using unrestricted programs and query formulas (provided we are given a probability distribution over the possible answer sets), this generality also means a relatively high complexity in terms of computability for inference-heavy tasks which rely on the repeated application of operator \u039b, even if we would avoid the transformation lp and restrict ourselves to the use of ASP syntax.\nThe obvious question now, addressed before for other probabilistic logics, is how to compute \u00b5, i.e., how to obtain a probability distribution over possible worlds (which tells us for each possible world the probability with which this possible world is the actual world) from a given annotated program\u039b in a sound and computationally inexpensive way. Generally, we can express the search for probability distributions in form of a number of constraints which constitute a system of linear inequalities (which reduce to linear equalities for point probabilities as weights). This system typically has multiple or even infinitely many solutions (even though we do not allow for probability intervals) and computation can be costly, depending on the number of possible worlds according to \u03c1(\u039b). We define the parameterized probability distribution \u00b5(\u039b,\u0398) over a set \u0398 of answer sets as the solution (for all Pr(\u03b8i)) of the following system of linear equations and an inequality (if precisely one solution exists) or as the solution with maximum entropy (Thimm and Kern-Isberner 2012), in case multiple solutions exist 2. We require that the given weights in a PrASP program are chosen such that the following constraint system has at least one solution.\n\u2211\n\u03b8i\u2208\u0398:\u03b8i \u039bf1\nPr(\u03b8i) = w(f1) (1)\n\u00b7 \u00b7 \u00b7 \u2211\n\u03b8i\u2208\u0398:\u03b8i \u039bfn\nPr(\u03b8i) = w(fn) (2)\n\u2211\n\u03b8i\u2208\u0398\n\u03b8i = 1 (3)\n\u2200\u03b8i \u2208 \u0398 : 0 \u2264 Pr(\u03b8i) \u2264 1 (4)\n2Since in this case the number of solutions of the system of linear equations is infinite, de facto we need to choose the maximum entropy solution of some finite subset. In the current prototype implementation, we generate a user-defined number of random solutions derived from a solution computed using a constrained variant of Singular Value Decomposition and the null space of the coefficient matrix of the system of linear equations (1)-(3).\nAt this, \u039b = {f1, ..., fn} is a PrASP program. The canonical probability distribution \u00b5(\u039b) of \u039b is defined as \u00b5(\u039b,\u0393(\u03c1(\u039b))). In the rest of the paper, we refer to \u00b5(\u039b) when we refer to the probability distribution over the answer sets of the spanning program of a given PrASP program \u039b."}, {"heading": "4 Inference", "text": "Given possible world weights (\u00b5(\u039b)), probabilistic inference becomes a model counting task where each model has a weight: we can compute the probability of any query formula \u03c6 by summing up the probabilities (weights) of those possible worlds (models) where \u03c6 is true. To make this viable even for larger sets of possible worlds, we optionally restrict the calculation of \u00b5(\u039b) to a number of answer sets sampled near-uniformly at random from the total set of answer sets of the spanning program, as described in Section 4.\nAdding a sampling step and computing probabilities All tasks described so far (solving the system of (in)equalities, counting of weighted answer sets) become intractable for very large sets of possible worlds. To tackle this issue, we want to restrict the application of these tasks to a sampled subset of all possible worlds. Concretely, we want to find a way to sample (near-)uniformly from the total set of answer sets without computing a very large number of answer sets. While this way the set of answer sets cannot be computed using only a single call of the ASP solver but requires a number of separate calls (each with different sampling constraints), the required solver calls can be performed in parallel. However, a shortcoming of the sampling approach is that there is currently no way to pre-compute the size of the minimally required set of samples.\nGuaranteeing near-uniformity in answer set sampling looks like a highly non-trivial task, since any set of answers obtained from ASP solvers as a subset of the total set of answer sets is typically not uniformly distributed but strongly biased in hardly foreseeable ways (due to various interplaying heuristics applied by modern solvers), so we could not simply request any single answer set from the solver.\nHowever, we can make use of so-called XOR constraints (a form of streamlining constraints in the area of SAT solving) for near-uniform sampling (Gomes, Sabharwal, and Selman 2006) to obtain samples from the space of all answer sets, within arbitrarily narrow probabilistic bounds, using any off-the-shelf ASP solver. Compared to approaches which use Markov Chain Monte Carlo (MCMC) methods to sample from some given distribution, this method has the advantage that the sampling process is typically faster and that it requires only an off-the-shelf ASP solver (which is in the ideal case employed only once per sample, in order to obtain a single answer set). However, a shortcoming is that we are not doing Importance Sampling this way - the probability of a possible world is not taken into account but computed later\nfrom the samples. Counting answer sets could also be achieved using XOR constraints, however, this is not covered in this paper, since it does not comprise weighted counting, and we could normally not use an unweighted counting approach directly.\nXOR constraints were originally defined over a set of propositional variables, which we identify with a set of ground atoms V = {a1, ..., an}. Each XOR constraint is represented by a subset D of V \u222a {true}. D is satisfied by some model if an odd number of elements of D are satisfied by this model (i.e., the constraint acts like a parity of D). In ASP syntax, an XOR constraint can be represented for example as :- #even{ a1, ..., an } (Gebser et al. 2011). In our approach, XOR constraints are independently at random drawn from a probability distribution X(|V |, 0.5) over the set V of all possible XOR constraints over all ground atoms of the ground answer set program resulting from \u03c1(\u039b). X(|V |, 0.5) is defined such that each XOR constraint is drawn from this distribution independently at random with probability 0.5 and includes true with probability 0.5. In effect, any given XOR constraint is drawn with probability 2\u2212(|V |+1|) (see (Gomes, Sabharwal, and Selman 2006) for details). Since adding an XOR constraint to an answer set program eliminates any given answer set with probability 0.5, it cuts the set of answer sets in half in expectation. Iteratively adding a small number of XOR constraints to an answer set program therefore reduces the number of answer sets to a small number also. If this process results in a single answer set, the remaining answer set is drawn near-uniformly from the original set of answer sets, as shown in (Gomes, Sabharwal, and Selman 2006). Since for answer set programs the costs of repeating the addition of constraints until precisely a single answer set remains appears to be higher than the costs of computing somewhat too many models, we just estimate the number of required constraints and choose randomly from the resulting set of answer sets. The following way of answer set sampling using XOR constraints has been used before in Xorro (a tool which is part of the Potassco set of ASP tools (Gebser et al. 2011)) in a very similar way.\nFunction sample: \u03c8 7\u2192 \u03b3 Given any disjunctive program\u03c8, the following procedure computes a random sample \u03b3 from the set of all answer sets of \u03c8: \u03c8g \u2190 ground(\u03c8) ga \u2190 atoms(\u03c8g) xors \u2190 XOR constraints {xor1, ..., xorn} over ga, drawn from X(|V |, 0.5) \u03c8\u2032 \u2190 \u03c8 \u222a xors \u03b3 \u2190 an answer set selected randomly from \u0393(\u03c8\u2032)\nAt this, the number of constraints n is set to a value large enough to produce one or a very low number of answer sets (log2(|ga|) in our experiments).\nWe can now compute \u00b5(\u039b,\u0398\u2032) (i.e., Pr(\u03b8) for each \u03b8 \u2208\n\u0398\u2032) for a set of samples \u0398\u2032 obtained by multiple (ideally parallel) calls of sample from the spanning program \u03c1(\u039b) of PrASP program \u039b, and subsequently sum up the weights of those samples (possible worlds) where the respective query formula (whose marginal probability we want to compute) is true. Precisely, we approximate Pr(\u03c6) for a (ground or non-ground) query formula \u03c6 using:\nPr(\u03c6) \u2248 \u2211\n{\u03b8\u2032\u2208\u0398\u2032:\u03b8\u2032|=\u039b\u03c6}\nPr(\u03b8\u2032) (5)\nfor a sufficiently large set \u0398\u2032 of samples. Conditional probabilities Pr(a|b) can simply be computed as Pr(a \u2227 b)/Pr(b).\nIf sampling is not useful (i.e., if the total number of answer sets \u0398 is moderate), inference is done in the same way, we just set \u0398\u2032 = \u0398. Sampling using XOR constraints costs time too (mainly because of repeated calls of the ASP solver), and making this approach more efficient is an important aspect of future work (see Section 6).\nAs an example for inference using our current implementation, consider the following PrASP formalization of a simple coin game: coin(1..3). [0.6] coin_out(1,heads). [[0.5]] coin_out(N,heads) :- coin(N), N != 1. 1{coin_out(N,heads), coin_out(N,tails)}1 :- coin(N). n_win :- coin_out(N,tails), coin(N). win :- not n_win.\nAt this, the line starting with [[0.5]]... is syntactic sugar for a set of weighted rules where variable N is instantiated with all its possible values (i.e., [0.5] coin_out(2,heads) :- coin(2), 2 != 1 and [0.5] coin_out(3,heads) :- coin(3), 3 != 1). It would also be possible to use [0.5] as annotation of this rule, in which case the weight 0.5 would specify the probability of the whole non-ground formula instead. Our prototypical implementation accepts query formulas in format [?] a (computes the marginal probability of a) and [?|b] a (computes the conditional probability Pr(a|b)). E.g., [?] coin_out(1,tails). [?] coin_out(1,heads) | coin_out(1,tails). [?] coin_out(1,heads) & coin_out(2,heads) & coin_out(3,heads). [?] win. [?|coin_out(1,heads) & coin_out(2,heads)\ncoin_out(3,heads)] win.\n...yields the following result [0.3999999999999999] coin_out(1,tails). [1] coin_out(1,heads) | coin_out(1,tails). [0.15] coin_out(1,heads) & coin_out(2,heads) & coin_out(3,heads). [0.15] win. [1|coin_out(1,heads) & coin_out(2,heads)\n& coin_out(3,heads)] win.\nIn this example, use of sampling does not make any difference due to its small size. An example where a difference can be observed is presented in Section 5. This example also\ndemonstrates that FOL and logic programming / ASP syntax can be freely mixed in background knowledge and queries. Another simple example shows the use of FOL-style variables and quantifiers mixed with ASP-style variables:\np(1). p(2). p(3). #domain p(X). [0.5] v(1). [0.5] v(2). [0.5] v(3). [0.1] v(X).\nWith this, the following query:\n[?] v(X). #domain p(Z). [?] ![Z]: v(Z). [?] ?[Z]: v(Z).\n...results in:\n[0.1] ![Z]: v(Z). [0.8499999999999989] ?[Z]: v(Z).\nThe result of query [?] ![Z]: v(Z) with universal quantifier ![Z] is Pr(\u2200z.v(z)) = 0.1, which is also the result of the equivalent queries [?] v(1) & v(2) & v(3) and [?] v(X). In our example, this marginal probability was directly given as weight in the background knowledge. In contrast to X, variable Z is a variable in the sense of first-order logic (over a finite domain). The result of ?[Z]: v(Z) is Pr(\u2203z.v(z)) (i.e., ?[Z]: represents the existential quantifier) and could likewise be calculated manually using the inclusion-exclusion principle as Pr(v(1) \u2228 v(2) \u2228 v(3)) = Pr(v(1)) + Pr(v(2)) + Pr(v(3))\u2212Pr(v(1)\u2227v(2))\u2212Pr(v(1)\u2227v(3))\u2212Pr(v(2)\u2227 v(3)) + Pr(v(1) \u2227 v(2) \u2227 v(3)) = 0.85. Of course, existential or universal quantifiers can also be used as sub-formulas and in PrASP programs."}, {"heading": "An alternative approach: conversion into an equivalent non-probabilistic answer set program", "text": "An alternative approach to probabilistic inference without computing \u00b5 and without counting of weighted possible worlds, would be to find an unannotated first-order program \u039b\u2032 which reflects the desired probabilistic nondeterminism (choice) of a given PrASP program \u039b. Instead of defining probabilities of possible worlds, \u039b\u2032 has answers sets whose frequency (number of occurrences within the total set of answer sets) reflects the given probabilities in the original (annotated) program. To make this idea more intuitive, imagine that each possible world corresponds to a room. Instead of encountering a certain room with a certain frequency, we create further rooms which have all, from the viewpoint of the observer, the same look, size and furniture. The number of these rooms reflects the probability of this type of room. E.g., to ensure probability 13 of some literal p, \u039b\n\u2032 is created in a way such that p holds in one third of all answer sets of \u039b\u2032. This task can be considered as an elaborate variant of the generation of the (much simpler) spanning program \u03c1(\u039b).\nFinding \u039b\u2032 could be formulated as an (intractable) rule search problem (plus subsequently the conversion into ASP\nsyntax and a simple unweighted model counting task): find a non-probabilistic program \u039b\u2032 such that for each annotated formula [p]f in the original program the following holds (under the provision that the given weights are probabilistically sound):\n|{m : m \u2208 \u0393(\u039b\u2032),m |= f}|\n|\u0393(\u039b\u2032)| = p. (6)\nUnfortunately, the direct search approach to this would be obviously intractable.\nHowever, in the special case of mutually independent formulas we can omit the rule learning task by conditioning each formula in \u039b by a nondeterministic choice amongst the truth conditions of a number of \u201chelper atoms\u201d hi (which will later be ignored when we count the resulting answer sets), in order to \u201cemulate\u201d the respective probability specified by the weight. If (and only if) the formulas are mutually independent, the obtained \u039b\u2032 is isomorphic to the original probabilistic program. In detail, conditioning means to replace each formula [w] f by formulas 1{h1, ..., hn}1, f \u2190 h1|...|hm and not f \u2190 not (h1|...|hm), where the hi are new names (the aforementioned \u201chelper atoms\u201d), m n = w and m < n (remember that we allow for weight constraints as well as FOL syntax).\nIn case the transformation accurately reflects the original uncertain program, we could now calculate marginal probabilities simply by determining the percentage of those answer sets in which the respective query formula is true (ignoring any helper atoms introduced in the conversion step), with no need for computing \u00b5(\u039b). As an example, consider the following program: coin(1..10). [0.6] coin_out(1,heads). [[0.5]] coin_out(N,heads) :- coin(N), N != 1.\n1{coin_out(N,heads), coin_out(N,tails)}1 :- coin(N). n_win :- coin_out(N,tails), coin(N). win :- not n_win.\nSince coin tosses are mutually independent, we can transform it into the following equivalent un-annotated form (the hpatomn are the \u201chelper atoms\u201d. Rules are written as disjunctions):\ncoin(1..10). 1{hpatom1,hpatom2,hpatom3,hpatom4,hpatom5}1. (coin_out(1,heads)) | -(hpatom1|hpatom2|hpatom3). not (coin_out(1,heads)) | (hpatom1|hpatom2|hpatom3). 1{hpatom6,hpatom7}1. (coin_out(10,heads)) | -(hpatom6). not (coin_out(10,heads)) | (hpatom6). 1{hpatom8,hpatom9}1. (coin_out(9,heads)) | -(hpatom8). not (coin_out(9,heads)) | (hpatom8). 1{hpatom10,hpatom11}1. (coin_out(8,heads)) | -(hpatom10). not (coin_out(8,heads)) | (hpatom10). 1{hpatom12,hpatom13}1. (coin_out(7,heads)) | -(hpatom12).\nnot (coin_out(7,heads)) | (hpatom12). 1{hpatom14,hpatom15}1. (coin_out(6,heads)) | -(hpatom14). not (coin_out(6,heads)) | (hpatom14). 1{hpatom16,hpatom17}1. (coin_out(5,heads)) | -(hpatom16). not (coin_out(5,heads)) | (hpatom16). 1{hpatom18,hpatom19}1. (coin_out(4,heads)) | -(hpatom18). not (coin_out(4,heads)) | (hpatom18). 1{hpatom20,hpatom21}1. (coin_out(3,heads)) | -(hpatom20). not (coin_out(3,heads)) | (hpatom20). 1{hpatom22,hpatom23}1. (coin_out(2,heads)) | -(hpatom22). not (coin_out(2,heads)) | (hpatom22). 1{coin_out(N,heads), coin_out(N,tails)}1\n:- coin(N). n_win :- coin_out(N,tails), coin(N). win :- not n_win.\nExemplary query results: [0.001171875] win. [0.998828125] not win. [0.6] coin_out(1,heads). [0.5] coin_out(2,heads).\nWhat is remarkable here is that no equation solving task (computation of \u00b5(\u039b)) is required to compute these results. However, this does not normally lead to improved inference speed, due to the larger amount of time required for the computation of models."}, {"heading": "5 Weight Learning", "text": "Generally, the task of parameter learning in probabilistic inductive logic programming is to find probabilistic parameters (weights) of logical formulas which maximize the likelihood given some data (learning examples) (Raedt and Kersting 2008). In our case, the hypothesis H (a set of formulas without weights) is provided by an expert, optionally together with some PrASP program as background knowledge B. The goal is then to discover weights w of the formulas H such that Pr(E|Hw\u222aB) is maximized given example formulas E = e1, e2, .... Formally, we want to compute argmaxw(Pr(E|Hw \u222aB)) = argmaxw( \u220f\nei\u2208E\nPr(ei|Hw \u222aB))\n(7)\n(Making the usual i.i.d. assumption regarding the individual examples in E. Hw denotes the hypothesis weighted with weight vector w.)\nThis results in an optimization task which is related but not identical to weight learning for, e.g., MLNs and (Corapi et al. 2011). In MLNs, typically a database (possible world) is given whose likelihood should be maximized, e.g. using a generative approach (Lowd and Domingos 2007) by gradient descent. Another related approach distinguishes a priori between evidence atoms X and query atoms Y and seeks to maximize the likelihood Pr(Y |X), again using gradient descent (Huynh and Mooney 2008). At this, costheavy inference is avoided as far as possible, e.g., by optimization of the pseudo-(log-)likelihood instead ot the (log)likelihood or by approximations of costly counts of true\nformula groundings in a certain possible world (the basic computation in MLN inference). In contrast, the current implementation of PrASP learns weights from any formulas and not just literals (or, more precisely as for MLNs: atoms, where negation is implicit using a closed-world assumption). Furthermore, the maximization targets are different (Pr(possible world) or Pr(Y |X)) vs. Pr(E|Hw \u222aB)).\nRegarding the need to reduce inference when learning, PrASP parameter estimation should in principle make no exception, since inference can still be costly even when probabilities are inferred only approximately by use of sampling. However, in our preliminary experiments we found that at least in relatively simple scenarios, there is no need to resort to inference-free approximations such as pseudo-(log)likelihood. The pseudo-(log-)likelihood approach presented in early works on MLNs (Richardson and Domingos 2006) would also require a probabilistic ground formula independence analysis in our case, since in PrASP there is no obvious equivalent to Markov blankets. Note that we assume that the example data is nonprobabilistic and fully observable.\nLet H = {f1, ..., fn} be a given set of formulas and a vector w = (w1, ..., wn) of (unknown) weights of these formulas. Using the Barzilai and Borwein method (Barzilai and Borwein 1988) (a variant of the gradient descent approach with possibly superlinear convergence), we seek to find w such that Pr(E|Hw \u222aB) is maximized (Hw denotes the formulas in H with the weights w such that each fi is weighted with wi). Any existing weights of formulas in the background knowledge ar not touched, which can significantly reduce learning complexity if H is comparatively small. Probabilistic or unobservable examples are not considered.\nThe learning algorithm (Barzilai and Borwein 1988) is as follows:\nRepeat for k = 0, 1, ... until convergence: Set sk = 1\u03b1k\u25bd(Pr(E|Hwk \u222aB)) Set wk+1 = wk + sk Set yk = \u25bd(Pr(E|Hwk+1 \u222aB))\u2212 \u25bd(Pr(E|Hwk \u222aB))\nSet \u03b1k+1 = sT k yk\nsT k sk\nAt this, the initial gradient ascent step size \u03b10 and the initial weight vectorw0 can be chosen freely.Pr(E|Hw\u222aB) denotes\n\u220f ei\u2208E\nPr(ei|Hw \u222a B) inferred using vector w as weights for the hypothesis formulas, and\n\u25bd(Pr(E|Hw \u222a B)) = (8)\n( \u2202\n\u2202w1 Pr(E|Hw \u222aB), ...,\n\u2202\n\u2202wn Pr(E|Hw \u222aB)) (9)\nSince we usually cannot practically express Pr(E|Hw \u222a B) in dependency of w in closed form, at a first glance, the above formalization appears to be not very helpful. However, we can still resort to numerical differentiation and ap-\nproximate\n\u25bd(Pr(E|Hw \u222aB)) = (10)\n( lim h\u21920\nPr(E|H(w1+h,...,wn) \u222aB)\u2212 Pr(E|H(w1,...,wn) \u222aB)\nh ,\n(11) ...,\nlim h\u21920\nPr(E|H(w1,...,wn+h) \u222a B)\u2212 Pr(E|H(w1,...,wn) \u222aB)\nh )\n(12)\nby computing the above vector (dropping the limit operator) for a sufficiently small h (in our prototypical implementation, h = \u221a \u01ebwi is used, where \u01eb is an upper bound to the rounding error using the machine\u2019s double-precision floating point arithmetic). This approach has the benefit of allowing in principle for any maximization target (not just E). In particular, any unweighted formulas (unnegated and negated facts as well as rules) can be used as (positive) examples.\nAs a small example both for inference and weight learning using our preliminary implementation, consider the following fragment of a an nonmonotonic indoor localization scenario, which consists of estimating the position of a person, and determining how this person moves a certain number of steps around the environment until a safe position is reached:\n[0.6] moved(1). [0.2] moved(2). point(1..100). 1{atpoint(X):point(X)}1. distance(1) :- moved(1). distance(2) :- moved(2). atpoint(29) | atpoint(30) | atpoint(31)\n| atpoint(32) | atpoint(33) | atpoint(34) | atpoint(35) | atpoint(36) | atpoint(37) -> selected.\nsafe :- selected, not exception. exception :- distance(1).\nThe spanning program of this example has 400 answer sets. Inference of Pr(safe |distance(2)) and Pr(safe |distance(1)) without sampling requires ca. 2250 ms using our current unoptimized prototype implementation. If we increase the number of points to 1000, inference is tractable only by use of sampling (see Section 4). To demonstrate how the probability of a certain hypothesis can be learned in this simple scenario, we remove [0.6] moved(1) from the program above (with 100 points) and turn this formula (without the weight annotation) into a hypothesis. Given example data safe, parameter estimation results in Pr(moved(1)) \u2248 0, learned in ca. 3170 ms using our current prototype implementation."}, {"heading": "6 Conclusions", "text": "With this introductory paper, we have presented a novel framework for uncertainty reasoning and parameter estimation based on Answer Set Programming, with support\nfor probabilistically weighted formulas in background knowledge, hypotheses and queries. While our current framework certainly leaves room for future improvements, we believe that we have already pointed out a new venue towards more practicable probabilistic inductive answer set programming with a high degree of expressiveness. Ongoing work is focusing on performance improvements, theoretical analysis (in particular regarding minimum number of samples wrt. inference accuracy), empirical evaluation and on the investigation of viable approaches to PrASP structure learning."}, {"heading": "Acknowledgments", "text": "This work is supported by the EU FP7 CityPulse Project under grant No. 603095. http://www.ict-citypulse.eu"}], "references": [{"title": "Probabilistic reasoning with answer sets", "author": ["Gelfond Baral", "C. Rushton 2009] Baral", "M. Gelfond", "N. Rushton"], "venue": "Theory Pract. Log. Program", "citeRegEx": "Baral et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baral et al\\.", "year": 2009}, {"title": "J", "author": ["J. Barzilai", "Borwein"], "venue": "M.", "citeRegEx": "Barzilai and Borwein 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "Probabilistic rule learning in nonmonotonic domains", "author": ["Corapi"], "venue": "In Proceedings of the 12th international conference on Computational logic in multi-agent systems,", "citeRegEx": "Corapi,? \\Q2011\\E", "shortCiteRegEx": "Corapi", "year": 2011}, {"title": "J", "author": ["Cussens"], "venue": "2000. Parameter estimation in stochastic logic programs. In Machine Learning,", "citeRegEx": "Cussens 2000", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning probabilistic relational models", "author": ["Friedman"], "venue": null, "citeRegEx": "Friedman,? \\Q1999\\E", "shortCiteRegEx": "Friedman", "year": 1999}, {"title": "Potassco: The potsdam answer set solving collection", "author": ["Gebser"], "venue": "AI Commun", "citeRegEx": "Gebser,? \\Q2011\\E", "shortCiteRegEx": "Gebser", "year": 2011}, {"title": "Conflict-driven answer set solving: From theory to practice", "author": ["Kaufmann Gebser", "M. Schaub 2012] Gebser", "B. Kaufmann", "T. Schaub"], "venue": "Artificial Intelligence", "citeRegEx": "Gebser et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gebser et al\\.", "year": 2012}, {"title": "and Lifschitz", "author": ["M. Gelfond"], "venue": "V.", "citeRegEx": "Gelfond and Lifschitz 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "C", "author": ["Gomes"], "venue": "P.; Sabharwal, A.; and Selman, B.", "citeRegEx": "Gomes. Sabharwal. and Selman 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "J", "author": ["Halpern"], "venue": "Y.", "citeRegEx": "Halpern 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "L", "author": ["K. Kersting", "Raedt"], "venue": "D.", "citeRegEx": "Kersting and Raedt 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "P", "author": ["K.B. Laskey", "Costa"], "venue": "C.", "citeRegEx": "Laskey and Costa 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Palla", "author": ["J. Lee"], "venue": "R.", "citeRegEx": "Lee and Palla 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Domingos", "author": ["D. Lowd"], "venue": "P.", "citeRegEx": "Lowd and Domingos 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "V", "author": ["R.T. Ng", "Subrahmanian"], "venue": "S.", "citeRegEx": "Ng and Subrahmanian 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "N", "author": ["Nilsson"], "venue": "J.", "citeRegEx": "Nilsson 1986", "shortCiteRegEx": null, "year": 1986}, {"title": "and Kersting", "author": ["L.D. Raedt"], "venue": "K.", "citeRegEx": "Raedt and Kersting 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "L", "author": ["Raedt"], "venue": "D.; Kimmig, A.; and Toivonen, H.", "citeRegEx": "Raedt. Kimmig. and Toivonen 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Pontelli", "author": ["E. Saad"], "venue": "E.", "citeRegEx": "Saad and Pontelli 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "H", "author": ["T. Sang", "P. Beame", "Kautz"], "venue": "A.", "citeRegEx": "Sang. Beame. and Kautz 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Kameya", "author": ["T. Sato"], "venue": "Y.", "citeRegEx": "Sato and Kameya 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and KernIsberner", "author": ["M. Thimm"], "venue": "G.", "citeRegEx": "Thimm and Kern.Isberner 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2014, "abstractText": "We propose a new formal language for the expressive representation of probabilistic knowledge based on Answer Set Programming (ASP). It allows for the annotation of first-order formulas as well as ASP rules and facts with probabilities and for learning of such weights from data (parameter estimation). Weighted formulas are given a semantics in terms of soft and hard constraints which determine a probability distribution over answer sets. In contrast to related approaches, we approach inference by optionally utilizing so-called streamlining XOR constraints, in order to reduce the number of computed answer sets. Our approach is prototypically implemented. Examples illustrate the introduced concepts and point at issues and topics for future research.", "creator": "LaTeX with hyperref package"}}}