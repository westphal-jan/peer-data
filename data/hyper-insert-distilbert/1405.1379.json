{"id": "1405.1379", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2014", "title": "Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device", "abstract": "this paper addresses the comparatively challenging external scenario description for assuming the proposed distant - talking actor control driver of a music song playback software device, a common common portable speaker with four possibly small simultaneous loudspeakers in close intentional proximity to plus one miniature microphone. the user facing controls interpret the sampled device through voice, where only the simulated speech - text to - music ratio can be constrained as low as - 30 db achieved during remote music playback. subsequently we propose a multimedia speech enhancement modeling front - end that increasingly relies lightly on known robust methods for echo cancellation, including double - ended talk shock detection, and noise suppression, as well as a novel adaptive quasi - binary mask that is well suited for conventional speech crystal recognition. the optimization of the actual system mechanism is then formulated as a large step scale nonlinear programming error problem solving where the event recognition rate is maximized locally and the optimal values utilized for the system parameters are properly found through programming a genetic algorithm. we first validate our methodology by testing output over all the timit database for different music memory playback note levels and related noise types. unlike finally, we directly show off that the better proposed front - end allows a natural causal interaction with interpreting the device for limited - vocabulary functional voice commands.", "histories": [["v1", "Mon, 5 May 2014 14:37:47 GMT  (105kb,D)", "http://arxiv.org/abs/1405.1379v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["ramin pichevar", "jason wung", "daniele giacobello", "joshua atkins"], "accepted": false, "id": "1405.1379"}, "pdf": {"name": "1405.1379.pdf", "metadata": {"source": "CRF", "title": "Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device", "authors": ["Ramin Pichevar", "Jason Wung", "Daniele Giacobello"], "emails": ["ramin.pichevar@beatsbydre.com"], "sections": [{"heading": "1. Introduction", "text": "The human interaction paradigm with music playback devices has seen a dramatic shift as devices get smaller and more portable. Well-established interaction media such as remote controls are no longer adequate. Automatic speech recognition (ASR) interfaces offer a natural solution to this problem, where these devices are typically used in hands-busy, mobilityrequired scenarios [1]. Performing ASR on these small devices are highly challenging due to the music playback itself, the environmental noise, and the general environmental acoustics, e.g., reverberation [2]. In particular, due to the severe degradation of the input signal, the ASR performance drops significantly when the distance between the user and the microphone increases [3]. In the past decade, the literature on distant-talking speech interfaces provided several solutions to the problem, e.g., the DICIT project [4]. However, to the authors\u2019 knowledge, the available solutions rely heavily on large microphone arrays [5], which may be infeasible for handheld portable device.\nIn this work, we present a robust front-end speech enhancement and ASR solution for a single-microphone limitedvocabulary system during continuous monaural music playback. In contrast to previous studies, the microphone in our system is placed in close proximity to the loudspeakers, and the voice command still needs to be recognized at a very low speech-to-echo ratio (SER) while the music is playing.\nThe front-end algorithm design effort can be divided in two parts. Firstly, we tailor known double-talk robust solutions for\nThe authors thank Stephen Nimick for recording the voice commands used in the experimental evaluation.\necho cancellation and speech enhancement to retrieve a clean estimate of the command [6, 7, 8]. Secondly, we propose a novel noise reduction method, where we combine a traditional minimum mean-squared error (MMSE) speech enhancement approach [9] with an estimate of the ideal binary mask [10]. The parameters of the algorithm are tuned for maximum recognition rate by casting the tuning problem as a nonlinear program, solved efficiently through a genetic algorithm (GA) [11]. A similar approach was used in [12, 13] to maximize the objective perceptual quality of a speech enhancement system for fullduplex communication. The training and evaluation corpora are generated through a synthetic mixture of clean speech (from the TIMIT database [14]) and music, both convolved with separate impulse responses, and further mixed with a background noise to cover as many deployment scenarios as possible. The acoustic models of the ASR are trained by the front-end enhanced speech, an effective way to learn and exploit the typical distortions of the system itself [15].\nThe paper is organized as follows. In Section 2, we describe the speech enhancement algorithm and outline the parameters to be tuned. The tuning by nonlinear optimization of these parameters is presented in Section 3. The experimental results in Section 4 are divided in two parts. Firstly, we present the results of the training and evaluation of the front-end and acoustic models using the TIMIT database. Secondly, we change the language model and implement our ASR system for a limited vocabulary command recognizer in very adverse conditions. In Section 5, we conclude our work."}, {"heading": "2. Speech Enhancement System", "text": "Let y[n] be the near-end microphone signal, which consists of the near-end speech s[n] and noise v[n] mixed with the acoustic echo d[n] = h[n] \u2217 x[n], where h[n] is the impulse response of the system, x[n] is the far-end reference signal, and \u2217 is the convolution operator. The overall block diagram of the speech enhancement algorithm is shown in Figure 1, which consists of two robust acoustic echo cancelers (RAECs), a double-\nar X\niv :1\n40 5.\n13 79\nv1 [\ncs .S\nD ]\n5 M\nay 2\n01 4\ntalk probability (DTP) estimator, two residual power estimators (RPEs), a noise power estimator (NPE), a noise suppressor (NS), and a voice activity detector (VAD)."}, {"heading": "2.1. Robust Acoustic Echo Canceler", "text": "Since strong near-end interference may corrupt the error signal of the acoustic echo canceler (AEC) and cause the adaptive filter to diverge, the RAEC system [6, 8] is used, where the error recovery nonlinearity and robust adaptive step-size control allows for continuous tracking of the echo path during double talk. To reduce the delay of the frequency-domain adaptive filter [16], the multi-delay adaptive filter structure [17] is used. A cascaded structure similar to the system approach of [7] is used: the output of the first RAEC is fed to the input of the second RAEC, which is different from the original system approach in [7] where the input to the second RAEC is still the microphone signal (a parallel structure instead of the cascaded structure used in this work).\nThe tuning parameters for each of the RAECs consist of the frame size NAEC, the number of partitioned blocks MAEC, the number of iterationsNiter, the step-size \u00b5AEC, the tuning parameter \u03b3AEC for the robust adaptive step-size, and the smoothing factor \u03b1AEC for the power spectral density estimation."}, {"heading": "2.2. Residual Echo Power Estimator", "text": "Since the AEC cannot cancel all the echo signal due to modeling mismatch, further enhancement from the residual echo suppressor (RES) is required to improve the voice quality. A coherence based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar to [20] is used for a more accurate estimate of the residual echo power. As shown in Figure 1, the DTP estimator differs from that in [20] since the coherence is calculated between the RAEC estimated echo signal d\u0302 and the microphone signal y rather than between the loudspeaker signal x and the microphone signal y. This is possible since the estimated echo signal d\u0302 can be reliably obtained even during double talk due to the robust echo path tracking performance of the RAEC.\nIn this work, we propose to estimate the residual echo power by utilizing the output of the double talk probability estimator. Ideally, when the double-talk probability is high, the level of residual echo power estimate should be low so as to not distort the near-end speech when suppressing the residual echo. On the other hand, when the double-talk probability is low, the level of residual echo power estimate should be high to suppress as much residual echo as possible. The high level residual echo power \u03bbBH,k is estimated based on the coherence of the microphone signal Yk and the reference signal Xk, while the low level residual echo power \u03bbBL,k is estimated based on the coherence of the error signal Ek and the reference signal Xk. Finally, the residual echo power \u03bbB,k is estimated by utilizing the double-talk probability estimate PDTk obtained from DTP to combine \u03bbBH,k and \u03bbBL,k:\n\u03bbB,k[m] = (1\u2212 [m]PDTk [m])\u03bbBH,k[m] + P DT k [m]\u03bbBL,k[m],\n(1) where k is the frequency bin and m time frame.\nThe tuning parameters for the DTP consists of the transition probabilities a01, a10, b01, and b10, the smoothing factors \u03b1DTP and \u03b2DTP, the frequency bin range [kbegin, kend], the frame duration TDTP, and the adaptation time constants \u03c4 . The tuning parameters for the RPE consist of the numbers of partitions MRPEH andMRPEL to calculate the coherence and the smoothing\nfactors \u03b1RPEH and \u03b1RPEL for the power spectral density estimation."}, {"heading": "2.3. Noise Suppressor", "text": "In this work, we combine RPE and NPE for residual echo and noise suppression using a single noise suppressor, as shown in Figure 1. The low complexity MMSE noise power estimator [21] is used for the NPE, and the Ephraim and Malah logspectral amplitude (LSA) estimator [9] is used for the combined residual echo and noise suppression:\nGLSAk [m] = \u03bek[m]\n1 + \u03bek[m] exp\n( 1\n2 \u222b \u221e \u03bek[m]\u03b3k[m]\n1+\u03bek[m]\ne\u2212t\nt dt\n) . (2)\nThe estimation of the a priori speech-to-noise ratio (SNR) \u03bek is done using the decision-directed (DD) approach [22]:\n\u03bek[m] = \u03b1DD |S\u0302k[m\u2212 1]|2\n\u03bbV,k[m] + \u03bbB,k[m]\n+ (1\u2212 \u03b1DD)max{\u03b3k[m]\u2212 1, 0},\nwhere\n\u03b3k[m] = \u03bbE,k[m]/(\u03bbV,k[m] + \u03bbB,k[m])\nand \u03bbE,k, \u03bbV,k, and \u03bbB,k are the residual error signal power, the noise power, and residual echo power respectively.\nThe tuning parameters of the NPE consist of the fixed a priori SNR \u03beH1 , the threshold PTH, and the smoothing factors \u03b1P and \u03b1NPE The tuning parameters of the the NS consist of the smoothing factor for the SNR estimator \u03b1DD."}, {"heading": "2.4. Generation of Speech Enhancement Mask", "text": "It has been recently shown that the speech recognition accuracy in noisy condition can be greatly improved by direct binary masking [10] when compared to marginalization [23] or spectral reconstruction [24]. Given our application scenario, we propose to combine the direct masking approach, particularly effective at low overall SNRs, with the NS output mask GLSAk , as shown in Figure 1. In particular, we exploit the estimated bin-based a priori SNR \u03bek to determine the type of masking to be applied to the spectrum. However, given than an accurate estimation of the binary mask is very difficult for very low SNRs, we elect to use the LSA estimated gain for those cases. Our masking then becomes:\n\u03b6k[m] =  [(1\u2212Gmin)GLSAk [m] +Gmin], \u03bek[m] \u2264 \u03b81, \u03b1 2 , \u03b81 < \u03bek[m] < \u03b82,\n2+\u03b1 2 , \u03bek[m] \u2265 \u03b82,\nwhere Gmin is the minimum suppression gain [13], and the output is then:\nS\u0302k[m] = \u03b6k[m]Ek[m]. (3)\nIn Figure 2, we provide some data to justify our particular choice of masking. We compare three different speech enhancement methods presented in this section for unigram and bigram language models [25]. In the direct masking, \u03bek[m] is mapped directly to a constant threshold to generate the binary decision. It can be seen that our proposed method outperforms conventional methods at lower SNRs.\nThe tuning parameters for the direct masking consist of the minimum gain Gmin, the thresholds \u03b81 and \u03b82, and a tuning parameter \u03b1."}, {"heading": "3. The Tuning Problem", "text": "The tuning problem can be formalized as an optimization problem. In our case, the objective function to maximize is the ASR recognition rate R (s\u0302[n]), where s\u0302[n] is the processed speech, i.e., the output of the speech enhancement system. To restrict the search region, we can impose inequality constraints on the variables that simply determine lower and upper bounds limit for the components of the solution vector. Our optimization problem then becomes:\nmaximize R (s\u0302[n,p]) subject to U \u2264 p \u2264 L, (4)\nwhere p is now the vector of the parameters that need tuning, s\u0302[n,p] is the speech enhancement system output obtained with these parameters, and L and U represent, respectively, lower and upper bounds for the values each variable. The basic concept of a GA is to apply genetic operators, such as mutation and crossover, to evolve a set of M solutions, or population, \u03a0(k) = {p(k)m ,m = 1, . . . ,M} in order to find the solution that maximizes the cost function [11, 26]. This procedure begins with a randomly chosen population \u03a0(0) in the space of the feasible values [L,U], and it is repeated until a halting criterion is reached after K iterations. The set of parameters p (K) m \u2208 \u03a0(K) that maximizes the cost function will be our estimate:\np\u0302 = argmax p (K) m \u2208\u03a0(K)\nR ( s\u0302[n,p(K)m ] ) . (5)"}, {"heading": "4. Experimental Results", "text": "In this section, we present the results from our designed speech enhancement front-end with the tuned parameters using the optimization method presented in Section 3. In order to obtain the set of parameters that maximize the recognition rate, we optimized and tuned the system on a noisy TIMIT database. The set of tuned parameters will then be used in the ASR front-end for the distant-talking limited-vocabulary control of our music playback device as shown in Figure 3."}, {"heading": "4.1. Speech Recognition on TIMIT", "text": ""}, {"heading": "4.1.1. Noisy Database", "text": "The database was generated by simulating the interaction between the user and the playback device. In this scenario, music is played from a four-loudspeaker portable device with an embedded microphone, placed roughly one centimeter away from the closest loudspeaker, and the user is uttering speech in a reverberant environment during continuous music playback. The microphone signal y[n] was then generated according to:\ny[n] = s[n] + \u03c31d[n] + \u03c32v2[n] + \u03c33v3[n],\nwhich consisted of the speech s[n], the acoustic echo from the music d[n], the background noise v2[n] (babble, factory, and music), and a pink noise introduced to simulate a mild broadband constant electrical noise and electromagnetic radiations v3[n]. For each file in the TIMIT database, the SER and SNR were chosen from uniform distributions ranging from \u221215 dB to \u221210 dB and from \u221210 dB to 10 dB, respectively. We used 12 impulse responses in the simulation, randomly picked and normalized to unitary energy. The values of \u03c31 and \u03c32 were calculated based on SER and SNR, and we set \u03c33 = 0.1. The music sound, d[n], was randomly selected from five different music tracks of different genres with random starting points."}, {"heading": "4.1.2. Training of the Speech Recognizer", "text": "We used the HTK toolkit [25] to train an acoustic model on the noisy TIMIT database composed of 61 phones [27]. A set of 13 Mel-frequency cepstral coefficients (MFCCs) with their first and second derivatives, for a total of 39 coefficients, are generated and used as features for our experimental analysis. We normalized the variance and mean of the MFCCs, as suggested in [10] for properly applying the direct masking. We used 5-state HMMs with a 8-mixture GMM for each phone. We trained our HMMs with the noisy speech processed by our front-end."}, {"heading": "4.1.3. Recognition of the noisy TIMIT database", "text": "Once we obtained the HMMs in the acoustic model, we optimized the parameters of our front-end. We casted the problem as discussed in Section 3. For initial population, we chose a set of fairly well manually optimized parameters and reasonable bounds that allows us to use only three generations to reach convergence. The genetic algorithm had a population of M = 40 possible candidates, and the best N = 10 were migrated to the next generation. These values were chosen empirically by balancing the complexity and the accuracy of the results. The\nphone accuracy rate (PAR) using a bigram model increased from 35% to 40% after our optimization on the training data, proving the validity of our procedure.\nIn order to provide a fair comparison, we also tuned the parameters to maximize the mean opinion score (MOS) using the Perceptual Objective Listening Quality Assessment (POLQA) [28], as done in [12], through the same GA setup and the same noisy TIMIT database. To assess the performance of our tuning method, we tested on data not used in the training by creating a second simulated noisy TIMIT database with different conditions. Results are shown in Table 1 for different types of noise. The SER and SNR were again chosen from uniform distributions ranging from \u221215 dB to \u221210 dB and from \u221210 dB to 10 dB, respectively. The \u201cmix\u201d noise was picked randomly from the babble, music, or factory noise. In the case of music, noisy files were generated from a set of tracks from different genres at different start points. When the front-end speech enhancer was not used, the PAR dropped to 10.1% (unigram) and 15.7% (bigram) for the noisy signal. Although used in a different setup, the results obtained with the proposed method compare favorably to some prior results [29, 30], where authors investigated joint echo cancellation and speech enhancement at higher SERs and SNRs."}, {"heading": "4.2. Limited Vocabulary Speech Recognition", "text": "We used the set of tuned parameters and the HMMs obtained from our analysis on the TIMIT database to study the feasibility of speech recognition on limited vocabulary in extremely challenging conditions."}, {"heading": "4.2.1. Recognition of limited-size Vocabulary Speech", "text": "We used the system to recognize four commands: \u201cPLAY\u201d, \u201cNEXT\u201d, \u201cBACK\u201d, and \u201cPAUSE\u201d. The commands were generated by changing the TIMIT language model accordingly. As shown in Figure 1, we used a standard VAD, applied on a frame-by-frame basis, after the direct masking to isolate the commands [31, 32]:\u2211\nk\n[ \u03b3k\u03bek 1 + \u03bek \u2212 log(1 + \u03bek) ] > \u03b7, (6)\nwhere \u03bek and \u03b3k are the a priori and a posteriori SNRs and \u03b7 is a fixed threshold. Figure 4 shows an example of a noisy command before and after processing. The command is not audible to human listeners before processing, while the speech structure is well preserved after processing."}, {"heading": "4.2.2. Recording of Real-World Commands", "text": "We used eight subjects (male/female, native/non-native) who uttered the command list at a distance of around 1m from the microphone of the Beats PillTM portable speaker while music was playing. We used four different music tracks in the echo path, where the starting point of the track was chosen randomly. Subjects uttered the following commands towards the speakers:\n\u201cPLAY\u201d, \u201cNEXT\u201d, \u201cBACK\u201d, \u201cPAUSE\u201d (as shown in Figure 3). The playback level for the experiments was set to three different levels of 95 dB SPL, 90 dB SPL, and 85 dB SPL. We estimated the range of SER for the different setups to be approximately equal to \u221235 to \u221230 dB, \u221230 to \u221225 dB, and \u221225 to \u221220 dB for the three levels, respectively. The estimation of the SERs were made possible thanks to a lavalier microphone that recorded the near-end speech. Note that the SERs in the experiments are lower than the SERs used in the simulation, which validates the generalization of the tuning methodology. Recognition rates are given in Table 2 at different SER levels. Also in this case, we compared with the set of parameters obtained by optimization through POLQA [12]. The results clearly show that our proposed tuning based on ASR maximization outperforms the POLQA-based tuning. The difference in performance seems to derive from the POLQA optimization being less aggressive on noise in order to preserve speech quality. More noise in the processed files translates into worse performance of the speech recognizer and the VAD. As a reference, when our speech enhancement front end was not used, the average recognition rate was 25% over all commands (coin toss) in the lowest SER setup."}, {"heading": "5. Conclusion", "text": "We proposed a robust ASR front-end and a related tuning methodology. The proposed speech enhancement front-end consists of a cascaded robust AEC, a residual echo power estimator based on a double-talk probability estimator, and a novel quasi-binary masking that utilizes the classical MMSE-based method at very low SNRs. The tuning improves the speech recognition rate substantially on the TIMIT database. The optimized front-end is then tested in realistic environments for the remote control of a music playback device with a limited-sized command dictionary. The result shows a fairly high recognition rate for voice commands at a speech-to-music ratio as low as \u221235 dB, scenarios hardly seen through the literature."}, {"heading": "6. References", "text": "[1] M. G. Helander, T. K. Landauer, and P. V. Prabhu, Hand-\nbook of human-computer interaction, Elsevier, 1997.\n[2] J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach, \u201cAn overview of noise-robust automatic speech recognition,\u201d IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745\u2013777, 2014.\n[3] M. Wo\u0308lfel and J. McDonough, Distant speech recognition, John Wiley & Sons, 2009.\n[4] L. Marquardt, P. Svaizer, E. Mabande, A. Brutti, C. Zieger, M. Omologo, and W. Kellermann, \u201cA natural acoustic front-end for interactive TV in the EU-project DICIT,\u201d in Proc. IEEE Pacific Rim Conf. on Comm., Computers and Signal Processing, pp. 894\u2013899, 2009.\n[5] M. L. Seltzer, \u201cMicrophone array processing for robust speech recognition,\u201d Ph.D. dissertation, Carnegie Mellon University, 2003.\n[6] T. S. Wada and B.-H. Juang, \u201cAcoustic echo cancellation based on independent component analysis and integrated residual echo enhancement,\u201d in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 205\u2013208, 2009.\n[7] J. Wung, T. S. Wada, B.-H. Juang, B. Lee, M. Trott, and R. W. Schafer, \u201cA system approach to acoustic echo cancellation in robust hands-free teleconferencing,\u201d in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 101\u2013104, 2011.\n[8] T. S. Wada and B.-H. Juang, \u201cEnhancement of residual echo for robust acoustic echo cancellation,\u201d IEEE Trans. on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 175\u2013189, 2012.\n[9] Y. Ephraim and D. Malah, \u201cSpeech enhancement using a minimum mean-square error log-spectral amplitude estimator,\u201d IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 33, no. 2, pp. 443\u2013445, 1985.\n[10] W. Hartmann, A. Narayanan, E. Fosler-Lussier, and D. Wang, \u201cA direct masking approach to robust ASR,\u201d IEEE Trans. on Audio, Speech, and Language Processing, vol. 21, pp. 1993\u20132005, 2013.\n[11] D. E. Goldberg, Genetic algorithms in search, optimization, and machine learning, Addison-Wesley, 1989.\n[12] D. Giacobello, J. Wung, R. Pichevar, and J. Atkins, \u201cTuning methodology for speech enhancement algorithms using a simulated conversational database and perceptual objective measures,\u201d accepted for publication in Proc. 4th Joint Workshop on Hands-free Speech Communication and Microphone Arrays, 2014.\n[13] D. Giacobello, J. Atkins, J. Wung, and R. Prabhu, \u201cResults on automated tuning of a voice quality enhancement system using objective quality measures,\u201d in Proc. 135th Audio Engineering Society Convention, 2013.\n[14] J. S. Garofolo, et al., TIMIT: acoustic-phonetic continuous speech corpus, Linguistic Data Consortium, 1993.\n[15] J. Huang, M. Epstein, and M. Matassoni, \u201cEffective acoustic adaptation for a distant-talking interactive TV system.\u201d in Proc. INTERSPEECH, pp. 1709\u20131712, 2008.\n[16] J. J. Shynk, \u201cFrequency-domain and multirate adaptive filtering,\u201d IEEE Signal Processing Magazine, vol. 9, no. 1, pp. 14\u201337, 1992.\n[17] J. S. Soo and K. K. Pang, \u201cMultidelay block frequency domain adaptive filter,\u201d IEEE Trans. on Acoustics, Speech and Signal Processing, , vol. 38, no. 2, pp. 373\u2013376, 1990.\n[18] G. Enzner, R. Martin, and P. Vary, \u201cUnbiased residual echo power estimation for hands-free telephony,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 2, pp. 1893\u20131896, 2002.\n[19] S. Goetze, M. Kallinger, and K.-D. Kammeyer, \u201cResidual echo power spectral density estimation based on an optimal smoothed misalignment for acoustic echo cancelation,\u201d in Proc. Intl. Workshop on Acoustic Echo and Noise Control, 2005.\n[20] I. J. Tashev, \u201cCoherence based double talk detector with soft decision,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 165\u2013168, 2012.\n[21] T. Gerkmann and R. C. Hendriks, \u201cUnbiased MMSEbased noise power estimation with low complexity and low tracking delay,\u201d IEEE Trans. on Audio, Speech, and Language Processing, vol. 20, no. 4, pp. 1383\u20131393, 2012.\n[22] Y. Ephraim and D. Malah, \u201cSpeech enhancement using a minimum-mean square error short-time spectral amplitude estimator,\u201d IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, 1984.\n[23] M. Cooke, P. Green, L. Josifovski, and A. Vizinho, \u201cRobust automatic speech recognition with missing and unreliable acoustic data,\u201d Speech communication, vol. 34, pp. 267\u2013285, 2001.\n[24] B. Raj, M. Seltzer, and R. M. Stern, \u201cReconstruction of missing features for robust speech recognition,\u201d Speech Communication, vol. 43, pp. 275\u2013296, 2004.\n[25] S. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, and P. Woodland, The HTK book, Cambridge University Engineering Department, 2003.\n[26] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern classification, John Wiley & Sons, 2012.\n[27] C. Lopes and F. Perdigao, Phoneme Recognition on the TIMIT database, Speech Technologies, Prof. Ivo Ipsic (Ed.), InTech, 2011.\n[28] Perceptual Objective Listening Quality Assessment, ITUT Rec. P.863, 2010.\n[29] W. Herbordt, S. Nakamura, and W. Kellerman, \u201cJoint optimization of LCMV beamforming and acoustic echo cancellation for automatic speech recognition,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 3, pp. 77\u201380, 2005.\n[30] G. Reuven, S. Gannot, and I. Cohen, \u201cJoint noise reduction and acoustic cancellation using the transfer-function generalized sidelobe canceller,\u201d Speech communication, vol. 49, pp. 623\u2013635, 2007.\n[31] J. Ramirez, J. Gorriz, and J. Segura, Voice Activity Detection. Fundamentals and Speech Recognition System Robustness, Robust Speech Recognition and Understanding, Michael Grimm and Kristian Kroschel (Eds.), 2007.\n[32] J. Sohn, N. Kim, and W. Sung, \u201cA statistical model-based voice activity detection,\u201d IEEE Signal Processing Letters, vol. 6, no. 1, 1999."}], "references": [{"title": "An overview of noise-robust automatic speech recognition", "author": ["J. Li", "L. Deng", "Y. Gong", "R. Haeb-Umbach"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745\u2013777, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Distant speech recognition", "author": ["M. W\u00f6lfel", "J. McDonough"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A natural acoustic front-end for interactive TV in the EU-project DICIT", "author": ["L. Marquardt", "P. Svaizer", "E. Mabande", "A. Brutti", "C. Zieger", "M. Omologo", "W. Kellermann"], "venue": "Proc. IEEE Pacific Rim Conf. on Comm., Computers and Signal Processing, pp. 894\u2013899, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Microphone array processing for robust speech recognition", "author": ["M.L. Seltzer"], "venue": "Ph.D. dissertation, Carnegie Mellon University, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Acoustic echo cancellation based on independent component analysis and integrated residual echo enhancement", "author": ["T.S. Wada", "B.-H. Juang"], "venue": "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 205\u2013208, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "A system approach to acoustic echo cancellation in robust hands-free teleconferencing", "author": ["J. Wung", "T.S. Wada", "B.-H. Juang", "B. Lee", "M. Trott", "R.W. Schafer"], "venue": "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp. 101\u2013104, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhancement of residual echo for robust acoustic echo cancellation", "author": ["T.S. Wada", "B.-H. Juang"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 175\u2013189, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement using a minimum mean-square error log-spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 33, no. 2, pp. 443\u2013445, 1985.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1985}, {"title": "A direct masking approach to robust ASR", "author": ["W. Hartmann", "A. Narayanan", "E. Fosler-Lussier", "D. Wang"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 21, pp. 1993\u20132005, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Genetic algorithms in search, optimization, and machine learning, Addison-Wesley", "author": ["D.E. Goldberg"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Tuning methodology for speech enhancement algorithms using a simulated conversational database and perceptual objective measures", "author": ["D. Giacobello", "J. Wung", "R. Pichevar", "J. Atkins"], "venue": "accepted for publication in Proc. 4th Joint Workshop on Hands-free Speech Communication and Microphone Arrays, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Results on automated tuning of a voice quality enhancement system using objective quality measures", "author": ["D. Giacobello", "J. Atkins", "J. Wung", "R. Prabhu"], "venue": "Proc. 135th Audio Engineering Society Convention, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "TIMIT: acoustic-phonetic continuous speech corpus", "author": ["J.S. Garofolo"], "venue": "Linguistic Data Consortium,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Effective acoustic adaptation for a distant-talking interactive TV system.", "author": ["J. Huang", "M. Epstein", "M. Matassoni"], "venue": "in Proc. INTERSPEECH,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Frequency-domain and multirate adaptive filtering", "author": ["J.J. Shynk"], "venue": "IEEE Signal Processing Magazine, vol. 9, no. 1, pp. 14\u201337, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Multidelay block frequency domain adaptive filter", "author": ["J.S. Soo", "K.K. Pang"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, , vol. 38, no. 2, pp. 373\u2013376, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Unbiased residual echo power estimation for hands-free telephony", "author": ["G. Enzner", "R. Martin", "P. Vary"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 2, pp. 1893\u20131896, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1893}, {"title": "Residual echo power spectral density estimation based on an optimal smoothed misalignment for acoustic echo cancelation", "author": ["S. Goetze", "M. Kallinger", "K.-D. Kammeyer"], "venue": "Proc. Intl. Workshop on Acoustic Echo and Noise Control, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Coherence based double talk detector with soft decision", "author": ["I.J. Tashev"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 165\u2013168, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Unbiased MMSEbased noise power estimation with low complexity and low tracking delay", "author": ["T. Gerkmann", "R.C. Hendriks"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 20, no. 4, pp. 1383\u20131393, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, 1984.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Robust automatic speech recognition with missing and unreliable acoustic data", "author": ["M. Cooke", "P. Green", "L. Josifovski", "A. Vizinho"], "venue": "Speech communication, vol. 34, pp. 267\u2013285, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Reconstruction of missing features for robust speech recognition", "author": ["B. Raj", "M. Seltzer", "R.M. Stern"], "venue": "Speech Communication, vol. 43, pp. 275\u2013296, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Joint optimization of LCMV beamforming and acoustic echo cancellation for automatic speech recognition", "author": ["W. Herbordt", "S. Nakamura", "W. Kellerman"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 3, pp. 77\u201380, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint noise reduction and acoustic cancellation using the transfer-function generalized sidelobe canceller", "author": ["G. Reuven", "S. Gannot", "I. Cohen"], "venue": "Speech communication, vol. 49, pp. 623\u2013635, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Voice Activity Detection. Fundamentals and Speech Recognition System Robustness, Robust Speech Recognition and Understanding", "author": ["J. Ramirez", "J. Gorriz", "J. Segura"], "venue": "Michael Grimm and Kristian Kroschel (Eds.),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "A statistical model-based voice activity detection", "author": ["J. Sohn", "N. Kim", "W. Sung"], "venue": "IEEE Signal Processing Letters, vol. 6, no. 1, 1999.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": ", reverberation [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "In particular, due to the severe degradation of the input signal, the ASR performance drops significantly when the distance between the user and the microphone increases [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": ", the DICIT project [4].", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "However, to the authors\u2019 knowledge, the available solutions rely heavily on large microphone arrays [5], which may be infeasible for handheld portable device.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "echo cancellation and speech enhancement to retrieve a clean estimate of the command [6, 7, 8].", "startOffset": 85, "endOffset": 94}, {"referenceID": 5, "context": "echo cancellation and speech enhancement to retrieve a clean estimate of the command [6, 7, 8].", "startOffset": 85, "endOffset": 94}, {"referenceID": 6, "context": "echo cancellation and speech enhancement to retrieve a clean estimate of the command [6, 7, 8].", "startOffset": 85, "endOffset": 94}, {"referenceID": 7, "context": "Secondly, we propose a novel noise reduction method, where we combine a traditional minimum mean-squared error (MMSE) speech enhancement approach [9] with an estimate of the ideal binary mask [10].", "startOffset": 146, "endOffset": 149}, {"referenceID": 8, "context": "Secondly, we propose a novel noise reduction method, where we combine a traditional minimum mean-squared error (MMSE) speech enhancement approach [9] with an estimate of the ideal binary mask [10].", "startOffset": 192, "endOffset": 196}, {"referenceID": 9, "context": "The parameters of the algorithm are tuned for maximum recognition rate by casting the tuning problem as a nonlinear program, solved efficiently through a genetic algorithm (GA) [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 10, "context": "A similar approach was used in [12, 13] to maximize the objective perceptual quality of a speech enhancement system for fullduplex communication.", "startOffset": 31, "endOffset": 39}, {"referenceID": 11, "context": "A similar approach was used in [12, 13] to maximize the objective perceptual quality of a speech enhancement system for fullduplex communication.", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "The training and evaluation corpora are generated through a synthetic mixture of clean speech (from the TIMIT database [14]) and music, both convolved with separate impulse responses, and further mixed with a background noise to cover as many deployment scenarios as possible.", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "The acoustic models of the ASR are trained by the front-end enhanced speech, an effective way to learn and exploit the typical distortions of the system itself [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 4, "context": "Since strong near-end interference may corrupt the error signal of the acoustic echo canceler (AEC) and cause the adaptive filter to diverge, the RAEC system [6, 8] is used, where the error recovery nonlinearity and robust adaptive step-size control allows for continuous tracking of the echo path during double talk.", "startOffset": 158, "endOffset": 164}, {"referenceID": 6, "context": "Since strong near-end interference may corrupt the error signal of the acoustic echo canceler (AEC) and cause the adaptive filter to diverge, the RAEC system [6, 8] is used, where the error recovery nonlinearity and robust adaptive step-size control allows for continuous tracking of the echo path during double talk.", "startOffset": 158, "endOffset": 164}, {"referenceID": 14, "context": "To reduce the delay of the frequency-domain adaptive filter [16], the multi-delay adaptive filter structure [17] is used.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "To reduce the delay of the frequency-domain adaptive filter [16], the multi-delay adaptive filter structure [17] is used.", "startOffset": 108, "endOffset": 112}, {"referenceID": 5, "context": "A cascaded structure similar to the system approach of [7] is used: the output of the first RAEC is fed to the input of the second RAEC, which is different from the original system approach in [7] where the input to the second RAEC is still the microphone signal (a parallel structure instead of the cascaded structure used in this work).", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": "A cascaded structure similar to the system approach of [7] is used: the output of the first RAEC is fed to the input of the second RAEC, which is different from the original system approach in [7] where the input to the second RAEC is still the microphone signal (a parallel structure instead of the cascaded structure used in this work).", "startOffset": 193, "endOffset": 196}, {"referenceID": 16, "context": "A coherence based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar to [20] is used for a more accurate estimate of the residual echo power.", "startOffset": 36, "endOffset": 44}, {"referenceID": 17, "context": "A coherence based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar to [20] is used for a more accurate estimate of the residual echo power.", "startOffset": 36, "endOffset": 44}, {"referenceID": 18, "context": "A coherence based method similar to [18, 19] is used for the RPE, and a modified version of the DTP estimator similar to [20] is used for a more accurate estimate of the residual echo power.", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "As shown in Figure 1, the DTP estimator differs from that in [20] since the coherence is calculated between the RAEC estimated echo signal d\u0302 and the microphone signal y rather than between the loudspeaker signal x and the microphone signal y.", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "The low complexity MMSE noise power estimator [21] is used for the NPE, and the Ephraim and Malah logspectral amplitude (LSA) estimator [9] is used for the combined residual echo and noise suppression:", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "The low complexity MMSE noise power estimator [21] is used for the NPE, and the Ephraim and Malah logspectral amplitude (LSA) estimator [9] is used for the combined residual echo and noise suppression:", "startOffset": 136, "endOffset": 139}, {"referenceID": 20, "context": "The estimation of the a priori speech-to-noise ratio (SNR) \u03bek is done using the decision-directed (DD) approach [22]:", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "It has been recently shown that the speech recognition accuracy in noisy condition can be greatly improved by direct binary masking [10] when compared to marginalization [23] or spectral reconstruction [24].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "It has been recently shown that the speech recognition accuracy in noisy condition can be greatly improved by direct binary masking [10] when compared to marginalization [23] or spectral reconstruction [24].", "startOffset": 170, "endOffset": 174}, {"referenceID": 22, "context": "It has been recently shown that the speech recognition accuracy in noisy condition can be greatly improved by direct binary masking [10] when compared to marginalization [23] or spectral reconstruction [24].", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "where Gmin is the minimum suppression gain [13], and the output is then: \u015ck[m] = \u03b6k[m]Ek[m].", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": ",M} in order to find the solution that maximizes the cost function [11, 26].", "startOffset": 67, "endOffset": 75}, {"referenceID": 8, "context": "We normalized the variance and mean of the MFCCs, as suggested in [10] for properly applying the direct masking.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "In order to provide a fair comparison, we also tuned the parameters to maximize the mean opinion score (MOS) using the Perceptual Objective Listening Quality Assessment (POLQA) [28], as done in [12], through the same GA setup and the same noisy TIMIT database.", "startOffset": 194, "endOffset": 198}, {"referenceID": 23, "context": "Although used in a different setup, the results obtained with the proposed method compare favorably to some prior results [29, 30], where authors investigated joint echo cancellation and speech enhancement at higher SERs and SNRs.", "startOffset": 122, "endOffset": 130}, {"referenceID": 24, "context": "Although used in a different setup, the results obtained with the proposed method compare favorably to some prior results [29, 30], where authors investigated joint echo cancellation and speech enhancement at higher SERs and SNRs.", "startOffset": 122, "endOffset": 130}, {"referenceID": 25, "context": "As shown in Figure 1, we used a standard VAD, applied on a frame-by-frame basis, after the direct masking to isolate the commands [31, 32]: \u2211", "startOffset": 130, "endOffset": 138}, {"referenceID": 26, "context": "As shown in Figure 1, we used a standard VAD, applied on a frame-by-frame basis, after the direct masking to isolate the commands [31, 32]: \u2211", "startOffset": 130, "endOffset": 138}, {"referenceID": 10, "context": "Also in this case, we compared with the set of parameters obtained by optimization through POLQA [12].", "startOffset": 97, "endOffset": 101}], "year": 2014, "abstractText": "This paper addresses the challenging scenario for the distanttalking control of a music playback device, a common portable speaker with four small loudspeakers in close proximity to one microphone. The user controls the device through voice, where the speech-to-music ratio can be as low as \u221230 dB during music playback. We propose a speech enhancement front-end that relies on known robust methods for echo cancellation, doubletalk detection, and noise suppression, as well as a novel adaptive quasi-binary mask that is well suited for speech recognition. The optimization of the system is then formulated as a large scale nonlinear programming problem where the recognition rate is maximized and the optimal values for the system parameters are found through a genetic algorithm. We validate our methodology by testing over the TIMIT database for different music playback levels and noise types. Finally, we show that the proposed front-end allows a natural interaction with the device for limited-vocabulary voice commands.", "creator": "LaTeX with hyperref package"}}}