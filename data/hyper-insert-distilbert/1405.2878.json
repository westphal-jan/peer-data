{"id": "1405.2878", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Approximate Policy Iteration Schemes: A Comparison", "abstract": "historically we consider the increasing infinite - tendency horizon discounted cumulative optimal control problem formalized by markov decision processes. instead we focus solely on selecting several approximate variations of the advanced policy iteration algorithm : approximate quarterly policy iteration, a conservative policy iteration ( versus cpi ), a natural statistical adaptation of the policy reversal search by stationary dynamic policy programming algorithm to the equivalent infinite - trend horizon dependent case ( psdp $ _ \\ $ infty $ ), and the newest recently proposed underlying non - stationary policy iteration ( at nspi ( nad m ) ). modeled for roughly all algorithms, we describe performance bounds, and make simply a graphical comparison calculated by paying a particular attention precisely to the predicted concentrability constants involved, including the number of adjustment iterations and the memory services required. furthermore our relaxation analysis highlights the approximate following cardinal points : 1 ) the underlying performance guarantee of - cpi can safely be arbitrarily better documented than fulfilled that goal of api / api ( $ \\ alpha $ ), satisfying but this example comes close at varying the cost difference of constructing a maximum relative - - - exponential utility in $ \\ binary frac { \u00d7 1 } { \\ epsilon epsilon } $ - - - * increase integration of both the number factor of iterations. 2 ) psdp $ _ \\ an infty $ enjoys the arguably best flexibility of constructing both worlds : finally its performance limit guarantee is similar to nearly that of ~ cpi, but implemented within a number of succeeding iterations implies similar performance to that of apr api. 3 ) contrary to api flexibility that requires a constant swap memory, the memory resources needed by cpi and psdp $ _ \\ infty $ is not proportional even to mean their compute number likelihood of iterations, limits which actually may be even problematic such when the actual discount factor $ \\ gamma $ $ is close either to 1 or merely the approximation error $ \\ or epsilon $, is close to $ 0 $ ; afterward we show that the nspi ( m ) algorithm allows to make an overall trade - off evaluation between virtual memory density and utilization performance. simulations improved with all these pricing schemes finally confirm through our nonlinear analysis.", "histories": [["v1", "Mon, 12 May 2014 19:11:03 GMT  (4923kb,D)", "http://arxiv.org/abs/1405.2878v1", "ICML (2014)"]], "COMMENTS": "ICML (2014)", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["bruno scherrer"], "accepted": true, "id": "1405.2878"}, "pdf": {"name": "1405.2878.pdf", "metadata": {"source": "META", "title": "Approximate Policy Iteration Schemes: A Comparison", "authors": ["Bruno Scherrer"], "emails": ["BRUNO.SCHERRER@INRIA.FR"], "sections": [{"heading": null, "text": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP\u221e), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012). For all algorithms, we describe performance bounds with respect the per-iteration error , and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API, but this comes at the cost of a relative\u2014exponential in 1 \u2014increase of the number of iterations. 2) PSDP\u221e enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP\u221e is proportional to their number of iterations, which may be problematic when the discount factor \u03b3 is close to 1 or the approximation error is close to 0; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis.\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s)."}, {"heading": "1. Introduction", "text": "We consider an infinite-horizon discounted Markov Decision Process (MDP) (Puterman, 1994; Bertsekas & Tsitsiklis, 1996) (S,A, P, r, \u03b3), where S is a possibly infinite state space, A is a finite action space, P (ds\u2032|s, a), for all (s, a), is a probability kernel on S, r : S \u2192 [\u2212Rmax, Rmax] is a reward function bounded byRmax, and \u03b3 \u2208 (0, 1) is a discount factor. A stationary deterministic policy \u03c0 : S \u2192 A maps states to actions. We write P\u03c0(ds\n\u2032|s) = P (ds\u2032|s, \u03c0(s)) for the stochastic kernel associated to policy \u03c0. The value v\u03c0 of a policy \u03c0 is a function mapping states to the expected discounted sum of rewards received when following \u03c0 from these states: for all s \u2208 S,\nv\u03c0(s) = E [ \u221e\u2211 t=0 \u03b3tr(st) \u2223\u2223\u2223\u2223\u2223s0 = s, st+1 \u223c P\u03c0(\u00b7|st) ] .\nThe value v\u03c0 is clearly bounded by Vmax = Rmax/(1\u2212\u03b3). It is well-known that v\u03c0 can be characterized as the unique fixed point of the linear Bellman operator associated to a policy \u03c0: T\u03c0 : v 7\u2192 r+ \u03b3P\u03c0v. Similarly, the Bellman optimality operator T : v 7\u2192 max\u03c0 T\u03c0v has as unique fixed point the optimal value v\u2217 = max\u03c0 v\u03c0 . A policy \u03c0 is greedy w.r.t. a value function v if T\u03c0v = Tv, the set of such greedy policies is written Gv. Finally, a policy \u03c0\u2217 is optimal, with value v\u03c0\u2217 = v\u2217, iff \u03c0\u2217 \u2208 Gv\u2217, or equivalently T\u03c0\u2217v\u2217 = v\u2217.\nThe goal of this paper is to study and compare several approximate Policy Iteration schemes. In the literature, such schemes can be seen as implementing an approximate greedy operator, G , that takes as input a distribution \u03bd and a function v : S \u2192 R and returns a policy \u03c0 that is ( , \u03bd)approximately greedy with respect to v in the sense that:\n\u03bd(Tv \u2212 T\u03c0v) = \u03bd(max \u03c0\u2032 T\u03c0\u2032v \u2212 T\u03c0v) \u2264 . (1)\nwhere for all x, \u03bdx denotes Es\u223c\u03bd [x(s)]. In practice, this approximation of the greedy operator can be achieved through a `p-regression of the so-called Q-function\u2014the stateaction value function\u2014(a direct regression is suggested by Kakade & Langford (2002), a fixed-point LSTD approach is used by Lagoudakis & Parr (2003b)) or through a\nar X\niv :1\n40 5.\n28 78\nv1 [\ncs .A\nI] 1\n2 M\nay 2\n01 4\n(cost-sensitive) classification problem (Lagoudakis & Parr, 2003a; Lazaric et al., 2010). With this operator in hand, we shall describe several Policy Iteration schemes in Section 2. Then Section 3 will provide a detailed comparative analysis of their performance guarantees, time complexities, and memory requirements. Section 4 will go on by providing experiments that will illustrate their behavior, and confirm our analysis. Finally, Section 5 will conclude and present future work."}, {"heading": "2. Algorithms", "text": "API We begin by describing the standard Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996). At each iteration k, the algorithm switches to the policy that is approximately greedy with respect to the value of the previous policy for some distribution \u03bd:\n\u03c0k+1 \u2190 G k+1(\u03bd, v\u03c0k). (2)\nIf there is no error ( k = 0) and \u03bd assigns a positive weights to every state, it can easily be seen that this algorithm generates the same sequence of policies as exact Policy Iterations since from Equation (1) the policies are exactly greedy.\nCPI/CPI(\u03b1)/API(\u03b1) We now turn to the description of Conservative Policy Iteration (CPI) proposed by (Kakade & Langford, 2002). At iteration k, CPI (described in Equation (3)) uses the distribution d\u03c0k,\u03bd = (1 \u2212 \u03b3)\u03bd(I \u2212 \u03b3P\u03c0k)\n\u22121\u2014the discounted cumulative occupancy measure induced by \u03c0k when starting from \u03bd\u2014for calling the approximate greedy operator, and uses a stepsize \u03b1k to generate a stochastic mixture of all the policies that are returned by the successive calls to the approximate greedy operator, which explains the adjective \u201cconservative\u201d:\n\u03c0k+1 \u2190 (1\u2212 \u03b1k+1)\u03c0k + \u03b1k+1G k+1(d\u03c0k,\u03bd , v\u03c0k) (3)\nThe stepsize \u03b1k+1 can be chosen in such a way that the above step leads to an improvement of the expected value of the policy given that the process is initialized according to the distribution \u03bd (Kakade & Langford, 2002). The original article also describes a criterion for deciding whether to stop or to continue. Though the adaptive stepsize and the stopping condition allows to derive a nice analysis, they are in practice conservative: the stepsize \u03b1k should be implemented with a line-search mechanism, or be fixed to some small value \u03b1. We will refer to this latter variation of CPI as CPI(\u03b1).\nIt is natural to also consider the algorithm API(\u03b1) (mentioned by Lagoudakis & Parr (2003a)), a variation of API that is conservative like CPI(\u03b1) in the sense that it mixes the new policy with the previous ones with weights \u03b1 and\n1\u2212\u03b1, but that directly uses the distribution \u03bd in the approximate greedy step:\n\u03c0k+1 \u2190 (1\u2212 \u03b1)\u03c0k + \u03b1G k+1(\u03bd, v\u03c0k) (4)\nBecause it uses \u03bd instead of d\u03c0k,\u03bd , API(\u03b1) is simpler to implement than CPI(\u03b1)1.\nPSDP\u221e We are now going to describe an algorithm that has a flavour similar to API\u2014in the sense that at each step it does a full step towards a new deterministic policy\u2014 but also has a conservative flavour like CPI\u2014in the sense that the policies considered evolve more and more slowly. This algorithm is a natural variation of the Policy Search by Dynamic Programming algorithm (PSDP) of Bagnell et al. (2003), originally proposed to tackle finite-horizon problems, to the infinite-horizon case; we thus refer to it as PSDP\u221e. To the best of our knowledge however, this variation has never been used in an infinite-horizon context.\nThe algorithm is based on finite-horizon non-stationary policies. Given a sequence of stationary deterministic policies (\u03c0k) that the algorithm will generate, we will write \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01 the k-horizon policy that makes the first action according to \u03c0k, then the second action according to \u03c0k\u22121, etc. Its value is v\u03c3k = T\u03c0kT\u03c0k\u22121 . . . T\u03c01r. We will write \u2205 the \u201cempty\u201d non-stationary policy. Note that v\u2205 = r and that any infinite-horizon policy that begins with \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01, which we will (somewhat abusively) denote \u201c\u03c3k . . . \u201d has a value v\u03c3k... \u2265 v\u03c3k \u2212\u03b3kVmax. Starting from \u03c30 = \u2205, the algorithm implicitely builds a sequence of non-stationary policies (\u03c3k) by iteratively concatenating the policies that are returned by the approximate greedy operator:\n\u03c0k+1 \u2190 G k+1(\u03bd, v\u03c3k) (5)\nWhile the standard PSDP algorithm of Bagnell et al. (2003) considers a horizon T and makes T iterations, the algorithm we consider here has an indefinite number of iterations. The algorithm can be stopped at any step k. The theory that we are about to describe suggests that one may return any policy that starts by the non-stationary policy \u03c3k. Since \u03c3k is an approximately good finite-horizon policy, and as we consider an infinite-horizon problem, a natural output that one may want to use in practice is the infinitehorizon policy that loops over \u03c3k, that we shall denote (\u03c3k) \u221e.\n1In practice, controlling the greedy step with respect to d\u03c0k,\u03bd requires to generate samples from this very distribution. As explained by Kakade & Langford (2002), one such sample can be done by running one trajectory starting from \u03bd and following \u03c0k, stopping at each step with probability 1 \u2212 \u03b3. In particular, one sample from d\u03c0k,\u03bd requires on average 1 1\u2212\u03b3 samples from the underlying MDP. With this respect, API(\u03b1) is much simpler to implement.\nFrom a practical point of view, PSDP\u221e and CPI need to store all the (stationary deterministic) policies generated from the start. The memory required by the algorithmic scheme is thus proportional to the number of iterations, which may be prohibitive. The aim of the next paragraph, that presents the last algorithm of this article, is to describe a solution to this potential memory issue.\nNSPI(m) We originally devised the algorithmic scheme of Equation (5) (PSDP\u221e) as a simplified variation of the Non-Stationary PI algorithm with a growing period algorithm (NSPI-growing) (Scherrer & Lesner, 2012)2. With respect to Equation (5), the only difference of NSPIgrowing resides in the fact that the approximate greedy step is done with respect to the value v(\u03c3k)\u221e of the policy that loops infinitely over \u03c3k (formally the algorithm does \u03c0k+1 \u2190 G k+1(\u03bd, v(\u03c3k)\u221e)) instead of the value v\u03c3k of only the first k steps here. Following the intuition that when k is big, these two values will be close to each other, we ended up considering PSDP\u221e because it is simpler. NSPIgrowing suffers from the same memory drawback as CPI and PSDP\u221e. Interestingly, the work of Scherrer & Lesner (2012) contains another algorithm, Non-Stationary PI with a fixed period (NSPI(m)), that has a parameter that directly controls the number of policies stored in memory.\nSimilarly to PSDP\u221e, NSPI(m) is based on non-stationary policies. It takes as an input a parameter m. It requires a set of m initial deterministic stationary policies \u03c0m\u22121, \u03c0m\u22122, . . . , \u03c00 and iteratively generates new policies \u03c01, \u03c02, . . . . For any k \u2265 0, we shall denote \u03c3mk the m-horizon non-stationary policy that runs in reverse order the last m policies, which one may write formally: \u03c3mk = \u03c0k \u03c0k\u22121 . . . \u03c0k\u2212m+1. Also, we shall denote (\u03c3mk )\n\u221e the m-periodic infinite-horizon nonstationary policy that loops over \u03c3mk . Starting from \u03c3 m 0 = \u03c00\u03c01 . . . \u03c0m\u22121, the algorithm iterates as follows:\n\u03c0k+1 \u2190 G k+1(\u03bd, v(\u03c3mk )\u221e) (6)\nEach iteration requires to compute an approximate greedy policy \u03c0k+1 with respect to the value v(\u03c3mk )\u221e of (\u03c3 m k ) \u221e, that is the fixed point of the compound operator3:\n\u2200v, Tk,mv = T\u03c0kT\u03c0k\u22121 . . . T\u03c0k\u2212m+1v.\nWhen one goes from iterations k to k+ 1, the process consists in adding \u03c0k+1 at the front of the (m \u2212 1)-horizon policy \u03c0k\u03c0k\u22121 . . . \u03c0k\u2212m+2, thus forming a new m-horizon\n2We later realized that it was in fact a very natural variation of PSDP. To \u201dgive Caesar his due and God his\u201d, we kept as the main reference the older work and gave the name PSDP\u221e.\n3Implementing this algorithm in practice can trivially be done through cost-sensitive classification in a way similar to Lazaric et al. (2010). It could also be done with a straight-forward extension of LSTD(\u03bb) to non-stationary policies.\npolicy \u03c3mk+1. Doing so, we forget about the oldest policy \u03c0k\u2212m+1 of \u03c3mk and keep a constant memory of size m. At any step k, the algorithm can be stopped, and the output is the policy \u03c0k,m = (\u03c3mk )\n\u221e that loops on \u03c3mk . It is easy to see that NSPI(m) reduces to API when m = 1. Furthermore, if we assume that the reward function is positive, add \u201cstop actions\u201d in every state of the model that lead to a terminal absorbing state with a null reward, and initialize with an infinite sequence of policies that only take this \u201cstop action\u201d, then NSPI(m) with m =\u221e reduces to PSDP\u221e."}, {"heading": "3. Analysis", "text": "For all considered algorithms, we are going to describe bounds on the expected loss Es\u223c\u00b5[v\u03c0\u2217(s) \u2212 v\u03c0(s)] = \u00b5(v\u03c0\u2217 \u2212 v\u03c0) of using the (possibly stochastic or nonstationary) policy \u03c0 ouput by the algorithms instead of the optimal policy \u03c0\u2217 from some initial distribution \u00b5 of interest as a function of an upper bound on all errors ( k). In order to derive these theoretical guarantees, we will first need to introduce a few concentrability coefficients that relate the distribution \u00b5 with which one wants to have a guarantee, and the distribution \u03bd used by the algorithms4.\nDefinition 1. Let c(1), c(2), . . . be the smallest coefficients in [1,\u221e)\u222a{\u221e} such that for all i and all sets of deterministic stationary policies \u03c01, \u03c02, . . . , \u03c0i, \u00b5P\u03c01P\u03c02 . . . P\u03c0i \u2264 c(i)\u03bd. For all m, k, we define the following coefficients in [1,\u221e) \u222a {\u221e}:\nC(1,k) = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic(i+ k),\nC(2,m,k) = (1\u2212 \u03b3)(1\u2212 \u03b3m) \u221e\u2211 i=0 \u221e\u2211 j=0 \u03b3i+jmc(i+ jm+ k).\nSimilarly, let c\u03c0\u2217(1), c\u03c0\u2217(2), . . . be the smallest coefficients in [1,\u221e)\u222a{\u221e} such that for all i, \u00b5(P\u03c0\u2217)i \u2264 c\u03c0\u2217(i)\u03bd. We define:\nC(1)\u03c0\u2217 = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic\u03c0\u2217(i).\nFinally let C\u03c0\u2217 be the smallest coefficient in [1,\u221e) \u222a {\u221e} such that d\u03c0\u2217,\u00b5 = (1\u2212 \u03b3)\u00b5(I \u2212 \u03b3P\u03c0\u2217)\u22121 \u2264 C\u03c0\u2217\u03bd.\nWith these notations in hand, our first contribution is to provide a thorough comparison of all the algorithms. This is done in Table 1. For each algorithm, we describe some performance bounds and the required number of iterations and memory. To make things clear, we only display the dependence with respect to the concentrability constants, the\n4The expected loss corresponds to some weighted `1-norm of the loss v\u03c0\u2217 \u2212 v\u03c0 . Relaxing the goal to controlling the weighted `p-norm for some p \u2265 2 allows to introduce some finer coefficients (Farahmand et al., 2010; Scherrer et al., 2012). Due to lack of space, we do not consider this here.\ndiscount factor \u03b3, the quality of the approximate greedy operator, and\u2014if applicable\u2014the main parameters \u03b1/m of the algorithms. For API(\u03b1), CPI(\u03b1), CPI and PSDP\u221e, the required memory matches the number of iterations. All but two bounds are to our knowledge original. The derivation of the new results are given in Appendix A.\nOur second contribution, that is complementary with the comparative list of bounds, is that we can show that there exists a hierarchy among the constants that appear in all the bounds of Table 1. In the directed graph of Figure 1, a constant B is a descendent of A if and only if the implication {B < \u221e \u21d2 A < \u221e} holds5. The \u201cif and only if\u201d is important here: it means that if A is a parent of B, and B is not a parent of A, then there exists an MDP for which A\n5Dotted arrows are used to underline the fact that the comparison of coefficients is restricted to the case where the parameter m is finite.\nis finite while B is infinite; in other words, an algorithm that has a guarantee with respect to A has a guarantee that can be arbitrarily better than that with constant B. Thus, the overall best concentrability constant is C\u03c0\u2217 , while the worst are C(2,1,0) and C(2,m,0). To make the picture complete, we should add that for any MDP and any distribution \u00b5, it is possible to find an input distribution \u03bd for the algorithm (recall that the concentrability coefficients depend on \u03bd and \u00b5) such that C\u03c0\u2217 is finite, though it is not the case for C (1) \u03c0\u2217 (and as a consequence all the other coefficients). The derivation of this order relations is done in Appendix B.\nThe standard API algorithm has guarantees expressed in terms of C(2,1,0) and C(1,0) only. Since CPI\u2019s analysis can be done with respect to C\u03c0\u2217 , it has a performance guarantee that can be arbitrarily better than that of API, though the opposite is not true. This, however, comes at the cost of an exponential increase of time complexity since CPI may require a number of iterations that scales inO ( 1 2 ) , while the\nguarantee of API only requires O ( log 1 ) iterations. When the analysis of CPI is relaxed so that the performance guarantee is expressed in terms of the (worse) coefficient C(1,0) (obtained also for API), we can slightly improve the rate\u2014 to O\u0303 ( 1 ) \u2014, though it is still exponentially slower than that of API. This second result for CPI was proved with a technique that was also used for CPI(\u03b1) and API(\u03b1). We conjecture that it can be improved for CPI(\u03b1), that should be as good as CPI when \u03b1 is sufficiently small.\nPSDP\u221e enjoys two guarantees that have a fast rate like those of API. One bound has a better dependency with respect to 11\u2212\u03b3 , but is expressed in terms of the worse coefficient C(1)\u03c0\u2217 . The second guarantee is almost as good as that\nof CPI since it only contains an extra log 1 term, but it has the nice property that it holds quickly with respect to : in timeO(log 1 ) instead ofO( 1 2 ), that is exponentially faster. PSDP\u221e is thus theoretically better than both CPI (as good but faster) and API (better and as fast).\nNow, from a practical point of view, PSDP\u221e and CPI need to store all the policies generated from the start. The memory required by these algorithms is thus proportional to the number of iterations. Even if PSDP\u221e may require much fewer iterations than CPI, the corresponding memory requirement may still be prohibitive in situations where is small or \u03b3 is close to 1. We explained that NSPI(m) can be seen as making a bridge between API and PSDP\u221e. Since (i) both have a nice time complexity, (ii) API has the best memory requirement, and (iii) NSPI(m) has the best performance guarantee, NSPI(m) is a good candidate for making a standard performance/memory trade-off. If the first two bounds of NSPI(m) in Table 1 extends those of API, the other two are made of two terms: the left terms are identical to those obtained for PSDP\u221e, while the two possible right terms are new, but are controlled by \u03b3m, which can thus be made arbitrarily small by increasing the memory parameter m. Our analysis thus confirms our intuition that NSPI(m) allows to make a performance/memory trade-off in between API (small memory) and PSDP\u221e (best performance). In other words, as soon as memory becomes a constraint, NSPI(m) is the natural alternative to PSDP\u221e."}, {"heading": "4. Experiments", "text": "In this section, we present some experiments in order to illustrate the empirical behavior of the different algorithms discussed in the paper. We considered the standard API as a baseline. CPI, as it is described by Kakade & Langford (2002), is very slow (in one sample experiment on a 100 state problem, it made very slow progress and took several millions of iterations before it stopped) and we did not evaluate it further. Instead, we considered two variations: CPI+ that is identical to CPI except that it chooses the step \u03b1k at each iteration by doing a line-search towards the policy output by the greedy operator6, and CPI(\u03b1) with \u03b1 = 0.1, that makes \u201crelatively but not too small\u201d steps at each iteration. To assess the utility for CPI to use the distribution d\u03bd,\u03c0 for the approximate greedy step, we also considered API(\u03b1) with \u03b1 = 0.1, the variation of API described in Equation (4) that makes small steps, and that only differs from CPI(\u03b1) by the fact that the approximate greedy step uses the distribution \u03bd instead of d\u03c0k,\u03bd . In addition to these algorithms, we considered PSDP\u221e and NSPI(m) for the values m \u2208 {5, 10, 30}.\n6We implemented a crude line-search mechanism, that looks on the set 2i\u03b1 where \u03b1 is the minimal step estimated by CPI to ensure improvement.\nIn order to assess their quality, we consider finite problems where the exact value function can be computed. More precisely, we consider Garnet problems first introduced by Archibald et al. (1995), which are a class of randomly constructed finite MDPs. They do not correspond to any specific application, but remain representative of the kind of MDP that might be encountered in practice. In brief, we consider Garnet problems with |S| \u2208 {50, 100, 200}, |A| \u2208 {2, 5, 10} and branching factors in {1, 2, 10}. The greedy step used by all algorithms is approximated by an exact greedy operator applied to a noisy orthogonal projection on a linear space of dimension |S|10 with respect to the quadratic norm weighted by \u03bd or d\u03bd,\u03c0 (for CPI+ and CPI(\u03b1)) where \u03bd is uniform.\nFor each of these 33 = 27 parameter instances, we generated 30 i.i.d. Garnet MDPs (Mi)1\u2264i\u226430. For each such MDP Mi, we ran API, API(0.1), CPI+, CPI(0.1), NSPI(m) for m \u2208 {5, 10, 30} and PSDP\u221e 30 times. For each run j and algorithm, we compute for all iterations k \u2208 (1, 100) the performance, i.e. the loss Lj,k = \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) with respect to the optimal policy. Figure 2 displays statistics about these random variables. For each algorithm, we display a learning curve with confidence regions that account for the variability across runs and problems. The supplementary material contains statistics that are respectively conditioned on the values of nS , nA and b, which gives some insight on the influence of these parameters.\nFrom these experiments and statistics, we can make a series of observations. The standard API scheme is much more variable than the other algorithms and tends to provide the worst performance on average. CPI+ and CPI(\u03b1) display about the same asymptotic performance on average. If CPI(\u03b1) has slightly less variability, it is much slower than CPI+, that always converges in very few iterations (most of the time less than 10, and always less than 20). API(\u03b1)\u2014 the naive conservative variation of API that is also simpler than CPI(\u03b1)\u2014is empirically close to CPI(\u03b1), while being on average slightly worse. CPI+, CPI(\u03b1) and PSDP\u221e have a similar average performance, but the variability of PSDP\u221e is significantly smaller. PSDP\u221e is the algorithm that overall gives the best results. NSPI(m) does indeed provide a bridge between API and PSDP\u221e. By increasing m, the behavior gets closer to that of PSDP\u221e. With m = 30, NSPI(m) is overall better than API(\u03b1), CPI+, and CPI(\u03b1), and close to PSDP\u221e. The above relative observations are stable with respect to the number of states nS and actions nA. Interestingly, the differences between the algorithms tend to vanish when the dynamics of the problem gets more and more stochastic (when the branching factor increases). This complies with our analysis based on concentrability coefficients: there are all finite when the dynamics mixes a lot, and their relative difference are the biggest in deterministic instances."}, {"heading": "5. Discussion, Summary and Future Work", "text": "We have considered several variations of the Policy Iteration schemes for infinite-horizon problems: API, CPI, NSPI(m), API(\u03b1) and PSDP\u221e7. We have in particular explained the fact\u2014to our knowledge so far unknown\u2014 that the recently introduced NSPI(m) algorithm generalizes API (that is obtained when m=1) and PSDP\u221e (that is very similar whenm =\u221e). Figure 1 synthesized the theoretical guarantees about these algorithms. Most of the bounds are to our knowledge new.\nOne of the first important message of our work is that what is usually hidden in the constants of the performance bounds does matter. The constants involved in the bounds for API, CPI, PSDP\u221e and for the main (left) terms of NSPI(m) can be sorted from the worst to the best as follows: C(2,1,0), C(1,0), C(1)\u03c0\u2217 , C\u03c0\u2217 . A detailed hierarchy of all constants was depicted in Figure 1. This is to our knowledge the first time that such an in-depth comparison of the bounds is done, and our hierarchy of constants has interesting implications that go beyond the Policy Iteration schemes we have been focusing on in this paper. As a matter of fact, several other dynamic programming algorithms, namely AVI (Munos, 2007), \u03bbPI (Scherrer, 2013), AMPI (Scherrer et al., 2012), come with guarantees involv-\n7We recall that to our knowledge, the use of PSDP\u221e (PSDP in an infinite-horizon context) is not documented in the literature.\ning the worst constant C(2,1,0), which suggests that they should not be competitive with the best algorithms we have described here.\nAt the purely technical level, several of our bounds come in pair; this is due to the fact that we have introduced a new proof technique. This led to a new bound for API, that improves the state of the art in the sense that it involves the constant C(1,0) instead of C(2,1,0). It also enabled us to derive new bounds for CPI (and its natural algorithmic variant CPI(\u03b1)) that is worse in terms of guarantee but has a better time complexity (O\u0303( 1 ) instead of O( 1 2 )). We believe this new technique may be helpful in the future for the analysis of other MDP algorithms.\nLet us sum up the main insights of our analysis. 1) The guarantee for CPI can be arbitrarily stronger than that of API/API(\u03b1), because it is expressed with respect to the best concentrability constant C\u03c0\u2217 , but this comes at the cost of a relative\u2014exponential in 1 \u2014increase of the number of iterations. 2) PSDP\u221e enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP\u221e is proportional to their number of iterations, which may be problematic in particular when the discount factor \u03b3 is close to 1 or the approximation error is close to 0; we showed that the NSPI(m) algorithm allows to make an overall trade-off between memory and perfor-\nmance.\nThe main assumption of this work is that all algorithms have at disposal an -approximate greedy operator. It may be unreasonable to compare all algorithms on this basis, since the underlying optimization problems may have different complexities: for instance, methods like CPI look in a space of stochastic policies while API moves in a space of deterministic policies. Digging and understanding in more depth what is potentially hidden in the term \u2014as we have done here for the concentrability constants\u2014constitutes a very natural research direction.\nLast but not least, we have run numerical experiments that support our worst-case analysis. On simulations on about 800 Garnet MDPs with various characteristics, CPI(\u03b1), CPI+ (CPI with a crude line-search mechanism), PSDP\u221e and NSPI(m) were shown to always perform significantly better than the standard API. CPI+, CPI(\u03b1) and PSDP\u221e performed similarly on average, but PSDP\u221e showed much less variability and is thus the best algorithm in terms of overall performance. Finally, NSPI(m) allows to make a bridge between API and PSDP\u221e, reaching an overall performance close to that of PSDP\u221e with a controlled memory. Implementing other instances of these algorithmic schemes, running and analyzing experiments on bigger domains constitutes interesting future work."}, {"heading": "A. Proofs for Table 1", "text": "PSDP\u221e: For all k, we have\nv\u03c0\u2217 \u2212 v\u03c3k = T\u03c0\u2217v\u03c0\u2217 \u2212 T\u03c0\u2217v\u03c3k\u22121 + T\u03c0\u2217v\u03c3k\u22121 \u2212 T\u03c0kv\u03c3k\u22121\n\u2264 \u03b3P\u03c0\u2217(v\u03c0 \u2212 v\u03c3k\u22121) + ek\nwhere we defined ek = max\u03c0\u2032 T\u03c0\u2032v\u03c3k\u22121 \u2212 T\u03c0kv\u03c3k\u22121 . As P\u03c0\u2217 is non negative, we deduce by induction:\nv\u03c0\u2217 \u2212 v\u03c3k \u2264 k\u22121\u2211 i=0 (\u03b3P\u03c0\u2217) iek\u2212i + \u03b3 kVmax.\nBy multiplying both sides by \u00b5, using the definition of the coefficients c\u03c0\u2217(i) and the fact that \u03bdej \u2264 j \u2264 , we get:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c3k) \u2264 k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0\u2217) iek\u2212i + \u03b3 kVmax (7)\n\u2264 k\u22121\u2211 i=0 \u03b3ic\u03c0\u2217(i) k\u2212i + \u03b3 kVmax\n\u2264 ( k\u22121\u2211 i=0 \u03b3ic\u03c0\u2217(i) ) + \u03b3kVmax.\nThe bound with respect toC(1)\u03c0\u2217 is obtained by using the fact that v\u03c3k... \u2265 v\u03c3k \u2212 \u03b3kVmax and taking k \u2265 \u2308 log 2Vmax 1\u2212\u03b3 \u2309 .\nStarting back in Equation (7) and using the definition of C\u03c0\u2217 (in particular the fact that for all i, \u00b5(\u03b3P\u03c0\u2217)\ni \u2264 1\n1\u2212\u03b3 d\u03c0\u2217,\u00b5 \u2264 C\u03c0\u2217 1\u2212\u03b3 \u03bd) and the fact that \u03bdej \u2264 j , we get:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c3k) \u2264 k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0\u2217) iek\u2212i + \u03b3 kVmax\n\u2264 C\u03c0\u2217 1\u2212 \u03b3 k\u2211 i=1 i + \u03b3 kVmax\nand the other bound is obtained by using the fact that v\u03c3k... \u2265 v\u03c3k \u2212 \u03b3kVmax, \u2211k i=1 i \u2264 k , and considering\nthe number of iterations k = \u2308\nlog 2Vmax 1\u2212\u03b3\n\u2309 .\nAPI/NSPI(m): API is identical to NSPI(1), and its bounds are particular cases of the first two bounds for NSPI(m), so we only consider NSPI(m). By following the proof technique of Scherrer & Lesner (2012), writing \u0393k,m = (\u03b3P\u03c0k)(\u03b3P\u03c0k\u22121) \u00b7 \u00b7 \u00b7 (\u03b3P\u03c0k\u2212m+1) and ek+1 = max\u03c0\u2032 T\u03c0\u2032v\u03c0k,m \u2212 T\u03c0k+1v\u03c0k,m , one can show that: v\u03c0\u2217 \u2212 v\u03c0k,m \u2264 k\u22121\u2211 i=0 (\u03b3P\u03c0\u2217) i(I \u2212 \u0393k\u2212i,m)\u22121ek\u2212i + \u03b3kVmax.\nMultiplying both sides by \u00b5 (and observing that ek \u2265 0) and the fact that \u03bdej \u2264 j \u2264 , we obtain:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k)\n\u2264 k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0\u2217) i(I \u2212 \u0393k\u2212i,m)\u22121ek\u2212i + \u03b3kVmax (8) \u2264 k\u22121\u2211 i=0  \u221e\u2211 j=0 \u03b3i+jmc(i+ jm) k\u2212i\n+ \u03b3kVmax (9) \u2264 k\u22121\u2211 i=0 \u221e\u2211 j=0 \u03b3i+jmc(i+ jm) + \u03b3kVmax, (10)\nwhich leads to the first bound by taking k \u2265 \u2308\nlog 2Vmax 1\u2212\u03b3\n\u2309 .\nStarting back on Equation (9), assuming for simplicity that \u2212k = 0 for all k \u2265 0, we get:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k )\u2212 \u03b3 kVmax\n\u2264 d k\u22121m e\u2211 l=0 m\u22121\u2211 h=0 \u221e\u2211 j=0 \u03b3h+(l+j)mc(h+ (l + j)m) k\u2212h\u2212lm \u2264 d k\u22121m e\u2211 l=0 m\u22121\u2211 h=0 \u221e\u2211 j=l \u03b3h+jmc(h+ jm) max k\u2212(l+1)m+1\u2264p\u2264k\u2212lm p \u2264 d k\u22121m e\u2211 l=0 m\u22121\u2211 h=0 \u221e\u2211 j=0 \u03b3h+jmc(h+ jm) max k\u2212(l+1)m+1\u2264p\u2264k\u2212lm p\n= ( m\u22121\u2211 h=0 \u221e\u2211 j=0 \u03b3h+jmc(h+ jm) ) d k\u22121m e\u2211 l=0 max l\u2212(l+1)m+1\u2264p\u2264k\u2212lm p\n\u2264 ( \u221e\u2211 i=0 \u03b3ic(i) )\u2308 k \u2212 1 m \u2309 , (11)\nwhich leads to the second bound by taking k =\u2308 log 2Vmax\n1\u2212\u03b3\n\u2309 . Last but not least, starting back on Equa-\ntion (8), and using the fact that (I \u2212 \u0393k\u2212i,m)\u22121 = I + \u0393k\u2212i,m(I \u2212 \u0393k\u2212i,m)\u22121 we see that:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k)\u2212 \u03b3kVmax \u2264 k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0\u2217) iek\u2212i +\n+ k\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0\u2217) i\u0393k\u2212i,m(I \u2212 \u0393k\u2212i,m)\u22121ek\u2212i.\nThe first term of the r.h.s. can be bounded exactly as for PSDP\u221e. For the second term, we have:\nk\u22121\u2211 i=0 \u00b5(\u03b3P\u03c0\u2217) i\u0393k\u2212i,m(I \u2212 \u0393k\u2212i,m)\u22121ek\u2212i\n\u2264 k\u22121\u2211 i=0 \u221e\u2211 j=1 \u03b3i+jmc(i+ jm) k\u2212i = \u03b3m k\u22121\u2211 i=0 \u221e\u2211 j=0 \u03b3i+jmc(i+ (j + 1)m) k\u2212i,\nand we follow the same lines as above (from Equation (9) to Equations (10) and (11)) to conclude.\nCPI, CPI(\u03b1), API(\u03b1): Conservative steps are addressed by a tedious generalization of the proof for API by Munos (2003). Due to lack of space, the proof is deferred to the Supplementary Material."}, {"heading": "B. Proofs for Figure 1", "text": "We here provide details on the order relation for the concentrability coefficients.\nC\u03c0\u2217 \u2192 C (1) \u03c0\u2217 : (i) We have C\u03c0\u2217 \u2264 C (1) \u03c0\u2217 because d\u03c0\u2217,\u00b5 = (1\u2212 \u03b3)\u00b5(I \u2212 \u03b3P\u03c0\u2217)\u22121 = (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3i\u00b5(P\u03c0\u2217) i\n\u2264 (1\u2212 \u03b3) \u221e\u2211 i=0 \u03b3ic\u03c0\u2217(i)\u03bd = C (1) \u03c0\u2217 \u03bd\nand C\u03c0\u2217 is the smallest coefficient C satisfying d\u03c0\u2217,\u00b5 \u2264 C\u03bd. (ii) We may have C\u03c0\u2217 <\u221e and C (1) \u03c0\u2217 =\u221e by design-\ning a MDP on N where \u03c0\u2217 induces a deterministic transition from state i to state i+ 1.\nC (1) \u03c0\u2217 \u2192 C(1,0): (i) We have C (1) \u03c0\u2217 \u2264 C(1,0) because for all i, c\u03c0\u2217(i) \u2264 c(i). (ii) It is easy to obtain C (1) \u03c0\u2217 < \u221e and C(1,0) = \u221e since C(1)\u03c0\u2217 only depends on one policy while C (1) \u03c0\u2217 depends on all policies.\nC(1,0) \u2192 C(2,m,0) and C(1,m) \u2192 C(2,m,m): (i) C(1,m) \u2264 11\u2212\u03b3mC (2,m,m) holds because\nC(1,m) 1\u2212 \u03b3 = \u221e\u2211 i=0 \u03b3ic(i+m) \u2264 \u221e\u2211 i=0 \u221e\u2211 j=0 \u03b3i+jmc(i+ (j + 1)m)\n= 1\n(1\u2212 \u03b3)(1\u2212 \u03b3m)C (2,m,m).\n(ii) One may have C(1,m) < \u221e and C(2,m,m) = \u221e when c(i) = \u0398( 1i2\u03b3i ), since the generic term of C\n(1,m) is \u0398( 1i2 ) (the sum converges) while that of C(2,m,m) is \u0398( 1i ) (the sum diverges). The reasoning is similar for the other relation.\nC(1,m) \u2192 C(1,0) and C(2,m,m) \u2192 C(2,m,0): We here assume that m < \u221e. (i) We have C(1,m) \u2264 1\u03b3mC\n(1,0) and C(2,m,m) \u2264 1\u03b3mC\n(2,m,0). (ii) It suffices that c(j) = \u221e for some j < m to have C(2,m,0) = \u221e while C(2,m,m) < \u221e, or to have C(1,0) =\u221e while C(1,m) <\u221e.\nC(2,1,0) \u2194 C(2,m,0): (i) We clearly have C(2,m,0) \u2264 1\u2212\u03b3m 1\u2212\u03b3 C (2,1,0). (ii) C(2,m,0) can be rewritten as follows:\nC(2,m,0) = (1\u2212 \u03b3)(1\u2212 \u03b3m) \u221e\u2211 i=0 ( 1 + \u230a i m \u230b) \u03b3ic(i).\nThen, using the fact that 1 + \u230a i m \u230b \u2265 max ( 1, im ) , we have\n1\u2212 \u03b3 1\u2212 \u03b3m C(2,m,0) \u2265 \u221e\u2211 i=0 max ( 1, i m ) \u03b3ic(i)\n\u2265 m\u22121\u2211 i=0 \u03b3ic(i) + \u221e\u2211 i=m i m \u03b3ic(i)\n\u2265 m\u22121\u2211 i=0 \u03b3ic(i) + m m+ 1 \u221e\u2211 i=m i+ 1 m \u03b3ic(i)\n= m\u22121\u2211 i=0 \u03b3ic(i) + m m+ 1\n( C(2,1,0) \u2212\nm\u22121\u2211 i=0 \u03b3ic(i)\n)\n= m\nm+ 1 C(2,1,0) +\n1\nm+ 1 m\u22121\u2211 i=0 \u03b3ic(i).\nThus, when m is finite, C(2,m,0) <\u221e\u21d2 C(2,1,0) <\u221e."}, {"heading": "C. Proof for CPI, CPI(\u03b1), API(\u03b1)", "text": "We begin by proving the following result:\nTheorem 1. At each iteration k < k\u2217 of CPI (Equation (3)), the expected loss satisfies:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 C(1,0) (1\u2212 \u03b3)2 k\u2211 i=1 \u03b1i i + e {(1\u2212\u03b3)\u2211ki=1 \u03b1i}Vmax.\nProof. Using the facts that T\u03c0k+1v\u03c0k = (1 \u2212 \u03b1k+1)v\u03c0k + \u03b1k+1T\u03c0k+1v\u03c0k and the notation ek+1 = max\u03c0\u2032 T\u03c0\u2032v\u03c0k \u2212 T\u03c0\u2032k+1v\u03c0k , we have:\nv\u03c0\u2217 \u2212 v\u03c0k+1 = v\u03c0\u2217 \u2212 T\u03c0k+1v\u03c0k + T\u03c0k+1v\u03c0k \u2212 T\u03c0k+1v\u03c0k+1 = v\u03c0\u2217 \u2212 (1\u2212 \u03b1k+1)v\u03c0k \u2212 \u03b1k+1T\u03c0\u2032k+1v\u03c0k + \u03b3P\u03c0k+1(v\u03c0k \u2212 v\u03c0k+1)\n= (1\u2212 \u03b1k+1)(v\u03c0\u2217 \u2212 v\u03c0k) + \u03b1k+1(T\u03c0\u2217v\u03c0\u2217 \u2212 T\u03c0\u2217v\u03c0k) + \u03b1k+1(T\u03c0\u2217v\u03c0k \u2212 T\u03c0\u2032k+1v\u03c0k) + \u03b3P\u03c0k+1(v\u03c0k \u2212 v\u03c0k+1)\n\u2264 [(1\u2212 \u03b1k+1)I + \u03b1k+1\u03b3P\u03c0\u2217 ] (v\u03c0 \u2212 v\u03c0k) + \u03b1k+1ek+1 + \u03b3P\u03c0k+1(v\u03c0k \u2212 v\u03c0k+1). (12)\nUsing the fact that v\u03c0k+1 = (I \u2212 \u03b3P\u03c0k+1)\u22121r, and the fact that (I \u2212 \u03b3P\u03c0k+1)\u22121 is non-negative, we can see that\nv\u03c0k \u2212 v\u03c0k+1 = (I \u2212 \u03b3P\u03c0k+1)\u22121(v\u03c0k \u2212 \u03b3P\u03c0k+1v\u03c0k \u2212 r) = (I \u2212 \u03b3P\u03c0k+1)\u22121(T\u03c0kv\u03c0k \u2212 T\u03c0k+1v\u03c0k) \u2264 (I \u2212 \u03b3P\u03c0k+1)\u22121\u03b1k+1ek+1.\nPutting this back in Equation (12), we obtain:\nv\u03c0\u2217 \u2212 v\u03c0k+1 \u2264 [(1\u2212 \u03b1k+1)I + \u03b1k+1\u03b3P\u03c0\u2217 ] (v\u03c0 \u2212 v\u03c0k) + \u03b1k+1(I \u2212 \u03b3P\u03c0k+1)\u22121ek+1.\nDefine the matrix Qk = [(1\u2212 \u03b1k)I + \u03b1k\u03b3P\u03c0\u2217 ], the set Ni,k = {j; k \u2212 i + 1 \u2264 j \u2264 k} (this set contains exactly i elements), the matrix Ri,k = \u220f j\u2208Ni,k Qj , and the coefficients \u03b2k = 1 \u2212 \u03b1k(1 \u2212 \u03b3) and \u03b4k = \u220fk i=1 \u03b2k. By repeatedly using the fact that the matrices Qk are non-negative, we get by induction\nv\u03c0\u2217 \u2212 v\u03c0k \u2264 k\u22121\u2211 i=0 Ri,k\u03b1k\u2212i(I \u2212 \u03b3P\u03c0k\u2212i)\u22121ek\u2212i + \u03b4kVmax. (13)\nLet Pj(Ni,k) be the set of subsets of Ni,k of size j. With this notation we have\nRi,k = i\u2211 j=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k(\u03b3P\u03c0\u2217) j\nwhere for all subset I of Ni,k, we wrote\n\u03b6I,i,k = (\u220f n\u2208I \u03b1n ) \u220f n\u2208Ni,k\\I (1\u2212 \u03b1n)  . Therefore, by multiplying Equation (13) by \u00b5, using the definition of the coefficients c(i), and the facts that \u03bd \u2264 (1 \u2212\n\u03b3)d\u03bd,\u03c0k+1 , we obtain:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 1\n1\u2212 \u03b3 k\u22121\u2211 i=0 i\u2211 j=0 \u221e\u2211 l=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k\u03b3 j+lc(j + l)\u03b1k\u2212i k\u2212i + \u03b4kVmax.\n= 1\n1\u2212 \u03b3 k\u22121\u2211 i=0 i\u2211 j=0 \u221e\u2211 l=j \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k\u03b3 lc(l)\u03b1k\u2212i k\u2212i + \u03b4kVmax\n\u2264 1 1\u2212 \u03b3 k\u22121\u2211 i=0 i\u2211 j=0 \u221e\u2211 l=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k\u03b3 lc(l)\u03b1k\u2212i k\u2212i + \u03b4kVmax\n= 1\n1\u2212 \u03b3 ( \u221e\u2211 l=0 \u03b3lc(l) ) k\u22121\u2211 i=0  i\u2211 j=0 \u2211 I\u2208Pj(Ni,k) \u03b6I,i,k \u03b1k\u2212i k\u2212i + \u03b4kVmax = 1\n1\u2212 \u03b3 ( \u221e\u2211 l=0 \u03b3lc(l) ) k\u22121\u2211 i=0  \u220f j\u2208Ni,k (1\u2212 \u03b1j + \u03b1j) \u03b1k\u2212i k\u2212i + \u03b4kVmax = 1\n1\u2212 \u03b3 ( \u221e\u2211 l=0 \u03b3lc(l) )( k\u22121\u2211 i=0 \u03b1k\u2212i k\u2212i ) + \u03b4kVmax.\nNow, using the fact that for x \u2208 (0, 1), log(1\u2212 x) \u2264 \u2212x, we can observe that\nlog \u03b4k = log k\u220f i=1 \u03b2i = k\u2211 i=1 log \u03b2i = k\u2211 i=1 log(1\u2212 \u03b1i(1\u2212 \u03b3)) \u2264 \u2212(1\u2212 \u03b3) k\u2211 i=1 \u03b1i.\nAs a consequence, we get \u03b4k \u2264 e\u2212(1\u2212\u03b3) \u2211k i=1 \u03b1i .\nIn the analysis of CPI, Kakade & Langford (2002) show that the learning steps that ensure the nice performance guarantee of CPI satisfy \u03b1k \u2265 (1\u2212\u03b3) 12\u03b3Vmax , the right term e {(1\u2212\u03b3)\u2211ki=1 \u03b1i} above tends 0 exponentially fast, and we get the following corollary that shows that CPI has a performance bound with the coefficient C(1,0) of API in a number of iterations O ( log 1 ) .\nCorollary 1. The smallest (random) iteration k\u2020 such that log Vmax 1\u2212\u03b3 \u2264 \u2211k\u2020 i=1 \u03b1i \u2264 log Vmax 1\u2212\u03b3 + 1 is such that k\n\u2020 \u2264 12\u03b3Vmax log Vmax\n(1\u2212\u03b3)2 and the policy \u03c0k\u2020 satisfies:\n\u00b5(v\u03c0\u2217 \u2212 v\u03c0k\u2020 ) \u2264\nC(1,0) (\u2211k\u2020 i=1 \u03b1i ) (1\u2212 \u03b3)2 + 1  \u2264 (C(1,0) (log Vmax + 1) (1\u2212 \u03b3)3 + 1 ) .\nSince the proof is based on a generalization of the analysis of API and thus does not use any of the specific properties of CPI, it turns out that the results we have just given can straightforwardly be specialized to CPI(\u03b1).\nCorollary 2. Assume we run CPI(\u03b1) for some \u03b1 \u2208 (0, 1), that is CPI (Equation (3)) with \u03b1k = \u03b1 for all k.\nIf k = \u2308 log Vmax \u03b1(1\u2212 \u03b3) \u2309 , then \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 \u03b1(k + 1)C(1,0) (1\u2212 \u03b3)2 \u2264 ( C(1,0) ( log Vmax + 1 ) (1\u2212 \u03b3)3 + 1 ) .\nThe above bound for CPI(\u03b1) involves the factor 1(1\u2212\u03b3)3 . A precise examination of the proof shows that this amplification is due to the fact that the approximate greedy operator uses the distribution d\u03c0k,\u03bd \u2265 (1\u2212 \u03b3)\u03bd instead of \u03bd (for API). In fact, using a very similar proof, it is easy to show that API(\u03b1) satisfies the following result.\nCorollary 3. Assume API(\u03b1) is run for some \u03b1 \u2208 (0, 1).\nIf k = \u2308 log Vmax \u03b1(1\u2212 \u03b3) \u2309 , then \u00b5(v\u03c0\u2217 \u2212 v\u03c0k) \u2264 \u03b1(k + 1)C(1,0) (1\u2212 \u03b3) \u2264 ( C(1,0) ( log Vmax + 1 ) (1\u2212 \u03b3)2 + 1 ) ."}, {"heading": "D. More details on the Numerical Simulations", "text": "Domain and Approximations In our experiments, a Garnet is parameterized by 4 parameters and is written G(nS , nA, b, p): nS is the number of states, nA is the number of actions, b is a branching factor specifying how many possible next states are possible for each state-action pair (b states are chosen uniformly at random and transition probabilities are set by sampling uniform random b \u2212 1 cut points between 0 and 1) and p is the number of features (for linear function approximation). The reward is state-dependent: for a given randomly generated Garnet problem, the reward for each state is uniformly sampled between 0 and 1. Features are chosen randomly: \u03a6 is a nS\u00d7p feature matrix of which each component is randomly and uniformly sampled between 0 and 1. The discount factor \u03b3 is set to 0.99 in all experiments.\nAll the algorithms we have discussed in the paper need to repeatedly compute G (\u03c1, v) for some distribution \u03c1 = \u03bd or \u03c1 = d\u03c0,\u03bd . In other words, they must be able to make calls to an approximate greedy operator applied to the value v of some policy for some distribution \u03c1. To implement this operator, we compute a noisy estimate of the value v with a uniform white noise u(\u03b9) of amplitude \u03b9, then projects this estimate onto the space spanned by \u03a6 with respect to the \u03c1-quadratic norm (projection that we write \u03a0\u03a6,\u03c1), and then applies the (exact) greedy operator on this projected estimate. In a nutshell, one call to the approximate greedy operator G (\u03c1, v) amounts to compute G\u03a0\u03a6,\u03c1(v + u(\u03b9)).\nSimulations We have run series of experiments, in which we callibrated the perturbations (noise, approximations) so that the algorithm are significantly perturbed but no too much (we do not want their behavior to become too erratic). After trial and error, we ended up considering the following setting. We used Garnet problems G(nS , nA, b, p) with the number of states nS \u2208 {50, 100, 200}, the number of actions nA \u2208 {2, 5, 10}, the branching factor b \u2208 {1, 2, 10}} (b = 1 corresponds to deterministic problems), the number of features to approximate the value p = nS10 , and the noise level \u03b9 = 0.1 (10%).\nIn addition to Figure 2 that shows the statistics overall for the all the parameter instances, Figure 3, 4 and 5 display statistics that are respectively conditioned on the values of nS , nA and b, which gives some insight on the influence of these parameters."}], "references": [{"title": "On the Generation of Markov Decision Processes", "author": ["T. References Archibald", "K. McKinnon", "L. Thomas"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "Archibald et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Archibald et al\\.", "year": 1995}, {"title": "Policy search by dynamic programming", "author": ["J.A. Bagnell", "S.M. Kakade", "A. Ng", "J. Schneider"], "venue": "In NIPS,", "citeRegEx": "Bagnell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2003}, {"title": "Error propagation for approximate policy and value iteration (extended version)", "author": ["A.M. Farahmand", "R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "In NIPS,", "citeRegEx": "Farahmand et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2010}, {"title": "Conservative and Greedy Approaches to Classification-based Policy Iteration", "author": ["M. Ghavamzadeh", "A. Lazaric"], "venue": "In AAAI,", "citeRegEx": "Ghavamzadeh and Lazaric,? \\Q2012\\E", "shortCiteRegEx": "Ghavamzadeh and Lazaric", "year": 2012}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Reinforcement Learning as Classification: Leveraging Modern Classifiers", "author": ["M. Lagoudakis", "R. Parr"], "venue": "In ICML,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Analysis of a Classification-based Policy Iteration Algorithm", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "In ICML,", "citeRegEx": "Lazaric et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lazaric et al\\.", "year": 2010}, {"title": "Error Bounds for Approximate Policy Iteration", "author": ["R. Munos"], "venue": "In ICML,", "citeRegEx": "Munos,? \\Q2003\\E", "shortCiteRegEx": "Munos", "year": 2003}, {"title": "Performance Bounds in Lp norm for Approximate Value Iteration", "author": ["R. Munos"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Munos,? \\Q2007\\E", "shortCiteRegEx": "Munos", "year": 2007}, {"title": "Performance Bounds for Lambda Policy Iteration and Application to the Game of Tetris", "author": ["B. Scherrer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Scherrer,? \\Q2013\\E", "shortCiteRegEx": "Scherrer", "year": 2013}, {"title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes", "author": ["B. Scherrer", "B. Lesner"], "venue": "In NIPS,", "citeRegEx": "Scherrer and Lesner,? \\Q2012\\E", "shortCiteRegEx": "Scherrer and Lesner", "year": 2012}, {"title": "Approximate Modified Policy Iteration", "author": ["Scherrer", "Bruno", "Ghavamzadeh", "Mohammad", "Gabillon", "Victor", "Geist", "Matthieu"], "venue": "In ICML,", "citeRegEx": "Scherrer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP\u221e), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012).", "startOffset": 283, "endOffset": 305}, {"referenceID": 7, "context": "Approximate Policy Iteration Schemes: A Comparison (cost-sensitive) classification problem (Lagoudakis & Parr, 2003a; Lazaric et al., 2010).", "startOffset": 91, "endOffset": 139}, {"referenceID": 6, "context": "Approximate Policy Iteration Schemes: A Comparison (cost-sensitive) classification problem (Lagoudakis & Parr, 2003a; Lazaric et al., 2010). With this operator in hand, we shall describe several Policy Iteration schemes in Section 2. Then Section 3 will provide a detailed comparative analysis of their performance guarantees, time complexities, and memory requirements. Section 4 will go on by providing experiments that will illustrate their behavior, and confirm our analysis. Finally, Section 5 will conclude and present future work. 2. Algorithms API We begin by describing the standard Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996). At each iteration k, the algorithm switches to the policy that is approximately greedy with respect to the value of the previous policy for some distribution \u03bd: \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c0k). (2) If there is no error ( k = 0) and \u03bd assigns a positive weights to every state, it can easily be seen that this algorithm generates the same sequence of policies as exact Policy Iterations since from Equation (1) the policies are exactly greedy. CPI/CPI(\u03b1)/API(\u03b1) We now turn to the description of Conservative Policy Iteration (CPI) proposed by (Kakade & Langford, 2002). At iteration k, CPI (described in Equation (3)) uses the distribution d\u03c0k,\u03bd = (1 \u2212 \u03b3)\u03bd(I \u2212 \u03b3P\u03c0k) \u22121\u2014the discounted cumulative occupancy measure induced by \u03c0k when starting from \u03bd\u2014for calling the approximate greedy operator, and uses a stepsize \u03b1k to generate a stochastic mixture of all the policies that are returned by the successive calls to the approximate greedy operator, which explains the adjective \u201cconservative\u201d: \u03c0k+1 \u2190 (1\u2212 \u03b1k+1)\u03c0k + \u03b1k+1G k+1(d\u03c0k,\u03bd , v\u03c0k) (3) The stepsize \u03b1k+1 can be chosen in such a way that the above step leads to an improvement of the expected value of the policy given that the process is initialized according to the distribution \u03bd (Kakade & Langford, 2002). The original article also describes a criterion for deciding whether to stop or to continue. Though the adaptive stepsize and the stopping condition allows to derive a nice analysis, they are in practice conservative: the stepsize \u03b1k should be implemented with a line-search mechanism, or be fixed to some small value \u03b1. We will refer to this latter variation of CPI as CPI(\u03b1). It is natural to also consider the algorithm API(\u03b1) (mentioned by Lagoudakis & Parr (2003a)), a variation of API that is conservative like CPI(\u03b1) in the sense that it mixes the new policy with the previous ones with weights \u03b1 and 1\u2212\u03b1, but that directly uses the distribution \u03bd in the approximate greedy step: \u03c0k+1 \u2190 (1\u2212 \u03b1)\u03c0k + \u03b1G k+1(\u03bd, v\u03c0k) (4) Because it uses \u03bd instead of d\u03c0k,\u03bd , API(\u03b1) is simpler to implement than CPI(\u03b1)1.", "startOffset": 118, "endOffset": 2382}, {"referenceID": 1, "context": "This algorithm is a natural variation of the Policy Search by Dynamic Programming algorithm (PSDP) of Bagnell et al. (2003), originally proposed to tackle finite-horizon problems, to the infinite-horizon case; we thus refer to it as PSDP\u221e.", "startOffset": 102, "endOffset": 124}, {"referenceID": 1, "context": "This algorithm is a natural variation of the Policy Search by Dynamic Programming algorithm (PSDP) of Bagnell et al. (2003), originally proposed to tackle finite-horizon problems, to the infinite-horizon case; we thus refer to it as PSDP\u221e. To the best of our knowledge however, this variation has never been used in an infinite-horizon context. The algorithm is based on finite-horizon non-stationary policies. Given a sequence of stationary deterministic policies (\u03c0k) that the algorithm will generate, we will write \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01 the k-horizon policy that makes the first action according to \u03c0k, then the second action according to \u03c0k\u22121, etc. Its value is v\u03c3k = T\u03c0kT\u03c0k\u22121 . . . T\u03c01r. We will write \u2205 the \u201cempty\u201d non-stationary policy. Note that v\u2205 = r and that any infinite-horizon policy that begins with \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01, which we will (somewhat abusively) denote \u201c\u03c3k . . . \u201d has a value v\u03c3k... \u2265 v\u03c3k \u2212\u03b3Vmax. Starting from \u03c30 = \u2205, the algorithm implicitely builds a sequence of non-stationary policies (\u03c3k) by iteratively concatenating the policies that are returned by the approximate greedy operator: \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c3k) (5) While the standard PSDP algorithm of Bagnell et al. (2003) considers a horizon T and makes T iterations, the algorithm we consider here has an indefinite number of iterations.", "startOffset": 102, "endOffset": 1201}, {"referenceID": 1, "context": "This algorithm is a natural variation of the Policy Search by Dynamic Programming algorithm (PSDP) of Bagnell et al. (2003), originally proposed to tackle finite-horizon problems, to the infinite-horizon case; we thus refer to it as PSDP\u221e. To the best of our knowledge however, this variation has never been used in an infinite-horizon context. The algorithm is based on finite-horizon non-stationary policies. Given a sequence of stationary deterministic policies (\u03c0k) that the algorithm will generate, we will write \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01 the k-horizon policy that makes the first action according to \u03c0k, then the second action according to \u03c0k\u22121, etc. Its value is v\u03c3k = T\u03c0kT\u03c0k\u22121 . . . T\u03c01r. We will write \u2205 the \u201cempty\u201d non-stationary policy. Note that v\u2205 = r and that any infinite-horizon policy that begins with \u03c3k = \u03c0k\u03c0k\u22121 . . . \u03c01, which we will (somewhat abusively) denote \u201c\u03c3k . . . \u201d has a value v\u03c3k... \u2265 v\u03c3k \u2212\u03b3Vmax. Starting from \u03c30 = \u2205, the algorithm implicitely builds a sequence of non-stationary policies (\u03c3k) by iteratively concatenating the policies that are returned by the approximate greedy operator: \u03c0k+1 \u2190 G k+1(\u03bd, v\u03c3k) (5) While the standard PSDP algorithm of Bagnell et al. (2003) considers a horizon T and makes T iterations, the algorithm we consider here has an indefinite number of iterations. The algorithm can be stopped at any step k. The theory that we are about to describe suggests that one may return any policy that starts by the non-stationary policy \u03c3k. Since \u03c3k is an approximately good finite-horizon policy, and as we consider an infinite-horizon problem, a natural output that one may want to use in practice is the infinitehorizon policy that loops over \u03c3k, that we shall denote (\u03c3k) \u221e. In practice, controlling the greedy step with respect to d\u03c0k,\u03bd requires to generate samples from this very distribution. As explained by Kakade & Langford (2002), one such sample can be done by running one trajectory starting from \u03bd and following \u03c0k, stopping at each step with probability 1 \u2212 \u03b3.", "startOffset": 102, "endOffset": 1888}, {"referenceID": 9, "context": "NSPI(m) We originally devised the algorithmic scheme of Equation (5) (PSDP\u221e) as a simplified variation of the Non-Stationary PI algorithm with a growing period algorithm (NSPI-growing) (Scherrer & Lesner, 2012)2. With respect to Equation (5), the only difference of NSPIgrowing resides in the fact that the approximate greedy step is done with respect to the value v(\u03c3k)\u221e of the policy that loops infinitely over \u03c3k (formally the algorithm does \u03c0k+1 \u2190 G k+1(\u03bd, v(\u03c3k)\u221e)) instead of the value v\u03c3k of only the first k steps here. Following the intuition that when k is big, these two values will be close to each other, we ended up considering PSDP\u221e because it is simpler. NSPIgrowing suffers from the same memory drawback as CPI and PSDP\u221e. Interestingly, the work of Scherrer & Lesner (2012) contains another algorithm, Non-Stationary PI with a fixed period (NSPI(m)), that has a parameter that directly controls the number of policies stored in memory.", "startOffset": 186, "endOffset": 790}, {"referenceID": 7, "context": "Implementing this algorithm in practice can trivially be done through cost-sensitive classification in a way similar to Lazaric et al. (2010). It could also be done with a straight-forward extension of LSTD(\u03bb) to non-stationary policies.", "startOffset": 120, "endOffset": 142}, {"referenceID": 2, "context": "Relaxing the goal to controlling the weighted `p-norm for some p \u2265 2 allows to introduce some finer coefficients (Farahmand et al., 2010; Scherrer et al., 2012).", "startOffset": 113, "endOffset": 160}, {"referenceID": 12, "context": "Relaxing the goal to controlling the weighted `p-norm for some p \u2265 2 allows to introduce some finer coefficients (Farahmand et al., 2010; Scherrer et al., 2012).", "startOffset": 113, "endOffset": 160}, {"referenceID": 7, "context": "(2)) C 1 (1\u2212\u03b3)2 1 1\u2212\u03b3 log 1 1 (Lazaric et al., 2010) (= NSPI(1)) C 1 (1\u2212\u03b3)2 log 1 API(\u03b1) (Eq.", "startOffset": 30, "endOffset": 52}, {"referenceID": 7, "context": "(2)) C 1 (1\u2212\u03b3)2 1 1\u2212\u03b3 log 1 1 (Lazaric et al., 2010) (= NSPI(1)) C 1 (1\u2212\u03b3)2 log 1 API(\u03b1) (Eq. (4) C 1 (1\u2212\u03b3)2 1 \u03b1(1\u2212\u03b3) log 1 CPI(\u03b1) C 1 (1\u2212\u03b3)3 1 \u03b1(1\u2212\u03b3) log 1 CPI (Eq. (3)) C (1,0) 1 (1\u2212\u03b3)3 log 1 1 1\u2212\u03b3 1 log 1 C\u03c0\u2217 1 (1\u2212\u03b3)2 \u03b3 2 (Kakade & Langford, 2002) PSDP\u221e (Eq. (5)) C\u03c0\u2217 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 (' NSPI(\u221e)) C (1) \u03c0\u2217 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 NSPI(m) (Eq. (6)) C 1 (1\u2212\u03b3)(1\u2212\u03b3m) 1 1\u2212\u03b3 log 1 m C m 1 (1\u2212\u03b3)2(1\u2212\u03b3m) log 1 1 1\u2212\u03b3 log 1 C (1) \u03c0\u2217 + \u03b3 mC 1\u2212\u03b3m 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 C\u03c0\u2217 + \u03b3 m C m(1\u2212\u03b3m) 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 Table 1. Upper bounds on the performance guarantees for the algorithms. Except when references are given, the bounds are to our knowledge new. A comparison of API and CPI based on the two known bounds was done by Ghavamzadeh & Lazaric (2012). The first bound of NSPI(m) can be seen as an adaptation of that provided by Scherrer & Lesner (2012) for the more restrictive `\u221e-norm setting.", "startOffset": 31, "endOffset": 750}, {"referenceID": 7, "context": "(2)) C 1 (1\u2212\u03b3)2 1 1\u2212\u03b3 log 1 1 (Lazaric et al., 2010) (= NSPI(1)) C 1 (1\u2212\u03b3)2 log 1 API(\u03b1) (Eq. (4) C 1 (1\u2212\u03b3)2 1 \u03b1(1\u2212\u03b3) log 1 CPI(\u03b1) C 1 (1\u2212\u03b3)3 1 \u03b1(1\u2212\u03b3) log 1 CPI (Eq. (3)) C (1,0) 1 (1\u2212\u03b3)3 log 1 1 1\u2212\u03b3 1 log 1 C\u03c0\u2217 1 (1\u2212\u03b3)2 \u03b3 2 (Kakade & Langford, 2002) PSDP\u221e (Eq. (5)) C\u03c0\u2217 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 (' NSPI(\u221e)) C (1) \u03c0\u2217 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 NSPI(m) (Eq. (6)) C 1 (1\u2212\u03b3)(1\u2212\u03b3m) 1 1\u2212\u03b3 log 1 m C m 1 (1\u2212\u03b3)2(1\u2212\u03b3m) log 1 1 1\u2212\u03b3 log 1 C (1) \u03c0\u2217 + \u03b3 mC 1\u2212\u03b3m 1 1\u2212\u03b3 1 1\u2212\u03b3 log 1 C\u03c0\u2217 + \u03b3 m C m(1\u2212\u03b3m) 1 (1\u2212\u03b3)2 log 1 1 1\u2212\u03b3 log 1 Table 1. Upper bounds on the performance guarantees for the algorithms. Except when references are given, the bounds are to our knowledge new. A comparison of API and CPI based on the two known bounds was done by Ghavamzadeh & Lazaric (2012). The first bound of NSPI(m) can be seen as an adaptation of that provided by Scherrer & Lesner (2012) for the more restrictive `\u221e-norm setting.", "startOffset": 31, "endOffset": 852}, {"referenceID": 0, "context": "More precisely, we consider Garnet problems first introduced by Archibald et al. (1995), which are a class of randomly constructed finite MDPs.", "startOffset": 64, "endOffset": 88}, {"referenceID": 9, "context": "As a matter of fact, several other dynamic programming algorithms, namely AVI (Munos, 2007), \u03bbPI (Scherrer, 2013), AMPI (Scherrer et al.", "startOffset": 78, "endOffset": 91}, {"referenceID": 10, "context": "As a matter of fact, several other dynamic programming algorithms, namely AVI (Munos, 2007), \u03bbPI (Scherrer, 2013), AMPI (Scherrer et al.", "startOffset": 97, "endOffset": 113}, {"referenceID": 12, "context": "As a matter of fact, several other dynamic programming algorithms, namely AVI (Munos, 2007), \u03bbPI (Scherrer, 2013), AMPI (Scherrer et al., 2012), come with guarantees involvWe recall that to our knowledge, the use of PSDP\u221e (PSDP in an infinite-horizon context) is not documented in the literature.", "startOffset": 120, "endOffset": 143}, {"referenceID": 10, "context": "By following the proof technique of Scherrer & Lesner (2012), writing \u0393k,m = (\u03b3P\u03c0k)(\u03b3P\u03c0k\u22121) \u00b7 \u00b7 \u00b7 (\u03b3P\u03c0k\u2212m+1) and ek+1 = max\u03c0\u2032 T\u03c0\u2032v\u03c0k,m \u2212 T\u03c0k+1v\u03c0k,m , one can show that: v\u03c0\u2217 \u2212 v\u03c0k,m \u2264 k\u22121 \u2211 i=0 (\u03b3P\u03c0\u2217) (I \u2212 \u0393k\u2212i,m)ek\u2212i + \u03b3Vmax.", "startOffset": 36, "endOffset": 61}, {"referenceID": 8, "context": "CPI, CPI(\u03b1), API(\u03b1): Conservative steps are addressed by a tedious generalization of the proof for API by Munos (2003). Due to lack of space, the proof is deferred to the Supplementary Material.", "startOffset": 106, "endOffset": 119}], "year": 2014, "abstractText": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP\u221e), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012). For all algorithms, we describe performance bounds with respect the per-iteration error , and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API, but this comes at the cost of a relative\u2014exponential in 1 \u2014increase of the number of iterations. 2) PSDP\u221e enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP\u221e is proportional to their number of iterations, which may be problematic when the discount factor \u03b3 is close to 1 or the approximation error is close to 0; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).", "creator": "LaTeX with hyperref package"}}}