{"id": "1405.3515", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Temporal Analysis of Language through Neural Language Models", "abstract": "suppose we independently provide a method for automatically detecting change in primary language across time through a successful chronologically trained statistical neural language model. structurally we train the model on the powerful google safari books ngram corpus to actively obtain word _ vector specific representations properties specific to congress each naming year, and identify words represented that have changed shape significantly slightly from > 1900 to 2009. the model readily identifies words such as \" the cell \" and \" gay \" as having changed during that specific time period. structurally the model simultaneously identifies the specific years between during which consistently such words likewise underwent similar change.", "histories": [["v1", "Wed, 14 May 2014 14:47:22 GMT  (246kb,D)", "http://arxiv.org/abs/1405.3515v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yoon kim", "yi-i chiu", "kentaro hanaki", "darshan hegde", "slav petrov"], "accepted": false, "id": "1405.3515"}, "pdf": {"name": "1405.3515.pdf", "metadata": {"source": "CRF", "title": "Temporal Analysis of Language through Neural Language Models", "authors": ["Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov"], "emails": ["dh1806}@nyu.edu", "slav@google.com"], "sections": [{"heading": "1 Introduction", "text": "Language changes across time. Existing words adopt additional senses (gay), new words are created (internet), and some words \u2018die out\u2019 (many irregular verbs, such as burnt, are being replaced by their regularized counterparts (Lieberman et al., 2007)). Traditionally, scarcity of digitized historical corpora has prevented applications of contemporary machine learning algorithms\u2014which typically require large amounts of data\u2014in such temporal analyses. Publication of the Google Books Ngram corpus in 2009, however, has contributed to an increased interest in culturomics, wherein researchers analyze changes in human culture through digitized texts (Michel et al., 2011).\nDeveloping computational methods for detecting and quantifying change in language is of interest to theoretical linguists as well as NLP researchers working with diachronic corpora. Methods employed in previous work have been varied, from analyses of word frequencies to more involved techniques (Guolordava et al. (2011); Mihalcea and Nataste (2012)). In our framework, we train a Neural Language Model (NLM) on yearly corpora to obtain word vectors for each year\nfrom 1900 to 2009. We chronologically train the model by initializing word vectors for subsequent years with the word vectors obtained from previous years.\nWe compare the cosine similarity of the word vectors for same words in different years to identify words that have moved significantly in the vector space during that time period. Our model identifies words such as cell and gay as having changed between 1900\u20132009. The model additionally identifies words whose change is more subtle. We also analyze the yearly movement of words across the vector space to identify the specific periods during which they changed. The trained word vectors are publicly available.1"}, {"heading": "2 Related Work", "text": "Previously, researchers have computationally investigated diachronic language change in various ways. Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and Strapparava (2013) employ statistical tests on frequencies of political, social, and emotional words to identify and characterize epochs.\nOur work contributes to the domain in sev-\n1http://www.yoon.io\nar X\niv :1\n40 5.\n35 15\nv1 [\ncs .C\nL ]\n1 4\nM ay\n2 01\n4\neral ways. Whereas previous work has generally involved researchers manually identifying words that have changed (with the exception of Gulordava and Baroni (2011)), we are able to automatically identify them. We are additionally able to capture a word\u2019s yearly movement and identify periods of rapid change. In contrast to previous work, we simultaneously identify words that have changed and also the specific periods during which they changed."}, {"heading": "3 Neural Language Models", "text": "Similar to traditional language models, NLMs involve predicting a set of future word given some history of previous words. In NLMs however, words are projected from a sparse, 1-of-V encoding (where V is the size of the vocabulary) onto a lower dimensional vector space via a hidden layer. This allows for better representation of semantic properties of words compared to traditional language models (wherein words are represented as indices in a vocabulary set). Thus, words that are semantically close to one another would have word vectors that are likewise \u2018close\u2019 (as measured by a distance metric) in the vector space. In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought. For example, if xw is the word vector for word w, they note that xapple \u2212 xapples \u2248 xcar \u2212 xcars \u2248 xfamily \u2212 xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples).\nNLMs are but one of many methods to obtain word vectors\u2014other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora.\nIn a Skip-gram model, each word in the corpus is used to predict a window of surrounding words (Figure 1). To ensure that words closer to the current word are given more weight in training, dis-\ntant words are sampled less frequently.2 Training is done through stochastic gradient descent and backpropagation. The word representations are found in the hidden layer. Despite its simplicity\u2014 and thus, computational efficiency\u2014compared to other NLMs, Mikolov et al. (2013b) note that the Skip-gram is competitive with other vector space models in the Semantic-Syntactic Word Relationship test set when trained on the same data."}, {"heading": "3.1 Training", "text": "The Google Books Ngram corpus contains Ngrams from approximately 8 million books, or 6% of all books published (Lin et al., 2012). We sample 10 million 5-grams from the English fiction corpus for every year from 1850\u20132009. We lower-case all words after sampling and restrict the vocabulary to words that occurred at least 10 times in the 1850\u20132009 corpus.\nFor the model, we use a window size of 4 and dimensionality of 200 for the word vectors. Within each year, we iterate over epochs until convergence, where the measure of convergence is defined as the average angular change in word vectors between epochs. That is, if V (y) is the vocabulary set for year y, and xw(y, e) is the word vector for word w in year y and epoch number e, we continue iterating over epochs until,\n1 |V (y)| \u2211\nw\u2208V (y)\narccos xw(y, e) \u00b7 xw(y, e\u2212 1) \u2016xw(y, e)\u2016\u2016xw(y, e\u2212 1)\u2016\nis below some threshold. The learning rate is set to 0.01 at the start of each epoch and linearly decreased to 0.0001.\n2Specifically, given a maximum window size of W , a random integer R is picked from range [1, W ] for each training word. The current training word is used to predict R previous and R future words.\nOnce the word vectors for year y have converged, we initialize the word vectors for year y+1 with the previous year\u2019s word vectors and train on the y + 1 data until convergence. We repeat this process for 1850\u20132009. Using an open source implementation in the gensim package, training took approximately 4 days on a 2.9 GHz machine."}, {"heading": "4 Results and Discussion", "text": "For the analysis, we treat 1850\u20131899 as an initialization period and begin our study from 1900."}, {"heading": "4.1 Word Comparisons", "text": "By comparing the cosine similarity between same words across different time periods, we are able to detect words whose usage has changed. We are also able to identify words that did not change. Table 1 has a list of 10 most/least changed words between 1900 and 2009. We note that almost all of the least changed words are function words. For the changed words, many of the identified words agree with intuition (e.g. gay, cell, ass). Others are not so obvious (e.g. checked, headed, actually). To better understand how these words have changed, we look at the composition of their neighboring words for 1900 and 2009 (Table 2).\nAs a further check, we search Google Books for sentences that contain the above words. Below are some example sentences from 1900 and 2009 with the word checked:\n1900: \u201cHowever, he checked himself in time, saying \u2014\u201d\n1900: \u201cShe was about to say something further, but she\nchecked herself.\u201d\n2009: \u201cHe\u2019d checked his facts on a notepad from his back\npocket.\u201d\n2009: \u201cI checked out the house before I let them go inside.\u201d\nAt the risk of oversimplifying, the resulting sentences indicate that in the past, checked was more frequently used with the meaning \u201cto hold in restraint\u201d, whereas now, it is more frequently used with the meaning \u201cto verify by consulting an authority\u201d or \u201cto inspect so as to determine accuracy\u201d. Given that check is a highly polysemous word, this seems to be a case in which the popularity of a word\u2019s sense changed over time.\nConducting a similar exercise for actually, we obtain the following sentences:\n1900: \u201cBut if ever he actually came into property, she must\nrecognize the change in his position.\u201d\n1900: \u201cWhenever a young gentleman was not actually\nengaged with his knife and fork or spoon \u2014\u201d\n2009: \u201cI can\u2019t believe he actually did that!\u201d\n2009: \u201cOur date was actually one of the most fun and\ncreative ones I had in years.\u201d\nLike the above, this seems to be a case in which the popularity of a word\u2019s sense changed over time (from \u201cto refer to what is true or real\u201d to \u201cto express wonder or surprise\u201d)."}, {"heading": "4.2 Periods of Change", "text": "As we chronologically train the model year-byyear, we can plot the time series of a word\u2019s distance to its neighboring words (from different years) to detect periods of change. Figure 2 (above) has such a plot for the word cell compared to its early neighbors, closet and dungeon, and the more recent neighbors, phone and cordless. Figure 2 (below) has a similar plot for gay.\nSuch plots allow us to identify a word\u2019s period of change relative to its neighboring words,\nand thus provide context as to how it evolved. This may be of use to researchers interested in understanding (say) when gay started being used as a synonym for homosexual. We can also identify periods of change independent of neighboring words by analyzing the cosine similarity of a word against itself from a reference year (Figure 3). As some of the change is due to sampling and random drift, we additionally plot the average cosine similarity of all words against their reference points in Figure 3. This allows us to detect whether a word\u2019s change during a given period is greater (or less) than would be expected from chance. We note that for cell, the identified period of change (1985\u2013 2009) coincides with the introduction\u2014and subsequent adoption\u2014of the cell phone by the general public.3 Likewise, the period of change for gay agrees with the gay movement which began around the 1970s (Wijaya and Yeniterzi, 2011)."}, {"heading": "4.3 Limitations", "text": "In the present work, identification of a changed word is conditioned on its occurring often enough\n3http://library.thinkquest.org/04oct/02001/origin.htm\nin the study period. If a word\u2019s usage decreased dramatically (or stopped being used altogether), its word vector will have remained the same and hence it will not show up as having changed. One way to overcome this may be to combine the cosine distance and the frequency to define a new metric that measures how a word\u2019s usage has changed."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper we provided a method for analyzing change in the written language across time through word vectors obtained from a chronologically trained neural language model. Extending previous work, we are able to not only automatically identify words that have changed but also the periods during which they changed. While we have not extensively looked for connections between periods identified by the model and real historical events, they are nevertheless apparent.\nAn interesting direction of research could involve analysis and characterization of the different types of change. With a few exceptions, we have been deliberately general in our analysis by saying that a word\u2019s usage has changed. We have avoided inferring the type of change (e.g. semantic vs syntactic, broadening vs narrowing, pejoration vs amelioration). It may be the case that words that undergo (say) a broadening in senses exhibit regularities in how they move about the vector space, allowing researchers to characterize the type of change that occurred."}], "references": [{"title": "Neural Probabilitistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent."], "venue": "Journal of Machine Learning Research 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Latent Dirichlet Allocation", "author": ["D. Blei", "A. Ng", "M. Jordan", "J. Lafferty."], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuglu", "P. Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman."], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 2011", "shortCiteRegEx": "Deerwester et al\\.", "year": 2011}, {"title": "A Distributional Similarity Approach to the Detection of Semantic Change in the Google Books Ngram Corpus", "author": ["K. Gulordava", "M. Baroni."], "venue": "Proceedings of the GEMS 2011 Workshop.", "citeRegEx": "Gulordava and Baroni.,? 2011", "shortCiteRegEx": "Gulordava and Baroni.", "year": 2011}, {"title": "Quantifying the evolutionary dynamics of language", "author": ["E. Lieberman", "J.B. Michel", "J. Jackson", "T. Tang", "M.A. Nowak."], "venue": "Nature, 449: 716\u2013716, October.", "citeRegEx": "Lieberman et al\\.,? 2007", "shortCiteRegEx": "Lieberman et al\\.", "year": 2007}, {"title": "Syntactic Annotations for the Google Books Ngram Corpus", "author": ["Y. Lin", "J.B. Michel", "E.L. Aiden", "J. Orwant", "W. Brockman", "S. Petrov."], "venue": "Proceedings of the Association for Computational Linguistics 2012.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Quantitative Analysis of Culture Using Millions of Digitized Books", "author": ["J.B Michel", "Y.K. Shen", "A.P. Aiden", "A. Veres", "M.K. Gray", "J.P. Pickett", "D. Hoiberg", "D. Clancy", "P. Norvig", "J. Orwant", "S.Pinker", "M.A. Nowak", "E.L. Aiden."], "venue": "Science, 331(6014): 176\u2013", "citeRegEx": "Michel et al\\.,? 2011", "shortCiteRegEx": "Michel et al\\.", "year": 2011}, {"title": "Word Epoch Disambiguation: Finding How Words Change Over Time", "author": ["R. Mihalcea", "V. Nastase."], "venue": "Proceedings of the Association for Computational Linguistics 2012.", "citeRegEx": "Mihalcea and Nastase.,? 2012", "shortCiteRegEx": "Mihalcea and Nastase.", "year": 2012}, {"title": "Recurrent Neural Network Based Language Model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur."], "venue": "Proceedings of Interspeech.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.T Yih", "G. Zweig."], "venue": "Proceedings of NAACL-HLT 2013, 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space arXiv Preprint", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J.Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Behind the Times: Detecting Epoch Changes using Large Corpora", "author": ["O. Popescu", "C. Strapparava."], "venue": "International Joint Conference on Natural Language Processing, 347\u2013355", "citeRegEx": "Popescu and Strapparava.,? 2013", "shortCiteRegEx": "Popescu and Strapparava.", "year": 2013}, {"title": "Semantic Density Analysis: Comparing Word Meaning across Time and Phonetic Space", "author": ["E. Sagi", "S. Kaufmann", "B. Clark"], "venue": "Proceedings of the EACL 2009 Workshop on GEMS: 104\u2013111. D.T. Wijaya, R. Yeniterzi. 2011. Understanding se-", "citeRegEx": "Sagi et al\\.,? 2009", "shortCiteRegEx": "Sagi et al\\.", "year": 2009}, {"title": "Learning Discriminative Projections for Text Similarity Measures", "author": ["W. Yih", "K. Toutanova", "J. Platt", "C. Meek."], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, 247\u2013256.", "citeRegEx": "Yih et al\\.,? 2011", "shortCiteRegEx": "Yih et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "Existing words adopt additional senses (gay), new words are created (internet), and some words \u2018die out\u2019 (many irregular verbs, such as burnt, are being replaced by their regularized counterparts (Lieberman et al., 2007)).", "startOffset": 196, "endOffset": 220}, {"referenceID": 7, "context": "Publication of the Google Books Ngram corpus in 2009, however, has contributed to an increased interest in culturomics, wherein researchers analyze changes in human culture through digitized texts (Michel et al., 2011).", "startOffset": 197, "endOffset": 218}, {"referenceID": 7, "context": "Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context.", "startOffset": 0, "endOffset": 28}, {"referenceID": 7, "context": "Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English.", "startOffset": 0, "endOffset": 166}, {"referenceID": 7, "context": "Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another.", "startOffset": 0, "endOffset": 314}, {"referenceID": 4, "context": "Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change.", "startOffset": 0, "endOffset": 28}, {"referenceID": 4, "context": "Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and Strapparava (2013) employ statistical tests on frequencies of political, social, and emotional words to identify and characterize epochs.", "startOffset": 0, "endOffset": 241}, {"referenceID": 4, "context": "Whereas previous work has generally involved researchers manually identifying words that have changed (with the exception of Gulordava and Baroni (2011)), we are able to automatically identify them.", "startOffset": 125, "endOffset": 153}, {"referenceID": 9, "context": "In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought.", "startOffset": 9, "endOffset": 32}, {"referenceID": 9, "context": "In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought. For example, if xw is the word vector for word w, they note that xapple \u2212 xapples \u2248 xcar \u2212 xcars \u2248 xfamily \u2212 xfamilies. That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples).", "startOffset": 9, "endOffset": 390}, {"referenceID": 1, "context": ", 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof.", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al.", "startOffset": 82, "endOffset": 126}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al.", "startOffset": 82, "endOffset": 151}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)).", "startOffset": 82, "endOffset": 170}, {"referenceID": 0, "context": "And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora.", "startOffset": 82, "endOffset": 236}, {"referenceID": 9, "context": "Despite its simplicity\u2014 and thus, computational efficiency\u2014compared to other NLMs, Mikolov et al. (2013b) note that the Skip-gram is competitive with other vector space models in the Semantic-Syntactic Word Relationship test set when trained on the same data.", "startOffset": 83, "endOffset": 106}, {"referenceID": 6, "context": "The Google Books Ngram corpus contains Ngrams from approximately 8 million books, or 6% of all books published (Lin et al., 2012).", "startOffset": 111, "endOffset": 129}], "year": 2014, "abstractText": "We provide a method for automatically detecting change in language across time through a chronologically trained neural language model. We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009. The model identifies words such as cell and gay as having changed during that time period. The model simultaneously identifies the specific years during which such words underwent change.", "creator": "TeX"}}}