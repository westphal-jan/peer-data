{"id": "1405.5459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2014", "title": "Projective simulation applied to the grid-world and the mountain-car problem", "abstract": "\u00bb we study above the model version of projective simulation ( ps ) which is a novel sophisticated approach to defining artificial object intelligence ( intelligent ai ). recently made it was definitely shown therefore that the new ps sand agent performs nicely well in defeating a number of simple task environments, well also when compared somewhat to recent standard models of reinforcement learning ( rl ). therefore in this paper we study the competitive performance of constructing the ps sa agent therefore further strengthened in more complicated strategic scenarios. to that end we chose by two other well - studied task benchmarking intelligence problems, namely the \" grid - like world \" challenge and namely the \" intelligent mountain - car \" problem, methods which challenge against the above model with large and continuous input space. we consistently compare the realistic performance of the ps agent model with those of existing actor models and systematically show that only the ps agent exhibits competitive cognition performance than also in such scenarios.", "histories": [["v1", "Wed, 21 May 2014 15:51:18 GMT  (1063kb,D)", "http://arxiv.org/abs/1405.5459v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alexey a melnikov", "adi makmal", "hans j briegel"], "accepted": false, "id": "1405.5459"}, "pdf": {"name": "1405.5459.pdf", "metadata": {"source": "CRF", "title": "Projective simulation applied to the grid-world and the mountain-car problem", "authors": ["Alexey A. Melnikov", "Adi Makmal", "Hans J. Briegel"], "emails": [], "sections": [{"heading": null, "text": "Projective simulation applied to the grid-world and the mountain-car problem\nAlexey A. Melnikov, Adi Makmal, and Hans J. Briegel Institut fu\u0308r Theoretische Physik, Universita\u0308t Innsbruck, Technikerstra\u00dfe 25, A-6020 Innsbruck and\nInstitut fu\u0308r Quantenoptik und Quanteninformation der O\u0308sterreichischen Akademie der Wissenschaften, Innsbruck, Austria\nWe study the model of projective simulation (PS) which is a novel approach to artificial intelligence (AI). Recently it was shown that the PS agent performs well in a number of simple task environments, also when compared to standard models of reinforcement learning (RL). In this paper we study the performance of the PS agent further in more complicated scenarios. To that end we chose two well-studied benchmarking problems, namely the \u201cgrid-world\u201d and the \u201cmountain-car\u201d problem, which challenge the model with large and continuous input space. We compare the performance of the PS agent model with those of existing models and show that the PS agent exhibits competitive performance also in such scenarios.\nI. INTRODUCTION\nProjective simulation (PS) provides a new approach for realizing an artificial intelligent (AI) agent, based on a stochastic processing of information [1]. The model, which can be naturally applied to reinforcement learning (RL) tasks [2\u20134], was tested in a recent paper [5] on a number of discrete toy-problems and was shown to perform well, also in comparison with the standard models of Q-learning [2] and extended learning classifier systems [6].\nIn this paper, we study the performance of the PS agent in more complicated scenarios. One particular type of real-world tasks is navigation, in which an agent has to find an optimal path to a target. Here we chose two canonical, well studied navigation tasks, namely the \u201cgrid-world\u201d [7] and the \u201cmountain-car\u201d [8] problem. The grid-world task is commonly used to examine the performance of AI approaches in handling large input space and delayed reward [7, 9, 10]. The mountain-car task presents an additional challenge, by imposing a continuous input space [11\u201318].\nThe paper has the following structure: Section II shortly describes the basic features of the PS model. Then, in Sections III and IV we examine the performance of PS in the grid-world and mountain-car tasks, respectively. Last, Section V summarizes the obtained results and concludes the paper."}, {"heading": "II. THE PS MODEL - BRIEF SUMMARY", "text": "For the benefit of the reader we first give a short description of the PS, for a more detailed description see [1, 5]. The PS is an AI model in which the information received by the agent is processed in a so-called episodic & compositional memory (ECM). The ECM is described by a weighted network of \u201cclips\u201d which are the units of the episodic memory: inputs lead to the excitation of corresponding percept clips, whereas the excitation of action clips triggers real action as output, as indicated in Fig. 1. Once a percept-clip is excited, the excitation hops between clips probabilistically until it reaches an action-clip. In other words, in the PS agent, perceptual input and action output are connected through a random walk in the agent\u2019s memory.\nFor illustration, let us consider a hypothetical PS network as shown in Fig. 1. Each edge connects some clip ci with a clip cj and has a time-dependent weight h\n(t)(ci, cj) which we denote as h-value. The h-values represent the unnormalized strength of the edges, and determine the hopping probabilities from clip ci to clip cj according to\np(t)(cj |ci) = h(t)(ci, cj)\u2211 k h (t)(ci, ck) . (1)\nIn this paper, for the sake of comparison with existing models, we also consider an alternative expression for the hopping probability, known as the \u201csoftmax\u201d (or Boltzmann) distribution function\np(t)(cj |ci) = e\u03b1 \u2032h(t)(ci,cj)\u2211 k e \u03b1\u2032h(t)(ci,ck) , (2)\nwhere we always set \u03b1\u2032 = 1, unless stated otherwise. Note that using the softmax expression merely rescales the hopping probabilities, such that a small difference in the h-values leads to a larger difference in the hopping probability.\nInitially, the h-values of all edges are set to h(0) = 1, implying that no particular path in the clip network is preferred over any other, and hence that no action is more probable than the others. Then, as experience is built\nar X\niv :1\n40 5.\n54 59\nv1 [\ncs .A\nI] 2\n1 M\nay 2\n01 4\n2\nup, the clip-network is dynamically changed according to rewards perceived from the environment. Formally, at each time step, the h-values are updated as follows\nh(t+1)(ci, cj) = h (t)(ci, cj)\u2212 \u03b3(h(t)(ci, cj)\u2212 1) + \u03bb, (3)\nwhere 0 \u2264 \u03b3 \u2264 1 is a damping parameter and \u03bb is a non-negative reward given by the environment. Note that at each time step the weights of all edges are damped, but only the weights of those edges that were traversed in the very last random walk are increased by the \u03bb reward. This update rule allows the agent to learn through experience, in such a way that the probability to take rewarded actions is increased with time.\nA useful generalization of the update rule of Eq. (3), denoted \u201cedge glow\u201d, was added to the model in [5]. The \u201cedge glow\u201d mechanism allows the agent to internally reward not only those edges (transitions) that were excited (taken) during the very last random walk, but also edges that were excited in previous time steps. This is realized by assigning to each edge of the PS network, apart from its weight, an additional time-dependent value 0 \u2264 g \u2264 1, denoted as \u201cglow value\u201d and by using the modified update rule:\nh(t+1)(ci, cj) = h (t)(ci, cj)\u2212 \u03b3(h(t)(ci, cj)\u2212 1) + g(t)(ci, cj)\u03bb. (4)\nEach time an edge is visited, the corresponding g-value is set to 1, following which it is decreased after each time step with a rate \u03b7:\ng(t+1)(ci, cj) = g (t)(ci, cj)(1\u2212 \u03b7). (5)\nThe decay of the g values ensures that the external reward has a different effect on edges that were excited at different time steps. In particular, edges that were excited in recent time steps are strengthen more than edges that were excited before, whose glow values g(t) have already decayed. Effectively, this means that percept-action pairs, that were experienced close to getting a reward, are more probable to reoccur than percept-action pairs that were encountered much before. The number of time steps that may pass between a random walk in the clip network and getting a reward such that the edges that were excited through that random walk are still strengthen by this reward, i.e. the number of time steps that are effectively \u201cremembered\u201d, is limited. It is, however, controlled by the \u03b7 parameter through an inverse relation: the higher the value of \u03b7, the smaller the number of remembered steps. In particular, the agent remembers only the last few decisions (or, to be more precise, only the edges of last few random walks are strengthen) when \u03b7 \u2192 1, and remembers almost every time step (that is, almost all previously excited edges are rewarded) when \u03b7 \u2192 0. By setting \u03b7 = 1, only the last random walk path is rewarded and we revert back to the update rule of Eq. (5).\nThe generalized update rule of Eqs. (4)-(5) enables the agent to correlate present rewards with previous actions and thereby to handle better with \u201ctemporal correlation\u201d scenarios, in which there exist a correlation between rewards and former actions. For example, this update rule was shown in [5] to be beneficial in scenarios in which the agent has to learn to refrain itself from actions that are instantly rewarded, at the benefit of obtaining a much larger reward for an action performed a few steps later, i.e. in scenarios where a greedy strategy is inferior. In this paper we further study the role of this update rule in scenarios in which a small reward is very much delayed.\n3"}, {"heading": "III. GRID WORLD", "text": "The grid-world environment [7, 9] is a maze in which an agent should learn an optimal path to a fixed goal. The world is divided into discrete cells (or rooms) in which the agent can reside. At each time step the agent can move to one of the neighboring cells by choosing among four actions: left, right, up or down. Here we consider the maze from [7] as shown in Fig. 2, which consists of 6 by 9 cells, some of which are walls (marked as black cells), and a goal (marked by a star) which is always located at the top right cell. At the beginning of each trial the agent is placed in the first cell of the third row from the top. If the agent decides to go to a square labeled as \u201cwall\u201d or to go beyond the grid, then no movement is performed but the time step is counted. The agent receives a reward of \u03bb = 1 only after reaching the goal, which also marks the end of the trial. A performance of an agent in this task is evaluated by the number of steps it makes before reaching the goal at each trial. A learning agent will need less and less steps as it goes through more and more trials. The learning time can be defined by the number of trials required to get a certain level of performance. A more efficient agent will require a smaller number of trials to attain a substantial improvement of its performance.\nThe main challenges posed by the grid-world task are its relative large input space (46 possible positions in our case) and the fact that the reward is much delayed. In fact, at the first trial, and for many time steps, the agent has no preference toward any direction until the goal is found by sheer coincidence. Only after the agent is rewarded for the first time, it can start developing a preference toward reaching the goal.\nIn the following we examine the performance of the PS agent in the grid-world task. To that end we use a twolayered clip network structure, as shown in Fig. 3, composed of 46 percept-clips (first row in Fig. 3) representing potential positions on the maze, 4 action-clips (second row in Fig. 3) and directed edges connecting percepts (s) to actions (a). Each edge (s,a) between percept and action is assigned a time dependent h-value h(t)(s, a) and a glowing value g(t)(s, a), as explained in Sec. II. Those values are then updated through experience, according to generalized update rules of Eqs. (4)-(5). To obtain statistically meaningful results we average the PS performance over 104 agents. (see [5] for an error bars analysis of the PS agent\u2019s learning curves, albeit in a different scenario).\nAs shown in previous works [1, 5] the PS performance depends on the value of its internal \u03b3 and \u03b7 damping parameters. In particular, it was shown that a nonzero damping parameter \u03b3, i.e. an ongoing process of forgetting, is beneficial when the environment changes, whereas for constant environments it merely limits the maximum achievable success probability of the agent. Since in the grid-world task the environment is constant we set \u03b3 = 0 to avoid forgetting and to observe the model\u2019s best performance. The dependence of the PS performance on the value of the glow-damping parameter \u03b7 is, however, more involved. Fig. 4(a) shows the PS performance, characterized by the number of steps required to find the goal after 100 trials, as a function of the \u03b7 parameter. We consider both the basic and the softmax probability functions p(t)(cj |ci) as given in Eqs. (1) and (2), shown in solid red and dashed blue lines, respectively. One can see that in both cases the PS agent performs quite badly with \u03b7 \u2192 0: even after 100 trials it requires more than 100 steps to reach the goal (in fact for \u03b7 = 0 the agents require 842 and 570 steps, after 100 trials, using the basic and the softmax functions, respectively, not shown). This is because a small \u03b7 parameter inhibits the decay of the edge glow, so that all previous actions are always rewarded with the same value \u03bb = 1. Since the PS agent starts the first trial with no preferred actions, the first path to the goal consists of completely random moves and is thus very long on average. The probability of taking again the same long path of random moves increases and makes it almost impossible to learn something. Setting \u03b7 = 1 may even be worse. The reason is that with \u03b7 = 1 all g values are damped to 0 and the \u201cedge glow\u201d mechanism is effectively turned off, such that only the last action in the trial can be learned. Setting an intermediate value can help as it allows to reward moves which are near the goal higher than those which are far from it. In other words, the last few actions before reaching the goal are highly\n4\nrewarded, whereas unwanted random moves at the beginning of the trial are less rewarded, if at all. From Fig. 4(a) one can see that there exists an optimal \u03b7 \u2248 0.07 for the PS agent with the basic transition probabilities of Eq. (1) (solid red curve). This is in agreement with the findings of [5] in which an optimum \u03b7 value was also shown to exist for a different \u201ctemporal correlation\u201d scenario. Using the softmax function to define the transition probabilities (dashed blue curve in Fig. 4(a)) leads to an improvement in the agents performance. This improvement makes sense because with the softmax function even a small reward is enough to establish a high probability to repeat the same action. Moreover, it is seen that when the softmax probability function is used, the resulting performance (i.e. after 100 trials) is more robust against changes of \u03b7. In particular, there is a larger region of \u03b7 values for which the performance is nearly the best.\nFig. 4(b) shows the average performance of the PS agent as a function of the number of trials, using the optimal values of \u03b3 = 0, \u03b7 = 0.07 (for the basic transition function, shown in solid red curve) and \u03b7 = 0.12 (for the softmax function, shown in dashed blue curve) as explained above. It is seen that as the number of trials increases, the PS agents find the goal in fewer and fewer steps on average, implying that the PS model is capable of learning in the grid-world environment. In particular, after 100 trials it is seen that on average the PS agents reach the goal in about 45 steps using the basic transition function and in about 15.4 steps when using the softmax probability function. It is also seen that the initial learning rate, as captured by the initial slope of the learning curves is greater when using the softmax function.\nThe performance of each individual agent may differ from the average performance. In order to check how much a\n5 single agent\u2019s behavior may deviate from the one of its fellow agents, i.e. differ from the average, we further computed the standard deviation (\u03c3) of the performance for each trial. Fig. 5 shows the averaged performance of the PS agents in the grid-world (solid curves), plotted in the center of a 2\u03c3 envelope (dotted curves). It is seen that at the beginning the agent\u2019s performance varies to a large extent, and that as the number of trials increases, the agent\u2019s performance converges into a small region. In other words, the standard deviation decreases with experience and the agents are expected to perform more and more alike and as suggested by the average of their performance.\nSo far, we have mostly concentrated on the performance of the agent in terms of the length of the chosen path. We saw that after 100 trials, different \u03b7 values result with different performances, and that there exist an optimal \u03b7 value for which this performance is the best. Next we also consider the effect of changing the value of the \u03b7 parameter on the initial speed of the learning. Fig. 6(a) shows three learning curves corresponding to three different values of \u03b7 = 0.03, 0.12 and 0.15, in solid red, dashed blue and dash-dotted black, respectively. It is seen that as the value of \u03b7 increases, the initial slope of the learning curve decreases and a better performance is reached. This illustrates that the choice of an optimal \u03b7 depends on the number of trials the agent is given. In particular, it is seen that after 100 trials the dashed blue curve of \u03b7 = 0.12 exhibits the best performance, but that with \u03b7 = 0.15 (dash-dotted black curve) one can reach better performance after 150 trials. Moreover Fig. 6(a) illustrates that for a finite number of trials there is a trade-off between learning speed and performance. In particular the solid red curve of \u03b7 = 0.03 shows that by slightly compromising on performance (with a path of less than 17 steps) a much faster learning is obtained. In general, by increasing the \u03b7 value further one can achieve better performance, but more learning trials are needed. The choice of \u03b7 can then be made according to the required property: performance versus speed. For completeness, we remark that by increasing the value of \u03b1\u2032, i.e. the exponent power of the softmax function of Eq. (2) one can improve on both the initial learning slope and the performance (not shown).\nNext we ask, is there a limit for the initial learning speed or is there an optimum \u03b7 value for which the initial slope is the largest? Fig. 6(b) shows that among \u03b7 = 0.001, 0.01 and 0.03 in dash-dotted black, dashed blue and solid red curves, respectively, the largest initial slope is obtained by the intermediate value of \u03b7 = 0.01. This shows that an optimal \u03b7 value does exist, also with respect to the achievable initial slope.\nWe now turn to evaluate the quality of the PS performance. To that end we compare the PS performance to the performance of the policy iteration (PI) model as reported in [7] where the exact same rules were employed. In particular, we compare the PS learning curve depicted in Fig. 4(b) to the learning curve of the PI agent as shown in Fig. 3 of Ref. [7]. The PI is a trial-and-error learning system. The policy, which is adjusted by a temporal-difference learning process, dictates which action is performed at any input state. It is realized as a table of values for each pair of input and action, and the PI agent takes an action stochastically using values from this table according to the softmax function. To allow a meaningful comparison we compare the performance of the basic PS model with those of the basic PI model, i.e. without any Dyna-style planning steps (or, equivalently, with no hypothetical experience, denoted as k = 0 in Ref. [7]). Moreover, since the PI employs the softmax policy, we compare it with the softmax curve of the PS shown in dashed blue in Fig. 4(b). It is seen in Fig. 3 of Ref. [7] that after about 80 trials the PI method (with no Dyna-style planning) reaches the goal in about 14 steps, i.e. roughly within the optimal number of steps. In comparison, the PS agents with the softmax function require on average 15.4 steps. These results are also summarized in Table I."}, {"heading": "IV. MOUNTAIN CAR", "text": "In the mountain-car task, defined in [11, 12], an agent drives a car on a surface between two hills, where a goal awaits at the top of the right hill, as shown in Fig. 7. At the beginning of each trial the agent has a random position x0 and a random velocity v0. Then, at each of the following time steps the agent receives its new position xnew and new velocity vnew as input and has to choose between three possible actions: forward thrust (to the right), no thrust, and reverse thrust (to the left). Once the agent finds the goal it is rewarded with \u03bb = 1 and the trial ends. Until then, like in the grid-world, the agent receives no rewards. To measure the performance of the agent we count the number of steps it requires to find the goal at each trial. Clearly, a well performed agent would require less steps as the number of trials increases.\nHere we follow the mountain-car rules as specified in [11]. In particular, the next (xnew, vnew) coordinates are determined by the agent\u2019s own action and by the effect of gravity, according to\nvnew = vold + 0.001 \u2217Action\u2212 0.0025 cos(3xold), Action \u2208 {\u22121, 0, 1} xnew = xold + vold.\n(6)\nThe position of the car is bounded inside \u22121.2 \u2264 x \u2264 0.5 and the goal is always placed at x = 0.5. Trying to go beyond these bounds leaves the car on the boundary with zero velocity as if the car hit a wall (for the positive boundary this would simply result with reaching the goal). The velocity is similarly bounded inside \u22120.07 \u2264 v \u2264 0.07, i.e. its absolute value is reset to 0.07 if this value is exceeded [11].\nThe mountain-car task is quite challenging: first, like in the grid-world, the reward is delayed and the agent has initially no information about the goal or its mere existence. It therefore has to move around randomly until it accidentally hits the goal and is finally rewarded; Second, unlike the grid-world scenario, an optimal path is not apparent. This is because in general it will not by sufficient to push a car directly to the goal, since its engine power is not strong enough and it will eventually roll back down. The agent would therefore need to drive the car back and forth to obtain sufficient potential energy. Appendix A provides a detailed analytical treatment of the physics that lays behind the game, and calculates an upper bound (though not necessarily tight) for the minimum number of required steps; Last but not least, the mountain-car task introduces a two-dimensional continuous input space, thereby confronting the agent with infinite number of possible input states.\nTo examine the PS model in the mountain-car problem, we simulate its performance with the two-layered clip network depicted in Fig. 8, where the continuous input space is discretized uniformly. Specifically, we chose a grid of\n7\n20 by 20 for a later comparison with existing results from the literature (see below). As before, we study the model with both choices for the transition probability given in Eqs. (1) and (2), and use the generalized update rules of Eq. (4)-(5). To get an optimal performance we set the damping parameter to \u03b3 = 0, as the environment does not change. To find an optimal \u03b7 parameter we look at the PS performance after 20 trials as a function of \u03b7, as shown in Fig. 9(a). Both transition function of Eqs. (1) and (2) are shown in solid red and dashed blue curves, respectively. It is seen that in both cases, an optimal \u03b7 value exists at 0.02. In general, it is seen that the dependence on \u03b7 resembles the one observed in the grid-world task (see Fig. 4(a)). In particular, the PS performance is quite bad for \u03b7 \u2192 0 and \u03b7 \u2192 1 for the same reasons described in Sec. III. In addition, we see here too that the softmax function of Eq. (2) improves the obtained performances and results with a performance curve that is less sensitive to changes in the \u03b7 parameter.\nFig. 9(b) shows the learning curve of the PS agent using the optimal value \u03b7 = 0.02, as found above. The average number of steps required to reach the goal is shown for each trial for both the basic and the softmax function in solid red and dashed blue curves, respectively. It is seen that the PS agents manage to find the goal with less steps when the number of trials increases, as required. As in the grid-world task, the softmax function for the probability function improves not only the final performance but also the rate of the learning. For later comparison we indicate that with the softmax function the agents make about 223 steps per trial, averaged over the first 20 trials.\nFor comparison, we next look at the performance of the SARSA algorithm [19] in the mountain-car problem, as reported in [11]. For completeness we shortly note that the SARSA algorithm estimates an \u201caction-value\u201d function which gives an expected future reward for any percept-action pair. At each time step the action that maximizes this future reward is deterministically chosen. In Ref. [11] the infinite input space of the mountain-car problem was represented by five 9 by 9 grids, each of which is offset by a random fraction of a one cell\u2019s width. Each of the five grids was associated with its own action-value functions and an action was chosen according to the largest sum over the corresponding values from all grids. Here a reward of \u22121 was given at all time steps, except when reaching the\ngoal, or as stated by the authors \u201cpassing the top ends the trial and ends this punishment\u201d. Compared with our rewarding scheme there is thus a constant shift of rewards by \u22121. The reported performance of the SARSA algorithm is 450 steps per trial, averaged over the first 20 trials. According to the above results of the PS, we note that the PS is twice as fast (with 223 steps per trial).\nComparing the PS agent with a more recent implementation of the SARSA algorithm for the mountain-car problem [18] we next consider the case in which the agent has a fixed initial position and velocity (as opposed to random ones). Following Ref. [18], we set the agent to x = \u22120.5 and v = 0 at the beginning of each trial, i.e. almost at the bottom of the hill with zero velocity, and checked its performance after 100 trials. We discretized the input space to a uniform 20 by 20 grid, as before, and chose an optimal glow parameter \u03b7 = 0.01 according to the agent\u2019s best performance after 100 trials (see Fig. 10(a)) using both the basic probability transition functions (solid red curve) and the softmax function (dashed blue curve). The corresponding learning curves are displayed in Fig. 10(b), where it is shown that the PS agent manages to learn and to reach the goal with a decreasing number of steps in this scenario too. This fixed initial starting point turns out, however, to be rather difficult for the agent as it should first drive away from the goal (see Appendix A). Empirically, we see that even after 100 trials the PS agent (with the softmax distribution) requires as many as 302 steps to find the goal, more steps than it needs in the case of random initial coordinates,\n9 after 20 trials alone. In what follows we compare the performances of the PS agent with those of the SARSA algorithm as presented in Fig. 3 of Ref. [18]. In this reference, a reward of \u22121 was given at each time step until the goal was found, an action was chosen according to an -greedy policy with = 0.1, and the value function was represented with 10 grids, each of 104 input states, using a linear function approximation. It is seen in Fig. 3 of Ref. [18] that after 100 trials the SARSA algorithm is able to find the goal in about 150 steps, i.e. twice as fast as the PS agent with the softmax transition function. We relate the relative success of SARSA in this case to the combined usage of a dense grid discretization (of 104 states), a large number of grids (10), and a function approximation for the value function [18]. As described above, the PS implementation for the mountain-car task is more economic: each agent is supplied with only a single network of 400 percept clips, where no kind of function approximation is used (implying that each percept-action edge has to be learned independently). This can be improved. For example, with 900 percepts the PS performance (using the softmax function) already improves by \u2248 10% and the agents find the goal after 276 steps on average.\nTable II summarizes the performances of the PS model in the mountain-car task, in both cases of random and fixed initial position. The performances of the SARSA algorithm in the corresponding scenarios are also shown, for comparison."}, {"heading": "V. CONCLUSION", "text": "We studied the performance of the projective simulation (PS) agent in the navigation tasks of grid-world and mountain-car, in which an agent is supposed to learn how to find a goal in minimal number of steps. In the grid-world task the agent has to deal with a delayed reward that is given only at the end of the trial and with a large input space. In particular, there exists many (infinite) different paths to the goal, but only a few of them are optimal. In the mountain-car task the input state space is even infinite, adding the challenge of how to learn with only finite number of possible input percepts.\nIn both tasks we saw that the PS agent manages to find the goal faster after each trial. The PS agent starts the first trial by randomly trying available actions until it accidentally reaches the goal. On average, during the first trial the PS finds the goal after 870 steps in the grid-world task, and after 735 steps (1450 steps when the initial coordinates are fixed) in the mountain-car task. With appropriately chosen damping parameters the PS greatly improves its performance: In the grid-world task the number of steps to reach the goal goes down to 15.4 after 100 trials; In the mountain-car task, the number of steps to reach the goal goes down to 129 steps after 20 trials using randomized initial coordinates, and to 302 steps after 100 trials using fixed initial coordinates. These results were obtained using the softmax transition probability function of Eq. (2) which, due to rescaling of the hopping probability, always improves the performance compared to the use of the basic transition probability function of Eq. (1). We further studied the performance of the PS as a function of the glow parameter \u03b7 and showed that the edge-glow mechanism of the PS plays an important role in scenarios where the reward is delayed.\nThe performance of the PS agent was compared with those of the policy iteration (PI) and the SARSA algorithms. Qualitatively, the performance of the PS model is comparable to those of the other models, and no major differences were observed. Quantitatively, we saw that in the mountain-car, when starting from a fixed coordinate, PS does not perform as good as SARSA. We showed, however, that to a certain extent one can improve the PS performance by increasing the input state space, i.e. the number of possible percept clips. We further showed that the PS agent performs almost as good as the PI agent in the grid-world task and that it outperforms the SARSA algorithm in the mountain-car task when the initial coordinates are chosen randomly. We thus conclude that the PS model performs\n10\nwell also in navigation scenarios with large and even continuous input space.1"}, {"heading": "Appendix A: Analyzing the physics background of the mountain-car problem", "text": "A car of mass m drives up the hill with a tangential velocity ~v, it has an engine acceleration ~a and experiences a gravitational acceleration ~g as illustrated in Fig. 11. Its equation of motion is therefore given by:\nd~v/dt = ~a+ ~g. (A1)\nBy projecting the vectors onto the direction of motion we get\ndv/dt = a\u2212 g cos\u03d5 dx/dt = vx = v,\n(A2)\nwhere \u03d5 is the angle between the vectors ~a and \u2212~g. Integration leads to\nvt = vt\u22121 + \u222b t t\u22121 a\u03c4d\u03c4 \u2212 g \u222b t t\u22121 cos\u03d5\u03c4d\u03c4\nxt = xt\u22121 + \u222b t t\u22121 v\u03c4d\u03c4.\n(A3)\nWe next assume that during the small time interval dt = 1 the integrands a\u03c4 , cos\u03d5\u03c4 and v\u03c4 do not change much, thus we obtain the following approximations for the (xt+1, vt+1) coordinates of the next time step:\nvt \u2248 vt\u22121 + at\u22121 \u2212 g cos(\u03d5t\u22121) xt \u2248 xt\u22121 + vt\u22121.\n(A4)\nComparing Eq. (A4) with Eq. (6) one obtains at\u22121 = 0.001 \u2217 Action and g cos\u03d5t\u22121 = 0.0025 cos(3xt\u22121). A change of the height is given by dh = dx cos\u03d5x, which integrates to\nhx = 0.0025\ng\n\u222b cos(3x)dx = 0.0025\n3g sin(3x) + C. (A5)\nThe height hx is plotted in Fig. 12 for g=0.0025 (using the same convention as in [11]) and C = 0 , where x is the coordinate perceived by an agent, e.g. marks on the road. Eq. (A5) indicates that the bottom of the hill is placed at x = \u2212\u03c0/6.\n1 Even though it is not the topic of this paper, it should be mentioned that the PS model has the additional benefit that it can be straightforwardly generalized to quantum mechanical operation [1, 20]. An agent based on quantum projective simulation (Q-PS) has recently been shown to provide a significant speed-up for active learning scenarios [20].\n11\nIf the agent starts near the bottom of the hill at x0 = \u22120.5 with a zero velocity v0 = 0 and pushes the car in the direction towards the top of the mountain, its engine power will not be sufficient. To see that we equate the net work done by an engine with the difference in total energy (potential plus kinetic)\nma(xgoal \u2212 x0) = mg(hgoal \u2212 h0) + mv2f\n2 (A6)\nfrom which we get\na(xgoal \u2212 x0) \u2265 g(hgoal \u2212 h0)\u21d2 xgoal \u2265 \u22120.5 + 5\n6\n( sin(3xgoal)\u2212 sin(\u22121.5) ) (A7)\nwhich does not hold for xgoal = 0.5. To get the maximum x value the agent can reach by always pushing right we use Eq. (A6) and set the final velocity vf = 0, which gives xmax \u2248 \u22120.27 (which is indeed much before the goal at x = 0.5). A similar analysis shows that by pushing the car always to the left the agent could reach x \u2248 \u22120.834. Fig. 12 marks in black circles the furthest points the agent can reach by pushing the car only in one direction (their hight differ because the initial position \u22120.5 is not exactly at the bottom of the hill). An additional green cross marks the point from which the agent could reach the goal by just pushing the car to the right. One possible strategy is to go from x = \u22120.5 to the left till the car stops at x \u2248 \u22120.834 and from that point on always push the car in the direction of the goal. With this strategy one can reach the goal in 89 steps: 36 actions to the left, followed by 63 actions to the right. Note that in this analysis we did not enforce explicitly the bounds on the velocity. This is, however, unnecessary as within the above particular strategy the absolute value of the velocity during the whole trial is always less than the maximum allowed value of 0.07.\n-1.0 -0.5 0.5 x\n-0.3\n-0.2\n-0.1\n0.1\n0.2\n0.3\nhx\n12\n[13] W. D. Smart and L. P. Kaelbling, in Proceedings of the International Conference on Machine Learning (Citeseer, 2000), pp. 903\u2013910. [14] C. E. Rasmussen, M. Kuss, et al., in Proceedings of the Conference on Neural Information Processing Systems (2003). [15] N. Jong and P. Stone, in Proceedings of the International Conference on Machine Learning (2006). [16] S. Whiteson and P. Stone, The Journal of Machine Learning Research 7, 877 (2006). [17] V. Heidrich-Meisner and C. Igel, in Recent Advances in Reinforcement Learning (Springer, 2008), pp. 136\u2013150. [18] R. S. Sutton, C. Szepesva\u0301ri, A. Geramifard, and M. P. Bowling, arXiv:1206.3285 (2012). [19] G. A. Rummery and M. Niranjan, On-line Q-learning using connectionist systems (University of Cambridge, 1994). [20] G. D. Paparo, V. Dunjko, A. Makmal, M. A. Martin-Delgado, and H. J. Briegel, submitted to Phys. Rev. X, arXiv:1401.4997\n(2014)."}], "references": [{"title": "Reinforcement learning: An introduction", "author": ["R.S.A.G. Sutton", "Barto"], "venue": "(MIT press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Advances in neural information processing systems", "author": ["R.S. Sutton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "(University of Cambridge,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "The model, which can be naturally applied to reinforcement learning (RL) tasks [2\u20134], was tested in a recent paper [5] on a number of discrete toy-problems and was shown to perform well, also in comparison with the standard models of Q-learning [2] and extended learning classifier systems [6].", "startOffset": 79, "endOffset": 84}, {"referenceID": 1, "context": "The mountain-car task presents an additional challenge, by imposing a continuous input space [11\u201318].", "startOffset": 93, "endOffset": 100}, {"referenceID": 1, "context": "In the mountain-car task, defined in [11, 12], an agent drives a car on a surface between two hills, where a goal awaits at the top of the right hill, as shown in Fig.", "startOffset": 37, "endOffset": 45}, {"referenceID": 2, "context": "For comparison, we next look at the performance of the SARSA algorithm [19] in the mountain-car problem, as reported in [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "[3] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[12] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[19] G.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "We study the model of projective simulation (PS) which is a novel approach to artificial intelligence (AI). Recently it was shown that the PS agent performs well in a number of simple task environments, also when compared to standard models of reinforcement learning (RL). In this paper we study the performance of the PS agent further in more complicated scenarios. To that end we chose two well-studied benchmarking problems, namely the \u201cgrid-world\u201d and the \u201cmountain-car\u201d problem, which challenge the model with large and continuous input space. We compare the performance of the PS agent model with those of existing models and show that the PS agent exhibits competitive performance also in such scenarios.", "creator": "LaTeX with hyperref package"}}}