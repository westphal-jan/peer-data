{"id": "1405.7397", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2014", "title": "An HMM Based Named Entity Recognition System for Indian Languages: The JU System at ICON 2013", "abstract": "this report paper reports about our work methods in composing the icon \u2192 2013 nlp compiler tools for contest on named entity software recognition. whereas we finally submitted runs for bengali, english, nearly hindi, plain marathi, with punjabi, tamil speaking and telugu. as a statistical hmm ( or hidden statistical markov equation models ) based reasoning model has been once used here to directly implement this our system. previously the system has been trained and tested continuously on the preliminary nlp technology tools contest : icon + 2013 datasets. collectively our system obtains binary f - irs measures of 0. 8599, 0. 760 7704, + 0. 501 7520, dt 0. 4289, \u20b9 0. 460 5455, 0. 4466, 405 and 0. 4003 scales for mixed bengali, english, hindi, mostly marathi, punjabi, tamil and telugu students respectively.", "histories": [["v1", "Wed, 28 May 2014 21:05:00 GMT  (190kb)", "http://arxiv.org/abs/1405.7397v1", "The ICON 2013 tools contest on Named Entity Recognition in Indian languages (IL) co-located with the 10th International Conference on Natural Language Processing(ICON), CDAC Noida, India,18-20 December, 2013"]], "COMMENTS": "The ICON 2013 tools contest on Named Entity Recognition in Indian languages (IL) co-located with the 10th International Conference on Natural Language Processing(ICON), CDAC Noida, India,18-20 December, 2013", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vivekananda gayen", "kamal sarkar"], "accepted": false, "id": "1405.7397"}, "pdf": {"name": "1405.7397.pdf", "metadata": {"source": "CRF", "title": "An HMM Based Named Entity Recognition System for Indian Languages: The JU System at ICON 2013", "authors": ["Vivekananda Gayen", "Kamal Sarkar"], "emails": ["vivek3gayen@gmail.com", "jukamal2001@yahoo.com"], "sections": [{"heading": "1 Introduction", "text": "Named entity recognition involves locating and classifying the names in text. The objective of named entity recognition is to identify and classify every word/term in a document into some predefined categories like person name, location name, organization name, miscellaneous name (date, time, percentage and monetary expressions etc.) and \u201cnone-of-the-above\u201d.\nNER is an important task, having applications in Information Extraction, Question Answering, Machine Translation, Summarization and other NLP applications. This paper presents a HMM (Hidden Markov Model) based NER system for Indian languages which is designed for the ICON 2013 NLP tool contest, the goal of which is to perform NE recognition on a variety of types: artifact, enter-\ntainment, facilities, location, locomotive, materials, organisms, organization, person, plants, count, distance, money, quantity, date, day, period, time and year.\nThe ICON 2013 NLP tool contest was defined to build the NER systems for seven Indian languages - Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu for which training data, development data and test data were provided. Since our HMM based system is of language independent nature. We have participated for all seven languages. We have not used gazetteers for these tasks because gazetteer lists for all seven languages were not available with us.\nThe earliest works on named entity recognition used a variety of techniques for named entity recognition (NER). The two major approaches to NER are: Rule based (Linguistic) approaches and Machine Learning (ML) based approaches.\nThe rule based approaches typically use rules manually written by human (Grishman, 1995; McDonald, 1996; Wakao et al., 1996).\nMachine learning (ML) based techniques for NER make use of a large amount of NE annotated training data to acquire high level language knowledge. Several ML techniques have been successfully used for the NER tasks such as Markov Model (HMM) (Bikel et al., 1997), Maximum Entropy (MaxEnt) (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Conditional Random Field (CRF) (Li and Mccallum, 2004) etc.\nCombinations of different ML approaches are also used. Srihari et al.(2000) combines MaxEnt,\nHidden Markov Model (HMM) and handcrafted rules to build an NER system.\nNER systems also use gazetteer lists for identifying names. Both the linguistic approach (Grishman, 1995; Wakao et al., 1996) and the ML based approach (Borthwick, 1999; Srihari et al., 2000) may use gazetteer lists.\nThe NER tasks for Hindi have been presented in (Cucerzan and Yarowsky, 1999; Li and Mccallum, 2004; Ekbal et.al., 2009).\nA discussion on the training data is given in Section 2. The HMM based NER system is described in Section 3. Various features used in NER are then discussed. Next we present the experimental results and related discussions in Section 4. Finally Section 5 concludes the paper."}, {"heading": "2 Training Data", "text": "The data used for the training of our systems was provided. The annotated data uses Shakti Standard Format (SSF). Our system converts the SSF format data into the IOB format before training and the data converted in IOB format is used for training. IOB format uses a B\u2212XXX tag that indicates the first word of an entity type XXX and I\u2212XXX that is used for subsequent words of an entity. The tag \u201cO\u201d indicates the word is outside of an NE (i.e., not a part of a named entity). An additional tag EXXX (this is not the part of IOB format) is also added to the tagset to tag the last word of an entity type XXX.\nAt the time of system development we have observed that the data (training, development and test), provided by the organizers of ICON 2013 NLP tool contest, contains several types of errors such as ambiguous POS tags (XC:?, QO:? Etc.), errors in NE tagging. These errors in the training corpora affects badly to the machine learning (ML) based models. But we have not made corrections of the errors in the training corpora at the time of our system development. All the results shown in the paper are obtained using the provided corpora without any modification in NE annotation."}, {"heading": "3 HMM based Named Entity Tagging", "text": "A named entity recognizer based on Hidden Markov Model (HMM) assigns the best sequence of NE tags 1 nt that is optimal for a given observation\nsequence 1 no . The tagging problem becomes equivalent to searching for\n1\n1 1 1arg max ( | ) ( ) n\nn n n\nt P o t P t (by the application of\nBayes\u2019 law), that is, we need to compute:\n1\n1 1 1 1 \u02c6 arg max ( | ) ( )\nn\nn n n n\nt t P o t P t= (1).\nWhere 1 nt is a tag sequence and 1 no is an observation sequence, 1( )\nnP t is the prior probability of the tag sequence and 1 1( | )\nn nP o t is the likelihood of the word sequence.\nIn general, HMM based sequence labeling tasks such as POS tagging use words in a sentence as an observation sequence (Sarkar and Gayen, 2012, 2013). But, the data released for ICON 2013 NLP tool contest is POS tagged and chunked, that is, some additional information such as POS tag and chunk tag for each word in a sentence is also available. To use this additional information for named entity recognition task, we consider a triplet:\n<word, POS-tag, chunk-tag > as an observed symbol, that is, for a sentence of n words, the corresponding observation sequence will be as follows:\n(<word1, POS-tag1, chunk-tag1>, <word2, POStag2, chunk-tag2>, <word3, POS-tag3, chunk-tag3>, .........., <wordn, POS-tagn, chunk-tagn>) . Here an observation symbol oi corresponds to <wordi, POStagi, chunk-tagi>.\nEquation (1) is too hard to compute directly. HMM taggers make Markov assumption which states that the probability of a tag is dependent only on a small, fixed number of previous tags. A bigram tagger considers that the probability of a tag depends only on the previous tag. For our proposed trigram model, the probability of a tag depends on two previous tags and 1( )\nnP t is computed as:\n1 1 21 ( ) ( | , )\nn n\ni i ii P t P t t t\u2212 \u2212=\u2248 \u03a0 (2)\nDepending on the assumption that the probability of a word appearing is dependent only on its own tag, 1 1( | ) n nP o t can be simplified to:\n1 1 1\n( | ) ( | ) n\nn n i i i P o t P o t = \u2248 \u220f (3)\nPlugging the above mentioned two equations (2) and (3) into (1) results in the following equation by which a bigram tagger estimates the most probable tag sequence:\n1 1\n1 1 1 1 1 1 \u02c6 argmax ( | ) ( ) argmax ( | ) ( | ) n n\nn n n n n\ni i i i it t t P t o Pt Po t Pt t\u2212 = = \u2248 \u220f (4)\nWhere: the tag transition probabilities, 1( | )i iP t t \u2212 , represent the probability of a tag given the previous tag. ( | )i iP o t represents the probability of an observed symbol given a tag. Considering a special tag tn+1 to indicate the end sentence boundary and two special tags t-1 and t0 at the starting boundary of the sentence and adding these three special tags to the tag set (Brants, 2000), gives the following equation for NE tagging:\n1\n1\n1 1 1 1\n1 2 1 1\n\u02c6 argmax ( | ) ( )\nargmax[ ( | ) ( | , )] ( | )\nn\nn\nn n n n\nt\nn\ni i i i i n n it\nt P t o P t\nP o t P t t t P t t\u2212 \u2212 + =\n= \u2248\n\u220f (5)\nThe equation (5) is still computationally expensive because we need to consider all possible tag sequence of length n. So, dynamic programming approach is used to compute the equation (5).\nAt the training phase of HMM based NE tagging, observation probability matrix and tag transition probability matrix are created."}, {"heading": "3.1 Computing Tag Transition Probabilities", "text": "As we can see from the equation (4) to find the most likely tag sequence for an observation sequence, we need to compute two kinds of probabilities: tag transition probabilities and word likelihoods or observation probabilities.\nOur developed trigram HMM tagger requires to compute tag trigram probability, 1 2( | , )i i iP t t t\u2212 \u2212 , which is computed by the maximum likelihood estimate from tag trigram counts. To overcome the data sparseness problem, tag trigram probability is smoothed based on the bigram and unigram probabilities using the following equation: 1 2 1 1 2 2 1 3\u02c6 \u02c6 \u02c6( | , ) ( | , ) ( | ) ( )i i i i i i i i iP t t t P t t t P t t P t\u03bb \u03bb \u03bb\u2212 \u2212 \u2212 \u2212 \u2212= + + (6)\n1 2 \u02c6 ( | , )i i iP t t t\u2212 \u2212 , 1\u02c6 ( | )i iP t t \u2212 and \u02c6( )iP t are the max-\nimum likelihood estimates from counts for tag trigram, tag bigram and tag unigram respectively:\n2 1\n2 1\n( , , ) 1 2 ( , ) \u02c6 ( | , ) i i i i i C t t t i i i C t tP t t t \u2212 \u2212\u2212 \u2212\u2212 \u2212 = ,\n1\n1\n( , ) 1 ( ) \u02c6 ( | ) i i i C t t i i C tP t t \u2212\u2212\u2212 = ,\n( )\u02c6 ( ) iC ti NP t = Where: 2 1( , , )i i iC t t t\u2212 \u2212 indicates the count of the tag sequence 2 1, ,i i it t t\u2212 \u2212< > and 1 2 3, ,\u03bb \u03bb \u03bb ( 1 2 3 1\u03bb \u03bb \u03bb+ + = ) are the weights for the maximum likelihood estimates of trigram, bigram and unigram tag probabilities respectively computed based on corpus statistics. The values of the parameters: 1 2 3, ,\u03bb \u03bb \u03bb are estimated using a smoothing technique called the deleted interpolation proposed in (Brants, 2000)."}, {"heading": "3.2 Computing Observation Probabilities", "text": "The observation probability of a observed triplet <word, POS-tag, chunk-tag >, which is the observed symbol in our case, is computed using the following equation (Sarkar and Gayen, 2012, 2013):\n( , ) ( )( | ) C o t C oP o t = (7)"}, {"heading": "3.3 Viterbi Decoding", "text": "The task of a decoder is to find the best hidden state sequence given an input HMM and a sequence of observations.\nThe Viterbi algorithm is the most common decoding algorithm used for HMM based tagging task. This is a standard application of the classic dynamic programming algorithm (Jurafsky and Martin, 2002). The Viterbi algorithm that we use, takes as input a single HMM and a set of observed sequence 1 2 3( ... )tO o o o o= and returns the most probable state sequence, 1 2 3( ... )tQ q q q q= , together with its probability.\nGiven a tag transition probability matrix and the observation probability matrix, Viterbi decoding (used at the testing phase) accepts a text document in Indian language and finds the most likely tag sequence for each POS-tagged chunked sentence in the input document. Here a sentence is also submitted to the viterbi as the observation sequence of triplets: (<word1, POS-tag1, chunk-tag1>, <word2, POStag2, chunk-tag2>, <word3, POS-tag3, chunk-tag3>, ..., <wordn, POS-tagn, chunk-tagn>). After assigning the tag sequence to the observation sequence as mentioned above, POS-tag and chunk-tag information are removed from the output and thus the out-\nput for an input sentence is converted to a NEtagged sentence.\nWe have used the Viterbi algorithm presented in (Jurafsky and Martin, 2002) for finding the most likely tag sequence for a given observation sequence.\nOne of the important problems to apply Viterbi decoding algorithm is how to handle unknown triplets in the input. The unknown triplets are triplets which are not present in the training set and hence their observation probabilities are not known. To handle this problem, we estimate the observation probability of an unknown one by analyzing POStag , chunk-tag and the suffix of the word associated with the corresponding the triplet. We estimate the observation probability of an unknown observed triplet in the following ways: The observation probabilities of unknown triplet <word, POS-tag, chunk-tag > corresponding to a word in the input sentence are decided according to the suffix of a pseudo word formed by adding POS-tag and chunk-tag to the end of the word. We find the observation probabilities of such unknown pseudo words using suffix analysis of all rare pseudo words (frequency <=2) in the training corpus for the concerned language. Since unknown pseudo words are infrequent and using suffixes of infrequent pseudo words in the lexicon is a better approximation for unknown pseudo words (Brants, 2000). The term suffix as used in this context means \u201ca sequence of characters occurring at the end of a pseudo word\u201d which is not necessarily a linguistically meaningful suffix. The maximum length of suffix has been tuned on the development set for the corresponding languages: suffix length is set to 8 for Bengali, 9 for English, 9 for Hindi, 9 for Marathi, 9 for Punjabi, 16 for Tamil and 13 for Telugu respectively. The probability of a tag given a suffix of length i is computed as: P(t |suffixof-len(i)). These probabilities are smoothed using successively shorter and shorter suffixes (Brants, 2000). This can be formulated in recursive way as:\n\u02c6( | ( )) ( | ( )) 1( | ( )) ii p t suffix of len i p t suffix of len iP t suffix of len i \u03b8\u03b8 \u2212 \u2212 + \u2212 \u2212 +\u2212 \u2212 = (8) Where: p\u0302 is the maximum likelihood probability based on the count of <tag, suffix> pair in all rare pseudo words (frequency <=2) in the corpus.\nAll i\u03b8 are set to the standard deviation of the unconditioned maximum likelihood probabilities\n\u02c6( ( ))ip t of the tags in the training corpus (Brants,\n2000). P(t|suffix-of-len(i)) gives an estimate of P(ti|wi ). But for HMM based tagging we need to compute the likelihood P(wi|ti) which is computed from P(ti|wi ) using Bayesian inversion that uses Bayes rule and prior P(ti)."}, {"heading": "4 Evaluation and Results", "text": "After getting the NE-tagged output in IOB format from the HMM model, we observed that the tagged output contains some occurrences of a sequence of I-XXXs where the left boundary of each such sequence is a transition from the tag \u201cO\u201d to I-XXXs (according to the IOB format, the left boundary of a named entity is a transition from any tag to B-XXX). We have also observed that the word sequence to which this type of tag sequence is assigned is not really a named entity. So, considering this as the errors of the model, we replace such a sequence of I-XXXs in the output by a sequence of \u201co\u201d. After doing this post-processing on the output produced by the HMM model, the final output file is produced.\nOur developed NER system has been evaluated using the traditional precision, recall and F-measure. For training, tuning and testing our system, we have used the datasets for 7 Indian languages, released by the organizers of the ICON 2013 NLP tool contest.\nWe train separately our developed named entity recognizer on the training data for each of the languages and tune the parameters of our system on the development data for the language under consideration. After learning the tuning parameters, we test our system on the test data for the concerned language.\nThe description of the data for each of 7 Indian languages is shown in the Table1\nLanguage\nTotal tokens NE Type s Training Development Test\nBengali 43732 6116 5938 24 English 91869 15839 14438 21 Hindi 68608 10678 8931 22 Marathi 72628 8975 7871 21 Punjabi 63253 8381 8008 21 Tamil 74077 7160 6608 25 Telugu 34910 6014 4288 22 Table1. The description of the data for each of 7 In-\ndian languages"}, {"heading": "5 Conclusion", "text": "This paper describes a named entity recognition system for Indian Languages namely Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu. The named entity recognition system has been developed using Visual Basic platform so that a suitable user interface can be designed for the novice users. The system has been designed in such a way that only changing the training corpus in a file can make the system portable to a new Indian language."}], "references": [{"title": "A Maximum Entropy Approach to Named Entity Recognition", "author": ["Andrew Borthwick."], "venue": "Ph.D. thesis, Computer Science Department, New York University.", "citeRegEx": "Borthwick.,? 1999", "shortCiteRegEx": "Borthwick.", "year": 1999}, {"title": "Language Independent Named Entity Recognition in Indian Languages", "author": ["Asif Ekbal", "Rejwanul Haque", "Amitava Das", "Venkateswarlu Poka", "Sivaji Bandyopadhyay."], "venue": "Proceedings of IJCNLP workshop on NERSSEAL.", "citeRegEx": "Ekbal et al\\.,? 2008", "shortCiteRegEx": "Ekbal et al\\.", "year": 2008}, {"title": "A conditional random field approach for named entity recognition in Bengali and Hindi", "author": ["Asif Ekbal", "Sivaji Bandyopadhyay."], "venue": "Linguistic Issues in Language Technology, 2(1).", "citeRegEx": "Ekbal and Bandyopadhyay.,? 2009", "shortCiteRegEx": "Ekbal and Bandyopadhyay.", "year": 2009}, {"title": "Speech and Language Processing An Intoduction to Natural Language Processing, Computational Linguistics and Speech Recognition", "author": ["Daniel Jurafsky", "James H. Martin"], "venue": "Preason Education Series", "citeRegEx": "Jurafsky and Martin.,? \\Q2002\\E", "shortCiteRegEx": "Jurafsky and Martin.", "year": 2002}, {"title": "Nymble: A High Performance Learning Name-finder", "author": ["Daniel M. Bikel", "Scott Miller", "Richard Schwartz", "Ralph Weischedel."], "venue": "Proceedings of the", "citeRegEx": "Bikel et al\\.,? 1997", "shortCiteRegEx": "Bikel et al\\.", "year": 1997}, {"title": "Internal and external evidence in the identification and semantic categorization of proper names", "author": ["David D. McDonald."], "venue": "B. Boguraev and J. Pustejovsky, editors, Corpus Processing for Lexical Acquisition, 21\u201339.", "citeRegEx": "McDonald.,? 1996", "shortCiteRegEx": "McDonald.", "year": 1996}, {"title": "A practical part-of-speech tagger for Bengali", "author": ["Kamal Sarkar", "Vivekananda Gayen."], "venue": "Proceedings of the third International conference on Emerging Applications of Information Technology (EAIT), Kolkata. pp. 36-40.", "citeRegEx": "Sarkar and Gayen.,? 2012", "shortCiteRegEx": "Sarkar and Gayen.", "year": 2012}, {"title": "A Trigram HMM-Based POS Tagger for Indian Languages", "author": ["Kamal Sarkar", "Vivekananda Gayen"], "venue": "In proceedings of International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA)", "citeRegEx": "Sarkar and Gayen.,? \\Q2013\\E", "shortCiteRegEx": "Sarkar and Gayen.", "year": 2013}, {"title": "Aggregating Machine Learning and Rule Based Heuristics for Named Entity Recgnition", "author": ["Karthik Gali", "Harshit Surana", "Ashwini Vaidya", "Praneeth Shishtla", "Dipti M. Sharma."], "venue": "Proceedings of IJCNLP workshop on NERSSEAL.", "citeRegEx": "Gali et al\\.,? 2008", "shortCiteRegEx": "Gali et al\\.", "year": 2008}, {"title": "Named Entity Recognition in Hindi using MEMM", "author": ["N. Kumar", "Pushpak Bhattacharyya."], "venue": "Technical Report, IIT Bombay, India.", "citeRegEx": "Kumar and Bhattacharyya.,? 2006", "shortCiteRegEx": "Kumar and Bhattacharyya.", "year": 2006}, {"title": "Hybrid Named Entity Recognition System for South-South East Indian Languages", "author": ["P. Praveen Kumar", "V. Ravi Kiran"], "venue": "InProceedings of IJCNLP workshop on NERSSEAL.", "citeRegEx": "Kumar and Kiran,? 2008", "shortCiteRegEx": "Kumar and Kiran", "year": 2008}, {"title": "The New York University System MUC-6 or Where\u2019s the syntax", "author": ["Ralph Grishman"], "venue": "In Proceedings of the Sixth Message Understanding Conference", "citeRegEx": "Grishman.,? \\Q1995\\E", "shortCiteRegEx": "Grishman.", "year": 1995}, {"title": "A Hybrid Approach for Named Entity and Sub-Type Tagging", "author": ["Rohini Srihari", "Cheng Niu", "Wei Li."], "venue": "Proceedings of the sixth conference on Applied natural language processing.", "citeRegEx": "Srihari et al\\.,? 2000", "shortCiteRegEx": "Srihari et al\\.", "year": 2000}, {"title": "Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence", "author": ["Silviu Cucerzan", "David Yarowsky."], "venue": "Proceedings of the Joint SIGDAT Conference on EMNLP and VLC 1999, 90\u201399.", "citeRegEx": "Cucerzan and Yarowsky.,? 1999", "shortCiteRegEx": "Cucerzan and Yarowsky.", "year": 1999}, {"title": "Evaluationof an algorithm for the recognition and classification of proper names", "author": ["Takahiro Wakao", "Robert Gaizauskas", "Yorick Wilks."], "venue": "Proceedings of COLING-96.", "citeRegEx": "Wakao et al\\.,? 1996", "shortCiteRegEx": "Wakao et al\\.", "year": 1996}, {"title": "TnT \u2013 A statistical part-ofspeech tagger", "author": ["Thorsten Brants."], "venue": "proceedings of the 6 Applied NLP Conference, pp. 224-231.", "citeRegEx": "Brants.,? 2000", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction (Short Paper)", "author": ["Wei Li", "Andrew McCallum."], "venue": "ACM Transactions on Computational Logic.", "citeRegEx": "Li and McCallum.,? 2004", "shortCiteRegEx": "Li and McCallum.", "year": 2004}], "referenceMentions": [{"referenceID": 11, "context": "The rule based approaches typically use rules manually written by human (Grishman, 1995; McDonald, 1996; Wakao et al., 1996).", "startOffset": 72, "endOffset": 124}, {"referenceID": 5, "context": "The rule based approaches typically use rules manually written by human (Grishman, 1995; McDonald, 1996; Wakao et al., 1996).", "startOffset": 72, "endOffset": 124}, {"referenceID": 14, "context": "The rule based approaches typically use rules manually written by human (Grishman, 1995; McDonald, 1996; Wakao et al., 1996).", "startOffset": 72, "endOffset": 124}, {"referenceID": 4, "context": "Several ML techniques have been successfully used for the NER tasks such as Markov Model (HMM) (Bikel et al., 1997), Maximum Entropy (MaxEnt) (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Conditional Random Field (CRF) (Li and Mccallum, 2004) etc.", "startOffset": 95, "endOffset": 115}, {"referenceID": 0, "context": ", 1997), Maximum Entropy (MaxEnt) (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Conditional Random Field (CRF) (Li and Mccallum, 2004) etc.", "startOffset": 34, "endOffset": 82}, {"referenceID": 9, "context": ", 1997), Maximum Entropy (MaxEnt) (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Conditional Random Field (CRF) (Li and Mccallum, 2004) etc.", "startOffset": 34, "endOffset": 82}, {"referenceID": 16, "context": ", 1997), Maximum Entropy (MaxEnt) (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Conditional Random Field (CRF) (Li and Mccallum, 2004) etc.", "startOffset": 115, "endOffset": 138}, {"referenceID": 0, "context": ", 1997), Maximum Entropy (MaxEnt) (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Conditional Random Field (CRF) (Li and Mccallum, 2004) etc. Combinations of different ML approaches are also used. Srihari et al.(2000) combines MaxEnt,", "startOffset": 35, "endOffset": 220}, {"referenceID": 11, "context": "Both the linguistic approach (Grishman, 1995; Wakao et al., 1996) and the ML based approach (Borthwick, 1999; Srihari et al.", "startOffset": 29, "endOffset": 65}, {"referenceID": 14, "context": "Both the linguistic approach (Grishman, 1995; Wakao et al., 1996) and the ML based approach (Borthwick, 1999; Srihari et al.", "startOffset": 29, "endOffset": 65}, {"referenceID": 0, "context": ", 1996) and the ML based approach (Borthwick, 1999; Srihari et al., 2000) may use gazetteer lists.", "startOffset": 34, "endOffset": 73}, {"referenceID": 12, "context": ", 1996) and the ML based approach (Borthwick, 1999; Srihari et al., 2000) may use gazetteer lists.", "startOffset": 34, "endOffset": 73}, {"referenceID": 13, "context": "The NER tasks for Hindi have been presented in (Cucerzan and Yarowsky, 1999; Li and Mccallum, 2004; Ekbal et.al., 2009).", "startOffset": 47, "endOffset": 119}, {"referenceID": 16, "context": "The NER tasks for Hindi have been presented in (Cucerzan and Yarowsky, 1999; Li and Mccallum, 2004; Ekbal et.al., 2009).", "startOffset": 47, "endOffset": 119}, {"referenceID": 15, "context": "Considering a special tag tn+1 to indicate the end sentence boundary and two special tags t-1 and t0 at the starting boundary of the sentence and adding these three special tags to the tag set (Brants, 2000), gives the following equation for NE tagging:", "startOffset": 193, "endOffset": 207}, {"referenceID": 15, "context": "technique called the deleted interpolation proposed in (Brants, 2000).", "startOffset": 55, "endOffset": 69}, {"referenceID": 3, "context": "This is a standard application of the classic dynamic programming algorithm (Jurafsky and Martin, 2002).", "startOffset": 76, "endOffset": 103}, {"referenceID": 3, "context": "We have used the Viterbi algorithm presented in (Jurafsky and Martin, 2002) for finding the most likely tag sequence for a given observation sequence.", "startOffset": 48, "endOffset": 75}, {"referenceID": 15, "context": "Since unknown pseudo words are infrequent and using suffixes of infrequent pseudo words in the lexicon is a better approximation for unknown pseudo words (Brants, 2000).", "startOffset": 154, "endOffset": 168}, {"referenceID": 15, "context": "These probabilities are smoothed using successively shorter and shorter suffixes (Brants, 2000).", "startOffset": 81, "endOffset": 95}, {"referenceID": 15, "context": "unconditioned maximum likelihood probabilities \u02c6 ( ( )) i p t of the tags in the training corpus (Brants, 2000).", "startOffset": 97, "endOffset": 111}], "year": 2013, "abstractText": "This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named Entity Recognition. We submitted runs for Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of 0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali, English, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.", "creator": "PScript5.dll Version 5.2.2"}}}