{"id": "1405.7716", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2014", "title": "Experimental Demonstration of Array-level Learning with Phase Change Synaptic Devices", "abstract": "the computational efficiency performance of the biological brain has long attracted highly significant ongoing interest and learning has all led to novel inspirations in operating algorithm principles, algorithms, programming and graphics architectures for computing neuroscience and signal computing processing. effective in understanding this combined work, additionally we focus on various hardware implementation processes of composite brain - like computational learning pattern in a brain - inspired architecture. we demonstrate, in hardware, that 2 - d / crossbar temporal arrays of sync phase change synaptic devices can achieve observed associative learning peaks and perform pattern recognition. mouse device prediction and array - level sequencing studies using concurrently an experimental 10x10 array of synchronized phase change versus synaptic devices above have only shown that pattern recognition is robust against synaptic resistance where variations drastically and large variations can rarely be tolerated indefinitely by increasing the occurrence number observed of successive training time iterations. our measurements show that yearly increase in path initial block variation from approximately 9 % to 60 % causes required training iterations to likely increase activity from 1 to typically 11.", "histories": [["v1", "Thu, 29 May 2014 20:22:00 GMT  (1389kb)", "http://arxiv.org/abs/1405.7716v1", "IEDM 2013"], ["v2", "Tue, 3 Jun 2014 05:45:54 GMT  (4626kb)", "http://arxiv.org/abs/1405.7716v2", "IEDM 2013"]], "COMMENTS": "IEDM 2013", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["s burc eryilmaz", "duygu kuzum", "rakesh g d jeyasingh", "sangbum kim", "matthew brightsky", "chung lam", "h -s philip wong"], "accepted": false, "id": "1405.7716"}, "pdf": {"name": "1405.7716.pdf", "metadata": {"source": "CRF", "title": "Experimental Demonstration of Array-level Learning with Phase Change Synaptic Devices", "authors": ["S. Burc Eryilmaz", "Duygu Kuzum", "Rakesh G. D. Jeyasingh", "SangBum Kim", "Matthew BrightSky", "Chung Lam", "Philip Wong"], "emails": ["eryilmaz@stanford.edu,"], "sections": [{"heading": null, "text": "I. Introduction\nSynaptic electronics is an emerging field of research aiming to build electronic systems that mimic computational energyefficiency and fault tolerance of biological brain in a compact space [1]. To date, synaptic electronics research has primarily focused on device level implementations of synaptic plasticity and array or chip level simulations [2-4]. Studies that involve on-chip demonstration of brain-like computation with synaptic\ndevices and integration of synaptic devices in a compact, massively parallel architecture remain to be the next big challenge [5]. Recent brain imaging studies suggest that neural connections are in the form of folding 2D sheets of parallel neuronal fibers that cross paths at right angles [6]. The grid-\nlike connectivity of the brain can be best emulated using a 2-D or layered 3-D crossbar array architecture with two terminal synaptic devices (Figure 1). In this work, we experimentally demonstrate that brain-like pattern recognition can be implemented using 2-D crossbar arrays of phase change synaptic devices. First we present individual synaptic device characteristics for the phase change devices in the 10\u00d710 array (section II), and then we explain array level operation and programming scheme and algorithms used for learning (section III). Finally, analysis on resistance variation and its effect on array level learning and pattern completion are discussed (section IV)."}, {"heading": "II. Synaptic Array", "text": "Nanoscale devices based on phase change memory, PCM (Ge2Sb2Te5), have been shown to mimic biological synapses [2-4]. In the experiments, 10-by-10 PCM cell arrays with\nselection transistors were used (Fig. 2) [7,8]. Ten bitlines (BL) and ten wordlines (WL) are connected to the top electrode of the device and the gate of the selection transistors, respectively. Microscope image of the array and TEM image of a single device [7,8] are shown in Figure 3(left) and (right), respectively. Typical DC switching and pulse switching characteristics of a single device arbitrarily chosen from one of the arrays are shown in Figures 4 and 5, respectively. Set and reset pulses with amplitudes of 1 V and 1.5 V and with (50 ns/300 ns/1 \u00b5s) and (20 ns/50 ns/5 ns) rise/width/fall time used at WL node results in a uniform distribution across the array (Fig. 6). Gradual programming to achieve analog storage is implemented by applying multiple pulses to the same PCM cells (Fig. 7). By adjusting pulse amplitudes and widths, it is possible to implement more gradual switching using PCM cells [2,3]."}, {"heading": "III. Array-level Learning", "text": "The crossbar array consists of 100 synaptic devices and 10 recurrently connected neurons (Fig. 2). Integrate-and-fire neurons are implemented by computer control and PCM cells were used as synaptic devices between neurons. The input terminal of each neuron is connected to a BL node, whereas the output terminal of each neuron is connected to a WL node (Fig. 8). In the learning phase, synaptic weights are updated via a simple form of Hebbian plasticity. The synapses between coactive neurons that fire in the same 100 \u03bcs time window, get stronger. Otherwise, synaptic weight does not change. At\neach learning iteration (epoch), if a neuron is not firing, it integrates the current from all the inputs due to the spiking neurons at that iteration. This is achieved by applying a 0.1 V read pulse at input node (BL) of a non-firing neuron. If the total current through the input node of a non-firing neuron is large enough to exceed the threshold, it fires in the next iteration. Firing can also occur due to external stimulation of a neuron. If a neuron is in firing mode at an iteration, it sends a gating pulse to the output node (WL) and a programming pulse to the input node (BL). This network algorithm is explained in Figure 8 and 9. As an illustration of this learning process, we choose two simple patterns to be learned. The two patterns of 10 pixels are shown in Fig. 10 (b) and (c), and the\nFigure 5. Single cell pulse switching between high resistance and low resistance states.\nFigure 6. Binary resistance distribution of the memory\nFigure 7. Gradual SET characteristics of a single cell.\nFigure 4. Single cell DC switching.\nneuron number associated to each pixel is shown in Figure 10(a)."}, {"heading": "IV. Resistance Variation", "text": "In order to explore the effect of resistance variation on pattern recognition performance, two arrays with high (60 %) and low (9 %) resistance variation are used. Figure 11 shows initial resistance distribution for Array I (high variation) and Array II (low variation). High variation is obtained by initially applying the same RESET pulse to every cell to bring them to the partial RESET state, and low variation is obtained by\ncarefully selecting pulse amplitudes individually for each cell and bringing them to the fully RESET state. The neurons of both synaptic arrays are stimulated with patterns 1 and 2 during the learning epochs. In Figure 11, darker cells show the synapses which get stronger after 11 training epochs. Note that after feeding each input pattern into the network, synapses between the ON neurons get stronger (resistance decreases). Evolution of cell resistance distribution for Array I (Figure 12) explains that the higher the initial resistance variation is, the more training epochs are required for the network to learn the pattern. After the learning phase, the network is presented with an incomplete pattern. Fig. 13 shows neuron input currents for non-firing neurons for Array I and II. For Array II with low initial resistance variation, one training epoch is sufficient for firing neurons (1-4) to recruit neuron 6 and complete the pattern. Input current for nonfiring neurons seem to have more variation for array I, resulting in a higher firing threshold requirement and hence more epochs (11 for array I) to recall original pattern (Figure 14). Both arrays with low and high initial resistance variation are able to recall original pattern after ten epochs. Total energy consumed to learn the pattern is measured to be 199 pJ and 8.4 nJ for arrays with low and high variation, respectively. Low energy consumption in Array II is due to smaller number of epochs required and the high resistance regime the devices operate. Resistance variations as high as 60% can be tolerated for pattern completion operation at the expense of more learning iterations and energy consumption."}, {"heading": "V. Conclusion", "text": "We report brain-like learning, in hardware, of a small-scale 10 \u00d7 10 crossbar array with synaptic devices. We demonstrated in hardware experiments that synaptic network can implement robust pattern recognition through brain-like learning. Test patterns were shown to be stored and recalled associatively via Hebbian plasticity in a manner similar to the biological brain. Initial resistance variations can be tolerated by adding more training epochs and at the expense of more energy consumption. Demonstration of robust brain-inspired learning in a small-scale synaptic array is a significant step towards building large-scale computation systems with brain-level computational efficiency. Our work inspired from the grid-like connectivity of the brain may lead to more biologicallyplausible brain prosthetics in future."}, {"heading": "Acknowledgments", "text": "This work is supported in part by SONIC, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA, the Nanoelectronics Research Initiative (NRI) of the Semiconductor Research Corporation (SRC) through the NSF/NRI Supplement to the NSF NSEC Center for Probing the Nanoscale (CPN), and the member companies of the Stanford Non-Volatile Memory Technology Research Initiative (NMTRI)."}], "references": [{"title": "Nanotechnology", "author": ["D. Kuzum", "S. Yu", "H.-S. Philip Wong"], "venue": "24, 382001 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Nano Letters", "author": ["D. Kuzum", "R.G.D. Jeyasingh", "B Lee", "H.S.P. Wong"], "venue": "pp. 2179-2186 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Pattern recall. Red squares represent spiking neurons for every epoch. Both arrays with low and high initial resistance distribution were able to recall pattern 1 successfully (note the recall of neuron #6). Array II recalls the original pattern after 1 epochs while Array I needs 11 epochs to recall the original pattern", "author": ["F. G"], "venue": "Close et al., IEDM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "space [1].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "To date, synaptic electronics research has primarily focused on device level implementations of synaptic plasticity and array or chip level simulations [2-4].", "startOffset": 152, "endOffset": 157}, {"referenceID": 1, "context": "Nanoscale devices based on phase change memory, PCM (Ge2Sb2Te5), have been shown to mimic biological synapses [2-4].", "startOffset": 110, "endOffset": 115}, {"referenceID": 2, "context": "Array pad microscope image (left) and single memory cell schematic and TEM image (right) [7,8].", "startOffset": 89, "endOffset": 94}, {"referenceID": 2, "context": "2) [7,8].", "startOffset": 3, "endOffset": 8}, {"referenceID": 2, "context": "Microscope image of the array and TEM image of a single device [7,8] are shown in Figure 3(left) and (right), respectively.", "startOffset": 63, "endOffset": 68}, {"referenceID": 1, "context": "By adjusting pulse amplitudes and widths, it is possible to implement more gradual switching using PCM cells [2,3].", "startOffset": 109, "endOffset": 114}], "year": 2013, "abstractText": "The computational performance of the biological brain has long attracted significant interest and has led to inspirations in operating principles, algorithms, and architectures for computing and signal processing. In this work, we focus on hardware implementation of brain-like learning in a braininspired architecture. We demonstrate, in hardware, that 2-D crossbar arrays of phase change synaptic devices can achieve associative learning and perform pattern recognition. Device and array-level studies using an experimental 10\u00d710 array of phase change synaptic devices have shown that pattern recognition is robust against synaptic resistance variations and large variations can be tolerated by increasing the number of training iterations. Our measurements show that increase in initial variation from 9 % to 60 % causes required training iterations to increase from 1 to 11.", "creator": "InControl Productions, Inc."}}}