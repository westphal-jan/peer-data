{"id": "1406.1580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Machine learning approach for text and document mining", "abstract": "sequential text categorization ( tc ), also known as text classification, means is simply the task of automatically - classifying a set of text documents potentially into simultaneously different categories also from identifying a naturally predefined set. moreover if visually a relevant document belongs exactly to entities exactly, one of the categories, possibly it is also a homogeneous single - label classification task ; that otherwise, typically it is a multi - label linguistic classification taxonomy task. tc uses several different tools stem from information language retrieval ( ir ) and machine learning ( an ml ) and sip has received much attention in the last years participation from educators both researchers in the academia community and technical industry developers. specifically in this paper, we usually first categorize the documents naturally using knn based machine learning systems approach and then internally return the specified most relevant type documents.", "histories": [["v1", "Fri, 6 Jun 2014 04:37:19 GMT  (479kb)", "http://arxiv.org/abs/1406.1580v1", "arXiv admin note: text overlap witharXiv:1003.1795,arXiv:1212.2065by other authors"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1003.1795,arXiv:1212.2065by other authors", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["vishwanath bijalwan", "pinki kumari", "jordan pascual", "vijay bhaskar semwal"], "accepted": false, "id": "1406.1580"}, "pdf": {"name": "1406.1580.pdf", "metadata": {"source": "CRF", "title": "Machine learning approach for text and document mining", "authors": ["Vishwanath Bijalwan", "Pinki Kumari", "Jordan Pascual", "Vijay Bhaskar Semwal"], "emails": [], "sections": [{"heading": null, "text": "Text Categorization (TC), also known as Text Classification, is the task of automatically classifying a set of text documents into different categories from a predefined set. If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task. TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers. In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents.\nKeywords: Text Mining, Na\u00efve Bayes, KNN, Event models, Document Mining, Term-\nGraph, Machine Learning."}, {"heading": "1. Introduction", "text": "Information Retrieval (IR) is the science of searching for information within relational databases, documents, text, multimedia files, and the World Wide Web. The applications of IR are diverse; they include but not limited to extraction of information from large documents, searching in digital libraries, information filtering, spam filtering, object extraction from images, automatic summarization, document classification and clustering, and web searching. The breakthrough of the Internet and web search engines have urged scientists and large firms to create very large scale retrieval systems to keep pace with the exponential growth of online data. Figure below depicts the architecture of a general IR system. The user first submits a query which is executed over the retrieval system. The latter, consults a database of document collection and returns the matching document. In general, in order to learn a classifier that is able to correctly classify unseen documents, it is necessary to train it with some pre-classified documents from each category, in such a way that the classifier is then able to generalize the model it has learned from the pre-classified documents and use that model to correctly classify the unseen documents. Figure 1 shows the overview of the document indexing and retrieval system. From experiment, KNN shows the maximum accuracy as compared to the Naive Bayes and Term-Graph. The drawback for KNN is that its time complexity is high but gives a better accuracy than others.\nIn recent years, text categorization has become an important research topic in machine learning and information retrieval and e-mail spam filtering. It also has become an important research topic in text mining, which analyses and extracts useful information from texts. More Learning techniques have been in research for dealing with text categorization. The existing text classification methods can be classified into below six [7], [8], [9] categories:\n(1) Based on Rocchio\u201fs method (Dumais, Platt, Heckerman, & Sahami, 1998; Hull,\n1994; Joachims, 1998; Lam & Ho, 1998).\n(2) Based on K-nearest neighbors (KNN) (Hull, 1994; Lam & Ho, 1998; Tan, 2005;\nTan, 2006; Yang & Liu, 1999).\n(3) Based on regression models (Yang, 1999; Yang & Liu, 1999).\n(4) Based on Na\u0131ve Bayes and Bayesian nets (Dumais et al., 1998; Hull, 1994; Yang\n& Liu, 1999; Sahami, 1996).\n(5) Based on decision trees (Fuhr & Buckley, 1991; Hull, 1994).\n(6) Based on decision rules (Apte`, Damerau, & Weiss, 1994; Cohen & Singer,\n1999).\nAmong the six types the survey aims in getting an intuitive understanding of KNN approach in which the application of various Machine Learning Techniques [11, 17, 18, 20, 21] to the text categorization problem like in the field of medicine, e-mail filtering, including rule learning for knowledge base systems has been explored. The survey is oriented towards the various probabilistic approach of KNN Machine Learning algorithm for which the text categorization aims to classify the document with optimal accuracy. Information retrieval is also used in image retrieval [19]. In recent works, to save and estimate accurate location moving object with energy constraint is proposed in [10, 14] using adaptive update algorithms. Some other recent approaches such as video summarization [12], 3D model of 2D image [13], gait pattern [15], and scale replica model [16] can also be integrated with the proposed approach to enhance the efficiency.\nThis paper categorizes the news articles into various categories. We work on two\nmajor scenarios:\na. Classification of documents into various categories.\nMaking it in the form of an application where user can upload an article and we will\nclassify it into various categories.\nb. On entering keywords by the user we show the most relevant document for the\nuser."}, {"heading": "2. Classification Methods", "text": "This paper concerns methods for the classification of natural language text, that is, methods that, given a set of training documents with known categories and a new document, which is usually called the query, will predict the query\u201fs category."}, {"heading": "2.1. Naive Bayes", "text": "The Naive Bayes classifier found its way into many applications nowadays due to its simple principle but yet powerful accuracy [2], [5]. Bayesian classifiers are based on a statistical principle. Here, the presence or absence of a word in a textual document determines the outcome of the prediction. In other words, each processed term is assigned a probability that it belongs to a certain category. This probability is calculated from the occurrences of the term in the training documents where the categories are already known. When all these probabilities are calculated, a new document can be classified according to the sum of the probabilities for each category of each term occurring within the document. However, this classifier does not take the number of occurrences into account, which is a potentially useful additional source of information. They are called \u201cnaive\u201d because the algorithm assumes that all terms occur independent from each other. Given a set of r document vectors D = {d1, \u2026, dr}, classified along a set C of q classes, C={c1, \u2026, cq}, Bayesian classifiers estimate the probabilities of each class ck given a document dj as:\n\ud835\udc43(\ud835\udc50\ud835\udc58 |\ud835\udc51\ud835\udc57 ) = \ud835\udc43(\ud835\udc50\ud835\udc58)\ud835\udc43 \ud835\udc51\ud835\udc57 \ud835\udc50\ud835\udc58)\n\ud835\udc43(\ud835\udc51\ud835\udc57 ) (1)\nIn this eq. 1, \ud835\udc43(\ud835\udc51\ud835\udc57 ) is the probability that a randomly picked document has vector \ud835\udc51\ud835\udc57 as its representation, and \ud835\udc43(\ud835\udc50\ud835\udc58) the probability that a randomly picked document belongs to ck. Because the number of possible documents \ud835\udc51\ud835\udc57 is very high, the estimation of \ud835\udc43 \ud835\udc51\ud835\udc57 \ud835\udc50\ud835\udc58) is problematic. To simplify the estimation of \ud835\udc43 \ud835\udc51\ud835\udc57 \ud835\udc50\ud835\udc58) , Naive Bayes assumes that the probability of a given word or term is independent of other terms that appear in the same document. While this may seem an over simplification, in fact Naive Bayes presents results that are very competitive with those obtained by more elaborate methods. Moreover, because only words and not combinations of words are used as predictors, this naive simplification allows the computation of the model of the data associated with this method to be far more efficient than other non naive Bayesian approaches. Using this simplification, it is possible to\ndetermine \ud835\udc43 \ud835\udc51\ud835\udc57 \ud835\udc50\ud835\udc58) as the product of the probabilities of each term that appears in the\ndocument. So, \ud835\udc43 \ud835\udc51\ud835\udc57 \ud835\udc50\ud835\udc58) may be estimated as:\n\ud835\udc43 \ud835\udc51\ud835\udc57 \ud835\udc50\ud835\udc58) = \ud835\udc43(\ud835\udc64\ud835\udc56\ud835\udc57 |\ud835\udc50\ud835\udc58) |\ud835\udc47| \ud835\udc56=1 (2)\nwhere, \ud835\udc51\ud835\udc57 = (\ud835\udc641\ud835\udc57, \u2026 , \ud835\udc64|\ud835\udc47|\ud835\udc57).\nAlgorithm: 1) Checking the keyword in Test document and storing it in a map.\n2) Calculating yes and no frequency of each keyword in the test document.\n3) Calculating the probability of each keyword of the test document.\n4) Classifying the Test Document into various categories on the basis of probability\ncalculated."}, {"heading": "2.2. Term Graph Model", "text": "The term graph model is an improved version of the vector space model [6] by weighting each term according to its relative \u201cimportance\u201d with regard to t erm associations. Specifically, for a text document Di, it is represented as a vector of term weights Di = <w1i, ...., w|T|i >, where T is the ordered set of terms that occur at least once in at least one document in the collection. Each weight wji represents how much the corresponding term tj contribute to the semantics of document di. Although a number of weighting schemes have been proposed (e.g., boolean weighting, frequency weighting, tf-idf weighting, etc.), those schemes determine the weight of each term individually. As a result, important yet rich information regarding the relationships among the terms are not captured in those weighting schemes.\nWe introduce to determine the weight of each term in a document collection by\nconstructing a term graph. The basic steps are as follows:\n1. Preprocessing Step: For a collection of document, extract all the terms.\nIn our term graph model, we will capture the relationships among terms using the frequent item set mining method. To do so, we consider each text document in the training collections as a transaction in which each word is an item. However, not all words in the document are important enough to be retained in the transaction. To reduce the processing space as well as increase the accuracy of our model, the text documents need to be preprocessed by (1) remove stop words, i.e., words that appear frequently in the document but have no essential meanings; and (2) retaining only the root form of words by stemming their affixes as well as prefixes.\n2. Graph Building Step:\n(a) For each document, we view it as a transaction: the document ID is the corresponding transaction ID; the terms contained in the document are the items contained in the corresponding transaction. Association rule mining algorithms can thus be applied to mine the frequently co-occurring terms that occur more than minsup times in the collection.\n(b) The frequent co-occurring terms are mapped to a weighted and directed graph,\ni.e., the term graph.\nAs mentioned above, we will capture the relationships among terms using the frequent item set mining method. While this idea has been explored by previous research [3], our approach distinguishes from previous approaches in that we maintain all such important associations in a graph. The graph not only reveals the important semantics of the document, but also provides a basis to extract novel features about the document, as we will show in the next section. After the preprocessing step, each document in the text collection will be stored as a transaction (list of items) in which each item (term) is represented by a unique non-negative integer. Then frequent item set mining algorithms can be used to find all the subset of items that appeared more than a threshold amount of times in the collection.\nIn our system, our goal is to explore the relationships among the important terms of the text in a category and try to define a strategy to make use of these relationships in\nthe classifier and other text mining tasks. Vector space model cannot express such rich relationship among terms. Graph is thus the most suitable data structure in our context, as, in general, each term may be associated with more than one terms. We propose to use the following simple method to construct the graph from the set of frequent item sets mined from the text collections. First, we construct a node for each unique term that appears at least once in the frequent item sets. Then we create edges between two node u and v if and only if they are both contained in one frequent item set. Furthermore, we assign weights to the edges in the following way: the weight of the edge between u and v is the largest support value among all the frequent item sets that contains both of them.\nFor, example, consider the frequent item sets and their absolute support shown in\nFigure 2(a). Its corresponding graph is shown in Figure 2(b).\nAlgorithm: 1) Setting each unique word occurring the document as nodes of the graph.\n2) Making Adjacency Matrix of the keywords.\n3) Making Distance Matrix using Dijkstra.\n4) Calculating similarity between the test document keywords and the keywords of each\ncategory."}, {"heading": "2.3. k-Nearest Neighbors", "text": "The initial application of k-Nearest Neighbors (KNN) to text categorization was reported in [4]. The basic idea is to determine the category of a given query based not only on the document that is nearest to it in the document space, but on the categories of the k documents that are nearest to it. Having this in mind, the Vector method can be viewed as an instance on the KNN method, where k=1. This work uses a vector-based, distance-weighted matching function, as did Yang, by calculating document\u201fs similarity like the Vector method.\nThen, it uses a voting strategy to find the query\u201fs class: each retrieved document contributes a vote for its class, weighted by its similarity to the query. The query\u201fs possible classifications will be ranked according to the votes they got in the previous step. Algorithm:\n1) Make vector for every document in the test set.\n2) Make centriod vector for each class.\n3) Calculate similarity between each document vector and class vector.\n4) Document belongs to the class for which the similarity is maximum.\nFigure 3 shows the flowchart of the proposed system for text and document mining using machine learning techniques."}, {"heading": "3. Experimental Results", "text": ""}, {"heading": "3.1. Dataset", "text": "The data set used for this paper is in the form of sgml files [3]. We have used Reuters-21578 dataset which is available at [1]. There are 21578 documents; according to the \u201eModApte\u201f split: 9603 training docs, 3299 test docs and 8676 unused docs. They were labeled manually by Reuters personnel. Labels belong to 5 different category classes, such as \u201epeople\u201f, \u201eplaces\u201f, \u201eExchange\u201f, \u201eOrganization\u201f and \u201etopics\u201f. The total number of categories is 672, but many of them occur only very rarely. The dataset is divided in 22 files of 1000 documents delimited by SGML tags."}, {"heading": "3.2. Implementation", "text": "For classifying the documents in Reuter-21578 we initially pre-processed the data by\nperforming various techniques:\na. Bag of words b. Stop word removal c. TF-IDF d. Case Folding e. Normalization\nThen after pre-processing, we applied KNN, Term Graph algorithm, and Na\u00efve Bayes algorithms to classify the documents in the training set into five categories (exchange, organization, people, places and topics). We further applied our classifier model on the test documents and calculated the accuracy by comparing it with the default answers given for the test documents. To compare the above mentioned algorithms, we used the following metric:\nAccuracy, which is defined as the percentage of correctly classified documents, is\ngenerally used to evaluate single-label TC tasks.\nWe then created an application where user can input some keywords and based on\nthe algorithm showing higher accuracy we show the relevant document to the user."}, {"heading": "3.3. Results", "text": "We compared the accuracy of Na\u00efve Bayes, Term Graph and KNN for Text and Document classification of our articles of Reuter 21578. As shown in the graph of Fig. 4, we found that KNN shows the best result with accuracy as provided in Table 1.\nFrom above results, we can say that KNN based learning technique is more suitable than Na\u00efve Bayes and Term Graph classification technique for the mining of text or documents. The accuracy reported for KNN is much high than Na\u00efve based method as shown in Table 1 for each category of the dataset."}, {"heading": "4. Conclusion", "text": "We conclude that KNN shows the maximum accuracy as compared to the Naive Bayes and Term-Graph. The drawback for KNN is that its time complexity is high but gives a better accuracy than others. We implemented Term-Graph with other methods rather than the traditional Term-Graph used with AFOPT. This hybrid shows a better result than the traditional combination. Finally we made an information retrieval application using Vector Space Model to give the result of the query entered by the client by showing the relevant document. We will focus more in future on Reducing Complexity, Increasing Accuracy and Text Summarization.\n5. References\n[1]. http://www.daviddlewis.com/resources/testcollections/reuters21578/. [2]. K. A. Vidhya and G. Aghila, \u201cA Survey of Na\u00efve Bayes Machine Learning approach in Text\nDocument Classification\u201d, International Journal of Computer Science and Information Security, vol. 7, no. 2, pp. 206-211, (2010).\n[3]. Z. Abdullah and M. S. Hitam, \u201cFeatures Extraction Algorithm from sgml for Classification\u201d, Journal of Theoretical and Applied Information Technology, vol. 3, pp. 72-78, (2007). [4]. L. Wang, X. Zhao, \u201cImproved knn Classification Algortihm Research in Text Categorization\u201d, In the Proceedings of the 2nd International Conference on Communications and Networks\n(CECNet), pp. 1848-1852, (2012).\n[5]. A. McCallum and K. Nigam, \u201cA Comparison of Event Models For Na\u00efve Bayes Text Classification\u201d, In The Proceedings Of The Workshop On Learning For Text Categorization, pp.\n41-48, (1998).\n[6]. W. Wang, D. B. Do, and X. Lin, \u201cTerm Graph Model for Text Classification\u201d, In the Proceedings of the International Conference on Advanced Data Mining and Applications, pp. 19-30, (2005). [7]. T. M. Mitchell, \u201cMachine Learning\u201d, Carnegie Mellon University, McGraw-Hill Book Co, (1997). [8]. Y. H. Chang and H. Y. Huang, \u201cAn automatic document classification Based on Na\u00efve Bayes Classifier and Ontology\u201d, In the Proceedings of the seventh International conference on Machine\nLearning and Cybernetics, pp. 3144-3149, (2008).\n[9]. V. Gupta, G. S. Lehal, \u201cA Survey of Text Mining Techniques and Applications\u201d, Journal of Emerging Technologies in Web Intelligence, vol. 1, no.1, (2009). [10]. V. B. Semwal, K. S. Kumar, V. S. Bhaskar, M. Sati, \u201cAccurate location estimation of moving object with energy constraint & adaptive update algorithms to save data\u201d, arXiv preprint\narXiv:1108.1321 (2011).\n[11]. K. S. Kumar, V. B. Semwal, and R. C. Tripathi, \u201cReal time face recognition using adaboost improved fast PCA algorithm\u201d, arXiv preprint arXiv:1108.1353 (2011). [12]. K. S. Kumar, S. Prasad, S. Banwral, V. B. Semwal, \u201cSports Video Summarization using Priority Curve Algorithm\u201d, International Journal 2 (2010). [13]. K. S. Kumar, V. B. Semwal, S. Prasad, and R. C. Tripathi, \u201cGenerating 3D Model Using 2D Images of an Object\u201d, International Journal of Engineering Science (2011). [14]. V. B. Semwal, K. S. Kumar, V. S. Bhaskar, M. Sati, \u201cAccurate location estimation of moving object with energy constraint & adaptive update algorithms to save data,\u201d arXiv preprint\narXiv:1108.1321 (2011).\n[15]. J. P. Gupta, N. Singh, P. Dixit, V. B. Semwal, S. R. Dubey, \u201cHuman Activity Recognition using Gait Pattern\u201d, International Journal of Computer Vision and Image Processing, vol. 3, no. 3, pp.\n31-53, (2013).\n[16]. Sati M. et al., (2014), A fault-tolerant mobile computing model based on scalable replica, International Journal of Interactive Multimedia and Artificial Intelligence, 2014.(2014) [17]. Bijalwan V. et al., (2014), KNN based Machine Learning Approach for Text and Document Mining, International Journal of Database Theory and Application, 7(1), 61-70, (2014). [18]. Kumari P. et al., (2011), Instant Face detection and attributes recognition, International Journal of Advanced Computer Science and Applications.(2011).\n[19]. Kumari P. et al., (2011), A Comparative study of Machine Learning algorithms for Emotion State Recognition through Physiological signal, Advances in Intelligent Systems and Computing,\nVol.236-Springer; ISBN 978-81-322-1601-8.(2011).\n[20]. Kumari P. et al., (2014), Brainwave\u201fs Energy feature Extraction using wavelet Transfor, IEEE SCEECS,(2014). [21]. Rub\u00e9n G.C. et al. (2013), Use of ARIMA mathematical analysis to model the implementation of expert system courses by means of free software OpenSim and Sloodle platforms in virtual\nuniversity campuses, Expert Systems with Applications, 40(18), 7381-7390(2013).\nAuthors\nVishwanath Bijalwan is working as assistance professor at Uttarakhand Technical University Dehradun. He is person with lot of potential. His research interest is wireless sensor network, WiMax, Wi-Fi, machine learning, Informtion Retrieval. He obtained B.Tech. and M.Tech from DIT Dehradun. So far he has published 4 high quality researches paper and\nwork on many MHRD funded project. He is carrying 5year of research experience."}], "references": [{"title": "A Survey of Na\u00efve Bayes Machine Learning approach in Text Document Classification", "author": ["K.A. Vidhya", "G. Aghila"], "venue": "International Journal of Computer Science and Information Security,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Features Extraction Algorithm from sgml for Classification", "author": ["Z. Abdullah", "M.S. Hitam"], "venue": "Journal of Theoretical and Applied Information Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Improved knn Classification Algortihm Research in Text Categorization", "author": ["L. Wang", "X. Zhao"], "venue": "In the Proceedings of the 2nd International Conference on Communications and Networks (CECNet),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A Comparison of Event Models For Na\u00efve Bayes Text Classification", "author": ["A. McCallum", "K. Nigam"], "venue": "In The Proceedings Of The Workshop On Learning For Text Categorization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Term Graph Model for Text Classification", "author": ["W. Wang", "D.B. Do", "X. Lin"], "venue": "In the Proceedings of the International Conference on Advanced Data Mining and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "An automatic document classification Based on Na\u00efve Bayes Classifier and Ontology", "author": ["Y.H. Chang", "H.Y. Huang"], "venue": "In the Proceedings of the seventh International conference on Machine Learning and Cybernetics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A Survey of Text Mining Techniques and Applications", "author": ["V. Gupta", "G.S. Lehal"], "venue": "Journal of Emerging Technologies in Web Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Accurate location estimation of moving object with energy constraint & adaptive update algorithms to save data", "author": ["V.B. Semwal", "K.S. Kumar", "V.S. Bhaskar", "M. Sati"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Real time face recognition using adaboost improved fast PCA algorithm", "author": ["K.S. Kumar", "V.B. Semwal", "R.C. Tripathi"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Sports Video Summarization using Priority Curve Algorithm", "author": ["K.S. Kumar", "S. Prasad", "S. Banwral", "V.B. Semwal"], "venue": "International Journal", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Generating 3D Model Using 2D Images of an Object", "author": ["K.S. Kumar", "V.B. Semwal", "S. Prasad", "R.C. Tripathi"], "venue": "International Journal of Engineering Science", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Accurate location estimation of moving object with energy constraint & adaptive update algorithms to save data,", "author": ["V.B. Semwal", "K.S. Kumar", "V.S. Bhaskar", "M. Sati"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Human Activity Recognition using Gait Pattern", "author": ["J.P. Gupta", "N. Singh", "P. Dixit", "V.B. Semwal", "S.R. Dubey"], "venue": "International Journal of Computer Vision and Image Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A fault-tolerant mobile computing model based on scalable replica", "author": ["M Sati"], "venue": "International Journal of Interactive Multimedia and Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "KNN based Machine Learning Approach for Text and Document Mining, International", "author": ["V Bijalwan"], "venue": "Journal of Database Theory and Application,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Instant Face detection and attributes recognition, International Journal of Advanced Computer Science and Applications.(2011)", "author": ["P Kumari"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A Comparative study of Machine Learning algorithms for Emotion State Recognition through Physiological signal, Advances in Intelligent Systems and Computing, Vol.236-Springer; ISBN 978-81-322-1601-8.(2011)", "author": ["P Kumari"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Brainwave\u201fs Energy feature Extraction using wavelet Transfor, IEEE SCEECS,(2014)", "author": ["P Kumari"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Use of ARIMA mathematical analysis to model the implementation of expert system courses by means of free software OpenSim and Sloodle platforms in virtual university campuses", "author": ["Rub\u00e9n G.C"], "venue": "Expert Systems with Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "The existing text classification methods can be classified into below six [7], [8], [9] categories:", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "The existing text classification methods can be classified into below six [7], [8], [9] categories:", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Among the six types the survey aims in getting an intuitive understanding of KNN approach in which the application of various Machine Learning Techniques [11, 17, 18, 20, 21] to the text categorization problem like in the field of medicine, e-mail filtering, including rule learning for knowledge base systems has been explored.", "startOffset": 154, "endOffset": 174}, {"referenceID": 14, "context": "Among the six types the survey aims in getting an intuitive understanding of KNN approach in which the application of various Machine Learning Techniques [11, 17, 18, 20, 21] to the text categorization problem like in the field of medicine, e-mail filtering, including rule learning for knowledge base systems has been explored.", "startOffset": 154, "endOffset": 174}, {"referenceID": 15, "context": "Among the six types the survey aims in getting an intuitive understanding of KNN approach in which the application of various Machine Learning Techniques [11, 17, 18, 20, 21] to the text categorization problem like in the field of medicine, e-mail filtering, including rule learning for knowledge base systems has been explored.", "startOffset": 154, "endOffset": 174}, {"referenceID": 17, "context": "Among the six types the survey aims in getting an intuitive understanding of KNN approach in which the application of various Machine Learning Techniques [11, 17, 18, 20, 21] to the text categorization problem like in the field of medicine, e-mail filtering, including rule learning for knowledge base systems has been explored.", "startOffset": 154, "endOffset": 174}, {"referenceID": 18, "context": "Among the six types the survey aims in getting an intuitive understanding of KNN approach in which the application of various Machine Learning Techniques [11, 17, 18, 20, 21] to the text categorization problem like in the field of medicine, e-mail filtering, including rule learning for knowledge base systems has been explored.", "startOffset": 154, "endOffset": 174}, {"referenceID": 16, "context": "Information retrieval is also used in image retrieval [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": "In recent works, to save and estimate accurate location moving object with energy constraint is proposed in [10, 14] using adaptive update algorithms.", "startOffset": 108, "endOffset": 116}, {"referenceID": 11, "context": "In recent works, to save and estimate accurate location moving object with energy constraint is proposed in [10, 14] using adaptive update algorithms.", "startOffset": 108, "endOffset": 116}, {"referenceID": 9, "context": "Some other recent approaches such as video summarization [12], 3D model of 2D image [13], gait pattern [15], and scale replica model [16] can also be integrated with the proposed approach to enhance the efficiency.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Some other recent approaches such as video summarization [12], 3D model of 2D image [13], gait pattern [15], and scale replica model [16] can also be integrated with the proposed approach to enhance the efficiency.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Some other recent approaches such as video summarization [12], 3D model of 2D image [13], gait pattern [15], and scale replica model [16] can also be integrated with the proposed approach to enhance the efficiency.", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "Some other recent approaches such as video summarization [12], 3D model of 2D image [13], gait pattern [15], and scale replica model [16] can also be integrated with the proposed approach to enhance the efficiency.", "startOffset": 133, "endOffset": 137}, {"referenceID": 0, "context": "The Naive Bayes classifier found its way into many applications nowadays due to its simple principle but yet powerful accuracy [2], [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "The Naive Bayes classifier found its way into many applications nowadays due to its simple principle but yet powerful accuracy [2], [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "The term graph model is an improved version of the vector space model [6] by weighting each term according to its relative \u201cimportance\u201d with regard to t erm associations.", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "While this idea has been explored by previous research [3], our approach distinguishes from previous approaches in that we maintain all such important associations in a graph.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "The initial application of k-Nearest Neighbors (KNN) to text categorization was reported in [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "The data set used for this paper is in the form of sgml files [3].", "startOffset": 62, "endOffset": 65}], "year": 2014, "abstractText": "Text Categorization (TC), also known as Text Classification, is the task of automatically classifying a set of text documents into different categories from a predefined set. If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task. TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers. In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents.", "creator": "Microsoft\u00ae Office Word 2007"}}}