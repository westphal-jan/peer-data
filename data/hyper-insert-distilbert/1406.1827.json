{"id": "1406.1827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Recursive Neural Networks Can Learn Logical Semantics", "abstract": "generalized supervised recursive constraint neural network models ( aka rnns ) challenging for sentence meaning have been ultimately successful in an array configuration of sophisticated language tasks, but it remains an even open question whether they can learn rational compositional semantic grammars that support the logical type deduction. we simultaneously address simultaneously this controversial question first directly by typing for the first time evaluating whether each of two semantic classes of neural model - - - plain rnns and dynamic recursive neural tensor modeling networks ( rntns ) - - - can therefore correctly learn basic relationships such quantities as entailment and infinite contradiction between normal pairs of sentences, where we both have generated controlled data sets far of inference sentences from encoding a distinct logical algebraic grammar. our 2013 first test experiment intensely evaluates whether precisely these models anyone can learn besides the basic algebra modeling of logical relations currently involved. our second and current third scientific experiments critically extend this hypothesis evaluation to further complex logic recursive language structures \u2013 and constraint sentences involving quantification. thereby we find that if the smaller plain only rnn prototype achieves only mixed results on all three available experiments, whereas the much stronger independent rntn model generalizes well in every setting and instead appears capable of learning suitable matrix representations for supporting natural natural language alongside logical inference.", "histories": [["v1", "Fri, 6 Jun 2014 22:09:27 GMT  (112kb)", "http://arxiv.org/abs/1406.1827v1", null], ["v2", "Sun, 14 Dec 2014 20:37:33 GMT  (153kb)", "http://arxiv.org/abs/1406.1827v2", null], ["v3", "Tue, 3 Mar 2015 19:48:45 GMT  (102kb,D)", "http://arxiv.org/abs/1406.1827v3", null], ["v4", "Thu, 14 May 2015 19:37:38 GMT  (119kb,D)", "http://arxiv.org/abs/1406.1827v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["samuel r bowman", "christopher potts", "christopher d manning"], "accepted": false, "id": "1406.1827"}, "pdf": {"name": "1406.1827.pdf", "metadata": {"source": "CRF", "title": "Recursive Neural Networks for Learning Logical Semantics", "authors": ["Samuel R. Bowman", "Christopher D. Manning"], "emails": ["sbowman@stanford.edu", "cgpotts@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n18 27\nv1 [\ncs .C\nL ]\n6 J\nun 2"}, {"heading": "1 Introduction", "text": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4]. These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move. To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12]. For robust natural language understanding, it is essential to model these phenomena in their full generality on complex linguistic structures.\nWe address this question in the context of natural language inference (also known as recognizing textual entailment; [13]), in which the goal is to determine the core inferential relationship between two sentences. Much of the theoretical work on this task (and some successful implemented models [14, 15]) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language. Following [16], we use the natural logic developed by [14] as our formal model. This logic defines seven core relations of synonymy, entailment, contradiction, and mutual consistency, as summarized in Table 1, and it provides rules of semantic combination for projecting these relations from the lexicon up to complex phrases. The formal properties and inferential strength of this system are now well-understood [17, 18].\nIn our experiments, we use this pre-specified logical grammar to generate controlled data sets encoding the semantic relationships between pairs of expressions and evaluate whether each of two\nclasses of neural model \u2014 plain RNNs and recursive neural tensor networks (RNTNs, [2]) \u2014 can learn those relationships correctly. In our first experiment (Section 3), we evaluate the ability of these models to learn the core relational algebra of natural logic from data. Our second experiment (Section 4) extends this evaluation to relations between complex recursive structures like (a or b) and not(nota and not b), and our third experiment (Section 5) involves relations between quantified statements like every reptile walks and no turtle moves. We find that the plain RNN achieves only mixed results in all three experiments, whereas the stronger RNTN model generalizes well in every case, suggesting that it has in fact learned, or at least learned to simulate, our target logical concepts. These experiments differentiate the increased power of RNTNs better than previous work and provide the most convincing demonstration to date of the ability of neural networks to model semantic inferences in complex natural language sentences."}, {"heading": "2 Recursive neural network models", "text": "We study neural models that adhere to the linguistic principle of compositionality, which says that the meanings for complex expressions are derived from the meanings of their constituent parts via specific composition functions [19, 20]. In our distributed setting, word meanings are vectors of length n. The composition function maps pairs of them to single vectors of length n, which can then be merged again to represent more complex phrases. Once the entire sentence-level representation has been derived, it serves as a fixed-length input for some subsequent function.\nWe use the model architecture proposed in [16] and depicted in Figure 1. The two phrases being compared are built up separately on each side of the tree, using the same composition function, until they have each been reduced to single vectors. The resulting vectors are fed into a separate comparison layer that is meant to generate a feature vector capturing the relation between the two phrases. The output of this layer is then given to a softmax classifier, which in turn produces a hypothesized distribution over the seven relations represented in Table 1.\nFor a composition layer, we evaluate models with both the plain neural network layer function (1) and the RNTN layer function (2) proposed in Chen et al. [21]. A sigmoid nonlinearity (element-wise tanh) is applied to the output of either layer function, following [2].\n~yRNN = f(M[~x (l); ~x(r)] +~b)(1)\n~yRNTN = f(~x (l)T T [1...n]~x(r) +M[~x(l); ~x(r)] +~b)(2)\nHere, ~x(l) and ~x(r) are the column vector representations for the left and right children of the node, and ~y is the node\u2019s output. The RNN concatenates them, multiplies them by an n \u00d7 2n matrix of learned weights, and applies the element-wise non-linearity to the resulting vector. The RNTN has the same basic structure, but with the addition of a learned third-order tensor T, dimension n\u00d7n\u00d7n, modeling multiplicative interactions between the child vectors. Both models include a bias vector~b.\nThe comparison layer uses the same kind of function template as the composition layers (either an NN layer or an NTN layer) with independently learned parameters and a separate nonlinearity function. Rather than use a tanh nonlinearity here, we follow [16] in using a rectified linear function. In particular, we use the leaky rectified linear function [22]: f(~x) = max(~x, 0) + 0.01min(~x, 0), applied element-wise.\nTo run the model forward and label a pair of phrases, the structure of the lower layers of the network is assembled so as to mirror the tree structures provided for each phrase. The word vectors are then looked up from the vocabulary matrix V (one of the model parameters), and the composition and comparison functions are used to pass information up the tree and into the classifier at the top. For an objective function, we use the negative log of the probability assigned to the correct label.\nWe train the model using stochastic gradient descent (SGD) with learning rates computed using AdaGrad [23]. The parameters (including the vocabulary) are initialized randomly using a uniform distribution over [\u22120.1, 0.1]. Because the seven classes are not balanced in general, we report performance using both accuracy and macroaveraged F1, since the latter emphasizes performance on infrequent classes. We compute macroaveraged F1 as the harmonic mean of mean precision and mean recall, both computed for all classes for which there is test data, setting precision to 0 where it is not defined. Source code and generated data will be released after the review period."}, {"heading": "3 Reasoning about semantic relations", "text": "If a model is to learn the behavior of a relational logic like the one presented here from a finite amount of data, it must learn to deduce new relations from seen relations in a sound manner. The simplest such deductions involve atomic statements using the relations in Table 1. For instance, given that a \u2290 b and b \u2290 c, one can conclude that a \u2290 c, by basic set-theoretic reasoning (transitivity of \u2290). Similarly, from a \u228f b and b \u2227 c, it follows that a | c. The full set of sound inferences of this form is summarized in Table 2; cells containing a dot correspond to pairs of relations for which no valid inference can be drawn in our logic.\nExperiments To test our models\u2019 ability to learn these inferential patterns, we create small boolean structures for our logic in which terms denote sets of entities from a small domain. Figure 2 depicts a structure of this form. The lattice gives the full model, for which all the statements on the right\nare valid. We divide these statements evenly into training and test sets, and remove from the test set those statements which cannot be proven from the training statements.\nIn our experiments, we create 80 randomly generated sets drawing from a domain of seven elements. This yields a data set consisting of 6400 statements about pairs of formulae. 3200 of these pairs are chosen as a test set, and that test set is further reduced to the 2960 examples that can be provably derived from the training data.\nWe trained versions of both the RNN model and the RNTN model on these data sets. In both cases, the models were implemented exactly as described in Section 2, but since the items being compared are single terms rather than full tree structures, the composition layer was not involved, and the two models differed only in which layer function was used for the comparison layer. We simply present the models with two single terms, each of which corresponds to a single vector in the (randomly initialized) vocabulary matrix V , thereby ensuring that the model has no information about the terms being compared except the relations between them.\nResults We found that the RNTN model worked best with 11-dimensional vector representations for the 80 sets and a 90-dimensional feature vector for the classifier, though the performance of the model was not highly sensitive to either dimensionality parameter in any of the experiments discussed here. This model was able to correctly label 99.4% (94.3 macroaveraged F1) of the provably derivable test examples, and 95.4% (92.2 macro-F1) of the remaining test examples. The simpler RNN model worked best with 11 and 75 dimensions, respectively, but was able to achieve accuracies of only 89.0% (80.7 macro-F1) and 62.5% (40.7 macro-F1), respectively. Training accuracy was 99.8% for the RNTN and 95.4% for the RNN.\nThese results are fairly straightforward to interpret. The RNTN model was able to accurately encode the relations between the terms in the geometric relations between their vectors, and was able to then use that information to recover relations that were not overtly included in the training data. In contrast, the RNN model was able to achieve this behavior only incompletely. It is possible but not likely that it could be made to find a good solution with further optimization on different learning algorithms, or that it would do better on a larger universe of sets, for which there would be a larger set of training data to learn from, but the RNTN is readily able to achieve these effects in the setting discussed here."}, {"heading": "4 Recursive structure in propositional logic", "text": "Recursive structure is a prominent feature of natural languages and an important part of their expressivity, as it allows people to use and interpret complex structures they have never encountered before.\nTheoretical accounts of natural language syntax and semantics therefore rely heavily on recursive structures, and we expect that accurate computational models will have to come to grips with them as well. The present section pursues this question for our two classes of RNN. In our evaluations, we exploit the fact that our logical language is infinite by testing on strings that are longer and more complex than any seen in training.\nExperiments As in Section 3, we test this phenomenon within the framework of natural logic, but we now replace the unanalyzed symbols from that experiment with complex formulae. These formulae represent a truth-functionally complete classical propositional logic: each atomic variable denotes either T or F, and the only operators are truth-functional ones. Table 3a defines this logic, and Table 3b gives some examples of relational statements that we can make in these terms. To compute these relations between statements, we exhaustively enumerate the sets of assignments of truth values to propositional variables that would satisfy each of the statements, and then we convert the set-theoretic relation between those assignments into one of the seven relations in Table 1. As a result, each relational statement represents a valid theorem of the propositional logic.\nSocher et al. [24] show that a matrix-vector RNN model somewhat similar to our RNTN can learn a logic, but the logic discussed here is a much better approximation of the kind of expressivity needed for natural language. The logic learned in that experiment is boolean, wherein the atomic symbols are simply the values 0 and 1, rather than variables over those values. While learning the operators of that logic is not trivial, the outputs of each operator can be represented accurately by a single bit. The statements of propositional logic learned here describe conditions on unknown truth values of propositions. As opposed to the two-way contrasts seen in [24], this logic distinguishes between 26 = 64 possible assignments of truth values, and expressions of this logic define arbitrary conditions on these possible assignments, for a total of 264 possible statements that the intermediate vector representations need to be able to distinguish if we are to learn this logical system in its full generality.\nFor our experiments, we randomly generate a large set of unique pairs of formulae and compute the relation that holds for each pair. We discard pairs in which either statement is a tautology or contradiction (a statement that is true of either all or no possible assignments), for which none of the seven relation labels in Table 1 can hold. The resulting set of formula pairs is then partitioned into bins based on the number of logical operators in the longer of the two formulae. We then randomly sample 15% of each bin for a held-out test set.\nIf we do not implement any constraint that the two statements being compared are similar in any way, then the generated data are dominated by statements in which the two formulae refer to largely separate subsets of the six variables, which means that the # relation is almost always correct. In an effort to balance the distribution of relation labels without departing from the basic task of modeling propositional logic, we disallow individual pairs of statements from referring to more than four of the six propositional variables.\nResults We trained both the RNN and RNTN models on data with up to 4 connectives (65k pairs) and tested it on examples of up to 12 connectives (44k pairs). We initialized the model parameters randomly, including the vector representations of the six variables. The results are shown in Figure 3. In tuning, we found that the RNN model was approximately optimal with 45-dimensional vector representations, and the RNTN model was approximately optimal with 25 dimensions. We fixed\nthe size of the feature vector for the classifier at 75 dimensions. We found that the RNTN model was able to perform almost perfectly on unseen small test examples, with accuracy above 98.7% on formulae below length four (macro-F1 was above 95.5). Starting at size four, performance gradually falls with increasing size. The RNN model did not perform well, reaching only 90.5% (81.4 microF1) accuracy on the smallest test examples, and declining from there to near-baseline performance on formulae of length 12. Training accuracy was 99.6% for the RNTN and 87.6% for the RNN.\nThe performance of the RNTN model on small unseen test examples indicates that it learned a correct approximation of the underlying logic. It appears that this approximation is accurate enough to yield correct answers when the composition layer is only applied a small number of times, but that the error in the approximation grows with increasing depth (and with the increasing complexity of the expressions), resulting in gradually dropping performance. In contrast, the RNN model did not appear to be able to learn a correct approximation of the logic for statements of any length, but it appears that the faulty logic that the RNN did learn decayed similarly with expression size. This is not necessarily a significant flaw in the model, since it remains possible that different training regimes would allow it to learn accurate representations of the larger expressions.\nIn an effort to better understand why the RNTN outperformed the plain RNN, we evaluated both models on pairs of long formulae involving binary connectives, to assess the extent to which they achieve substantially different representations for semantically distinct formulae and substantially similar representations for synonymous formulae. We found that both models are able to identify synonymous pairs like (a and a) and (a and (a and (a and a))), with both and and or, but only the RNTN is able to learn substantially different representations for pairs of differing formulae like (a and (a and a)) and (a and (a and b)) when the difference between the two is embedded under multiple operators. Figure 4 summarizes this result."}, {"heading": "5 Reasoning with natural language quantifiers and negation", "text": "We have seen that the RNTN can learn an approximation of propositional logic. However, natural languages can express functional meanings of considerably greater complexity than this. As a first step towards investigating whether our models can capture this complexity, we now attempt to directly measure the degree to which they are able to develop suitable representations for the semantics of natural language quantifiers like some and all. Quantification is far from the only place in natural language where complex functional meanings are found, but it is a natural starting point, since it can be tested in sentences whose structures are otherwise quite simple, and since it has formed a standard case study in prior formal work on natural language inference [18].\nThis experiment replicates similar work described in [16], which found that RNTNs can learn to reason well with quantifier meanings given sufficient training data. This paper replaces the partially\nmanually annotated data in that paper with data that is generated directly using the logical system that we hope to learn, yielding results that we believe to be substantially more straightforward to interpret.\nExperiments Our experimental data consist of pairs of sentences generated from a small artificial grammar. Each sentence contains a quantifier, a noun which may be negated, and an intransitive verb which may be negated. We use the basic quantifiers some, most, all, two, and three, and their negations no, not-all, not-most, less-than-two, and less-than-three. We also include five nouns, four intransitive verbs, and the negation symbol not. In order to be able to define relations between sentences with differing lexical items, we define the lexical relations between each noun\u2013noun pair, each verb\u2013verb pair, and each quantifier\u2013quantifier pair. The grammar accepts aligned pairs of sentences of this form and calculates the natural logic relationship between them. Some examples of these data are provided in Table 4. As in previous sections, the goal of learning is then to assign these relational labels accurately to unseen pairs of sentences.\nWe evaluate the model using two experimental settings. In the simpler setting, ALL SPLIT, we randomly sample 85% of the data and evaluate on the remaining 15%. In this setting, the model is being asked to learn a complete reasoning system for the limited language and logic presented in the training data, but it is not being asked to generalize to test examples that are substantially different from those it was trained on. Crucially, though, to succeed on this task, the model must be able to recognize all of the lexical relations between the nouns, verbs, and quantifiers and how they interact. For instance, it might see (3) and (4) in training and, from that information, determine (5).\n(most turtle) swim | (no turtle) move(3)\n(all lizard) reptile \u228f (some lizard) animal(4)\n(most turtle) reptile | (no turtle) animal(5)\nWhile our primary interest is in discovering the extent to which our models can learn to encode the logic given an arbitrary amount of data, we are also interested in the degree to which they can infer a correct representation for the logic from more constrained training data. To this end, we segment the sentence pairs according to which quantifiers appear in each pair, and then hold out one such\npair for testing. We hypothesize that a model that can efficiently learn to represent a logic should be able to construct an accurate representation of each of the two quantifiers in the pair from the way that it interacts with the other nine quantifiers with which it is seen in training. Since running this experiment requires choosing a pair of quantifiers to hold out before training, the resource demands of training prevent us from testing each of the 55 possible pairs of quantifiers, so we choose only four pairs to test on. Three of these (two/less-than-two, not-all/not-most, and all/some) were chosen because they allow for the most different class labels in the test data. The fourth is a self-pair (no/no), meant to test that the model correctly handles equality.\nResults The results for these experiments are shown in Figure 5. We compare the results to a most frequent class baseline, which reflects the frequency in the test data of the most frequent class in the training data, #. After some cross-validation, we chose 16- and 20-dimensional representations for the RNN and RNTN, respectively, and 75-dimensional feature vectors for the classifier. Training accuracy was near 80% for the each of the five RNN experiments, and near 98% for each of the five RNTN experiments.\nThe RNN performed poorly at this task, even though the sentences used in these examples are short enough to avoid the pathology shown in Figure 4. However, the perfect performance by the RNTN on the ALL SPLIT and PAIR TWO/LESS-THAN-TWO experiments, and its strong performance on the PAIR NO/NO experiment, suggest that this stronger model is able to handle quantifiers correctly given sufficient training data, but the weaker results on two of the training settings suggest that it may not be able to generalize from this kind of restricted training data in general. However, though the question of how much data is necessary to accurately capture quantifier behavior in a na\u0131\u0308ve model remains open, the fact that this model is able to perform so well is promising."}, {"heading": "6 General discussion", "text": "This paper evaluated two recursive models on a series of three increasingly challenging interpretive tasks involving natural language inference: the core relational algebra of natural logic with entailment and exclusion; recursive propositional logic structures; and statements involving quantification and negation. The results suggest that RNTNs, but not plain RNNs, have the capacity to meet the challenges of these tasks with reasonably-sized training sets. These positive results are promising for the future of learned representation models in the applied modeling of compositional semantics.\nOf course, challenges remain. In terms of our experimental data, even the RNTN falls short of perfection in our more complex tasks, with performance falling off steadily as the depth of recursion grows. It remains to be seen whether these deficiencies can be overcome with improvements to the model, the optimization procedures, or the linguistic representations [3, 12]. In addition, there remain subtle questions about how to fairly assess whether these models have truly generalized in the way we want them to. There is a constant tension between showing the models training data that gives them a chance to learn the target logical functions and revealing the answer to them in a way that leads to overfitting. The underlying logical theories provide only limited guidance on this point. Finally, we have only scratched the surface of the logical complexity of natural language; in future experiments, we hope to test sentences with embedded quantifiers, multiple interacting quantifiers, relative clauses, and other kinds of recursive structure. Nonetheless, the rapid progress the field has made with these models in recent years provides ample reason to be optimistic that they can be trained to meet the challenges of natural language semantics."}, {"heading": "Acknowledgments", "text": "We thank Jeffrey Pennington, Richard Socher, and audiences at CSLI and Nuance."}], "references": [{"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "TACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "An efficient easily adaptable system for interpreting natural language queries", "author": ["David H.D. Warren", "Fernando C.N. Pereira"], "venue": "American Journal of Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1982}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M. Zelle", "Raymond J. Mooney"], "venue": "In Proceedings of AAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Concrete sentence spaces for compositional distributional models of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh", "Stephen Clark", "Bob Coecke", "Stephen Pulman"], "venue": "In Proceedings of IWCS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In Proceedings of EACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "An extended model of natural logic", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of IWCS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A latent discriminative model for compositional entailment relation recognition using natural logic", "author": ["Yotaro Watanabe", "Junta Mizuno", "Eric Nichols", "Naoaki Okazaki", "Kentaro Inui"], "venue": "In Proceedings of COLING,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Can recursive neural tensor networks learn logical reasoning? arXiv preprint arXiv:1312.6192", "author": ["Samuel R. Bowman"], "venue": "Presented at ICLR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A complete calculus of monotone and antitone higher-order functions", "author": ["Thomas F. Icard", "Lawrence S. Moss"], "venue": "Proceedings of Topology, Algebra, and Categories in Logic,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Recent progress on monotonicity", "author": ["Thomas F. Icard", "Lawrence S. Moss"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Partee. Compositionality", "author": ["H. Barbara"], "venue": "Varieties of Formal Semantics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1984}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In Proceedings of ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 166, "endOffset": 172}, {"referenceID": 1, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 166, "endOffset": 172}, {"referenceID": 2, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis [1, 2], image description [3], and paraphrase detection [4].", "startOffset": 222, "endOffset": 225}, {"referenceID": 4, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 5, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 6, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 7, "context": "These results are encouraging for the ability of these models to learn compositional semantic grammars, but it remains an open question whether they can achieve the same results as grammars based in logical forms [5, 6, 7, 8] when it comes to the core semantic concepts of quantification, entailment, and contradiction needed to identify the relationships between sentences like every animal walks, every turtle moves, and most reptiles don\u2019t move.", "startOffset": 213, "endOffset": 225}, {"referenceID": 8, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 9, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 10, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 11, "context": "To date, experimental investigations of these concepts using distributed representations have been largely confined to short phrases [9, 10, 11, 12].", "startOffset": 133, "endOffset": 148}, {"referenceID": 12, "context": "We address this question in the context of natural language inference (also known as recognizing textual entailment; [13]), in which the goal is to determine the core inferential relationship between two sentences.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Much of the theoretical work on this task (and some successful implemented models [14, 15]) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language.", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": "Much of the theoretical work on this task (and some successful implemented models [14, 15]) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language.", "startOffset": 82, "endOffset": 90}, {"referenceID": 15, "context": "Following [16], we use the natural logic developed by [14] as our formal model.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Following [16], we use the natural logic developed by [14] as our formal model.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "The formal properties and inferential strength of this system are now well-understood [17, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "The formal properties and inferential strength of this system are now well-understood [17, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 13, "context": "Table 1: The seven natural logic relations of [14].", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "classes of neural model \u2014 plain RNNs and recursive neural tensor networks (RNTNs, [2]) \u2014 can learn those relationships correctly.", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "We study neural models that adhere to the linguistic principle of compositionality, which says that the meanings for complex expressions are derived from the meanings of their constituent parts via specific composition functions [19, 20].", "startOffset": 229, "endOffset": 237}, {"referenceID": 15, "context": "We use the model architecture proposed in [16] and depicted in Figure 1.", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "A sigmoid nonlinearity (element-wise tanh) is applied to the output of either layer function, following [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "Rather than use a tanh nonlinearity here, we follow [16] in using a rectified linear function.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "In particular, we use the leaky rectified linear function [22]: f(~x) = max(~x, 0) + 0.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "We train the model using stochastic gradient descent (SGD) with learning rates computed using AdaGrad [23].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "[24] show that a matrix-vector RNN model somewhat similar to our RNTN can learn a logic, but the logic discussed here is a much better approximation of the kind of expressivity needed for natural language.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "As opposed to the two-way contrasts seen in [24], this logic distinguishes between 2 = 64 possible assignments of truth values, and expressions of this logic define arbitrary conditions on these possible assignments, for a total of 2 possible statements that the intermediate vector representations need to be able to distinguish if we are to learn this logical system in its full generality.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Quantification is far from the only place in natural language where complex functional meanings are found, but it is a natural starting point, since it can be tested in sentences whose structures are otherwise quite simple, and since it has formed a standard case study in prior formal work on natural language inference [18].", "startOffset": 321, "endOffset": 325}, {"referenceID": 15, "context": "This experiment replicates similar work described in [16], which found that RNTNs can learn to reason well with quantifier meanings given sufficient training data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "It remains to be seen whether these deficiencies can be overcome with improvements to the model, the optimization procedures, or the linguistic representations [3, 12].", "startOffset": 160, "endOffset": 167}, {"referenceID": 11, "context": "It remains to be seen whether these deficiencies can be overcome with improvements to the model, the optimization procedures, or the linguistic representations [3, 12].", "startOffset": 160, "endOffset": 167}], "year": 2014, "abstractText": "Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, but it remains an open question whether they can learn compositional semantic grammars that support logical deduction. We address this question directly by for the first time evaluating whether each of two classes of neural model \u2014 plain RNNs and recursive neural tensor networks (RNTNs) \u2014 can correctly learn relationships such as entailment and contradiction between pairs of sentences, where we have generated controlled data sets of sentences from a logical grammar. Our first experiment evaluates whether these models can learn the basic algebra of logical relations involved. Our second and third experiments extend this evaluation to complex recursive structures and sentences involving quantification. We find that the plain RNN achieves only mixed results on all three experiments, whereas the stronger RNTN model generalizes well in every setting and appears capable of learning suitable representations for natural language logical inference.", "creator": "LaTeX with hyperref package"}}}