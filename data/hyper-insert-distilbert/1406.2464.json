{"id": "1406.2464", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Music and Vocal Separation Using Multi-Band Modulation Based Features", "abstract": "realizing the potential correct use of a non - linear speech matrix features has but not intentionally been primarily investigated for music analysis application although other commonly used speech features like hybrid mel tuned frequency measurement ceptral coefficients ( mfcc ) and pitch have regularly been used statistical extensively. detailed in this paper, we assume an audio signal to be matching a sum difference of audio modulated frequencies sinusoidal and then use the sequential energy separation algorithm to decompose the audio into amplitude and phase frequency modulation components. using the basic non - linear teager - lange kaiser energy operator. often we say first identify the distribution of separating these non - additive linear features convenient for designing music only division and voice only segments in applying the audio contrast signal but in different intervals mel multiple spaced frequency coding bands and show why that there they have proved the ability to discriminate. the partially proposed method based roughly on standardized kullback - leibler divergence measure is evaluated using a set of exclusively indian classical songs downloaded from representing three different artists. experimental results show that determining the discrimination mapping ability technique is probably evident indirectly in reaching certain low and mid specified frequency bands ( 200 - 200 1500 - hz ).", "histories": [["v1", "Tue, 10 Jun 2014 08:29:58 GMT  (103kb,D)", "http://arxiv.org/abs/1406.2464v1", "5 pages, 5 figures, 2010 IEEE Symposium on Industrial Electronics Applications (ISIEA)"]], "COMMENTS": "5 pages, 5 figures, 2010 IEEE Symposium on Industrial Electronics Applications (ISIEA)", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["sunil kumar kopparapu", "meghna pandharipande", "g sita"], "accepted": false, "id": "1406.2464"}, "pdf": {"name": "1406.2464.pdf", "metadata": {"source": "CRF", "title": "Music and Vocal Separation Using Multi-Band Modulation Based Features", "authors": ["Sunil Kumar Kopparapu", "Meghna Pandharipande"], "emails": ["Meghna.Pandharipande}@TCS.Com"], "sections": [{"heading": null, "text": "Key Words: Music Voice Separation, Music discrimination, modulation features\nI. INTRODUCTION Increased availability and use of large digital music corpora, requires intelligent music management systems. This has resulted in the development of a variety of intelligent music content management systems. Automatic segmentation of song into vocal and music regions is a very important step for several applications like singer identification, musical instrument analysis, preference based searches of music, to name a few. Audio search, annotation and browsing applications also benefit greatly with automatic music voice segmentation. Early work in this area includes the work carried out by Berenzweig and Ellis [1] where they suggest the use of an artificial neural network (ANN) to train on radio recordings to segment songs into vocal and non-vocal (music) regions. Kim and Whitman [2] filter the audio signal using a bandpass filter and then used hormonicity as a measure to detect vocal regions and separate them from music. More recently, Sridhar and Geetha [3], identified swaras in South Indian classical music by finding the pitch for a particular segment which in turn gives information about middle octave swara. Ramona et al [4] use support vector machine to separate singing voice from pure instrumental region while Demir and others [5] use hidden Markov model (HMM) based acoustic models to calculate posterior probabilities to segment an audio signal as speech and music. Zhou [6] discriminates voice and music using novel spectral feature like averaged cepstrum. and Kos et al [7] on-line segment speech music for broadcast news domain using Mel-Frequency Cepstral Coefficients Variance (MFCCV). Barbedo and others [8] propose a mechanism to\ndiscriminate speech and music signal by extracting four features and then combining them linearly into a unique parameter. Didiot [9] propose a wavelet based signal decomposition instead of Fourier Transform for discriminating speech and music. In most of the work cited above the use of conventional speech features is apparent because of processing audio using linear system theory. In this paper, we propose and investigate the use of non-linear feature set to discriminate speech and music. Non-linear speech models attempt to model the spectral variability of the speech signal and decompose it into amplitude modulation (AM) and frequency modulation (FM) components. Modulation of the amplitude and/or frequency of a sine wave has been used extensively in communication systems for transmitting information [10], [11].\nUse of nonlinear analysis for speech processing [12] has of late received attention specifically for speaker recognition, speech analysis, voice pathologies, speech recognition and speech enhancement. Specifically, decomposing a nonstationary, bandpass signal into its AM and FM components has been addressed by many researchers and a number of techniques have been published in literature. The most popular approaches are based on the auditory motivated decomposition proposed by Quatieri et al [10] and Teager energy based algorithms proposed by Dimitrios et al [13]. Features derived using non-linear speech framework could reveal the potential of alternative speech models in various speech applications such as speaker identification [14], vocal fold pathology assessment [15] and even emotion classification [16]. Dimitrios et al [17] have used these AM-FM features for phoneme classification and speech recognition tasks [18]. They concluded that these non-linear speech features could be efficiently used in speech classification and recognition tasks.\nIn this paper, we use Teager energy based algorithm [13] to obtain modulation based features from an audio stream. These features are then used in a supervised learning scheme for segment-wise discrimination of vocal and music component in an audio stream. We first analyze the signal to obtain the instantaneous frequency distribution in a number of Mel scale signified frequency bands for the entire duration of the audio stream. The task of identifying an audio segment as being vocal or music is determined by measuring the well known Kullback-Leibler divergence between the distributions of reference music and vocal regions with the corresponding distributions of the test audio stream. The rest of the paper is organized as follows: Section II explains the theory of nonlinear speech modeling and feature extraction method, Section\nar X\niv :1\n40 6.\n24 64\nv1 [\ncs .S\nD ]\n1 0\nJu n\n20 14\nIII explains about the data and the methodology used in our experiments. We conclude in Section IV."}, {"heading": "II. MODULATION BASED FEATURE EXTRACTION", "text": "Audio signal x(t) is non-linear, time-varying and can be looked upon as a AM-FM model as follows (as mentioned in [13])\nx(t) = a(t) cos (\u03c6(t)) (1)\nwhere, a(t) is the time varying amplitude and \u03c6(t) is defined as\n\u03c6 (t) = \u03c9ct+ \u03c9m \u222b t 0 q (\u03c4) d\u03c4 + \u03b8 (2)\nwhere \u03c9c is the center frequency and \u03c9m is the maximum frequency deviation from the \u03c9c, |q (t)| \u2264 1 and \u03b8 = \u03c6 (0) is some arbitrary constant phase offset. The time varying instantaneous angular frequency \u03c9i id defined as\n\u03c9i (t) def =\nd dt \u03c6 (t) = \u03c9c + \u03c9mq (t) (3)\nNote that (1) has both an AM and FM structure hence we call x(t) an AM-FM signal. It has been shown that this non-linear modeling of speech helps in extraction of robust features for speech [18]. These features measure the amount of amplitude and frequency modulation that exists in the audio signal and attempt to model aspects of speech acoustic information. Further, two different information signals can be simultaneously transmitted in the amplitude a(t) and the frequency \u03c9i(t). Such AM-FM signals are very frequently used in communication systems [10]. The AM-FM model can be used to represent a speech signal s(t) as a sum of AM-FM signals, namely\ns(t) = K\u2211 k=1 ak(t)cos(\u03c6k(t)) (4)\nwhere K is the number of speech formats. Clearly, a(t) and \u03c9i(t) for k = 1, 2, \u00b7 \u00b7 \u00b7K represents the speech signal s(t). Now, given a speech signal over some time interval, the problem is to estimate the amplitude envelope |a(t)| and the instantaneous frequency \u03c9i (t) of each k and at each time t. One of the ways to estimate a(t) and \u03c9i(t), is to first isolate individual resonance by bandpass filtering the speech signal around its formants and then estimating amplitude and frequency modulating signals of each resonance based on an \u201denergy-tracking operator\u201d as described in [10]. The Teager energy operator \u03c8 (TEO) is defined as,\n\u03c8c[x(t)] def =\n[ d\ndt x(t)\n]2 \u2212 x(t) [ d2\ndt2 x(t)\n] (5)\nWhen \u03c8 given by (5) is applied to the bandpass filtered speech signal (1), we get the instantaneous source energy, namely,\n\u03c8[x(t)] \u2248 a2(t)\u03c92i (t) (6)\nIn the discrete form as is applicable to most speech processing systems [19] (5) can be written as\n\u03c8 [x [n]] = x2 [n]\u2212 x [n+ 1]x [n\u2212 1] (7)\nwhere x[n] is the sampled speech signal.\nThe AM-FM demodulation can be achieved by separating the instantaneous energy given in (6) into its amplitude and frequency components. \u03c8 is the main ingredient of the first Energy Separation Algorithm (ESA) developed in [13] and used for signal and speech AM-FM demodulation.\nf [n] \u2248 cos\u22121 ( 1\u2212 \u03c8 [y[n]] + \u03c8 [y[n+ 1]]\n4\u03c8 [x[n]]\n) (8)\nand\n|a[n]| \u2248 \u221a\u221a\u221a\u221a\u221a \u03c8 [x[n]][ 1\u2212 ( 1\u2212 \u03c8[y[n]]+\u03c8[y[n+1]]4\u03c8[x[n]]\n)2] (9) where y[n] = x[n]\u2212 x[n\u2212 1] and f [n] is the FM component at sample n and a[n] is the AM component at sample n. In practice the speech signal is bandpass filtered using Gabor filters because of their optimal time-frequency discriminability [13], namely,\ns (t) = x (t) \u2217 g (t) (10)\nwhere g(t) is given by,\ng(t) = 1\u221a 2\u03c0\u03c3 exp\u2212 t2 2\u03c32 expi(2\u03c0\u03c90t) (11)\nwhere \u03c90 is the center frequency and \u03c3 is the bandwidth of the Gabor filter. In the case of speech or audio signals, a Gabor filter-bank (placed at various critical band frequencies such as formant frequencies or at frequencies determined by Mel-scale) with a narrow bandwidth are used. The extraction of AM-FM components (8) and (9) from the bandpass filtered signal may be carried out using the Teager energy of the filtered signal. The efficiency of non-linear speech features, namely instantaneous modulation features such as instantaneous amplitude and instantaneous frequencies around different resonance frequencies of the speech signal have been studied for various applications in speech processing area such as phoneme classification, speech recognition [17], [18], assessment of vocal fold pathology [15], stress detection [20]. In this paper, we investigate the performance of instantaneous frequency modulation features to automatically discriminate vocal and music dominant regions in an audio track. The hypothesis that the instantaneous modulation feature distribution may be different for vocal and music dominant regions is derived from the observation that the generative sources of voice and music are different. For instance, the singing voice (vocal) is replete with large pitch modulations unlike the music component. Additionally, the voice harmonics in the spectrum are observed to be below 5 kHz where as the music energy is observed to be spread throughout the spectrum up to 10 kHz with certain frequency bands dominated entirely by the music energy."}, {"heading": "III. EXPERIMENTAL RESULTS AND DISCUSSION", "text": "To identify and test the performance of non-linear features for voice music separation, we collected several audio signals which had portions of speech (or voice) and music. Distinct voice and music are an essential part of Indian classical music; so we collected a large set of Indian classical music and stored\nthem in wav format. The collection database consisted of a total length of approximately 475 s of audio stream. The audio was sampled at 22.05 kHz and was manually labeled as V (for voice) or M (for music) using a semi-automatic process and later manually checked for the correctness of classification. We use the Mel spaced Gabor filter-bank [17] to filter the audio into the first three bands. For each of these filtered signal we computed the non-linear instantaneous features. We restricted our analysis to the lower three filter bands as in our preliminary investigations, we found that the discrimination power to segment voice and music is evident in these three bands. Further it was observed that the instantaneous amplitudes in various bands are not discriminative enough for different audio segments. Hence in all our experiments we have not considered instantaneous amplitudes. The three Mel-spaced center frequencies (\u03c90 in (11)) we have analyzed are, 240 Hz, 738 Hz and 1361 Hz with a bandwidth (\u03c3 in (11)) of 200 Hz, 606 Hz and 1246 Hz respectively. These reference audio segments which have been tagged asM and V are first bandpass filtered using Gabor filter-bank (11) at three different \u03c90\u2019s namely, 240, 738 and 1361. Instantaneous frequency components are obtained for each of the filtered signal using (8) and (9). Figure 1 to Figure 6 show the typical distributions for vocal (V) and music (M) segments of the audio. We took the audio signal and extracted the instantaneous frequencies using three different Gabor filters. We then segregated the instantaneous frequencies based on the tags, namely, V and M. These tags are used to get the instantaneous feature distribution for voice and music segments of the audio, for three different bands, namely, band 1 (center frequency \u03c90 = 240), band 2 (center frequency \u03c90 = 738) and band 3 (center frequency \u03c90 = 1361) are shown in Figure 1, Figure 2 and Figure 3 respectively.\nIt can also be seen from the distribution plots that the instantaneous frequency has a very distinct distributions for voice and music segments in all the three frequency bands. Additionally, the instantaneous frequency distribution of similar tags (namely, for V and M) show similar distribution (see Figures 4, 5 and 6). This observation suggested that non-linear speech parameters do have the ability to distinguish voice and\nmusic components very reliably. In all our experiments we use the Kullback-Leibler (KL) divergence metric, namely,\nD(p1(\u03be), p2(\u03be)) = \u222b p1(\u03be) log ( p1(\u03be)\np2(\u03be)\n) d\u03be (12)\nto compare the distance between any two distributions. If p1(\u03be) and p2(\u03be) are two distributions then the distance of the distribution p2(\u03be) from p1(\u03be) is given by (12).\nIn all we had 250 voice and musics segments of which 100 segments had the tag M and 150 segments had the tag V . Is segment was of an average duration of 2 s. The reference instantaneous frequency distribution for the voice and music segments of the audio signal is computed from the reference vocal and music segments for all the three bands. The reference is created using 20% of the segments in each category, namely 20 segments for M and 30 segments for V , and the rest namely 80 segments of M and 120 segments of V which were not part of the reference segments were used to test the performance of the proposed approach to recognize M and V . Using these audio reference ( 20 segments for M and 30 segments for V) we construct the distributions pM and pV . A test audio segment (T ) is taken (which is not part of\nthe audio that has been used to create the reference) and the instantaneous frequency distribution of T is computed, as pT . The distribution of the test segment (pT ) is compared with the distribution of the reference music pM and reference vocal pV distributions using (12). Namely, we compute D(pV , pT ) and D(pM, pT ). If D(pV , pT ) < D(pM, pT ) the T is classified as V , else T is classified as M.\nA 5 fold cross validation was used to arrive at the performance of using non-linear features to discriminate voice and speech. Table I tabulates the 5 fold cross validation experimental results. As it can be seen the use of non-linear features for segmentation of voice and music is able to segment music and voice quite well. The use of MFCC as the features resulted in large number of misrecognitions as compared to the misrecognitions dues to non-linear instantaneous frequency features."}, {"heading": "IV. CONCLUSIONS", "text": "Use of non-linear speech features has not been used for music and voice classification though it has been used in some areas of speech recognition, speaker identification. In this paper we have used the instantaneous frequency calculated over band filtered speech signal to discriminate speech and voice. We first assumed a sum of modulated sinusoidal model for audio signal and investigated the performance of instantaneous frequency modulation feature in discriminating voice and music segments. We used Gabor filters to restrict the analysis to a limited number of carrier frequencies which are nothing but the center frequencies of the bandpass filters. We first observed that the distribution of the instantaneous frequency feature over three bands (centered at 240, 738 and 1361) is able to discriminate voice and music. This observation was exploited to classify the audio stream into music and voice segments. Future work would involve extensive testing of the method with larger data-set for consistency of the results and testing across various genres of music."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank the members of the TCS Innovation Labs - Mumbai for the great working environment."}], "references": [{"title": "Locating singing voice segments to improve artist classification of music", "author": ["A.L. Berenzweig", "D.P.W. Ellis"], "venue": "pp. 21\u201324, Oct. 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Singer identification in popular music recordings using voice coding features", "author": ["Y.E. Kim", "B. Whitman"], "venue": "in Proc. 5th International Conf. on Music Information Retrieval, Nagoya, Japan, Oct. 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Swara indentification for South Indian classical music", "author": ["R. Sridhar", "T.V. Geetha"], "venue": "ICIT Proceedings of the 9th International Conference on Information Technology. IEEE Computer Society, 2006, pp. 143\u2013144.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Vocal detection in music with support vector machines", "author": ["G.R. Mathieu Ramona", "B.David"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference, 2008, pp. 1885\u2013 1888.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Speech-music segmentation system for speech recognition", "author": ["C. Demir", "M. Dogan"], "venue": "Signal Processing and Communications Applications Conference, 2009. SIU 2009. IEEE 17th, 2009, pp. 624\u2013627.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Feature extraction for speech and music discrimination", "author": ["J.R. Huiyu Zhou", "Sadka A.", "B. Univ."], "venue": "Content-Based Multimedia Indexing, 2008. CBMI 2008. International Workshop, 2008, pp. 170\u2013173.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "On-line speech/music segmentation for broadcast news domain", "author": ["M.V.D. Kos", "M. Grasic", "Z K."], "venue": "Systems, Signals and Image Processing, 2009. IWSSIP 2009. 16th International Conference, 2009, pp. 1\u20134.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "A robust and computationally efficient speech/music discriminator", "author": ["L. Barbedo", "Jayme Garcia Arnal", "Amauri"], "venue": "New Paltz,NY, pp. 571\u2013 588, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "A wavelet-based parameterization for speech/music discrimination", "author": ["D.F.E. Didiot", "I. Illina", "O.M. Loria"], "venue": "Computer Speech and Language Volume 24, Issue 2, 2010, pp. 341\u2013357.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Am-fm separation using auditory-motivated filters", "author": ["T.F. Quatieri", "T.E. Hanna", "G.C. O-Leary"], "venue": "IEEE Trans. Speech and Audio Proc., vol. 5, pp. 465\u2013480, Sep. 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Information Transmission, Modulation, and Noise", "author": ["M. Schwartz"], "venue": "New York: McGraw-Hill,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1980}, {"title": "Nonlinear Analyses and Algorithms for Speech Processing, International Conference on Non-Linear Speech Processing, NOLISP", "author": ["M. Fa\u00fandez-Zanuy", "L. Janer-Garc\u0131\u0301a", "A. Esposito", "A. Satu\u00e9-Villar", "J. Roure", "V. Espinosa-Duro", "Eds"], "venue": "Revised Selected Papers, ser. Lecture Notes in Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Energy separation in signal modulations with applications to speech analysis", "author": ["P. Maraso", "J.F. Kaiser", "T.F. Quatieri"], "venue": "IEEE Trans. Signal Proc., vol. 41, pp. 3024\u20133051, 1993.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Speaker identification based on nonlinear speech models", "author": ["S. Wenndt", "S. Shamsander"], "venue": "in 29th Asilomar Conference on Signals, Systems and Computers, 1995, p. 1031.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Vocal fold pathology assessment using am auto-correlation analysis of the teager energy operator", "author": ["L.G.C. John", "J.H.L. Hansen", "J.F. Kaiser"], "venue": "in Fourth Int. Conf. Spoken Language, vol. 2, 1996, pp. 757 \u2013 760.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Emotion classification of mandarin speech based on teo nonlinear features", "author": ["G. Hui", "C. Shanguang", "S. Guangchuan"], "venue": "Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, ACIS International Conference on, vol. 3, pp. 394\u2013398, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Continuous energy demodulation methods and application to speech analysis", "author": ["D. Dimitriadis", "P. Maragos"], "venue": "Speech Communication, vol. 48, pp. 819\u2013837, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust am-fm features for speech recognition", "author": ["D. Dimitriadis", "P. Maragos", "A. Potamianos"], "venue": "IEEE Signal Process. Lett., vol. 12, pp. 621\u2013 624, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonlinear feature based classification of speech under stress", "author": ["J.H.H. Guojun Zhou", "J.F. Kaiser"], "venue": "IEEE Trans. Signal Proc., vol. 9, p. 203, March 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Frequency distribution based weighted sub-band approach for classification of emotional/stressful content in speech", "author": ["R. Mandar", "H. John"], "venue": "in EUROSPEECH-2003, 2003, pp. 721\u2013724.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Early work in this area includes the work carried out by Berenzweig and Ellis [1] where they suggest the use of an", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Kim and Whitman [2] filter the audio signal using a bandpass filter and then used hormonicity as a measure to detect vocal regions and separate them from music.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "More recently, Sridhar and Geetha [3], identified swaras in South Indian classical", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "Ramona et al [4] use support vector machine to separate singing voice from pure instrumental region while Demir and others [5] use hidden Markov model (HMM) based acoustic models to", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Ramona et al [4] use support vector machine to separate singing voice from pure instrumental region while Demir and others [5] use hidden Markov model (HMM) based acoustic models to", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "Zhou [6] discriminates voice and music using novel spectral feature like averaged cepstrum.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "and Kos et al [7] on-line segment speech music for broadcast news domain using Mel-Frequency Cepstral Coefficients Variance (MFCCV).", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "Barbedo and others [8] propose a mechanism to discriminate speech and music signal by extracting four features and then combining them linearly into a unique parameter.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "Didiot [9] propose a wavelet based signal decomposition instead of Fourier Transform for discriminating speech and", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "Modulation of the amplitude and/or frequency of a sine wave has been used extensively in communication systems for transmitting information [10], [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "Modulation of the amplitude and/or frequency of a sine wave has been used extensively in communication systems for transmitting information [10], [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "Use of nonlinear analysis for speech processing [12] has of late received attention specifically for speaker recogni-", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "proposed by Quatieri et al [10] and Teager energy based algorithms proposed by Dimitrios et al [13].", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "proposed by Quatieri et al [10] and Teager energy based algorithms proposed by Dimitrios et al [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "Features derived using non-linear speech framework could reveal the potential of alternative speech models in various speech applications such as speaker identification [14], vocal fold pathology assessment", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "[15] and even emotion classification [16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[15] and even emotion classification [16].", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "Dimitrios et al [17] have used these AM-FM features for phoneme classification and speech recognition tasks [18].", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "Dimitrios et al [17] have used these AM-FM features for phoneme classification and speech recognition tasks [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "In this paper, we use Teager energy based algorithm [13] to", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "Audio signal x(t) is non-linear, time-varying and can be looked upon as a AM-FM model as follows (as mentioned in [13])", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "It has been shown that this non-linear modeling of speech helps in extraction of robust features for speech [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "Such AM-FM signals are very frequently used in communication systems [10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "One of the ways to estimate a(t) and \u03c9i(t), is to first isolate individual resonance by bandpass filtering the speech signal around its formants and then estimating amplitude and frequency modulating signals of each resonance based on an \u201denergy-tracking operator\u201d as described in [10].", "startOffset": 281, "endOffset": 285}, {"referenceID": 18, "context": "In the discrete form as is applicable to most speech processing systems [19] (5) can be written as", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "\u03c8 is the main ingredient of the first Energy Separation Algorithm (ESA) developed in [13] and used for signal and speech AM-FM demodulation.", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "In practice the speech signal is bandpass filtered using Gabor filters because of their optimal time-frequency discriminability [13], namely, s (t) = x (t) \u2217 g (t) (10)", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "instantaneous amplitude and instantaneous frequencies around different resonance frequencies of the speech signal have been studied for various applications in speech processing area such as phoneme classification, speech recognition [17], [18], assessment of vocal fold pathology [15], stress detection [20].", "startOffset": 234, "endOffset": 238}, {"referenceID": 17, "context": "instantaneous amplitude and instantaneous frequencies around different resonance frequencies of the speech signal have been studied for various applications in speech processing area such as phoneme classification, speech recognition [17], [18], assessment of vocal fold pathology [15], stress detection [20].", "startOffset": 240, "endOffset": 244}, {"referenceID": 14, "context": "instantaneous amplitude and instantaneous frequencies around different resonance frequencies of the speech signal have been studied for various applications in speech processing area such as phoneme classification, speech recognition [17], [18], assessment of vocal fold pathology [15], stress detection [20].", "startOffset": 281, "endOffset": 285}, {"referenceID": 19, "context": "instantaneous amplitude and instantaneous frequencies around different resonance frequencies of the speech signal have been studied for various applications in speech processing area such as phoneme classification, speech recognition [17], [18], assessment of vocal fold pathology [15], stress detection [20].", "startOffset": 304, "endOffset": 308}, {"referenceID": 16, "context": "use the Mel spaced Gabor filter-bank [17] to filter the audio into the first three bands.", "startOffset": 37, "endOffset": 41}], "year": 2014, "abstractText": "The potential use of non-linear speech features has not been investigated for music analysis although other commonly used speech features like Mel Frequency Ceptral Coefficients (MFCC) and pitch have been used extensively. In this paper, we assume an audio signal to be a sum of modulated sinusoidal and then use the energy separation algorithm to decompose the audio into amplitude and frequency modulation components using the non-linear Teager-Kaiser energy operator. We first identify the distribution of these non-linear features for music only and voice only segments in the audio signal in different Mel spaced frequency bands and show that they have the ability to discriminate. The proposed method based on Kullback-Leibler divergence measure is evaluated using a set of Indian classical songs from three different artists. Experimental results show that the discrimination ability is evident in certain low and mid frequency bands (200 1500 Hz).", "creator": "LaTeX with hyperref package"}}}