{"id": "1406.2572", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "abstract": "a broader central challenge to undertaking many fields of science surveying and statistical engineering involves minimizing non - convex error analytic functions over continuous, high dimensional spaces. gradient descent or quasi - objective newton methods instead are always almost ubiquitously popularly used to constantly perform such global minimizations, globally and it is often thought about that perhaps a additional main source of difficulty for these local methods managing to find the longest global smaller minimum estimation is the fast proliferation of large local domain minima with less much higher real error than losing the global minimum. firmly here we argue, based on consistent results from classic statistical analytical physics, random matrix theory, neural string network theory, rigorous and empirical evidence, asserting that a broader deeper and more readily profound difficulty also originates from the proliferation of saddle points, not mere local minima, itself especially embodied in particularly high dimensional problems of considerable practical cognitive interest. here such robust saddle points above are surrounded by high error plateaus that can dramatically reduce slow down learning, intersect and simultaneously give the illusory approximate impression of the simultaneous existence of missing a global local regional minimum. motivated by these small arguments, we themselves propose a new approach to second - order error optimization, the simplest saddle - free turing newton modeling method, that can rapidly escape high dimensional saddle points, rather unlike gradient field descent implementation and modern quasi - zero newton methods. lately we sometimes apply this algorithm to counter deep bypass or recurrent crypt neural security network training, and provide numerical evidence for its superior optimization performance.", "histories": [["v1", "Tue, 10 Jun 2014 14:52:14 GMT  (1044kb,D)", "http://arxiv.org/abs/1406.2572v1", "The theoretical review and analysis in this article draw heavily fromarXiv:1405.4604[cs.LG]"]], "COMMENTS": "The theoretical review and analysis in this article draw heavily fromarXiv:1405.4604[cs.LG]", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["yann n dauphin", "razvan pascanu", "\u00e7aglar g\u00fcl\u00e7ehre", "kyunghyun cho", "surya ganguli", "yoshua bengio"], "accepted": true, "id": "1406.2572"}, "pdf": {"name": "1406.2572.pdf", "metadata": {"source": "CRF", "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "authors": ["Yann N. Dauphin", "Razvan Pascanu"], "emails": ["dauphiya@iro.umontreal.ca", "r.pascanu@gmail.com", "gulcehrc@iro.umontreal.ca", "kyunghyun.cho@umontreal.ca", "sganguli@standford.edu", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": null, "text": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014)."}, {"heading": "1 Introduction", "text": "It is often the case that our geometric intuition, derived from experience within a low dimensional physical world, is inadequate for thinking about the geometry of typical error surfaces in highdimensional spaces. To illustrate this, consider minimizing a randomly chosen error function of a single scalar variable, given by a single draw of a Gaussian process. (Rasmussen and Williams, 2005) have shown that such a random error function would have many local minima and maxima, with high probability over the choice of the function, but saddles would occur with negligible probability. On the other-hand, as we review below, typical, random Gaussian error functions over N scalar variables, or dimensions, are increasingly likely to have saddle points rather than local minima as N increases. Indeed the ratio of the number of saddle points to local minima increases exponentially with the dimensionality N .\nar X\niv :1\n40 6.\n25 72\nv1 [\ncs .L\nA typical problem for both local minima and saddle-points is that they are often surrounded by plateaus of small curvature in the error. While gradient descent dynamics are repelled away from a saddle point to lower error by following directions of negative curvature, this repulsion can occur slowly due to the plateau. Second order methods, like the Newton method, are designed to rapidly descend plateaus surrounding local minima by rescaling gradient steps by the inverse eigenvalues of the Hessian matrix. However, the Newton method does not treat saddle points appropriately; as argued below, saddle-points instead become attractive under the Newton dynamics.\nThus, given the proliferation of saddle points, not local minima, in high dimensional problems, the entire theoretical justification for quasi-Newton methods, i.e. the ability to rapidly descend to the bottom of a convex local minimum, becomes less relevant in high dimensional non-convex optimization. In this work, which is an extension of the previous report Pascanu et al. (2014), we first want to raise awareness of this issue, and second, propose an alternative approach to second-order optimization that aims to rapidly escape from saddle points. This algorithm leverages second-order curvature information in a fundamentally different way than quasi-Newton methods, and also, in numerical experiments, outperforms them in some high dimensional problems involving deep or recurrent networks."}, {"heading": "2 The prevalence of saddle points in high dimensions", "text": "Here we review arguments from disparate literatures suggesting that saddle points, not local minima, provide a fundamental impediment to rapid high dimensional non-convex optimization. One line of evidence comes from statistical physics. Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach).\nOne particular result by Bray and Dean (2007) derives how critical points are distributed in the vs \u03b1 plane, where \u03b1 is the index, or the fraction of negative eigenvalues of the Hessian at the critical point, and is the error attained at the critical point. Within this plane, critical points concentrate on a monotonically increasing curve as \u03b1 ranges from 0 to 1, implying a strong correlation between the error and the index \u03b1: the larger the error the larger the index. The probability of a critical point to be an O(1) distance off the curve is exponentially small in the dimensionality N , for large N . This implies that critical points with error much larger than that of the global minimum, are exponentially likely to be saddle points, with the fraction of negative curvature directions being an increasing function of the error. Conversely, all local minima, which necessarily have index 0, are likely to have an error very close to that of the global minimum. Intuitively, in high dimensions, the chance that all the directions around a critical point lead upward (positive curvature) is exponentially small w.r.t. the number of dimensions, unless the critical point is the global minimum or stands at an error level close to it, i.e., it is unlikely one can find a way to go further down.\nThese results may also be understood via random matrix theory. We know that for a large Gaussian random matrix the eigenvalue distribution follows Wigner\u2019s famous semicircular law (Wigner, 1958), with both mode and mean at 0. The probability of an eigenvalue to be positive or negative is thus 1/2. Bray and Dean (2007) showed that the eigenvalues of the Hessian at a critical point are distributed in the same way, except that the semicircular spectrum is shifted by an amount determined by . For the global minimum, the spectrum is shifted so far right, that all eigenvalues are positive. As increases, the spectrum shifts to the left and accrues more negative eigenvalues as well as a density of eigenvalues around 0, indicating the typical presence of plateaus surrounding saddle points at large error. Such plateaus would slow the convergence of first order optimization methods, yielding the illusion of a local minimum.\nThe random matrix perspective also concisely and intuitively crystallizes the striking difference between the geometry of low and high dimensional error surfaces. For N = 1, an exact saddle point is a 0\u2013probability event as it means randomly picking an eigenvalue of exactly 0. As N grows it becomes exponentially unlikely to randomly pick all eigenvalues to be positive or negative, and therefore most critical points are saddle points.\nFyodorov and Williams (2007) review qualitatively similar results derived for random error functions superimposed on a quadratic error surface. These works indicate that for typical, generic functions chosen from a random Gaussian ensemble of functions, local minima with high error are\nexponentially rare in the dimensionality of the problem, but saddle points with many negative and approximate plateau directions are exponentially likely. However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest?\nBaldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer. Such an error surface shows only saddle-points and no local minima. This result is qualitatively consistent with the observation made by Bray and Dean (2007). Indeed Saxe et al. (2014) analyzed the dynamics of learning in the presence of these saddle points, and showed that they arise due to scaling symmetries in the weight space of a deep linear MLP. These scaling symmetries enabled Saxe et al. (2014) to find new exact solutions to the nonlinear dynamics of learning in deep linear networks. These learning dynamics exhibit plateaus of high error followed by abrupt transitions to better performance. They qualitatively recapitulate aspects of the hierarchical development of semantic concepts in infants (Saxe et al., 2013).\nIn (Saad and Solla, 1995) the dynamics of stochastic gradient descent are analyzed for soft committee machines. This work explores how well a student network can learn to imitate a randomly chosen teacher network. Importantly, it was observed that learning can go through an initial phase of being trapped in the symmetric submanifold of weight space. In this submanifold, the student\u2019s hidden units compute similar functions over the distribution of inputs. The slow learning dynamics within this submanifold originates from saddle point structures (caused by permutation symmetries among hidden units), and their associated plateaus (Rattray et al., 1998; Inoue et al., 2003). The exit from the plateau associated with the symmetric submanifold corresponds to the differentiation of the student\u2019s hidden units to mimic the teacher\u2019s hidden units. Interestingly, this exit from the plateau is achieved by following directions of negative curvature associated with a saddle point. sin directions perpendicular to the symmetric submanifold.\nMizutani and Dreyfus (2010) look at the effect of negative curvature on learning and implicitly at the effect of saddle points in the error surface. Their findings are similar. They show that the error surface of a single layer MLP has saddle points where the Hessian matrix is indefinite."}, {"heading": "3 Experimental validation of the prevalence of saddle points", "text": "In this section, we experimentally test whether the theoretical predictions presented by Bray and Dean (2007) for random Gaussian fields hold for neural networks. To our knowledge, this is the first attempt to measure the relevant statistical properties of neural network error surfaces and to test if the theory developed for random Gaussian fields generalizes to such cases.\nIn particular, we are interested in how the critical points of a single layer MLP are distributed in the \u2013\u03b1 plane, and how the eigenvalues of the Hessian matrix at these critical points are distributed. We used a small MLP trained on a down-sampled version of MNIST and CIFAR-10. Newton method was used to identify critical points of the error function. The results are in Fig. 1. More details about the setup are provided in Appendix C.\nThis empirical test confirms that the observations by Bray and Dean (2007) qualitatively hold for neural networks. Critical points concentrate along a monotonically increasing curve in the \u2013\u03b1 plane. Thus the prevalence of high error saddle points do indeed pose a severe problem for training\nSTART\nneural networks. While the eigenvalues do not seem to be exactly distributed according to the semicircular law, their distribution does shift to the left as the error increases. The large mode at 0 indicates that there is a plateau around any critical point of the error function of a neural network."}, {"heading": "4 Dynamics of optimization algorithms near saddle points", "text": "Given the prevalence of saddle points, it is important to understand how various optimization algorithms behave near them. Let us focus on non-degenerate saddle points for which the Hessian is not singular. These critical points can be locally analyzed by re-parameterizing the function according to Morse\u2019s lemma below (see chapter 7.3, Theorem 7.16 in Callahan (2010) or Appendix B:\nf(\u03b8\u2217 + \u2206\u03b8) = f(\u03b8\u2217) + 1\n2\nn\u03b8\u2211\ni=1\n\u03bbi\u2206v 2 i , (1)\nwhere \u03bbi represents the ith eigenvalue of the Hessian, and \u2206vi are the new parameters of the model corresponding to motion along the eigenvectors ei of the Hessian of f at \u03b8\u2217.\nA step of the gradient descent method always points in the right direction close to a saddle point (SGD in Fig. 2). If an eigenvalue \u03bbi is positive (negative), then the step moves toward (away) from \u03b8\u2217 along \u2206vi because the restriction of f to the corresponding eigenvector direction \u2206vi, achieves a minimum (maximum) at \u03b8\u2217. The drawback of the gradient descent method is not the direction, but the size of the step along each eigenvector. The step, along any direction ei, is given by \u2212\u03bbi\u2206vi, and so small steps are taken in directions corresponding to eigenvalues of small absolute value.\nThe Newton method solves the slowness problem by rescaling the gradients in each direction with the inverse of the corresponding eigenvalue, yielding the step \u2212\u2206vi. However, this approach can result in moving in the wrong direction. Specifically, if an eigenvalue is negative, the Newton step moves along the eigenvector in a direction opposite to the gradient descent step, and thus moves in the direction of increasing error towards \u03b8\u2217. Since it also moves towards \u03b8\u2217 along eigen-directions with positive eigenvalue, the saddle point \u03b8\u2217 becomes an attractor for the Newton method (see Fig. 2). This justifies using the Newton method to find critical points of any index in Fig. 1.\nA trust region approach is a practical implementation of second order methods for non-convex problems. In one such method, the Hessian is damped to remove negative curvature by adding a constant \u03b1 to its diagonal, which is equivalent to adding \u03b1 to each of its eigenvalues. Rescaling the gradient by the inverse of the modified eigenvalues \u03bbi + \u03b1 yields the step \u2212 ( \u03bbi/\u03bbi+\u03b1 ) \u2206vi. To ensure descent along every eigen-direction, one must increase the damping coefficient \u03b1 enough so that \u03bbmin + \u03b1 > 0 even for the most negative eigenvalue \u03bbmin. Therefore, the drawback is again a potentially small step size in many eigen-directions incurred by a large damping factor \u03b1.\nBesides damping, another approach to deal with negative curvature is to ignore them. This can be done regardless of the approximation strategy used for the Newton method such as a truncated Newton method or a BFGS approximation (see Nocedal and Wright (2006) chapters 4 and 7). However, such algorithms cannot escape saddle points, as they ignore the very directions of negative curvature that must be followed to achieve escape.\nNatural gradient descent is a first order method that relies on the curvature of the parameter manifold. That is, natural gradient descent takes a step that induces a constant change in the behaviour of the\nmodel as measured by the KL-divergence between the model before and after taking the step. The resulting algorithm is similar to the Newton method, except that it relies on the Fisher Information matrix F.\nIt is argued by Rattray et al. (1998); Inoue et al. (2003) that natural gradient descent can address certain saddle point structures effectively. Specifically, it can resolve those saddle points arising from having units behaving very similarly. Mizutani and Dreyfus (2010), however, argue that natural gradient descent also suffers with negative curvature. One particular known issue is the over-realizable regime, where around the stationary solution \u03b8\u2217, the Fisher matrix is rank-deficient. Numerically, this means that the Gauss-Newton direction can be orthogonal to the gradient at some distant point from \u03b8\u2217 (Mizutani and Dreyfus, 2010), causing optimization to converge to some non-stationary point. Another weakness is that the difference S between the Hessian and the Fisher Information Matrix can be large near certain saddle points that exhibit strong negative curvature. This means that the landscape close to these critical points may be dominated by S, meaning that the rescaling provided by F\u22121 is not optimal in all directions.\nThe same is true for TONGA (Le Roux et al., 2007), an algorithm similar to natural gradient descent. It uses the covariance of the gradients as the rescaling factor. As these gradients vanish approaching a critical point, their covariance will result in much larger steps than needed near critical points."}, {"heading": "5 Generalized trust region methods", "text": "In order to attack the saddle point problem, and overcome the deficiencies of the above methods, we will define a class of generalized trust region methods, and search for an algorithm within this space. This class involves a straightforward extension of classical trust region methods via two simple changes: (1) We allow the minimization of a first-order Taylor expansion of the function instead of always relying on a second-order Taylor expansion as is typically done in trust region methods, and (2) we replace the constraint on the norm of the step \u2206\u03b8 by a constraint on the distance between \u03b8 and \u03b8+\u2206\u03b8. Thus the choice of distance function and Taylor expansion order specifies an algorithm. If we define Tk(f, \u03b8,\u2206\u03b8) to indicate the k-th order Taylor series expansion of f around \u03b8 evaluated at \u03b8 + \u2206\u03b8, then we can summarize a generalized trust region method as:\n\u2206\u03b8 = arg min \u2206\u03b8\nTk{f, \u03b8,\u2206\u03b8} with k \u2208 {1, 2}\ns. t. d(\u03b8, \u03b8 + \u2206\u03b8) \u2264 \u2206. (2)\nFor example, the \u03b1-damped Newton method described above arises as a special case with k = 2 and d(\u03b8, \u03b8 + \u2206\u03b8) = ||\u2206\u03b8||22, where \u03b1 is implicitly a function of \u2206."}, {"heading": "6 Attacking the saddle point problem", "text": "We now search for a solution to the saddle-point problem within the family of generalized trust region methods. In particular, the analysis of optimization algorithms near saddle points discussed in Sec. 4 suggests a simple heuristic solution: rescale the gradient along each eigen-direction ei by 1/|\u03bbi|. This achieves the same optimal rescaling as the Newton method, while preserving the sign of the gradient, thereby turning saddle points into repellers, not attractors, of the learning dynamics. The idea of taking the absolute value of the eigenvalues of the Hessian was suggested before. See, for example, (Nocedal and Wright, 2006, chapter 3.4) or Murray (2010, chapter 4.1). However, we are not aware of any proper justification of this algorithm or even a detailed exploration (empirical or otherwise) of this idea. One cannot simply replace H by |H|, where |H| is the matrix obtained by taking the absolute value of each eigenvalue of H, without proper justification. For instance, one obvious question arises: are we still optimizing the same function? While we might be able to argue that this heuristic modification does the right thing near critical points, is it still the right thing far away from the critical points? Here we show this heuristic solution arises naturally from our generalized trust region approach.\nUnlike classical trust region approaches, we consider minimizing a first-order Taylor expansion of the loss (k = 1 in Eq. (2)). This means that the curvature information has to come from the constraint by picking a suitable distance measure d (see Eq. (2)). Since the minimum of the first order approximation of f is at infinity, we know that this optimization dynamics will always jump to the\nborder of the trust region. So we must ask how far from \u03b8 can we trust the first order approximation of f? One answer is to bound the discrepancy between the first and second order Taylor expansions of f by imposing the following constraint:\nd(\u03b8, \u03b8 + \u2206\u03b8) = \u2223\u2223\u2223\u2223f(\u03b8) +\u2207f\u2206\u03b8 + 1\n2 \u2206\u03b8>H\u2206\u03b8 \u2212 f(\u03b8)\u2212\u2207f\u2206\u03b8\n\u2223\u2223\u2223\u2223 = 1\n2\n\u2223\u2223\u2206\u03b8>H\u2206\u03b8 \u2223\u2223 \u2264 \u2206, (3)\nAlgorithm 1 Approximate saddle-free Newton Require: Function f(\u03b8) to minimize\nfor i = 1\u2192M do V\u2190 k Lanczos vectors of \u22022f\u2202\u03b82 f\u0302(\u03b1)\u2190 g(\u03b8 + V\u03b1) |H\u0302| \u2190 \u2223\u2223\u2223 \u2202 2f\u0302 \u2202\u03b12\n\u2223\u2223\u2223 by using an eigen decomposition of H\u0302 for j = 1\u2192 m do\ng\u2190 \u2212 \u2202f\u0302\u2202\u03b1 \u03bb\u2190 arg min\u03bb f\u0302(g(|H\u0302|+ \u03bbI)\u22121) \u03b8 \u2190 \u03b8 + g(|H\u0302|+ \u03bbI)\u22121V\nend for end for\nwhere \u2207f is the partial derivative of f with respect to \u03b8 and \u2206 \u2208 R is some small value that indicates how much discrepancy we are willing to accept. Note that the distance measure d takes into account the curvature of the function.\nEq. (3) is not easy to solve for \u2206\u03b8 in more than one dimension. Alternatively, one could take the square of the distance, but this would yield an optimization problem with a constraint that is quartic in \u2206\u03b8, and therefore also difficult to solve. We circumvent these difficulties through a Lemma: Lemma 1. Let A be a nonsingular square matrix in Rn \u00d7 Rn, and x \u2208 Rn be some vector. Then it holds that |x>Ax| \u2264 x>|A|x, where |A| is the matrix obtained by taking the absolute value of each of the eigenvalues of A.\nProof. See Appendix D for the proof.\nInstead of the originally proposed distance measure in Eq. (3), we approximate the distance by its upper bound \u2206\u03b8|H|\u2206\u03b8 based on Lemma 1. This results in the following generalized trust region method:\n\u2206\u03b8 = arg min \u2206\u03b8\nf(\u03b8) +\u2207f\u2206\u03b8\ns. t. \u2206\u03b8>|H|\u2206\u03b8 \u2264 \u2206. (4)\nNote that as discussed before, we can replace the inequality constraint with an equality one, as the first order approximation of f has a minimum at infinity and the algorithm always jumps to the border of the trust region. Similar to (Pascanu and Bengio, 2014), we use Lagrange multipliers to obtain the solution of this constrained optimization. This gives (up to a scalar that we fold into the learning rate) a step of the form:\n\u2206\u03b8 = \u2212\u2207f |H|\u22121 (5)\nThis algorithm, which we call the saddle-free Newton method (SFN), leverages curvature information in a fundamentally different way, to define the shape of the trust region, rather than Taylor expansion to second order, as in classical methods. Unlike gradient descent, it can move further (less) in the directions of low (high) curvature. It is identical to the Newton method when the Hessian is positive definite, but unlike the Newton method, it can escape saddle points. Furthermore, unlike gradient descent, the escape is rapid even along directions of weak negative curvature (see Fig. 2).\nThe exact implementation of this algorithm is intractable in a high dimensional problem, because it requires the exact computation of the Hessian. Instead we use an approach similar to Krylov subspace descent (Vinyals and Povey, 2012). We optimize that function in a lower-dimensional Krylov subspace f\u0302(\u03b1) = f(\u03b8+\u03b1V). The k Krylov subspace vectors V are found through Lanczos iteration of the Hessian. These vectors will span the k biggest eigenvectors of the Hessian with high-probability. This reparametrization through \u03b1 greatly reduces the dimensionality and allows us to use exact saddle-free Newton in the subspace.1 See Alg. 1 for the pseudocode.\n1 In the Krylov subspace, \u2202f\u0302 \u2202\u03b1 = V ( \u2202f \u2202\u03b8 )> and \u2202 2f\u0302 \u2202\u03b12 = V ( \u22022f \u2202\u03b82 ) V>."}, {"heading": "7 Experimental validation of the saddle-free Newton method", "text": "In this section, we empirically evaluate the theory suggesting the existence of many saddle points in high-dimensional functions by training neural networks."}, {"heading": "7.1 Feedforward Neural Networks", "text": ""}, {"heading": "7.1.1 Existence of Saddle Points in Neural Networks", "text": "In this section, we validate the existence of saddle points in the cost function of neural networks, and see how each of the algorithms we described earlier behaves near them. In order to minimize the effect of any type of approximation used in the algorithms, we train small neural networks on the scaled-down version of MNIST and CIFAR-10, where we can compute the update directions by each algorithm exactly. Both MNIST and CIFAR-10 were downsampled to be of size 10\u00d7 10. We compare minibatch stochastic gradient descent (MSGD), damped Newton and the proposed saddle-free Newton method (SFN). The hyperparameters of SGD were selected via random search (Bergstra and Bengio, 2012), and the damping coefficients for the damped Newton and saddle-free Newton2 methods were selected from a small set at each update.\nThe theory suggests that the number of saddle points increases exponentially as the dimensionality of the function increases. From this, we expect that it becomes more likely for the conventional algorithms such as SGD and Newton methods to stop near saddle points, resulting in worse performance (on training samples). Figs. 3 (a) and (d) clearly confirm this. With the smallest network, all the algorithms perform comparably, but as the size grows, the saddle-free Newton algorithm outperforms the others by a large margin.\nA closer look into the different behavior of each algorithm is presented in Figs. 3 (b) and (e) which show the evolution of training error over optimization. We can see that the proposed saddle-free Newton escapes, or does not get stuck at all, near a saddle point where both SGD and Newton methods appear trapped. Especially, at the 10-th epoch in the case of MNIST, we can observe the saddle-free Newton method rapidly escaping from the saddle point. Furthermore, Figs. 3 (c) and (f) provide evidence that the distribution of eigenvalues shifts more toward the right as error decreases\n2Damping is used for numerical stability.\nfor all algorithms, consistent with the theory of random error functions. The distribution shifts more for SFN, suggesting it can successfully avoid saddle-points on intermediary error (and large index)."}, {"heading": "7.1.2 Effectiveness of saddle-free Newton Method in Deep Neural Networks", "text": "Here, we further show the effectiveness of the proposed saddle-free Newton method in a larger neural network having seven hidden layers. The neural network is a deep autoencoder trained on (full-scale) MNIST and considered a standard benchmark problem for assessing the performance of optimization algorithms on neural networks (Sutskever et al., 2013). In this large-scale problem, we used the Krylov subspace descent approach described earlier with 500 subspace vectors.\nWe first trained the model with SGD and observed that learning stalls after achieving the meansquared error (MSE) of 1.0. We then continued with the saddle-free Newton method which rapidly escaped the (approximate) plateau at which SGD was stuck (See Fig. 4 (a)). Furthermore, even in these large scale experiments, we were able to confirm that the distribution of Hessian eigenvalues shifts right as error decreases, and that the proposed saddle-free Newton algorithm accelerates this shift (See Fig. 4 (b)).\nThe model trained with SGD followed by the saddle-free Newton method was able to get the stateof-the-art MSE of 0.57 compared to the previous best error of 0.69 achieved by the Hessian-Free method (Martens, 2010). Saddle free Newton method does better."}, {"heading": "7.2 Recurrent Neural Networks: Hard Optimization Problem", "text": "Recurrent neural networks are widely known to be more difficult to train than feedforward neural networks (see, e.g., Bengio et al., 1994; Pascanu et al., 2013). In practice they tend to underfit, and in this section, we want to test if the proposed saddle-free Newton method can help avoiding underfitting, assuming that that it is caused by saddle points. We trained a small recurrent neural network having 120 hidden units for the task of character-level language modeling on Penn Treebank corpus. Similarly to the previous experiment, we trained the model with SGD until it was clear that the learning stalled. From there on, training continued with the saddle-free Newton method.\nIn Fig. 4 (c), we see a trend similar to what we observed with the previous experiments using feedforward neural networks. The SGD stops progressing quickly and does not improve performance, suggesting that the algorithm is stuck in a plateau, possibly around a saddle point. As soon as we apply the proposed saddle-free Newton method, we see that the error drops significantly. Furthermore, Fig. 4 (d) clearly shows that the solution found by the saddle-free Newton has fewer negative eigenvalues, consistent with the theory of random Gaussian error functions. In addition to the saddle-free Newton method, we also tried continuing with the truncated Newton method with damping to see whether if it can improve SGD where it got stuck, but without much success."}, {"heading": "8 Conclusion", "text": "In summary, we have drawn from disparate literatures spanning statistical physics and random matrix theory to neural network theory, to argue that (a) non-convex error surfaces in high dimensional spaces generically suffer from a proliferation of saddle points, and (b) in contrast to conventional wisdom derived from low dimensional intuition, local minima with high error are exponentially rare in high dimensions. Moreover, we have provided the first experimental tests of these theories by performing new measurements of the statistical properties of critical points in neural network error surfaces. These tests were enabled by a novel application of Newton\u2019s method to search for critical points of any index (fraction of negative eigenvalues), and they confirmed the main qualitative prediction of theory that the index of a critical point tightly and positively correlates with its error level.\nMotivated by this theory, we developed a framework of generalized trust region methods to search for algorithms that can rapidly escape saddle points. This framework allows us to leverage curvature information in a fundamentally different way than classical methods, by defining the shape of the trust region, rather than locally approximating the function to second order. Through further approximations, we derived an exceedingly simple algorithm, the saddle-free Newton method, which rescales gradients by the absolute value of the inverse Hessian. This algorithm had previously remained heuristic and theoretically unjustified, as well as numerically unexplored within the context of deep and recurrent neural networks. Our work shows that near saddle points it can achieve rapid escape by combining the best of gradient descent and Newton methods while avoiding the pitfalls of both. Moreover, through our generalized trust region approach, our work shows that this algorithm is sensible even far from saddle points. Finally, we demonstrate improved optimization on several neural network training problems.\nFor the future, we are mainly interested in two directions. The first direction is to explore methods beyond Kyrylov subspaces, such as one in (Sohl-Dickstein et al., 2014), that allow the saddlefree Newton method to scale to high dimensional problems, where we cannot easily compute the entire Hessian matrix. In the second direction, the theoretical properties of critical points in the problem of training a neural network will be further analyzed. More generally, it is likely that a deeper understanding of the statistical properties of high dimensional error surfaces will guide the design of novel non-convex optimization algorithms that could impact many fields across science and engineering."}, {"heading": "Acknowledgments", "text": "We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Que\u0301bec for providing computational resources. Razvan Pascanu is supported by a DeepMind Google Fellowship. Surya Ganguli thanks the Burroughs Wellcome and Sloan Foundations for support."}, {"heading": "A Description of the different types of saddle-points", "text": "In general, consider an error function f(\u03b8) where \u03b8 is an N dimensional continuous variable. A critical point is by definition a point \u03b8 where the gradient of f(\u03b8) vanishes. All critical points of f(\u03b8) can be further characterized by the curvature of the function in its vicinity, as described by the eigenvalues of the Hessian. Note that the Hessian is symmetric and hence the eigenvalues are real numbers. The following are the four possible scenarios:\n\u2022 If all eigenvalues are non-zero and positive, then the critical point is a local minimum. \u2022 If all eigenvalues are non-zero and negative, then the critical point is a local maximum. \u2022 If the eigenvalues are non-zero and we have both positive and negative eigenvalues, then\nthe critical point is a saddle point with a min-max structure (see Figure 5 (b)). That is, if we restrict the function f to the subspace spanned by the eigenvectors corresponding to positive (negative) eigenvalues, then the saddle point is a maximum (minimum) of this restriction.\n\u2022 If the Hessian matrix is singular, then the degenerate critical point can be a saddle point, as it is, for example, for \u03b83, \u03b8 \u2208 R or for the monkey saddle (Figure 5 (a) and (c)). If it is a saddle, then, if we restrict \u03b8 to only change along the direction of singularity, the restricted function does not exhibit a minimum nor a maximum; it exhibits, to second order, a plateau. When moving from one side to other of the plateau, the eigenvalue corresponding to this picked direction generically changes sign, being exactly zero at the critical point. Note that an eigenvalue of zero can also indicate the presence of a gutter structure, a degenerate minimum, maximum or saddle, where a set of connected points are all minimum, maximum or saddle structures of the same shape and error. In Figure 5 (d) it is shaped as a circle. The error function looks like the bottom of a wine bottle, where all points along this circle are minimum of equal value.\nA plateau is an almost flat region in some direction. This structure is given by having the eigenvalues (which describe the curvature) corresponding to the directions of the plateau be close to 0, but not exactly 0. Or, additionally, by having a large discrepancy between the norm of the eigenvalues. This large difference would make the direction of \u201crelative\u201d small eigenvalues look like flat compared to the direction of large eigenvalues."}, {"heading": "B Reparametrization of the space around saddle-points", "text": "This reparametrization is given by taking a Taylor expansion of the function f around the critical point. If we assume that the Hessian is not singular, then there is a neighbourhood around this critical point where this approximation is reliable and, since the first order derivatives vanish, the Taylor expansion is given by:\nf(\u03b8\u2217 + \u2206\u03b8) = f(\u03b8\u2217) + 1\n2 (\u2206\u03b8)>H\u2206\u03b8 (6)\nLet us denote by e1, . . . , en\u03b8 the eigenvectors of the Hessian H and by \u03bb1, . . . , \u03bbn\u03b8 the corresponding eigenvalues. We can now make a change of coordinates into the space span by these eigenvectors:\n\u2206v = 1\n2\n  e1 >\n. . . en\u03b8 >\n \u2206\u03b8 (7)\nf(\u03b8\u2217 + \u2206\u03b8) = f(\u03b8\u2217) + 1\n2\nn\u03b8\u2211\ni=1\n\u03bbi(ei >\u2206\u03b8)2 = f(\u03b8\u2217) +\nn\u03b8\u2211\ni=1\n\u03bbi\u2206v 2 i (8)"}, {"heading": "C Empirical exploration of properties of critical points", "text": "To obtain the plot on MNIST we used the Newton method to discover nearby critical points along the path taken by the saddle-free Newton algorithm. We consider 20 different runs of the saddlefree algorithm, each using a different random seed. We then run 200 jobs. The first 100 jobs are looking for critical points near the value of the parameters obtained after some random number of epochs (between 0 and 20) of a randomly selected run (among the 20 different runs) of saddle-free Newton method. To this starting position uniform noise is added of small amplitude (the amplitude is randomly picked between the different values {10\u22121, 10\u22122, 10\u22123, 10\u22124} The last 100 jobs look for critical points near uniformally sampled weights (the range of the weights is given by the unit cube). The task (dataset and model) is the same as the one used previously.\nTo obtain the plots on CIFAR, we have trained multiple 3-layer deep neural networks using SGD. The activation function of these networks is the tanh function. We saved the parameters of these networks for each epoch. We trained 100 networks with different parameter initializations between 10 and 300 epochs (chosen randomly). The networks were then trained using the Newton method to find a nearby critical point. This allows us to find many different critical points along the learning trajectories of the networks."}, {"heading": "D Proof of Lemma 1", "text": "Lemma 2. Let A be a nonsingular square matrix in Rn \u00d7 Rn, and x \u2208 Rn be some vector. Then it holds that |x>Ax| \u2264 x>|A|x, where |A| is the matrix obtained by taking the absolute value of each of the eigenvalues of A.\nProof. Let e1, . . . en be the different eigenvectors of A and \u03bb1, . . . \u03bbn the corresponding eigenvalues. We now re-write the identity by expressing the vector x in terms of these eigenvalues:\n|x>Ax| = \u2223\u2223\u2223\u2223\u2223 \u2211\ni\n(x>ei)ei >Ax \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 \u2211\ni\n(x>ei)\u03bbi(ei >x) \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 \u2211\ni\n\u03bbi(x >ei) 2 \u2223\u2223\u2223\u2223\u2223\nWe can now use the triangle inequality |\u2211i xi| \u2264 \u2211 i |xi| and get that\n|x>Ax| \u2264 \u2211\ni\n|(x>ei)2\u03bbi| = \u2211\ni\n(x>ei)|\u03bbi|(ei>x) = x>|A|x\nE Implementation details for approximate saddle-free Newton\nThe Krylov subspace is obtained through a slightly modified Lanczos process (see Algorithm 2). The initial vector of the algorithm is the gradient of the model. As noted by Vinyals and Povey (2012), we found it was useful to include the previous search direction as the last vector of the subspace. As described in the main paper, we have \u2202f\u0302\u2202\u03b1 = V ( \u2202f \u2202\u03b8 )> and \u2202 2f\u0302 \u2202\u03b12 = V ( \u22022f \u2202\u03b82 ) V>. Note that the calculation of the Hessian in the subspace can be greatly sped up by memorizing the vectors Vi \u2202 2f \u2202\u03b82 during the Lanczos process. Once memorized, the Hessian is simply the product of the two matrices V and Vi \u2202 2f \u2202\u03b82 .\nWe have found that it is beneficial to perform multiple optimization steps within the subspace. We do not recompute the Hessian for these steps under the assumption that the Hessian will not change much.\nAlgorithm 2 Obtaining the Lanczos vectors Require: g\u2190 \u2212\u2202f\u2202\u03b8 Require: \u2206\u03b8 (The past weight update) V0 \u2190 0 V1 \u2190 g\u2016g\u2016 \u03b21 \u2190 0 for i = 1\u2192 k \u2212 1 do wi \u2190 Vi \u2202 2f \u2202\u03b82 (Implemented efficient through L-Op (Pearlmutter, 1994))\nif i = k \u2212 1 then wi \u2190 \u2206\u03b8 end if \u03b1i \u2190 wiVi wi \u2190 wi \u2212 \u03b1iVi \u2212 \u03b2iVi\u22121 \u03b2i+1 \u2190 \u2016wi\u2016 Vi+1 \u2190 w\u2016wi\u2016\nend for"}, {"heading": "F Experiments", "text": "F.1 Existence of Saddle Points in Neural Networks\nFor feedforward networks using SGD, we choose the following hyperparameters using the random search strategy (Bergstra and Bengio, 2012):\n\u2022 Learning rate \u2022 Size of minibatch \u2022 Momentum coefficient\nFor random search, we draw 80 samples and pick the best one.\nFor both the Newton and saddle-free Newton methods, the damping coefficient is chosen at each update, to maximize the improvement, among { 100, 10\u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125 } .\nF.2 Effectiveness of saddle-free Newton Method in Deep Neural Networks\nThe deep auto-encoder was first trained using the protocol used by Sutskever et al. (2013). In these experiments we use classical momentum.\nF.3 Recurrent Neural Networks: Hard Optimization Problem\nWe initialized the recurrent weights of RNN to be orthogonal as suggested by Saxe et al. (2014). The number of hidden units of RNN is fixed to 120. For recurrent neural networks using SGD, we choose the following hyperparameters using the random search strategy:\n\u2022 Learning rate \u2022 Threshold for clipping the gradient (Pascanu et al., 2013) \u2022 Momentum coefficient\nFor random search, we draw 64 samples and pick the best one. Just like in the experiment using feedforward neural networks, the damping coefficient of both the Newton and saddle-free Newton methods was chosen at each update, to maximize the improvement.\nWe clip the gradient and saddle-free update step if it exceeds certain threshold as suggested by Pascanu et al. (2013).\nSince it is costly to compute the exact Hessian for RNN\u2019s, we used the eigenvalues of the Hessian in the Krylov subspace to plot the distribution of eigenvalues for Hessian matrix in Fig. 4 (d)."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks, 2(1), 53\u201358.", "citeRegEx": "Baldi and Hornik,? 1989", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "5(2), 157\u2013166. Special Issue on Recurrent Neural Networks, March 94.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 13, 281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy).", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Statistics of critical points of gaussian fields on large-dimensional spaces", "author": ["A.J. Bray", "D.S. Dean"], "venue": "Physics Review Letter, 98, 150201.", "citeRegEx": "Bray and Dean,? 2007", "shortCiteRegEx": "Bray and Dean", "year": 2007}, {"title": "Advanced Calculus: A Geometric View", "author": ["J. Callahan"], "venue": "Undergraduate Texts in Mathematics. Springer.", "citeRegEx": "Callahan,? 2010", "shortCiteRegEx": "Callahan", "year": 2010}, {"title": "Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity", "author": ["Y.V. Fyodorov", "I. Williams"], "venue": "Journal of Statistical Physics, 129(5-6), 1081\u20131116.", "citeRegEx": "Fyodorov and Williams,? 2007", "shortCiteRegEx": "Fyodorov and Williams", "year": 2007}, {"title": "On-line learning theory of soft committee machines with correlated hidden units steepest gradient descent and natural gradient descent", "author": ["M. Inoue", "H. Park", "M. Okada"], "venue": "Journal of the Physical Society of Japan, 72(4), 805\u2013810.", "citeRegEx": "Inoue et al\\.,? 2003", "shortCiteRegEx": "Inoue et al\\.", "year": 2003}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N. Le Roux", "Manzagol", "P.-A.", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Roux et al\\.,? 2007", "shortCiteRegEx": "Roux et al\\.", "year": 2007}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "International Conference in Machine Learning, pages 735\u2013742.", "citeRegEx": "Martens,? 2010", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "An analysis on negative curvature induced by singularity in multi-layer neural-network learning", "author": ["E. Mizutani", "S. Dreyfus"], "venue": "Advances in Neural Information Processing Systems, pages 1669\u20131677.", "citeRegEx": "Mizutani and Dreyfus,? 2010", "shortCiteRegEx": "Mizutani and Dreyfus", "year": 2010}, {"title": "Newton-type methods", "author": ["W. Murray"], "venue": "Technical report, Department of Management Science and Engineering, Stanford University.", "citeRegEx": "Murray,? 2010", "shortCiteRegEx": "Murray", "year": 2010}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer.", "citeRegEx": "Nocedal and Wright,? 2006", "shortCiteRegEx": "Nocedal and Wright", "year": 2006}, {"title": "Mean field theory of spin glasses: statistics and dynamics", "author": ["G. Parisi"], "venue": "Technical Report Arxiv 0706.0094.", "citeRegEx": "Parisi,? 2007", "shortCiteRegEx": "Parisi", "year": 2007}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Pascanu and Bengio,? 2014", "shortCiteRegEx": "Pascanu and Bengio", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML\u20192013.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": "Technical Report Arxiv 1405.4604.", "citeRegEx": "Pascanu et al\\.,? 2014", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation, 6, 147\u2013 160.", "citeRegEx": "Pearlmutter,? 1994", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2005", "shortCiteRegEx": "Rasmussen and Williams", "year": 2005}, {"title": "Natural Gradient Descent for On-Line Learning", "author": ["M. Rattray", "D. Saad", "S.I. Amari"], "venue": "Physical Review Letters, 81(24), 5461\u20135464.", "citeRegEx": "Rattray et al\\.,? 1998", "shortCiteRegEx": "Rattray et al\\.", "year": 1998}, {"title": "On-line learning in soft committee machines", "author": ["D. Saad", "S.A. Solla"], "venue": "Physical Review E, 52, 4225\u20134243.", "citeRegEx": "Saad and Solla,? 1995", "shortCiteRegEx": "Saad and Solla", "year": 1995}, {"title": "Learning hierarchical category structure in deep neural networks", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "Proceedings of the 35th annual meeting of the Cognitive Science Society, pages 1271\u20131276.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural network", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Saxe et al\\.,? 2014", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["J. Sohl-Dickstein", "B. Poole", "S. Ganguli"], "venue": "ICML\u20192014.", "citeRegEx": "Sohl.Dickstein et al\\.,? 2014", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "S. Dasgupta and D. Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 1139\u20131147. JMLR Workshop and Conference Proceedings.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Krylov Subspace Descent for Deep Learning", "author": ["O. Vinyals", "D. Povey"], "venue": "AISTATS.", "citeRegEx": "Vinyals and Povey,? 2012", "shortCiteRegEx": "Vinyals and Povey", "year": 2012}, {"title": "On the distribution of the roots of certain symmetric matrices", "author": ["E.P. Wigner"], "venue": "The Annals of Mathematics, 67(2), 325\u2013327.", "citeRegEx": "Wigner,? 1958", "shortCiteRegEx": "Wigner", "year": 1958}, {"title": "The number of hidden units of RNN is fixed to 120", "author": ["We initialized the recurrent weights of RNN to be orthogonal as suggested by Saxe"], "venue": "For recurrent neural networks using SGD, we choose the following hyperparameters using the random search strategy:", "citeRegEx": "Saxe,? 2014", "shortCiteRegEx": "Saxe", "year": 2014}, {"title": "Since it is costly to compute the exact Hessian for RNN\u2019s, we used the eigenvalues of the Hessian in the Krylov subspace to plot the distribution of eigenvalues for Hessian matrix in Fig", "author": ["Pascanu"], "venue": "4 (d). 14", "citeRegEx": "Pascanu,? 2013", "shortCiteRegEx": "Pascanu", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "This work extends the results of Pascanu et al. (2014).", "startOffset": 33, "endOffset": 55}, {"referenceID": 19, "context": "(Rasmussen and Williams, 2005) have shown that such a random error function would have many local minima and maxima, with high probability over the choice of the function, but saddles would occur with negligible probability.", "startOffset": 0, "endOffset": 30}, {"referenceID": 16, "context": "In this work, which is an extension of the previous report Pascanu et al. (2014), we first want to raise awareness of this issue, and second, propose an alternative approach to second-order optimization that aims to rapidly escape from saddle points.", "startOffset": 59, "endOffset": 81}, {"referenceID": 27, "context": "We know that for a large Gaussian random matrix the eigenvalue distribution follows Wigner\u2019s famous semicircular law (Wigner, 1958), with both mode and mean at 0.", "startOffset": 117, "endOffset": 131}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach).", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach).", "startOffset": 0, "endOffset": 51}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach).", "startOffset": 0, "endOffset": 201}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach). One particular result by Bray and Dean (2007) derives how critical points are distributed in the vs \u03b1 plane, where \u03b1 is the index, or the fraction of negative eigenvalues of the Hessian at the critical point, and is the error attained at the critical point.", "startOffset": 0, "endOffset": 286}, {"referenceID": 5, "context": "Bray and Dean (2007); Fyodorov and Williams (2007) study the nature of critical points of random Gaussian error functions on high dimensional continuous domains using replica theory (see Parisi (2007) for a recent review of this approach). One particular result by Bray and Dean (2007) derives how critical points are distributed in the vs \u03b1 plane, where \u03b1 is the index, or the fraction of negative eigenvalues of the Hessian at the critical point, and is the error attained at the critical point. Within this plane, critical points concentrate on a monotonically increasing curve as \u03b1 ranges from 0 to 1, implying a strong correlation between the error and the index \u03b1: the larger the error the larger the index. The probability of a critical point to be an O(1) distance off the curve is exponentially small in the dimensionality N , for large N . This implies that critical points with error much larger than that of the global minimum, are exponentially likely to be saddle points, with the fraction of negative curvature directions being an increasing function of the error. Conversely, all local minima, which necessarily have index 0, are likely to have an error very close to that of the global minimum. Intuitively, in high dimensions, the chance that all the directions around a critical point lead upward (positive curvature) is exponentially small w.r.t. the number of dimensions, unless the critical point is the global minimum or stands at an error level close to it, i.e., it is unlikely one can find a way to go further down. These results may also be understood via random matrix theory. We know that for a large Gaussian random matrix the eigenvalue distribution follows Wigner\u2019s famous semicircular law (Wigner, 1958), with both mode and mean at 0. The probability of an eigenvalue to be positive or negative is thus /2. Bray and Dean (2007) showed that the eigenvalues of the Hessian at a critical point are distributed in the same way, except that the semicircular spectrum is shifted by an amount determined by .", "startOffset": 0, "endOffset": 1861}, {"referenceID": 22, "context": "They qualitatively recapitulate aspects of the hierarchical development of semantic concepts in infants (Saxe et al., 2013).", "startOffset": 104, "endOffset": 123}, {"referenceID": 21, "context": "In (Saad and Solla, 1995) the dynamics of stochastic gradient descent are analyzed for soft committee machines.", "startOffset": 3, "endOffset": 25}, {"referenceID": 20, "context": "The slow learning dynamics within this submanifold originates from saddle point structures (caused by permutation symmetries among hidden units), and their associated plateaus (Rattray et al., 1998; Inoue et al., 2003).", "startOffset": 176, "endOffset": 218}, {"referenceID": 8, "context": "The slow learning dynamics within this submanifold originates from saddle point structures (caused by permutation symmetries among hidden units), and their associated plateaus (Rattray et al., 1998; Inoue et al., 2003).", "startOffset": 176, "endOffset": 218}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer.", "startOffset": 123, "endOffset": 147}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer. Such an error surface shows only saddle-points and no local minima. This result is qualitatively consistent with the observation made by Bray and Dean (2007). Indeed Saxe et al.", "startOffset": 123, "endOffset": 400}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer. Such an error surface shows only saddle-points and no local minima. This result is qualitatively consistent with the observation made by Bray and Dean (2007). Indeed Saxe et al. (2014) analyzed the dynamics of learning in the presence of these saddle points, and showed that they arise due to scaling symmetries in the weight space of a deep linear MLP.", "startOffset": 123, "endOffset": 427}, {"referenceID": 0, "context": "However, is this result for generic error landscapes applicable to the error landscapes of practical problems of interest? Baldi and Hornik (1989) analyzed the error surface of a multilayer perceptron (MLP) with a single linear hidden layer. Such an error surface shows only saddle-points and no local minima. This result is qualitatively consistent with the observation made by Bray and Dean (2007). Indeed Saxe et al. (2014) analyzed the dynamics of learning in the presence of these saddle points, and showed that they arise due to scaling symmetries in the weight space of a deep linear MLP. These scaling symmetries enabled Saxe et al. (2014) to find new exact solutions to the nonlinear dynamics of learning in deep linear networks.", "startOffset": 123, "endOffset": 648}, {"referenceID": 5, "context": "In this section, we experimentally test whether the theoretical predictions presented by Bray and Dean (2007) for random Gaussian fields hold for neural networks.", "startOffset": 89, "endOffset": 110}, {"referenceID": 5, "context": "This empirical test confirms that the observations by Bray and Dean (2007) qualitatively hold for neural networks.", "startOffset": 54, "endOffset": 75}, {"referenceID": 6, "context": "16 in Callahan (2010) or Appendix B:", "startOffset": 6, "endOffset": 22}, {"referenceID": 13, "context": "This can be done regardless of the approximation strategy used for the Newton method such as a truncated Newton method or a BFGS approximation (see Nocedal and Wright (2006) chapters 4 and 7).", "startOffset": 148, "endOffset": 174}, {"referenceID": 11, "context": "Numerically, this means that the Gauss-Newton direction can be orthogonal to the gradient at some distant point from \u03b8\u2217 (Mizutani and Dreyfus, 2010), causing optimization to converge to some non-stationary point.", "startOffset": 120, "endOffset": 148}, {"referenceID": 17, "context": "It is argued by Rattray et al. (1998); Inoue et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 8, "context": "(1998); Inoue et al. (2003) that natural gradient descent can address certain saddle point structures effectively.", "startOffset": 8, "endOffset": 28}, {"referenceID": 8, "context": "(1998); Inoue et al. (2003) that natural gradient descent can address certain saddle point structures effectively. Specifically, it can resolve those saddle points arising from having units behaving very similarly. Mizutani and Dreyfus (2010), however, argue that natural gradient descent also suffers with negative curvature.", "startOffset": 8, "endOffset": 243}, {"referenceID": 15, "context": "Similar to (Pascanu and Bengio, 2014), we use Lagrange multipliers to obtain the solution of this constrained optimization.", "startOffset": 11, "endOffset": 37}, {"referenceID": 26, "context": "Instead we use an approach similar to Krylov subspace descent (Vinyals and Povey, 2012).", "startOffset": 62, "endOffset": 87}, {"referenceID": 3, "context": "The hyperparameters of SGD were selected via random search (Bergstra and Bengio, 2012), and the damping coefficients for the damped Newton and saddle-free Newton2 methods were selected from a small set at each update.", "startOffset": 59, "endOffset": 86}, {"referenceID": 25, "context": "The neural network is a deep autoencoder trained on (full-scale) MNIST and considered a standard benchmark problem for assessing the performance of optimization algorithms on neural networks (Sutskever et al., 2013).", "startOffset": 191, "endOffset": 215}, {"referenceID": 10, "context": "69 achieved by the Hessian-Free method (Martens, 2010).", "startOffset": 39, "endOffset": 54}, {"referenceID": 16, "context": "Recurrent neural networks are widely known to be more difficult to train than feedforward neural networks (see, e.g., Bengio et al., 1994; Pascanu et al., 2013).", "startOffset": 106, "endOffset": 160}, {"referenceID": 24, "context": "The first direction is to explore methods beyond Kyrylov subspaces, such as one in (Sohl-Dickstein et al., 2014), that allow the saddlefree Newton method to scale to high dimensional problems, where we cannot easily compute the entire Hessian matrix.", "startOffset": 83, "endOffset": 112}, {"referenceID": 4, "context": "We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 48, "endOffset": 93}, {"referenceID": 1, "context": "We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 48, "endOffset": 93}], "year": 2014, "abstractText": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014).", "creator": "LaTeX with hyperref package"}}}