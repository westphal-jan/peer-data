{"id": "1406.3339", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2014", "title": "Algorithms for CVaR Optimization in MDPs", "abstract": "hence in many sequential decision - making algorithms problems we may want him to accurately manage risk behavior by minimizing some measure of variability especially in planning costs in addition therefore to minimizing precisely a convenient standard intercept criterion. conditional fixed value - at - estimated risk ( cvar ) correlation is a relatively minor new risk measure that essentially addresses substantially some weakness of the unique shortcomings of the well - done known standard variance - related risk measures, and because of availability its practical computational efficiencies easily has gained popularity throughout in finance and bankruptcy operations law research. in this 1984 paper, we properly consider the mean - cvar optimization regression problem in mdps. we first derive hence a formula for estimation computing systematically the gradient of predicting this risk - sensitive objective function. we then clearly devise policy class gradient and actor - constraint critic acceleration algorithms that determine each node uses a specific method to estimate essentially this gradient and updates the policy parameters in using the descent backward direction. we quickly establish successfully the convergence of converge our algorithms to locally risk - sensitive independent optimal policies. finally, otherwise we need demonstrate just the overwhelming usefulness need of concentrating our algorithms in an estimated optimal stopping problem.", "histories": [["v1", "Thu, 12 Jun 2014 19:56:16 GMT  (94kb)", "https://arxiv.org/abs/1406.3339v1", "Submitted to NIPS 14"], ["v2", "Tue, 17 Jun 2014 18:05:38 GMT  (93kb)", "http://arxiv.org/abs/1406.3339v2", "Submitted to NIPS 14"], ["v3", "Thu, 10 Jul 2014 21:59:26 GMT  (99kb)", "http://arxiv.org/abs/1406.3339v3", "Submitted to NIPS 14"]], "COMMENTS": "Submitted to NIPS 14", "reviews": [], "SUBJECTS": "cs.AI math.OC", "authors": ["yinlam chow", "mohammad ghavamzadeh"], "accepted": true, "id": "1406.3339"}, "pdf": {"name": "1406.3339.pdf", "metadata": {"source": "CRF", "title": "Algorithms for CVaR Optimization in MDPs", "authors": ["Yinlam Chow"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n33 39\nv3 [\ncs .A\nI] 1\n0 Ju"}, {"heading": "1 Introduction", "text": "A standard optimization criterion for an infinite horizon Markov decision process (MDP) is the expected sum of (discounted) costs (i.e., finding a policy that minimizes the value function of the initial state of the system). However in many applications, we may prefer to minimize some measure of risk in addition to this standard optimization criterion. In such cases, we would like to use a criterion that incorporates a penalty for the variability (due to the stochastic nature of the system) induced by a given policy. In risk-sensitive MDPs [18], the objective is to minimize a risk-sensitive criterion such as the expected exponential utility [18], a variance-related measure [32, 16], or the percentile performance [17]. The issue of how to construct such criteria in a manner that will be both conceptually meaningful and mathematically tractable is still an open question.\nAlthough most losses (returns) are not normally distributed, the typical Markiowitz mean-variance optimization [22], that relies on the first two moments of the loss (return) distribution, has dominated the risk management for over 50 years. Numerous alternatives to mean-variance optimization have emerged in the literature, but there is no clear leader amongst these alternative risk-sensitive objective functions. Value-atrisk (VaR) and conditional value-at-risk (CVaR) are two promising such alternatives\n\u2217Mohammad Ghavamzadeh is at Adobe Research, on leave of absence from INRIA Lille - Team SequeL.\nthat quantify the losses that might be encountered in the tail of the loss distribution, and thus, have received high status in risk management. For (continuous) loss distributions, while VaR measures risk as the maximum loss that might be incurred w.r.t. a given confidence level \u03b1, CVaR measures it as the expected loss given that the loss is greater or equal to VaR\u03b1. Although VaR is a popular risk measure, CVaR\u2019s computational advantages over VaR has boosted the development of CVaR optimization techniques. We provide the exact definitions of these two risk measures and briefly discuss some of the VaR\u2019s shortcomings in Section 2. CVaR minimization was first developed by Rockafellar and Uryasev [29] and its numerical effectiveness was demonstrated in portfolio optimization and option hedging problems. Their work was then extended to objective functions consist of different combinations of the expected loss and the CVaR, such as the minimization of the expected loss subject to a constraint on CVaR. This is the objective function that we study in this paper, although we believe that our proposed algorithms can be easily extended to several other CVaR-related objective functions. Boda and Filar [10] and Ba\u0308uerle and Ott [25, 4] extended the results of [29] to MDPs (sequential decision-making). While the former proposed to use dynamic programming (DP) to optimize CVaR, an approach that is limited to small problems, the latter showed that in both finite and infinite horizon MDPs, there exists a deterministic history-dependent optimal policy for CVaR optimization (see Section 3 for more details).\nMost of the work in risk-sensitive sequential decision-making has been in the context of MDPs (when the model is known) and much less work has been done within the reinforcement learning (RL) framework. In risk-sensitive RL, we can mention the work by Borkar [11, 12] who considered the expected exponential utility and those by Tamar et al. [34] and Prashanth and Ghavamzadeh [21] on several variance-related risk measures. CVaR optimization in RL is a rather novel subject. Morimura et al. [24] estimate the return distribution while exploring using a CVaR-based risk-sensitive policy. Their algorithm does not scale to large problems. Petrik and Subramanian [27] propose a method based on stochastic dual DP to optimize CVaR in large-scale MDPs. However, their method is limited to linearly controllable problems. Borkar and Jain [15] consider a finite-horizon MDP with CVaR constraint and sketch a stochastic approximation algorithm to solve it. Finally, Tamar et al. [35] have recently proposed a policy gradient algorithm for CVaR optimization.\nIn this paper, we develop policy gradient (PG) and actor-critic (AC) algorithms for mean-CVaR optimization in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then propose several methods to estimate this gradient both incrementally and using system trajectories (update at each time-step vs. update after observing one or more trajectories). We then use these gradient estimations to devise PG and AC algorithms that update the policy parameters in the descent direction. Using the ordinary differential equations (ODE) approach, we establish the asymptotic convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem. In comparison to [35], while they develop a PG algorithm for CVaR optimization in stochastic shortest path problems that only considers continuous loss distributions, uses a biased estimator for VaR, is not incremental, and has no convergence proof, here we study mean-CVaR optimization, consider both discrete and\ncontinuous loss distributions, devise both PG and (several) AC algorithms (trajectorybased and incremental \u2013 plus AC helps in reducing the variance of PG algorithms), and establish convergence proof for our algorithms."}, {"heading": "2 Preliminaries", "text": "We consider problems in which the agent\u2019s interaction with the environment is modeled as a MDP. A MDP is a tuple M = (X ,A, C, P, P0), where X = {1, . . . , n} and A = {1, . . . ,m} are the state and action spaces; C(x, a) \u2208 [\u2212Cmax, Cmax] is the bounded cost random variable whose expectation is denoted by c(x, a) = E [ C(x, a) ] ; P (\u00b7|x, a) is the transition probability distribution; and P0(\u00b7) is the initial state distribution. For simplicity, we assume that the system has a single initial state x0, i.e., P0(x) = 1{x = x0}. All the results of the paper can be easily extended to the case that the system has more than one initial state. We also need to specify the rule according to which the agent selects actions at each state. A stationary policy \u00b5(\u00b7|x) is a probability distribution over actions, conditioned on the current state. In policy gradient and actor-critic methods, we define a class of parameterized stochastic policies { \u00b5(\u00b7|x; \u03b8), x \u2208 X , \u03b8 \u2208 \u0398 \u2286 R\u03ba1 } , estimate the gradient of a performance measure w.r.t. the policy parameters \u03b8 from the observed system trajectories, and then improve the policy by adjusting its parameters in the direction of the gradient. Since in this setting a policy \u00b5 is represented by its \u03ba1-dimensional parameter vector \u03b8, policy dependent functions can be written as a function of \u03b8 in place of \u00b5. So, we use \u00b5 and \u03b8 interchangeably in the paper. We denote by d\u00b5\u03b3(x|x0) = (1 \u2212 \u03b3) \u2211\u221e k=0 \u03b3 k P(xk = x|x0 = x0;\u00b5) and \u03c0\u00b5\u03b3 (x, a|x0) = d\u00b5\u03b3(x|x0)\u00b5(a|x) the \u03b3-discounted visiting distribution of state x and state-action pair (x, a) under policy \u00b5, respectively.\nLet Z be a bounded-mean random variable, i.e., E[|Z|] < \u221e, with the cumulative distribution function F (z) = P(Z \u2264 z) (e.g., one may think of Z as the loss of an investment strategy \u00b5). We define the value-at-risk at the confidence level \u03b1 \u2208 (0, 1) as VaR\u03b1(Z) = min { z | F (z) \u2265 \u03b1 } . Here the minimum is attained because F is nondecreasing and right-continuous in z. When F is continuous and strictly increasing, VaR\u03b1(Z) is the unique z satisfying F (z) = \u03b1, otherwise, the VaR equation can have no solution or a whole range of solutions. Although VaR is a popular risk measure, it suffers from being unstable and difficult to work with numerically when Z is not normally distributed, which is often the case as loss distributions tend to exhibit fat tails or empirical discreteness. Moreover, VaR is not a coherent risk measure [2] and more importantly does not quantify the losses that might be suffered beyond its value at the \u03b1-tail of the distribution [28]. An alternative measure that addresses most of the VaR\u2019s shortcomings is conditional value-at-risk, CVAR\u03b1(Z), which is the mean of the \u03b1-tail distribution of Z . If there is no probability atom at VaR\u03b1(Z), CVaR\u03b1(Z) has a unique value that is defined as CVaR\u03b1(Z) = E [ Z | Z \u2265 VaR\u03b1(Z) ] . Rockafellar and Uryasev [29] showed that CVaR\u03b1(Z) = min\n\u03bd\u2208R H\u03b1(Z, \u03bd)\n\u25b3 = min\n\u03bd\u2208R\n{ \u03bd + 1 1\u2212 \u03b1E [ (Z \u2212 \u03bd)+ ]} . (1)\nNote that as a function of \u03bd, H\u03b1(\u00b7, \u03bd) is finite and convex (hence continuous)."}, {"heading": "3 CVaR Optimization in MDPs", "text": "For a policy \u00b5, we define the loss of a state x (state-action pair (x, a)) as the sum of (discounted) costs encountered by the agent when it starts at state x (state-action pair (x, a)) and then follows policy \u00b5, i.e., D\u03b8(x) = \u2211\u221e k=0 \u03b3\nkC(xk, ak) | x0 = x, \u00b5 and D\u03b8(x, a) = \u2211\u221e k=0 \u03b3\nkC(xk, ak) | x0 = x, a0 = a, \u00b5. The expected value of these two random variables are the value and action-value functions of policy \u00b5, i.e., V \u03b8(x) = E [ D\u03b8(x) ] and Q\u03b8(x, a) = E [ D\u03b8(x, a) ] . The goal in the standard discounted formulation is to find an optimal policy \u03b8\u2217 = argmin\u03b8 V \u03b8(x0).\nFor CVaR optimization in MDPs, we consider the following optimization problem: For a given confidence level \u03b1 \u2208 (0, 1) and loss tolerance \u03b2 \u2208 R,\nmin \u03b8\nV \u03b8(x0) subject to CVaR\u03b1 ( D\u03b8(x0) ) \u2264 \u03b2. (2)\nBy Theorem 16 in [28], the optimization problem (2) is equivalent to (H\u03b1 is defined by (1))\nmin \u03b8,\u03bd\nV \u03b8(x0) subject to H\u03b1 ( D\u03b8(x0), \u03bd ) \u2264 \u03b2. (3)\nTo solve (3), we employ the Lagrangian relaxation procedure [5] to convert it to the following unconstrained problem:\nmax \u03bb min \u03b8,\u03bd\n( L(\u03b8, \u03bd, \u03bb) \u25b3 = V \u03b8(x0) + \u03bb ( H\u03b1 ( D\u03b8(x0), \u03bd ) \u2212 \u03b2 )) , (4)\nwhere \u03bb is the Lagrange multiplier. The goal here is to find the saddle point of L(\u03b8, \u03bd, \u03bb), i.e., a point (\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) that satisfies L(\u03b8, \u03bd, \u03bb\u2217) \u2265 L(\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) \u2265 L(\u03b8\u2217, \u03bd\u2217, \u03bb),\u2200\u03b8, \u03bd,\u2200\u03bb > 0. This is achieved by descending in (\u03b8, \u03bd) and ascending in \u03bb using the gradients of L(\u03b8, \u03bd, \u03bb) w.r.t. \u03b8, \u03bd, and \u03bb, i.e.,1\n\u2207\u03b8L(\u03b8, \u03bd, \u03bb) = \u2207\u03b8V \u03b8(x0) + \u03bb (1\u2212 \u03b1)\u2207\u03b8E [( D\u03b8(x0)\u2212 \u03bd )+] , (5)\n\u2202\u03bdL(\u03b8, \u03bd, \u03bb) = \u03bb ( 1 +\n1 (1\u2212 \u03b1)\u2202\u03bdE [( D\u03b8(x0)\u2212 \u03bd )+] ) \u220b \u03bb ( 1\u2212 1 (1\u2212 \u03b1)P ( D\u03b8(x0) \u2265 \u03bd )) ,\n(6)\n\u2207\u03bbL(\u03b8, \u03bd, \u03bb) = \u03bd + 1 (1\u2212 \u03b1)E [( D\u03b8(x0)\u2212 \u03bd )+]\u2212 \u03b2. (7)\nWe assume that there exists a policy \u00b5(\u00b7|\u00b7; \u03b8) such that CVaR\u03b1 ( D\u03b8(x0) ) \u2264 \u03b2 (feasibility assumption). As discussed in Section 1, Ba\u0308uerle and Ott [25, 4] showed that there exists a deterministic history-dependent optimal policy for CVaR optimization. The important point is that this policy does not depend on the complete history, but only on the current time step k, current state of the system xk, and accumulated discounted cost \u2211k i=0 \u03b3\nic(xi, ai). In the following, we present a policy gradient (PG) algorithm (Sec. 4) and several actor-critic (AC) algorithms (Sec. 5.5) to optimize (4). While the PG algorithm updates its parameters after observing several trajectories, the AC algorithms are incremental and update their parameters at each time-step.\n1The notation \u220b in (6) means that the right-most term is a member of the sub-gradient set \u2202\u03bdL(\u03b8, \u03bd, \u03bb)."}, {"heading": "4 A Trajectory-based Policy Gradient Algorithm", "text": "In this section, we present a policy gradient algorithm to solve the optimization problem (4). The unit of observation in this algorithm is a system trajectory generated by following the current policy. At each iteration, the algorithm generates N trajectories by following the current policy, use them to estimate the gradients in (5)-(7), and then use these estimates to update the parameters \u03b8, \u03bd, \u03bb.\nLet \u03be = {x0, a0, c0, x1, a1, c1, . . . , xT\u22121, aT\u22121, cT\u22121, xT } be a trajectory generated by following the policy \u03b8, where x0 = x0 and xT is usually a terminal state of the system. After xk visits the terminal state, it enters a recurring sink state xR at the next time step, incurring zero cost, i.e., C(xR, a) = 0, \u2200a \u2208 A. Time index T is referred as the stopping time of the MDP. Since the transition is stochastic, T is a non-deterministic quantity. Here we assume that the policy \u00b5 is proper, i.e.,\u2211\u221e\nk=0 P(xk = x|x0 = x0, \u00b5) < \u221e for every x 6\u2208 {xS , xT }. This further means that with probability 1, the MDP exits the transient states and hits xT (and stays in xS) in finite time T . For simplicity, we assume that the agent incurs zero cost in the terminal state. Analogous results for the general case with a non-zero terminal cost can be derived using identical arguments. The loss and probability of \u03be are defined as D(\u03be) = \u2211T\u22121 k=0 \u03b3 kc(xk, ak) and P\u03b8(\u03be) = P0(x0) \u220fT\u22121\nk=0 \u00b5(ak|xk; \u03b8)P (xk+1|xk, ak), respectively. It can be easily shown that \u2207\u03b8 logP\u03b8(\u03be) = \u2211T\u22121 k=0 \u2207\u03b8 log\u00b5(ak|xk; \u03b8).\nAlgorithm 1 contains the pseudo-code of our proposed policy gradient algorithm. What appears inside the parentheses on the right-hand-side of the update equations are the estimates of the gradients of L(\u03b8, \u03bd, \u03bb) w.r.t. \u03b8, \u03bd, \u03bb (estimates of (5)-(7)) (see Appendix A.2). \u0393\u0398 is an operator that projects a vector \u03b8 \u2208 R\u03ba1 to the closest point in a compact and convex set \u0398 \u2282 R\u03ba1 , and \u0393N and \u0393\u039b are projection operators to [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] and [0, \u03bbmax], respectively. These projection operators are necessary to ensure the convergence of the algorithm. The step-size schedules satisfy the standard conditions for stochastic approximation algorithms, and ensures that the VaR parameter \u03bd update is on the fastest time-scale { \u03b63(i) } , the policy parameter \u03b8 update is on the intermediate time-scale { \u03b62(i) } , and the Lagrange multiplier \u03bb update is on the slowest time-scale { \u03b61(i) } (see Appendix A.1 for the conditions on the step-size schedules). This results in a three time-scale stochastic approximation algorithm. We prove that our policy gradient algorithm converges to a (local) saddle point of the risk-sensitive objective function L(\u03b8, \u03bd, \u03bb) (see Appendix A.3)."}, {"heading": "5 Incremental Actor-Critic Algorithms", "text": "As mentioned in Section 4, the unit of observation in our policy gradient algorithm (Algorithm 1) is a system trajectory. This may result in high variance for the gradient estimates, especially when the length of the trajectories is long. To address this issue, in this section, we propose actor-critic algorithms that use linear approximation for some quantities in the gradient estimates and update the parameters incrementally (after each state-action transition). To develop our actor-critic algorithms, we should show how the gradients of (5)-(7) are estimated in an incremental fashion. We show this in the next four subsections, followed by a subsection that contains the algorithms.\nAlgorithm 1 Trajectory-based Policy Gradient Algorithm for CVaR Optimization Input: parameterized policy \u00b5(\u00b7|\u00b7; \u03b8), confidence level \u03b1, loss tolerance \u03b2 and Lagrangian threshold \u03bbmax Initialization: policy parameter \u03b8 = \u03b80, VaR parameter \u03bd = \u03bd0, and the Lagrangian parameter \u03bb = \u03bb0 while 1 do\nfor i = 0, 1, 2, . . . do for j = 1, 2, . . . do\nGenerate N trajectories {\u03bej,i}Nj=1 by starting at x0 = x0 and following the current policy \u03b8i.\nend for\n\u03bd Update: \u03bdi+1 = \u0393N [ \u03bdi \u2212 \u03b63(i) ( \u03bbi \u2212\n\u03bbi (1\u2212 \u03b1)N\nN\u2211\nj=1\n1 { D(\u03bej,i) \u2265 \u03bdi\n})]\n\u03b8 Update: \u03b8i+1 = \u0393\u0398 [ \u03b8i \u2212 \u03b62(i) ( 1\nN\nN\u2211\nj=1\n\u2207\u03b8 log P\u03b8(\u03bej,i)|\u03b8=\u03b8iD(\u03bej,i)\n+ \u03bbi\n(1\u2212 \u03b1)N\nN\u2211\nj=1\n\u2207\u03b8 log P\u03b8(\u03bej,i)|\u03b8=\u03b8i ( D(\u03bej,i)\u2212 \u03bdi ) 1 { D(\u03bej,i) \u2265 \u03bdi\n})]\n\u03bb Update: \u03bbi+1 = \u0393\u039b [ \u03bbi + \u03b61(i) ( \u03bdi \u2212 \u03b2 + 1\n(1\u2212 \u03b1)N\nN\u2211\nj=1\n( D(\u03bej,i)\u2212 \u03bdi ) 1 { D(\u03bej,i) \u2265 \u03bdi\n})]\nend for if {\u03bbi} converges to \u03bbmax then\nSet \u03bbmax \u2190 2\u03bbmax. else\nreturn parameters \u03bd, \u03b8, \u03bb and break end if\nend while"}, {"heading": "5.1 Gradient w.r.t. the Policy Parameters \u03b8", "text": "The gradient of our objective function w.r.t. the policy parameters \u03b8 in (5) may be rewritten as\n\u2207\u03b8L(\u03b8, \u03bd, \u03bb) = \u2207\u03b8 ( E [ D\u03b8(x0) ] +\n\u03bb (1\u2212 \u03b1)E [( D\u03b8(x0)\u2212 \u03bd )+] ) . (8)\nGiven the original MDP M = (X ,A, C, P, P0) and the parameter \u03bb, we define the augmented MDP M\u0304 = (X\u0304 , A\u0304, C\u0304, P\u0304 , P\u03040) as X\u0304 = X \u00d7 R, A\u0304 = A, P\u03040(x, s) = P0(x)1{s = s0}, and\nC\u0304(x, s, a) =\n{ \u03bb(\u2212s)+/(1\u2212 \u03b1) if x = xT\nC(x, a) otherwise , P\u0304 (x\u2032, s\u2032|x, s, a) =\n{ P (x\u2032|x, a) if s\u2032 = ( s\u2212C(x, a) ) /\u03b3\n0 otherwise\nwhere xT is any terminal state of the original MDP M and sT is the value of the s part of the state when a policy \u03b8 reaches a terminal state xT after T steps, i.e., sT = 1 \u03b3T ( s0 \u2212 \u2211T\u22121k=0 \u03b3kC(xk, ak) ) . We define a class of parameterized stochastic\npolicies { \u00b5(\u00b7|x, s; \u03b8), (x, s) \u2208 X\u0304 , \u03b8 \u2208 \u0398 \u2286 R\u03ba1 } for this augmented MDP. Thus, the total (discounted) loss of this trajectory can be written as\nT\u22121\u2211\nk=0\n\u03b3kC(xk, ak) + \u03b3 T C\u0304(xT , sT , a) = D \u03b8(x0) + \u03bb (1\u2212 \u03b1) ( D\u03b8(x0)\u2212 s0 )+ . (9)\nFrom (9), it is clear that the quantity in the parenthesis of (8) is the value function of the policy \u03b8 at state (x0, s0 = \u03bd) in the augmented MDP M\u0304, i.e., V \u03b8(x0, \u03bd). Thus, it is easy to show that (the proof of the second equality can be found in the literature, e.g., [26])\n\u2207\u03b8L(\u03b8, \u03bd, \u03bb) = \u2207\u03b8V \u03b8(x0, \u03bd) = 1 1\u2212 \u03b3 \u2211\nx,s,a\n\u03c0\u03b8\u03b3(x, s, a|x0, \u03bd) \u2207 log \u00b5(a|x, s; \u03b8) Q\u03b8(x, s, a),\n(10)\nwhere \u03c0\u03b8\u03b3 is the discounted visiting distribution (defined in Section 2) and Q \u03b8 is the action-value function of policy \u03b8 in the augmented MDP M\u0304. We can show that 1 1\u2212\u03b3\u2207 log\u00b5(ak|xk, sk; \u03b8) \u00b7 \u03b4k is an unbiased estimate of \u2207\u03b8L(\u03b8, \u03bd, \u03bb), where \u03b4k = C\u0304(xk, sk, ak) + \u03b3V\u0302 (xk+1, sk+1)\u2212 V\u0302 (xk, sk) is the temporal-difference (TD) error in M\u0304, and V\u0302 is an unbiased estimator of V \u03b8 (see e.g. [8]). In our actor-critic algorithms, the critic uses linear approximation for the value function V \u03b8(x, s) \u2248 v\u22a4\u03c6(x, s) = V\u0303 \u03b8,v(x, s), where the feature vector \u03c6(\u00b7) is from low-dimensional space R\u03ba2 ."}, {"heading": "5.2 Gradient w.r.t. the Lagrangian Parameter \u03bb", "text": "We may rewrite the gradient of our objective function w.r.t. the Lagrangian parameters \u03bb in (7) as \u2207\u03bbL(\u03b8, \u03bd, \u03bb) = \u03bd\u2212\u03b2+\u2207\u03bb ( E [ D\u03b8(x0) ] + \u03bb\n(1\u2212 \u03b1)E [( D\u03b8(x0)\u2212 \u03bd )+] ) (a) = \u03bd\u2212\u03b2+\u2207\u03bbV \u03b8(x0, \u03bd).\n(11) Similar to Section 5.1, (a) comes from the fact that the quantity in the parenthesis in (11) is V \u03b8(x0, \u03bd), the value function of the policy \u03b8 at state (x0, \u03bd) in the augmented MDP M\u0304. Note that the dependence of V \u03b8(x0, \u03bd) on \u03bb comes from the definition of the cost function C\u0304 in M\u0304. We now derive an expression for \u2207\u03bbV \u03b8(x0, \u03bd), which in turn will give us an expression for \u2207\u03bbL(\u03b8, \u03bd, \u03bb).\nLemma 1 The gradient of V \u03b8(x0, \u03bd) w.r.t. the Lagrangian parameter \u03bb may be written as\n\u2207\u03bbV \u03b8(x0, \u03bd) = 1 1\u2212 \u03b3 \u2211\nx,s,a\n\u03c0\u03b8\u03b3(x, s, a|x0, \u03bd) 1\n(1\u2212 \u03b1)1{x = xT }(\u2212s) +. (12)\nProof. See Appendix B.2. From Lemma 1 and (11), it is easy to see that \u03bd\u2212\u03b2+ 1(1\u2212\u03b3)(1\u2212\u03b1)1{x = xT }(\u2212s)+ is an unbiased estimate of \u2207\u03bbL(\u03b8, \u03bd, \u03bb). An issue with this estimator is that its value is fixed to \u03bdk \u2212 \u03b2 all along a system trajectory, and only changes at the end to \u03bdk \u2212 \u03b2 + 1 (1\u2212\u03b3)(1\u2212\u03b1)(\u2212sT )+. This may affect the incremental nature of our actor-critic algorithm. To address this issue, we propose a different approach to estimate the gradients w.r.t. \u03b8 and \u03bb in Sec. 5.4 (of course this does not come for free).\nAnother important issue is that the above estimator is unbiased only if the samples are generated from the distribution \u03c0\u03b8\u03b3(\u00b7|x0, \u03bd). If we just follow the policy, then we may use \u03bdk \u2212\u03b2+ \u03b3 k\n(1\u2212\u03b1)1{xk = xT }(\u2212sk)+ as an estimate for \u2207\u03bbL(\u03b8, \u03bd, \u03bb) (see (20) and (22) in Algorithm 2). Note that this is an issue for all discounted actor-critic algorithms that their (likelihood ratio based) estimate for the gradient is unbiased only if the samples are generated from \u03c0\u03b8\u03b3 , and not just when we simply follow the policy. Although this issue was known in the community, there is a recent paper that investigates it in details [36]. Moreover, this might be a main reason that we have no convergence analysis (to the best of our knowledge) for (likelihood ratio based) discounted actorcritic algorithms.2"}, {"heading": "5.3 Sub-Gradient w.r.t. the VaR Parameter \u03bd", "text": "We may rewrite the sub-gradient of our objective function w.r.t. the VaR parameters \u03bd in (6) as\n\u2202\u03bdL(\u03b8, \u03bd, \u03bb) \u220b \u03bb ( 1\u2212 1 (1\u2212 \u03b1)P ( \u221e\u2211\nk=0\n\u03b3kC(xk, ak) \u2265 \u03bd | x0 = x0; \u03b8 )) . (13)\nFrom the definition of the augmented MDP M\u0304, the probability in (13) may be written as P(sT \u2264 0 | x0 = x0, s0 = \u03bd; \u03b8), where sT is the s part of the state in M\u0304 when we reach a terminal state, i.e., x = xT (see Section 5.1). Thus, we may rewrite (13) as\n\u2202\u03bdL(\u03b8, \u03bd, \u03bb) \u220b \u03bb ( 1\u2212 1 (1\u2212 \u03b1)P ( sT \u2264 0 | x0 = x0, s0 = \u03bd; \u03b8 )) . (14)\nFrom (14), it is easy to see that \u03bb\u2212\u03bb1{sT \u2264 0}/(1\u2212\u03b1) is an unbiased estimate of the sub-gradient of L(\u03b8, \u03bd, \u03bb) w.r.t. \u03bd. An issue with this (unbiased) estimator is that it can be only applied at the end of a system trajectory (i.e., when we reach the terminal state xT ), and thus, using it prevents us of having a fully incremental algorithm. In fact, this is the estimator that we use in our semi trajectory-based actor-critic algorithm (see (21) in Algorithm 2).\nOne approach to estimate this sub-gradient incrementally, hence having a fully incremental algorithm, is to use simultaneous perturbation stochastic approximation (SPSA) method [9]. The idea of SPSA is to estimate the sub-gradient g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) using two values of g at \u03bd\u2212 = \u03bd \u2212 \u2206 and \u03bd+ = \u03bd + \u2206, where \u2206 > 0 is a positive\n2Note that the discounted actor-critic algorithm with convergence proof in [6] is based on SPSA.\nperturbation (see Sec. 5.5 for the detailed description of \u2206).3 In order to see how SPSA can help us to estimate our sub-gradient incrementally, note that\n\u2202\u03bdL(\u03b8, \u03bd, \u03bb) = \u03bb+ \u2202\u03bd ( E [ D\u03b8(x0) ] +\n\u03bb (1\u2212 \u03b1)E [( D\u03b8(x0)\u2212 \u03bd )+] ) (a) = \u03bb+ \u2202\u03bdV \u03b8(x0, \u03bd).\n(15)\nSimilar to Sections 5.1 and 5.2, (a) comes from the fact that the quantity in the parenthesis in (15) is V \u03b8(x0, \u03bd), the value function of the policy \u03b8 at state (x0, \u03bd) in the augmented MDP M\u0304. Since the critic uses a linear approximation for the value function, i.e., V \u03b8(x, s) \u2248 v\u22a4\u03c6(x, s), in our actor-critic algorithms (see Section 5.1 and Algorithm 2), the SPSA estimate of the sub-gradient would be of the form g(\u03bd) \u2248 \u03bb+ v\u22a4 [ \u03c6(x0, \u03bd+)\u2212 \u03c6(x0, \u03bd\u2212) ] /2\u2206 (see (18) in Algorithm 2)."}, {"heading": "5.4 An Alternative Approach to Compute the Gradients", "text": "In this section, we present an alternative way to compute the gradients, especially those w.r.t. \u03b8 and \u03bb. This allows us to estimate the gradient w.r.t. \u03bb in a (more) incremental fashion (compared to the method of Section 5.2), with the cost of the need to use two different linear function approximators (instead of one used in Algorithm 2). In this approach, we define the augmented MDP slightly different than the one in Section 5.2. The only difference is in the definition of the cost function, which is defined here as (note that C(x, a) has been replaced by 0 and \u03bb has been removed)\nC\u0304(x, s, a) =\n{ (\u2212s)+/(1\u2212 \u03b1) if x = xT ,\n0 otherwise, where xT is any terminal state of the original MDP M. It is easy to see that the term\n1 (1\u2212\u03b1)E\n[( D\u03b8(x0)\u2212 \u03bd )+] appearing in the gradients of (5)-(7) is the value function of\nthe policy \u03b8 at state (x0, \u03bd) in this augmented MDP. As a result, we have Gradient w.r.t. \u03b8: It is easy to see that now this gradient (5) is the gradient of the value function of the original MDP, \u2207\u03b8V \u03b8(x0), plus \u03bb times the gradient of the value function of the augmented MDP, \u2207\u03b8V \u03b8(x0, \u03bd), both at the initial states of these MDPs (with abuse of notation, we use V for the value function of both MDPs). Thus, using linear approximators u\u22a4f(x, s) and v\u22a4\u03c6(x, s) for the value functions of the original and augmented MDPs, \u2207\u03b8L(\u03b8, \u03bd, \u03bb) can be estimated as \u2207\u03b8 log\u00b5(ak|xk, sk; \u03b8) \u00b7 (\u01ebk + \u03bb\u03b4k), where \u01ebk and \u03b4k are the TD-errors of these MDPs. Gradient w.r.t. \u03bb: Similar to the case for \u03b8, it is easy to see that this gradient (7) is \u03bd \u2212 \u03b2 plus the value function of the augmented MDP, V \u03b8(x0, \u03bd), and thus, can be estimated incrementally as \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2248 \u03bd \u2212 \u03b2 + v\u22a4\u03c6(x, s). Sub-Gradient w.r.t. \u03bd: This sub-gradient (6) is \u03bb times one plus the gradient w.r.t. \u03bd of the value function of the augmented MDP, \u2207\u03bdV \u03b8(x0, \u03bd), and thus using SPSA, can be estimated incrementally as \u03bb ( 1 + v\u22a4 [ \u03c6(x0,\u03bd+)\u2212\u03c6(x0,\u03bd\u2212) ] 2\u2206 ) . Algorithm 3 in Appendix B.3 contains the pseudo-code of the resulting algorithm. 3SPSA-based gradient estimate was first proposed in [33] and has been widely used in various settings, especially those involving high-dimensional parameter. The SPSA estimate described above is two-sided. It can also be implemented single-sided, where we use the values of the function at \u03bd and \u03bd+. We refer the readers to [9] for more details on SPSA and to [21] for its application in learning in risk-sensitive MDPs."}, {"heading": "5.5 Actor-Critic Algorithms", "text": "In this section, we present two actor-critic algorithms for optimizing the risk-sensitive measure (4). These algorithms are based on the gradient estimates of Sections 5.1-5.3. While the first algorithm (SPSA-based) is fully incremental and updates all the parameters \u03b8, \u03bd, \u03bb at each time-step, the second one updates \u03b8 at each time-step and updates \u03bd and \u03bb only at the end of each trajectory, thus given the name semi trajectory-based. Algorithm 2 contains the pseudo-code of these algorithms. The projection operators \u0393\u0398, \u0393N , and \u0393\u039b are defined as in Section 4 and are necessary to ensure the convergence of the algorithms. The step-size schedules satisfy the standard conditions for stochastic approximation algorithms, and ensures that the critic update is on the fastest time-scale{ \u03b64(k) } , the policy and VaR parameter updates are on the intermediate time-scale, with \u03bd-update { \u03b63(k) } being faster than \u03b8-update { \u03b62(k) } , and finally the Lagrange multiplier update is on the slowest time-scale { \u03b61(k) } (see Appendix B.1 for the conditions on these step-size schedules). This results in four time-scale stochastic approximation algorithms. We prove that these actor-critic algorithms converge to a (local) saddle point of the risk-sensitive objective function L(\u03b8, \u03bd, \u03bb) (see Appendix B.4)."}, {"heading": "6 Experimental Results", "text": "We consider an optimal stopping problem in which the state at each time step k \u2264 T consists of the cost ck and time k, i.e., x = (ck, k), where T is the stopping time. The agent (buyer) should decide either to accept the present cost or wait. If she accepts or when k = T , the system reaches a terminal state and the cost ck is received, otherwise, she receives the cost ph and the new state is (ck+1, k + 1), where ck+1 is fuck w.p. p and fdck w.p. 1\u2212p (fu > 1 and fd < 1 are constants). Moreover, there is a discounted factor \u03b3 \u2208 (0, 1) to account for the increase in the buyer\u2019s affordability. The problem has been described in more details in Appendix C. Note that if we change cost to reward and minimization to maximization, this is exactly the American option pricing problem, a standard testbed to evaluate risk-sensitive algorithms (e.g., [34]). Since the state space is continuous, solving for an exact solution via DP is infeasible, and thus, it requires approximation and sampling techniques.\nWe compare the performance of our risk-sensitive policy gradient Alg. 1 (PGCVaR) and two actor-critic Algs. 2 (AC-CVaR-SPSA,AC-CVaR-Semi-Traj) with their riskneutral counterparts (PG and AC) (see Appendix C for the details of these experiments). Fig. 1 shows the distribution of the discounted cumulative cost D\u03b8(x0) for the policy \u03b8 learned by each of these algorithms. From left to right, the columns display the first two moments, the whole (distribution), and zoom on the right-tail of these distributions. The results indicate that the risk-sensitive algorithms yield a higher expected loss, but less variance, compared to the risk-neutral methods. More precisely, the loss distributions of the risk-sensitive algorithms have lower right-tail than their risk-neutral counterparts. Table 1 summarizes the performance of these algorithms. The numbers reiterate what we concluded from Fig. 1.\nAlgorithm 2 Actor-Critic Algorithm for CVaR Optimization Input: Parameterized policy \u00b5(\u00b7|\u00b7; \u03b8) and value function feature vector \u03c6(\u00b7) (both over the augmented MDP M\u0304), confidence level \u03b1, loss tolerance \u03b2 and Lagrangian threshold \u03bbmax Initialization: policy parameters \u03b8 = \u03b80; VaR parameter \u03bd = \u03bd0; Lagrangian parameter \u03bb = \u03bb0; value function weight vector v = v0 while 1 do\n// (1) SPSA-based Algorithm: for k = 0, 1, 2, . . . do\nDraw action ak \u223c \u00b5(\u00b7|xk, sk; \u03b8k); Observe cost C\u0304(xk, sk, ak); Observe next state (xk+1, sk+1) \u223c P\u0304 (\u00b7|xk, sk, ak); // note that sk+1 = (sk \u2212 C ( xk, ak) ) /\u03b3 (see Sec. 5.1)\nTD Error: \u03b4k(vk) = C\u0304(xk, sk, ak) + \u03b3v \u22a4 k \u03c6(xk+1, sk+1)\u2212 v\u22a4k \u03c6(xk, sk)\n(16)\nCritic Update: vk+1 = vk + \u03b64(k)\u03b4k(vk)\u03c6(xk, sk) (17)\nActor Updates: \u03bdk+1 = \u0393N ( \u03bdk \u2212 \u03b63(k) ( \u03bbk + v\u22a4k [ \u03c6 ( x0, \u03bdk +\u2206k ) \u2212 \u03c6(x0, \u03bdk \u2212\u2206k) ]\n2\u2206k\n))\n(18)\n\u03b8k+1 = \u0393\u0398 ( \u03b8k \u2212 \u03b62(k) 1\u2212 \u03b3\u2207\u03b8 log \u00b5(ak|xk, sk; \u03b8)|\u03b8=\u03b8k \u00b7 \u03b4k(vk) )\n(19)\n\u03bbk+1 = \u0393\u039b ( \u03bbk + \u03b61(k) ( \u03bdk \u2212 \u03b2 + \u03b3k 1\u2212 \u03b11{xk = xT }(\u2212sk) + ))\n(20)\nend for // (2) Semi Trajectory-based Algorithm: for i = 0, 1, 2, . . . do\nSet k = 0 and (xk, sk) = (x0, \u03bdi) while xk 6= xT do\nDraw action ak \u223c \u00b5(\u00b7|xk, sk; \u03b8k); Observe C\u0304(xk, sk, ak) and (xk+1, sk+1) \u223c P\u0304 (\u00b7|xk, sk, ak) For fixed values of \u03bdi and \u03bbi, execute (16)-(17) and (19) with (\u03b64(k), \u03b62(k)) replaced by (\u03b64(i), \u03b62(i)); k \u2190 k + 1;\nend while // we reach a terminal state (xT , sT ) (end of the trajectory)\n\u03bd Update: \u03bdi+1 = \u0393N ( \u03bdi \u2212 \u03b62(i) ( \u03bbi \u2212\n\u03bbi 1\u2212 \u03b11\n{ sT \u2264 0 })) (21)\n\u03bb Update: \u03bbi+1 = \u0393\u039b ( \u03bbi + \u03b61(i) ( \u03bdi \u2212 \u03b2 + \u03b3 T (1\u2212 \u03b1) (\u2212sT ) +)) (22)\nend for if {\u03bbi} converges to \u03bbmax then\nSet \u03bbmax \u2190 2\u03bbmax. else\nreturn policy and value function parameters v, \u03bd, \u03b8, \u03bb and break end if\nend while"}, {"heading": "7 Conclusions and Future Work", "text": "We proposed novel policy gradient and actor critic (AC) algorithms for CVaR optimization in MDPs. We provided proofs of convergence (in the appendix) to locally risk-sensitive optimal policies for the proposed algorithms. Further, using an optimal stopping problem, we observed that our algorithms resulted in policies whose loss distributions have lower right-tail compared to their risk-neutral counterparts. This is extremely important for a risk averse decision-maker, especially if the right-tail contains catastrophic losses. Future work includes: 1) Providing convergence proofs for our AC algorithms when the samples are generated by following the policy and not from its discounted visiting distribution (this can be wasteful in terms of samples), 2) Here we established asymptotic limits for our algorithms. To the best of our knowledge, there are no convergence rate results available for multi-timescale stochastic approximation schemes, and hence, for AC algorithms. This is true even for the AC algorithms that\ndo not incorporate any risk criterion. It would be an interesting research direction to obtain finite-time bounds on the quality of the solution obtained by these algorithms, 3) Since interesting losses in the CVaR optimization problems are those that exceed the VaR, in order to compute more accurate estimates of the gradients, it is necessary to generate more samples in the right-tail of the loss distribution (events that are observed with a very low probability). Although importance sampling methods have been used to address this problem [3, 35], several issues, particularly related to the choice of the sampling distribution, have remained unsolved that are needed to be investigated, and finally, 4) Evaluating our algorithms in more challenging problems."}, {"heading": "A Technical Details of the Trajectory-based Policy Gradient Algorithm", "text": ""}, {"heading": "A.1 Assumptions", "text": "We make the following assumptions for the step-size schedules in our algorithms:\n(A1) For any state-action pair (x, a), \u00b5(a|x; \u03b8) is continuously differentiable in \u03b8 and \u2207\u03b8\u00b5(a|x; \u03b8) is a Lipschitz function in \u03b8 for every a \u2208 A and x \u2208 X .\n(A2) The Markov chain induced by any policy \u03b8 is irreducible and aperiodic.\n(A3) The step size schedules {\u03b63(i)}, {\u03b62(i)}, and {\u03b61(i)} satisfy\n\u2211\ni\n\u03b61(i) = \u2211\ni\n\u03b62(i) = \u2211\ni \u03b63(i) = \u221e, (23) \u2211\ni\n\u03b61(i) 2,\n\u2211\ni\n\u03b62(i) 2,\n\u2211\ni\n\u03b63(i) 2 < \u221e, (24)\n\u03b61(i) = o ( \u03b62(i) ) , \u03b62(i) = o ( \u03b63(i) ) . (25)\n(23) and (24) are standard step-size conditions in stochastic approximation algorithms, and (25) indicates that the update corresponds to {\u03b63(i)} is on the fastest timescale, the update corresponds to {\u03b62(i)} is on the intermediate time-scale, and the update corresponds to {\u03b61(i)} is on the slowest time-scale."}, {"heading": "A.2 Computing the Gradients", "text": "i) \u2207\u03b8L(\u03b8, \u03bd, \u03bb): Gradient of L(\u03b8, \u03bd, \u03bb) w.r.t. \u03b8 By expanding the expectations in the definition of the objective function L(\u03b8, \u03bd, \u03bb) in (4), we obtain\nL(\u03b8, \u03bd, \u03bb) = \u2211\n\u03be\nP\u03b8(\u03be)D(\u03be) + \u03bb\u03bd + \u03bb 1\u2212 \u03b1 \u2211\n\u03be\nP\u03b8(\u03be) ( D(\u03be) \u2212 \u03bd )+ \u2212 \u03bb\u03b2.\nBy taking gradient with respect to \u03b8, we have\n\u2207\u03b8L(\u03b8, \u03bd, \u03bb) = \u2211\n\u03be\n\u2207\u03b8P\u03b8(\u03be)D(\u03be) + \u03bb 1\u2212 \u03b1 \u2211\n\u03be\n\u2207\u03b8P\u03b8(\u03be) ( D(\u03be)\u2212 \u03bd )+ .\nThis gradient can rewritten as\n\u2207\u03b8L(\u03b8, \u03bd, \u03bb) = \u2211\n\u03be\nP\u03b8(\u03be)\u00b7\u2207\u03b8 logP\u03b8(\u03be) ( D(\u03be) + \u03bb\n1\u2212 \u03b1 ( D(\u03be) \u2212 \u03bd ) 1 { D(\u03be) \u2265 \u03bd\n}) ,\n(26)\nwhere\n\u2207\u03b8 logP\u03b8(\u03be) =\u2207\u03b8 { T\u22121\u2211\nk=0\nlogP (xk+1|xk, ak) + log\u00b5(ak|xk; \u03b8) + log 1{x0 = x0} }\n= T\u22121\u2211\nk=0\n1\n\u00b5(ak|xk; \u03b8) \u2207\u03b8\u00b5(ak|xk; \u03b8)\n=\nT\u22121\u2211\nk=0\n\u2207\u03b8 log \u00b5(ak|xk; \u03b8).\nii) \u2202\u03bdL(\u03b8, \u03bd, \u03bb): Sub-differential of L(\u03b8, \u03bd, \u03bb) w.r.t. \u03bd From the definition of L(\u03b8, \u03bd, \u03bb), we can easily see that L(\u03b8, \u03bd, \u03bb) is a convex\nfunction in \u03bd for any fixed \u03b8 \u2208 \u0398. Note that for every fixed \u03bd and any \u03bd\u2032, we have ( D(\u03be)\u2212 \u03bd\u2032 )+ \u2212 ( D(\u03be)\u2212 \u03bd )+ \u2265 g \u00b7 (\u03bd\u2032 \u2212 \u03bd),\nwhere g is any element in the set of sub-derivatives:\ng \u2208 \u2202\u03bd ( D(\u03be)\u2212 \u03bd )+ \u25b3 =    \u22121 if \u03bd < D(\u03be), \u2212q : q \u2208 [0, 1] if \u03bd = D(\u03be), 0 otherwise.\nSince L(\u03b8, \u03bd, \u03bb) is finite-valued for any \u03bd \u2208 R, by the additive rule of sub-derivatives, we have\n\u2202\u03bdL(\u03b8, \u03bd, \u03bb) =   \u2212 \u03bb 1\u2212 \u03b1 \u2211\n\u03be\nP\u03b8(\u03be)1 { D(\u03be) > \u03bd } \u2212 \u03bbq 1\u2212 \u03b1 \u2211\n\u03be\nP\u03b8(\u03be)1 { D(\u03be) = \u03bd } + \u03bb | q \u2208 [0, 1]    .\n(27) In particular for q = 1, we may write the sub-gradient of L(\u03b8, \u03bd, \u03bb) w.r.t. \u03bd as\n\u2202\u03bdL(\u03b8, \u03bd, \u03bb)|q=0 = \u03bb\u2212 \u03bb 1\u2212 \u03b1\n\u2211\n\u03be\nP\u03b8(\u03be)\u00b71 { D(\u03be) \u2265 \u03bd } or \u03bb\u2212 \u03bb 1\u2212 \u03b1 \u2211\n\u03be\nP\u03b8(\u03be)\u00b71 { D(\u03be) \u2265 \u03bd } \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb).\niii) \u2207\u03bbL(\u03b8, \u03bd, \u03bb): Gradient of L(\u03b8, \u03bd, \u03bb) w.r.t. \u03bb Since L(\u03b8, \u03bd, \u03bb) is a linear function in \u03bb, obviously one can express the gradient of\nL(\u03b8, \u03bd, \u03bb) w.r.t. \u03bb as follows:\n\u2207\u03bbL(\u03b8, \u03bd, \u03bb) = \u03bd \u2212 \u03b2 + 1 1\u2212 \u03b1 \u2211\n\u03be\nP\u03b8(\u03be) \u00b7 ( D(\u03be) \u2212 \u03bd ) 1 { D(\u03be) \u2265 \u03bd } . (28)"}, {"heading": "A.3 Proof of Convergence of the Policy Gradient Algorithm", "text": "In this section, we prove the convergence of our policy gradient algorithm (Algorithm 1).\nTheorem 2 Suppose \u03bb\u2217 \u2208 [0, \u03bbmax). Then the sequence of (\u03b8, \u03bb)\u2212updates in Algorithm 1 converges to a (local) saddle point (\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) of our objective function L(\u03b8, \u03bd, \u03bb) almost surely, i.e., it satisfies L(\u03b8, \u03bd, \u03bb\u2217) \u2265 L(\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) \u2265 L(\u03b8\u2217, \u03bd\u2217, \u03bb), \u2200(\u03b8, \u03bd) \u2208 \u0398 \u00d7 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] \u2229 B(\u03b8\u2217,\u03bd\u2217)(r) for some r > 0 and \u2200\u03bb \u2208 [0, \u03bbmax]. Note that B(\u03b8\u2217,\u03bd\u2217)(r) represents a hyper-dimensional ball centered at (\u03b8\u2217, \u03bd\u2217) with radius r.\nSince \u03bd converges on the faster timescale than \u03b8 and \u03bb, the \u03bd-update can be rewritten by assuming (\u03b8, \u03bb) as invariant quantities, i.e.,\n\u03bdi+1 = \u0393N [ \u03bdi \u2212 \u03b63(i) ( \u03bb\u2212 \u03bb\n(1 \u2212 \u03b1)N\nN\u2211\nj=1\n1 { D(\u03bej,i) \u2265 \u03bdi })] . (29)\nConsider the continuous time dynamics of \u03bd defined using differential inclusion\n\u03bd\u0307 \u2208 \u03a5\u03bd [\u2212g(\u03bd)] , \u2200g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb), (30)\nwhere\n\u03a5\u03bd [K(\u03bd)] := lim 0<\u03b7\u21920 \u0393N (\u03bd + \u03b7K(\u03bd))\u2212 \u0393N (\u03bd) \u03b7 .\nand \u0393N is the Euclidean projection operator of \u03bd to [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ], i.e., \u0393N (\u03bd) = argmin \u03bd\u0302\u2208[\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] 1 2\u2016\u03bd\u2212 \u03bd\u0302\u201622. In general \u0393N (\u03bd) is not necessarily differentiable. \u03a5\u03bd [K(\u03bd)] is the left directional derivative of the function \u0393N (\u03bd) in the direction of K(\u03bd). By using the left directional derivative \u03a5\u03bd [\u2212g(\u03bd)] in the sub-gradient descent algorithm for \u03bd, the gradient will point at the descent direction along the boundary of \u03bd whenever the \u03bd\u2212update hits its boundary.\nFurthermore, since \u03bd converges on the faster timescale than \u03b8, and \u03bb is on the slowest time-scale, the \u03b8-update can be rewritten using the converged \u03bd\u2217(\u03b8) and assuming \u03bb as an invariant quantity, i.e.,\n\u03b8i+1 =\u0393\u0398 [ \u03b8i \u2212 \u03b62(i) ( 1\nN\nN\u2211\nj=1\n\u2207\u03b8 logP\u03b8(\u03bej,i)|\u03b8=\u03b8iD(\u03bej,i)\n+ \u03bb\n(1 \u2212 \u03b1)N\nN\u2211\nj=1\n\u2207\u03b8 logP\u03b8(\u03bej,i)|\u03b8=\u03b8i ( D(\u03bej,i)\u2212 \u03bd ) 1 { D(\u03bej,i) \u2265 \u03bd\u2217(\u03b8i) })] .\nConsider the continuous time dynamics of \u03b8 \u2208 \u0398:\n\u03b8\u0307 = \u03a5\u03b8 [\u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)] |\u03bd=\u03bd\u2217(\u03b8), (31)\nwhere\n\u03a5\u03b8[K(\u03b8)] := lim 0<\u03b7\u21920 \u0393\u0398(\u03b8 + \u03b7K(\u03b8))\u2212 \u0393\u0398(\u03b8) \u03b7 .\nand \u0393\u0398 is the Euclidean projection operator of \u03b8 to \u0398, i.e., \u0393\u0398(\u03b8) = argmin\u03b8\u0302\u2208\u0398 1 2\u2016\u03b8\u2212 \u03b8\u0302\u201622. Similar to the analysis of \u03bd, \u03a5\u03b8[K(\u03b8)] is the left directional derivative of the\nfunction \u0393\u0398(\u03b8) in the direction of K(\u03b8). By using the left directional derivative \u03a5\u03b8 [\u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)] in the gradient descent algorithm for \u03b8, the gradient will point at the descent direction along the boundary of \u0398 whenever the \u03b8\u2212update hits its boundary.\nFinally, since \u03bb-update converges in a slowest time-scale, the \u03bb-update can be rewritten using the converged \u03b8\u2217(\u03bb) and \u03bd\u2217(\u03bb), i.e.,\n\u03bbi+1 = \u0393\u039b  \u03bbi + \u03b61(i) ( \u03bd\u2217(\u03bbi) + 1\n1\u2212 \u03b1 1 N\nN\u2211\nj=1\n( D(\u03bej,i)\u2212 \u03bd\u2217(\u03bbi) )+ \u2212 \u03b2 )  .\n(32) Consider the continuous time system\n\u03bb\u0307(t) = \u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb) ] , \u03bb(t) \u2265 0, (33)\nwhere\n\u03a5\u03bb[K(\u03bb)] := lim 0<\u03b7\u21920\n\u0393\u039b ( \u03bb+ \u03b7K(\u03bb) ) \u2212 \u0393\u039b(\u03bb)\n\u03b7 .\nand\u0393\u039b is the Euclidean projection operator of \u03bb to [0, \u03bbmax], i.e., \u0393\u039b(\u03bb) = argmin\u03bb\u0302\u2208[0,\u03bbmax] 1 2\u2016\u03bb\u2212 \u03bb\u0302\u201622. Similar to the analysis of (\u03bd, \u03b8), \u03a5\u03bb[K(\u03bb)] is the left directional derivative of the function \u0393\u039b(\u03bb) in the direction of K(\u03bb). By using the left directional derivative \u03a5\u03bb [\u2207\u03bbL(\u03b8, \u03bd, \u03bb)] in the gradient ascent algorithm for \u03bb, the gradient will point at the ascent direction along the boundary of [0, \u03bbmax] whenever the \u03bb\u2212update hits its boundary.\nDefine L\u2217(\u03bb) = L(\u03b8\u2217(\u03bb), \u03bd\u2217(\u03bb), \u03bb),\nfor \u03bb \u2265 0 where (\u03b8\u2217(\u03bb), \u03bd\u2217(\u03bb)) \u2208 \u0398\u00d7 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] is a local minimum of L(\u03b8, \u03bd, \u03bb) for fixed \u03bb \u2265 0, i.e., L(\u03b8, \u03bd, \u03bb) \u2265 L(\u03b8\u2217(\u03bb), \u03bd\u2217(\u03bb), \u03bb) for any (\u03b8, \u03bd) \u2208 \u0398\u00d7[\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ]\u2229 B(\u03b8\u2217(\u03bb),\u03bd\u2217(\u03bb))(r) for some r > 0. Next, we want to show that the ODE (33) is actually a gradient ascent of the Lagrangian function using the envelope theorem in mathematical economics [23]. The envelope theorem describes sufficient conditions for the derivative of L\u2217 with respect to \u03bb where it equals to the partial derivative of the objective function L with respect to \u03bb, holding (\u03b8, \u03bd) at its local optimum (\u03b8, \u03bd) = (\u03b8\u2217(\u03bb), \u03bd\u2217(\u03bb)). We will show that \u2207\u03bbL\u2217(\u03bb) coincides with with \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb) as follows.\nTheorem 3 The value function L\u2217 is absolutely continuous. Furthermore,\nL\u2217(\u03bb) = L\u2217(0) +\n\u222b \u03bb\n0\n\u2207\u03bb\u2032L(\u03b8, \u03bd, \u03bb\u2032) \u2223\u2223\u2223 \u03b8=\u03b8\u2217(s),\u03bd=\u03bd\u2217(s),\u03bb\u2032=s ds, \u03bb \u2265 0. (34)\nProof. The proof follows from analogous arguments of Lemma 4.3 in [13]. From the definition of L\u2217, observe that for any \u03bb\u2032, \u03bb\u2032\u2032 \u2265 0 with \u03bb\u2032 < \u03bb\u2032\u2032, |L\u2217(\u03bb\u2032\u2032)\u2212 L\u2217(\u03bb\u2032)| \u2264 sup\n\u03b8\u2208\u0398,\u03bd\u2208[\u2212Cmax 1\u2212\u03b3 , Cmax 1\u2212\u03b3 ]\n|L(\u03b8, \u03bd, \u03bb\u2032\u2032)\u2212 L(\u03b8, \u03bd, \u03bb\u2032)|\n= sup \u03b8\u2208\u0398,\u03bd\u2208[\u2212Cmax\n1\u2212\u03b3 , Cmax 1\u2212\u03b3 ]\n\u2223\u2223\u2223\u2223\u2223 \u222b \u03bb\u2032\u2032 \u03bb\u2032 \u2207\u03bbL(\u03b8, \u03bd, s)ds \u2223\u2223\u2223\u2223\u2223\n\u2264 \u222b \u03bb\u2032\u2032\n\u03bb\u2032 sup\n\u03b8\u2208\u0398,\u03bd\u2208[\u2212Cmax 1\u2212\u03b3 , Cmax 1\u2212\u03b3 ]\n|\u2207\u03bbL(\u03b8, \u03bd, s)| ds \u2264 3Cmax\n(1\u2212 \u03b1)(1\u2212 \u03b3) (\u03bb \u2032\u2032 \u2212 \u03bb\u2032).\nThis implies that L\u2217 is absolutely continuous. Therefore, L\u2217 is continuous everywhere and differentiable almost everywhere.\nBy the Milgrom-Segal envelope theorem of mathematical economics (Theorem 1 of [23]), one can conclude that the derivative of L\u2217(\u03bb) coincides with the derivative of L(\u03b8, \u03bd, \u03bb) at the point of differentiability \u03bb and \u03b8 = \u03b8\u2217(\u03bb), \u03bd = \u03bd\u2217(\u03bb). Also since L\u2217 is absolutely continuous, the limit of (L\u2217(\u03bb) \u2212 L\u2217(\u03bb\u2032))/(\u03bb \u2212 \u03bb\u2032) at \u03bb \u2191 \u03bb\u2032 (or \u03bb \u2193 \u03bb\u2032) coincides with the lower/upper directional derivatives if \u03bb\u2032 is a point of nondifferentiability. Thus, there is only a countable number of non-differentiable points in L\u2217 and each point of non-differentiability has the same directional derivatives as the point slightly beneath (in the case of \u03bb \u2193 \u03bb\u2032) or above (in the case of \u03bb \u2191 \u03bb\u2032) it. As the set of non-differentiable points of L\u2217 has measure zero, it can then be interpreted that \u2207\u03bbL\u2217(\u03bb) coincides with \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb), i.e., expression (34) holds. Remark 1 It can be easily shown that L\u2217(\u03bb) is a concave function. Since for given \u03b8 and \u03bd, L(\u03b8, \u03bd, \u03bb) is a linear function in \u03bb. Therefore, for any \u03b1\u2032 \u2208 [0, 1], \u03b1\u2032L\u2217(\u03bb1) + (1\u2212\u03b1\u2032)L\u2217(\u03bb2) \u2264 L\u2217(\u03b1\u2032\u03bb1+(1\u2212\u03b1\u2032)\u03bb2), i.e., L\u2217(\u03bb) is a concave function. Concavity of L\u2217 implies that it is continuous and directionally (both left hand and right hand) differentiable in int dom(L\u2217). Furthermore at any \u03bb = \u03bb\u0303 such that the derivative of L(\u03b8, \u03bd, \u03bb) with respect of \u03bb at \u03b8 = \u03b8\u2217(\u03bb), \u03bd = \u03bd\u2217(\u03bb) exists, by Theorem 1 of [23], \u2207\u03bbL\u2217(\u03bb)|\u03bb=\u03bb\u0303+ = (L\n\u2217(\u03bb\u0303+)\u2212L\u2217(\u03bb\u0303))/(\u03bb\u0303+\u2212\u03bb\u0303) \u2265 \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bb\u0303 \u2265 (L\u2217(\u03bb\u0303\u2212) \u2212 L\u2217(\u03bb\u0303))/(\u03bb\u0303\u2212 \u2212 \u03bb\u0303) = \u2207\u03bbL\u2217(\u03bb)|\u03bb=\u03bb\u0303\u2212 . Furthermore concavity of L\n\u2217 implies \u2207\u03bbL\u2217(\u03bb)|\u03bb=\u03bb\u0303+ \u2264 \u2207\u03bbL\n\u2217(\u03bb)|\u03bb=\u03bb\u0303\u2212 . Combining these arguments, one obtains \u2207\u03bbL\u2217(\u03bb)|\u03bb=\u03bb\u0303+ = \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bb\u0303 = \u2207\u03bbL\n\u2217(\u03bb)|\u03bb=\u03bb\u0303\u2212 . In order to prove the main convergence result, we need the following standard\nassumptions and remarks. Assumption 4 For any given x0 \u2208 X and \u03b8 \u2208 \u0398, the set {( \u03bd, g(\u03bd) )\n| g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) } is closed.\nRemark 2 For any given \u03b8 \u2208 \u0398, \u03bb \u2265 0, and g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb), we have |g(\u03bd)| \u2264 3\u03bb(1 + |\u03bd|)/(1 \u2212 \u03b1). (35)\nTo see this, recall from definition that g can be parameterized by q as, for q \u2208 [0, 1],\ng(\u03bd) = \u2212 \u03bb (1\u2212 \u03b1)\n\u2211\n\u03be\nP\u03b8(\u03be)1 {D(\u03be) > \u03bd} \u2212 \u03bbq 1\u2212 \u03b1 \u2211\n\u03be\nP\u03b8(\u03be)1 {D(\u03be) = \u03bd}+ \u03bb.\nIt is obvious that |1 {D(\u03be) = \u03bd}| , |1 {D(\u03be) > \u03bd}| \u2264 1+|\u03bd|. Thus, \u2223\u2223\u2223 \u2211 \u03be P\u03b8(\u03be)1 {D(\u03be) > \u03bd} \u2223\u2223\u2223 \u2264 sup\u03be |1 {D(\u03be) > \u03bd}| \u2264 1 + |\u03bd|, and \u2223\u2223\u2223 \u2211 \u03be P\u03b8(\u03be)1 {D(\u03be) = \u03bd} \u2223\u2223\u2223 \u2264 1 + |\u03bd|. Recalling 0 < (1\u2212 q), (1 \u2212 \u03b1) < 1, these arguments imply the claim of (35).\nBefore getting into the main result, we need the following technical proposition.\nProposition 5 \u2207\u03b8L(\u03b8, \u03bd, \u03bb) is Lipschitz in \u03b8.\nProof. Recall that\n\u2207\u03b8L(\u03b8, \u03bd, \u03bb) = \u2211\n\u03be\nP\u03b8(\u03be) \u00b7 \u2207\u03b8 logP\u03b8(\u03be) ( D(\u03be) + \u03bb\n1\u2212 \u03b1 ( D(\u03be)\u2212 \u03bd ) 1 { D(\u03be) \u2265 \u03bd\n})\nand\u2207\u03b8 logP\u03b8(\u03be) = \u2211T\u22121\nk=0 \u2207\u03b8\u00b5(ak|xk; \u03b8)/\u00b5(ak|xk; \u03b8) whenever\u00b5(ak|xk; \u03b8) \u2208 (0, 1]. Now Assumption (A1) implies that \u2207\u03b8\u00b5(ak|xk; \u03b8) is a Lipschitz function in \u03b8 for any a \u2208 A and k \u2208 {0, . . . , T \u2212 1} and \u00b5(ak|xk; \u03b8) is differentiable in \u03b8. Therefore, by recalling that P\u03b8(\u03be) = \u220fT\u22121 k=0 P (xk+1|xk, ak)\u00b5(ak|xk; \u03b8)1{x0 = x0} and by combining these arguments and noting that the sum of products of Lipschitz functions is Lipschitz, one concludes that \u2207\u03b8L(\u03b8, \u03bd, \u03bb) is Lipschitz in \u03b8.\nRemark 3 \u2207\u03b8L(\u03b8, \u03bd, \u03bb) is Lipschitz in \u03b8 implies that \u2016\u2207\u03b8L(\u03b8, \u03bd, \u03bb)\u20162 \u2264 2(\u2016\u2207\u03b8L(\u03b80, \u03bd, \u03bb)\u2016+ \u2016\u03b80\u2016)2 + 2\u2016\u03b8\u20162 which further implies that\n\u2016\u2207\u03b8L(\u03b8, \u03bd, \u03bb)\u20162 \u2264 K1(1 + \u2016\u03b8\u20162).\nfor K1 = 2max(1, (\u2016\u2207\u03b8L(\u03b80, \u03bd, \u03bb)\u2016 + \u2016\u03b80\u2016)2) > 0. Similarly, \u2207\u03b8 logP\u03b8(\u03be) is Lipschitz implies that \u2016\u2207\u03b8 logP\u03b8(\u03be)\u20162 \u2264 K2(\u03be)(1 + \u2016\u03b8\u20162). for a positive random variableK2(\u03be). Furthermore, since T < \u221e w.p. 1, \u00b5(ak|xk; \u03b8) \u2208 (0, 1] and \u2207\u03b8\u00b5(ak|xk; \u03b8) is Lipschitz for any k < T , K2(\u03be) < \u221e w.p. 1.\nWe are now in a position to prove the convergence analysis of Theorem 2.\nProof. [Proof of Theorem 2] We split the proof into the following four steps:\nStep 1 (Convergence of \u03bd\u2212update) Since \u03bd converges in a faster time scale than \u03b8 and \u03bb, one can assume both \u03b8 and \u03bb as fixed quantities in the \u03bd-update, i.e.,\n\u03bdi+1 = \u0393N  \u03bdi + \u03b63(i)   \u03bb (1\u2212 \u03b1)N N\u2211\nj=1\n1 { D(\u03bej,i) \u2265 \u03bdi } \u2212 \u03bb+ \u03b4\u03bdi+1     , (36)\nand\n\u03b4\u03bdi+1 = \u03bb\n1\u2212 \u03b1\n \u2212 1\nN\nN\u2211\nj=1\n1 { D(\u03bej,i) \u2265 \u03bdi } + \u2211\n\u03be\nP\u03b8(\u03be)1{D(\u03be) \u2265 \u03bdi}   . (37)\nFirst, one can show that \u03b4\u03bdi+1 is square integrable, i.e.\nE[\u2016\u03b4\u03bdi+1\u20162 | F\u03bd,i] \u2264 4 ( \u03bbmax 1\u2212 \u03b1 )2\nwhere F\u03bd,i = \u03c3 ( \u03bdm, \u03b4\u03bdm, m \u2264 i ) is the filtration of \u03bdi generated by different independent trajectories. Second, since the history trajectories are generated based on the sampling probability mass function P\u03b8(\u03be), expression (27) implies that E [\u03b4\u03bdi+1 | F\u03bd,i] = 0. Therefore, the \u03bd-update is a stochastic approximation of the ODE (30) with a Martingale difference error term, i.e.,\n\u03bb 1\u2212 \u03b1 \u2211\n\u03be\nP\u03b8(\u03be)1{D(\u03be) \u2265 \u03bdi} \u2212 \u03bb \u2208 \u2212\u2202\u03bdL(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bdi .\nThen one can invoke Corollary 4 in Chapter 5 of [14] (stochastic approximation theory for non-differentiable systems) to show that the sequence {\u03bdi}, \u03bdi \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax1\u2212\u03b3 ] converges almost surely to a fixed point \u03bd\u2217 \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax1\u2212\u03b3 ] of differential inclusion (31), where \u03bd\u2217 \u2208 Nc := {\u03bd \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax1\u2212\u03b3 ] : \u03a5\u03bd [\u2212g(\u03bd)] = 0, g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb)}. To justify the assumptions of this theorem, 1) from Remark 2, the Lipschitz property is satisfied, i.e., supg(\u03bd)\u2208\u2202\u03bdL(\u03b8,\u03bd,\u03bb) |g(\u03bd)| \u2264 3\u03bb(1 + |\u03bd|)/(1 \u2212 \u03b1), 2) \u2202\u03bdL(\u03b8, \u03bd, \u03bb) is a convex compact set by definition, 3) Assumption 4 implies that {(\u03bd, g(\u03bd)) | g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb)} is a closed set. This implies \u2202\u03bdL(\u03b8, \u03bd, \u03bb) is an upper semi-continuous set valued mapping 4) the step-size rule follows from (A.1), 5) the Martingale difference assumption follows from (37), and 6) \u03bdi \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ], \u2200i implies that supi \u2016\u03bdi\u2016 < \u221e almost surely. Consider the ODE of \u03bd \u2208 R in (30), we define the set-valued derivative of L as follows:\nDtL(\u03b8, \u03bd, \u03bb) = { g(\u03bd)\u03a5\u03bd [ \u2212 g(\u03bd) ] | \u2200g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) } .\nOne may conclude that\nmax g(\u03bd)\nDtL(\u03b8, \u03bd, \u03bb) = max { g(\u03bd)\u03a5\u03bd [ \u2212 g(\u03bd) ] | g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) } .\nWe have the following cases: Case 1: When \u03bd \u2208 (\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ). For every g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb), there exists a sufficiently small \u03b70 > 0 such that \u03bd \u2212 \u03b70g(\u03bd) \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] and\n\u0393N ( \u03b8 \u2212 \u03b70g(\u03bd) ) \u2212 \u03b8 = \u2212\u03b70g(\u03bd).\nTherefore, the definition of \u03a5\u03b8[\u2212g(\u03bd)] implies\nmax g(\u03bd)\nDtL(\u03b8, \u03bd, \u03bb) = max { \u2212 g2(\u03bd) | g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) } \u2264 0. (38)\nThe maximum is attained because \u2202\u03bdL(\u03b8, \u03bd, \u03bb) is a convex compact set and g(\u03bd)\u03a5\u03bd [ \u2212 g(\u03bd) ]\nis a continuous function. At the same time, we have maxg(\u03bd) DtL(\u03b8, \u03bd, \u03bb) < 0 whenever 0 6\u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb). Case 2: When \u03bd \u2208 {\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 } and for any g(\u03bd) \u2208 \u2202L\u03bd(\u03b8, \u03bd, \u03bb) such that \u03bd \u2212 \u03b7g(\u03bd) \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ], for any \u03b7 \u2208 (0, \u03b70] and some \u03b70 > 0. The condition \u03bd \u2212 \u03b7g(\u03bd) \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] implies that\n\u03a5\u03bd [ \u2212 g(\u03bd) ] = \u2212g(\u03bd).\nThen we obtain\nmax g(\u03bd)\nDtL(\u03b8, \u03bd, \u03bb) = max { \u2212 g2(\u03bd) | g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) } \u2264 0. (39)\nFurthermore, we have maxg(\u03bd) DtL(\u03b8, \u03bd, \u03bb) < 0 whenever 0 6\u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb). Case 3: When \u03bd \u2208 {\u2212Cmax1\u2212\u03b3 , Cmax1\u2212\u03b3 } and there exists a non-empty set G(\u03bd) := {g(\u03bd) \u2208 \u2202L\u03bd(\u03b8, \u03bd, \u03bb) | \u03b8 \u2212 \u03b7g(\u03bd) 6\u2208 [\u2212Cmax1\u2212\u03b3 , Cmax1\u2212\u03b3 ], \u2203\u03b7 \u2208 (0, \u03b70], \u2200\u03b70 > 0}. First, consider any g(\u03bd) \u2208 G(\u03bd). For any \u03b7 > 0, define \u03bd\u03b7 := \u03bd \u2212 \u03b7g(\u03bd). The above condition implies that when 0 < \u03b7 \u2192 0, \u0393N [ \u03bd\u03b7 ]\nis the projection of \u03bd\u03b7 to the tangent space of [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ]. For any elements \u03bd\u0302 \u2208 [\u2212 Cmax 1\u2212\u03b3 , Cmax 1\u2212\u03b3 ], since the following set {\u03bd \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] : \u2016\u03bd \u2212 \u03bd\u03b7\u20162 \u2264 \u2016\u03bd\u0302 \u2212 \u03bd\u03b7\u20162} is compact, the projection of \u03bd\u03b7 on [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] exists. Furthermore, since f(\u03bd) := 1 2 (\u03bd \u2212 \u03bd\u03b7)2 is a strongly convex function and \u2207f(\u03bd) = \u03bd \u2212 \u03bd\u03b7, by first order optimality condition, one obtains\n\u2207f(\u03bd\u2217\u03b7)(\u03bd \u2212 \u03bd\u2217\u03b7) = (\u03bd\u2217\u03b7 \u2212 \u03bd\u03b7)(\u03bd \u2212 \u03bd\u2217\u03b7 ) \u2265 0, \u2200\u03bd \u2208 [ \u2212Cmax 1\u2212 \u03b3 , Cmax 1\u2212 \u03b3 ]\nwhere \u03bd\u2217\u03b7 is an unique projection of \u03bd\u03b7 (the projection is unique because f(\u03bd) is strongly convex and [\u2212Cmax1\u2212\u03b3 , Cmax1\u2212\u03b3 ] is a convex compact set). Since the projection (minimizer) is unique, the above equality holds if and only if \u03bd = \u03bd\u2217\u03b7 .\nTherefore, for any \u03bd \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ] and \u03b7 > 0,\ng(\u03bd)\u03a5\u03bd [ \u2212 g(\u03bd) ] = g(\u03bd) ( lim\n0<\u03b7\u21920 \u03bd\u2217\u03b7 \u2212 \u03bd \u03b7\n)\n= ( lim\n0<\u03b7\u21920 \u03bd \u2212 \u03bd\u03b7 \u03b7\n)( lim\n0<\u03b7\u21920 \u03bd\u2217\u03b7 \u2212 \u03bd \u03b7\n) = lim\n0<\u03b7\u21920 \u2212\u2016\u03bd\u2217\u03b7 \u2212 \u03bd\u20162 \u03b72 + lim 0<\u03b7\u21920 ( \u03bd\u2217\u03b7 \u2212 \u03bd\u03b7 )(\u03bd\u2217\u03b7 \u2212 \u03bd \u03b72 ) \u2264 0.\nSecond, for any g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb)\u2229G(\u03bd)c , one obtains \u03bd\u2212\u03b7g(\u03bd) \u2208 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ], for any \u03b7 \u2208 (0, \u03b70] and some \u03b70 > 0. In this case, the arguments follow from case 2 and the following expression holds, \u03a5\u03bd [ \u2212 g(\u03bd) ] = \u2212g(\u03bd).\nCombining these arguments, one concludes that\nmax g(\u03bd) DtL(\u03b8, \u03bd, \u03bb)\n\u2264max { max { g(\u03bd) \u03a5\u03bd [ \u2212 g(\u03bd) ] | g(\u03bd) \u2208 G(\u03bd) } ,max { \u2212 g2(\u03bd) | g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) \u2229 G(\u03bd)c }} \u2264 0.\n(40)\nThis quantity is non-zero whenever 0 6\u2208 {g(\u03bd) \u03a5\u03bd [ \u2212 g(\u03bd) ] | \u2200g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb)} (this is because, for any g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb)\u2229G(\u03bd)c , one obtains g(\u03bd) \u03a5\u03bd [ \u2212 g(\u03bd) ] = \u2212g(\u03bd)2). Thus, by similar arguments one may conclude that maxg(\u03bd) DtL(\u03b8, \u03bd, \u03bb) \u2264 0 and\nit is non-zero if \u03a5\u03bd [ \u2212 g(\u03bd) ] 6= 0 for every g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb). Therefore, by Lasalle\u2019s invariance principle for differential inclusion (see Theorem 2.11 [30]), the above arguments imply that with any initial condition \u03b8(0), the state trajectory \u03bd(t) of (31) converges to a stable stationary point \u03bd\u2217 in the positive invariant set Nc. Since maxg(\u03bd) DtL(\u03b8, \u03bd, \u03bb) \u2264 0, \u03bd\u0307 is a descent direction of L(\u03b8, \u03bd, \u03bb) for fixed \u03b8 and \u03bb, i.e., L(\u03b8, \u03bd\u2217, \u03bb) \u2264 L(\u03b8, \u03bd(t), \u03bb) \u2264 L(\u03b8, \u03bd(0), \u03bb) for any t \u2265 0.\nStep 2 (Convergence of \u03b8\u2212update) Since \u03b8 converges in a faster time scale than \u03bb and \u03bd converges faster than \u03b8, one can assume \u03bb as a fixed quantity and \u03bd as a converged quantity \u03bd\u2217(\u03b8) in the \u03b8-update. The \u03b8-update can be rewritten as a stochastic approximation, i.e.,\n\u03b8i+1 = \u0393\u0398 ( \u03b8i + \u03b62(i) ( \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8i,\u03bd=\u03bd\u2217(\u03b8i) + \u03b4\u03b8i+1 )) , (41)\nwhere\n\u03b4\u03b8i+1 =\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8i,\u03bd=\u03bd\u2217(\u03b8i)\u2212 1\nN\nN\u2211\nj=1\n\u2207\u03b8 logP\u03b8(\u03bej,i) |\u03b8=\u03b8i D(\u03bej,i)\n\u2212 \u03bb (1\u2212 \u03b1)N\nN\u2211\nj=1\n\u2207\u03b8 logP\u03b8(\u03bej,i)|\u03b8=\u03b8i ( D(\u03bej,i)\u2212 \u03bd\u2217(\u03b8i) ) 1 { D(\u03bej,i) \u2265 \u03bd\u2217(\u03b8i) } .\n(42)\nFirst, one can show that \u03b4\u03b8i+1 is square integrable, i.e., E[\u2016\u03b4\u03b8i+1\u20162 | F\u03b8,i] \u2264 Ki(1 + \u2016\u03b8i\u20162) for some Ki > 0, where F\u03b8,i = \u03c3 ( \u03b8m, \u03b4\u03b8m, m \u2264 i ) is the filtration of \u03b8i generated by different independent trajectories. To see this, notice that\n\u2016\u03b4\u03b8i+1\u20162 \u22642 ( \u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8i,\u03bd=\u03bd\u2217(\u03b8i) )2 + 2\nN2 ( Cmax 1\u2212 \u03b3 + 2\u03bbCmax (1\u2212 \u03b1)(1\u2212 \u03b3)\n)2 ( N\u2211\nj=1\n\u2207\u03b8 log P\u03b8(\u03bej,i) |\u03b8=\u03b8i\n)2\n\u22642K1,i(1 + \u2016\u03b8i\u20162) + 2 N\nN2 ( Cmax 1\u2212 \u03b3 + 2\u03bbmaxCmax (1\u2212 \u03b1)(1\u2212 \u03b3)\n)2 ( N\u2211\nj=1\n\u2016\u2207\u03b8 log P\u03b8(\u03bej,i) |\u03b8=\u03b8i\u20162 )\n\u22642K1,i(1 + \u2016\u03b8i\u20162) + 2N\nN2 ( Cmax 1\u2212 \u03b3 + 2\u03bbmaxCmax (1\u2212 \u03b1)(1\u2212 \u03b3)\n)2 ( N\u2211\nj=1\nK2(\u03bej,i)(1 + \u2016\u03b8i\u20162) )\n\u22642 ( K1,i+ 2N\u22121\nN ( Cmax 1\u2212 \u03b3 + 2\u03bbmaxCmax (1\u2212 \u03b1)(1\u2212 \u03b3) )2 max 1\u2264j\u2264N K2(\u03bej,i) ) (1+\u2016\u03b8i\u20162)\nThe Lipschitz upper bounds are due to results in Remark 3. Since K2(\u03bej,i) < \u221e w.p. 1, there exists K2,i < \u221e such that max1\u2264j\u2264N K2(\u03bej,i) \u2264 K2,i. Furthermore, T < \u221e\nw.p. 1 implies E[T 2 | F\u03b8,i] < \u221e. By combining these results, one concludes that E[\u2016\u03b4\u03b8i+1\u20162 | F\u03b8,i] \u2264 Ki(1+\u2016\u03b8i\u20162) where\nKi = 2 ( K1,i+\n2N\u22121K2,i N ( Cmax 1\u2212 \u03b3 + 2\u03bbmaxCmax (1 \u2212 \u03b1)(1 \u2212 \u03b3) )2) < \u221e.\nSecond, since the history trajectories are generated based on the sampling probability mass function P\u03b8i(\u03be), expression (26) implies that E [\u03b4\u03b8i+1 | F\u03b8,i] = 0. Therefore, the \u03b8-update is a stochastic approximation of the ODE (31) with a Martingale difference error term. In addition, from the convergence analysis of \u03bd\u2212update, \u03bd\u2217(\u03b8) is an asymptotically stable equilibrium point of {\u03bdi}. From (27), \u2202\u03bdL(\u03b8, \u03bd, \u03bb) is a Lipschitz set-valued mapping in \u03b8 (since P\u03b8(\u03be) is Lipschitz in \u03b8), it can be easily seen that \u03bd\u2217(\u03b8) is a Lipschitz continuous mapping of \u03b8.\nNow consider the continuous time system \u03b8 \u2208 \u0398 in (31). We may write\ndL(\u03b8, \u03bd, \u03bb)\ndt\n\u2223\u2223\u2223\u2223 \u03bd=\u03bd\u2217(\u03b8) = ( \u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) )\u22a4 \u03a5\u03b8 [ \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) ] . (43)\nWe have the following cases: Case 1: When \u03b8 \u2208 \u0398\u25e6. Since \u0398\u25e6 is the interior of the set \u0398 and \u0398 is a convex compact set, there exists a sufficiently small \u03b70 > 0 such that \u03b8 \u2212 \u03b70\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) \u2208 \u0398 and\n\u0393\u0398 ( \u03b8 \u2212 \u03b70\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) ) \u2212 \u03b8 = \u2212\u03b70\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8).\nTherefore, the definition of \u03a5\u03b8 [ \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) ] implies\ndL(\u03b8, \u03bd, \u03bb)\ndt\n\u2223\u2223\u2223\u2223 \u03bd=\u03bd\u2217(\u03b8) = \u2212 \u2225\u2225\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) \u2225\u22252 \u2264 0. (44)\nAt the same time, we have dL(\u03b8, \u03bd, \u03bb)/dt|\u03bd=\u03bd\u2217(\u03b8) < 0whenever \u2016\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8)\u2016 6= 0. Case 2: When \u03b8 \u2208 \u2202\u0398 and \u03b8 \u2212 \u03b7\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) \u2208 \u0398 for any \u03b7 \u2208 (0, \u03b70] and some \u03b70 > 0. The condition \u03b8 \u2212 \u03b7\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) \u2208 \u0398 implies that\n\u03a5\u03b8 [ \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) ] = \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8).\nThen we obtain\ndL(\u03b8, \u03bd, \u03bb)\ndt\n\u2223\u2223\u2223\u2223 \u03bd=\u03bd\u2217(\u03b8) = \u2212 \u2225\u2225\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) \u2225\u22252 \u2264 0. (45)\nFurthermore, dL(\u03b8, \u03bd, \u03bb)/dt|\u03bd=\u03bd\u2217(\u03b8) < 0 when \u2016\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8)\u2016 6= 0. Case 3: When \u03b8 \u2208 \u2202\u0398 and \u03b8 \u2212 \u03b7\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) 6\u2208 \u0398 for some \u03b7 \u2208 (0, \u03b70] and any \u03b70 > 0. For any \u03b7 > 0, define \u03b8\u03b7 := \u03b8 \u2212 \u03b7\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8). The above condition implies\nthat when 0 < \u03b7 \u2192 0, \u0393\u0398 [ \u03b8\u03b7 ] is the projection of \u03b8\u03b7 to the tangent space of \u0398. For any elements \u03b8\u0302 \u2208 \u0398, since the following set {\u03b8 \u2208 \u0398 : \u2016\u03b8 \u2212 \u03b8\u03b7\u20162 \u2264 \u2016\u03b8\u0302 \u2212 \u03b8\u03b7\u20162} is compact, the projection of \u03b8\u03b7 on \u0398 exists. Furthermore, since f(\u03b8) := 12\u2016\u03b8\u2212 \u03b8\u03b7\u201622 is a strongly convex function and \u2207f(\u03b8) = \u03b8\u2212 \u03b8\u03b7, by first order optimality condition, one obtains \u2207f(\u03b8\u2217\u03b7)\u22a4(\u03b8 \u2212 \u03b8\u2217\u03b7) = (\u03b8\u2217\u03b7 \u2212 \u03b8\u03b7)\u22a4(\u03b8 \u2212 \u03b8\u2217\u03b7) \u2265 0, \u2200\u03b8 \u2208 \u0398 where \u03b8\u2217\u03b7 is an unique projection of \u03b8\u03b7 (the projection is unique because f(\u03b8) is strongly convex and \u0398 is a convex compact set). Since the projection (minimizer) is unique, the above equality holds if and only if \u03b8 = \u03b8\u2217\u03b7 .\nTherefore, for any \u03b8 \u2208 \u0398 and \u03b7 > 0,\n( \u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) )\u22a4 \u03a5\u03b8 [ \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) ] = ( \u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8)\n)\u22a4 (\nlim 0<\u03b7\u21920 \u03b8\u2217\u03b7 \u2212 \u03b8 \u03b7\n)\n= ( lim\n0<\u03b7\u21920 \u03b8 \u2212 \u03b8\u03b7 \u03b7\n)\u22a4 ( lim\n0<\u03b7\u21920 \u03b8\u2217\u03b7 \u2212 \u03b8 \u03b7\n) = lim\n0<\u03b7\u21920 \u2212\u2016\u03b8\u2217\u03b7 \u2212 \u03b8\u20162 \u03b72 + lim 0<\u03b7\u21920 ( \u03b8\u2217\u03b7 \u2212 \u03b8\u03b7 )\u22a4 ( \u03b8\u2217\u03b7 \u2212 \u03b8 \u03b72 ) \u2264 0.\nFrom these arguments, one concludes that dL(\u03b8, \u03bd, \u03bb)/dt|\u03bd=\u03bd\u2217(\u03b8) \u2264 0 and this quantity is non-zero whenever \u2225\u2225\u03a5\u03b8 [ \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8)\n]\u2225\u2225 6= 0. Therefore, by Lasalle\u2019s invariance principle [19], the above arguments imply that\nwith any initial condition \u03b8(0), the state trajectory \u03b8(t) of (31) converges to a stable stationary point \u03b8\u2217 in the positive invariant set \u0398c andL(\u03b8\u2217, \u03bd\u2217(\u03b8\u2217), \u03bb) \u2264 L(\u03b8(t), \u03bd\u2217(\u03b8(t)), \u03bb) \u2264 L(\u03b8(0), \u03bd\u2217(\u03b8(0)), \u03bb) for any t \u2265 0.\nBased on the above properties and noting that 1) from Proposition 5, \u2207\u03b8L(\u03b8, \u03bd, \u03bb) is a Lipschitz function in \u03b8, 2) the step-size rule follows from Section A.1, 3) expression (47) implies that \u03b4\u03b8i+1 is a square integrable Martingale difference, and 4) \u03b8i \u2208 \u0398, \u2200i implies that supi \u2016\u03b8i\u2016 < \u221e almost surely, one can invoke Theorem 2 in Chapter 6 of [14] (multi-time scale stochastic approximation theory) to show that the sequence {\u03b8i}, \u03b8i \u2208 \u0398 converges almost surely to a fixed point \u03b8\u2217 \u2208 \u0398 of ODE (31), where \u03b8\u2217 \u2208 \u0398c := {\u03b8 \u2208 \u0398 : \u03a5\u03b8[\u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8)] = 0}. Also, it can be easily seen that \u0398c is a closed subset of the compact set \u0398, which is a compact set as well.\nStep 3 (Local Minimum) Now, we want to show that {\u03b8i, \u03bdi} converges to a local minimum ofL(\u03b8, \u03bd, \u03bb) for fixed \u03bb. Recall {\u03b8i, \u03bdi} converges to (\u03b8\u2217, \u03bd\u2217) := (\u03b8\u2217, \u03bd\u2217(\u03b8\u2217)). From previous arguments on (\u03bd, \u03b8) convergence analysis imply that with any initial condition (\u03b8(0), \u03bd(0)), the state trajectories \u03b8(t) and \u03bd(t)) of (30) and (31) converge to the set of stationary points (\u03b8\u2217, \u03bd\u2217) in the positive invariant set \u0398c\u00d7Nc andL(\u03b8\u2217, \u03bd\u2217, \u03bb) \u2264 L(\u03b8(t), \u03bd\u2217(\u03b8(t)), \u03bb) \u2264 L(\u03b8(0), \u03bd\u2217(\u03b8(0)), \u03bb) \u2264 L(\u03b8(0), \u03bd(t), \u03bb) \u2264 L(\u03b8(0), \u03bd(0), \u03bb) for any t \u2265 0.\nBy contradiction, suppose (\u03b8\u2217, \u03bd\u2217) is not a local minimum. Then there exists (\u03b8\u0304, \u03bd\u0304) \u2208 \u0398\u00d7[\u2212Cmax1\u2212\u03b3 , Cmax1\u2212\u03b3 ]\u2229B(\u03b8\u2217,\u03bd\u2217)(r) such that L(\u03b8\u0304, \u03bd\u0304, \u03bb) = min(\u03b8,\u03bd)\u2208\u0398\u00d7[\u2212Cmax1\u2212\u03b3 ,Cmax1\u2212\u03b3 ]\u2229B(\u03b8\u2217,\u03bd\u2217)(r) L(\u03b8, \u03bd, \u03bb). The minimum is attained by Weierstrass extreme value theorem. By putting \u03b8(0) = \u03b8\u0304, the above arguments imply that\nL(\u03b8\u0304, \u03bd\u0304, \u03bb) = min (\u03b8,\u03bd)\u2208\u0398\u00d7[\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ]\u2229B(\u03b8\u2217,\u03bd\u2217)(r)\nL(\u03b8, \u03bd, \u03bb) < L(\u03b8\u2217, \u03bd\u2217, \u03bb) \u2264 L(\u03b8\u0304, \u03bd\u0304, \u03bb)\nwhich is clearly a contradiction. Therefore, the stationary point (\u03b8\u2217, \u03bd\u2217) is a local minimum of L(\u03b8, \u03bd, \u03bb) as well.\nStep 4 (Convergence of \u03bb\u2212update) Since \u03bb-update converges in the slowest time scale, it can be rewritten using the converged \u03b8\u2217(\u03bb) = \u03b8\u2217(\u03bd\u2217(\u03bb), \u03bb) and \u03bd\u2217(\u03bb), i.e.,\n\u03bbi+1 = \u0393\u039b ( \u03bbi + \u03b61(i) ( \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bbi),\u03bd=\u03bd\u2217(\u03bbi),\u03bb=\u03bbi + \u03b4\u03bbi+1 )) (46)\nwhere \u03b4\u03bbi+1 = \u2212\u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bbi + ( \u03bd\u2217(\u03bbi) + 1 1\u2212 \u03b1 1 N N\u2211\nj=1\n( D(\u03bej,i)\u2212 \u03bd\u2217(\u03bbi) )+ \u2212 \u03b2 ) .\n(47)\nFrom (28), it is obvious that \u2207\u03bbL(\u03b8, \u03bd, \u03bb) is a constant function of \u03bb. Similar to \u03b8\u2212update, one can easily show that \u03b4\u03bbi+1 is square integrable, i.e.,\nE[\u2016\u03b4\u03bbi+1\u20162 | F\u03bb,i] \u2264 2 ( \u03b2 +\n3Cmax (1\u2212 \u03b3)(1\u2212 \u03b1)\n)2 ,\nwhere F\u03bb,i = \u03c3 ( \u03bbm, \u03b4\u03bbm, m \u2264 i ) is the filtration of \u03bb generated by different independent trajectories. Furthermore, expression (28) implies that E [\u03b4\u03bbi+1 | F\u03bb,i] = 0. Therefore, the \u03bb-update is a stochastic approximation of the ODE (33) with a Martingale difference error term. In addition, from the convergence analysis of (\u03b8, \u03bd)\u2212update, (\u03b8\u2217(\u03bb), \u03bd\u2217(\u03bb)) is an asymptotically stable equilibrium point of {\u03b8i, \u03bdi}. From (26), \u2207\u03b8L(\u03b8, \u03bd, \u03bb) is a linear mapping in \u03bb, it can be easily seen that (\u03b8\u2217(\u03bb), \u03bd\u2217(\u03bb)) is a Lipschitz continuous mapping of \u03bb.\nConsider the ODE of \u03bb \u2208 [0, \u03bbmax] in (33). Analogous to the arguments in the \u03b8\u2212update, we may write\ndL(\u03b8, \u03bd, \u03bb)\ndt\n\u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb) = \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb) \u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb) ] .\nand show that dL(\u03b8, \u03bd, \u03bb)/dt|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb) \u2264 0, this quantity is non-zero whenever\u2225\u2225\u03a5\u03bb [ dL(\u03b8, \u03bd, \u03bb)/d\u03bb|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb)\n]\u2225\u2225 6= 0. Lasalle\u2019s invariance principle implies that \u03bb\u2217 \u2208 \u039bc := {\u03bb \u2208 [0, \u03bbmax] : \u03a5\u03bb[\u2207\u03bbL(\u03b8, \u03bd, \u03bb) |\u03bd=\u03bd\u2217(\u03bb),\u03b8=\u03b8\u2217(\u03bb)] = 0} is a stable equilibrium point.\nBased on the above properties and noting that the step size rule follows from Section A.1, one can apply the multi-time scale stochastic approximation theory (Theorem 2 in Chapter 6 of [14]) to show that the sequence {\u03bbi} converges almost surely to a fixed point \u03bb\u2217 \u2208 [0, \u03bbmax] of ODE (33), where \u03bb\u2217 \u2208 \u039bc := {\u03bb \u2208 [0, \u03bbmax] : \u03a5\u03bb[\u2207\u03bbL(\u03b8, \u03bd, \u03bb) |\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb)] = 0}. Since \u039bc is a closed set of [0, \u03bbmax], it is a compact set as well. Following the same lines of arguments and recalling the envelope theorem (Theorem 3) for local optimum, one further concludes that \u03bb\u2217 is a local maximum of L(\u03b8\u2217(\u03bb), \u03bd\u2217(\u03bb), \u03bb) = L\u2217(\u03bb).\nStep 5 (Saddle Point) By letting \u03b8\u2217 = \u03b8\u2217 ( \u03bd\u2217(\u03bb\u2217), \u03bb\u2217 ) and \u03bd\u2217 = \u03bd\u2217(\u03bb\u2217), we will show that (\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) is a (local) saddle point of the objective function L(\u03b8, \u03bd, \u03bb) if \u03bb\u2217 \u2208 [0, \u03bbmax).\nNow suppose the sequence {\u03bbi} generated from (46) converges to a stationary point \u03bb\u2217 \u2208 [0, \u03bbmax). Since step 3 implies that (\u03b8\u2217, \u03bd\u2217) is a local minimum of L(\u03b8, \u03bd, \u03bb\u2217) over feasible set (\u03b8, \u03bd) \u2208 \u0398\u00d7 [\u2212Cmax1\u2212\u03b3 , Cmax 1\u2212\u03b3 ], there exists a r > 0 such that\nL(\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) \u2264 L(\u03b8, \u03bd, \u03bb\u2217), \u2200(\u03b8, \u03bd) \u2208 \u0398\u00d7 [ \u2212Cmax 1\u2212 \u03b3 , Cmax 1\u2212 \u03b3 ] \u2229B(\u03b8\u2217,\u03bd\u2217)(r).\nIn order to complete the proof, we must show\n\u03bd\u2217 + 1 1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+] \u2264 \u03b2, (48)\nand\n\u03bb\u2217 ( \u03bd\u2217 + 1 1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+]\u2212 \u03b2 ) = 0. (49)\nThese two equations imply\nL(\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) =V \u03b8 \u2217 (x0)+\u03bb\u2217 ( \u03bd\u2217 + 1 1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+]\u2212 \u03b2 )\n=V \u03b8 \u2217 (x0) \u2265V \u03b8\u2217(x0)+\u03bb ( \u03bd\u2217 + 1\n1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+]\u2212 \u03b2 ) = L(\u03b8\u2217, \u03bd\u2217, \u03bb),\nwhich further implies that (\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) is a saddle point of L(\u03b8, \u03bd, \u03bb). We now show that (48) and (49) hold.\nRecall that \u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bb\u2217 ] |\u03bb=\u03bb\u2217 = 0. We show (48) by\ncontradiction. Suppose \u03bd\u2217 + 11\u2212\u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+] > \u03b2. This then implies that for \u03bb\u2217 \u2208 [0, \u03bbmax), we have\n\u0393\u039b ( \u03bb\u2217 \u2212 \u03b7 ( \u03b2 \u2212 ( \u03bd\u2217 + 1\n1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+]) )) = \u03bb\u2217\u2212\u03b7 ( \u03b2\u2212 ( \u03bd\u2217+ 1 1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212\u03bd\u2217 )+]) )\nfor any \u03b7 \u2208 (0, \u03b7max] for some sufficiently small \u03b7max > 0. Therefore,\n\u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bb\u2217 ] \u2223\u2223\u2223\u2223\u2223 \u03bb=\u03bb\u2217 = \u03bd\u2217+ 1 1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+]\u2212\u03b2 > 0. This contradicts with \u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bb\u2217 ] |\u03bb=\u03bb\u2217 = 0. Therefore, (48) holds. To show that (49) holds, we only need to show that \u03bb\u2217 = 0 if \u03bd\u2217+ 11\u2212\u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+] < \u03b2. Suppose \u03bb\u2217 \u2208 (0, \u03bbmax), then there exists a sufficiently small \u03b70 > 0 such that 1\n\u03b70\n( \u0393\u039b ( \u03bb\u2217 \u2212 \u03b70 ( \u03b2 \u2212 ( \u03bd\u2217 + 1\n1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+])) ) \u2212 \u0393\u039b(\u03bb\u2217) )\n=\u03bd\u2217 + 1 1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+]\u2212 \u03b2 < 0.\nThis again contradicts with the assumption\u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bb\u2217 ] |\u03bb=\u03bb\u2217 = 0 from (70). Therefore (49) holds. Combining the above arguments, we finally conclude that (\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) is a (local) saddle point of L(\u03b8, \u03bd, \u03bb) if \u03bb\u2217 \u2208 [0, \u03bbmax). Remark 4 When \u03bb\u2217 = \u03bbmax and \u03bd\u2217 + 11\u2212\u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+] > \u03b2,\n\u0393\u039b\n( \u03bb\u2217 \u2212 \u03b7 ( \u03b2 \u2212 ( \u03bd\u2217 + 1\n1\u2212 \u03b1E [( D\u03b8 \u2217 (x0)\u2212 \u03bd\u2217 )+]) )) = \u03bbmax\nfor any \u03b7 > 0 and\n\u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb)|\u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bb\u2217 ] |\u03bb=\u03bb\u2217= 0.\nIn this case one cannot guarantee feasibility using the above analysis, and (\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) is not a local saddle point. Such \u03bb\u2217 is referred as a spurious fixed point [20]. Practically, by incrementally increasing \u03bbmax (see Algorithm 1 for more details), when \u03bbmax becomes sufficiently large, one can ensure that the policy gradient algorithm will not get stuck at the spurious fixed point."}, {"heading": "B Technical Details of the Actor-Critic Algorithms", "text": ""}, {"heading": "B.1 Assumptions", "text": "We make the following assumptions for the proof of our actor-critic algorithms: (B1) For any state-action pair (x, s, a) in the augmented MDP M\u0304, \u00b5(a|x, s; \u03b8) is continuously differentiable in \u03b8 and \u2207\u03b8\u00b5(a|x, s; \u03b8) is a Lipschitz function in \u03b8 for every a \u2208 A, x \u2208 X and s \u2208 R.\n(B2) The augmented Markov chain induced by any policy \u03b8, M\u0304\u03b8, is irreducible and aperiodic.\n(B3) The basis functions { \u03c6(i) }\u03ba2 i=1\nare linearly independent. In particular, \u03ba2 \u226a n and\u03a6 is full rank.4 Moreover, for every v \u2208 R\u03ba2 , \u03a6v 6= e, where e is the n-dimensional vector with all entries equal to one.\n(B4) For each (x\u2032, s\u2032, a\u2032) \u2208 X\u0304 \u00d7 A\u0304, there is a positive probability of being visited, i.e., \u03c0\u03b8\u03b3(x\n\u2032, s\u2032, a\u2032|x, s) > 0. Note that from the definition of the augmented MDP M\u0304, X\u0304 = X \u00d7 R and A\u0304 = A. (B5) The step size schedules {\u03b64(k)}, {\u03b63(k)}, {\u03b62(k)}, and {\u03b61(k)} satisfy\n\u2211\nk\n\u03b61(k) = \u2211\nk\n\u03b62(k) = \u2211\nk\n\u03b63(k) = \u2211\nk \u03b64(k) = \u221e, (50) \u2211\nk\n\u03b61(k) 2,\n\u2211\nk\n\u03b62(k) 2,\n\u2211\nk\n\u03b63(k) 2,\n\u2211\nk\n\u03b64(k) 2 < \u221e, (51)\n\u03b61(k) = o ( \u03b62(k) ) , \u03b62(k) = o ( \u03b63(k) ) , \u03b63(k) = o ( \u03b64(k) ) . (52)\nThis indicates that the updates correspond to {\u03b64(k)} is on the fastest time-scale, the update corresponds to {\u03b63(k)}, {\u03b62(k)} are on the intermediate time-scale, where \u03b63(k) converges faster than \u03b62(k), and the update corresponds to {\u03b61(k)} is on the slowest time-scale. (B6) The SPSA step size {\u2206k} satisfies \u2206k \u2192 \u221e as k \u2192 \u221e and \u2211 k(\u03b62(k)/\u2206k)\n2 < \u221e.\nTechnical assumptions for the convergence of the actor-critic algorithm will be given in the section for the proof of convergence."}, {"heading": "B.2 Gradient with Respect to \u03bb (Proof of Lemma 1)", "text": "Proof. By taking the gradient of V \u03b8(x0, \u03bd) w.r.t. \u03bb (just a reminder that both V and Q are related to \u03bb through the dependence of the cost function C\u0304 of the augmented MDP M\u0304 on \u03bb), we obtain\n4We may write this as: In particular, the (row) infinite dimensional matrix \u03a6 has column rank \u03ba2.\n\u2207\u03bbV \u03b8(x0, \u03bd) = \u2211\na\u2208A\u0304\n\u00b5(a|x0, \u03bd; \u03b8)\u2207\u03bbQ\u03b8(x0, \u03bd, a)\n= \u2211\na\u2208A\u0304\n\u00b5(a|x0, \u03bd; \u03b8)\u2207\u03bb [ C\u0304(x0, \u03bd, a) + \u2211\n(x\u2032,s\u2032)\u2208X\u0304\n\u03b3P\u0304 (x\u2032, s\u2032|x0, \u03bd, a)V \u03b8(x\u2032, s\u2032) ]\n= \u2211\na \u00b5(a|x0, \u03bd; \u03b8)\u2207\u03bbC\u0304(x0, \u03bd, a) \ufe38 \ufe37\ufe37 \ufe38\nh(x0,\u03bd)\n+\u03b3 \u2211\na,x\u2032,s\u2032\n\u00b5(a|x0, \u03bd; \u03b8)P\u0304 (x\u2032, s\u2032|x0, \u03bd, a)\u2207\u03bbV \u03b8(x\u2032, s\u2032)\n= h(x0, \u03bd) + \u03b3 \u2211\na,x\u2032,s\u2032\n\u00b5(a|x0, \u03bd; \u03b8)P\u0304 (x\u2032, s\u2032|x0, \u03bd, a)\u2207\u03bbV \u03b8(x\u2032, s\u2032) (53)\n= h(x0, \u03bd) + \u03b3 \u2211\na,x\u2032,s\u2032\n\u00b5(a|x0, \u03bd; \u03b8)P\u0304 (x\u2032, s\u2032|x0, \u03bd, a) [ h(x\u2032, s\u2032)\n+ \u03b3 \u2211\na\u2032,x\u2032\u2032,s\u2032\u2032\n\u00b5(a\u2032|x\u2032, s\u2032; \u03b8)P\u0304 (x\u2032\u2032, s\u2032\u2032|x\u2032, s\u2032, a\u2032)\u2207\u03bbV \u03b8(x\u2032\u2032, s\u2032\u2032) ]\nBy unrolling the last equation using the definition of \u2207\u03bbV \u03b8(x, s) from (53), we obtain\n\u2207\u03bbV \u03b8(x0, \u03bd) = \u221e\u2211\nk=0\n\u03b3k \u2211\nx,s\nP(xk = x, sk = s | x0 = x0, s0 = \u03bd; \u03b8)h(x, s)\n= 1 1\u2212 \u03b3 \u2211\nx,s\nd\u03b8\u03b3(x, s|x0, \u03bd)h(x, s) = 1 1\u2212 \u03b3 \u2211\nx,s,a\nd\u03b8\u03b3(x, s|x0, \u03bd)\u00b5(a|x, s)\u2207\u03bbC\u0304(x, s, a)\n= 1 1\u2212 \u03b3 \u2211\nx,s,a\n\u03c0\u03b8\u03b3(x, s, a|x0, \u03bd)\u2207\u03bbC\u0304(x, s, a)\n= 1 1\u2212 \u03b3 \u2211\nx,s,a\n\u03c0\u03b8\u03b3(x, s, a|x0, \u03bd) 1\n1\u2212 \u03b11{x = xT }(\u2212s) +.\nB.3 Actor-Critic Algorithm with the Alternative Approach to Compute the Gradients"}, {"heading": "B.4 Convergence of the Actor Critic Algorithms", "text": "In this section we want to derive the following convergence results.\nTheorem 6 Suppose v\u2217 \u2208 argminv \u2016T\u03b8[\u03a6v]\u2212 \u03a6v\u20162d\u03b8\u03b3 , where\nT\u03b8[V ](x, s) = \u2211\na\n\u00b5(a|x, s; \u03b8)   C\u0304(x, s, a) + \u2211\nx\u2032,s\u2032\nP\u0304 (x\u2032, s\u2032|x, s, a)V (x\u2032, s\u2032)   \nand V\u0303 \u2217(x, s) = \u03c6\u22a4(x, s)v\u2217 is the projected Bellman fixed point of V \u03b8(x, s), i.e., V\u0303 \u2217(x, s) = \u03a0T\u03b8[V\u0303 \u2217](x, s). Also suppose the \u03b3\u2212stationary distribution \u03c0\u03b8\u03b3 is used\nAlgorithm 3 Actor-Critic Algorithm for CVaR Optimization (Alternative Gradient Computation)\nwhile 1 do Input: Parameterized policy \u00b5(\u00b7|\u00b7; \u03b8), value function feature vectors f(\u00b7) and \u03c6(\u00b7), confidence level \u03b1, and loss tolerance \u03b2 Initialization: policy parameters \u03b8 = \u03b80; VaR parameter \u03bd = \u03bd0; Lagrangian parameter \u03bb = \u03bb0; value function weight vectors u = u0 and v = v0 for k = 0, 1, 2, . . . do\nDraw action ak \u223c \u00b5(\u00b7|xk, sk; \u03b8k) Observe next state (xk+1, sk+1) \u223c P\u0304 (\u00b7|xk, sk, ak); // note that sk+1 = (sk \u2212 C ( xk, ak) )\n/\u03b3 (see Sec. 5.1) Observe costs C(xk, ak) and C\u0304(xk, sk, ak) // C\u0304 and P\u0304 are the cost and transition functions of the\n// augmented MDP M\u0304 defined in Sec. 5.4, while C is the cost function of the original MDP M\nTD Errors: \u01ebk(uk) = C(xk, ak) + \u03b3u \u22a4 k f(xk+1)\u2212 u\u22a4k f(xk) (54)\n\u03b4k(vk) = C\u0304(xk, sk, ak) + \u03b3v \u22a4 k \u03c6(xk+1, sk+1)\u2212 v\u22a4k \u03c6(xk, sk)\n(55)\nCritic Updates: uk+1 = uk + \u03b64(k)\u01ebk(uk)f(xk) (56)\nvk+1 = vk + \u03b64(k)\u03b4k(vk)\u03c6(xk, sk) (57)\nActor Updates: \u03bdk+1 = \u0393N ( \u03bdk \u2212 \u03b63(k)\u03bbk ( 1 + v\u22a4k [ \u03c6 ( x0, \u03bdk +\u2206k ) \u2212 \u03c6(x0, \u03bdk \u2212\u2206k) ]\n2(1\u2212 \u03b1)\u2206k\n))\n(58)\n\u03b8k+1 = \u0393\u0398 ( \u03b8k \u2212 \u03b62(k) 1\u2212 \u03b3\u2207\u03b8 log \u00b5(ak|xk, sk; \u03b8)|\u03b8=\u03b8k \u00b7 ( \u01ebk(uk) + \u03bbk 1\u2212 \u03b1\u03b4k(vk) ))\n(59)\n\u03bbk+1 = \u0393\u039b ( \u03bbk + \u03b61(k) ( \u03bdk \u2212 \u03b2 + v\u22a4\u03c6(xk, sk)\n1\u2212 \u03b1\n)) (60)\nend for if {\u03bbi} converges to \u03bbmax then\nSet \u03bbmax \u2190 2\u03bbmax. else\nreturn policy and value function parameters v, u, \u03bd, \u03b8, \u03bb and break end if\nend while\nto generate samples of (xk, sk, ak) for any k \u2208 {0, 1, . . . , }. Then the v\u2212updates in the actor critic algorithms converge to v\u2217 almost surely.\nNext define \u01eb\u03b8(vk) = \u2016T\u03b8[\u03a6vk]\u2212 \u03a6vk\u20162d\u03b8\u03b3\nas the residue of the value function approximation at step k induced by policy\u00b5(\u00b7|\u00b7, \u00b7; \u03b8). By triangular inequality and fixed point theorem T\u03b8[V \u2217] = V \u2217, it can be easily seen that \u2016V \u2217\u2212\u03a6vk\u20162d\u03b8\u03b3 \u2264 \u01eb\u03b8(vk)+\u2016T\u03b8[\u03a6vk]\u2212T\u03b8[V \u2217]\u20162 d\u03b8\u03b3\n\u2264 \u01eb\u03b8(vk)+\u03b3\u2016\u03a6vk\u2212V \u2217\u20162d\u03b8\u03b3 . The last inequality follows from the contraction mapping argument. Thus, one concludes that \u2016V \u2217 \u2212 \u03a6vk\u20162d\u03b8\u03b3 \u2264 \u01eb\u03b8(vk)/(1\u2212 \u03b3).\nTheorem 7 Suppose \u03bb\u2217 \u2208 [0, \u03bbmax), \u01eb\u03b8k(vk) \u2192 0 as t goes to infinity and the \u03b3\u2212stationary distribution \u03c0\u03b8\u03b3 is used to generate samples of (xk, sk, ak) for any k \u2208 {0, 1, . . . , }. For SPSA based algorithm, also suppose the perturbation sequence {\u2206k} satisfies \u01eb\u03b8k(vk)E[1/\u2206k] \u2192 0. Then the sequence of (\u03b8, \u03bd, \u03bb)-updates in Algorithm 2 converges to a (local) saddle point (\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) of our objective function L(\u03b8, \u03bd, \u03bb) almost surely, it satisfies L(\u03b8, \u03bd, \u03bb\u2217) \u2265 L(\u03b8\u2217, \u03bd\u2217, \u03bb\u2217) \u2265 L(\u03b8\u2217, \u03bd\u2217, \u03bb), \u2200(\u03b8, \u03bd) \u2208 \u0398 \u00d7 [\u2212Cmax/(1 \u2212 \u03b3), Cmax/(1 \u2212 \u03b3)] \u2229 B(\u03b8\u2217,\u03bd\u2217)(r) for some r > 0 and \u2200\u03bb \u2208 [0, \u03bbmax]. Note that B(\u03b8\u2217,\u03bd\u2217)(r) represents a hyper-dimensional ball centered at (\u03b8\n\u2217, \u03bd\u2217) with radius r.\nSince the proof of the Multi-loop algorithm and the SPSA based algorithm is almost identical (except the \u03bd\u2212update), we will focus on proving the SPSA based actor critic algorithm."}, {"heading": "B.4.1 Proof of Theorem 6: TD(0) Critic Update (v\u2212update)", "text": "By the step length conditions, one notices that {vk} converges in a faster time scale than {\u03b8k}, {\u03bdk} and {\u03bbk}, one can assume (\u03b8, \u03bd, \u03bb) in the v\u2212update as fixed quantities. The critic update can be re-written as follows:\nvk+1 = vk + \u03b64(k)\u03c6(xk , sk)\u03b4k(vk) (61)\nwhere the scaler\n\u03b4k (v) = \u2212\u03c6\u22a4(xk, sk)v + \u03b3\u03c6\u22a4 (xk+1, sk+1) v + C\u0304(xk, sk, ak).\nis known as the temporal difference (TD). Define\nA = \u2211\ny,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)\u03c6(y, s\u2032)\n \u03c6\u22a4(y, s\u2032)\u2212 \u03b3 \u2211\nz,s\u2032\u2032\nP\u0304 (z, s\u2032\u2032|y, s\u2032, a)\u03c6\u22a4 (z, s\u2032\u2032)\n \n(62)\nand b = \u2211\nyX,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)\u03c6(y, s\u2032)C\u0304(y, s\u2032, a\u2032). (63)\nBased on the definitions of matrices A and b, it is easy to see that the TD(0) critic update vk in (61) can be re-written as the following stochastic approximation scheme:\nvk+1 = vk + \u03b64(k)(b \u2212Avk + \u03b4Ak+1) (64)\nwhere the noise term \u03b4Ak+1 is a square integrable Martingale difference, i.e, E[\u03b4Ak+1 | Fk] = 0 if the \u03b3\u2212stationary distribution \u03c0\u03b8\u03b3 used to generate samples of (xk, sk, ak). Fk is the filtration generated by different independent trajectories. By writing\n\u03b4Ak+1 = \u2212(b\u2212Avk) + \u03c6(xk, sk)\u03b4k(vk)\nand noting E\u03c0\u03b8\u03b3 [\u03c6(xk, sk)\u03b4k(vk) | Fk] = \u2212Avk + b, one can easily check that the stochastic approximation scheme in (61) is equivalent to the TD(0) iterates in (61) and \u03b4Ak+1 is a Martingale difference, i.e., E\u03c0\u03b8\u03b3 [\u03b4Ak+1 | Fk] = 0. Let\nh (v) = \u2212Av + b.\nBefore getting into the convergence analysis, we have the following technical lemma.\nLemma 8 Every eigenvalues of matrix A has positive real part.\nProof. To complete this proof, we need to show that for any vector v \u2208 R\u03ba2 , v\u22a4Av > 0. Now, for any fixed v \u2208 R\u03ba2 , define y(x, s) = v\u22a4\u03c6\u22a4(x, s). It can be easily seen from the definition of A that v\u22a4Av = \u2211\nx,x\u2032,a,s,s\u2032\ny(x, s)\u03c0\u03b8\u03b3(x, s, a|x0 = x0, s0 = \u03bd)\u00b7(1{x\u2032 = x, s\u2032 = s}\u2212\u03b3P\u0304 (x\u2032, s\u2032|x, s, a))y(x\u2032, s\u2032).\nBy convexity of quadratic functions and Jensen\u2019s inequality, one can derive the following expressions:\n\u2211\nx,x\u2032,a,s,s\u2032\ny(x, s)\u03c0\u03b8\u03b3(x, s, a|x0 = x0, s0 = \u03bd)\u03b3P\u0304 (x\u2032, s\u2032|x, s, a)y(x\u2032, s\u2032)\n\u2264\u2016y\u2016d\u03b8\u03b3 \u221a \u03b3\n\u221a \u2211\nx,x\u2032,a,s,s\u2032\nd\u03b8\u03b3(x, s|x0 = x0, s0 = \u03bd)\u03b3\u00b5(a|x, s; \u03b8)P (x\u2032, s\u2032|x, s, a)(y(x\u2032, s\u2032))2\n=\u2016y\u2016d\u03b8\u03b3 \u221a\u2211\ny,s\u2032\n( d\u03b8\u03b3(y, s\u2032|x0, \u03bd)\u2212 (1\u2212 \u03b3)1{x0 = y, \u03bd = s\u2032} ) (y(x\u2032, s\u2032))2\n<\u2016y\u20162d\u03b8\u03b3\nwhere d\u03b8\u03b3(x, s|x0 = x0, s0 = \u03bd)\u00b5(a|x, s; \u03b8) = \u03c0\u03b8\u03b3(x, s, a|x0 = x0, s0 = \u03bd) and\n\u2016y\u20162d\u03b8\u03b3 = \u2211\nx,s\nd\u03b8\u03b3(x, s|x0 = x0, s0 = \u03bd)(y(x, s))2.\nThe first inequality is due to the fact that \u00b5(a|x, s; \u03b8), P\u0304 (y, s\u2032|x, s, a) \u2208 [0, 1] and convexity of quadratic function, the second equality is based on the stationarity property of a \u03b3\u2212visiting distribution: d\u03b8\u03b3(y, s\u2032|x0, \u03bd) \u2265 0, \u2211 y,s\u2032 d \u03b8 \u03b3(y, s\n\u2032|x0, \u03bd) = 1 and \u2211\nx\u2032,s,a\n\u03c0\u03b8\u03b3(x \u2032, s, a|x0 = x0, s0 = \u03bd)\u03b3P\u0304 (y, s\u2032|x\u2032, s, a\u2032) = d\u03b8\u03b3(y, s\u2032|x0, \u03bd)\u2212(1\u2212\u03b3)1{x0 = y, \u03bd = s\u2032}.\nAs the above argument holds for any v \u2208 R\u03ba2 and y(x, s) = v\u22a4\u03c6(x, s), one shows that v\u22a4Av > 0 for any v \u2208 R\u03ba2 . This further implies v\u22a4A\u22a4v > 0 and v\u22a4(A\u22a4+A)v > 0 for any v \u2208 R\u03ba2 . Therefore, A + A\u22a4 is a symmetric positive definite matrix, i.e. there exists a \u01eb > 0 such that A + A\u22a4 > \u01ebI . To complete the proof, suppose by contradiction that there exists an eigenvalue \u03bb of A which has a non-positive real-part. Let v\u03bb be the corresponding eigenvector of \u03bb. Then, by pre- and post-multiplying v\u2217\u03bb and v\u03bb to A + A\u22a4 > \u01ebI and noting that the hermitian of a real matrix A is A\u22a4, one obtains 2Re(\u03bb)\u2016v\u03bb\u20162 = v\u2217\u03bb(A + A\u22a4)v\u03bb = v\u2217\u03bb(A + A\u2217)v\u03bb > \u01eb\u2016v\u03bb\u20162. This implies Re(\u03bb) > 0, i.e., a contradiction. By combining all previous arguments, one concludes that every eigenvalues A has positive real part.\nWe now turn to the analysis of the TD(0) iteration. Note that the following properties hold for the TD(0) update scheme in (61):\n1. h (v) is Lipschitz.\n2. The step size satisfies the following properties in Appendix B.1.\n3. The noise term \u03b4Ak+1 is a square integrable Martingale difference.\n4. The function hc (v) := h (cv) /c, c \u2265 1\nconverges uniformly to a continuous function h\u221e (v) for any w in a compact set, i.e., hc (v) \u2192 h\u221e (v) as c \u2192 \u221e.\n5. The ordinary differential equation (ODE)\nv\u0307 = h\u221e (v)\nhas the origin as its unique globally asymptotically stable equilibrium.\nThe fourth property can be easily verified from the fact that the magnitude of b is finite and h\u221e (v) = v. The fifth property follows directly from the facts that h\u221e (v) = \u2212Av and all eigenvalues of A have positive real parts. Therefore, by Theorem 3.1 in [14], these five properties imply the following condition:\nThe TD iterates {vk} is bounded almost surely, i.e., sup k \u2016vk\u2016 < \u221e almost surely.\nFinally, from the standard stochastic approximation result, from the above conditions, the convergence of the TD(0) iterates in (61) can be related to the asymptotic behavior of the ODE v\u0307 = h (v) = b\u2212Av. (65) By Theorem 2 in Chapter 2 of [14], when property (1) to (3) in (65) hold, then vk \u2192 v\u2217 with probability 1 where the limit v\u2217 depends on (\u03b8, \u03bd, \u03bb) and is the unique solution satisfying h (v\u2217) = 0, i.e., Av\u2217 = b. Therefore, the TD(0) iterates converges to the unique fixed point v\u2217 almost surely, at k \u2192 \u221e."}, {"heading": "B.4.2 Proof of Theorem 7", "text": "Step 1 (Convergence of v\u2212update) The proof of the critic parameter convergence follows directly from Theorem 6.\nStep 2 (Convergence of SPSA based \u03bd\u2212update) In this section, we present the \u03bd\u2212update for the incremental actor critic method. This update is based on the SPSA perturbation method. The idea of this method is to estimate the sub-gradient g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) using two simulated value functions corresponding to \u03bd\u2212 = \u03bd \u2212 \u2206 and \u03bd+ = \u03bd + \u2206. Here \u2206 \u2265 0 is a positive random perturbation that vanishes asymptotically.\nThe SPSA-based estimate for a sub-gradient g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb) is given by:\ng(\u03bd) \u2248 \u03bb+ 1 2\u2206\n( \u03c6\u22a4 ( x0, \u03bd +\u2206 ) \u2212 \u03c6\u22a4 ( x0, \u03bd \u2212\u2206 )) v\nwhere \u2206 \u2265 0 is a \u201csmall\u201d random perturbation of the finite difference sub-gradient approximation.\nNow, we turn to the convergence analysis of sub-gradient estimation and \u03bd\u2212update. Since v converges faster than \u03bd, and \u03bd converges faster then \u03b8 and \u03bb, the \u03bd\u2212update in (18) can be rewritten using the converged critic-parameter v\u2217(\u03bd) and (\u03b8, \u03bb) in this expression is viewed as constant quantities, i.e.,\n\u03bdk+1 = \u0393N ( \u03bdk \u2212 \u03b63(k) ( \u03bb+ 1\n2\u2206k\n( \u03c6\u22a4 ( x0, \u03bdk +\u2206k ) \u2212 \u03c6\u22a4 ( x0, \u03bdk \u2212\u2206k )) v\u2217(\u03bdk) )) .\n(66) First, we have the following assumption on the feature functions in order to prove\nthe SPSA approximation is asymptotically unbiased.\nAssumption 9 For any v \u2208 R\u03ba1 , the feature function satisfies the following conditions\n|\u03c6\u22a4V ( x0, \u03bd +\u2206 ) v \u2212 \u03c6\u22a4V ( x0, \u03bd \u2212\u2206 ) v| \u2264 K1(v)(1 + \u2206).\nFurthermore, the Lipschitz constants are uniformly bounded, i.e., supv\u2208R\u03ba1 K 2 1(v) < \u221e. This assumption is mild because the expected utility objective function implies that L(\u03b8, \u03bd, \u03bb) is Lipschitz in \u03bd, and \u03c6\u22a4V ( x0, \u03bd ) v is just a linear function approximation of V \u03b8(x0, \u03bd). Then, we establish the bias and convergence of stochastic sub-gradient estimates. Let g(\u03bdk) \u2208 argmax {g : g \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bdk} and\n\u039b1,k+1 =\n(( \u03c6\u22a4 ( x0, \u03bdk +\u2206k ) \u2212 \u03c6\u22a4 ( x0, \u03bdk \u2212\u2206k )) v\u2217(\u03bdk)\n2\u2206k \u2212 EM (k)\n) ,\n\u039b2,k =\u03bbk + E L M (k)\u2212 g(\u03bdk), \u039b3,k =EM (k)\u2212 ELM (k),\nwhere\nEM (k) :=E\n[ 1\n2\u2206k\n( \u03c6\u22a4 ( x0, \u03bdk +\u2206k ) \u2212 \u03c6\u22a4 ( x0, \u03bdk \u2212\u2206k )) v\u2217(\u03bdk) | \u2206k\n]\nELM (k) :=E\n[ 1\n2\u2206k\n( V \u03b8 ( x0, \u03bdk +\u2206k ) \u2212 V \u03b8 ( x0, \u03bdk \u2212\u2206k )) | \u2206k ] .\nNote that (66) is equivalent to\n\u03bdk+1 = \u03bdk \u2212 \u03b63(k) (g(\u03bdk) + \u039b1,k+1 + \u039b2,k + \u039b3,k) (67)\nFirst, it is obvious that \u039b1,k+1 is a Martingale difference as E[\u039b1,k+1 | Fk] = 0, which implies\nMk+1 =\nk\u2211\nj=0\n\u03b63(j)\u039b1,j+1\nis a Martingale with respect to filtration Fk. By Martingale convergence theorem, we can show that if supk\u22650 E[M 2 k ] < \u221e, when k \u2192 \u221e, Mk converges almost surely and \u03b63(k)\u039b1,k+1 \u2192 0 almost surely. To show that supk\u22650 E[M2k ] < \u221e, for any t \u2265 0 one observes that,\nE[M2k+1] =\nk\u2211\nj=0\n(\u03b63(j)) 2 E[E[\u039b21,j+1 | \u2206j ]]\n\u22642 k\u2211\nj=0\nE\n[( \u03b63(j)\n2\u2206j\n)2 { E [( ( \u03c6\u22a4 ( x0, \u03bdj +\u2206j ) \u2212 \u03c6\u22a4 ( x0, \u03bdj \u2212\u2206j ) ) v\u2217(\u03bdj) )2 | \u2206j ]\n+E [( \u03c6\u22a4 ( x0, \u03bdj +\u2206j ) \u2212 \u03c6\u22a4 ( x0, \u03bdj \u2212\u2206j ) ) v\u2217(\u03bdj) | \u2206j\n]2} ]\nNow based on Assumption 9, the above expression implies\nE[M2k+1] \u22642 k\u2211\nj=0\nE\n[( \u03b63(j)\n2\u2206j\n)2 2K21 (1 + \u2206j) 2\n]\nCombining the above results with the step length conditions, there exists K = 4K21 > 0 such that\nsup k\u22650\nE[M2k+1] \u2264 K \u221e\u2211\nj=0\nE\n[( \u03b63(j)\n2\u2206j\n)2] + (\u03b62(j)) 2 < \u221e.\nSecond, by the \u201cMin Common/Max Crossing\u201d theorem, one can show \u2202\u03bdL(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bdk is a non-empty, convex and compact set. Therefore, by duality of directional directives and sub-differentials, i.e.,\nmax {g : g \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bdk} = lim \u03be\u21930 L(\u03b8, \u03bdk + \u03be, \u03bb)\u2212 L(\u03b8, \u03bdk \u2212 \u03be, \u03bb) 2\u03be ,\none concludes that for \u03bbk = \u03bb (converges in a slower time scale),\n\u03bb+ ELM (k) = g(\u03bdk) +O(\u2206k), almost surely.\nThis further implies that\n\u039b2,k = O(\u2206k), i.e., \u039b2,k \u2192 0 as k \u2192 \u221e, almost surely.\nThird, since d\u03b8\u03b3(x 0, \u03bd|x0, \u03bd) = 1, from definition of \u01eb\u03b8(v\u2217(\u03bdk)) it is obvious that |\u039b3,k| \u2264 2\u01eb\u03b8(v\u2217(\u03bdk))E[1/\u2206k]. When t goes to infinity, \u01eb\u03b8(v\u2217(\u03bdk))E[1/\u2206k] \u2192 0 by assumption and \u039b3,k \u2192 0. Finally, as we have just showed that \u03b62(k)\u039b1,k+1 \u2192 0, \u039b2,k \u2192 0 and \u039b3,k \u2192 0 almost surely, the \u03bd\u2212update in (67) is a stochastic approximations of an element in the differential inclusion\nNow we turn to the convergence analysis of \u03bd. It can be easily seen that the \u03bd\u2212update in (18) is a noisy sub-gradient descent update with vanishing disturbance bias. This update can be viewed as an Euler discretization of the following differential inclusion \u03bd\u0307 \u2208 \u03a5\u03bd [\u2212g(\u03bd)] , \u2200g(\u03bd) \u2208 \u2202\u03bdL(\u03b8, \u03bd, \u03bb), (68) Thus, the \u03bd\u2212convergence analysis follows from analogous convergence analysis in step 1 of Theorem 2\u2019s proof.\nStep 3 (Convergence of \u03b8\u2212update) We first analyze the actor update (\u03b8\u2212update). Since \u03b8 converges in a faster time scale than \u03bb, one can assume \u03bb in the \u03b8\u2212update as a fixed quantity. Furthermore, since v and \u03bd converge in a faster scale than \u03b8, one can also replace v and \u03bd with their limits v\u2217(\u03b8) and \u03bd\u2217(\u03b8) in the convergence analysis. In the following analysis, we assume that the initial state x0 \u2208 X is given. Then the \u03b8\u2212update in (19) can be re-written as follows:\n\u03b8k+1 = \u0393\u0398 ( \u03b8k \u2212 \u03b62(k) ( \u2207\u03b8 log\u00b5(ak|xk, sk; \u03b8)|\u03b8=\u03b8k \u03b4k(v \u2217(\u03b8k))\n1\u2212 \u03b3\n)) . (69)\nSimilar to the trajectory based algorithm, we need to show that the approximation of \u2207\u03b8L(\u03b8, \u03bd, \u03bb) is Lipschitz in \u03b8 in order to show the convergence of the \u03b8 parameter. This result is generalized in the following proposition.\nProposition 10 The following function is a Lipschitz function in \u03b8:\n1 1\u2212 \u03b3 \u2211\nx,a,s \u03c0\u03b8\u03b3(x, s, a|x0 = x0, s0 = \u03bd)\u2207\u03b8 log\u00b5(a|x, s; \u03b8)  \u2212v\u22a4\u03c6(x, s) + \u03b3 \u2211\nx\u2032,s\u2032\nP\u0304 (x\u2032, s\u2032|x, s, a)v\u22a4\u03c6(x\u2032, s\u2032) + C\u0304(x, s, a)   .\nProof. First consider the feature vector v. Recall that the feature vector satisfies the linear equation Av = b where A and b are functions of \u03b8 found from the Hilbert space projection of Bellman operator. It has been shown in Lemma 1 of [7] that, by exploiting the inverse of A using Cramer\u2019s rule, one can show that v is continuously differentiable\nof \u03b8. Next, consider the \u03b3\u2212 visiting distribution \u03c0\u03b8\u03b3 . From an application of Theorem 2 of [1] (or Theorem 3.1 of [31]), it can be seen that the stationary distribution \u03c0\u03b8\u03b3 of the process (xk, sk) is continuously differentiable in \u03b8. Recall from Assumption (B1) that \u2207\u03b8\u00b5(ak|xk, sk; \u03b8) is a Lipschitz function in \u03b8 for any a \u2208 A and k \u2208 {0, . . . , T \u2212 1} and \u00b5(ak|xk, sk; \u03b8) is differentiable in \u03b8. Therefore, by combining these arguments and noting that the sum of products of Lipschitz functions is Lipschitz, one concludes that \u2207\u03b8L(\u03b8, \u03bd, \u03bb) is Lipschitz in \u03b8.\nConsider the case in which the value function for a fixed policy \u00b5 is approximated by a learned function approximator, \u03c6\u22a4(x, s)v\u2217. If the approximation is sufficiently good, we might hope to use it in place of V \u03b8(x, s) and still point roughly in the direction of the true gradient. Recall the temporal difference error (random variable) for given (xk, sk) \u2208 X \u00d7 R\n\u03b4k (v) = \u2212v\u22a4\u03c6(xk, sk) + \u03b3v\u22a4\u03c6 (xk+1, sk+1) + C\u0304(xk, sk, ak).\nDefine the v\u2212dependent approximated advantage function\nA\u0303\u03b8,v(x, s, a) = Q\u0303\u03b8,v(x, s, a)\u2212 v\u22a4\u03c6(x, s),\nwhere Q\u0303\u03b8,v(x, s, a) = \u03b3 \u2211\nx\u2032,s\u2032\nP\u0304 (x\u2032, s\u2032|x, s, a)v\u22a4\u03c6(x\u2032, s\u2032) + C\u0304(x, s, a).\nThe following Lemma first shows that \u03b4k(v) is an unbiased estimator of A\u0303\u03b8,v.\nLemma 11 For any given policy \u00b5 and v \u2208 R\u03ba2 , we have\nA\u0303\u03b8,v(x, s, a) = E[\u03b4k(v) | xk = x, sk = s, ak = a].\nProof. Note that for any v \u2208 R\u03ba2 , E[\u03b4k(v) | xk = x, sk = s, ak = a, \u00b5] = C\u0304(x, s, a)\u2212v\u22a4\u03c6(x, s)+\u03b3E [ v\u22a4\u03c6(xk+1, sk+1) | xk = x, sk = s, ak = a ] .\nwhere\nE [ v\u22a4\u03c6(xk+1, sk+1) | xk = x, sk = s, ak = a ] = \u2211\nx\u2032,s\u2032\nP\u0304 (x\u2032, s\u2032|x, s, a)v\u22a4\u03c6(x\u2032, s\u2032).\nBy recalling the definition of Q\u0303\u03b8,v(x, s, a), the proof is completed. Now, we turn to the convergence proof of \u03b8.\nTheorem 12 Suppose \u03b8\u2217 is the equilibrium point of the continuous system \u03b8 satisfying\n\u03a5\u03b8 [ \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) ] = 0. (70)\nThen the sequence of \u03b8\u2212updates in (19) converges to \u03b8\u2217 almost surely.\nProof. First, the \u03b8\u2212update from (69) can be re-written as follows:\n\u03b8k+1 = \u0393\u0398 ( \u03b8k + \u03b62(k) ( \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8),\u03b8=\u03b8k + \u03b4\u03b8k+1 + \u03b4\u03b8\u01eb ))\nwhere\n\u03b4\u03b8k+1 = \u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8k\u03b3 (x \u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8k))\u2207\u03b8 log \u00b5(a\u2032|x\u2032, s\u2032; \u03b8)|\u03b8=\u03b8k\nA\u0303\u03b8k,v \u2217(\u03b8k)(x\u2032, s\u2032, a\u2032)\n1\u2212 \u03b3\n\u2212\u2207\u03b8 log \u00b5(ak|xk, sk; \u03b8)|\u03b8=\u03b8k \u03b4k(v\n\u2217(\u03b8k))\n1\u2212 \u03b3 . (71)\nis a square integrable stochastic term of the \u03b8\u2212update and\n\u03b4\u03b8\u01eb = \u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8k\u03b3 (x \u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8k)) \u2207\u03b8 log \u00b5(a\u2032|x\u2032, s\u2032; \u03b8)|\u03b8=\u03b8k 1\u2212 \u03b3 (A \u03b8k (x\u2032, s\u2032, a\u2032)\u2212 A\u0303\u03b8k,v \u2217(\u03b8k)(x\u2032, s\u2032, a\u2032))\n\u2264\u2016\u03c8\u03b8k\u2016\u221e 1\u2212 \u03b3\n\u221a( 1 + \u03b3\n1\u2212 \u03b3\n) \u01eb\u03b8k (v \u2217(\u03b8k)).\nwhere \u03c8\u03b8(x, s, a) = \u2207\u03b8 log\u00b5(a|x, s; \u03b8) is the \u201ccompatible feature\u201d. The last inequality is due to the fact that for \u03c0\u03b8\u03b3 being a probability measure, convexity of quadratic functions implies\n\u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(x \u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8))(A\u03b8(x\u2032, s\u2032, a\u2032)\u2212 A\u0303\u03b8,v(x\u2032, s\u2032, a\u2032))\n\u2264 \u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(x \u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8))(Q\u03b8(x\u2032, s\u2032, a\u2032)\u2212 Q\u0303\u03b8,v(x\u2032, s\u2032, a\u2032))\n+ \u2211\nx\u2032,s\u2032\nd\u03b8\u03b3(x \u2032, s\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8))(V \u03b8(x\u2032, s\u2032)\u2212 V\u0303 \u03b8,v(x\u2032, s\u2032))\n=\u03b3 \u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(x \u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8))\n\u2211\nx\u2032\u2032,s\u2032\u2032\nP\u0304 (x\u2032\u2032, s\u2032\u2032|x\u2032, s\u2032, a\u2032)(V \u03b8(x\u2032\u2032, s\u2032\u2032)\u2212 \u03c6\u22a4(x\u2032\u2032, s\u2032\u2032)v)\n+\n\u221a\u2211\nx\u2032,s\u2032\nd\u03b8\u03b3(x\u2032, s\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8))(V \u03b8(x\u2032, s\u2032)\u2212 V\u0303 \u03b8,v(x\u2032, s\u2032))2\n\u2264\u03b3 \u221a \u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(x\u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd\u2217(\u03b8)) \u2211\nx\u2032\u2032,s\u2032\u2032\nP\u0304 (x\u2032\u2032, s\u2032\u2032|x\u2032, s\u2032, a\u2032)(V \u03b8(x\u2032\u2032, s\u2032\u2032)\u2212 \u03c6\u22a4(x\u2032\u2032, s\u2032\u2032)v)2\n+\n\u221a \u01eb\u03b8(v)\n1\u2212 \u03b3\n\u2264\u221a\u03b3 \u221a \u2211\nx\u2032\u2032,s\u2032\u2032\n( d\u03b8\u03b3(x\u2032\u2032, s\u2032\u2032|x0, \u03bd\u2217(\u03b8))\u2212 (1\u2212 \u03b3)1{x0 = x\u2032\u2032, \u03bd = s\u2032\u2032} ) (V \u03b8(x\u2032\u2032, s\u2032\u2032)\u2212 \u03c6\u22a4(x\u2032\u2032, s\u2032\u2032)v)2 +\n\u221a \u01eb\u03b8(v)\n1\u2212 \u03b3\n\u2264 \u221a( 1 + \u03b3\n1\u2212 \u03b3\n) \u01eb\u03b8(v)\nThen by Lemma 11, if the \u03b3\u2212stationary distribution \u03c0\u03b8\u03b3 is used to generate samples of (xk, sk, ak), one obtains E [\u03b4\u03b8k+1 | F\u03b8,k] = 0, where F\u03b8,k = \u03c3(\u03b8m, \u03b4\u03b8m, m \u2264 k) is the filtration generated by different independent trajectories. On the other hand, |\u03b4\u03b8\u01eb| \u2192 0 as \u01eb\u03b8k(v\u2217(\u03b8k)) \u2192 0. Therefore, the \u03b8\u2212update in (69) is a stochastic approximation of the ODE\n\u03b8\u0307 = \u03a5\u03b8 [ \u2212\u2207\u03b8L(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bd\u2217(\u03b8) ]\nwith an error term that is a sum of a vanishing bias and a Martingale difference. Thus, the convergence analysis of \u03b8 follows analogously from the step 2 of Theorem 2\u2019s proof.\nStep 4 (Local Minimum) The proof of local minimum of (\u03b8\u2217, \u03bd\u2217) follows directly from the arguments in Step 3 of Theorem 2\u2019s proof.\nStep 5 (The \u03bb\u2212update and Convergence to Saddle Point) Notice that \u03bb\u2212update converges in a slowest time scale, (18) can be rewritten using the converged v\u2217(\u03bb), \u03b8\u2217(\u03bb) and \u03bd\u2217(\u03bb), i.e.,\n\u03bbk+1 = \u0393\u039b ( \u03bbk + \u03b61(k) ( \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bbk + \u03b4\u03bbk+1 )) (72)\nwhere \u03b4\u03bbk+1 = \u2212\u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb),\u03bb=\u03bbk + ( \u03bd\u2217(\u03bbk) + (\u2212sk)+ (1\u2212 \u03b1)(1 \u2212 \u03b3)1{xk = xT } \u2212 \u03b2 ) (73) is a square integrable stochastic term of the \u03bb\u2212update. Similar to the \u03b8\u2212update, by using the \u03b3\u2212stationary distribution \u03c0\u03b8\u03b3 , one obtains E [\u03b4\u03bbk+1 | F\u03bb,k] = 0 where F\u03bb,k = \u03c3(\u03bbm, \u03b4\u03bbm, m \u2264 k) is the filtration of \u03bb generated by different independent trajectories. As above, the \u03bb\u2212update is a stochastic approximation of the ODE\n\u03bb\u0307 = \u03a5\u03bb [ \u2207\u03bbL(\u03b8, \u03bd, \u03bb) \u2223\u2223\u2223\u2223 \u03b8=\u03b8\u2217(\u03bb),\u03bd=\u03bd\u2217(\u03bb) ]\nwith an error term that is a Martingale difference. Then the \u03bb\u2212convergence and the (local) saddle point analysis follows from analogous arguments in step 4 and 5 of Theorem 2\u2019s proof.\nStep 2\u2032 (Convergence of Multi-loop \u03bd\u2212update) Since \u03bd converges on a faster timescale than \u03b8 and \u03bb, the \u03bd\u2212update in (21) can be rewritten using the fixed (\u03b8, \u03bb), i.e.,\n\u03bdi+1 = \u0393N ( \u03bdi \u2212 \u03b62(i) ( \u03bb\u2212 \u03bb\n1\u2212 \u03b1 ( P ( sT \u2264 0 | x0 = x0, s0 = \u03bdi, \u00b5 ) + \u03b4\u03bdM,i+1\n)))\n(74) and\n\u03b4\u03bdM,i+1 = \u2212P ( sT \u2264 0 | x0 = x0, s0 = \u03bdi, \u00b5 ) + 1 {sT \u2264 0} (75)\nis a square integrable stochastic term of the \u03bd\u2212update. It is obvious thatE [\u03b4\u03bdM,i+1 | F\u03bd,i] = 0, where F\u03bd,i = \u03c3(\u03bdm, \u03b4\u03bdm, m \u2264 i) is the corresponding filtration of \u03bd, the \u03bd\u2212update in (21) is a stochastic approximations of an element in the differential inclusion \u2202\u03bdL(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bdi for any i with an error term that is a Martingale difference, i.e.,\n\u03bb 1\u2212 \u03b1P ( sT \u2264 0 | x0 = x0, s0 = \u03bdi, \u00b5 ) \u2212 \u03bb \u2208 \u2212\u2202\u03bdL(\u03b8, \u03bd, \u03bb)|\u03bd=\u03bdi .\nThus, the \u03bd\u2212update in (74) can be viewed as an Euler discretization of the differential inclusion in (68), and the \u03bd\u2212convergence analysis follows from analogous convergence analysis in step 1 of Theorem 2\u2019s proof."}, {"heading": "C Experimental Results", "text": ""}, {"heading": "C.1 Problem Setup and Parameters", "text": "The house purchasing problem can be reformulated as follows\nmin \u03b8\nE [ D\u03b8(x0) ] subject to CVaR\u03b1 ( D\u03b8(x0 ) \u2264 \u03b2. (76)\nwhere D\u03b8(x0) = \u2211T\nk=0 \u03b3 k (1{uk = 1}ck + 1{uk = 0}ph) | x0 = x, \u00b5. We will set\nthe parameters of the MDP as follows: x0 = [1; 0], ph = 0.1, T = 20, \u03b3 = 0.95, fu = 1.5, fd = 0.8 and p = 0.65. For the risk constrained policy gradient algorithm, the step-length sequence is given as follows,\n\u03b61(i) = 0.1\ni , \u03b62(i) =\n0.05 i0.8 , \u03b63(i) = 0.01 i0.55 , \u2200i.\nThe CVaR parameter and constraint threshold are given by \u03b1 = 0.9 and \u03b2 = 1.9. The number of sample trajectories N is set to 100.\nFor the risk constrained actor critic algorithm, the step-length sequence is given as follows,\n\u03b61(i) = 1\ni , \u03b62(i) =\n1\ni0.85 , \u03b63(i) =\n0.5 i0.7 , \u03b63(i) = 0.5 i0.55 , \u2206k = 0.5 i0.1 , \u2200i.\nThe CVaR parameter and constraint threshold are given by \u03b1 = 0.9 and \u03b2 = 2.5. One can later see that the difference in risk thresholds is due to the different family of parametrized Boltzmann policies.\nThe parameter bounds are given as follows: \u03bbmax = 1000, \u0398 = [\u221260, 60]\u03ba1 and Cmax = 4000 > x0 \u00d7 fTu ."}, {"heading": "C.2 Trajectory Based Algorithms", "text": "In this section, we have implemented the following trajectory based algorithms.\n1. PG: This is a policy gradient algorithm that minimizes the expected discounted cost function, without considering any risk criteria.\n2. PG-CVaR: This is the CVaR constrained simulated trajectory based policy gradient algorithm that is given in Section 4.\nIt is well known that a near-optimal policy \u00b5 was obtained using the LSPI algorithm with 2-dimensional radial basis function (RBF) features. We will also implement the 2-dimensional RBF feature function \u03c6 and consider the family Boltzmann policies for policy parametrization\n\u00b5(a|x; \u03b8) = exp(\u03b8 \u22a4\u03c6(x, a))\u2211\na\u2032\u2208A exp(\u03b8 \u22a4\u03c6(x, a\u2032))\n.\nThe experiments for each algorithm comprised of the following two phases:\n1. Tuning phase: Here each iteration involved the simulation run with the nominal policy parameter \u03b8 where the run length for a particular policy parameter is at most T steps. We run the algorithm for 1000 iterations and stop when the parameter (\u03b8, \u03bd, \u03bb) converges.\n2. Converged run: Followed by the tuning phase, we obtained the converged policy parameter \u03b8\u2217. In the converged run phase, we perform simulation with this policy parameter for 1000 runs where each simulation generates a trajectory of at most T steps. The results reported are averages over these iterations.\nC.3 Incremental Based Algorithm\nOn the other hand, we have also implemented the following incremental based algorithms.\n1. AC: This is an actor critic algorithm that minimizes the expected discounted cost function, without considering any risk criteria. This is similar to Algorithm 1 in [6].\n2. AC-CVaR-Semi-Traj.: This is the CVaR constrained multi-loop actor critic algorithm that is given in Section 5.\n3. AC-CVaR-SPSA: This is the CVaR constrained SPSA actor critic algorithm that is given in Section 5.\nSimilar to the trajectory based algorithms, we will implement the RBFs as feature functions for [x; s] and consider the family of augmented state Boltzmann policies,\n\u00b5(a|(x, s); \u03b8) = exp(\u03b8 \u22a4\u03c6(x, s, a))\u2211\na\u2032\u2208A exp(\u03b8 \u22a4\u03c6(x, s, a\u2032))\n.\nSimilarly, the experiments also comprise of two phases: 1) the tuning phase where the set of parameters (v, \u03b8, \u03bd, \u03bb) is obtained after the algorithm converges, and 2) the converged run where the policy parameter is simulated for 1000 runs."}, {"heading": "D Bellman Equation and Projected Bellman Equation for Expected Utility Function", "text": ""}, {"heading": "D.1 Bellman Operator for Expected Utility Functions", "text": "First, we want find the Bellman equation for the objective function\nE [ D\u03b8(x0) | x0 = x0, s0 = s0, \u00b5 ] + \u03bb 1\u2212 \u03b1E [[ D\u03b8(x0)\u2212 s0 ]+ | x0 = x0, s0 = s0, \u00b5 ] (77) where \u03bb and (x0, s0) \u2208 X \u00d7 R are given.\nFor any function V : X \u00d7 R \u2192 R, recall the following Bellman operator on the augmented space X \u00d7 R:\nT\u03b8[V ](x, s) := \u2211\na\u2208A\n\u00b5(a|x, s; \u03b8)   C\u0304(x, s, a) + \u2211\nx\u2032,s\u2032\n\u03b3P\u0304 (x\u2032, s\u2032|x, s, a)V (x\u2032, s\u2032)    .\nFirst, it is easy to show that this Bellman operator satisfies the following properties.\nProposition 13 The Bellman operator T\u03b8[V ] has the following properties:\n\u2022 (Monotonicity) If V1(x, s) \u2265 V2(x, s), for anyx \u2208 X , s \u2208 R, then T\u03b8[V1](x, s) \u2265 T\u03b8[V2](x, s).\n\u2022 (Constant shift) For K \u2208 R, T\u03b8[V +K](x, s) = T\u03b8[V ](x, s) + \u03b3K .\n\u2022 (Contraction) \u2016T\u03b8[V1]\u2212 T\u03b8[V2]\u2016\u221e \u2264 \u03b3\u2016V1 \u2212 V2\u2016\u221e,\nwhere \u2016f\u2016\u221e = maxx\u2208X ,s\u2208R |f(x, s)|.\nProof. The proof of monotonicity and constant shift properties follow directly from the definitions of the Bellman operator. Furthermore, denote c = \u2016V1 \u2212 V2\u2016\u221e. Since\nV2(x, s) \u2212 \u2016V1 \u2212 V2\u2016\u221e \u2264 V1(x, s) \u2264 V2(x, s) + \u2016V1 \u2212 V2\u2016\u221e, \u2200x \u2208 X , s \u2208 R,\nby monotonicity and constant shift property,\nT\u03b8[V2](x, s)\u2212\u03b3\u2016V1\u2212V2\u2016\u221e \u2264 T\u03b8[V1](x, s) \u2264 T\u03b8[V2](x, s)+\u03b3\u2016V1\u2212V2\u2016\u221e \u2200x \u2208 X , s \u2208 R.\nThis further implies that\n|T\u03b8[V1](x, s) \u2212 T\u03b8[V2](x, s)| \u2264 \u03b3\u2016V1 \u2212 V2\u2016\u221e \u2200x \u2208 X , s \u2208 R\nand the contraction property follows. The following theorems show there exists a unique fixed point solution to T\u03b8[V ](x, s) = V (x, s), where the solution equals to the value function expected utility.\nTheorem 14 (Equivalence Condition) For any bounded function V0 : X \u00d7 R \u2192 R, there exists a limit function V \u03b8 such that V \u03b8(x, s) = limN\u2192\u221e TN\u03b8 [V0](x, s). Furthermore,\nV \u03b8(x0, s0) = E [ D\u03b8(x0) | x0 = x0, \u00b5 ] + \u03bb 1\u2212 \u03b1E [[ D\u03b8(x0)\u2212 s0 ]+ | x0 = x0, s0 = s0, \u00b5 ] .\nProof. The first part of the proof is to show that for any x \u2208 X and s \u2208 R,\nVn(x, s) := T n \u03b8 [V0](x 0, s0) = E\n[ n\u22121\u2211\nk=0\n\u03b3kC\u0304(xk, sk, ak) + \u03b3 nV0(xn, sn) | x0 = x, s0 = s, \u00b5\n]\n(78) by induction. For n = 1, V1(x, s) = T\u03b8[V0](x, s) = E [ C\u0304(x0, s0, a0) + \u03b3V0(x1, s1) | x0 = x, s0 = s, \u00b5 ] . By induction hypothesis, assume (78) holds at n = k. For n = k + 1,\nVk+1(x, s) :=T k+1 \u03b8 [V0](x, s) = T\u03b8[Vk](x, s)\n= \u2211\na\u2208A\u0304\n\u00b5(a|x, s; \u03b8)   C\u0304(x, s, a) + \u2211\nx\u2032,s\u2032\n\u03b3P\u0304 (x\u2032, s\u2032|x, s, a)Vk ( x\u2032, s\u2032 )   \n= \u2211\na\u2208A\u0304\n\u00b5(a|x, s; \u03b8)\n \nC\u0304(x, s, a) + \u2211\nx\u2032,s\u2032\n\u03b3P\u0304 (x\u2032, s\u2032|x, s, a)\nE\n[ k\u22121\u2211\nk=0\n\u03b3kC\u0304(xk, sk, ak) + \u03b3 kV0(xk, sk) | x0 = x\u2032, s0 = s\u2032, \u00b5\n]}\n= \u2211\na\u2208A\u0304\n\u00b5(a|x, s; \u03b8)   C\u0304(x, s, a) + \u2211\nx\u2032,s\u2032\n\u03b3P\u0304 (x\u2032, s\u2032|x, s, a)\nE\n[ k\u2211\nt=1\n\u03b3kC\u0304(xk, sk, ak) + \u03b3 kV0(xk+1, sk+1) | x1 = x\u2032, s1 = s\u2032, \u00b5\n]}\n=E\n[ k\u2211\nk=0\n\u03b3kC\u0304(xk, sk, ak) + \u03b3 k+1V0(xk+1, sk+1) | x0 = x, s0 = s, \u00b5 ] .\nThus, the equality in (78) is proved by induction. The second part of the proof is to show that V \u03b8(x0, s0) := limn\u2192\u221e Vn(x0, s0) and\nV \u03b8(x0, s0) = E [ D\u03b8(x0) | x0 = x0, \u00b5 ] + \u03bb 1\u2212 \u03b1E [[ D\u03b8(x0)\u2212 s0 ]+ | x0 = x0, s0 = s0, \u00b5 ] .\nFrom the assumption of transient policies, one note that for any \u01eb > 0 there exists a sufficiently large k > N(\u01eb) such that \u2211\u221e t=k P(xn = z|x0, \u00b5) < \u01eb for z \u2208 X . This implies P(T < \u221e) > 1 \u2212 \u01eb. Since V0(x, s) is bounded for any x \u2208 X and s \u2208 R, the\nabove arguments imply V \u03b8(x0, s0) \u2264E [ T\u22121\u2211\nk=0\n\u03b3kC\u0304(xk, sk, ak) | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb) + \u01eb ( \u03bb\n1\u2212 \u03b1 (|s 0|+ Cmax) + Cmax 1\u2212 \u03b3\n)\n+ lim n\u2192\u221e E\n[ n\u22121\u2211\nt=T\n\u03b3kC\u0304(xk, sk, ak) + \u03b3 nV0(xn, sn) | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb)\n\u2264 lim n\u2192\u221e E\n[ T\u22121\u2211\nk=0\n\u03b3kC(xk, ak) | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb) + \u01eb ( 1\u2212 \u01eb \u01eb \u03b3n\u2016V0\u2016\u221e + \u03bb 1\u2212 \u03b1 (|s 0|+ Cmax) + Cmax 1\u2212 \u03b3 )\n+ E [ \u03b3T C\u0304(xT , sT , aT ) | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb)\n=E [ D\u03b8(x0) | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb)\n+ \u03bb 1\u2212 \u03b1E [ \u03b3T (\u2212sT )+ | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb) + \u01eb ( \u03bb 1\u2212 \u03b1 (|s 0|+ Cmax) + Cmax 1\u2212 \u03b3 )\n=E [ D\u03b8(x0) | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb)\n+ \u03bb 1\u2212 \u03b1E [[ D\u03b8(x0)\u2212 s0 ]+ | x0 = x0, s0 = s0, \u00b5 ] (1\u2212 \u01eb) + \u01eb ( \u03bb 1\u2212 \u03b1 (|s 0|+ Cmax) + Cmax 1\u2212 \u03b3 ) .\nThe first inequality is due to the fact for x0 = x0, s0 = s0,\nlim n\u2192\u221e\nn\u2211\nk=0\n\u03b3kC\u0304(xk, sk, ak) \u2264 \u03bb\n1\u2212 \u03b1 |s 0|+\n( 1 + \u03bb\n1\u2212 \u03b1\n) \u221e\u2211\nk=0\n\u03b3k|c(xk, ak)| \u2264 \u03bb\n1\u2212 \u03b1 (|s 0|+Cmax)+ Cmax 1\u2212 \u03b3 ,\nthe second inequality is due to 1) V0 is bounded, C\u0304(x, s, a) = C(x, a) when x 6= xT and 2) for sufficiently large k > N(\u01eb) and any z \u2208 X , \u221e\u2211\nt=k\n\u2211\ns\nP(xk = z, sk = s|x0 = x0, s0 = s0, \u00b5)ds = \u221e\u2211\nt=k\nP(xk = z|x0 = x0, s0 = s0, \u00b5) < \u01eb.\nThe first equality follows from the definition of transient policies and the second equality follows from the definition of stage-wise cost in the \u03bd\u2212augmented MDP.\nBy similar arguments, one can also show that\nV \u03b8(x0, s0) \u2265 \u01eb ( \u2212 lim\nn\u2192\u221e (1\u2212 \u01eb)\u03b3n\u2016V0\u2016\u221e/\u01eb\u2212 Cmax/(1\u2212 \u03b3)\n) + (1 \u2212 \u01eb)\n( E [ D\u03b8(x0) | x0 = x0, s0 = s0, \u00b5 ] + \u03bb 1\u2212 \u03b1E [[ D\u03b8(x0)\u2212 s0 ]+ | x0 = x0, s0 = s0, \u00b5 ]) .\nTherefore, by taking \u01eb \u2192 0, we have just shown that for any (x0, s0) \u2208 X \u00d7 R, V \u03b8(s0, s0) = E [ D\u03b8(x0) | x0 = x0, s0 = s0, \u00b5 ] +\u03bb/(1\u2212\u03b1)E [[ D\u03b8(x0)\u2212 s0 ]+ | x0 = x0, s0 = s0, \u00b5 ] .\nApart from the analysis in [4] where a fixed point result is defined based on the following specific set of functions V\u03b8 , we are going to provide the fixed point theorem for general spaces of augmented value functions.\nTheorem 15 (Fixed Point Theorem) There exists a unique solution to the fixed point equation: T\u03b8[V ](x, s) = V (x, s), \u2200x \u2208 X and s \u2208 R. Let V \u2217 : X \u00d7 R \u2192 R be such unique fixed point solution. Then,\nV \u2217(x, s) = V \u03b8(x, s), \u2200x \u2208 X , s \u2208 R.\nProof. For Vk+1(x, s) = T\u03b8[Vk](x, s) starting at V0 : X \u00d7 R \u2192 R one obtains by contraction that \u2016Vk+1 \u2212 Vk\u2016\u221e \u2264 \u03b3\u2016Vk \u2212 Vk\u22121\u2016\u221e. By the recursive property, this implies \u2016Vk+1 \u2212 Vk\u2016\u221e \u2264 \u03b3k\u2016V1 \u2212 V0\u2016\u221e. It follows that for every k \u2265 0 and m \u2265 1,\n\u2016Vk+m \u2212 Vk\u2016\u221e \u2264 m\u2211\ni=1\n\u2016Vk+i \u2212 Vk+i\u22121\u2016\u221e \u2264 \u03b3k(1 + \u03b3 + . . .+ \u03b3m\u22121)\u2016V1 \u2212 V0\u2016\u221e\n\u2264 \u03b3 k\n1\u2212 \u03b3 \u2016V1 \u2212 V0\u2016\u221e.\nTherefore, {Vk} is a Cauchy sequence and must converge to V \u2217 since (B(X \u00d7 R), \u2016 \u00b7 \u2016\u221e) is a complete space. Thus, we have for k \u2265 1,\n\u2016T\u03b8[V \u2217]\u2212V \u2217\u2016\u221e \u2264 \u2016T\u03b8[V \u2217]\u2212Vk\u2016\u221e+\u2016Vk\u2212V \u2217\u2016\u221e \u2264 \u03b3\u2016Vk\u22121\u2212V \u2217\u2016\u221e+\u2016Vk\u2212V \u2217\u2016\u221e.\nSince Vk converges to V \u2217, the above expression implies T\u03b8[V \u2217](x, s) = V \u2217(x, s) for any (x, s) \u2208 X \u00d7R. Therefore, V \u2217 is a fixed point. Suppose there exists another fixed point V\u0303 \u2217. Then,\n\u2016V\u0303 \u2217 \u2212 V \u2217\u2016\u221e = \u2016T\u03b8[V\u0303 \u03b8]\u2212 T\u03b8[V \u03b8]\u2016\u221e \u2264 \u03b3\u2016V\u0303 \u03b8 \u2212 V \u03b8\u2016\u221e\nfor \u03b3 \u2208 (0, 1). This implies that V\u0303 \u2217 = V \u2217. Furthermore, since V \u03b8(x, s) = limn\u2192\u221e T n\u03b8 [V0](x, s) with V0 : X \u00d7 R \u2192 R being an arbitrary initial value function. By the following convergence rate bound inequality\n\u2016T k\u03b8 [V0]\u2212 V \u2217\u2016\u221e = \u2016T k\u03b8 [V0]\u2212 T k\u03b8 [V \u2217]\u2016\u221e \u2264 \u03b3k\u2016V0 \u2212 V \u2217\u2016\u221e, \u03b3 \u2208 (0, 1),\none concludes that V \u03b8(x, s) = V \u2217(x, s) for any (x, s) \u2208 X \u00d7 R."}, {"heading": "D.2 The Projected Bellman Operator", "text": "Consider the v\u2212dependent linear value function approximation of V \u03b8(x, s), in the form of \u03c6\u22a4(x, s)v, where \u03c6(x, s) \u2208 R\u03ba2 represents the state-dependent feature. The feature vectors can also be dependent on \u03b8 as well. But for notational convenience, we drop the indices corresponding to \u03b8. The low dimensional subspace is therefore SV = {\u03a6v|v \u2208 R\u03ba2} where \u03c6 : X \u00d7 R \u2192 R\u03ba2 is a function mapping such that \u03a6(x, s) = \u03c6\u22a4(x, s). We also make the following standard assumption on the rank of matrix \u03c6. More information relating to the feature mappings and function approximation \u03c6 can be found in Appendix. Let v\u2217 \u2208 R\u03ba2 be the best approximation parameter vector. Then V\u0303 \u2217(x, s) = (v\u2217)\u22a4\u03c6(x, s) is the best linear approximation of V \u03b8(x, s).\nOur goal is to estimate v\u2217 from simulated trajectories of the MDP. Thus, it is reasonable to consider the projections from R onto SV with respect to a norm that is weighted according to the occupation measure d\u03b8\u03b3(x\n\u2032, s\u2032|x, s), where (x0, s0) = (x, s) is the initial condition of the augmented MDP. For a function y : X \u00d7 R \u2192 R, we introduce the weighted norm: \u2016y\u2016d = \u221a\u2211 x,s d(x\n\u2032, s\u2032|x, s)(y(x\u2032, s\u2032))2 where d is the occupation measure (with non-negative elements). We also denote by \u03a0 the projection from X \u00d7R to SV . We are now ready to describe the approximation scheme. Consider the following projected fixed point equation\nV (x, s) = \u03a0T\u03b8[V ](x, s)\nwhere T\u03b8 is the Bellman operator with respect to policy \u03b8 and let V\u0303 \u2217 denote the solution of the above equation. We will show the existence of this unique fixed point by the following contraction property of the projected Bellman operator: \u03a0T\u03b8.\nLemma 16 There exists \u03ba \u2208 (0, 1) such that\n\u2016\u03a0T\u03b8[V1]\u2212\u03a0T\u03b8[V2]\u2016d \u2264 \u03ba\u2016V1 \u2212 V2\u2016d.\nProof. Note that the projection operator \u03a0 is non-expansive:\n\u2016\u03a0T\u03b8[V1]\u2212\u03a0T\u03b8[V2]\u20162d \u2264 \u2016T\u03b8[V1]\u2212 T\u03b8[V2]\u20162d.\nOne further obtains the following expression:\n\u2016T\u03b8 [V1]\u2212 T\u03b8[V2]\u20162d\n= \u2211\nx,s\nd(x, s|x, s)\n  \u2211\ny,a,s\u2032\n\u03b3\u00b5(a|x, s; \u03b8)P\u0304 (y, s\u2032|x, s, a)(V1(y, s\u2032)\u2212 V2(y, s\u2032))\n  2\n\u2264 \u2211\nx,s\nd(x, s|x, s)\n  \u2211\ny,a,s\u2032\n\u03b32\u00b5(a|x, s; \u03b8)P\u0304 (y, s\u2032|x, s, a)(V1(y, s\u2032)\u2212 V2(y, s\u2032))2  \n= \u2211\ny,s\u2032\n( d(y, s\u2032|x, s)\u2212 (1\u2212 \u03b3)1{x = y, s = s\u2032} ) \u03b3(V1(y, s \u2032)\u2212 V2(y, s\u2032))2\n\u2264\u03b3\u2016V1 \u2212 V2\u20162d.\nThe first inequality is due to the fact that \u00b5(a|x, s; \u03b8), P\u0304 (y, s\u2032|x, s, a) \u2208 [0, 1] and convexity of quadratic function, the second equality is based on the property of \u03b3\u2212visiting distribution. Thus, we have just shown that \u03a0T\u03b8 is contractive with \u03ba = \u221a \u03b3 \u2208 (0, 1).\nTherefore, by Banach fixed point theorem, a unique fixed point solution exists for equation: \u03a0T\u03b8[V ](x, s) = V (x, s) for any x \u2208 X , s \u2208 R. Denote by V\u0303 \u2217 the fixed point solution and v\u2217 be the corresponding weight, which is unique by the full rank assumption. From Lemma 16, one obtains a unique value function estimates from the following projected Bellman equation:\n\u03a0T\u03b8[V\u0303 \u2217](x, s) = V\u0303 \u2217(x, s), V\u0303 \u2217(x, s, a) = (v\u2217)\u22a4\u03c6(x, s). (79)\nAlso we have the following error bound of the value function approximation.\nLemma 17 Let V \u2217 be the fixed point solution of T\u03b8[V ](x, s) = V (x, s) and v\u2217 be the unique solution for \u03a0T\u03b8[\u03a6v](x, s) = \u03c6\u22a4(x, s)v. Then, for some \u03ba \u2208 (0, 1),\n\u2016V \u2217 \u2212 V\u0303 \u2217\u2016d = \u2016V \u2217 \u2212 \u03a6v\u2217\u2016d \u2264 1\u221a 1\u2212 \u03b3 \u2016V \u2217 \u2212\u03a0V \u2217\u2016d.\nProof. Note that by the Pythagorean theorem of projection,\n\u2016V \u2217 \u2212 \u03a6v\u2217\u20162d = \u2016V \u2217 \u2212 \u03a0V \u2217\u20162d + \u2016\u03a0V \u2217 \u2212\u03a6v\u2217\u20162d = \u2016V \u2217 \u2212 \u03a0V \u2217\u20162d + \u2016\u03a0T\u03b8[V \u2217]\u2212 \u03a0T\u03b8[\u03a6v\u2217]\u20162d \u2264 \u2016V \u2217 \u2212 \u03a0V \u2217\u20162d + \u03ba2\u2016V \u2217 \u2212 \u03a6v\u2217\u20162d\nTherefore, by recalling \u03ba = \u221a \u03b3, the proof is completed by rearranging the above inequality. This implies that if V \u2217 \u2208 SV , V \u2217(x, s) = V\u0303 \u2217(x, s) for any (x, s) \u2208 X \u00d7 R.\nNote that we can re-write the projected Bellman equation in explicit form as follows:\n\u03a0T\u03b8[\u03a6v \u2217] = \u03a6v\u2217\n\u21d0\u21d2 \u03a0\n     \u2211\na\u2208A\n\u00b5(a|x, s; \u03b8)  C\u0304(x, s, a) + \u03b3 \u2211\ny,s\u2032\nP\u0304 (y, s\u2032|x, s, a)(v\u2217)\u22a4\u03c6 ( y, s\u2032 )     \nx\u2208X ,s\u2208R\n  = \u03a6v\u2217.\nBy the definition of projection, the unique solution v\u2217 \u2208 R\u2113 satisfies v\u2217 \u2208 argmin\nv \u2016T\u03b8[\u03a6v]\u2212 \u03a6v\u20162d\u03b8\u03b3\n\u21d0\u21d2 v\u2217 \u2208 argmin v\n\u2211\ny,s\u2032\nd\u03b8\u03b3(y, s \u2032|x, s)\u00b7\n  \u2211\na\u2032\u2208A\n\u00b5(a\u2032|y, s\u2032; \u03b8)  C\u0304(y, s\u2032, a\u2032) + \u03b3 \u2211\nz,s\u2032\u2032\nP\u0304 (z, s\u2032\u2032|y, s\u2032, a\u2032)\u03c6\u22a4 ( z, s\u2032\u2032 ) vds\u2032\u2032  \u2212 \u03c6\u22a4(y, s\u2032)v   2 .\nBy the projection theorem on Hilbert space, the orthogonality condition for v\u2217 becomes:\n\u2211\ny,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)\u03c6(y, s\u2032)(v\u2217)\u22a4\u03c6(y, s\u2032)\n= \u2211\ny,a\u2032,s\u2032\n{ \u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)\u03c6(y, s\u2032)C\u0304(y, s\u2032, a\u2032) + \u03b3 \u2211\nz,s\u2032\u2032\n\u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)P\u0304 (z, s\u2032\u2032|y, s\u2032, a\u2032)\u03c6(y, s\u2032)\u03c6\u22a4 ( z, s\u2032\u2032 )} v\u2217.\nThis condition can be written as Av\u2217 = b where\nA = \u2211\ny,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)\u03c6(y, s\u2032)\n \u03c6\u22a4(y, s\u2032)\u2212 \u03b3 \u2211\nz,s\u2032\u2032\nP\u0304 (z, s\u2032\u2032|y, s\u2032, a)\u03c6\u22a4 (z, s\u2032\u2032) ds\u2032\u2032  \n(80) is a finite dimensional matrix in R\u03ba2\u00d7\u03ba2 and\nb = \u2211\ny,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)\u03c6(y, s\u2032)C\u0304(y, s\u2032, a\u2032). (81)\nis a finite dimensional vector in R\u03ba2 . The matrix A is invertible since Lemma 16 guarantees that (79) has a unique solution v\u2217. Note that the projected equation Av = b can be re-written as v = v \u2212 \u03be(Av \u2212 b) for any positive scaler \u03be \u2265 0. Specifically, since\nAv\u2212b = \u2211\ny,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(y, s \u2032, a\u2032|x, s)\u03c6(y, s\u2032)\n v\u22a4\u03c6(y, s\u2032)\u2212 \u2211\nz,s\u2032\u2032\nP\u0304 (z, s\u2032\u2032|y, s\u2032, a\u2032)(\u03b3v\u22a4\u03c6 (z, s\u2032\u2032) + C\u0304(y, s\u2032, a\u2032))   ,\none obtains\nAv \u2212 b = E\u03c0\u03b8\u03b3 [ \u03c6(xk, sk) ( v\u22a4\u03c6(xk, sk)\u2212 \u03b3v\u22a4\u03c6 (xk+1, sk+1)\u2212 C\u0304(xk, sk, ak) )]\nwhere the occupation measure \u03c0\u03b8\u03b3(x, s, a|x0, \u03bd) is a valid probability measure. Recall from the definitions of (A, b) that\nA =E\u03c0 \u03b8 \u03b3 [ \u03c6(xk, sk) ( \u03c6\u22a4(xk, sk)\u2212 \u03b3\u03c6\u22a4 (xk+1, sk+1) )] ,\nb =E\u03c0 \u03b8 \u03b3 [ \u03c6(xk, sk)C\u0304(xk, sk, ak) ]\nwhere E\u03c0 \u03b8 \u03b3 is the expectation induced by the occupation measure (which is a valid probability measure)."}, {"heading": "E Supplementary: Gradient with Respect to \u03b8", "text": "By taking gradient of V \u03b8 with respect to \u03b8, one obtains\n\u2207\u03b8V \u03b8(x0, \u03bd) = \u2211\na\n\u2207\u03b8\u00b5(a|x0, \u03bd; \u03b8)Q\u03b8(x0, \u03bd, a) + \u00b5(a|x0, \u03bd; \u03b8)\u2207\u03b8Q\u03b8(x0, \u03bd, a)\n= \u2211\na\n\u2207\u03b8\u00b5(a|x0, \u03bd; \u03b8)Q\u03b8(x0, \u03bd, a) + \u00b5(a|x0, \u03bd; \u03b8)\u2207\u03b8  C\u0304(x0, \u03bd, a) + \u2211\nx\u2032,s\u2032\n\u03b3P\u0304 (x\u2032, s\u2032|x0, \u03bd, a)V \u03b8 ( x\u2032, s\u2032 )  \n= \u2211\na\n\u2207\u03b8\u00b5(a|x0, \u03bd; \u03b8)Q\u03b8(x0, \u03bd, a) + \u03b3\u00b5(a|x0, \u03bd; \u03b8)\n  \u2211\nx1,s1\n\u03b3P\u0304 (x1, s1|x0, \u03bd, a)\u2207\u03b8V \u03b8 ( x1, s1\n) \n\n=h\u03b8(x0, \u03bd) + \u03b3 \u2211\nx1,s1,a0\n\u00b5(a0|x0, \u03bd; \u03b8)P\u0304 (x1, s1|x0, \u03bd, a0)\u2207\u03b8V \u03b8 ( x1, s1 )\nwhere h\u03b8(x0, \u03bd) = \u2211\na\n\u2207\u03b8\u00b5(a|x0, \u03bd; \u03b8)Q\u03b8(x0, \u03bd, a).\nSince the above expression is a recursion, one further obtains \u2207\u03b8V \u03b8(x0, \u03bd) =h\u03b8(x0, \u03bd) + \u03b3 \u2211\na,x1,s1\n\u00b5(a|x0, \u03bd; \u03b8)P\u0304 (x1, s1|x0, \u03bd, a)\n h\u03b8(x1, s1) + \u03b3 \u2211\na1,x2,s2\n\u00b5(a1|x1, s1; \u03b8)P\u0304 (x2, s2|x1, s1, a1)\u2207\u03b8V \u03b8 ( x2, s2\n) \n .\nBy the definition of occupation measures, the above expression becomes\n\u2207\u03b8V \u03b8(x0, \u03bd) = \u221e\u2211\nk=0\n\u03b3k \u2211\nx\u2032,a\u2032,s\u2032\n\u00b5(a\u2032|x\u2032, s\u2032; \u03b8)P\u0304 (xk = x\u2032, sk = s\u2032|x0 = x0, s0 = \u03bd)h\u03b8(x\u2032, s\u2032)\n= 1 1\u2212 \u03b3 \u2211\nx\u2032,s\u2032\nd\u03b8\u03b3(x \u2032, s\u2032|x0 = x0, s0 = \u03bd)h\u03b8(x\u2032, s\u2032)\n= 1 1\u2212 \u03b3 \u2211\nx\u2032,s\u2032\nd\u03b8\u03b3(x \u2032, s\u2032|x0 = x0, s0 = \u03bd)\n\u2211\na\u2032\u2208A\n\u2207\u03b8\u00b5(a\u2032|x\u2032, s\u2032; \u03b8)Q\u03b8(x\u2032, s\u2032, a\u2032)\n= 1 1\u2212 \u03b3 \u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(x \u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd)\u2207\u03b8 log\u00b5(a\u2032|x\u2032, s\u2032; \u03b8)Q\u03b8(x\u2032, s\u2032, a\u2032)\n= 1 1\u2212 \u03b3 \u2211\nx\u2032,a\u2032,s\u2032\n\u03c0\u03b8\u03b3(x \u2032, s\u2032, a\u2032|x0 = x0, s0 = \u03bd)\u2207\u03b8 log\u00b5(a\u2032|x\u2032, s\u2032; \u03b8)A\u03b8(x\u2032, s\u2032, a\u2032)\n(82)\nwhere A\u03b8(x, s, a) = Q\u03b8(x, s, a)\u2212 V \u03b8(x, s)\nis the advantage function. The last equality is due to the fact that \u2211\na\n\u00b5(a|x, s; \u03b8)\u2207\u03b8 log\u00b5(x|s, a; \u03b8)V \u03b8(x, s) =V \u03b8(x, s) \u00b7 \u2211\na\n\u2207\u03b8\u00b5(a|x, s; \u03b8)\n=V \u03b8(x, s) \u00b7 \u2207\u03b8 \u2211\na\n\u00b5(a|x, s; \u03b8) = \u2207\u03b8(1) \u00b7 V \u03b8(x, s) = 0.\nThus, the gradient of the Lagrangian function is\n\u2207\u03b8L(\u03b8, \u03bd, \u03bb) = \u2207\u03b8V \u03b8(x, s) \u2223\u2223\u2223\u2223 x=x0,s=\u03bd ."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.", "creator": "LaTeX with hyperref package"}}}