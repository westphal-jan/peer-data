{"id": "1406.3852", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2014", "title": "A low variance consistent test of relative dependency", "abstract": "where we accurately describe either a novel non spectral parametric statistical hypothesis test of residual relative dependence between a source trigger variable and typically two candidate - target variables. computing such a test enables the one to effectively answer whether one source variable is significantly no more weight dependent thereby on knowing the first target targets variable or the second. partial dependence is measured via the hilbert - schmidt domain independence criterion ( quantum hsic ), jointly resulting in a sufficient pair of consecutive empirical criterion dependence measures ( source - matched target block 1, source - fix target 2 ). modeling the mutual covariance problem between these classical hsic individual statistics tests leads authors to providing a provably more easily powerful test than essentially the standard construction of independent inverse hsic statistics by sub linear sampling. the resulting test is visually consistent and unbiased, and ( usually being systematically based on quantum u - n statistics ) has favorable computational convergence properties. then the test can be naturally computed in quadratic trace time, matching through the cumulative computational complexity of existing standard empirical hsic estimators. we indeed demonstrate dramatically the effectiveness of the test on a simultaneous real - world linguistics problem of identifying suitable language groups from each a multilingual corpus.", "histories": [["v1", "Sun, 15 Jun 2014 19:23:11 GMT  (1079kb,D)", "https://arxiv.org/abs/1406.3852v1", null], ["v2", "Fri, 11 Jul 2014 17:12:58 GMT  (313kb,D)", "http://arxiv.org/abs/1406.3852v2", null], ["v3", "Wed, 27 May 2015 08:25:19 GMT  (978kb,D)", "http://arxiv.org/abs/1406.3852v3", "International Conference on Machine Learning, Jul 2015, Lille, France"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO", "authors": ["wacha bounliphone", "arthur gretton", "arthur tenenhaus", "matthew b blaschko"], "accepted": true, "id": "1406.3852"}, "pdf": {"name": "1406.3852.pdf", "metadata": {"source": "META", "title": "A low variance consistent test of relative dependency", "authors": ["Wacha Bounliphone", "Arthur Gretton", "Arthur Tenenhaus", "Matthew B. Blaschko"], "emails": ["WACHA.BOUNLIPHONE@CENTRALESUPELEC.FR", "ARTHUR.GRETTON@GMAIL.COM", "ARTHUR.TENENHAUS@CENTRALESUPELEC.FR", "MATTHEW.BLASCHKO@INRIA.FR"], "sections": [{"heading": null, "text": "Proceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s)."}, {"heading": "1. Introduction", "text": "Tests of dependence are important tools in statistical analysis, and are widely applied in many data analysis contexts. Classical criteria include Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4 , which can detect non-linear monotonic dependencies. More recent research on dependence measurement has focused on non-parametric measures of dependence, which apply even when the dependence is nonlinear, or the variables are multivariate or non-euclidean (for instance images, strings, and graphs). The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois & Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sze\u0301kely et al., 2007; Sejdinovic et al., 2013b), kernel regression tests (Cortes et al., 2009; Gunn & Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton & Gyorfi, 2010; Reshef et al., 2011; Kinney & Atwal, 2014). Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996).\nFor many problems in data analysis, however, the question of whether dependence exists is secondary: there may be multiple dependencies, and the question becomes which dependence is the strongest. For instance, in neuroscience, multiple stimuli may be present (e.g. visual and audio), and it is of interest to determine which of the two has a stronger influence on brain activity (Trommershauser et al., 2011). In automated translation (Peters et al., 2012), it is of interest to determine whether documents in a source language are a significantly better match to those in one target language than to another target language, either as a measure of difficulty of the respective learning tasks, or as a basic tool for comparative linguistics.\nar X\niv :1\n40 6.\n38 52\nv3 [\nst at\n.M L\n] 2\n7 M\nWe present a statistical test which determines whether two target variables have a significant difference in their dependence on a third, source variable. The dependence between each of the target variables and the source is computed using the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005; 2008).1 Care must be taken in analyzing the asymptotic behavior of the test statistics, since the two measures of dependence will themselves be correlated: they are both computed with respect to the same source. Thus, we derive the joint asymptotic distribution of both dependencies. The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 1981; Arcones & Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (1981) to determine the asymptotic joint distributions of the statistics (see Theorem 4). Consequently, we derive the lowest variance unbiased estimator of the test statistic.\nWe prove our approach to have greater statistical power than constructing two uncorrelated statistics on the same data by subsampling, and testing on these. In experiments, we are able to successfully test which of two variables is most strongly related to a third, in synthetic examples, in a language group identification task, and in a task for identifying the relative strength of factors for Glioma type in a pediatric patient population.\nTo our knowledge, there do not exist competing nonparametric tests to determine which of two dependencies is strongest. One related area is that of multiple regression analysis (e.g. (Sen & Srivastava, 2011)). In this case a linear model is assumed, and it is determined whether individual inputs have a statistically significant effect on an output variable. The procedure does not address the question of whether the influence of one variable is higher than that of another to a statistically significant degree. The problem of variable selection has also been investigated in the case of nonlinear relations between the inputs and outputs (Cortes et al., 2009; 2012; Song et al., 2012), however this again does not address which of two variables most strongly influences a third. A less closely related area is that of detecting three-variable interactions (Sejdinovic et al., 2013a), where it is determined whether there exists any factorization of the joint distribution over three variables. This test again does not address the issue of finding which connections are strongest, however.\n1Dependency can also be tested with the correlation operator. However, Fukumizu et al., (2007) show that unlike the covariance operator, the asymptotic distribution of the norm of the correlation operator is unknown, so the construction of a computationally efficient test of relative dependence remains an open problem."}, {"heading": "2. Definitions and description of HSIC", "text": "We base our underlying notion of dependence on the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005; 2008; Song et al., 2012). All results in this section except for Problem 1 can be found in these previous works.\nDefinition 1. (Gretton et al., 2005, Definition 1,Lemma 1: Hilbert-Schmidt Independence Criterion)\nLet Pxy be a Borel probability measure over over (X \u00d7 Y,\u0393\u00d7\u039b) with \u0393 and \u039b the respective Borel sets on X and Y , and Px and Py the marginal distributions on domains X and Y . Given separable RKHSs F and G, the HilbertSchmidt Independence Criterion (HSIC) is defined as the squared HS-norm of the associated cross-covariance operator Cxy . When the kernels k, l are associated uniquely withs respective RKHSs F and G and bounded, HSIC can be expressed in terms of expectations of kernel functions\nHSIC(F ,G, Pxy) := \u2016Cxy\u20162HS = Exx\u2032yy\u2032 [k(x, x\u2032)l(y, y\u2032)] + Exx\u2032 [k(x, x\u2032)]Eyy\u2032 [l(y, y\u2032)] \u2212 2Exy [Ex\u2032 [k(x, x\u2032)]Ey\u2032 [l(y, y\u2032)]] . (1)\nHSIC determines independence: HSIC = 0 iff Pxy = PxPy when kernels k and l are characteristic on their respective marginal domains (Gretton, 2015).\nWith this choice, the problem we would like to solve is described as follows:\nProblem 1. Given separable RKHSs F , G, and H with HSIC(F ,G, Pxy) > 0 and HSIC(F ,H, Pxz) > 0, we test the null hypothesis H0 : HSIC(F ,G, Pxy) \u2264 HSIC(F ,H, Pxz) versus the alternative hypothesis H1 : HSIC(F ,G, Pxy) > HSIC(F ,H, Pxz) at a given significance level \u03b1.\nWe now describe the asymptotic behavior of the HSIC for dependent variables.\nTheorem 1. (Song et al., 2012, Theorem 2: Unbiased estimator for HSIC(F ,G, Pxy)) We denote by S the set of observations {(x1, y1), ..., (xm, ym)} of size m drawn i.i.d. from Pxy . The unbiased estimator HSICm(F ,G,S) is given by\nHSICm(F ,G,S) = 1\nm(m\u2212 3) \u00d7 (2)[\nTr(K\u0303L\u0303) + 1\u2032K\u030311\u2032L\u03031\n(m\u2212 1)(m\u2212 2) \u2212 2 m\u2212 2 1\u2032K\u0303L\u03031\n]\nwhere K\u0303 and L\u0303 are related to K and L by K\u0303ij = (1 \u2212 \u03b4ij)K\u0303ij and L\u0303ij = (1\u2212 \u03b4ij)L\u0303ij . Theorem 2. (Song et al., 2012, Theorem 3: U-statistic of HSIC) This finite sample unbiased estimator of HSICXYm\ncan be written as a U-statistic,\nHSICXYm = (m) \u22121 4 \u2211 (i,j,q,r)\u2208im4 hijqr (3)\nwhere (m)4 := m!\n(m\u2212 4)! , the index set im4 denotes the\nset of all 4\u2212tuples drawn without replacement from the set {1, . . .m}, and the kernel h of the U-statistic is defined as\nhijqr = 1\n24 (i,j,q,r)\u2211 (s,t,u,v) kst(lst + luv \u2212 2lsu) (4)\nwhere the kernels k and l are associated uniquely with respective reproducing kernel Hilbert spaces F and G. Theorem 3. (Gretton et al., 2008, Theorem 1: Asymptotic distribution of HSICm) If E[h2] < \u221e, and source and targets are not independent, then, underH1, as m\u2192\u221e,\n\u221a m(HSICXYm \u2212HSIC(F ,G, Pxy))\nd\u2212\u2192 N (0, \u03c32XY ) (5)\nwhere \u03c32XY = 16 ( Ei (Ej,q,rhijqr)2 \u2212HSIC(F ,G, Pxy)) ) with Ej,q,r := ESj ,Sq,Sr . Its empirical estimate is \u03c3\u0302XY = 16 ( RXY \u2212 (HSICXYm )2 ) where\nRXY = 1\nm m\u2211 i=1 (m\u2212 1)\u221213 \u2211 (j,q,r)\u2208im3 \\{i} hijqr 2 and the index set im3 \\ {i} denotes the set of all 3\u2212tuples drawn without replacement from the set {1, . . .m} \\ {i}."}, {"heading": "3. A test of relative dependence", "text": "In this section we calculate two dependent HSIC statistics and derive the joint asymptotic distribution of these dependent quantities, which is used to construct a consistent test for Problem 1. We next construct a simpler consistent test, by computing two independent HSIC statistics on sample subsets. While the simpler strategy is superficially attractive and less effort to implement, we prove the dependent strategy is strictly more powerful."}, {"heading": "3.1. Joint asymptotic distribution of HSIC and test", "text": "In the present section, we compute each HSIC estimate on the full dataset, and explicitly obtain the correlations between the resulting empirical dependence measurements HSICXYm and HSIC XZ m . We denote by S1 = (X,Y, Z) the joint sample of observations which are drawn i.i.d. with respective Borel probability measure Pxyz defined on the domain X \u00d7 Y \u00d7 Z . The kernels k, l and d are associated uniquely with respective reproducing kernel Hilbert spaces F , G and H. Moreover, K, L and D \u2208 Rm\u00d7m are kernel matrices containing kij = k(xi, xj), lij = l(yi, yj) and\ndij = d(zi, zj). Let HSICXYm and HSIC XZ m be respectively the unbiased estimators of HSIC(F ,G, Pxy) and HSIC(F ,H, Pxz), written as a sum of U-statistics with respective kernels hijqr and gijqr as described in (4),\nhijqr = 1\n24 (i,j,q,r)\u2211 (s,t,u,v) kst(lst + luv \u2212 2lsu),\ngijqr = 1\n24 (i,j,q,r)\u2211 (s,t,u,v) kst(dst + duv \u2212 2dsu). (6)\nTheorem 4. (Joint asymptotic distribution of HSIC) If E[h2] <\u221e and E[g2] <\u221e, then\n\u221a m (( HSICXYm HSICXZm ) \u2212 ( HSIC(F ,G, Pxy) HSIC(F ,H, Pxz) )) d\u2212\u2192 N (( 0 0 ) , ( \u03c32XY \u03c3XYXZ\n\u03c3XYXZ \u03c3 2 XZ\n)) , (7)\nwhere \u03c32XY and \u03c3 2 XZ are as in Theorem 3. The empirical estimate of \u03c3XYXZ is \u03c3\u0302XYXZ = 16\nm\n( RXYXZ \u2212HSICXYm HSICXZm ) , where\nRXYXZ = 1\nm m\u2211 i=1 (m\u2212 1)\u221223 \u2211 (j,q,r)\u2208im3 \\{i} hijqrgijqr  . (8)\nProof. Eq. (8) is constructed with the definition of variance of a U-statistic as given by Serfling, Ch. 5 (1981), where one variable is fixed. Eq. (7) follows from the application of Hoeffding, Theorem 7.1 (1963), which gives the joint asymptotic distribution of U-statistics.\nBased on the joint asymptotic distribution of HSIC described in Theorem 4, we can now describe a statistical test to solve Problem 1: given a sample S1 as described in Section 3.1, T (S1) : {(X \u00d7 Y \u00d7 Z)m} \u2192 {0, 1} is used to test the null hypothesis H0 : HSIC(F ,G, Pxy) \u2264 HSIC(F ,H, Pxz) versus the alternative hypothesis H1 : HSIC(F ,G, Pxy) > HSIC(F ,H, Pxz) at a given significance level \u03b1. This is achieved by projecting the distribution to 1D using the statistic HSICXYm \u2212 HSICXZm , and determining where the statistic falls relative to a conservative estimate of the the 1 \u2212 \u03b1 quantile of the null. We now derive this conservative estimate. A simple way of achieving this is to rotate the distribution by \u03c0 4 counter-clockwise about the origin, and to integrate the resulting distribution projected onto the first axis (cf. Fig. 3). Denote the asymptotically normal distribution of\u221a m[HSICXYm HSIC XZ m ]\nT as N (\u00b5,\u03a3). The distribution resulting from rotation and projection is\nN ( [Q\u00b5]1, [Q\u03a3Q T ]11 ) , (9)\nwhere Q = \u221a 2\n2\n( 1 \u22121 1 1 ) is the rotation matrix by \u03c04 and\n[Q\u00b5]1 =\n\u221a 2\n2 (HSIC(F ,G, Pxy)\u2212HSIC(F ,H, Pxz)) ,\n(10)\n[Q\u03a3QT ]11 = 1\n2 (\u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ). (11)\nFollowing the empirical distribution from Eq. (9), a test with statistic HSICXYm \u2212HSICXZm has p-value\np \u2264 1\u2212\u03a6 ( (HSICXYm \u2212HSICXZm )\u221a \u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ ) , (12)\nwhere \u03a6 is the CDF of a standard normal distribution, and we have made the most conservative possible assumption that HSIC(F ,G, Pxy) \u2212 HSIC(F ,H, Pxz) = 0 under the null (the null also allows for the difference in population dependence measures to be negative).\nTo implement the test in practice, the variances of \u03c32XY , \u03c3 2 XZ and \u03c3 2 XYXZ may be replaced by their empirical estimates. The test will still be consistent for a large enough sample size, since the estimates will be sufficiently well converged to ensure the test is calibrated. Eq. (8) is expensive to compute na\u0131\u0308vely, because even computing the kernels hijqr and gijqr of the U -statistic itself is a non trivial task. Following (Song et al., 2012, Section 2.5), we first form a vector hXY with entries corresponding to\u2211\n(j,q,r)\u2208im3 \\{i} hijqr, and a vector hXZ with entries corresponding to \u2211\n(j,q,r)\u2208im3 \\{i} gijqr. Collecting terms in\nEq. (4) related to kernel matrices K\u0303 and L\u0303, hXY can be written as\nhXY = (m\u2212 2)2 ( K\u0303 L\u0303 ) 1\u2212m(K\u03031) (L\u03031) (13)\n+ (m\u2212 2) ( (Tr(K\u0303L\u0303))1\u2212 K\u0303(L\u03031)\u2212 L\u0303(K\u03031) )\n+ (1T L\u03031)K\u03031+ (1T K\u03031)L\u03031\u2212 ((1T K\u0303)(L\u03031))1\nwhere denotes the Hadamard product. Then RXYXZ in Eq. (8) can be computed as RXYXZ = (4m)\u22121(m \u2212 1)\u221223 hXY\nThXZ. Using the order of operations implied by the parentheses in Eq. (13), the computational cost of the cross covariance term is O(m2). Combining this with the unbiased estimator of HSIC in Eq. (2) leads to a final computational complexity of O(m2).\nIn addition to the asymptotic consistency result, we provide a finite sample bound on the deviation between the difference of two population HSIC statistics and the difference of two empirical HSIC estimates.\nTheorem 5 (Generalization bound on the difference of empirical HSIC statistics). Assume that k, l, and d are bounded almost everywhere by 1, and are non-negative.\nThen for m > 1 and all \u03b4 > 0 with probability at least 1 \u2212 \u03b4, for all pxyz , the generalization bound on the difference of empirical HSIC statistics is\n| {HSIC(F ,G, Pxy)\u2212HSIC(F ,H, Pxz)} \u2212 { HSICXYm \u2212HSICXZm } |\n\u2264 2\n{\u221a log(6/\u03b4)\n\u03b12m + C m\n} (14)\nwhere \u03b1 > 0.24 and C are constants.\nProof. In Gretton et al., (2005) a finite sample bound is given for a single HSIC statistic. Eq. (14) is proved by using a union bound.\nCorollary 1. HSICXYm \u2212HSICXZm converges to the population statistic at rate O( \u221a m)."}, {"heading": "3.2. A simple consistent test via uncorrelated HSICs", "text": "From the result in Eq. (5), a simple, consistent test of relative dependence can be constructed as follows: split the samples from Px into two equal sized sets denoted by X \u2032 and X \u2032\u2032, and drop the second half of the sample pairs with Y and the first half of the sample pairs with Z. We will denote the remaining samples as Y \u2032 and Z \u2032\u2032. We can now estimate the joint distribution of\u221a m[HSICX \u2032Y \u2032\nm/2 , HSIC X\u2032\u2032Z\u2032\u2032 m/2 ] T as\nN ((\nHSIC(F ,G, Pxy) HSIC(F ,H, Pxz)\n) , ( \u03c32X\u2032Y \u2032 0\n0 \u03c32X\u2032\u2032Z\u2032\u2032\n)) , (15)\nwhich we will write as N (\u00b5\u2032,\u03a3\u2032). Given this joint distribution, we need to determine the distribution over the half space defined by HSIC(F ,G, Pxy) < HSIC(F ,H, Pxz). As in the previous section, we achieve this by rotating the distribution by \u03c04 counter-clockwise about the origin, and integrating the resulting distribution projected onto the first axis (cf. Fig. 3). The resulting projection of the rotated distribution onto the primary axis is\nN ( [Q\u00b5\u2032]1 , [ Q\u03a3\u2032QT ] 11 ) (16)\nwhere\n[Q\u00b5\u2032]1 =\n\u221a 2\n2 (HSIC(F ,G, Pxy)\u2212HSIC(F ,H, Pxz)) ,\n(17)\n[Q\u03a3\u2032QT ]11 = 1\n2 (\u03c32X\u2032Y \u2032 + \u03c3 2 X\u2032\u2032Z\u2032\u2032). (18)\nFrom this empirically estimated distribution, it is straightforward to construct a consistent test (cf. Eq. (12)). The power of this test varies inversely with the variance of the distribution in Eq. (16)."}, {"heading": "3.3. The dependent test is more powerful", "text": "While discarding half the samples leads to a consistent test, we might expect some loss of power over the approach in Section 3.1, due to the increase in variance with lower sample size. In this section, we prove the Section 3.1 test is more powerful than that of Section 3.2, regardless of Pxy and Pxz .\nWe call the simple and consistent approach in Section 3.2, the independent approach, and the lower variance approach in Section 3.1, the dependent approach. The following theorem compares these approaches.\nTheorem 6. The asymptotic relative efficiency (ARE) of the independent approach relative to the dependent approach is always greater than 1.\nRemark 1. The asymptotic relative efficiency (ARE) is defined in e.g. Serfling (1981, Chap.5, Section 1.15.4). If mA andmB are the sample sizes at which tests \u201dperform equivalently\u201d (i.e. have equal power), then the ratio mAmB represents the relative efficiency. WhenmA andmB tend to +\u221e and the ratio mAmB \u2192 L (at equivalent performance), then the value L represents the asymptotic relative efficiency of procedure B relative to procedure A. This example is relevant to our case since we are comparing two test statistics with different asymptotically Normal distributions.\nThe following lemma is used for the proof of Theorem 6.\nLemma 1. (Lower Variance) The variance of the dependent test statistic is smaller than the variance of the independent test statistic.\nProof. From the convergence of moments in the application of the central limit theorem (von Bahr, 1965), we have that \u03c32X\u2032Y \u2032 = 2\u03c3 2 XY . Then the variance summary in Eq. (11) is 12 (\u03c3 2 XY +\u03c3 2 XZ \u2212 2\u03c3XYXZ) and the variance summary in Equation (18) is 12 (2\u03c3 2 XY + 2\u03c3 2 XZ) where in\nboth cases the statistic is scaled by \u221a m. We have that the variance of the independent test statistic is smaller than the variance of the dependent test statistic when\n1 2 (\u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ) < 1 2 (2\u03c32XY + 2\u03c3 2 XZ)\n\u21d0\u21d2 \u22122\u03c3XYXZ < \u03c32XY + \u03c32XZ (19)\nwhich is implied by the positive definiteness of \u03a3.\nProof of Theorem 6. The Type II error probability of the independent test at level \u03b1 is\n\u03a6 \u03a6\u22121(1\u2212 \u03b1)\u2212 m\u22121/2 ( HSIC(F ,G, Pxy) \u2212HSIC(F ,H, Pxz) )\u221a\n\u03c32X\u2032Y \u2032 + \u03c3 2 X\u2032\u2032Z\u2032\u2032\n , (20)\nwhere we again make the most conservative possible assumption that HSIC(F ,G, Pxy) \u2212HSIC(F ,H, Pxz) = 0 under the null. The Type II error probability of the dependent test at level \u03b1 is\n\u03a6 \u03a6\u22121(1\u2212 \u03b1)\u2212 m\u22121/2 ( HSIC(F ,G, Pxy) \u2212HSIC(F ,H, Pxz) )\u221a\n\u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ  (21) where \u03a6 is the CDF of the standard normal distribution. The numerator in Eq. (20) is the same as the numerator in Eq. (21), and the denominator in Eq. (21) is smaller due to Lemma 1. The lower variance dependent test therefore has higher ARE, i.e., for a sufficient sample size m > \u03c4 for some distribution dependent \u03c4 \u2208 N+, the dependent test will be more powerful than the independent test."}, {"heading": "4. Generalizing to more than two HSIC statistics", "text": "The generalization of the dependence test to more than three random variables follows from the earlier derivation by applying successive rotations to a higher dimensional joint Gaussian distribution over multiple HSIC statistics. We assume a sample S of size m over n domains with kernels k1, . . . , kn associated uniquely with respective reproducing kernel Hilbert spaces F1, . . . ,Fn. We define a generalized statistical test, Tg(S) \u2192 {0, 1} to test the null hypothesis H0 : \u2211 (x,y)\u2208{1,...,n}2 v(x,y)HSIC(Fx,Fy, Pxy) \u2264\n0 versus the alternative hypothesis Hm :\u2211 (x,y)\u2208{1,...,n}2 v(x,y)HSIC(Fx,Fy, Pxy) > 0, where v is a vector of weights on each HSIC statistic. We may recover the test in the previous section by setting v(1,2) = +1 v(1,3) = \u22121 and v(i,j) = 0 for all (i, j) \u2208 {1, 2, 3}2 \\ {(1, 2), (1, 3)}.\nThe derivation of the test follows the general strategy used in the previous section: we construct a rotation matrix so as to project the joint Gaussian distribution onto the first axis, and read the p-value from a standard normal table. To construct the rotation matrix, we simply need to rotate v such that it is aligned with the first axis. Such a rotation can be computed by composing n 2-dimensional rotation matrices as in Algorithm 1."}, {"heading": "5. Experiments", "text": "We apply our estimates of statistical dependence to three challenging problems. The first is a synthetic data experiment, in which we can directly control the relative degree of functional dependence between variates. The second experiment uses a multilingual corpus to determine the relative relations between European languages. The last exper-\nAlgorithm 1 Successive rotation for generalized highdimensional relative tests of dependency (cf. Section 4) Require: v \u2208 Rn Ensure: [Qv]i = 0 \u2200i 6= 1, QTQ = I Q = I for i = 2 to n do Qi = I; \u03b8 = \u2212 tan\u22121 vi[Qv]1 [Qi]11 = cos(\u03b8); [Qi]1i = \u2212 sin(\u03b8) [Qi]i1 = sin(\u03b8); [Qi]ii = cos(\u03b8) Q = QiQ\niment is a 3-block dataset which combines gene expression, comparative genomic hybridization, and a qualitative phenotype measured on a sample of Glioma patients."}, {"heading": "5.1. Synthetic experiment", "text": "We constructed 3 distributions as defined in Eq. (22) and illustrated in Figure 1.\nLet t \u223c U [(0, 2\u03c0)], (22) (a) x1 \u223c t+ \u03b31N (0, 1) y1 \u223c sin(t) + \u03b31N (0, 1) (b) x2 \u223c t cos(t) + \u03b32N (0, 1) y2 \u223c t sin(t) + \u03b32N (0, 1) (c) x3 \u223c t cos(t) + \u03b33N (0, 1) y3 \u223c t sin(t) + \u03b33N (0, 1)\nThese distributions are specified so that we can control the relative degree of functional dependence between the variates by varying the relative size of noise scaling parameters \u03b31, \u03b32 and \u03b33. The question is then whether the dependence between (a) and (b) is larger than the dependence between (a) and (c). In these experiments, we fixed \u03b31 = \u03b32 = 0.3, while we varied \u03b33, and used a Gaussian kernel with bandwidth \u03c3 selected as the median pairwise distance between data points. This kernel is sufficient to obtain good performance, although others choices exist (Gretton et al., 2012).\nFigure 2 shows the power of the dependent and the independent tests as we vary \u03b33. It is clear from these results that the dependent test is far more powerful than the independent test over the great majority of \u03b33 values considered. Figure 3 demonstrates that this superior test power arises due to the tighter and more concentrated distribution of the dependent statistic."}, {"heading": "5.2. Multilingual data", "text": "In this section, we demonstrate dependence testing to predict the relative similarity of different languages. We use a real world dataset taken from the parallel European Parliament corpus (Koehn, 2005). We choose 3000 random documents in common written in: Finnish (fi), Italian (it), French (fr), Spanish (es), Portuguese (pt), English (en), Dutch (nl), German (de), Danish (da) and Swedish (sv). These languages can be broadly categorized into either the Romance, Germanic or Uralic groups (Gray & Atkinson, 2003). In this dataset, we considered each language as a random variable and each document as an observation.\nOur first goal is to test if the statistical dependence between two languages in the same group is greater than the statistical dependence between languages in different groups. For pre-processing, we removed stop-words (http:// www.nltk.org) and performed stemming (http:// snowball.tartarus.org). We applied the TF-IDF model as a feature representation and used a Gaussian kernel with the bandwidth \u03c3 set per language as the median pairwise distance between documents.\nIn Table 1, a selection of tests between language groups (Germanic, Romance, and Uralic) is given: all p-values strongly support that our relative dependence test finds the different language groups with very high significance.\nFurther, if we focus on the Romance family, our test enables one to answer more fine-grained questions about the relative similarity of languages within the same group. As before, we determine the ground truth similarities from the topology of the tree of European languages determined by the linguistics community (Gray & Atkinson, 2003; Bouckaert et al., 2012) as illustrated in Fig. 4 for the Romance\ngroup. We have run the test on all triplets from the corpus for which the topology of the tree specifies a correct ordering of the dependencies. In a fraction of a second (excluding kernel computation), we are able to recover certain features of the subtree of relationships between languages present in the Romance language group (Table 2). The test always indicates the correct relative similarity of languages when nearby languages (pt,es) are compared with those further away (ft,it), however errors are made when comparing triplets of languages for which the nearest common ancestor is more than one link removed.\nIn our next tests, we evaluate our more general framework for testing relative dependencies with more than two HSIC statistics. We chose four languages, and tested whether the average dependence between languages in the same group is higher than the dependence between groups. The results of these tests are in Table 3. As before, our test is able to distinguish language groups with high significance."}, {"heading": "5.3. Pediatric glioma data", "text": "Brain tumors are the most common solid tumors in children and have the highest mortality rate of all pediatric cancers. Despite advances in multimodality therapy, children with pediatric high-grade gliomas (pHGG) invariably have an overall survival of around 20% at 5 years. Depending on their location (e.g. brainstem, central nuclei, or supratentorial), pHGG present different characteristics in terms of radiological appearance, histology, and prognosis. The hypothesis is that pHGG have different genetic origins and oncogenic pathways depending on their location. Thus, the biological processes involved in the development of the tumor may be different from one location to another.\nIn order to evaluate such hypotheses, pre-treatment frozen tumor samples were obtained from 53 children with newly diagnosed pHGG from Necker Enfants Malades (Paris, France) from Puget et al, (2012). The 53 tumors are divided into 3 locations: supratentorial (HEMI), central nuclei (MIDL), and brain stem (DIPG). The final dataset is organized in 3 blocks of variables defined for the 53 tumors: X is a block of indicator variables describing the location category, the second data matrix Y provides the expression of 15 702 genes (GE). The third data matrix Z contains the imbalances of 1229 segments (CGH) of chromosomes.\nFor X, we use a linear kernel, which is characteristic for indicator variables, and for Y and Z, the kernel was chosen to be the Gaussian kernel with \u03c3 selected as the median of pairwise distances. The p-value of our relative dependency test is < 10\u22125. This shows that the tumor location in the brain is more dependent on gene expression than on chromosomal imbalances. By contrast with Section 5.1, the independent test was also able to find the same order-\ning of dependence, but with a p-value that is three orders of magnitude larger (p = 0.005). Figure 5 shows iso-curves of the Gaussian distributions estimated in the independent and dependent tests. The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson & Gutmann, 2007; Palm et al., 2009; Puget et al., 2012)."}, {"heading": "6. Conclusions", "text": "We have described a novel statistical test that determines whether a source random variable is more strongly dependent on one target random variable or another. This test, built on the Hilbert-Schmidt Independence Criterion, is low variance, consistent, and unbiased. We have shown that our test is strictly more powerful than a test that does not exploit the covariance between HSIC statistics, and empirically achieves p-values several orders of magnitude smaller. We have empirically demonstrated the test performance on synthetic data, where the degree of dependence could be controlled; on the challenging problem of identifying language groups from a multilingual corpus; and for finding the most important determinant of Glioma type. The computation and memory requirements of the test are quadratic in the sample size, matching the performance of HSIC and related tests for dependence between two random variables. The test is therefore scalable to the wide range of problem instances where non-parametric dependency tests are currently applied. We have generalized the test framework to more than two HSIC statistics, and have given an algorithm to construct a consistent, low-variance, unbiased test in this setting."}, {"heading": "Acknowledgements", "text": "We thank Ioannis Antonoglou for helpful discussions. The first author is supported by a fellowship from Centrale-\nSupe\u0301lec. This work is partially funded by the European Commission through ERC Grant 259112 and FP7MCCIG334380."}], "references": [{"title": "Limit theorems for Uprocesses", "author": ["M.A. Arcones", "E. Gine"], "venue": "The Annals of Probability,", "citeRegEx": "Arcones and Gine,? \\Q1993\\E", "shortCiteRegEx": "Arcones and Gine", "year": 1993}, {"title": "Mapping the origins and expansion of the Indo-European language family", "author": ["R. Bouckaert", "P. Lemey", "M. Dunn", "S.J. Greenhill", "A.V. Alekseyenko", "A.J. Drummond", "R.D. Gray", "M.A. Suchard", "Q.D. Atkinson"], "venue": null, "citeRegEx": "Bouckaert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouckaert et al\\.", "year": 2012}, {"title": "A geometric approach to compare variables in a regression model", "author": ["J. Bring"], "venue": "The American Statistician,", "citeRegEx": "Bring,? \\Q1996\\E", "shortCiteRegEx": "Bring", "year": 1996}, {"title": "Learning non-linear combinations of kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cortes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2012}, {"title": "Multiple regression in psychological research and practice", "author": ["Darlington", "Richard B"], "venue": "Psychological bulletin,", "citeRegEx": "Darlington and B.,? \\Q1968\\E", "shortCiteRegEx": "Darlington and B.", "year": 1968}, {"title": "Nonlinear canonical analysis and independence tests", "author": ["J. Dauxois", "G.M. Nkiet"], "venue": "Annals of Statistics,", "citeRegEx": "Dauxois and Nkiet,? \\Q1998\\E", "shortCiteRegEx": "Dauxois and Nkiet", "year": 1998}, {"title": "Statistical consistency of kernel canonical correlation analysis", "author": ["K. Fukumizu", "F.R. Bach", "A. Gretton"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2007}, {"title": "Kernel measures of conditional dependence", "author": ["K. Fukumizu", "A. Gretton", "X. Sun", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fukumizu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2008}, {"title": "Tumorigenesis in the brain: location, location, location", "author": ["R.J. Gilbertson", "D.H. Gutmann"], "venue": "Cancer research,", "citeRegEx": "Gilbertson and Gutmann,? \\Q2007\\E", "shortCiteRegEx": "Gilbertson and Gutmann", "year": 2007}, {"title": "Language-tree divergence times support the Anatolian theory of Indo-European origin", "author": ["R.D. Gray", "Q.D. Atkinson"], "venue": "Nature, 426(6965):435\u2013439,", "citeRegEx": "Gray and Atkinson,? \\Q2003\\E", "shortCiteRegEx": "Gray and Atkinson", "year": 2003}, {"title": "A simpler condition for consistency of a kernel independence test", "author": ["A. Gretton"], "venue": null, "citeRegEx": "Gretton,? \\Q2015\\E", "shortCiteRegEx": "Gretton", "year": 2015}, {"title": "Consistent nonparametric tests of independence", "author": ["A. Gretton", "L. Gyorfi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton and Gyorfi,? \\Q2010\\E", "shortCiteRegEx": "Gretton and Gyorfi", "year": 2010}, {"title": "Measuring statistical dependence with Hilbert-Schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic Learning Theory, pp", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "Teo", "C.-H", "L. Song", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu", "B.K. Sriperumbudur"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Structural modelling with sparse kernels", "author": ["S.R. Gunn", "J.S. Kandola"], "venue": "Machine Learning,", "citeRegEx": "Gunn and Kandola,? \\Q2002\\E", "shortCiteRegEx": "Gunn and Kandola", "year": 2002}, {"title": "A consistent multivariate test of association based on ranks of distances", "author": ["R. Heller", "Y. Heller", "M. Gorfine"], "venue": null, "citeRegEx": "Heller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heller et al\\.", "year": 2013}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Equitability, mutual information, and the maximal information coefficient", "author": ["J.B. Kinney", "G.S. Atwal"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kinney and Atwal,? \\Q2014\\E", "shortCiteRegEx": "Kinney and Atwal", "year": 2014}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn,? \\Q2005\\E", "shortCiteRegEx": "Koehn", "year": 2005}, {"title": "Multilingual Information Retrieval: From Research to Practice", "author": ["C. Peters", "M. Braschler", "P. Clough"], "venue": null, "citeRegEx": "Peters et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2012}, {"title": "Detecting novel associations in large", "author": ["D. Reshef", "Y. Reshef", "H. Finucane", "S. Grossman", "G. McVean", "P. Turnbaugh", "E. Lander", "M. Mitzenmacher", "P. Sabeti"], "venue": null, "citeRegEx": "Reshef et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reshef et al\\.", "year": 2011}, {"title": "A kernel test for three-variable interactions", "author": ["D. Sejdinovic", "A. Gretton", "W. Bergsma"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "Equivalence of distance-based and RKHSbased statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "Annals of Statistics,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "Regression Analysis \u2013 Theory, Methods, and Applications", "author": ["A. Sen", "M. Srivastava"], "venue": null, "citeRegEx": "Sen and Srivastava,? \\Q2011\\E", "shortCiteRegEx": "Sen and Srivastava", "year": 2011}, {"title": "Approximation theorems of mathematical statistics", "author": ["R.J. Serfling"], "venue": "Wiley Series in Probability and Statistics. Wiley,", "citeRegEx": "Serfling,? \\Q1981\\E", "shortCiteRegEx": "Serfling", "year": 1981}, {"title": "Feature selection via dependence maximization", "author": ["L. Song", "A. Smola", "A. Gretton", "J. Bedo", "K. Borgwardt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Song et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G. Sz\u00e9kely", "M. Rizzo", "N. Bakirov"], "venue": "Annals of Statistics,", "citeRegEx": "Sz\u00e9kely et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sz\u00e9kely et al\\.", "year": 2007}, {"title": "Sensory Cue Integration", "author": ["J. Trommershauser", "K. Kording", "M.S. Landy"], "venue": null, "citeRegEx": "Trommershauser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Trommershauser et al\\.", "year": 2011}, {"title": "On the convergence of moments in the central limit theorem", "author": ["von Bahr", "Bengt"], "venue": "The Annals of Mathematical Statistics, 36(3):808\u2013818,", "citeRegEx": "Bahr and Bengt.,? \\Q1965\\E", "shortCiteRegEx": "Bahr and Bengt.", "year": 1965}, {"title": "Kernel-based conditional independence test and application in causal discovery", "author": ["K. Zhang", "J. Peters", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "In 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois & Nkiet, 1998; Fukumizu et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 31, "context": "The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois & Nkiet, 1998; Fukumizu et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 8, "context": ", 2011) and correlation (Dauxois & Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al.", "startOffset": 24, "endOffset": 70}, {"referenceID": 28, "context": ", 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013b), kernel regression tests (Cortes et al.", "startOffset": 68, "endOffset": 116}, {"referenceID": 3, "context": ", 2013b), kernel regression tests (Cortes et al., 2009; Gunn & Kandola, 2002), rankings (Heller et al.", "startOffset": 34, "endOffset": 77}, {"referenceID": 17, "context": ", 2009; Gunn & Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton & Gyorfi, 2010; Reshef et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 22, "context": ", 2013), and space partitioning approaches (Gretton & Gyorfi, 2010; Reshef et al., 2011; Kinney & Atwal, 2014).", "startOffset": 43, "endOffset": 110}, {"referenceID": 2, "context": ", 2013b), kernel regression tests (Cortes et al., 2009; Gunn & Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton & Gyorfi, 2010; Reshef et al., 2011; Kinney & Atwal, 2014). Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996).", "startOffset": 35, "endOffset": 351}, {"referenceID": 2, "context": "Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996).", "startOffset": 138, "endOffset": 151}, {"referenceID": 29, "context": "visual and audio), and it is of interest to determine which of the two has a stronger influence on brain activity (Trommershauser et al., 2011).", "startOffset": 114, "endOffset": 143}, {"referenceID": 21, "context": "In automated translation (Peters et al., 2012), it is of interest to determine whether documents in a source language are a significantly better match to those in one target language than to another target language, either as a measure of difficulty of the respective learning tasks, or as a basic tool for comparative linguistics.", "startOffset": 25, "endOffset": 46}, {"referenceID": 13, "context": "The dependence between each of the target variables and the source is computed using the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005; 2008).", "startOffset": 128, "endOffset": 156}, {"referenceID": 18, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 1981; Arcones & Gine, 1993).", "startOffset": 71, "endOffset": 126}, {"referenceID": 26, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 1981; Arcones & Gine, 1993).", "startOffset": 71, "endOffset": 126}, {"referenceID": 11, "context": "The dependence between each of the target variables and the source is computed using the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005; 2008).1 Care must be taken in analyzing the asymptotic behavior of the test statistics, since the two measures of dependence will themselves be correlated: they are both computed with respect to the same source. Thus, we derive the joint asymptotic distribution of both dependencies. The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 1981; Arcones & Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (1981) to determine the asymptotic joint distributions of the statistics (see Theorem 4).", "startOffset": 129, "endOffset": 621}, {"referenceID": 11, "context": "The dependence between each of the target variables and the source is computed using the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005; 2008).1 Care must be taken in analyzing the asymptotic behavior of the test statistics, since the two measures of dependence will themselves be correlated: they are both computed with respect to the same source. Thus, we derive the joint asymptotic distribution of both dependencies. The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 1981; Arcones & Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (1981) to determine the asymptotic joint distributions of the statistics (see Theorem 4).", "startOffset": 129, "endOffset": 641}, {"referenceID": 3, "context": "The problem of variable selection has also been investigated in the case of nonlinear relations between the inputs and outputs (Cortes et al., 2009; 2012; Song et al., 2012), however this again does not address which of two variables most strongly influences a third.", "startOffset": 127, "endOffset": 173}, {"referenceID": 27, "context": "The problem of variable selection has also been investigated in the case of nonlinear relations between the inputs and outputs (Cortes et al., 2009; 2012; Song et al., 2012), however this again does not address which of two variables most strongly influences a third.", "startOffset": 127, "endOffset": 173}, {"referenceID": 7, "context": "However, Fukumizu et al., (2007) show that unlike the covariance operator, the asymptotic distribution of the norm of the correlation operator is unknown, so the construction of a computationally efficient test of relative dependence remains an open problem.", "startOffset": 9, "endOffset": 33}, {"referenceID": 13, "context": "We base our underlying notion of dependence on the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005; 2008; Song et al., 2012).", "startOffset": 90, "endOffset": 137}, {"referenceID": 27, "context": "We base our underlying notion of dependence on the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005; 2008; Song et al., 2012).", "startOffset": 90, "endOffset": 137}, {"referenceID": 11, "context": "HSIC determines independence: HSIC = 0 iff Pxy = PxPy when kernels k and l are characteristic on their respective marginal domains (Gretton, 2015).", "startOffset": 131, "endOffset": 146}, {"referenceID": 25, "context": "(8) is constructed with the definition of variance of a U-statistic as given by Serfling, Ch. 5 (1981), where one variable is fixed.", "startOffset": 80, "endOffset": 103}, {"referenceID": 18, "context": "(7) follows from the application of Hoeffding, Theorem 7.1 (1963), which gives the joint asymptotic distribution of U-statistics.", "startOffset": 36, "endOffset": 66}, {"referenceID": 11, "context": "In Gretton et al., (2005) a finite sample bound is given for a single HSIC statistic.", "startOffset": 3, "endOffset": 26}, {"referenceID": 15, "context": "This kernel is sufficient to obtain good performance, although others choices exist (Gretton et al., 2012).", "startOffset": 84, "endOffset": 106}, {"referenceID": 20, "context": "We use a real world dataset taken from the parallel European Parliament corpus (Koehn, 2005).", "startOffset": 79, "endOffset": 92}, {"referenceID": 1, "context": "As before, we determine the ground truth similarities from the topology of the tree of European languages determined by the linguistics community (Gray & Atkinson, 2003; Bouckaert et al., 2012) as illustrated in Fig.", "startOffset": 146, "endOffset": 193}], "year": 2015, "abstractText": "We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the HilbertSchmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by subsampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github. com/wbounliphone/reldep. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}