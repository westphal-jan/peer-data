{"id": "1406.4447", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2014", "title": "Automatic Fado Music Classification", "abstract": "in late period 2011, online fado data was elevated to recognised the 5th oral and intangible artefacts heritage of archaeological humanity by unesco. this study aims successfully to develop a tool prototype for rapid automatic detection of new fado music encoding based analysis on the audio signal. before to do this, special frequency spectrum - sounding related characteristics were captured form the digital audio measurement signal : in addition added to the mel tuned frequency specific cepstral coefficients ( mfccs ) input and the energy of monitoring the signal, additionally the signal feedback was further analysed in connecting two frequency ranges, providing additional transmission information. tests objectives were run both in a modified 10 - fold fold cross - validation setup ( nominally 97. 6 % accuracy ), succeed and result in rejecting a traditional quality train / factory test setup ( \u2264 95. 8 % accuracy ). here the generally good results might reflect merely the fact that fado software is a really very distinctive musical measurement style.", "histories": [["v1", "Tue, 17 Jun 2014 17:44:45 GMT  (183kb,D)", "http://arxiv.org/abs/1406.4447v1", "4 pages, 1 figure, 5 tables"]], "COMMENTS": "4 pages, 1 figure, 5 tables", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["pedro gir\\~ao antunes", "david martins de matos", "ricardo ribeiro", "isabel trancoso"], "accepted": false, "id": "1406.4447"}, "pdf": {"name": "1406.4447.pdf", "metadata": {"source": "CRF", "title": "Automatic Fado Music Classification", "authors": ["Pedro Gir\u00e3o Antunes", "David Martins de Matos", "Ricardo Ribeiro", "Isabel Trancoso"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nFado music was born in the popular contexts of the 1800s Lisbon, Portugal. It incorporates both music and poetry. It was originally introduced among the poorest social groups of the city and it was often performed on the streets by amateurs (figure 1). It later became an actual profession, and Fado performers \u2013 Fadistas \u2013 now can be found in specialized Casas de Fado (\u201chouses of Fado\u201d), typical Portuguese places dedicated to Fado shows.\nFado songs are typically performed by a solo singer, male or female, traditionally accompanied by a classical guitar and the Portuguese guitar \u2013 a pear-shaped cittern with twelve wire strings, unique to Portugal. In the past few decades, the instrumental accompaniment was expanded to two Portuguese guitars, a classical guitar, and an acoustic bass guitar. The limited instrumental setup helps to better define Fado from a signal analysis point of view.\nFig. 1. Fado being performed on the streets.\nThis article studies how to automatically determine if a song is Fado or not. It is organized as follows: we start by reviewing relevant related work, both in terms of feature extraction and audio-based music genre classification (section II); our Fado classifier is introduced in section III; the experimental setups,\nas well as the used dataset, are presented in section IV; section V presents some insights about the classification results; and, finally, section VI presents the conclusions."}, {"heading": "II. RELATED WORK", "text": "The music genre tags are often generated by individual users, this is the cheapest way to do so but can lead to contradictions since people do not have the exact same perception of a given genre. Songs can be manually tagged by experts, however this represents an expensive task, mainly when dealing with big collections of music. Finally, the automatic genre tagging from an acoustic analysis takes the best from the above mentioned methods, as the tagging is done in a consistent way, but it is cheaper to run on large music collections. The problem with this approach is how well a machine can be set to determine the genre of a track. McKay and Fujinaga [9] introduce arguments for and against automatic genre classification. In the next subsection some of the most important works in this area are presented."}, {"heading": "A. Automatic Audio-based Music Genre Classification", "text": "Tzanetakis and Cook [17] were among the first to approach the problem of automatic classification of musical genre based on the audio signal. In their work they introduced a dataset called GTZAN also used by Li [8] and Panagakis [11]. Tzanetakis and Cook used features to represent the timbral content, rhythmic content and pitch content and statistical pattern recognition classifier.\nLi et al. [8] introduced a new feature extraction method for music genre classification. The DWCHs (Daubechies Wavelet Coefficient Histograms) to capture the local and global information of music signals simultaneously by computing histograms on their Daubechies wavelet coefficient. Support Vector Machines and Linear Discriminant Analysis where used to compare the effectiveness of DWCHs with previously used features. It was shown that it improves the accuracy of music genre classification.\nAhrendt et al. [1] used a multivariate autoregressive model of the first 6 MFCCs and a generalized linear model classifier.\nBergstra et al. [4] used a variety of timbral related features, including FFT coefficients, MFCCs, zero-crossing rate, among others. Each feature was computed and m consecutive frames were grouped in segments. Each segment was then independently classified using AdaBoost. The song is classified according to the \u201cmost voted\u201d label of its segments.\nAnnesi et al. [2], in addition to the timbral, rhythmic, and pitch features, introduced a new one, which they called Volume Reverse. It consists simply of subtracting the audio wave of a\nar X\niv :1\n40 6.\n44 47\nv1 [\ncs .S\nD ]\n1 7\nJu n\n20 14\n2 channel to the other one and compute the absolute value. This feature was built upon the claim that each musical genre is recorded differently and that is reflected on the balancing of stereo channels.\nSilla Jr. et al. [14] introduced musical genre classification using multiple feature vectors, from the beginning, middle, and end of the song, and a set binary classifiers that were merged to obtain the final genre label.\nPanagakis used Auditory Temporal Modulation and a Sparse Representation-based classifier and obtained the best results on the datasets used so far. However their results were recently contested by Sturm and Noorzad [15].\nSalamon et al. [12] presented a classifier based on highlevel melodic features that are extracted directly from the audio signal of polyphonic music.\nAryafar and Shokoufandeh [3] used Explicit Semantic Analysis of textual documents to represent audio samples to feed an support vector machine (SVM) and a k-nearest neighbor (kNN) clustering classifier.\nTable I provides information about the used datasets and features, and the classification results."}, {"heading": "B. Evaluation Concerns", "text": "Termens [16] notes that the final accuracy varies inversely with the number of genres used, according to the equation:\n% of accuracy = A\u00d7 r#genres (1)\nIn this equation, A = 93.36 and r = 0.9618, according to the regression method used. The data presented in table I is consistent with equation 1. This fact raises the question about the validity of music genre classification, that is, how defined in terms of individual perception is genre tagging? For some users, a song is considered to be Pop, while for others it can be Rock, Pop-Rock, or any other variety of similar tags. This is the cause of the inverse relation between accuracy and number of genres considered. Eventually, it would be the same for any manual human tagging [13]. Of course, there is also some argument about what is and what is not music. However, due to the traditional origin and limited musical instrument in the specific genre, Fado music is much more well defined then, for instance, Rock.\nTwo works present genre classification in a one against all setup [8], [3]. These works will be useful later, when discussing our results."}, {"heading": "III. FADO CLASSIFIER", "text": "Our Fado classifier was implemented using two main tools: MIRtoolbox [7] for Matlaba\u030and libsvm [5]. The first one was used to extract the audio features and the second to classify songs. Before introducing the audio features, it must be noted that the audio files used were songs in the WAV format, downsampled to 22050Hz, normalized according to the root mean square energy (RMS), and that each feature was computed for a 10-second excerpt of each song. The excerpt size is discussed in the next section."}, {"heading": "A. Audio Features", "text": "We extracted three features: one related to the rhythmic information, one that is related to the timbre, and another related to the musical dynamics.\nThe rhythmic feature was based on the work of Mitri, Uitdenbogerd and Ciesielski [10]. This feature is composed of two 9-dimensional vectors: one referring to low frequencies and another to the high frequencies. The first one is computed based on the FFT coefficients on the 20Hz to 100Hz frequency range, the other is on 8000Hz to 11025Hz. Each set of FFT coefficients was extracted from 50-ms frames, half overlapping. Considering this, each component of the 9-dimensional vector is described in table II.\nThe purpose of this feature is to capture the rhythmic information present in both, the low frequencies and the high frequencies. The fact is that Fado in general does not present much information in the low range frequencies, for example, it does not have electric bass or drum kick. Moreover, it is certainly richer in terms of high range frequencies range, mostly because of the Portuguese guitar. Then, this feature can present a good way to distinguish Fado music from other musical genres.\nThe timbre feature is the well known and extensively used MFCCs. We computed the MFCCs based on the FFT coefficients from the whole spectrum. Tests were made in order to use only the mid-range frequencies (from 200Hz to 8000Hz). However, using only this range of frequencies represents a loss of information. The MFCCs were extracted from 50-ms frames, half overlapping, and then the average was computed for each of the 13 components of the vector.\nThe dynamic feature was simply the computation of the RMS energy of the 10-second audio signal before the normalization.\n3 These three features are then concatenated into a 32- dimensional vector, where the first component is the RMS energy, then the two rhythmic features, first the low range one then the high range, and finally the MFCCs."}, {"heading": "B. Support Vector Machines", "text": "SVMs (Support Vector Machines) are a useful technique for data classification, extensively used in the bibliography [8], [2], [3]. Basically, SVMs aim at searching for a hyperplane that separates the positive data points and the negative data points with maximum margin.\nSVMs try to map the original training data into a higher (maybe infinite) dimensional space by a function \u03c6. For that, SVMs create a linear separating hyperplane with the maximal margin in this higher dimensional space. From the mathematical point of view, given a training set of instance label pairs (xi, yi), i = 1...l, where xi \u2208 Rn and y \u2208 1, 1l, SVMs search the solution of the following optimization problem:\nminw,b,\u03be 1\n2 wTw + C\n\u2211 i = 1l\u03bei\nsubject to: yi(wT\u03c6(xi) + b) \u2265 1\u2212 \u03bei; \u03be > 0 (2)\nHere training vectors xi are mapped into the higher dimensional space by the function \u03c6. C > 0 is the penalty parameter of the error term. Furthermore, K(xi, xj) = \u03c6(xi)T\u03c6(xj) is called the kernel function. There are four basic kernel functions: linear, polynomial, radial basis function and sigmoid but, for the music classification problems, the Polynomial and Radial Basis Function (RBF) are the most commonly used. In our case, the RBF was used as kernel:\nK(xi, xj) = exp(\u03b3 \u2016 xixj \u20162)\u03b3 > 0 (3)\nIn equation 3, \u03b3 is a kernel parameter. The implementation of SVM used was the libsvm, which is an integrated software for support vector classification, regression and distribution estimation [5]."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we introduce the experiments made, as well as the dataset used. One of the posed questions was \u201chow long would the sample need to be to characterise the musical genre?\u201d. In general, a value from 10 to 30 seconds is used. In practice, we observed that an average person was able to recognize the musical genre, in most of the cases, in less then 10 seconds. Thus, we used 10-second music samples."}, {"heading": "A. Dataset", "text": "The goal is to classify Fado music specifically. Then, it is about applying a simple binary classifier, where one class is Fado and the other is every other musical genre (i.e., not Fado). However, it is practically impossible to include every specific single genre and their subgenres in a set of songs. Even Fado itself can be divided into several different subgenres [6]. In this case, it makes sense to use as non-Fado, the largest number of different other genres. We used ten: Classical, Rock, Jazz, Pop, Folk, Medieval, Blues, Country, Tribal, and Electronic.\nThen, the Fado dataset used is a local set composed by 500 songs: 250 Fado songs and 250 non-Fado songs.\nWhen we consider 10-second samples, a question poses itself: where should we take it from? We experimented with four hypotheses: from the beginning of the track, from the end, from the middle, or the 10-second segment with the maximum RMS moment.\nThe first three do not imply extra computation, thus are a simple way of taking the audio samples. However, the fourth one does require some extra computation to determine the maximum RMS moment. In order to do so, the whole song has to be analysed, meaning that the RMS is computed frame by frame (50-ms frames, half overlapping). The chosen 10s are centered in the maximum RMS frame.\nIn musical terms, the beginning and end of a song, in general, represent specific and characteristic musical events of the genre in question. For example, a Fado song generally begins with an instrumental introduction, and it ends with an instrumental arrangement. Taking a frame from the middle of the song is an attempt to capture a song in its \u201csteady state\u201d, that is, with all the instruments and vocals present.\nThe maximum RMS is an attempt to capture a sample of the song where a relevant event occurs. That would be a refrain entrance, a dynamic change, making use of intensity, and so on. Since Fado tends to be consistent in terms of intensity, the idea is to capture that difference regarding other musical genres."}, {"heading": "V. RESULTS AND DISCUSSION", "text": "The results, using a 10-fold cross validation setup, are shown in table III. The results show that the maximum RMS and the beginning of the song are the best samples to distinguish Fado from other musical genres using these features. This suggests that Fado indeed has a characteristic beginning and that it is relatively more uniform then other musical genres. However, the middle sample also gets a high result.\nTo analyse song-specific results, the dataset was randomly divided in two sets, implementing a traditional train/test setup: approximately 2/3 to train (334 songs) and 1/3 to test (166 songs). The training was done using the parameters C (eq. 2) and \u03b3 (eq. 3), estimated using a grid algorithm from libsvm [5]. The results are shown in table IV.\n4 Analysing the false negatives for the maximum RMS sample, which was the one getting the best score, there are 3 misclassified songs (table V) as non-Fado. From these 3, two can actually be considered as non-Fado. They were included among a Fado compilation, yet they can be classified as popular Portuguese music for dancing. These two songs include drum beat and electric bass, which are not commonly featured in Fado music. The third song is a typical Fado song. However, in that particular song, the Portuguese guitar sounds different, almost as an electric guitar, what is probably enough to confuse the classifier.\nAmong the false positives, there are 4 misclassified songs (table V). These 4 songs all have many things in common. They are medieval music and include two features that make them quite similar sounding to Fado: a string instrument that sounds a lot like a Portuguese guitar and the vocalist\u2019s singing voice is very similar to that of Fado singers."}, {"heading": "VI. CONCLUSION", "text": "We introduced an audio-based Fado classifier using an SVM classifier [5]. We used three sets of features: one to model rhythm, one to model timbre (MFCCs), and one related to music dynamics (RMS). The results are comparable to those presented in [8], [3], although it is quite difficult to make an accurate comparison, because of the different datasets used, and different features and musical genres in question. However, since the accuracy results are above 90% (in general) and above 95% (in our best setups, even considering crossvalidation), we think they can be considered state of the art for this kind of task. We introduced comparative study on sample capturing and a detailed analysis on false negatives and false positives.\nThis work can be seen as a first step to Fado classification, one possible direction would be to classify different subgenres of Fado music, which would find application in musicological studies. Another possible benefit of this work would be to use this classifier to expand other music classifiers to include Fado classification."}, {"heading": "Acknowledgments", "text": "A special thank to Daniel Gouveia, for providing his text on Fado Music and a large collection of Fado audio tracks."}], "references": [{"title": "Music Genre Classification using the multivariate AR feature integration model. MIREX genre classification contest", "author": ["Peter Ahrendt", "Anders Meng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Audio feature engineering for automatic music genre classification", "author": ["Paolo Annesi", "Roberto Basili", "Raffaele Gitto"], "venue": "Conference RIAO2007, Pittsburgh PA, U.S.A.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Music genre classification using explicit semantic analysis. Proceedings of the 1st international ACM workshop on Music information retrieval with user-centered and multimodal strategies - MIRUM", "author": ["Kamelia Aryafar", "Ali Shokoufandeh"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Aggregate features and ADABOOST for music classification", "author": ["James Bergstra", "Norman Casagrande", "Dumitru Erhan", "Douglas Eck", "Bal\u00e1zs K\u00e9gl"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "MIRtoolbox 1.4 User\u2019s Manual", "author": ["Olivier Lartillot"], "venue": "Finnish Centre of Excelence in Interdisciplinary Music Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A comparative study on contentbased music genre classification", "author": ["Tao Li", "Mitsunori Ogihara", "Qi Li"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval - SIGIR \u201903,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Musical genre classification: Is it worth pursuing and how can it be improved", "author": ["Cory Mckay", "Ichiro Fujinaga"], "venue": "Proc. of the 7th Int. Conf. on Music Information Retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Automatic music classification problems", "author": ["G Mitri", "AL Uitdenbogerd", "V Ciesielski"], "venue": "Proceedings of the 27th,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Music genre classification via sparse representations of auditory temporal modulations", "author": ["Yannis Panagakis", "Constantine Kotropoulos", "GR Arce"], "venue": "Proc. XVII European Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Musical Genre Classification Using Melody Features Extracted from Polyphonic Music Signals", "author": ["J Salamon", "B Rocha", "E G\u00f3mez"], "venue": "mtg.upf.es,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Automatic genre classification of music content: a survey", "author": ["Nicolas Scaringella", "Giorgio Zoia", "Daniel Mlynek"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "A machine learning approach to automatic music genre classification", "author": ["Carlos N. Silla Jr.", "Alessandro L. Koerich", "Celso a. a. Kaestner"], "venue": "Journal of the Brazilian Computer Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "On Automatic Music Genre Recognition by Sparse Representation Classification using Auditory Temporal Modulations", "author": ["Bob L Sturm", "Pardis Noorzad"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Audio content processing for automatic music genre classification: descriptors, databases, and classifiers", "author": ["EG Termens"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "McKay and Fujinaga [9] introduce arguments for and against automatic genre classification.", "startOffset": 19, "endOffset": 22}, {"referenceID": 15, "context": "Tzanetakis and Cook [17] were among the first to approach the problem of automatic classification of musical genre based on the audio signal.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "In their work they introduced a dataset called GTZAN also used by Li [8] and Panagakis [11].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "In their work they introduced a dataset called GTZAN also used by Li [8] and Panagakis [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "[8] introduced a new feature extraction method for music genre classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] used a multivariate autoregressive model of the first 6 MFCCs and a generalized linear model classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] used a variety of timbral related features, including FFT coefficients, MFCCs, zero-crossing rate, among others.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], in addition to the timbral, rhythmic, and pitch features, introduced a new one, which they called Volume Reverse.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] introduced musical genre classification using multiple feature vectors, from the beginning, middle, and", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "However their results were recently contested by Sturm and Noorzad [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "[12] presented a classifier based on highlevel melodic features that are extracted directly from the audio signal of polyphonic music.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Aryafar and Shokoufandeh [3] used Explicit Semantic Analysis of textual documents to represent audio samples to feed an support vector machine (SVM) and a k-nearest neighbor (kNN) clustering classifier.", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 185, "endOffset": 188}, {"referenceID": 12, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 229, "endOffset": 233}, {"referenceID": 9, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 266, "endOffset": 270}, {"referenceID": 10, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 311, "endOffset": 315}, {"referenceID": 2, "context": "Work Dataset Size Genres Results [17] GTZAN 1000 10 61 [8] GTZAN, Local 1000, 756 10, 5 79, 74 [1] Uspop, Magnatune 1414, 1515 6, 10 78, 61 [4] Uspop, Magnatune 1414, 1515 6, 10 75, 87 [2] Magnatune, Local 1515, 500 10, 5 82, 92 [14] Latin Music Database 3169 10 65 [11] GTZAN, ISMIR 04 1000, 1458 10, 6 91, 94 [12] Local 500 5 90 [3] Garageband 1886 9 59", "startOffset": 331, "endOffset": 334}, {"referenceID": 14, "context": "Termens [16] notes that the final accuracy varies inversely with the number of genres used, according to the equation:", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Eventually, it would be the same for any manual human tagging [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "Two works present genre classification in a one against all setup [8], [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Two works present genre classification in a one against all setup [8], [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "Our Fado classifier was implemented using two main tools: MIRtoolbox [7] for Matlab\u00e5nd libsvm [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "Our Fado classifier was implemented using two main tools: MIRtoolbox [7] for Matlab\u00e5nd libsvm [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "The rhythmic feature was based on the work of Mitri, Uitdenbogerd and Ciesielski [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "SVMs (Support Vector Machines) are a useful technique for data classification, extensively used in the bibliography [8], [2], [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "SVMs (Support Vector Machines) are a useful technique for data classification, extensively used in the bibliography [8], [2], [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "SVMs (Support Vector Machines) are a useful technique for data classification, extensively used in the bibliography [8], [2], [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "The implementation of SVM used was the libsvm, which is an integrated software for support vector classification, regression and distribution estimation [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We introduced an audio-based Fado classifier using an SVM classifier [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "The results are comparable to those presented in [8], [3], although it is quite difficult to make an accurate comparison, because of the different datasets used, and different features and musical genres in question.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "The results are comparable to those presented in [8], [3], although it is quite difficult to make an accurate comparison, because of the different datasets used, and different features and musical genres in question.", "startOffset": 54, "endOffset": 57}], "year": 2014, "abstractText": "In late 2011, Fado was elevated to the oral and intangible heritage of humanity by UNESCO. This study aims to develop a tool for automatic detection of Fado music based on the audio signal. To do this, frequency spectrum-related characteristics were captured form the audio signal: in addition to the Mel Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the signal was further analysed in two frequency ranges, providing additional information. Tests were run both in a 10-fold cross-validation setup (97.6% accuracy), and in a traditional train/test setup (95.8% accuracy). The good results reflect the fact that Fado is a very distinctive musical style.", "creator": "LaTeX with hyperref package"}}}