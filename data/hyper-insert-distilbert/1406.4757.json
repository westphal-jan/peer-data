{"id": "1406.4757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "An Experimental Evaluation of Nearest Neighbour Time Series Classification", "abstract": "data mapping mining collaborative research into time distributed series classification ( tsc ) has focussed on alternative distance measures for nearest friend neighbour classifiers. it therefore is standard empirical practice to use statistics 1 - nn with euclidean curve or dynamic time warping ( inverse dtw ) standardized distance averages as follows a straw man for comparison. offered as likely part of a collaborative wider initial investigation guide into elastic distance analysis measures for achieving tsc ~ \\ int cite { lines14elastic }, we first perform perform a series table of experiments primarily to thoroughly test whether this standard recommended practice design is valid.", "histories": [["v1", "Wed, 18 Jun 2014 15:09:21 GMT  (117kb,D)", "http://arxiv.org/abs/1406.4757v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anthony bagnall", "jason lines"], "accepted": false, "id": "1406.4757"}, "pdf": {"name": "1406.4757.pdf", "metadata": {"source": "CRF", "title": "Technical Report CMP-C14-01: An Experimental Evaluation of Nearest Neighbour Time Series Classification", "authors": ["Anthony Bagnall"], "emails": ["ajb@uea.ac.uk", "jason.lines@uea.ac.uk"], "sections": [{"heading": null, "text": "Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to standard classifiers, examine whether the performance of 1-NN Euclidean approaches that of 1-NN DTW as the number of cases increases, assess whether there is any benefit of setting k for k-NN through cross validation whether it is worth setting the warping path for DTW through cross validation and finally is it better to use a window or weighting for DTW. Based on experiments on 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to beat but 1-NN with DTW is not, if window size is set through cross validation.\nI. INTRODUCTION\nTime-series classification (TSC) problems involve training a classifier on a set of cases, where each case contains an ordered set of real valued attributes and a class label. Time-series classification problems arise in a wide range of fields including, but not limited to, data mining, statistics, machine learning, signal processing, environmental sciences, computational biology, image processing and chemometrics. A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).\nIn [9], Bagnall et al. argue that the easiest way to gain improvement in accuracy on TSC problems is to transform into an alternative data space where the discriminatory features are more easily detected. They constructed classifiers on data in the time, frequency, autocorrelation and principal component domains and combined predictions through alternative ensemble schemes. The main conclusion in [9] is that for problems where the discriminatory features are based on similarity in change and similarity in shape, operating in a different data space produces a better performance improvement than designing a more complex classifier for the time domain. However, the issue of what is the best technique in a single data domain is not addressed in [9]. Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12]. The general\nconsensus amongst data mining researchers is that \u201csimple nearest neighbor classification is very difficult to beat\u201d [7]. For problems with few training cases, an elastic distance measure such as dynamic time warping (DTW) or longest common subsequence (LCSS) is often superior to Euclidean distance, but as the number of series increases \u201cthe accuracy of elastic measures converge with that of Euclidean distance\u201d [3].\nOur objective is to empirically test various aspects of these commonly made assertions. We examine whether one nearest neighbour (1-NN) is in fact not significantly worse than other classifiers and whether setting the parameter k for nearest neighbour through cross validation on the training data improves performance. For DTW, the key parameter is the warping window size. This dictates the largest allowable displacement between two points in the warping path. We evaluate whether setting the warping window through cross validation makes the classifier more accurate. Several alternative forms of DTW have been proposed in the literature. A version of DTW that weights against large warpings (WDTW) is described in [8]. The weighting scheme can be used in conjunction with dynamic time warping and an alternative version based on first order differences (DDTW), described in [13]. We extend the experiments described in [8] to test whether their conclusions hold over a large number of data sets with parameter optimisations for all of the algorithms considered. All datasets and code to reproduce experiments and results are available online [14].\nTo summarise, we have conducted extensive experiments to answer the following questions:\n1) Is Euclidean/DTW nearest neighbour (1-NN) really better than other commonly used algorithms such as tree-based or probabilistic classifiers? 2) Does the accuracy of 1-NN Euclidean approach that of 1-NN DTW as training set size increases? 3) Is it better to use k nearest neighbours (k-NN), with k set through cross validation, rather than 1-NN? 4) Is it worthwhile setting the warping window for DTW through cross validation?\nWe have answered these questions through over three million experiments on 77 TSC problems. 43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].\nar X\niv :1\n40 6.\n47 57\nv1 [\ncs .L\nG ]\n1 8\nJu n\n20 14\nThe structure of this paper is as follows. In Section II we provide background into TSC and review related research on DTW. In Section III we detail the 77 data sets we used in experiments. In Section IV we present our results and in Section V we summarise our conclusions."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": ""}, {"heading": "A. Time Series Classification (TSC)", "text": "We define time series classification as the problem of building a classifier from a collection of labelled training time series. We limit our attention to problems where each time series has the same number of observations. We define a time series xi as a set of ordered observations\nxi =< xi1, . . . , xim >\nand an associated class label yi. The training set is a set of n labelled pairs\nD = {(x1, y1), . . . , (xn, yn)}.\nFor traditional classification problems, the order of the attributes is unimportant and the interaction between variables is considered independent of their relative positions. For time series data, the ordering of the variables is often crucial in finding the best discriminating features. There are three broad categories of TSC discriminating features which are described by three general approaches to measuring similarity between time series: similarity in shape, similarity in change and similarity in time.\nSimilarity in shape describes the scenario where class membership is characterised by a common shape but the discriminatory shape is phase independent. If the common shape involves the whole series, but is phase shifted between instances of the same class, then transformation into the frequency domain is often the best approach (for example, see [17]). If the common shape is local and embedded in confounding noise, then subsequence techniques such as Shapelets can be employed [5], [11].\nSimilarity in change refers to the situation where the relevant discriminatory features are related to the autocorrelation function of each series. The most common approach in this situation is to fit an ARMA model, then base similarity on differences in model parameters [4]. The common element to similarity in shape and change is that similarity between series is not measured in the time domain.\nHowever, the majority of the data mining research into TSC has concentrated in similarity in time. This can be quantified by measures such as Euclidean distance or correlation [10], [6]. Similarity in time is characterised by the situation where the series from each class are observations of an underlying common curve in the time dimension. Variation around this underlying common shape is caused by noise in observation, and also by possible noise in indexing which may cause a slight phase shift. A classic example of this type of similarity is the Cylinder-Bell-Funnel artificial data set, where there is noise around the underlying shape, but also noise in the index of where the underlying shape transitions [10].\nThe commonly used benchmark classification algorithm for problems with small phase shift is 1-NN with an elastic\nmeasure such as DTW or LCSS to allow for small shifts in the time axis. In a comprehensive study [3], DTW was found to be as least as good as other elastic measures based on edit distance, and constraining the warping window was found to speed up computation, \u201cwhile yielding the same or even better accuracy\u201d [3]. The experimentation in [3] addresses the issue of what distance measure to use and is the starting point for our research. We begin by testing their assumptions about classifier selection and parameter setting before investigating alternative variants of DTW and combination schemes for the classifiers."}, {"heading": "B. Classification Algorithms", "text": "The nearest neighbour classifier is a lazy classifier (i.e. requires no training) that classifies new cases by finding the closest case in the training set with a distance function, then using the class of the closest case as the predicted class for the new case. Given the focus on distance functions in time series data mining research, it is perhaps unsurprising that the majority of classification has used 1-NN. Whilst often highly effective, 1-NN is known to be susceptible to problems such as outliers in the training set and large numbers of redundant features. Outliers can be compensated for by using k nearest neighbours and a voting scheme. Redundant features may be dealt with by filtering or by employing one of the plethora of alternative classifiers. Filtering was found to be not effective in [9]. We compare nearest neighbour classifiers against C4.5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23]."}, {"heading": "C. Dynamic Time Warping", "text": "For similarity in shape, Dynamic Time Warping (DTW) is commonly used to mitigate against distortions in the time axis [24]. Suppose we want to measure the distance between two series, a = {a1, a2, . . . , am} and b = {b1, b2, . . . , bm}. Let M(a,b) be the m\u00d7m pointwise distance matrix between a and b, where Mi,j = (ai \u2212 bj)2.\nA warping path P =< (e1, f1), (e2, f2), . . . , (es, fs) > is a set of points (i.e. pairs of indexes) that define a traversal of matrix M . So, for example, the Euclidean distance dE(a,b) =\u2211m\ni=1(ai \u2212 bi)2 is the path along the diagonal of M , i.e. Pe =< (1, 1, (2, 2), . . . , (m,m) >.\nA valid warping path must satisfy the conditions (e1, f1) = (1, 1) and (es, fs) = (m,m) and that 0 \u2264 ei+1 \u2212 ei \u2264 1 and 0 \u2264 fi+1 \u2212 fi \u2264 1 for all i < m.\nThe DTW distance between series is the path through M that minimizes the total distance, subject to constraints on the amount of warping allowed. Let pi =Maei ,bfi be the distance between element at position ei of a and at position fi of b for the ith pair of points in a proposed warping path P . The distance for any path P is\nDP (a,b) = s\u2211 i=1 pi.\nIf P is the space of all possible paths, the DTW path P \u2217 is the path that has the minimum distance, i.e.\nP \u2217 = min P\u2208P (DP (a,b)),\nand hence the DTW distance between series is\nDP\u2217(a,b) = k\u2211 i=1 pi.\nThe optimal path P \u2217 can be found exactly through a dynamic programming formulation. This can be a time consuming operation, and it is common to put a restriction on the amount of warping allowed. This restriction is equivalent to putting a maximum allowable distance between any pairs of indexes in a proposed path. If the warping window, r, is the proportion of warping allowed, then the optimal path is constrained so that\n|ei \u2212 fi| \u2264 r \u00b7m \u2200(ei, fi) \u2208 P \u2217."}, {"heading": "D. Longest Common Subsequence", "text": "The Longest Common Subsequence Distance (LCSS) is based on the solution to the longest common subsequence problem in pattern matching. The typical problem is to find the longest subsequence that is common to two discrete series based on the edit distance. This approach can be extended to consider real-valued time series by using a distance threshold , which defines the maximum difference between a pair of values that is allowed for them to be considered a match [25]. LCSS finds the optimal alignment between two series by inserting gaps to find the greatest number of matching pairs. For example, consider the discrete case where we have two strings S = \u201cABCADACDAB\u201d and T = \u201cBCDADBCACB\u201d. If we simply observe the point-wise matches between the two sequences, we can extract matching pairs for the substring \u201cADCB\u201d as shown in Figure 1.\nABCADA--CDA-B\n-BC-DADBC-ACB\nHowever, using L SS we can find a longer matching subsequence by inserting spaces into the two strings. In this case, the elasticity of the measure means we find the subsequence \u201cBCDACAB\u201d, as depicted in Figure 2.\nABCADACDAB\nBCDADBCACB\nThe LCSS between two series a and b can be found using Algorithm 1.\nThe LCSS distance between a and b is\ndLCSS(a,b) = 1\u2212 LCSS(a,b)\nm .\nAlgorithm 1 LCSS (a,b) 1: Let L be an (m+1)\u00d7 (m+1) matrix initialised to zero. 2: for i\u2190 m to 1 do 3: for j \u2190 m to 1 do 4: Li,j \u2190 Li+1,j+1 5: if ai = bj then 6: Li,j \u2190 Li,j + 1 7: else if Li,j+1 > Li,j then 8: Li,j \u2190 Li,j+1 9: else if Li+1,j > Li,j then 10: Li,j \u2190 Li+1,j 11: return L1,1"}, {"heading": "E. Derivative Dynamic Time Warping", "text": "Keogh and Pazzani proposed a modification of DTW called Derivative Dynamic Time Warping (DDTW) [13] that first transforms the series into a series of first differences. Given a series a = {a1, a2, . . . , am}, the difference series is a\u2032 = {a\u20322, a\u20322, . . . , a\u2032m\u22121} where a\u2032i is defined as the average of the slopes between ai\u22121 and ai and ai and ai+1, i.e.\na\u2032i = (ai \u2212 ai\u22121) + (ai+1 \u2212 ai\u22121)/2\n2 ,\nfor 1 < i < m. DDTW is designed to mitigate against noise in the series that can adversely affect DTW."}, {"heading": "F. Weighted Dynamic Time Warping", "text": "A weighted form of DTW (WDTW) was proposed by Jeong et al. [8]. WDTW adds a multiplicative weight penalty based on the warping distance between points in the warping path. It favours reduced warping, and is a smooth alternative to the cutoff point approach of using a warping window. When creating the distance matrix M , a weight penalty w|i\u2212j| for a warping distance of |i\u2212 j| is applied, so that\nMi,j = w|i\u2212j|(ai \u2212 bj)2.\nA logistic weight function is proposed in [8], so that a warping of a places imposes a weighting of\nw(a) = wmax\n1 + e\u2212g\u00b7(a\u2212m/2) ,\nwhere wmax is an upper bound on the weight (set to 1), m is the series length and g is a parameter that controls the penalty level for large warpings. The larger g is, the greater the penalty for warping.\nJeong et al. compared WDTW to Euclidean distance, full window DTW and LCSS on 20 UCR data sets using a 1-NN classifier. Half of the test data was used as a validation set for setting the value of g and the other half was used to measure accuracy. They state their results demonstrate \u201cWDTW and WDDTW clearly outperform standard DTW, DDTW and LCSS measures.\u201d We test this assertion in Section IV."}, {"heading": "III. DATA SETS", "text": "We have collected 77 data sets, the names of which are shown in Table I. 43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time. Further information and the data sets we have permission to circulate are available from [14].\nWe have grouped the problems into categories to help aid interpretation. The group of Sensor readings forms the largest category with 31 data sets. If we had more data sets it would be sensible to split the sensor categories into subtypes, such as human sensors and spectrographs. However, at this point such sub-typing would lead to groups that are too small. Image outline classification is the second largest category, with 29 problem sets. Many of the image outline problems, such as BeetleFly, are not rotationally aligned, and the expectation would be that classifiers in the time domain will not necessarily perform well with these data. The group of 14 Motion problems contains data taken from motion capture devices attached to human subjects. The final category is simulated data sets."}, {"heading": "IV. RESULTS", "text": "We conducted our classification experiments with WEKA [26] source code adapted for time series classification, using the 77 data sets described in Section III.\nAll datasets are split into a training and testing set, and all parameter optimisation is conducted on the training set only. We use a train/test split for several reasons. Firstly, it is common practice to do so with the UCR datasets. Secondly, some of the data sets are designed so the train/test split removes bias. Combining to perform a cross validation would reintroduce the bias. And finally, it is not computationally feasible to cross validate everything. We ran over 3 million experiments on a 4148 core High Performance Cluster (with a theoretical peak performance of 65TFlops) over a period of a month. Adding another level of cross validation would have increased the time required, the number of experiments and the complexity of the code by an order of magnitude.\nFor hypothesis testing purposes, we are assuming that these data sets are a random sample from the set of all possible TSC problems. This is true in the sense that we have not collected these data with any agenda in mind. We are not attempting to find data sets more suitable for one particular algorithm. However, as Table I demonstrates, there is a bias in areas of application we are considering. There are, for example, no problems from econometrics or finance. Because of this, we also present results split by problem type where appropriate. All the results are available on an Excel spreadsheet from [14]."}, {"heading": "A. Are 1-NN Classifiers are hard to beat?", "text": "Our null hypothesis is that the average accuracy of 1-NN Euclidean and 1-NN DTW are the same as other classifiers in the time domain. If we can reject this hypothesis for the one sided alternative that the average accuracy is significantly worse than at least one other classifier, then we have evidence that the assertion \u201d1-NN Classifiers are hard to beat\u201d is incorrect.\nFigure 3 presents the critical difference diagram for nine different classifiers, all trained with the default Weka parameters. It shows that the bottom group of C4.5, Naive Bayes and the Bayesian Network are significantly worse that the other classifiers. It also shows that there is no significant difference between the top three classifiers (1-NN with DTW, quadratic support vector machine and rotation forest). The middle group of linear SVM, 1-NN Euclidean and Random Forest are tightly grouped, and are all significantly worse than the top performing classifier, rotation forest.\nClearly, there is no evidence to refute the hypothesis concerning 1-NN DTW. Figure 3 would suggest that 1-NN with Euclidean distance is significantly worse than the rotation forest. Paired tests of difference in average for 1-NN Euclidean vs SVMQ, Rotation Forest and 1-NN DTW all indicate that we should reject the null hypothesis that difference in means and medians is zero in each case. We used a paired T test for the mean and a Wilcoxon signed rank test for the median, full results are available from [14]. Our conclusion from these experiments is that, whilst it is true that 1-NN DTW is hard to beat, 1-NN with Euclidean distance is beaten by two off-theshelf classifiers constructed with no parameter optimisation.\nTable II shows the average ranks of the classifiers split by problem type. This indicates that the NN classifiers perform much better on the motion data sets than the image outline problems. The poor performance on outlines is caused in part by rotation in the image data sets.\nWe also found no evidence that 1-NN Euclidean converges to 1-NN DTW based on training size, although this is probably because of a lack of problems with large train set sizes. Figure 4 shows the plot of the difference in accuracy of 1- NN Euclidean and 1-NN DTW against the number of training cases.\nTABLE I. DATA SETS GROUPED BY PROBLEM TYPE. THE ACTUAL FILE NAMES ARE IN A STRING ARRAY IN CLASS TIMESERIESCLASSIFICATION.FILENAMES\nImage Outline Classification DistPhalanxAge DistPhalanxOutline DistPhalanxTW FaceAll FaceFour WordSynonyms MidPhalanxAge MidPhalanxOutline MidPhalanxTW OSULeaf Phalanges yoga ProxPhalanxAge ProxPhalanxOutline ProxPhalanxTW ShapesAll SwedishLeaf MedicalImages\nSymbols Adiac ArrowHead BeetleFly BirdChicken DiatomSize FacesUCR fiftywords fish HandOutlines Herring\nMotion Classification CricketX CricketY CricketZ UWaveX UWaveY UWaveZ\nUWaveAll GunPoint Haptics InlineSkate ToeSeg1 ToeSeg2 Sensor Reading Classification\nBeef Car Chlorine CinCECG Coffee Computers FordA FordB ItalyPower LargeKitchen Lighting2 Lighting7\nStarLightCurves Trace TwoLeadECG wafer RefrigerationDevices MoteStrain Earthquakes ECG200 ECGFiveDays ElectricDevices SonyRobot1 SonyRobot2\nOliveOil Plane ScreenType SmallKitchenAppliances MALLAT ECGThorax1 ECGThorax2\nSimulated Classification Problems ARSim CBF SyntheticControl ShapeletSim TwoPatterns\n0\n10\n20\n30\n0 200 400 600 800 1000 1200 1400 1600 1800 2000\n-30\n-20\n-10\nFig. 4. (Euclidean accuracy - DTW accuracy) plotted against the number of training cases and the least squares regression line. The slope of the regression line is not significantly different to zero.\nOur conclusion from these experiments is that 1-NN with Euclidean distance should no longer be used as a standard benchmark against which to compare new algorithms for time series classification. We would recommend that nearest neighbour DTW classifiers should be the new default for distance measure based classifiers, and that the results for SVMQ and rotation forest should also be reported. We address the exact nature of how to use DTW nearest neighbour classifiers in the remainder of this section.\nB. Is it worth setting k through cross validation?\nOne commonly used method of improving nearest neighbour classifiers is to set k through cross validation on the training data. This requires the calculation of the distance matrix for the training set, and so adds some time overhead. This is particularly time consuming when cross validating against window size for DTW, since every new window size may create a different distance matrix. We are using 7 variants of distance measure with nearest neighbour classifiers: Eulcidean (Euclid), DTW with full warping window (DTWR1), DTW with warping window set through cross validation (DTWRN), Derivative DTW with full and CV set warping windows\n(DDTWR1 and DDTWRN) and weighted DTW and DDTW (WDTW and WDDTW).\nTable III shows the summary of the improvement in test accuracy of finding k through cross validation rather than setting k = 1. The largest average improvement is with WDTW, which improved 6.67%. However, this was skewed by two data sets with very large improvements, and the non-parametric Wilcoxon signed rank test could not detect a significant difference. The algorithm that showed significant improvement through setting k was Derivative DTW. We conclude that if it is feasible to do so, then it is worthwhile setting k through cross validation, but not doing so is unlikely to have a significant effect on accuracy.\nC. Is it worth finding the DTW window size through cross validation?\nThe short answer to this question is yes. Figure 5 summarises the improvement in test accuracy over all 77 data sets. The mean improvement is 1.8% and the median is 0.3% (both significant at the 5% level)."}, {"heading": "D. Which is better for DTW, setting window size or setting weights?", "text": "The results published for the weighting algorithm proposed in [8] are summarised in the critical difference diagram in Figure 6.\nTheir claim that the weighting leads to clear improvement is not backed up by these results. Figure 6 indicates that although weighted versions have the highest rank, the result cannot be claimed to be significant. The only significant difference is between the weighted versions of DTW and Euclidean distance. This demonstrates the need for testing on a large number of data sets. Furthermore, the comparisons they make are biased. Firstly, they compare full window DTW to the weighting scheme, when it would be more appropriate to compare against DTW with window size set through cross validation. Secondly, they use half the testing set data to validate the weighting parameter, but do not allow the other classifiers access to this data set.\nThe average rank is presented, along with the groups within which there is no significant difference in rank. The average difference in accuracy between DTW and WDTW is just 0.02145, but the difference between DTWCV and WDTW is merely 0.0056. We claim that DTWCV vs WDTW is a fairer comparison because WDTW has had the parameter g set through cross validation.\nWe have implemented their weighting algorithm and evaluated it on the 77 data sets described in Section III. In our experiments all parameter optimisation is conducted on the training set through cross validation. Figure 7 shows that al-\nthough weighted DTW still has the highest rank, the difference between WDTW and DTWCV is very small and there is no significant difference between the top four classifiers. There is a significant difference between the full window classifiers and those that restrict the window, but there is no evidence to suggest the weighting algorithm is better than the window algorithm. LCSS performs surprisingly well. Table IV shows the average ranks split by problem type. LCSS performs well on the motion data and image, but ranks poorly on sensor. Conversely, weighted and windowed DTW rank highly on motion and sensor but relatively poorly on image outlines. LCSS is in many ways closer to a Shapelet approach [5] than DTW, and this result suggests that subsequence matching techniques such as LCSS and Shapelets may be better for image outline classification."}, {"heading": "V. CONCLUSIONS", "text": "We have conducted extensive experiments on the largest set of time series classification problems ever used in the literature (to the best of our knowledge). We have performed these tests to validate commonly held assumptions, evaluate a recently published algorithm and assess methods for ensembling.\nFirstly, we conclude that comparisons against 1-NN with Euclidean distance for new TSC algorithms are not particularly informative, since three standard classifiers applied to the raw data perform significantly better with no parameter tuning at all. We think that a new algorithm is only of interest in terms of accuracy if it can significantly outperform 1-NN DTW with a full warping window. Secondly, when using a NN classifier with DTW on a new problem, we would advise that it is not particularly important to set k through cross validation, but that setting the warping window size is worthwhile. Thirdly, we conclude that the weighting algorithm for DTW described in [8] is significantly better than DTW with a full warping window, but not significantly different to DTW with the window set through cross validation."}], "references": [{"title": "Time series classification with ensembles of elastic distance measures", "author": ["J. Lines", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery, vol. Accepted for publication, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Support vector machines of intervalbased features for time series classification", "author": ["J. Rodriguez", "C. Alonso"], "venue": "Knowledge-Based Systems, vol. 18, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Querying and mining of time series data: Experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proc. 34th VLDB, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Time series clustering and classification by the autoregressive metric", "author": ["M. Corduas", "D. Piccolo"], "venue": "Computational Statistics and Data Analysis, vol. 52, no. 4, pp. 1860\u20131872, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1860}, {"title": "Time series shapelets: A new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "Proc. 15th ACM SIGKDD, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "An integrated framework for simultaneous classification and regression of time-series data", "author": ["Z. Abraham", "P. Tan"], "venue": "Proc. 10th SDM, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A complexity-invariant distance measure for time series", "author": ["G. Batista", "X. Wang", "E. Keogh"], "venue": "Proc. 11th SDM, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Weighted dynamic time warping for time series classification", "author": ["Y. Jeong", "M. Jeong", "O. Omitaomu"], "venue": "Pattern Recognition, vol. 44, pp. 2231\u20132240, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Transformation based ensembles for time series classification", "author": ["A. Bagnall", "L. Davis", "J. Hills", "J. Lines"], "venue": "Proc. 12th SDM, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification trees for time series", "author": ["A. Douzal-Chouakria", "C. Amblard"], "venue": "Pattern Recognition, vol. 45, no. 3, pp. 1076\u20131091, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification of time series by shapelet transformation", "author": ["J. Hills", "J. Lines", "E. Baranauskas", "J. Mapp", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery, vol. online first, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A time series forest for classification and feature extraction", "author": ["H. Deng", "G. Runger", "E. Tuv", "M. Vladimir"], "venue": "Information Sciences, vol. 239, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Derivative dynamic time warping", "author": ["E.Keogh", "M.Pazzani"], "venue": "Proc. 1st SDM, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Time series classification", "author": ["A. Bagnall"], "venue": "http://www.uea.ac.uk/computing/machine-learning/time-seriesclassification.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 0}, {"title": "The UCR time series classification/clustering homepage", "author": ["E. Keogh", "Q. Zhu", "B. Hu", "Y. Hao", "X. Xi", "L. Wei", "C. Ratanamahatana"], "venue": "http://www.cs.ucr.edu/ eamonn/time_series_data/, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Shapelet based time-series classification", "author": ["A. Bagnall"], "venue": "http://www.uea.ac.uk/computing/machine-learning/shapelets.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Efficient Similarity Search In Sequence Databases", "author": ["R. Agrawal", "C. Faloutsos", "A. Swami"], "venue": "Proc. 4th FODO, 1993.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "C4.5: programs for machine learning", "author": ["J. Quinlan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Rotation forest: A new classifier ensemble method", "author": ["J. Rodriguez", "L. Kuncheva", "C. Alonso"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 28, no. 10, pp. 1619\u20131630, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Naive (bayes) at forty: The independence assumption in information retrieval", "author": ["D. Lewis"], "venue": "Proc. 10th ECML, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1988}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Three myths about dynamic time warping data mining", "author": ["C. Ratanamahatana", "E. Keogh"], "venue": "Proc. 5th SDM, 2005.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Hand shape classification using DTW and LCSS as similarity measures for vision-based gesture recognition system", "author": ["A. Kuzmanic", "V. Zanchi"], "venue": "2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 11, no. 1, pp. 10\u201318, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "As part of a wider investigation into elastic distance measures for TSC [1], we perform a series of experiments to test whether this standard practice is valid.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "A wide range of algorithms have been proposed for solving TSC problems (see, for example [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": "In [9], Bagnall et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "The main conclusion in [9] is that for problems where the discriminatory features are based on similarity in change and similarity in shape, operating in a different data space produces a better performance improvement than designing a more complex classifier for the time domain.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "However, the issue of what is the best technique in a single data domain is not addressed in [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 190, "endOffset": 193}, {"referenceID": 9, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 195, "endOffset": 199}, {"referenceID": 11, "context": "Our aim is to experimentally determine the best method for constructing classifiers in the time domain, an area that has drawn most of the attention of TSC data mining researchers [7], [8], [2], [10], [12].", "startOffset": 201, "endOffset": 205}, {"referenceID": 6, "context": "The general consensus amongst data mining researchers is that \u201csimple nearest neighbor classification is very difficult to beat\u201d [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "For problems with few training cases, an elastic distance measure such as dynamic time warping (DTW) or longest common subsequence (LCSS) is often superior to Euclidean distance, but as the number of series increases \u201cthe accuracy of elastic measures converge with that of Euclidean distance\u201d [3].", "startOffset": 293, "endOffset": 296}, {"referenceID": 7, "context": "A version of DTW that weights against large warpings (WDTW) is described in [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "The weighting scheme can be used in conjunction with dynamic time warping and an alternative version based on first order differences (DDTW), described in [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 7, "context": "We extend the experiments described in [8] to test whether their conclusions hold over a large number of data sets with parameter optimisations for all of the algorithms considered.", "startOffset": 39, "endOffset": 42}, {"referenceID": 13, "context": "All datasets and code to reproduce experiments and results are available online [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 15, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "43 datasets come from the the UCR repository [15], 24 problems are from other published research, including [5], [16], and 5 are new data sets on electricity device classification problems described in [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 16, "context": "If the common shape involves the whole series, but is phase shifted between instances of the same class, then transformation into the frequency domain is often the best approach (for example, see [17]).", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "If the common shape is local and embedded in confounding noise, then subsequence techniques such as Shapelets can be employed [5], [11].", "startOffset": 126, "endOffset": 129}, {"referenceID": 10, "context": "If the common shape is local and embedded in confounding noise, then subsequence techniques such as Shapelets can be employed [5], [11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 3, "context": "The most common approach in this situation is to fit an ARMA model, then base similarity on differences in model parameters [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 9, "context": "This can be quantified by measures such as Euclidean distance or correlation [10], [6].", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "This can be quantified by measures such as Euclidean distance or correlation [10], [6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "A classic example of this type of similarity is the Cylinder-Bell-Funnel artificial data set, where there is noise around the underlying shape, but also noise in the index of where the underlying shape transitions [10].", "startOffset": 214, "endOffset": 218}, {"referenceID": 2, "context": "In a comprehensive study [3], DTW was found to be as least as good as other elastic measures based on edit distance, and constraining the warping window was found to speed up computation, \u201cwhile yielding the same or even better accuracy\u201d [3].", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "In a comprehensive study [3], DTW was found to be as least as good as other elastic measures based on edit distance, and constraining the warping window was found to speed up computation, \u201cwhile yielding the same or even better accuracy\u201d [3].", "startOffset": 238, "endOffset": 241}, {"referenceID": 2, "context": "The experimentation in [3] addresses the issue of what distance measure to use and is the starting point for our research.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "Filtering was found to be not effective in [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 17, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "5 [18], Random Forest [19], Rotation Forest [20], Naive Bayes [21] and Bayesian networks [22] and Support Vector Machines with linear and quadratic kernels [23].", "startOffset": 156, "endOffset": 160}, {"referenceID": 23, "context": "For similarity in shape, Dynamic Time Warping (DTW) is commonly used to mitigate against distortions in the time axis [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "This approach can be extended to consider real-valued time series by using a distance threshold , which defines the maximum difference between a pair of values that is allowed for them to be considered a match [25].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "Keogh and Pazzani proposed a modification of DTW called Derivative Dynamic Time Warping (DDTW) [13] that first transforms the series into a series of first differences.", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "A logistic weight function is proposed in [8], so that a warping of a places imposes a weighting of", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "43 of these are available from the UCR repository [15], 29 were used in other published work [11], [5], [16] and 5 are new data sets we present for the first time.", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "Further information and the data sets we have permission to circulate are available from [14].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "We conducted our classification experiments with WEKA [26] source code adapted for time series classification, using the 77 data sets described in Section III.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "All the results are available on an Excel spreadsheet from [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "We used a paired T test for the mean and a Wilcoxon signed rank test for the median, full results are available from [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "The results published for the weighting algorithm proposed in [8] are summarised in the critical difference diagram in Figure 6.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "Data taken directly from [8].", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "LCSS is in many ways closer to a Shapelet approach [5] than DTW, and this result suggests that subsequence matching techniques such as LCSS and Shapelets may be better for image outline classification.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "Thirdly, we conclude that the weighting algorithm for DTW described in [8] is significantly better than DTW with a full warping window, but not significantly different to DTW with the window set through cross validation.", "startOffset": 71, "endOffset": 74}], "year": 2014, "abstractText": "Data mining research into time series classification (TSC) has focussed on alternative distance measures for nearest neighbour classifiers. It is standard practice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a straw man for comparison. As part of a wider investigation into elastic distance measures for TSC [1], we perform a series of experiments to test whether this standard practice is valid. Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to standard classifiers, examine whether the performance of 1-NN Euclidean approaches that of 1-NN DTW as the number of cases increases, assess whether there is any benefit of setting k for k-NN through cross validation whether it is worth setting the warping path for DTW through cross validation and finally is it better to use a window or weighting for DTW. Based on experiments on 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to beat but 1-NN with DTW is not, if window size is set through cross validation.", "creator": "LaTeX with hyperref package"}}}