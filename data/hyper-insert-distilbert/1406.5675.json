{"id": "1406.5675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2014", "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions", "abstract": "many other kernel methods clearly suffer from seemingly high minimum time and spectral space complexities, so they inherently are prohibitive investments in very big - scaled data applications. to closely tackle the computational challenge, identifying the generalized nystr \\ \" \u03b4 om'method has largely been extensively used to reduce time variability and space complexities by implying sacrificing needs some accuracy. the specific nystr \\ \" r om method speedups computation by \" constructing an approximation of the lambda kernel matrix in question using eliminating only a few columns scaled of the matrix. recently, combining a variant rendering of the nystr \\ \" om \u201c method internally called the modified hp nystr \\ \" $ om m method and has potentially demonstrated significant improvement times over the typical standard / nystr \\ \" ^ om one method in approximation accuracy, both theoretically and physically empirically.", "histories": [["v1", "Sun, 22 Jun 2014 05:02:06 GMT  (257kb,D)", "http://arxiv.org/abs/1406.5675v1", null], ["v2", "Thu, 15 Jan 2015 10:05:36 GMT  (533kb,D)", "http://arxiv.org/abs/1406.5675v2", null], ["v3", "Sat, 28 Mar 2015 07:43:49 GMT  (543kb,D)", "http://arxiv.org/abs/1406.5675v3", null], ["v4", "Sun, 11 Oct 2015 08:49:11 GMT  (814kb,D)", "http://arxiv.org/abs/1406.5675v4", null], ["v5", "Sun, 20 Dec 2015 09:47:37 GMT  (888kb,D)", "http://arxiv.org/abs/1406.5675v5", null], ["v6", "Fri, 20 May 2016 06:32:12 GMT  (348kb,D)", "http://arxiv.org/abs/1406.5675v6", "Journal of Machine Learning Research, 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shusen wang", "luo luo", "zhihua zhang"], "accepted": false, "id": "1406.5675"}, "pdf": {"name": "1406.5675.pdf", "metadata": {"source": "CRF", "title": "The Modified Nystro\u0308m Method: Theories, Algorithms, and Extension", "authors": ["Shusen Wang", "Zhihua Zhang"], "emails": ["wss@zju.edu.cn", "zhihua}@sjtu.edu.cn"], "sections": [{"heading": null, "text": "In this paper we provide theoretical analysis, efficient algorithms, and a simple but highly accurate extension for the modified Nystro\u0308m method. First, we prove that the modified Nystro\u0308m method is exact under certain conditions, and we establish a lower error bound for the modified Nystro\u0308m method. Second, we develop two efficient algorithms to make the modified Nystro\u0308m method efficient and practical. We devise a simple column selection algorithm with a provable error bound. With the selected columns at hand, we propose an algorithm that computes the modified Nystro\u0308m approximation in lower time complexity than the approach in the previous work. Third, the extension which we call the SS-Nystro\u0308m method has much stronger error bound than the modified Nystro\u0308m method, especially when the spectrum of the kernel matrix decays slowly. Our proposed SS-Nystro\u0308m can be computed nearly as efficiently as the modified Nystro\u0308m method. Finally, experiments on real-world datasets demonstrate that the proposed column selection algorithm is both efficient and accurate and that the SS-Nystro\u0308m method always leads to much higher kernel approximation accuracy than the standard/modified Nystro\u0308m method.\nKeywords: Kernel methods, kernel approximation, matrix factorization, the Nystro\u0308m method\nar X\niv :1\n40 6.\n56 75\nv1 ["}, {"heading": "1. Introduction", "text": "The kernel methods are important tools in machine learning, computer vision, and data mining (Scho\u0308lkopf and Smola, 2002, Shawe-Taylor and Cristianini, 2004). However, many kernel methods require matrix computations of high time and space complexities. Let n be the number of data instances. For example, the Gaussian process regression, kernel ridge regression, and least squares SVM all compute the inverse of some n \u00d7 n matrices which costs time O(n3) and space O(n2); the kernel PCA, Isomap, and Laplacian eigenmaps all perform the truncated eigenvalue decomposition which takes time O(n2k) and space O(n2), where k is the target rank of the decomposition.\nBesides high time complexities, these matrix operations also have high space complexities and are difficult to implement in distributed computing facilities. The matrix decomposition and matrix (pseudo) inverse operations are generally solved by numerical iterative algorithms, which go many passes through the data matrix until convergence. Thus the whole data matrix had better be placed in RAM, otherwise in each iteration there would be a swap between RAM and disk, which is extremely slow. Unless the algorithm is pass-efficient, that is, it goes through the data matrix only constant times, the space complexity should be at least the size of the data matrix. Such iterative algorithms cannot be efficiently implemented and performed in distributed computing facilities like MapReduce for two reasons. First, the RAM cost is too expensive for each individual machine to stand. Second, communication and synchronization must be performed in each iteration of the numerical algorithms, so the cost of each iteration is high.\nOne possible approach to making matrix computation and kernel methods scalable is to use randomized matrix approximations to reduce the time and space costs, among which the most famous one is perhaps the Nystro\u0308m method (Nystro\u0308m, 1930). The Nystro\u0308m method approximates an arbitrary symmetric positive semidefinite (SPSD) kernel matrix using a small subset of its columns, and the method reduces the time complexities of many matrix operations from O(n3) or O(n2k) to O(nc2) and space complexities from O(n2) to O(nc), where k is the target rank, c is the number of selected columns, and it holds in general that k < c n. In this way, time and space costs are only linearly in n, so many kernel methods can be efficiently solved even when n is large.\nThe Nystro\u0308m method has been widely used to speedup various kernel methods, such as the Gaussian process regression (Williams and Seeger, 2001), kernel SVMs (Zhang et al., 2008, Yang et al., 2012), kernel ridge regression (Yang et al., 2012, Cortes et al., 2010), spectral clustering (Fowlkes et al., 2004, Li et al., 2011), kernel PCA and manifold learning (Zhang et al., 2008, Zhang and Kwok, 2010, Talwalkar et al., 2013), determinantal processes (Affandi et al., 2013), etc.\nTo construct a low-rank matrix approximation, the Nystro\u0308m method requires a small number of columns (say, c columns) to be selected from the kernel matrix by a column sampling technique. The approximation accuracy is largely determined by the sampling technique; that is, a better sampling technique results in a Nystro\u0308m approximation with lower approximation error. In the previous work much attention has been made on improving the error bounds of the Nystro\u0308m method: additive-error bound has been explored by Drineas and Mahoney (2005), Shawe-taylor et al. (2005), Kumar et al. (2012), Jin et al.\n(2013), etc. Recently, Gittens and Mahoney (2013) established the first relative-error bound which is more interesting than additive-error bound (Mahoney, 2011).\nHowever, the approximation quality cannot be arbitrarily improved by devising a very good sampling technique. Wang and Zhang (2013) showed that no matter what sampling technique is used to construct the Nystro\u0308m approximation, the incurred error (in the spectral norm or the squared Frobenius norm) must grow with the matrix size n at least linearly. Thus, the Nystro\u0308m approximation can be very rough when n is large, unless large number columns are selected. As was pointed out by Cortes et al. (2010), the tighter kernel approximation leads to the better learning accuracy, so it is useful to find a kernel approximation model that is more accurate than the Nystro\u0308m method.\nTo improve the approximation accuracy, Kumar et al. (2012) devised a variant of the Nystro\u0308m method called the ensemble Nystro\u0308m method, which generates t (> 1) standard Nystro\u0308m approximations and takes the average. With the ensemble Nystro\u0308m method at hand, some specific n\u00d7n linear systems can be solved in timeO(nc2 log t) when implemented in paralled (Kumar et al., 2012). Empirical results show that the ensemble Nystro\u0308m method is much more accurate than the standard Nystro\u0308m method. However, the ensemble Nystro\u0308m method has the same lower error bound as the standard Nystro\u0308m method (Wang and Zhang, 2013), which implies that the ensemble Nystro\u0308m method does not lead to substantial improvements in the kernel approximation accuracy.\nRecently Wang and Zhang (2013) proposed a new alternative called the modified Nystro\u0308m method. The modified Nystro\u0308m method can be applied in the same way exactly as the standard Nystro\u0308m method to speedup kernel methods. The modified Nystro\u0308m method has an advantage that the error does not grow with matrix size n. Therefore, by using the modified Nystro\u0308m method instead of the standard Nystro\u0308m method, a significantly smaller number of columns is needed to attain the same accuracy as the standard Nystro\u0308m method. Although it has higher time complexity than the standard Nystro\u0308m method, the modified Nystro\u0308m method is pass-efficient and has low RAM cost, so the modified Nystro\u0308m method can be used in big data problems.\nWe explore in this paper the theoretical properties of the modified Nystro\u0308m method and develop algorithms for efficiently computing the modified Nystro\u0308m method. We also propose a simple extension of the modified Nystro\u0308m method called the modified Nystro\u0308m by spectral shifting (SS-Nystro\u0308m). SS-Nystro\u0308m is more accurate than the modified Nystro\u0308m method, especially when the the spectrum of the kernel matrix decays slowly, where the standard and modified Nystro\u0308m methods are very inaccurate."}, {"heading": "1.1 Our Contributions", "text": "Our contributions mainly include three aspects: theoretical analysis, computational efficient algorithms, and extensions.1 They are summarized as follows.\n1. An early version of the results in Section 4 and Section 5 are published in (Wang and Zhang, 2014). The ISS-Nystro\u0308m method described in Section 6.2 is published in (Wang et al., 2014); an improved model\u2014the SS-Nystro\u0308m method\u2014is formulated and analyzed in this paper."}, {"heading": "1.1.1 Our Contributions: Theories", "text": "Kumar et al. (2009), Talwalkar and Rostamizadeh (2010) previously showed that the standard Nystro\u0308m approximation is exact when the original kernel matrix is low-rank. In Section 4.1 we show that the modified Nystro\u0308m method also exactly recovers the original SPSD matrix under the same conditions.\nFurthermore, Wang and Zhang (2013) proved the lower error bounds of the standard Nystro\u0308m method. Analogously, in Section 4.2 we establish a lower error bound for the modified Nystro\u0308m method. The lower bound of the modified Nystro\u0308m method has a strong resemblance with the lower bound of the column selection problem, which is known to be tight, so we conjecture that our established lower bound is tight."}, {"heading": "1.1.2 Our Contributions: Algorithms", "text": "Though the modified Nystro\u0308m method is more accurate than the standard Nystro\u0308m method, the modified Nystro\u0308m approximation is more expensive to compute. In this paper we seek to make the modified Nystro\u0308m method efficient and practical.\nIn Section 5.1 we provide an efficient algorithm for computing the intersection matrix of the modified Nystro\u0308m method. Under a certain condition, this algorithm can significantly reduce the time cost.\nIn Section 5.2 we devise a simple and efficient column selection algorithm for the modified Nystro\u0308m method. We call it the uniform+adaptive2 algorithm. Our uniform+adaptive2 algorithm is more efficient and much easier to implement than the near-optimal+adaptive algorithm of Wang and Zhang (2013), yet its error bound is comparable with the nearoptimal+adaptive algorithm."}, {"heading": "1.1.3 Our Contributions: Extension", "text": "The standard/modified Nystro\u0308m methods generate low-rank approximations to kernel matrices, and their approximation errors cannot be better than the rank-c truncated SVD, where c is the number of columns selected by the Nystro\u0308m methods. When the spectrum of a kernel matrix decays slowly (that is, the c + 1 to n largest eigenvalues are not small enough), the low-rank approximations constructed by the partial eigenvalue decomposition or the standard/modified Nystro\u0308m methods are far from the original kernel matrix.\nInspired by a very recent work of Zhang (2014), we propose in this paper a new method called the modified Nystro\u0308m by spectral shifting (SS-Nystro\u0308m) to make the approximation still effective even when the spectrum of the original kernel matrix decays slowly. Unlike the standard/modified Nystro\u0308m methods which approximate the kernel matrix K \u2208 Rn\u00d7n by a low-rank factorization K \u2248 CUCT , our SS-Nystro\u0308m approximates K by K \u2248 C\u0304UssC\u0304T + \u03b4ssIn, where C, C\u0304 \u2208 Rn\u00d7c, U,Uss \u2208 Rc\u00d7c, and \u03b4ss \u2265 0. When the spectrum of K decays slowly, the term \u03b4ssIn helps improve the approximation accuracy significantly. We show that SS-Nystro\u0308m method has a provably tighter bound than the standard/modified Nystro\u0308m methods. In Section 6 we describe the SS-Nystro\u0308m method in detail."}, {"heading": "1.2 Paper Organization", "text": "The remainder of this paper is organized as follows. In Section 2 we define the notation that will be used in the paper. In Section 3 we describe the Nystro\u0308m approximation models, column selection algorithm, and the applications to kernel methods. Then we present our work\u2014theories, algorithms, and extensions\u2014respectively in Sections 4, 5, 6. Finally in Section 7 we empirically evaluate the models and algorithms proposed in this paper. All proofs are all deferred to the appendix."}, {"heading": "2. Notation", "text": "The notation used in this paper follows that of Wang and Zhang (2013). For an m\u00d7n matrix A = [aij ], we let a (i) be its i-th row, aj be its j-th column, \u2016A\u2016F = ( \u2211 i,j a 2 ij)\n1/2 be its Frobenius norm, and \u2016A\u20162 = maxx 6=0 \u2016Ax\u20162/\u2016x\u20162 be its spectral norm.\nLetting \u03c1 = rank(A), we write the condensed singular value decomposition (SVD) of A as A = UA\u03a3AV T A, where the (i, i)-th entry of \u03a3A \u2208 R\u03c1\u00d7\u03c1 is the i-th largest singular value of A, denoted by \u03c3i(A). We also let UA,k and VA,k be the first k (< \u03c1) columns of UA and VA, respectively, and \u03a3A,k be the k\u00d7 k top sub-block of \u03a3A. Then the m\u00d7 n matrix Ak = UA,k\u03a3A,kV T A,k is the \u201cclosest\u201d rank-k approximation to A.\nIf A is a normal, we let A = UA\u039bAU T A be the eigenvalue decomposition, and denote the i-th diagonal entry of \u039bA by \u03bbi(A), where |\u03bb1(A)| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbn(A)|. When A is SPSD, the SVD and the eigenvalue decomposition of A are equivalent.\nBased on SVD, the matrix coherence of the columns of A relative to the best rank-k\napproximation is defined by \u00b5k = n k maxj \u2225\u2225V(j)A,k\u2225\u222522. Let A\u2020 = VA\u03a3\u22121A UTA be the MoorePenrose inverse of A. When A is nonsingular, the Moore-Penrose inverse is identical to the matrix inverse. Given another n\u00d7 c matrix C, we define PCA = CC\u2020A as the projection of A onto the column space of C and PC,kA = C \u00b7 argminrank(X)\u2264k \u2016A\u2212CX\u2016F as the rank restricted projection. It is obvious that \u2016A\u2212 PCA\u2016F \u2264 \u2016A\u2212 PC,kA\u2016F .\nFinally, we discuss the time complexities of the matrix operations mentioned above. For an m\u00d7n general matrix A (assume m \u2265 n), it takes O(mn2) flops to compute the full SVD and O(mnk) flops to compute the truncated SVD of rank k (< n). The computation of A\u2020 takes O(mn2) flops. It is worth mentioning that although multiplying an m\u00d7n matrix by an n\u00d7p matrix takes mnp flops, matrix multiplication is pass-efficient, and it can be performed in full parallel by partitioning the matrices into blocks. Thus, the time and space expense of large-scale matrix multiplication is not a challenge in real-world applications. We denote the time complexity of such a matrix multiplication by TMultiply(mnp), which can be tremendously smaller than O(mnp) in parallel computing facilities (Halko et al., 2011). An algorithm can still be efficient even if it demands large-scale matrix multiplications."}, {"heading": "3. The Nystro\u0308m Methods", "text": "In Section 3.1 we formally describe the Nystro\u0308m approximation methods, including the standard Nystro\u0308m method of Nystro\u0308m (1930), Williams and Seeger (2001), the modified Nystro\u0308m method of Wang and Zhang (2013), and the SS-Nystro\u0308m method proposed in this work. Since the Nystro\u0308m approximation is constructed by using a small portion of columns, in Section 3.2 we introduce some popular column sampling algorithms, especially\nthose with theoretical guarantees. In Section 3.3 we discuss how to apply the Nystro\u0308m methods to make kernel methods scalable."}, {"heading": "3.1 Models", "text": "Suppose we are given an n \u00d7 n symmetric matrix K, our goal is to compute a fast factorization of K such that the matrix inverse (K + \u03b1In)\n\u22121 and/or the eigenvalue decomposition of K can be approximately computed highly efficiently. The Nystro\u0308m methods tackle this problem by approximating K in terms of a subset of its columns, denoted by C \u2208 Rn\u00d7c. Without loss of generality, K and C can be permuted such that\nK = [ W KT21 K21 K22 ] and C = [ W K21 ] , (1)\nwhere W is of size c\u00d7c. Based on the above notation, three kinds of Nystro\u0308m approximation models are defined as follows. As well as the standard Nystro\u0308m mehtod, the modified/SS Nystro\u0308m methods can be trivially extended to the ensemble Nystro\u0308m method (Kumar et al., 2012), the SPSD Sketching Model (Gittens and Mahoney, 2013), and the memory efficient kernel approximation method (Si et al., 2014)."}, {"heading": "3.1.1 The standard Nystro\u0308m method", "text": "The standard Nystro\u0308m method is defined by\nK\u0303nysc , CU nysCT = CW\u2020CT ,\nwhere Unys = W\u2020 is called the intersection matrix. The (standard) Nystro\u0308m method was proposed by Nystro\u0308m (1930), and it was first introduced to the machine learning community by Williams and Seeger (2001). With the selected columns at hand, the standard Nystro\u0308m method needs not to see the whole matrix K, and it takes only O(c3) + TMultiply(nc2) time and O(c2) space to compute the intersection matrix Unys."}, {"heading": "3.1.2 The Modified Nystro\u0308m Method", "text": "The modified Nystro\u0308m method is defined by\nK\u0303modc , CU modCT = C\n( C\u2020K(C\u2020)T ) CT ,\nwhich is essentially the projection of K onto the column space of C and the row space of CT . This model is proposed by Wang and Zhang (2013), and it is not strictly the Nystro\u0308m method because it uses a different intersection matrix Umod , C\u2020A(C\u2020)T . With the selected columns at hand, the modified Nystro\u0308m method needs to go only one pass through the data, and the time and space costs are in general O(nc2) + TMultiply(n2c) and O(nc), respectively.\nAlthough more expensive to compute, the modified Nystro\u0308m method is a more accurate approximation. Since Umod = C\u2020K(C\u2020)T is the minimizer of the optimization problem\nmin U \u2016K\u2212CUCT \u2016F , (2)\nso the modified Nystro\u0308m method is in general more accurate than the standard Nystro\u0308m method in that \u2225\u2225K\u2212CUmodC\u2225\u2225 F \u2264 \u2225\u2225K\u2212CUnysC\u2225\u2225 F ."}, {"heading": "3.1.3 The SS-Nystro\u0308m Method", "text": "We propose in this paper an extension of the modified Nystro\u0308m method, which we call the modified Nystro\u0308m method by spectral shifting (SS-Nystro\u0308m). The SS-Nystro\u0308m approximation of K is defined as\nK\u0303ssc = C\u0304U ssC\u0304T + \u03b4ssIn.\nHere \u03b4ss \u2265 0 is called the spectral shifting term. This approximation is computed in three steps. First, (approximately) compute the initial spectral shifting term\n\u03b4\u0304 = 1\nn\u2212 k\n( tr(K)\u2212 k\u2211 j=1 \u03c3j(K) ) , (3)\nand then perform spectral shift K\u0304 = K \u2212 \u03b4\u0304In, where k \u2264 c is the target rank. Actually, exactly setting the initial spectral shifting term to be \u03b4\u0304 is unnecessary; later in Section 6 we will show that SS-Nystro\u0308m has better upper error bound than the modified Nystro\u0308m method whenever the initial spectral shifting term falls in the interval (0, \u03b4\u0304]. Second, use some column sampling algorithm to select c columns of K\u0304 to form C\u0304. Finally, with C\u0304 at hand, compute Uss and \u03b4ss by\n\u03b4ss = 1\nn\u2212 rank(C\u0304)\n( tr(K)\u2212 tr ( C\u0304\u2020KC\u0304 )) Uss = C\u0304\u2020K(C\u0304\u2020)T \u2212 \u03b4ss(C\u0304T C\u0304)\u2020. (4)\nWe will show that K\u0303ssc is positive (semi)definite if K is positive (semi)definite.\nThe SS-Nystro\u0308m method is an extension of the modified Nystro\u0308m method. With columns selected from K to form C, the modified Nystro\u0308m method is obtained by solving the minimization problem (2) to compute the intersection matrix Umod. Analogously, with the columns selected from K\u0304 = K\u2212 \u03b4\u0304In to form C\u0304, SS-Nystro\u0308m is obtained by solving(\nUss, \u03b4ss )\n= argmin U,\u03b4 \u2225\u2225K\u2212 C\u0304UC\u0304T \u2212 \u03b4In\u2225\u22252F , (5) to obtain the intersection matrix Uss and the spectral shifting term \u03b4ss."}, {"heading": "3.2 Column Sampling Algorithms", "text": "The column selection problem has been widely studied in the theoretical computer science community (Boutsidis et al., 2011, Mahoney, 2011, Guruswami and Sinop, 2012) and the numerical linear algebra community (Gu and Eisenstat, 1996, Stewart, 1999), and numerous algorithms have been devised and analyzed. Here we focus on some theoretically guaranteed algorithms studied in the theoretical computer science community.\nIn the previous work much attention has been paid on improving column sampling algorithms such that the Nystro\u0308m approximation is more accurate. Uniform sampling is the simplest and most time-efficient column selection algorithm, and it has provable error bounds when applied to the standard Nystro\u0308m method (Gittens, 2011, Kumar et al., 2012, Jin et al., 2013, Gittens and Mahoney, 2013). To improve the approximation accuracy, many importance sampling algorithms have been proposed, among which the adaptive sampling (Deshpande et al., 2006, Kumar et al., 2012, Wang and Zhang, 2013) (see Algorithm 2) and the leverage score based sampling (Drineas et al., 2008, Ma et al., 2014) are widely studied. The leverage score based sampling has provable bounds when applied to the standard Nystro\u0308m method (Gittens and Mahoney, 2013), and the adaptive sampling has provable bounds when applied to the modified Nystro\u0308m method (Wang and Zhang, 2013). Additionally, quadratic Re\u0301nyi entropy based active subset selection (De Brabanter et al., 2010) and k-means clustering based selection (Zhang and Kwok, 2010) are also effective algorithms, but they do not have additive-error or relative-error bounds.\nParticularly, Wang and Zhang (2013) proposed an algorithm for the modified Nystro\u0308m method by combining the near-optimal column sampling algorithm (Boutsidis et al., 2011) and the adaptive sampling algorithm (Deshpande et al., 2006). The error bound of the algorithm is the tightest among all the feasible algorithms for the Nystro\u0308m methods. We show it in the following lemma.\nLemma 1 (The Near-Optimal+Adaptive Algorithm) (Wang and Zhang, 2013) Given a symmetric matrix K \u2208 Rn\u00d7n and a target rank k, the algorithm samples totally c = O(k \u22122) columns of K to construct the approximation. We run the algorithm t \u2265 (2 \u22121 + 1) log(1/p) times (independently in parallel) and choose the sample that minimizes \u2016K\u2212C ( C\u2020K(C\u2020)T ) CT \u2225\u2225 F\n, then the inequality\u2225\u2225K\u2212C(C\u2020K(C\u2020)T )CT\u2225\u2225 F \u2264 (1 + )\u2016K\u2212Kk\u2016F\nholds with probability at least 1\u2212p. The algorithm costs O ( nc2 2+nk3 \u22122/3 ) +TMultiply ( n2c ) time and O(nc) space in computing C.\nThe near-optimal+adaptive algorithm is effective and efficient, but its implementation is very complicated. Its main component\u2014the near-optimal column selection algorithm\u2014 consists of three steps: approximate SVD via random projection (Boutsidis et al., 2011, Halko et al., 2011), the dual-set sparsification algorithm (Boutsidis et al., 2011), and the adaptive sampling algorithm (Deshpande et al., 2006). Without careful implementation of the first two steps, the time and space costs roar, making the near-optimal+adaptive algorithm inefficient."}, {"heading": "3.3 Applications to Kernel Methods", "text": "We discuss in this section how to speedup matrix inverse and eigenvalue decomposition using the Nystro\u0308m methods. Many kernel methods will become scalable if the matrix inverse and eigenvalue decomposition can be efficiently solved.\n\u2022 Gaussian process regression (Williams and Seeger, 2001), least squares SVM (Suykens and Vandewalle, 1999), and kernel ridge regression (Saunders et al., 1998) all require\nsolving this kind of linear system:\n(K + \u03b1In)b = y, (6)\nwhich amounts to the matrix inverse problem b = (K+\u03b1In) \u22121y. Here \u03b1 is a constant.\n\u2022 Spectral clustering (Fowlkes et al., 2004, Li et al., 2011), kernel PCA (Zhang and Kwok, 2010), and many manifold learning (Zhang et al., 2008, Talwalkar et al., 2013) need to perform the truncated eigenvalue decomposition. The sampling algorithm of determinantal processes (Hough et al., 2006, Affandi et al., 2013) performs the full eigenvalue decomposition.\nLet K \u2208 Rn\u00d7n be the kernel matrix, and let the SS-Nystro\u0308m approximation of K be defined by\nK\u0303ssc = CUC T + \u03b4In,\nwhere C and U are n\u00d7c and c\u00d7c matrices, respectively. We show that when K is replaced by K\u0303ssc , the aforementioned linear system and eigenvalue decomposition can be efficiently solved. When using K\u0303nysc or K\u0303modc to replace K, one can still use the results below by setting \u03b4 = 0.\nWe first show how to approximately compute the matrix inverse (K + \u03b1In) \u22121. Let U = Z\u039bZT be the condensed eigenvalue decomposition of the intersection matrix of SSNystro\u0308m, where Z \u2208 Rc\u00d7r, \u039b \u2208 Rr\u00d7r, and r = rank(U) \u2264 c. We expand (K\u0303ssc + \u03b1In)\u22121 by the Sherman-Morrison-Woodbury formula and obtain(\nK\u0303ssc + \u03b1In )\u22121 = ( C\u0304Z\u039bZT C\u0304T + \u03c4In )\u22121 = \u03c4\u22121In \u2212 \u03c4\u22121C\u0304Z ( \u03c4\u039b\u22121 + ZT C\u0304T C\u0304Z )\u22121 ZT C\u0304T , (7)\nwhere \u03c4 = \u03b4+\u03b1. In this way the linear system (6) can be approximately computed in only O(nc2) time and O(nc) space.\nNow we show how to approximately compute the eigenvalue decomposition of K. We let C = UC\u03a3CVC be the condensed SVD of C. Suppose r = rank(C), we let\nS = \u03a3CVCUV T C\u03a3 T C \u2208 Rr\u00d7r,\nand we write the eigenvalue decomposition of S as S = US\u039bSU T S . Now we can write the eigenvalue decomposition of K\u0303ssc as\nK\u0303ssc = (UCUS)\u039bS(UCUS) T + \u03b4In = (UCUS)(\u039bS + \u03b4Ir)(UCUS) T + U\u22a5(\u03b4In)U T \u22a5. (8)\nHere U\u22a5 \u2208 Rn\u00d7(n\u2212r) is a column orthogonal complementary matrix of (UCUS)."}, {"heading": "4. Theories", "text": "In Section 4.1 we show that the modified Nystro\u0308m approximation is exact when K is lowrank. In Section 4.2 we provide a lower error bound of the modified Nystro\u0308m method."}, {"heading": "4.1 Theoretical Justifications", "text": "Kumar et al. (2009), Talwalkar and Rostamizadeh (2010) showed that the standard Nystro\u0308m method is exact when rank(W) = rank(K). We present a similar result for the modified Nystro\u0308m approximation in Theorem 2.\nTheorem 2 For a symmetric matrix K defined in (1), the following three statements are equivalent: (i) rank(W) = rank(K), (ii) K = CW\u2020CT , (iii) K = CC\u2020K(C\u2020)TCT .\nTheorem 2 implies that the standard and modified Nystro\u0308m methods are equivalent when rank(W) = rank(K), that is, the kernel matrix K is low rank. However, it holds in general that rank(K) c \u2265 rank(W), showing that the two models are not equivalent and that\n\u2225\u2225K \u2212 K\u0303modc \u2225\u2225F \u2264 \u2225\u2225K \u2212 K\u0303nysc \u2225\u2225F is satisfied. In other words, the modified Nystro\u0308m method is more accurate than the standard Nystro\u0308m method in general."}, {"heading": "4.2 Lower Error Bounds", "text": "We establish a lower error bound of the modified Nystro\u0308m method in Theorem 3. Theorem 3 shows that whatever a column sampling algorithm is used to construct the modified Nystro\u0308m approximation, at least c \u2265 2k \u22121 columns must be chosen to attain the 1 + bound.\nTheorem 3 (Lower Error Bound of the Modified Nystro\u0308m Method) Whatever a column sampling algorithm is used, there exists an n \u00d7 n SPSD matrix K such that the error incurred by the modified Nystro\u0308m method obeys:\u2225\u2225K\u2212CUmodCT\u2225\u22252\nF \u2265 n\u2212 c\nn\u2212 k\n( 1 + 2k\nc\n) \u2016K\u2212Kk\u20162F .\nHere k is an arbitrary target rank, c is the number of selected columns, and Umod = C\u2020K(C\u2020)T .\nBoutsidis et al. (2011) established a lower error bound for the column selection problem, and the lower error bound is tight because it is attained by the optimal column selection algorithm of Guruswami and Sinop (2012). Boutsidis et al. (2011) showed that whatever a column sampling algorithm is used, there exists an m \u00d7 n matrix A such that the error incurred by the projection of A onto the column space of C is lower bounded by\u2225\u2225A\u2212CC\u2020A\u2225\u22252\nF \u2265 n\u2212 c\nn\u2212 k\n( 1 + k\nc\n) \u2016A\u2212Ak\u20162F , (9)\nwhere k is an arbitrary target rank, c is the number of selected columns.\nInterestingly, the modified Nystro\u0308m approximation is the projection of K onto the column space of C and the row space of CT simultaneously, so there is a strong resemblance between the modified Nystro\u0308m approximation and the column selection problem. As we see, the lower error bound of the modified Nystro\u0308m approximation in Theorem 3 differs from (9) only by a factor of 2. So it is a reasonable conjecture that the lower bound in Theorem 3 is tight, as well as the lower bound of the column selection problem in (9). We leave it as an open problem."}, {"heading": "5. Algorithms", "text": "In Section 5.1 we propose a fast approach for computing the intersection matrices of the modified/SS Nystro\u0308m methods. In Section 5.2 we devise a simple and efficient column selection algorithm which is nearly as accurate as the state-of-the-art algorithm of Wang and Zhang (2013)."}, {"heading": "5.1 Fast Computation of the Intersection Matrices", "text": "Naively computing the intersection matrix Umod = C\u2020K(C\u2020)T takes time O(nc2) + TMultiply(n\n2c), which is much more expensive than computing Unys = W\u2020 for the standard Nystro\u0308m method. In this section we propose a more efficient algorithm for computing the intersection matrix, which only takes time O(c3) + TMultiply ( (n \u2212 c)2c ) . The algorithm is described in Theorem 4. The algorithm is obtained by expanding the Moore-Penrose inverse of C using the theorem of (Ben-Israel and Greville, 2003, Page 179).\nTheorem 4 For an n\u00d7n symmetric matrix K, when the submatrix W is nonsingular, the intersection matrix of the modified Nystro\u0308m method Umod = C\u2020K(C\u2020)T can be computed in time O(c3) + TMultiply ( (n\u2212 c)2c ) by the following formula:\nUmod = C\u2020K(C\u2020)T = T1 ( W + T2 + T T 2 + T3 ) TT1 ,\nwhere the intermediate matrices are computed by\nT0 = K T 21K21, T1 = W \u22121(Ic + W\u22121T2)\u22121, T2 = T0W \u22121, T3 = W \u22121(KT21K22K21)W\u22121.\nThe four intermediate matrices are all of size c\u00d7 c, and the matrix inverse operations are on c\u00d7 c small matrices.\nRemark 5 Since the submatrix W is not in general nonsingular, before using the algorithm, the user should first test the rank of W, which takes time O(c3). Empirically, for the radial basis function (RBF) kernel matrix K defined by\nk\u03b3ij = exp\n( \u2212 1\n2\u03b32 \u2016xi \u2212 xj\u201622\n) , (10)\nthe submatrix W is usually nonsingular2, and the algorithm is useful; for the linear kernel, W is often singular, so the algorithm does not work.\nTo illustrate the effect of our algorithm for computing the intersection matrix Umod = C\u2020K(C\u2020)T , we generate kernel matrices of the Letters Dataset which has 15, 000 instances and 16 attributes (see Table 2). We first generate a dense RBF kernel matrix with scale parameter \u03b3 = 0.2, and then obtain a sparse symmetric matrix by by truncating the entries with small magnitude such that 1% entries are nonzero. The sizes of the kernel matrices are\n2. The matrix W is nonsingular if the scaling parameter \u03b3 is positive and the selected c data instances are distinct points (Scho\u0308lkopf and Smola, 2002, Theorem 2.18).\nboth 15, 000\u00d7 15, 000. We sample c columns uniformly to compute the intersection matrix Umod = C\u2020K(C\u2020)T (the modified Nystro\u0308m) and Unys = W\u2020 (the standard Nystro\u0308m). We vary c from 100 to 2, 000 and plot the time for computing U is plotted in Figure 1. In both cases, our algorithm is faster than the naive approach, and the speedup is particularly significant when K is sparse.\nFollowing the proof of Theorem 4, we derive an algorithm for efficiently computing the intersection matrix Uss and the spectral shifting term \u03b4ss of the SS-Nystro\u0308m method. The algorithm is described in the following corollary, whose proof is analogous to that of Theorem 4.\nCorollary 6 For an n\u00d7 n symmetric matrix K, let \u03b4\u0304 be the initial spectral shifting term, K\u0304 = K\u2212 \u03b4\u0304In, and C\u0304 be c columns selected from K\u0304. According to the partition in (1), denote the submatrices of K\u0304 by W\u0304 = W\u2212 \u03b4\u0304Ic and K\u030422 = K22\u2212 \u03b4\u0304In\u2212c. When the submatrix W\u0304 is nonsingular, the intersection matrix Uss and spectral shifting term \u03b4ss of the SS-Nystro\u0308m method defined in (4) can be computed in time O(c3) + TMultiply ( (n\u2212 c)2c ) by the following formula:\n\u03b4ss = 1\nn\u2212 c\n( tr(K)\u2212 tr ( C\u0304\u2020KC\u0304 )) =\n1\nn\u2212 c\n( tr(K)\u2212 tr(T4)\u2212 c\u03b4\u0304 ) ,\nUss = C\u0304\u2020K(C\u0304\u2020)T \u2212 \u03b4ss(C\u0304T C\u0304)\u22121 = T5 + (\u03b4\u0304 \u2212 \u03b4ss) ( C\u0304T C\u0304 )\u2020 ,\nwhere the intermediate matrices are computed by\nT0 = K T 21K21, T3 = W\u0304 \u22121(Ic + T1W\u0304\u22121) T1 = W\u0304 \u22121T0,, T4 = T3 ( W\u03042 + T0 + T1W\u0304 + T2 ) ,\nT2 = W\u0304 \u22121KT21K\u030422K21, T5 = T3 ( W\u0304 + T1 + T T 1 + T2W\u0304 \u22121)TT3 . The intermediate matrices are all of size c \u00d7 c, and the matrix inverse operations are on c\u00d7 c small matrices.\nAlgorithm 1 The Uniform+Adaptive2 Algorithm.\n1: Input: an n\u00d7n symmetric matrix K, target rank k, error parameter \u2208 (0, 1], matrix coherence \u00b5. 2: Uniform Sampling. Uniformly sample\nc1 = 8.7\u00b5k log (\u221a 5k )\ncolumns of K without replacement to construct C1; 3: Adaptive Sampling. Sample\nc2 = 10k \u22121\ncolumns of K to construct C2 using adaptive sampling algorithm 2 according to the residual K\u2212 PC1K;\n4: Adaptive Sampling. Sample\nc3 = 2 \u22121(c1 + c2)\ncolumns of K to construct C3 using adaptive sampling algorithm 2 according to the residual K\u2212 P[C1, C2]K;\n5: return C = [C1,C2,C3] and U = C \u2020K(C\u2020)T ."}, {"heading": "5.2 An Efficient Column Sampling Algorithm", "text": "In this paper we propose a column sampling algorithm which is efficient, effective, and very easy to implement. The algorithm consists of a uniform sampling step and two adaptive sampling steps, so we call it the uniform+adaptive2 algorithm. The algorithm is described in Algorithm 1 and analyzed in Theorem 7. We will empirically evaluate the column selection algorithms in Section 7.\nThe idea behind the uniform+adaptive2 algorithm is quite intuitive. Since the modified Nystro\u0308m method is the simultaneous projection of K onto the column space of C and the row space of CT , the approximation error will get lower if span(C) better approximates span(K). After the initialization by uniform sampling, the columns of K far from span(C1) have large residuals and are thus likely to get chosen by the adaptive sampling. After two rounds of adaptive sampling, columns of K are likely to be near span(C).\nIt is worth mentioning that our uniform+adaptive2 algorithm is a special instance the adaptive-full algorithm of (Kumar et al., 2012, Figure 3). The adaptive-full algorithm consists of a random initialization followed by multiple adaptive sampling steps. Obviously, using multiple adaptive sampling steps can surely reduce the approximation error. However, the update of sampling probability in each step is expensive, so we choose to do only two steps. Importantly, the adaptive-full algorithm of (Kumar et al., 2012, Figure 3) is merely a heuristic scheme without a theoretical guarantee, whereas our uniform+adaptive2 algorithm has a strong error bound which is nearly as good as the state-of-the-art algorithm of Wang and Zhang (2013) (See Theorem 7).\nTheorem 7 (The Uniform+Adaptive2 Algorithm.) Given an n\u00d7n symmetric matrix K and a target rank k, we let \u00b5k denote the matrix coherence of K. Algorithm 1 samples\nAlgorithm 2 The Adaptive Sampling Algorithm.\n1: Input: a residual matrix B \u2208 Rn\u00d7n and number of selected columns c (< n). 2: Compute sampling probabilities pj = \u2016bj\u201622/\u2016B\u20162F for j = 1, \u00b7 \u00b7 \u00b7 , n; 3: Select c indices in c i.i.d. trials, in each trial the index j is chosen with probability pj ; 4: return an index set containing the indices of the selected columns.\ntotally\nc = O ( k \u22122 + \u00b5k \u22121k log k )\ncolumns of K to construct the approximation. We run Algorithm 1\nt \u2265 (20 \u22121 + 18) log(1/p)\ntimes (independently in parallel) and choose the sample that minimizes \u2016K\u2212C ( C\u2020K(C\u2020)T ) CT \u2225\u2225 F\n, then the inequality\u2225\u2225K\u2212C(C\u2020K(C\u2020)T )CT\u2225\u2225 F \u2264 ( 1 +\n)\u2225\u2225K\u2212Kk\u2225\u2225F holds with probability at least 1 \u2212 p. The algorithm costs O ( nc2 2 ) + TMultiply ( n2c ) time and O(nc) space in computing C.\nRemark 8 Theoretically, Algorithm 1 requires to compute the matrix coherence of K in order to determine c1, c2, and c3. However, computing the matrix coherence takes time O(m2k) and is thus impractical; even the fast approximation approach of Drineas et al. (2012) is not feasible here because K is a square matrix. The use of the matrix coherence here is merely for theoretical analysis; setting the parameter \u00b5 in Algorithm 1 to be exactly the matrix coherence does not certainly result in the highest accuracy. According to our off-line experiments, the resulting approximation accuracy is not sensitive to the value of \u00b5. Thus we strongly suggest the users to set \u00b5 in Algorithm 1 to be a constant (say 1), rather than actually computing the matrix coherence.\nTable 1 presents comparisons between the near-optimal+adaptive algorithm of Wang and Zhang (2013) and our uniform+adaptive2 algorithm over the time and space costs for computing C, the number of columns required to attain 1 + relative-error bound, and the hardness of implementation. The time cost of our algorithm is lower than the nearoptimal+adaptive algorithm, and the space cost of the two algorithms are the same. To attain the same error bound, our algorithm needs to select c = O ( k \u22122 + \u00b5k \u22121k log k ) columns, which is a little larger than that of the near-optimal+adaptive algorithm. When \u2192 0, we have that O ( k \u22122 + \u00b5k \u22121k log k ) = O ( k \u22122). Therefore, the error bound of our algorithm is nearly as good as the near-optimal+adaptive algorithm because is usually set to be a very small value."}, {"heading": "6. The SS-Nystro\u0308m Approximation", "text": "In this section we propose a variant of the modified Nystro\u0308m method which is still effective when the spectrum of K decays slowly. We call the proposed method the modified Nystro\u0308m\n( ) ( ) ( ) ( )\nAlgorithm 3 The Modified Nystro\u0308m by Spectral Shifting (SS-Nystro\u0308m).\n1: Input: an n\u00d7 n SPSD matrix K, a target rank k, the oversampling parameter l. 2: // approximately compute the initial spectral shifting term \u03b4\u0304 3: \u2126\u2190\u2212 n\u00d7 l standard Gaussian matrix; 4: Q\u2190\u2212 the l orthonormal basis of K\u2126 \u2208 Rn\u00d7l; 5: s\u2190\u2212 sum of the top k singular values of QTK \u2208 Rl\u00d7n; 6: \u03b4\u0303 = 1n\u2212k ( tr(K)\u2212 s ) \u2248 \u03b4\u0304; 7: // spectral shifting and column selection 8: K\u0304\u2190 K\u2212 \u03b4\u0303In \u2208 Rn\u00d7n; 9: C\u0304\u2190\u2212 c columns of K\u0304 selected by some column sampling algorithm;\n10: // compute the spectral shifting parameter and the intersection matrix 11: \u03b4ss \u2190\u2212 1 n\u2212rank(C\u0304) ( tr(K)\u2212 tr ( C\u0304\u2020KC\u0304 )) ; 12: Uss \u2190\u2212 C\u0304\u2020K(C\u0304\u2020)T \u2212 \u03b4ss(C\u0304T C\u0304)\u2020; 13: return the SS-Nystro\u0308m approximation K\u0303ssc = C\u0304U ssC\u0304T + \u03b4ssIn.\nmethod by spectral shifting (SS-Nystro\u0308m). In Section 6.1 we formulate and justify the SSNystro\u0308m method. In Section 6.2 we provide upper error bounds of the SS-Nystro\u0308m method. In Section 6.3 we devise an algorithm for efficiently computing the initial spectral shifting parameter. The whole procedure is described in Algorithm 3. We will empirically compare SS-Nystro\u0308m with the standard/modified Nystro\u0308m methods in Section 7."}, {"heading": "6.1 Model Formulation", "text": "As was discussed in Section 1.1.3, when the bottom eigenvalues of a kernel matrix are large, low-rank matrix approximation methods\u2014the standard/modified Nystro\u0308m methods and even the partial eigenvalue decomposition\u2014work poorly. To improve the kernel approximation accuracy, Zhang (2014) proposed a kernel approximation model called the matrix ridge approximation (MRA). MRA approximates any SPSD matrix by K \u2248 AAT + \u03b4In where A is an n \u00d7 c matrix and \u03b4 > 0 is the average of the n \u2212 c bottom eigenvalues. The MRA AAT + \u03b4In has better condition number than K, so it works well no matter whether the bottom eigenvalues are large or small. However, MRA is solved by an iterative algorithm, so it is not pass-efficient. When the kernel matrix does not fit in RAM, MRA becomes inefficient.\nInspired by MRA of Zhang (2014), we propose a novel kernel approximation model which inherits the efficiency of the Nystro\u0308m method and is effective when the bottom eigenvalues are large. We call our model the modified Nystro\u0308m method by spectral shifting (SS-Nystro\u0308m),\nwhich is defined by K\u0303ssc = C\u0304U ssC\u0304T + \u03b4ssIn \u2248 K. (11) Here \u03b4ss and Uss are previously defined in (4), C\u0304 contains c columns of K\u0304 = K\u2212 \u03b4\u0304In, and \u03b4\u0304 is the initial spectral shifting term defined in (3). The following theorem shows some properties of the SS-Nystro\u0308m method.\nTheorem 9 The pair (\u03b4ss,Uss) defined in (4) is the global optimum minimizer of problem (5), which indicates that using any other (\u03b4,U) to replace (\u03b4ss,Uss) results in lower approximation accuracy. Furthermore, if K is positive (semi)definite, then the approximation C\u0304UssC\u0304T + \u03b4ssIn is also positive (semi)definite."}, {"heading": "6.2 Error Analysis", "text": "Directly analyzing the theoretical error bound of SS-Nystro\u0308m is not easy, so we formulate a variant of SS-Nystro\u0308m called the modified Nystro\u0308m method by inexact spectral shifting (ISSNystro\u0308m) and instead analyze the error bound of ISS-Nystro\u0308m. We will provide theoretical error bounds of the ISS-Nystro\u0308m method in Theorem 12 and Corollary 13 in this subsection. It follows from Theorem 9 that ISS-Nystro\u0308m is less accurate than SS-Nysto\u0308m in that\u2225\u2225K\u2212 K\u0303ssc \u2225\u2225F \u2264 \u2225\u2225K\u2212 K\u0303issc \u2225\u2225F , thus the error bounds of ISS-Nystro\u0308m still hold if K\u0303issc is replaced by K\u0303 ss c .\nISS-Nystro\u0308m is defined by\nK\u0303issc = C\u0304U\u0304C\u0304 T + \u03b4In. (12)\nHere \u03b4 > 0 is the spectral shifting term, and C\u0304U\u0304C\u0304T is the modified Nystro\u0308m approximation of K\u0304 = K\u2212 \u03b4In.\nWe first show how to set the spectral shifting term \u03b4. It follows from the definition in (12) directly that the approximation error is K \u2212 K\u0303issc = K\u0304 \u2212 C\u0304U\u0304C\u0304T ; Lemma 1 and Theorem 7 indicate that by selecting sufficiently many columns of K\u0304 to construct C\u0304 and U\u0304, it holds with high probability that\u2225\u2225K\u2212 K\u0303issc \u2225\u2225F = \u2225\u2225K\u0304\u2212 C\u0304U\u0304C\u0304T\u2225\u2225F \u2264 (1 + )\u2225\u2225K\u0304\u2212 K\u0304k\u2225\u2225F . Apparently, for fixed k, the smaller the error \u2016K\u0304 \u2212 K\u0304k\u2016F is, the tighter error bound the ISS-Nystro\u0308m has; if \u2016K\u0304\u2212 K\u0304k\u2016F \u2264 \u2016K\u2212Kk\u2016F , then ISS-Nystro\u0308m has a better error bound than the modified Nystro\u0308m. Therefore our goal is to make \u2016K\u0304\u2212 K\u0304k\u2016F as small as possible, so we formulate the following optimization problem to compute \u03b4:\nmin \u03b4\u22650 \u2225\u2225K\u0304\u2212 K\u0304k\u2225\u22252F ; s.t. K\u0304 = K\u2212 \u03b4In. However, since K\u0304 is in general indefinite, it requires all of the eigenvalues of K to solve the problem exactly. Since computing the full eigenvalue decomposition is expensive, we attempt to relax the problem. Considering that\n\u2225\u2225K\u0304\u2212 K\u0304k\u2225\u22252F = min|J |=n\u2212k\u2211 j\u2208J ( \u03c3j(K)\u2212 \u03b4 )2 \u2264 n\u2211 j=k+1 ( \u03c3j(K)\u2212 \u03b4 )2 , (13)\nwe seek to minimize the upper bound of \u2016K\u0304 \u2212 K\u0304k\u20162F , which is the righthand side of (13), to compute \u03b4, leading to the solution\n\u03b4\u0304 = 1\nn\u2212 k n\u2211 j=k+1 \u03c3j(K) = 1 n\u2212 k ( tr(K)\u2212 k\u2211 j=1 \u03c3j(K) ) . (14)\nNotice that \u03b4\u0304 is also used as the initial spectral shifting term of SS-Nystro\u0308m. If we choose \u03b4 = 0, then ISS-Nystro\u0308m degenerates to the modified Nystro\u0308m method. The following theorem indicates that the ISS-Nystro\u0308m with any \u03b4 \u2208 (0, \u03b4\u0304] has a stronger relative-error bound than the modified Nystro\u0308m method.\nTheorem 10 Give an n\u00d7n SPSD matrix K, we let K\u0304 = K\u2212\u03b4In and \u03b4\u0304 be defined in (14). Then for any \u03b4 \u2208 (0, \u03b4\u0304], the following inequality holds:\u2225\u2225K\u0304\u2212 K\u0304k\u2225\u22252F \u2264 \u2225\u2225K\u2212Kk\u2225\u22252F . Remark 11 Using the same column selection algorithm to sample c columns, if the modified Nystro\u0308m method attains the error bound\u2225\u2225K\u2212 K\u0303modc \u2225\u2225F \u2264 (1 + ) \u2225\u2225K\u2212Kk\u2016F with high probability, then the ISS-Nystro\u0308m method attains the error bound\u2225\u2225K\u2212 K\u0303issc \u2225\u2225F \u2264 (1 + )\u2225\u2225K\u0304\u2212 K\u0304k\u2016F with the same probability. Therefore, due to Theorem 10, the upper error bound of ISSNystro\u0308m is always better than the modified Nystro\u0308m method.\nWe give an example in Figure 2 to illustrate why ISS-Nystro\u0308m is better than the modified Nystro\u0308m method. We use the toy data matrix K: an n \u00d7 n SPSD matrix whose the t-th eigenvalue is 1.05\u2212t. We set n = 100, k = 30, and thus \u03b4\u0304 = 0.064. From the plot of the eigenvalues we can see that the \u201ctail\u201d of the eigenvalues becomes thinner after the spectral shifting. Specifically, \u2016K\u2212Kk\u20162F = 0.52 and \u2016K\u0304\u2212 K\u0304k\u20162F \u2264 0.24. When the same number of columns are selected to construct the ISS-Nystro\u0308m or the modified Nystro\u0308m approximations, ISS-Nystro\u0308m has much tighter error bound because \u2016K\u0304 \u2212 K\u0304k\u20162F is much smaller than \u2016K\u2212Kk\u20162F .\nWe provide error analysis for the ISS-Nystro\u0308m method in Theorem 12, which shows that ISS-Nystro\u0308m always has tighter error bound than the modified Nystro\u0308m method. We also demonstrate in Example 1 that in some cases the ISS-Nystro\u0308m method can be better than any other low-rank matrix approximation methods.\nTheorem 12 Suppose there is a column selection algorithm Acol such that for any n \u00d7 n symmetric matrix S and target rank k ( n), by selecting c \u2265 C(n, k, ) columns of S using algorithm Acol, the modified Nystro\u0308m method attains the error bound\u2225\u2225S\u2212 S\u0303modc \u2225\u22252F \u2264 (1 + )\u2225\u2225S\u2212 Sk\u2225\u22252F .\nThen for any n \u00d7 n SPSD matrix K, we compute \u03b4\u0304 according to (14) and compute K\u0304 = K \u2212 \u03b4\u0304In. By using Acol to select c \u2265 C(n, k, ) columns of K\u0304, the ISS-Nystro\u0308m defined in (5) attains the error bound\n\u2225\u2225K\u2212 K\u0303issc \u2225\u22252F \u2264 (1 + )(\u2225\u2225K\u2212Kk\u2225\u22252F \u2212 [\u2211n i=k+1 \u03bbi(K) ]2\nn\u2212 k\n) .\nIf the columns of K\u0304 are selected by the near-optimal+adaptive column sampling algorithm of Wang and Zhang (2013) which is the best practical algorithm for the modified Nystro\u0308m method, then the error bound incurred by ISS-Nystro\u0308m is given in the following corollary.\nCorollary 13 Suppose we are given an SPSD matrix K and we sample c = O(k \u22122) columns of K\u0304 to form C\u0304 using the near-optimal+adaptive column sampling algorithm (Lemma 1). We run the algorithm t \u2265 (2 \u22121 + 1) log(1/p) times (independently in parallel) and choose the sample that minimizes \u2016K\u0304\u2212 C\u0304 ( C\u0304\u2020K\u0304(C\u0304\u2020)T ) C\u0304T \u2225\u2225 F , then the inequality\n\u2225\u2225K\u2212 K\u0303issc \u2225\u22252F \u2264 (1 + )(\u2016K\u2212Kk\u20162F \u2212 [\u2211n i=k+1 \u03bbi(K) ]2\nn\u2212 k ) holds with probability at least 1\u2212 p.\nRemark 14 If we set the initial spectral shifting term of the SS-Nystro\u0308m method equal to the spectral shifting term of ISS-Nystro\u0308m, that is, SS-Nystro\u0308m and ISS-Nystro\u0308m have the same C\u0304, then it follows directly from Theorem 9 that Theorem 12 and Corollary 13 still hold if K\u0303issc is replaced by K\u0303 ss c .\nUsing the near-optimal+adaptive algorithm\u2014the best practical algorithm for the modified Nystro\u0308m method\u2014to sample c = O(k \u22122) columns, the upper error bound\u2225\u2225K\u2212 K\u0303modc \u2225\u22252F \u2264 (1 + ) \u2225\u2225K\u2212Kk\u2225\u22252F\nholds with high probability (see Lemma 1). When the bottom eigenvalues \u03bbk+1(K), \u00b7 \u00b7 \u00b7 , \u03bbn(K) are large, we can see from Corollary 13 that the error bound of ISS-Nystro\u0308m is much better than that of the modified Nystro\u0308m method. Here we give an example to demonstrate the superiority of SS-Nystro\u0308m over the the standard/modified Nystro\u0308m methods and even the truncated SVD of the same scale.\nExample 1 Let K be an n \u00d7 n SPSD matrix such that \u03bb1(K) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbk(K) > \u03b8 = \u03bbk+1(K) = \u00b7 \u00b7 \u00b7 = \u03bbn(K) > 0. By sampling c = O(k) columns by the near-optimal+adaptive algorithm of Wang and Zhang (2013), we have that\u2225\u2225K\u2212 K\u0303ssc \u2225\u22252F = \u2225\u2225K\u2212 K\u0303issc \u2225\u22252F = 0, and that\n(n\u2212 c)\u03b82 = \u2225\u2225K\u2212Kc\u2225\u22252F \u2264 \u2225\u2225K\u2212 K\u0303modc \u2225\u22252F \u2264 \u2225\u2225K\u2212 K\u0303nysc \u2225\u22252F .\nIn this example SS-Nystro\u0308m is far better than the other approximation methods if we set \u03b8 a large constant."}, {"heading": "6.3 Efficient Algorithm for Computing \u03b4\u0304", "text": "The SS-Nystro\u0308m method uses \u03b4\u0304 as the initial spectral shifting term. However, computing \u03b4\u0304 according to (14) requires the partial eigenvalue decomposition which costs time O(n2k) and space O(n2). This can be accelerated by computing the top-k singular values approximately using random projection techniques (Boutsidis et al., 2011, Halko et al., 2011). We depict the algorithm for approximately computing \u03b4\u0304 using random projections in Lines 2\u20136 of Algorithm 3. The performance of the approximation is analyzed in the following theorem.\nTheorem 15 Let \u03b4\u0304 be defined in (14) and \u03b4\u0303, k, l, n be defined in Algorithm 3. The following inequality holds in expectation:\nE [\u2223\u2223\u03b4\u0304 \u2212 \u03b4\u0303\u2223\u2223 / \u03b4\u0304] \u2264 k/\u221al,\nwhere the expectation is taken w.r.t. the Gaussian random matrix \u2126 in Algorithm 3. Lines 2\u20136 in Algorithm 3 compute \u03b4\u0303 in time O(nl2) + Tmultiply(n2l) and space O(nl).\nBy using Algorithm 3 to compute \u03b4\u0304 approximately, it costs only O(nl2) + Tmultiply(n2l) more time to compute the SS-Nystro\u0308m approximation than the modified Nystro\u0308m approximation.\nWe evaluate the accuracy of the approximation to \u03b4\u0304 (Lines 2\u20136 in Algorithm 3) proposed in Theorem 15. We generate RBF kernel matrices of the datasets listed in Table 2, and we set the scaling parameter \u03b3 such that \u03b7 defined in (15) equals to 0.5 or 0.9. The details of experiment settings are described later in Section 7.1. We use the error ratio |\u03b4\u0304 \u2212 \u03b4\u0303|/\u03b4\u0304 to evaluate the approximation performance. We repeat the experiments 20 times and plot the average error ratio versus l/k in Figure 3. Here \u03b4\u0303, l, and k are defined in Theorem 15.\nWe can see from Figure 3 that the approximation of \u03b4\u0304 is of very high quality: when l = 4k, the error ratios are less than 0.03 in all cases, no matter whether the spectrum\nof K decays fast or slow. So we set l = 4k in all of the subsequent kernel approximation experiments in order to obtain a low over-sampling rate with a high accuracy at the same time. Since it costs O(nc2) + Tmultiply(n2c) time to compute the modified Nystro\u0308m (in\ngeneral) and c should be set as O(k \u22122), if we set l = 4k, then the time complexity of computing SS-Nystro\u0308m is the same as computing the modified Nystro\u0308m."}, {"heading": "7. Experiments on Kernel Approximation", "text": "In this section we empirically compare between the three Nystro\u0308m approximation models: the standard Nystro\u0308m method of Nystro\u0308m (1930), Williams and Seeger (2001), the modified Nystro\u0308m method of Wang and Zhang (2013), and the SS-Nystro\u0308m method proposed in this work. We also evaluate our uniform+adaptive2 column sampling algorithm proposed in Section 5.2; the uniform sampling and the near-optimal+adaptive column sampling algorithm of Wang and Zhang (2013) are employed for comparison."}, {"heading": "7.1 The Setup", "text": "We perform experiments on several datasets released by UCI (Frank and Asuncion, 2010) and Statlog (Michie et al., 1994). We obtain the data collected on the LIBSVM website3 where the data are scaled to [0,1]. We summarize the datasets in Table 2. For each dataset, we generate a radial basis function (RBF) kernel matrix K defined by kij = exp(\u2212 12\u03b3 \u2016xi \u2212 xj\u2016 2 2). Here \u03b3 > 0 is the scaling parameter; the larger the scaling parameter \u03b3 is, the faster the spectrum of the kernel decays (Gittens and Mahoney, 2013). Experience from previous work indicates that for the same dataset, with different settings of \u03b3, the Nystro\u0308m methods and the sampling algorithms can have very different performance.\nHere we discuss how to set \u03b3. Let p = d0.05ne, we define\n\u03b7 ,\n\u2211p i=1 \u03bb\n2 i (K)\u2211n\ni=1 \u03bb 2 i (K) = \u2016Kp\u20162F \u2016K\u20162F , (15)\nwhich denotes the weight of the top 5% eigenvalues of the kernel matrix K. In general large \u03b3 results in large \u03b7. For each dataset, we use two different settings of \u03b3 such that \u03b7 = 0.5 or \u03b7 = 0.9. Obviously, when \u03b7 is small, the bottom eigenvalues of K are large, and our SS-Nystro\u0308m is in a favorable position.\nFor each of the three modes , that is, the standard/modified/SS Nystro\u0308m methods, we use three column selection algorithms: the uniform sampling, the uniform+adaptive2 (Algorithm 1), and the near-optimal+adaptive algorithm of Wang and Zhang (2013). We implement all the algorithms in MATLAB and run the algorithms on a workstation with Intel Xeon 2.40GHz CPUs, 24GB RAM, and 64bit Windows Server 2008 system. To compare the running time, we set MATLAB in single thread mode by the command \u201cmaxNumCompThreads(1)\u201d. For the target rank k used throughout this paper, we set k = dn/100e.\n3. http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/\nWe report the approximation errors and running time of each algorithm for each method. The approximation accuracy is evaluated by\nApproximation Error = \u2016K\u2212 K\u0303\u2016F /\u2016K\u2016F ,\nwhere K\u0303 is the approximation generated by each method. Every time when we do column sampling, we run each sampling algorithm 10 times and report the minimal approximation error of the 10 repeats. We report the average elapsed time of the 10 repeat rather than the total elapsed time because the 10 repeats can be done in parallel on 10 machines. For the kernel matrices with \u03b7 = 0.5 (\u03b7 is defined in (15)), the approximation errors and the average running time are depicted in Figures 4 and 6 respectively. For the kernel matrices with \u03b7 = 0.9, the approximation errors are depicted in Figures 5; the curve of the running time is very similar to Figure 6, so we do not show it here."}, {"heading": "7.2 Comparisons among the Kernel Approximation Models", "text": "The experiments show that our SS-Nystro\u0308m achieves the highest kernel approximation accuracy among the three Nystro\u0308m approximation methods. For the kernel matrices with \u03b7 = 0.5 where the spectrum decays slowly and the bottom eigenvalues are large, our SS-Nystro\u0308m method is tremendously more accurate than the standard/modified Nystro\u0308m methods, which is in accordance with our theoretical analysis. When \u03b7 = 0.9 where the spectrum of kernel matrix decays fast, our SS-Nystro\u0308m method is still more accurate than the other two methods, but the advantage is not as evident as the \u03b7 = 0.5 cases.\nAs for the running time, our SS-Nystro\u0308m is a little slower than the modified Nystro\u0308m because SS-Nystro\u0308m needs to compute \u03b4\u0304 approximately by randomized SVD, which costs time O(nk2)+Tmultiply(n2k) (as we set l = 4k). Since it costs time O(nc2)+Tmultiply(n2c) to compute the modified Nystro\u0308m approximation, so our SS-Nystro\u0308m should be only constant times slower than the modified Nystro\u0308m; this is verified by experiments."}, {"heading": "7.3 Comparisons among the Column Selection Algorithms", "text": "The empirical results in the figures show that our uniform+adaptive2 algorithm achieves accuracy comparable with the state-of-the-art algorithm\u2014the near-optimal+adaptive algorithm of Wang and Zhang (2013). Especially, when c is large, those two algorithms have virtually the same accuracy, which is in accordance with our analysis in the last paragraph of Section 5.2: large c implies small error term , and the error bounds of the two algorithms coincide when is small.\nAs for the running time, we can see that our uniform+adaptive2 algorithm performs column selection very efficiently and the elapsed time grows slowly in c. In comparison, our algorithm is much more efficient than the near-optimal+adaptive algorithm."}, {"heading": "8. Concluding Remarks", "text": "In this paper we have provided a comprehensive study of the modified Nystro\u0308m method. First, we have proved that the modified Nystro\u0308m approximation is exact when the original matrix is low-rank. We have also established a lower error bound for the modified Nystro\u0308m method: at least c \u2265 2k \u22121 columns must be chosen to attain the 1 + bound. We have conjectured this lower error bound to be tight. Notice that the best known algorithm for the modified Nystro\u0308m method requires at most c = k \u22122 columns to attain the 1 + bound, so there is a gap between the lower and upper error bounds. It remains an open problem whether there exists an algorithm attaining the lower error bound or not. It is worthy of mentioning that the very recent work (Boutsidis and Woodruff, 2014) showed that O(k \u22121) columns and rows suffice to achieve 1 + relative-error bound for the CUR matrix decomposition problem, which is a generalization of modified Nystro\u0308m method to general rectangular matrices.\nSecond, we have devised a column selection algorithm called uniform+adaptive2 and provided a relative-error bound for the algorithm. The algorithm is highly efficient and effective as well as very easy to implement. The error bound of the algorithm is nearly as strong as that of the state-of-the-art algorithm\u2014the near-optimal+adaptive algorithm\u2014 which is complicated and difficult to implement. The experimental results have shown that our uniform+adaptive2 algorithm is more efficient than the near-optimal+adaptive algorithm, while their accuracies are comparable. We have also devised an algorithm for computing the intersection matrix of the modified Nystro\u0308m approximation; under certain conditions, our algorithm can significantly improve the time complexity. The speedup induced by this algorithm has also been verified empirically.\nThird, to improve the kernel approximation accuracy when the spectrum of the kernel matrix decays slowly, we have proposed an extension of the modified Nystro\u0308m method called the SS-Nystro\u0308m method. The SS-Nystro\u0308m method can speedup many kernel methods in the same way as the standard/modified Nystro\u0308m methods. We have shown that SSNystro\u0308m has a much stronger error bound than the standard/modified Nystro\u0308m methods. Especially, when the bottom eigenvalues of a kernel matrix are not sufficiently small, the approximation accuracy of the standard/modified Nystro\u0308m method or even the truncated SVD is unsatisfactory, while our SS-Nystro\u0308m can still generate approximations of high accuracy. We have also devised an algorithm for computing SS-Nystro\u0308m efficiently. The experiments have further demonstrated that our SS-Nystro\u0308m method is tremendously more accurate than the standard/modified Nystro\u0308m methods when the bottom eigenvalues of the kernel matrix are large.\nTo summarize, the modified/SS Nystro\u0308m method are much more accurate than the standard Nystro\u0308m method, both theoretically and empirically; but the modified/SS Nystro\u0308m methods are slower to compute. If users want higher kernel approximation accuracy, we suggest using the modified Nystro\u0308m method or the SS-Nystro\u0308m method. If the spectrum of the kernel matrix decays very fast, then there is little difference between the modified Nystro\u0308m method and the SS-Nystro\u0308m method. However, if the spectrum decays slowly, we strongly recommend using the SS-Nystro\u0308m method, because neither of the standard/modfied Nystro\u0308m method can achieve satisfactory accuracy. As for the column selection, we recommend using the uniform+adaptive2 algorithm proposed in this paper."}, {"heading": "Appendix A. Proof of Theorem 2", "text": "Proof Suppose that rank(W) = rank(K). We have that rank(W) = rank(C) = rank(K) because rank(K) \u2265 rank(C) \u2265 rank(W). (16) Thus there exists a matrix X such that[\nKT21 K22\n] = CXT = [ WXT\nK21X T\n] ,\nand it follows that K21 = XW and K22 = K21X T = XWXT . Then we have that\nK =\n[ W (XW)T\nXW XWXT\n] = [ I X ] W [ I XT ] , (17)\nCW\u2020CT =\n[ W\nXW\n] W\u2020 [ W (XW)T ] = [ I X ] W [ I XT ] . (18)\nHere the second equality in (18) follows from WW\u2020W = W. We obtain that K = CW\u2020C. Then we show that K = CC\u2020K(C\u2020)TCT .\nSince C\u2020 = (CTC)\u2020CT , we have that\nC\u2020 = ( W(I + XTX)W )\u2020 W [I , XT ],\nand thus C\u2020K(C\u2020)TW = ( W(I + XTX)W )\u2020 W(I + XTX) [ W(I + XTX)W ( W(I + XTX)W )\u2020 W ]\n= ( W(I + XTX)W )\u2020 W(I + XTX)W,\nwhere the second equality follows from Lemma 16 because (I + XTX) is positive definite. Similarly we have\nWC\u2020K(C\u2020)TW = W ( W(I + XTX)W )\u2020 W(I + XTX)W = W.\nThus we have\nCC\u2020K(C\u2020)TC = [ I X ] WC\u2020K(C\u2020)TW [ I XT ] = [ I X ] W [ I XT ] . (19)\nIt follows from Equations (17) (18) (19) that K = CW\u2020CT = CC\u2020K(C\u2020)TCT . Conversely, when K = CW\u2020CT , we have that rank(K) \u2264 rank(W\u2020) = rank(W). By applying (16) we have that rank(K) = rank(W). When K = CC\u2020K(C\u2020)TCT , we have rank(K) \u2264 rank(C). Thus there exists a matrix\nX such that [ KT21 K22 ] = CXT = [ WXT K21X T ] ,\nand therefore K21 = XW. Then we have that\nC = [ W K21 ] = [ I X ] W,\nso rank(C) \u2264 rank(W). Apply (16) again we have rank(K) = rank(W).\nLemma 16 XTVX ( XTVX )\u2020 XT = XT for any positive definite matrix V.\nProof Since the positive definite matrix V have a decomposition V = BTB for some nonsingular matrix B, so we have\nXTVX ( XTVX )\u2020 XT = (BX)T ( BX ( (BX)T (BX) )\u2020) (BX)TB(BTB)\u22121\n= (BX)T ( (BX)T )\u2020 (BX)T (BT )\u22121 = (BX)T (BT )\u22121 = XT ."}, {"heading": "Appendix B. Proof of Theorem 3", "text": "In Section B.1 we provide several key lemmas, and then in Section B.2 we prove Theorem 3 using Lemmas 19 and 18.\nB.1 Key Lemmas\nLemma 17 provides a useful tool for expanding the Moore-Penrose inverse of partitioned matrices, and the lemma will be used to prove Lemma 19 and Theorem 3.\nLemma 17 (Page 179 of Ben-Israel and Greville (2003)) Given a matrix X \u2208 Rm\u00d7n of rank of at least c which has a nonsingular c \u00d7 c submatrix X11. By rearrangement of columns and rows by permutation matrices P and Q, the submatrix X11 can be bought to the top left corner of X, that is,\nPXQ = [ X11 X12 X21 X22 ] .\nThen the Moore-Penrose inverse of X is\nX\u2020 = Q [ Ic TT ] ( Ic + TT T )\u22121 X\u2212111 ( Ic + SS T )\u22121 [ Ic S T ] P,\nwhere T = X\u2212111 X12 and S = X21X \u22121 11 .\nLemmas 18 and 19 will be used to prove Theorem 3.\nLemma 18 (Lemma 19 of Wang and Zhang (2013)) Given n and k, we let B be an n k\u00d7 n k matrix whose diagonal entries equal to one and off-diagonal entries equal to \u03b1 \u2208 [0, 1). We let A be an n\u00d7 n block-diagonal matrix\nA = diag(B, \u00b7 \u00b7 \u00b7 ,B\ufe38 \ufe37\ufe37 \ufe38 k blocks ). (20)\nLet Ak be the best rank-k approximation to the matrix A, then we have that\n\u2016A\u2212Ak\u2016F = (1\u2212 \u03b1) \u221a n\u2212 k.\nLemma 19 For an n \u00d7 n matrix B with diagonal entries equal to one and off-diagonal entries equal to \u03b1, the error incurred by the modified Nystro\u0308m method is lower bounded by\n\u2016B\u2212 B\u0303modc \u20162F \u2265 (1\u2212 \u03b1)2(n\u2212 c) ( 1 + 2\nc \u2212 (1\u2212 \u03b1)1 + o(1) \u03b1cn/2\n) .\nProof Without loss of generality, we assume the first c column of B are selected to construct C. We partition B and C as:\nB = [ W BT21 B21 B22 ] and C = [ W B21 ] .\nHere the matrix W can be expressed by W = (1\u2212 \u03b1)Ic + \u03b11c1Tc . We apply the ShermanMorrison-Woodbury formula\n(X + YZR)\u22121 = X\u22121 \u2212X\u22121Y(Z\u22121 + RX\u22121Y)\u22121RX\u22121\nto compute W\u22121, yielding\nW\u22121 = 1\n1\u2212 \u03b1 Ic \u2212\n\u03b1\n(1\u2212 \u03b1)(1\u2212 \u03b1+ c\u03b1) 1c1\nT c . (21)\nWe expand the Moore-Penrose inverse of C by Lemma 17 and obtain C\u2020 = W\u22121 ( Ic + S TS )\u22121 [ Ic S T ]\nwhere S = B21W \u22121 = \u03b1\n1\u2212 \u03b1+ c\u03b1 1n\u2212c1\nT c .\nIt is easily verified that STS = (\n\u03b1 1\u2212\u03b1+c\u03b1 )2 (n\u2212 c)1c1Tc .\nNow we express the matrix constructed by the modified Nystro\u0308m method in a partitioned form:\nB\u0303modc = CC \u2020B ( C\u2020 )T CT\n= [ W B21 ] W\u22121 ( Ic + S TS )\u22121 [ Ic S T ] B [ Ic S ] ( Ic + S TS )\u22121 W\u22121 [ W B21 ]T = [ ( Ic + S TS )\u22121\nB21W \u22121(Ic + STS)\u22121\n] [ Ic S T ] B [ Ic S ][ ( Ic + S TS )\u22121 B21W \u22121(Ic + STS)\u22121 ]T . (22)\nWe then compute the submatrices ( Ic + S TS )\u22121 and B21W \u22121(Ic + STS)\u22121 respectively\nas follows. We apply the Sherman-Morrison-Woodbury formula to compute ( Ic + S TS )\u22121\n, yielding\n( Ic + S TS )\u22121 = ( Ic + ( \u03b1 1\u2212 \u03b1+ c\u03b1 )2 (n\u2212 c)1c1Tc )\u22121 = Ic \u2212 \u03b311c1Tc , (23)\nwhere\n\u03b31 = n\u2212 c nc+ (\n1\u2212\u03b1 \u03b1 )2 + 2(1\u2212\u03b1)c\u03b1 .\nIt follows from (21) and (23) that W\u22121 ( Ic + S TS )\u22121 = (\u03b32Ic \u2212 \u03b331c1Tc )(Ic \u2212 \u03b311c1Tc ) = \u03b32Ic + (\u03b31\u03b33c\u2212 \u03b31\u03b32 \u2212 \u03b33)1c1Tc ,\nwhere\n\u03b32 = 1\n1\u2212 \u03b1 and \u03b33 =\n\u03b1\n(1\u2212 \u03b1)(1\u2212 \u03b1+ \u03b1c) .\nThen we have that B21W \u22121(Ic + STS)\u22121 = \u03b1(\u03b31\u03b33c2 \u2212 \u03b33c\u2212 \u03b31\u03b32c+ \u03b32)1n\u2212c1Tc , \u03b31n\u2212c1Tc , (24)\nwhere \u03b3 = \u03b1 ( \u03b31\u03b33c 2 \u2212 \u03b33c\u2212 \u03b31\u03b32c+ \u03b32 ) = \u03b1(\u03b1c\u2212 \u03b1+ 1)\n2\u03b1c\u2212 2\u03b1\u2212 2\u03b12c+ \u03b12 + \u03b12cn+ 1 . (25)\nSince B21 = \u03b11n\u2212c1 T c and B22 = (1\u2212 \u03b1)In\u2212c + \u03b11n\u2212c1Tn\u2212c, it is easily verified that[\nIc S T ] B [ Ic S ] = [ Ic S T ] [ W BT21\nB21 B22\n] [ Ic S ] = (1\u2212 \u03b1)Ic + \u03bb1c1Tc , (26)\nwhere\n\u03bb = \u03b1(3\u03b1n\u2212 \u03b1c\u2212 2\u03b1+ \u03b12c\u2212 3\u03b12n+ \u03b12 + \u03b12n2 + 1)\n(\u03b1c\u2212 \u03b1+ 1)2\nIt follows from (22), (23), (24), and (26) that\nB\u0303modc = [ Ic \u2212 \u03b311c1Tc \u03b31n\u2212c1 T c ]( (1\u2212 \u03b1)Ic + \u03bb1c1Tc )[ Ic \u2212 \u03b311c1Tc \u03b31n\u2212c1 T c ]T , [ B\u030311 B\u0303 T 21 B\u030321 B\u030322 ] ,\nwhere\nB\u030311 = (1\u2212 \u03b1)Ic + [ (1\u2212 \u03b31c)(\u03bb\u2212 \u03bb\u03b31c\u2212 (1\u2212 \u03b1)\u03b31)\u2212 (1\u2212 \u03b1)\u03b31 ] 1c1 T c = (1\u2212 \u03b1)Ic + \u03b711c1Tc , B\u030321 = A\u0303 T 12 = \u03b3(1\u2212 \u03b31c)(1\u2212 \u03b1+ \u03bbc)1n\u2212c1Tc = \u03b721n\u2212c1Tc , B\u030322 = \u03b3 2c(1\u2212 \u03b1+ \u03bbc)1n\u2212c1Tn\u2212c = \u03b731n\u2212c1Tn\u2212c,\nwhere\n\u03b71 = (1\u2212 \u03b31c)(\u03bb\u2212 \u03bb\u03b31c\u2212 (1\u2212 \u03b1)\u03b31)\u2212 (1\u2212 \u03b1)\u03b31, \u03b72 = \u03b3(1\u2212 \u03b31c)(1\u2212 \u03b1+ \u03bbc), \u03b73 = \u03b3 2c(1\u2212 \u03b1+ \u03bbc),\nBy dealing with the four blocks of B\u0303modc respectively, we finally obtain that\n\u2016B\u2212 B\u0303modc \u20162F = \u2016W \u2212 B\u030311\u20162F + 2\u2016B21 \u2212 B\u030321\u20162F + \u2016B22 \u2212 B\u030322\u20162F = c2(\u03b1\u2212 \u03b71)2 + 2c(n\u2212 c)(\u03b1\u2212 \u03b72)2\n+(n\u2212 c)(n\u2212 c\u2212 1)(\u03b1\u2212 \u03b73)2 + (n\u2212 c)(1\u2212 \u03b73)2 = (n\u2212 c)(\u03b1\u2212 1)2 ( 1 + 2 c \u2212 ( 1 + o(1) ) 1\u2212 \u03b1 \u03b1cn/2 ) .\nB.2 Proof of the Theorem\nNow we prove Theorem 3 using Lemma 19 and Lemma 18. Let C consist of c column sampled from A and C\u0302i consist of ci columns sampled from the i-th block diagonal matrix in A. Without loss of generality, we assume C\u0302i consists of the first ci columns of B. Then the intersection matrix U is computed by\nU = C\u2020A ( CT )\u2020 = [ diag ( C\u03021, \u00b7 \u00b7 \u00b7 , C\u0302k )]\u2020 A [ diag ( C\u0302T1 , \u00b7 \u00b7 \u00b7 , C\u0302Tk )]\u2020 = diag ( C\u0302\u20201B ( C\u0302\u20201 )T , \u00b7 \u00b7 \u00b7 , C\u0302\u2020kB ( C\u0302\u2020k )T) .\nThe modified Nystro\u0308m approximation of A is\nA\u0303modc = CUC T = diag ( C\u03021C\u0302 \u2020 1B ( C\u0302\u20201 )T C\u0302T1 , \u00b7 \u00b7 \u00b7 , C\u0302kC\u0302 \u2020 kB ( C\u0302\u2020k )T C\u0302Tk ) ,\nand thus the approximation error is\n\u2225\u2225A\u2212 A\u0303modc \u2225\u22252F = k\u2211 i=1 \u2225\u2225\u2225B \u2212 C\u0302iC\u0302\u2020iB(C\u0302\u2020i)T C\u0302Ti \u2225\u2225\u22252 F\n\u2265 (1\u2212 \u03b1)2 k\u2211 i=1 (p\u2212 ci) ( 1 + 2 ci \u2212 (1\u2212 \u03b1) (1 + o(1) \u03b1cip/2 ))\n= (1\u2212 \u03b1)2 ( k\u2211 i=1 (p\u2212 ci) + k\u2211 i=1 2(p\u2212 ci) ci ( 1\u2212 (1\u2212 \u03b1)(1 + o(1)) \u03b1p )) \u2265 (1\u2212 \u03b1)2(n\u2212 c) ( 1 + 2k\nc\n( 1\u2212 k(1\u2212 \u03b1)(1 + o(1))\n\u03b1n\n)) ,\nwhere the former inequality follows from Lemma 19, and the latter inequality follows by minimizing over c1, \u00b7 \u00b7 \u00b7 , ck. Finally we apply Lemma 18, and the theorem follows by setting \u03b1\u2192 1."}, {"heading": "Appendix C. Proof of Theorem 4", "text": "Proof Let C \u2208 Rm\u00d7c consists of a subset of columns of K. By row permutation C can be expressed as\nPC = [ W K21 ] .\nThen according to Lemma 17, the Moore-Penrose inverse of C can be written as\nC\u2020 = W\u22121 ( Ic + S TS )\u22121 [ Ic S T ] P,\nwhere S = K21W \u22121. Then the intersection matrix of modified Nystro\u0308m approximation to K can be expressed as\nU = C\u2020K ( C\u2020 )T\n= W\u22121 ( Ic + S TS )\u22121 [ Ic S T ] PKPT [ Ic S ] ( Ic + S TS )\u22121 W\u22121\n= W\u22121 ( Ic + S TS )\u22121 [ Ic S T ] [ W KT21\nK21 K22\n] [ Ic S ] ( Ic + S TS )\u22121 W\u22121\n= W\u22121 ( Ic + S TS )\u22121( W + KT21S + (K T 21S) T + STK22S )( Ic + S TS )\u22121 W\u22121\n, T1 ( W + T2 + T T 2 + T3 ) TT1 .\nHere the intermediate matrices are computed by\nT0 = K T 21K21, T1 = W \u22121(Ic + STS)\u22121 = W\u22121(Ic + W\u22121T0W\u22121)\u22121,\nT2 = K T 21S = K T 21K21W \u22121 = T0W \u22121, T3 = S TK22S = W \u22121(KT21K22K21)W\u22121. The matrix inverse operations are on c \u00d7 c matrices which costs O(c3) time. The matrix multiplication KT21K22K21 requires time TMultiply ( (n\u2212 c)2c ) ."}, {"heading": "Appendix D. Proof of Theorem 7", "text": "The error analysis for the uniform+adaptive2 algorithm relies on Lemma 20, which guarantees the error incurred by its uniform sampling step. The proof of Lemma 20 essentially follows Gittens (2011). We prove Lemma 20 using probability inequalities and some techniques of Boutsidis et al. (2011), Gittens (2011), Gittens and Mahoney (2013), Tropp (2012); the proof is in Appendix D.1.\nLemma 20 (Uniform Column Sampling) Given an m\u00d7n matrix A and a target rank k, let \u00b5k denote the matrix coherence of A. By sampling\nc = \u00b5kk log(k/\u03b4)\n\u03b8 log \u03b8 \u2212 \u03b8 + 1 ,\ncolumns uniformly without replacement to construct C, the following inequality\u2225\u2225A\u2212 PC,kA\u2225\u22252F \u2264 (1 + \u03b4\u22121\u03b8\u22121)\u2225\u2225A\u2212Ak\u2225\u22252F . holds with probability at least 1 \u2212 2\u03b4. Here \u03b4 \u2208 (0, 0.5) and \u03b8 \u2208 (0, 1) are arbitrary real numbers.\nThe error analysis for the two adaptive sampling steps of the uniform+adaptive2 algorithm relies on Lemma 21, which follows immediately from (Wang and Zhang, 2013, Corollary 7 and Section 4.5).\nLemma 21 Given an n\u00d7n symmetric matrix K and a target rank k, we let C1 contain the c1 columns of K selected by a column sampling algorithm such that the following inequality holds: \u2225\u2225K\u2212 PC1K\u2225\u22252F \u2264 f\u2225\u2225K\u2212Kk\u2225\u22252F . Then we select c2 = kf \u22121 columns to construct C2 and c3 = (c1 + c2) \u22121 columns to construct C3, both using the adaptive sampling according to the residual B1 = K \u2212 PC1K and B2 = K\u2212 P[C1,C2]K, respectively. Let C = [C1,C2,C3], we have that\nP {\u2225\u2225K\u2212C(C\u2020K(C\u2020)T )CT\u2225\u2225 F\u2225\u2225K\u2212Kk\u2225\u2225F \u2265 1 + s } \u2264 1 + 1 + s ,\nwhere s is an arbitrary constant greater than 1.\nFinally Theorem 7 is proved by combining Lemma 20 and Lemma 21. The proof is in Appendix D.2.\nD.1 Proof of Lemma 20\nProof We use uniform column sampling to select c column of A to construct C = AS. Here the n\u00d7 c random matrix S has one entry equal to one and the rest equal to zero in each column, and at most one nonzero entry in each row, and S is uniformly distributed among (nc ) such kind of matrices. Applying Lemma 7 of Boutsidis et al. (2011), we get\u2225\u2225A\u2212 PC,kA\u2225\u22252F \u2264 \u2225\u2225A\u2212Ak\u2225\u22252F + \u2225\u2225(A\u2212Ak)S\u2225\u22252F \u2225\u2225(VTA,kS)\u2020\u2225\u222522. (27) Now we bound\n\u2225\u2225(A\u2212Ak)S\u2225\u222522 and \u2225\u2225(VTA,kS)\u2020\u2225\u222522 respectively using the techniques of Gittens (2011), Gittens and Mahoney (2013), Tropp (2012).\nLet I \u2282 [n] be a random index set corresponding to S. The support of I is uniformly distributing among all the index sets in 2[n] with cardinality c. According to Gittens and Mahoney (2013), the expectation of \u2225\u2225(A\u2212Ak)S\u2225\u22252F can be written as E \u2225\u2225(A\u2212Ak)S\u2225\u22252F = E\u2225\u2225(A\u2212Ak)I\u2225\u22252F = cE\u2225\u2225(A\u2212Ak)i\u2225\u22252F = cn\u2225\u2225A\u2212Ak\u2225\u22252F .\nApplying Markov\u2019s inequality, we have that\nP {\u2225\u2225(A\u2212Ak)S\u2225\u22252F \u2265 cn\u03b4\u2225\u2225A\u2212Ak\u2225\u22252F } \u2264 E \u2225\u2225(A\u2212Ak)S\u2225\u22252F c n\u03b4\n\u2225\u2225A\u2212Ak\u2225\u22252F = \u03b4. (28) Here \u03b4 \u2208 (0, 0.5) is a real number defined later.\nNow we establish the bound for E \u2225\u2225\u2126\u20202\u2225\u222522 as follows. Let \u03bbi(X) be the i-th largest\neigenvalue of X. Following the proof of Lemma 1 of Gittens (2011), we have\n\u2225\u2225(VTA,kS)\u2020\u2225\u222522 = \u03bb\u22121k (VTA,kSSTVA,k) = \u03bb\u22121k ( c\u2211 i=1 Xi ) \u2264 \u03bb\u22121min ( c\u2211 i=1 Xi ) , (29)\nwhere the random matrices X1, \u00b7 \u00b7 \u00b7 ,Xc are chosen uniformly at random from the set{( VTA,k ) i ( VTA,k )T i }n i=1\nwithout replacement. The random matrices are of size k \u00d7 k. We accordingly define\nR = max i \u03bbmax(Xi) = max i \u2225\u2225(VTA,k)i\u2225\u222522 = kn\u00b5k, where \u00b5k is the matrix coherence of A, and define\n\u03b2min = c\u03bbmin ( EX1 ) = \u03bbmin ( c n VTA,kVA,k ) = c n .\nThen we apply Lemma 22 and obtained the following inequality:\nP [ \u03bbmin ( c\u2211 i=1 Xi ) \u2264 \u03b8c n ] \u2264 k [ e\u03b8\u22121 \u03b8\u03b8 ] c k\u00b5k , \u03b4, (30)\nwhere \u03b8 \u2208 (0, 1] is a real number, and it follows that\nc = \u00b5kk log(k/\u03b4)\n\u03b8 log \u03b8 \u2212 \u03b8 + 1 .\nApplying (29) and (30), we have P {\u2225\u2225(VTA,kS)\u2020\u2225\u222522 \u2265 n\u03b8c} \u2264 \u03b4. (31)\nCombining (28) and (31) and applying the union bound, we have the following inequality: P {\u2225\u2225(A\u2212Ak)S\u2225\u22252F \u2265 cn\u03b4\u2225\u2225A\u2212Ak\u2225\u22252F or \u2225\u2225(VTA,kS)\u2020\u2225\u222522 \u2265 n\u03b8c } \u2264 2\u03b4. (32)\nFinally, from (27) and (32) we have that the inequality\u2225\u2225A\u2212 PC,kA\u2225\u22252F \u2264 (1 + \u03b4\u22121\u03b8\u22121)\u2225\u2225A\u2212Ak\u2225\u22252F holds with probability at least 1\u2212 2\u03b4, by which the lemma follows.\nLemma 22 (Theorem 2.2 of Tropp (2012)) We are given l independent random d\u00d7 d SPSD matrices X1, \u00b7 \u00b7 \u00b7 ,Xl with the property\n\u03bbmax(Xi) \u2264 R for i = 1, \u00b7 \u00b7 \u00b7 , l.\nWe define Y = \u2211l i=1 Xi and \u03b2min = l\u03bbmin ( EX1 ) . Then for any \u03b8 \u2208 (0, 1], the following inequality holds:\nP { \u03bbmin(Y) \u2264 \u03b8\u03b2min } \u2264 d [ e\u03b8\u22121\n\u03b8\u03b8\n]\u03b2min R\n.\nD.2 Proof of the Theorem\nProof The matrix C1 consists of c1 columns selected by uniform sampling, and C2 \u2208 Rn\u00d7c2 and C3 \u2208 Rn\u00d7c3 are constructed by adaptive sampling. We set \u03b4 = 1/ \u221a 5 and \u03b8 = \u221a 5/4 for Lemma 20, then we have\nf = 1 + \u03b4\u22121\u03b8\u22121 = 5,\nc1 = \u00b5kk log(k/\u03b4)\n\u03b8 log \u03b8 \u2212 \u03b8 + 1 = 8.7\u00b5kk log(\n\u221a 5k).\nThen we set\nc2 = kf \u22121 = 5k \u22121, and c3 = (c1 + c2) \u22121,\naccording to Lemma 21. Letting s > 1 be an arbitrary constant, we have that\nP {\u2225\u2225K\u2212CUCT\u2225\u2225 F\u2225\u2225K\u2212Kk\u2225\u2225F \u2264 1 + s }\n\u2265 P {\u2225\u2225K\u2212CUCT\u2225\u2225 F\u2225\u2225K\u2212Kk\u2225\u2225F \u2264 1 + s \u2223\u2223\u2223\u2223\u2223 \u2225\u2225K\u2212 PC1K\u2225\u22252F\u2225\u2225K\u2212Kk\u2225\u22252F \u2264 f } \u00b7 P {\u2225\u2225K\u2212 PC1K\u2225\u22252F\u2225\u2225K\u2212Kk\u2225\u22252F \u2264 f }\n\u2265 (\n1\u2212 1 + 1 + s\n)( 1\u2212 2\u03b4 ) .\nwhere the last inequality follows from Lemma 20 and Lemma 21.\nRepeating the sampling procedure for t times and letting C[i] and U[i] be the i-th sample, we obtain an upper error bound on the failure probability:\nP { min i\u2208[t] {\u2225\u2225K\u2212C[i]U[i]CT[i]\u2225\u2225F\u2225\u2225K\u2212Kk\u2225\u2225F } \u2265 1 + s } \u2264 ( 1\u2212 ( 1\u2212 1 + 1 + s )( 1\u2212 2\u03b4 ))t = ( 1 +\n(s\u2212 1)(1\u2212 2\u03b4) \u22121 + 1 + 2\u03b4(s\u2212 1)\n)\u2212t , p.\nTaking logarithm of both sides of the equality and applying log(1 +x) \u2248 x when x is small, we have\nt = [ log ( 1 + (1\u2212 2\u03b4)(s\u2212 1)\n\u22121 + 1 + 2\u03b4(s\u2212 1)\n)]\u22121 log 1\np \u2248 \u22121 + 1 + 2\u03b4(s\u2212 1) (1\u2212 2\u03b4)(s\u2212 1) log 1 p .\nSetting s = 2, we have that t \u2248 (10 \u22121 + 18) log(1/p). Hence by sampling totally\nc = ( 1 + \u22121 )( 5k \u22121 + 8.7\u00b5kk log( \u221a 5k) )\ncolumns and repeating the procedure for\nt \u2265 (10 \u22121 + 18) log(1/p)\ntimes, the algorithm attains the upper error bound\u2225\u2225K\u2212C(C\u2020K(C\u2020)T )CT\u2225\u2225 F \u2264 ( 1 + 2 )\u2225\u2225K\u2212Kk\u2225\u2225F with probability at least 1\u2212 p. Substituting 2 by \u2032 yields the error bound in the theorem.\nTime complexity complexity of the uniform+adaptive2 is calculated as follows. The uniform sampling costs O(n) time; the first adaptive sampling round costs O(nc21) + TMultiply(n\n2c1) time; the second adaptive sampling round costsO(n(c1+c2)2)+TMultiply(n2(c1+ c2)) time. So the total time complexity is O(nc2 2) + TMultiply(n2c )."}, {"heading": "Appendix E. Proof of Theorem 9", "text": "In Section E.1 we derive the solution to the optimization problem (5). In Section E.2 we prove that the solutions are global optimum. In Section E.3 we prove that the resulting solution is SPSD when K is SPSD.\nE.1 Solution to the Optimization Problem (5)\nWe denote the objective function of the optimization problem (5) by f(U, \u03b4) = \u2225\u2225K\u2212 C\u0304UC\u0304T \u2212 \u03b4In\u2225\u22252F .\nFirst, we take the derivative of f(U, \u03b4) w.r.t. U to be zero\n\u2202f(U, \u03b4)\n\u2202U =\n\u2202\n\u2202U tr(C\u0304UC\u0304T C\u0304UC\u0304T \u2212 2KC\u0304UC\u0304T + 2\u03b4C\u0304UC\u0304T )\n= 2C\u0304T C\u0304UC\u0304T C\u0304\u2212 2C\u0304TKC\u0304 + 2\u03b4C\u0304T C\u0304 = 0,\nand obtain that\nUss = (C\u0304T C\u0304)\u2020(C\u0304TKC\u0304\u2212 \u03b4ssC\u0304T C\u0304)(C\u0304T C\u0304)\u2020\n= (C\u0304T C\u0304)\u2020C\u0304TKC\u0304(C\u0304T C\u0304)\u2020 \u2212 \u03b4ss(C\u0304T C\u0304)\u2020C\u0304T C\u0304(C\u0304T C\u0304)\u2020 = C\u0304\u2020K(C\u0304\u2020)T \u2212 \u03b4ss(C\u0304T C\u0304)\u2020.\nSimilarly, we take the derivative of f(U, \u03b4) w.r.t. \u03b4 to be zero\n\u2202f(U, \u03b4)\n\u2202\u03b4 =\n\u2202\n\u2202\u03b4 tr(\u03b42In \u2212 2\u03b4K + 2\u03b4C\u0304UC\u0304T ) = 2n\u03b4 \u2212 2tr(K) + 2tr(C\u0304UC\u0304T ) = 0,\nand it follows that\n\u03b4ss = 1\nn\n( tr(K)\u2212 tr(C\u0304UssC\u0304T ) ) = 1\nn\n( tr(K)\u2212 tr ( C\u0304C\u0304\u2020K(C\u0304\u2020)T C\u0304T ) + \u03b4sstr ( C\u0304(C\u0304T C\u0304)\u2020C\u0304T )) = 1\nn\n( tr(K)\u2212 tr ( C\u0304T C\u0304C\u0304\u2020K(C\u0304\u2020)T ) + \u03b4sstr ( C\u0304C\u0304\u2020 )) = 1\nn\n( tr(K)\u2212 tr ( C\u0304TK(C\u0304\u2020)T ) + \u03b4ssrank(C\u0304) )\nand thus\n\u03b4ss = 1\nn\u2212 rank(C\u0304)\n( tr(K)\u2212 tr ( C\u0304\u2020KC\u0304) )) .\nE.2 Proof of Optimality\nThe Hessian matrix of f(U, \u03b4) with respect to (U, \u03b4) is\nH =  \u22022f(U, \u03b4) \u2202vec(U)\u2202vec(U)T \u22022f(U, \u03b4) \u2202vec(U)\u2202\u03b4 \u22022f(U, \u03b4)\n\u2202\u03b4\u2202vec(U)T \u22022f(U, \u03b4) \u2202\u03b42\n = 2 [ (C\u0304T C\u0304)\u2297 (C\u0304T C\u0304) vec(C\u0304T C\u0304) vec(C\u0304T C\u0304)T n ] .\nFor any X \u2208 Rc\u00d7c and b \u2208 R, we let\nq(X, b) = [ vec(X)T b ] H [ vec(X) b ] = vec(X)T ( (C\u0304T C\u0304)\u2297 (C\u0304T C\u0304) ) vec(X) + 2b vec(C\u0304T C\u0304)T vec(X) + nb2\n= vec(X)T vec ( (C\u0304T C\u0304)X(C\u0304T C\u0304) ) + 2b vec(C\u0304T C\u0304)T vec(X) + nb2\n= tr(XT C\u0304T C\u0304XC\u0304T C\u0304) + 2b tr(C\u0304T C\u0304X) + nb2.\nLet C\u0304XC\u0304T = Y \u2208 Rn\u00d7n, then\nq(X, b) = tr(C\u0304XT C\u0304T C\u0304XC\u0304T ) + 2b tr(C\u0304XC\u0304T ) + nb2\n= tr(YTY) + 2b tr(Y) + nb2\n= n\u2211 i=1 n\u2211 j=1 y2ij + 2b n\u2211 l=1 yll + nb 2\n= \u2211 i 6=j y2ij + n\u2211 l=1 (yll + b) 2 \u2265 0,\nwhich shows that the Hessian matrix H is SPSD. Hence f(Uss, \u03b4ss) is the global minimum of f .\nE.3 Proof of SPSD\nWe denote the thin SVD of C\u0304 by C\u0304 = UC\u0304\u03a3C\u0304V T C\u0304 . The approximation is\nC\u0304UssC\u0304T + \u03b4ssIn = C\u0304 ( C\u0304\u2020K(C\u0304\u2020)T \u2212 \u03b4ss(C\u0304T C\u0304)\u2020 ) C\u0304T + \u03b4ssIn\n= C\u0304 ( C\u0304\u2020K(C\u0304\u2020)T ) C\u0304T + \u03b4ss ( In \u2212 C\u0304(C\u0304T C\u0304)\u2020C\u0304T ) .\nIf K is positive (semi)definite, the first term C\u0304 ( C\u0304\u2020K(C\u0304\u2020)T ) C\u0304T is positive (semi)definite. So it remains to be shown that \u03b4ss \u2265 0 and In \u2212 C\u0304(C\u0304T C\u0304)\u2020C\u0304T is SPSD. The scalar \u03b4ss is positive because\ntr(K)\u2212 tr ( C\u0304\u2020KC\u0304 ) = tr(K)\u2212 tr ( C\u0304C\u0304\u2020K ) = tr(K)\u2212 tr ( UC\u0304U T C\u0304K ) \u2265 0.\nHere the inequality follows from that \u03c3i(K) \u2265 \u03c3i(UTC\u0304KUC\u0304) for i = 1, \u00b7 \u00b7 \u00b7 , c (Horn and Johnson, Lemma 3.3.1).\nLet U\u22a5 C\u0304 be an n\u00d7 (n\u2212 c) column orthogonal matrix orthogonal to UC\u0304, we have that\nIn \u2212 C\u0304(C\u0304T C\u0304)\u2020C\u0304T = In \u2212UC\u0304UTC\u0304 = U \u22a5 C\u0304U \u22a5 C\u0304\nT\nis SPSD. Hence the approximation C\u0304UssC\u0304T + \u03b4ssIn is positive (semi)definite when K is positive (semi)definite."}, {"heading": "Appendix F. Proof of Theorem 10", "text": "Proof Since the righthand side of (13) is convex and \u03b4\u0304 is the minimizer of the righthand of (13), so for any \u03b4 \u2208 (0, \u03b4\u0304], it holds that\nn\u2211 j=k+1 ( \u03c3j(K)\u2212 \u03b4 )2 \u2264 n\u2211 j=k+1 ( \u03c3j(K)\u2212 0 )2 = \u2225\u2225K\u2212Kk\u2225\u22252F .\nThen the theorem follows by the inequality in (13)."}, {"heading": "Appendix G. Proof of Theorem 12", "text": "Proof The error incurred by SS-Nystro\u0308m is\u2225\u2225K\u2212 K\u0303ssc \u2225\u22252F = \u2225\u2225(K\u0304 + \u03b4\u0304In)\u2212 (C\u0304U\u0304C\u0304T + \u03b4\u0304In)\u2225\u22252F = \u2225\u2225K\u0304\u2212 C\u0304U\u0304C\u0304T\u2225\u22252F \u2264 (1 + )\n\u2225\u2225K\u0304\u2212 K\u0304k\u2225\u22252F = (1 + ) n\u2211 i=k+1 \u03c32i ( K\u0304 ) = (1 + ) n\u2211 i=k+1 \u03bbi ( K\u03042 ) .\nHere the inequality follows from the property of the column selection algorithm Acol. The i-th largest eigenvalue of K\u0304 is \u03bbi(K) \u2212 \u03b4\u0304, so the n eigenvalues of K\u03042 are all in the set {(\u03bbi(K) \u2212 \u03b4\u0304)2}ni=1. The sum of the smallest n \u2212 k of the n eigenvalues of K\u03042 must be less than or equal to the sum of any n\u2212 k of the eigenvalues, thus we have\nn\u2211 i=k+1 \u03bbi ( K\u03042 ) \u2264\nn\u2211 i=k+1 ( \u03bbi(K)\u2212 \u03b4\u0304 )2 =\nn\u2211 i=k+1 \u03bb2i (K)\u2212 2 n\u2211 i=k+1 \u03b4\u0304\u03bbi(K) + (n\u2212 k)(\u03b4\u0304)2\n= \u2016K\u2212Kk\u20162F \u2212 1\nn\u2212 k [ n\u2211 i=k+1 \u03bbi(K) ]2 ,\nby which the theorem follows."}, {"heading": "Appendix H. Proof of Theorem 15", "text": "Proof Let K\u0303 = Q(QTK)k, where Q is defined in Line 4 in Algorithm 3. Boutsidis et al. (2011) showed that\nE\u2016K\u2212 K\u0303\u20162F \u2264 (1 + k/l) \u2016K\u2212Kk\u20162F , (33)\nwhere the expectation is taken w.r.t. the random Gaussian matrix \u2126. It follows from Lemma 23 that\n\u2016\u03c3K \u2212 \u03c3K\u0303\u2016 2 2 \u2264 \u2016K\u2212 K\u0303\u20162F ,\nwhere \u03c3K and \u03c3K\u0303 contain the singular values in a descending order. Since K\u0303 has a rank at most k, the k + 1 to n entries of \u03c3K\u0303 are zero. We split \u03c3K and \u03c3K\u0303 into vectors of length k and n\u2212 k:\n\u03c3K = [ \u03c3K,k \u03c3K,\u2212k ] and \u03c3K\u0303 = [ \u03c3K\u0303,k 0 ] and thus\n\u2016\u03c3K,k \u2212 \u03c3K\u0303,k\u2016 2 2 + \u2016\u03c3K,\u2212k\u201622 \u2264 \u2016K\u2212 K\u0303\u20162F . (34)\nSince \u2016\u03c3K,\u2212k\u201622 = \u2016K\u2212Kk\u20162F , it follows from (33) and (34) that\nE\u2016\u03c3K,k \u2212 \u03c3K\u0303,k\u2016 2 2 \u2264\nk l \u2016\u03c3K,\u2212k\u201622.\nSince \u2016x\u20162 \u2264 \u2016x\u20161 \u2264 \u221a k\u2016x\u20162 for any x \u2208 Rk, we have that\nE \u2225\u2225\u03c3K,k \u2212 \u03c3K\u0303,k\u2225\u22251 \u2264 k\u221al \u2225\u2225\u03c3K,\u2212k\u2225\u22251.\nThen it follows from (14) and Line 6 in Algorithm 3 that E \u2223\u2223\u03b4\u0304 \u2212 \u03b4\u0303\u2223\u2223 = E[ 1\nn\u2212 k \u2223\u2223\u2223\u2223 k\u2211 i=1 \u03c3i(K)\u2212 k\u2211 i=1 \u03c3i(K\u0303) \u2223\u2223\u2223\u2223 ]\n\u2264 1 n\u2212 k E \u2225\u2225\u03c3K,k \u2212 \u03c3K\u0303,k\u2225\u22251 \u2264 k\u221al 1n\u2212 k \u2225\u2225\u03c3K,\u2212k\u2225\u22251 = k\u221al \u03b4\u0304.\nLemma 23 Let A and B be n\u00d7n matrices and \u03c3A and \u03c3B contain the singular values in a descending order. Then we have that\n\u2016\u03c3A \u2212 \u03c3B\u201622 \u2264 \u2016A\u2212B\u20162F .\nProof It is easy to show that\n\u2016A\u2212B\u20162F = tr(ATA) + tr(BTB)\u2212 2tr(ATB)\n= n\u2211 i=1 \u03c32i (A) + n\u2211 i=1 \u03c32i (B)\u2212 2tr(ATB). (35)\nWe also have\ntr(ATB) \u2264 n\u2211 i=1 \u03c3i(A TB) \u2264 n\u2211 i=1 \u03c3i(A)\u03c3i(B), (36)\nwhere the first inequality follows from (Horn and Johnson, Theorem 3.3.13) and the second inequality follows from (Horn and Johnson, Theorem 3.3.14). Combining (35) and (36) we have that\n\u2016A\u2212B\u20162F \u2265 n\u2211 i=1 ( \u03c32i (A) + \u03c3 2 i (B)\u2212 2\u03c3i(A)\u03c3i(B) ) = \u2016\u03c3A \u2212 \u03c3B\u201622,\nby which the theorem follows."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Many kernel methods suffer from high time and space complexities, so they are prohibitive<lb>in big-data applications. To tackle the computational challenge, the Nystr\u00f6m method<lb>has been extensively used to reduce time and space complexities by sacrificing some<lb>accuracy. The Nystr\u00f6m method speedups computation by constructing an approximation<lb>of the kernel matrix in question using only a few columns of the matrix. Recently, a<lb>variant of the Nystr\u00f6m method called the modified Nystr\u00f6m method has demonstrated<lb>significant improvement over the standard Nystr\u00f6m method in approximation accuracy,<lb>both theoretically and empirically.<lb>In this paper we provide theoretical analysis, efficient algorithms, and a simple but<lb>highly accurate extension for the modified Nystr\u00f6m method. First, we prove that the<lb>modified Nystr\u00f6m method is exact under certain conditions, and we establish a lower error<lb>bound for the modified Nystr\u00f6m method. Second, we develop two efficient algorithms to<lb>make the modified Nystr\u00f6m method efficient and practical. We devise a simple column<lb>selection algorithm with a provable error bound. With the selected columns at hand,<lb>we propose an algorithm that computes the modified Nystr\u00f6m approximation in lower<lb>time complexity than the approach in the previous work. Third, the extension which we<lb>call the SS-Nystr\u00f6m method has much stronger error bound than the modified Nystr\u00f6m<lb>method, especially when the spectrum of the kernel matrix decays slowly. Our proposed<lb>SS-Nystr\u00f6m can be computed nearly as efficiently as the modified Nystr\u00f6m method.<lb>Finally, experiments on real-world datasets demonstrate that the proposed column selection<lb>algorithm is both efficient and accurate and that the SS-Nystr\u00f6m method always leads to<lb>much higher kernel approximation accuracy than the standard/modified Nystr\u00f6m method.<lb>", "creator": "LaTeX with hyperref package"}}}