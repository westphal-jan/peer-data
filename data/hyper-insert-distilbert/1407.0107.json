{"id": "1407.0107", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2014", "title": "Randomized Block Coordinate Descent for Online and Stochastic Optimization", "abstract": "two types of uniformly low flow cost - per - iteration posterior gradient distance descent methods do have also been extensively through studied methods in parallel. only one concept is regular online or stochastic flow gradient level descent ( / ogd / sgd ), and the other is randomzied coordinate descent ( rbcd ). in this paper, for quite the first time, we combine effectively the individual two types out of methods together and propose online interactive randomized block stream coordinate curve descent ( orbcd ). at each indexed iteration, orbcd only smoothly computes the partial rate gradient of one concurrent block symmetric coordinate of one mini - batch samples. pure orbcd sampling is usually well suited partially for the composite minimization problem where one given function is the average of meeting the losses of a specific large increasing number of samples ; and one the other is for a simple regularizer defined on high dependent dimensional random variables. typically we show on that choosing the iteration complexity type of orbcd has perhaps the same step order as single ogd + or ordinary sgd. for performing strongly convex functions, partly by dramatically reducing improving the phase variance of filtered stochastic gradients, we show that orbcd can converge at providing a nonlinear geometric adjustment rate in expectation, continuously matching the convergence averaging rate values of uniformly sgd with distinct variance reduction and rbcd.", "histories": [["v1", "Tue, 1 Jul 2014 05:57:43 GMT  (223kb)", "http://arxiv.org/abs/1407.0107v1", null], ["v2", "Sat, 12 Jul 2014 21:03:06 GMT  (223kb)", "http://arxiv.org/abs/1407.0107v2", null], ["v3", "Sat, 26 Jul 2014 19:16:39 GMT  (232kb)", "http://arxiv.org/abs/1407.0107v3", "The errors in the proof of ORBCD with variance reduction have been corrected"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["huahua wang", "arindam banerjee"], "accepted": false, "id": "1407.0107"}, "pdf": {"name": "1407.0107.pdf", "metadata": {"source": "CRF", "title": "Randomized Block Coordinate Descent for Online and Stochastic Optimization", "authors": ["Huahua Wang"], "emails": ["huwang@cs.umn.edu", "banerjee@cs.umn.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 7.\n01 07\nv1 [\ncs .L\nG ]\n1 J\nul 2"}, {"heading": "1 Introduction", "text": "In recent years, considerable efforts in machine learning have been devoted to solving the following composite objective minimization problem:\nmin x\nf(x) + g(x) = 1\nI\nI \u2211\ni=1\nfi(x) + J \u2211\nj=1\ngj(xj) , (1)\nwhere x \u2208 Rn\u00d71 and xj is a block coordinate of x. f(x) is the average of some smooth functions, and g(x) is a simple function which may be non-smooth. In particular, g(x) is block separable and blocks are non-overlapping. A variety of machine learning and statistics problems can be cast into the problem (1). In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1]. While f is separable among samples, g is separable among features. For example, in lasso [32], fi is a square loss or logistic loss function and g(x) = \u03bb\u2016x\u20161 where \u03bb is the tuning parameter. In group lasso [37], gj(xj) = \u03bb1\u2016xj\u20162, which enforces group sparsity among variables. To induce both group sparsity and sparsity, sparse group lasso [9] uses composite regularizers gj(xj) = \u03bb1\u2016xj\u20162 + \u03bb2\u2016xj\u20161.\nDue to the simplicity, gradient descent (GD) type methods have been widely used to solve problem (1). If gj is nonsmooth but simple enough for proximal mapping [], it is better to just use the gradient of fi but keep gj untouched in GD. This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8]. Without loss of generality, we\nsimply use GD to represent GD and its variants in the rest of this paper. Let m be the number of samples and n be dimension of features. m samples are divided into I blocks (mini-batch), and n features are divided into J non-overlapping blocks. If both m and n are large, solving (1) using batch methods like gradient descent (GD) type methods is computationally expensive. To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.\nInstead of computing gradients of all samples in GD at each iteration, OGD/SGD only computes the gradient of one block samples, and thus the cost-per-iteration is just one I-th of GD. For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30]. OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35]. OGD and SGD use a decreasing step size and converge at a slower rate than GD. In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38]. Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21]. However, SVG needs to store all gradients, which becomes an issue for large scale datasets. It is also difficult to understand the intuition behind the proof of SVG. To address the issue of storage and better explain the faster convergence, [13] proposed an explicit variance reduction scheme into SGD. The two scheme SGD is refered as stochastic variance reduction gradient (SVRG). SVRG computes the full gradient periodically and progressively mitigates the variance of stochastic gradient by removing the difference between the full gradient and stochastic gradient. For smooth and strongly convex functions, SVRG converges at a geometric rate in expectation. Compared to SVG, SVRG is free from the storage of full gradients and has a much simpler proof. The similar idea was also proposed independently by [19]. The results of SVRG is then improved in [15]. In [36], SVRG is generalized to solve composite minimization problem by incorporate the variance reduction technique into proximal gradient method.\nOn the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers. RBCD randomly chooses a block coordinate to update at each iteration. The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17]. RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17]. Compared to GD, the cost-per-iteration of RBCD is much cheaper. Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18]. Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].\nWhile OGD/SGD is well suitable for problems with a large number of samples, RBCD is suitable for high dimension problems with non-overlapping composite regularizers. For large scale high dimensional problems, particular with non-overlapping composite regularizers, it is more economic to combine the two type of methods together. In this paper, we propose a new method named online randomized overlapping block coordinate descent (ORBCD) which combines the well-known OGD/SGD and RBCD together for the first time. ORBCD first randomly picks up one block samples and one block coordinates, then performs the block coordinate gradient descent on the randomly chosen samples at each iteration. Essentially, ORBCD performs RBCD in the online and stochastic setting. If fi is a linear function, the cost-per-iteration of ORBCD is O(1) and thus is far smaller than O(n) in OGD/SGD and O(m) in RBCD. We show that the iteration complexity for ORBCD has the same order as OGD/SGD. In the stochastic setting, ORBCD is still suffered from the variance of stochastic gradient. To accelerate the convergence speed of ORBCD, we adopt the varaince reduction technique [13] to alleviate the effect of randomness. As expected, the\nlinear convergence rate for ORBCD with variance reduction (ORBCDVD) is established for strongly convex functions for stochastic optimization. Moreover, ORBCDVD does not necessarily require to compute the full gradient at once which is necessary in SVRG and prox-SVRG. Instead, \u00b5\u0303 can be partially computed at each iteration and then stored for the next retrieval, which may be useful when the computation of full gradient is expensive or data is partially available at the moment [23].\nThe rest of the paper is organized as follows. In Section 2, we review the SGD and RBCD. ORBCD and ORBCD with variance reduction are proposed in Section 3. The convergence results are given in Section 4. The paper is concluded in Section 5."}, {"heading": "2 Related Work", "text": "In this section, we briefly review the two types of low cost-per-iteration gradient descent (GD) methods, i.e., OGD/SGD and RBCD. Applying GD on (1), we have the following iterate:\nxt+1 = argmin x \u3008\u2207f(xt),x\u3009+ g(x) + \u03b7t\n2 \u2016x\u2212 xt\u201622 . (2)\nIn some cases, e.g. g(x) is \u21131 norm, (2) can have a closed-form solution."}, {"heading": "2.1 Online and Stochastic Gradient Descent", "text": "In (2), it requires to compute the full gradient of m samples at each iteration, which could be computationally expensive if m is too large. Instead, OGD/SGD simply computes the gradient of one block samples.\nIn the online setting, at time t+ 1, OGD first presents a solution xt+1 by solving\nxt+1 = argmin x \u3008\u2207ft(xt),x\u3009+ g(x) + \u03b7t 2 \u2016x\u2212 xt\u201622 . (3)\nwhere ft is given and assumed to be convex. Then a function ft+1 is revealed which incurs the loss ft(xt). The performance of OGD is measured by the regret bound, which is the discrepancy between the cumulative loss over T rounds and the best decision in hindsight,\nR(T ) = T \u2211\nt=1\n[ft(x t) + g(xt)]\u2212 [ft(x\u2217) + g(x\u2217)] , (4)\nwhere x\u2217 is the best result in hindsight. The regret bound of OGD is O( \u221a T ) when using decreasing step size \u03b7t = O( 1\u221at). For strongly convex functions, the regret bound of OGD is O(log T ) when using the step size \u03b7t = O(1t ). Since ft can be any convex function, OGD considers the worst case and thus the mentioned regret bounds are optimal.\nIn the stochastic setting, SGD first randomly picks up it-th block samples and then computes the gradient of the selected samples as follows:\nxt+1 = argmin x \u3008\u2207fit(xt),x\u3009+ g(x) + \u03b7t 2 \u2016x\u2212 xt\u201622 . (5)\nxt depends on the observed realization of the random variable \u03be = {i1, \u00b7 \u00b7 \u00b7 , it\u22121} or generally {x1, \u00b7 \u00b7 \u00b7 ,xt\u22121}. Due to the effect of variance of stochastic gradient, SGD has to choose decreasing step size, i.e., \u03b7t = O( 1\u221at), leading to slow convergence speed. For general convex functions, SGD converges at a rate of O( 1\u221a t ). For\nstrongly convex functions, SGD converges at a rate of O(1 t ). In contrast, GD converges linearly if functions are strongly convex. To accelerate the SGD by reducing the variance of stochastic gradient, stochastic variance reduced gradient (SVRG) was proposed by [13]. [36] extends SVRG to composite functions (1), called prox-SVRG. SVRGs have two stages, i.e., outer stage and inner stage. The outer stage maintains an estimate x\u0303 of the optimal point x\u2217 and computes the full gradient of x\u0303\n\u00b5\u0303 = 1\nn\nn \u2211\ni=1\n\u2207fi(x\u0303) = \u2207f(x\u0303) . (6)\nAfter the inner stage is completed, the outer stage updates x\u0303. At the inner stage, SVRG first randomly picks it-th sample, then modifies stochastis gradient by subtracting the difference between the full gradient and stochastic gradient at x\u0303,\nvt = \u2207fit(xt)\u2212\u2207fit(x\u0303) + \u00b5\u0303 . (7)\nIt can be shown that the expectation of vt given xt\u22121 is the full gradient at xt, i.e., Evt = \u2207f(xt). Although vt is also a stochastic gradient, the variance of stochastic gradient progressively decreases. Replacing \u2207fit(xt) by vt in SGD step (5),\nxt+1 = argmin x \u3008vt,x\u3009+ g(x) +\n\u03b7 2 \u2016x\u2212 xt\u201622 . (8)\nBy reduding the variance of stochastic gradient, xt can converge to x\u2217 at the same rate as GD, which has been proved in [13, 36]. For strongly convex functions, prox-SVRG [36] can converge linearly in expectation if \u03b7 > 4L and m satisfy the following condition:\n\u03c1 = \u03b72 \u03b3(\u03b7 \u2212 4L)m + 4L(m+ 1) (\u03b7 \u2212 4L)m < 1 . (9)\nwhere L is the constant of Lipschitz continuous gradient. Note the step size is 1/\u03b7 here."}, {"heading": "2.2 Randomized Block Coordinate Descent", "text": "Assume xj(1 \u2264 j \u2264 J) are non-overlapping blocks. At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem:\nxt+1jt = argminxjt \u3008\u2207jtf(x t),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622 . (10)\nTherefore, xt+1 = (xt+1jt ,x t k 6=jt). x t depends on the observed realization of the random variable\n\u03be = {j1, \u00b7 \u00b7 \u00b7 , jt\u22121} . (11)\nSetting the step size \u03b7t = Ljt where Ljt is the Lipshitz constant of jt-th coordinate of the gradient \u2207f(xt), the iteration complexity of RBCD is O(1\nt ). For strongly convex function, RBCD has a linear convergence\nrate. Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17]."}, {"heading": "3 Online Randomized Block Coordinate Descent", "text": "In this section, our goal is to combine OGD/SGD and RBCD together to solve problem (1). We call the algorithm online randomized block coordinate descent (ORBCD), which computes one block coordinate of the gradient of one block of samples at each iteration. ORBCD essentially performs RBCD in online and stochastic setting.\nLet {x1, \u00b7 \u00b7 \u00b7 ,xJ},xj \u2208 Rnj\u00d71 be J non-overlapping blocks of x. Let Uj \u2208 Rn\u00d7nj be nj columns of an n\u00d7 n permutation matrix U, corresponding to j block coordinates in x. For any partition of x and U,\nx =\nJ \u2211\nj=1\nUjxj ,xj = U T j x . (12)\nThe j-th coordinate of gradient of f can be denoted as\n\u2207jf(x) = UTj \u2207f(x) . (13)\nThroughout the paper, we assume that the minimum of problem (1) is attained. In addition, ORBCD needs the following assumption :\nAssumption 1 ft or f has block-wise Lipschitz continuous gradient with constant Lj , e.g.,\n\u2016\u2207jft(x+ Ujhj)\u2212\u2207jft(x)\u20162 \u2264 Lj\u2016hj\u20162 , (14)\nAssumption 2 1. \u2016\u2207ft(xt)\u20162 \u2264 Rf , or \u2016\u2207f(xt)\u20162 \u2264 Rf ; 2. xt is assumed in a bounded set X , i.e., sup\nx,y\u2208X \u2016x\u2212 y\u20162 = D.\nWhile the Assumption 1 is used in RBCD, the Assumption 2 is used in OGD/SGD. We may assume the sum of two functions is strongly convex.\nAssumption 3 ft(x) + g(x) or f(x) + g(x) is \u03b3-strongly convex, e.g., we have\nft(x) + g(x) \u2265 ft(y) + g(y) + \u3008\u2207ft(y) + g\u2032(y),x \u2212 xt\u3009+ \u03b3\n2 \u2016x\u2212 y\u201622 . (15)\nwhere \u03b3 > 0 and g\u2032(y) denotes the subgradient of g at y."}, {"heading": "3.1 ORBCD for Online Learning", "text": "In online setting, ORBCD considers the worst case and runs at rounds. At time t, given any function ft which may be agnostic, ORBCD randomly chooses jt-th block coordinate and presents the solution by solving the following problem:\nxt+1jt = argminxjt \u3008\u2207jtft(x t),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622\n= Proxgjt (xjt \u2212 1\n\u03b7t \u2207jtft(xt)) , (16)\nwhere Prox denotes the proximal mapping. If ft is a linear function, e.g., ft = ltxt, then \u2207jtft(xt) = ljt , so solving (16) is J times cheaper than OGD. Thus, xt+1 = (xt+1jt ,x t k 6=jt), or\nxt+1 = xt + Ujt(x t+1 jt \u2212 xtjt) . (17)\nAlgorithm 1 Online Randomized Block Coordinate Descent for Online Learning\n1: Initialization: x1 = 0 2: for t = 1 to T do 3: randomly pick up jt block coordinates 4: xt+1jt = argminxjt\u2208Xj \u3008\u2207jtft(x\nt),xjt\u3009+ gjt(xjt) + \u03b7t2 \u2016xjt \u2212 xtjt\u201622 5: xt+1 = xt + Ujt(x t+1 jt\n\u2212 xtjt) 6: receives the function ft+1(x) + g(x) and incurs the loss ft+1(xt+1) + g(xt+1) 7: end for\nThen, ORBCD receives a loss function ft+1(x) which incurs the loss ft+1(xt+1). The algorithm is summarized in Algorithm 1.\nxt is independent of jt but depends on the sequence of observed realization of the random variable\n\u03be = {j1, \u00b7 \u00b7 \u00b7 , jt\u22121}. (18)\nLet x\u2217 be the best solution in hindsight. The regret bound of ORBCD is defined as\nR(T ) =\nT \u2211\nt=1\n{ E\u03be[ft(x t) + g(xt)]\u2212 [ft(x\u2217) + g(x\u2217)] } . (19)\nBy setting \u03b7t = \u221a t+ L where L = maxj Lj , the regret bound of ORBCD is O( \u221a T ). For strongly convex functions, the regret bound of ORBCD is O(log T ) by setting \u03b7t = \u03b3t J + L."}, {"heading": "3.2 ORBCD for Stochastic Optimization", "text": "In the stochastic setting, ORBCD first randomly picks up it-th block sample and then randomly chooses jt-th block coordinate. The algorithm has the following iterate:\nxt+1jt = argminxjt \u3008\u2207jtfit(x t),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622\n= Proxgjt (xjt \u2212\u2207jtfit(x t)) . (20)\nFor high dimensional problem with non-overlapping composite regularizers, solving (20) is computationally cheaper than solving (5) in SGD. The algorithm of ORBCD in both settings is summarized in Algorithm 2.\nxt+1 depends on (it, jt), but jt and it are independent. xt is independent of (it, jt) but depends on the observed realization of the random variables\n\u03be = {(i1, j1), \u00b7 \u00b7 \u00b7 , (it\u22121, jt\u22121)} . (21)\nThe online-stochastic conversion rule [7, 8, 35] still holds here. The iteration complexity of ORBCD can be obtained by dividing the regret bounds in the online setting by T . Setting \u03b7t = \u221a t+L where L = maxj Lj , the iteration complexity of ORBCD is\nE\u03be[f(x\u0304 t) + g(x\u0304t)]\u2212 [f(x) + g(x)] \u2264 O( 1\u221a\nT ) . (22)\nAlgorithm 2 Online Randomized Block Coordinate Descent for Stochastic Optimization\n1: Initialization: x1 = 0 2: for t = 1 to T do 3: randomly pick up it block samples and jt block coordinates 4: xt+1jt = argminxjt\u2208Xj \u3008\u2207jtfit(x\nt),xjt\u3009+ gjt(xjt) + \u03b7t2 \u2016xjt \u2212 xtjt\u201622 5: xt+1 = xt + Ujt(x t+1 jt\n\u2212 xtjt) 6: end for\nAlgorithm 3 Online Randomized Block Coordinate Descent with Variance Reduction 1: Initialization: x1 = 0 2: for t = 1 to T do 3: x0 = x\u0303 = x\nt. 4: for k = 0 to m\u2212 1 do 5: randomly pick up ik block samples 6: randomly pick up jk block coordinates 7: v\nik jk = \u2207jkfik(xk)\u2212\u2207jkfik(x\u0303) + \u00b5\u0303jk where \u00b5\u0303jk = \u2207jkf(x\u0303) 8: xkjk = argminxjk \u3008vikjk ,xjk\u3009+ gjk(xjk) + \u03b7k 2 \u2016xjk \u2212 xkjk\u2016 2 2 9: xk+1 = xk + Ujk(x k+1 jj\n\u2212 xkjk) 10: end for 11: xt+1 = 1\nm \u2211m k=1 x k\n12: end for\nFor strongly convex functions, setting \u03b7t = \u03b3t J + L,\nE\u03be[f(x\u0304 t) + g(x\u0304t)]\u2212 [f(x) + g(x)] \u2264 O( log T\nT ) . (23)\nThe iteration complexity of ORBCD match that of SGD. Simiarlar as SGD, the convergence speed of ORBCD is also slowed down by the variance of stochastic gradient."}, {"heading": "3.3 ORBCD with variance reduction", "text": "In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD. As SVRG and prox-SVRG, ORBCDVD consists of two stages. At time t+1, the outer stage maintains an estimate x\u0303 = xt of the optimal x\u2217 and updates x\u0303 every m iterations. The inner stage takes m iterations which is indexed by k = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1. At the k-th iteration, ORBCDVD randomly picks ik-th sample and jk-th coordinate and compute\nv ik jk = \u2207jkfik(xk)\u2212\u2207jkfik(x\u0303) + \u00b5\u0303jk , (24)\nwhere\n\u00b5\u0303jk = 1\nn\nn \u2211\ni=1\n\u2207jkfi(x\u0303) = \u2207jkf(x\u0303) . (25)\nvitjt depends on (it, jt), and it and jt are independent. Conditioned on x k, taking expectation over ik, jk gives\nEv ik jk = EikEjk [\u2207jkfik(xk)\u2212\u2207jkfik(x\u0303) + \u00b5\u0303jk ]\n= 1\nJ Eik [\u2207fik(xk)\u2212\u2207fik(x\u0303) + \u00b5\u0303]\n= 1\nJ \u2207f(xk) . (26)\nAlthough vikjk is stochastic gradient, the variance E\u2016v ik jk \u2212 \u2207jkf(xk)\u201622 decreases progressively and is bounded by\nE\u2016vikjk \u2212\u2207jkfik(x k)\u201622 \u2264\n4L\nJ [h(xk)\u2212 h(x\u2217) + h(x\u0303)\u2212 h(x\u2217)] , (27)\nwhich is much smaller than E\u2016\u2207fit(xt) \u2212 \u2207f(xt)\u201622. Using the variance reduced gradient vikjk , ORBCD then performs RBCD as follows:\nxk+1jk = argminxjk \u3008vikjk ,xjk\u3009+ gjk(xjk) +\n\u03b7 2 \u2016xjk \u2212 xkjk\u2016 2 2 . (28)\nAfter m iterations, the outer stage updates xt+1 = 1 m \u2211m k=1 x k. The algorithm is summarized in Algorithm 3. At the outer stage, ORBCDVD does not necessarily require to compute the full gradient at once. The computation of full gradient may require substantial computational eorts, let alone data may be only partially available at the moment [23]. In ORBCD, \u00b5\u0303 can be partially computed at each iteration and then stored for the next retrieval.\nLet h(x) = f(x) + g(x). Assume \u03b7 > 3L and m satisfy the following condition:\n\u03c1 = 2L \u03b7 \u2212 3L + (\u03b7 \u2212 L)J (\u03b7 \u2212 3L)m \u2212 1 m + \u03b7(\u03b7 \u2212 L)J (\u03b7 \u2212 3L)m\u03b3 < 1 . (29)\nThen h(x) converges linearly in expectation, i.e.,\nE\u03beh(x t)\u2212 h(x\u2217) \u2264 O(\u03c1t) . (30)\nWithout loss of generality, assume L/\u03b3 \u2265 1. Setting \u03b7 = \u03b1L in (29) yields\n\u03c1 = 2(m\u2212 1) (\u03b1\u2212 3)m + (\u03b1 \u2212 1)J (\u03b1\u2212 3)m \u2212 1 m + \u03b1(\u03b1\u2212 1)LJ (\u03b1\u2212 3)m\u03b3 < 2 \u03b1\u2212 3 + (\u03b1+ 1)(\u03b1 \u2212 1)JL (\u03b1\u2212 3)m\u03b3 . (31)\nIf \u03b7 = 11L,m = 100JL/\u03b3, then \u03c1 \u2264 0.4. In particular, if picking up all coordinates (J = 1), m = 100L/\u03b3. For prox-SVRG [36], setting \u03b7 = 10L and m = 100L/\u03b3, \u03c1 \u2248 5/6 in (9). If also setting \u03b7 = 11L and m = 100L/\u03b3, \u03c1 \u2248 0.75 in (9), which is almost two times larger than our result \u03c1 = 0.4."}, {"heading": "4 The Rate of Convergnce", "text": "The following lemma is a key building block of the proof of the convergence of ORBCD in both online and stochastic setting.\nLemma 1 Let the Assumption 1 and 2 hold. Let xt be the sequences generated by ORBCD. jt is sampled randomly and uniformly from {1, \u00b7 \u00b7 \u00b7 , J}. We have\n\u3008\u2207jtft(xt) + g\u2032jt(xtjt),xtjt \u2212 xjt\u3009 \u2264 \u03b7t 2 (\u2016x \u2212 xt\u201622 \u2212 \u2016x\u2212 xt+1\u201622) + R2f 2(\u03b7t \u2212 L) + g(xt)\u2212 g(xt+1) .\n(32)\nwhere L = maxk Lk.\nProof: The optimality condition is\n\u3008\u2207jtft(xt) + \u03b7t(xt+1jt \u2212 x t jt ) + g\u2032jt(x t+1 jt ),xt+1jt \u2212 xjt\u3009 \u2264 0 . (33)\nRearranging the terms yields\n\u3008\u2207jtft(xt) + g\u2032jt(x t+1 jt ),xt+1jt \u2212 xjt\u3009 \u2264 \u2212\u03b7t\u3008x t+1 jt \u2212 xtjt ,x t+1 jt \u2212 xjt\u3009\n\u2264 \u03b7t 2 (\u2016xjt \u2212 xtjt\u201622 \u2212 \u2016xjt \u2212 x t+1 jt \u201622 \u2212 \u2016xt+1jt \u2212 x t jt\u201622) = \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 \u2016x\u2212 xt+1\u201622 \u2212 \u2016xt+1jt \u2212 x t jt\u201622) , (34)\nwhere the last equality uses xt+1 = (xt+1jt ,x t k 6=jt). By the smoothness of ft, we have\nft(x t+1) \u2264 ft(xt) + \u3008\u2207jft(xt),xt+1j \u2212 xtj\u3009+ Lj 2 \u2016xt+1j \u2212 xtj\u201622 . (35)\nSince xt+1 \u2212 xt = Ujt(xt+1jt \u2212 xtjt), we have\nft(x t+1) + g(xt+1)\u2212 [ft(xt) + g(xt)]\n\u2264 \u3008\u2207jtft(xt),xt+1jt \u2212 x t jt \u3009+ Ljt\n2 \u2016xt+1jt \u2212 x t jt \u201622 + gjt(xt+1jt )\u2212 gjt(xjt) + gjt(x t jt )\u2212 gjt(xjt)\n\u2264 \u3008\u2207jtft(xt) + g\u2032jt(x t+1 jt ),xt+1jt \u2212 xjt\u3009+ Ljt 2 \u2016xt+1jt \u2212 x t jt \u201622 \u2212 \u3008\u2207jtft(xt) + g\u2032jt(x t jt ),xtjt \u2212 xjt\u3009 \u2264 \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 \u2016x\u2212 xt+1\u201622) + Ljt \u2212 \u03b7t 2 \u2016xt+1jt \u2212 x t jt\u201622 \u2212 \u3008\u2207jtft(xt) + g\u2032jt(xtjt),xtjt \u2212 xjt\u3009 . (36)\nRearranging the terms yields\n\u3008\u2207jtft(xt) + g\u2032jt(xt),xtjt \u2212 xjt\u3009 \u2264 \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 \u2016x\u2212 xt+1\u201622) + Ljt \u2212 \u03b7t 2 \u2016xt+1jt \u2212 x t jt \u201622\n+ ft(x t) + g(xt)\u2212 [ft(xt+1) + g(xt+1)] . (37)\nThe convexity of ft gives\nft(x t)\u2212 ft(xt+1) \u2264 \u3008\u2207ft(xt),xt \u2212 xt+1\u3009 = \u3008\u2207jtft(xt),xtjt \u2212 x t+1 jt \u3009 \u2264 1 2\u03b1 \u2016\u2207jtft(xt)\u201622 + \u03b1 2 \u2016xtjt \u2212 x t+1 jt\n\u201622 . (38)\nwhere the equality uses xt+1 = (xt+1jt ,x t k 6=jt). Plugging into (37), we have\n\u3008\u2207jtft(xt) + g\u2032jt(xtjt),xtjt \u2212 xjt\u3009\n\u2264 \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 \u2016x\u2212 xt+1\u201622) + Ljt \u2212 \u03b7t 2 \u2016xt+1jt \u2212 x t jt \u201622 + \u3008\u2207jtft(xt),xtjt \u2212 x t+1 jt \u3009+ g(xt)\u2212 g(xt+1) \u2264 \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 \u2016x\u2212 xt+1\u201622) + Ljt \u2212 \u03b7t 2 \u2016xt+1jt \u2212 x t jt\u201622 + \u03b1 2 \u2016xtjt \u2212 xt+1jt \u2016 2 2 + 1 2\u03b1 \u2016\u2207jtft(xt)\u201622 .\n(39)\nLet L = maxk Lk. Setting \u03b1 = \u03b7t \u2212 L where \u03b7t > L completes the proof. This lemma is also a key building block in the proof of iteration complexity of GD, OGD/SGD and RBCD. In GD, by setting \u03b7t = L, the iteration complexity of GD can be established. In RBCD, by simply setting \u03b7t = Ljt , the iteration complexity of RBCD can be established."}, {"heading": "4.1 Online Optimization", "text": "Note xt depends on the sequence of observed realization of the random variable \u03be = {j1, \u00b7 \u00b7 \u00b7 , jt\u22121}. The following theorem establishes the regret bound of ORBCD.\nTheorem 1 Let \u03b7t = \u221a t+L in the ORBCD and the Assumption 1 and 2 hold. jt is sampled randomly and uniformly from {1, \u00b7 \u00b7 \u00b7 , J}. The regret bound R(T ) of ORBCD is\nR(T ) \u2264 J( \u221a T + L\n2 D2 +\n\u221a TR2 + g(x1)\u2212 g(x\u2217)) . (40)\nProof: In (32), conditioned on xt, take expectation over jt, we have\n1 J \u3008\u2207ft(xt) + g\u2032(xt),xt \u2212 x\u3009 \u2264 \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 E\u2016x\u2212 xt+1\u201622) +\nR2\n2(\u03b7t \u2212 L) + g(xt)\u2212 Eg(xt+1) .\n(41)\nUsing the convexity, we have\nft(x t) + g(xt)\u2212 [ft(x) + g(x)] \u2264 \u3008\u2207ft(xt) + g\u2032(xt),xt \u2212 x\u3009 . (42)\nTogether with (41), we have\nft(x t) + g(xt)\u2212 [ft(x) + g(x)] \u2264 J\n{\n\u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 E\u2016x\u2212 xt+1\u201622) +\nR2\n2(\u03b7t \u2212 L) + g(xt)\u2212 Eg(xt+1)\n}\n.\n(43)\nTaking expectation over \u03be on both sides, we have\nE\u03be\n[\nft(x t) + g(xt)\u2212 [ft(x) + g(x)]\n] \u2264 J {\u03b7t 2 (E\u03be\u2016x\u2212 xt\u201622 \u2212 E\u03be\u2016x\u2212 xt+1\u201622)\n+ R2\n2(\u03b7t \u2212 L) + E\u03beg(x\nt)\u2212 E\u03beg(xt+1) } . (44)\nSumming over t and setting \u03b7t = \u221a t+ L, we obtain the regret bound\nR(T ) =\nT \u2211\nt=1\n{\nE\u03be[ft(x t) + g(xt)]\u2212 [ft(x) + g(x)]\n}\n\u2264 J {\n\u2212\u03b7T 2 E\u03be\u2016x\u2212 xT+1\u201622 +\nT \u2211\nt=1\n(\u03b7t \u2212 \u03b7t\u22121)E\u03be\u2016x\u2212 xt\u201622 + T \u2211\nt=1\nR2\n2(\u03b7t \u2212 L) + g(x1)\u2212 E\u03beg(xT+1)\n}\n\u2264 J {\n\u03b7T 2 D2 +\nT \u2211\nt=1\nR2\n2(\u03b7t \u2212 L) + g(x1)\u2212 g(x\u2217)\n}\n\u2264 J {\u221a T + L\n2 D2 +\nT \u2211\nt=1\nR2\n2 \u221a t + g(x1)\u2212 g(x\u2217)\n}\n\u2264 J( \u221a T + L\n2 D2 +\n\u221a TR2 + g(x1)\u2212 g(x\u2217)) , (45)\nwhich completes the proof.\nIf one of the functions is strongly convex, ORBCD can achieve a log(T ) regret bound, which is established in the following theorem.\nTheorem 2 Let the Assumption 1-3 hold and \u03b7t = \u03b3t J +L in ORBCD. jt is sampled randomly and uniformly from {1, \u00b7 \u00b7 \u00b7 , J}. The regret bound R(T ) of ORBCD is\nR(T ) \u2264 J2R2 log(T ) + J(g(x1)\u2212 g(x\u2217)) . (46)\nProof: Using the strong convexity of ft + g in (15), we have\nft(x t) + g(xt)\u2212 [ft(x) + g(x)] \u2264 \u3008\u2207ft(xt) + g\u2032(xt),xt \u2212 x\u3009 \u2212\n\u03b3 2 \u2016x\u2212 xt\u201622 . (47)\nTogether with (41), we have\nft(x t) + g(xt)\u2212 [ft(x) + g(x)] \u2264 J\u03b7t \u2212 \u03b3 2 \u2016x\u2212 xt\u201622 \u2212 J\u03b7t 2 E\u2016x\u2212 xt+1\u201622)\n+ JR2\n2(\u03b7t \u2212 L) + J [g(xt)\u2212 Eg(xt+1)] . (48)\nTaking expectation over \u03be on both sides, we have\nE\u03be\n[\nft(x t) + g(xt)\u2212 [ft(x) + g(x)]\n] \u2264 J\u03b7t \u2212 \u03b3 2 E\u03be\u2016x\u2212 xt\u201622 \u2212 J\u03b7t 2 E\u03be[\u2016x\u2212 xt+1\u201622])\n+ JR2\n2(\u03b7t \u2212 L) + J [E\u03beg(x\nt)\u2212 E\u03beg(xt+1)] . (49)\nSumming over t and setting \u03b7t = \u03b3t J + L, we obtain the regret bound\nR(T ) =\nT \u2211\nt=1\n{\nE\u03be[ft(x t) + g(xt)]\u2212 [ft(x) + g(x)]\n}\n\u2264 \u2212J\u03b7T 2\nE\u03be\u2016x\u2212 xT+1\u201622 + T \u2211\nt=1\nJ\u03b7t \u2212 \u03b3 \u2212 J\u03b7t\u22121 2 E\u03be\u2016x\u2212 xt\u201622 + T \u2211\nt=1\nJR2\n2(\u03b7t \u2212 L) + J(g(x1)\u2212 E\u03beg(xT+1))\n\u2264 T \u2211\nt=1\nJ2R2\n2\u03b3t + J(g(x1)\u2212 g(x\u2217))\n\u2264 J2R2 log(T ) + J(g(x1)\u2212 g(x\u2217)) , (50)\nwhich completes the proof.\nIn general, ORBCD can achieve the same order of regret bound as OGD and other first-order online optimization methods, although the constant could be J times larger."}, {"heading": "4.2 Stochastic Optimization", "text": "In the stochastic setting, ORBCD first randomly chooses the it-th block sample and the jt-th block coordinate. jt and it are independent. xt depends on the observed realization of the random variables \u03be = {(i1, j1), \u00b7 \u00b7 \u00b7 , (it\u22121, jt\u22121)}. The following theorem establishes the iteration complexity of ORBCD for general convex functions. Theorem 3 Let \u03b7t = \u221a t + L and x\u0304T = 1\nT \u2211T t=1 x t in the ORBCD. it, jt are sampled randomly and uniformly from {1, \u00b7 \u00b7 \u00b7 , I} and {1, \u00b7 \u00b7 \u00b7 , J} respectively. The iteration complexity of ORBCD is\nE\u03be[f(x\u0304 t) + g(x\u0304t)]\u2212 [f(x) + g(x)] \u2264 J(\n\u221a T+L 2 D 2 + \u221a TR2 + g(x1)\u2212 g(x\u2217)) T . (51)\nProof: In the stochastic setting, let ft be fit in (32), we have\n\u3008\u2207jtfit(xt) + g\u2032jt(xt),xtjt \u2212 xjt\u3009 \u2264 \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 \u2016x\u2212 xt+1\u201622) +\nR2\n2(\u03b7t \u2212 L) + g(xt)\u2212 g(xt+1) .\n(52)\nNote it, jt are independent of xt. Conditioned on xt, taking expectation over it and jt, the RHS is\nE\u3008\u2207jtfit(xt) + g\u2032jt(xt),xtjt \u2212 xjt\u3009 = Eit [Ejt [\u3008\u2207jtfit(xt) + g\u2032jt(xt),xtjt \u2212 xjt\u3009]]\n= 1\nJ Eit [\u3008\u2207fit(xt),xt \u2212 x\u3009+ \u3008g\u2032(xt),xt \u2212 x\u3009]\n= 1\nJ \u3008\u2207f(xt) + g\u2032(xt),xt \u2212 x\u3009 . (53)\nPlugging back into (52), we have\n1 J \u3008\u2207f(xt) + g\u2032(xt),xt \u2212 x\u3009 \u2264 \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 E\u2016x\u2212 xt+1\u201622) +\nR2\n2(\u03b7t \u2212 L) + g(xt)\u2212 Eg(xt+1) . (54)\nUsing the convexity of f + g, we have\nf(xt) + g(xt)\u2212 [f(x) + g(x)] \u2264 \u3008\u2207f(xt) + g\u2032(xt),xt \u2212 x\u3009 . (55)\nTogether with (54), we have\nf(xt) + g(xt)\u2212 [f(x) + g(x)] \u2264 J { \u03b7t 2 (\u2016x\u2212 xt\u201622 \u2212 E\u2016x\u2212 xt+1\u201622) +\nR2\n2(\u03b7t \u2212 L) + g(xt)\u2212 Eg(xt+1)\n}\n.\n(56)\nTaking expectation over \u03be on both sides, we have\nE\u03be\n[ f(xt) + g(xt) ] \u2212 [f(x) + g(x)] \u2264 J {\u03b7t 2 (E\u03be\u2016x\u2212 xt\u201622 \u2212 E\u03be[\u2016x\u2212 xt+1\u201622])\n+ R2\n2(\u03b7t \u2212 L) + E\u03beg(x\nt)\u2212 E\u03beg(xt+1) } . (57)\nSumming over t and setting \u03b7t = \u221a t+ L, following similar derivation in (45), we have\nT \u2211\nt=1\n{\nE\u03be[f(x t) + g(xt)]\u2212 [f(x) + g(x)]\n} \u2264 J( \u221a T + L\n2 D2 +\n\u221a TR2 + g(x1)\u2212 g(x\u2217)) . (58)\nDividing both sides by T , using the Jensen\u2019s inequality and denoting x\u0304T = 1 T \u2211T t=1 x t complete the proof.\nFor strongly convex functions, we have the following results.\nTheorem 4 For strongly convex function, setting \u03b7t = \u03b3t J + L in the ORBCD. it, jt are sampled randomly and uniformly from {1, \u00b7 \u00b7 \u00b7 , I} and {1, \u00b7 \u00b7 \u00b7 , J} respectively. Let x\u0304T = 1 T \u2211T t=1 x\nt. The iteration complexity of ORBCD is\nE\u03be[f(x\u0304 T ) + g(x\u0304T )]\u2212 [f(x) + g(x)] \u2264 J 2R2 log(T ) + J(g(x1)\u2212 g(x\u2217)) T . (59)\nProof: If f + g is strongly convex, we have\nf(xt) + g(xt)\u2212 [f(x) + g(x)] \u2264 \u3008\u2207f(xt) + g\u2032(xt),xt \u2212 x\u3009 \u2212 \u03b3 2 \u2016x\u2212 xt\u201622 . (60)\nPlugging back into (54), following similar derivation in Theorem 2 and Theorem 3 complete the proof."}, {"heading": "4.3 ORBCD with Variance Reduction", "text": "The following results mostly follow [36, 13].\nLemma 2 Define h(x) = f(x) + g(x). Let x\u2217 be an optimal solution and L = maxi Li, we have\n1\nI\nI \u2211\ni=1\n\u2016\u2207fi(x)\u2212\u2207fi(x\u2217)\u201622 \u2264 2L[h(x) \u2212 h(x\u2217)] . (61)\nLemma 3 Let vikjk and x k+1 jk be generated by (24)-(28). Conditioned on xk, we have\nE\u2016vikjk \u2212\u2207jkf(x k)\u201622 \u2264\n4L\nJ [h(xk)\u2212 h(x\u2217) + h(x\u0303)\u2212 h(x\u2217)] . (62)\nProof: Conditioned on xk, we have\nEik [\u2207fik(xk)\u2212\u2207fik(x\u0303) + \u00b5\u0303] = 1\nI\nI \u2211\ni=1\n[\u2207fi(xk)\u2212\u2207fi(x\u0303) + \u00b5\u0303] = \u2207f(xk) . (63)\nNote xk is independent of ik, jk. Conditioned on xk, using (24) gives\nE\u2016vikjk \u2212\u2207jkf(x k)\u201622 = Eik [Ejk\u2016v ik jk \u2212\u2207jkf(xk)\u201622] = Eik [Ejk\u2016\u2207jkfik(xk)\u2212\u2207jkfik(x\u0303) + \u00b5\u0303jk \u2212\u2207jkf(xk)\u201622]\n= 1\nJ Eik\u2016\u2207fik(xk)\u2212\u2207fik(x\u0303) + \u00b5\u0303\u2212\u2207f(xk)\u201622\n\u2264 1 J Eik\u2016\u2207fik(xk)\u2212\u2207fik(x\u0303)\u201622 \u2264 2 J Eik\u2016\u2207fik(xk)\u2212\u2207fik(x\u2217)\u201622 + 2 J Eik\u2016\u2207fik(x\u0303)\u2212\u2207fik(x\u2217)\u201622\n= 2\nIJ\nI \u2211\ni=1\n\u2016\u2207fi(xk)\u2212\u2207fi(x\u2217)\u201622 + 2\nIJ\nI \u2211\ni=1\n\u2016\u2207fi(x\u0303)\u2212\u2207fi(x\u2217)\u201622\n\u2264 4L J [h(xk)\u2212 h(x\u2217) + h(x\u0303)\u2212 h(x\u2217)] . (64)\nThe first inequality uses the fact E\u2016\u03b6 \u2212 E\u03b6\u201622 \u2264 E\u2016\u03b6\u201622 given a random variable \u03b6 , the second inequality uses \u2016a+ b\u201622 \u2264 2\u2016a\u201622 + 2\u2016b\u201622, and the last inequality uses Lemma 2.\nNow, we are ready to establish the linear convergence rate of ORBCD with variance reduction for strongly convex functions.\nTheorem 5 Let h(x) = f(x)+ g(x) and xt be generated by ORBCD with variance reduction (25)-(28). jk is sampled randomly and uniformly from {1, \u00b7 \u00b7 \u00b7 , J}. Assume \u03b7 > 3L and m satisfy the following condition:\n\u03c1 = 2L \u03b7 \u2212 3L + (\u03b7 \u2212 L)J (\u03b7 \u2212 3L)m \u2212 1 m + \u03b7(\u03b7 \u2212 L)J (\u03b7 \u2212 3L)m\u03b3 < 1 . (65)\nThen h(x) converges linearly in expectation, i.e.,\nE\u03beh(x t+1)\u2212 h(x\u2217) \u2264 \u03c1[E\u03beh(xt)\u2212 h(x\u2217)] . (66)\nProof: The optimality condition of (28) is\n\u3008vikjk + \u03b7(x k+1 jk \u2212 xkjk) + g \u2032 jk (xk+1jk ),x k+1 jk \u2212 xjk\u3009 \u2264 0 . (67)\nRearranging the terms yields\n\u3008vikjk + g \u2032 jk (xk+1jk ),x k+1 jk \u2212 xjk\u3009 \u2264 \u2212\u03b7\u3008xk+1jk \u2212 x k jk ,xk+1jk \u2212 xjk\u3009\n\u2264 \u03b7 2 (\u2016xjk \u2212 xkjk\u2016 2 2 \u2212 \u2016xjk \u2212 xk+1jk \u2016 2 2 \u2212 \u2016xk+1jk \u2212 x k jk \u201622) = \u03b7\n2 (\u2016x\u2212 xk\u201622 \u2212 \u2016x\u2212 xk+1\u201622 \u2212 \u2016xk+1jk \u2212 x k jk \u201622) , (68)\nwhere the last equality uses xk+1 = (xk+1jk ,x t k 6=jk). Using the convecxity of gj and the fact that g(x k) \u2212 g(xk+1) = gjk(x k)\u2212 gjk(xk+1), we have\n\u3008vikjk ,x k jk \u2212 xjk\u3009+ gjk(xk)\u2212 gjk(x) \u2264 \u3008v ik jk ,xkjk \u2212 x k+1 jk \u3009+ g(xk)\u2212 g(xk+1)\n+ \u03b7\n2 (\u2016x\u2212 xk\u201622 \u2212 \u2016x\u2212 xk+1\u201622 \u2212 \u2016xk+1jk \u2212 x k jk \u201622) . (69)\nSince f is smooth, we have\n\u3008\u2207jkf(xk),xkjk \u2212 x k+1 jk \u3009 \u2264 f(xk)\u2212 f(xk+1) + L 2 \u2016xkjk \u2212 x k+1 jk \u201622 . (70)\nLetting x = x\u2217 and using the smoothness of f , we have\n\u3008vikjk ,x k jk \u2212 xjk\u3009+ gjk(xk)\u2212 gjk(x\u2217) \u2264 \u3008v ik jk \u2212\u2207jkf(xk),xkjk \u2212 x k+1 jk \u3009+ h(xk)\u2212 h(xk+1)\n+ \u03b7\n2 (\u2016x\u2217 \u2212 xk\u201622 \u2212 \u2016x\u2217 \u2212 xk+1\u201622 \u2212 \u2016xk+1jk \u2212 x k jk \u201622) +\nL 2 \u2016xkjk \u2212 x k+1 jk \u201622\n\u2264 1 2(\u03b7 \u2212 L)\u2016v ik jk \u2212\u2207jkf(xk)\u201622 + h(xk)\u2212 h(xk+1) + \u03b7 2 (\u2016x\u2217 \u2212 xk\u201622 \u2212 \u2016x\u2217 \u2212 xk+1\u201622) , (71)\nTaking expectation over ik, jk on both sides and using Lemma 3, we have\nE[\u3008vikjk ,x k jk \u2212 x\u2217jk\u3009+ gjk(x k)\u2212 gjk(x\u2217)]\n\u2264 2L J(\u03b7 \u2212 L) [h(x k)\u2212 h(x\u2217) + h(x\u0303)\u2212 h(x\u2217)] + h(xk)\u2212 Eh(xk+1) + \u03b7 2 (\u2016x\u2217 \u2212 xk\u201622 \u2212 E\u2016x\u2217 \u2212 xk+1\u201622) .\n(72)\nThe left hand side can be rewritten as\nE[\u3008vikjk ,x k jk \u2212 x\u2217jk\u3009+ gjk(x k)\u2212 gjk(x\u2217)] =\n1 J [Eik\u3008vik ,xk \u2212 x\u2217\u3009+ g(xk)\u2212 g(x\u2217)]\n= 1 J [\u3008\u2207f(xk),xk \u2212 x\u2217\u3009+ g(xk)\u2212 g(x\u2217)] \u2265 1 J [h(xk)\u2212 h(x\u2217)] . (73)\nPlugging into (72) gives\n1 J [h(xk)\u2212 h(x\u2217)] \u2264 2L J(\u03b7 \u2212 L) [h(x k)\u2212 h(x\u2217) + h(x\u0303)\u2212 h(x\u2217)]\n+ h(xk)\u2212 Eh(xk+1) + \u03b7 2 (\u2016x\u2217 \u2212 xk\u201622 \u2212 E\u2016x\u2217 \u2212 xk+1\u201622) . (74)\nRearranging the terms yields\n\u03b7 \u2212 3L J(\u03b7 \u2212 L) [h(x k)\u2212 h(x\u2217)] \u2264 2L J(\u03b7 \u2212 L) [h(x\u0303)\u2212 h(x \u2217)]\n+ h(xk)\u2212 Eh(xk+1) + \u03b7 2 (\u2016x\u2212 xk\u201622 \u2212 E\u2016x\u2212 xk+1\u201622) . (75)\nAt time t + 1, we have x0 = x\u0303 = xt. By running the algorithm m steps, xt+1 = 1m \u2211m\nk=1 xk. Summing over k = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1 and taking expectation with respect to the history of random variable \u03be, we have\n\u03b7 \u2212 3L J(\u03b7 \u2212 L)\nm\u22121 \u2211\nk=0\n[E\u03beh(xk)\u2212 h(x\u2217)]\n\u2264 2Lm J(\u03b7 \u2212 L) [E\u03beh(x\u0303)\u2212 h(x \u2217)] + E\u03beh(x0)\u2212 E\u03beh(xm) + \u03b7 2 (E\u03be\u2016x\u2217 \u2212 x0\u201622 \u2212 E\u03be\u2016x\u2217 \u2212 xm\u201622) . (76)\nRearranging the terms gives\n\u03b7 \u2212 3L J(\u03b7 \u2212 L)\nm\u22121 \u2211\nk=1\n[E\u03beh(x k)\u2212 h(x\u2217)] + E\u03beh(xm)\u2212 h(x\u2217)\n\u2264 2Lm J(\u03b7 \u2212 L) [E\u03beh(x\u0303)\u2212 h(x \u2217)] + (1\u2212 \u03b7 \u2212 3L J(\u03b7 \u2212 L) )[E\u03beh(x0)\u2212 h(x \u2217)] + \u03b7 2 (E\u03be\u2016x\u2217 \u2212 x0\u201622 \u2212 E\u03be\u2016x\u2217 \u2212 xm\u201622) .\n(77)\nSince 0 < \u03b7\u22123L J(\u03b7\u2212L) < 1 for any \u03b7 > 3L and Eh(xm)\u2212 h(x\u2217) \u2265 0, we have\n\u03b7 \u2212 3L J(\u03b7 \u2212 L)\nm \u2211\nk=1\n[E\u03beh(x k)\u2212 h(x\u2217)]\n\u2264 2Lm J(\u03b7 \u2212 L) [E\u03beh(x\u0303)\u2212 h(x \u2217)] + (1\u2212 \u03b7 \u2212 3L J(\u03b7 \u2212 L))[E\u03beh(x0)\u2212 h(x \u2217)] + \u03b7 2 (\u2016x \u2212 x0\u201622 \u2212 E\u03be\u2016x\u2212 xm\u201622) .\n(78)\nApplying the Jesen\u2019s inequality to the left hand side and using h(xt+1) \u2264 1 m \u2211m k=1 h(xk) where x t+1 = 1 m \u2211m k=1 xk, we have\n\u03b7 \u2212 3L J(\u03b7 \u2212 L)m[E\u03beh(x t+1)\u2212 h(x\u2217)] \u2264 [ 2Lm J(\u03b7 \u2212 L) + 1\u2212 \u03b7 \u2212 3L J(\u03b7 \u2212 L) ][E\u03beh(x t)\u2212 h(x\u2217)] + \u03b7 2 E\u03be\u2016x\u2217 \u2212 xt\u201622 ,\n(79)\nwhere ther right hand side uses xt = x0 = x\u0303. Assuming h(x) is \u03b3-strongly convex, we have\n\u2016x\u2217 \u2212 xt\u201622 \u2264 2 \u03b3 [h(xt)\u2212 h(x\u2217)] . (80)\nPlugging into (79) yields\n\u03b7 \u2212 3L J(\u03b7 \u2212 L)m[E\u03beh(x t+1)\u2212 h(x\u2217)] \u2264 [ 2Lm J(\u03b7 \u2212 L) + 1\u2212 \u03b7 \u2212 3L J(\u03b7 \u2212 L) + \u03b7 \u03b3 ][E\u03beh(x t)\u2212 h(x\u2217)] . (81)\nDividing both sides by \u03b7\u22123L J(\u03b7\u2212L)m, we have\nE\u03beh(x t+1)\u2212 h(x\u2217) \u2264 \u03c1[E\u03beh(xt)\u2212 h(x\u2217)] , (82)\nwhere\n\u03c1 = 2L \u03b7 \u2212 3L + (\u03b7 \u2212 L)J (\u03b7 \u2212 3L)m \u2212 1 m + \u03b7(\u03b7 \u2212 L)J (\u03b7 \u2212 3L)m\u03b3 < 1 , (83)\nwhich completes the proof."}, {"heading": "5 Conclusions", "text": "We proposed online randomized block coordinate descent (ORBCD) which combines online/stochastic gradient descent and randomized block coordinate descent. ORBCD is well suitable for large scale high dimensional problems with non-overlapping composite regularizers. We established the rate of convergence for ORBCD, which has the same order as OGD/SGD. For stochastic optimization with strongly convex functions, ORBCD can converge at a geometric rate in expectation by reducing the variance of stochastic gradient."}], "references": [{"title": "Convex Optimization with Sparsity-Inducing Norms", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["J. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Coordinate descent method for large-scale l2-loss linear support vector machines", "author": ["K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Proximal splitting methods in signal processsing. Fixed-Point Algorithms for Inverse Problems in Science and Engineering", "author": ["P. Combettes", "J. Pesquet"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Composite objective mirror descent", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A note on the group lasso and sparse group lasson", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["C.-J. Hsieh", "K.-W. Chang", "S. Keerthi C.-J. Lin", "S. Sundararajan"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Juditsky", "A. Nemirovski", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Richtarik. Semi-stochastic gradient descent methods", "author": ["P.J. Konecny"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Coordinate descent optimization for l1 minimization with application to compressed sensing; a greedy algorithm", "author": ["Y. Li", "S. Osher"], "venue": "Inverse Problems and Imaging,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "On the complexity analysis of randomized block-coordinate descent methods", "author": ["Z. Lu", "L. Xiao"], "venue": "ArXiv,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "On the convergence of the coordinate descent method for convex differentiable minimization", "author": ["Z.-Q. Luo", "P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Mixed optimization for smooth functions", "author": ["L. Zhang M. Mahdavi", "R. Jin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["N. Le Roux M. Schmidt", "F. Bach"], "venue": "Technical Report HAL 00860051,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Introductory Lectures on Convex Optimization: A", "author": ["Y. Nesterov"], "venue": "Basic Course. Kluwer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "Technical Report 76, Center for Operation Research and Economics (CORE), Catholic University of Louvain (UCL),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization methods", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richtarik", "M. Takac"], "venue": "Mathematical Programming,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["P. Richtarik", "M. Takac"], "venue": "ArXiv,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N. Le Roux", "M. Schmidt", "F. Bach"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "On the non-asymptotic convergence of cyclic coordinate descent methods", "author": ["A. Saha", "A. Tewari"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Stochastic methods for l1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Mini-batch primal and dual methods for svms", "author": ["M. Takac", "A. Bijral", "P. Richtarik", "N. Srebro"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Linear convergence with condition number independent access of full gradients", "author": ["L. Zhang", "M. Mahdavi", "R. Jin"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "In regularized risk minimization problems [10], f is the average of losses of a large number of samples and g is a simple regularizer on high dimensional features to induce structural sparsity [1].", "startOffset": 193, "endOffset": 196}, {"referenceID": 30, "context": "For example, in lasso [32], fi is a square loss or logistic loss function and g(x) = \u03bb\u2016x\u20161 where \u03bb is the tuning parameter.", "startOffset": 22, "endOffset": 26}, {"referenceID": 35, "context": "In group lasso [37], gj(xj) = \u03bb1\u2016xj\u20162, which enforces group sparsity among variables.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "To induce both group sparsity and sparsity, sparse group lasso [9] uses composite regularizers gj(xj) = \u03bb1\u2016xj\u20162 + \u03bb2\u2016xj\u20161.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 54, "endOffset": 57}, {"referenceID": 32, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 1, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "This variant of GD is often called proximal splitting [6] or proximal gradient descent (PGD) [34, 2] or forward/backward splitting method (FOBOS) [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 25, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 13, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 3, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 37, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 10, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 6, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 7, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 33, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 131, "endOffset": 160}, {"referenceID": 22, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 2, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 24, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 23, "context": "To address the computational bottleneck, two types of low cost-per-iteration methods, online/stochastic gradient descent (OGD/SGD) [26, 14, 4, 39, 11, 7, 8, 35] and randomized block coordinate descent (RBCD) [23, 3, 25, 24], have been rigorously studied in both theory and applications.", "startOffset": 208, "endOffset": 223}, {"referenceID": 29, "context": "For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30].", "startOffset": 75, "endOffset": 87}, {"referenceID": 28, "context": "For large scale problems, it has been shown that OGD/SGD is faster than GD [31, 29, 30].", "startOffset": 75, "endOffset": 87}, {"referenceID": 21, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 5, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 32, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 1, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 6, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 7, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 33, "context": "OGD and SGD have been generalized to handle composite objective functions [22, 6, 34, 2, 7, 8, 35].", "startOffset": 74, "endOffset": 98}, {"referenceID": 26, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 19, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 34, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 12, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 18, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 36, "context": "In stochastic optimization, the slow convergence speed is caused by the variance of stochastic gradients due to random samples, and considerable efforts have thus been devoted to reducing the variance to accelerate SGD [27, 20, 36, 13, 19, 38].", "startOffset": 219, "endOffset": 243}, {"referenceID": 26, "context": "Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21].", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "Stochastic average gradient (SVG) [27] is the first SGD algorithm achieving the linear convergence rate for stronly convex functions, catching up with the convergence speed of GD [21].", "startOffset": 179, "endOffset": 183}, {"referenceID": 12, "context": "To address the issue of storage and better explain the faster convergence, [13] proposed an explicit variance reduction scheme into SGD.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "The similar idea was also proposed independently by [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "The results of SVRG is then improved in [15].", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "In [36], SVRG is generalized to solve composite minimization problem by incorporate the variance reduction technique into proximal gradient method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 23, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 16, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 28, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 4, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 11, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 15, "context": "On the other hand, RBCD [23, 24, 17, 30, 5, 12, 16] has become increasingly popular due to high dimensional problem with structural regularizers.", "startOffset": 24, "endOffset": 51}, {"referenceID": 22, "context": "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].", "startOffset": 120, "endOffset": 128}, {"referenceID": 16, "context": "The iteration complexity of RBCD was established in [23], improved and generalized to composite minimization problem by [24, 17].", "startOffset": 120, "endOffset": 128}, {"referenceID": 22, "context": "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].", "startOffset": 121, "endOffset": 133}, {"referenceID": 23, "context": "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].", "startOffset": 121, "endOffset": 133}, {"referenceID": 16, "context": "RBCD can choose a constant step size and converge at the same rate as GD, although the constant is usually J times worse [23, 24, 17].", "startOffset": 121, "endOffset": 133}, {"referenceID": 27, "context": "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].", "startOffset": 97, "endOffset": 109}, {"referenceID": 31, "context": "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].", "startOffset": 97, "endOffset": 109}, {"referenceID": 17, "context": "Block coordinate descent (BCD) methods have also been studied under a deterministic cyclic order [28, 33, 18].", "startOffset": 97, "endOffset": 109}, {"referenceID": 31, "context": "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].", "startOffset": 60, "endOffset": 68}, {"referenceID": 17, "context": "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].", "startOffset": 60, "endOffset": 68}, {"referenceID": 27, "context": "Although the convergence of cyclic BCD has been established [33, 18], the iteration of complexity is still unknown except for special cases [28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "To accelerate the convergence speed of ORBCD, we adopt the varaince reduction technique [13] to alleviate the effect of randomness.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "Instead, \u03bc\u0303 can be partially computed at each iteration and then stored for the next retrieval, which may be useful when the computation of full gradient is expensive or data is partially available at the moment [23].", "startOffset": 212, "endOffset": 216}, {"referenceID": 12, "context": "To accelerate the SGD by reducing the variance of stochastic gradient, stochastic variance reduced gradient (SVRG) was proposed by [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 34, "context": "[36] extends SVRG to composite functions (1), called prox-SVRG.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "(8) By reduding the variance of stochastic gradient, x can converge to x\u2217 at the same rate as GD, which has been proved in [13, 36].", "startOffset": 123, "endOffset": 131}, {"referenceID": 34, "context": "(8) By reduding the variance of stochastic gradient, x can converge to x\u2217 at the same rate as GD, which has been proved in [13, 36].", "startOffset": 123, "endOffset": 131}, {"referenceID": 34, "context": "For strongly convex functions, prox-SVRG [36] can converge linearly in expectation if \u03b7 > 4L and m satisfy the following condition:", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt \u3008\u2207jtf(x ),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622 .", "startOffset": 21, "endOffset": 33}, {"referenceID": 23, "context": "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt \u3008\u2207jtf(x ),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622 .", "startOffset": 21, "endOffset": 33}, {"referenceID": 16, "context": "At iteration t, RBCD [23, 24, 17] randomly picks jt-th coordinate and solves the following problem: x jt = argminxjt \u3008\u2207jtf(x ),xjt\u3009+ gjt(xjt) + \u03b7t 2 \u2016xjt \u2212 xtjt\u201622 .", "startOffset": 21, "endOffset": 33}, {"referenceID": 22, "context": "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].", "startOffset": 90, "endOffset": 102}, {"referenceID": 23, "context": "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].", "startOffset": 90, "endOffset": 102}, {"referenceID": 16, "context": "Therefore, RBCD converges at the same rate as GD, although the constant is J times larger [23, 24, 17].", "startOffset": 90, "endOffset": 102}, {"referenceID": 6, "context": "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.", "startOffset": 43, "endOffset": 53}, {"referenceID": 7, "context": "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.", "startOffset": 43, "endOffset": 53}, {"referenceID": 33, "context": "(21) The online-stochastic conversion rule [7, 8, 35] still holds here.", "startOffset": 43, "endOffset": 53}, {"referenceID": 34, "context": "3 ORBCD with variance reduction In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD.", "startOffset": 101, "endOffset": 109}, {"referenceID": 12, "context": "3 ORBCD with variance reduction In the stochastic setting, we apply the variance reduction technique [36, 13] to accelerate the rate of convergence of ORBCD, abbreviated as ORBCDVD.", "startOffset": 101, "endOffset": 109}, {"referenceID": 22, "context": "The computation of full gradient may require substantial computational eorts, let alone data may be only partially available at the moment [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "For prox-SVRG [36], setting \u03b7 = 10L and m = 100L/\u03b3, \u03c1 \u2248 5/6 in (9).", "startOffset": 14, "endOffset": 18}, {"referenceID": 34, "context": "3 ORBCD with Variance Reduction The following results mostly follow [36, 13].", "startOffset": 68, "endOffset": 76}, {"referenceID": 12, "context": "3 ORBCD with Variance Reduction The following results mostly follow [36, 13].", "startOffset": 68, "endOffset": 76}], "year": 2017, "abstractText": "Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent ( OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, for the first time, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}