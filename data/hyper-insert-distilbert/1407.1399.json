{"id": "1407.1399", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2014", "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM", "abstract": "higher - graded order identity tensors also are becoming prevalent here in many scientific emerging areas departments such as computer vision, social network analysis, data mining studies and neuroscience. traditional tensor decomposition reasoning approaches rarely face to three major challenges : vector model selecting, gross corruptions and computational formulation efficiency. to please address finding these complex problems, we first propose construct a common parallel computational trace norm classical regularized functional tensor decomposition method, examine and formulate utilizing it as a convex computational optimization problem. this improved method does not truly require the rank of each mode simply to be specified beforehand, and users can again automatically efficiently determine the number likelihood of bias factors generated in each mode through our optimization treatment scheme. by considering the simple low - hierarchy rank structure of the second observed tensor, we immediately analyze the equivalent performance relationship of restoring the trace norm between a low - tier rank tensor and add its core database tensor. then, we effectively cast converts a non - static convex tensor tensor database decomposition model into a poorly weighted combination composed of multiple much richer smaller - less scale matrix linear trace norm minimization. finally, we might develop two parallel alternating column direction methods of multipliers ( admm ) to solve our problems. flawed experimental results verify that our matrix regularized formulation calculation is effective, respectively and instead our integration methods sometimes are robust to noise effects or outliers.", "histories": [["v1", "Sat, 5 Jul 2014 11:58:30 GMT  (106kb)", "http://arxiv.org/abs/1407.1399v1", "9 pages, 5 figures, AAAI 2014"]], "COMMENTS": "9 pages, 5 figures, AAAI 2014", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["fanhua shang", "yuanyuan liu", "james cheng"], "accepted": true, "id": "1407.1399"}, "pdf": {"name": "1407.1399.pdf", "metadata": {"source": "CRF", "title": "Generalized Higher-Order Tensor Decomposition via Parallel ADMM", "authors": ["Fanhua Shang", "Yuanyuan Liu", "James Cheng"], "emails": ["jcheng}@cse.cuhk.edu.hk;", "yyliu@se.cuhk.edu.hk"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 7.\n13 99\nv1 [\ncs .N\nA ]\n5 J\nul 2\n01 4"}, {"heading": "Introduction", "text": "The term tensor used in the context of this paper refers to a multi-dimensional array, also known as a multi-way or multi-mode array. For example, if X \u2208 RI1\u00d7I2\u00d7I3 , then we say X is a third-order tensor, where order is the number of ways or modes of the tensor. Thus, vectors and matrices are first-order and second-order tensors, respectively. Higher-order tensors arise in a wide variety of application areas, such as machine learning (Tomioka and Suzuki, 2013; Signoretto et al., 2014), computer vision (Liu et al., 2009), data mining (Yilmaz et al., 2011; Morup, 2011; Narita et al., 2012; Liu et al., 2014), numerical linear algebra (Lathauwer et al., 2000a; 2000b), and so on. Especially, with the rapid development of modern computer technology in recent years, tensors are becoming increasingly ubiquitous such as multi-channel images and videos, and have become increasingly popular (Kolda and Bader, 2009). When working with high-order tensor data, various new computational\nCopyright c\u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nchallenges arise due to the exponential increase in time and memory space complexity when the number of orders increases. This is called the curse of dimensionality. In practice, the underlying tensor is often low-rank, even though the actual data may not be due to noise or arbitrary errors. Essentially, the major component contained in the given tensor is often governed by a relatively small number of latent factors.\nOne standard tool to alleviate the curse is tensor decomposition. Decomposition of high-order tensors into a small number of factors has been one of the main tasks in multi-way data analysis, and commonly takes two forms: Tucker decomposition (Tucker, 1966) and CANDECOMP/PARAFAC (CP) (Harshman, 1970) decomposition. There are extensive studies in the literature for finding the Tucker decomposition and the CP decomposition for higherorder tensors (Kolda and Bader, 2009). In those tensor decomposition methods, their goal is to (approximately) reconstruct the input tensor as a sum of simpler components with the hope that these simpler components would reveal the latent structure of the data. However, existing tensor decomposition methods face three major challenges: rank selection, outliers and gross corruptions, and computational efficiency. Since the Tucker and CP decomposition methods are based on least-squares approximation, they are also very sensitive to outliers and gross corruptions (Goldfarb and Qin, 2014). In addition, the performance of those methods is usually sensitive to the given ranks of the involved tensor (Liu et al., 2009). To address the problems, we propose two robust and parallel higher-order tensor decomposition methods with trace norm regularization.\nRecently, much attention has been drawn to the low-rank tensor recovery problem, which arises in a number of realword applications, such as 3D image recovery, video inpainting, hyperspectral data recovery, and face reconstruction. Compared with matrix-based analysis methods, tensorbased multi-linear data analysis has shown that tensor models are capable of taking full advantage of the high-order structure to provide better understanding and more precision. The key idea of low-rank tensor completion and recovery methods is to employ matrix trace norm minimization (also known as the nuclear norm, which is the convex surrogate of the rank of the involved matrix). In addition, there are some theoretical developments that guarantee reconstruction\nof a low-rank tensor from partial measurements or grossly corrupted observations via solving the trace norm minimization problem under some reasonable conditions (Tomioka et al., 2011; Shi et al., 2013). Motivated by the recent progress in tensor completion and recovery, one goal of this paper is to extend the trace norm regularization to robust higherorder tensor decomposition.\nDifferent from existing tensor decomposition methods, we first propose a parallel trace norm regularized tensor decomposition method, which can automatically determine the number of factors in each mode through our optimization scheme. In other words, this method does not require the rank of each mode to be specified beforehand. In addition, by considering the low-rank structure of the observed tensor and further improving the scalability of our convex method, we analyze the equivalent relationship of the trace norm between a low-rank tensor and its core tensor. Then, we cast the non-convex trace norm regularized higher-order orthogonal iteration model into a weighted combination of multiple much-smaller-scale matrix trace norm minimization. Moreover, we design two parallel alternating direction methods of multipliers (ADMM) to solve the proposed problems, which are shown to be fast, insensitive to initialization and robust to noise and/or outliers with extensive experiments."}, {"heading": "Notations and Related Work", "text": "We first introduce the notations, and more details can be seen in Kolda and Bader (2009). An N th-order tensor is denoted by a calligraphic letter, e.g., T \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , and its entries are denoted by ti1\u00b7\u00b7\u00b7in\u00b7\u00b7\u00b7iN , where in \u2208 {1, \u00b7 \u00b7 \u00b7 , In} for 1 \u2264 n \u2264 N . Fibers are the higher-order analogue of matrix rows and columns. The mode-n fibers are vectors ti1\u00b7\u00b7\u00b7in\u22121in+1\u00b7\u00b7\u00b7iN that are obtained by fixing the values of {i1, \u00b7 \u00b7 \u00b7 , iN}\\in.\nThe mode-n unfolding, also known as matricization, of an N th-order tensor T is denoted by T(n) \u2208 RIn\u00d7\u03a0j 6=nIj and arranges the mode-n fibers to be the columns of the resulting matrix T(n) such that the mode-n fiber becomes the row index and all other (N \u2212 1) modes become the column indices. The tensor element (i1, i2, \u00b7 \u00b7 \u00b7 , iN) is mapped to the matrix element (in, j), where\nj = 1 +\nN\u2211\nk=1,k 6=n\n(ik \u2212 1)Jk with Jk = k\u22121\u220f\nm=1,m 6=n\nIm.\nThe inner product of two same-sized tensors A \u2208 RI1\u00d7I2\u00d7 \u00b7\u00b7\u00b7\u00d7IN and B \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN is the sum of the product of their entries, < A,B >=\u2211\ni1,\u00b7\u00b7\u00b7iN ai1\u00b7\u00b7\u00b7iN bi1\u00b7\u00b7\u00b7iN . The Frobenius norm of an N th-\norder T is defined as:\n\u2016T \u2016F :=\n\u221a\u221a\u221a\u221a I1\u2211\ni1=1\n\u00b7 \u00b7 \u00b7 IN\u2211\niN=1\nt2i1\u00b7\u00b7\u00b7iN .\nLet A and B be two matrices of size m \u00d7 n and p \u00d7 q, respectively. The Kronecker product of two matrices A and B, denoted by A\u2297B, is an mp\u00d7 nq matrix given by:\nA\u2297B = [aijB]mp\u00d7nq.\nThe n-mode matrix product of a tensor T with a matrix U \u2208 R\nJ\u00d7In , denoted by T \u00d7n U \u2208 RI1\u00d7\u00b7\u00b7\u00b7In\u22121\u00d7J\u00d7In+1\u00d7\u00b7\u00b7\u00b7\u00d7IN , is defined as:\n(T \u00d7n U)i1\u00b7\u00b7\u00b7in\u22121jin+1\u00b7\u00b7\u00b7iN = In\u2211\nin=1\nti1i2\u00b7\u00b7\u00b7iNujin ."}, {"heading": "Tensor Decomposition", "text": "We will review two popular models for tensor decomposition, i.e., the Tucker decomposition and the CANDECOMP/PARAFAC (CP) decomposition. It is well known that finding the CP decomposition with the minimum tensor rank is a hard problem, and there is no straightforward algorithm for computing the rank for higher-order tensors (Hillar and Lim, 2013). The Tucker decomposition decomposes a given tensor T into a core tensor multiplied by a factor matrix along each mode as follows:\nT = G \u00d71 U1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7N UN , (1)\nwhere Un \u2208 RIn\u00d7Rn are the factor matrices, which can be thought of as the principal components in each mode, and the entries of the core tensor G \u2208 RR1\u00d7R2\u00b7\u00b7\u00b7\u00d7RN show the level of interaction between the different components. Since the decomposition rank Rn (n = 1, \u00b7 \u00b7 \u00b7 , N) is in general much smaller than In (n = 1, \u00b7 \u00b7 \u00b7 , N), the core tensor G can be thought of as a low-rank version of T (e.g., the Tucker decomposition of a third-order tensor is illustrated in Figure 1). In this sense, the storage of the Tucker decomposition form can be significantly smaller than that of the original tensor. Moreover, unlike the rank of the tensor, Rn, i.e., the mode-n rank (n = 1, \u00b7 \u00b7 \u00b7 , N), is the rank of the mode-n unfolding, and is clearly computable. Hence, we are particularly interested in extending the Tucker decomposition for tensor analysis.\nIf the factor matrices of the Tucker decomposition are constrained orthogonal, the decomposition form is referred to as the higher-order singular value decomposition (HOSVD, Lathauwer et al., 2000a) or higher-order orthogonal iteration (HOOI, Lathauwer et al., 2000b), where the latter leads to the estimation of best rank(R1, R2, . . . , RN ) approximations while the truncation of HOSVD may achieve a good rank-(R1, R2, . . . , RN ) approximation but in general not the best possible one (Lathauwer et al., 2000b). Therefore, HOOI is the most widely used tensor decomposition method (Kolda and Bader, 2009), and takes the following form\nmin G,Un\n\u2016T \u2212 G \u00d71 U1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7N UN\u2016 2 F ,\ns.t., UTn Un = IRn , n = 1, \u00b7 \u00b7 \u00b7 , N. (2)"}, {"heading": "Convex Tensor Decomposition Method", "text": ""}, {"heading": "Convex Tensor Decomposition Model", "text": "Given a tensor T , our goal is to find a low-rank tensor X , in order to minimize the Frobenius norm of their difference as follows:\nmin X\n1 2 \u2016X \u2212 T \u20162F . (3)\nDifferent from the matrix case, the low-rank tensor estimation problem (3) is in general hard to solve (Narita et al., 2012). Following the progress in tensor completion, we cast it into a (weighted) trace norm minimization problem:\nmin X\nN\u2211\nn=1\n\u2016X(n)\u2016tr + \u03bb\n2 \u2016X \u2212 T \u20162F , (4)\nwhere \u2016X(n)\u2016tr denotes the trace norm of the unfolded matrix X(n), i.e., the sum of its singular values, \u03bb > 0 is a regularization parameter. For handling the unbalanced target tensor, we briefly introduce the preselected weights \u03b1n \u2265 0 (satisfying \u2211 n \u03b1n = 1) to the trace norm term in (4)."}, {"heading": "Parallel Optimization Algorithm", "text": "Due to the interdependent matrix trace norm terms, the proposed tensor decomposition model (4) is very difficult to solve. Thus, we introduce some auxiliary variables Mn into the model (4) and reformulate it into the following equivalent form:\nmin X ,{Mn}\nN\u2211\nn=1\n\u2016Mn,(n)\u2016tr + \u03bb\n2 \u2016X \u2212 T \u20162F ,\ns.t., Mn = X , n = 1, \u00b7 \u00b7 \u00b7 , N.\n(5)\nIn the following, we will design a parallel alternating direction method of multipliers (ADMM) for solving the problem (5). The algorithm decomposes the original problem into smaller subproblems and solves them individually in parallel at each iteration. The parallel ADMM for the problem (5) is derived by minimizing the augmented Lagrange function L\u00b5 with respect to (X , {Mn}) in parallel, and then updating the multiplier tensor Yn. But the parallelization is likely to diverge, therefore, it is necessary to modify the common algorithm in a certain way to guarantee its convergence. Several variants of parallel ADMM have been proposed in He (2009) and Deng et al., (2013) by taking additional correction steps at every iteration. Similar to Deng et al., (2013), we add some proximal terms to each subproblems and attach a relaxation parameter \u03b3 > 0 for the update of the penalty parameter \u00b5 > 0.\nComputing {Mk+1n , n = 1, \u00b7 \u00b7 \u00b7 , N}: By keeping all other variables fixed, the optimal Mkn is the solution to the following problem:\nmin Mn\n\u2016Mn, (n)\u2016tr + \u00b5\n2 \u2016Mn \u2212X\nk + Ykn/\u00b5\u2016 2 F\n+ \u03c4n 2 \u2016Mn \u2212M k n\u2016 2 F ,\n(6)\nAlgorithm 1 Solving problem (5) via parallel ADMM Input: Given T , \u03bb, and \u00b5. Initialize: M0n = X\n0 = Y0n = 0, \u2200n \u2208 {1, \u00b7 \u00b7 \u00b7 , N}. 1: while not converged do 2: for n = 1, \u00b7 \u00b7 \u00b7 , N do 3: Update Mk+1n by (7),\nwhere UnSnV Tn = \u00b5X k(n)\u2212Y k (n)+\u03c4nM k (n) \u00b5+\u03c4n .\n4: end for 5: Update X k+1 by (9). 6: for n = 1, \u00b7 \u00b7 \u00b7 , N do 7: Yk+1n = Y k n + \u03b3\u00b5(M k+1 n \u2212X\nk+1). 8: end for 9: Check the convergence condition,\n\u2016X k+1 \u2212 T \u2016F < Tol. 10: end while 11: G = X k+1 \u00d71 UT1 \u00b7 \u00b7 \u00b7 \u00d7N U T N . Output: X k, G, and Un, n = 1, \u00b7 \u00b7 \u00b7 , N .\nwhere \u03c4n > 0 is a constant for the proximal term \u2016Mn \u2212 Mkn\u2016 2 F . Following (Cai et al., 2010), we obtain a closedform solution to the problem (6) via the following proximal operator of the trace norm:\nMk+1n = refold(prox 1 \u00b5+\u03c4n\n( \u00b5X k(n) \u2212 Y k (n) + \u03c4nM k (n)\n\u00b5+ \u03c4n )),\n(7) where refold(\u00b7) denotes the refolding of the matrix into a tensor, i.e., the reverse process of unfolding.\nComputing X k+1: The optimal X k+1 with all other variables fixed is the solution to the following problem:\nmin X\n\u03bb 2 \u2016X \u2212 T \u20162F +\nN\u2211\nn=1\n\u00b5 2 \u2016Mkn \u2212X + Y k n/\u00b5\u2016 2 F\n+ \u03c4N+1 2 \u2016X \u2212 X k\u20162F ,\n(8)\nwhere \u03c4N+1 > 0 is a constant for the proximal term \u2016X \u2212X k\u20162F . Since the problem (8) is a smooth convex optimization problem, it is easy to show that the optimal solution to (8) is given by\nX k+1 =\n\u2211N n=1(\u00b5M k n + Y k n) + \u03bbT + \u03c4N+1X k\nN\u00b5+ \u03bb+ \u03c4N+1 . (9)\nBased on the description above, we develop a parallel ADMM algorithm for the convex tensor decomposition (CTD) problem (5), as outlined in Algorithm 1. In Algorithm 1, we use a Jacobi-type scheme to update (N + 1) blocks of variables, {M1, \u00b7 \u00b7 \u00b7 ,MN} and X . By the definition f(M1, \u00b7 \u00b7 \u00b7 ,MN ) := \u2211N n=1 \u2016Mn,(n)\u2016tr and g(X ) := \u03bb 2 \u2016X \u2212 T \u2016 2 F , it is easy to verify that the problem (5) and Algorithm 1 satisfy the convergence conditions in Deng et al., (2013).\nTheorem 1 Let \u03c4i > \u00b5( N2\u2212\u03b3 \u2212 1), i = 1, . . . , N + 1. Then the sequence {Mk1 , \u00b7 \u00b7 \u00b7 ,M k N ,X\nk} generated by Algorithm 1 converges to an optimal solution {M\u22171, \u00b7 \u00b7 \u00b7 ,M \u2217 N ,X \u2217} of\nthe problem (5). Hence, the sequence {X k} converges to an optimal solution to the low-rank tensor decomposition problem (4).\nOur convex Tucker decomposition method employs matrix trace norm regularization and uses the singular value decomposition (SVD) in Algorithm 1, which becomes a little slow or even not applicable for large-scale problems. Motivated by this, we will propose a scalable non-convex lowrank tensor decomposition method."}, {"heading": "Non-Convex Tensor Decomposition Method", "text": "Several researchers (Keshavan et al., 2010; Wen et al., 2012) have provided some matrix rank estimation strategies to compute some good values (r1, r2, . . . , rN ) for the N-rank of the involved tensor. Thus, we only set some relatively large integers (R1, R2, . . . , RN ) such that Rn \u2265 rn, n = 1, \u00b7 \u00b7 \u00b7 , N .\nTheorem 2 Let X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN with N -rank= (r1, r2, \u00b7 \u00b7 \u00b7 , rN ) and G \u2208 RR1\u00d7R2\u00d7\u00b7\u00b7\u00b7\u00d7RN satisfy X = G \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN , and UTn Un = IRn , n = 1, 2, \u00b7 \u00b7 \u00b7 , N , then\n\u2016X(n)\u2016tr = \u2016G(n)\u2016tr, n = 1, 2, \u00b7 \u00b7 \u00b7 , N. (10)\nThe proof of Theorem 2 can be found in the supplemental material. According to the theorem, it is cleat that although the core tensor G of size (R1, R2, \u00b7 \u00b7 \u00b7 , RN ) has much smaller sizes than the original tensor X (usually, Rn \u226a In, n = 1, 2, \u00b7 \u00b7 \u00b7 , N ), their trace norm is identical. In other words, each unfolding G(n) \u2208 RRn\u00d7\u03a0j 6=nRj of the core tensor G has much smaller sizes than that of the original tensor, X(n) \u2208 RIn\u00d7\u03a0j 6=nIj . Therefore, we use the trace norm of each unfolding of the core tensor to replace that of the original tensor."}, {"heading": "Generalized HOOI Model with Trace Norm Penalties", "text": "According to the analysis above, our trace norm regularized HOOI model is formulated into the following form:\nmin G,{Un}\nN\u2211\nn=1\n\u2016G(n)\u2016tr + \u03bb\n2 \u2016T \u2212 G \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN\u2016\n2 F ,\ns.t., UTn Un = IRn , n = 1, 2, \u00b7 \u00b7 \u00b7 , N. (11)\nThe core tensor trace norm regularized HOOI model (11), also called Generalized HOOI, can alleviate the SVD computational burden of large unfolded matrices involved in the convex Tucker decomposition problem (4). Furthermore, we use the trace norm regularization term in (11) to promote the robustness of the rank selection, while the original Tucker decomposition method is usually sensitive to the given ranks (R1, R2, \u00b7 \u00b7 \u00b7 , RN ) (Liu et al., 2009). Due to the interdependent matrix trace norm terms, we apply the variablesplitting technique and introduce some auxiliary variables Gn \u2208 RRn\u00d7\u03a0j 6=nRj into our model (11), and then reformu-\nlate the model (11) into the following equivalent form:\nmin G,{Un,Gn}\nN\u2211\nn=1\n\u2016Gn\u2016tr + \u03bb\n2 \u2016T \u2212 G \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN\u2016\n2 F ,\ns.t., Gn = G(n), U T n Un = IRn , n = 1, 2, \u00b7 \u00b7 \u00b7 , N.\n(12)"}, {"heading": "Parallel Optimization Procedure", "text": "In this part, we will also propose a parallel ADMM algorithm to solve the problem (12).\nUpdating {Gk+1, Uk+11 , \u00b7 \u00b7 \u00b7 , U k+1 N }: The optimization problem with respect to G and {U1, \u00b7 \u00b7 \u00b7 , UN} is formulated as follows:\nmin G,{Un}\nN\u2211\nn=1\n\u00b5 2 \u2016G(n) \u2212G k n + Y k n /\u00b5\u2016 2 F\n+ \u03bb\n2 \u2016T \u2212 G \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN\u2016\n2 F .\n(13)\nFollowing (Lathauwer et al., 2000b), it is sufficient to determine the matrices {U1, \u00b7 \u00b7 \u00b7 , UN} for the optimization of the problem (13). For any estimate of these matrices, the optimal solution with respect to G is given by the following theorem.\nTheorem 3 For given matrices {U1, \u00b7 \u00b7 \u00b7 , UN}, the optimal core tensor G to the optimization problem (13) is given by\nGk+1 = \u03bb\n\u03bb+N\u00b5 T \u00d71 (U\nk 1 ) T \u00b7 \u00b7 \u00b7 \u00d7N (U k N ) T\n+ \u00b5\n\u03bb+N\u00b5\nN\u2211\nn=1\nrefold(Gkn \u2212 Y k n /\u00b5).\n(14)\nThe proof of Theorem 3 can be found in the supplemental material. In the following, we design a generalized higher order orthogonal iteration scheme for solving {U1, \u00b7 \u00b7 \u00b7 , UN} that is an alternating least squares (ALS) approach to solve the rank-(R1, . . . , RN ) problem. Analogous with Theorem 4.2 in Lathauwer et al., (2000b), we first state that the minimization problem (13) can be formulated as follows.\nTheorem 4 Assume a real N th-order tensor T \u2208 R\nI1\u00d7I2\u00d7...IN . Then the minimization of the problem (13) over the matrices U1, . . . , UN having orthonormal columns is equivalent to the maximization of the following problem\nh(U1, . . . , UN) = \u3008T , G \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN \u3009. (15)\nThe proof of Theorem 4 can be found in the supplemental material. According to the theorem, a generalized higherorder orthogonal iteration scheme is proposed to solve the problem (15) that solves Un, n = 1, . . . , N by fixing other variables Uj , j 6= n in parallel. Imagine that the matrices {U1, . . . , Un\u22121, Un+1, . . . , UN} are fixed and that the optimization problem (13) is thought of as a quadratic expression in the components of the matrix Un that is being optimized. Considering that the matrix having orthonormal columns, we have\nh = trace((Uk+1n ) TT(n)W T n ), (16)\nwhere trace(A) denotes the trace of the matrix A, and\nWn = G(n)[(U k 1 ) T \u00b7 \u00b7 \u00b7 \u2297 (Ukn\u22121) T\n\u2297 (Ukn+1) T \u00b7 \u00b7 \u00b7 \u2297 (UkN )\nT ]. (17)\nThis is actually the well-known orthogonal procrustes problem (Nick, 1995), whose global optimal solution is given by the singular value decomposition of T(n)WTn , i.e.,\nUk+1n = U\u0302n(V\u0302n) T , (18)\nwhere U\u0302n and V\u0302n are obtained by SVD of T(n)WTn . Repeating the procedure above in parallel for different modes leads to an alternating least squares scheme for solving the maximization of the problem (15).\nUpdating {Gk+11 , \u00b7 \u00b7 \u00b7 , G k+1 N }: By keeping all other variables fixed, Gk+1n is updated by solving the following problem:\nmin Gn\n\u2016Gn\u2016tr + \u00b5\n2 \u2016Gn\u2212G\nk (n)\u2212Y k n /\u00b5\u2016 2 F + \u03c4n 2 \u2016Gn\u2212G k n\u2016 2 F .\n(19) Similar to the problem (6), it is easy to obtain a closed-form solution to the problem (19):\nGk+1n = prox1/(\u00b5+\u03c4n)( \u00b5Gk(n) + Y k n + \u03c4nG k n\n\u00b5+ \u03c4n ). (20)\nFrom (20), it is clear that only some smaller sized matrices (\u00b5Gk(n) + Y k n + \u03c4nG k n)/(\u00b5 + \u03c4n) \u2208 R\nRn\u00d7\u03a0j 6=nRj need to perform SVD. Thus, our non-convex trace norm regularized method has a significantly lower computational complexity with O( \u2211 n R 2 n \u00d7\u03a0j 6=nRj), while the computational complexity of our convex algorithm for the problem (4) is O( \u2211 n I 2 n \u00d7\u03a0j 6=nIj) at each iteration.\nBased on the analysis above, we develop a parallel ADMM algorithm for solving the low-rank non-convex tensor decomposition (NCTD) problem (11), as outlined in Algorithm 2. Our algorithm is essentially a Jacobi-type scheme of ADMM, and is well suited for parallel and distributed computing and hence is particularly attractive for solving certain large-scale problems. Moreover, the update strategy of Gauss-Seidel version of ADMM is easily implemented. This algorithm can be accelerated by adaptively changing \u00b5. An efficient strategy (Lin et al., 2011) is to let \u00b50 = \u00b5 (initialized in Algorithm 1 and Algorithm 2) and increase \u00b5k iteratively by \u00b5k+1 = \u03c1\u00b5k, where \u03c1 \u2208 (1.0, 1.1] in general and \u00b50 is a small constant. Moreover, the stability and efficiency of our NCTD method can be validated in the experimental section."}, {"heading": "Complexity Analysis", "text": "We now discuss the time complexity of our NCTD algorithm. For the problem (11), the main running time of our NCTD algorithm is consumed by performing SVD for the proximal operator and some multiplications. The time complexity of performing the proximal operator in (20) is O1 := O( \u2211 R2n\u03a0j 6=nRj). The time complexity of some\nmultiplication operators is O2 := O( \u2211\nInRn\u03a0j 6=nIj +\u2211 Rn\u03a0j 6=nIjRj) and O3 := O( \u2211 R2nIn). Thus, the total time complexity of our NCTD method is O(T (O1 + O2 + O3)), where T is the number of iterations.\nAlgorithm 2 Solving problem (11) via Parallel ADMM Input: T , the tensor ranks (R1, R2, \u00b7 \u00b7 \u00b7 , RN ), and \u03bb. Initialize: Y 0n = 0, G 0 n = 0, U 0 n = rand(In, Rn), \u00b5\n0 = 10\u22124, \u00b5max = 1010, \u03c1 = 1.05, and Tol = 10\u22125.\n1: while not converged do 2: Update Gk+1 by (14). 3: for n = 1, \u00b7 \u00b7 \u00b7 , N do 4: Update Uk+1n by (18); 5: Update Gk+1n by (20); 6: Update the multiplier Y k+1n by\nY k+1n = Y k n + \u03b3\u00b5 k(Gk+1(n) \u2212G k+1 n ).\n7: end for 8: Update \u00b5k+1 by \u00b5k+1 = min(\u03c1\u00b5k, \u00b5max). 9: Check the convergence condition,\nmax{\u2016Gk+1(n) \u2212G k+1 n \u2016F , n = 1, \u00b7 \u00b7 \u00b7 , N} < Tol.\n10: end while Output: G and Un, n = 1, \u00b7 \u00b7 \u00b7 , N ."}, {"heading": "Experimental Results", "text": "In this section, we evaluate both the effectiveness and efficiency of our methods for solving tensor decomposition problems using both synthetic and real-world data. All experiments were performed on an Intel(R) Core (TM) i5-4570 (3.20 GHz) PC running Windows 7 with 8GB main memory."}, {"heading": "Synthetic Data", "text": "In this part, we generated a low-rank N th-order tensor T \u2208 RI1\u00d7I2\u00b7\u00b7\u00b7\u00d7IN , which was used as the ground truth data. The tensor data followed the Tucker model, i.e., T = G \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN , where the core tensor G \u2208 Rr\u00d7r\u00b7\u00b7\u00b7\u00d7r and the factor matrices Un were generated with i.i.d. standard Gaussian entries. The order of the tensors varied from three to four, and the rank r was set to {5, 10, 20}. Finally, we decomposed the input tensor T + \u03b4\u2206 by our CTD and NCTD methods and the state-of-the-art algorithms including HOSVD (Vannieuwenhoven et al., 2012), HOOI (Lathauwer et al., 2000b), Mixture (Tomioka et al., 2013) and ADMM (Gandy et al., 2011), where \u2206 is a noise tensor with independent normally distributed entries, and \u03b4 is set to 0.02.\nWe set the tensor ranks Rn = \u230a1.2r\u230b, n = 1, . . . , N and Tol = 10\u22125 for all these algorithms. We set the regularization parameter \u03bb = 100 for our methods. Other parameters of Mixture, ADMM, HOSVD and HOOI are set to their default values. The relative square error (RSE) of the estimated tensor X is given by RSE = \u2016X \u2212T \u2016F/\u2016T \u2016F . The average experimental results (RSE and time cost) of 50 independent runs are shown in Table 1, where the order of tensor data varies from three to four. From the results shown in Table 1, we can see that our methods can yield much more accurate solutions, and outperform their individual competitors, HOSVD, HOOI, Mixture and ADMM, in terms of both RSE and efficiency.\nA phase transition plot uses greyscale colors to depict how likely a certain kind of low-rank tensors can be recovered by these algorithms for a range of different given ranks and noise variances \u03b4. Phase transition plots are important means\nto compare the performance of different tensor estimation methods. If the relative error RSE \u2264 10\u22122, the estimation is regarded as successful. Figure 2 depicts the phase transition plots of HOSVD, HOOI, CTD and NCTD on the third-order tensor data with the rank r = 10, where the given tensor ranks Rn, n = 1, 2, 3 varied from 10 to 50 with increment 4 and \u03b4 from 0 to 0.05 with increment 0.005. For each setting, 50 independent trials were run. From the experimental results shown in Figure 2, we can see that CTD and NCTD perform significantly better than HOSVD and HOOI."}, {"heading": "MRI Data", "text": "This part compares our CTD and NCTD methods, HOSVD and HOOI on a 181\u00d7217\u00d7181 brain MRI data used in (Liu et al., 2009). This data set is approximately low-rank: for the three mode unfoldings, the numbers of singular values larger\nthan 1% of the largest one are 17, 21, and 17, respectively. Figure 3 shows the average relative errors and running times of ten independent trials for each setting of the given ranks. From the results, we see that our CTD and NCTD methods consistently attain much lower relative errors than those by HOSVD and HOOI. Moreover, our NCTD method is usually faster than the other methods."}, {"heading": "Conclusions", "text": "In this paper we first proposed a convex trace norm regularized tensor decomposition method, which can automatically determine the number of factors in each mode through our optimization scheme. In addition, by considering the lowrank structure of input tensors, we analyzed the equivalence relationship of the trace norm between a low-rank tensor and its core tensor. Then, we cast the non-convex tensor decomposition model into a weighted combination of multiple much-smaller-scale matrix trace norm minimization. Finally, we developed two efficient parallel ADMM algorithms for solving the proposed problems. Convincing experimental results demonstrate that our regularized formulation is reasonable, and our methods are robust to noise or outliers. Moreover, our tensor decomposition methods can handle some tensor recovery problems, such as tensor completion, and low-rank and sparse tensor decomposition."}, {"heading": "Acknowledgements", "text": "We thank the reviewers for their useful comments that have helped improve the paper significantly. The first and third authors are supported by the CUHK Direct Grant No. 4055017 and Microsoft Research Asia Grant 6903555."}, {"heading": "Convergence Behaviors of Our Algorithms", "text": "We also study the convergence behaviors of our CTD and NCTD algorithms on the synthetic tensor data of size 200\u00d7 200 \u00d7 200 with the given ranks, R1 = R2 = R3 = 20, as shown in Figure 1, where the ordinate is the residual of max{\u2016Gk(n) \u2212G k n\u2016F , n = 1, \u00b7 \u00b7 \u00b7 , N} or \u2016X k \u2212T \u2016F , or the relative change of X k, and the abscissa denotes the number of iterations. Moreover, we also provide the convergence results of HOOI. We can observe that the relative change or the residual of CTD and NCTD algorithms drops much quickly, and converges much fast within 50 iterations. Especially, the relative change or the residual of CTD and NCTD drops much more quickly than HOOI."}, {"heading": "Rank Estimation", "text": "In this part, we evaluate the ability of our CTD method to estimate the tensor ranks, as shown in Figure 2. As in the experimental section, we randomly generated T of size 200\u00d7200\u00d7200with the tensor ranks r1 = r2 = r3 = r =10 or 20. From Figure 2, we see that our CTD method can accurately estimate the rank of each mode unfolding of the tensor."}, {"heading": "Robustness Against Outliers", "text": "Figure 3 depicts the phase transition plots of HOSVD, HOOI, CTD and NCTD on the third-order tensor data with rank r = 10, where the given tensor ranks Rn, n = 1, 2, 3 varied from 10 to 50 with increment 4 and the error sparsity ratio \u2016\u2206\u20160/\u03a0nIn from 0 to 0.05 with increment 0.005. We generated \u2206 as a sparse tensor whose non-zero entries are independent and uniformly distributed in the range [\u22121, 1], and whose support is chosen uniformly at random. For each setting, 50 independent trials were run. From the experi-\nmental results shown in Figure 3, it is clear that our methods, CTD and NCTD, significantly outperform HOSVD and HOOI."}, {"heading": "APPENDIX A", "text": "Proof of Theorem 2: Before giving a proof of Theorem 2, we will first present some properties of matrices and tensors in the following.\nProperty 1: Let U \u2208 Rm\u00d7p, S \u2208 Rp\u00d7q , and V \u2208 Rn\u00d7q, then\n\u2016USV T \u2016tr = \u2016S\u2016tr,\nwhere UTU = Ip and V TV = Iq . Property 2: Let A \u2208 Rm\u00d7n, B \u2208 Rp\u00d7q, and C and D are two matrices of suitable sizes, then we have the following results: (I). (A\u2297B)\u2297 C = A\u2297 (B \u2297 C). (II). (A\u2297B)(C \u2297D) = AC \u2297BD. (III). (A\u2297B)T = AT \u2297BT .\nProperty 3: Let X = G\u00d71U1\u00d72 \u00b7 \u00b7 \u00b7\u00d7N UN , where X \u2208 R\nI1\u00d7I2\u00b7\u00b7\u00b7\u00d7IN , G \u2208 RR1\u00d7R2\u00b7\u00b7\u00b7\u00d7RN , and Un \u2208 RIn\u00d7Rn , n = 1, . . . , N , then\nX(n) = UnG(n)(UN \u2297 . . . Un+1 \u2297 Un\u22121 . . .\u2297 U1) T .\nProof of Theorem 2: According to Property 3, we have\n\u2016X(n)\u2016tr = \u2016UnG(n)(UN \u2297 . . . Un+1 \u2297Un\u22121 . . .\u2297U1) T \u2016tr.\nLet Pn = UN \u2297 . . . Un+1 \u2297Un\u22121 . . .\u2297U1 and UTn Un = IRn , n = 1, . . . , N , and according to Properties 2 and 3, we have the following conclusion:\nPTn Pn\n=(UN \u2297 . . .\u2297 Un+1 \u2297 Un\u22121 \u2297 . . .\u2297 U1) T (UN \u2297 . . .\u2297\nUn+1 \u2297 Un\u22121 \u2297 . . .\u2297 U1),\n=(UTN \u2297 . . .\u2297 U T n+1 \u2297 U T n\u22121 \u2297 . . .\u2297 U T 1 )(UN \u2297 . . .\u2297\nUn+1 \u2297 Un\u22121 \u2297 . . .\u2297 U1),\n=(UTN \u2297 . . .\u2297 U T n+1 \u2297 U T n\u22121 \u2297 . . .\u2297 U T 2 )(UN\u2297\n. . .\u2297 Un+1 \u2297 Un\u22121 \u2297 . . .\u2297 U2)\u2297 (U T 1 U1),\n=(UTN \u2297 . . .\u2297 U T n+1 \u2297 U T n\u22121 \u2297 . . .\u2297 U T 2 )(UN\u2297\n. . .\u2297 Un+1 \u2297 Un\u22121 \u2297 . . .\u2297 U2)\u2297 I1,\n...\n=IN \u2297 . . . In+1 \u2297 In\u22121 \u2297 . . . I2 \u2297 I1,\n=I\u0303n,\nwhere In \u2208 RRn\u00d7Rn , n = 1, . . . , N, I\u0303n \u2208 RJn\u00d7Jn , and Jn = \u03a0j 6=nRj .\nAccording to Property 1, and PTn Pn = I\u0303n, we have\n\u2016X(n)\u2016tr = \u2016UnG(n)(UN \u2297 . . . Un+1 \u2297 Un\u22121 . . .\u2297 U1) T \u2016tr\n= \u2016G(n)\u2016tr.\nThis completes the proof."}, {"heading": "APPENDIX B", "text": ""}, {"heading": "Proof of Theorem 3:", "text": "The optimization problem (13) with respect to G is written by\n(21) min G\nJ(G) = N\u2211\nn=1\n\u00b5k\n2 \u2016G(n) \u2212G\nk n + Y k n /\u00b5 k\u20162F\n+ \u03bb\n2 \u2016T \u2212 G \u00d71 U\nk 1 \u00b7 \u00b7 \u00b7 \u00d7N U k N\u2016 2 F .\nThe problem (21) above is a smooth convex optimization problem, thus we can obtain the derivative of the function J in the following form:\n(22)\n\u2202J \u2202G =\u03bb(G \u2212 T \u00d71 (U k 1 ) T \u00b7 \u00b7 \u00b7 \u00d7N (U k N ) T )\n+ N\u2211\nn=1\n\u00b5k(G \u2212 refold(Gkn \u2212 Y k n /\u00b5 k))\n=(N\u00b5k + \u03bb)G \u2212 \u00b5k N\u2211\nn=1\nrefold(Gkn \u2212 Y k n /\u00b5 k)\n\u2212 \u03bbT \u00d71 (U k 1 ) T \u00b7 \u00b7 \u00b7 \u00d7N (U k N) T ,\nwhere refold(\u00b7) denotes the refolding of the matrix into a tensor.\nLet \u2202J\u2202G =0, and\n(23)\nM = T \u00d71 (U k 1 ) T \u00b7 \u00b7 \u00b7 \u00d7N (U k N) T ,\nN = N\u2211\nn=1\nrefold(Gkn \u2212 Y k n /\u00b5 k),\nthe optimal solution to (21) is given by\n(24)\nG\u2217 = \u03bb\n\u03bb+N\u00b5k T \u00d71 (U\nk 1 ) T \u00b7 \u00b7 \u00b7 \u00d7N (U k N ) T\n+ \u00b5k\n\u03bb+N\u00b5k\nN\u2211\nn=1\nrefold(Gkn \u2212 Y k n /\u00b5 k),\n= \u03bb\n\u03bb+N\u00b5k M+\n\u00b5k\n\u03bb+N\u00b5k N .\nThis completes the proof."}, {"heading": "APPENDIX C", "text": ""}, {"heading": "Proof of Theorem 4:", "text": "Let\n(25) H(G, {Un}) =\nN\u2211\nn=1\n\u00b5k\u2016G(n) \u2212G k n + Y k n /\u00b5 k\u20162F\n+\u03bb\u2016T \u2212 G \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN\u2016 2 F ,\nthen the closed-form solution of (25) with respect to G can be obtained by (14), and it can be rewritten as\nG\u2217 = 1\n\u03bb+N\u00b5k (\u03bbM + \u00b5kN ).\nHence, we have\n(26)\nH(G\u2217, {Un})\n=\nN\u2211\nn=1\n\u00b5k\u2016G\u2217(n) \u2212G k n + Y k n /\u00b5 k\u20162F + \u03bb\u2016T \u2016 2 F + \u03bb\u2016G \u2217\u20162F\n\u2212 2\u03bb\u3008T ,G\u2217 \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN\u3009,\n=\u03b7 \u2212 2\u03bbh(U1, U2, . . . , UN),\nwhere \u03b7 = \u2211N\nn=1 \u00b5 k\u2016G\u2217(n) \u2212G k n + Y k n /\u00b5 k\u20162F + \u03bb\u2016T \u2016 2 F +\n\u03bb\u2016G\u2217\u20162F is a constant, and\nh(U1, U2, . . . , UN ) = \u3008T ,G \u2217 \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7N UN\u3009.\nCombination with the results above proves the theorem. This completes the proof."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Higher-order tensors are becoming prevalent in many<lb>scientific areas such as computer vision, social network<lb>analysis, data mining and neuroscience. Traditional ten-<lb>sor decomposition approaches face three major chal-<lb>lenges: model selecting, gross corruptions and compu-<lb>tational efficiency. To address these problems, we first<lb>propose a parallel trace norm regularized tensor de-<lb>composition method, and formulate it as a convex op-<lb>timization problem. This method does not require the<lb>rank of each mode to be specified beforehand, and can<lb>automatically determine the number of factors in each<lb>mode through our optimization scheme. By considering<lb>the low-rank structure of the observed tensor, we ana-<lb>lyze the equivalent relationship of the trace norm be-<lb>tween a low-rank tensor and its core tensor. Then, we<lb>cast a non-convex tensor decomposition model into a<lb>weighted combination of multiple much smaller-scale<lb>matrix trace norm minimization. Finally, we develop<lb>two parallel alternating direction methods of multipli-<lb>ers (ADMM) to solve our problems. Experimental re-<lb>sults verify that our regularized formulation is effective,<lb>and our methods are robust to noise or outliers. Introduction<lb>The term tensor used in the context of this paper refers to<lb>a multi-dimensional array, also known as a multi-way or<lb>multi-mode array. For example, if X \u2208 R123 , then<lb>we say X is a third-order tensor, where order is the num-<lb>ber of ways or modes of the tensor. Thus, vectors and ma-<lb>trices are first-order and second-order tensors, respectively.<lb>Higher-order tensors arise in a wide variety of application ar-<lb>eas, such as machine learning (Tomioka and Suzuki, 2013;<lb>Signoretto et al., 2014), computer vision (Liu et al., 2009),<lb>data mining (Yilmaz et al., 2011; Morup, 2011; Narita et<lb>al., 2012; Liu et al., 2014), numerical linear algebra (Lath-<lb>auwer et al., 2000a; 2000b), and so on. Especially, with<lb>the rapid development of modern computer technology in<lb>recent years, tensors are becoming increasingly ubiquitous<lb>such as multi-channel images and videos, and have become<lb>increasingly popular (Kolda and Bader, 2009). When work-<lb>ing with high-order tensor data, various new computational Copyright c<lb>\u00a9 2014, Association for the Advancement of Artificial<lb>Intelligence (www.aaai.org). All rights reserved.<lb>challenges arise due to the exponential increase in time and<lb>memory space complexity when the number of orders in-<lb>creases. This is called the curse of dimensionality. In prac-<lb>tice, the underlying tensor is often low-rank, even though<lb>the actual data may not be due to noise or arbitrary errors.<lb>Essentially, the major component contained in the given ten-<lb>sor is often governed by a relatively small number of latent<lb>factors.<lb>One standard tool to alleviate the curse is tensor de-<lb>composition. Decomposition of high-order tensors into<lb>a small number of factors has been one of the main<lb>tasks in multi-way data analysis, and commonly takes two<lb>forms: Tucker decomposition (Tucker, 1966) and CANDE-<lb>COMP/PARAFAC (CP) (Harshman, 1970) decomposition.<lb>There are extensive studies in the literature for finding the<lb>Tucker decomposition and the CP decomposition for higher-<lb>order tensors (Kolda and Bader, 2009). In those tensor de-<lb>composition methods, their goal is to (approximately) re-<lb>construct the input tensor as a sum of simpler components<lb>with the hope that these simpler components would reveal<lb>the latent structure of the data. However, existing tensor de-<lb>composition methods face three major challenges: rank se-<lb>lection, outliers and gross corruptions, and computational<lb>efficiency. Since the Tucker and CP decomposition meth-<lb>ods are based on least-squares approximation, they are also<lb>very sensitive to outliers and gross corruptions (Goldfarb<lb>and Qin, 2014). In addition, the performance of those meth-<lb>ods is usually sensitive to the given ranks of the involved ten-<lb>sor (Liu et al., 2009). To address the problems, we propose<lb>two robust and parallel higher-order tensor decomposition<lb>methods with trace norm regularization.<lb>Recently, much attention has been drawn to the low-rank<lb>tensor recovery problem, which arises in a number of real-<lb>word applications, such as 3D image recovery, video in-<lb>painting, hyperspectral data recovery, and face reconstruc-<lb>tion. Compared with matrix-based analysis methods, tensor-<lb>based multi-linear data analysis has shown that tensor mod-<lb>els are capable of taking full advantage of the high-order<lb>structure to provide better understanding and more preci-<lb>sion. The key idea of low-rank tensor completion and recov-<lb>ery methods is to employ matrix trace norm minimization<lb>(also known as the nuclear norm, which is the convex surro-<lb>gate of the rank of the involved matrix). In addition, there are<lb>some theoretical developments that guarantee reconstruction of a low-rank tensor from partial measurements or grossly<lb>corrupted observations via solving the trace norm minimiza-<lb>tion problem under some reasonable conditions (Tomioka et<lb>al., 2011; Shi et al., 2013). Motivated by the recent progress<lb>in tensor completion and recovery, one goal of this paper<lb>is to extend the trace norm regularization to robust higher-<lb>order tensor decomposition.<lb>Different from existing tensor decomposition methods,<lb>we first propose a parallel trace norm regularized tensor de-<lb>composition method, which can automatically determine the<lb>number of factors in each mode through our optimization<lb>scheme. In other words, this method does not require the<lb>rank of each mode to be specified beforehand. In addition,<lb>by considering the low-rank structure of the observed tensor<lb>and further improving the scalability of our convex method,<lb>we analyze the equivalent relationship of the trace norm be-<lb>tween a low-rank tensor and its core tensor. Then, we cast<lb>the non-convex trace norm regularized higher-order orthog-<lb>onal iteration model into a weighted combination of multiple<lb>much-smaller-scale matrix trace norm minimization. More-<lb>over, we design two parallel alternating direction methods of<lb>multipliers (ADMM) to solve the proposed problems, which<lb>are shown to be fast, insensitive to initialization and robust<lb>to noise and/or outliers with extensive experiments. Notations and Related Work<lb>We first introduce the notations, and more details can be seen<lb>in Kolda and Bader (2009). An N th-order tensor is denoted<lb>by a calligraphic letter, e.g., T \u2208 R12N , and its<lb>entries are denoted by ti1\u00b7\u00b7\u00b7in\u00b7\u00b7\u00b7iN , where in \u2208 {1, \u00b7 \u00b7 \u00b7 ,<lb>In}<lb>for 1 \u2264 n \u2264 N . Fibers are the higher-order analogue of<lb>matrix rows and columns. The mode-n fibers are vectors<lb>ti1\u00b7\u00b7\u00b7in\u22121in+1\u00b7\u00b7\u00b7iN that are obtained by fixing the values of<lb>{i1, \u00b7 \u00b7 \u00b7 ,<lb>iN}\\in.<lb>The mode-n unfolding, also known as matricization, of an<lb>N th-order tensor T is denoted by T(n) \u2208 Rnj 6=nIj and<lb>arranges the mode-n fibers to be the columns of the result-<lb>ing matrix T(n) such that the mode-n fiber becomes the row<lb>index and all other (N \u2212 1) modes become the column in-<lb>dices. The tensor element (i1, i2, \u00b7 \u00b7 \u00b7 , iN) is mapped to the<lb>matrix element (in, j), where", "creator": "LaTeX with hyperref package"}}}