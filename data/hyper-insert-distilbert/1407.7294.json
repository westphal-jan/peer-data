{"id": "1407.7294", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2014", "title": "Online Learning and Profit Maximization from Revealed Preferences", "abstract": "we consider creating the typical problem of learning from revealed preferences \" in an inherently online foraging setting. in exactly our framework, each day period a consumer buys an optimal bundle resource of goods purchasing from within a merchant according to finding her ( linear ) utility function sensitivity and optimal current prices, seemingly subject to a budget compensation constraint. the merchant utility observes seller only sells the purchased bundle goods, and seeks to adapt high prices to optimize out his profits. afterwards we give an efficient algorithm for learning the merchant'\u2033 s problem that consists of generating a learning sequential phase in which quantity the consumer's utility function quantity is ( very perhaps partially ) inferred, which followed by a discrete price optimization first step. we today also directly consider solving an ideal alternative online learning algorithm for dominating the setting where prices are set upwards exogenously, respectively but the merchant would still strongly like to predict the bundle options that then will be bought deliberately by consuming the consumer normally for purposes of inventory or supply chain asset management. essentially in contrast interviews with most similar prior practice work on the seemingly revealed preferences problem, but we simply demonstrate that by making radically stronger assumptions on the form fitting of utility efficiency functions, efficient algorithms responsible for efficient both learning and purchasing profit maximization are possible, however even in adaptive, online settings.", "histories": [["v1", "Sun, 27 Jul 2014 23:38:09 GMT  (91kb,D)", "http://arxiv.org/abs/1407.7294v1", null], ["v2", "Fri, 28 Nov 2014 21:45:08 GMT  (93kb,D)", "http://arxiv.org/abs/1407.7294v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.GT cs.LG", "authors": ["kareem amin", "rachel cummings", "lili dworkin", "michael kearns", "aaron roth"], "accepted": true, "id": "1407.7294"}, "pdf": {"name": "1407.7294.pdf", "metadata": {"source": "CRF", "title": "Online Learning and Profit Maximization from Revealed Preferences", "authors": ["Kareem Amin", "Rachel Cummings", "Lili Dworkin", "Michael Kearns", "Aaron Roth"], "emails": ["akareem@cis.upenn.edu", "ldworkin@cis.upenn.edu", "mkearns@cis.upenn.edu", "aaroth@cis.upenn.edu", "rachelc@caltech.edu."], "sections": [{"heading": null, "text": "In contrast with most prior work on the revealed preferences problem, we demonstrate that by making stronger assumptions on the form of utility functions, efficient algorithms for both learning and profit maximization are possible, even in adaptive, online settings.\n\u2217Computer and Information Science, University of Pennsylvania; {akareem,ldworkin,mkearns,aaroth}@cis.upenn.edu \u2020Computing and Mathematical Sciences, California Institute of Technology; rachelc@caltech.edu. Research\nperformed while the author was visiting the University of Pennsylvania.\nar X\niv :1\n40 7.\n72 94\nv1 [\ncs .D\nS] 2\n7 Ju\nl 2 01\n4"}, {"heading": "1 Introduction", "text": "We consider algorithmic and learning-theoretic aspects of the classical revealed preferences problem. In this problem, a consumer has a fixed but unknown utility function u over n goods, and is price sensitive. At each period t, she observes a price vector pt and purchases a bundle of goods xt to maximize her utility given her budget (i.e. xt \u2208 arg maxx\u00b7pt\u2264B u(x)). Given a sequence of T observations (p1, x1), . . . , (pT , xT ) the revealed preferences problem (introduced by [7]; see [8] for a survey) is to determine whether the observations are consistent with a consumer optimizing any utility function u, subject to some mild constraints (e.g. monotonicity of u). In this paper, however, we have different and stronger objectives, motivated by what power the merchant has to set prices. We consider two scenarios:\n(Price-Setting) First, we consider a monopolist merchant who has the power to set prices as he wishes (without fear of losing the consumer to competition). In this setting, we consider the natural goal of merchant profit maximization. The merchant has a fixed unit cost associated with each good, and his profit, when the consumer buys a bundle x, is his revenue minus the cost of the bundle purchased. The merchant wishes to adaptively set prices so as to minimize his costs, which maximizes his profits. Every round, the consumer purchases her utility maximizing bundle subject to the merchant\u2019s prices and her budget constraint. If the merchant knew the consumer\u2019s utility function, he could optimally set prices, but he does not \u2014 instead, the merchant faces a learning problem. For the case when the consumer has a linear utility function, we give an efficient algorithm for setting prices to quickly learn the consumer\u2019s utility function and then exploit this knowledge to set profit-maximizing prices.\n(Exogenous Prices) Second, we consider a merchant who cannot unilaterally set prices, but instead must react to a stream of exogenously chosen prices. This setting is relevant to a seller of commodity goods, or the owner of a franchise that must set prices given by the parent company. Despite his lack of control over prices, this merchant would nevertheless like to be able to predict which bundle the consumer is going to buy in the next period (e.g. to optimize inventory or streamline the supply chain). The problem remains that the consumer\u2019s utility function is unknown, and now the price sequence is also unknown and arbitrary (i.e. it could in the worst case be chosen adaptively, by an adversary). In this setting, when the consumer has a linear utility function, we give an efficient algorithm with a small mistake bound \u2014 in other words, even against an adaptively chosen set of price vectors, in the worst case over consumer utility functions, our algorithm makes only a bounded number of mistaken predictions of bundles purchased.\nWe note that there are a variety of scenarios that fall under the two frameworks above. These include sponsored search or contextual advertising on the web (where advertisers typically must obey periodic budget constraints, and prices are set exogenously by the bids of competitors or endogenously by an ad exchange or publisher); consumers who regularly receive gift certificates which can only be used for purchases from a single merchant such as Amazon, who in turn has price-setting powers; consumers with poor self-control who repeatedly spend all of their disposable income in a single category of goods (e.g. alcohol) from a price-setting merchant (e.g. local liquor store); and crowdsourcing or labor management settings where a manager (merchant) can set rewards or payments for a set of (say) daily tasks, and workers (consumers) with a budget of time or effort select tasks according to the incentives and their own abilities.\nFrom a learning-theoretic point of view, the price-setting framework can be viewed as a form of query model, since the merchant is free to experiment with prices (both for the purpose of learning about the consumer, and subsequently in order to optimize prices); while the exogenous price model\nfalls into the large literature on adversarial, worst-case online learning. The learning problem we consider in both settings is unusual, in that the \u201ctarget function\u201d we are trying to learn is the vector-valued arg max (optimal bundle) of the consumer\u2019s utility function. Additionally, despite the linearity of the consumer\u2019s utility function, the merchant\u2019s reward function is a non-convex function of prices, which further complicates learning.\nOur major assumptions \u2014 that consumers always spend their entire budget, that there is only one unit of each good available in each round, and that consumers repeatedly return to the same merchant \u2014 are standard in the classic revealed preferences model (see e.g. textbook treatments in [5] and [6]). In order to provide efficient learning algorithms in this setting, we necessarily impose additional restrictions on the form of the consumer\u2019s utility function. In particular, we assume that the utility function is linear, and that the coefficients are discretized and lower-bounded. The discretization assumption is necessary to learn the utility function exactly, which is required for the merchant\u2019s optimization. Even if two functions differ by an arbitrarily small amount, they can induce the consumer to buy very different bundles. We also assume an upper bound on prices, and without loss of generality we rescale this upper bound to be 1. Without such an assumption, the merchant could maximize his profits by setting all prices to infinity. Unbounded prices are neither founded in reality, nor lead to an interesting optimization problem."}, {"heading": "1.1 Our Results", "text": "We first consider the case of a monopolist merchant who has the ability to set prices arbitrarily, and is facing a consumer with an unknown linear utility function. In this setting, we give an algorithm with bounded regret with respect to the optimal (profit-maximizing) set of prices in hindsight. Our argument proceeds in two steps. We first show that, if we knew the consumer\u2019s utility function u, then we could efficiently compute the optimal profit-maximizing prices p\u2217:\nTheorem 1 (Informal). There is an efficient algorithm (running in time O(n2 log n)), which given as input the linear consumer utility function u, outputs the profit-maximizing prices p\u2217.\nThe analysis of this algorithm first assumes we know only the set of goods purchased by the consumer under optimal prices (but not the optimal prices themselves), and introduces a family of linear programs with one free parameter. We then show there is a small set of values for this parameter, one of which yields the optimal prices.\nNote that although the consumer \u2019s optimization problem when selecting a bundle to purchase given prices is simply a fractional knapsack problem, the problem of computing optimal prices is substantially more complex. The optimal price vector p\u2217 is actually a subgame perfect Nash equilibrium strategy for the merchant in a two-stage extensive form game between the merchant and the consumer (the merchant first picks prices, and then the consumer best responds). Viewed in this way, the fractional knapsack problem that the consumer solves at the second stage is simply her best response function; what we give is an algorithm for computing the merchant\u2019s subgame perfect equilibrium strategy in the first stage of this game. Note that doing this in polynomial time is non-trivial, because the merchant\u2019s strategy space is continuous (and even after discretization, is exponentially large).\nWe next give an algorithm that learns the consumer\u2019s unknown linear utility function by making price queries. A price query specifies a price vector, and receives in return the bundle purchased by the consumer. The key metric of interest here is the number of queries necessary to learn the utility function. We prove the following theorem:\nTheorem 2 (Informal). There is an efficient algorithm that learns, after at most O(n) price queries, the utility coefficients for all goods except those that are so preferred they will be bought regardless of prices.\nThis algorithm has two phases. In the first phase, after setting all prices to 1 (the maximum possible price), the algorithm gradually lowers the price of unpurchased items in succession until they are purchased, thus learning the ratio of their utility coefficient to that of the least preferred good purchased under the price vector of all 1s. Note that learning a relative rather than exact coefficient value is sufficient because the bundle bought by the consumer (i.e. the solution to the knapsack problem) is unchanged by a scaling of the coefficients. Indeed, because of this property, it is impossible for any algorithm to learn the exact values.\nThe harder coefficients to learn are those corresponding to goods purchased even when all prices are 1 \u2014 these are the consumer\u2019s most preferred goods. Some of these are learned by gradually raising the prices of unpurchased goods until a switch of purchased goods occurs; for the ones that cannot be learned via this procedure, we prove that they are so favored they will be purchased under any prices, so learning their coefficients is not necessary for price optimization.\nFinally, we put these two algorithms together to prove our first main result:\nTheorem 3 (Informal). There is a price-setting algorithm that, when interacting with a consumer with an unknown linear utility function for T rounds, achieves regret O(n2/T ) to the profit obtained by the optimal (profit-maximizing) price vector.\nIn the last part of the paper, we consider the case of a commodity merchant who does not have the power to set prices. The merchant wishes to predict the bundles that a consumer with an unknown linear utility function will buy, in the face of a stream of arbitrary price vectors. Here, the main quantity of interest is how many mistakes we make (by predicting the incorrect bundle) in the worst case over both consumer utility functions and sequences of price vectors. We call this the mistake bound of the algorithm (by analogy to the mistake bounded model of learning). Here we prove our second main result:\nTheorem 4 (Informal). There exists a polynomial time algorithm in the online exogenous price model that has a mistake bound of O(n2) with high probability."}, {"heading": "1.2 Related Work", "text": "There is a long line of work on the revealed preference problem, which was first introduced by Samuelson [7]. For a textbook introduction to the standard model, see [5] and [6], and for a survey of recent work, see [8]. Most previous efforts have focused on the construction of utility functions that explain a finite sequence of price/bundle observations. Afriat\u2019s Theorem is the seminal result in this field, and proves that a sequence of observations is rationalizable (i.e. can be explained by a utility function) if and only if the sequence is rationalizable by a piecewise linear, monotone, concave utility function. The proof is by construction, and therefore yields a \u201clearning\u201d algorithm for this large class of utility functions. However, the hypothesis generated has description length proportional to the number of observations, and hence although it can explain previous observations, it usually does not generalize to predict the bundles purchased given new price vectors.\nThe problem of finding a utility function that is both consistent and predictive was first considered by Beigman and Vohra [1], who formalize the statement that \u201cAfriat\u2019s Theorem Learners\u201d\ndo not generalize. [1] studies a PAC-like learning model in which they assume a distribution over observations and seek to find a hypothesis that performs well on future observations drawn from the same distribution. Their results essentially show that it is only possible to find predictive hypotheses if we restrict the class of allowable utility functions beyond those that are rationalizable. Roth and Zadimoghaddam [9] extended this line of work by providing computationally efficient learning algorithms for two specific classes of utility functions \u2014 linear and linearly separable and concave utility functions. Cummings, Echenique, and Wierman [2] consider the revealed preferences problem when the consumer can strategically choose bundles to subvert the merchant\u2019s learning. They show that in this setting, without assuming the consumer\u2019s utility function is linearly separable, the merchant is unable to learn anything. Like this previous work, we also seek to find predictive hypotheses for the class of linear utility functions, but we consider two new learning models: one in which prices are directly controlled, rather than observed (which corresponds to a query model of learning), and furthermore we wish to learn optimal prices; and one in which prices are chosen adversarially and adaptively, and arrive online (which corresponds to online learning in the mistake bound model).\nOur results in the second model are inspired by the classic halving algorithm for the online learning setting, which is credited to Littlestone [4]. The approach is simple but powerful: given a finite set of hypotheses and a sequence of observations, predict according to the majority of remaining hypotheses, and then discard all hypotheses that made a mistake. To implement the algorithm efficiently, we instead maintain a continuous hypothesis space from which we predict using a randomly sampled hypothesis (rather than predicting using a majority vote). We track the volume of the hypothesis space (rather than the number of consistent hypotheses), and show that after a bounded number of mistakes, we must have learned one coefficient of the consumer valuation function."}, {"heading": "2 Preliminaries", "text": "We consider a set of n divisible goods that a merchant wishes to sell to a consumer. We represent a bundle of goods x \u2208 [0, 1]n by a vector specifying what fraction of each of the n goods is purchased. The consumer has an unknown utility function u : [0, 1]n \u2192 R specifying her utility for each possible bundle. The merchant has the power to set prices (one for each good), also represented by a vector p \u2208 [0, 1]n (we normalize so that the price of every good i is pi \u2264 1). Written in this way, the price of a bundle x is simply x \u00b7 p = \u2211n i=1 pi \u00b7 xi. Finally, the consumer behaves as follows: facing a price vector p, the consumer purchases her most preferred bundle subject to a budget constraint B \u2265 0. That is, she purchases a bundle in arg maxx\u00b7p\u2264B u(x). There may not always be a unique utility-maximizing bundle for the consumer, and so we let X(u, p,B) = arg maxx\u00b7p\u2264B u(x), the set-valued collection of utility-maximizing bundles. If X(u, p,B) is a singleton set, we say that the consumer\u2019s choice is uniquely specified by p.\nWe restrict our attention to linear utility functions, which are defined by a valuation vector v \u2208 Rn such that u(x) = x \u00b7 v. We assume the valuations vectors are discretized to some increment \u03b4; i.e. each vi \u2208 {0, \u03b4, 2\u03b4, . . . , 1}. For this family of utility functions, the consumer\u2019s optimization problem to compute X(u, p,B) is a fractional knapsack problem. The capacity of the knapsack is B, and the weight and value of a good i are pi and vi, respectively. This problem can be solved greedily by ranking the goods in decreasing order of their vi/pi (i.e. bang per buck) ratios, and then buying in this order until the budget is exhausted. Note that in the optimal bundle, there will be\nat most one fractionally purchased good. Since this ratio is important in many of our algorithms, given u and p, we will denote vi/pi by ri(u, p), or by ri when u and p are clear from context. If ri \u2264 rj we say that the consumer prefers item i to item j.\nWe consider two problem variants. In the first, the merchant has the power to set prices, and has a production cost ci \u2264 1 for each good i. Hence, the merchant\u2019s profit when the consumer buys a bundle x at prices p is x\u00b7(p\u2212c). It always improves the consumer\u2019s utility to saturate her budget and so x\u00b7p = B for any x \u2208 X(u, p,B) and x\u00b7(p\u2212c) = B\u2212x\u00b7c. Hence, maximizing the merchant\u2019s profit is equivalent to minimizing his costs x\u00b7c. The merchant\u2019s goal is to obtain profit close to the maximum possible profit OPT = maxp\u2208[0,1]n maxx\u2208X(u,p,B) x \u00b7 (p\u2212 c) = maxp\u2208[0,1]n maxx\u2208X(u,p,B)B \u2212 x \u00b7 c.\nNote that solving this problem requires both learning something about the unknown utility function u, as well as the ability to solve the optimization problem given u. At every round t, the algorithm chooses some price vector pt, and the consumer responds by selecting any consistent xt \u2208 X(u, pt, B). We measure our success over T time steps with respect to our regret to the optimal profit we could have obtained had we priced optimally at every round, which is defined as\nRegret(p1, . . . , pT ) = OPT\u2212 1 T T\u2211 t=1 xt \u00b7 (pt \u2212 c)\nIn the second variant, we view price vectors p1, . . . , pT arriving one at a time, chosen (possibly adversarially) by Nature. In this setting, we have no control over the bundle purchased by the consumer, and wish only to predict it. At each time step t, after learning pt, we get to predict a bundle x\u0302t. Following our prediction, we observe the bundle xt = X(u, pt, B) actually purchased. We say that we make a mistake if x\u0302t 6= xt, and our goal is to design an algorithm that makes a bounded number of mistakes in the worst case over both u and the sequence of prices p1, . . . , pT ."}, {"heading": "3 Maximizing Profit in the Price-Setting Model", "text": "We begin by considering the first model, in which the merchant controls prices, and seeks to maximize profit. First we show that, given the coefficients vi of the consumer\u2019s linear utility function, we can efficiently compute the profit-maximizing prices. We will then combine this algorithm with a query algorithm for learning the coefficients, thus yielding an online no-regret pricing algorithm."}, {"heading": "3.1 Computing Optimal Prices Offline", "text": "In this section we assume that all the coefficients vi of the consumer\u2019s utility function are known to the merchant. Even then, it is not clear a priori that there exists an efficient algorithm for computing a profit-maximizing price vector p. As previously mentioned, the optimal prices are a subgame perfect Nash equilibrium strategy for the merchant in a two-stage extensive form game, in which the merchant has exponentially many strategies. Straightforwardly computing this equilibrium strategy via backwards induction would therefore be inefficient. We show that nevertheless, this task can be accomplished in time only (nearly) quadratic in the number of goods.\nThe key to the algorithm\u2019s efficiency will stem from the observation that there exists a restricted family of pricing vectors P \u2282 [0, 1]n containing a (nearly) profit-maximizing vector p\u2217. This subset P will still be exponentially large in n, but will be \u201cderived\u201d (in a manner which will be made more precise) from a small set of vectors p(1), ..., p(n). This derivation will allow the algorithm to\nefficiently search for p\u2217. We define p(k) by letting p (k) i = min(vi/vk, 1). In other words, the price of every good whose value is less than the kth good is set to the ratio vi/vk. Otherwise, if vi > vk, the price of good i in p(k) is set to the ceiling of 1.\nTo understand the operation of the algorithm, consider the consumer\u2019s behavior under the prices p(k). Any good priced at vi/vk will have a bang per buck ratio ri = vk. Therefore, the consumer\u2019s choice is not uniquely specified by p(k) in general (since the consumer is indifferent between any of the previously mentioned goods). Moreover, the consumer\u2019s choice will have great impact on the merchant\u2019s profit since the goods between which the consumer is indifferent might have very different production costs ci. The algorithm therefore proceeds by computing, for each p\n(k), which bundle x(k) the merchant would like the consumer to purchase under p(k). More precisely, for each k, the algorithm computes x(k) \u2208 arg maxx\u2208X(u,p(k),B) x \u00b7 (p(k) \u2212 c). Note that if the merchant were to actually play the price vector p(k), the consumer would be under no obligation in our model to respond by selecting x(k). Therefore, the final step of the algorithm is to output a price vector which attains nearly OPT profit, but for which the consumer\u2019s behavior is uniquely specified.\nThe pseudocode is given in Algorithm OptPrice. The analysis proceeds by proof of three key facts. (1) For some k, the optimal profit is attained by (p(k), x(k)), or rather, OPT = x(k) \u00b7 (p(k)\u2212 c) for some k. (2) Given any p(k), x(k) can be computed efficiently (in O(n log n) time). Finally, (3) there is some price p\u0302 for which the consumer\u2019s choice x is uniquely specified, and where x \u00b7 (p\u0302\u2212 c) is close to OPT.\nAlgorithm OptPrice(B, v, c, )\nfor k = 1 to n do p (k) i = min(vi/vk, 1) for all i x(k) = arg maxx\u2208X(u,p(k),B) x \u00b7 (p(k) \u2212 c) . O(n log n) computation Profit(k) = x(k) \u00b7 (p(k) \u2212 c)\nkmax = arg maxk Profit(k) p\u2217 = p(kmax), x\u2217 = x(kmax) for i = 1 to n do if x\u2217i = 0 then\np\u0302i = 1 else if x\u2217i = 1 then\np\u0302i = p \u2217 i \u2212\nelse p\u0302i = p \u2217 i \u2212 /2\nreturn p\u0302\nTheorem 1. Algorithm OptPrice (runs in time O(n2 log n)), takes coefficients v1, . . . , vn as input and computes prices, p\u0302 for which the consumer\u2019s choice x\u0302 is uniquely specified and that for any > 0 achieves profit x(p\u0302\u2212 c) \u2265 OPT\u2212 .\nProof. We prove the above by theorem by establishing the three key facts listed above. The first key Lemma establishes that optimal profit is attained by some (p(k), x(k)) for some k.\nLemma 1. Let p(k) be the pricing vector such that p (k) i = min(vi/vk, 1). For any consumer utility parameterized by (u,B), there exists a k and an x \u2208 X(u, p(k), B), such that OPT = x \u00b7 (p(k) \u2212 c).\nProof. Fix u and B. Consider a profit-maximizing price p\u2217, and corresponding bundle x\u2217 \u2208 X(u, p\u2217, B), so that OPT = x\u2217 \u00b7 (p\u2217\u2212 c). Let O = {i : x\u2217i > 0} be the set of purchased goods in x\u2217. If there is a fractionally purchased good in x\u2217, we denote its index by f (i.e. 0 < x\u2217f < 1).\nWe note that there must exist a threshold \u03c4 such that ri(u, p \u2217) \u2264 \u03c4 whenever i 6\u2208 O and ri(u, p \u2217) \u2265 \u03c4 whenever i \u2208 O. In other words, in order for the bundle x\u2217 to maximize the consumer\u2019s utility, the bang per buck of every purchased good must be at least as large as the bang per buck of every unpurchased good. If there is a fractional good in x\u2217, we take \u03c4 to be rf (u, p\n\u2217). Otherwise, we take it to be maxi 6\u2208O ri(u, p\n\u2217), the bang per buck of the most desirable unpurchased good. Given \u03c4 , we can write the linear program in (1). We claim that any solution to this LP is also a profit-maximizing price. More precisely, if p(LP ) is a solution to the LP, there is an x(LP ) \u2208 X(u, p(LP ), B) such that OPT = x(LP )(p(LP ) \u2212 c). The following argument proves this claim.\nmax \u2211 i\u2208O pi s.t. vi/pi \u2265 \u03c4 \u2200i \u2208 O vi/pi \u2264 \u03c4 \u2200i /\u2208 O pi \u2264 1 \u2200i \u2208 [n]\n(1)\nWe can straightforwardly characterize any solution p(LP ) to the LP. Note that the constraints on pi are disjoint, and therefore for each i \u2208 O, pi can increased independently until a constraint is saturated. Thus, the LP is optimized by setting p\n(LP ) i = min{vi/\u03c4, 1} for each i \u2208 O, and\np (LP ) i \u2265 vi/\u03c4 for each i 6\u2208 O (which is always possible since p\u2217 is a feasible solution).\nThe LP constraints imply that, under p(LP ), the consumer prefers any good in O to any good not in O. Moreover, if there is a fractional good, the definition of \u03c4 ensures that any good in O\\{f} is preferred to f . Finally, we know that \u2211 i\u2208O p \u2217 i \u2264 \u2211 i\u2208O p (LP ) i since p\n\u2217 is a feasible solution to the LP. This, along with the preference ordering on goods imposed by p\u2217, tells us that the consumer can saturate her budget at least as quickly under p(LP ) as under p\u2217. In other words, under p(LP ), the consumer may saturate her budget before purchasing all goods in O \\ {f}, or may require a smaller allocation of good f , but she will never require a larger allocation of good f or purchase any goods not in O. More precisely, there exists an x(LP ) \u2208 X(u, p(LP ), B) such that point-wise x (LP ) i \u2264 x\u2217i . Thus, OPT = x\u2217(p\u2217\u2212 c) = B\u2212 x\u2217 \u00b7 c \u2264 B\u2212 x(LP ) \u00b7 c = x(LP ) \u00b7 (p(LP )\u2212 c), and so p(LP ) must be a profit-maximizing price. We have established that p(LP ), which sets pi = min(vi/\u03c4, 1), is profit-maximizing, and now need only to show that we can take \u03c4 = vk for some k. Consider again a profit-maximizing price p \u2217 and corresponding bundle x\u2217. Notice that for any i 6\u2208 O, we can modify prices to set p\u2217i = 1, and (x\u2217, p\u2217) will still maximize profit because this change only makes the unpurchased goods more undesirable. Notice that with this modification, maxi 6\u2208O ri(u, p \u2217) = maxi 6\u2208O vi = vk\u2217 for some k \u2217. Next consider modifying the price of the fractional good. The price of f can be increased to vf/vk\u2217 if vf/vk\u2217 < 1, and 1 otherwise. This increases the price of f while still keeping it the fractionally purchased good, thus reducing the merchant\u2019s cost. This will result in either rf = vk\u2217 or rf = vf . In either case, there exists a (x\u2217, p\u2217) such that \u03c4 = vk for some k. Thus an optimal price p\n(LP ) can be derived by searching over at most n possible values of \u03c4 , and sets pi = min(vi/vk, 1) for some vk. (Lemma 1)\nLemma 1 establishes that for some k, p(k) is optimal, in the sense that there exists an x(k) \u2208\nX(u, p(k), B) such that OPT = x(k)(p(k) \u2212 c). The algorithm proceeds by computing for each such p(k), the most profitable bundle (for the merchant) that the consumer (who is indifferent between all bundles in X(u, p(k), B)) could purchase. X(u, p(k), B) is potentially a very large set. For example, if the consumer has identical values for all goods (i.e. vi \u2261 c for all i), and p(k) is the all-1s vector, then X(u, p(k), B) contains any budget-saturating allocation. Despite the potential size of this set, Lemma 2 shows that computing maxx\u2208X(u,p(k),B) x \u00b7 (p \u2212 c) simply requires solving a fractional knapsack instance, this time from the merchant\u2019s perspective.\nLemma 2. For any p, u, and B, maxx\u2208X(u,p,B) x \u00b7 (p\u2212 c) = B\u2212x \u00b7 c can be computed in O(n log n) time.\nProof. Let p be an arbitrary price vector. The bundle maxx\u2208X(u,p,B)B \u2212 x \u00b7 c can be computed as follows. First sort the ri in decreasing order, so that ri1 \u2265 ... \u2265 rin . The consumer will buy goods in this order until the budget B is exhausted. Thus, we can simulate the consumer\u2019s behavior by iteratively \u201cbuying\u201d goods and decrementing the budget. The consumer\u2019s behavior is uniquely specified unless we reach some sequence of goods with rij = rij+1 = . . . = rij+d , with B \u2032 budget\nremaining, where \u2211d\nl=0 pij+l > B \u2032. In other words, the consumer\u2019s behavior is unique unless she is\nindifferent between some set of goods, and can select different subsets of these goods to exhaust her remaining budget B\u2032.\nIn that case, we know that for any bundle x \u2208 X(u, p,B), xil = 1 if l < j, and xil = 0 if l > j + d. For the remaining goods, the merchant\u2019s profit is maximized when x \u00b7 c is minimized. This occurs when the consumer saturates the remaining budget B\u2032 while minimizing the cost c to the merchant. This is an instance of min-cost fractional knapsack wherein the size of the goods are pij , ..., pij+d and the cost of the goods are cij , ..., cij+d . A solution to this problem can be computed greedily, so the most profitable bundle for p can be computed with at most two sorts (first for ri then for pi/ci). (Lemma 2)\nFinally, to induce the consumer to buy the bundle x\u2217, rather than another member ofX(u, p\u2217, B), we perturb the vector p\u2217 slightly, and show that this has an arbitrarily small effect on profit.\nLemma 3. There exists a price vector p\u0302 which uniquely specifies a bundle x\u0302, such that for any > 0, x\u0302 \u00b7 (p\u0302\u2212 c) \u2265 OPT\u2212 .\nProof. Recall that the merchant would like the consumer to purchase the bundle x\u2217, which is a member of the set X(u, p\u2217, B). Even if the merchant sets prices at p\u2217, there is no guarantee that the consumer will purchase x\u2217 rather than some other bundle in the set. Our goal is to output a vector p\u0302 that results from perturbing p\u2217 slightly, so that the consumer will always purchase some x\u0302 that is arbitrarily close to x\u2217. For any good i such that x\u2217i = 0 (a good that consumer should not buy at all), we simply set p\u0302i = 1. For any good i such that x \u2217 i = 1 (a good that that the consumer should buy in its entirety), we set p\u0302i = p \u2217 i \u2212 0. Finally, for any good i such that 0 < x\u2217i < 1 (a good that the consumer should buy fractionally), we set p\u0302i = p \u2217\u2212 0/2. These perturbations ensure that the consumer will buy goods in the order desired by the merchant. We have decreased each price by at most 0, so the consumer might have additional n 0 budget to spend. Recall that prices are chosen by the algorithm to be min(vi/vk, 1) for some i, k. Because values are discretized and lower-bounded, the minimum price possible is therefore \u03b4. Consider setting 0 = \u03b4 /n, which yields \u03b4 additional budget. Then the consumer can afford to purchase at most an additional \u03b4 /\u03b4 = fraction of a good. In the worst case, if this good is of maximum cost\n1, the merchant will incur an additional cost of . (Lemma 3) (Theorem 1)"}, {"heading": "3.2 Learning Consumer Valuations", "text": "We now provide a query algorithm for learning the coefficients vi of the consumer\u2019s utility function. For the analysis only, we assume without loss of generality that the goods are ordered by decreasing value, i.e. v1 > . . . > vn. Our algorithm can learn the values in some suffix of this unknown ordering; the values that cannot be learned are irrelevant for setting optimal prices, since those goods will always be purchased by the consumer.\nTheorem 2. Algorithm LearnVal, given the ability to set prices, after at most O(n log((1\u2212\u03b4)/\u03b42)) price queries, outputs the ratio vi/vn for all goods i except those that will be bought under all price vectors, when values are discretized by \u03b4.\nAlgorithm LearnVal(\u03b4)\npi = 1 \u2200i . First price query x\u2190 consumer1(p) while \u00ac\u2203i such that 0 < xi < 1 do . Find some fractionally bought good\nChoose k such that xk = 0 pk = pk \u2212 \u03b4 x\u2190 consumer(p)\nj = i . j is least-preferred purchased good for k = j + 1 to n do . Learn sk for j + 1, . . . , n\nwhile xk = 0 do pk = pk \u2212 \u03b4 x\u2190 consumer(p)\nvk/vj = pk\nsk = (vk/vj)/(vn/vj) \u2200k \u2265 j . Renormalize ratios for k = j \u2212 1 to 1 do . Learn sk for j \u2212 1, . . . , 1\nfor \u03b1 = 1 to \u03b4 (in increments of \u03b4) do pi = 1 i \u2264 j \u2212 1 pi = \u03b1si \u2200i > j \u2212 1 x\u2190 consumer(p) if xk > 0 then\nsk = 1/\u03b1 break\nif sk is undefined then . k was always bought break\nreturn s\n1The notation x\u2190 consumer(p) specifies the bundle bought by the consumer at prices p.\nProof. Algorithm LearnVal proceeds by making price queries. That is, on each day we choose a particular price vector, observe the bundle purchased at those prices, and then use this information\nas part of the learning process. For our first query, we set pi = 1 for all i and observe the bundle x bought by the consumer. Let j be the index of the least-preferred good that the consumer purchases under this price vector. If the consumer buys some good i fractionally (which the algorithm can observe), then j = i. Otherwise, to learn j, we can incrementally lower the price of some good k that the consumer did not purchase, until k is purchased instead of another good i. Then we have learned j = i.\nIn the next phase of this algorithm, we will learn the ratio vk/vj for all goods k > j that were not originally purchased. To do so, we lower pk (while keeping all other prices at 1) until item k is purchased. This will occur when vk/pk = vj/pj , or vk/vj = pk. Recall that we assume each vi is discretized to a multiple of \u03b4. Therefore to guarantee that we learn the ratio vk/vj exactly, we must learn the ratio up to a precision of mink 6=k\u2032 |(vk \u2212 v\u2032k)|/vj . This quantity is minimized at vk = v \u2032 k + \u03b4 and vj = 1 (because vj \u2264 1), so it is sufficient to learn the ratio to within \u00b1\u03b4. Thus if we perform a discrete binary search on pk, it will require O(n log(1/\u03b4)) steps to exactly identify the desired ratio. Finally, we renormalize the ratios we have learned in terms of vn. That is, for all k \u2265 j, we define sk = vk/vn = (vk/vj)/(vn/vj).\nWe next attempt to learn sk = vk/vn for all k < j. These are the most preferred goods that were originally purchased under the all-1s price vector. We learn sk inductively in decreasing order of k, so as we learn sk, we already know the value si for all i > k. The goal is to now set prices so that the consumer will be indifferent to all goods j > k (i.e. will have a tie in the bang per buck for all these goods). The bang per buck on these goods is initially set low, and gradually raised by adjusting prices. At some critical point, a switch in the behavior of the consumer will be observed in which k is no longer purchased, and sk is learned.\nWe therefore introduce a parameter \u03b1, which controls the bang per buck ratio of goods k + 1, . . . , n. Define p(\u03b1, k) to be the price vector where pi = 1 for i \u2264 k, and pi = \u03b1sk for i > k. Let r(\u03b1, k) denote the corresponding bang per buck vector; i.e. r(\u03b1, k)i = vi/pi. It is easy to see that r(\u03b1, k) = (v1, . . . , vk, vn/\u03b1, . . . , vn/\u03b1). Thus, by lowering \u03b1, we lower the prices and raise the desirability of goods k+1, . . . , n. This process is illustrated in Figure 1, whereby gradually lowering \u03b1 from 1 we eventually reach a first point where goods k + 1, ..., n are preferred to good k. This switch occurs when the bang per buck ratio of goods k + 1, . . . , n equals the bang per buck ratio of k, i.e. vn/\u03b1 = vk, or \u03b1 = vn/vk. Our goal will be to identify this value of \u03b1, which we denote \u03b1\u2217(k). Once we know \u03b1\u2217(k), we will have learned sk, since sk = vk/vn = 1/\u03b1\n\u2217(k). We will know that we have found \u03b1\u2217(k) when we identify the highest value of \u03b1 for which some good goes from being purchased to unpurchased. This good must be k, because as we decrease \u03b1, we increase the desirability of goods k + 1, . . . , n, so none of these goods will go from being purchased to unpurchased. Of goods 1, . . . , k, good k is the least preferred, so this will be the first good to become unpurchased.\nTo learn the value of \u03b1\u2217(k) = vn/vk exactly, we must have a precision of mink 6=k |vn(|1/vk \u2212 1/v\u2032k|). This quantity is minimized at vn = \u03b4 (because we assume a lower bound on values, and therefore vn \u2265 \u03b4), vk = 1, and v\u2032k = 1 \u2212 \u03b4, and the corresponding value is \u03b42/(1 \u2212 \u03b4). Thus, we should search for the critical \u03b1\u2217(k) over the interval [0, 1] in increments of this size. In the algorithm\u2019s implementation, we can perform a binary rather than linear search over the range of \u03b1, which therefore requires requires O(n log((1 \u2212 \u03b4)/\u03b42)) steps. Once we have identified the value of \u03b1\u2217(k) to within \u03b42/1\u2212 \u03b4, we set sk = 1/\u03b1\u2217(k).\nIn this manner, we can inductively find the next ratio sk by equalizing preferences for the later goods and searching for the critical \u03b1\u2217(k). The only problem that might arise is that good k is\nsufficiently valued, and the budget sufficiently large, that good k is always purchased no matter how low \u03b1 is set. The Lemma 4 shows that if this occurs for some k, then no matter how we price goods 1, . . . , k, the consumer will always purchase these goods in full. Thus for the purposes of setting prices optimally for the merchant, it is unnecessary to learn the values of these goods, since he can set the corresponding prices to the maximum of 1.\nLemma 4. If goods 1, . . . , k are purchased at price vector p(\u03b1\u2032, k), where \u03b1\u2032 = \u03b1\u2217(k)\u2212 \u03b42/(1\u2212 \u03b4), then goods 1, . . . , k will be purchased at any price vector.\nProof. Consider the corresponding bang per buck vector r(\u03b1\u2032, k). For i \u2264 k, r(\u03b1\u2032, k)i = vi \u2265 vk, and for i < k, r(\u03b1\u2032, k)i > vk. Thus, good k has the lowest bang per buck of all goods. Since good k is still purchased, all other goods j 6= k must be purchased as well. Consider raising the price of some good j from its price under p(\u03b1\u2032, k), so that good j becomes less desirable. It must be that j > k, because all other prices are already maximized at 1, so this does not change the fact that goods 1, . . . , k will be purchased. Next consider lowering the price of some good j. This simply frees up more of the consumer\u2019s budget and allows the consumer more purchasing power, so all goods will remain purchased. (Lemma 4)\nThe last phase of the algorithm is the bottleneck, and so the overall running time (i.e. number of queries made) is O(n log((1 \u2212 \u03b4)/\u03b42)), which is linear in the bit description length of values.\n(Theorem 2)"}, {"heading": "3.3 Putting It All Together", "text": "We now combine the previous two sections into a complete online, no-regret algorithm. The informal description of Algorithm ProfitMax is as follows. First we use Algorithm LearnVal to learn all possible si ratios. For any good i for which we did not learn si, we set pi at 1. Then we apply Algorithm OptPrice to the subset of remaining goods, using the si ratios as input. The following main result shows that this approach achieves no-regret.\nAlgorithm ProfitMax(B, c, \u03b4, , T )\nProfit = 0 s\u2190 LearnVal(\u03b4) . Learn si ratios for i = 1 to n do\nif si = 0 then pi = 1\nv = {si | si \u2265 0} p\u2190 OptPrice(B, v, c, ) . Compute optimal prices while t \u2264 T do . Set prices optimally\nxt \u2190 consumer(p) Profit = Profit + xt \u00b7 (p\u2212 c)\nTheorem 3. For any > 0, Algorithm ProfitMax achieves regret O((n2/T ) log((1\u2212\u03b4)/\u03b42)) to the profit obtained by the optimal price vector, when values are discretized by \u03b4.\nProof. First we show that this algorithm correctly composes Algorithms LearnVal and OptPrice and indeed generates an approximately optimal price vector p.\nLemma 5. An approximately optimal pricing for goods {1, . . . , n} is obtained by setting pi = 1 for all goods for which Algorithm LearnVal could not learn si, and then applying Algorithm OptPrice to the si ratios of the remaining goods.\nProof. According to Lemma 4, for any good i for which Algorithm LearnVal could not learn si, the consumer will always purchase i, regardless of pi. Thus, our profit is maximized by setting pi = 1. Furthermore, recall that the bundle bought by the consumer is invariant to scaling, and so the consumer will buy the same bundle regardless of whether we price according to the si = vi/vn ratios or the actual vi values. Because our profit depends only on the bundle bought, it is therefore sufficient to use Algorithm OptPrice to price approximately optimally for the si ratios. (Lemma 5)\nAs the above lemma shows, every time we price according to p, we receive approximately optimal profit. In particular, according to Theorem 1, our profit will be less than OPT by at most , for any > 0. Furthermore, Algorithm LearnVal uses at most O(n log((1 \u2212 \u03b4)/\u03b42)) price queries, so there are O(n log((1 \u2212 \u03b4)/\u03b42)) days on which we might incur maximum regret. On any given day, the maximum possible profit is B, while the minimum possible is B \u2212 n, yielding a maximum regret of n. Thus, our overall per-step regret is bounded by O((n2/T ) log((1\u2212 \u03b4)/\u03b42) + ). Setting = (n2/T ) log((1\u2212 \u03b4)/\u03b42) yields the bound in the theorem statement. (Theorem 3)"}, {"heading": "4 Predicting Bundles in the Exogenous Price Model", "text": "We now consider our second model, in which an arbitrary and possibly adversarially selected price vector arrives every day, and the goal of our algorithm is to predict the bundle purchased by the consumer. Recall that the motivating scenario for such a setting is a merchant who is forced to set prices according to the market or the choices of parent company. At each day t, the algorithm observes a price vector pt and makes a prediction x\u0302t. The algorithm then learns the bundle purchased, xt \u2208 X(u, pt, B), and is said to make a mistake if xt 6= x\u0302t. Our goal is to prove an upper bound on the total number of mistakes ever made by our algorithm, in the worst case over price vectors and utility functions. We call such an upper bound a mistake bound.\nWe first informally describe the algorithm, given more precisely as Algorithm ExogLearnVal. The algorithm maintains the set of valuation vectors v consistent with the observations seen thus far: initially this feasible set is simply C0 = [0, 1]\nn. At every round t, the observed pair (pt, xt) provides a set of linear constraints which we add to further constrain our feasible set Ct. In particular, we have the following lemma, first observed in [9].\nLemma 6 ([9]). For any pair of goods i, j \u2208 [n] where xti > xtj, it must be that vi/pti \u2265 vj/ptj.\nThis lemma follows immediately from the fact that the vector xt is the solution to a fractional knapsack problem, for which the optimal algorithm is to buy goods in decreasing order of vi/p t i. The set of all such constraints learned so far at time t forms the feasible set Ct, which is a convex polytope. The idea of the algorithm is to sample a hypothesis valuation function vt uniformly at random from Ct at each stage, and predict using the bundle that would be purchased if the buyer had valuation function vt. The event that this hypothesis makes a mistake in its prediction is exactly the event that the hypothesis is eliminated when we update the consistent set to Ct+1, and so the probability of making a mistake is exactly equal to the fraction of volume eliminated from our consistent set, which allows us to charge our mistakes to the final volume of our consistent set. However, we need a way to lower bound the final volume of our consistent set.\nWe define the width of a polytope K in dimension i as widthi(K) = maxx,y\u2208K |xi \u2212 yi|. Note that the width can be efficiently computed using a linear program. We take advantage of the fact that the true valuation function takes values that are discretized to multiples of \u03b4. Hence, if at any point the width of our consistent set Ct in some dimension i is less than \u03b4/2, then we can exactly identify the ith coefficient of the consumer\u2019s valuation function. Note that if Vol(Ct) < \u03b4\nn, then there must be some dimension in which widthi(Ct) < \u03b4/2. When we detect such a dimension, we fix that coefficient, and restart the algorithm by maintaining a consistent set in one fewer dimension.\nHence, at each epoch, we maintain a set of consistent valuation functions restricted to those indices among those that are not yet fixed, and predict according to the composite of the valuation vectors sampled from this consistent set, together with the fixed indices. Every time we fix an index, a new epoch begins. The volume of the consistent set can never go below \u03b4n within an epoch, and since we fix an index at the end of every epoch, there can be at most n such epochs.\nThe only computationally challenging step is sampling a point uniformly at random from the consistent set Ct, which can be done in polynomial time using the technique of [3].\nTheorem 4. Algorithm ExogLearnVal runs in polynomial time per round, and with probability 1\u2212 \u03b2, makes at most O ( n2 log(1/\u03b4) + n \u221a log(1/\u03b2) log(1/\u03b4) ) mistakes over any sequence of adaptively\nchosen price vectors.\nAlgorithm ExogLearnVal(B, \u03b4)\nFixed = \u2205, wi = 0 for all i. . Initialize the fixed coordinates, initially none C0 = {z \u2208 [0, 1]n | 0 \u2264 zi \u2264 1 \u2200i} . Initialize the set of consistent hypotheses for t = 0 to T do\nObserve pt zt \u2190 sample1(Ct) . Sample a valuation uniformly from consistent set vt = (zt\u2212Fixed, wFixed) . Combine sampled and fixed coefficients Predict x\u0302t \u2208 arg maxx\u00b7pt\u2264B x \u00b7 vt . Predict according to sampled valuation xt \u2190 consumer(pt) Ct+1 = Ct for i, j 6\u2208 Fixed | xti > xtj do . Update Constraints\nCt+1 = Ct+1 \u2229 {zi/pti \u2265 zj/ptj} for i 6\u2208 Fixed, j \u2208 Fixed | xti > xtj do . Update Constraints\nCt+1 = Ct+1 \u2229 {zi/pti \u2265 wj/ptj} for j 6\u2208 Fixed, i \u2208 Fixed | xti > xtj do . Update Constraints\nCt+1 = Ct+1 \u2229 {wi/pti \u2265 zj/ptj} if There exists i 6\u2208 Fixed such that widthi(Ct+1) < \u03b4/2 then . Start a new Epoch\nfor Each i 6\u2208 Fixed such that widthi(Ct+1) < \u03b4/2 do . Fix each determined coordinate Fix(i) Ct+1 = {z \u2208 [0, 1]n\u2212|Fixed| | 0 \u2264 zi \u2264 1 \u2200i 6\u2208 Fixed} . Re-initialize in the unfixed coordinates\nprocedure Fix(i) Fixed = Fixed \u222a {i} z \u2190 sample(Ct+1) . Sample a value for coordinate i wi = round 2(zi/\u03b4) \u00b7 \u03b4 . Round sampled value to nearest discrete value and fix it\n1sample is any polynomial algorithm for sampling uniformly at random from a convex polytope. 2round(x) rounds to the nearest integer.\nProof. We prove a bound on the number of mistakes made by the algorithm in a single epoch (recall that each epoch ends when a new coordinate is fixed). Since there are only n coordinates, and hence at most n epochs, our final mistake bound is at most n times the mistake bound per epoch, giving the theorem.\nConsider an epoch i in which there remain d \u2264 n unfixed coordinates. We will track the ddimensional volume of the sets Ct during this epoch. Let Si be the first stage of the epoch, and let Fi be the final stage of the epoch. Note that Vol(CSi) = 1 (because CSi is always initialized to be the d-dimensional hypercube). Note also that for Si \u2264 t < Fi, any hypothesis in Ct that leads to an incorrect prediction of xt is eliminated from Ct+1. Hence, if Mt is the indicator random variable specifying whether our algorithm makes a mistake at round t, it is also the indicator random variable specifying whether our algorithm sampled a hypothesis that will be eliminated in the next round. Because we sample our hypothesis in round t at random from Ct, we have:\nE[Mt] = 1\u2212 Vol(Ct+1)\nVol(Ct)\nNote that we can write the volume of CFi as a telescoping product:\nVol(CFi) = Vol(CSi) Fi\u22121\u220f t=Si Vol(Ct+1) Vol(Ct) = Fi\u22121\u220f t=Si Vol(Ct+1) Vol(Ct)\nNote also that before the end of an epoch, the width of Ct in every coordinate is at least \u03b4/2. Hence we know Vol(CFi) \u2265 \u03b4d \u2265 \u03b4n. Combining these facts, we can write:\n\u03b4n \u2264 Fi\u22121\u220f t=Si Vol(Ct+1) Vol(Ct) = Fi\u22121\u220f t=Si (1\u2212 E[Mt]) \u2264 Fi\u22121\u220f t=Si exp(\u2212E[Mt]) = exp \u2212E Fi\u22121\u2211 t=Si Mt  Solving for the expected number of mistakes made in a single epoch, we find that:\nE  Fi\u2211 t=Si Mt  \u2264 E Fi\u22121\u2211 t=Si Mt + 1 \u2264 1 + n ln(1 \u03b4 ) = O ( n ln ( 1 \u03b4 ))\nNow consider the expected number of mistakes made by our algorithm for its entire run, t \u2208 {1, . . . , T}. Since we can partition each time step into one of at most n epochs, and have a bound on the expected number of mistakes in each epoch, by linearity of expectation, we have:\nE [ T\u2211 t=1 Mt ] = n\u2211 i=1 E  Fi\u2211 t=Si Mt  \u2264 n+ n2 ln(1 \u03b4 ) = O ( n2 ln ( 1 \u03b4 )) Note that each of the random variables Mi is independent and bounded in [0, 1]. We can\ntherefore apply a multiplicative Chernoff bound. For any < 1, we have:\nPr [ T\u2211 t=1 Mt \u2265 (1 + )E [ T\u2211 t=1 Mt ]] \u2264 exp ( \u2212 2 3 E [ T\u2211 t=1 Mt ])\nSetting the right hand side to be at most \u03b2, plugging in our bound on E [\u2211T\nt=1Mt\n] and solving\nfor allows us to take\n= O\n(\u221a ln(1/\u03b2)\nn2 ln(1/\u03b4) ) Plugging this into the Chernoff bound proves the theorem.\n(Theorem 4)"}, {"heading": "5 Conclusion and Future Work", "text": "Our work provides efficient learning algorithms for two new preference learning models. We leave as an open question whether it is possible to remove our assumptions of discretized and lower-bounded valuations. In the price-setting model, one might also wish to devise an algorithm in which the merchant can approximately optimize profits during the learning phase. Finally, there are several variants of our model to be considered in future work, such as multiple buyers with different utility functions, and stochastic budgets that vary daily."}], "references": [{"title": "Learning from revealed preference", "author": ["Eyal Beigman", "Rakesh Vohra"], "venue": "In Proceedings of the 7th ACM Conference on Electronic Commerce, EC", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "The empirical implications of privacy-aware choice", "author": ["Rachel Cummings", "Federico Echenique", "Adam Wierman"], "venue": "In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A random polynomial-time algorithm for approximating the volume of convex bodies", "author": ["Martin Dyer", "Alan Frieze", "Ravi Kannan"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm", "author": ["Nick Littlestone"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Lecture notes in microeconomic theory: the economic agent", "author": ["Ariel Rubinstein"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A note on the pure theory of consumers", "author": ["Paul A. Samuelson"], "venue": "behavior. Economica,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1938}, {"title": "Revealed preference", "author": ["Hal R. Varian"], "venue": "Samuelsonian economics and the twenty-first century,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Efficiently learning from revealed preference", "author": ["Morteza Zadimoghaddam", "Aaron Roth"], "venue": "In Proceedings of the 8th International Conference on Internet and Network Economics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": ", (pT , xT ) the revealed preferences problem (introduced by [7]; see [8] for a survey) is to determine whether the observations are consistent with a consumer optimizing any utility function u, subject to some mild constraints (e.", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": ", (pT , xT ) the revealed preferences problem (introduced by [7]; see [8] for a survey) is to determine whether the observations are consistent with a consumer optimizing any utility function u, subject to some mild constraints (e.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "textbook treatments in [5] and [6]).", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "There is a long line of work on the revealed preference problem, which was first introduced by Samuelson [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "For a textbook introduction to the standard model, see [5] and [6], and for a survey of recent work, see [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "For a textbook introduction to the standard model, see [5] and [6], and for a survey of recent work, see [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "The problem of finding a utility function that is both consistent and predictive was first considered by Beigman and Vohra [1], who formalize the statement that \u201cAfriat\u2019s Theorem Learners\u201d", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "[1] studies a PAC-like learning model in which they assume a distribution over observations and seek to find a hypothesis that performs well on future observations drawn from the same distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Roth and Zadimoghaddam [9] extended this line of work by providing computationally efficient learning algorithms for two specific classes of utility functions \u2014 linear and linearly separable and concave utility functions.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Cummings, Echenique, and Wierman [2] consider the revealed preferences problem when the consumer can strategically choose bundles to subvert the merchant\u2019s learning.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Our results in the second model are inspired by the classic halving algorithm for the online learning setting, which is credited to Littlestone [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": "We represent a bundle of goods x \u2208 [0, 1]n by a vector specifying what fraction of each of the n goods is purchased.", "startOffset": 35, "endOffset": 41}, {"referenceID": 0, "context": "The consumer has an unknown utility function u : [0, 1]n \u2192 R specifying her utility for each possible bundle.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "The merchant has the power to set prices (one for each good), also represented by a vector p \u2208 [0, 1]n (we normalize so that the price of every good i is pi \u2264 1).", "startOffset": 95, "endOffset": 101}, {"referenceID": 0, "context": "The merchant\u2019s goal is to obtain profit close to the maximum possible profit OPT = maxp\u2208[0,1]n maxx\u2208X(u,p,B) x \u00b7 (p\u2212 c) = maxp\u2208[0,1]n maxx\u2208X(u,p,B)B \u2212 x \u00b7 c.", "startOffset": 88, "endOffset": 93}, {"referenceID": 0, "context": "The merchant\u2019s goal is to obtain profit close to the maximum possible profit OPT = maxp\u2208[0,1]n maxx\u2208X(u,p,B) x \u00b7 (p\u2212 c) = maxp\u2208[0,1]n maxx\u2208X(u,p,B)B \u2212 x \u00b7 c.", "startOffset": 127, "endOffset": 132}, {"referenceID": 0, "context": "The key to the algorithm\u2019s efficiency will stem from the observation that there exists a restricted family of pricing vectors P \u2282 [0, 1]n containing a (nearly) profit-maximizing vector p\u2217.", "startOffset": 130, "endOffset": 136}, {"referenceID": 0, "context": "Thus, we should search for the critical \u03b1\u2217(k) over the interval [0, 1] in increments of this size.", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "The algorithm maintains the set of valuation vectors v consistent with the observations seen thus far: initially this feasible set is simply C0 = [0, 1] n.", "startOffset": 146, "endOffset": 152}, {"referenceID": 7, "context": "In particular, we have the following lemma, first observed in [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "Lemma 6 ([9]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "The only computationally challenging step is sampling a point uniformly at random from the consistent set Ct, which can be done in polynomial time using the technique of [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 0, "context": "Initialize the fixed coordinates, initially none C0 = {z \u2208 [0, 1]n | 0 \u2264 zi \u2264 1 \u2200i} .", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "Fix each determined coordinate Fix(i) Ct+1 = {z \u2208 [0, 1]n\u2212|Fixed| | 0 \u2264 zi \u2264 1 \u2200i 6\u2208 Fixed} .", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "Note that each of the random variables Mi is independent and bounded in [0, 1].", "startOffset": 72, "endOffset": 78}], "year": 2017, "abstractText": "We consider the problem of learning from revealed preferences in an online setting. In our framework, each period a consumer buys an optimal bundle of goods from a merchant according to her (linear) utility function and current prices, subject to a budget constraint. The merchant observes only the purchased goods, and seeks to adapt prices to optimize his profits. We give an efficient algorithm for the merchant\u2019s problem that consists of a learning phase in which the consumer\u2019s utility function is (perhaps partially) inferred, followed by a price optimization step. We also consider an alternative online learning algorithm for the setting where prices are set exogenously, but the merchant would still like to predict the bundle that will be bought by the consumer for purposes of inventory or supply chain management. In contrast with most prior work on the revealed preferences problem, we demonstrate that by making stronger assumptions on the form of utility functions, efficient algorithms for both learning and profit maximization are possible, even in adaptive, online settings. \u2217Computer and Information Science, University of Pennsylvania; {akareem,ldworkin,mkearns,aaroth}@cis.upenn.edu \u2020Computing and Mathematical Sciences, California Institute of Technology; rachelc@caltech.edu. Research performed while the author was visiting the University of Pennsylvania. ar X iv :1 40 7. 72 94 v1 [ cs .D S] 2 7 Ju l 2 01 4", "creator": "TeX"}}}