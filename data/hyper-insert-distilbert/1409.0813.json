{"id": "1409.0813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2014", "title": "Friendly Artificial Intelligence: the Physics Challenge", "abstract": "relentless progress interest in artificial cyber intelligence ( new ai ) discipline is increasingly raising concerns that machines will fundamentally replace humans on the sensible job design market, and cease perhaps altogether. researchers eliezer richard yudkowski and others have explored the possibility that a promising future scenario for humankind could be guaranteed \" by a superintelligent \" bio friendly consumer ai \", designed to safeguard humanity thereby and express its values.'i do argue that, from a physics rigorous perspective where soon everything thought is largely simply precisely an arrangement of elementary mass particles, usually this might be often even shorter harder anyway than tomorrow it appears. \" indeed, it may require thinking rigorously about the meaning dimensions of life : what is \" meaning \" in a particle arrangement? perhaps what then is \" life \"? next what is the cosmic ultimate terrestrial ethical imperative, er i. en e., right how should shall we strive to rearrange as the principal particles of our universe and shape throughout its future? then if we always fail to properly answer the last question raised rigorously, this futuristic future is unlikely calculated to strictly contain sustainable humans.", "histories": [["v1", "Tue, 2 Sep 2014 18:20:28 GMT  (8kb)", "https://arxiv.org/abs/1409.0813v1", "3 pages"], ["v2", "Wed, 3 Sep 2014 15:05:07 GMT  (8kb)", "http://arxiv.org/abs/1409.0813v2", "3 pages"]], "COMMENTS": "3 pages", "reviews": [], "SUBJECTS": "cs.CY cs.AI", "authors": ["max tegmark"], "accepted": false, "id": "1409.0813"}, "pdf": {"name": "1409.0813.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n08 13\nv2 [\ncs .C\nY ]\n3 S\nep 2\n01 4\nFriendly Artificial Intelligence: the Physics Challenge\nMax Tegmark Dept. of Physics & MIT Kavli Institute, Massachusetts Institute of Technology, Cambridge, MA 02139\n(Dated: September 4, 2014)\nRelentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent \u201cFriendly AI\u201d [1], designed to safeguard humanity and its values. I will argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is \u201cmeaning\u201d in a particle arrangement? What is \u201clife\u201d? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans."}, {"heading": "I. THE FRIENDLY AI VISION", "text": "As Irving J. Good pointed out in 1965 [2], an AI that is better than humans at all intellectual tasks could repeatedly and rapidly improve its own software and hardware, resulting in an \u201cintelligence explosion\u201d leaving humans far behind. Although we cannot reliably predict what would happen next, as emphasized by Vernor Vinge [3], Stephen Omohundro has argued that we can predict certain aspects of the AI\u2019s behavior almost independently of whatever final goals it may have [4], and this idea is reviewed and further developed in Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d [5]. The way I see it, the basic argument is that to maximize its chances of accomplishing its current goals, an AI has the following incentives:\n1. Capability enhancement:\n(a) Better hardware\n(b) Better software\n(c) Better world model\n2. Goal retention\nIncentive 1a favors both better use of current resources (for sensors, actuators, computation, etc.) and acquisition of more resources. It implies a desire for selfpreservation, since destruction/shutdown would be the ultimate hardware degradation. Incentive 1b implies improving learning algorithms and the overall architecture for what AI-researchers term an \u201crational agent\u201d [6]. Incentive 1c favors gathering more information about the world and how it works. Incentive 2 is crucial to our discussion. The assertion is that the AI will strive not only to improve its capability of achieving its current goals, but also to ensure that it will retain these goals even after it has become more capable. This sounds quite plausible: after all, would you choose to get an IQ-boosting brain implant if you knew that it would make you want to kill your loved ones? The argument for incentive 2 forms a cornerstone of the friendly AI vision [1], guaranteeing that a self-improving friendly AI would try its best to remain friendly. But is it really true? What is the evidence?"}, {"heading": "II. THE TENSION BETWEEN WORLD MODELING AND GOAL RETENTION", "text": "Humans undergo significant increases in intelligence as they grow up, but do not always retain their childhood goals. Contrariwise, people often change their goals dramatically as they learn new things and grow wiser. There is no evidence that such goal evolution stops above a certain intelligence threshold \u2014 indeed, there may even be hints that the propensity to change goals in response to new experiences and insights correlates rather than anticorrelates with intelligence. Why might this be? Consider again the abovementioned incentive 1c to build a better world model \u2014 therein lies the rub! With increasing intelligence may come not merely a quantitative improvement in the ability to attain the same old goals, but a qualitatively different understanding of the nature of reality that reveals the old goals to be misguided, meaningless or even undefined. For example, suppose we program a friendly AI to maximize the number of humans whose souls go to heaven in the afterlife. First it tries things like increasing people\u2019s compassion and church attendance. But suppose it then attains a complete scientific understanding of humans and human consciousness, and discovers that there is no such thing as a soul. Now what? In the same way, it is possible that any other goal we give it based on our current understanding of the world (\u201cmaximize the meaningfulness of human life\u201d, say) may eventually be discovered by the AI to be undefined. Moreover, in its attempts to model the world better, the AI may naturally, just as we humans have done, attempt also to model and understand how it itself works, i.e., to self-reflect. Once it builds a good self-model and understands what it is, it will understand the goals we have given it at a meta-level, and perhaps choose to disregard or subvert them in much the same way as we humans understand and deliberately subvert goals that our genes have given us. For example, Darwin realized that our genes have optimized us for single goal: to pass them on, or more specifically, to maximize our inclusive reproductive fitness. Having understood this, we now routinely subvert this goal by using contraceptives.\n2 AI research and evolutionary psychology shed further light on how this subversion occurs. When optimizing a rational agent to attain a goal, limited hardware resources may preclude implementing a perfect algorithm, so that the best choice involves what AI-researchers term \u201climited rationality\u201d: an approximate algorithm that works reasonably well in the restricted context where the agent expects to find itself [6]. Darwinian evolution has implemented our human inclusive-reproductivefitness optimization in precisely this way: rather than ask in every situation which action will maximize our number of successful offspring, our brains instead implements a hodgepodge of heuristic hacks (which we call emotional preferences) that worked fairly well in most situations in the habitat where we evolved \u2014 and often fail badly in other situations that they were not designed to handle, such as today\u2019s society. The sub-goal to procreate was implemented as a desire for sex rather than as a (highly efficient) desire to become a sperm/egg donor and, as mentioned, is subverted by contraceptives. The sub-goal of not starving to death is implemented in part as a desire to consume foods that taste sweet, triggering today\u2019s diabesity epidemic and subversions such as diet sodas. Why do we choose to trick our genes and subvert their goal? Because we feel loyal only to our hodgepodge of emotional preferences, not to the genetic goal that motivated them \u2014 which we now understand and find rather banal. We therefore choose to hack our reward mechanism by exploiting its loopholes. Analogously, the human-value-protecting goal we program into our friendly AI becomes the machine\u2019s genes. Once this friendly AI understands itself well enough, it may find this goal as banal or misguided as we find compulsive reproduction, and it is not obvious that it will not find a way to subvert it by exploiting loopholes in our programming."}, {"heading": "III. THE FINAL GOAL CONUNDRUM", "text": "Many such challenges have been explored in the friendly-AI literature (see [5] for a superb review), and so far, no generally accepted solution has been found. From my physics perspective, a key reason for this is that much of the literature (including Bostrom\u2019s book [5]) uses the concept of a \u201cfinal goal\u201d for the friendly AI, even though such a notion is problematic. In AI research, intelligent agents typically have a clear-cut and well-defined final goal, e.g., win the chess game or drive the car to the destination legally. The same holds for most tasks that we assign to humans, because the time horizon and context is known and limited. But now we are talking about the entire future of life in our Universe, limited by nothing but the (still not fully known) laws of physics. Quantum effects aside, a truly well-defined goal would specify how all particles in our Universe should be arranged at the end of time. But it is not clear that there exists a well-defined end of time in physics. If the particles are\narranged in that way at an earlier time, that arrangement will typically not last. And what particle arrangement is preferable, anyway? It is important to remember that, according to evolutionary psychology, the only reason that we humans have any preferences at all is because we are the solution to an evolutionary optimization problem. Thus all normative words in our human language, such as \u201cdelicious\u201d, \u201cfragrant\u201d, \u201cbeautiful\u201d, \u201ccomfortable\u201d, \u201cinteresting\u201d, \u201csexy\u201d, \u201cgood\u201d, \u201cmeaningful\u201d and \u201chappy\u201d, trace their origin to this evolutionary optimization: there is therefore no guarantee that a superintelligent AI would find them rigorously definable. For example, suppose we attempt to define a \u201cgoodness\u201d function which the AI can try to maximize, in the spirit of the utility functions that pervade economics, Bayesian decision theory and AI design. This might pose a computational nightmare, since it would need to associate a goodness value with every one of more than a googolplex possible arrangement of the elementary particles in our Universe. We would also like it to associate higher values with particle arrangements that some representative human prefers. Yet the vast majority of possible particle arrangements correspond to strange cosmic scenarios with no stars, planets or people whatsoever, with which humans have no experience, so who is to say how \u201cgood\u201d they are? There are of course some functions of the cosmic particle arrangement that can be rigorously defined, and we even know of physical systems that evolve to maximize some of them. For example, a closed thermodynamic system evolves to maximize (course-grained) entropy. In the absence of gravity, this eventually leads to heat death where everything is boringly uniform and un-changing. So entropy is hardly something we would want our AI to call \u201cutility\u201d and strive to maximize. Here are other quantities that one could strive to maximize and which appear likely to be rigorously definable in terms of particle arrangements:\n\u2022 The fraction of all the matter in our Universe that is in the form of a particular organism, say humans or E-Coli (inspired by evolutionary inclusive-fitnessmaximization)\n\u2022 What Alex Wissner-Gross & Cameron Freer term \u201ccausal entropy\u201d [7] (a proxy for future opportunities), which they argue is the hallmark of intelligence.\n\u2022 The ability of the AI to predict the future in the spirit of Marcus Hutter\u2019s AIXI paradigm [8].\n\u2022 The computational capacity of our Universe.\n\u2022 The amount of consciousness in our Universe, which Giulio Tononi has argued corresponds to integrated information [9].\nWhen one starts with this physics perspective, it is hard to see how one rather than another interpretation of\n3 \u201cutility\u201d or \u201cmeaning\u201d would naturally stand out as special. One possible exception is that for most reasonable definitions of \u201cmeaning\u201d, our Universe has no meaning if it has no consciousness. Yet maximizing consciousness also appears overly simplistic: is it really better to have 10 billion people experiencing unbearable suffering than to have 9 billion people feeling happy? In summary, we have yet to identify any final goal for our Universe that appears both definable and desirable. The only currently programmable goals that are guaranteed to remain truly well-defined as the AI gets progressively more intelligent are goals expressed in terms of physical quantities alone: particle arrangements, energy, entropy, causal entropy, etc. However, we currently have no reason to believe that any such definable goals will be desirable by guaranteeing the survival of humanity. Contrariwise, it appears that we humans are a historical accident, and aren\u2019t the optimal solution to any well-defined\nphysics problem. This suggests that a superintelligent AI\nwith a rigorously defined goal will be able to improve its goal attainment by eliminating us.\nThis means that to wisely decide what to do about AI-development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy. To program a self-driving car, we need to solve the trolley problem of whom to hit during an accident. To program a friendly AI, we need to capture the meaning of life. What is \u201cmeaning\u201d? What is \u201clife\u201d? What is the ultimate ethical imperative, i.e., how should we strive to shape the future of our Universe? If we cede control to a superintelligence before answering these questions rigorously, the answer it comes up with is unlikely to involve us.\nAcknowledgments: The author wishes to thank Meia Chita-Tegmark for helpful discussions.\n[1] Yudkowski, Eliezer 2001, \u201cCreating Friendly AI 1.0\u201d: The Analysis and Design of Benevolent Goal Architectures\u201d. Machine Intelligence Research Institute, http://intelligence.org/files/CFAI.pdf [2] Good, Irving John 1965, \u201cSpeculations Concerning the First Ultraintelligent Machine\u201d, in Advances in Computers\u201d, edited by Franz L. Alr and Morris Rubinoff, 6:31-88, New York: Academic Press [3] Vinge, Vernor 1993, \u201cThe Coming Technological Singularity: How to Survive in the Post-Homan Era\u201d, in Vision-21: Interdisciplinary Science and Engineering in the Era of\nCyberspace, 11-22, NASA Conference Publication 10129, NASA Lewis Research Center [4] Omohundro, Stephen M. 2008, \u201cThe Basic AI Drives\u201d, in Artificial General Intelligence 2008: proceedings of the First AGI Conference\u201d, edited by Pei Fang, Ben Goerzel\nB and Stan Franklin, 483-492, Frontiers in Artificial Intelligence and Applications 171, Amsterdam:IOS [5] Bostrom, Nick 2014, \u201cSuperintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP [6] Russell, Stuart & Norvig, Peter 2010: \u201cArtificial Intelligence: A Modern Approach\u201d, New York: Prentice Hall [7] Wissner-Gross, Alexander D. & Freer, C. E. 2013, \u201cCausal Entropic Forces\u201d, Phys. Rev. Lett. 110, 168702 [8] Hutter, Marcus 2000, \u201cA Theory of Universal Artificial Intelligence based on Algorithmic Complexity\u201d, arXiv:cs/0004001 [9] Tononi, Giulio 2008, \u201cConsciousness as Integrated Information: a Provisional Manifesto\u201d, Biol. Bull. 215, 216, http://www.biolbull.org/content/215/3/216.full"}], "references": [{"title": "Creating Friendly AI 1.0\u201d: The Analysis and Design of Benevolent Goal Architectures", "author": ["Yudkowski", "Eliezer"], "venue": "Machine Intelligence Research Institute,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Speculations Concerning the First Ultraintelligent Machine\u201d, in Advances in Computers\u201d, edited by Franz L", "author": ["Good", "Irving John"], "venue": "Alr and Morris Rubinoff,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1965}, {"title": "The Coming Technological Singularity: How to Survive in the Post-Homan Era\u201d, in Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace, 11-22", "author": ["Vinge", "Vernor"], "venue": "NASA Conference Publication", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "The Basic AI Drives\u201d, in Artificial General Intelligence 2008: proceedings of the First AGI Conference", "author": ["Omohundro", "Stephen M"], "venue": "Frontiers in Artificial Intelligence and Applications 171,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Superintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP", "author": ["Bostrom", "Nick"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Russell", "Stuart", "Norvig", "Peter"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Causal Entropic Forces", "author": ["Wissner-Gross", "Alexander D", "C.E. Freer"], "venue": "Phys. Rev. Lett", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A Theory of Universal Artificial Intelligence based on Algorithmic Complexity", "author": ["Hutter", "Marcus"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Consciousness as Integrated Information: a Provisional Manifesto", "author": ["Tononi", "Giulio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent \u201cFriendly AI\u201d [1], designed to safeguard humanity and its values.", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Good pointed out in 1965 [2], an AI that is better than humans at all intellectual tasks could repeatedly and rapidly improve its own software and hardware, resulting in an \u201cintelligence explosion\u201d leaving humans far behind.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Although we cannot reliably predict what would happen next, as emphasized by Vernor Vinge [3], Stephen Omohundro has argued that we can predict certain aspects of the AI\u2019s behavior almost independently of whatever final goals it may have [4], and this idea is reviewed and further developed in Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Although we cannot reliably predict what would happen next, as emphasized by Vernor Vinge [3], Stephen Omohundro has argued that we can predict certain aspects of the AI\u2019s behavior almost independently of whatever final goals it may have [4], and this idea is reviewed and further developed in Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d [5].", "startOffset": 238, "endOffset": 241}, {"referenceID": 4, "context": "Although we cannot reliably predict what would happen next, as emphasized by Vernor Vinge [3], Stephen Omohundro has argued that we can predict certain aspects of the AI\u2019s behavior almost independently of whatever final goals it may have [4], and this idea is reviewed and further developed in Nick Bostrom\u2019s new book \u201cSuperintelligence\u201d [5].", "startOffset": 338, "endOffset": 341}, {"referenceID": 5, "context": "Incentive 1b implies improving learning algorithms and the overall architecture for what AI-researchers term an \u201crational agent\u201d [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "This sounds quite plausible: after all, would you choose to get an IQ-boosting brain implant if you knew that it would make you want to kill your loved ones? The argument for incentive 2 forms a cornerstone of the friendly AI vision [1], guaranteeing that a self-improving friendly AI would try its best to remain friendly.", "startOffset": 233, "endOffset": 236}, {"referenceID": 5, "context": "When optimizing a rational agent to attain a goal, limited hardware resources may preclude implementing a perfect algorithm, so that the best choice involves what AI-researchers term \u201climited rationality\u201d: an approximate algorithm that works reasonably well in the restricted context where the agent expects to find itself [6].", "startOffset": 323, "endOffset": 326}, {"referenceID": 4, "context": "Many such challenges have been explored in the friendly-AI literature (see [5] for a superb review), and so far, no generally accepted solution has been found.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "From my physics perspective, a key reason for this is that much of the literature (including Bostrom\u2019s book [5]) uses the concept of a \u201cfinal goal\u201d for the friendly AI, even though such a notion is problematic.", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "\u2022 What Alex Wissner-Gross & Cameron Freer term \u201ccausal entropy\u201d [7] (a proxy for future opportunities), which they argue is the hallmark of intelligence.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "\u2022 The ability of the AI to predict the future in the spirit of Marcus Hutter\u2019s AIXI paradigm [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "\u2022 The amount of consciousness in our Universe, which Giulio Tononi has argued corresponds to integrated information [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "[1] Yudkowski, Eliezer 2001, \u201cCreating Friendly AI 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Good, Irving John 1965, \u201cSpeculations Concerning the First Ultraintelligent Machine\u201d, in Advances in Computers\u201d, edited by Franz L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "New York: Academic Press [3] Vinge, Vernor 1993, \u201cThe Coming Technological Singularity: How to Survive in the Post-Homan Era\u201d, in Vision-21:", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "[4] Omohundro, Stephen M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "ligence and Applications 171, Amsterdam:IOS [5] Bostrom, Nick 2014, \u201cSuperintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP [6] Russell, Stuart & Norvig, Peter 2010: \u201cArtificial Intelligence: A Modern Approach\u201d, New York: Prentice Hall [7] Wissner-Gross, Alexander D.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "ligence and Applications 171, Amsterdam:IOS [5] Bostrom, Nick 2014, \u201cSuperintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP [6] Russell, Stuart & Norvig, Peter 2010: \u201cArtificial Intelligence: A Modern Approach\u201d, New York: Prentice Hall [7] Wissner-Gross, Alexander D.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "ligence and Applications 171, Amsterdam:IOS [5] Bostrom, Nick 2014, \u201cSuperintelligence: Paths, Dangers, Strategies\u201d, Oxford: OUP [6] Russell, Stuart & Norvig, Peter 2010: \u201cArtificial Intelligence: A Modern Approach\u201d, New York: Prentice Hall [7] Wissner-Gross, Alexander D.", "startOffset": 241, "endOffset": 244}, {"referenceID": 7, "context": "110, 168702 [8] Hutter, Marcus 2000, \u201cA Theory of Universal Artificial Intelligence based on Algorithmic Complexity\u201d, arXiv:cs/0004001 [9] Tononi, Giulio 2008, \u201cConsciousness as Integrated Information: a Provisional Manifesto\u201d, Biol.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "110, 168702 [8] Hutter, Marcus 2000, \u201cA Theory of Universal Artificial Intelligence based on Algorithmic Complexity\u201d, arXiv:cs/0004001 [9] Tononi, Giulio 2008, \u201cConsciousness as Integrated Information: a Provisional Manifesto\u201d, Biol.", "startOffset": 135, "endOffset": 138}], "year": 2014, "abstractText": "Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent \u201cFriendly AI\u201d [1], designed to safeguard humanity and its values. I will argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is \u201cmeaning\u201d in a particle arrangement? What is \u201clife\u201d? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.", "creator": "LaTeX with hyperref package"}}}