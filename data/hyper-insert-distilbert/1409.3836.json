{"id": "1409.3836", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "Hardness of parameter estimation in graphical models", "abstract": "we consider the problem of learning the assumed canonical parameters specifying locally an alternative undirected input graphical model ( markov random finite field ) from taking the mean parameters. particularly for graphical models representing a minimal exponential family, the appropriate canonical parameters are uniquely determined uniquely by the mean parameters, roughly so the feasible problem response is generally feasible in principle. beside the goal of writing this this paper is to investigate the computational feasibility of this statistical model task. quite our main result shows in that parameter variance estimation actually is in particular general already intractable : furthermore no algorithm can learn readily the more canonical parameters of describing a generic pair - measure wise binary binary analytic graphical model from guessing the mean model parameters contained in time cycle bounded by assuming a different polynomial in the number of variables ( unless therefore rp = np ). indeed, such a result that has apparently been strongly believed to be potentially true ( \u2013 see the dual monograph by matthew wainwright smith and jordan ( 2008 ) ) but no proof was known.", "histories": [["v1", "Fri, 12 Sep 2014 19:57:59 GMT  (26kb)", "https://arxiv.org/abs/1409.3836v1", "14 pages. To appear in NIPS 2014"], ["v2", "Wed, 17 Sep 2014 19:57:51 GMT  (26kb)", "http://arxiv.org/abs/1409.3836v2", "15 pages. To appear in NIPS 2014"]], "COMMENTS": "14 pages. To appear in NIPS 2014", "reviews": [], "SUBJECTS": "cs.CC cs.AI cs.IT math.IT stat.CO", "authors": ["guy bresler", "david gamarnik", "devavrat shah"], "accepted": true, "id": "1409.3836"}, "pdf": {"name": "1409.3836.pdf", "metadata": {"source": "CRF", "title": "Hardness of parameter estimation in graphical models", "authors": ["Guy Bresler", "David Gamarnik", "Devavrat Shah"], "emails": ["gbresler@mit.edu", "gamarnik@mit.edu", "devavrat@mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n38 36\nv2 [\ncs .C\nC ]\n1 7"}, {"heading": "1 Introduction", "text": "Graphical models are a powerful framework for succinct representation of complex highdimensional distributions. As such, they are at the core of machine learning and artificial intelligence, and are used in a variety of applied fields including finance, signal processing, communications, biology, as well as the modeling of social and other complex networks. In this paper we focus on binary pairwise undirected graphical models, a rich class of models with wide applicability. This is a parametric family of probability distributions, and for the models we consider, the canonical parameters \u03b8 are uniquely determined by the vector \u00b5 of mean parameters, which consist of the node-wise and pairwise marginals.\nTwo primary statistical tasks pertaining to graphical models are inference and parameter estimation. A basic inference problem is the computation of marginals (or conditional probabilities) given the model, that is, the forward mapping \u03b8 7\u2192 \u00b5. Conversely, the backward mapping \u00b5 7\u2192 \u03b8 corresponds to learning the canonical parameters from the mean parameters. The backward mapping is defined only for \u00b5 in the marginal polytope M of realizable mean parameters, and this is important in what follows. The backward mapping captures maximum likelihood estimation of parameters; the study of the statistical properties of maximum likelihood estimation for exponential families is a classical and important subject.\nIn this paper we are interested in the computational tractability of these statistical tasks. A basic question is whether or not these maps can be computed efficiently (namely in time polynomial in\nthe problem size). As far as inference goes, it is well known that approximating the forward map (inference) is computational hard in general. This was shown by Luby and Vigoda [2] for the hardcore model, a simple pairwise binary graphical model (defined in (2.1)). More recently, remarkably sharp results have been obtained, showing that computing the forward map for the hard-core model is tractable if and only if the system exhibits the correlation decay property [3, 4]. In contrast, to the best of our knowledge, no analogous hardness result exists for the backward mapping (parameter estimation), despite its seeming intractability [1].\nTangentially related hardness results have been previously obtained for the problem of learning the graph structure underlying an undirected graphical model. Bogdanov et al. [5] showed hardness of determining graph structure when there are hidden nodes, and Karger and Srebro [6] showed hardness of finding the maximum likelihood graph with a given treewidth. Computing the backward mapping, in comparison, requires estimation of the parameters when the graph is known.\nOur main result, stated precisely in the next section, establishes hardness of approximating the backward mapping for the hard-core model. Thus, despite the problem being statistically feasible, it is computationally intractable.\nThe proof is by reduction, showing that the backward map can be used as a black box to efficiently estimate the partition function of the hard-core model. The reduction, described in Section 4, uses the variational characterization of the log-partition function as a constrained convex optimization over the marginal polytope of realizable mean parameters. The gradient of the function to be minimized is given by the backward mapping, and we use a projected gradient optimization method. Since approximating the partition function of the hard-core model is known to be computationally hard, the reduction implies hardness of approximating the backward map.\nThe main technical difficulty in carrying out the argument arises because the convex optimization is constrained to the marginal polytope, an intrinsically complicated object. Indeed, even determining membership (or evaluating the projection) to within a crude approximation of the polytope is NP-hard [7]. Nevertheless, we show that it is possible to do the optimization without using any knowledge of the polytope structure, as is normally required by ellipsoid, barrier, or projection methods. To this end, we prove that the polytope boundary has an inherent repulsive property that keeps the iterates inside the polytope without actually enforcing the constraint. The consequence of the boundary repulsion property is stated in Proposition 4.6 of Section 4, which is proved in Section 5.\nOur reduction has a close connection to the variational approach to approximate inference [1]. There, the conjugate-dual representation of the log-partition function leads to a relaxed optimization problem defined over a tractable bound for the marginal polytope and with a simple surrogate to the entropy function. What our proof shows is that accurate approximation of the gradient of the entropy obviates the need to relax the marginal polytope.\nWe mention a related work of Kearns and Roughgarden [8] showing a polynomial-time reduction from inference to determining membership in the marginal polytope. Note that such a reduction does not establish hardness of parameter estimation: the empirical marginals obtained from samples are guaranteed to be in the marginal polytope, so an efficient algorithm could hypothetically exist for parameter estimation without contradicting the hardness of marginal polytope membership.\nAfter completion of our manuscript, we learned that Montanari [9] has independently and simultaneously obtained similar results showing hardness of parameter estimation in graphical models from the mean parameters. His high-level approach is similar to ours, but the details differ substantially."}, {"heading": "2 Main result", "text": "In order to establish hardness of learning parameters from marginals for pairwise binary graphical models, we focus on a specific instance of this class of graphical models, the hard-core model. Given a graph G = (V,E) (where V = {1, . . . , p}), the collection of independent set vectors I(G) \u2286 {0, 1}V consist of vectors \u03c3 such that \u03c3i = 0 or \u03c3j = 0 (or both) for every edge {i, j} \u2208 E. Each vector \u03c3 \u2208 I(G) is the indicator vector of an independent set. The hard-core model assigns nonzero probability only to independent set vectors, with\nP\u03b8(\u03c3) = exp\n(\u2211\ni\u2208V \u03b8i\u03c3i \u2212 \u03a6(\u03b8)\n) for each \u03c3 \u2208 I(G) . (2.1)\nThis is an exponential family with vector of sufficient statistics \u03c6(\u03c3) = (\u03c3i)i\u2208V \u2208 {0, 1}p and vector of canonical parameters \u03b8 = (\u03b8i)i\u2208V \u2208 Rp. In the statistical physics literature the model is usually parameterized in terms of node-wise fugacity (or activity) \u03bbi = e\u03b8i . The log-partition function\n\u03a6(\u03b8) = log\n( \u2211\n\u03c3\u2208I(G) exp\n(\u2211\ni\u2208V \u03b8i\u03c3i\n))\nserves to normalize the distribution; note that \u03a6(\u03b8) is finite for all \u03b8 \u2208 Rp. Here and throughout, all logarithms are to the natural base.\nThe set M of realizable mean parameters plays a major role in the paper, and is defined as M = {\u00b5 \u2208 Rp| there exists a \u03b8 such that E\u03b8[\u03c6(\u03c3)] = \u00b5} .\nFor the hard-core model (2.1), the set M is a polytope equal to the convex hull of independent set vectors I(G) and is called the marginal polytope. The marginal polytope\u2019s structure can be rather complex, and one indication of this is that the number of half-space inequalities needed to represent M can be very large, depending on the structure of the graph G underlying the model [10, 11]. The model (2.1) is a regular minimal exponential family, so for each \u00b5 in the interior M\u25e6 of the marginal polytope there corresponds a unique \u03b8(\u00b5) satisfying the dual matching condition\nE\u03b8[\u03c6(\u03c3)] = \u00b5 .\nWe are concerned with approximation of the backward mapping \u00b5 7\u2192 \u03b8, and we use the following notion of approximation.\nDefinition 2.1. We say that y\u0302 \u2208 R is a \u03b4-approximation to y \u2208 R if y(1 \u2212 \u03b4) \u2264 y\u0302 \u2264 (1 + \u03b4). A vector v\u0302 \u2208 Rp is a \u03b4-approximation to v \u2208 Rp if each entry v\u0302i is a \u03b4-approximation to vi.\nWe next define the appropriate notion of efficient approximation algorithm.\nDefinition 2.2. A fully polynomial randomized approximation scheme (FPRAS) for a mapping fp : Xp \u2192 R is a randomized algorithm that for each \u03b4 > 0 and input x \u2208 Xp, with probability at least 3/4 outputs a \u03b4-approximation f\u0302p(x) to fp(x) and moreover the running time is bounded by a polynomial Q(p, \u03b4\u22121).\nOur result uses the complexity classes RP and NP, defined precisely in any complexity text (such as [12]). The class RP consists of problems solvable by efficient (randomized polynomial) algorithms, and NP consists of many seemingly difficult problems with no known efficient algorithms. It is widely believed that NP 6= RP. Assuming this, our result says that there cannot be an efficient approximation algorithm for the backward mapping in the hard-core model (and thus also for the more general class of binary pairwise graphical models).\nWe recall that approximating the backward mapping entails taking a vector \u00b5 as input and producing an approximation of the corresponding vector of canonical parameters \u03b8 as output. It should be noted that even determining whether a given vector \u00b5 belongs to the marginal polytope M is known to be an NP-hard problem [7]. However, our result shows that the problem is NP-hard even if the input vector \u00b5 is known a priori to be an element of the marginal polytope M. Theorem 2.3. Assuming NP 6= RP, there does not exist an FPRAS for the backward mapping \u00b5 7\u2192 \u03b8.\nAs discussed in the introduction, Theorem 2.3 is proved by showing that the backward mapping can be used as a black-box to efficiently estimate the partition function of the hard core model, known to be hard. This uses the variational characterization of the log-partition function as well as a projected gradient optimization method. Proving validity of the projected gradient method requires overcoming a substantial technical challenge: we show that the iterates remain within the marginal polytope without explicitly enforcing this (in particular, we do not project onto the polytope). The bulk of the paper is devoted to establishing this fact, which may be of independent interest.\nIn the next section we give necessary background on conjugate-duality and the variational characterization as well as review the result we will use on hardness of computing the log-partition function. The proof of Theorem 2.3 is then given in Section 4."}, {"heading": "3 Background", "text": ""}, {"heading": "3.1 Exponential families and conjugate duality", "text": "We now provide background on exponential families (as can be found in the monograph by Wainwright and Jordan [1]) specialized to the hard-core model (2.1) on a fixed graph G = (V,E). General theory on conjugate duality justifying the statements of this subsection can be found in Rockafellar\u2019s book [13].\nThe basic relationship between the canonical and mean parameters is expressed via conjugate (or Fenchel) duality. The conjugate dual of the log-partition function \u03a6(\u03b8) is\n\u03a6\u2217(\u00b5) := sup \u03b8\u2208Rd\n{ \u3008\u00b5, \u03b8\u3009 \u2212 \u03a6(\u03b8) } .\nNote that for our model \u03a6(\u03b8) is finite for all \u03b8 \u2208 Rp and furthermore the supremum is uniquely attained. On the interior M\u25e6 of the marginal polytope, \u2212\u03a6\u2217 is the entropy function. The logpartition function can then be expressed as\n\u03a6(\u03b8) = sup \u00b5\u2208M\n{ \u3008\u03b8, \u00b5\u3009 \u2212 \u03a6\u2217(\u00b5) } , (3.1)\nwith \u00b5(\u03b8) = argmax\n\u00b5\u2208M\n{ \u3008\u03b8, \u00b5\u3009 \u2212 \u03a6\u2217(\u00b5) } . (3.2)\nThe forward mapping \u03b8 7\u2192 \u00b5 is specified by the variational characterization (3.2) or alternatively by the gradient map \u2207\u03a6 : Rp \u2192 M. As mentioned earlier, for each \u00b5 in the interior M\u25e6 there is a unique \u03b8(\u00b5) satisfying the dual matching condition E\u03b8(\u00b5)[\u03c6(\u03c3)] = (\u2207\u03a6)(\u03b8(\u00b5)) = \u00b5. For mean parameters \u00b5 \u2208 M\u25e6, the backward mapping \u00b5 7\u2192 \u03b8(\u00b5) to the canonical parameters is given by\n\u03b8(\u00b5) = argmax \u03b8\u2208Rp\n{ \u3008\u00b5, \u03b8\u3009 \u2212 \u03a6(\u03b8) }\nor by the gradient \u2207\u03a6\u2217(\u00b5) = \u03b8(\u00b5) .\nThe latter representation will be the more useful one for us."}, {"heading": "3.2 Hardness of inference", "text": "We describe an existing result on the hardness of inference and state the corollary we will use. The result says that, subject to widely believed conjectures in computational complexity, no efficient algorithm exists for approximating the partition function of certain hard-core models. Recall that the hard-core model with fugacity \u03bb is given by (2.1) with \u03b8i = ln\u03bb for each i \u2208 V . Theorem 3.1 ([3, 4]). Suppose d \u2265 3 and \u03bb > \u03bbc(d) = (d\u22121) d\u22121\n(d\u22122)d . Assuming NP 6= RP, there exists no FPRAS for computing the partition function of the hard-core model with fugacity \u03bb on regular graphs of degree d. In particular, no FPRAS exists when \u03bb = 1 and d \u2265 5.\nWe remark that the source of hardness is the long-range dependence property of the hard-core model for \u03bb > \u03bbc(d). It was shown in [14] that for \u03bb < \u03bbc(d) the model exhibits decay of correlations and there is an FPRAS for the log-partition function (in fact there is a deterministic approximation scheme as well). We note that a number of hardness results are known for the hardcore and Ising models, including [15, 16, 3, 2, 4, 17, 18, 19]. The result stated in Theorem 3.1 suffices for our purposes.\nFrom this section we will need only the following corollary, proved in the Appendix. The proof, standard in the literature, uses the self-reducibility of the hard-core model to express the partition function in terms of marginals computed on subgraphs. Corollary 3.2. Consider the hard-core model (2.1) on graphs of degree most d with parameters \u03b8i = 0 for all i \u2208 V . Assuming NP 6= RP, there exists no FPRAS \u00b5\u0302(0) for the vector of marginal probabilities \u00b5(0), where error is measured entry-wise as per Definition 2.1."}, {"heading": "4 Reduction by optimizing over the marginal polytope", "text": "In this section we describe our reduction and prove Theorem 2.3. We define polynomial constants\n\u01eb = p\u22128 , q = p5 , and s = ( \u01eb 2p )2 , (4.1)\nwhich we will leave as \u01eb, q, and s to clarify the calculations. Also, given the asymptotic nature of the results, we assume that p is larger than a universal constant so that certain inequalities are satisfied.\nProposition 4.1. Fix a graph G on p nodes. Let \u03b8\u0302 : M\u25e6 \u2192 Rp be a black box giving a \u03b3approximation for the backward mapping \u00b5 7\u2192 \u03b8 for the hard-core model (2.1). Using 1/\u01eb\u03b32 calls to \u03b8\u0302, and computation bounded by a polynomial in p, 1/\u03b3, it is possible to produce a 4\u03b3p7/2/q\u01eb2approximation \u00b5\u0302(0) to the marginals \u00b5(0) corresponding to all zero parameters.\nWe first observe that Theorem 2.3 follows almost immediately.\nProof of Theorem 2.3. A standard median amplification trick (see e.g. [20]) allows to decrease the probability 1/4 of erroneous output by a FPRAS to below 1/p\u01eb\u03b32 usingO(log(p\u01eb\u03b32)) function calls. Thus the assumed FPRAS for the backward mapping can be made to give a \u03b3-approximation \u03b8\u0302 to \u03b8 on 1/\u01eb\u03b32 successive calls, with probability of no erroneous outputs equal to at least 3/4. By taking \u03b3 = \u03b3\u0303q\u01eb2p\u22127/2/2 in Proposition 4.1 we get a \u03b3\u0303-approximation to \u00b5(0) with computation bounded by a polynomial in p, 1/\u03b3\u0303. In other words, the existence of an FPRAS for the mapping \u00b5 7\u2192 \u03b8 gives an FPRAS for the marginals \u00b5(0), and by Corollary 3.2 this is not possible if NP 6= RP.\nWe now work towards proving Proposition 4.1, the goal being to estimate the vector of marginals \u00b5(0) for some fixed graph G. The desired marginals are given by the solution to the optimization (3.2) with \u03b8 = 0:\n\u00b5(0) = \u2212 argmin \u00b5\u2208M \u03a6\u2217(\u00b5) . (4.2)\nWe know from Section 3 that for x \u2208 M\u25e6 the gradient \u2207\u03a6\u2217(x) = \u03b8(x), that is, the backward mapping amounts to a gradient first order (gradient) oracle. A natural approach to solving the optimization problem (4.2) is to use a projected gradient method. For reasons that will be come clear later, instead of projecting onto the marginal polytope M, we project onto the shrunken marginal polytope M1 \u2282 M defined as M1 = {\u00b5 \u2208 M\u2229 [q\u01eb,\u221e)p : \u00b5+ \u01eb \u00b7 ei \u2208 M for all i} , (4.3) where ei is the ith standard basis vector.\nAs mentioned before, projecting onto M1 is NP-hard, and this must therefore be avoided if we are to obtain a polynomial-time reduction. Nevertheless, we temporarily assume that it is possible to do the projection and address this difficulty later. With this in mind, we propose to solve the optimization (4.2) by a projected gradient method with fixed step size s,\nxt+1 = PM1(xt \u2212 s\u2207\u03a6\u2217(xt)) = PM1(xt \u2212 s\u03b8(xt)) , (4.4)\nIn order for the method (4.4) to succeed a first requirement is that the optimum is inside M1. The following lemma is proved in the Appendix. Lemma 4.2. Consider the hard core model (2.1) on a graph G with maximum degree d on p \u2265 2d+1 nodes and canonical parameters \u03b8 = 0. Then the corresponding vector of mean parameters \u00b5(0) is in M1.\nOne of the benefits of operating within M1 is that the gradient is bounded by a polynomial in p, and this will allow the optimization procedure to converge in a polynomial number of steps. The following lemma amounts to a rephrasing of Lemmas 5.3 and 5.4 in Section 5 and the proof is omitted. Lemma 4.3. We have the gradient bound \u2016\u2207\u03a6\u2217(x)\u2016\u221e = \u2016\u03b8(x)\u2016\u221e \u2264 p/\u01eb = p9 for any x \u2208 M1.\nNext, we state general conditions under which an approximate projected gradient algorithm converges quickly. Better convergence rates are possible using the strong convexity of \u03a6\u2217 (shown in Lemma 4.5 below), but this lemma suffices for our purposes. The proof is standard (see [21] or Theorem 3.1 in [22] for a similar statement) and is given in the Appendix for completeness.\nLemma 4.4 (Projected gradient method). Let G : C \u2192 R be a convex function defined over a compact convex set C with minimizer x\u2217 \u2208 argminx\u2208C G(x). Suppose we have access to an approximate gradient oracle \u2207\u0302G(x) for x \u2208 C with error bounded as supx\u2208C \u2016\u2207\u0302G(x)\u2212\u2207G(x)\u20161 \u2264 \u03b4/2. Let L = supx\u2208C \u2016\u2207\u0302G(x)\u2016. Consider the projected gradient method xt+1 = PC(xt \u2212 s\u2207\u0302G(xt)) starting at x1 \u2208 C and with fixed step size s = \u03b4/2L2. After T = 4\u2016x1 \u2212 x\u2217\u20162L2/\u03b42 iterations the average x\u0304T = 1T \u2211T t=1 x t satisfies G(x\u0304T )\u2212G(x\u2217) \u2264 \u03b4.\nTo translate accuracy in approximating the function\u03a6\u2217(x\u2217) to approximatingx\u2217, we use the fact that \u03a6\u2217 is strongly convex. The proof (in the Appendix) uses the equivalence between strong convexity of \u03a6\u2217 and strong smoothness of the Fenchel dual \u03a6, the latter being easy to check. Since we only require the implication of the lemma, we defer the definitions of strong convexity and strong smoothness to the appendix where they are used.\nLemma 4.5. The function \u03a6\u2217 : M\u25e6 \u2192 R is p\u2212 32 -strongly convex. As a consequence, if \u03a6\u2217(x) \u2212 \u03a6\u2217(x\u2217) \u2264 \u03b4 for x \u2208 M\u25e6 and x\u2217 = argminy\u2208M\u25e6 \u03a6\u2217(y), then \u2016x\u2212 x\u2217\u2016 \u2264 2p 3 2 \u03b4.\nAt this point all the ingredients are in place to show that the updates (4.4) rapidly approach \u00b5(0), but a crucial difficulty remains to be overcome. The assumed black box \u03b8\u0302 for approximating the mapping \u00b5 7\u2192 \u03b8 is only defined for \u00b5 inside M, and thus it is not at all obvious how to evaluate the projection onto the closely related polytope M1. Indeed, as shown in [7], even approximate projection onto M is NP-hard, and no polynomial time reduction can require projecting onto M1 (assuming P 6= NP). The goal of the subsequent Section 5 is to prove Proposition 4.6 below, which states that the optimization procedure can be carried out without any knowledge about M or M1. Specifically, we show that thresholding coordinates suffices, that is, instead of projecting onto M1 we may project onto the translated non-negative orthant [q\u01eb,\u221e)p. Writing P\u2265 for this projection, we show that the original projected gradient method (4.4) has identical iterates xt as the much simpler update rule\nxt+1 = P\u2265(xt \u2212 s\u03b8(xt)) . (4.5) Proposition 4.6. Choose constants as per (4.1). Suppose x1 \u2208 M1, and consider the iterates xt+1 = P\u2265(xt \u2212 s\u03b8\u0302(xt)) for t \u2265 1, where \u03b8\u0302(xt) is a \u03b3-approximation of \u03b8(xt) for all t \u2265 1. Then xt \u2208 M1, for all t \u2265 1, and thus the iterates are the same using either P\u2265 or PM1 .\nThe next section is devoted to the proof of Proposition 4.6. We now complete the reduction.\nProof of Proposition 4.1. We start the gradient update procedure xt+1 = P\u2265(xt \u2212 s\u03b8\u0302(xt)) at the point x1 = ( 12p , 1 2p , . . . , 1 2p ), which we claim is within M1 for any graph G for p = |V | large enough. To see this, note that ( 1p , 1 p , . . . , 1 p ) is in M, because it is a convex combination (with weight 1/p each) of the independent set vectors e1, . . . , ep. Hence x1+ 12p \u00b7ei \u2208 M, and additionally x1i = 1 2p \u2265 q\u01eb, for all i.\nWe establish that xt \u2208 M1 for each t \u2265 1 by induction, having verified the base case t = 1 in the preceding paragraph. Let xt \u2208 M1 for some t \u2265 1. At iteration t of the update rule we make a call to the black box \u03b8\u0302(xt) giving a \u03b3-approximation to the backward mapping \u03b8(xt), compute xt \u2212 s\u03b8\u0302(xt), and then project onto [q\u01eb,\u221e)p. Proposition 4.6 ensures that xt+1 \u2208 M1. Therefore, the update xt+1 = P\u2265(xt \u2212 s\u03b8\u0302(xt)) is the same as xt+1 = PM1(xt \u2212 s\u03b8\u0302(xt)). Now we can now apply Lemma 4.4 with G = \u03a6\u2217, C = M1, \u03b4 = 2\u03b3p2/\u01eb and L = supx\u2208C \u2016\u2207\u0302G(x)\u20162 \u2264 \u221a p(p/\u01eb)2 = p3/2/\u01eb. After\nT = 4\u2016x1 \u2212 x\u2217\u20162L2/\u03b42 \u2264 4p(p3/\u01eb2)/(4\u03b32p4/\u01eb2) = 1/\u03b32\niterations the average x\u0304T = 1T \u2211T t=1 x t satisfies G(x\u0304T )\u2212G(x\u2217) \u2264 \u03b4.\nLemma 4.5 implies that \u2016x\u0304T \u2212 x\u2217\u20162 \u2264 2\u03b4p 32 , and since x\u2217i \u2265 q\u01eb, we get the entry-wise bound |x\u0304Ti \u2212 x\u2217i | \u2264 2\u03b4p 3 2x\u2217i /q\u01eb for each i \u2208 V . Hence x\u0304T is a 4\u03b3p7/2/q\u01eb2-approximation for x\u2217."}, {"heading": "5 Proof of Proposition 4.6", "text": "In Subsection 5.1 we prove estimates on the parameters \u03b8 corresponding to \u00b5 close to the boundary of M1, and then in Subsection 5.2 we use these estimates to show that the boundary of M1 has a certain repulsive property that keeps the iterates inside."}, {"heading": "5.1 Bounds on gradient", "text": "We start by introducing some helpful notation. For a node i, let N (i) = {j \u2208 [p] : (i, j) \u2208 E} denote its neighbors. We partition the collection of independent set vectors as\nI = Si \u222a S\u2212i \u222a S\u2298i , where\nSi = {\u03c3 \u2208 I : \u03c3i = 1} = {Ind sets containing i} S\u2212i = {\u03c3 \u2212 ei : \u03c3 \u2208 Si} = {Ind sets where i can be added} S\u2298i = {\u03c3 \u2208 I : \u03c3j = 1 for some j \u2208 N (i)} = {Ind sets conflicting with i} .\nFor a collection of independent set vectors S \u2286 I we write P(S) as shorthand for P\u03b8(\u03c3 \u2208 S) and\nf(S) = P(S) \u00b7 e\u03a6(\u03b8) = \u2211\n\u03c3\u2208S exp\n(\u2211\nj\u2208V \u03b8j\u03c3j\n) .\nWe can then write the marginal at node i as \u00b5i = P(Si), and since Si, S \u2212 i , S \u2298 i partition I, the space of all independent sets of G, 1 = P(Si) + P(S \u2212 i ) + P(S \u2298 i ). For each i let\n\u03bdi = P(S \u2298 i ) = P(a neighbor of i is in \u03c3) .\nThe following lemma specifies a condition on \u00b5i and \u03bdi that implies a lower bound on \u03b8i. Lemma 5.1. If \u00b5i + \u03bdi \u2265 1\u2212 \u03b4 and \u03bdi \u2264 1\u2212 \u03b6\u03b4 for \u03b6 > 1, then \u03b8i \u2265 ln(\u03b6 \u2212 1).\nProof. Let \u03b1 = e\u03b8i , and observe that f(Si) = \u03b1f(S \u2212 i ). We want to show that \u03b1 \u2265 \u03b6 \u2212 1.\nThe first condition \u00b5i + \u03bdi \u2265 1\u2212 \u03b4 implies that f(Si) + f(S \u2298 i ) \u2265 (1 \u2212 \u03b4)(f(Si) + f(S\u2298i ) + f(S\u2212i )) = (1 \u2212 \u03b4)(f(Si) + f(S\u2298i ) + \u03b1\u22121f(Si)) , and rearranging gives\nf(S\u2298i ) + f(Si) \u2265 1\u2212 \u03b4 \u03b4 \u03b1\u22121f(Si) . (5.1)\nThe second condition \u03bdi \u2264 1\u2212 \u03b6\u03b4 reads f(S\u2298i ) \u2264 (1\u2212 \u03b6\u03b4)(f(Si) + f(S\u2298i ) + f(S\u2212i )) or\nf(S\u2298i ) \u2264 1\u2212 \u03b6\u03b4 \u03b6\u03b4 f(Si)(1 + \u03b1 \u22121) (5.2)\nCombining (5.1) and (5.2) and simplifying results in \u03b1 \u2265 \u03b6 \u2212 1.\nWe now use the preceding lemma to show that if a coordinate is close to the boundary of the shrunken marginal polytope M1, then the corresponding parameter is large. Lemma 5.2. Let r be a positive real number. If \u00b5 \u2208 M1 and \u00b5+ r\u01eb \u00b7 ei /\u2208 M, then \u03b8i \u2265 ln ( q r \u2212 1 ) .\nProof. We would like to apply Lemma 5.1 with \u03b6 = q/r and \u03b4 = r\u01eb, which requires showing that (a) \u03bdi \u2264 1 \u2212 q\u01eb and (b) \u00b5i + \u03bdi \u2265 1 \u2212 r\u01eb. To show (a), note that if \u00b5 \u2208 M1, then \u00b5i \u2265 q\u01eb by definition of M1. It follows that \u03bdi \u2264 1\u2212 \u00b5i \u2264 1\u2212 q\u01eb. We now show (b). Since \u00b5i = P(Si), \u03bdi = P(S \u2298 i ), and 1 = P(Si) + P(S \u2298 i ) + P (S \u2212 i ), (b) is equivalent to P(S\u2212i ) \u2264 r\u01eb. We assume that \u00b5 + r\u01eb \u00b7 ei /\u2208 M and suppose for the sake of\ncontradiction that P(S\u2212i ) > r\u01eb. Writing \u03b7\u03c3 = P(\u03c3) for \u03c3 \u2208 I, so that \u00b5 = \u2211\n\u03c3\u2208I \u03b7\u03c3 \u00b7 \u03c3, we define a new probability measure\n\u03b7\u2032\u03c3 =   \n\u03b7\u03c3 + \u03b7\u03c3\u2212ei if \u03c3 \u2208 Si 0 if \u03c3 \u2208 S\u2212i \u03b7\u03c3 otherwise .\nOne can check that \u00b5\u2032 = \u2211\n\u03c3\u2208I \u03b7 \u2032 \u03c3\u03c3 has \u00b5 \u2032 j = \u00b5j for each i 6= j and \u00b5\u2032i = \u00b5i + P(S\u2212i ) > \u00b5i + r\u01eb.\nThe point \u00b5\u2032, being a convex combination of independent set vectors, must be in M, and hence so must \u00b5+ r\u01eb \u00b7 ei. But this contradicts the hypothesis and completes the proof of the lemma.\nThe proofs of the next two lemmas are similar in spirit to Lemma 8 in [23] and are proved in the Appendix. The first lemma gives an upper bound on the parameters (\u03b8i)i\u2208V corresponding to an arbitrary point in M1. Lemma 5.3. If \u00b5+ \u01eb \u00b7 ei \u2208 M, then \u03b8i \u2264 p/\u01eb. Hence if \u00b5 \u2208 M1, then \u03b8i \u2264 p/\u01eb for all i.\nThe next lemma shows that if a component \u00b5i is not too small, the corresponding parameter \u03b8i is also not too negative. As before, this allows to bound from below the parameters corresponding to an arbitrary point in M1. Lemma 5.4. If \u00b5i \u2265 q\u01eb, then \u03b8i \u2265 \u2212p/q\u01eb. Hence if \u00b5 \u2208 M1, then \u03b8i \u2265 \u2212p/q\u01eb for all i."}, {"heading": "5.2 Finishing the proof of Proposition 4.6", "text": "We sketch the remainder of the proof here; full detail is given in Section D of the Supplement.\nStarting with an arbitrary xt in M1, our goal is to show that xt+1 = P\u2265(xt \u2212 s\u03b8\u0302(xt)) remains in M1. The proof will then follow by induction, because our initial point x1 is in M1 by the hypothesis.\nThe argument considers separately each hyperplane constraint for M of the form \u3008h, x\u3009 \u2264 1. The distance of x from the hyperplane is 1\u2212 \u3008h, x\u3009. Now, the definition of M1 implies that if x \u2208 M1, then x+ \u01eb \u00b7ei \u2208 M1 for all coordinates i, and thus 1\u2212\u3008h, x\u3009 \u2265 \u01eb\u2016h\u2016\u221e for all constraints. We call a constraint \u3008h, x\u3009 \u2264 1 critical if 1\u2212 \u3008h, x\u3009 < \u01eb\u2016h\u2016\u221e, and active if \u01eb\u2016h\u2016\u221e \u2264 1\u2212 \u3008h, x\u3009 < 2\u01eb\u2016h\u2016\u221e. For xt \u2208 M1 there are no critical constraints, but there may be active constraints. We first show that inactive constraints can at worst become active for the next iterate xt+1, which requires only that the step-size is not too large relative to the magnitude of the gradient (Lemma 4.3 gives the desired bound). Then we show (using the gradient estimates from Lemmas 5.2, 5.3, and 5.4) that the active constraints have a repulsive property and that xt+1 is no closer than xt to any active constraint, that is, \u3008h, xt+1\u3009 \u2264 \u3008h, xt\u3009. The argument requires care, because the projection P\u2265 may prevent coordinates i from decreasing despite xti\u2212s\u03b8\u0302i(xt) being very negative if xti is already small. These arguments together show that xt+1 remains in M1, completing the proof."}, {"heading": "6 Discussion", "text": "This paper addresses the computational tractability of parameter estimation for the hard-core model. Our main result shows hardness of approximating the backward mapping \u00b5 7\u2192 \u03b8 to within a small polynomial factor. This is a fairly stringent form of approximation, and it would be interesting to strengthen the result to show hardness even for a weaker form of approximation. A possible goal would be to show that there exists a universal constant c > 0 such that approximation of the backward mapping to within a factor 1 + c in each coordinate is NP-hard."}, {"heading": "Acknowledgments", "text": "GB thanks Sahand Negahban for helpful discussions. Also we thank Andrea Montanari for sharing his unpublished manuscript [9]. This work was supported in part by NSF grants CMMI-1335155 and CNS-1161964, and by Army Research Office MURI Award W911NF-11-1-0036."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "A Miscellaneous proofs", "text": ""}, {"heading": "A.1 Proof of Corollary 3.2", "text": "The proof is standard and uses the self-reducibility of the hard-core model, meaning that conditioning on \u03c3i = 0 amounts to removing node i from the graph. Fix a graph G and parameters \u03b8 = 0. We show that given an algorithm to approximately compute the marginals for induced subgraphs H \u2286 G, it is possible to approximate the partition function e\u03a6(0), denoted here by Z . We first claim that\nZ =\np\u220f\ni=1\n1\n1\u2212 \u00b5i(G \\ [i\u2212 1]) . (A.1)\nThe graph G \\ [i \u2212 1] is obtained by removing nodes labeled 1, 2, . . . , i \u2212 1, and \u00b5i(G \\ [i \u2212 1]) is the marginal at node i for this graph. We use induction on the number of nodes. The base case with one node is trivial: Z = 1 + e0 = 2 = 1/(1 \u2212 \u00b5). Suppose now that the formula (A.1) holds for graphs on k nodes and that |V | = k + 1. Let Z0 and Z1 denote the partition function summation restricted to \u03c31 = 0 or \u03c31 = 1, respectively. Thus\nZ = Z0 + Z1 = Z0( Z0 + Z1\nZ0 ) = Z0 1\u2212 \u00b51 .\nNow Z0 is the partition function of a new graph obtained by deleting vertex i, and the inductive assumption proves the formula.\nFrom (A.1) we see that in order to compute a \u03b3-approximation to Z\u22121, it suffices to compute a \u03b3/p approximation to each of the marginals. Now for small \u03b3, a \u03b3 approximation to Z\u22121 gives a 2\u03b3 approximation to Z , and this completes the proof."}, {"heading": "A.2 Proof of Lemma 4.2", "text": "We wish to show that \u00b5(0) \u2208 M1 for a graph G = (V,E) of maximum degree d and p \u2265 2d+1. Consider a particular node i \u2208 V with neighbors N(i), and let di = |N(i)| denote its degree. We use the notation Si, S \u2212 i , S \u2298 i defined in Subsection 5.1. A collection of independent set vectors S \u2286 I(G) is assigned probability P(S) = |S|/|I(G)| for our choice \u03b8 = 0, so it suffices to argue about cardinalities.\nWe first claim that |Si| \u2265 2\u2212d|S\u2298i |. This follows by observing that each set in S\u2298i gets mapped to a set in Si by removing the neighbors Ni, and moreover at most 2d sets are mapped to the same set in Si. Next, we note that |Si| = |S\u2212i | since the removal of node i is a bijection from Si to S\u2212i and hence they are of the same cardinality. Combining these observations with the fact that P(Si) + P(S \u2212 i ) + P(S \u2298 i ) = 1, we get the estimate \u00b5i = P(Si) \u2265 1/(2\u2212d + 2) \u2265 2\u2212d\u22121.\nNext, we show for each coordinate i that the vector \u00b5\u2032 = \u00b5+2\u2212d\u22121ei is in M, which will complete the proof that \u00b5(0) is M1. Let \u03b7\u03c3 = P0(\u03c3) denote the probability assigned to \u03c3 under the distribution with parameters \u03b8 = 0, so that \u00b5 = \u2211 \u03c3\u2208I(G) \u03b7\u03c3 \u00b7 \u03c3. Similarly to the proof of Lemma 5.2, we define a new probability measure\n\u03b7\u2032\u03c3 =    \u03b7\u03c3 + 2 \u2212d\u22121 if \u03c3 \u2208 Si\n\u03b7\u03c3 \u2212 2\u2212d\u22121 if \u03c3 \u2208 S\u2212i \u03b7\u03c3 otherwise .\nThis is a valid probability distribution because \u03b7\u03c3 \u2265 2\u2212d\u22121 for \u03c3 \u2208 S\u2212i . One can check that \u00b5\u2032 = \u2211 \u03c3\u2208I \u03b7 \u2032 \u03c3\u03c3 has \u00b5 \u2032 j = \u00b5j for each j 6= i and \u00b5\u2032i = \u00b5i + 2\u2212d\u22121. The point \u00b5\u2032, being a convex combination of independent set vectors, must be in M, and hence so must \u00b5+ 2\u2212d\u22121ei."}, {"heading": "B Proofs for projected gradient method", "text": ""}, {"heading": "B.1 Proof of Lemma 4.4", "text": "The proof here is a slight modification of the proof of Theorem 3.1 in [22].\nObserve first that if P is the projection onto a convex set, then P is a contraction: \u2016P(x)\u2212P(y)\u20162 \u2264 \u2016x\u2212y\u20162 (cf. Prop 2.1.3 in [24]). Using the the convexity inequality G(x)\u2212G(x\u2217) \u2264 \u2207G(x)T (x\u2212 x\u2217), the definition \u03b7 = supx\u2208C \u2016\u2207\u0302G(x)\u2212\u2207G(x)\u20161, and the update formula xt+1 = xt\u2212s\u2207\u0302G(xt), it follows that\nG(xt)\u2212G(x\u2217) \u2264 \u2207G(xt)T (xt \u2212 x\u2217) = \u2207\u0302G(xt)T (xt \u2212 x\u2217) + (\u2207\u0302G(xt)T \u2212\u2207G(xt)T )(xt \u2212 x\u2217) \u2264 \u2207\u0302G(xt)T (xt \u2212 x\u2217) + \u03b7\u2016xt \u2212 x\u2217\u2016\u221e = 1\ns (xt \u2212 xt+1)T (xt \u2212 x\u2217) + \u03b7\n= 1\n2s (\u2016xt \u2212 x\u2217\u201622 + \u2016xt \u2212 xt+1\u201622 \u2212 \u2016xt+1 \u2212 x\u2217\u201622) + \u03b7\n= 1\n2s (\u2016xt \u2212 x\u2217\u201622 \u2212 \u2016xt+1 \u2212 x\u2217\u201622) +\ns 2 \u2016 \u2207\u0302G(xt)\u201622 + \u03b7 .\nAdding the preceding inequality for t = 1 to t = T , the sum telescopes and we get\nT\u2211\nt=1\n[G(xt)\u2212G(x\u2217)] \u2264 R 2\n2s +\ns 2 L2T + \u03b7T = RL\n\u221a T + \u03b7T . (B.1)\nHere we used the definitions R = \u2016x1 \u2212 x\u2217\u2016 and L = supx\u2208C \u2016\u2207\u0302G(x)\u2016 and the last equality is by the choice s = R\nL \u221a T . Now defining x\u0304T = 1T \u2211T t=1 x t, dividing (B.1) through by T and using the\nconvexity of G to apply Jensen\u2019s inequality gives\nG(x\u0304T )\u2212G(x\u2217) \u2264 RL\u221a T + \u03b7 .\nThus in order to make the right hand side smaller than \u03b4 it suffices to take T = 4R2L2/\u03b42 and \u03b7 = \u03b4/2."}, {"heading": "B.2 Proof of Lemma 4.5", "text": "We start by showing that the gradient \u2207\u03a6 is p 32 -Lipschitz. Recall that \u2207\u03a6(\u03b8) = \u00b5(\u03b8). We prove a bound on |\u00b5i(\u03b8) \u2212 \u00b5i(\u03b8\u2032)| by changing one coordinate of \u03b8 at a time. Let \u03b8(r) = (\u03b81, . . . , \u03b8r, \u03b8 \u2032 r+1, . . . , \u03b8 \u2032 p). The triangle inequality gives\n|\u00b5i(\u03b8)\u2212 \u00b5i(\u03b8\u2032)| = p\u22121\u2211\nr=0\n|\u00b5i(\u03b8(r))\u2212 \u00b5i(\u03b8(r+1))| .\nA direct calculation shows that\n\u2202\n\u2202\u03b8r \u00b5i(\u03b8) = P(\u03c3i = \u03c3r = 1)\u2212 \u00b5i(\u03b8)\u00b5r(\u03b8) .\nSince this is uniformly bounded by one in absolute value, we obtain the inequality |\u00b5i(\u03b8)\u2212\u00b5i(\u03b8\u2032)| \u2264 \u2016\u03b8 \u2212 \u03b8\u2032\u20161 or \u2016\u00b5(\u03b8)\u2212 \u00b5(\u03b8\u2032)\u20161 \u2264 p\u2016\u03b8 \u2212 \u03b8\u2032\u20161 Hence\n\u2016\u00b5(\u03b8)\u2212 \u00b5(\u03b8\u2032)\u20162 \u2264 \u2016\u00b5(\u03b8)\u2212 \u00b5(\u03b8\u2032)\u20161 \u2264 p\u2016\u03b8 \u2212 \u03b8\u2032\u20161 \u2264 p 3 2 \u2016\u03b8 \u2212 \u03b8\u2032\u20162 , i.e., \u2207\u03a6 is p 32 -Lipschitz.\nNow the function \u2207\u03a6 being p 32 -Lipschitz implies that \u03a6 is p 32 -strongly smooth, where \u03a6 is \u03b2strongly smooth if\n\u03a6(x +\u2206)\u2212 \u03a6(x) \u2264 \u3008\u2207\u03a6(x),\u2206\u3009 + 1 2 \u03b2\u2016\u2206\u20162 .\nTo see this, we write \u03a6(x+\u2206)\u2212 \u03a6(x) = \u222b 1\n0\n\u3008\u2207\u03a6(x + \u03c4\u2206),\u2206\u3009d\u03c4 = \u3008\u2207\u03a6(x),\u2206\u3009 + \u222b 1\n0\n( \u2207\u03a6(x+ \u03c4\u2206) \u2212\u2207\u03a6(x) ) d\u03c4\n\u2264 \u3008\u2207\u03a6(x),\u2206\u3009 + p 32 \u222b 1\n0\n\u3008\u03c4\u2206,\u2206\u3009d\u03c4\n= \u3008\u2207\u03a6(x),\u2206\u3009 + 12p 3 2 \u2016\u2206\u20162 .\nNow Theorem 6 from [25] or Chapter 5 of [26] imply that \u03a6\u2217, being the Fenchel conjugate of \u03a6, is p\u2212 3 2 -strongly convex, meaning\n\u03a6\u2217(x+\u2206)\u2212 \u03a6\u2217(x) \u2265 \u3008\u2207\u03a6\u2217(x),\u2206\u3009 + 12p\u2212 3 2 \u2016\u2206\u20162 . This gives the desired bound on \u2016x\u2212 x\u2217\u2016 in terms of \u03a6\u2217(x)\u2212 \u03a6\u2217(x\u2217)."}, {"heading": "C Proofs of gradient bounds", "text": ""}, {"heading": "C.1 Proof of Lemma 5.3", "text": "We suppose for the sake of deriving a contradiction that \u03b8i > p/\u03b4. Let \u00b5\u0304 = \u00b5 + \u03b4 \u00b7 ei, and let \u03b7\u2032 be a probability measure such that \u00b5\u0304 = \u2211 \u03c3\u2208I \u03b7 \u2032 \u03c3\u03c3. Now \u03b7\n\u2032(Si) = \u00b5\u0304i \u2265 \u03b4, and we define the non-negative measure \u03b3 (summing to less than one) with support Si as\n\u03b3\u03c3 = { \u03b7\u2032\u03c3 \u00b7 \u03b4\u03b7\u2032(Si) if \u03c3 \u2208 Si 0 otherwise .\nIn this way, \u03b3\u03c3 \u2264 \u03b7\u2032\u03c3 and \u03b3(Si) = \u03b4. We define a new probability measure\n\u03b7\u03c3 =    \u03b7\u2032\u03c3 \u2212 \u03b3\u03c3 if \u03c3 \u2208 Si \u03b7\u2032\u03c3 + \u03b3\u03c3\u222a{i} if \u03c3 \u2208 S\u2212i \u03b7\u2032\u03c3 otherwise ,\n(C.1)\nand one may check that \u00b5 = \u2211\n\u03c3\u2208I \u03b7\u03c3\u03c3 and \u03b7(S \u2212 i ) \u2265 \u03b3(Si) = \u03b4. We use the definitions in\nSubsection 5.1 to get\nF\u00b5(\u03b8) , \u00b5 \u00b7 \u03b8 \u2212 log (\u2211\n\u03c3\u2208I exp(\u03c3 \u00b7 \u03b8)\n)\n= \u2211\n\u03c1\u2208I \u03b7\u03c1 log exp(\u03c1 \u00b7 \u03b8)\u2211 \u03c3 exp(\u03c3 \u00b7 \u03b8)\n(a) =\u2264\n\u2211\n\u03c1\u2208S\u2212i\n\u03b7\u03c1 log exp(\u03c1 \u00b7 \u03b8)\nf(S\u2212i ) + e \u03b8if(S\u2212i ) + f(S \u2298 i )\n(b) \u2264 \u2211\n\u03c1\u2208S\u2212i\n\u03b7\u03c1 log f(S\u2212i )\ne\u03b8if(S\u2212i )\n\u2264 \u2212\u03b7(S\u2212i )\u03b8i (c) < \u2212p\n(d)\n\u2264 \u2212 log |I| = F (0) . Here (a) follows by restricting the sum to S\u2212i \u2286 I(G) and from the fact that \u2211 \u03c3 exp(\u03c3 \u00b7 \u03b8) = f(S\u2212i ) + e \u03b8if(S\u2212i ) + f(S \u2298 i ), (b) follows by retaining only the term e\n\u03b8if(S\u2212i ) in the denominator and replacing exp(\u03c1 \u00b7 \u03b8) for \u03c1 \u2208 S\u2212i with f(S\u2212i ) = \u2211 \u03c1\u2208S\u2212 i exp(\u03c1 \u00b7 \u03b8), thereby increasing the argument to the logarithm, (c) uses the fact that \u03b7(S\u2212i ) \u2265 \u03b4 and the assumption that \u03b8i > p/\u03b4, and (d) follows from the crude bound on number of independent sets |I| \u2264 2p and log 2 < 1. Finally, the relation \u03b8(\u00b5) = argmax\u03b8 F\u00b5(\u03b8) from Section 3 contradicts F\u00b5(\u03b8) < F (0)."}, {"heading": "C.2 Proof of Lemma 5.4", "text": "We suppose for the sake of contradiction that \u03b8i < \u2212p/\u03b4 and show that \u03b8 cannot be the vector of canonical parameters corresponding to \u00b5. Since \u00b5 \u2208 M, there exists a non-negative measure \u03b7 so that \u00b5 = \u2211\u03c3\u2208I \u03b7\u03c3\u03c3, and furthermore \u03b7(Si) = \u00b5i \u2265 \u03b4. Now arguments similar to the proof of Lemma 5.3 above give\nF\u00b5(\u03b8) = \u00b5 \u00b7 \u03b8 \u2212 log (\u2211\n\u03c3\nexp(\u03c3 \u00b7 \u03b8) )\n= \u2211\n\u03c1\u2208I \u03b7\u03c1 log exp(\u03c1 \u00b7 \u03b8)\u2211 \u03c3 exp(\u03c3 \u00b7 \u03b8)\n\u2264 \u2211\n\u03c1\u2208Si \u03b7\u03c1 log exp(\u03c1 \u00b7 \u03b8) f(S\u2212i ) + e \u03b8if(S\u2212i ) + f(S \u2217 i )\n\u2264 \u2211\n\u03c1\u2208Si \u03b7\u03c1 log\ne\u03b8if(S\u2212i )\nf(S\u2212i ) + e \u03b8if(S\u2212i ) + f(S \u2217 i )\n\u2264 \u2211\n\u03c1\u2208Si \u03b7\u03c1\u03b8i = \u03b7(Si)\u03b8i < \u2212\u03b4p/\u03b4 = \u2212p \u2264 \u2212 log |I| = F (0) .\nAs before, this contradicts the relation \u03b8(\u00b5) = argmax\u03b8 F\u00b5(\u03b8)."}, {"heading": "D Proof of Proposition 4.6", "text": "Starting with xt in M1, our goal is to show that xt+1 = P\u2265(xt \u2212 s\u03b8\u0302(xt)) remains in M1. The proof will then follow by induction, because our initial point x1 is in M1 by the hypothesis. We will use the fact that all hyperplane constraints for M, except for the non-negativity constraints xi \u2265 0, can be written as \u3008h, x\u3009 \u2264 1 for a vector h \u2208 [0, 1]p. This can be justified using the fact that ei \u2208 M for each i together with the property that for any \u00b5 \u2208 M, any coordinate of \u00b5 can be set to zero while remaining in M. Given our current iterate xt, we call a constraint \u3008h, x\u3009 \u2264 1 active if\n1\u2212 2\u01eb\u2016h\u2016\u221e < \u3008h, xt\u3009 \u2264 1\u2212 \u01eb\u2016h\u2016\u221e (D.1) and critical if 1\u2212 \u01eb\u2016h\u2016\u221e < \u3008h, xt\u3009 . (D.2) Observe that an active constraint has a coordinate i (namely i with hi = \u2016h\u2016\u221e) with \u3008h, xt + 2\u01eb \u00b7 ei\u3009 = \u3008h, xt\u3009+2hi\u01eb > 1 and similarly a critical constraint has a coordinate i with \u3008h, xt + \u01eb \u00b7 ei\u3009 = \u3008h, xt\u3009+ hi\u01eb > 1. For xt \u2208 M1 there are (by definition) no critical constraints, but there may be active constraints. We will first show that inactive constraints can at worst become active for the next iterate xt+1, which requires only that the step-size is not too large relative to the magnitude of the gradient. Then we show that the active constraints have a repulsive property and that xt+1 is no closer than xt to any active constraint, that is, \u3008h, xt+1\u3009 \u2264 \u3008h, xt\u3009. Thus, if xt is in M1, then there are no critical constraints for xt+1 and every coordinate i satisfies \u3008h, xt+1 + \u01eb \u00b7 ei\u3009 \u2264 1 for all constraint vectors h. Since the projection P\u2265 ensures that xt+1i \u2265 q\u01eb, the update xt+1 is in M1. We now focus on inactive constraints.\nInactive constraint. We consider an inactive constraint h, meaning that \u3008h, xt\u3009 + 2\u01eb\u2016h\u2016\u221e \u2264 1 . By assumption the step size s = ( \u01eb 2p )2 so the increment in any coordinate j is bounded as\nxt+1j \u2212 xtj \u2264 s|\u03b8\u0302j(xt)| \u2264 s|\u03b8\u0302j(xt)\u2212 \u03b8j(xt)|+ s|\u03b8j(xt)| \u2264 (1 + \u03b3)s|\u03b8j(xt)| \u2264 \u01eb/p\nusing Lemma 5.3 and fact that \u03b3 \u2264 1. These bounds give \u3008h, xt+1\u3009 = \u3008h, xt\u3009+ \u3008h, xt+1 \u2212 xt\u3009 \u2264 \u3008h, xt\u3009+ \u2211\nj\nhj(x t+1 j \u2212 xtj)\n\u2264 \u3008h, xt\u3009+ p(\u01eb/p)\u2016h\u2016\u221e \u2264 1\u2212 \u01eb\u2016h\u2016\u221e which shows that the constraint is not critical for xt+1 and at worst becomes active.\nActive constraint. The rough idea is that if a coordinate i cannot be increased by 2\u01eb while remaining in M, then the parameter \u03b8i must be sufficiently large, and the next iterate xt+1 will decrease enough to overcome the possible increase in other coordinates. This argument does not work, however, because it might be the case that xti = q\u01eb, which prevents any decrease (i.e., x t+1 i \u2265 xti) due to the projection P\u2265. Instead, we start by showing that if some coordinate cannot be increased by 2\u01eb, then there must be a reasonably large coordinate which cannot be increased by 4p\u01eb.\nLemma D.1. If h is an active constraint, then there is a coordinate \u2113 \u2208 V with xt + (4p\u01eb)e\u2113 /\u2208 M and xt\u2113 \u2265 2q\u01eb.\nProof. If h is active then 1\u2212 2\u01eb\u2016h\u2016\u221e < \u3008h, xt\u3009. Using the fact that hj \u2264 1 for all j we have 1\u2212 2\u01eb \u2264 1\u2212 2\u01eb\u2016h\u2016\u221e < \u3008h, xt\u3009 . (D.3)\nLet B \u2286 V consist of coordinates j with small entries xtj \u2264 2\u01ebq. Then\n\u3008h, xt\u3009 = \u2211\nj\u2208B hjx\nt + \u2211\nj\u2208Bc hjx\nt \u2264 |B|(2\u01ebq) + \u2211\nj\u2208Bc hjx\nt \u2264 2 p +\n\u2211\nj\u2208Bc hjx\nt j . (D.4)\nThe last inequality used the crude estimate |B| \u2264 p. Combining (D.3) and (D.4) and rearranging gives \u2211\nj\u2208Bc hjx\nt j \u2265 1\u2212 2\u01eb\u2212 2/p \u2265 1\u2212 3/p ,\nand it follows that there is an \u2113 \u2208 Bc for which h\u2113 \u2265 h\u2113xt\u2113 \u2265 1/2p. Adding h\u2113 \u00b7 (4p\u01eb) \u2265 2\u01eb to both sides of (D.3) shows that xt + (4p\u01eb)e\u2113 violates the inequality \u3008h, x\u3009 \u2264 1. This proves the lemma, since xt\u2113 > 2q\u01eb for \u2113 \u2208 Bc.\nWe are now ready to prove that \u3008h, xt+1\u3009 \u2264 \u3008h, xt\u3009. Let \u2113 be the coordinate promised by Lemma D.1, with xt + (4p\u01eb)e\u2113 /\u2208 M and xt\u2113 \u2265 2q\u01eb. From Lemma 5.2, we know that \u03b8\u2113(xt) \u2265 log ( q 4p \u2212 1 ) \u2265 3 log p, for p large enough. By definition of \u03b8\u0302 being a \u03b3-approximation to \u03b8, \u03b8\u0302\u2113(x t) \u2265 (1 \u2212 \u03b3)\u03b8\u2113(xt). Therefore, since \u03b3 \u2192 0 as p \u2192 \u221e, it follows that for p large enough \u03b8\u0302\u2113(x t) \u2265 log p. This implies\nxt+1\u2113 \u2212 xt\u2113 \u2264 \u2212min(s\u03b8\u0302(xt), s log p) \u2264 \u2212s log p . (D.5) Here we used the fact that xt\u2113 \u2265 q\u01eb+ s log p so the projection P\u2265 does not affect this coordinate. Denote by D the set of coordinates\nD = {j \u2208 [p] : \u3008h, xt\u3009+ q2\u01ebhj > 1} . These coordinates have non-positive increment: since xj \u2265 q\u01eb for x \u2208 M1, Lemma 5.1 implies that \u03b8j \u2265 0, and hence \u03b8\u0302j \u2265 (1 \u2212 \u03b3)\u03b8j \u2265 0, or\nxt+1j \u2212 xtj \u2264 0 for j \u2208 D .\nIn contrast, coordinates in Dc might increase, but by a limited amount: since xt \u2208 M1, all coordinates j \u2208 Dc satisfy xtj \u2265 q\u01eb, and Lemma 5.4 gives the bound \u03b8j \u2265 \u2212p/q\u01eb, or\nxt+1j \u2212 xtj \u2264 (1 + \u03b3)| \u2212 s\u03b8j | \u2264 2sp/q\u01eb for all j \u2208 Dc . (D.6) Additionally, by the definition of D and the fact that increasing coordinate \u2113 by 4p\u01eb violates \u3008h, x\u3009 \u2264 1, if j \u2208 Dc, then 4p\u01ebh\u2113 > q\u01ebhj/2, or\nhj < 8ph\u2113/q for all j \u2208 Dc . (D.7)\nUsing the crude bound |Dc| \u2264 p together with (D.6) and (D.7) gives \u2211\nj\u2208Dc hj(x\nt+1 j \u2212 xtj) \u2264 |Dc| 8ph\u2113 q \u00b7 2sp q\u01eb \u2264 s4p 2 q2\u01eb h\u2113 \u2264 4sh\u2113 . (D.8)\nCounting the contributions from Dc in (D.8) in addition to D (none) and \u2113 (negative as per (D.5)), it follows that\n\u3008c, xt+1\u3009 = \u3008h, xt\u3009+ \u3008h, xt+1 \u2212 xt\u3009 \u2264 \u3008h, xt\u3009+ sh\u2113(4 \u2212 \u03b8\u2113) \u2264 \u3008h, xt\u3009+ sh\u2113(4 \u2212 ln p) \u2264 \u3008h, xt\u3009 .\nHere we have used the fact that p is large enough (p \u2265 e4 suffices for this last step). In words, we move away from any active hyperplane constraint. This completes the proof."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "We consider the problem of learning the canonical parameters specifying an undi-<lb>rected graphical model (Markov random field) from the mean parameters. For<lb>graphical models representing a minimal exponential family, the canonical param-<lb>eters are uniquely determined by the mean parameters, so the problem is feasible<lb>in principle. The goal of this paper is to investigate the computational feasibil-<lb>ity of this statistical task. Our main result shows that parameter estimation is in<lb>general intractable: no algorithm can learn the canonical parameters of a generic<lb>pair-wise binary graphical model from the mean parameters in time bounded by a<lb>polynomial in the number of variables (unless RP = NP). Indeed, such a result has<lb>been believed to be true (see [1]) but no proof was known.<lb>Our proof gives a polynomial time reduction from approximating the partition<lb>function of the hard-core model, known to be hard, to learning approximate pa-<lb>rameters. Our reduction entails showing that the marginal polytope boundary has<lb>an inherent repulsive property, which validates an optimization procedure over<lb>the polytope that does not use any knowledge of its structure (as required by the<lb>ellipsoid method and others).", "creator": "LaTeX with hyperref package"}}}