{"id": "1409.4614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2014", "title": "Lexical Normalisation of Twitter Data", "abstract": "twitter shares with circulation over, 500 million users globally, snap generates over approximately 100, 000 tweets typically per working minute. despite the 140 character pitch limit per stop tweet has, perhaps explained unintentionally, encourages users seeking to use its shorthand notations together and to strip spellings to depict their maximum bare to minimum \" pronunciation syllables \" or elisions while e. j g. \" srsly \". the rapid analysis of twitter messages which really typically contain misspellings, grammatical elisions, timing and grammatical definition errors, poses a recurring challenge to established natural language processing ( nlp ) or tools which are generally particularly designed with maintaining the assumption that the printed data conforms exactly to the least basic grammatical structure commonly used principally in speaking english and language. in either order to make sense of other twitter content messages it is necessary to fairly first transform them strictly into quite a canonical english form, consistent with unlike the dictionary or html grammar. this interactive process, basically performed best at decreasing the level of individual tokens ( \" words \" ), here is technically called sequential lexical normalisation. indeed this paper investigates what various functional techniques suggested for computing lexical normalisation of the tweeter data and presents by the findings illustrated as the software techniques are applied individually to process : raw data from tweeter.", "histories": [["v1", "Tue, 16 Sep 2014 12:59:07 GMT  (495kb)", "http://arxiv.org/abs/1409.4614v1", "4 pages"], ["v2", "Wed, 17 Sep 2014 02:48:53 GMT  (480kb)", "http://arxiv.org/abs/1409.4614v2", "4 pages; removed typos"], ["v3", "Sun, 13 Sep 2015 04:56:13 GMT  (466kb)", "http://arxiv.org/abs/1409.4614v3", "4 pages; removed typos"], ["v4", "Sun, 20 Sep 2015 01:11:53 GMT  (466kb)", "http://arxiv.org/abs/1409.4614v4", "Removed typos"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bilal ahmed"], "accepted": false, "id": "1409.4614"}, "pdf": {"name": "1409.4614.pdf", "metadata": {"source": "CRF", "title": "Lexical Normalisation of Twitter Data", "authors": ["Bilal Ahmed"], "emails": ["bahmad@student.unimelb.edu.au"], "sections": [{"heading": null, "text": "Twitter with over 500 million users globally, generates over 100,000 tweets per minute1. The 140 character limit per tweet has, perhaps unintentionally, encourages users to use shorthand notations and to strip spellings to their bare minimum \u201csyllables\u201d or elisions e.g. \u201csrsly\u201d. The analysis of twitter messages which typically contain misspellings, elisions, and grammatical errors, poses a challenge to established Natural Language Processing (NLP) tools which are generally designed with the assumption that the data conforms to the basic grammatical structure commonly used in English language. In order to make sense of Twitter messages it is necessary to first transform them into a canonical form, consistent with the dictionary or grammar. This process, performed at the level of individual tokens (\u201cwords\u201d), is called lexical normalisation. This paper investigates various techniques for lexical normalisation of Tweeter data and presents the findings as the techniques are applied to process raw data from Tweeter."}, {"heading": "1. Introduction", "text": "A Tweeter message or \u201ctweet\u201d consists of 140 or fewer characters, and generally contains hash tags and @ symbols. In order to lexically analyse a Tweeter message each token needs to be identified on a case by case basis before normalisation techniques are applied to correct spelling mistakes and make sense of the various acronyms and elisions frequently used in Tweeter messages. The proceeding sections describe in detail the various techniques that are applied to identity: \u201cknown\u201d or \u201cin-vocabulary\u201d words; punctuation and special symbols (both general and Tweeter specific); and candidates for normalisation. We then apply various normalisation techniques to correct out of vocabulary (\u201cOOV\u201d) tokens."}, {"heading": "2. In Vocabulary Tokens", "text": "The first step is to identify tokens or words that\n1 Twitter in numbers, The Telegraph, March 2013\nexact match of the word. A token is tagged as \u201cin vocabulary (\u201cIV\u201d) if an exact match is found. For the purpose of this project we have used a lexicon of 115,326 words (words.txt) to identify \u201cin vocabulary\u201d words. Tokens that fall outside of this vocabulary are then considered as candidates for normalisation and are further processed or marked as non-candidates if deemed not fit for normalisation."}, {"heading": "3. Non-Candidate Tokens", "text": "In addition to common punctuation symbols a Tweeter message or \u201ctweet\u201d generally contains hash tags, the \u201c#\u201d symbol, to mark keywords or topics in a tweet and the \u201c@\u201d symbol followed by a user\u2019s Twitter username to refer to a user when replying or commenting. The tokens are parsed using regular expression to identify special characters, punctuation and tweeter specific symbols. These special tokens are marked as non-candidates (\u201cNO\u201d) and are not processed for normalisation."}, {"heading": "4. Normalisation of Out of Vocabulary Tokens", "text": "Lexical normalisation is the process of transforming tokens into a canonical form consistent with the dictionary and grammar. These tokens include words that are misspelt or intentionally shortened (elisions) due to character limit in case of Twitter.\nWhen a word falls outside the vocabulary as defined by the collection of words in word.txt file, and does not contain any special characters, punctuation or tweeter specific symbols, it is marked as out of vocabulary (\u201cOOV\u201d) and is processed as a candidate for normalisation.\nOverview of the Normalisation Process: Once a candidate have been identified for normalisation, firstly, edit distance (Levenshtein distance) technique is applied to find matches from (words.utf-8.txt) which are within 2 (inclusive) edit distance of the query. The results are stored in an array. We refer to this set as the \u201cFirst Set of Matches based on Edit Distance\u201d since they contain approximate matches based on their textual similarity to the query.\nThe second step in the process is to apply Refined Soundex technique to this set of matches based on edit distance. This refines the set and results in approximate matches that are phonetically similar to the query. The results are stored in another array. This refined and phonetically similar set of words is referred to as \u201cPhonetic Matches\u201d.\nThe third step is to find an approximate match using Peter Norvig\u2019s Algorithm. This returns 1 match deemed closest to the query by the algorithm.\nThe forth step compares the result of the Peter Norwig algorithm with those obtained in Step 2 by applying Refined Soundex technique. If both the results are exactly the same, i.e. only 1 phonetic match is found by Refined Soundex technique and is exactly the same as that returned by Peter Norvig\u2019s Algorithm, then no further processing is performed, and the result is used as the normalised version for the query.\nIf more than 1 phonetic match is returned by Refined Soundex technique then based on rules described in Section 4.3 a further 5-Gram Context Matching is performed.\nThe fifth step is to perform a 5-Gram Context Matching technique using each phonetic match as the query in the following regular expression:\n{Previous word, Query, Next word}\nThis takes into account the previous and next words, to the query, and performs an exhaustive search to find the most commonly pattern. This technique uses each phonetic match to see if it is a likely candidate based on its occurrence as defined by the pattern above. This technique is further explained in Section 4.4. The outcome of this search is used as the normalised version for the query.\n2 *words.txt is used to search for in-vocabulary words. + words.utf-8.txt is used to search for approximate matches for normalisation of \u201cOOV\u201d words ^ big.txt consists of about a million words. The file is a concatenation of several public domain books from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus. This is used by Peter Norvig\u2019s algorithm for naturalisation. # w5_.txt is used to perform context based 5-Gram\nmatching\nApproximate matching techniques are performed to extract relevant matches from over 3.5 M words contained in corpora listed in Table 1.\nThe following sections explain in detail the techniques utilised to find the closest matches to \u201cout of vocabulary\u201d (OOV) tokens.\n4.1.Edit Distance (Step 1)\nThe OOV or \u201cquery\u201d token is compared against the 645,288 words contained in the words.utf-8.txt file. The first set of crude matches is gathered by calculating the Levenshtein distance between the query and the words in the dictionary.\nLevenshtein distance is a string metric for measuring the difference between two sequences. It is defined as the minimum number of single character edits (insertion, deletion, substitution) required to change one word into the other. The phrase edit distance is often used to refer specifically to Levenshtein distance.\nUsing this technique which employs matching based on the textual representation of the characters in the query; the dictionary (words.utf-8.txt) is searched for matches that are within 2 (inclusive) Levenshtein distance of the query. This generates the first set of approximate matches based on the textual similarity to the query. The results are stored in an array. This set generally contains words that may have been misspelt in the query.\n4.2.Phonetic Matching (Step 2)\nPhonetic matching algorithms match two different words with similar pronunciation to the same code. These algorithms compare and index words that are phonetically similar and can be used for spelling correction.\nRefined Soundex algorithm is an improvement to the original Soundex algorithm, in which the letters are divided into more groups (Fig 1.) based on their sound. Also, the length of the result is not truncated, so the code does not have a fixed length. This provides better resolution for phonetic matching as compared to the original Soundex.\nFigure 1\nOrginal Soundex\nb, f, p, v 1\nc, g, j, k, q, s, x, z 2\nd, t 3\nl 4\nm, n 5\nr 6\nRefined Soundex\nb, p 1\nf, v 2\nc, k, s 3\ng, j 4\nq, x, z 5\nd, t 6\nl 7\nm, n 8\nr 9\nRefined Soundex is used to further analyse and phonetically match words gathered in the first set, based on their Levenshtein distance to the query as described in Section 4.1. The words in the array are filtered based on their phonetic similarity to the query as shown in Figure 2 below.\nThis produces an array containing closer matches and is a set of words which are:\ni. Phonetically similar and, ii. Within 2 or less Levenshtein distance to the\n4.3.Peter Norvig\u2019s Algorithm (Step 3)\nPeter Norvig\u2019s Algorithm generates all possible terms with an edit distance of less than or equal to 2 (which includes deletes, transposes, replaces, and inserts) from the query term and searches them in the dictionary (big.txt, see Table1).\nFor a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time. This is much better than the naive approach, but still expensive at search time (114,324 terms for n=9, a=36, d=2) and is language dependent. Because the alphabets are used to generate the terms, and are different in many languages, it could potentially lead to a very large number of search terms. E.g. In Chinese: a=70,000 Unicode Han characters. Never the less, it usually achieves 80-90% accuracy averaging at about 10 words per second.\nFor the purpose of this experiment we apply Peter Norvig Algorithm to find the best match for a given query as shown in Figure 3, below.\nThe result is then compared (Figure 4) with the phonetically matched words derived in Section 4.2 based on the following rules:\na) Peter Norwig\u2019s result takes precedence and is returned as the normalised word for a\nquery, if 0 phonetic matches are found after applying Refined Soundex algorithm (in Section 4.2).\nb) If both Refined Soundex and Peter Norwig algorithm derive the same result, i.e. only 1\nphonetic match is found which, is the same as Peter Norwig\u2019s result, then no further processing is conduced and the result is returned as the normalised version for a query.\nc) If Refined Soundex returns more than 1 phonetic match, then the query is further\nanalysed using 5-Gram Context Maching technique as detailed in Section 4.4.\n4.4. 5-Gram Context Matching\n(Step 5)\nIf there are more than 1 phonetic matches found, in other words if Refined Soundex technique (Section 4.2) returns more than one phonetic match then a 5-Gram Context Matching technique is applied using each phonetic match as the query in the following regular expression:\n{Previous word, Query, Next word}\nThe following rules are applied to assemble the\nregular expression for 5-Gram Matching:\na) If the previous and next words to the query are both in vocabulary, then following\npattern is used: {Previous word, Query, Next word}\nb) If only the previous word is in vocabulary then: {Previous word, Query} is used.\nc) Else if only the next word is in vocabulary then: {Query, Next word} is used\nThe query which returns the maximum number of occurrences in w5_.txt (which consists of over a\nmillion words as 5-Grams) is returned as the normalised version of the query as shown in Figure 5. Here the most common occurrence of the {Previous word, Query, Next word} is returned as the result, where each phonetic match is used as query to find a likely candidate based on its occurrence as defined by the pattern above"}, {"heading": "5. Conclusions", "text": "Normalising tokens with high accuracy can be quite a challenge given the number of possible variations for a given token. This is further compounded by the ever increasing and evolving elisions and acronyms frequently used in social media tools such as Twitter. It is important to take into consideration the various normalisation techniques that are available and to pick the ones that best suit the purpose. A blend of techniques such as edit distance and Soundex or Refined Soundex usually results in better accuracy as compared to their standalone application. Techniques based on context such as Peter Norvig\u2019s algorithm increase the accuracy of normalisation. Similarly, N-Gram matching, although exhaustive, can be optimised to produce accurate results based on the context."}], "references": [{"title": "Using the Web for Language Independent Spellchecking and Autocorrection Google Inc., Pyrmont NSW 2009, Australia", "author": ["G. Ellis"], "venue": null, "citeRegEx": "Ellis,? \\Q2013\\E", "shortCiteRegEx": "Ellis", "year": 2013}], "referenceMentions": [], "year": 2013, "abstractText": "Twitter with over 500 million users globally, generates over 100,000 tweets per minute. The 140 character limit per tweet has, perhaps unintentionally, encourages users to use shorthand notations and to strip spellings to their bare minimum \u201csyllables\u201d or elisions e.g. \u201csrsly\u201d. The analysis of twitter messages which typically contain misspellings, elisions, and grammatical errors, poses a challenge to established Natural Language Processing (NLP) tools which are generally designed with the assumption that the data conforms to the basic grammatical structure commonly used in English language. In order to make sense of Twitter messages it is necessary to first transform them into a canonical form, consistent with the dictionary or grammar. This process, performed at the level of individual tokens (\u201cwords\u201d), is called lexical normalisation. This paper investigates various techniques for lexical normalisation of Tweeter data and presents the findings as the techniques are applied to process raw data from Tweeter.", "creator": "Microsoft\u00ae Word 2010"}}}