{"id": "1409.5616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "A Survey on Soft Subspace Clustering", "abstract": "subspace clustering ( sc ) is a promising solution clustering technology techniques to identify clusters groups based on their associations connected with matching subspaces in high dimensional data spaces. sc databases can mostly be classified into hard subspace clustering ( hsc ). and soft subspace symmetric clustering ( ssc ). while persistent hsc algorithms have allegedly been already extensively been studied unsuccessfully and perhaps well accepted nonetheless by the scientific collaboration community, portable ssc algorithms are relatively new but gaining a more prestigious attention in recent the years typically due to better search adaptability. later in answering the paper, a substantially comprehensive text survey on linking existing ssc layout algorithms and the recent computational development are presented. the ssc layout algorithms are largely classified systematically into literally three main categories, first namely, conventional ssc ( simplified cssc ), membrane independent ssc ( intermediate issc ) and extended ssc ( trans xssc ). externally the genetic characteristics of these algorithms routinely are highlighted annually and as the promising potential future successful development of generalized ssc is currently also somewhat discussed.", "histories": [["v1", "Fri, 19 Sep 2014 12:01:08 GMT  (372kb)", "http://arxiv.org/abs/1409.5616v1", null], ["v2", "Fri, 8 Apr 2016 02:08:55 GMT  (1234kb)", "http://arxiv.org/abs/1409.5616v2", "This paper has been published in Information Sciences Journal in 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhaohong deng", "kup-sze choi", "yizhang jiang", "jun wang", "shitong wang"], "accepted": false, "id": "1409.5616"}, "pdf": {"name": "1409.5616.pdf", "metadata": {"source": "CRF", "title": "A Survey on Soft Subspace Clustering", "authors": ["Zhaohong Deng", "Kup-Sze Choi", "Jun Wang", "Shitong Wang"], "emails": ["zhdeng@ucdavis.edu,", "dzh666828@aliyun.com"], "sections": [{"heading": null, "text": "* Corresponding author: zhdeng@ucdavis.edu, dzh666828@aliyun.com\nbased on their associations with subspaces in high dimensional spaces. SC can be classified into\nhard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have\nbeen extensively studied and well accepted by the scientific community, SSC algorithms are\nrelatively new but gaining more attention in recent years due to better adaptability. In the paper, a\ncomprehensive survey on existing SSC algorithms and the recent development are presented.\nThe SSC algorithms are classified systematically into three main categories, namely,\nconventional SSC (CSSC), independent SSC (ISSC) and extended SSC (XSSC). The\ncharacteristics of these algorithms are highlighted and the potential future development of SSC is\nalso discussed.\nKeywords: Soft subspace clustering; fuzzy weighting; entropy weighting; fuzzy\nC-means/K-means model; mixture model\n1 Introduction\nDespite extensive studies of clustering techniques over the past decades in various application\nareas like statistics, machine learning and database [1-2], the conventional clustering techniques\nfall short when clustering is performed in high dimensional spaces [4-6]. A key challenge to most\nclustering algorithms is that, in many real world problems, data points in different clusters are\noften correlated with some subsets of features, i.e., clusters may exist in different subspaces or a\ncertain subspace of all features. Therefore, for any given pair of data points within the same\ncluster, it is possible that the points are indeed far apart from each other in a few dimensions of\nhigh dimensional space.\nRecent years have witnessed proliferating development of subspace clustering (SC) techniques\nto overcome this challenge. The goal of SC is to locate clusters in different subspaces or a certain\nsubspace of the original data space. The two main schools of SC algorithms are Hard Subspace\nClustering (HSC) and Soft Subspace Clustering (SSC). Research in SC begins with in depth\nstudy of HSC methods for clustering high dimensional data. This category of SC algorithms\nattempts to identify the exact subspaces for different clusters, which can be further divided into\nbottom-up and top-down subspace search methods [4]. Examples of the former include CLIQUE\n[8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC\n[13],  -Clusters [14], and PROCLUS [15]. Other common HSC algorithms also include HARP\n[16] and LDR [17]. A detailed review of HSC algorithms can be found in [4-7].\nWhile the goal of HSC is to identify exact subspaces, SSC algorithms performs clustering in\nhigh dimensional spaces by assigning a weight to each dimension to measure the contribution of\nindividual dimensions to the formation of a particular cluster. SSC can be considered as an\nextension of the conventional feature weighting clustering [18-27]. In this paper, SSC algorithms\nare hierarchically classified into three main categories, namely, (1) conventional SSC (CSSC), (2)\nindependent SSC (ISSC), and extended SSC (ESSC). Here, CSSC refers to conventional feature\nweighting clustering algorithms, i.e., all clusters share the same subspace and a common weight\nvector. On the contrary, the weight vectors in ISSC are different for different clusters. In other\nwords, each cluster has an independent subspace. Thus, ISSC may also be referred to as multiple\nfeatures weighting clustering. XSSC represents a category of algorithms developed by extending\nthe CSSC or ISSC algorithms with the introduction of new mechanisms to enhance clustering\nperformance. The definitions of these three types of SSC are given in Table 1. A detailed review\nof the characteristics of the algorithms will be discussed.\nThe rest of this paper is organized as follows. The classification of the existing SSC\nalgorithms is given in Section 2. The CSSC, ISSC and ESSC algorithms are comprehensively\nreviewed in Sections 3, 4 and 5 respectively. The perspectives and prospects of SSC are\ndiscussed in Section 6 and conclusions are given in Section 7. For clarity, the notations used in\nthis paper are defined in Table 2.\nTable 1 Definitions of three categories of SSC algorithms\nSCC algorithms Descriptions\nConventional SSC (CSSC) algorithms\nClassical feature weighting clustering algorithms with all the clusters sharing the same subspace and a common weight.\nIndependent SSC (ISSC) algorithms\nMultiple feature weighting clustering algorithms with all the clusters having their own weight vectors, i.e., each cluster has an independent subspace, and the weight vectors are controllable by different mechanisms.\nExtended SSC (XSSC) algorithms\nAlgorithms extending the CSSC or ISSC algorithms with new clustering mechanisms for performance enhancement.\n2 Classification of SSC\nAs discussed previously, SSC algorithms can be broadly classified into three main categories,\nCSSC, ISSC and XSSC. Each of these categories can be further divided into subcategories based\non variations in clustering mechanisms, as shown in Table 3. For CSSC, clustering is performed\nby first identifying the subspace with some strategies and then carrying out clustering in the\nobtained subspace to partition the data. This is referred to as separated feature weighting where\ndata partitioning involves two separate processes \u2013 subspace identification and clustering in\nsubspace. On the other hand, clustering can also be conducted with the two processes performed\nsimultaneously, which is thus referred to as coupled feature weighting. For ISSC, algorithms are\ndeveloped based on fuzzy C-means (FCM)/K-means model or mixture model, where fuzzy\nweighting, entropy weighting or other weighting mechanisms are adopted to implement the\nclustering models. Finally, XSSC algorithms can be subdivided into six subcategories depending\non the strategies used to enhance the CSSC and ISSC algorithms, including between-class\nseparation, multi-view learning, evolutionary learning, adoption of new metrics, unbalanced\nclusters, and other approaches like reliability mechanism and those used for clustering\ncategorical dataset. The classification of SSC algorithms is presented in Table 3. These\nalgorithms will be discussed in the sequent sections.\n3 CSSC\nCSSC algorithms can be classified based on the methods of feature weighting approaches\nadopted. They are thus divided into two categories, one adopting separated feature weighting and\nthe other coupled employing feature weighting. For the former, the weights are determined\nbefore clustering is performed; while the weights are learned during the clustering process in the\nlatter.\n3.1 Separated Feature Weighting Algorithms\n(1) OVW-UAT\nThe Optimal Variable Weighting for Ultrametric and Additive Tree (OVW-UAT) is a feature\nweighting strategy developed for hierarchical clustering methods to solve the variable weighting\nproblems [18]. In this approach, two objective functions are developed to determine the weights\nfor trees in ultrametric and additive forms respectively. Once the optimal variable weights are\nobtained, the resulting inter-object dissimilarities can be applied to any of the existing ultrametric\nor additive tree fitting procedures. Since hierarchical clustering methods are computationally\ncomplex, the OVW-UAT approach cannot handle large data sets efficiently. Makarenkov and\nLegendre extended the OVW-UAT approach to optimal variable weighting for the k-means\nclustering [19]. The simulation results showed that the method was effective for identifying\nimportant variables, but still not scalable to large data sets.\n(2) C-K-means\nThe Convex K-means (C-K-means) is a method proposed specifically for variable weighting\nin k-means clustering [20]. This method aims to optimize variable weights in order to achieve the\nbest clustering result by minimizing the generalized Fisher ratio Q \u2013 the ratio of the average\nwithin-cluster distortion to the average between-cluster distortion. To find the minimum Q\nvalue, a set of feasible weight groups are first defined. For each weight group, the k-means\nalgorithm is used to generate a data partition and Q is calculated from the partition. The most\ndesirable cluster is then determined as the partition having the minimum Q value. However,\nthis method of finding optimal weights from a predefined set of variable weights may not\nguarantee that the predefined set of weights would always contain the optimal weights. Besides,\nit is not practical to obtain a predefined set of weights for high dimensional data.\n(3) WFCM\nThe Weighted FCM (WFCM) algorithm is another clustering method grouped under the\ncategory of separated feature weighting [21]. In the algorithm, clustering is performed by\nemploying the weighted Euclidean distance as metric that incorporates feature weights into the\ncommonly used Euclidean distance. The algorithm begins by estimating the weight vector using\nthe objective function below and the gradient descent learning technique,\n ( ) ( ) 1 1,\n2 1 min ( ) (1 ) (1 )\n( 1) 2\nN N ij ij ij ij i j j i E N N               w ww\n( )\n( )\n1\n1 ij ijd      w w , ( ) 2 2 1\n( ) D\nij k ik jk k d w x x \n w\nwhere ( )E w is the function of weighting variable w for obtaining the optimized weights; N\nis the number of samples; ( )ijd w and ( )ij w denote ijd and ij in the original space respectively.\nOnce the weights are determined, WFCM can be implemented by replacing the common\nEuclidean distance in FCM by the weighted Euclidean distance.\n3.2 Coupled Feature Weighting Algorithms\n(1) SYNCLUS\nThe Synthesized Clustering (SYNCLUS) algorithm is developed to deal with variable\nweighting in k-means clustering [22]. The algorithm is divided into two stages. Starting from an\ninitial set of weights, SYNCLUS first employs k-means clustering to partition data into k\nclusters, followed by the estimation of a set of new weights for different features by optimizing a\nweighted mean-square stress-like cost function. The two stages iterate until convergence to an\noptimal set of weights is achieved. The algorithm is computationally intensive and very\ntime-consuming [3], making it not suitable for handling large data sets.\n(2) FWFKM\nThe Feature Weighted Fuzzy K-means (FWFKM) algorithm performs clustering through an\niterative procedure based on the fuzzy k-means algorithm and the supervised ReliefF algorithm\n[23]. With D as the number of features, the FWFKM algorithm begins by setting the weights as\n1/ D and implementing the fuzzy k-means algorithm with the weighted distances to obtain the\ninitial clustering result and label the data. With the labeled data, the supervised ReliefF algorithm\nis then used to assign the new weights for every feature. This procedure is conducted iteratively\nwith the weights updated repeatedly until the final clustering result is achieved.\n(3) W-k-means\nHuang et al. proposed the automated variable weighting in k-means type clustering algorithm\n(W-k-means) by using the following objective function [24],\n2\n1 1 1\nmin ( , , ) ( ) C N D\nW k means ij k jk ik i j k J u w x v       U V w\ns.t.  0,1iju  , 1 1  \nC i iju , Nu N j ij  1 0 , 0 1kw  , and 1 1 D k k w   .\nWith the current partition in the iterative k-means clustering process, the W-k-means algorithm\ncalculates a new weight for each variable, i.e., feature, based on the variance of the within-cluster\ndistances. The new weights are used to decide the cluster memberships of objects in the next\niteration. The optimal weights are found when the algorithm converges, which can then be used\nto identify important features for clustering. The features that may indeed be noise to the\nclustering process can be removed in future analysis. The W-k-means algorithm has received\nincreasing attention, based on which modified algorithms are proposed, such as the fuzzy\nsubspace clustering algorithm [32, 33] to be reviewed in Subsection 4.1.1.\n(4) MWLA\nCheung and Zeng proposed the Maximum Weighted Likelihood (MWL) learning framework\nin the context of Gaussian mixture model to identify the clustering structure and the relevant\nfeatures automatically and simultaneously [25]. The MWL based algorithm (MWLA) is\nperformed by introducing two sets of weight functions \u2013 one to reward the significance of each\ncomponent in the mixture, and the other to discriminate the relevance of each feature of the\nclustering structure.\n(5) FWSA\nTsai and Chiu proposed the Feature Weight Self-Adjustment Algorithm (FWSA) based on the\nk-means clustering model [26]. The algorithm adopts the objective function below,\n2\n1 1 1\nmin ( , ) ( ) C N D\nFWSA ij ik jk ik i j k J u w x v      U V\ns.t.  0,1iju  , 1 1  \nC i iju , Nu N j ij  1 0 , 0 1ikw  , and 1 1 D ik k w   .\nFurthermore, the following sub-optimization function is used to adjust the weights\n2\n1 1\n|| || ( )\nmax ( ) ( )\nC D i ik ok ik i k\nFWSA\nC w v v\nE J\n \n   \nw w\n,\nwhere || ||iC denotes the number of the i th cluster obtained in the current iteration and\n1[ , , ] T o o oDv vv  is the global center of all data objects in the data set. The final clustering\nresults and weights are then obtained iteratively.\n(6) MA-DDC-FW\nFor the problems of unsupervised discrete feature selection/weighting, the Model-based\nApproach for Discrete Data Clustering and Feature Weighting (MA-DDC-FW) [27] is proposed\nthat makes use of a probabilistic approach to assign relevance weights to the discrete features. In\nthe algorithm, the features are considered as random variables modeled by finite discrete\nmixtures. Bayesian and information-theoretic approaches through stochastic complexity are both\nemployed for the learning of the model. The feasibility and merits of MA-DDC-FW are well\ndemonstrated in difficult problems involving clustering and recognition of visual concepts in\nimage data. The algorithm has also achieved success in text clustering.\n4 ISSC\nISSC algorithms are distinct from CSSC in that each cluster has an independent subspace\nassociated with a weight vector. The algorithms can be implemented based on FCM/K-means\nmodel, mixture model and other models, and with the application of different weighting\napproaches such as fuzzy weighting, entropy weighting and other mechanisms.\n4.1 FCM/K-means model based ISSC\n4.1.1 Fuzzy Weighting\nFor ISSC algorithms based on FCM/K-means model and fuzzy weighting mechanism, the parameters ikw  are used for fuzzy weighting with  as the fuzzy indices for weighting, where  can control the distributions of ikw effectively in the clustering procedure. For example, 1ikw D with a very large  .\n(1) AWFCM Keller and Klawonn proposed the Attribute Weighting Fuzzy Clustering (AWFCM) based on\nFCM model with the objective function below [28].\n2\n1 1 1\nmin ( , , ) ( ) C N D\nm AWFC ij ik jk ik\ni j k\nJ u w x v\n  \n  U V W\ns.t. [0,1]iju  , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ik w .\nTo our knowledge, AWFCM is the first fuzzy weighing ISSC algorithm. Based on AWFCM,\nimproved versions of the algorithm have been proposed, such as EFWSSC algorithm [41].\n(2) SCAD Frigui and Nasraoui proposed the Simultaneous Clustering and Attribute Discrimination (SCAD) method [29]. Two versions of SCAD algorithms, SCAD-1 and SCAD-2, were proposed with the objective functions as follows,\n2 2 2 1\n1 1 1 1 1\nmin ( , , ) ( ) C N D C D\nSCAD ij ik jk ik i ik i j k i k J u w x v w           U V W\ns.t.  0,1iju  , 1 1  \nC i iju , 10  ijw , 1 1   D k ikw ,\n2 2\n1 1 1\nmin ( , , ) ( ) C N D\nm SCAD ij ik jk ik\ni j k J u w x v      U V W\ns.t.  0,1iju  , 1 1  \nC i iju , 10  ijw , 1 1   D k ikw ,\nBy comparing the objective functions of AWFCM and SCAD-2, it can be found that they are\nessentially fuzzy weighting SSC algorithms of the same kind.\n(3) AWA The Attribute Weighting Algorithm (AWA) developed by Chan et al. [30] employed an objective function similar to that of the W-k-means algorithm [24]. However, the sharing weights\nkw  of the kth feature of all the clusters [24] were replaced by the weights ikw  of the k-th feature of each cluster. The objective function is expressed as,\n2\n1 1 1\nmin ( , , ) ( ) C N D\nAWA ij ik jk ik i j k\nJ u w x v\n  \n  U V W\ns.t. }1,0{iju , 1 1  \nC i iju , 10  ijw , 1 1   D k ikw .\nIn fact, it is also clear from the objective function of AWA and that of AWFCM as discussed\nabove [28] that AWA is a hard clustering version of the AWFCM algorithm.\n(4) FWKM A weakness of AWA is that when some of the attributes have zero standard deviation, the learning rules derived by the algorithm fail to work. The Fuzzy Weighting K-Means (FWKM) algorithm [31] is proposed to overcome this problem. The objective function of FWKM is as follows,\n2\n1 1 1\nmin ( , , ) ( ) C N D\nFWKM ij ik jk ik i j k J u w x v          U V W\nDN\nox N j D k kjk\n\n  \n  1 1\n2)(\n , Nxo N\nj jkk  1\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ik w .\n(5) FSC Gan et al. proposed the Fuzzy Subspace Clustering (FSC) algorithm [32, 33] using an objective similar to that of the FWKM algorithm [31] discussed previously. A detailed analysis of the properties of FSC can be found in [33].\n2 0\n1 1 1 1 1\nmin ( , , ) ( ) C N D C D\nFSC ij ik jk ik ik i j k i k J u w x v w           U V W\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ikw .\nIt is clear from these five fuzzy weighting ISSC algorithms that the fuzzy weighting ikw can\nbe regarded as an extension of the classical weighting ikw . The fuzzy index  is usually set such that 1 in order to ensure the convergence of the algorithms [28-33]. When 1 , the fuzzy weighting ikw is reduced to the classical weighting ikw . Compared with classical weighting, algorithms adopting fuzzy weighting have better elasticity and, meanwhile, can be\neasily analyzed using fuzzy optimization techniques.\n4.1.2 Entropy Weighting\nAnother category of ISSC algorithms has been developed based on the FCM/K-means model\nand entropy weighting. Unlike the fuzzy weighting based algorithms described in the previous\nsubsection, the weighting in this category of algorithms are controllable by entropy. Two\nrepresentative entropy weighting FCM/K-means model based ISSC algorithms are described.\n(1) EWKM\nJing et al. proposed the Entropy Weighting K-Means (EWKM) clustering algorithm [34]\nusing the following objective function,\n2\n1 1 1 1 1\nmin ( , , ) ( ) ln C N D C D\nEWKM ij ik jk ik ik ik i j k i k J u w x v w w          U V W\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ikw ,\nwhere the second term in the objective function is the Shannon entropy and  is used to\nbalance its influence on the clustering procedure. By introducing the entropy term, the\nobtained weight can be effectively controllable by entropy. For example, if  is very large,\nthe features will be apt to be assigned with equal values. EWKM has become a benchmarking\nISSC algorithm and is further extended to develop various XSSC algorithms, such as ESSC\nalgorithm [34].\n(2) LAC\nThe weighting in the Local Adaptive Clustering (LAC) algorithm [35] is also controllable by\nentropy. The objective function can be expressed as\n1 1 1 1\nmin ( , , ) ln C D C D\nLAC ik ik ik ik i k i k J w X w w       U V W ,\n        \nN j ij N j ikjkijik uvxuX 11 2)(\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ik w .\nThe objective functions of EWKM and LAC are indeed very similar. The only difference is that\nthe effect of cluster size is considered in LAC but disregarded in EWKM.\nIn addition to the entropy weighting LAC in [35], Domeniconi proposed an alternative LAC\nalgorithm with a different objective function [36] as follows\n  1 1\nmax ( , , ) exp C D\nO LAC ik ik i k J w h X    U V W ,\n        \nN j ij N j ikjkijik uvxuX 11 2)(\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1 2   D k ikw .\nNote that the objective function is maximized to solve the solution variables, which is distinct\nfrom the entropy-based LAC approach [35] discussed earlier.\n4.2 Mixture Model based ISSC\nIn addition to classical FCM/K-means model, probability mixture model is also adopted to\ndevelop ISSC algorithms. In this subsection, two representative algorithms are introduced.\n(1) FPC/MPC\nChen et al. proposed the Fuzzy/Model Projective Clustering (FPC/MPC) algorithm based on\nthe mixture model [37, 38]. In FPC/MPC, each projected dimension is assumed to fit in the\nGaussian mixture distribution as follows,\n1 ( ; ) ( | , )\nC\nk k i k ik ii F x G x v    \n   2\n2\n1 ( | , ) exp\n22\nTik k ik i i ik\nik\nw G x v \n\n      \n  x v e\nwith 1\n1 D\nik k w   , where ,ik iv  denote the means and variances respectively. Furthermore, the\nfollowing Kullback-Leibler (KL) divergence is minimized for parameter learning\n( ; ) ( ) ( ; ) ln\n( ; )\nk j\nj k j j\nk j\nF x R F x dx\nF x\n        .\nBased on the above criterion, the objective function for clustering is finally given by\n  2\n/ 2 1 1 1 1 1 1 1 ( , , , , , ) ( ) ln ln 2 2\nC N D N C N Cij ik T i\nFPC MPC j i ik ij ij iji i j k j i ji i\nu w J D u D u u \n              U V W \u03b1 \u03c3 e x v e\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , 1 1 D ik k w   , 0 1i  and 1 1 C i i    .\nWhen only axis-aligned subspace is considered, the above objective function can be reduced to\n2 / 2 1\n1 1 1 1 1 1\n( , , , , ) ( ) ln ln 2 2\nC N D N C N Cij ik k\nFPC MPC jk ik ij ij iji i j k j i ji i\nu w J x v D u D u u \n             U V W \u03b1 \u03c3 .\nIt can be seen that the FPC/MPC algorithm also involves the entropy term for the clustering\nprocedure, however, the entropy term here is used to control the partition iju instead of the\nfeature weight ijw .\n(2) EWMM\nPeng and Zhang proposed the entropy weighting mixture model (EWMM) algorithm [39]. By\nusing an approach similar to that in FPC/MPC, the objective function of EWMM can be\nformulated as follows,\n2\n2 2 1 1 1\n1 ( , , , , ) ln ( ) ln ln\n2 2\nC N D ik ik ij i jk ik ij i j k i i w w J u x v u N                       U V W \u03b1 \u03c3\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , 1 1 D ik k w   , 0 1i  and 1 1 C i i    .\nFurthermore, with the extra control of weighting by using entropy, the final objective function\nfor clustering is given by\n2\n2 2 1 1 1 1 1\n1 ( , , , , ) ln ( ) ln ln ln\n2 2\nC N D C D ik ik\nEWMM ij i jk ik ij i ik ik i j k i ki i\nw w J u x v u w w\nN  \n     \n            \n       U V W \u03b1 \u03c3 .\nNote that although both mixture model based algorithms, i.e., Fuzzy/Model algorithm and\nEWMM algorithm, contain an entropy term, the purposes are very different. The former uses it\nto control the partition iju while the latter use it for controlling the weight ijw .\nWhen compared with the classical FCM/K-means model based ISSC algorithms, the two\nmixture model based algorithms are expected to possess stronger adaptation abilities to the data\ndistributions, as evident from that promising clustering results achieved on high dimensional data\n[39]. However, the mixture model based ISSC algorithms are much more complicated than the\nFCM/K-means based algorithms and therefore not very popular in the field of SSC.\n4.3 Other Model based ISSC\nIn addition to the FCM/K-Means and mixture model based ISSC algorithms, other ISSC algorithms have also been proposed. Among them, the Clustering Objects on Subsets of Attributes (COSA) is a representative algorithm which was proposed by Friedman and Meulman [40] with the following objective function.\n2 1 ( ) ( ) 1\n2 2 2 1 1 ( ) ( ) 1 1 1 1\n( , ) ( ln ) ln( )\nln ln( )\nC D i\nCOSA ik jj k ik ik i z j z j i ki\nC D C D C D i i i ik jj k ik ik i k z j z j i i k i ki i i\nJ w d w w D n\nw d w w D n n n\n \n \n    \n        \n      \n \n       \n \n  \n      \nU W\ns.t. 10  ijw , 1\n1 D\nik k w   ,\nwhere i are the cluster weights and in is the number of objects assigned to the ith cluster. COSA is an entropy-based subspace clustering algorithm. As discussed in [34], a shortcoming of COSA is that it may not be scalable to accommodate large data sets.\n5 XSSC\nXSSC algorithms refers to a category of SSC methods developed to improve the performance\nof CSSC and ISSC algorithms by introducing new learning mechanisms to improve the\nclustering performance. In the paper, we divide the XSSC algorithms into six subcategories. The\ntechniques applied are reviewed respectively in this subsection.\n5.1 Between-cluster Separation\nMost SSC algorithms perform clustering by optimizing the within-cluster compactness\nwithout making use of the between-cluster information. Recently, algorithms integrating\nbetween-cluster separation with within-cluster compactness have been developed. Three\nrepresentative algorithms are reviewed.\n(1) ESSC\nBased on the EWKM method [34], the Enhanced SSC (ESSC) algorithm was proposed to\nimprove clustering performance by minimizing the within-cluster compactness in the weighting\nsubspace and maximizing the between-cluster separation simultaneously. The objective function\nof the ESSC algorithm is as follows,\n2 2 0\n1 1 1 1 1 1 1 1\n( , , ) ( ) ln ( ) ( ) C N D C D C N D\nm m ESSC ij ik jk ik ik ik ij ik ik k\ni j k i k i j k J u w x v w w u w v v                   U V W\ns.t. 10  iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ikw .\nThe objective function contains three terms \u2013 the weighting within-cluster compactness, the entropy of weights and the weighting between-cluster separation. The first and second terms are directly inherited from the objective function of EWKM subspace clustering except that the\nk-means model is replaced by FCM model. In this objective function, the parameters  ( 0  )\nand  ( 0  ) are used to control the influences of entropy and the weighting between-cluster\nseparation respectively. It is noteworthy to point out that when 1m and 0 , the ESSC\nalgorithm is reduced to the EWKM algorithm [34]. Thus, the EWKM algorithm can be regarded as a special case of the ESSC algorithm.\n(2) EFWSSC Using a similar strategy, Guan improved the FSC algorithm [32, 33] by making use of between-cluster separation and proposed the Enhanced Fuzzy Weighting Soft Subspace Clustering (EFWSSC) algorithm [42]. The objective function of the algorithm is\n2 2 0\n1 1 1 1 1 1 1 1\n( , , ) ( ) ( ) ( ) C N D C D C N D\nm m EFWSSC ij ik jk ik ik ij ik ik k\ni j k i k i j k J u w x v w u w v v                     U V W\ns.t. 10  iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ikw .\nSimilarly, when 1m  and 0  , the EFWSSC algorithm degenerates into the FSC\nalgorithm, i.e., the FSC algorithm is as a special case of the EFWSSC algorithm.\n(3) IEWKM By integrating the between-cluster separation, Li et al. proposed the Improved Entropy\nWeighting K-means (IEWKM) with the following objective function [43],\n2\n1 1 1\n2 1 1 0\n1 1\n( )\n( , , ) ln\n( )\nC N D ij ik jk ik C D i j k\nIEWKM ik ikC D i k\nik ik k i k\nu w x v\nJ w w\nw v v\n  \n \n \n\n \n\n  \n U V W (15)\nAlthough the IEWKM algorithm here and the ESSC algorithm [41] are both based on EWKM and they both employ the between-cluster separation, the former is much more difficult to be optimized. Besides, from the viewpoint of optimization, the learning rules for the cluster centers in the IEWKM algorithm lack rigorous derivation [43].\n5.2 Multi-view Learning\nMulti-view learning is becoming a popular approach in the fields of machine learning for learning on multi-view data. It has been used for SSC recently and two representative multi-view SSC algorithms are introduced as follows.\n(1) TW-k-means Chen et al. proposed the Two-level variable Weighting k-means (TW-k-means) by introducing the view weighting mechanism [44]. It is a multi-view SSC algorithm based on traditional weighting clustering methods. The objective function of TW-k-means is given by\n2 1 2\n1 1 1 1 1 ( , , , ) ( ) ln ln t t\nC N T T T\nEWKM ij t k jk ik k k t t i j t k G t k G t J u w w x v w w w w              U V w w   \ns.t. {0,1}iju  , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1, 1, , t k k G w t T     , 0 1kw  ,\n1\n1 T\nt t w    . In this algorithm, feature weighting is performed in the first level weighting where the importance of features in each view is adjusted. View weighting is then performed in the second level weighting where the influence of each view in the clustering procedure is adjusted.\n(2) FG-k-means Another multi-view SSC algorithm to be introduced is the Feature Groups weighting k-means (FG-k-means) algorithm [45]. It is based on the EWKM algorithm and the corresponding objective function is given by\n2 1 2\n1 1 1 1 1 ( , , , ) ( ) ln ln t t\nC N T T T\nEWKM ij t ik jk ik ik ik t t i j t k G t k G t J u w w x v w w w w              U V W w   \ns.t. {0,1}iju  , 1 1  \nC i iju , Nu N j ij  1 0 , 0 1ikw  , and 1, 1, , t ik k G w t T     , 0 1tw  ,\n1\n1 T\nt t w    . By comparing the FG-k-means and TW-k-means algorithms, it can be seen that former is\nindeed an extension of the latter from CSSC to ISSC.\n5.3 Evolutionary Leaning\nMost of the existing SSC algorithms make use of iterative learning strategy for optimizing the\nobjective functions. However, these algorithms suffer from poor initialization sensitivity and\nlocal optimization of solutions. In order to overcome the deficiencies, evolutionary learning\ntechnique has been introduced to optimize the objective functions of SSC. Several representative\nXSSC algorithms developed based on evolutionary learning are reviewed below.\n(1) Coevolutionary SSC\nGangrski et al. proposed two SSC algorithms based on coevolution learning [46]. The first\nalgorithm was inspired by the Lamarck theory and used the distance-based cost function defined\nin the AWC algorithm [30] as the fitness function. The second algorithm employed a fitness\nfunction based on a new partitioning quality measure. The experimental results in [46]\nhighlighted the benefits of using coevolutionary feature weighting methods to improve the\nknowledge discovery process.\n(2) PSOVW\nLu et al. proposed the Particle Swarm Optimizer for Variable Weighting (PSOVW) algorithm\nby using the particle swarm optimizer as the evolutionary strategy for SSC. The following\nobjective function is employed for variable optimization [47],\n2\n1 1 1\n1\n( , , ) ( ) C N D\nik PSOVW ij jk ikD\ni j k ik\nk\nw J u x v\nw\n\n   \n\n    U V W\ns.t. }1,0{iju , 1 1  \nC i iju , 10  ijw .\nBy transforming the original constrained variable weighting problem into a problem with\nbound constraints, i.e., using a normalized representation of variable weights, the particle swarm\noptimizer can easily minimize the objective function to search for the global optima for the\nvariable weighting problem. Experimental results show that the PSOVW algorithm greatly\nimproves the clustering performance and the clustering results are much less dependent on the\ninitial cluster centers.\n(3) MOEA-SSC The Multi-Objective Evolutionary Approach for SSC (MOEA-SSC) is a multi-objective evolutionary algorithm developed by Xia et al. that makes use of new encoding and operators [48]. Here, two objective functions are adopted for SSC, i.e.,\n2\n1 1 1\n( , , ) ( ) C N D\nm IN ij ik jk ik\ni j k J u w x v      U V W\n21 1 1\n1\n( , , ) ln\n( )\nC C D i\nAdd ik ikC i i k\nik i k i\nA J w w\nv v   \n      U V W\nwith 1\n1\n1 if >1 D ,\n0 Otherwise\nD\nik k ikk\ni kD\nk k\nw w\nA\n\n\n\n\n\n   \n\n\n\ns.t. 10  iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ikw ,\nwhere INJ denotes the within-cluster dispersion, and AddJ contains both the information of\nthe negative weight entropy and the separation between clusters. Similar to other evolutionary\nlearning based SSC algorithms, MOEA-SSC algorithm is less dependent on the initial cluster\ncenters.\n5.4 Adaptive Metric\nIn SSC, Euclidean distance is commonly used as the metric for defining the distance between\nthe data points and the cluster centers in each dimension. In order to improve clustering\nperformance, a number of modified metrics have been proposed. Two of the metrics are\nintroduced below, along with the corresponding XSSC algorithms thus developed.\n(1) Kernel Metric\nShen et al. develop an improved version of fuzzy weighting SSC algorithm, namely, the\nWeighted Fuzzy Kernel-Clustering Algorithm (WFKCA), by proposing a new metric in the\nMercer kernel space, i.e., Mercer kernel distance. This metric is used instead of the Euclidean\ndistance in the original space for each dimension. The objective function of WFKCA can be\nformulated as follows,\n2\n1 1 1\n( , , ) ( ) ( ) C N D\nm WFKCA ij ik jk ik\ni j k J u w x v        U V W\n2 ( ) ( ) ( , ) ( , ) 2 ( , ) jk ik jk jk jk jk jk jkx v k x x k x x k x x    \nwhere ( )jkx is the mapped feature vector of feature jkx and ( , )jk jkk x x . With Mercer kernel\ndistance as metric, WFKCA is able to improve the clustering performance and adaptive abilities\nof the algorithm when compared with the traditional Euclidean distance based algorithms.\n(2) Minkowski Metric\nAmorim and Mirkin also attempted to improve fuzzy weighting SSC algorithms by employing\nan alternative metric, where the Minkowski metric was used to replace Euclidean distance to\ndevelop the Minkowski metric Weighted K-Means (MWK-Means) algorithm. The objective\nfunction of MWK-Means algorithm is as follows,\n1 1 1\n( , , ) C N D\np\nMWK Means ij ik jk ik i j k J u w x v      U V W ,\nwhere the traditional Euclidean distance is replaced by the Minkowski metric with p as the\nparameter. By comparing MWK-Means with AWA [30], we can easily see that when 2p  ,\nAWA is indeed a special case of MWK-Means.\n5.5 Imbalanced Clusters\nFor data with imbalanced cluster, a XSSC algorithm, called the Weighted LAC (WLAC), is\nproposed based on elite selection of weighted clusters [51]. The objective function of WLAC can\nbe expressed as follows,\n1 2 1 1 1 1 1\n( , , , ) ln ln C D C D C\nWLAC i ik ik ik ik i i i k i k i J d w X w w d d            U V W d ,\n        \nN j ij N j ikjkijik uvxuX 11 2)(\ns.t. }1,0{iju , 1 1  \nC i iju , Nu N j ij  1 0 , 10  ijw , and 1 1   D k ik w ,\n0 1id  , 1\n1 C\ni i d   ,\nwhere id is the co-efficient representing the diameter of the ith cluster and is used to balance the influence of imbalanced clusters. In particular, strategies such as ensemble learning have also\nbeen studied to reduce the influence of the parameters 1 and 2 in the WLAC algorithm.\n5.6 Other XSSC Algorithms\nIn addition to the five subcategories of XSSC algorithms discussed above, many other XSSC\nalgorithms have also been reported recently [52-59] and are described briefly as follows.\n(1) Category Data While the IEWKM algorithm is first proposed for category and numeric mixture data [52], algorithms developed specifically for this type of data have also been made available. For example, a modified version of EWKM [53] was proposed by Ahmad and Dey for handling category and numeric mixture data. Bai et al. also proposed a modified SSC algorithm based on AWA and the use of an improved metric for category data [54].\n(2) Ensemble Learning Ensemble learning is a useful technique to enhance clustering performance by fusing different clustering results on the same data set. Domeniconi and Al-razgan proposed a graph-partitioning-based clustering ensemble approach called Weighted Similarity Partitioning Algorithm (WSPA) to overcome the problem of parameter sensitivity of LAC algorithm [55]. The WSPA is able to combine multiple clustering results obtained from different runs of the LAC SSC algorithm.\nBesides, Gullo et al. proposed the Projective Clustering Ensembles (PCE) algorithms for ensemble learning of soft clustering [56]. The PCE algorithm was developed with two different formulations, namely, single-objective PCE and two-objective PCE. The former was implemented as an EM-like algorithm and called EM-PCE. The latter employed techniques similar to those in the realm of multi-objective evolutionary algorithms, and thus known as MOEA-PCE algorithm. Experimental evaluation shows that MOEA-PCE generally produces\nhigher quality projective consensus clustering results, while it is not as efficient as the EM-PCE algorithm.\n(3) Data Reliability Boongoen et al. proposed a novel approach to SSC based on the measure of data reliability, named as reliability-based SSC algorithms [57]. This approach is advantageous in that it is applicable to various clustering algorithms, while existing wrappers are only suitable to FCM/K-means model and/or mixture model. Research has been conducted to make the reliability-based SSC algorithms more efficient and feasible for dealing with large data sets. Evaluation on gene expression data shows that the algorithms can improve their corresponding baseline techniques and outperform important soft and crisp subspace clustering methods.\n6 Future Development\nFollowing a comprehensive review of various SSC methods, the potential future development of SSC is discussed in this section. In particular, the discussions will be made from the perspectives of new SSC algorithms based on transfer learning and multi-view learning.\n6.1 Transfer Learning\nThe effectiveness of traditional clustering algorithms depends on the sufficiency of data and information. Most of the algorithms fail when the data available is insufficient for performing the clustering tasks, which is not uncommon in real-world applications. This presents a major challenge for traditional clustering algorithms. A promising approach to deal with the issue is to capitalize transfer learning techniques to improve the performance of the clustering algorithms. Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.\nThe conceptual diagram in Fig. 1 illustrates the improvement in clustering performance by exploiting transfer learning. The inset in the left shows that the data in the target domain, delineating the situation where it is difficult to obtain satisfactory clustering result with the data using traditional no-transfer learning based clustering algorithms. However, if a reference scene,\ni.e., the source domain, is available and the information is taken into consideration (right of Fig. 1), more promising clustering result can be obtained. For example, if it known that there are two related domains and the number of clusters in these two domains is expected to be the same, then the induced knowledge in the source domain, such as the cluster centers, can be employed to guide the clustering process in the target domain. It can be seen from this example that the realization of effective knowledge/information transfer mechanisms from the source domain to the target domain is the key to develop transfer clustering algorithms. This issue requires special attention and should be properly addressed in the development of effective transfer learning strategy for SSC. For example, the knowledge induced in the source domain, such as the cluster centers and/or the subspace of the clustering results, should be used appropriately to boost the clustering performance in the target domain.\nDespite the potential of transfer learning and the extensive study for classification and regression tasks in recent years, the application of transfer learning for clustering remains very scarce, especially for high dimensional data. Therefore, research towards the development of transfer leaning based SSC algorithms is promising. It is expected to yield effective algorithms for high dimensional data clustering analyses where transfer learning is useful to enhance clustering performance.\n6.2 Enhanced Multi-view Leanring\nAnother direction of further development in SSC is to leverage multi-view learning for advanced clustering algorithms. Multi-view data are becoming popular in many real-world modeling tasks, such as multi-view visual recognition. The research presented in [44, 45] indicates that multi-view clustering is a very effective approach to improve clustering\nperformance for multi-view learning scenes.\nHowever, the existing multi-view SSC algorithms are yet to meet the requirement of the real-world data mining tasks since the multi-view learning mechanisms adopted are still very simple and not effective enough for clustering the high dimensional datasets such as gene expression datasets and text dataset. The development of effective multi-view learning mechanisms is thus an important research area and will make significant contribution to multi-view clustering analysis of high dimensional data.\n6.3 Scalable Learning for Large Dataset\nAnother challenge to SSC is the fast and scalable learning abilities for the large datasets. As reviewed for the existing SSC algorithms, most of them are ineffective for clustering the large datasets due to high computational complexity. This problem is hampering further extensive applications of SSC. In order to overcome this challenge, some existing techniques about the scalable and fast learning on large datasets in classical clustering algorithm, can be integrated into the SSC to design the effective algorithms. Related work can be seen in [66]. Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.\n7 Conclusions\nIn this paper, a comprehensive survey of SSC has been presented. A wide variety of existing algorithms are systematically classified into three main categories, i.e., CSSC, ISSC and XSSC. These three categories of SSC algorithms, along with the different subcategories have been reviewed and discussed in detail. We foresee that transfer learning and multi-view learning will play an important role in the development of SCC in the future. A thorough understanding of SSC algorithms and insights into the advancement of SSC can be obtained through this survey paper.\nAcknowledgement\nThis work was supported in part by the National Natural Science Foundation of China (61170122, 61272210), the Ministry of education program for New Century Excellent Talents (NCET-120882), and the General Research Fund of Hong Kong RGC (PolyU 5134/12E)."}], "references": [{"title": "Survey of clustering algorithms,IEEE", "author": ["R. Xu", "D. Wunsch"], "venue": "Trans. Neural Networks", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Data clustering: 50 years beyond K-means, Pattern Recognition Letters", "author": ["A.K. Jain"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Partitive clustering (K-means family), Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["Y. Xiao", "J. Yu"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Subspace clustering for high dimensional data: a review, ACM SIGKDD Explorations Newsletter", "author": ["L. Parsons", "E. Haque", "H. Liu H"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A survey on enhanced subspace clustering, Data Mining and Knowledge Discovery", "author": ["K. Sim", "V. Gopalkrishnan", "A. Zimek A"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering, ACM Transactions on Knowledge Discovery from Data", "author": ["H.P. Kriegel", "P. Kr\u00f6ger", "A. Zimek"], "venue": "Article No", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Subspace clustering, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["H.P. Kriegel", "P. Kr\u00f6ger", "A. Zimek"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Automatic subspace clustering of high dimensional data for data mining applications", "author": ["R. Agrawal", "J. Gehrke", "D. Gunopulos", "P. Raghavan"], "venue": "Proceedings of ACM SIGMOD Int\u2019l Conf. Management of Data,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Entropy-based subspace clustering for mining numerical data", "author": ["C.H. Cheng", "A.W. Fu", "Y. Zhang"], "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "MAFIA: Efficient and scalable subspace clustering for very large data sets", "author": ["S. Goil", "H. Nagesh", "A. Choudhary"], "venue": "Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Finding generalized projected clusters in high dimensional spaces", "author": ["C.C. Aggarwal", "P.S. Yu"], "venue": "in: Proc. ACM SIGMOD Int\u2019l Conf. Management of Data,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "FINDIT: a fast and intelligent subspace clustering algorithm using dimension voting, Information and Software Technology", "author": ["K.G. Woo", "J.H. Lee", "M.H. Kim"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A Monte Carlo algorithm for fast projective clustering", "author": ["C.M. Procopiuc", "M. Jones", "P.K. Agarwal"], "venue": "Proceedings of the 2002 ACM SIGMOD international conference on Management of data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "\u03b4-clusters: Capturing subspace correlation in a large data set, Proceedings", "author": ["J. Yang", "W. Wang", "H. Wang"], "venue": "18th International Conference on Data Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Fast algorithms for projected clustering", "author": ["C.C. Aggarwal", "J.L. Wolf", "P.S. Yu"], "venue": "ACM SIGMOD Record,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Harp: A practical projected clustering algorithm", "author": ["K.Y. Yip", "D.W. Cheung", "M.K. Ng"], "venue": "IEEE Trans. Knowledge and Data Engineering", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Local dimensionality reduction: A new approach to indexing high dimensional spaces", "author": ["K. Chakrabarti", "S. Mehrotra"], "venue": "Proceedings of the 26th International Conference on Very Large Data Bases", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Optimal variable weighting for ultrametric and additive tree clustering, Quality and Quantity", "author": ["G. De Soete"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "Optimal variable weighting for ultrametric and additive trees and K-means partitioning: Methods and software, Journal of Classification", "author": ["V. Makarenkov", "P. Legendre"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Feature weighting in k-means clustering, Machine learning", "author": ["D.S. Modha", "W.S. Spangler"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Improving fuzzy c-means clustering based on feature-weight learning, Pattern Recognition Letters", "author": ["X. Wang", "Y. Wang", "L. Wang"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Synthesized clustering: A method for amalgamating alternative clustering bases with differential weighting of variables, Psychometrika", "author": ["W.S. DeSarbo", "J.D. Carroll", "L.A. Clark"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1984}, {"title": "A new feature weighted fuzzy clustering algorithm, In: Rough Sets, Fuzzy Sets, Data Mining, and Granular Computing", "author": ["J. Li", "X. Gao", "L. Jiao"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Automated variable weighting in k-means type clustering,IEEE", "author": ["J.Z. Huang", "M.K Ng", "H. Rong"], "venue": "Trans. Pattern Analysis and Machine Intelligence", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "A maximum weighted likelihood approach to simultaneous model selection and feature weighting in Gaussian mixture", "author": ["Y. Cheung", "H. Zeng"], "venue": "Artificial Neural Networks\u2013ICANN", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Developing a feature weight self-adjustment mechanism for a K-means clustering algorithm, Computational statistics & data analysis", "author": ["C.Y. Tsai", "C.C. Chiu"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A model-based approach for discrete data clustering and feature weighting using MAP and stochastic complexity,IEEE", "author": ["N. Bouguila"], "venue": "Trans. Knowledge and Data Engineering", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Fuzzy clustering with weighting of data variables, International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems", "author": ["A. Keller", "F. Klawonn"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Unsupervised learning of prototypes and attribute weights, Pattern recognition", "author": ["H. Frigui", "O. Nasraoui"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "An optimization algorithm for clustering using weighted dissimilarity measures, Pattern recognition", "author": ["E.Y. Chan", "W.K. Ching", "M.K. Ng"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Subspace clustering of text documents with feature weighting k-means algorithm, In: Advances in Knowledge Discovery and Data Mining", "author": ["L. Jing", "M.K. Ng", "J. Xu"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "A fuzzy subspace algorithm for clustering high dimensional data, In: Advanced Data Mining and Applications", "author": ["G. Gan", "J. Wu", "Z. Yang"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "A convergence theorem for the fuzzy subspace clustering (FSC) algorithm, Pattern Recognition", "author": ["G. Gan", "J. Wu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "An entropy weighting k-means algorithm for subspace clustering of high-dimensional sparse data,IEEE", "author": ["L. Jing", "M.K. Ng", "J.Z. Huang"], "venue": "Trans. Knowledge and Data Engineering", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Locally adaptive metrics for clustering high dimensional data, Data Mining and Knowledge Discovery", "author": ["C. Domeniconi", "D. Gunopulos", "S. Ma"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Subspace clustering of high dimensional data", "author": ["C. Domeniconi", "D. Papadopoulos", "D. Gunopulos"], "venue": "SIAM Int. Conf. on Data Mining,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "A probability model for projective clustering on high dimensional data", "author": ["L. Chen", "Q. Jiang", "S. Wang"], "venue": "Eighth IEEE International Conference on Data Mining,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Model-based method for projective clustering", "author": ["L. Chen", "Q. Jiang", "S. Wang"], "venue": "IEEE Trans. Knowledge and Data Engineering", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "An entropy weighting mixture model for subspace clustering of high-dimensional data, Pattern Recognition Letters", "author": ["L. Peng", "J. Zhang"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Meulman, Clustering objects on subsets of attributes (with discussion), Journal of the Royal Statistical Society: Series B (Statistical Methodology", "author": ["J.J.J.H. Friedman"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2004}, {"title": "Enhanced soft subspace clustering integrating within-cluster and  26  between-cluster information, Pattern Recognition", "author": ["Z. Deng", "K.S. Choi", "F.L. Chung"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "The study on enhanced soft subspace clustering techniques", "author": ["Q. Guan"], "venue": "Master Dissertation,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "An improved k-means algorithm for clustering using entropy weighting measures, the 7th World", "author": ["T. Li", "Y. Chen"], "venue": "Congress on Intelligent Control and Automation,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "TW-k-means: Automated Two-level Variable Weighting Clustering Algorithm for Multi-View", "author": ["X. Che", "X. Xu", "J. Huang"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "A feature group weighting method for subspace clustering of high-dimensional data[J", "author": ["X. Chen", "Y. Ye", "X. Xu"], "venue": "Pattern Recognition", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Comparison between two coevolutionary feature weighting algorithms in clustering, Pattern Recognition", "author": ["P. Gan\u00e7arski", "A. Blansche", "A. Wania"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Particle swarm optimizer for variable weighting in clustering high-dimensional data, Machine learning", "author": ["Y. Lu", "S. Wang", "S. Li"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Multiobjective evolutionary algorithm-based soft subspace clustering, 2012", "author": ["L. Zhu", "L. Cao", "J. Yang"], "venue": "IEEE Congress on Evolutionary Computation,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Attribute weighted mercer kernel based fuzzy clustering algorithm for general non-spherical datasets, Soft Computing", "author": ["H. Shen", "J. Yang", "S. Wang"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "Minkowski metric, feature weighting and anomalous cluster initializing in K-Means clustering", "author": ["R. Cordeiro de Amorim", "B. Mirkin"], "venue": "Pattern Recognition,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "A clustering ensemble framework based on elite selection of weighted clusters, Advances in Data Analysis and Classification", "author": ["H. Parvin", "B. Minaei-Bidgoli"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "A weight entropy k-means algorithm for clustering dataset with mixed numeric and categorical data", "author": ["T. Li", "Y. Chen"], "venue": "Fifth International Conference on Fuzzy Systems and Knowledge Discovery,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2008}, {"title": "A K-means type clustering algorithm for subspace clustering of mixed numeric and categorical datasets, Pattern Recognition Letters", "author": ["A. Ahmad", "L. Dey"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "A novel attribute weighting algorithm for clustering high-dimensional categorical data, Pattern Recognition", "author": ["L. Bai", "J. Liang", "C. Dang"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Weighted cluster ensembles: Methods and analysis, ACM Transactions on Knowledge Discovery from Data", "author": ["C. Domeniconi", "M. Al-Razgan"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Projective clustering ensembles, Data Mining and Knowledge Discovery", "author": ["F. Gullo", "C. Domeniconi", "A. Tagarelli"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Extending data reliability measure to a filter approach for soft subspace clustering", "author": ["T. Boongoen", "C. Shang", "N. Iam-On"], "venue": "IEEE Trans. Systems, Man, and Cybernetics, Part B: Cybernetics", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Soft subspace clustering with an improved feature weight self-adjustment mechanism, International Journal of Machine Learning and Cybernetics", "author": ["G. Guo", "S. Chen", "L. Chen"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Double indices induced FCM clustering and its integration with fuzzy subspace clustering,2012", "author": ["J Wang", "S. Wang S", "Z. Deng"], "venue": "IEEE International Conference on Fuzzy Systems,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowledge and Data Engineering", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Transfer latent variable model based on divergence analysis, Pattern Recognition", "author": ["X. Gao", "X. Wang", "X. Li"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2011}, {"title": "Robust visual domain adaptation with low-rank reconstruction,2012", "author": ["I.H. Jhuo", "D. Liu", "D.T. Lee"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2012}, {"title": "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach, 2012", "author": ["L. Duan", "D. Xu", "S.F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approac", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2011}, {"title": "Fuzzy partition based soft subspace clustering and its applications in high dimensional data", "author": ["J. Wang", "S.T. Wang", "F.L. Chung", "Z.H. Deng"], "venue": "Information Sciences,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "Fuzzy c-means for very large data,IEEE", "author": ["T.C. Havens", "J.C. Bezdek", "C. Leckie", "L.O. Hall", "M. Palaniswami"], "venue": "Trans. Fuzzy Systems", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "Core vector machines: fast SVM training on large data sets", "author": ["I.W. Tsang", "J.T. Kwo", "P.M. Cheung"], "venue": "Journal of Machine Learning Research", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Large-scale maximum margin discriminant analysis using core vector machines", "author": ["I.W. Tsang", "A. Kocsor", "J.T. Kwok"], "venue": "IEEE Trans. Neural Networks", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2008}, {"title": "FRSDE: fast reduced set density estimator using minimal enclosing ball approximation, Pattern Recognition", "author": ["Z.H. Deng", "F.L. Chung", "S.T. Wang"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2008}, {"title": "From minimum enclosing ball to fast fuzzy inference system training on large datasets", "author": ["F.L. Chung", "Z.H. Deng", "S.T. Wang"], "venue": "IEEE Trans. Fuzzy Systems", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2009}, {"title": "Scalable TSK fuzzy modeling for very large datasets using minimal enclosing ball approximation", "author": ["Z.H. Deng", "K.S.Choi", "F.L. Chung", "S.T. Wang"], "venue": "IEEE. Trans. Fuzzy systems", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 149, "endOffset": 154}, {"referenceID": 1, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 149, "endOffset": 154}, {"referenceID": 3, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 262, "endOffset": 267}, {"referenceID": 4, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 262, "endOffset": 267}, {"referenceID": 5, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 262, "endOffset": 267}, {"referenceID": 3, "context": "This category of SC algorithms attempts to identify the exact subspaces for different clusters, which can be further divided into bottom-up and top-down subspace search methods [4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 7, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "Other common HSC algorithms also include HARP [16] and LDR [17].", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "Other common HSC algorithms also include HARP [16] and LDR [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 4, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 5, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 6, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 17, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 18, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 19, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 20, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 21, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 22, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 23, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 24, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 25, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 26, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 17, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 21, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 22, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 23, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 24, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 25, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 26, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 27, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 28, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 29, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 30, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 31, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 32, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 33, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 24, "endOffset": 28}, {"referenceID": 34, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 30, "endOffset": 34}, {"referenceID": 35, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 40, "endOffset": 44}, {"referenceID": 37, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 45, "endOffset": 49}, {"referenceID": 38, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 50, "endOffset": 54}, {"referenceID": 39, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 55, "endOffset": 59}, {"referenceID": 40, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 0, "endOffset": 7}, {"referenceID": 41, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 0, "endOffset": 7}, {"referenceID": 42, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 0, "endOffset": 7}, {"referenceID": 43, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 8, "endOffset": 12}, {"referenceID": 44, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 14, "endOffset": 18}, {"referenceID": 45, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 19, "endOffset": 26}, {"referenceID": 46, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 19, "endOffset": 26}, {"referenceID": 47, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 19, "endOffset": 26}, {"referenceID": 48, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 27, "endOffset": 31}, {"referenceID": 49, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 33, "endOffset": 37}, {"referenceID": 50, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 38, "endOffset": 42}, {"referenceID": 51, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 52, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 53, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 54, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 55, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 56, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 57, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 58, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 64, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 17, "context": "1 Separated Feature Weighting Algorithms (1) OVW-UAT The Optimal Variable Weighting for Ultrametric and Additive Tree (OVW-UAT) is a feature weighting strategy developed for hierarchical clustering methods to solve the variable weighting problems [18].", "startOffset": 247, "endOffset": 251}, {"referenceID": 18, "context": "Makarenkov and Legendre extended the OVW-UAT approach to optimal variable weighting for the k-means clustering [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "(2) C-K-means The Convex K-means (C-K-means) is a method proposed specifically for variable weighting in k-means clustering [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "(3) WFCM The Weighted FCM (WFCM) algorithm is another clustering method grouped under the category of separated feature weighting [21].", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "2 Coupled Feature Weighting Algorithms (1) SYNCLUS The Synthesized Clustering (SYNCLUS) algorithm is developed to deal with variable weighting in k-means clustering [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "The algorithm is computationally intensive and very time-consuming [3], making it not suitable for handling large data sets.", "startOffset": 67, "endOffset": 70}, {"referenceID": 22, "context": "7 The Feature Weighted Fuzzy K-means (FWFKM) algorithm performs clustering through an iterative procedure based on the fuzzy k-means algorithm and the supervised ReliefF algorithm [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 23, "context": "proposed the automated variable weighting in k-means type clustering algorithm (W-k-means) by using the following objective function [24],", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "The W-k-means algorithm has received increasing attention, based on which modified algorithms are proposed, such as the fuzzy subspace clustering algorithm [32, 33] to be reviewed in Subsection 4.", "startOffset": 156, "endOffset": 164}, {"referenceID": 32, "context": "The W-k-means algorithm has received increasing attention, based on which modified algorithms are proposed, such as the fuzzy subspace clustering algorithm [32, 33] to be reviewed in Subsection 4.", "startOffset": 156, "endOffset": 164}, {"referenceID": 24, "context": "(4) MWLA Cheung and Zeng proposed the Maximum Weighted Likelihood (MWL) learning framework in the context of Gaussian mixture model to identify the clustering structure and the relevant features automatically and simultaneously [25].", "startOffset": 228, "endOffset": 232}, {"referenceID": 25, "context": "8 (5) FWSA Tsai and Chiu proposed the Feature Weight Self-Adjustment Algorithm (FWSA) based on the k-means clustering model [26].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "(6) MA-DDC-FW For the problems of unsupervised discrete feature selection/weighting, the Model-based Approach for Discrete Data Clustering and Feature Weighting (MA-DDC-FW) [27] is proposed that makes use of a probabilistic approach to assign relevance weights to the discrete features.", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "(1) AWFCM Keller and Klawonn proposed the Attribute Weighting Fuzzy Clustering (AWFCM) based on FCM model with the objective function below [28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "[0,1] ij u \uf0ce , 1 1 \uf03d \uf0e5 \uf03d C", "startOffset": 0, "endOffset": 5}, {"referenceID": 40, "context": "Based on AWFCM, improved versions of the algorithm have been proposed, such as EFWSSC algorithm [41].", "startOffset": 96, "endOffset": 100}, {"referenceID": 28, "context": "(2) SCAD Frigui and Nasraoui proposed the Simultaneous Clustering and Attribute Discrimination (SCAD) method [29].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "[30] employed an objective function similar to that of the W-k-means algorithm [24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[30] employed an objective function similar to that of the W-k-means algorithm [24].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "10 k w \uf074 of the kth feature of all the clusters [24] were replaced by the weights ik w \uf074 of the k-th feature of each cluster.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "In fact, it is also clear from the objective function of AWA and that of AWFCM as discussed above [28] that AWA is a hard clustering version of the AWFCM algorithm.", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "The Fuzzy Weighting K-Means (FWKM) algorithm [31] is proposed to overcome this problem.", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "proposed the Fuzzy Subspace Clustering (FSC) algorithm [32, 33] using an objective similar to that of the FWKM algorithm [31] discussed previously.", "startOffset": 55, "endOffset": 63}, {"referenceID": 32, "context": "proposed the Fuzzy Subspace Clustering (FSC) algorithm [32, 33] using an objective similar to that of the FWKM algorithm [31] discussed previously.", "startOffset": 55, "endOffset": 63}, {"referenceID": 30, "context": "proposed the Fuzzy Subspace Clustering (FSC) algorithm [32, 33] using an objective similar to that of the FWKM algorithm [31] discussed previously.", "startOffset": 121, "endOffset": 125}, {"referenceID": 32, "context": "A detailed analysis of the properties of FSC can be found in [33].", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 28, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 29, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 30, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 31, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 32, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 33, "context": "proposed the Entropy Weighting K-Means (EWKM) clustering algorithm [34] using the following objective function,", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "EWKM has become a benchmarking ISSC algorithm and is further extended to develop various XSSC algorithms, such as ESSC algorithm [34].", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "(2) LAC The weighting in the Local Adaptive Clustering (LAC) algorithm [35] is also controllable by entropy.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "In addition to the entropy weighting LAC in [35], Domeniconi proposed an alternative LAC algorithm with a different objective function [36] as follows", "startOffset": 44, "endOffset": 48}, {"referenceID": 35, "context": "In addition to the entropy weighting LAC in [35], Domeniconi proposed an alternative LAC algorithm with a different objective function [36] as follows", "startOffset": 135, "endOffset": 139}, {"referenceID": 34, "context": "Note that the objective function is maximized to solve the solution variables, which is distinct from the entropy-based LAC approach [35] discussed earlier.", "startOffset": 133, "endOffset": 137}, {"referenceID": 36, "context": "proposed the Fuzzy/Model Projective Clustering (FPC/MPC) algorithm based on the mixture model [37, 38].", "startOffset": 94, "endOffset": 102}, {"referenceID": 37, "context": "proposed the Fuzzy/Model Projective Clustering (FPC/MPC) algorithm based on the mixture model [37, 38].", "startOffset": 94, "endOffset": 102}, {"referenceID": 38, "context": "(2) EWMM Peng and Zhang proposed the entropy weighting mixture model (EWMM) algorithm [39].", "startOffset": 86, "endOffset": 90}, {"referenceID": 38, "context": "When compared with the classical FCM/K-means model based ISSC algorithms, the two mixture model based algorithms are expected to possess stronger adaptation abilities to the data distributions, as evident from that promising clustering results achieved on high dimensional data [39].", "startOffset": 278, "endOffset": 282}, {"referenceID": 39, "context": "Among them, the Clustering Objects on Subsets of Attributes (COSA) is a representative algorithm which was proposed by Friedman and Meulman [40] with the following objective function.", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "As discussed in [34], a shortcoming of COSA is that it may not be scalable to accommodate large data sets.", "startOffset": 16, "endOffset": 20}, {"referenceID": 33, "context": "(1) ESSC Based on the EWKM method [34], the Enhanced SSC (ESSC) algorithm was proposed to", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "It is noteworthy to point out that when 1 \uf0ae m and 0 \uf03d \uf068 , the ESSC algorithm is reduced to the EWKM algorithm [34].", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "(2) EFWSSC Using a similar strategy, Guan improved the FSC algorithm [32, 33] by making use of between-cluster separation and proposed the Enhanced Fuzzy Weighting Soft Subspace Clustering (EFWSSC) algorithm [42].", "startOffset": 69, "endOffset": 77}, {"referenceID": 32, "context": "(2) EFWSSC Using a similar strategy, Guan improved the FSC algorithm [32, 33] by making use of between-cluster separation and proposed the Enhanced Fuzzy Weighting Soft Subspace Clustering (EFWSSC) algorithm [42].", "startOffset": 69, "endOffset": 77}, {"referenceID": 41, "context": "(2) EFWSSC Using a similar strategy, Guan improved the FSC algorithm [32, 33] by making use of between-cluster separation and proposed the Enhanced Fuzzy Weighting Soft Subspace Clustering (EFWSSC) algorithm [42].", "startOffset": 208, "endOffset": 212}, {"referenceID": 42, "context": "proposed the Improved Entropy Weighting K-means (IEWKM) with the following objective function [43],", "startOffset": 94, "endOffset": 98}, {"referenceID": 40, "context": "Although the IEWKM algorithm here and the ESSC algorithm [41] are both based on EWKM and they both employ the between-cluster separation, the former is much more difficult to be optimized.", "startOffset": 57, "endOffset": 61}, {"referenceID": 42, "context": "Besides, from the viewpoint of optimization, the learning rules for the cluster centers in the IEWKM algorithm lack rigorous derivation [43].", "startOffset": 136, "endOffset": 140}, {"referenceID": 43, "context": "proposed the Two-level variable Weighting k-means (TW-k-means) by introducing the view weighting mechanism [44].", "startOffset": 107, "endOffset": 111}, {"referenceID": 44, "context": "(2) FG-k-means Another multi-view SSC algorithm to be introduced is the Feature Groups weighting k-means (FG-k-means) algorithm [45].", "startOffset": 128, "endOffset": 132}, {"referenceID": 45, "context": "proposed two SSC algorithms based on coevolution learning [46].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "The first algorithm was inspired by the Lamarck theory and used the distance-based cost function defined in the AWC algorithm [30] as the fitness function.", "startOffset": 126, "endOffset": 130}, {"referenceID": 45, "context": "The experimental results in [46] highlighted the benefits of using coevolutionary feature weighting methods to improve the knowledge discovery process.", "startOffset": 28, "endOffset": 32}, {"referenceID": 46, "context": "The following objective function is employed for variable optimization [47],", "startOffset": 71, "endOffset": 75}, {"referenceID": 47, "context": "that makes use of new encoding and operators [48].", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "By comparing MWK-Means with AWA [30], we can easily see that when 2 p \uf03d ,", "startOffset": 32, "endOffset": 36}, {"referenceID": 50, "context": "5 Imbalanced Clusters For data with imbalanced cluster, a XSSC algorithm, called the Weighted LAC (WLAC), is proposed based on elite selection of weighted clusters [51].", "startOffset": 164, "endOffset": 168}, {"referenceID": 51, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 52, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 53, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 54, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 55, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 56, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 57, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 58, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 51, "context": "(1) Category Data While the IEWKM algorithm is first proposed for category and numeric mixture data [52], algorithms developed specifically for this type of data have also been made available.", "startOffset": 100, "endOffset": 104}, {"referenceID": 52, "context": "For example, a modified version of EWKM [53] was proposed by Ahmad and Dey for handling category and numeric mixture data.", "startOffset": 40, "endOffset": 44}, {"referenceID": 53, "context": "also proposed a modified SSC algorithm based on AWA and the use of an improved metric for category data [54].", "startOffset": 104, "endOffset": 108}, {"referenceID": 54, "context": "Domeniconi and Al-razgan proposed a graph-partitioning-based clustering ensemble approach called Weighted Similarity Partitioning Algorithm (WSPA) to overcome the problem of parameter sensitivity of LAC algorithm [55].", "startOffset": 213, "endOffset": 217}, {"referenceID": 55, "context": "proposed the Projective Clustering Ensembles (PCE) algorithms for ensemble learning of soft clustering [56].", "startOffset": 103, "endOffset": 107}, {"referenceID": 56, "context": "proposed a novel approach to SSC based on the measure of data reliability, named as reliability-based SSC algorithms [57].", "startOffset": 117, "endOffset": 121}, {"referenceID": 59, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 60, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 61, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 62, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 63, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 43, "context": "The research presented in [44, 45] indicates that multi-view clustering is a very effective approach to improve clustering Target domain / Current scene Source domain / Reference scene", "startOffset": 26, "endOffset": 34}, {"referenceID": 44, "context": "The research presented in [44, 45] indicates that multi-view clustering is a very effective approach to improve clustering Target domain / Current scene Source domain / Reference scene", "startOffset": 26, "endOffset": 34}, {"referenceID": 65, "context": "Related work can be seen in [66].", "startOffset": 28, "endOffset": 32}, {"referenceID": 66, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 67, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 68, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 69, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 70, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}], "year": 2014, "abstractText": "Subspace clustering (SC) is a promising clustering technology to identify clusters based on their associations with subspaces in high dimensional spaces. SC can be classified into hard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have been extensively studied and well accepted by the scientific community, SSC algorithms are relatively new but gaining more attention in recent years due to better adaptability. In the paper, a comprehensive survey on existing SSC algorithms and the recent development are presented. The SSC algorithms are classified systematically into three main categories, namely, conventional SSC (CSSC), independent SSC (ISSC) and extended SSC (XSSC). The characteristics of these algorithms are highlighted and the potential future development of SSC is also discussed.", "creator": "\u00fe\u00ff"}}}