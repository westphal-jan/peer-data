{"id": "1409.5705", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "abstract": "multiclass logistic flow regression ( computational mlr ) is traditionally a fundamental integrated machine learning learning diffusion model to usually do efficient multiclass classification. however, it is very challenging to intentionally perform mlr on large scale data instances where the feature distribution dimension calculation is at high, moreover the effective number of classes is large and the number of data samples is remarkably numerous. in this paper, we to build a specific distributed framework to immediately support classical large scale multiclass logistic gradient regression. using applied stochastic gradient descent to optimize mlr, finally we now find that approaching the data gradient matrix is itself computed incorrectly as the outer product of this two vectors. this procedure grants us an improved opportunity to greatly intentionally reduce effective communication cost : only instead of communicating for the gradient matrix information among distant machines, together we can likewise only communicate crossing the two communicating vectors continually and so use programming them efficiently to reconstruct the gradient matrix after initial communication. we design into a sufficient vector reconstruction broadcaster ( svb ) backbone to formally support this unified communication pattern. svb synchronizes the parameter recovery matrix of implementing mlr by broadcasting the sufficient vectors among machines separately and migrates towards gradient variable matrix computation on establishing the random receiver sending side. returning svb basically can only reduce the communication cost upstream from existing quadratic combination to linear array without easily incurring any loss benefit of correctness. we finally evaluate the system on the imagenet dataset and demonstrate the efficiency and concurrency effectiveness of our distributed framework.", "histories": [["v1", "Fri, 19 Sep 2014 15:42:28 GMT  (210kb,D)", "https://arxiv.org/abs/1409.5705v1", null], ["v2", "Mon, 7 Sep 2015 12:14:30 GMT  (7540kb,D)", "http://arxiv.org/abs/1409.5705v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["pengtao xie", "jin kyu kim", "yi zhou", "qirong ho", "abhimanu kumar", "yaoliang yu", "eric xing"], "accepted": false, "id": "1409.5705"}, "pdf": {"name": "1409.5705.pdf", "metadata": {"source": "CRF", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "authors": ["Pengtao Xie", "Jin Kyu Kim", "Yi Zhou", "Qirong Ho", "Abhimanu Kumar", "Yaoliang Yu", "Eric Xing"], "emails": ["jkspruce@gmail.com", "hoqirong@gmail.com", "abhimanyu.kumar@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix W. For example, in MLR, rows of W represent the classification coefficient vectors corresponding to different classes; whereas in SC rows of W correspond to the basis vectors used for reconstructing the observed data. A learning algorithm, such as stochastic gradient descent (SGD), would iteratively compute an update \u2206W from data, to be aggregated with the current version of W. We call such models matrix-parameterized models (MPMs).\nLearning MPMs in large scale ML problems is challenging: ML application scales have risen dramatically, a good example being the ImageNet [12] compendium with millions of images grouped into tens of thousands of classes. To ensure fast running times when scaling up MPMs to such large problems, it is desirable to turn to distributed computation; however, a unique challenge to MPMs is that the parameter matrix grows rapidly with problem size, causing straightforward parallelization strategies to perform less ideally. Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14]. However, this synchronization can quickly become a bottleneck: take MLR for example, in which the parameter matrix W is of size J \u00d7 D, where J is the number of classes and D is the feature dimensionality. In one application of MLR to Wikipedia [26], J = 325k and D > 10, 000, thus W contains several billion entries (tens of GBs of memory). Because typical computer cluster networks can only transfer a few GBs per second at the most, inter-machine synchronization of W can dominate and bottleneck the actual algorithmic computation. In recent years, many distributed frameworks\nar X\niv :1\n40 9.\n57 05\nv2 [\ncs .L\nG ]\n7 S\nep 2\nhave been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22]. When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21]. As the matrices become larger due to increasing problem sizes, so do communication costs and synchronization delays \u2014 hence, reducing such costs is a key priority when using these frameworks.\nIn this paper, we investigate the structure of matrix-parameterized models, in order to design efficient communication strategies that can be realized in distributed ML frameworks. We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.g. it can be written as the outer product of two vectors u and v: 4W = uv>. The vectors u and v are sufficient factors (SF, meaning that they are sufficient to reconstruct the update matrix 4W). A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector. Similarly, when solving an `2 regularized MLR problem using SDCA, the update matrix4W also admits such as a structure, where u is the update vector of a dual variable and v is the feature vector. Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].\nLeveraging this property, we propose a computation model called Sufficient Factor Broadcasting (SFB), and evaluate its effectiveness in a peer-to-peer implementation (while noting that SFB can also be used in other distributed frameworks). SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21]. The basic idea is as follows: since4W can be exactly constructed from the sufficient factors, rather than communicating the full (update) matrix between workers, we can instead broadcast only the sufficient factors and have workers reconstruct the updates. SFB is thus highly communicationefficient; transmission costs are linear in the dimensions of the parameter matrix, and the resulting faster communication greatly reduces waiting time in synchronous systems (e.g. Hadoop and Spark), or improves parameter freshness in (bounded) asynchronous systems (e.g. GraphLab, Petuum-PS and [22]). SFs have been used to speed up some (but not all) network communication in deep learning [7]; our work differs primarily in that we always transmit SFs, never full matrices.\nSFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy. We provide theoretical analysis of SFB under synchronous and bounded-async consistency, and demonstrate that SFB learning of matrix-parametrized models significantly outperforms strategies that communicate the full parameter/update matrix, on a variety of applications including distance metric learning [35], sparse coding [24] and unregularized/`2-regularized multiclass logistic regression. Using our own C++ implementations of each application, our experiments show that, for parameter matrices with 5-10 billion entries, replacing full-matrix communication with SFB improves convergence times by 3-4 fold. Notably, our SFB implementation of `2-MLR is approximately 9 times faster than the Spark v1.3.1 implementation. We expect the performance benefit of SFB (versus full matrix communication) to improve with even larger matrix sizes."}, {"heading": "2 Sufficient Factor Property of Matrix-Parametrized Models", "text": "The core goal of Sufficient Factor Broadcasting (SFB) is to reduce network communication costs for matrix-parametrized models; specifically, those that follow an optimization formulation\n(P) min W 1 N N\u2211 i=1 fi(Wai) + h(W) (1)\nwhere the model is parametrized by a matrix W \u2208 RJ\u00d7D. The loss function fi(\u00b7) is typically defined over a set of training samples {(ai,bi)}Ni=1, with the dependence on bi being suppressed. We allow fi(\u00b7) to be either convex or nonconvex, smooth or nonsmooth (with subgradient everywhere); examples include `2 loss and multiclass logistic loss, amongst others. The regularizer h(W) is\nassumed to admit an efficient proximal operator proxh(\u00b7) [3]. For example, h(\u00b7) could be an indicator function of convex constraints, `1-, `2-, trace-norm, to name a few. The vectors ai and bi can represent observed features, supervised information (e.g., class labels in classification, response values in regression), or even unobserved auxiliary information (such as sparse codes in sparse coding [24]) associated with data sample i. The key property we exploit below ranges from the matrixvector multiplication Wai. This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following:\nDistance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality. The vector ai is the difference of the feature vectors in the ith data pair and fi(\u00b7) can be either a quadratic function or a hinge loss function, depending on the similarity/dissimilarity label bi of the data pair. In both cases, h(\u00b7) can be an `1-, `2-, trace-norm regularizer or simply h(\u00b7) = 0 (no regularization). Sparse coding (SC) [24] learns a dictionary of basis from data, so that the data can be re-represented sparsely (and thus efficiently) in terms of the dictionary. In SC, W is the dictionary matrix, ai are the sparse codes, bi is the input feature vector and fi(\u00b7) is a quadratic function [24]. To prevent the entries in W from becoming too large, each column Wk must satisfy \u2016Wk\u20162 \u2264 1. In this case, h(W) is an indicator function which equals 0 if W satisfies the constraints and equals\u221e otherwise."}, {"heading": "2.1 Optimization via proximal SGD and SDCA", "text": "To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.\nProximal SGD: In proximal SGD, a stochastic estimate of the gradient,4W, is first computed over one data sample (or a mini-batch of samples), in order to update W via W\u2190W\u2212 \u03b74W (where \u03b7 is the learning rate). Following this, the proximal operator prox\u03b7h(\u00b7) is applied to W. Notably, the stochastic gradient4W in (P) can be written as the outer product of two vectors4W = uv>, where u = \u2202f(Wai,bi)\u2202(Wai) , v = ai, according to the chain rule. Later, we will show that this low rank structure of4W can greatly reduce inter-worker communication. Stochastic DCA: SDCA applies to problems (P) where fi(\u00b7) is convex and h(\u00b7) is strongly convex [36] (e.g. when h(\u00b7) contains the squared `2 norm); it solves the dual problem of (P), via stochastic coordinate ascent on the dual variables. More specifically, introducing the dual matrix U = [u1, . . . ,uN ] \u2208 RJ\u00d7N and the data matrix A = [a1, . . . ,aN ] \u2208 RD\u00d7N , we can write the dual problem of (P) as\n(D) min U 1 N N\u2211 i=1 f\u2217i (\u2212ui) + h\u2217( 1NUA >) (2)\nwhere f\u2217i (\u00b7) and h\u2217(\u00b7) are the Fenchel conjugate functions of fi(\u00b7) and h(\u00b7), respectively. The primal-dual matrices W and U are connected by1 W = \u2207h\u2217(Z), where the auxiliary matrix Z := 1 NUA\n>. Algorithmically, we need to update the dual matrix U, the primal matrix W, and the auxiliary matrix Z: every iteration, we pick a random data sample i, and compute the stochastic update4ui by minimizing (D) while holding {uj}j 6=i fixed. The dual variable is updated via ui \u2190 ui \u22124ui, the auxiliary variable via Z\u2190 Z\u22124uia>i , and the primal variable via W\u2190 \u2207h\u2217(Z). Similar to SGD, the update of the SDCA auxiliary variable Z is also the outer product of two vectors: 4ui and ai, which can be exploited to reduce communication cost. Sufficient Factor property in SGD and SDCA: In both SGD and SDCA, the parameter matrix update can be computed as the outer product of two vectors \u2014 we call these sufficient factors (SFs). This property can be leveraged to improve the communication efficiency of distributed ML systems: instead of communicating parameter/update matrices among machines, we can communicate the SFs and reconstruct the update matrices locally at each machine. Because the SFs are much smaller in size, synchronization costs can be dramatically reduced. See ?? below for a detailed analysis.\nLow-rank Extensions: More generally, the update matrix4W may not be exactly rank-1, but still of very low rank. For example, when each machine uses a mini-batch of size K, 4W is of rank at\n1The strong convexity of h is equivalent to the smoothness of the conjugate function h\u2217.\nmost K; in Restricted Boltzmann Machines [30], the update of the weight matrix is computed from four vectors u1,v1,u2,v2 as u1v>1 \u2212 u2v>2 , i.e. rank-2; for the BFGS algorithm [4], the update of the inverse Hessian is computed from two vectors u,v as \u03b1uu> \u2212 \u03b2(uv> + vu>), i.e. rank-3. Even when the update matrix 4W is not genuinely low-rank, to reduce communication cost, it might still make sense to send only a certain low-rank approximation. We intend to investigate these possibilities in future work.\n3 Sufficient Factor Broadcasting\nLeveraging the SF property of the update matrix in problems (P) and (D), we propose a Sufficient Factor Broadcasting (SFB) model of computation, that supports efficient (low-communication) distributed learning of the parameter matrix W. We assume a setting with P workers, each of which holds a data shard and a copy of the parameter matrix2 W. Stochastic updates to W are generated via proximal SGD or SDCA, and communicated between machines to ensure parameter consistency. In proximal SGD, on every iteration, each worker p computes SFs (up,vp), based on one data sample xi = (ai,bi) in the worker\u2019s data shard. The worker then broadcasts\n(up,vp) to all other workers; once all P workers have performed their broadcast (and have thus received all SFs), they re-construct the P update matrices (one per data sample) from the P SFs, and apply them to update their local copy of W. Finally, each worker applies the proximal operator proxh(\u00b7). When using SDCA, the above procedure is instead used to broadcast SFs for the auxiliary matrix Z, which is then used to obtain the primal matrix W = \u2207h\u2217(Z). ?? illustrates SFB operation: 4 workers compute their respective SFs (u1,v1), . . . , (u4,v4), which are then broadcast to the other 3 workers. Each worker p uses all 4 SFs (u1,v1), . . . , (u4,v4) to exactly reconstruct the update matrices 4Wp = upv>p , and update their local copy of the parameter matrix: Wp \u2190 Wp \u2212 \u22114 q=1 uqv > q . While the above description reflects synchronous execution, asynchronous and bounded-asynchronous extensions are also possible (Section 4).\nSFB vs client-server architectures: The SFB peer-to-peer topology can be contrasted with a \u201cfullmatrix\u201d client-server architecture for parameter synchronization, e.g. as used by Project Adam [7] to learn neural networks: there, a centralized server maintains the global parameter matrix, and each client keeps a local copy. Clients compute sufficient factors and send them to the server, which uses the SFs to update the global parameter matrix; the server then sends the full, updated parameter matrix back to clients. Although client-to-server costs are reduced (by sending SFs), server-to-client costs are still expensive because full parameter matrices need to be sent. In contrast, the peer-topeer SFB topology never sends full matrices; only SFs are sent over the network. We also note that under SFB, the update matrices are reconstructed at each of the P machines, rather than once at a central server (for full-matrix architectures). Our experiments show that the time taken for update reconstruction is empirically negligible compared to communication and SF computation.\nMini-batch proximal SGD/SDCA: SFB can also be used in mini-batch proximal SGD/SDCA; every iteration, each worker samples a mini-batch of K data points, and computes K pairs of sufficient factors {(ui,vi)}Ki=1. These K pairs are broadcast to all other workers, which reconstruct the originating worker\u2019s update matrix as4W = 1K \u2211K i=1 uiv T i ."}, {"heading": "4 Sufficient Factor Broadcaster: An Implementation", "text": "In this section, we present Sufficient Factor Broadcaster (SFBcaster) \u2014 an implementation of SFB \u2014 including consistency models, programing interface and implementation details. We stress that SFB does not require a special system; it can be implemented on top of existing distributed frameworks, using any suitable communication topology \u2014 such as star3, ring, tree, fully-connected and\n2For simplicity, we assume each worker has enough memory to hold a full copy of the parameter matrix W. If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.e. out of core operation). We plan to investigate these strategies as future work.\n3For example, each worker sends the SFs to a hub machine, which re-broadcasts them to all other workers.\nHalton-sequence [21]. As future work, we intend to investigate the effect of these topologies on performance."}, {"heading": "4.1 Flexible Consistency Models", "text": "Our SFBcaster implementation supports three consistency models: Bulk Synchronous Parallel (BSP-SFB), Asynchronous Parallel (ASP-SFB), and Stale Synchronous Parallel (SSP-SFB), and we provide theoretical convergence guarantees for BSP-SFB and SSP-SFB in the next section.\nBSP-SFB: Under BSP [11, 23, 38], an end-ofiteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration. BSP is a strong consistency model, that guarantees the same computational outcome (and thus algorithm convergence) each time.\nASP-SFB: BSP can be sensitive to stragglers (slow workers) [15, 31], limiting the distributed system to the speed of the slowest worker.\nThe Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others. ASP is efficient in terms of iteration throughput, but carries the risk that worker parameter copies can end up greatly out of synchronization, which can lead to algorithm divergence [15].\nSSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a boundedasynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s. SSP alleviates the straggler issue while guaranteeing algorithm convergence [15, 31]. Under SSP-SFB, each worker p tracks the number of SF pairs computed by itself, tp, versus the number \u03c4 qp (tp) of SF pairs received from each worker q. If there exists a worker q such that tp\u2212\u03c4 qp (tp) > s (i.e. some worker q is likely more than s iterations behind worker p), then worker p pauses until q is no longer s iterations or more behind. When s = 0, SSP-SFB reduces to BSP-SFB [11, 38], and when s =\u221e, SSP-SFB becomes ASP-SFB."}, {"heading": "4.2 Programming Interface", "text": "The SFBcaster programming interface is simple; users need to provide a SF computation function to specify how to compute the sufficient factors. To send out SF pairs (u,v), the user adds them to a buffer object sv list, via: write u(vec u), write v(vec v), which set i-th SF u or v to vec u or vec v. All SF pairs are sent out at the end of an iteration, which is signaled by commit(). Finally, in order to choose between BSP, ASP and SSP consistency, users simply set staleness to an appropriate value (0 for BSP, \u221e for ASP, all other values for SSP). SFBcaster automatically updates each worker\u2019s local parameter matrix using all SF pairs \u2014 including both locally computed SF pairs added to sv list, as well as SF pairs received from other workers.\nFigure 2 shows SFBcaster pseudocode for multiclass logistic regression. For proximal SGD/SDCA algorithms, SFBcaster requires users to write an additional function, prox(mat), which applies the proximal operator proxh(\u00b7) (or the SDCA dual operator h\u2217(\u00b7)) to the parameter matrix mat. Figure 3 shows the sample code of implementing sparse coding in SFB. D is the feature dimensionality of data and J is the dictionary size. Users need to write a SF computation function to specify how to compute the sufficient factors: for each data sample xi, we first compute its sparse code a based on the dictionary B stored in the parameter matrix sc.para mat. Given a, the sufficient factor u can be computed as Ba \u2212 xi and the sufficient factor v is simply a. In addition, users need to provide a proximal operator function to specify how to project B to the `2 ball constraint set."}, {"heading": "5 Implementation Details", "text": "Figure 4 shows the implementation details on each worker in SFBcaster. Each worker maintains three threads: SF computing thread, parameter update thread and communication thread. Each worker holds a local copy of the parameter matrix and a partition of the training data. It also maintains an input SF queue which stores the sufficient factors computed locally and received remotely and an output SF queue which stores SFs to be sent to other workers. In each iteration, the SF computing thread checks the consistency policy detailed in Section 4 in the main paper. If permitted, this thread randomly chooses a minibatch of samples from the training data, computes the SFs and pushes them to the input and output SF queue. The parameter update thread fetches SFs from the input SF queue and uses them to update the parameter matrix. In proximal-SGD/SDCA, the proximal/dual operator function (provided by the user) is automatically called by this thread as a function pointer. The communication thread receives SFs from other workers and pushes them into the input SF queue and broadcasts SFs in the output SF queue to other workers. One worker is in charge of measuring the objective value. Once the algorithm converges, this worker notifies all other workers to terminate the job. We implemented SFBcaster in C++. OpenMPI was used for communication between workers and OpenMP was used for multicore parallelization within each machine.\nThe decentralized architecture of SFBcaster makes it robust to machine failures. If one worker fails, the rest of workers can continue to compute and broadcast the sufficient factors among themselves. In addition, SFBcaster possesses high elasticity [22]: new workers can be added and existing workers can be taken offline, without restarting the running framework. A thorough study of fault tolerance and elasticity will be left for future work."}, {"heading": "6 Cost Analysis and Theory", "text": "We now examine the costs and convergence behavior of SFB under synchronous and bounded-async (e.g. SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes.\nCost Analysis: Table 5 compares the communications, space and time (to apply updates to W) costs of peer-to-peer SFB, against full matrix synchronization (FMS) under a client-server architecture [7]. For SFB, in each minibatch, every worker broadcasts K SF pairs (u,v) to P \u2212 1 other workers, i.e. O(P 2K(J +D)) values are sent per iteration \u2014 linear in matrix dimensions J,D, and quadratic in P . Because SF pairs cannot be aggregated before transmission, the cost has a dependency on K. In contrast, the communication cost in FMS is O(PJD), linear in P , quadratic in matrix dimensions, and independent of K. For both SFB and FMS, the cost of storing W is O(JD) on every machine. As for the time taken to update W per iteration, FMS costs O(PJD) at the server (to aggregate P client update matrices) and O(PKJD) at the P clients (to aggregate K updates into one update matrix for the server). By comparison, SFB bears a cost of O(P 2KJD) due to the additional overhead of reconstructing each update matrix P times.\nCompared with FMS, SFB achieves communication savings by paying an extra computation cost. In a number of practical scenarios, such a tradeoff is worthwhile. Consider large problem scales where min(J,D) \u2265 10000, and moderate minibatch sizes 1 \u2264 K \u2264 1000 (as studied in this paper); when using a moderate number of machines (around 10-100), the O(KP 2(J +D)) communications cost of SFB is lower than the O(PJD) cost for FMS, and the relative benefit of SFB improves as the dimensions J,D of W grow. As for the time needed to apply updates to W, it turns out that the additional cost of reconstructing each update matrix P times in SFB is negligible in practice \u2014 we have observed in our experiments that the time spent computing SFs, as well as communicating SFs over the network, greatly dominates the cost of reconstructing update matrices using SFs. Overall, the communication savings dominate the added computational overhead, which we validated in experiments (Section 7).\nIn order to make SFB applicable to data centers with 1000s of machines [10, 22], we note that SFB costs can be further reduced via efficient broadcast strategies: e.g. in the Halton-sequence strategy [21], each machine connects with and broadcasts messages to Q = O(log(P )) machines, rather than all P machines. We leave the issue of broadcast strategies as future study.\nConvergence Analysis: We study the convergence of SF minibatch SGD (with extensions to proximal-SGD and SDCA being a topic for future study). We wish to solve the optimization problem minW \u2211M m=1 fm(W), where M is the number of training data minibatches, and fm corresponds to the loss function on the m-th minibatch. Assume the training data minibatches {1, ...,M} are divided into P disjoint subsets {S1, ..., SP } with |Sp| denoting the number of minibatches in Sp. Denote F = \u2211M m=1 fm as the total loss, and for p = 1, . . . , P , Fp := \u2211 j\u2208Sp fj is the loss on Sp (residing on the p-th machine).\nConsider a distributed system with P machines. Each machine p keeps a local variable Wp and the training data in Sp. At each iteration, machine p draws one minibatch Ip uniformly at random from partition Sp, and computes the partial gradient \u2211 j\u2208Ip \u2207fj(Wp). Each machine updates its local variable by accumulating partial updates from all machines. Denote \u03b7c as the learning rate at c-th iteration on every machine. The partial update generated by machine p at its c-th iteration is denoted as Up(Wcp, I c p) = \u2212\u03b7c|Sp| \u2211 j\u2208Icp \u2207fj(Wcp). Note that Icp is random and the factor |Sp| is to restore unbiasedness in expectation. Then the local update rule of machine p is\nWcp = W 0 + \u2211P q=1 \u2211\u03c4qp (c) t=0 Uq(W t q, I t q), 0 \u2264 (c\u2212 1)\u2212 \u03c4 qp (c) \u2264 s, (3)\nwhere W0 is the common initializer for all P machines, and \u03c4 qp (c) is the number of iterations machine q has transmitted to machine pwhen machine p conducts its c-th iteration. Clearly, \u03c4pp (c) = c. Note that we also require \u03c4 qp (c) \u2264 c\u22121, i.e., machine pwill not use any partial updates of machine q that are too fast forward. This is to avoid correlation in the theoretical analysis. Hence, machine p (at its c-th iteration) accumulates updates generated by machine q up to iteration \u03c4 qp (c), which is restricted to be at most s iterations behind. Such bounded-asynchronous communication addresses the slow-worker problem caused by bulk synchronous execution, while ensuring that the updates accumulated by each machine are not too outdated. The following standard assumptions are needed for our theoretical analysis: Assumption 1. (1) For all j, fj is continuously differentiable and F is bounded from below; (2) \u2207F , \u2207Fp are Lipschitz continuous with constants LF and Lp, respectively, and let L = \u2211P p=1 Lp; (3) There exists B, \u03c32 such that for all p and c, we have (almost surely) \u2016Wcp\u2016 \u2264 B and E\u2016 |Sp| \u2211 j\u2208Ip \u2207fj(W)\u2212\u2207Fp(W) \u2016 2 2 \u2264 \u03c32.\nOur analysis is based on the following auxiliary update Wc = W0 + \u2211P q=1 \u2211c\u22121 t=0 Uq(W t q, I t q), (4)\nCompare to the local update (??) on machine p, essentially this auxiliary update accumulates all c\u22121 updates generated by all machines, instead of the \u03c4 qp (c) updates that machine p has access to. We show that all local machine parameter sequences are asymptotically consistent with this auxiliary sequence: Theorem 1. Let {Wcp}, p = 1, . . . , P , and {Wc} be the local sequences and the auxiliary sequence generated by SFB for problem (P) (with h \u2261 0), respectively. Under ?? and set the learning rate \u03b7\u22121c = LF 2 + 2sL+ \u221a c, then we have\n\u2022 lim inf c\u2192\u221e E\u2016\u2207F (Wc)\u2016 = 0, hence there exists a subsequence of \u2207F (Wc) that almost surely vanishes; \u2022 lim c\u2192\u221e\nmaxp \u2016Wc\u2212Wcp\u2016 = 0, i.e. the maximal disagreement between all local sequences and the auxiliary sequence converges to 0 (almost surely); \u2022 There exists a common subsequence of {Wcp} and {Wc} that converges almost surely to a stationary point of F , with the rate min c\u2264C E\u2016 \u2211P p=1\u2207Fp(Wcp)\u201622 \u2264 O ( (L+LF )\u03c3 2Ps logC\u221a C ) .\nIntuitively, Theorem 1 says that, given a properly-chosen learning rate, all local worker parameters {Wcp} eventually converge to stationary points (i.e. local minima) of the objective function F , despite the fact that SF transmission can be delayed by up to s iterations. Thus, SFB learning is robust even under bounded-asynchronous communication (such as SSP). Our analysis differs from [5] in two ways: (1) [5] explicitly maintains a consensus model which would require transmitting the parameter matrix among worker machines \u2014 a communication bottleneck that we were able to avoid; (2) we allow subsampling in each worker machine. Accordingly, our theoretical guarantee is probabilistic, instead of the deterministic one in [5]. Note that our analysis also covers worker parameters {Wcp}, which improves upon analyses that only show convergence of a central server parameter [15, 8, 1]."}, {"heading": "7 Experiments", "text": "We demonstrate how four popular models can be efficiently learnt using SFB: (1) multiclass logistic regression (MLR) and distance metric learning (DML)4 based on SGD; (2) sparse coding (SC) based on proximal SGD; (3) `2 regularized multiclass logistic regression (L2-MLR) based on SDCA. For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models. In FMS, workers send update matrices to the central server, which then sends up-to-date parameter matrices to workers5. Due to data sparsity, both the update matrices and sufficient factors are sparse; we\n4For DML, we use the parametrization proposed in [34], which learns a linear projection matrix L \u2208 Rd\u00d7k, where d is the feature dimension and k is the latent dimension.\n5This has the same communication complexity as [7], which sends SFs from clients to servers, but sends full matrices from servers to clients (which dominates the overall cost).\nuse this fact to reduce communication and computation costs. Our experiments used a 12-machine cluster; each machine has 64 2.1GHz AMD cores, 128G memory, and a 10Gbps network interface.\nDatasets and Experimental Setup We used two datasets for our experiments: (1) ImageNet [12] ILSFRC2012 dataset, which contains 1.2 million images from 1000 categories; the images are represented with LLC features [33], whose dimensionality is 172k. (2) Wikipedia [26] dataset, which contains 2.4 million documents from 325k categories; documents are represented with td-idf, with a dimensionality of 20k. We ran MLR, DML, SC, L2-MLR on the Wikipedia, ImageNet, ImageNet, Wikipedia datasets respectively, and the parameter matrices contained up to 6.5b, 8.6b, 8.6b, 6.5b entries respectively (the largest latent dimension for DML and largest dictionary size for SC were both 50k). The tradeoff parameters in SC and L2-MLR were set to 0.001 and 0.1. We tuned the minibatch size, and found that K = 100 was near-ideal for all experiments. All experiments used the same constant learning rate (tuned in the range [10\u22125, 1]).\nConvergence Speed and Quality Figure 6 shows the time taken to reach a fixed objective value, for different model sizes, using BSP consistency. Figure 7 shows the convergence time versus model size for MLR, DML, SC, L2-MLR, under SSP with staleness=20. SFB converges faster than FMS, as well as Spark v1.3.16. This is because SFB has lower communication costs, hence a greater proportion of running time gets spent on computation rather than network waiting. This is shown in Figure 8, which plots data samples processed per second7 (throughput) and algorithm progress per sample for MLR, under BSP consistency and varying minibatch sizes. The middle graph shows that SFB processes far more samples per second than FMS, while the rightmost graph shows that SFB and FMS produce exactly the same algorithm progress per sample under BSP. For this experiment, minibatch sizes betweenK = 10 and 100 performed the best as indicated by the leftmost graph. We point out that larger model sizes should further improve SFB\u2019s advantage over FMS, because SFB has linear communications cost in the matrix dimensions, whereas FMS has quadratic costs.\nScalability In all experiments that follow, we set the number of (L2)-MLR classes, DML latent dimension, SC dictionary size to 325k, 50k, 50k respectively. Figure 9 shows SFB scalability with varying machines under BSP, for MLR, DML, SC, L2-MLR. Figure 10 shows how SFB scales with machine count, under SSP with staleness=20. In general, we observed close to linear (ideal) speedup, with a slight drop at 12 machines. Future work will focus on reducing peer-to-peer broadcast costs (e.g. via Halton sequences), in order to maintain linear scalability with more machines.\nComputation Time vs Network Waiting Time Figure 11 shows the total computation and network time required for SFB and FMS to converge, across a range of SSP staleness values8 \u2014 in general, higher communication cost and lower staleness induce more network waiting. For all staleness values, SFB requires far less network waiting (because SFs are much smaller than full matrices in FMS). Computation time for SFB is slightly longer than FMS because (1) update matrices must be\n6Spark is about 2x slower than PS [15, 22] based C++ implementation of FMS, due to JVM and RDD overheads.\n7We use samples per second instead of iterations, so different minibatch sizes can be compared. 8The Spark implementation does not easily permit this time breakdown, so we omit it.\nreconstructed on each SFB worker, and (2) SFB requires a few more iterations for convergence, because peer-to-peer communication causes a slightly more parameter inconsistency under staleness. Overall, the SFB reduction in network waiting time remains far greater than the added computation time, and outperforms FMS in total time. For both FMS and SFB, the shortest convergence times are achieved at moderate staleness values, confirming the importance of bounded-asynchronous communication."}, {"heading": "8 Related Works and Discussion", "text": "A number of system and algorithmic solutions have been proposed to reduce communication cost in distributed ML. On the system side, [10] proposed to reduce communication overhead by reducing the frequency of parameter/gradient exchanges between workers and the central server. [22] used filters to select part of \u201cimportant\u201d parameters/updates for transmission to reduce the number of data entries to be communicated. On the algorithm side, [32] and [36] studied the tradeoffs between communication and computation in distributed dual averaging and distributed stochastic dual coordinate ascent respectively. [28] proposed an approximate Newton-type method to achieve communication efficiency in distributed optimization. SFB is orthogonal to these existing approaches and be potentially combined with them to further reduce communication cost.\nPeer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21]. Our SFBcaster system also adopt such an architecture, but with the specific purpose of supporting the SFB computation model, which is not explored by existing peer-to-peer ML frameworks.\nFor very large models, the size of the local parameter matrix W may exceed each machine\u2019s memory capacity \u2014 to address this issue, we would like to investigate partitioning W over a small number of nearby machines, or using out-of-core (disk-based) storage to hold W in future work.\nFinally, a promising extension to the SF idea is to re-parameterize the model W completely in terms of SFs, rather than just the updates. For example, if we initialize the parameter matrix W to be of low rank R, i.e., W0 = \u2211R j=1 ujv > j , after I iterations (updates), W I = \u2211R+I j=1 ujv > j . Leveraging this fact, for SGD without proximal operation and SDCA where h(\u00b7) is a `2 regularizer, we can re-parametrize WI using a set of SFs {(uj ,vj)}R+Ij=1 , rather than maintaining W explicitly. This re-parametrization can possibly reduce both computation and storage cost, which we will investigate in the future."}, {"heading": "A Proof of Convergence", "text": "Proof of Theorem 1:\nProof. Let Fc := \u03c3{I\u03c4p : p = 1, . . . , P, \u03c4 = 1, . . . , c} be the filtration generated by the random samplings I\u03c4p up to iteration counter c, i.e., the information up to iteration c. Note that for all p and c, Wcp and W c are Fc\u22121 measurable (since \u03c4 qp (c) \u2264 c \u2212 1 by assumption), and Icp is independent\nof Fc\u22121. Recall that the partial update generated by machine p at its c-th iteration is Up(W c p, I c p) = \u2212\u03b7c|Sp| \u2211 j\u2208Icp \u2207fj(Wcp)\nThen it holds that Up(W c p) = E[Up(Wcp, Icp)|Fc\u22121] = \u2212\u03b7c\u2207Fp(Wcp). (Note that we have suppressed the dependence of Up on the iteration counter c.)\nThen, we have\nE [ P\u2211 p=1 Up(W c p, I c p) | Fc\u22121 ] = P\u2211 p=1 E[Up(Wcp, Icp) | Fc\u22121] = P\u2211 p=1 Up(W c p). (5)\nSimilarly we have\nE [\u2225\u2225 P\u2211 p=1 Up(W c p, I c p) \u2225\u22252 2 | Fc\u22121 ] = P\u2211 p,q=1 E[\u3008Up(Wcp, Icp), Uq(Wcq, Icq )\u3009 | Fc\u22121] (6)\n= P\u2211 p,q=1 \u3008Up(Wcq), Uq(Wcq)\u3009 (7)\n+ P\u2211 p=1 E [ \u2016Up(Wcp, Icp)\u2212 Up(Wcp)\u201622 | Fc\u22121 ] . (8)\nThe variance term in the above equality can be bounded as P\u2211 p=1 E [ \u2016Up(Wcp, Icp)\u2212 Up(Wcp)\u201622 | Fc\u22121 ] = \u03b72c P\u2211 p=1 E \u2016|Sp|\u2211 j\u2208Icp \u2207fj(Wcp)\u2212\u2207Fp(Wcp)\u201622 | Fc\u22121 \n\ufe38 \ufe37\ufe37 \ufe38 \u03c3\u03022P\n\u2264 \u03b72c \u03c3\u03022P,\nNow use the update rule Wc+1p = W c p + \u2211P p=1 Up(W c p, I c p) and the descent lemma [5], we have\nF (Wc+1)\u2212 F (Wc) \u2264 \u3008Wc+1 \u2212Wc,\u2207F (Wc)\u3009+ LF 2 \u2016Wc+1 \u2212Wc\u201622 (9)\n= \u3008 P\u2211 p=1 Up(W c p, I c p),\u2207F (Wc)\u3009+ LF 2 \u2016 P\u2211 p=1 Up(W c p, I c p)\u201622 (10)\nThen take expectation on both sides, we obtain E [ F (Wc+1)\u2212 F (Wc) | Fc\u22121 ] \u2264 \u3008 P\u2211 p=1 Up(W c p),\u2207F (Wc)\u3009+ LF 2 \u2016 P\u2211 p=1 Up(W c p)\u201622 + LF \u03b7 2 c \u03c3\u0302 2P 2\n(11)\n= ( LF 2 \u2212 \u03b7\u22121c )\u2016 P\u2211 p=1 Up(W c p)\u201622 \u2212 \u03b7\u22121c \u3008 P\u2211 p=1 Up(W c p), P\u2211 p=1 [Up(W c)\u2212 Up(Wcp)]\u3009+ LF \u03b7 2 c \u03c3\u0302 2P 2\n(12)\n\u2264 (LF 2 \u2212 \u03b7\u22121c )\u2016 P\u2211 p=1 Up(W c p)\u201622 + \u2016 P\u2211 p=1 Up(W c p)\u2016 P\u2211 p=1 Lp\u2016Wc \u2212Wcp\u2016+ LF \u03b7 2 c \u03c3\u0302 2P 2 , (13)\nNow take expectation w.r.t all random variables, we obtain\nE [ F (Wc+1)\u2212 F (Wc) ] \u2264 (LF\n2 \u2212 \u03b7\u22121c )E\n[ \u2016\nP\u2211 p=1 Up(W c p)\u201622\n] (14)\n+ P\u2211 p=1 LpE\n[ \u2016\nP\u2211 p=1 Up(W c p)\u2016\u2016Wc \u2212Wcp\u2016\n] + LF \u03b7 2 c\u03c3 2P\n2 (15)\nNext we proceed to bound the term E\u2016 \u2211P p=1 Up(W c p)\u2016\u2016Wc \u2212Wcp\u2016. We list the auxiliary update rule and the local update rule here for convenience.\nWc = W0 + P\u2211 q=1 c\u22121\u2211 t=0 Uq(W t q, I t q), (16)\nWcp = W 0 + P\u2211 q=1 \u03c4qp (c)\u2211 t=0 Uq(W t q, I t q). (17)\nNow subtract the above two and use the bounded delay assumption 0 \u2264 (c \u2212 1) \u2212 \u03c4 qp (c) \u2264 s, we obtain\n\u2016Wc \u2212Wcp\u2016 = \u2016 P\u2211 q=1 c\u22121\u2211 t=\u03c4qp (c)+1 Uq(W t q, I t q)\u2016 (18)\n\u2264 \u2016 P\u2211 q=1 c\u22121\u2211 t=c\u2212s Uq(W t q, I t q)\u2016+ \u2016 P\u2211 q=1 \u03c4qp (c)\u2211 t=c\u2212s Uq(W t q, I t q)\u2016 (19) \u2264 c\u22121\u2211 t=c\u2212s \u2016 P\u2211 q=1 Uq(W t q, I t q)\u2016+ \u03b7c\u2212sG, (20)\nwhere the last inequality follows from the facts that \u03b7c is strictly decreasing, and \u2016 \u2211P q=1 \u2211\u03c4qp (c) t=c\u2212s\u2207Fq(Wtq, Itq)\u2016 is bounded by some constant G since \u2207Fq is continuous and all the sequences Wcp are bounded. Thus by taking expectation, we obtain\nE [ \u2016\nP\u2211 p=1 Up(W c p)\u2016 \u2016Wc \u2212Wcp\u2016 ] \u2264 E\n[ \u2016\nP\u2211 p=1 Up(W c p)\u2016 ( c\u22121\u2211 t=c\u2212s \u2016 P\u2211 q=1 Uq(W t q, I t q)\u2016+ \u03b7c\u2212sG )] (21)\n= c\u22121\u2211 t=c\u2212s E\n[ \u2016\nP\u2211 p=1 Up(W c p)\u2016\u2016 P\u2211 q=1 Uq(W t q, I t q)\u2016\n] + \u03b7c\u2212sG \u00b7 E [ \u2016\nP\u2211 p=1 Up(W c p)\u2016 ] (22)\n\u2264 c\u22121\u2211 t=c\u2212s E\n[ \u2016\nP\u2211 p=1 Up(W c p)\u201622 + \u2016 P\u2211 q=1 Uq(W t q, I t q)\u201622\n] + E\u2016\nP\u2211 p=1 Up(W c p)\u201622 + \u03b72c\u2212sG2\n(23)\n\u2264 (s+ 1)E\u2016 P\u2211 p=1 Up(W c p)\u201622 + c\u22121\u2211 t=c\u2212s\n[ E\u2016\nP\u2211 q=1 Uq(W t q)\u201622 + \u03b72t \u03c32P ] + \u03b72c\u2212sG 2.\n(24) Now plug this into the previous result in (??):\nEF (Wc+1)\u2212 EF (Wc) \u2264 (LF 2 \u2212 \u03b7\u22121c )E\u2016 P\u2211 p=1 Up(W c p)\u201622 + (s+ 1)LE\u2016 P\u2211 p=1 Up(W c p)\u201622 (25)\n+ c\u22121\u2211 t=c\u2212s\n[ LE\u2016\nP\u2211 p=1 Up(W c p)\u201622 + \u03b72tL\u03c32P ] + \u03b72c\u2212sG 2L+ LF \u03b7 2 c\u03c3 2P 2\n(26)\n= ( LF 2 + (s+ 1)L\u2212 \u03b7\u22121c )E\u2016 P\u2211 p=1 Up(W c p)\u201622 (27)\n+ c\u22121\u2211 t=c\u2212s\n[ LE\u2016\nP\u2211 p=1 Up(W c p)\u201622 + \u03b72tL\u03c32P ] + \u03b72c\u2212sG 2L+ LF \u03b7 2 c\u03c3 2P 2\n(28)\nSum both sides over c = 0, ..., C:\nEF (WC+1)\u2212 EF (W0) \u2264 C\u2211 c=0 [ ( LF 2 + (2s+ 1)L\u2212 \u03b7\u22121c )E\u2016 P\u2211 p=1 Up(W c p)\u201622 ] (29)\n+ (L\u03c32Ps+ LF\u03c3\n2P\n2 ) C\u2211 c=0 \u03b72c +G 2L C\u2211 c=0 \u03b72c\u2212s. (30)\nAfter rearranging terms we finally obtain\nC\u2211 c=0 [ \u03b72c (\u03b7 \u22121 c \u2212 LF 2 \u2212 2(s+ 1)L)E\u2016 P\u2211 p=1 \u2207Fp(Wcp)\u201622 ] \u2264 EF (W0)\u2212 EF (WC+1) (31)\n+ (L\u03c32Ps+ LF\u03c3\n2P\n2 ) C\u2211 c=0 \u03b72c +G 2L C\u2211 c=0 \u03b72c\u2212s.\n(32)\nNow set \u03b7\u22121c = LF 2 + 2sL +\n\u221a c. Then, the above inequality becomes (ignoring some universal\nconstants):\nC\u2211 c=0 [ 1\u221a c E\u2016 P\u2211 p=1 \u2207Fp(Wcp)\u201622 ] \u2264 O (( (L+ LF )\u03c3 2Ps ) C\u2211 c=0 1 c ) . (33)\nSince \u2211C c=0 1 c = o( \u2211C c=0 1\u221a c ), we must have\nlim inf c\u2192\u221e E\u2016 P\u2211 p=1 \u2207Fp(Wcp)\u2016 = 0, (34)\nproving the first claim.\nOn the other hand, the bound of \u2016Wc \u2212Wcp\u2016 in (??) gives\n\u2016Wc \u2212Wcp\u2016 \u2264 c\u22121\u2211 t=c\u2212s \u03b7t\u2016 P\u2211 q=1 |Sq| \u2211 j\u2208Itq \u2207fj(Wtq)\u2016+ \u03b7c\u2212sG. (35)\nBy assumption the sequences {Wcp}p,c and {Wc}c are bounded and the gradient of fj is continuous, thus \u2207fj(Wtq) is bounded. Now take c \u2192 \u221e in the above inequality and notice that lim\nc\u2192\u221e \u03b7c = 0,\nwe have lim c\u2192\u221e \u2016Wc \u2212Wcp\u2016 = 0 almost surely, proving the second claim.\nLastly, the Lipschitz continuity of\u2207Fp further implies\n0 = lim inf c\u2192\u221e E\u2016 P\u2211 p=1 \u2207Fp(Wcp)\u2016 \u2265 lim inf c\u2192\u221e E\u2016 P\u2211 p=1 \u2207Fp(Wc)\u2016 = lim inf c\u2192\u221e E\u2016\u2207F (Wc)\u2016 = 0.\nThus there exists a common limit point of Wc,Wcp that is a stationary point almost surely. From (??) and use the estimate \u2211C c=1 1 c \u2248 logC, we have\nmin c=1,...,C\nE [ \u2016\nP\u2211 p=1 \u2207Fp(Wcp)\u201622\n] \u2264 O ( (L+ LF )\u03c3\n2Ps logC\u221a C\n) . (36)\nThe proof is now complete."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Scalable inference in latent variable models", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A.J. Smola"], "venue": "In WSDM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Nonlinear programming", "author": ["D.P. Bertsekas"], "venue": "Athena scientific Belmont,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Distributed decision-tree induction in peer-to-peer systems. Statistical Analysis and Data Mining: The ASA Data", "author": ["K. Bhaduri", "R. Wolff", "C. Giannella", "H. Kargupta"], "venue": "Science Journal,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Project adam: building an efficient and scalable deep learning training system", "author": ["T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman"], "venue": "In OSDI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "High-performance distributed ml at scale through parameter server consistency models", "author": ["W. Dai", "A. Kumar", "J. Wei", "Q. Ho", "G. Gibson", "E.P. Xing"], "venue": "In AAAI", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A local asynchronous distributed privacy preserving feature selection algorithm for large peer-to-peer networks", "author": ["K. Das", "K. Bhaduri", "H. Kargupta"], "venue": "Knowledge and information systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Powergraph: distributed graph-parallel computation on natural graphs", "author": ["J.E. Gonzalez", "Y. Low", "H. Gu", "D. Bickson", "C. Guestrin"], "venue": "In OSDI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Distributed training of large-scale logistic models", "author": ["S. Gopal", "Y. Yang"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J.K. Kim", "P.B. Gibbons", "G.A. Gibson", "G. Ganger", "E. Xing"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Passcode: Parallel asynchronous stochastic dual co-ordinate descent", "author": ["C.-J. Hsieh", "H.-F. Yu", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Communicationefficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1c", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "On model parallelization and scheduling strategies for distributed machine learning", "author": ["S. Lee", "J.K. Kim", "X. Zheng", "Q. Ho", "G.A. Gibson", "E.P. Xing"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Malt: distributed data-parallelism for existing ml applications", "author": ["H. Li", "A. Kadav", "E. Kruus", "C. Ungureanu"], "venue": "In Proceedings of the Tenth European Conference on Computer Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B.-Y. Su"], "venue": "In OSDI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "In SIGMOD,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Gossip learning with linear models on fully distributed data", "author": ["R. Orm\u00e1ndi", "I. Heged\u0171s", "M. Jelasity"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Lshtc: A benchmark for large-scale text classification", "author": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari"], "venue": "[cs.IR],", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "author": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Large-scale distributed non-negative sparse coding and sparse dictionary learning", "author": ["V. Sindhwani", "A. Ghoting"], "venue": "In KDD,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1986}, {"title": "Replicated data consistency explained through baseball", "author": ["D. Terry"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Communication/computation tradeoffs in consensus-based distributed optimization", "author": ["K. Tsianos", "S. Lawlor", "M.G. Rabbat"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "In CVPR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In NIPS,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "In NSDI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix W.", "startOffset": 129, "endOffset": 132}, {"referenceID": 34, "context": "1 Introduction For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix W.", "startOffset": 165, "endOffset": 169}, {"referenceID": 23, "context": "1 Introduction For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix W.", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "Learning MPMs in large scale ML problems is challenging: ML application scales have risen dramatically, a good example being the ImageNet [12] compendium with millions of images grouped into tens of thousands of classes.", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 9, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 20, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 6, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 28, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 13, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 25, "context": "In one application of MLR to Wikipedia [26], J = 325k and D > 10, 000, thus W contains several billion entries (tens of GBs of memory).", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 199, "endOffset": 203}, {"referenceID": 1, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 264, "endOffset": 267}, {"referenceID": 9, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 279, "endOffset": 283}, {"referenceID": 14, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 295, "endOffset": 299}, {"referenceID": 6, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 314, "endOffset": 317}, {"referenceID": 21, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 322, "endOffset": 326}, {"referenceID": 10, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 9, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 28, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 13, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 6, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 20, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 9, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 139, "endOffset": 150}, {"referenceID": 14, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 139, "endOffset": 150}, {"referenceID": 6, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 139, "endOffset": 150}, {"referenceID": 15, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 26, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 35, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 17, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 16, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 23, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 18, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 34, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 36, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 6, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 6, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 37, "endOffset": 40}, {"referenceID": 34, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 127, "endOffset": 131}, {"referenceID": 36, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 179, "endOffset": 183}, {"referenceID": 15, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 9, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 14, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 26, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 35, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 17, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 6, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 16, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 20, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 21, "context": "GraphLab, Petuum-PS and [22]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "SFs have been used to speed up some (but not all) network communication in deep learning [7]; our work differs primarily in that we always transmit SFs, never full matrices.", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 100, "endOffset": 112}, {"referenceID": 22, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 100, "endOffset": 112}, {"referenceID": 37, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 127, "endOffset": 138}, {"referenceID": 1, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 127, "endOffset": 138}, {"referenceID": 9, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 127, "endOffset": 138}, {"referenceID": 4, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 184, "endOffset": 195}, {"referenceID": 14, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 184, "endOffset": 195}, {"referenceID": 30, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 184, "endOffset": 195}, {"referenceID": 34, "context": "We provide theoretical analysis of SFB under synchronous and bounded-async consistency, and demonstrate that SFB learning of matrix-parametrized models significantly outperforms strategies that communicate the full parameter/update matrix, on a variety of applications including distance metric learning [35], sparse coding [24] and unregularized/`2-regularized multiclass logistic regression.", "startOffset": 304, "endOffset": 308}, {"referenceID": 23, "context": "We provide theoretical analysis of SFB under synchronous and bounded-async consistency, and demonstrate that SFB learning of matrix-parametrized models significantly outperforms strategies that communicate the full parameter/update matrix, on a variety of applications including distance metric learning [35], sparse coding [24] and unregularized/`2-regularized multiclass logistic regression.", "startOffset": 324, "endOffset": 328}, {"referenceID": 2, "context": "assumed to admit an efficient proximal operator proxh(\u00b7) [3].", "startOffset": 57, "endOffset": 60}, {"referenceID": 23, "context": ", class labels in classification, response values in regression), or even unobserved auxiliary information (such as sparse codes in sparse coding [24]) associated with data sample i.", "startOffset": 146, "endOffset": 150}, {"referenceID": 23, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 18, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 34, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 36, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 6, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 34, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "Sparse coding (SC) [24] learns a dictionary of basis from data, so that the data can be re-represented sparsely (and thus efficiently) in terms of the dictionary.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "In SC, W is the dictionary matrix, ai are the sparse codes, bi is the input feature vector and fi(\u00b7) is a quadratic function [24].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 14, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 6, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 20, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 15, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 26, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 35, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 17, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 16, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 35, "context": "Stochastic DCA: SDCA applies to problems (P) where fi(\u00b7) is convex and h(\u00b7) is strongly convex [36] (e.", "startOffset": 95, "endOffset": 99}, {"referenceID": 29, "context": "most K; in Restricted Boltzmann Machines [30], the update of the weight matrix is computed from four vectors u1,v1,u2,v2 as u1v 1 \u2212 u2v 2 , i.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "rank-2; for the BFGS algorithm [4], the update of the inverse Hessian is computed from two vectors u,v as \u03b1uu> \u2212 \u03b2(uv> + vu>), i.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "as used by Project Adam [7] to learn neural networks: there, a centralized server maintains the global parameter matrix, and each client keeps a local copy.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.", "startOffset": 72, "endOffset": 84}, {"referenceID": 21, "context": "If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.", "startOffset": 72, "endOffset": 84}, {"referenceID": 19, "context": "If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.", "startOffset": 72, "endOffset": 84}, {"referenceID": 20, "context": "Halton-sequence [21].", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "BSP-SFB: Under BSP [11, 23, 38], an end-ofiteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration.", "startOffset": 19, "endOffset": 31}, {"referenceID": 22, "context": "BSP-SFB: Under BSP [11, 23, 38], an end-ofiteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration.", "startOffset": 19, "endOffset": 31}, {"referenceID": 37, "context": "BSP-SFB: Under BSP [11, 23, 38], an end-ofiteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration.", "startOffset": 19, "endOffset": 31}, {"referenceID": 14, "context": "ASP-SFB: BSP can be sensitive to stragglers (slow workers) [15, 31], limiting the distributed system to the speed of the slowest worker.", "startOffset": 59, "endOffset": 67}, {"referenceID": 30, "context": "ASP-SFB: BSP can be sensitive to stragglers (slow workers) [15, 31], limiting the distributed system to the speed of the slowest worker.", "startOffset": 59, "endOffset": 67}, {"referenceID": 12, "context": "The Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others.", "startOffset": 32, "endOffset": 43}, {"referenceID": 1, "context": "The Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others.", "startOffset": 32, "endOffset": 43}, {"referenceID": 9, "context": "The Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others.", "startOffset": 32, "endOffset": 43}, {"referenceID": 14, "context": "ASP is efficient in terms of iteration throughput, but carries the risk that worker parameter copies can end up greatly out of synchronization, which can lead to algorithm divergence [15].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "Figure 3: Sample code of sparse coding in SFB SSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a boundedasynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s.", "startOffset": 88, "endOffset": 99}, {"referenceID": 14, "context": "Figure 3: Sample code of sparse coding in SFB SSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a boundedasynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s.", "startOffset": 88, "endOffset": 99}, {"referenceID": 30, "context": "Figure 3: Sample code of sparse coding in SFB SSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a boundedasynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s.", "startOffset": 88, "endOffset": 99}, {"referenceID": 14, "context": "SSP alleviates the straggler issue while guaranteeing algorithm convergence [15, 31].", "startOffset": 76, "endOffset": 84}, {"referenceID": 30, "context": "SSP alleviates the straggler issue while guaranteeing algorithm convergence [15, 31].", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "When s = 0, SSP-SFB reduces to BSP-SFB [11, 38], and when s =\u221e, SSP-SFB becomes ASP-SFB.", "startOffset": 39, "endOffset": 47}, {"referenceID": 37, "context": "When s = 0, SSP-SFB reduces to BSP-SFB [11, 38], and when s =\u221e, SSP-SFB becomes ASP-SFB.", "startOffset": 39, "endOffset": 47}, {"referenceID": 6, "context": "Computational Model Total comms, per iter W storage per machine W update time, per iter SFB (peer-to-peer) O(P K(J +D)) O(JD) O(P KJD) FMS (client-server [7]) O(PJD) O(JD) O(PJD) at server, O(PKJD) at clients Figure 5: Cost of using SFB versus FMS, where K is minibatch size, J,D are dimensions of W, and P is the number of workers.", "startOffset": 154, "endOffset": 157}, {"referenceID": 21, "context": "In addition, SFBcaster possesses high elasticity [22]: new workers can be added and existing workers can be taken offline, without restarting the running framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes.", "startOffset": 4, "endOffset": 14}, {"referenceID": 14, "context": "SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes.", "startOffset": 4, "endOffset": 14}, {"referenceID": 7, "context": "SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes.", "startOffset": 4, "endOffset": 14}, {"referenceID": 6, "context": "Cost Analysis: Table 5 compares the communications, space and time (to apply updates to W) costs of peer-to-peer SFB, against full matrix synchronization (FMS) under a client-server architecture [7].", "startOffset": 195, "endOffset": 198}, {"referenceID": 9, "context": "In order to make SFB applicable to data centers with 1000s of machines [10, 22], we note that SFB costs can be further reduced via efficient broadcast strategies: e.", "startOffset": 71, "endOffset": 79}, {"referenceID": 21, "context": "In order to make SFB applicable to data centers with 1000s of machines [10, 22], we note that SFB costs can be further reduced via efficient broadcast strategies: e.", "startOffset": 71, "endOffset": 79}, {"referenceID": 20, "context": "in the Halton-sequence strategy [21], each machine connects with and broadcasts messages to Q = O(log(P )) machines, rather than all P machines.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Our analysis differs from [5] in two ways: (1) [5] explicitly maintains a consensus model which would require transmitting the parameter matrix among worker machines \u2014 a communication bottleneck that we were able to avoid; (2) we allow subsampling in each worker machine.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "Our analysis differs from [5] in two ways: (1) [5] explicitly maintains a consensus model which would require transmitting the parameter matrix among worker machines \u2014 a communication bottleneck that we were able to avoid; (2) we allow subsampling in each worker machine.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Accordingly, our theoretical guarantee is probabilistic, instead of the deterministic one in [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 14, "context": "Note that our analysis also covers worker parameters {W p}, which improves upon analyses that only show convergence of a central server parameter [15, 8, 1].", "startOffset": 146, "endOffset": 156}, {"referenceID": 7, "context": "Note that our analysis also covers worker parameters {W p}, which improves upon analyses that only show convergence of a central server parameter [15, 8, 1].", "startOffset": 146, "endOffset": 156}, {"referenceID": 0, "context": "Note that our analysis also covers worker parameters {W p}, which improves upon analyses that only show convergence of a central server parameter [15, 8, 1].", "startOffset": 146, "endOffset": 156}, {"referenceID": 37, "context": "For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models.", "startOffset": 153, "endOffset": 161}, {"referenceID": 21, "context": "For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models.", "startOffset": 153, "endOffset": 161}, {"referenceID": 33, "context": "Due to data sparsity, both the update matrices and sufficient factors are sparse; we For DML, we use the parametrization proposed in [34], which learns a linear projection matrix L \u2208 Rd\u00d7k, where d is the feature dimension and k is the latent dimension.", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "This has the same communication complexity as [7], which sends SFs from clients to servers, but sends full matrices from servers to clients (which dominates the overall cost).", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "Datasets and Experimental Setup We used two datasets for our experiments: (1) ImageNet [12] ILSFRC2012 dataset, which contains 1.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "2 million images from 1000 categories; the images are represented with LLC features [33], whose dimensionality is 172k.", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "(2) Wikipedia [26] dataset, which contains 2.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "Computation time for SFB is slightly longer than FMS because (1) update matrices must be Spark is about 2x slower than PS [15, 22] based C++ implementation of FMS, due to JVM and RDD overheads.", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "Computation time for SFB is slightly longer than FMS because (1) update matrices must be Spark is about 2x slower than PS [15, 22] based C++ implementation of FMS, due to JVM and RDD overheads.", "startOffset": 122, "endOffset": 130}, {"referenceID": 9, "context": "On the system side, [10] proposed to reduce communication overhead by reducing the frequency of parameter/gradient exchanges between workers and the central server.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "[22] used filters to select part of \u201cimportant\u201d parameters/updates for transmission to reduce the number of data entries to be communicated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "On the algorithm side, [32] and [36] studied the tradeoffs between communication and computation in distributed dual averaging and distributed stochastic dual coordinate ascent respectively.", "startOffset": 23, "endOffset": 27}, {"referenceID": 35, "context": "On the algorithm side, [32] and [36] studied the tradeoffs between communication and computation in distributed dual averaging and distributed stochastic dual coordinate ascent respectively.", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "[28] proposed an approximate Newton-type method to achieve communication efficiency in distributed optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 8, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 24, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 20, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 4, "context": "p=1 E \uf8f0\u2016|Sp|\u2211 j\u2208Ic p \u2207fj(W p)\u2212\u2207Fp(W p)\u20162 | Fc\u22121 \uf8f9\uf8fb } {{ } \u03c3\u03022P \u2264 \u03b7 c \u03c3\u0302P, Now use the update rule W p = W c p + \u2211P p=1 Up(W c p, I c p) and the descent lemma [5], we have F (W)\u2212 F (W) \u2264 \u3008W \u2212W,\u2207F (W)\u3009+ LF 2 \u2016W \u2212W\u20162 (9)", "startOffset": 158, "endOffset": 161}], "year": 2015, "abstractText": "Matrix-parametrized models, including multiclass logistic regression and sparse coding, are used in machine learning (ML) applications ranging from computer vision to computational biology. When these models are applied to large-scale ML problems starting at millions of samples and tens of thousands of classes, their parameter matrix can grow at an unexpected rate, resulting in high parameter synchronization costs that greatly slow down distributed learning. To address this issue, we propose a Sufficient Factor Broadcasting (SFB) computation model for efficient distributed learning of a large family of matrix-parameterized models, which share the following property: the parameter update computed on each data sample is a rank-1 matrix, i.e. the outer product of two \u201csufficient factors\u201d (SFs). By broadcasting the SFs among worker machines and reconstructing the update matrices locally at each worker, SFB improves communication efficiency \u2014 communication costs are linear in the parameter matrix\u2019s dimensions, rather than quadratic \u2014 without affecting computational correctness. We present a theoretical convergence analysis of SFB, and empirically corroborate its efficiency on four different matrix-parametrized ML models.", "creator": "LaTeX with hyperref package"}}}