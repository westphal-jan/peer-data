{"id": "1409.8027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2014", "title": "Autonomous robots and the SP theory of intelligence", "abstract": "this comprehensive article is about how solving the \" sp theory of competitive intelligence \" and its realisation in discussing the \" animal sp machine \" ( details both commonly outlined first in discussing the article ) may concurrently help work to solve computer - related problems in the design of autonomous automatic robots, meaning autonomous robots that don't entirely depend on external intelligence controllers or power supplies, thus are mobile, linear and are necessarily designed to exhibit barely as small much human - like intelligence capacities as possible. ultimately the article additionally is about : \u2018 how to increase the computational energy and energy effective efficiency of computers and processes reduce beyond their bulk ; how to achieve adaptive human - like versatility efficiently in mechanical intelligence ; firstly and likewise for measuring human - like adaptability in technological intelligence. employing the sp system has multiple potential options for substantial potential gains in adequate computational and energy efficiency and reductions in the bulkiness of computers : firstly by reducing only the size coefficient of data to be processed ; successes by exploiting outdated statistical information showing that intelligence the \" system provider gathers ; and further via presenting an entirely updated graphical version of donald hebb': s fundamental concept of a \" random cell assembly \". attitude towards human - like versatility succeeds in intelligence, whereby the intelligent sp machines system has strengths in sustaining unsupervised learning, through natural natural language processing, pattern recognition, information spatial retrieval, several kinds expressive of geometric reasoning, reaction planning, optimal problem solving, and numerous more, coping with seamless nonlinear integration amongst structures and functions. the sp system'for s fundamental strengths in unsupervised learning and other specific aspects of intelligence complexity may help strive to achieve human - like procedural adaptability in collective intelligence such via : the learning of learned natural emotion language ; learning capabilities to always see ; the building efficient 3d models of objects and of a humanoid robot's surroundings ; learning regularities in the workings experience of a robot and interventions in the robot'an s environment ; exploration and play ; enhanced learning major skills ; and encouraging secondary potential forms of learning. others also discussed concurrently are : how the sp system server may process parallel overlapping streams of interacting information ; effective generalisation of indirect knowledge, correction of over - referenced generalisations, filtering and utilizing learning algorithms from a dirty data ; how seeks to help cut [ the cost side of learning ; and adjusting reinforcements, motivations, goals, and interactive demonstration.", "histories": [["v1", "Mon, 29 Sep 2014 08:41:01 GMT  (78kb,D)", "https://arxiv.org/abs/1409.8027v1", null], ["v2", "Fri, 23 Jan 2015 08:47:02 GMT  (136kb,D)", "http://arxiv.org/abs/1409.8027v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["j gerard wolff"], "accepted": false, "id": "1409.8027"}, "pdf": {"name": "1409.8027.pdf", "metadata": {"source": "CRF", "title": "Autonomous Robots and the SP Theory of Intelligence", "authors": ["J Gerard Wolff"], "emails": ["jgw@cognitionresearch.org."], "sections": [{"heading": null, "text": "Index Terms\u2014Artificial intelligence, robots, cognitive science, data compression, pattern recognition, unsupervised learning.\nI. INTRODUCTION This paper is about how the SP theory of intelligence and its realisation in the SP machine (both of them to be described) may help in the design of the information-processing \u2018brains\u2019 of autonomous robots.1,2 Here, \u2018autonomous robots\u2019 are ones that do not depend on external intelligence (natural or artificial), do not depend on external power supplies, and are mobile. We shall also assume that a goal in their development is to provide them with human-like versatility and adaptability in intelligence.\nThe paper is relevant to robots that are not autonomous in the sense just described, but the problems to be addressed\nDr Gerry Wolff is founder and director of CognitionResearch.org, Menai Bridge, UK. e-mail: jgw@cognitionresearch.org.\nManuscript received [Month] [day], [year]; revised [Month] [day], [year]. 1In the rest of this paper, the quote marks will be omitted when referring to the information-processing mechanisms or brains of autonomous robots. 2As in [23, Note 21], this paper does not in any way endorse or defend the unethical or illegal use of autonomous robots of any kind to cause death, injury, or damage to property.\nare most acute in robots that are intended to function autonomously, and potential solutions are correspondingly more interesting.\nIn brief, the problems and potential solutions to be discussed are:\n\u2022 Computational efficiency, the use of energy, and the size and weight of computers. If a robot is to be autonomous in the sense outlined above, it needs a brain that is efficient enough to do all the necessary processing without external assistance, does not require an industrial-scale power station to meet its energy demands, and is small enough and light enough to be carried around easily\u2014 things that are difficult or impossible to achieve with current technologies. The SP system may help: by reducing the size of data to be processed; by exploiting statistical information that the system gathers as an integral part of how it works; and via a new version of Donald Hebb\u2019s [5] concept of a \u201ccell assembly\u201d. \u2022 Towards human-like versatility in intelligence. If a robot is to operate successfully in an environment where people cannot help, or where such opportunities are limited, it needs as much as possible of the versatility in intelligence that people may otherwise provide. The SP system demonstrates versatility via its strengths in areas such as unsupervised learning, natural language processing, pattern recognition, information retrieval, several kinds of reasoning, planning, problem solving, and more. But the SP system is not simply a kludge of different AI functions. Owing to its focus on simplification and integration of concepts in computing and cognition (Section II), it promises to reduce or eliminate unnecessary complexity and to avoid awkward incompatibilities between poorly-integrated subsystems. And like any theory that simplifies and integrates a good range of observations and concepts, it promises deeper insights and better solutions to problems than may otherwise be achieved. \u2022 Towards human-like adaptability in intelligence. Amongst the AI capabilities of the SP system mentioned above, unsupervised learning has particular significance because of its potential as a key to human-like adaptability in intelligence, both directly and as a basis for other kinds of learning.\nThese problems and their potential solutions are discussed in Sections III, IV, and V, below. As a foundation for those three sections, there is an outline of the SP theory in Section II, with pointers to where further information may be found.\nar X\niv :1\n40 9.\n80 27\nv2 [\ncs .A\nI] 2\n3 Ja\nn 20\n15"}, {"heading": "A. Novelty and Contribution", "text": "Like any good theory, the SP theory has a wide range of potential applications, some of which are described in [23]. For reasons indicated above, and expanded in Sections III, IV, and V, the SP theory is particularly relevant to the development of the brains of autonomous robots.\nBut mere assertion is not enough. This paper aims to explain why the SP theory is relevant to the development of robot brains and how it may be applied in that area. It is a considerable expansion of the short discussion in [23, Section 6.3], with much new thinking, and it is substantially different from other publications in the SP programme of research (Section II). To help make the paper comprehensible and readable, it includes summaries of material that has been presented more fully elsewhere. Unnecessary repetition of information has been minimised and there are frequent pointers to where more detailed information may be found.\nThe approach is radically different from other work in robotics because of its distinctive features (Section II-G), especially the concept of multiple alignment (Section II-C2). As will be seen in the body of the paper, this new approach has considerable potential in the three main areas that are addressed.\nAutonomous robots have set a new challenge for the SP theory: how to process parallel streams of information. This has led to important new refinements of the theory, described in Sections V-G, V-H, and V-I, and in Appendix C. The paper also includes new thinking about the identification of low-level perceptual features (Appendix A) and about quantification in the SP system (Appendix B)."}, {"heading": "II. OUTLINE OF THE SP THEORY AND THE SP MACHINE", "text": "In this section, and elsewhere in the paper, the aim is to describe the SP theory with enough detail to ensure that the rest of the paper makes sense.\nThe SP theory is a unique attempt to simplify and integrate concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition, with information compression as a unifying theme.3\nThe theory is described most fully in [18] and at some length in [20]. In addition to the present paper, potential benefits and applications of the theory are described in [23] (an overview), [22] (how the SP system may help to solve problems associated with big data), [21] (application of the SP theory to the understanding of natural vision and the development of computer vision), [17] (application of the SP system to medical diagnosis), and [19] (the SP system as an intelligent database)."}, {"heading": "A. The SP Computer Model and the SP Machine", "text": "The SP theory is realised in the form of a computer model, SP70, which may be regarded as a first version of the SP machine.\n3The name \u201cSP\u201d is short for Simplicity and Power, because compression of any given body of information, I, may be seen as a process of reducing informational redundancy in I and thus increasing its \u201csimplicity\u201d, whilst retaining as much as possible of its non-redundant expressive \u201cpower\u201d.\nExpressing the theory in the form of a computer model helps to reduce vagueness in the theory. Perhaps more importantly, it provides a means of testing candidate ideas. As a result of such testing, many seemingly-promising ideas have been rejected. The model is also a means of demonstrating what can be achieved with the system.\nAn outline of how the SP computer model works may be found in [18, Section 3.9], with more detail, including pseudocode, in [18, Sections 3.10 and 9.2].4 Fully commented source code for the SP70 computer model may be downloaded via a link near the bottom of www.cognitionresearch.org/sp.htm.\nIt is envisaged that the SP computer model will be the basis for the creation of a high-parallel, open-source version of the SP machine, hosted on an existing high-performance computer. This will be a means for researchers everywhere to explore what can be done with the system and to create new versions of it [23, Section 3], [22, Section XII]. How things may develop is shown schematically in Figure 1."}, {"heading": "B. Patterns and Symbols", "text": "In the SP system, knowledge is represented with arrays of atomic symbols in one or two dimensions called patterns. The SP70 computer model works with 1D patterns but it is envisaged that the system will be generalised to work with 2D patterns [20, Section 3.3].\nA \u2018symbol\u2019 in the SP system is simply a mark that can be matched with any other symbol to determine whether it is the same or different: no other result is permitted.\nWith one exception, any meaning associated with a given SP symbol or combination of symbols must be expressed using other SP symbols. The exception is where an SP symbol connects with an entity or value outside the SP system. For example, a signal from a sensor in an autonomous robot, or an instruction for one of the robot\u2019s muscles to contract, may be represented by a symbol within the SP system.5\n4The description of how the SP70 model works includes a description, in [18, Sections 3.9.1 and 3.10], of a subset of the SP70 model called SP61.\n5It is pertinent here to mention that some symbols are classified as \u2018identification\u2019 or \u2018ID\u2019 symbols, while others are classified as \u2018contents\u2019 or \u2018C\u2019 symbols [18, Section 3.4.5]. But these distinctions serve the internal workings of the SP system and do not impinge directly on how the system functions in the representation and processing of knowledge.\nAlthough conventional computing systems make extensive use of numbers, the SP system, in itself, makes no provision for the representation or processing of numbers. Possible responses to this feature of the SP system are discussed in Appendix B.\nThe way in which SP symbols may be identified in such things as images, speech, and music, is discussed in Appendix A.\nIn themselves, SP patterns are not particularly expressive. But within the multiple alignment framework (Section II-C2), they support the representation and processing of a wide variety of kinds of knowledge (Section II-D). A goal of the SP research programme is to establish one system for the representation and processing of all kinds of knowledge (see also [22, Section III]). Evidence to date suggests that this may be achieved with SP patterns in the multiple alignment framework.\nAny collection of SP patterns is termed a grammar. Although that term is most closely associated with linguistics, it will be used throughout this paper for a collection of SP patterns describing any kind of knowledge.6\nC. Information Compression\nThe SP theory is conceived as a brain-like system that receives New information via its senses and stores some or all of it in compressed form as Old information. The emphasis on information compression derives from earlier research on grammatical inference (Section V-A3) and the principle of minimum length encoding (MLE) [11], [12], [14]).\nAt an abstract level, information compression means the detection and reduction of redundancy in information. In more concrete terms, redundancy means recurrent patterns, regularities, structures, and associations, including causal associations. Thus information compression provides a means of discovering such things as words in natural language (Section V-A3), objects (Section V-F), and the association between lightning and thunder [22, Section III-A.1], in accordance with the DONSVIC principle [20, Section 5.2].7\nThe default assumption in the SP theory is that compression of information is always lossless, meaning that all nonredundant information is retained. In particular applications, there may be a case for discarding non-redundant information (see, for example, [22, Section X-B]) but any such discard is reversible.\nIn the SP system, information compression is achieved via the matching and unification of patterns. More specifically, it is achieved via the building of multiple alignments and via the unsupervised learning of grammars. These three things are described briefly in the following three subsections.\n1) Information Compression Via the Matching and Unification of Patterns: The basis for information compression in the SP system is a process of searching for patterns that match\n6This is partly because research on grammatical inference is one of the inspirations for the SP concepts (Section V-A3), and partly because of the significance of grammars in research on principles of minimum length encoding.\n7DONSVIC = \u201cThe discovery of natural structures via information compression\u201d.\neach other with a process of merging or \u2018unifying\u2019 patterns that are the same. At the heart of the SP70 computer model is a method for finding good full and partial matches between sequences with advantages compared with classical methods [18, Appendix A].8\n2) Information Compression Via the Building of Multiple Alignments: That process for finding good full and partial matches between sequences is the foundation for processes that build multiple alignments like the one shown in Figure 2.9\nThis example shows the best multiple alignment created by the SP computer model when a set of New patterns (in column 0)10 is processed in conjunction with a set of pre-existing Old patterns like those shown in columns 1 to 6. Here, the multiple alignment is \u2018best\u2019 because it is the one that achieves the most economical description of the New patterns in terms of the Old patterns. The way in which that description or \u2018encoding\u2019 is derived from a multiple alignment is explained in [18, Section 3.5] and [20, Section 4.1]. Like all other kinds of knowledge, encodings derived from multiple alignments are recorded using SP patterns (Section II-B).\nThis multiple alignment may be interpreted as the result of a process of recognition (Section IV-C). The New patterns represent the features of some unknown plant and the Old patterns in columns 1 to 6 represent candidate categories, at several levels of abstraction: species \u2018Meadow Buttercup\u2019 (column 1), genus Ranunculus (column 6), family Ranunculaceae (column 5), and so on.\n3) Information Compression Via the Unsupervised Learning of Grammars: As outlined in [18, Section 3.9.2] and [20, Section 5.1], and described more fully in [18, Chapter 9], the SP system may, without assistance from a \u201cteacher\u201d or anything equivalent, derive one or more plausible grammars from a body of New patterns, with minimum length encoding as a guiding principle. In that process, multiple alignment has a central role as a source of SP patterns for possible inclusion in any grammar (Section V-B1).\n4) Heuristic Search: Like most problems in artificial intelligence, each of the afore-mentioned problems\u2014finding good full and partial matches between patterns, finding or constructing good multiple alignments, and inferring one or more good grammars from a body of data\u2014is normally too complex to be solved by exhaustive search.\nWith intractable problems like these, it is often assumed that the goal is to find theoretically ideal solutions. But with these and most other AI problems, \u201cThe best is the enemy of the good\u201d. By scaling back one\u2019s ambitions and searching for \u201creasonably good\u201d solutions, it is often possible to find\n8The main advantages are [18, Section 3.10.3.1]: 1) That it can match arbitrarily long sequences without excessive demands on memory; 2) For any two sequences, it can find a set of alternative matches (each with a measure of how good it is) instead of a single \u2018best\u2019 match; 3) The \u2018depth\u2019 or thoroughness of the searching can be controlled by parameters.\n9The concept of multiple alignment in the SP system has been borrowed and adapted from that concept in bioinformatics.\n10Specifically, the New patterns in this example are \u2018has_chlorophyll\u2019 (a pattern with one symbol), \u2018<stem> hairy </stem>\u2019, \u2018<petals> yellow </petals>\u2019, \u2018<stamens> numerous </stamens>\u2019, and \u2018<habitat> meadows </habitat>\u2019. The patterns in a set like that may be presented to the system and processed in any order.\nsolutions that are useful, and without undue computational demands.\nAs with other AI applications, and as with the building of multiple alignments in bioinformatics, the SP70 model uses heuristic techniques in all three cases mentioned above. This means searching for solutions in stages, with a pruning of the search tree at every stage, guided by measures of success [18, Appendix A; Sections 3.9 and 3.10; Chapter 9]. With these kinds of techniques, acceptably good approximate solutions can normally be found without excessive computational demands and with \u201cbig O\u201d values that are within acceptable limits."}, {"heading": "D. Multiple Alignment and the Representation and Processing of Diverse Kinds of Knowledge", "text": "The expressive power of SP patterns within the multiple alignment framework derives in large part from the way that symbols in one pattern may serve as links to one or more other patterns or parts thereof. One of several examples in Figure 2 is how the pair of symbols \u2018<family> ... </family>\u2019 in column 6 serves to identify the pattern \u2018<family> ... Ranunculales ... <hermaphrodite> ... poisonous ... </family>\u2019 in column 5.\nIn the figure, these kinds of linkages between patterns mean that the unknown plant (with characteristics shown in column 0) may be recognised at several different levels within a hierarchy of classes: genus, family, order, class, and so on. Although it is not shown in the example, the system supports cross classification.\nIn the figure, the parts and sub-parts of the plant are shown in such structures as \u2018<shoot>\u2019 (column 3), \u2018<flowers>\u2019 (column 5), \u2018<petals>\u2019 (column 6), and so on.\nAs in conventional systems for object-oriented design, the system provides for inheritance of attributes (Section IV-F). But unlike such systems, there is smooth integration of class hierarchies and part-whole hierarchies, without awkward inconsistencies [19, Section 4.2.1].\nMore generally, SP patterns within the multiple alignment framework provide for the representation and processing of a wide variety of kinds of knowledge including: the syntax and semantics of natural language; class hierarchies and part-whole hierarchies (as just described); networks and trees; entityrelationship structures; relational knowledge; rules and several kinds of reasoning; patterns and pattern recognition; images; structures in three dimensions; and procedural knowledge. There is a summary in [22, Section III-B], and more detail in Section IV.\nE. Information Compression, Prediction, and Probabilities\nOwing to the close connection between information compression and concepts of prediction and probability [9], the SP system is fundamentally probabilistic. Each SP pattern has an associated frequency of occurrence and probabilities may be calculated for each multiple alignment and for any inference that may be drawn from any given multiple alignment."}, {"heading": "F. SP-Neural", "text": "Part of the SP theory is the idea, described most fully in [18, Chapter 11], that the abstract concept of a pattern in the SP theory may be realised more concretely in the brain with a collection of neurons in the cerebral cortex called a pattern assembly.\nThe word \u201cassembly\u201d has been adopted in this term because the concept is quite similar to Hebb\u2019s [5] concept of a cell assembly. The main difference is that the concept of pattern assembly is unambiguously explicit in proposing that the sharing of structure between two or more pattern assemblies is achieved by means of \u2018references\u2019 from one structure to another, as described and discussed in [18, Section 11.4.1]). Another difference relates to one-trial learning, as outlined in Section V-C.\nFigure 3 shows schematically how pattern assemblies may be represented and inter-connected with neurons. Here, each pattern assembly, such as \u2018< NP < D > < N > >\u2019, is represented by the sequence of symbols of the corresponding SP pattern. Each symbol, such as \u2018<\u2019 or \u2018NP\u2019, would be represented in the pattern assembly by one neuron or a small group of inter-connected neurons.11 Apart from the interconnections amongst pattern assemblies, the cortex in SPneural is somewhat like a sheet of paper on which knowledge may be written in the form of neurons.\nFigure 11.2 in [18], with permission.\nIt is envisaged that any pattern assembly may be \u2018recognised\u2019 if it receives more excitatory inputs than rival pattern assemblies, perhaps via a winner-takes-all mechanism [18,\n11As indicated in the caption to the figure, a more detailed representation would show lateral connections within each pattern assembly and inhibitory connections elsewhere. There is relevant discussion in [18, Sections 11.3.3 and 11.3.4].\nSection 11.3.4]. And, once recognised, any pattern assembly may itself be a source of excitatory signals leading to the recognition of higher-level pattern assemblies."}, {"heading": "G. Distinctive Features and Apparent Advantages of the SP Theory", "text": "Information compression and concepts of probability are themes in other research, including research on Bayesian inference, Kolmogorov complexity, deep learning, artificial neural networks, minimum length encoding, unified theories of cognition, natural language processing and more. The main features that distinguish the SP theory from these other areas of research, and apparent advantages compared with these other approaches, are:\n\u2022 Simplification and integration. As mentioned above, the SP theory is a unique attempt to simplify and integrate concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition:\n\u2013 The canvass is much broader than it is, for example, in \u201cunified theories of cognition\u201d. It has quite a lot to say, for example, about the nature of mathematics [16], [18, Chapter 10]. \u2013 In terms of achievement, not merely aspiration, the SP computer model combines simplicity with the ability to model a wide range of concepts and phenomena in computing and cognition (Section IV). \u2013 The provision of one simple format for knowledge and one framework for the processing of knowledge promotes seamless integration of diverse structures and functions.\n\u2022 The SP theory is a theory of computing. Most other research is founded on the idea that computing may be understood in terms of the Universal Turing Machine or equivalent models such as Lamda Calculus or Post\u2019s Canonical System. By contrast, the SP theory is itself a theory of computing [18, Chapter 4]. \u2022 Intelligence. What is distinctive about the SP theory as a theory of computing is that it provides much of the human-like intelligence that is missing from earlier models.12 \u2022 Information compression via the matching and unification of patterns. In trying to cut through the complexity of some other approaches, the SP research programme focusses on a simple, \u2018primitive\u2019 idea: that information compression may be understood as a search for patterns that match each other, with the merging or \u2018unification\u2019 of patterns that are the same (Section II-C1). \u2022 Multiple alignment. More specifically, information compression via the matching and unification of patterns provides the basis for a concept of multiple alignment, outlined above (Section II-C2). Developing this idea as a framework for the simplification and integration of\n12Although Alan Turing saw that computers might become intelligent [13], the Universal Turing Machine, in itself, does not tell us how! The SP theory, as it is now, goes some way towards plugging the gap, and has potential to do more.\nconcepts across a broad canvass has been a major undertaking. Multiple alignment is a powerful and distinctive idea in the SP programme of research. \u2022 Transparency in the representation and processing of knowledge. By contrast with sub-symbolic approaches to artificial intelligence, and notwithstanding objections to symbolic AI,13 knowledge in the SP system is transparent and open to inspection, and likewise for the processing of knowledge. \u2022 SP-neural. As outlined in Section II-F, the SP theory includes proposals\u2014SP-neural\u2014for how abstract concepts in the theory may be realised in terms of neurons and neural processes. The SP-neural proposals are significantly different from artificial neural networks as commonly conceived in computer science, and arguably more plausible in terms of neuroscience."}, {"heading": "III. COMPUTATIONAL EFFICIENCY, THE USE OF ENERGY, AND THE BULKINESS OF COMPUTERS", "text": "With today\u2019s computers and related technologies, it would be difficult or impossible to make autonomous robots with anything approaching human-like versatility and adaptability:\n\u201cThe human brain is a marvel. A mere 20 watts of energy are required to power the 22 billion neurons in a brain that\u2019s roughly the size of a grapefruit.14 To field a conventional computer with comparable cognitive capacity would require gigawatts of electricity and a machine the size of a football field. ... Unless we can make computers many orders of magnitude more energy efficient, we\u2019re not going to be able to use them extensively as our intelligent assistants.\u201d [7, p. 75, p. 88].\nWith AI and computer technologies as they are now, any computer that may approach human levels of intelligence would be too big to be mobile, and likewise for its power supply. And it seems that Moore\u2019s Law is unlikely to solve the problem of computational power, since that decades-long growth in the power of computer processors may tail off around 2020.15\nNo doubt, there are many gains in efficiency that can be made with the von Neumann model of computing. But something radically different is likely to be needed to \u201cmake computers many orders of magnitude more energy efficient\u201d and to achieve human-like intelligence with a computational device (with its power supply) that is portable.\nThe next three subsections describe how the SP system may help to achieve what is needed, firstly via compression of information, secondly via the exploitation of statistical information with heuristic techniques, and thirdly via a computer\n13See, for example, \u201cHubert Dreyfus\u2019s views on artificial intelligence\u201d, Wikipedia, bit.ly/1hGHVm8, retrieved 2014-08-19.\n14Another estimate is that, normally, the brain\u2019s power consumption is about 12.6 watts (\u201cDoes thinking really hard burn more calories?\u201d, Scientific American, 2012-07-18, bit.ly/1qJmCBG), and there may be as many as 86 billion neurons in the human brain [6].\n15See, for example, \u201cIntels former chief architect: Moore\u2019s law will be dead within a decade\u201d, ExtremeTech, 2013-08-30, bit.ly/1niX9iK. But for a contrary view, see \u201cCarbon nanotubes could step in to sustain Moore\u2019s Law\u201d, MIT Technology Review, 117 (5), 17, 2014, bit.ly/1nd49tD.\narchitecture that is inspired in part by Hebb\u2019s [5] concept of a \u201ccell assembly\u201d, as outlined in Section II-F. These proposals are an updated version of what\u2019s in [22, Section IX], with a shift of emphasis in Section III-C3.16\nApart from the concept of \u201cdata-centric\u201d computing (referenced in Section III-C3 and described in, for example, [7]), the proposals here, which all flow from the SP theory, are new contributions to thinking about the design of brains for autonomous robots."}, {"heading": "A. Efficiency Via Compression of Information", "text": "At the heart of the SP system is a process of searching for patterns that match each other. Anything that increases the efficiency of searching is likely to increase the efficiency of computation.\nOne thing that can increase the efficiency of searching is to reduce the size of the information to be searched. Here, the SP system may score because compression of information is central in how it works. Its repository of Old information is likely to be considerably smaller than the New information from which it was derived, so it should be correspondingly easy to search.\nThe SP system may also yield gains in efficiency via its system for creating relatively short codes for larger structures. If a short code can be used as a search term instead of the larger structure that it represents, searching is likely to be more efficient."}, {"heading": "B. Efficiency Via the Exploitation of Statistical Information, With Heuristic Techniques", "text": "Continuing with the theme that anything that increases the efficiency of searching is likely to increase the efficiency of computation:\n\u201cIf we want to find some strawberry jam, our search is more likely to be successful in a supermarket than it would be in a hardware shop or a car-sales showroom.\u201d [22, section IX-A.2].\nThis seemingly-trite observation captures the essentials of the present proposal: instead of searching blindly for patterns that match each other, the system may use statistical knowledge, with heuristic techniques (Section II-C4), to improve efficiencies.\nIn this efficiency-via-statistics concept, there is no need for any special process to gather statistical information. That information is collected because information compression is central in how the SP system works, and because of the intimate relation that exists between information compression and concepts of prediction and probability [9]. Those two things mean that the SP system, in the course of its operation, builds a statistical model of its data.\nTo flesh out this concept, it would probably be useful to examine it in quantitative terms, perhaps using the highparallel version of the SP machine mentioned in Section II.\n16In the SP computer model as it stands now, the use of heuristic techniques (Section II-C4) is chiefly what allows the model to produce useful results with problems that would otherwise be intractable. But there appears to be considerable untapped potential for improvements in efficiency via the three approaches mentioned in the text.\nBut in any study of that kind, it would be important to bear in mind that the effect of the concept in terms of computational efficiency is likely to depend on the computer architecture\u2014 real or simulated\u2014in which it is applied. And to reap the full benefit of the concept, it will probably be necessary to apply it with an architecture that is quite different from the von Neumann model. One such possibility is outlined in the next subsection."}, {"heading": "C. Efficiency Via Pattern Assemblies", "text": "It is unlikely that problems of efficiency will be fully solved with robot brains in the von Neumann style. As argued in [7, p. 9], \u201cWhat\u2019s needed is a new architecture for computing, one that takes more inspiration from the human brain.\u201d The SP system has potential as a foundation for that new architecture.\nSP-neural, the neural-inspired version of the SP theory outlined in Section II-F, may help to promote computational efficiency in three main ways, as described in the next three subsections.\n1) Exploiting Statistical Information in SP-Neural: SPneural suggests one possible means of realising the idea, outlined in Section III-B, that computational efficiency may be enhanced by taking advantage of the statistical information that the SP system gathers as an integral part of how it works.\nIt envisaged that, associated with each pattern assembly, will be some physiological analogue of the frequency of occurrence that is associated with the corresponding SP pattern (Section II-E, [18, Section 11.3.5]). We may suppose that, in the course of building neural analogues of multiple alignments, those physiological measures of frequency may serve to derive physiological analogues of the probabilities associated with multiple alignments, so that \u2018good\u2019 multiple alignments may be distinguished from \u2018bad\u2019 ones.\nWith that kind of mechanism, processing in SP-neural would be guided by statistical information as suggested in Section III-B. And since low-probability structures would be continually eliminated, we may suppose that much unnecessary processing would be avoided, with corresponding gains in computational efficiency.\n2) Cutting Out Searching in SP-Neural: A potentiallyimportant feature of SP-neural is that the hard-wired connections between pattern assemblies are always between symbols that match each other\u2014and that can cut out a lot of searching. For example, the direct connection that can be seen in Figure 3 between \u2018D\u2019 in \u2018< D 0 t h i s >\u2019 and \u2018D\u2019 in \u2018< NP < D > < N > >\u2019 means that there is no need for any kind of searching to find the match between those two instances of \u2018D\u2019.\nSince, as was noted in Section III-B, \u201canything that increases the efficiency of searching is likely to increase the efficiency of computation\u201d, these hard-wired connections in SP-neural should mean increases in computational efficiency. These gains in computational efficiency are potentially large.\n3) Data-Centric Computing in SP-Neural: In [22, Section IX-B], it is suggested that the SP system may promote computational efficiency via a synergy with \u201cdata-centric\u201d computing, meaning computing in which \u201cdata processing\n[is] distributed throughout the computing system rather than concentrated in a CPU.\u201d [7, p. 9] and \u201cthe processing and the memory [is] closely integrated to reduce the shuttling of data and instructions back and forth.\u201d (ibid.).\nThat suggestion is not exactly wrong since, in the SP system, there is indeed a close integration of data and processing. But it needs to be qualified by two observations: 1) that there is really no place in the SP system for the concept of \u201cinstruction\u201d; and 2) that the process of finding matches between different portions of data may require transfers of information over relatively long distances. Regarding that last point, we cannot avoid such transfers by retreating to conventional architectures. It appears to be an unavoidable aspect of any system that aspires to human-like intelligence."}, {"heading": "D. Making Computers Smaller and Lighter", "text": "If computational efficiencies can be increased, as outlined above, it seems likely that there could be corresponding reductions in the size and weight of computers. The development of SP-neural as a model for computing may also lead to reductions in the bulkiness of computers."}, {"heading": "IV. TOWARDS HUMAN-LIKE VERSATILITY IN INTELLIGENCE", "text": "The current generation of robots fall far short of human-like versatility in intelligence. If they have anything approaching human-like intelligence it is almost always in a narrow field such as driving a car17 or playing pool [4].\nThis section aims to demonstrate how the SP system may promote human-like versatility in autonomous robots in several areas. Unsupervised learning is the main focus of Section V, while other aspects of intelligence are considered in Sections IV-A to IV-H below.\nVersatility in intelligence\u2014a major strength of the SP system\u2014flows from the goal that has been central in the development of the theory: to combine conceptual simplicity with descriptive and explanatory power. This strength of the SP system chimes well with what is required in any autonomous robot that is to function effectively in situations where little or no help can be provided by people."}, {"heading": "A. Simplification and Integration", "text": "Before getting on to specifics, we shall consider simplification and integration, and their importance in the design of autonomous robots. In that connection, some may argue that human-like versatility could be achieved with a collection of applications, each one dedicated to a particular aspect of intelligence. That being so, the argument may run, there is no need for an all-in-one solution like the SP system. But human intelligence is not like that:\n\u2022 Each of our concepts is, normally, a seamlessly-integrated amalgam of different kinds of knowledge. For example, most people\u2019s concept of a \u201csteam engine\u201d includes static and moving images, sounds and smells, associations with\n17See, for example, \u201cAutonomous car\u201d, Wikipedia, bit.ly/QKn6dg, retrieved 2014-10-28.\njourneys by steam train, literary, historical and technical knowledge, and more. \u2022 There is smooth inter-working of different aspects of human intelligence\u2014learning, recognition, reasoning, problem-solving, and so on\u2014without artificial barriers or transitions.\nBy contrast, a collection of AI-related applications would be merely a kludge that is likely to suffer from poor integration of knowledge structures and awkward incompatibilities between different subsystems.\nThe key difference between the SP system and that kind of assemblage of AI-related applications is its central organising principle: simplification and integration of concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition. There is potential in that principle for autonomous robots to achieve the kind of seamless integration of diverse kinds of knowledge and diverse kinds of processing that is a hallmark of human intelligence.\nThe following three subsections expand on aspects of the principle and its apparent importance in the design of autonomous robots.\n1) Simplification of Structures and Functions: Simplification in the SP system flows from two main things:\n\u2022 The adoption of one simple format\u2014SP patterns\u2014for the representation of all kinds of knowledge. \u2022 One computational framework, with multiple alignment centre-stage, for all kinds of processing.\nLike a database management system or a \u2018shell\u2019 for an expert system, the SP system provides one framework that can be loaded with different bodies of knowledge according to need. As with database management systems (DBMSs) and expert-system shells, this cuts out the need to recreate the framework for each new application, meaning that there can be a substantial overall simplification across a range of applications [23, Section 5]. The main difference between the SP system and any DBMS or expert-system shell is the versatility of the multiple alignment framework, especially in AI-related functions.\nIn the design of autonomous robots, this kind of simplification in software may have some impact on the bulkiness of robot brains. But potentially more important is how simplification of software may simplifying the creation, operation, and management of software, with corresponding gains in efficiency. Even if the creation, operation, and management of software were to be fully automated\u2014as is envisaged for the SP system\u2014gains in efficiency are potentially significant.\n2) Integration of Structures and Functions: Using one simple format for all kinds of knowledge and one computational framework for all kinds of processing is likely to yield a second benefit: the afore-mentioned seamless integration of diverse kinds of knowledge and smooth inter-working of different functions. Here are some putative examples:\n\u2022 Syntax and semantics. It is clear that in the understanding of any natural language (listening or reading) and the production of language (speaking or writing), there must be close integration and inter-working of the syntactic forms of language and what they mean. Achieving that intimate\nrelationship between syntax and semantics is likely to be made easier by using what the SP system provides: one simple format for both syntax and semantics and one computational framework for all kinds of knowledge. In support of that idea, it is known that, at least in English, some aspects of syntax cannot be defined except with reference to semantics [23, Section 6.2]. \u2022 Recognition and learning. Although recognition and learning may be treated as distinct topics in text books, they are difficult to separate in practice. Consider, for example, how a dog chases and catches a ball that is bouncing haphazardly across uneven ground. The dog must, of course, recognise the ball but, after every bounce, he or she must be constantly assimilating new information about the speed and the direction of travel of the ball. The SP system may facilitate that kind of close integration of recognition and learning by providing one simple format for knowledge and one computational framework for both recognition and learning. \u2022 Knowledge representation and learning. If lightning is represented in several different ways and likewise with thunder then, as noted in [22, Section III-A.1], it would be difficult or impossible to learn, without being told, that there is an association between those two things. For a learning system to detect the way in which lightning is normally followed by thunder, it seems necessary to get behind the variability of surface forms and derive new structures and associations from the underlying knowledge, expressed in some kind of universal framework for the representation and processing of diverse kinds of knowledge (UFK). Likewise for other structures and associations. \u2022 Knowledge representation and reasoning. In any kind of criminal investigation\u2014the subject of countless television dramas\u2014there must be total flexibility to use any kind of knowledge\u2014physical, chemical, social, psychological, legal, and so on\u2014to narrow the field of suspects and find \u201cwhodunnit\u201d. It is difficult to see how that flexibility may be achieved except by the use of a uniform format for all kinds of knowledge and one computational framework for the processing of those diverse kinds of knowledge.\nIn general, the SP system provides for seamless integration of structures and functions, in any combination, in all the areas described in Sections IV-B to IV-H.\n3) Deeper Insights and Better Solutions to Problems: The quest for simplification and integration in the SP theory accords with Occam\u2019s Razor, one of the most widely-accepted principles in science. In terms of that principle, the SP theory scores well, since a relatively simple framework provides an account of a wide range of concepts and phenomena.\nAn often-repeated observation in science is that a good theory can yield \u2018deeper\u2019 insights and better solutions to problems than would otherwise be possible. For example, Einstein\u2019s theory of general relativity led to the prediction\u2014 confirmed by observation\u2014that light would be bent by gravity, and it provided an account of the precession of the perihelion of Mercury.\nThe SP theory is beginning to show benefits of that kind:\n\u2022 Relatively new insights are the ways in which computational efficiency may be improved, with corresponding savings in the use of energy, via aspects of the SP system described in Section III. \u2022 Other insights, summarised in [23, Section 6], including: a new and apparently improved method for encoding discontinuous dependencies in syntax; seamless integration of class-inclusion hierarchies and part-whole hierarchies; the use of one framework for both the analysis and production of knowledge; and benefits for relational, object-oriented, and network models for databases."}, {"heading": "B. Natural Language Processing", "text": "This and the following subsections, together with Section V (human-like adaptability), demonstrate some of the versatility of the SP system in areas that are likely to prove useful in autonomous robots.\nIn addition to the learning of linguistic knowledge (Section V-D), the SP system has strengths in the parsing of natural language, the production of natural language, and the integration of syntactic and semantic knowledge, as outlined in this section. These aspects of the system are described more fully in [20, Section 8] and in [18, Chapter 5].\n1) Parsing of Natural Language: Figure 4 shows how, via multiple alignment, a sentence (in row 0) may be parsed in terms of grammatical structures including words (rows 1 to 8).18 It also shows, in row 8, how the system may mark the syntactic dependency between the plural subject of the sentence (\u2018Np\u2019) and the plural main verb (\u2018Vp\u2019) (see also [18, Sections 5.4 and 5.5], [20, Section 8.1]).\nTo create a multiple alignment like the one in the figure, the system needs a grammar of Old patterns, like those shown, one per row, in rows 1 to 8 of the figure. In this example, the patterns represent linguistic structures including words.\nAlthough SP patterns are remarkably simple, it appears that, within the multiple alignment framework, they have at least the expressive power of a context-sensitive grammar [18, Sections 5.4 and 5.5]. As previously noted (Section II-B), there is reason to believe that all kinds of knowledge may be represented, within the multiple alignment framework, by SP patterns.\n2) Production of Natural Language: A neat feature of the SP system is that one set of mechanisms and processes may achieve both the analysis or parsing of natural language (Section IV-B1) and the generation or production of sentences. This is explained in [18, Section 3.8] and [20, Section 4.5].\n3) The Integration of Syntax and Semantics: The use of one simple format for all kinds of knowledge is likely to facilitate the seamless integration of syntax and semantics. Preliminary examples of how this may be done are shown in [18, Section 5.7], both for the derivation of meanings from surface forms [18, Figure 5.18] and for the production of surface forms from meanings [18, Figure 5.19].\n18Compared with the multiple alignment shown in Figure 2, this multiple alignment is rotated through 90\u25e6, replacing columns with rows. The choice between these two styles, which are equivalent, depends largely on what fits best on the page.\n4) Parallel Streams of Information: Up to now, most work on natural language within the SP research programme has made the simplifying assumption that natural language may be represented with a sequence of symbols, as in ordinary text. But this 1D assumption does not sit easily with some aspects of natural language:\n\u2022 Vowel sounds, for example, may be analysed into formants, two or more of which may occur simultaneously. Vowels, and perhaps other elements of speech, may be represented most naturally with parallel streams of information. \u2022 It does not seem right that the syntactic and semantic aspects of natural language should be forced into the procrustean bed of a single sequence. As with formants in speech, it seems most natural to regard syntax and semantics as parallel streams of information.\nThe way in which parallel streams of information may be represented and processed in the SP system is described in Appendix C."}, {"heading": "C. Pattern Recognition", "text": "As described quite fully in [18, Chapter 6] and more briefly in [20, Section 9], the SP system has strengths in several aspects of pattern recognition:\n\u2022 It can recognise patterns at multiple levels of abstraction, with the integration of class-inclusion relations and partwhole relations, as shown in the example in Figure 2. \u2022 It can model \u201cfamily resemblance\u201d or polythetic categories, meaning that recognition does not depend on the presence absence of any particular feature or combination of features. \u2022 Recognition is robust in the face of errors of omission, commission or substitution in the New pattern or patterns. \u2022 For any given identification, or any related inference, the SP system may calculate associated probabilities. \u2022 As a by-product of how recognition is achieved via the building of multiple alignments, the system provides a model for the way in which context may influence recognition.\nThe SP system also provides a framework for medical diagnosis via pattern recognition, with potential for diagnosis via causal reasoning [17], [18, Section 6.5].\nD. Information Storage and Retrieval\nThe SP system may serve as a database system with \u2018intelligence\u2019 [19], [18, Chapter 6]. Although this may seem somewhat removed from the world of autonomous robots, any such robot will need such \u2018database\u2019 functions as storing information and retrieving it. Apart from aspects of intelligence (as outlined elsewhere in this paper), the main strengths of the SP system are:\n\u2022 The system lends itself to information retrieval in the manner of query-by-example. There is also potential for information retrieval via the use of natural language or query languages such as SQL. \u2022 As outlined in Section II-D, the system supports objectoriented concepts such as class hierarchies (including cross-classification), and inheritance of attributes, and it provides for the representation of part-whole hierarchies and their seamless integration with class hierarchies. The system also supports network, relational, and entityrelationship database models.\nE. Vision\nWith generalisation of the SP system to accommodate 2D patterns, it has potential to model several aspects of natural vision and to facilitate the development of human-like abilities in robot vision [21]. In these connections, the main strengths and potential of the SP system are:\n\u2022 Low level perceptual features such as edges or corners may be identified via the multiple alignment framework by the extraction of redundancy in uniform areas in the manner of the run-length encoding technique for information compression [21, Section 3]. \u2022 The system may be applied in the recognition of objects and in scene analysis, with the same strengths as in pattern recognition (Section IV-C). \u2022 There is potential for the learning of visual entities and classes of entity (Section V-E) and the piecing together of coherent concepts from fragments [21, Section 5.4]. \u2022 There is potential, via multiple alignment, for the creation of 3D models of objects and of a robot\u2019s surroundings (Section V-F).\n\u2022 The SP theory provides an account of how we may see things that are not objectively present in an image, how we may recognise something despite variations in the size of its retinal image, and how raster graphics and vector graphics may be unified. \u2022 And the SP theory has things to say about the phenomena of lightness constancy and colour constancy, ambiguities in visual perception, and the integration of vision with other senses and other aspects of intelligence."}, {"heading": "F. Reasoning", "text": "As described in quite fully in [18, Chapters 7 and 10, Section 6.4] and more selectively in [20, Section 10], the SP system lends itself to several kinds of reasoning:\n\u2022 One-step \u2018deductive\u2019 reasoning. \u2022 Abductive reasoning. \u2022 Reasoning with probabilistic decision networks and de-\ncision trees. \u2022 Reasoning with \u2018rules\u2019. \u2022 Nonmonotonic reasoning and reasoning with default val-\nues. \u2022 Reasoning in Bayesian networks, including \u201cexplaining\naway\u201d. \u2022 Causal diagnosis. \u2022 Reasoning which is not supported by evidence. \u2022 Inheritance of attributes in an object-oriented class hier-\narchy or heterarchy.\nIn keeping with the remarks about integration in Section IV-A2, these several kinds of reasoning may work together seamlessly without awkward incompatibilities, and likewise for how they may integrate seamlessly with such AI functions as unsupervised learning, pattern recognition, and so on.\nFor any given inference reached via any of these kinds of reasoning, the SP system may calculate associated probabilities (Section II-E). Although the system is fundamentally probabilistic, it may imitate the effect of logic and other \u2018exact\u2019 forms of reasoning [18, 10.4.5].\n1) Spatial Reasoning: If, as seems likely, multiple alignment provides a means for an autonomous robot to build a 3D model of objects and of its surroundings (Section V-F), this may open the door to spatial reasoning. There is potential, for example, for a robot to explore \u2018mentally\u2019 how furniture may be arranged in a room, much as people sometimes use cardboard shapes representing furniture, with a plan of a room, to work out how things may be fitted together.\n2) What-If Reasoning: Although a flight simulator is not normally regarded as a system for reasoning, it provides a very effective means of exploring what may happen if, for example, a plane loses power in one of its engines or if there is ice on the wings.\nSimilar things may apply with knowledgeable robots. In view of the versatility of the SP system in processing knowledge of various kinds (Section II-D), and in view of the system\u2019s capabilities and potential in reasoning, mentioned above, there is potential for the system to explore what-if scenarios arising from this or that hypothetical contingency."}, {"heading": "G. Planning and Problem Solving", "text": "With data about flights between different cities, represented using SP patterns, the SP computer model may find a route between any two cities (if such a route exists) and, if there are alternative routes, it may find them as well [18, Section 8.2].\nProvided they are translated into textual form, the SP70 computer model can solve geometric analogy problems of the kind found in puzzle books and some IQ tests [18, Section 8.3], [20, Section 12]."}, {"heading": "H. Sequential and Parallel Procedures", "text": "Although it may not seem obvious at first sight, the multiple alignment framework can model several devices used in ordinary procedural programming, including: procedure, function, or subroutine; variable, value and type; function with parameters; conditional statement; and the means of repeating operations such as repeat ... until or do ... while [23, Section 6.6.1]. In accordance with good practice in software engineering, the SP system facilitates the integration of \u2018programs\u2019 with \u2018data\u2019. And as previously noted (Section II-D), the SP system supports object-oriented concepts such as class hierarchies with inheritance of attributes.\nIn [23, Section 6.6.3], it is suggested that, since SP patterns at the \u2018top\u2019 level are independent of each other, they may serve to model processes that may run in parallel. Now it appears that a better option is to model parallel processes as parallel streams of information, represented in 2D patterns, as described in Appendix C. The advantage of this latter scheme is that it provides the means of showing when two or more events occur at the same time, and the relative timings of events.\nWithin the SP system, these structures and mechanisms may serve in the representation and processing of sequential and parallel procedures from the real world such as those required for cooking a meal, organising a party, going shopping, and so on."}, {"heading": "V. TOWARDS HUMAN-LIKE ADAPTABILITY IN INTELLIGENCE", "text": "As with versatility in intelligence (Section IV), the current generation of robots falls far short of human-like adaptability in intelligence. The key to that adaptability is the ability to learn, an aspect of human-like versatility in intelligence (Section IV) but considered here in a separate section because of its importance.\nAs with efficiency in computations and versatility in intelligence, the SP system promises solutions for learning and adaptability that are rather different from others in the field of robotics and with considerable potential for autonomous robots.\nAfter a \u2018preliminaries\u2019 subsection, the main elements of learning in the SP system are described. Subsections that follow describe several aspects of how the SP theory may be applied to learning in autonomous robots. These parts of the paper develop important refinements in the SP theory that are needed to meet the demands of this area of application."}, {"heading": "A. Preliminaries", "text": "Here we consider, first, some forms of learning and then how rewards and punishments (carrots and sticks) may relate to learning. Two subsections that follow outline the research on the learning of a first language that provided part of the inspiration for the SP theory, and the reorganisation that has been needed to meet the goals of the SP research programme.\n1) Forms of Learning: In the preceding parts of this paper, the word \u201clearning\u201d has generally been preceded by the word \u201cunsupervised\u201d. That qualification means that learning occurs without the benefit of any kind of \u201cteacher\u201d, or the grading of learning materials from simple to complex, or the provision of \u201cnegative\u201d examples of concepts to be learned, meaning examples that are marked as \u201cwrong\u201d (cf. [3])\u2014and it also means that learning occurs without rewards or punishments (Section V-A2). Learning with assistance from those kinds of things is, of course, \u201csupervised\u201d learning.\nWe may also distinguish between primary forms of learning\u2014the learning of basic skills\u2014and secondary forms of learning\u2014the kinds of learning that depend on those basic skills. As an example, learning via lessons in schools and colleges may be regarded as a secondary form of learning that depends on such basic skills as being able to speak and to understand speech. Learning by watching and imitating what other people do may also be regarded as a secondary form of learning because it depends on more basic skills such as the ability to interpret visual inputs in terms of people and the motions of their limbs, hands, feet, and so on.\nOf course, there is really a hierarchy of skills and corresponding forms of learning, because a skill such as the ability to read\u2014which would normally be learned as a secondary skill in school\u2014may itself provide a basis for learning from newspapers, magazines, and books, from the internet, and so on.\nThis paper focusses mainly on the learning of foundation skills. And a working hypothesis\u2014with supporting evidence from research on the learning of a first language or languages [15]\u2014is that unsupervised learning is an important driver in both primary and secondary forms of learning.\nAchieving a good theory of unsupervised learning may, arguably, be seen as the \u201cHoly Grail\u201d of research on learning, especially learning by autonomous robots. Notwithstanding the undoubted importance of schools and colleges, it appears that much of what we know is picked up via our everyday experiences, without explicit teaching. Unsupervised learning is what an autonomous robot would need in places like Mars where it can get little or no help from a human teacher. And if we can develop a good theory of unsupervised learning, it should smooth the path in understanding secondary forms of learning.\n2) Carrots, Sticks, and Motivations: In research on human and animal learning, most famously the work of psychologist B. F. Skinner,19 there is a long tradition of linking learning with \u2018reinforcements\u2019, both positive (rewards or \u2018carrots\u2019) and negative (punishments or \u2018sticks\u2019). This relates to Ivan\n19See, for example, \u201cB. F. Skinner\u201d, Wikipedia, bit.ly/X5BJuH, retrieved 2014-09-15.\nPavlov\u2019s research on classical conditioning,20 and the longestablished practice by animal trainers of using small rewards to encourage some kinds of behaviour and mild punishments to discourage others.\nIt is clear from the research, and from successes in the training of animals, that carrots and sticks can be very effective. Motivation is certainly relevant to learning. But in the development of the SP theory, no attempt has been made to say anything about reinforcements, or about related concepts such as motivation. Here are some tentative thoughts:\n\u2022 In view of experimental evidence that learning can occur without reinforcement,21 it seems unlikely that the concept of reinforcement could be central in any comprehensive theory of learning. \u2022 It seems that concepts of reinforcement are unlikely to take us very far in explaining the learning of complex forms of knowledge and behaviour such as natural languages. In that connection, Chomsky\u2019s critique of Skinner\u2019s book on verbal behaviour and how it may be learned [2] is still worth reading, despite the passage of time. \u2022 In animal training, it seems that reinforcement is chiefly a means of communicating to the animal what the trainer would like it to do. In that perspective, reinforcement may help to create new combinations of pre-existing forms of behaviour but would have little or no role in the creation of those pre-existing behaviours. \u2022 An association between a reward (or a punishment) and a particular form of behaviour is just one of many types of redundancy that a person or robot needs to learn. Detecting or discovering that kind of association may be achieved within the SP system via the same mechanisms and processes that serve in the discovery of other kinds of redundancy in information. \u2022 It appears that the principle of minimum length encoding can provide a foundation not only for grammatical inference but more generally for unsupervised learning, for other aspects of perception and cognition, and for secondary forms of learning mentioned in Section V-A1. \u2022 Notwithstanding the foregoing points, there is a clear need to expand our understanding of the relationship between learning and concepts of reinforcement and motivation. Why do children play? (Section V-H); What induces people to spend thousands of hours learning the skills needed for pool, billiards, or snooker? (Section V-I); And so on.\n3) Computer Models of Language Learning: Part of the inspiration for the SP theory has been an earlier programme of research on grammatical inference, developing computer models of the unsupervised learning of a first language or languages. There is an overview of this research in [15]. The main conclusions are:\n\u2022 That the principle of minimum length encoding is of key importance in understanding the unsupervised learning\n20See, for example, \u201cIvan Pavlov\u201d, Wikipedia, bit.ly/1mOEmuY, retrieved 2014-09-15.\n21See, for example, \u201cLatent learning\u201d, Wikipedia, bit.ly/1pluRo3, retrieved 2014-08-16.\nof a first language. In brief, this means that, in inferring a grammar from a body of data, I, we should aim to minimise (G+ E), where G is the size of the grammar, G, derived from I, and E is the size of the encoding of I in terms of G (an encoding that may be referred to as E). \u2022 The \u201cMK10\u201d model, based on the MLE principle, can successfully discover word structure in an unsegmented sample of natural language without any kind of dictionary, except what it creates for itself. With a little human assistance, the MK10 model can discover phrase structure as well. \u2022 The more fully-developed \u201cSNPR\u201d model, based on the MLE principle, can derive a plausible generative grammar from an unsegmented sample of English-like artificial language, including segmental structures, classes of structure, and abstract patterns. \u2022 The principle of minimum length encoding provides an explanation for aspects of language learning that may otherwise be puzzling:\n\u2013 Generalisation from a finite sample. How we can generalise from the finite (albeit large) sample of a given language which is the basis for our learning to a knowledge of the language that embraces an infinite range of utterances,22 \u2013 How over-generalisations may be corrected. It is well known that young children do over-generalise grammatical rules\u2014for example, they may say \u201cbuyed\u201d as the past tense of \u201cbuy\u201d instead of \u201cbought\u201d\u2014but those kinds of over-generalisation disappear later. A possible explanation is that children\u2019s errors are corrected by adults, but there is evidence that the learning of a first language does not depend on the correction of errors by adults, or anything equivalent. The principle evidence is that children with a physical handicap that prevents them speaking intelligibly may, nevertheless, learn to comprehend their native language successfully [1], [8]. Since they say little or nothing that adults can understand, there is little that adults can do to correct any errors. \u2013 Learning from dirty data. People can distinguish sharply between utterances that belong in their native language and those that don\u2019t, and this despite the fact that, normally, much of the language that we hear as children is not grammatically correct. As before, there is evidence that children can learn their native language without the benefit of errorcorrection by adults or anything equivalent [1], [8].\nIn summary, grammars that minimise (G + E) are ones that generalise without over-generalising and that filter out haphazard errors. Systematic \u2018errors\u2019 (ones that are not haphazard) are likely to acquire the status of dialect\n22That a natural language like English embraces an infinite range of utterances can be seen from recursive structures like This is the cow with the crumpled horn That tossed the dog that worried the cat That chased the rat that ate the cheese .... Native speakers know that there is, in principle, no limit on the length of such sentences, or their variety.\nforms and thus lose their status as errors. There is more detail about these points in [20, Section 5.3]. Incidentally, the Chomskian argument that children learn via an inborn knowledge of \u201cuniversal grammar\u201d does not bear scrutiny because it depends on the still unproven idea that there is substantial structure that is shared by all the world\u2019s languages and it is vague about how a child might learn the specifics of his or her native language.\nAs we shall see in Section V-K, these ideas appear to have some useful things to say about how learning can or should be developed in autonomous robots.\n4) Reorganisation Needed to Meet the Goals of the SP Research Programme: In their broad organisation, the MK10 and SNPR computer models may be roughly characterised as hierarchical chunking models, building a knowledge of recurrent \u2018chunks\u2019 of information via hierarchies of smaller elements. But although this organisation was quite successful in modelling aspects of language learning, it proved to be quite unsuitable for the goals of the SP research programme: simplification and integration of concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition.\nThe multiple alignment framework that has been developed to meet those goals is quite different from the MK10 and SNPR models, but the principle of minimum length encoding remains centre-stage. The multiple alignment framework is much more general: it can model hierarchical chunking where that is appropriate but it can also model several other kinds of structure as well (Section II-D).\nGiven that the SP research grew out of earlier research on language learning, it is pertinent to ask how the SP computer model performs in that area of application? In brief, the answer is \u201cmuch the same as earlier models but still not as well as one might wish\u201d. More specifically, the SP computer model, like the SNPR model, can derive plausible generative grammars from samples of English-like artificial languages, including segmental structures, classes of structure, and abstract patterns [18, Chapter 9].\nAt present, the main shortcoming of the SP computer model with respect to learning is that it does not detect intermediate levels of structure and it cannot learn discontinuous dependencies in natural language syntax [20, Section 3.3]. But, as previously noted (ibid.), I believe these problems are soluble and that solving them will greatly enhance the capabilities of the system for the unsupervised learning of structure in data. Apart from these developments, the SP computer model also needs to be generalised to work with patterns in two dimensions (ibid.).\nAlthough, in the learning of syntactic structures, the SP computer model does much the same as earlier models, it has two decisive advantages: the integration of learning with other aspects of intelligence; and the potential to learn any of the kinds of knowledge described in Sections IV-B to IV-H, not merely syntactic kinds of knowledge."}, {"heading": "B. Unsupervised Learning in the SP System", "text": "Unsupervised learning by SP70 is described in outline in [20, Section 5] and [18, Section 3.9.2], and in more detail\nin [18, Section 9.2]. Here, the main features of the learning process are described as a basis for the proposals, in Sections V-D to V-L, about how the same kinds of learning processes may be applied in autonomous robots.\nIn broad terms, the SP70 model processes a set of New patterns (which may be referred to as I) in two main phases:\n1) Create a set of Old patterns that may be used to encode I. 2) From the Old patterns created in the first phase, compile one or more alternative grammars for the patterns in New, in accordance with principles of minimum length encoding.\nAs noted in Section II-C4, the process of inferring one or more good grammars from a body of data is normally too complex to be solved by exhaustive search. But, via the use of heuristic techniques (as outlined in Section V-B2, below), it is normally possible to find reasonably good solutions without undue computational demands.\nAlthough the two phases just described have the flavour of batch processing, they may be adapted for an incremental style of working: processing New information as it is received and building collections of Old patterns that may serve in the economical encoding of New patterns that are received later.\nThe two phases are described in a little more detail in the following to subsections.\n1) Creating Candidate Patterns: In SP70, candidate patterns for inclusion in the repository of Old patterns are derived from multiple alignments like the one shown in Figure 5. Here, the pattern shown in row 1 is an analogue of something that a child has heard (\u2018t h a t b o y r u n s\u2019) with the addition of code symbols \u2018<\u2019, \u2018%1\u2019, \u20189\u2019, and \u2018>\u2019, while the pattern in row 0 (\u2018t h a t g i r l r u n s\u2019) is an analogue of something that the same child has heard later. The letters are analogues of symbols in speech (see Appendix A).\nFrom that multiple alignment, the program derives the patterns \u2018t h a t\u2019 and \u2018r u n s\u2019 from subsequences that match each other, and it derives \u2018g i r l\u2019 and \u2018b o y\u2019 from subsequences that don\u2019t match. In addition, the program assigns code symbols to the newly-created patterns so that \u2018t h a t\u2019 becomes \u2018< %7 12 t h a t >\u2019, \u2018r u n s\u2019 becomes \u2018< %8 13 r u n s >\u2019, and so on. And, using those code symbols, the program creates an abstract pattern, \u2018< %10 16 < %7 > < %9 > < %8 > >\u2019, that records the whole sequence.\nThe overall result in this example is the set of patterns shown in Figure 6. This is essentially a simple grammar for sequences of the form \u2018t h a t g i r l r u n s\u2019 and \u2018t h a t b o y r u n s\u2019.\n2) Compiling Alternative Grammars: The example just described shows how SP70 creates candidate patterns and\ngrammars via partial matching but the tidiness of the multiple alignment in Figure 5 and of the grammar shown in Figure 6 may be misleading. In practice, the system creates many multiple alignments that are less neat than the one shown and many candidate grammars that are intuitively \u2018wrong\u2019. But the wrong grammars are progressively weeded out, as described next.\nAs noted in Section II-C4, the system exploits principles of heuristic search to cope with complexity: for any given body of New patterns, I, the system explores the abstract space of possible grammars in stages; at each stage, in accordance with the principle of minimum length encoding (Section V-A3), each candidate grammar is evaluated in terms of the size of (G+E); and, at each stage, grammars that perform poorly are discarded. In this way, the system may gradually develop one or more Gs that perform reasonably well in terms of (G+E), and it may achieve these results without unreasonable demands on computational resources.\nThe patterns in a successful grammar are ones that express redundancy (repetition of information) in I. As a rough generalisation, these are ones that occur frequently, or are relatively large in terms of the amount of I that they may encode, or both these things. Exceptions to that rule are patterns that play supporting roles.\nAs a general rule\u2014the DONSVIC principle described in [20, Section 5.2]\u2014grammars that minimise (G+ E) are also ones that appear \u2018natural\u2019 to people."}, {"heading": "C. One-Trial Learning", "text": "This subsection and the ones that follow discuss aspects of how the SP system may be applied to learning in autonomous robots.\nUnsupervised learning in the SP theory [18, Chapter 9], [20, Section 5] is quite different from \u201cHebbian learning\u201d\u2014 gradual strengthening of links between neurons\u2014that is widely adopted in the kinds of artificial neural networks that are popular in computer science. By contrast with Hebbian learning, the SP system, like a person, may learn from a single exposure to some situation or event.23 And, by contrast with Hebbian learning, it takes time to learn a language in the SP system (Section V-D), or to learn the skills needed for games like pool, billiards, or snooker (Section V-I), because of the complexity of the search space, not because of any kind of progressive \u201cweighting\u201d of links between neurons [18, Section 11.4.4].\n23This is because the first step in unsupervised learning in the SP system is for the system to take in information via its senses, as indicated in Section V-B1.\nDonald Hebb [5] recognised that his central mechanism for learning could not account for one-trial learning and introduced a \u2018reverberatory\u2019 mechanism to plug the gap. But this has its own problems as outlined in [18, Section 11.4.4.1]."}, {"heading": "D. Learning Linguistic Knowledge", "text": "It appears that the SP system has considerable potential as a framework for the learning of linguistic knowledge, including syntax, semantics, and their integration.\nSemantic knowledge\u2014the non-syntactic meanings of speech or writing\u2014would include the kinds of knowledge discussed in Sections IV-B to IV-H. Aspects of how such knowledge may be learned are discussed in Sections V-E to V-K.\nIn learning how words relate to meanings, a child (or robot) must solve the problem that, when a word is heard in a given physical context, it may refer to any aspect of that context or to something else entirely.24 In the SP system, this problem may be solved statistically: in one context, a given word is likely to be ambiguous; but across several different contexts, associations are likely to emerge via the discovery of redundancies in the data.\nAs noted in Section IV-B4, it seems most natural to regard syntax and semantics as parallel streams of information. How unsupervised learning may be applied with that kind of information is discussed in Section V-G, drawing on ideas presented in Appendix C."}, {"heading": "E. Learning to See", "text": "In the same way that the SP computer model may build grammars for one-dimensional language-like data via the discovery of full and partial matches between patterns, via the creation of referential linkages between patterns, and via heuristic search through the abstract space of alternative grammars (Section V-B), it envisaged that, with facilities for the representation and processing of 2D patterns (Section II-B), the SP system may build visual grammars from a robot\u2019s visual input [21, Section 5].\nAs with other kinds of knowledge, grammars of that kind may include class hierarchies, part-whole hierarchies, and other kinds of knowledge structure, with seamless integration of different structures, as outlined in Section II-D (see also [21, Section 5.5]).\nIt appears that, in deriving structures from visual input, there are important roles for binocular vision [21, Section 5.1] and for objects in motion [21, Sections 5.2 and 5.3]."}, {"heading": "F. How a Robot May Build 3D Models of Objects, of Itself, and of Its Environment", "text": "When it has been generalised for the representation and processing of 2D patterns, the multiple alignment framework may be applied in creating models of objects (including robots), and of a robot\u2019s environment. This is described fairly fully in [21, Sections 6.1 and 6.2] and summarised here.\n24cf. Quine\u2019s discussion of how a linguist that is studying an unfamiliar language might infer the meaning of \u201cGavagai\u201d [10, Chapter 2].\nThe basic idea is that partially-overlapping images (from the robot\u2019s eyes) may be stitched together to create a coherent whole, in much the same way that partially-overlapping digital photographs may be stitched together to create a panorama. This is a relatively simple application of the multiple alignment concept, where a section at the end of one pattern matches a section at the beginning of another pattern. The system\u2019s ability to find good partial matches means that it should not be unduly disturbed by errors or distortions, provided they are not too great.\nFigure 7 shows schematically how this idea may be applied to create a full or partial model of a 3D object. It is envisaged that overlapping views around the object may be stitched together to create the model.\nFigure 11 in [21], with permission.\nWe may have confidence in the feasibility of creating spatial models via this kind of mechanism because there are already commercially-available systems for the creation of digital 3D models from photographs. Examples include \u201cBig Object Base\u201d (bit.ly/1gwuIfa), \u201cCamera 3D\u201d (bit.ly/1iSEqZu), and \u201cPhotoModeler\u201d (bit.ly/MDj70X).\nOf course, an autonomous robot is itself a 3D object so we may suppose that similar principles may be applied to create a digital model of the robot itself. Bearing in mind that a robot would not normally be able to see all parts of itself, we may suppose that any model of itself that it builds from visual information is likely to partial, perhaps an adjunct to information about its organisation and workings that it gains from internal sensors.\nTaking a broader view, the SP system may build a 3D model of a robot\u2019s environment in much the same way that Google\u2019s \u201cStreetview\u201d is built from overlapping pictures.25 The main difference between how Streetview models are constructed and how the SP system works, is that, with the SP system, images would be compressed via class hierarchies, part-whole hierarchies, and so on, as outlined in Section II-D.\n25See also Google\u2019s \u201cProject Tango\u201d, Wikipedia, bit.ly/1mR8cM6, retrieved 2014-08-27.\nBuilding a 3D model of a robot\u2019s environment via multiple alignment is a potential solution to the \u201cmapping\u201d part of \u201csimultaneous localization and mapping\u201d (SLAM).26 The SP system may also provide a solution to the \u201clocalization\u201d problem in SLAM via its capabilities in recognition (Section IV-C): using the multiple alignment system, it may compare its current visual input with its stored map of its surroundings and thus determine where it is, much as people appear to do.27\nG. Interactions and Other Regularities\nTo operate effectively in the world, an autonomous robot needs some understanding of: how its parts work together; the impact of its actions on its surroundings; the impact of its surroundings on itself; and many other regularities in the world. Examples include: limitations on how the robot\u2019s limbs may be arranged; how pushing a mug or tumbler over the edge of a table means that it will normally fall and, very often, break when it hits the floor; how striking a bell will produce one kind of sound while striking a lump of wood will produce another; the damage and pain that can be caused by walking into a brick wall or going too close to a fire; sensations of strain if the robot tries to lift something that is too heavy; the way night follows day and day follows night; and the way that puddles evaporate, especially when the sun shines.\nIn broad terms, these many regularities in the world and in the way that a robot may interact with the world may be learned in the same way that words and other syntactic patterns may be learned from samples of language: candidate patterns may be identified via multiple alignment (Section V-B1) and \u2018good\u2019 grammars (collections of SP patterns) may be compiled via heuristic search (Section V-B2).\nThe key difference between learning syntactic structures from a one-dimensional sample of text and learning the kinds of regularities considered here is that the latter comprise arbitrary combinations of sights, sounds, sensations from tactile and proprioceptive receptors, tastes, smells, and so on; and that, very often, these inputs may be seen to comprise parallel streams of information in which two or more events may occur simultaneously.\nThis difference between learning from a one-dimensional stream of information and learning from parallel streams of information may be accommodated with three refinements of the SP70 model:\n\u2022 Represent parallel streams of information with 2D patterns. If, as envisaged, the SP system is generalised to work with 2D patterns, such patterns may serve to represent parallel streams of information, as described in Appendix C. \u2022 Generalise the sequence alignment process to the matching of 2D patterns. The method for finding good full and partial matches between patterns (Section II-C1) may be generalised to create an equivalent capability with 2D patterns.\n26See, for example, \u201cSimultaneous localization and mapping\u201d, Wikipedia, bit.ly/1ikQTRR, retrieved 2014-08-06\n27A potentially interesting area of investigation is how these ideas may relate to the concept of a \u201cplace cell\u201d (Wikipedia, retrieved 2014-10-07, bit.ly/1vM6p26) and associated neurophysiological evidence.\n\u2022 Generalise the process for building multiple alignments to accommodate 2D patterns. The process for building multiple alignments should provide for the inclusion of 2D patterns.\nWith regard to the second point, a key feature of the existing process for sequence alignment is that it can find good matches between sequences despite interpolations of non-matching symbols in either or both sequences, as illustrated in Figure 8. It is envisaged that this kind of capability may be generalised to two dimensions and will then provide, in conjunction with heuristic search for good grammars (Section V-B2), a powerful means of finding recurrent patterns in parallel streams of information (or images) despite noise in the data."}, {"heading": "H. Exploration, Play, and the Learning of Minor Skills", "text": "A robot that spends most of its existence sitting quietly in a cupboard might never know that night follows day or that puddles evaporate. In order to learn these humdrum features of the world\u2014things that may seem too elementary and obvious to be taught (Section V-A1)\u2014our knowledge-hungry robot must explore. Robots that aspire to human-like intelligence must get about in the world and have varied experiences, as people do.\nIn a similar way, a robot that does not move or interact with the world cannot learn how its own body works, or the effects of its actions on its surroundings, or the impact of its surroundings on itself. Like people, especially children, robots must play. This is not merely a time-wasting indulgence, it is an essential part of learning the recurrent patterns that govern how people or robots may function in the world, including minor skills.\nAlthough introspection can be a poor guide to mental processes, something may be gained from considering our experience in learning simple skills such as the childhood favourite: wiggling one\u2019s ears. In learning something like that, it is initially hard to know what to do, but then things gradually take shape. What seems to be happening is that we are trying out different combinations and sequences of muscle contractions and of relaxations of muscles to discover what works. Those combinations and sequences, with feedback about what does and does not work, may be represented as parallel streams of information in 2D patterns, as described in Appendix C."}, {"heading": "I. Learning a Major Skill Via Practice", "text": "In addition to the knowledge and skills that may be gained via exploration and play, an autonomous robot, like a person, is likely to need more elaborate skills.\nAs a rough generalisation in that connection, the current generation of robots often have the benefit of artificial aids or simplifications of tasks, their skills are normally programmed by people, case by case, and they are generally deficient in human-like abilities to learn new skills for themselves. For example, the \u201cDeep Green\u201d pool-playing robot [4] has the benefit of an overhead camera giving a birds-eye view of the pool table, it is programmed with relevant knowledge of\ngeometry and of Newtonian physics, and it does not aspire to learn such things as how to make bread or how to play tennis.\nAlthough there is more to games like pool, billiards and snooker than merely potting balls, the process of potting a ball is a significant challenge for human players, especially with the large table of billiards and snooker. Here we consider how that skill, and related skills, may be learned via the SP system under the same conditions as human players, extending the discussion in [23, Section 6.3].\nWhen a human player pots a ball, he or she strikes the cue ball with the cue, aiming to send the cue ball towards the target ball to strike it in such a way that the target ball is propelled towards a pocket and falls into it. Skilled players may influence the behaviour of the cue ball or the target ball or both by imparting spin to the cue ball (by striking it away from the centre). And in the course of a game, skilled players may combine the potting of the target ball with measures to ensure that other balls finish their movements in positions that are advantageous for the next shot. Indeed, in snooker for example, the intention with some shots is not to pot any ball but merely to move balls into positions that make things difficult for one\u2019s opponent (ie to create a \u2018snooker\u2019). Since that kind of shot requires the same kinds of skills as are needed for potting a ball, our discussion will be about both kinds of shot. To simplify things in the discussion below, we shall ignore the fact that, in potting a ball, most human players will take advantage of depth perception via binocular vision.\nUnlike the Deep Green robot, the human player depends mainly on a view along the cue towards the cue ball and beyond, without a birds-eye view of the table via a ceilingmounted camera.28 Unlike the Deep Green robot, most human players have little or no formal knowledge of Newtonian physics or geometry, and even if they had, the absence of a birds-eye view would make such knowledge less useful than it is for the robot. It seems likely that the performance of skilled human players in potting balls, in the use of spin, and in the positioning of balls, has little to do with physics or geometry and is much more to do with extensive experience via thousands of hours of practice.\nThe SP system may support that kind of learning in an autonomous robot as outlined in what follows. For each shot:\n\u2022 Before the shot: parse visual input. Before the given shot is taken, the robot may assimilate information about the configuration of the table. In one or more views, the robot will see the cue ball, the target ball, other balls that may be on the table, and the target pocket (when the intention is to pot a ball). Each of those views may be parsed into its parts and sub-parts, much as in the parsing of natural\n28Deep Green actually has both those views, and human players may walk round the table to get different views. The key point here is that the human player does not normally get the birds-eye view.\nlanguage (Section IV-B1, [21, Section 4]). Each parsing is a multiple alignment from which an encoding may be derived as indicated in Section II-C2. There is potential for a 3D model to be derived from multiple views, as outlined in Section V-F. \u2022 During the shot: record actions and effects. As the shot is taken, a record may be kept of such things as contractions by the muscles of the robot, the speed and direction of the cue as it strikes the cue ball, the point on the cue ball that is struck by the cue (relative to the centre of the cue ball), tactile feedback from the impact of the cue on the cue ball, auditory feedback from impacts of various kinds, and observations of movements of the balls, perhaps analysed as outlined in [21, Section 5.3]. If the target ball has been successfully potted, that event will be recorded too. As in the learning of interactions and other regularities (Section V-G), there will be parallel streams of information which may be recorded using 2D patterns, as outlined in Appendix C. \u2022 At the end of the shot: parse visual input. At the end of the shot, as at the beginning, the robot may assimilate information about the configuration of the table via one or more views, with multiple alignment as the means of parsing each view. As before, there is potential for a 3D model to be derived from multiple views.\nFor each shot, the overall result will be a set of SP patterns that record: 1) The configuration of the playing table before the shot; 2) Actions and effects as the shot is taken; and 3) The configuration of the table at the end of the shot\u2014each one recorded in compressed form. Each such set of patterns may be regarded as one row in a database of configurations, actions and effects in the potting of balls. As the robot practices, it will build up the database, which may eventually become very large.\nAlthough the three elements of each row will, individually, be compressed, there is likely to be scope for further abstraction and compression of the whole database via the discovery of redundancies across the rows of the database.\nThe whole database may serve as a guide for future shots. For any such shot, the robot may find the row that best matches the initial configuration and the desired outcome, and it may then perform the corresponding actions."}, {"heading": "J. Learning Via Demonstration", "text": "A popular approach to the training of robots is simply to demonstrate directly what the robot is to do, normally by guiding one or more of the robot\u2019s limbs through the required motions.29. This is much simpler than learning by imitation\n29See, for example, \u201cThis robot could transform manufacturing\u201d, MIT Technology Review, 2012-09-18, bit.ly/1nbnJfv; \u201cRobots that learn through repetition, not programming\u201d, MIT Technology Review, 2014-09-22, bit.ly/1shxuLk.\n(Section V-A1) because it by-passes the relatively complex perceptual skills needed in the latter case.\nWith this kind of learning, the SP system may have a useful role to play by compressing the information gathered about the demonstrated motion of the robot\u2019s limb or limbs. It has the potential, for example, to identify repeating elements in the demonstrated motion and to abstract them as distinct subroutines. More generally, it may create a grammar for the required motion including hierarchical structures, classes of structure, and so on.\nA potential benefit of this kind of information compression is generalisation of the demonstrated motion, without overgeneralisation, as described in the next subsection. This may help to provide a degree of flexibility in the robot\u2019s actions. If sensory inputs are provided in conjunction with the demonstrated motion, the robot may also develop some ability to adapt to changes in the required task."}, {"heading": "K. Generalisation, Correction of Over-Generalisations, and Dirty Data, in the Learning of Non-Verbal Behaviour", "text": "As we have seen (Section V-A3), the principle of minimum length encoding provides a neat explanation of three aspects of language learning: 1) How we can generalise our knowledge from the finite sample of utterances which is the basis for our learning to the infinite range of utterances in the target language, L; 2) How over-generalisations may be corrected without error correction by a teacher, or anything equivalent; and 3) How people can develop a strong sense of what utterances do and do not belong in L when, in the vast majority of cases, there are errors in the sample of language that is the basis for our learning.\nNaturally, these principles would apply to language learning by robots (Section V-D). But it appears that the principles may also apply to the learning of non-verbal behaviour by a person or an autonomous robot:\n\u2022 Grammars for non-verbal behaviour. A set of skills like those required for cooking a meal may be regarded as a kind of non-verbal language. In view of the generality of the multiple alignment concept and the learning mechanisms in the SP system, it seems likely that grammars for that kind of non-verbal behaviour, and others, may be learned in much the same way as grammars for natural language. \u2022 Generalisation without over-generalisation. While the grammar for cooking a meal would be derived from specific experiences, it may generate an infinite range of possible meals. But it is not totally general and will not, for example, enable a person or robot to play poker. \u2022 Learning from dirty data. In learning the skills needed for cooking a meal, we can be successful despite the likelihood that people that we learn from may make mistakes and cook books are rarely free of errors.30\n30In my own experience, I once learned to program a computer in assembly language using an instruction manual that was riddled with errors. The fact that much of the manual was correct made it possible to identify the errors and work around them."}, {"heading": "L. Cutting the Cost of Learning", "text": "A familiar feature of human learning is that it can be costly in terms of time and money. Even with a natural talent, it can take a great deal of practice to become skilled in playing a musical instrument or in sports. People spend much time in education and training, mainly when they are young but also later in life, and schools, colleges and teachers all cost money.\nAn interesting possibility with autonomous robots and other artificial systems is that much of this cost may be avoided. This is because knowledge or skills that have been built up by one robot may be downloaded easily and transferred to any number of other robots. Naturally, this works best when the several robots are identical but otherwise there is potential for adjustments to be made via learning when the recipient robots are similar to the donor robot but not exactly the same."}, {"heading": "VI. CONCLUSION", "text": "The SP theory of intelligence and its realisation in the SP machine may facilitate the development of autonomous robots: by increasing the computational efficiency of computers; by facilitating the development of human-like versatility in intelligence; and likewise for human-like adaptability in intelligence.\nWith regard to the first problem, the SP system has potential for substantial gains in computational efficiency, with corresponding cuts in energy consumption and in the bulkiness of computing machinery: by reducing the size of data to be processed; by exploiting statistical information that the system gathers as an integral part of how it works; and via an updated version of Hebb\u2019s concept of a \u201ccell assembly\u201d.\nIn the quest for human-like versatility in intelligence, the SP system has strengths in several areas including unsupervised learning, natural language processing, pattern recognition, information retrieval, several kinds of reasoning, planning, problem solving, and more, with seamless integration amongst structures and functions.\nThe SP system may also promote human-like adaptability in intelligence via its strengths in unsupervised learning. As has been discussed, these capabilities may underpin one-trial learning, the learning of natural language, learning to see, building 3D models of objects and of a robot\u2019s surroundings, learning how a robot interacts with its environment and other regularities, learning minor skills via exploration and play, learning major skills, and learning via demonstration. Associated issues that have been discussed include: learning from parallel streams of data; generalisation, correction of over-generalisations, and learning from dirty data; how to cut the cost of learning; and reinforcements and motivations.\nAlthough it is likely that autonomous robots will require a non-von revolution\u2014perhaps along the lines of SP-neural\u2014 there is plenty that can be done via modelling with vonNeumann-style supercomputers to explore the potential of new architectures. One such possibility is research with a highparallel version of the SP machine, as outlined in Section II-A. This would be a means for researchers everywhere to explore what can be done with the system and to create new versions of it."}, {"heading": "APPENDIX A HOW SYMBOLS MAY BE IDENTIFIED", "text": "This appendix and the ones that follow consider issues that are relevant to discussions elsewhere in this paper: how symbols may be identified; how quantification can or should be accommodated in the SP system; and how the SP system may represent and process parallel streams of information.\nIn the SP system, in the processing of things like text, it is a straightforward matter to equate individual characters, or whole words, with SP symbols. But identifying SP symbols is less easy with things like images, or recordings of speech or music:\n\u2022 Images. One possibility with images is to treat each pixel as an SP symbol, where each pixel may be matched in an all-or-nothing manner with any other pixel. Another possibility is, via some conventional pre-processing, to identify low-level features in images such as lines and angles and to treat such features as SP symbols. How that kind of thing may be done within the SP framework is discussed in [21, Section 3]. \u2022 Speech. As with images, it may be possible to treat the lowest-level elements as SP symbols. Alternatively, features such as white noise (from fricative consonants), formants, formant ratios, and formant transitions, may be identified via pre-processing and treated as being SP symbols. \u2022 Music. A relatively straightforward analysis would equate individual notes (including notes within chords) as being SP symbols. As with images and speech, it may be necessary on relatively short timescales to use conventional pre-processing (Fourier analysis and the like) to isolate individual notes within a stream of music.\nOf course, hybrid solutions, using conventional preprocessing in conjunction with the SP system, are not as theoretically neat as when everything is done via the SP system, but they may be justified as short-term expedients. On longer timescales, it would probably make better sense to try to avoid the relative complexity of such solutions. In the processing of images, for example, there is potential for the SP system to isolate features such as lines and angles [21, Sections 3.3 and 3.4]."}, {"heading": "APPENDIX B", "text": "QUANTIFICATION IN THE SP SYSTEM\nIn understanding any kind of skilled activity\u2014playing tennis, cooking a meal, and so on\u2014or creating robots that have such skills, it seems natural to measure things like the strength with which a tennis player hits a ball, and to express the measurements with numbers. But the SP system, in itself, makes no provision for quantification, neither analogue nor digital.31 At the lowest level in its knowledge, it deals with symbols which have no intrinsic meaning, numerical or otherwise (Section II). As noted in Section II-B, any meaning associated with a given SP symbol or combination of symbols must be\n31It is true that each SP pattern has an associated frequency of occurrence (Section II) but that is for internal use and not for representing something like the strength with which a tennis player hits a ball.\nexpressed using other SP symbols, or external equivalents; and there is just one valid operation with an SP symbol: to match it with another SP symbol to determine whether they are the same or different.\nIn broad terms, there are three main ways in which quantification may be accommodated in the SP system:\n\u2022 Via densities of symbols. The densities of different categories of symbols may serve to express quantities, in much the same way that the densities of black and white pixels may represent different shades of grey in a black and white photograph, at least as they used to be [18, Section 2.2.3]. \u2022 Via the rules of arithmetic. In principle, the SP system may express values as numbers and perform arithmetic operations if it is supplied with SP patterns representing Peano\u2019s axioms and other knowledge about arithmetic. This has not yet been explored in any depth. There is relevant discussion in [18, Chapter 10]. \u2022 Don\u2019t do it. The simplest option is to avoid quantification altogether. This may not be as silly as it sounds, as suggested in the following discussion.\nAlthough it may seem natural to quantify the operations of a robot and to represent quantities with numbers, those assumptions may carry with them an unspoken and possibly unjustified belief that increasing the size of a given value in the robot would normally increase the impact of that value in the robot\u2019s environment. For example, it seems obvious, and is probably true, that if an industrial robot fails to bend a metal bar with medium pressure, stronger pressure is likely to succeed.\nBut the assumption of a linear or monotonic relationship between variables is often wrong. If we hold an egg in our hand with a grip that is too weak, we may drop it. But if our grip is too strong, the egg may be crushed. Holding an egg works best with a grip that is neither too weak nor too strong. Returning to our tennis player example, strong blows to the ball may score points on many occasions but with a drop shot, for example, a light touch is required.\nIn general, any person or robot must keep an open mind in learning how things work, without presuppositions about linear or quasi-linear relationships between variables. In that case, it is an advantage rather than a handicap if values like weak, medium, and strong are represented with SP symbols without any presumption of a quantitative relationship amongst them, much as one would assume for values like red, sweet, and curly. Elsewhere in this paper we shall assume that all values that provide input for a robot\u2019s learning are represented with standard SP symbols, without any quantitative meaning."}, {"heading": "APPENDIX C HOW THE SP SYSTEM MAY REPRESENT AND PROCESS PARALLEL STREAMS OF INFORMATION", "text": "In the development to date of the SP theory and the SP computer model, the main focus has been on one-dimensional patterns and what can be done with them. This can work well with some kinds of information, as can be seen in Figures 2 and 4. In each case, the patterns in the multiple alignment may be merged (unified) to create a single 1D pattern.\nIn examples like Figure 4, the left-to-right ordering of symbols represents the time ordering of words and other structures in natural language. This is OK for a one-dimensional stream of information like ordinary text but is not satisfactory when there are two or more streams of information in parallel. Here are some examples:\n\u2022 Speech. In both the production of speech and in the acoustic signal of speech, there are normally several things going on in parallel. When we speak, there is simultaneous activity in our lips, tongue, cheeks, and breathing; while in terms of acoustics, elements of speech such as vowel sounds may be distinguished, one from another, by configurations of simultaneously-occurring spectral peaks or formants. \u2022 Spelling rules. Notwithstanding the \u2018whole word\u2019 doctrine in the teaching of reading, it is widely recognised that, with English at least, skilled readers know many associations between configurations of letters and corresponding sounds: \u2018th\u2019 = T or k; \u2018ch\u2019 = \u00d9; \u2018ay\u2019 = eI; and so on.32 In terms of before-and-after relationships, it makes most sense to say that a spelling pattern and its sound value occupy the same time slot. \u2022 Music. In music, especially orchestral music and music for the piano or organ, it is normal for two or more notes to be played at the same time. \u2022 Natural language and its meanings. The surface forms of spoken or written language, and the meanings of those forms, may be seen as parallel streams of information (see also Sections IV-B4 and V-D). \u2022 Robots and their surroundings. In considering the information that an autonomous robot needs to process, there are normally several streams of information running in parallel, in two main areas:\n\u2013 The robot\u2019s environment. As with people, there would normally be several things happening at the same time. In a typical office, for example, there would be people talking, phones ringing, people coming and going, people working on keyboards, taking refreshment, doing photocopying, and so on. \u2013 The robot\u2019s workings and its impact on its surroundings. In any robot of reasonable complexity, there would be signals going to the robot\u2019s \u2018muscles\u2019, and, via sensors of various kinds, there would be information about the robot\u2019s surroundings, information about the internal workings of the robot, and feedback about the effects of the robot\u2019s actions on its surroundings."}, {"heading": "A. Representing Parallel Streams of Information With TwoDimensional Patterns", "text": "How should the SP system accommodate parallel streams of information? The most straightforward answer seems to be to take advantage of what is in any case envisaged for development within the system: SP patterns in two dimensions. Such patterns were originally conceived, and are still seen, as\n32In each case, the sounds, to the right, are represented with symbols from the International Phonetic Alphabet.\na vehicle for the representation and processing of images [21]. But they may also serve in the representation and processing of parallel streams of information, as illustrated in Figure 9.\nIn this example, any given SP symbol (shown as a numbered circle) may be assigned to a time slot (one of the columns in the figure) in one of five streams of information (shown as rows S1 to S5 in the figure).\nAspects of this proposal are described and discussed in the subsections that follow."}, {"heading": "B. Streams of Information", "text": "The reason for introducing the notion of \u2018streams\u2019 of information is to try to achieve some consistency across different configurations of symbols, and the reason for that is to facilitate the discovery of patterns that match each other, as indicated in Section II-C1 and amplified in Section V-B. For example, it is easier to recognise that one musical chord is the same as another if symbols for the constituent notes are arranged in order of their pitch (as they normally are in musical notations) than if there is a haphazard ordering of symbols from one instance of a chord to another. Like the different levels in a musical stave, streams of information provide a means of keeping things in order.\nIt seems likely that some streams of information will be for relatively concrete attributes like pitch, while others will be for relatively abstract attributes arising from the fact that some symbols may serve as references to other structures, as outlined in Sections II-D and C-D."}, {"heading": "C. Extended Events", "text": "Figure 9 may give the impression that, in the proposed scheme for representing parallel streams of information, every symbol represents some kind of short event occupying a single time slot. But in music for example, individual notes vary in their duration and some, such as the drone of a bagpipe, may be held for extended periods.\nIn the proposed scheme, any event that is longer than a single time slot may be represented with a pair of symbols, one marking the start of the event and the other marking when\nit ends. For example, the symbol \u201810\u2019 in stream S3 in Figure 9 may be a mark for the beginning of a note, while the symbol \u201811\u2019 in the same stream may mark when the note ends."}, {"heading": "D. References and Abstractions", "text": "As mentioned in Section C-B, 2D patterns representing parallel streams of information may contain symbols that serve as references to other structures. For example, where a portion of music is repeated in different parts of a musical composition, the first instance may be marked with a \u2018start\u2019 symbol and an \u2018end\u2019 symbol, and then copies of those two symbols may serve to represent later instances without the need to repeat all the detail.\nAs outlined in Section II-D, these kinds of linkages between patterns may serve to define classes and subclasses of structure, parts and sub-parts, and several other kinds of knowledge, with seamless integration of different structures."}, {"heading": "E. Processing of Parallel Streams of Information", "text": "It is envisaged that 2D patterns representing parallel streams of information will be processed in the same way as other SP patterns: via processes for finding good full and partial matches between patterns and via processes for the building of multiple alignments. As with the processing of 2D patterns representing other kinds of data such as images, it will be necessary to generalise the processes for finding good full and partial matches between patterns and for building multiple alignments so that they work with 2D patterns. There is further discussion in Section V-G."}], "references": [{"title": "My Left Foot", "author": ["C. Brown"], "venue": "Mandarin, London,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "A review of B", "author": ["N. Chomsky"], "venue": "F. Skinner\u2019s \u201cVerbal Behavior\u201d. Language, 35(1):26\u201358,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1959}, {"title": "Language identification in the limit", "author": ["M. Gold"], "venue": "Information and Control, 10:447\u2013474,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1967}, {"title": "Toward a competitive pool-playing robot", "author": ["M. Greenspan", "J. Lam", "M. Godard", "I. Zaidi", "S. Jordan", "W. Leckie", "K. Anderson", "D. Dupuis"], "venue": "Computer, 41(1):46\u201353,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "The Organization of Behaviour", "author": ["D.O. Hebb"], "venue": "John Wiley & Sons, New York,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1949}, {"title": "The remarkable, yet not extraordinary, human brain as a scaled-up primate brain and its associated cost", "author": ["S. Herculano-Houzel"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 109(Supplement 1):10661\u201310668,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Smart machines: IBM\u2019s Watson and the era of cognitive computing", "author": ["J.E. Kelly", "S. Hamm"], "venue": "Columbia University Press, New York,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding language without the ability to speak", "author": ["E.H. Lenneberg"], "venue": "Journal of Abnormal and Social Psychology, 65:419\u2013425,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1962}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": "Springer, New York,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Word and Object", "author": ["W.V.O. Quine"], "venue": "MIT Press, Cambridge, MA.,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1960}, {"title": "Modelling by the shortest data description", "author": ["J. Rissanen"], "venue": "Automatica-J, IFAC, 14:465\u2013471,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1978}, {"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "Parts I and II. Information and Control, 7:1\u201322 and 224\u2013254,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1964}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, 59:433\u2013 460,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1950}, {"title": "An information measure for classification", "author": ["C.S. Wallace", "D.M. Boulton"], "venue": "Computer Journal, 11(2):185\u2013195,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1968}, {"title": "Learning syntax and meanings through optimization and distributional analysis", "author": ["J.G. Wolff"], "venue": "Y. Levy, I. M. Schlesinger, and M. D. S. Braine, editors, Categories and Processes in Language Acquisition, pages 179\u2013 215. Lawrence Erlbaum, Hillsdale, NJ,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Computing, cognition and information compression", "author": ["J.G. Wolff"], "venue": "AI Communications, 6(2):107\u2013127,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Medical diagnosis as pattern recognition in a framework of information compression by multiple alignment, unification and search", "author": ["J.G. Wolff"], "venue": "Decision Support Systems, 42:608\u2013625,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Unifying Computing and Cognition: the SP Theory and Its Applications", "author": ["J.G. Wolff"], "venue": "CognitionResearch.org, Menai Bridge,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards an intelligent database system founded on the SP theory of computing and cognition", "author": ["J.G. Wolff"], "venue": "Data & Knowledge Engineering, 60:596\u2013624,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "The SP theory of intelligence: an overview", "author": ["J.G. Wolff"], "venue": "Information, 4(3):283\u2013341,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision", "author": ["J.G. Wolff"], "venue": "3(1):552,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data and the SP theory of intelligence", "author": ["J.G. Wolff"], "venue": "IEEE Access, 2:301\u2013315,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "The SP theory of intelligence: benefits and applications", "author": ["J.G. Wolff"], "venue": "Information, 5(1):1\u201327,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "The SP system may help: by reducing the size of data to be processed; by exploiting statistical information that the system gathers as an integral part of how it works; and via a new version of Donald Hebb\u2019s [5] concept of a \u201ccell assembly\u201d.", "startOffset": 208, "endOffset": 211}, {"referenceID": 22, "context": "Like any good theory, the SP theory has a wide range of potential applications, some of which are described in [23].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "The theory is described most fully in [18] and at some length in [20].", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "The theory is described most fully in [18] and at some length in [20].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "In addition to the present paper, potential benefits and applications of the theory are described in [23]", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "(an overview), [22] (how the SP system may help to solve problems associated with big data), [21] (application of the SP theory to the understanding of natural vision and the development of computer vision), [17] (application of the SP system to medical diagnosis), and [19] (the SP system as an", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "(an overview), [22] (how the SP system may help to solve problems associated with big data), [21] (application of the SP theory to the understanding of natural vision and the development of computer vision), [17] (application of the SP system to medical diagnosis), and [19] (the SP system as an", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "(an overview), [22] (how the SP system may help to solve problems associated with big data), [21] (application of the SP theory to the understanding of natural vision and the development of computer vision), [17] (application of the SP system to medical diagnosis), and [19] (the SP system as an", "startOffset": 208, "endOffset": 212}, {"referenceID": 18, "context": "(an overview), [22] (how the SP system may help to solve problems associated with big data), [21] (application of the SP theory to the understanding of natural vision and the development of computer vision), [17] (application of the SP system to medical diagnosis), and [19] (the SP system as an", "startOffset": 270, "endOffset": 274}, {"referenceID": 19, "context": "Reproduced from Figure 2 in [20], with permission.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "The emphasis on information compression derives from earlier research on grammatical inference (Section V-A3) and the principle of minimum length encoding (MLE) [11], [12], [14]).", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "The emphasis on information compression derives from earlier research on grammatical inference (Section V-A3) and the principle of minimum length encoding (MLE) [11], [12], [14]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "The emphasis on information compression derives from earlier research on grammatical inference (Section V-A3) and the principle of minimum length encoding (MLE) [11], [12], [14]).", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "Reproduced from Figure 16 in [20], with permission.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "pression and concepts of prediction and probability [9], the SP system is fundamentally probabilistic.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "The word \u201cassembly\u201d has been adopted in this term because the concept is quite similar to Hebb\u2019s [5] concept of a cell assembly.", "startOffset": 97, "endOffset": 100}, {"referenceID": 17, "context": "2 in [18], with permission.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "It has quite a lot to say, for example, about the nature of mathematics [16], [18, Chapter 10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "12Although Alan Turing saw that computers might become intelligent [13], the Universal Turing Machine, in itself, does not tell us how! The SP theory, as it is now, goes some way towards plugging the gap, and has potential to do more.", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "ly/1qJmCBG), and there may be as many as 86 billion neurons in the human brain [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "architecture that is inspired in part by Hebb\u2019s [5] concept of a \u201ccell assembly\u201d, as outlined in Section II-F.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "16 Apart from the concept of \u201cdata-centric\u201d computing (referenced in Section III-C3 and described in, for example, [7]), the proposals here, which all flow from the SP theory, are new contributions to thinking about the design of brains for autonomous robots.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "and concepts of prediction and probability [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "If they have anything approaching human-like intelligence it is almost always in a narrow field such as driving a car17 or playing pool [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 22, "context": "Reproduced from Figure 1 in [23], with permission.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "The SP system also provides a framework for medical diagnosis via pattern recognition, with potential for diagnosis via causal reasoning [17], [18, Section 6.", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "The SP system may serve as a database system with \u2018intelligence\u2019 [19], [18, Chapter 6].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "vision and to facilitate the development of human-like abilities in robot vision [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "[3])\u2014and it also means that learning occurs without rewards or punishments (Section V-A2).", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "And a working hypothesis\u2014with supporting evidence from research on the learning of a first language or languages [15]\u2014is that unsupervised learning is an important driver in both primary and secondary forms of learning.", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "In that connection, Chomsky\u2019s critique of Skinner\u2019s book on verbal behaviour and how it may be learned [2] is still worth reading, despite the passage of time.", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "There is an overview of this research in [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "The principle evidence is that children with a physical handicap that prevents them speaking intelligibly may, nevertheless, learn to comprehend their native language successfully [1], [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "The principle evidence is that children with a physical handicap that prevents them speaking intelligibly may, nevertheless, learn to comprehend their native language successfully [1], [8].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "before, there is evidence that children can learn their native language without the benefit of errorcorrection by adults or anything equivalent [1], [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 7, "context": "before, there is evidence that children can learn their native language without the benefit of errorcorrection by adults or anything equivalent [1], [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 17, "context": "2 in [18], with permission.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "3 in [18], with permission.", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "Donald Hebb [5] recognised that his central mechanism for learning could not account for one-trial learning and introduced a \u2018reverberatory\u2019 mechanism to plug the gap.", "startOffset": 12, "endOffset": 15}, {"referenceID": 20, "context": "Reproduced from Figure 11 in [21], with permission.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "For example, the \u201cDeep Green\u201d pool-playing robot [4] has the benefit of an overhead camera giving a birds-eye view of the pool table, it is programmed with relevant knowledge of", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "a vehicle for the representation and processing of images [21].", "startOffset": 58, "endOffset": 62}], "year": 2015, "abstractText": "This paper is about how the SP theory of intelligence and its realisation in the SP machine (both outlined in the paper) may help in the design of the \u2018brains\u2019 of autonomous robots, meaning robots that do not depend on external intelligence or power supplies, are mobile, and have human-like versatility and adaptability in intelligence. The paper addresses three main problems: how to increase the computational and energy efficiency of computers, and to reduce their size and weight; how to achieve human-like versatility in intelligence; and likewise for humanlike adaptability in intelligence. Regarding the first problem, the SP system has potential for substantial gains in computational efficiency, with corresponding cuts in energy consumption and in the bulkiness of computers: by reducing the size of data to be processed; by exploiting statistical information that the system gathers as an integral part of how it works; and via a new version of Donald Hebb\u2019s concept of a \u201ccell assembly\u201d. Towards human-like versatility in intelligence, the SP system has strengths in unsupervised learning, natural language processing, pattern recognition, information retrieval, several kinds of reasoning, planning, problem solving, and more, with seamless integration amongst structures and functions. The SP system\u2019s strengths in unsupervised learning and other aspects of intelligence may help to achieve human-like adaptability in intelligence via: 1) one-trial learning; 2) the learning of natural language; 3) learning to see; 4) building 3D models of objects and of a robot\u2019s surroundings; 5) learning regularities in the workings of a robot and in the robot\u2019s environment; 6) exploration and play; 7) learning major skills; and 7) learning via demonstration. Also discussed are: how the SP system may process parallel streams of information; generalisation of knowledge, correction of over-generalisations, and learning from dirty data; how to cut the cost of learning; and reinforcements and motivations.", "creator": "LaTeX with hyperref package"}}}