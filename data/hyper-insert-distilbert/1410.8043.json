{"id": "1410.8043", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2014", "title": "High-Performance Distributed ML at Scale through Parameter Server Consistency Models", "abstract": "as computer machine learning ( ml ) intelligence applications increase immensely in data volume size and model generation complexity, mls practitioners actively turn to distributed functional clusters devised to partially satisfy the increased computational and memory demands.... unfortunately, effective kernel use instead of clusters for individual ml computers requires more considerable expertise in writing accurately distributed code, while highly - engineered abstracted hardware frameworks like hadoop have otherwise not, in practice, approached the performance seen in specialized ml implementations. utilizing the recent parameter server ( sap ps ) verification paradigm is a middle ground between these security extremes, ideally allowing easy successful conversion of dedicated single - table machine biased parallel ml applications into densely distributed control ones, for while ensuring maintaining high geometric throughput yields through the relaxed \" inconsistent consistency models \" that allow persistent inconsistent parameter type reads. hence however, due themselves to insufficient theoretical study, though it is not clear yet which of its these robust consistency models directly can really ensure correct ml algorithm solving output ; albeit at the same time, consequently there both remain also many theoretically - socially motivated but obviously undiscovered opportunities to truly maximize comparable computational throughput. motivated by this challenge, we study successfully both the theoretical guarantees and empirical behavior of iterative - prone convergent ml algorithms in existing reference ps consistency models. we then use along the upstream gleaned dynamics insights interface to improve managing a consistency model using an \" eager \" ps communication communication mechanism, and implement similar it performance as a new standardized ps system simulation that enables ml compatible algorithms to reach their solution suppliers more quickly.", "histories": [["v1", "Wed, 29 Oct 2014 16:19:21 GMT  (2446kb,D)", "http://arxiv.org/abs/1410.8043v1", "19 pages, 2 figures"]], "COMMENTS": "19 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["wei dai", "abhimanu kumar", "jinliang wei", "qirong ho", "garth a gibson", "eric p xing"], "accepted": true, "id": "1410.8043"}, "pdf": {"name": "1410.8043.pdf", "metadata": {"source": "CRF", "title": "High-Performance Distributed ML at Scale through Parameter Server Consistency Models", "authors": ["Wei Dai", "Abhimanu Kumar", "Jinliang Wei", "Qirong Ho", "Garth Gibson", "Eric P. Xing"], "emails": ["wdai@cs.cmu.edu,", "abhimank@cs.cmu.edu,", "jinlianw@cs.cmu.edu,", "garth@cs.cmu.edu,", "epxing@cs.cmu.edu,", "hoqirong@gmail.com"], "sections": [{"heading": "Introduction", "text": "The surging data volumes generated by internet activity and scientific research [7] put tremendous pressure on Machine Learning (ML) methods to scale beyond the computation and memory of a single machine. On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines. A typical solution to this problem is data paralllelism, in which the data is partitioned and distributed across different machines, which train the (shared) ML model using their local data. In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].\nMany general-purpose Parameter Server (PS) systems [8, 5, 6] of ML computation provide a Distributed Shared Memory (DSM) solution to the Big Data and Big Model issues. DSM allows ML programmers to treat the entire cluster as a single memory pool, where every machine can read/write to any model parameter via a simple programming interface; this greatly simplifies the implementation of distributed ML programs, because programmers may treat a cluster like a \u201csupercomputer\u201d that can run thousands of computational threads, without worrying about low-level communication between machines. It should be noted that not all PS systems provide a DSM interface; some espouse an arguably less-convenient push/pull interface that requires users to explicitly decide which parts of the ML model need to be communicated [12].\nar X\niv :1\n41 0.\n80 43\nv1 [\ncs .L\nG ]\n2 9\nO ct\n2 01\nAt the same time, the iterative-convergent nature of ML programs presents unique opportunities and challenges that do not manifest in traditional database applications: for example, ML programs lend themselves well to stochastic subsampling or randomized algorithms, but at the same time exhibit complex dependencies or correlations between parameters that can make parallelization difficult [3, 11]. Recent works [16, 8, 9] have introduced relaxed consistency models to trade off between parameter read accuracy and read throughput, and show promising speedups over fully-consistent models; their success is underpinned by the error-tolerant nature of ML, that \u201cplays nicely\u201d with relaxed synchronization guarantees \u2014 and in turn, relaxed synchronization allows system designers to achieve higher read throughput, compared to fully-consistent models.\nHowever, we still possess limited understanding of (1) how relaxed consistency affects ML algorithmconvergence rate and stability, and (2) what opportunities still exist for improving the performance of boththe ML algorithm (how much progress it makes per iteration), and the throughput of the PS system (howmany ML algorithm iterations can be executed per second). Recent works on PS have only focused on system optimizations in PS using various heuristics like async relaxation [4] and uneven updates propagation based on parameter values [12]. Our work instead starts from an ML-theoretic standpoint and provides principled insights to improve PS design. Concretely, we examine the theoretical and empirical behavior of PS consistency models from new angles, such as the distribution of stale reads and the impact of staleness on solution stability. We then apply the learnt insights to design more efficient consistency models and PS system that outperform previous work.\nTo study these issues, we formulate a new Value-Bounded Asynchronous Parallel (VAP) model, and show that it provides an ideal, gold-standard target in terms of theoretical behavior (high progress per ML algorithm iteration). However, VAP, of which the basic idea or principle is attempted in [14], can be problematic because bounding the value of in-transit updates amounts to tight synchronization. We propose Eager Stale Synchronous Parallel (ESSP), a variant of Stale Synchronous Parallel (SSP, a bounded-iteration model that is fundamentally different from VAP) introduced in [8], and formally and empirically show that ESSP is a practical and easily realizable scheme for parallelization. Specifically, we develop new variance bounds for both ESSP and VAP, and show that ESSP attains the same guarantees as VAP. These variance bounds provide a deeper characterization of convergence (particularly solution stability) under SSP and VAP, unlike existing PS theory that is focused only on expectation bounds [8]. We develop an efficient implementation of ESSP and shows that it outperforms SSP in convergence (both real time and per iteration) by reducing the average staleness, consistent with our theoretical results."}, {"heading": "Consistency Models for Parameter Servers", "text": "A key idea for large-scale distributed ML is to carefully trade off parameter consistency for increased parameter read throughput (and thus faster algorithm execution), in a manner that guarantees the final output of an ML algorithm is still \u201ccorrect\u201d (meaning that it has reached a locally-optimal answer). This is possible because ML algorithms are iterative-convergent and error-tolerant: ML algorithms will converge to a local optimum even when there are errors in the algorithmic procedure itself (such as stochasticity in randomized methods).\nIn a distributed-parallel environment, multiple workers must simultaneously generate updates to shared global parameters. Hence, enforcing strong consistency (parameters updates are immediately reflected) quickly leads to frequent, time-consuming synchronization and thus very limited speed up from parallelization. One must therefore define a relaxed consistency model that enables low-synchronization parallelism while closely approximating the strong consistency of sequential execution. The insight is that, to an iterative-convergent ML algorithm, inconsistent parameter reads have essentially the same effect as errors due to the algorithmic procedure \u2014 implying that convergence to local optima can still happen even under inconsistent reads, provided the degree of inconsistency is carefully controlled. We now explain two possible\nPS consistency models, and the trade-offs they introduce.\nThe ideal but inefficient Value-bounded Asynchronous Parallel (VAP) model We first introduce Value-bounded Asynchronous Parallel (VAP), an ideal model that directly approximates strong consistency (e.g. in the sequential setting) by bounding the difference in magnitude between the strongly consistent view of values (i.e. values under the single-thread model) and the actual parameter views on the workers. Formally, let x represent all model parameters, and assume that each worker in the ML algorithm produces additive updates (x \u2190 x + u, where u is the update)1. Given P workers, we say that an update u is in transit if u has been seen by P \u2212 1 or fewer workers \u2014 in other words, it is yet visible by all workers. Update u is no longer in transit once seen by all workers. The VAP requires the following condition:\nVAP condition: Let up,i be the updates from worker p that are in transit, and up := \u2211\ni up,i. VAP requires that, whenever any worker performs a computation involving the model variables x, the condition ||up||\u221e \u2264 vthr holds for a specified (time-varying) value bound parameter vthr. In other words, the aggregated intransit updates from all workers cannot be too large.\nTo analyze VAP, we must identify algorithmic properties common to ML algorithms. Broadly speaking, most ML algorithms are either optimization-based or sampling-based. Within the former, many Big Data ML algorithms are stochastic gradient descent-based (SGD), because SGD allows each worker to operate on its own data partition (i.e. no need to transfer data between workers), while the algorithm parameters are globally shared (hence a PS is necessary). SGD\u2019s popularity makes it a good choice for grounding our analysis \u2014 in later section, we show that VAP approximates strong consistency well in these senses: (1) SGD with VAP errors converges in expectation to an optimum; (2) the parameter variance decreases in successive iterations, guaranteeing the quality and stability of the final result.\nWhile being theoretically attractive, the VAP condition is too strong to be implemented efficiently in practice: before any worker can perform computation on x, it must ensure that the in-transit updates from all other workers sum to at most vthr component-wise due to the max-norm. This poses a chicken-and-egg conundrum: for a worker to ensure the VAP condition holds, it needs to know the updates from all other workers \u2014 which, in general, requires the same amount of communication as strong consistency, defeating the purpose of VAP. While it may be possible to relax the VAP condition for specific problem structures, in general, value-bounds are difficult to achieve for a generic PS."}, {"heading": "Eager Stale Synchronous Parallel (ESSP)", "text": "In order to design a consistency model that is practically efficient while providing proper correctness guarantees, we consider an iteration-based consistency model called Stale Synchronous Parallel (SSP) [8], that lends itself to an efficient PS implementation. At a high level, SSP imposes bounds on clock, which represents some unit of work in an ML algorithm, akin to iteration. Given P workers, SSP assigns each worker a clock cp that is initially zero. Then, each worker repeats the following operations: (1) perform computation using shared parameters x stored in the PS, (2) make additive updates u to the PS, and (3) advance its own clock cp by 1. The SSP model limits fast workers\u2019 progress so that the clock difference between the fastest and slowest worker is \u2264 s, s being a specified staleness parameter. This is achieved via:\nSSP Condition (informal): Let c be the clock of the fastest workers. They may not make further progress until all other workers\u2019 updates up that were made at clocks at or before c\u2212 s\u2212 1 become visible.\nWe present the formal condition in the next section. Crucially, there are multiple update communication strategies that can meet the SSP condition. We present Eager SSP (ESSP) as a class of implementations that eagerly propagate the updates to reduce empirical staleness beyond required by SSP. ESSP does not\n1This is common in algorithms such as gradient descent (u being the gradient) and sampling methods.\nprovide new guarantees besides warranted by SSP, but we show that by reducing the average staleness ESSP achieves faster convergence theoretically and empirically.\nHow can we show that ESSP reduces the staleness of parameter reads? While it is difficult to a priori predict the behavior of complex software such as a PS, we can still empirically measure the staleness of parameter reads during PS algorithm execution, from which we can draw conclusions about the PS\u2019s behavior. Consider Figure 1 (left), which shows the distribution of parameter stalenesses observed in matrix factorization implemented on SSP and ESSP. Our measure of staleness is a \u201cclock differential\u201d: when a worker reads a parameter, that worker\u2019s clock could be be behind (or ahead) of other workers, and clock differential simply measures this clock difference. Under SSP, the distribution of clock differentials is nearly uniform, because SSP \u201cwaits until the last minute\u201d to update the local parameter cache. On the other hand, ESSP frequently updates the local parameter caches via its eager communication, which reduces the negative tail in clock differential distribution \u2014 this improved staleness profile is ESSP\u2019s most salient advantage over SSP. In the sequel, we will show that better staleness profiles lead to faster ML algorithm convergence, by proving new, tighter convergence bounds based on average staleness and the staleness distributions (unlike the simpler worst-case bounds in [8]).\nOur analyses and experiments show that ESSP combines the strengths of VAP and SSP: (1) ESSP achieves strong theoretical properties comparable to VAP; (2) ESSP can be efficiently implemented, with excellent empirical performance on two ML algorithms: matrix completion using SGD, and topic modeling using sampling. We also show that ESSP achieves higher throughput than SSP, thanks to system optimizations that exploit ESSP\u2019s aggressive scheduling."}, {"heading": "Theoretical Analysis", "text": "In this section, we theoretically analyze VAP and ESSP, and show how they affect ML algorithm convergence. For space reasons, all proofs are placed in the appendix. As explained earlier, we ground our analysis on ML algorithms in the stochastic gradient descent (SGD) family (due to its high popularity for Big Data), and prove the convergence of SGD under VAP and ESSP. We now explain SGD in the context of a matrix completion problem."}, {"heading": "SGD for Low Rank Matrix Factorization", "text": "Matrix completion involves decomposing an N \u00d7M matrix D into two low rank matrices L \u2208 RN\u00d7K and R \u2208 RK\u00d7M such that LR \u2248 D, where K << min{M,N} is a user-specified rank. The problem is to predict those missing entries based on known entries Dobs by solving the following `2-penalized optimization\nproblem:\nmin L,R \u2211 (i,j)\u2208Dobs ||Dij \u2212 K\u2211 k=1 LikRkj ||2 + \u03bb(||L||2F + ||R||2F )\nwhere || \u00b7 ||F is the Frobenius norm and \u03bb is the regularization parameter. The stochastic gradient updates for each observed entry Dij \u2208 Dobs are\nLi\u2217 \u2190 Li\u2217 + \u03b3(eijR>\u2217j \u2212 \u03bbLi\u2217) R>\u2217j \u2190 R\u2217j + \u03b3(eijL>i\u2217 \u2212 \u03bbR\u2217j)\nwhere Li\u2217, R\u2217j are row and column of L,R respectively, and Li\u2217R\u2217j is the vector product. eij = Dij \u2212 Li\u2217R\u2217j . We absorb constants into the step-size \u03b3. Since L,R are being updated by each gradient, we put them in the parameter server to allow all works access them and make additive updates. The data Dobs are partitioned into worker nodes and stored locally.\nVAP We formally introduce the VAP computation model: given P workers that produce updates at regular intervals which we call \u201cclocks\u201d, let up,c \u2208 Rn be the update from worker p at clock c applied to the system state x \u2208 Rn via x\u2190 x + up,c. Consider the update sequence u\u0302t that orders the updates based on the global time-stamp they are generated. We can define \u201creal-time sequence\u201d x\u0302t as\nx\u0302t := x0 + t\u2211\nt\u2032=1\nu\u0302t\u2032\nassuming all workers start from the agreed-upon initial state x0. (Note that x\u0302t is different from the parameter server view as the updates from different workers can arrive the server in a different order due to network.) Let x\u0306t be the noisy view some worker w sees when generating update u\u0302t, i.e., u\u0302t := G(x\u0306t) for some function G. The VAP condition guarantees\n||x\u0306t \u2212 x\u0302t||\u221e \u2264 vt = v0\u221a t\n(1)\nwhere we require the value bound vt to shrink over time from the initial bound v0. Notice that x\u0306t \u2212 x\u0302t is exactly the updates in transit w.r.t. worker w. We make mild assumptions to avoid pathological cases.2\nTheorem 1. (SGD under VAP, convergence in expectation) Given convex function f(x) = \u2211T\nt=1 ft(x) such that components ft are also convex. We search for minimizer x\u2217 via gradient descent on each component \u2207ft with step-size \u03b7\u0306t close to \u03b7t = \u03b7\u221at such that the update u\u0302t = \u2212\u03b7\u0306t\u2207ft(x\u0306t) is computed on noisy view x\u0306t. The VAP bound follows the decreasing vt described above. Under suitable conditions (ft are L-Lipschitz and bounded diameter D(x\u2016x\u2032) \u2264 F 2),\nR[X] := T\u2211 t=1 ft(x\u0306t)\u2212 f(x\u2217) = O( \u221a T )\nand thus R[X]T \u2192 0 as T \u2192\u221e.\nTheorem 1 implies that the worker\u2019s noisy VAP view x\u0306t converges to the global optimum x\u2217, as measured by f , in expectation at the rate O(T\u22121/2). The analysis is similar to [8], but we use the real-time sequence x\u0302t as our reference sequence and VAP condition instead of SSP. Loosely speaking, Theorem 1 shows that VAP execution is unbiased. We now present a new bound on the variance.\n2To avoid pathological cases where a worker is delayed indefinitely, we assume that each worker\u2019s updates are finitely apart in sequence u\u0302t. In other words, all workers generate updates with sufficient frequency. For SGD, we further assume that each worker updates its step-sizes sufficiently often that the local step-size \u03b7\u0306t = \u03b7\u221at\u2212r for some bounded drift r \u2265 0 and thus \u03b7\u0306t is close to the global step size schedule \u03b7t = \u03b7\u221at .\nTheorem 2. (SGD under VAP, bounded variance) Assuming f(x), \u03b7\u0306t, and vt similar to theorem 1 above, and f(x) has bounded and invertible Hessian, \u2126\u2217 defined at optimal point x\u2217. Let Var t := E[x\u03062t ] \u2212 E[x\u0306t]2 (Var t is the sum of component-wise variance3), and g\u0306t = \u2207ft(x\u0306t) is the gradient, then:\nVar t+1 = Var t \u2212 2cov(x\u0302t,E\u2206t [g\u0306t]) +O(\u03b4t) (2) +O(\u03b7\u03062t \u03c12t ) +O\u2217\u03b4t (3)\nnear the optima x\u2217. The covariance cov(v1,v2) := E[vT1 v2]\u2212E[vT1 ]E[v2] uses inner product. \u03b4t = ||\u03b4t||\u221e and \u03b4t = x\u0306t\u2212 x\u0302t. \u03c1t = ||x\u0306t\u2212x\u2217||. \u2206t is a random variable capturing the randomness of update u\u0302t = \u2212\u03b7tg\u0306t conditioned on x\u0302t (see appendix).\ncov(x\u0302t,E\u2206t [g\u0306t]) \u2265 0 in general as the change in xt and average gradient E\u2206t [g\u0306t] are of the same direction. Theorem 2 implies that under VAP the variance decreases in successive iterations for sufficiently small \u03b4t, which can be controlled via VAP threshold vt. However, as we argued in the previous section, the VAP condition requires the same amount of synchronization as strong consistency, which makes it of little practical benefit. This motivates our following analysis of the SSP model.\nSSP We return to the (p, c) index. Under the SSP worker p at clock c only has access to a noisy view x\u0303p,c of the system state (x\u0303 is different from the noisy view in VAP x\u0306). Update up,c = G(x\u0303p,c) is computed on the noisy view x\u0303p,c for some function G(). Assuming all workers start from the agreed-upon initial state x0, the SSP condition is:\nSSP Bounded-Staleness (formal): For a fixed staleness s, the noisy state x\u0303p,c is equal to\nx\u0303p,c = x0 + c\u2212s\u22121\u2211 c\u2032=1 P\u2211 p\u2032=1 up\u2032,c\u2032  \ufe38 \ufe37\ufe37 \ufe38\nguaranteed pre-window updates\n+  \u2211 (p\u2032,c\u2032)\u2208Sp,c up\u2032,c\u2032  \ufe38 \ufe37\ufe37 \ufe38\nbest-effort in-window updates\n,\nfor some Sp,c \u2286 Wp,c = {1, ..., P} \u00d7 {c \u2212 s, ..., c + s \u2212 1} which is some subset of updates in the 2s window issued by all P workers during clock c\u2212 s to c+ s\u2212 1. The noisy view consists of (1) guaranteed pre-window updates for clock 1 to c \u2212 s \u2212 1, and (2) best-effort updates indexed by Sp,c.4 We introduce a clock-major index t:\nx\u0303t := x\u0303(t mod P ),bt/P c ut := u(t mod P ),bt/P c\nand analogously for St and Wt. We can now define a reference sequence (distinct from x\u0302t in VAP) which we informally refers to as the \u201ctrue\u201d sequence:\nxt = x0 + t\u2211\nt\u2032=0\nut\u2032 (4)\nThe sum loops over workers (t mod P ) and clocks bt/P c . Notice that this sequence is unrelated to the server view.\nTheorem 3. (SGD under SSP, convergence in expectation [8], Theorem 1) Given convex function f(x) =\u2211T t=1 ft(x) with suitable conditions as in Theorem 1, we use gradient descent with updates ut = \u2212\u03b7t\u2207ft(x\u0303t) generated from noisy view x\u0303t and \u03b7t = \u03b7\u221at . Then\n3Var t = \u2211d i=1 E[x\u0306 2 ti] \u2212 E[x\u0306ti]2 4In contrast to [8], we do not assume read-my-write.\nR[X] := T\u2211 t=1 ft(x\u0303t)\u2212 f(x\u2217) = O( \u221a T )\nand thus R[X]T \u2192 0 as T \u2192\u221e.\nTheorem 3 is the SSP-counterpart of Theorem 1. The analysis of Theorem 3 only uses the worst-case SSP bounds. However, in practice many updates are much less stale than the SSP bound. In Fig 1 (left) both implementations have small portion of updates with maximum staleness.\nWe now use moment statistics to further characterize the convergence. We begin by decomposing x\u0303t. Let u\u0304t := 1P (2s+1) \u2211 t\u2032\u2208Wt ||ut\u2032 ||2 be the average of `2 of the updates. We can write the noisy view x\u0303t as\nx\u0303t = xt + u\u0304t\u03b3t (5)\nwhere \u03b3t \u2208 Rd is a vector of random variables whose randomness lies in the network communication. Note that the decomposition in eq. 5 is always possible since u\u0304t = 0 iff ut\u2032 = 0 for all updates ut\u2032 in the 2s window. Using SSP we can bound u\u0304t and \u03b3t:\nLemma 4. u\u0304t \u2264 \u03b7\u221atL and \u03b3t := ||\u03b3t||2 \u2264 P (2s+ 1).\nTherefore \u00b5\u03b3 = E[\u03b3t] and \u03c3\u03b3 = var(\u03b3t) are well-defined. We now provide an exponential tail-bound characterizing convergence in finite steps.\nTheorem 5. (SGD under SSP, convergence in probability) Given convex function f(x) = \u2211T\nt=1 ft(x) such that components ft are also convex. We search for minimizer x\u2217 via gradient descent on each component \u2207ft under SSP with staleness s and P workers. Let ut := \u2212\u03b7t\u2207tft(x\u0303t) with \u03b7t = \u03b7\u221at . Under suitable conditions (ft are L-Lipschitz and bounded divergence D(x||x\u2032) \u2264 F 2), we have\nP\n[ R [X]\nT \u2212 1\u221a\nT\n( \u03b7L2 + F 2\n\u03b7 + 2\u03b7L2\u00b5\u03b3\n) \u2265 \u03c4 ] \u2264 exp { \u2212T\u03c42\n2\u03b7\u0304T\u03c3\u03b3 + 2 3\u03b7L 2(2s+ 1)P\u03c4\n}\nwhere R[X] := \u2211T\nt=1 ft(x\u0303t)\u2212 f(x\u2217), and \u03b7\u0304T = \u03b72L4(lnT+1) T = o(T ).\nThis means that R[X]T converges to O(T \u22121/2) in probability with an exponential tail-bound. Also note\nthat the convergence is faster for smaller \u00b5\u03b3 and \u03c3\u03b3 . We need a few mild assumptions on the staleness \u03b3t in order to derive variance bound:\nAssumption 1. \u03b3t are i.i.d. random variable with well-defined mean \u00b5\u03b3 and variance \u03c3\u03b3 .\nAssumption 2. \u03b3t is independent of xt and ut.\nAssumption 1 is satisfied by Lemma 4, while Assumption 2 is valid since \u03b3t are only influenced by the computational load and network bandwidth at each machine, which are themselves independent of the actual values of the computation (ut and xt). We now present an SSP variance bound.\nTheorem 6. (SGD under SSP, decreasing variance) Given the setup in Theorem 5 and assumption 1-3. Further assume that f(x) has bounded and invertible Hessian \u2126\u2217 at optimum x\u2217 and \u03b3t is bounded. Let\nVar t := E[x\u03032t ] \u2212 E[x\u0303t]2, gt = \u2207ft(x\u0303t) then for x\u0303t near the optima x\u2217 such that \u03c1t = ||x\u0303t \u2212 x\u2217|| and \u03bet = ||gt|| \u2212 ||gt+1|| are small:\nVar t+1 = Var t \u2212 2\u03b7tcov(xt,E\u2206t [gt]) +O(\u03b7t\u03bet) (6) +O(\u03b72t \u03c12t ) +O\u2217\u03b3t (7)\nwhere the covariance cov(v1,v2) := E[vT1 v2]\u2212 E[vT1 ]E[v2] uses inner product. O\u2217\u03b3t represents high order (\u2265 5th) terms involving \u03b3t = ||\u03b3t||\u221e. \u2206t is a random variable capturing the randomness of update ut conditioned on xt (see appendix).\nAs argued before, cov(xt,E\u2206t [gt]) \u2265 0 in general. Therefore the theorem implies that Var t monotonically decreases over time when SGD is close to an optima."}, {"heading": "Comparison of VAP and ESSP", "text": "From Theorem 2 and 6 we see that both VAP and (E)SSP achieves decreasing variance. However, VAP convergence is much more sensitive to its tuning parameter (the VAP threshold) than (E)SSP, whose tuning parameter is the staleness s. This is evident from the O(\u03b4t) term in Eq. 3, which is bounded by the VAP threshold. In contrast, (E)SSP\u2019s variance only involves staleness \u03b3t in high order terms O\u2217\u03b3t (Eq. 7), where \u03b3t is bounded by staleness. This implies that staleness-induced variance vanishes quickly in (E)SSP. The main reason for (E)SSP\u2019s weak dependency on staleness because SGD\u2019s step-size already tunes the update magnitude approaching optimum, and thus for the same number of missing updates (such as the 2s window in SSP), their total magnitude is decreasing as well, which is conducive for lowering variance. VAP on the other hand, does not make use of the decreasing step-size and thus needs to directly rely on the VAP threshold, resulting in strong dependency on the threshold.\nAn intuitive analogy is that of postmen: VAP is like a postman who only ever delivers mail above a certain weight threshold W . (E)SSP is like a postman who delivers mail late, but no later than T days. Intuitively, the (E)SSP postman is more reliable than the VAP postman due to his regularity. The only way for the VAP postman to be reliable, is to decrease the weight threshold W \u2192 0 \u2014 this becomes important when the ML algorithm is approaching convergence, because the algorithm\u2019s updates become diminishingly small. However, there are two drawbacks to decreasing W : first, much like step-size tuning, it must be done at a carefully controlled rate \u2014 this requires either specific knowledge about the ML problem, or a sophisticated, automatic scheme (that may also be domain-specific). Second, asW decreases, VAP produces more communication, which increases the running time of the distributed algorithm.\nIn contrast to VAP, ESSP does not suffer as much from these drawbacks, because: (1) SSP has a weaker theoretical dependency on its staleness threshold (than VAP does on its value-bound threshold), thus it is usually unnecessary to decrease the staleness as the ML algorithm approaches convergence; this is evidenced by the SSP paper [8], which achieved stable convergence even though they did not decrease staleness gradually during ML algorithm execution. (2) Because ESSP proactively pushes out fresh parameter values, the distribution of stale reads is usually close to zero-staleness, regardless of the actual staleness threshold used (see Figure 1) \u2014 hence, fine-grained tuning of the staleness threshold is rarely necessary under ESSP."}, {"heading": "ESSPTable: An efficient ESSP System", "text": "Our theory suggests that the ESSP consistency model, with its aggressive parameter updates, should considerably outperform SSP. In order to verify this, we implemented ESSP inside a Parameter Server (PS), which we call ESSPTable.\nPS Interface: Each physical machine runs one ESSPTable process with three types of threads: 1) computation threads; 2) communication threads and 3) server threads. Each computation thread is regarded as a distinct worker by the system. The computation threads execute application logic and access the global parameters stored in ESSPTable through a key-value store interface\u2014read a table-row via GET and write via INC. Once a computation thread completes a clock tick, it notifies the system via CLOCK, which increments the worker\u2019s clock by 1. As required by the SSP consistency , a READ issued by a worker at clock c is guaranteed to observe all updates generated in clock [0, c \u2212 s \u2212 1], where s is the user-defined staleness threshold. Ensuring Consistency Guarantees: The ESSPTable client library caches locally accessed parameters. In case the parameter is too large to fit into client machine\u2019s memory, cold parameters are evicted using an approximate Least-Recently-Used (LRU) policy. When a computation thread issues a GET request, it first checks the local cache for the requested parameters. If the requested parameter is not found in local cache, a read request is sent to the server.\nEach parameter in the client local cache is associated with a clock cparam; cparam = x means that all updates from all workers generated before clock x have already been applied to this parameter. cparam is compared with the worker\u2019s clock cworker. Only if cparam > cworker\u2212s, the requested parameter is returned to the worker. Communication Protocol: The updates generated by computation threads are coalesced since they are commutative and associative. These updates are sent to the server at the end of each clock tick.\nThe server sends updated parameters to the client through call-backs. When a client request a tablerow for the first time, it registers a call-back on the server. This is the only time the client makes read request to the server. Subsequently, when a server table\u2019s clock advances from getting the clock tick from all clients, it pushes out the table-rows to the respective registered clients. This differs from the SSPTable in [8] where the server passively sends out updates upon client\u2019s read request (which happens each time a client\u2019s local cache becomes too stale). The call-back mechanism exploits the fact that computation threads often revisit the same parameters in iterative-convergent algorithms, and thus the server can push out table-rows to registered clients without clients\u2019 explicit request. Our server-push model causes more eager communication and thus lower empirical staleness than SSPTable in [8] as shown in Fig. 1 (left).\nWe empirically observed that the time needed to communicate the coalesced updates accumulated in one clock is usually less than the computation time. Thus computation threads usually observe parameters with staleness 1 regardless of the user-specified staleness threshold s. That relieves the burden of staleness tuning. Also, since the server pushes out updated parameters to registered clients in batches, it reduces the overall latency from sending each parameter separately upon clients\u2019 requests (which is the case in SSPTable). This improvement is shown in our experiments."}, {"heading": "Experiments", "text": "We show that ESSP improves the speed and quality of convergence (versus SSP) for collapsed gibbs sampling in topic model and stochastic gradient descent (SGD) in matrix factorization. Furthermore, ESSP is robust against the staleness setting, relieving the user from worrying about an additional tuning parameter. The experimental setups are:\n\u2022 ML Models and algorithms: LDA topic modeling (using collapsed Gibbs sampling) and Matrix Factorization (stochastic gradient descent). Both algorithms are implemented using ESSPTable\u2019s interface. For LDA we use 50% minibatch in each Clock() call, and we use log-likelihood as measure of training quality. For MF we use 1% and 10% minibatch in each Clock() and record the squared loss (instead of the `2-penalized objective) for convenient comparison with GraphLab (see appendix). The step size for MF is chosen to be large while the algorithm still converges with staleness 0.\n\u2022 Datasets Topic model: New York Times (N = 100m tokens, V = 100k vocabularies, and K = 100 topics). Matrix factorization: Netflix dataset (480k by 18k matrix with 100m nonzeros.) We use rank K = 100.\n\u2022 Compute cluster Matrix factorization experiments were run on 64 nodes, each with 2 cores and 16GB RAM, connected via 1Gbps ethernet. LDA experiments were run on 8 nodes, each with 64 cores and 128GB memory, connected via 1Gbps ethernet.\nSpeed of Convergence: Figure 2 shows the objective over iteration and time for LDA and matrix factorization. In both cases ESSP converges faster or comparable to SSP with respect to iteration and run time. The speed up over iteration is due to the reduced staleness as shown in the staleness profile (Figure 1, left). This is consistent with the fact that in SSP, computation making use of fresh data makes more progress [8]. Also it is worth pointing out that staleness helps SSP substantially but much less so for ESSP because ESSP is less susceptible to staleness.\nRobustness to Staleness: One important tuning knob in SGD-type of algorithms are the step size. Using step sizes that are too small leads to slow convergence, while step sizes that are too large cause divergence. The problem of stepsize tuning is aggravated in the distributed setting, where staleness could aggregate the updates in a non-deterministic manner, thus causing unpredictable performance (dependent on network congestion and the varying machine speeds). In the case of MF, SSP diverges under high staleness, as staleness effective increases the step size. However, ESSP is robust across all investigated staleness values due to the concentrated staleness profile (see Figure 1, left). Even when high SSP staleness does not produce divergence, the convergence is \u201cshaky\u201d due to the variance introduced by staleness. ESSP produces lower variance for all staleness settings, consistent with our theoretical analyses. This improvement largely reduced the need for user to tune the staleness parameter introduced in SSP.\nSystem Opportunity. In addition to faster convergence per iteration, ESSP provides opportunities for system to optimize the communication. By sending updates preemptively, ESSP not only reduces the staleness but also reduces the chance of client threads being blocked to wait for updates. In some sense ESSP is a more \u201cpipelined\u201d version of SSP. Figure 1 (right) shows the breakdown of communication and compuation time for varying staleness. This contributes to the overall convergence per second in Figure 2, where ESSP has larger margin of speed gain over SSP than the convergence per iteration."}, {"heading": "Related Work and Discussion", "text": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].\nThe primary differences between the general-purpose frameworks (including this work) and the specialpurpose solvers are (1) the former are user-programmable and can be extended to handle arbitrary ML applications, while the latter are non-programmable and restricted to predefined ML applications; (2) because the former must support arbitrary ML programs, their focus is on improving the \u201csystems\u201d code (notably, communication and synchronization protocols for model state) to increase the efficiency of all ML algorithms, particularly through the careful design of consistency models (graph consistency in GraphLab, and iteration/value-bounded consistency in Parameter Servers) \u2014 in contrast, the special-purpose systems combine both systems code improvements and algorithmic (i.e. mathematical) improvements tailor-made for their specific category of ML applications.\nAs a paper about general-purpose distributed ML, we focus on consistency models and systems code, and we deliberately use (relatively) simple algorithms for our benchmark applications, for two reasons: (1) to provide a fair comparison, we must match the code/algorithmic complexity of the benchmarks for other frameworks like GraphLab and SSP PS [8] (practically, this means our applications should use the same update equations); (2) a general-purpose ML framework should not depend on highly-specialized algorithmic techniques tailored only to specific ML categories. The crux is that general-purpose frameworks should democratize distributed ML in a way that special-purpose solvers cannot, by enabling those ML applications that have been under-served by the distributed ML research community to benefit from cluster computing. Since our benchmark applications are kept algorithmically simple, they are unlikely to beat the special-purpose solvers in running time \u2014 but we note that many algorithmic techniques featured in those solvers can be applied to our benchmark applications, by dint of the general-purpose nature of PS programming.\nIn [13], the authors propose and implement a PS consistency model that has similar theoretical guarantees to the ideal VAP model presented herein. However, we note that their implementation does not strictly enforce the conditions of their consistency model. Their implementation implicitly assumes zero latency for transmission over network, while in a real cluster, there could be arbitrarily long network delay. In their system, reads do not wait for delayed updates, so a worker may compute with highly inconsistent parameters in the case of congested network.\nOn the wider subject of Big Data, Hadoop [2] and Spark [19] are popular programming frameworks, which ML applications are sometimes developed on top of. To the best of our knowledge, there is no recent work showing that Hadoop or Spark have superior ML algorithm performance compared to frameworks designed for ML like GraphLab and PSes (let alone the special-purpose solvers mentioned earlier). The most salient difference is that Hadoop and Spark only feature strict consistency, and do not support flexible consistency models like graph- or bounded-consistency. On the positive side, Hadoop and Spark ensure program portability, reliability and fault tolerance at a level that GraphLab and PSes have yet to match."}, {"heading": "Appendix", "text": "Theorem 1 (SGD under VAP, convergence in expectation) Given convex function f(x) = \u2211T\nt=1 ft(x) such that components ft are also convex. We search for minimizer x\u2217 via gradient descent on each component \u2207ft with step-size \u03b7\u0306t close to \u03b7t = \u03b7\u221at such that the update u\u0302t = \u2212\u03b7\u0306t\u2207ft(x\u0306t) is computed on noisy view x\u0306t. The VAP bound follows the decreasing vt described above. Under suitable conditions (ft are L-Lipschitz and bounded diameter D(x\u2016x\u2032) \u2264 F 2),\nR[X] := T\u2211 t=1 ft(x\u0306t)\u2212 f(x\u2217) = O( \u221a T )\nand thus R[X]T \u2192 0 as T \u2192\u221e.\nProof. We will use real-time sequence x\u0302t defined by\nx\u0302t := x0 + t\u2211\nt\u2032=1\nu\u0302t\u2032\nR[X] = T\u2211 t=1 ft(x\u0306t)\u2212 f(x\u2217)\n\u2264 T\u2211 t=1 \u3008\u2207ft(x\u0306t), x\u0306t \u2212 x\u2217\u3009 (ft are convex)\n= T\u2211 t=1 \u3008g\u0306t, x\u0306t \u2212 x\u2217\u3009\nwhere g\u0306t := \u2207ft(x\u0306t). From Lemma A.1 below we have\nR[X] \u2264 T\u2211 t=1 1 2 \u03b7\u0306t||g\u0306t||2 + D(x\u2217||x\u0302t)\u2212D(x\u2217||x\u0302t+1) \u03b7\u0306t + \u3008x\u0306t \u2212 x\u0302t, g\u0306t\u3009\nWe now bound each term:\nT\u2211 t=1 1 2 \u03b7\u0306t||g\u0306t||2 \u2264 T\u2211 t=1 1 2 \u03b7\u0306tL 2 (Lipschitz assumption)\n= T\u2211\nt=r+1\n1\n2 \u03b7\u221a t\u2212 r L2 + const (r > 0 is the finite clock drift in VAP)\n= 1\n2 \u03b7L2 T\u2211 t=r+1 1\u221a t\u2212 r + const\n\u2264 1 2 \u03b7L2 \u222b T t=r+1 1\u221a t\u2212 r dt+ const \u2264 1 2 \u03b7L2( \u221a T \u2212 r \u2212 1) + const = O( \u221a T )\nwhere the clock drift comes from the fact that \u03b7\u0306t is not exactly \u03b7t = \u03b7\u221at in VAP.\nT\u2211 t=1 D(x\u2217||x\u0302t)\u2212D(x\u2217||x\u0302t+1) \u03b7\u0306t = D(x\u2217||x\u03021) \u03b7\u03061 \u2212 D(x \u2217||x\u0302T+1) \u03b7\u0306T + T\u2211 t=2 [ D(x\u2217||x\u0302t) ( 1 \u03b7\u0306t \u2212 1 \u03b7\u0306t\u22121 )]\n\u2264 F 2\n\u03b7 + 0 +\nF 2\n\u03b7 T\u2211 t=2 [\u221a t\u2212 k \u2212 \u221a t\u2212 r ] (clock drift)\n\u2264 F 2 \u03b7 + F 2 \u03b7 \u222b T t=max(k,r) (\u221a t\u2212 k \u2212 \u221a t\u2212 r ) dt+ const = F 2\n\u03b7 + F 2 \u03b7\n[ (t\u2212 k)3/2 \u2212 (t\u2212 r)3/2 ]T max(k,r) + const\n= F 2 \u03b7 + F 2 \u03b7\n[ (T \u2212 k)3/2 \u2212 (T \u2212 r)3/2 ] + const\n= F 2 \u03b7 + F 2 \u03b7\n[( T 3 2 + 3\n2 kT\n1 2 +O( \u221a T ) ) \u2212 ( T 3 2 + 3\n2 rT\n1 2 +O( \u221a T ) )] + const (binomial expansion)\n= O( \u221a T )\nT\u2211 t=1 \u3008x\u0306t \u2212 x\u0302t, g\u0306t\u3009 \u2264 T\u2211 t=1 ||x\u0306t \u2212 x\u0302t||2||g\u0306t||2\n\u2264 T\u2211 t=1 \u221a dvtL (using eq.(2) from main text)\n= \u221a dL T\u2211 t=1 v0\u221a t = \u221a dLv0 \u221a T = O( \u221a T )\nTogether, we have R[X] \u2264 O( \u221a T ) as desired.\nLemma A.1 For x\u2217, x\u0306t \u2208 X , and X = Rd,\n\u3008g\u0306t, x\u0306t \u2212 x\u2217\u3009 = 1\n2 \u03b7\u0306t||g\u0306t||2 + D(x\u2217||x\u0302t)\u2212D(x\u2217||x\u0302t+1) \u03b7\u0306t + \u3008x\u0306t \u2212 x\u0302t, g\u0306t\u3009\nwhere D(x||x\u2032) := 12 ||x\u2212 x \u2032||2.\nProof.\nD(x\u2217||x\u0302t)\u2212D(x\u2217||x\u0302t+1) = 1 2 ||x\u2217 \u2212 x\u0302t + x\u0302t \u2212 x\u0302t+1||2 \u2212 1 2 ||x\u2217 \u2212 x\u0302t||2\n= 1\n2 ||x\u2217 \u2212 x\u0302t + \u03b7\u0306tg\u0306t||2 \u2212\n1 2 ||x\u2217 \u2212 x\u0302t||2\n= 1\n2 \u03b7\u0306t||g\u0306t||2 \u2212 \u03b7\u0306t\u3008x\u0302t \u2212 x\u2217, g\u0306t\u3009\nDivide both sides by \u03b7\u0306t gets the desired answer.\nLemma 4 u\u0304t \u2264 \u03b7\u221atL and \u03b3t := ||\u03b3t||2 \u2264 P (2s+ 1).\nProof. ||ut||2 = ||\u2212\u03b7t\u2207ft||2 \u2264 \u03b7\u221atL since f isL-Lipschitz. Therefore u\u0304t = 1 P (2s+1) \u2211 t\u2032\u2208Wt ||ut\u2032 ||2 \u2264 \u03b7\u221a t L since |Wt| \u2264 P (2s+ 1). If u\u0304t = 0, then \u03b3t = 0 and the lemma holds trivially. For u\u0304t > 0. \u03b3t = 1u\u0304t (x\u0303t \u2212 xt) = 1 u\u0304t \u2211 t\u2032\u2208St ut\u2032 .\nThus ||\u03b3t||2 = 1u\u0304t || \u2211 t\u2032\u2208St ut\u2032 ||2 \u2264 1 u\u0304t \u2211 t\u2032\u2208St ||ut\u2032 ||2 \u2264 1 u\u0304t \u2211 t\u2032\u2208Wt ||ut\u2032 ||2 = P (2s+ 1).\nTheorem 5 (SGD under SSP, convergence in probability) Given convex function f(x) = \u2211T\nt=1 ft(x) such that components ft are also convex. We search for minimizer x\u2217 via gradient descent on each component \u2207ft under SSP with staleness s and P workers. Let ut := \u2212\u03b7t\u2207tft(x\u0303t) with \u03b7t = \u03b7\u221at . Under suitable conditions (ft are L-Lipschitz and bounded divergence D(x||x\u2032) \u2264 F 2), we have\nP\n[ R [X]\nT \u2212 1\u221a\nT\n( \u03b7L2 + F 2\n\u03b7 + 2\u03b7L2\u00b5\u03b3\n) \u2265 \u03c4 ] \u2264 exp { \u2212T\u03c42\n2\u03b7\u0304T\u03c3\u03b3 + 2 3\u03b7L 2(2s+ 1)P\u03c4\n}\nwhere R[X] := \u2211T\nt=1 ft(x\u0303t)\u2212 f(x\u2217), and \u03b7\u0304T = \u03b72L4(lnT+1) T = o(T ).\nProof. From lemma A.1, substitute x\u0306t with x\u0303t we have\nR [X] \u2264 T\u2211 t=1 \u3008g\u0303t, x\u0303t \u2212 x\u2217\u3009\n= T\u2211 t=1 1 2 \u03b7t \u2016g\u0303t\u20162 + D (x\u2217\u2016xt)\u2212D (x\u2217\u2016xt+1) \u03b7t + \u3008x\u0303t \u2212 xt, g\u0303t\u3009\n\u2264 \u03b7L2 \u221a T + F 2\n\u03b7\n\u221a T + T\u2211 t=1 \u3008u\u0304t\u03b3t, g\u0303t\u3009\n\u2264 \u03b7L2 \u221a T + F 2\n\u03b7\n\u221a T + T\u2211 t=1 \u03b7\u221a t L2\u03b3t\nWhere the last step uses the fact\n\u3008u\u0304t\u03b3t, g\u0303t\u3009 \u2264 u\u0304t||\u03b3t||2||g\u0303t||2 \u2264 \u03b3t\n\u03b7\u221a t L2 (Lemma 4)\nDividing T on both sides,\nR [X] T \u2212 \u03b7L\n2\n\u221a T \u2212 F\n2\n\u03b7 \u221a T \u2264\n\u2211T t=1 \u03b7\u221a t L2\u03b3t\nT (8)"}, {"heading": "Let at := \u03b7\u221atL", "text": "2(\u03b3t \u2212 \u00b5\u03b3). Notice that at zero-mean, and |at| \u2264 \u03b7L2 maxt(\u03b3t) \u2264 \u03b7L2(2s + 1)P . Also, 1 T \u2211T t=1 var(at) = 1 T \u2211T t=1 \u03b72 t L 4\u03c3\u03b3 < \u03b72L4\u03c3\u03b3 T (lnT + 1) = \u03b7\u0304T\u03c3\u03b3 where \u03b7\u0304T = \u03b72L4(lnT+1) T . Bernstein\u2019s\ninequality gives, for \u03c4 > 0,\nP \u2211Tt=1 \u03b7\u221atL2\u03b3t \u2212 \u03b7\u221atL2\u00b5\u03b3 T \u2265 \u03c4  \u2264 exp{ \u2212T\u03c42 2\u03b7\u0304T\u03c3\u03b3 + 2 3\u03b7L 2(2s+ 1)P\u03c4 } (9)\nNote the following identity: b\u2211 i=a 1\u221a i \u2264 2 \u221a b\u2212 a+ 1 (10)\nThus 1\nT T\u2211 t=1 \u03b7\u221a t L2\u00b5\u03b3 \u2264 2\u03b7L2\u00b5\u03b3\u221a T\n(11)\nPlugging eq. 8 and 11 to eq. 9, we have\nP\n[ R [X]\nT \u2212 1\u221a\nT\n( \u03b7L2 + F 2\n\u03b7 + 2\u03b7L2\u00b5\u03b3\n) \u2265 \u03c4 ] \u2264 exp { \u2212T\u03c42\n2\u03b7\u0304T\u03c3\u03b3 + 2 3\u03b7L 2(2s+ 1)P\u03c4\n}\nWe need the following Lemma to prove Theorem 2 and 6.\nLemma A.2 Let \u2126\u2217 be the hessian of the loss at optimum x\u2217, then\ngt := \u2207f(x\u0303t) = (x\u0303t \u2212 x\u2217)\u2126\u2217 +O(\u03c12t )\nfor x\u0303t close to the optimum such that O(\u03c1t) = O(||x\u0303t \u2212 x\u2217||) is small. Here \u2126\u2217 = \u22072f(x) \u2223\u2223 x=x\u2217 is the Hessian at the optimum\nProof. Using Taylor\u2019s theorem and expanding around x\u2217,\nf(x\u0303t) = f(x\u2217) + (x\u0303t \u2212 x\u2217)T \u2207f(x)|x=x\u2217\n+ 1\n2 (x\u0303t \u2212 x\u2217)T\u2126\u2217(x\u0303t \u2212 x\u2217) +O(||x\u0303t \u2212 x\u2217||3)\n= f(x\u2217) + 1\n2 (x\u0303t \u2212 x\u2217)T\u2126\u2217(x\u0303t \u2212 x\u2217) +O(||x\u0303t \u2212 x\u2217||3)\nwhere the last step uses\u2207f(x) = 0 at x\u2217. Taking gradient w.r.t. x\u0303t,\n\u2207f(x\u0303t) = (x\u0303t \u2212 x\u2217)T\u2126\u2217 +O(||x\u0303t \u2212 x\u2217||2) = (x\u0303t \u2212 x\u2217)T\u2126\u2217 +O(\u03c12t )\nTheorem 6 (SGD under SSP, decreasing variance) Given the setup in Theorem 5 and assumption 1-3. Further assume that f(x) has bounded and invertible Hessian \u2126\u2217 at optimum x\u2217 and \u03b3t is bounded. Let Var t := E[x\u03032t ] \u2212 E[x\u0303t]2, gt = \u2207ft(x\u0303t) then for x\u0303t near the optima x\u2217 such that \u03c1t = ||x\u0303t \u2212 x\u2217|| and \u03bet = ||gt|| \u2212 ||gt+1|| are small:\nVar t+1 = Var t \u2212 2\u03b7tcov(xt,E\u2206t [gt]) +O(\u03b7t\u03bet) +O(\u03b72t \u03c12t ) +O\u2217\u03b3t\nwhere the covariance cov(v1,v2) := E[vT1 v2] \u2212 E[vT1 ]E[v2] uses inner product. O\u2217\u03b3t represents high order (\u2265 5th) terms involving \u03b3t = ||\u03b3t||\u221e. \u2206t is a random variable capturing the randomness of update ut conditioned on xt.\nProof. We write eq. 3 from the main text as x\u0303t = xt + \u03b4t with \u03b4t = u\u0304t\u03b3t. Conditioned on xt, we have\np(x\u0303t|xt)dx\u0303t = p(Vt(\u03b4t, xt))dVt (12)\nwhere Vt is a random variable representing the state of \u03b4t conditioned on xt. We can express Ex\u0303t [f(x\u0303t)] in terms of Ext for any function f() of x\u0303t:\nEx\u0303t [f(x\u0303t)] = \u222b\nx\u0303t f(x\u0303t)p(x\u0303t)dx\u0303t\n= \u222b x\u0303t \u222b xt f(x\u0303t)p(x\u0303t|xt)p(xt)dxtdx\u0303t (using eq. 12)\n= \u222b xt \u222b Vt f(x\u0303t)p(Vt(\u03b4t, xt))dVtdxt\n= Ext [ EVt [f(x\u0303t)] ] (13)\nSimilarly, we have Ex\u0303t+1 [f(x\u0303t+1)] = Ext+1 [ EVt+1 [f(x\u0303t+1)] ] (14)\nIn the same vein, we introduce random variable \u2206, conditioned on xt:\np(xt+1|xt)dxt+1 = p(\u2206t(ut, xt))d\u2206t (15)\nsince xt+1 = xt + ut (eq. 2 in the main text). Here \u2206 is a random variable representing the state of ut conditioned on xt. Analogous to eq. 13, we have\nExt+1 [f(xt+1)] = Ext [E\u2206t [f(xt+1)]] (16)\nfor some function f() of xt+1. There are a few facts we will use throughout: Ext [ h(xt, u\u0304t)EVt [\u03b3t] ] = Ext [h(xt, u\u0304t)]EVt [\u03b3t] (since \u03b3t\u22a5xt, u\u0304t) (17)\nExt [ E\u2206t [xTt g(ut)] ] = Ext [ xTt E\u2206t [g(ut)] ] (\u2206t conditioned on xt) (18)\nE\u2206t [u\u0304t+1] = u\u0304t+1 (19)\nwhere h(xt, u\u0304t) is some function of xt and u\u0304t, and similarly for g(). Eq. 19 follows from u\u0304t+1 being an average over the randomness represented by \u2206t. We can now expand Var t:\nVar t = Ex\u0303t [x\u03032t ]\u2212 (Ex\u0303t [x\u0303t])2\n= Ext [EVt [x\u03032t ]]\u2212 (Ext [EVt [x\u0303t]])2 (using eq. 13) = Ext [EVt [x2t + \u03b42t + 2xTt \u03b4t]]\u2212 (Ext [EVt [xt + \u03b4t]])2 (20)\nWe expand each term:\nExt [EVt [x2t + \u03b42t + 2xTt \u03b4t]] = Ext [x2t + EVt [\u03b42t ] + 2xTt EVt [\u03b4t]] = Ext [x2t ] + Ext [u\u03042tEVt [\u03b32t ]] + 2Ext [xTt u\u0304tEVt [\u03b3t]] = Ext [x2t ] + Ext [u\u03042t ]EVt [\u03b32t ] + 2Ext [xTt u\u0304t]EVt [\u03b3t]\n(Ext [EVt [xt + \u03b4t]])2\n= (Ext [xt + EVt [\u03b4t]])2 = (Ext [xt + u\u0304tEVt [\u03b3t]])2 = (Ext [xt] + Ext [u\u0304t]EVt [\u03b3t]])2 = Ext [xt]2 + Ext [u\u0304t]2EVt [\u03b3t]2 + 2Ext [xTt ]Ext [u\u0304t]EVt [\u03b3t]\nTherefore\nVar t = Ext [x2t ] + Ext [u\u03042t ]EVt [\u03b32t ] + 2Ext [xTt u\u0304t]EVt [\u03b3t] \u2212 Ext [xt]2 \u2212 Ext [u\u0304t]2EVt [\u03b3t]2 \u2212 2Ext [xTt ]Ext [u\u0304t]EVt [\u03b3t]\n(21)\nFollowing similar procedures, we can write Var t+1 as\nVar t+1 = Ext+1 [x2t+1] + Ext+1 [u\u03042t+1]EVt+1 [\u03b32t+1] + 2Ext+1 [xTt+1u\u0304t+1]EVt+1 [\u03b3t+1] \u2212 Ext+1 [xt+1]2 \u2212 Ext+1 [u\u0304t+1]2EVt+1 [\u03b3t+1]2\n\u2212 2Ext+1 [xTt+1]Ext+1 [u\u0304t+1]EVt+1 [\u03b3t+1]\n(22)\nWe tackle each term separately:\nExt+1 [x2t+1] = Ext [ E\u2206t [(xt + ut)2] ] (using eq. 16, 2 main text)\n= Ext [x2t ] + Ext [ E\u2206t [u2t ] ] + 2Ext [ xTt E\u2206t [ut] ] (using eq. 18)\n2Ext+1 [xTt+1u\u0304t+1]EVt+1 [\u03b3t+1] = 2Ext [ E\u2206t [(xt + ut)T u\u0304t+1] ] EVt+1 [\u03b3t+1] (using eq. 16, 2 main text)\n= 2Ext [ E\u2206t [xTt u\u0304t+1] ] EVt+1 [\u03b3t+1]\n+ 2Ext [ E\u2206t [uTt u\u0304t+1] ] EVt+1 [\u03b3t+1]\n= 2Ext [ xTt u\u0304t+1 ] EVt+1 [\u03b3t+1] (using eq. 18 and 19)\n+ 2Ext [ E\u2206t [uTt u\u0304t+1] ] EVt+1 [\u03b3t+1]\n\u2212Ext+1 [xt+1]2 = \u2212Ext [ E\u2206t [xt + ut] ]2 = \u2212Ext [xt]2 \u2212 Ext [ E\u2206t [ut] ]2 \u2212 2Ext [xTt ]Ext [E\u2206t [ut]]\n\u2212 2Ext+1 [xTt+1]Ext+1 [u\u0304t+1]EVt+1 [\u03b3t+1] = \u22122Ext [ E\u2206t [(xt + ut)T ] ] Ext [ E\u2206t [u\u0304t+1] ] EVt+1 [\u03b3t+1]\n= \u22122Ext [ E\u2206t [uTt ] ] Ext [u\u0304t+1]EVt+1 [\u03b3t+1]\u2212 2Ext [xTt ]Ext [u\u0304t+1]EVt+1 [\u03b3t+1]\nAssuming stationarity for \u03b3t, and thus \u03b3\u0304 := EVt [\u03b3t] = EVt+1 [\u03b3t+1], we have Var t+1 \u2212Var t = 2 { Ext [ xTt E\u2206t [ut] ] \u2212 Ext [xTt ]Ext [ E\u2206t [ut] ]} \u2212 2 { Ext [xTt (u\u0304t \u2212 u\u0304t+1)\u03b3\u0304]\u2212 Ext [xTt ]Ext [(u\u0304t \u2212 u\u0304t+1)\u03b3\u0304]\n} + { Ext [ E\u2206t [u2t ] ] + Ext+1 [u\u03042t+1]EVt+1 [\u03b32t+1]\u2212 Ext [ E\u2206t [ut]\n]2 \u2212 Ext [u\u0304t+1]2\u03b3\u03042 \u2212 Ext [u\u03042t ]EVt [\u03b32t ] + Ext [u\u0304t]2EVt [\u03b32t ] +2Ext [ E\u2206t [uTt u\u0304t+1] ] \u03b3\u0304 \u2212 2Ext [ E\u2206t [uTt ] ] Ext [u\u0304t+1]\u03b3\u0304\n} = 2cov(xt,E\u2206t [ut]) +O(\u03b7t\u03bet) +O(\u03b72t \u03c12t ) +O\u2217\nwhere \u03bet = ||gt||\u2212 ||gt+1|| andO\u2217 are higher order terms. In the last step we use the fact that ||gt|| = O(\u03c1t) (lemma A.2) and thus ||ut|| = \u03b7t||\u2207f(xt)|| and u\u0304t are both O(\u03b7t\u03c1t). Notice that cov(v1,v2) := E[vT1 v2]\u2212 E[vT1 ]E[v2] uses inner product. Thus,\nVar t+1 = Var t \u2212 2\u03b7tcov(xt,E\u2206t [gt]) +O(\u03b7t\u03bet) +O(\u03b72t \u03c12t ) +O\u2217 (23)\nTheorem 2 (SGD under VAP, bounded variance) Assuming f(x), \u03b7\u0306t, and vt similar to theorem 1, and f(x) has bounded and invertible Hessian, \u2126\u2217 defined at optimal point x\u2217. Let Var t := E[x\u03062t ] \u2212 E[x\u0306t]2 (Var t is the sum of component-wise variance5), and g\u0306t = \u2207ft(x\u0306t) is the gradient, then:\nVar t+1 = Var t \u2212 2cov(x\u0302t,E\u2206t [g\u0306t]) +O(\u03b4t) +O(\u03b7\u03062t \u03c12t ) +O\u2217\u03b4t near the optima x\u2217. The covariance cov(v1,v2) := E[vT1 v2]\u2212 E[vT1 ]E[v2] uses inner product. \u03b4t = ||\u03b4t||\u221e and \u03b4t = x\u0306t \u2212 x\u0302t. \u03c1t = ||x\u0306t \u2212 x\u2217||. \u2206t is a random variable capturing the randomness of update u\u0302t = \u2212\u03b7tg\u0306t conditioned on x\u0302t.\nProof. The proof is similar to the proof of Theorem 6. Starting off with x\u0306t = x\u0302t + \u03b4t, we define Vt, \u2206t analogously. We have\nVar t = Ex\u0302t [x\u03022t ] + Ex\u0302t [EVt [\u03b42t ]] + 2Ex\u0302t [x\u0302 T t EVt [\u03b4t]]\n\u2212 Ex\u0302t [x\u0302t]2 \u2212 Ex\u0302t [EVt [\u03b42t ]]\u2212 2Ex\u0302t [x\u0302t]Ex\u0302 T t [EVt [\u03b4t]]\nSimilar algebra as in Theorem 6 leads to\nVar t+1 \u2212Var t = 2cov(x\u0302t,E\u2206t [u\u0302t]) + 2cov(x\u0302t,EVt [\u03b4t]\u2212 E\u2206t [EVt+1 [\u03b4t+1]]) +O(\u03b42t ) +O(\u03b7\u03062t \u03c12t ) +O(\u03b7\u0306t\u03b4t) +O\u2217\n= \u22122cov(x\u0302t,E\u2206t [g\u0306t]) +O(\u03b4t) +O(\u03b7\u03062t \u03c12t ) +O\u2217\u03b4t where \u03b4t = ||\u03b4t||\u221e. This is the desired result in the theorem statement.\n5Var t = \u2211d i=1 E[x\u0306 2 ti] \u2212 E[x\u0306ti]2"}], "references": [{"title": "Scalable inference in latent variable models", "author": ["Amr Ahmed", "Moahmed Aly", "Joseph Gonzalez", "Shravan Narayanamurthy", "Alexander J. Smola"], "venue": "In WSDM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The hadoop distributed file system: Architecture and design", "author": ["Dhruba Borthakur"], "venue": "Hadoop Project Website,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Parallel coordinate descent for l1regularized loss minimization", "author": ["Joseph K. Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin"], "venue": "In International Conference on Machine Learning (ICML", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Solving the straggler problem with bounded staleness", "author": ["James Cipar", "Qirong Ho", "Jin Kyu Kim", "Seunghak Lee", "Gregory R. Ganger", "Garth Gibson", "Kimberly Keeton", "Eric Xing"], "venue": "In HotOS \u201913. Usenix,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Exploiting bounded staleness to speed up big data analytics", "author": ["Henggang Cui", "James Cipar", "Qirong Ho", "Jin Kyu Kim", "Seunghak Lee", "Abhimanu Kumar", "Jinliang Wei", "Wei Dai", "Gregory R. Ganger", "Phillip B. Gibbons", "Garth A. Gibson", "Eric P. Xing"], "venue": "In 2014 USENIX Annual Technical Conference (USENIX ATC", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["J Dean", "G Corrado", "R Monga", "K Chen", "M Devin", "Q Le", "M Mao", "M Ranzato", "A Senior", "P Tucker", "K Yang", "A Ng"], "venue": "NIPS 2012", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "J.-K. Kim", "S. Lee", "P.B. Gibbons", "G. Gibson", "G.R. Ganger", "E.P. Xing"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "and A Strehl", "author": ["J Langford", "L Li"], "venue": "Vowpal wabbit online learning project", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Primitives for dynamic big model parallelism", "author": ["Seunghak Lee", "Jin Kyu Kim", "Xun Zheng", "Qirong Ho", "Garth A Gibson", "Eric P Xing"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["Mu Li", "David G. Andersen", "Jun Woo Park", "Alexander J. Smola", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene J. Shekita", "Bor-Yiing Su"], "venue": "In Operating Systems Design and Implementation (OSDI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Parameter server for distributed machine learning", "author": ["Mu Li", "Li Zhou Zichao Yang", "Aaron Li Fei Xia", "David G. Andersen", "Alexander Smola"], "venue": "NIPS workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Parameter server for distributed machine learning, big learning workshop", "author": ["Mu Li", "Li Zhou", "Zichao Yang", "Aaron Li", "Fei Xia", "Dave Andersen", "Alex Smola"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Graphlab: A new parallel framework for machine learning", "author": ["Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M. Hellerstein"], "venue": "In Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Plda: Parallel latent dirichlet allocation for large-scale applications", "author": ["Yi Wang", "Hongjie Bai", "Matt Stanton", "Wen-Yen Chen", "Edward Y. Chang"], "venue": "In Proceedings of the 5th International Conference on Algorithmic Aspects in Information and Management,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit S Dhillon"], "venue": "In ICDM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Spark: Cluster computing with working sets", "author": ["Matei Zaharia", "N.M. Mosharaf Chowdhury", "Michael Franklin", "Scott Shenker", "Ion Stoica"], "venue": "Technical Report UCB/EECS-2010-53, EECS Department,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "The surging data volumes generated by internet activity and scientific research [7] put tremendous pressure on Machine Learning (ML) methods to scale beyond the computation and memory of a single machine.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 7, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 4, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 5, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 10, "context": "On one hand, very large data sizes (Big Data) require too much time for complex ML models to process on a single machine [1, 8, 5, 6, 12], which necessitates distributed-parallel computation over an entire cluster of machines.", "startOffset": 121, "endOffset": 137}, {"referenceID": 0, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 7, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 4, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 5, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 10, "context": "In order to share the model across machines, practitioners have recently turned to a \u201dParameter server\u201d (PS) paradigm [1, 8, 5, 6, 12].", "startOffset": 118, "endOffset": 134}, {"referenceID": 7, "context": "Many general-purpose Parameter Server (PS) systems [8, 5, 6] of ML computation provide a Distributed Shared Memory (DSM) solution to the Big Data and Big Model issues.", "startOffset": 51, "endOffset": 60}, {"referenceID": 4, "context": "Many general-purpose Parameter Server (PS) systems [8, 5, 6] of ML computation provide a Distributed Shared Memory (DSM) solution to the Big Data and Big Model issues.", "startOffset": 51, "endOffset": 60}, {"referenceID": 5, "context": "Many general-purpose Parameter Server (PS) systems [8, 5, 6] of ML computation provide a Distributed Shared Memory (DSM) solution to the Big Data and Big Model issues.", "startOffset": 51, "endOffset": 60}, {"referenceID": 10, "context": "It should be noted that not all PS systems provide a DSM interface; some espouse an arguably less-convenient push/pull interface that requires users to explicitly decide which parts of the ML model need to be communicated [12].", "startOffset": 222, "endOffset": 226}, {"referenceID": 2, "context": "At the same time, the iterative-convergent nature of ML programs presents unique opportunities and challenges that do not manifest in traditional database applications: for example, ML programs lend themselves well to stochastic subsampling or randomized algorithms, but at the same time exhibit complex dependencies or correlations between parameters that can make parallelization difficult [3, 11].", "startOffset": 392, "endOffset": 399}, {"referenceID": 9, "context": "At the same time, the iterative-convergent nature of ML programs presents unique opportunities and challenges that do not manifest in traditional database applications: for example, ML programs lend themselves well to stochastic subsampling or randomized algorithms, but at the same time exhibit complex dependencies or correlations between parameters that can make parallelization difficult [3, 11].", "startOffset": 392, "endOffset": 399}, {"referenceID": 14, "context": "Recent works [16, 8, 9] have introduced relaxed consistency models to trade off between parameter read accuracy and read throughput, and show promising speedups over fully-consistent models; their success is underpinned by the error-tolerant nature of ML, that \u201cplays nicely\u201d with relaxed synchronization guarantees \u2014 and in turn, relaxed synchronization allows system designers to achieve higher read throughput, compared to fully-consistent models.", "startOffset": 13, "endOffset": 23}, {"referenceID": 7, "context": "Recent works [16, 8, 9] have introduced relaxed consistency models to trade off between parameter read accuracy and read throughput, and show promising speedups over fully-consistent models; their success is underpinned by the error-tolerant nature of ML, that \u201cplays nicely\u201d with relaxed synchronization guarantees \u2014 and in turn, relaxed synchronization allows system designers to achieve higher read throughput, compared to fully-consistent models.", "startOffset": 13, "endOffset": 23}, {"referenceID": 3, "context": "Recent works on PS have only focused on system optimizations in PS using various heuristics like async relaxation [4] and uneven updates propagation based on parameter values [12].", "startOffset": 114, "endOffset": 117}, {"referenceID": 10, "context": "Recent works on PS have only focused on system optimizations in PS using various heuristics like async relaxation [4] and uneven updates propagation based on parameter values [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "However, VAP, of which the basic idea or principle is attempted in [14], can be problematic because bounding the value of in-transit updates amounts to tight synchronization.", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "We propose Eager Stale Synchronous Parallel (ESSP), a variant of Stale Synchronous Parallel (SSP, a bounded-iteration model that is fundamentally different from VAP) introduced in [8], and formally and empirically show that ESSP is a practical and easily realizable scheme for parallelization.", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "These variance bounds provide a deeper characterization of convergence (particularly solution stability) under SSP and VAP, unlike existing PS theory that is focused only on expectation bounds [8].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "Eager Stale Synchronous Parallel (ESSP) In order to design a consistency model that is practically efficient while providing proper correctness guarantees, we consider an iteration-based consistency model called Stale Synchronous Parallel (SSP) [8], that lends itself to an efficient PS implementation.", "startOffset": 245, "endOffset": 248}, {"referenceID": 7, "context": "In the sequel, we will show that better staleness profiles lead to faster ML algorithm convergence, by proving new, tighter convergence bounds based on average staleness and the staleness distributions (unlike the simpler worst-case bounds in [8]).", "startOffset": 243, "endOffset": 246}, {"referenceID": 7, "context": "The analysis is similar to [8], but we use the real-time sequence x\u0302t as our reference sequence and VAP condition instead of SSP.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "(SGD under SSP, convergence in expectation [8], Theorem 1) Given convex function f(x) = \u2211T t=1 ft(x) with suitable conditions as in Theorem 1, we use gradient descent with updates ut = \u2212\u03b7t\u2207ft(x\u0303t) generated from noisy view x\u0303t and \u03b7t = \u03b7 t .", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "Then Var t = \u2211d i=1 E[x\u0306 2 ti] \u2212 E[x\u0306ti] In contrast to [8], we do not assume read-my-write.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "In contrast to VAP, ESSP does not suffer as much from these drawbacks, because: (1) SSP has a weaker theoretical dependency on its staleness threshold (than VAP does on its value-bound threshold), thus it is usually unnecessary to decrease the staleness as the ML algorithm approaches convergence; this is evidenced by the SSP paper [8], which achieved stable convergence even though they did not decrease staleness gradually during ML algorithm execution.", "startOffset": 333, "endOffset": 336}, {"referenceID": 7, "context": "This differs from the SSPTable in [8] where the server passively sends out updates upon client\u2019s read request (which happens each time a client\u2019s local cache becomes too stale).", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Our server-push model causes more eager communication and thus lower empirical staleness than SSPTable in [8] as shown in Fig.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "This is consistent with the fact that in SSP, computation making use of fresh data makes more progress [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 228, "endOffset": 232}, {"referenceID": 7, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 262, "endOffset": 269}, {"referenceID": 12, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 262, "endOffset": 269}, {"referenceID": 16, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 356, "endOffset": 360}, {"referenceID": 8, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 498, "endOffset": 502}, {"referenceID": 15, "context": "Existing software that is tailored towards distributed (rather than merely single-machine parallel), scalable ML can be roughly grouped into two categories: general-purpose, programmable libraries or frameworks such as GraphLab [15] and Parameter Servers (PSes) [8, 14], or special-purpose solvers tailored to specific categories of ML applications: CCD++ [18] for Matrix Factorization, Fugue [9] for constrained MF, Vowpal Wabbit for regression/classification problems via stochastic optimization [10], and Yahoo LDA as well as Google plda for topic modeling [17].", "startOffset": 560, "endOffset": 564}, {"referenceID": 7, "context": "As a paper about general-purpose distributed ML, we focus on consistency models and systems code, and we deliberately use (relatively) simple algorithms for our benchmark applications, for two reasons: (1) to provide a fair comparison, we must match the code/algorithmic complexity of the benchmarks for other frameworks like GraphLab and SSP PS [8] (practically, this means our applications should use the same update equations); (2) a general-purpose ML framework should not depend on highly-specialized algorithmic techniques tailored only to specific ML categories.", "startOffset": 346, "endOffset": 349}, {"referenceID": 11, "context": "In [13], the authors propose and implement a PS consistency model that has similar theoretical guarantees to the ideal VAP model presented herein.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "On the wider subject of Big Data, Hadoop [2] and Spark [19] are popular programming frameworks, which ML applications are sometimes developed on top of.", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "On the wider subject of Big Data, Hadoop [2] and Spark [19] are popular programming frameworks, which ML applications are sometimes developed on top of.", "startOffset": 55, "endOffset": 59}], "year": 2014, "abstractText": "As Machine Learning (ML) applications increase in data size and model complexity, practitioners turn to distributed clusters to satisfy the increased computational and memory demands. Unfortunately, effective use of clusters for ML requires considerable expertise in writing distributed code, while highly-abstracted frameworks like Hadoop have not, in practice, approached the performance seen in specialized ML implementations. The recent Parameter Server (PS) paradigm is a middle ground between these extremes, allowing easy conversion of single-machine parallel ML applications into distributed ones, while maintaining high throughput through relaxed \u201cconsistency models\u201d that allow inconsistent parameter reads. However, due to insufficient theoretical study, it is not clear which of these consistency models can really ensure correct ML algorithm output; at the same time, there remain many theoretically-motivated but undiscovered opportunities to maximize computational throughput. Motivated by this challenge, we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models. We then use the gleaned insights to improve a consistency model using an \u201ceager\u201d PS communication mechanism, and implement it as a new PS system that enables ML algorithms to reach their solution more quickly.", "creator": "LaTeX with hyperref package"}}}