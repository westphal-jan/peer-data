{"id": "1410.8516", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2014", "title": "NICE: Non-linear Independent Components Estimation", "abstract": "namely we propose a deep learning process framework for modeling complex high - dimensional variability densities via non - linear data independent component estimation ( nice ). ideally it is completely based strictly on promoting the idea that that a strictly good representation technique is precisely one in which the average data type has a nonlinear distribution utility that is substantially easy to model. for continuing this purpose, ultimately a non - sequential linear deterministic transformation of the data body is widely learned that maps it coordinates to represent a symmetric latent space precisely so as impossible to finally make the internally transformed data which conform to completely a factorized differential distribution, < i. most e., resulting polynomials in independent parameter latent discrete variables. we consequently parametrize gradually this transformation backward so that actively computing the determinant of the jacobian and inverse jacobian components is trivial, yet else we maintain even the equivalent ability to learn different complex structured non - linear transformations, via a composition of simple building blocks, each outcome based merely on a deep integrated neural network. the training function criterion problem is simply the minimal exact log - likelihood, estimated which is somewhat tractable, and unbiased ancestral sampling measurement is also easy. we show that creating this approach yields potentially good generative models formed on four related image datasets and then can be used for inpainting.", "histories": [["v1", "Thu, 30 Oct 2014 19:44:20 GMT  (2325kb,D)", "http://arxiv.org/abs/1410.8516v1", null], ["v2", "Fri, 19 Dec 2014 22:40:18 GMT  (1454kb,D)", "http://arxiv.org/abs/1410.8516v2", null], ["v3", "Tue, 6 Jan 2015 18:10:44 GMT  (1454kb,D)", "http://arxiv.org/abs/1410.8516v3", "11 pages, under review as a conference paper at ICLR 2015"], ["v4", "Mon, 9 Mar 2015 18:06:58 GMT  (1455kb,D)", "http://arxiv.org/abs/1410.8516v4", "11 pages, under review as a conference paper at ICLR 2015"], ["v5", "Thu, 12 Mar 2015 06:25:20 GMT  (1456kb,D)", "http://arxiv.org/abs/1410.8516v5", "10 pages and 2 pages Appendix, under review as a conference paper at ICLR 2015"], ["v6", "Fri, 10 Apr 2015 12:27:56 GMT  (1457kb,D)", "http://arxiv.org/abs/1410.8516v6", "11 pages and 2 pages Appendix, workshop paper at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["laurent dinh", "david krueger", "yoshua bengio"], "accepted": true, "id": "1410.8516"}, "pdf": {"name": "1410.8516.pdf", "metadata": {"source": "CRF", "title": "NICE: Non-linear Independent Components Estimation", "authors": ["Laurent Dinh", "David Krueger", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "One of the central questions in unsupervised learning is how to capture complex data distributions that have unknown structure. Deep learning approaches (Bengio, 2009) rely on the learning of a representation of the data that would capture its most important factors of variation. This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model. In this paper, we consider the special case where we ask the learner to find a transformation h = f(x) of the data into a new space such that the resulting distribution factorizes, i.e., the components hd are independent:\npH(h) = \u220f d pHd(hd).\nWe can think of the transformation f as acting on the distribution by twisting and folding the space so as to achieve that objective. In particular, if we think geometrically about a low-dimensional manifold near which the data distribution concentrates, the learned transformation tries to flatten the manifold. A flat manifold has the property that if two vectors v1 and v2 are on it, then a convex combination \u03b1v1 + (1\u2212 \u03b1)v2 is likely to be on the manifold as well. In this paper, we can observe such flattening as found in (Bengio et al., 2013) with stacked autoencoders and RBMs. Indeed, if the components of h are independent, it means that if hd \u2208 Ad (whereAd is an interval) is likely under pHd(hd), then so are all the points in the volume A1\u00d7A2\u00d7 . . ., which thus forms a convex set of probable configuration. In addition to flattening the manifold to separate it from the ambiant space, using a marginally independent prior also puts pressure on the transformation f to stretch the space non-linearly in order to make the different hd marginally independent.\n\u2217Yoshua Bengio is a CIFAR Senior Fellow.\nar X\niv :1\n41 0.\n85 16\nv1 [\ncs .L\nG ]\n3 0\nO ct\n2 01\nThe proposed training criterion is directly derived from the log-likelihood. More specifically, we consider a change of variable h = f(x), which assumes that f is invertible and the dimension of h is the same as the dimension of x, in order to fit a distribution pH . The change of variable rule gives us:\npX(x) = pH(f(x))|det \u2202f(x)\n\u2202x |. (1)\nwhere \u2202f(x)\u2202x is the Jacobian matrix of function f at x. In this paper, we choose f such that the determinant of the Jacobian is trivially obtained. Moreover, its inverse f\u22121 is also trivially obtained, allowing us to sample from pX(x) easily as follows:\nh \u223c pH(h) x = f\u22121(h). (2)\nA key novelty of this paper is the design of such a transformation f that yields these two properties of \u201ceasy determinant of the Jacobian\u201d and \u201ceasy inverse\u201d, while allowing us to have as much capacity as needed in order to learn complex transformations. The core idea behind this is that we can split x into two blocks (x1, x2) and apply as building block the transformation from (x1, x2) to (y1, y2) of the form:\ny1 = x1\ny2 = x2 +m(x1) (3)\nwhere m is an arbitrarily complicated function. This building block has a unit Jacobian determinant for any m and is trivially invertible since:\nx1 = y1\nx2 = y2 \u2212m(y1). (4)\nThe details, surrounding discussion and experimental results are developed below."}, {"heading": "2 Issues of training with continuous data", "text": "We consider the problem of learning a density from a parametric family of densities {p\u03b8, \u03b8 \u2208 \u0398} over finite dataset D of N examples, each living in a space X ; typically X = RD. In this setting there is no natural upper limit on the log-likelihood that can be achieved (unlike for discrete data). As a result, several issues can arise from naive attempts at maximizing likelihood. For instance, as highlighted in (Bishop, 2006), a fully parametrized a mixture of gaussians can use one of its mixture component to model one of the datapoints with arbitrary precision, arbitrarily raising the training log-likelihood. As the test log-likelihood of such a model is often correspondingly low, this can be considered an overfitting issue. However, we will show two cases where similar singularity issues arise and generalize to test data.\nContinuous data is recorded with a finite amount of precision, generally much less than present-day computer precision. For the purposes of this paper, we can consider the level of precision represented on the computer as continuous, and coarser data as \u201cquantized\u201d. This kind of quantization allows the log-likelihood to be increased arbitrarily not only on the training set but also on the test set. For example, one can achieve this by building a mixture of Gaussians covering every quantum with infinite precision.\nIntroducing noise can counter this effect. For example, we can stochastically dequantize the data by introducing a uniform noise to reflect the uncertainty introduced by quantization1. Moreover, such noise added to the train and test sets introduces an upper bound on log-likelihood in expectation. Treating the data as discrete also results in an upper bound, but at the price of differentiability.\nData preprocessing is a widespread practice in machine learning. It can provide a more useful signal to the machine learning algorithm than the raw data and as long as the preprocessing is invertible it remains very relevant to\n1This is not the exact procedure but it is less cumbersome than to reverse-engineer the more complex quantization process.\n(b) Sampling: h \u223c pH(h), x = f\u22121(h) (c) Inpainting: maxxH log(pX((xO, xH))) = maxxH log(pH(f((xO, xH)))) + log(|det( \u2202f((xO,xH)) \u2202x )|)\nunsupervised tasks like generation, denoising or inpainting. For example wavelets (Mallat, 1999) may provide a sparser signal to model, and normalization and whitening remove trivial scaling and correlation allowing the model to focus on more interesting structure of the data and ease the optimization process. However, even invertible preprocessings allow arbitrary increase in likelihood. For example, when using a family of gaussian distributions to model data via maximum-likelihood, its log-likelihood on training data will be close to \u2212 12 log(det(\u03a3\u0302)) where \u03a3\u0302 is the empirical covariance matrix. The test log-likeihood will probably also have a similar value, given enough training data. Thus log-likelihood as a metric is not invariant to scaling.\nThe reason is that when using a bijective preprocessing f and training on preprocessed data (f(x))x\u2208D, one must use the change of variable formula of Eq. 1, where f is the preprocessing. In the case of orthonormal complete wavelet transform, we have unit Jacobian determinant. But in general, this Jacobian determinant can be a significant influence on the log-likelihood."}, {"heading": "3 Learning bijective transformations", "text": "Instead of modelling directly complex data by learning over a complex parametric family of distributions, we will learn a non-linear transformation of the data distribution into a simpler distribution via maximum likelihood using the following formula:\nlog(pX(x)) = log(pH(f(x))) + log(|det( \u2202f(x)\n\u2202x )|)\nwhere pH(h) will be a predefined density function for simplicity, the prior distribution2, which will often be a factorized distribution, i.e., with independent dimensions, for example a standard isotropic Gaussian. Let h = f(x) the code or the latent variable. If the prior distribution is factorial, then we obtain the following non-linear independent components estimation (NICE) criterion:\nlog(pX(x)) = D\u2211 d=1 log(pHd(fd(x))) + log(|det( \u2202f(x) \u2202x )|)\nIn line with previous work with auto-encoders and in particular the variational auto-encoder, we call f the encoder and its inverse f\u22121 the decoder. With f\u22121 given, sampling from the model can proceed very easily by ancestral sampling in the directed graphical model H \u2192 X , i.e., as per Eq. 2.\n2Note that this prior distribution does not need to be constant and could also be learned"}, {"heading": "4 Triangular structure", "text": "The architecture of the model becomes crucial at this point, since ideally we want a family of bijections whose Jacobian determinant is tractable and whose computation is straightforward, both forwards (the encoder f ) and backwards (the decoder f\u22121). If we use a layered or composed transformation f = fL \u25e6 . . . \u25e6 f2 \u25e6 f1, the forward and backward computations are defined by its layers, and its Jacobian determinant is the product of the its layers\u2019 Jacobian determinants. Therefore we will first aim at defining those more elementary components.\nFirst we consider linear transformations. (Rezende et al., 2014) and (Kingma and Welling, 2014) provide formulas for the inverse and determinant when using diagonal matrices, or diagonal matrices with rank-1 correction, as transformation matrices. Another family of matrices with tractable determinant are triangular matrices, whose determinants are simply the product of their diagonal elements. Inverting triangular matrices at test time is reasonable in terms of computation. Many square matrices, M can also be expressed as a product M = LU of upper and lower triangular matrices.\nOne way to use this observation would be to build a neural network with triangular weight matrices and bijective nonlinearities, but this highly constrains the architecture, limiting design choices to depth and selection of non-linearities. Alternatively, we can consider a family of functions with triangular Jacobian. By ensuring that the diagonal elements of the Jacobian are easy to compute, the determinant of the Jacobian is also made easy to compute.\nIn particular, we propose a family of bijections inspired from symmetric-key encryption. Let x = (xkey, xplain) \u2208 X , where xkey = x1:d is the key and xplain = xd+1:D is the message (the division is arbitrary), and m a function defined on Rd, we can define y = (ykey, ycipher) where ykey = xkey and ycipher = g(xplain | m(xkey)), where g : Rd \u00d7m(RD\u2212d) \u2192 Rd is the coupling law, an invertible map with respect to its first argument given the second. The corresponding computational graph is shown Fig 2. The Jacobian of this function is:\n\u2202y \u2202x =\n[ Id 0\n\u2202ycipher \u2202xkey \u2202ycipher \u2202xplain ] Which means that det \u2202y\u2202x = det \u2202ycipher \u2202xplain\n. Also, we observe we can invert the mapping using xkey = ykey and xplain = g\n\u22121(ycipher | m(ykey)). We call such a transformation a coupling layer with coupling function m. For simplicity, we choose an additive coupling law g(a | b) = a+bwithm : Rd \u2192 RD\u2212d then ycipher = xplain+m(xkey) and xplain = ycipher \u2212m(ykey) therefore the inverse of this transformation is only as computationally expensive as the transformation itself. Moreover, the Jacobian becomes:[\nId 0 \u2202m(xkey) \u2202xkey ID\u2212d\n]\nTherefore, an additive coupling layer transformation has a unit Jacobian determinant and a trivial inverse. One could also choose other types of coupling like multiplicative coupling law g(a | b) = a b, b 6= 0 or affine coupling law g(a, b) = a b1 + b2, b1 6= 0 if m : Rd \u2192 RD\u2212d \u00d7 RD\u2212d.\nSince a coupling layer leaves part of its input unchanged, we exchange the role of message and key in alternating layers, so that the composition of two coupling layers modifies every dimension. Examining the Jacobian, we observe that at least three coupling layers are necessary to allow all dimensions to influence one another. We include a diagonal scaling matrix S as the top layer, which multiplies the d-th ouput value by Sdd, resulting in the following simplified version of the NICE criterion:\nlog(pX(x)) = D\u2211 i=d log(pHd(fd(x))) + log(Sdd)\nWe can interpret these scaling factors as a kind of spectrum, showing how much variation is present in each of the latent dimensions (the larger Sdd is, the less important the dimension d is). The important dimensions of the spectrum can be viewed as a manifold learned by the algorithm. The prior term tries to make Sdd small (trying minimize entropy of H), while the determinant term logSdd prevents Sdd from ever reaching 0."}, {"heading": "5 Related methods", "text": "Significant advances have been made in generative models. Undirected graphical models like deep Boltzmann machines (DBM) (Salakhutdinov and Hinton, 2009) were for a while the most successful due to efficient approximate inference and learning techniques that these models allowed. However, these models require MCMC sampling procedure for training and sampling and these MCMCs are generally slowly mixing when the target distribution has sharp modes. In addition, the log-likelihood is intractable, and the best known estimation procedure, annealed importance sampling (AIS) (Salakhutdinov and Murray, 2008), might yield an overly optimistic evaluation (Roger Grosse and Salakhutdinov, 2013).\nDirected graphical models lack the conditional independence structure that allows DBMs efficient inference. However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training. In constrast with the NICE model, these approaches use a stochastic encoder Q(h|x) and an imperfect decoder, and add a reconstruction term, logP (x|h), to the cost, to ensure that the decoder approximately inverts the encoder. This injects noise into the auto-encoder loop, since h is sampled from Q(h|x), which is a variational approximation to the true posterior, P (h|x). The cost function also involves a term for maximizing the entropy of Q(h|x), which is not required here. The resulting training criterion is the variational lower bound on the log-likelihood of the data. The generally fast ancestral sampling technique that directed graphical models provide make these models appealing. Moreover, the importance sampling estimator of the log-likelihood is guaranteed not to be optimistic in expectation. However, using a lower bound criterion might yield a suboptimal solution with respect to the true log-likelihood. Furthermore, with reconstruction error typically being non-negligeable, the \u201ccorrect\u201d generative model always wants to add iid noise at the last generative step, which in the case of images, speech, or text, would often make for unnatural-looking samples. In practice, this issue can be avoided by taking the expectation of P (x|h) as a sample, instead of actually sampling the distribution. The use of a deterministic decoder can be motivated as a rigorous way of eliminating such noise.\nThe NICE criterion is very similar to the criterion of the variational autoencoder. More specifically, as the transformation and its inverse can be seen as a perfect autoencoder pair (Bengio, 2014), the reconstruction term is a constant that can be ignored. This leaves the Kullback-Leibler divergence term of the variational criterion; log(pH(f(x))) can be seen as the prior term, which forces the code to be likely with respect to the prior distribution, and log(|det \u2202f(x)\u2202x |) can be seen as the entropy term. This entropy term reflects the local volume expansion around the data (for the encoder), which translates into contraction in the decoder f\u22121. In a similar fashion, the entropy term in the variational criterion encourages the approximate posterior distribution to occupy volume, which also translates into contraction from the decoder. The drawback of perfect reconstruction/bijectivity is that we also have to model the noise, which is generally\nhandled by the conditional model P (x|h) in these graphical models.\nWe also observe that by combining the variational criterion with the reparametrization trick, (Kingma and Welling, 2014) is effectively maximizing the joint log-likelihood of the pair (x, ) in a NICE model with two affine coupling layers (where is the auxiliary noise variable).\nAnother interesting comparison is with the work of (Ozair and Bengio, 2014), which also considers a deterministic encoder, but with discrete input and latent variables, which is not always invertible. This can be interpreted as a hash function, whereas the transformation used in this paper can be seen as perfect hash function, i.e. injective. Hence they have to learn a decoder which is not going to be perfect in practice. They also have to confront the challenges of gradient-based optimization of discrete functions, which do not arise in the continuous case.\nThe change of variable formula for probability density functions is prominently used in inverse transform sampling (which is effectively the procedure used for sampling here). Independent component analysis (ICA) (Hyva\u0308rinen and Oja, 2000), and more specifically its maximum likelihood formulation, learn an orthogonal transformation of the data, necessitating a costly orthogonalization proceedure between parameter updates. Learning a richer family of transformations was proposed in (Bengio, 1991), but the proposed class of transformations, neural networks, lacks the structure to make the inference and optimization practical.\n(Rippel and Adams, 2013) reintroduces this idea but drops the bijectivity constraint but has to rely on a composite proxy to optimize the log-likelihood. A more principled proxy of log-likelihood, the variational lower bound, is used more successfully in (Kingma and Welling, 2014) and (Rezende et al., 2014). Generative adversarial networks (GAN) (Goodfellow et al., 2014) also train a generative model to transform a simple (e.g. factorial) distribution into the data distribution, but do not require an encoder that goes in the other direction. GAN sidesteps the difficulties of inference by learning a secondary deep network that discriminates between GAN samples and data. This classifier then provides a training signal to the GAN generative model, telling it how to change its output in order for it to be undistinguishable from the training data.\nLike the variational auto-encoders, the NICE model uses an encoder to avoid the difficulties of inference, but its encoding is deterministic. The log-likelihood is tractable and the training procedure does not require any sampling (apart from dequantizing the data). The triangular structure used in NICE to obtain tractability is also present in another tractable density model, the neural autoregressive density estimator (NADE) (Larochelle and Murray, 2011), inspired by (Bengio and Bengio, 2000). Indeed, the adjacency matrix in the NADE directed graphical model is strictly triangular. However the element-by-element autoregressive scheme of NADE make the ancestral sampling procedure computationally expensive for generative tasks on high-dimensional data, such as image data. A NICE model using one coupling layer can be seen as a block version of NADE with two blocks."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Log-likelihood and generation", "text": "We train NICE on MNIST (LeCun and Cortes, 1998), the Toronto Face Dataset 3 (TFD) (Susskind et al., 2010), the Street View House Numbers dataset (SVHN) (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2010). As mentioned earlier we use a dequantized version of the data, and the data is rescaled to be in [0, 1]D after dequantization. These two steps correspond to the following preprocessing:\nx\u0303 = 255\n256 x+ u (5)\nwhere u \u223c U([0, 256\u22121]). CIFAR-10 is normalized to be in [\u22121, 1]D.\n3We train on unlabeled data for this dataset.\nThe architecture for MNIST and SVHN used is a stack of four coupling layers with a diagonal positive scaling for the last stage, parametrized exponentially:\nSdd = e add . (6)\nWe partition the input space between key and message by separating odd and even components. The coupling function used for each coupling layer is a deep rectified network with linear output units. For MNIST and SVHN, we stack four such coupling layers, using 392 \u2212 1000 \u2212 1000 \u2212 1000 \u2212 1000 \u2212 1000 \u2212 392 units4 for MNIST and 1536\u22122000\u22122000\u22121536 for SVHN. We use eight coupling layers that have (in order) 3\u22123\u22122\u22122\u22121\u22121\u22121\u22121 hidden layers with 2000 hidden units for TFD, and 1\u22121\u22123\u22123\u22122\u22122\u22121\u22121 hidden layers with 2400 units for CIFAR.\nA standard logistic distribution is used as prior for MNIST and TFD, as its negative log-likelihood function correspond to the smooth L1 penalty (Hyva\u0308rinen et al., 2009) x 7\u2192 log cosh(x). A standard normal distribution is used as prior for SVHN and CIFAR-10.\nThe models are training with RMSProp (Tieleman and Hinton, 2012) with learning rate 10\u22123 exponentially decreasing to 10\u22124 with exponential decay of 1.0005, that is a learning rate:\n\u03b1 = max(1.0005\u2212e \u00d7 10\u22123, 10\u22124)\nWhere e is the number of epochs. For CIFAR-10 the learning rate goes from 2 \u00d7 10\u22124 to 10\u22125 with the same decay rate. The decay coefficient of RMSProp is \u03b2 = 0.95 and maximum scaling of 100. The momentum is initially \u03c1 = 0 and becomes \u03c1 = 0.5 at the fifth epoch, giving the update rule:\ng2t+1 =\u03b2g 2 t + (1\u2212 \u03b2)\n\u2202L\n\u2202\u03b8\n2\n\u00b5t+1 =\u03c1\u00b5t \u2212 \u03b1min((g2t+1)\u22121, 100) \u2202L \u2202\u03b8 \u03b8t+1 =\u03b8t + \u00b5t+1\nThe operations are elementwise, \u03b8 is the parameter and L is the loss function, here negative log-likleihood. We select the best model in terms of validation log-likelihood after 1500 epochs, or using early stopping.\nWe obtained a test log-likelihood of 1980.50 on MNIST, 5369.16 on TFD, 7457 for SVHN and 3800 for CIFAR-10. Samples are shown in Fig. 3, to illustrate the learned manifold. We also take a random rotationR of the cartesian product of two centered circles of radius D\u22122 that is {(D\u22122)(cos(a), sin(a), cos(b), sin(b)), (a, b) \u2208 [0, 2\u03c0]2} = S\u00d7S, a scaled Clifford torus, in latent space and transform it to data space, the result f\u22121(R(S \u00d7 S)) is shown Fig 4.\nWe also examined the last diagonal scaling layer and looked at its coefficients (Sdd)d\u2264D. If we consider jointly the prior distribution and the diagonal scaling layer, \u03c3d = S\u22121dd can be considered as the scale parameter of each independent component. This shows us the importance that the model has given to each component and ultimately how successful the model was at learning manifolds. We sort (\u03c3d)d\u2264D and plot it Fig 5."}, {"heading": "6.2 Inpainting", "text": "Here we consider a naive iterative procedure to implement inpainting with the trained generative models. For inpainting we clamp the observed dimensions to their values and maximize log-likelihood with respect to the hidden dimensions using projected gradient ascent with step \u03b1 = 11+i , where i is the iteration. The result is shown on test examples of MNIST and TFD Fig 6 and 7 respectively. Although the model is not trained for this task, this inpainting procedure seems to yield reasonable qualitative performance, though we can notice the presence of spurious modes.\n4we give the dimensions of inputs, hidden and output layers"}, {"heading": "7 Conclusion", "text": "In this work we presented a new flexible architecture for learning a highly non-linear transformation that maps the training data to a space where its distribution is approximately factorized, and a framework to achieve this by directly maximizing log-likelihood. Our model features efficient unbiased ancestral sampling and achieves competitive results on log-likelihood and inpainting.\nNote that the architecture of our model could be trained using other inductive principles capable of exploiting its advantages, like toroidal subspace analysis (TSA) (Cohen and Welling, 2014).\nWe also briefly make a connection with variational autoencoders. Additional work can be made in that direction to allow more powerful approximate inference, with a more complex family of approximate posterior distributions, or a richer family of priors."}, {"heading": "Acknowledgements", "text": "We would like to thank Yann Dauphin, Vincent Dumoulin, Aaron Courville, Kyle Kastner, Dustin Webb and Li Yao for discussions and feedback. Vincent Dumoulin provided code for visualization. They are grateful towards the developers of Theano (Bergstra et al., 2011) (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013), and for the computational resources provided by Compute Canada and Calcul Que\u0301bec, and for the research funding provided by NSERC, CIFAR, and Canada Research Chairs."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Artificial Neural Networks and their Application to Sequence Recognition", "author": ["Y. Bengio"], "venue": "PhD thesis, McGill University, (Computer Science), Montreal, Canada.", "citeRegEx": "Bengio,? 1991", "shortCiteRegEx": "Bengio", "year": 1991}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers.", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Y. Bengio"], "venue": "Technical report, arXiv preprint arXiv:1407.7906.", "citeRegEx": "Bengio,? 2014", "shortCiteRegEx": "Bengio", "year": 2014}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Y. Bengio", "S. Bengio"], "venue": "Solla, S., Leen, T., and M\u00fcller, K.-R., editors, Advances in Neural Information Processing Systems 12 (NIPS\u201999), pages 400\u2013406. MIT Press.", "citeRegEx": "Bengio and Bengio,? 2000", "shortCiteRegEx": "Bengio and Bengio", "year": 2000}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML\u201913). ACM.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: Deep learning on gpus with python", "author": ["J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I.J. Goodfellow", "A. Bergeron", "Y. Bengio"], "venue": "Big Learn workshop, NIPS\u201911.", "citeRegEx": "Bergstra et al\\.,? 2011", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Learning the irreducible representations of commutative lie groups", "author": ["T. Cohen", "M. Welling"], "venue": "arXiv preprint arXiv:1402.4437.", "citeRegEx": "Cohen and Welling,? 2014", "shortCiteRegEx": "Cohen and Welling", "year": 2014}, {"title": "Generative adversarial networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Technical Report arXiv:1406.2661, arxiv.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Pylearn2: a machine learning research library", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "arXiv preprint arXiv:1308.4214.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "I. Danihelka", "A. Mnih", "C. Blundell", "D. Wierstra"], "venue": "ICML\u20192014.", "citeRegEx": "Gregor et al\\.,? 2014", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Natural Image Statistics: A probabilistic approach to early computational vision", "author": ["A. Hyv\u00e4rinen", "J. Hurri", "P.O. Hoyer"], "venue": "Springer-Verlag.", "citeRegEx": "Hyv\u00e4rinen et al\\.,? 2009", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2009}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks, 13(4):411\u2013430.", "citeRegEx": "Hyv\u00e4rinen and Oja,? 2000", "shortCiteRegEx": "Hyv\u00e4rinen and Oja", "year": 2000}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Welling,? 2014", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Convolutional deep belief networks on CIFAR-10", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto. Unpublished Manuscript: http://www.cs.utoronto.ca/ kriz/conv-cifar10-aug2010.pdf.", "citeRegEx": "Krizhevsky,? 2010", "shortCiteRegEx": "Krizhevsky", "year": 2010}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011), volume 15 of JMLR: W&CP.", "citeRegEx": "Larochelle and Murray,? 2011", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "A wavelet tour of signal processing", "author": ["S. Mallat"], "venue": "Academic press.", "citeRegEx": "Mallat,? 1999", "shortCiteRegEx": "Mallat", "year": 1999}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "ICML\u20192014.", "citeRegEx": "Mnih and Gregor,? 2014", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Deep directed generative autoencoders", "author": ["S. Ozair", "Y. Bengio"], "venue": "Technical report, U. Montreal, arXiv:1410.0630.", "citeRegEx": "Ozair and Bengio,? 2014", "shortCiteRegEx": "Ozair and Bengio", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "Technical report, arXiv:1401.4082.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "High-dimensional probability estimation with deep density models", "author": ["O. Rippel", "R.P. Adams"], "venue": "arXiv preprint arXiv:1302.5125.", "citeRegEx": "Rippel and Adams,? 2013", "shortCiteRegEx": "Rippel and Adams", "year": 2013}, {"title": "Annealing between distributions by averaging moments", "author": ["C.M. Roger Grosse", "R. Salakhutdinov"], "venue": "ICML\u20192013.", "citeRegEx": "Grosse and Salakhutdinov,? 2013", "shortCiteRegEx": "Grosse and Salakhutdinov", "year": 2013}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, pages 448\u2013455.", "citeRegEx": "Salakhutdinov and Hinton,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "Cohen, W. W., McCallum, A., and Roweis, S. T., editors, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML\u201908), volume 25, pages 872\u2013879. ACM.", "citeRegEx": "Salakhutdinov and Murray,? 2008", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "The Toronto face dataset", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical Report UTML TR 2010001, U. Toronto.", "citeRegEx": "Susskind et al\\.,? 2010", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Deep learning approaches (Bengio, 2009) rely on the learning of a representation of the data that would capture its most important factors of variation.", "startOffset": 25, "endOffset": 39}, {"referenceID": 14, "context": "This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model.", "startOffset": 77, "endOffset": 149}, {"referenceID": 22, "context": "This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model.", "startOffset": 77, "endOffset": 149}, {"referenceID": 21, "context": "This raises the question: what is a good representation? Like in recent work (Kingma and Welling, 2014; Rezende et al., 2014; Ozair and Bengio, 2014), we take the view that a good representation is one in which the distribution of the data is easy to model.", "startOffset": 77, "endOffset": 149}, {"referenceID": 5, "context": "In this paper, we can observe such flattening as found in (Bengio et al., 2013) with stacked autoencoders and RBMs.", "startOffset": 58, "endOffset": 79}, {"referenceID": 7, "context": "For instance, as highlighted in (Bishop, 2006), a fully parametrized a mixture of gaussians can use one of its mixture component to model one of the datapoints with arbitrary precision, arbitrarily raising the training log-likelihood.", "startOffset": 32, "endOffset": 46}, {"referenceID": 18, "context": "For example wavelets (Mallat, 1999) may provide a sparser signal to model, and normalization and whitening remove trivial scaling and correlation allowing the model to focus on more interesting structure of the data and ease the optimization process.", "startOffset": 21, "endOffset": 35}, {"referenceID": 22, "context": "(Rezende et al., 2014) and (Kingma and Welling, 2014) provide formulas for the inverse and determinant when using diagonal matrices, or diagonal matrices with rank-1 correction, as transformation matrices.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": ", 2014) and (Kingma and Welling, 2014) provide formulas for the inverse and determinant when using diagonal matrices, or diagonal matrices with rank-1 correction, as transformation matrices.", "startOffset": 12, "endOffset": 38}, {"referenceID": 25, "context": "Undirected graphical models like deep Boltzmann machines (DBM) (Salakhutdinov and Hinton, 2009) were for a while the most successful due to efficient approximate inference and learning techniques that these models allowed.", "startOffset": 63, "endOffset": 95}, {"referenceID": 26, "context": "In addition, the log-likelihood is intractable, and the best known estimation procedure, annealed importance sampling (AIS) (Salakhutdinov and Murray, 2008), might yield an overly optimistic evaluation (Roger Grosse and Salakhutdinov, 2013).", "startOffset": 124, "endOffset": 156}, {"referenceID": 14, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 22, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 19, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 11, "context": "However, recent advances in the framework of variational auto-encoders (VAE) - which come under a variety of names and variants (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014; Gregor et al., 2014) - allowed effective approximate inference for training.", "startOffset": 128, "endOffset": 220}, {"referenceID": 3, "context": "More specifically, as the transformation and its inverse can be seen as a perfect autoencoder pair (Bengio, 2014), the reconstruction term is a constant that can be ignored.", "startOffset": 99, "endOffset": 113}, {"referenceID": 14, "context": "We also observe that by combining the variational criterion with the reparametrization trick, (Kingma and Welling, 2014) is effectively maximizing the joint log-likelihood of the pair (x, ) in a NICE model with two affine coupling layers (where is the auxiliary noise variable).", "startOffset": 94, "endOffset": 120}, {"referenceID": 21, "context": "Another interesting comparison is with the work of (Ozair and Bengio, 2014), which also considers a deterministic encoder, but with discrete input and latent variables, which is not always invertible.", "startOffset": 51, "endOffset": 75}, {"referenceID": 13, "context": "Independent component analysis (ICA) (Hyv\u00e4rinen and Oja, 2000), and more specifically its maximum likelihood formulation, learn an orthogonal transformation of the data, necessitating a costly orthogonalization proceedure between parameter updates.", "startOffset": 37, "endOffset": 62}, {"referenceID": 1, "context": "Learning a richer family of transformations was proposed in (Bengio, 1991), but the proposed class of transformations, neural networks, lacks the structure to make the inference and optimization practical.", "startOffset": 60, "endOffset": 74}, {"referenceID": 23, "context": "(Rippel and Adams, 2013) reintroduces this idea but drops the bijectivity constraint but has to rely on a composite proxy to optimize the log-likelihood.", "startOffset": 0, "endOffset": 24}, {"referenceID": 14, "context": "A more principled proxy of log-likelihood, the variational lower bound, is used more successfully in (Kingma and Welling, 2014) and (Rezende et al.", "startOffset": 101, "endOffset": 127}, {"referenceID": 22, "context": "A more principled proxy of log-likelihood, the variational lower bound, is used more successfully in (Kingma and Welling, 2014) and (Rezende et al., 2014).", "startOffset": 132, "endOffset": 154}, {"referenceID": 9, "context": "Generative adversarial networks (GAN) (Goodfellow et al., 2014) also train a generative model to transform a simple (e.", "startOffset": 38, "endOffset": 63}, {"referenceID": 16, "context": "The triangular structure used in NICE to obtain tractability is also present in another tractable density model, the neural autoregressive density estimator (NADE) (Larochelle and Murray, 2011), inspired by (Bengio and Bengio, 2000).", "startOffset": 164, "endOffset": 193}, {"referenceID": 4, "context": "The triangular structure used in NICE to obtain tractability is also present in another tractable density model, the neural autoregressive density estimator (NADE) (Larochelle and Murray, 2011), inspired by (Bengio and Bengio, 2000).", "startOffset": 207, "endOffset": 232}, {"referenceID": 17, "context": "1 Log-likelihood and generation We train NICE on MNIST (LeCun and Cortes, 1998), the Toronto Face Dataset 3 (TFD) (Susskind et al.", "startOffset": 55, "endOffset": 79}, {"referenceID": 27, "context": "1 Log-likelihood and generation We train NICE on MNIST (LeCun and Cortes, 1998), the Toronto Face Dataset 3 (TFD) (Susskind et al., 2010), the Street View House Numbers dataset (SVHN) (Netzer et al.", "startOffset": 114, "endOffset": 137}, {"referenceID": 20, "context": ", 2010), the Street View House Numbers dataset (SVHN) (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2010).", "startOffset": 54, "endOffset": 75}, {"referenceID": 15, "context": ", 2011) and CIFAR-10 (Krizhevsky, 2010).", "startOffset": 21, "endOffset": 39}, {"referenceID": 12, "context": "A standard logistic distribution is used as prior for MNIST and TFD, as its negative log-likelihood function correspond to the smooth L1 penalty (Hyv\u00e4rinen et al., 2009) x 7\u2192 log cosh(x).", "startOffset": 145, "endOffset": 169}, {"referenceID": 28, "context": "The models are training with RMSProp (Tieleman and Hinton, 2012) with learning rate 10\u22123 exponentially decreasing to 10\u22124 with exponential decay of 1.", "startOffset": 37, "endOffset": 64}, {"referenceID": 8, "context": "Note that the architecture of our model could be trained using other inductive principles capable of exploiting its advantages, like toroidal subspace analysis (TSA) (Cohen and Welling, 2014).", "startOffset": 166, "endOffset": 191}], "year": 2017, "abstractText": "We propose a deep learning framework for modeling complex high-dimensional densities via Nonlinear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable, and unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.", "creator": "LaTeX with hyperref package"}}}