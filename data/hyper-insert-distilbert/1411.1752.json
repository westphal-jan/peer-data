{"id": "1411.1752", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets", "abstract": "to simply cope comfortably with the high variable level complications of ambiguity faced in domains such as computer vision search or intelligent natural language description processing, robust distributed prediction analytical methods often search estimates for collectively a reasonably diverse set of high - minimum quality candidate solutions groups or regression proposals. in numerical structured prediction problems, this becomes a daunting task, as shrinking the solution space ( image sample labelings, output sentence revision parses, etc. ) is exponentially large. thereby we study specific greedy processing algorithms for continually finding internally a diverse subset of solutions presented in structured - output data spaces identified by drawing new connections between submodular valued functions learned over combinatorial item sets and appropriate high - degree order potentials ( parameter hops ) studied for graphical approximation models. specifically, whilst we show via examples illustrating that when applying marginal gains of appropriate submodular diversity functions allow easily structured population representations, this enables appropriate efficient ( polynomial sub - linear time ) approximate growth maximization approximation by reducing the ideally greedy rational augmentation optimization step attendant to inference in construct a factor generator graph with appropriately specifically constructed small hops. we discuss benefits, identify tradeoffs, opportunities and show that eventually our constructions lead to giving significantly better rational proposals.", "histories": [["v1", "Thu, 6 Nov 2014 20:07:37 GMT  (9246kb,D)", "http://arxiv.org/abs/1411.1752v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.IR stat.ML", "authors": ["adarsh prasad", "stefanie jegelka", "dhruv batra"], "accepted": true, "id": "1411.1752"}, "pdf": {"name": "1411.1752.pdf", "metadata": {"source": "CRF", "title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets", "authors": ["Adarsh Prasad", "Stefanie Jegelka"], "emails": ["adarsh@cs.utexas.edu", "stefje@eecs.berkeley.edu", "dbatra@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "Many problems in Computer Vision, Natural Language Processing and Computational Biology involve mappings from an input space X to an exponentially large space Y of structured outputs. For instance, Y may be the space of all segmentations of an image with n pixels, each of which may take L labels, so |Y| = Ln. Formulations such as Conditional Random Fields (CRFs) [25], Max-Margin Markov Networks (M3N) [33], and Structured Support Vector Machines (SSVMs) [34] have successfully provided principled ways of scoring all solutions y \u2208 Y and predicting the single highest scoring or maximum a posteriori (MAP) configuration, by exploiting the factorization of a structured output into its constituent \u201cparts\u201d.\nIn a number of scenarios, the posterior P(y|x) has several modes due to ambiguities, and we seek not only a single best prediction but a set of good predictions: (1) Interactive Machine Learning. Systems like Google Translate (for machine translation) or Photoshop (for interactive image segmentation) solve structured prediction problems that are often ambiguous (\"what did the user really mean?\"). Generating a small set of relevant candidate solutions for the user to select from can greatly improve the results. (2) M-Best hypotheses in cascades. Machine learning algorithms are often cascaded, with the output of one model being fed into another [35]. Hence, at the initial stages it is not necessary to make a single perfect prediction. We rather seek a set of plausible predictions that are subsequently re-ranked, combined or processed by a more sophisticated mechanism. In both scenarios, we ideally want a small set of M plausible (i.e., high scoring) but non-redundant (i.e., diverse) structured-outputs to hedge our bets.\nSubmodular Maximization and Diversity. The task of searching for a diverse high-quality subset of items from a ground set V has been well-studied in information retrieval [5], sensor placement [23], document summarization [27], viral marketing [18], and robotics [10]. Across these domains, submodularity has emerged as an a fundamental and practical concept \u2013 a property of functions for measuring diversity of a subset of items. Specifically, a set function F : 2V \u2192 R is submodular if its marginal gains, F (a|S) \u2261 F (S\u222aa)\u2212F (S) are decreasing, i.e. F (a|S) \u2265 F (a|T )\nar X\niv :1\n41 1.\n17 52\nv1 [\ncs .L\nG ]\n6 N\nov 2\n01 4\nfor all S \u2286 T and a /\u2208 T . In addition, if F is monotone, i.e., F (S) \u2264 F (T ), \u2200S \u2286 T , then a simple greedy algorithm (that in each iteration t adds to the current set St the item with the largest marginal gain F (a|St)) achieves an approximation factor of (1 \u2212 1e ) [28]. This result has had significant practical impact [22]. Unfortunately, if the number of items |V | is exponentially large, then even a single linear scan for greedy augmentation is infeasible.\nIn this work, we study conditions under which it is feasible to greedily maximize a submodular function over an exponentially large ground set V = {v1, . . . , vN} whose elements are combinatorial objects, i.e., labelings of a base set of n variables y = {y1, y2, . . . , yn}. For instance, in image segmentation, the base variables yi are pixel labels, and each item a \u2208 V is a particular labeling of the pixels. Or, if each base variable ye indicates the presence or absence of an edge e in a graph, then each item may represent a spanning tree or a maximal matching. Our goal is to find a set of M plausible and diverse configurations efficiently, i.e. in time sub-linear in |V | (ideally scaling as a low-order polynomial in log |V |). We will assume F (\u00b7) to be monotone submodular, nonnegative and normalized (F (\u2205) = 0), and base our study on the greedy algorithm. As a running example, we focus on pixel labeling, where each base variable takes values in a set [L] = {1, . . . , L} of labels. Contributions. Our principal contribution is a conceptual one. We observe that marginal gains of a number of submodular functions allow structured representations, and this enables efficient greedy maximization over exponentially large ground sets \u2013 by reducing the greedy augmentation step to a MAP inference query in a discrete factor graph augmented with a suitably constructed HighOrder Potential (HOP). Thus, our work draws new connections between two seemingly disparate but highly related areas in machine learning \u2013 submodular maximization and inference in graphical models with structured HOPs. As specific examples, we construct submodular functions for three different, task-dependent definitions of diversity, and provide reductions to three different HOPs for which efficient inference techniques have already been developed. Moreover, we present a generic recipe for constructing such submodular functions, which may be \u201cplugged\u201d with efficient HOPs discovered in future work. Our empirical contribution is an efficient algorithm for producing a set of image segmentations with significantly higher oracle accuracy1 than previous works. The algorithm is general enough to transfer to other applications. Fig. 1 shows an overview of our approach.\nRelated work: generating multiple solutions. Determinental Point Processesare an elegant probabilistic model over sets of items with a preference for diversity. Its generalization to a structured setting [24] assumes a tree-structured model, an assumption that we do not make. Guzman-Rivera et al. [15, 16] learn a set of M models, each producing one solution, to form the set of solutions. Their approach requires access to the learning sub-routine and repeated re-training of the models, which is not always possible, as it may be expensive or proprietary. We assume to be given a single (pretrained) model from which we must generate multiple diverse, good solutions. Perhaps the closest to our setting are recent techniques for finding diverse M -best solutions [2, 29] or modes [7, 8] in graphical models. While [7] and [8] are inapplicable since they are restricted to chain and tree graphs, we compare to other baselines in Section 3.2 and 4."}, {"heading": "1.1 Preliminaries and Notation", "text": "We select from a ground set V of N items. Each item is a labeling y = {y1, y2, . . . , yn} of n base variables. For clarity, we use non-bold letters a \u2208 V for items, and boldface letters y for base set configurations. Uppercase letters refer to functions over the ground set items F (a|A), R(a|A), D(a|A), and lowercase letters to functions over base variables f(y), r(y), d(y).\n1The accuracy of the most accurate segmentation in the set.\nFormally, there is a bijection \u03c6 : V 7\u2192 [L]m that maps items a \u2208 V to their representation as base variable labelings y = \u03c6(a). For notational simplicity, we often use y \u2208 S to mean \u03c6\u22121(y) \u2208 S, i.e. the item corresponding to the labeling y is present in the set S \u2286 V . We write ` \u2208 y if the label ` is used in y, i.e. \u2203j s.t. yj = `. For a set c \u2286 [n], we use yc to denote the tuple {yi | i \u2208 c}. Our goal to find an ordered set or list of items S \u2286 V that maximizes a scoring function F . Lists generalize the notation of sets, and allow for reasoning of item order and repetitions. More details about list vs set prediction can be found in [31, 10].\nScoring Function. We trade off the relevance and diversity of list S \u2286 V via a scoring function F : 2V \u2192 R of the form F (S) = R(S) + \u03bbD(S), (1) where R(S) = \u2211 a\u2208S R(a) is a modular nonnegative relevance function that aggregates the quality of all items in the list; D(S) is a monotone normalized submodular function that measure the diversity of items in S; and \u03bb \u2265 0 is a trade-off parameter. Similar objective functions were used e.g. in [27]. They are reminiscent of the general paradigm in machine learning of combining a loss function that measures quality (e.g. training error) and a regularization term that encourages desirable properties (e.g. smoothness, sparsity, or \u201cdiversity\u201d).\nSubmodular Maximization. We aim to find a list S that maximizes F (S) subject to a cardinality constraint |S| \u2264 M . For monotone submodular F , this may be done via a greedy algorithm that starts out with S0 = \u2205, and iteratively adds the next best item: St = St\u22121 \u222a at, at \u2208 argmaxa\u2208V F (a | St\u22121). (2) The final solution SM is within a factor of (1 \u2212 1e ) of the optimal solution S\u2217: F (SM ) \u2265 (1 \u2212 1 e )F (S\n\u2217) [28]. The computational bottleneck is that in each iteration, we must find the item with the largest marginal gain. Clearly, if |V | has exponential size, we cannot touch each item even once. Instead, we propose \u201caugmentation sub-routines\u201d that exploit the structure of V and maximize the marginal gain by solving an optimization problem over the base variables."}, {"heading": "2 Marginal Gains in Configuration Space", "text": "To solve the greedy augmentation step via optimization over y, we transfer the marginal gain from the world of items to the world of base variables and derive functions on y from F :\nF (\u03c6\u22121(y) | S)\ufe38 \ufe37\ufe37 \ufe38 f(y|S) = R(\u03c6\u22121(y))\ufe38 \ufe37\ufe37 \ufe38 r(y) +\u03bbD(\u03c6\u22121(y) | S)\ufe38 \ufe37\ufe37 \ufe38 d(y|S) . (3)\nMaximizing F (a|S) now means maximizing f(y|S) for y = \u03c6(a). This can be a hard combinatorial optimization problem in general. However, as we will see, there is a broad class of useful functions F for which f inherits exploitable structure, and argmaxy f(y|S) can be solved efficiently, exactly or at least approximately.\nRelevance Function. We use a structured relevance function R(a) that is the score of a factor graph defined over the base variables y. Let G = (V, E) be a graph defined over {y1, y2, . . . , yn}, i.e. V = [n], E \u2286 (V 2 ) . Let C = {C | C \u2286 V} be a set of cliques in the graph, and let \u03b8C : [L]|C| 7\u2192 R be the log-potential functions (or factors) for these cliques. The quality of an item a = \u03c6\u22121(y) is then given by R(a) = r(y) = \u2211 C\u2208C \u03b8C(yC). For instance, with only node and edge factors,\nthis quality becomes r(y) = \u2211 p\u2208V \u03b8p(yp) + \u2211 (p,q)\u2208E \u03b8pq(yp, yq). In this model, finding the single highest quality item corresponds to maximum a posteriori (MAP) inference in the factor graph.\nAlthough we refer to terms with probabilistic interpretations such as \u201cMAP\u201d, we treat our relevance function as output of an energy-based model [26] such as a Structured SVM [34]. For instance, r(y) = \u2211 C\u2208C \u03b8C(yC) = w\n\u1d40\u03c8(y) for parameters w and feature vector \u03c8(y). Moreover, we assume that the relevance function r(y) is nonnegative2. This assumption ensures that F (\u00b7) is monotone. If F is non-monotone, algorithms other than the greedy are needed [4, 13]. We leave this generalization for future work. In most application domains the relevance function is learned from data and thus our positivity assumption is not restrictive \u2013 one can simply learn a positive relevance function. For instance, in SSVMs, the relevance weights are learnt to maximize the margin between the correct labeling and all incorrect ones. We show in the supplement that SSVM parameters that assign nonnegative scores to all labelings achieve exactly the same hinge loss (and thus the same generalization error) as without the nonnegativity constraint.\n2Strictly speaking, this condition is sufficient but not necessary. We only need nonnegative marginal gains."}, {"heading": "3 Structured Diversity Functions", "text": "We next discuss a general recipe for constructing monotone submodular diversity functions D(S), and for reducing their marginal gains to structured representations over the base variables d(y|S). Our scheme relies on constructing groups Gi that cover the ground set, i.e. V = \u22c3 iGi. These groups will be defined by task-dependent characteristics \u2013 for instance, in image segmentation, G` can be the set of all segmentations that contain label `. The groups can be overlapping. For instance, if a segmentation y contains pixels labeled \u201cgrass\u201d and \u201ccow\u201d, then y \u2208 Ggrass and y \u2208 Gcow. Group Coverage: Count Diversity. Given V and a set of groups {Gi}, we measure the diversity of a list S in terms of its group coverage, i.e., the number of groups covered jointly by items in S:\nD(S) = \u2223\u2223\u2223{i | Gi \u2229 S 6= \u2205}\u2223\u2223\u2223, (4)\nwhere we define Gi\u2229S as the intersection of Gi with the set of unique items in S. It is easy to show that this function is monotone submodular. If G` is the group of all segmentations that contain label `, then the diversity measure of a list of segmentations S is the number of object labels that appear in any a \u2208 S. The marginal gain is the number of new groups covered by a:\nD(a | S) = \u2223\u2223\u2223{i | a \u2208 Gi and S \u2229Gi = \u2205}\u2223\u2223\u2223. (5)\nThus, the greedy algorithm will try to find an item/segmentation that belongs to as many as yet unused groups as possible.\nGroup Coverage: General Diversity. More generally, instead of simply counting the number of groups covered by S, we can use a more refined decay\nD(S) = \u2211 i h (\u2223\u2223Gi \u2229 S\u2223\u2223). (6)\nwhere h is any nonnegative nondecreasing concave scalar function. This is a sum of submodular functions and hence submodular. Eqn. (4) is a special case of Eqn. (6) with h(y) = min{1, y}. Other possibilities are \u221a\u00b7, or log(1 + \u00b7). For this general definition of diversity, the marginal gain is\nD(a | S) = \u2211\ni:Gi3a\n[ h ( 1 + \u2223\u2223Gi \u2229 S\u2223\u2223)\u2212 h(\u2223\u2223Gi \u2229 S\u2223\u2223)]. (7) Since h is concave, the gain h ( 1 +\n\u2223\u2223Gi \u2229 S\u2223\u2223)\u2212 h(\u2223\u2223Gi \u2229 S\u2223\u2223) decreases as S becomes larger. Thus, the marginal gain of an item a is proportional to how rare each group Gi 3 a is in the list S. In each step of the greedy algorithm, we maximize r(y) + \u03bbd(y|S). We already established a structured representation of r(y) via a factor graph on y. In the next few subsections, we specify three example definitions of groups Gi that instantiate three diversity functions D(S). For each D(S), we show how the marginal gains D(a|S) can be expressed as a specific High-Order Potential (HOP) d(y|S) in the factor graph over y. These HOPs are known to be efficiently optimizable, and hence we can solve the augmentation step efficiently. Table 1 summarizes these connections.\nDiversity and Parsimony. If the groups Gi are overlapping, some y can belong to many groups simultaneously. While such a y may offer an immediate large gain in diversity, in many applications it is more natural to seek a small list of complementary labelings rather than having all labels occur in the same y. For instance, in image segmentation with groups defined by label presence (Sec. 3.1), natural scenes are unlikely to contain many labels at the same time. Instead, the labels should be spread across the selected labelings y \u2208 S. Hence, we include a parsimony factor p(y) that biases towards simpler labelings y. This term is a modular function and does not affect the diversity functions directly. We next outline some example instantiations of the functions (4) and (6)."}, {"heading": "3.1 Diversity of Labels", "text": "For the first example, letG` be the set of all labelings y containing the label `, i.e. y \u2208 G` if and only if yj = ` for some j \u2208 [n]. Such a diversity function arises in multi-class image segmentation \u2013 if the highest scoring segmentation contains \u201csky\u201d and \u201cgrass\u201d, then we would like to add complementary segmentations that contain an unused class label, say \u201csheep\u201d or \u201ccow\u201d.\nStructured Representation of Marginal Gains. The marginal gain for this diversity function turns out to be a HOP called label cost [9]. It penalizes each label that occurs in a previous segmentation. Let lcountS(`) be the number of segmentations in S that contain label `. In the simplest case of coverage diversity (4), the marginal gain provides a constant reward for every as yet unseen label `:\nd(y | S) = \u2223\u2223\u2223{` | y \u2208 G`, S \u2229G` = \u2205}\u2223\u2223\u2223 = \u2211\n`\u2208y,lcountS(`)=0 1. (8)\nFor the general group coverage diversity (6), the gain becomes d(y|S) = \u2211\n`:G`3y\n[ h ( 1 + \u2223\u2223G` \u2229 S\u2223\u2223)\u2212 h(\u2223\u2223G` \u2229 S\u2223\u2223)] =\u2211 `\u2208y [ h ( 1 + lcountS(`) ) \u2212 h ( lcountS(`) )] .\nThus, d(y|S) rewards the presence of a label ` in y by an amount proportional to how rare ` is in the segmentations already chosen in S. The parsimony factor in this setting is p(y) = \u2211 `\u2208y c(`). In the simplest case, c(`) = \u22121, i.e. we are charged a constant for every label used in y. With this type of diversity (and parsimony terms), the greedy augmentation step is equivalent to performing MAP inference in a factor graph augmented with label reward HOPs: argmaxy r(y) + \u03bb(d(y | S) + p(y)). Delong et al. [9] show how to perform approximate MAP inference with such label costs via an extension to the standard \u03b1-expansion [3] algorithm.\nLabel Transitions. Label Diversity can be extended to reward not just the presence of previously unseen labels, but also the presence of previously unseen label transitions (e.g., a person in front of a car or a person in front of a house). Formally, we define one group G`,`\u2032 per label pair `, `\u2032, and y \u2208 G`,`\u2032 if it contains two adjacent variables yi, yj with labels yi = `, yj = `\u2032. This diversity function rewards the presence of a label pair (`, `\u2032) by an amount proportional to how rare this pair is in the segmentations that are part of S. For such functions, the marginal gain d(y|S) becomes a HOP called cooperative cuts [17]. The inference algorithm in [20] gives a fully polynomial-time approximation scheme for any nondecreasing, nonnegative h, and the exact gain maximizer for the count function h(y) = min{1, y}. Further details may be found in the supplement."}, {"heading": "3.2 Diversity via Hamming Balls", "text": "The label diversity function simply rewarded the presence of a label `, irrespective of which or how many variables yi were assigned that label. The next diversity function rewards a large Hamming distance Ham(y1,y2) = \u2211n i=1[[y 1 i 6= y2i ]] between configurations (where [[\u00b7]] is the Iverson bracket.) Let Bk(y) denote the k-radius Hamming ball centered at y, i.e. B(y) = {y\u2032 | Ham(y\u2032,y) \u2264 k}. The previous section constructed one group per label `. Now, we construct one group Gy for each configuration y, which is the k-radius Hamming ball centered at y, i.e. Gy = Bk(y). Structured Representation of Marginal Gains. For this diversity, the marginal gain d(y|S) becomes a HOP called cardinality potential [32]. For count group coverage, this becomes\nd(y|S) = \u2223\u2223\u2223{y\u2032 | Gy\u2032 \u2229 (S \u222a y) 6= \u2205}\u2223\u2223\u2223\u2212 \u2223\u2223\u2223{y\u2032 | Gy\u2032 \u2229 S 6= \u2205}\u2223\u2223\u2223 (9a)\n= \u2223\u2223\u2223 \u22c3 y\u2032\u2208S\u222ay Bk(y\u2032) \u2223\u2223\u2223\u2212 \u2223\u2223\u2223 \u22c3 y\u2032\u2208S Bk(y\u2032) \u2223\u2223\u2223 = \u2223\u2223\u2223Bk(y)\u2223\u2223\u2223\u2212 \u2223\u2223\u2223\u2223Bk(y) \u2229 [ \u22c3 y\u2032\u2208S Bk(y\u2032) ]\u2223\u2223\u2223\u2223, (9b) i.e., the marginal gain of adding y is the number of new configurations y\u2032 covered by the Hamming ball centered at y. Since the size of the intersection of Bk(y) with a union of Hamming balls does not have a straightforward structured representation, we maximize a lower bound on d(y|S) instead:\nd(y | S) \u2265 dlb(y | S) \u2261 \u2223\u2223Bk(y)\u2223\u2223\u2212\u2211\ny\u2032\u2208S\n\u2223\u2223Bk(y) \u2229 Bk(y\u2032)\u2223\u2223 (10)\nThis lower bound dlb(y|S) overcounts the intersection in Eqn. (9b) by summing the intersections with each Bk(y\u2032) separately. We can also interpret this lower bound as clipping the series arising from the inclusion-exclusion principle to the first-order terms. Importantly, (10) depends on y only via its Hamming distance to y\u2032. This is a cardinality potential that depends only on the number of variables yi assigned to a particular label. Specifically, ignoring constant terms, the lower bound can be written as a summation of cardinality factors (one for each previous solution y\u2032 \u2208 S): dlb(y|S) =\u2211\ny\u2032\u2208S \u03b8y\u2032(y), where \u03b8y\u2032(y) = b |S| \u2212 Iy\u2032(y), b is a constant (size of a k-radius Hamming ball), and\nIy\u2032(y) is the number of points in the intersection of k-radius Hamming balls centered at y\u2032 and y.\nWith this approximation, the greedy step means performing MAP inference in a factor graph augmented with cardinality potentials: argmaxy r(y) + \u03bbdlb(y|S). This may be solved via messagepassing, and all outgoing messages from cardinality factors can be computed in O(n log n) time [32]. While this algorithm does not offer any approximation guarantees, it performs well in practice. A subtle point to note is that dlb(y|S) is always decreasing w.r.t. |S| but may become negative due to over-counting. We can fix this by clamping dlb(y|S) to be greater than 0, but in our experiments this was unnecessary \u2013 the greedy algorithm never chose a set where dlb(y|S) was negative. Comparison to DivMBest. The greedy algorithm for Hamming diversity is similar in spirit to the recent work of Batra et al. [2], who also proposed a greedy algorithm (DivMBest) for finding diverse MAP solutions in graphical models. They did not provide any justification for greedy, and our formulation sheds some light on their work. Similar to our approach, at each greedy step, DivMBest involves maximizing a diversity-augmented score: argmaxy r(y)+\u03bb \u2211 y\u2032\u2208S \u03b8y\u2032(y). However, their\ndiversity function grows linearly with the Hamming distance, \u03b8y\u2032(y) = Ham(y\u2032,y) = \u2211n i=1[[y \u2032 i 6= yi]]. Linear diversity rewards are not robust, and tend to over-reward diversity. Our formulation uses a robust diversity function \u03b8y\u2032(y) = b|S| \u2212 Iy\u2032(y) that saturates as y moves far away from y\u2032. In our experiments, we make the saturation behavior smoothly tunable via a parameter \u03b3: Iy\u2032(y) = e\u2212\u03b3 Ham(y\n\u2032,y). A larger \u03b3 corresponds to Hamming balls of smaller radius, and can be set to optimize performance on validation data. We found this to work better than directly tuning the radius k."}, {"heading": "4 Experiments", "text": "We apply our greedy maximization algorithms to two image segmentation problems: (1) interactive binary segmentation (object cutout) (Section 4.1); (2) category-level object segmentation on the PASCAL VOC 2012 dataset [11] (Section 4.2). We compare all methods by their respective oracle accuracies, i.e. the accuracy of the most accurate segmentation in the set ofM diverse segmentations returned by that method. For a small value of M \u2248 5 to 10, a high oracle accuracy indicates that the algorithm has achieved high recall and has identified a good pool of candidate solutions for further processing in a cascaded pipeline. In both experiments, the label \u201cbackground\u201d is typically expected to appear somewhere in the image, and thus does not play a role in the label cost/transition diversity functions. Furthermore, in binary segmentation there is only one non-background label. Thus, we report results with Hamming diversity only (label cost and label transition diversities are not applicable). For the multi-class segmentation experiments, we report experiments with all three.\nBaselines. We compare our proposed methods against DivMBest [2], which greedily produces diverse segmentation by explicitly adding a linear Hamming distance term to the factor graph. Each Hamming term is decomposable along the variables yi and simply modifies the node potentials \u03b8\u0303(yi) = \u03b8(yi)+\u03bb \u2211 y\u2032\u2208S [[yi 6= y\u2032i]]. DivMBest has been shown to outperform techniques such as MBest-MAP [36, 1], which produce high scoring solutions without a focus on diversity, and samplingbased techniques, which produce diverse solutions without a focus on the relevance term [2]. Hence, we do not include those methods here. We also report results for combining different diversity functions via two operators: (\u2297), where we generate the top Mk solutions for each of k diversity functions and then concatenate these lists; and (\u2295), where we linearly combine diversity functions (with coefficients chosen by k-D grid search) and generateM solutions using the combined diversity."}, {"heading": "4.1 Interactive segmentation", "text": "In interactive foreground-background segmentation, the user provides partial labels via scribbles. One way to minimize interactions is for the system to provide a set of candidate segmentations for the user to choose from. We replicate the experimental setup of [2], who curated 100 images from the PASCAL VOC 2012 dataset, and manually provided scribbles on objects contained in them. For each image, the relevance model r(y) is a 2-label pairwise CRF, with a node term for each\nsuperpixel in the image and an edge term for each adjacent pair of superpixels. At each superpixel, we extract colour and texture features. We train a Transductive SVM from the partial supervision provided by the user scribbles. The node potentials are derived from the scores of these TSVMs. The edge potentials are contrast-sensitive Potts. Fifty of the images were used for tuning the diversity parameters \u03bb, \u03b3, and the other 50 for reporting oracle accuracies. The 2-label contrast-sensitive Potts model results in a supermodular relevance function r(y), which can be efficiently maximized via graph cuts [21]. The Hamming ball diversity dlb(y|S) is a collection of cardinality factors, which we optimize with the Cyborg implementation [32].\nResults. For each of the 50 test images in our dataset we generated the single best y1 and 5 additional solutions {y2, . . . ,y6} using each method. Table 3 shows the average oracle accuracies for DivMBest, Hamming ball diversity, and their two combinations. We can see that the combinations slightly outperform both approaches."}, {"heading": "4.2 Category level Segmentation", "text": "In category-level object segmentation, we label each pixel with one of 20 object categories or background. We construct a multi-label pairwise CRF on superpixels. Our node potentials are outputs of category-specific regressors trained by [6], and our edge potentials are multi-label Potts. Inference in the presence of diversity terms is performed with the implementations of Delong et al. [9] for label costs, Tarlow et al. [32] for Hamming ball diversity, and Boykov et al. [3] for label transitions.\nResults. We evaluate all methods on the PASCAL VOC 2012 data [11], consisting of train, val and test partitions with about 1450 images each. We train the regressors of [6] on train, and report oracle accuracies of different methods on val (we cannot report oracle results on test since those annotations are not publicly available). Diversity parameters (\u03b3, \u03bb) are chosen by performing cross-val on val. The standard PASCAL accuracy is the corpus-level intersection-over-union measure, averaged over all categories. For both label cost and transition, we try 3 different concave\nfunctions h(\u00b7) = min{1, \u00b7}, \u221a\n(\u00b7) and log(1 + \u00b7). Table 2 shows the results.3 Hamming ball diversity performs the best, followed by DivMBest, and label cost/transitions are worse here. We found that while worst on average, label transition diversity helps in an interesting scenario \u2013 when the first best segmentation y1 includes a pair of rare or mutually confusing labels (say dog-cat). Fig. 3 shows an example, and more illustrations are provided in the supplement. In these cases, searching for a different label transition produces a better segmentation. Finally, we note that lists produced with combined diversity significantly outperform any single method (including DivMBest)."}, {"heading": "5 Discussion and Conclusion", "text": "In this paper, we study greedy algorithms for maximizing scoring functions that promote diverse sets of combinatorial configurations. This problem arises naturally in domains such as Computer Vision, Natural Language Processing, or Computational Biology, where we want to search for a set of diverse high-quality solutions in a structured output space.\nThe diversity functions we propose are monotone submodular functions by construction. Thus, if r(y) + p(y) \u2265 0 for all y, then the entire scoring function F is monotone submodular. We showed that r(y) can simply be learned to be positive. The greedy algorithm for maximizing monotone submodular functions has proved useful in moderately-sized unstructured spaces. To the best of our knowledge, this is the first generalization to exponentially large structured output spaces. In particular, our contribution lies in reducing the greedy augmentation step to inference with structured, efficiently solvable HOPs. This insight makes new connections between submodular optimization and work on inference in graphical models. We now address some questions.\nCan we sample? One question that may be posed is how random sampling would perform for large ground sets V . Unfortunately, the expected value of a random sample of M elements can be much worse than the optimal value F (S\u2217), especially if N is large. Lemma 1 is proved in the supplement. Lemma 1. Let S \u2286 V be a sample of size M taken uniformly at random. There exist monotone submodular functions where E[F (S)] \u2264 MN max|S|=M F (S).\nGuarantees? If F is nonnegative, monotone submodular, then using an exact HOP inference algorithm will clearly result in an approximation factor of 1\u2212 1/e. But many HOP inference procedures are approximate. Lemma 4 formalizes how approximate inference affects the approximation bounds. Lemma 2. Let F \u2265 0 be monotone submodular. If each step of the greedy algorithm uses an approximate marginal gain maximizer bt+1 with F (bt+1 | St) \u2265 \u03b1maxa\u2208V F (a | St)\u2212 t+1, then F (SM ) \u2265 (1\u2212 1e\u03b1 ) max|S|\u2264M F (S)\u2212 \u2211M i=1 t. Parts of Lemma 4 have been observed in previous work [14, 31]; we show the combination in the supplement. If F is monotone but not nonnegative, then Lemma 4 can be extended to a relative error bound F (S\nM )\u2212Fmin F (S\u2217)\u2212Fmin \u2265 (1 \u2212 1 e\u03b1 ) \u2212\n\u2211 i i\nF (S\u2217)\u2212Fmin that refers to Fmin = minS F (S) and the optimal solution S\u2217. While stating these results, we add that further additive approximation losses occur if the approximation bound for inference is computed on a shifted or reflected function (positive scores vs positive energies). We pose theoretical improvements as an open question for future work. That said, our experiments convincingly show that the algorithms perform very well in practice, even when there are no guarantees (as with Hamming Ball diversity).\nGeneralization. In addition to the three specific examples in Section 3, our constructions generalize to the broad HOP class of upper-envelope potentials [19]. The details are provided in the supplement.\nAcknowledgements. We thank Xiao Lin for his help. The majority of this work was done while AP was an intern at Virginia Tech. AP and DB were partially supported by the National Science Foundation under Grant No. IIS-1353694 and IIS-1350553, the Army Research Office YIP Award W911NF-14-1-0180, and the Office of Naval Research Award N00014-14-1-0679, awarded to DB. SJ was supported by gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Apple, C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and Yahoo!."}, {"heading": "A Structured SVMs with nonnegativity constraint", "text": "In this section, we will show that SSVMs have no natural origin, and that parameters learnt with non-negativity constraints achieve exactly the same hinge loss as achieved without the nonnegativity constraint. For a set of ` training instances (xn,yn) \u2208 X \u00d7 Y, n = 1, . . . , ` from a sample space X and label space Y , the structured SVM minimizes the following regularized risk function.\nmin w \u2016w\u20162 + C \u2211\u0300 n=1 max y\u2208Y (\u2206(yn,y) +w\u2032\u03a8(xn,y)\u2212w\u2032\u03a8(xn,yn)) (11)\nThe function \u2206 : Y \u00d7 Y \u2192 R+ measures a distance in label space and is an arbitrary function satisfying \u2206(y,y\u2032) \u2265 0 and \u2206(y,y) = 0 \u2200y,y\u2032 \u2208 Y . The function \u03a8 : X \u00d7 Y \u2192 Rd is a feature function, extracting some feature vector from a given sample and label.\nBecause the regularized risk function above is non-differentiable, it is often reformulated in terms of a quadratic program by introducing one slack variable \u03ben for each sample, each representing the value of the maximum. The standard structured SVM primal formulation is given as follows.\nmin w,\u03be \u2016w\u20162 + C\u2211`n=1 \u03ben s.t. w\u2032\u03a8(xn,yn)\u2212w\u2032\u03a8(xn,y) + \u03ben \u2265 \u2206(yn,y), n = 1, . . . , `, \u2200y \u2208 Y\n(12)\nWe will refer to the primal objective in Equation 12 as P1 and the optimal solution as w1, \u03be1. Next, consider the following augmented formulation, where w\u0302 = [ w b ] , \u03a8\u0302(xn,yn) = [ \u03a8(xn,yn) 1 ] :\nmin w\u0302,\u03be \u2016w\u20162 + C\u2211`n=1 \u03ben s.t. w\u0302\u2032\u03a8\u0302(xn,yn)\u2212 w\u0302\u2032\u03a8\u0302(xn,y) + \u03ben \u2265 \u2206(yn,y), n = 1, . . . , `, \u2200y \u2208 Y\n(13)\nWe will refer to the primal objective in Equation 13 as P2 and the optimal solution as w\u03022 =[ w2 b2 ] , \u03be2. Note: We do not regularize b.\nClaim 1. P1(w1, \u03be1) = P2(w\u03022, \u03be2) Proof. Adding a bias feature (b) doesn\u2019t affect the objective function and every constraint is invariant of it. Hence, the two problems are equivalent.\nNext, consider the formulation, where we extend P2 to enforce non-negativity of scores.\nmin w\u0302,\u03be \u2016w\u20162 + C\u2211`n=1 \u03ben s.t. w\u0302\u2032\u03a8\u0302(xn,yn)\u2212 w\u0302\u2032\u03a8\u0302(xn,y) + \u03ben \u2265 \u2206(yn,y), n = 1, . . . , `, \u2200y \u2208 Y\nw\u0302\u2032\u03a8\u0302(xn,y) \u2265 0 n = 1, . . . , `, \u2200y \u2208 Y (14)\nWe will refer to the primal objective in Equation 14 as P3 and the optimal solution as w\u03023 =[ w3 b3 ] , \u03be3.\nClaim 2. P2(w\u03022, \u03be2) = P3(w\u03023, \u03be3) Proof. It is easy to see that P2(w\u03022, \u03be2) \u2264 P3(w\u03023, \u03be3) as (w\u03022, \u03be2) is the optimal solution for P2 and (w\u03023, \u03be3) is also a feasible solution for P2.\nConsider the vector w\u0302\u22172 =\n[ w2\n\u2212min n min y w\u20322\u03a8(x n,y)\n] , Since the bias term doesn\u2019t occur in the\nobjective function, hence P3(w\u0302\u22172 , \u03be2) = P2(w\u03022, \u03be2).\nAlso, (w\u0302\u22172 , \u03be2) is a feasible solution to P3, hence, P3(w\u03023, \u03be3) \u2264 P3(w\u0302\u22172 , \u03be2) = P2(w\u03022, \u03be2). Therefore, P2(w\u03022, \u03be2) = P3(w\u03023, \u03be3).\nTherefore, even after adding the nonnegativity constraints, the solutions achieve the same values for the regularised risk function and hence are expected to have the same generalization guarantees. In practice, the non-negativity constraints can be added in a cutting-plane procedure via a MAP call."}, {"heading": "B Label Transitions", "text": "Groups and Motivating Scenario. In this section, we generalize the label cost diversity function to reward not just the presence of certain labels, but the presence of certain label transitions. For instance, if the highest scoring segmentation contains a \u201ccow\u201d on \u201cgrass\u201d, this diversity function will reward other segmentations for containing novel label transitions, such as \u201csheep-grass\u201d or \u201ccowground\u201d or \u201csheep-sky\u201d. Formally, we define one group G`,`\u2032 per label pair `, `\u2032, and an item a belongs to G`,`\u2032 if y = \u03c6(a) contains two adjacent variables yi, yj with labels yi = `, yj = `\u2032.\nStructured Representation of Marginal Gains. For diversity of label transitions, the marginal gain D(a | S) becomes a HOP called cooperative cuts [17]. Let Cuty(`, `\u2032) = {(yi, yj) \u2208 E | yi = `, yj = `\n\u2032} be the cut set for a specific label transition (`, `\u2032). Further, let #CutS(`, `\u2032) count the number of items a \u2208 S that contain at least one (`, `\u2032) label transition: #CutS(`, `\u2032) = |{a \u2208 S | Cut\u03c6(a)(`, `\n\u2032) 6= \u2205}| = |S \u2229G`,`\u2032 |. The marginal gains for this diversity function are: d(y | S) = D(\u03c6\u22121(y) | S) (15a) = \u2211 `,`\u2032 h(#CutS\u222a\u03c6\u22121(y)(`, ` \u2032))\u2212 h(#CutS(`, `\u2032)) (15b)\n= \u2211 `,`\u2032 Cuty(`,` \u2032)6=\u2205 h(1 + #CutS(`, ` \u2032))\u2212 h(#CutS(`, `\u2032)) (15c)\nSimilar to single label groups, the gain for a label pair (`, `\u2032) decreases as #CutS(`, `\u2032) grows. Thus, d(y | S) rewards the presence of pair (`, `\u2032) by an amount proportional to how rare it is in the segmentations in S. Analogously to the label costs, the parsimony factor in this setting is p(y) = \u2211 Cuty(`,`\u2032)6=\u2205 c(`, `\n\u2032) encouraging each individual y to have a small number of label transitions. Specifically, when using a count coverage and parsimony term with c(`, `\u2032) = \u22121,we eventually maximize\nr(y) + p(y) + d(y) (16a) = r(y)\u2212 \u2211\n`,`\u2032:S\u2208G`,`\u2032 min{#Cuty(`, `\u2032), 1}, (16b)\nwhich is a supermodular function on the set of cut edges for each label transition. Thus, the multilabel cooperative cut inference algorithm by Kohli et al. [20] applies. The construction for general h looks similar, with a degrading cost in front of the min."}, {"heading": "C Experiments", "text": "For the sake of completeness and to show the difference in sets of solutions generated by different diversity functions, we show sample sets of solutions generated for a given image (Fig. 5, 6 and 7). These results help in understanding the behavior of different diversity functions."}, {"heading": "D Proof of Lemma 1", "text": "Lemma 3. Let S be a sample of size M taken uniformly at random. There exist monotone submodular functions where E[F (S)] \u2264 (M/N + /M) max|S|\u2264M F (S) for any \u2265 0.\nThe bound in the main paper follows with = 0.\nProof. To prove this statement, we consider a specific worst-case function. Let R \u2286 V be a fixed set of size M , and let\nF (S) = |S \u2229R|+ min{|S \\R|, 1}. (17)\nThe function F is obviously nondecreasing and it is also submodular. For this function, the cardinality-constrained optimum is\nmax |S|\u2264M F (S) = F (R) = M. (18)\nThe expected value of an M -sized sample is the expectation of a hypergeometric distribution plus a correction taking into account that every set except R will have value at least 1 (using the second part of F ):\nES [F (S)] = ( N\nM )\u22121 \u2211 S\u2286V,|S|=M |S \u2229R|+ ( N M ) \u2212  (19) = ( N\nM )\u22121( M\u2211 r=1 ( M r )( N \u2212M M \u2212 r ) r \u2212 ) + (20)\n= M2\nN + \u2212\n( N\nM\n)\u22121 (21)\n< (M/N + /M)F (R). (22)\nThis is also a fairly tight bound: if we sample each element with probability M/N , then, using Lemma 2.2 in [12] and the monotonicity of F , it holds that ES [F (S)] \u2265 MN F (V ) \u2265 MN F (S\u2217)."}, {"heading": "E Proof of Lemma 2", "text": "Lemma 4. Let F \u2265 0 be monotone submodular. If each step of the greedy algorithm uses an approximate gain maximizer bi+1 with F (bi+1 | Si) \u2265 \u03b1maxa\u2208V F (a | Si)\u2212 i+1, then\nF (SM ) \u2265 (1\u2212 1 e\u03b1 ) max |S|\u2264M F (S)\u2212 M\u2211 i=1 i.\nTo prove Lemma 2, we employ a helpful intermediate observation. We will denote the optimal solution by S\u2217 \u2208 arg max|S|\u2264M F (S). Lemma 5. If F (bi+1 | Si) \u2265 \u03b1maxa\u2208V F (a | Si)\u2212 i+1, then\nF (bi+1 | Si) \u2265 \u03b1 M (F (S\u2217)\u2212 F (Si))\u2212 i+1.\nProof. (Lemma 5). Define T i = S\u2217 \\ Si to be the set of all elements that are in S\u2217 but have not yet been selected. Order the elements in T i in an arbitrary order as t1, . . . th (note that h \u2264M because |S\u2217| \u2264M . By monotonicity of F and the fact that S\u2217 \u2286 Si \u222a T i, it holds that\nF (S\u2217)\u2212 F (Si) \u2264 F (T i \u222a Si)\u2212 F (Si) (23)\n= h\u2211 j=1 F (tj | Si \u222a {t1, . . . , tj\u22121) (24) \u2264 h\u2211 j=1 F (tj | Si) (25)\n\u2264M max a\u2208V \\Si F (a | Si) (26)\n\u2264 M \u03b1 (F (bi+1 | Si) + i+1) (27)\nIn Equation (25), we use diminishing marginal returns. In the end, we use the assumption of the lemma, which implies that\nmax a F (a | S) \u2264 1 \u03b1\n(F (bi+1 | Si) + i+1). (28) Rearranging yields the result of the lemma.\nNow we are equipped to prove Lemma 4.\nProof. Lemma 5 implies that F (bi+1 | Si) = F (Si+1)\u2212 F (Si) (29)\n\u2265 \u03b1 M\n( F (S\u2217)\u2212 F (Si) ) \u2212 i+1. (30)\nWe rearrange this to F (S\u2217)\u2212 F (Si+1) (31) \u2264 (1\u2212 \u03b1\nM )F (S\u2217)\u2212 (1\u2212 \u03b1 M )F (Si) + i+1 (32)\n= (1\u2212 \u03b1 M )(F (S\u2217)\u2212 F (Si)) + i+1 (33)\n\u2264 (1\u2212 \u03b1 M\n)i+1(F (S\u2217)\u2212 F (S0)) + i\u2211\nj=1\n( 1\u2212 \u03b1\nM\n)i\u2212j j (34)\n\u2264 (1\u2212 \u03b1 M\n)i+1(F (S\u2217)\u2212 F (S0)) + i\u2211\nj=1\nj (35)\nWith F (S0) = F (\u2205) = 0 we rearrange to\nF (SM ) \u2265 (1\u2212 (1\u2212 \u03b1 M )M )F (S\u2217)\u2212 M\u2211 j=1 j (36)\n\u2265 (1\u2212 e\u2212\u03b1)F (S\u2217)\u2212 M\u2211 j=1 j . (37)"}, {"heading": "F Relative Error", "text": "If we have a monotone but not nonnegative function, we may shift the function and obtain a relative bound:\nLemma 6. Let F be an arbitrary monotone submodular function, and let Fmin = minS\u2286V F (S). We can define a new, shifted monotone non-negative version of F as F+(S) , F (S)\u2212 Fmin . If we apply the greedy algorithm to F+, we obtain a solution S\u0302 that satisfies F+(S\u0302) \u2265 \u03b1F+(S\u2217), then the solution S\u0302 has a bounded relative approximation error:\nF (S\u0302)\u2212 Fmin F (S\u2217)\u2212 Fmin \u2265 \u03b1.\nProof. By a proof analogous to that of Lemma 2, we get, for \u03b1 = (1\u2212 1e ), F+(S\u0302) \u2265 \u03b1F+(S\u2217) (38)\n\u21d4 F (S\u0302)\u2212 Fmin \u2265 \u03b1(F (S\u2217)\u2212 Fmin ) (39)\n\u21d4 F (S\u0302)\u2212 Fmin F (S\u2217)\u2212 Fmin \u2265 \u03b1 = 1\u2212 1 e . (40)"}, {"heading": "G Generalization: Upper Envelope Potentials", "text": "In addition to the three specific examples given in the main document (Section 4.1, 4.1, 4.2), we can also generalize these constructions to a broad class of HOPs called upper envelope potentials [19].\nLet G1, G2, . . . , Gq, . . . , Gb be disjoint groups where b is polynomial in the size of the number of base variables (n). We consider the group count diversity. At iteration t in the greedy algorithm, assume without loss of generalization that we have covered G1, G2, . . . , Gk. Then for the (t+ 1) th iteration, the marginal gain of y is:\nd(y | St) = {\n0 if y \u2208 {G1, .., Gk} 1 otherwise.\n(41)\nWe can now express d(y | St) as an upper-envelope potential, i.e. d(y | St) \u2261 maxq \u03bdq(y) where: \u03bdq(y) = \u00b5q + \u2211 i\u2208[n] \u2211 `\u2208L wqi`\u03b4i(`) (42)\nwhere \u03b4i(`) returns 1 if yi = `.\nEach linear function \u03bdq(y) encodes two pieces of information:\n1. \u00b5q encodes if Gq is uncovered at the end of the tth step, i.e.\n\u00b5q = { 0 if q \u2208 {1, .., k} 1 otherwise.\n(43)\n2. The second part of (42) indicates whether or not y lies in the group Gq (defined below).\nFor general groups, it may not be possible to have linear encodings for membership. By construction, we describe one practical example:\nRegion Consistency Diversity. Consider a region C in the image whose superpixels yC we want to bias towards a homogenous or uniform labeling. Thus, when we search for diverse labelings, we want to encourage the entire set of superpixels yc to change together. In this case, each group G` corresponds to the set of segmentations that assign label ` to the region yc. For such a diversity function, we define wqi` as:\nwqi` = { 0 if i \u2208 [C] and ` = q \u2212\u221e otherwise. (44)\nWith this definition of wqi`, the second part of (42) indicates whether or not y lies in the group Gq It is known [30, 19] that maximizing any upper-envelope HOP can be reduced to the maximization of a pairwise function with the addition of an auxiliary switching variable x that takes values from the index set of q (in this case [L]).\nmax yc d(y | St) = max yc,x (\u03c6x(q) + \u2211 i\u2208V \u03c6xi(q, yi)) (45)\nwhere \u03c6x(q) = \u00b5q and \u03c6xi(q, yi) = w q iyi . This pairwise function can be maximised using standard message passing algorithms such as TRW and BP. However, for some cases, such as the region consistency diversity defined above, the pairwise function is supermodular, and graph cuts can be used."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "To cope with the high level of ambiguity faced in domains such as Computer<lb>Vision or Natural Language processing, robust prediction methods often search<lb>for a diverse set of high-quality candidate solutions or proposals. In structured<lb>prediction problems, this becomes a daunting task, as the solution space (image<lb>labelings, sentence parses, etc.) is exponentially large. We study greedy algo-<lb>rithms for finding a diverse subset of solutions in structured-output spaces by<lb>drawing new connections between submodular functions over combinatorial item<lb>sets and High-Order Potentials (HOPs) studied for graphical models. Specifically,<lb>we show via examples that when marginal gains of submodular diversity functions<lb>allow structured representations, this enables efficient (sub-linear time) approxi-<lb>mate maximization by reducing the greedy augmentation step to inference in a<lb>factor graph with appropriately constructed HOPs. We discuss benefits, trade-<lb>offs, and show that our constructions lead to significantly better proposals.", "creator": "LaTeX with hyperref package"}}}