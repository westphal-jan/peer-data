{"id": "1411.2021", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2014", "title": "Partitioning Well-Clustered Graphs: Spectral Clustering Works!", "abstract": "\u2026 we already study a strongly suitable class of well - clustered graphs distributions that admit good topological k - 2 way partitions and similarly present the first almost - local linear time algorithm for with almost - optimal binary approximation i guarantees partitioning such density graphs. a good colored k - way edge partition is called a partition of the same vertices because of input a graph given into disjoint sparse clusters ( subsets ) $ \\ { vertex s _ g i \\ } _ { i = 1 } ^, k $, namely such that implementing each cluster is overall better connected on just the inside than towards serving the outside. often this minimum problem theorem is even a key building structure block in algorithm design, security and its has wide applications in online community detection and network analysis.", "histories": [["v1", "Fri, 7 Nov 2014 20:23:50 GMT  (48kb)", "https://arxiv.org/abs/1411.2021v1", "32 pages"], ["v2", "Tue, 17 Nov 2015 17:08:00 GMT  (39kb)", "http://arxiv.org/abs/1411.2021v2", "28 pages. A preliminary version of this paper appeared in the 28th Annual Conference on Learning Theory (COLT 2015)"], ["v3", "Tue, 31 Jan 2017 15:07:33 GMT  (44kb)", "http://arxiv.org/abs/1411.2021v3", "A preliminary version of this paper appeared in COLT'15; the full version is to appear in SIAM Journal on Computing"]], "COMMENTS": "32 pages", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["richard peng", "he sun", "luca zanetti"], "accepted": false, "id": "1411.2021"}, "pdf": {"name": "1411.2021.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Richard Peng", "He Sun", "Luca Zanetti"], "emails": ["(rpeng@cc.gatech.edu)", "(h.sun@bristol.ac.uk)", "(luca.zanetti@bristol.ac.uk)"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n20 21\nv3 [\ncs .D\nS] 3\n1 Ja\nn 20\n17\nWe also give a nearly-linear time algorithm for partitioning well-clustered graphs based on computing a matrix exponential and approximate nearest neighbor data structures.\nKeywords: graph partitioning, spectral clustering, k-means, heat kernel\n\u2217A preliminary version of this paper appeared in the 28th Annual Conference on Learning Theory (COLT 2015).\n\u2020Georgia Institute of Technology, Atlanta, USA. (rpeng@cc.gatech.edu) \u2021University of Bristol, Bristol, UK. (h.sun@bristol.ac.uk) Questions, comments, or corrections to this doc-\nument may be directed to that email address. \u00a7University of Bristol, Bristol, UK. (luca.zanetti@bristol.ac.uk)\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3"}, {"heading": "2 Preliminaries 4", "text": ""}, {"heading": "3 Connection Between Eigenvectors and Indicator Vectors of Clusters 5", "text": ""}, {"heading": "4 Analysis of Spectral Clustering 10", "text": "4.1 k-Means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.2 Analysis of the Spectral Embedding . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.3 Approximation Guarantees of Spectral Clustering . . . . . . . . . . . . . . . . . . 13 4.4 Proof of Theorem 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15"}, {"heading": "5 Partitioning Well-Clustered Graphs in Nearly-Linear Time 17", "text": "5.1 The Seeding Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 5.2 The Grouping Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.3 Approximation Analysis of the Algorithm . . . . . . . . . . . . . . . . . . . . . . 24 5.4 Fast computation of the required embedding . . . . . . . . . . . . . . . . . . . . . 25 5.5 Proof of Theorem 1.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27"}, {"heading": "1 Introduction", "text": "Partitioning a graph into two or more pieces is one of the most fundamental problems in combinatorial optimization, and has comprehensive applications in various disciplines of computer science.\nOne of the most studied graph partitioning problems is the edge expansion problem, i.e., finding a cut with few crossing edges normalized by the size of the smaller side of the cut. Formally, let G = (V,E) be an undirected graph. For any set S, the conductance of set S is defined by\n\u03c6G(S) , |E(S, V \\ S)|\nvol(S) ,\nwhere vol(S) is the total weight of edges incident to vertices in S, and let the conductance of G be\n\u03c6(G) , min S:vol(S)6vol(G)/2 \u03c6G(S).\nThe edge expansion problem asks for a set S \u2286 V of vol(S) 6 vol(V )/2 such that \u03c6G(S) = \u03c6(G). This problem is known to be NP-hard [26], and the current best approximation algorithm achieves an approximation ratio of O (\u221a log n ) [5].\nThe k-way partitioning problem is a natural generalization of the edge expansion problem. We call subsets of vertices (i.e. clusters) A1, . . . , Ak a k-way partition of G if Ai \u2229 Aj = \u2205 for different i and j, and \u22c3k i=1 Ai = V . The k-way partitioning problem asks for a k-way partition of G such that the conductance of any Ai in the partition is at most the k-way expansion constant, defined by\n\u03c1(k) , min partition A1,...,Ak max 16i6k \u03c6G(Ai). (1.1)\nClusters of low conductance in networks appearing in practice usually capture the notion of community, and algorithms for finding these subsets have applications in various domains such as community detection and network analysis. In computer vision, most image segmentation procedures are based on region-based merge and split [10], which in turn rely on partitioning graphs into multiple subsets [36]. On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].\nDespite widespread use of various graph partitioning schemes over the past decades, the quantitative relationship between the k-way expansion constant and the eigenvalues of the graph Laplacians were unknown until a sequence of very recent results [22, 24]. For instance, Lee et al. [22] proved the following higher-order Cheeger inequality:\n\u03bbk 2\n6 \u03c1(k) 6 O(k2) \u221a\n\u03bbk, (1.2)\nwhere 0 = \u03bb1 6 . . . 6 \u03bbn 6 2 are the eigevalues of the normalized Laplacian matrix L of G. Informally, the higher-order Cheeger inequality shows that a graph G has a k-way partition with low \u03c1(k) if and only if \u03bbk is small. Indeed, (1.2) implies that a large gap between \u03bbk+1 and \u03c1(k) guarantees (i) existence of a k-way partition {Si}ki=1 with bounded \u03c6G(Si) 6 \u03c1(k), and (ii) any (k + 1)-way partition of G contains a subset with significantly higher conductance \u03c1(k + 1) > \u03bbk+1/2 compared with \u03c1(k). Hence, a suitable lower bound on the gap \u03a5(k) for some k, defined by\n\u03a5(k) , \u03bbk+1 \u03c1(k) , (1.3)\nimplies the existence of a k-way partition for which every cluster has low conductance, and that G is a well-clustered graph.\nWe study well-clustered graphs which satisfy a gap assumption on \u03a5(k) in this paper. Our gap assumption on \u03a5(k) is slightly weaker than assuming gaps between the eigenvalues,\nbut nonetheless related via Cheeger-type inequalities. Our assumption is also well-grounded in practical studies: clustering algorithms have been studied before under this assumption in machine learning, e.g. [1]. Sharp drop-offs between two consecutive eigenvalues have also been observed to give good indicators for the number of clusters, e.g. [40] and Section D in [14]."}, {"heading": "1.1 Our Results", "text": "We give structural results that show close connections between the eigenvectors and the indicator vectors of the clusters. This characterization allows us to show that many variants of spectral clustering, that are based on the spectral embedding and that work \u201cin practice\u201d, can be rigorously analyzed \u201cin theory\u201d. Moreover, exploiting our gap assumption, we can approximate this spectral embedding using the heat kernel of the graph. Combining this with approximate nearest neighbor data structures, we give a nearly-linear time algorithm for the k-way partitioning problem.\nOur structural results can be summarized as follows. Let {fi}ki=1 be the eigenvectors corresponding to the k smallest eigenvalues of L, and {Si}ki=1 be a k-way partition of G achieving \u03c1(k) defined in (1.1). We define {gi}ki=1 to be the indicator vectors of the clusters {Si}ki=1, where gi(u) = 1 if u \u2208 Si, and gi(u) = 0 otherwise. We further use {g\u0304i}ki=1 to express the normalized indicator vectors of the clusters {Si}ki=1, defined by\ng\u0304i = D1/2gi \u2016D1/2gi\u2016 .\nWe show that, under the condition of \u03a5(k) = \u2126(k2), the span of {g\u0304i}ki=1 and the span of {fi}ki=1 are close to each other, which is stated formally in Theorem 1.1.\nTheorem 1.1 (The Structure Theorem). Let {Si}ki=1 be a k-way partition of G achieving \u03c1(k), and let \u03a5(k) = \u03bbk+1/\u03c1(k) = \u2126(k\n2). Let {fi}ki=1 and {g\u0304i}ki=1 be defined as above. Then, the following statements hold:\n1. For every g\u0304i, there is a linear combination of {fi}ki=1, called f\u0302i, such that \u2016gi \u2212 f\u0302i\u20162 6 1/\u03a5(k).\n2. For every fi, there is a linear combination of {gi}ki=1, called g\u0302i, such that \u2016fi \u2212 g\u0302i\u20162 6 1.1k/\u03a5(k).\nThis theorem generalizes the result shown by Arora et al. ([2], Theorem 2.2), which proves the easier direction (the first statement, Theorem 1.1), and can be considered as a stronger version of the well-known Davis-Kahan theorem [12]. We remark that, despite that we use the higher-order Cheeger inequality (1.2) to motivate the definition of \u03a5(k), our proof of the structure theorem is self-contained. Specifically, it omits much of the machinery used in the proofs of higher-order and improved Cheeger inequalities [21, 22].\nThe structure theorem has several applications. For instance, we look at the well-known spectral embedding F : V [G] \u2192 Rk defined by\nF (u) , 1\nNormalizationFactor(u) \u00b7 (f1(u), . . . , fk(u))\u22ba , (1.4)\nwhere NormalizationFactor(u) \u2208 R is a normalization factor for u \u2208 V [G]. We use Theorem 1.1 to show that this well-known spectral embedding exhibits very nice geometric properties: (i) all points F (u) from the same cluster are close to each other, and (ii) most pairs of points F (u), F (v) from different clusters are far from each other; (iii) the bigger the value of \u03a5(k), the higher concentration the embedded points within the same cluster.\nBased on these facts, we analyze the performance of spectral clustering, aiming at answering the following longstanding open question: Why does spectral clustering perform well in practice?\nWe show that the partition {Ai}ki=1 produced by spectral clustering gives a good approximation of any \u201coptimal\u201d partition {Si}ki=1: every Ai has low conductance, and has large overlap with its corresponding Si. This algorithm has comprehensive applications, and has been the subject of extensive experimental studies for more than 20 years, e.g. [28, 40]. Prior to this work, similar results on spectral clustering mainly focus on graphs generated from the stochastic block model. Instead, our gap assumption captures more general classes of graphs by replacing the input model with a structural condition. Our result represents the first rigorous analysis of spectral clustering for the general family of graphs that exhibit a multi-cut structure but are not captured by the stochastic block model. Our result is as follows:\nTheorem 1.2 (Approximation Guarantee of Spectral Clustering). Let G be a graph satisfying the condition \u03a5(k) = \u03bbk+1/\u03c1(k) = \u2126(k\n3), and k \u2208 N. Let F : V [G] \u2192 Rk be the embedding defined in (1.4). Let {Ai}ki=1 be a k-way partition by any k-means algorithm running in Rk that achieves an approximation ratio APT. Then, the following statements hold: (i) vol(Ai\u25b3Si) = O ( APT \u00b7 k3/\u03a5(k) ) vol(Si), and (ii) \u03c6G(Ai) = 1.1 \u00b7 \u03c6G(Si) +O ( APT \u00b7 k3/\u03a5(k) ) .\nWe further study fast algorithms for partitioning well-clustered graphs. Notice that, for moderately large values of k, e.g. k = \u03c9(log n), directly applying k-means algorithms and Theorem 1.2 does not give a nearly-linear time algorithm, since (i) obtaining the spectral embedding (1.4) requires \u2126(mk) time for computing k eigenvectors, and (ii) most k-means algorithms run in \u2126(nk) time.\nTo overcome the first obstacle, we study the so-called heat kernel embedding xt : V [G] \u2192 Rn, an embedding from V to Rn defined by\nxt(u) , 1 NormalizationFactor(u) \u00b7 ( e\u2212t\u00b7\u03bb1f1(u), \u00b7 \u00b7 \u00b7 , e\u2212t\u00b7\u03bbnfn(u) )\nfor some t \u2208 R>0. The heat kernel of a graph is a well-studied mathematical concept and is related to, for example, the study of random walks [34]. We exploit the heat kernel embedding to approximate the squared-distance \u2016F (u)\u2212F (v)\u20162 of the embedded points F (u) and F (v) via their heat-kernel distance \u2016xt(u)\u2212xt(v)\u20162. Since the heat kernel distances between vertices can be approximated in nearly-linear time [29], this approach avoids the computation of eigenvectors for a large value of k. For the second obstacle, instead of applying k-means algorithms as a black-box, we apply approximate nearest-neighbor data structures. This can be viewed as an ad-hoc version of a k-means algorithm, and indicates that in many scenarios the standard Lloydtype heuristic widely used in k-means algorithms can eventually be avoided. Our result is as follows:\nTheorem 1.3 (Nearly-Linear Time Algorithm For Partitioning Graphs). Let G = (V,E) be a graph of n vertices and m edges, and k = \u03c9(log n) be the number of clusters. Assume that \u03a5(k) = \u03bbk+1/\u03c1(k) = \u2126\u0303(k\n5), and {Si}ki=1 is a k-way partition such that \u03c6G(Si) 6 \u03c1(k). Then there is an algorithm which runs in O\u0303(m) time and outputs a k-way partition {Ai}ki=1 such that (i) vol(Ai\u25b3Si) = O\u0303 ( k4/\u03a5(k) ) vol(Si), and (ii) \u03c6G(Ai) = 1.1 \u00b7 \u03c6G(Si) + O\u0303 ( k4/\u03a5(k) ) . The O\u0303(\u00b7) and \u2126\u0303(\u00b7) terms here hide a factor of poly log n. We remark that bounds of other expansion parameters of k-way partitioning can be derived from our analysis as well. For instance, it is easy to see that \u03c1(k) and the normalized cut [36] studied in machine learning, which is defined as the sum of the conductance of all returned clusters, differ by at most a factor of k, and the normalized cut value of a k-way partition from spectral clustering can be derived from our results."}, {"heading": "1.2 Related Work", "text": "In the broadest sense, our algorithms are clustering routines. Clustering can be formulated in many ways, and the study of algorithms in many such formulations are areas of active work\n[7, 8, 17, 25]. Among these, our work is most closely related to spectral clustering, which is closely related to normalized or low conductance cuts [36]. The k-way expansion that we study is always within a factor of k of k-way normalized cuts.\nTheoretical studies of graph partitioning are often based on augmenting the fractional relaxation of these cut problems with additional constraints in the form of semidefinite programs or Lasserre hierarchy. The goal of our study is to obtain similar bounds using more practical tools such as k-means and heat-kernel embedding.\nOveis Gharan and Trevisan [32] formulate the notion of clusters with respect to the inner and outer conductance: a cluster S should have low outer conductance, and the conductance of the induced subgraph by S should be high. Under a gap assumption between \u03bbk+1 and \u03bbk, they present a polynomial-time algorithm which finds a k-way partition {Ai}ki=1 that satisfies the inner- and outer-conductance condition. In order to ensure that every Ai has high inner conductance, they assume that \u03bbk+1 > poly(k)\u03bb 1/4 k , which is much stronger than ours. Moreover, their algorithm runs in polynomial-time, in contrast to our nearly-linear time algorithm. Dey et al. [13] studies the properties of the spectral embedding for graphs having a gap between \u03bbk and \u03bbk+1 and presents a k-way partition algorithm, which is based on k-center clustering and is similar in spirit to our work. Using combinatorial arguments, they are able to show that the clusters concentrate around k distant points in the spectral embedding. In contrast to our work, their result only holds for bounded-degree graphs, and cannot provide an approximate guarantee for individual clusters. Moreover, their algorithm runs in nearly-linear time only if k = O(poly log n).\nWe also explore the separation between \u03bbk and \u03bbk+1 from an algorithmic perspective, and show that this assumption interacts well with heat-kernel embeddings. The heat kernel has been used in previous algorithms on local partitioning [9], balanced separators [29]. It also plays a key role in current efficient approximation algorithms for finding low conductance cuts [30, 35]. However, most of these theoretical guarantees are through the matrix multiplicative weights update framework [3, 4]. Our algorithm instead directly uses the heat-kernel embedding to find low conductance cuts.\nThere is also a considerable amount of research on partitioning random graphs. For instance, in the Stochastic Block Model (SBM) [27], the input graph with k clusters is generated according to probabilities p and q with p > q: an edge between any two vertices within the same cluster is placed with probability p, and an edge between any two vertices from different clusters is placed with probability q. It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41]. However, the analysis of these algorithms cannot be easily generalized into our setting: we consider graphs where edges are not necessarily chosen independently with certain probabilities, but can be added in an \u201cadversarial\u201d way. For this reason, standard perturbation theorems used in the analysis of algorithms for SBMs, such as the Davis-Kahan theorem [12], cannot be always applied, and ad-hoc arguments specific for graphs, like our structure theorem (Theorem 1.1), become necessary."}, {"heading": "2 Preliminaries", "text": "Let G = (V,E) be an undirected and unweighted graph with n vertices and m edges. The set of neighbors of a vertex u is represented by N(u), and its degree is du = |N(u)|. For any set S \u2286 V , let vol(S) , \u2211u\u2208S du. For any set S, T \u2286 V , we define E(S, T ) to be the set of edges between S and T , aka E(S, T ) , {{u, v}|u \u2208 S and v \u2208 T}. For simplicity, we write \u2202S = E(S, V \\S) for any set S \u2286 V . For two sets X and Y , the symmetric difference of X and Y is defined as X\u25b3Y , (X \\ Y ) \u222a (Y \\X).\nWe work extensively with algebraic objects related to G. We use D to denote the n \u00d7 n diagonal matrix with Duu = du for u \u2208 V [G]. The Laplacian matrix of G is defined by L , D \u2212A, where A is the adjacency matrix of G defined by Au,v = 1 if {u, v} \u2208 E[G], and\nAu,v = 0 otherwise. The normalized Laplacian matrix of G is defined by L , D\u22121/2LD\u22121/2 = I \u2212 D\u22121/2AD\u22121/2. For this matrix, we denote its n eigenvalues with 0 = \u03bb1 6 \u00b7 \u00b7 \u00b7 6 \u03bbn 6 2, with their corresponding orthonormal eigenvectors f1, . . . , fn. Note that if G is connected, the first eigenvector is f1 = D 1/2f , where f is any non-zero constant vector.\nFor a vector x \u2208 Rn, the Euclidean norm of x is given by \u2016x\u2016 = (\u2211n\ni=1 x 2 i\n)1/2 . For any\nf : V \u2192 R and h , D\u22121/2f , the Rayleigh quotient of f with respect to graph G is given by\nR(f) , f \u22baLf \u2016f\u20162 = h\u22baLh \u2016h\u2016D =\n\u2211 {u,v}\u2208E(G) (h(u)\u2212 h(v))2\u2211\nu duh(u) 2\n,\nwhere \u2016h\u2016D , h\u22baDh. Based on the Rayleigh quotient, the conductance of a set Si can be expressed as \u03c6G(Si) = R(g\u0304i), and the gap \u03a5(k) can be written as\n\u03a5(k) = \u03bbk+1 \u03c1(k) = min 16i6k \u03bbk+1 \u03c6G(Si) = min 16i6k \u03bbk+1 R(g\u0304i) . (2.1)\nSince k is always fixed as part of the algorithm\u2019s input, throughout the rest of the paper we always use \u03a5 to express \u03a5(k) for simplicity. We will also use S1, . . . , Sk to express a k-way partition of G achieving \u03c1(k). Note that this partition may not be unique."}, {"heading": "3 Connection Between Eigenvectors and Indicator Vectors of", "text": "Clusters\nIn this section we study the relations between the multiple cuts of a graph and the eigenvectors of the graph\u2019s normalized Laplacian matrix. Given clusters S1 . . . Sk, define the indicator vector of cluster Si by\ngi(u) = { 1 if u \u2208 Si, 0 if u 6\u2208 Si,\n(3.1)\nand define the corresponding normalized indicator vector by\ngi = D1/2gi\n\u2016D1/2gi\u2016 . (3.2)\nA basic result in spectral graph theory states that G has k connected components if and only if the k smallest eigenvalues are 0, implying that the spaces spanned by f1, \u00b7 \u00b7 \u00b7 , fk and g\u03041, \u00b7 \u00b7 \u00b7 , g\u0304k are the same. Generalizing this result, we expect that these two spaces would be still similar if these k components of G are loosely connected, in the sense that (i) every eigenvector fi can be approximately expressed by a linear combination of {gi}ki=1, and (ii) every indicator vector g\u0304i can be approximately expressed by a linear combination of {fi}ki=1. This leads to our structure theorem, which is illustrated in fig. 1.\nTheorem 3.1 (The Structure Theorem, Formal Statement). Let \u03a5 = \u2126(k2), and 1 6 i 6 k. Then, the following statements hold:\n1. There is a linear combination of the eigenvectors f1, . . . , fk with coefficients \u03b1 (i) j : f\u0302i =\n\u03b1 (i) 1 f1 + \u00b7 \u00b7 \u00b7+ \u03b1 (i) k fk, such that \u2225\u2225\u2225gi \u2212 f\u0302i \u2225\u2225\u2225 2 6 1/\u03a5.\n2. There is a linear combination of the vectors g\u03041, . . . , g\u0304k with coefficients \u03b2 (i) j : g\u0302i = \u03b2 (i) 1 g\u03041 +\n\u00b7 \u00b7 \u00b7 + \u03b2(i)k g\u0304k, such that \u2016fi \u2212 g\u0302i\u2016 2 6 1.1k/\u03a5.\nPart 1 of Theorem 3.1 shows that the normalized indicator vectors g\u0304i of every cluster Si can be approximated by a linear combination of the first k eigenvectors, with respect to the value of \u03a5. The proof follows from the fact that if g\u0304i has small Rayleigh quotient, then the inner product between g\u0304i and the eigenvectors corresponding to larger eigenvalues must be small. This statement was also shown implicitly in Theorem 2.2 of [2].\nProof of Part 1 of Theorem 3.1. We write gi as a linear combination of the eigenvectors of L, i.e.\ngi = \u03b1 (i) 1 f1 + \u00b7 \u00b7 \u00b7+ \u03b1(i)n fn\nand let the vector f\u0302i be the projection of vector g\u0304i on the subspace spanned by {fi}ki=1, i.e.\nf\u0302i = \u03b1 (i) 1 f1 + \u00b7 \u00b7 \u00b7+ \u03b1 (i) k fk.\nBy the definition of Rayleigh quotients, we have that\nR(gi) = ( \u03b1 (i) 1 f1 + \u00b7 \u00b7 \u00b7+ \u03b1(i)n fn )\u22ba L ( \u03b1 (i) 1 f1 + \u00b7 \u00b7 \u00b7 + \u03b1(i)n fn )\n= ( \u03b1 (i) 1 )2 \u03bb1 + \u00b7 \u00b7 \u00b7 + ( \u03b1(i)n )2 \u03bbn > ( \u03b1 (i) 2 )2 \u03bb2 + \u00b7 \u00b7 \u00b7 + ( \u03b1 (i) k )2 \u03bbk + ( 1\u2212 \u03b1\u2032 \u2212 ( \u03b1 (i) 1 )2) \u03bbk+1,\nwhere \u03b1\u2032 , ( \u03b1 (i) 2 )2 + \u00b7 \u00b7 \u00b7+ ( \u03b1 (i) k )2 . Therefore, we have that\n1\u2212 \u03b1\u2032 \u2212 ( \u03b1 (i) 1 )2 6 R(gi)/\u03bbk+1 6 1/\u03a5,\nand\n\u2016gi \u2212 f\u0302i\u20162 = ( \u03b1 (i) k+1 )2 + \u00b7 \u00b7 \u00b7+ ( \u03b1(i)n )2 = 1\u2212 \u03b1\u2032 \u2212 ( \u03b1 (i) 1 )2 6 1/\u03a5,\nwhich finishes the proof.\nPart 2 of Theorem 3.1 is more interesting, and shows that the opposite direction holds as well, i.e., any fi (1 6 i 6 k) can be approximated by a linear combination of the normalized indicator vectors {gi}ki=1. To sketch the proof, note that if we could write every gi exactly as a linear combination of {fi}ki=1, then we could write every fi (1 6 i 6 k) as a linear combination of {gi}ki=1. This is because both of {fi}ki=1 and {gi}ki=1 are sets of linearly independent vectors of the same dimension and span {g1, . . . , gk} \u2286 span {f1, . . . , fk}. However, the gi\u2019s are only close to a linear combination of the first k eigenvectors, as shown in Part 1. We will denote this combination as f\u0302i, and use the fact that the errors of approximation are small to show that these {f\u0302i}ki=1 are almost orthogonal between each other. This allows us to show that span {f\u03021, . . . , f\u0302k} = span {f1, . . . , fk}, which implies Part 2.\nWe will use the following two classical results in our proof.\nTheorem 3.2 (Gers\u030cgorin Circle Theorem). Let A be an n \u00d7 n matrix , and let Ri(A) =\u2211 j 6=i |Ai,j|, for 1 6 i 6 n. Then, all eigenvalues of A are in the union of Gers\u030cgorin Discs defined by n\u22c3\ni=1\n{z \u2208 C : |z \u2212Ai,i| 6 Ri(A)} .\nTheorem 3.3 (Corollary 6.3.4, [15]). Let A be an n \u00d7 n real and symmetric matrix with eigenvalues \u03bb1, . . . , \u03bbn, and E be an n\u00d7n matrix. If \u03bb\u0302 is an eigenvalue of A+E, then there is some eigenvalue \u03bbi of A for which |\u03bb\u0302\u2212 \u03bbi| 6 \u2016E\u2016. Proof of Part 2 of Theorem 3.1. By Part 1, every gi is approximated by a vector f\u0302i defined by\nf\u0302i = \u03b1 (i) 1 f1 + \u00b7 \u00b7 \u00b7\u03b1 (i) k fk.\nDefine a k by k matrix A such that Ai,j = \u03b1 (j) i , i.e., the jth column of matrix A consists of\nvalues { \u03b1 (j) i }k i=1 representing f\u0302j. We express the jth column of A by a vector \u03b1 (j), defined as\n\u03b1(j) = ( \u03b1 (j) 1 , \u00b7 \u00b7 \u00b7 , \u03b1 (j) k )\u22ba .\nWe will show that the vectors { \u03b1(j) }k j=1\nare linearly independent, which implies that {f\u0302 (j)}kj=1 are linearly independent as well. To prove this, we will show that A\u22baA has no zero eigenvalue, and hence A is invertible.\nFirst of all, notice that it holds by the orthonormality of {fi}ki=1 that \u2223\u2223\u2223 \u2329 \u03b1(i), \u03b1(j) \u232a\u2223\u2223\u2223 = \u2223\u2223\u2223 \u2329 f\u0302i, f\u0302j \u232a\u2223\u2223\u2223 = \u2223\u2223\u2223 \u2329 g\u0304i \u2212 (g\u0304i \u2212 f\u0302i), g\u0304j \u2212 (g\u0304j \u2212 f\u0302j) \u232a\u2223\u2223\u2223\n= \u2223\u2223\u2223\u3008g\u0304i, g\u0304j\u3009 \u2212 \u2329 g\u0304i \u2212 f\u0302i, g\u0304j \u232a \u2212 \u2329 g\u0304i, g\u0304j \u2212 f\u0302j \u232a + \u2329 g\u0304i \u2212 f\u0302i, g\u0304j \u2212 f\u0302j \u232a\u2223\u2223\u2223 6 \u2225\u2225\u2225g\u0304i \u2212 f\u0302i \u2225\u2225\u2225+ \u2225\u2225\u2225g\u0304j \u2212 f\u0302j \u2225\u2225\u2225+ \u2225\u2225\u2225g\u0304i \u2212 f\u0302i \u2225\u2225\u2225 \u2225\u2225\u2225g\u0304j \u2212 f\u0302j \u2225\u2225\u2225 6 2 \u221a 1/\u03a5+ 1/\u03a5,\nwhere the first inequality follows from the orthonormality of g\u0304i and g\u0304j , and the second inequality follows by Part 1 of Theorem 3.1. So it holds for any i 6= j that\n|(A\u22baA)i,j| = \u2223\u2223\u2223\u2223\u2223 k\u2211\n\u2113=1\nA\u2113,iA\u2113,j \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 k\u2211\n\u2113=1\n\u03b1 (i) \u2113 \u03b1 (j) \u2113 \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223 \u2329 \u03b1(i), \u03b1(j) \u232a\u2223\u2223\u2223 6 3 \u221a 1/\u03a5\nwhile\n(A\u22baA)i,i = k\u2211\n\u2113=1\n( \u03b1 (i) \u2113 )2 > 1\u2212 1/\u03a5.\nThen, by the Gers\u030cgorin Circle Theorem (cf. Theorem 3.2), it holds that all the eigenvalues of A\u22baA are at least\n1\u2212 1/\u03a5 \u2212 (k \u2212 1) \u00b7 3 \u221a 1/\u03a5.\nTherefore, A has no eigenvalue with value 0 as long as \u03a5 > 10k2, proving that the vectors{ \u03b1(j)\n}k j=1\nare linearly independent. Combining this with the fact that span {f\u03021, . . . , f\u0302k} \u2286 span {f1, . . . , fk} and dim(span ({f1, . . . , fk})) = k, it holds that\nspan {f\u03021, . . . , f\u0302k} = span {f1, . . . , fk}.\nHence, we can write every fi (1 6 i 6 k) as a linear combination of {f\u0302i}ki=1, i.e.,\nfi = \u03b2 (i) 1 f\u03021 + \u03b2 (i) 2 f\u03022 + \u00b7 \u00b7 \u00b7+ \u03b2 (i) k f\u0302k. (3.3)\nNow define the value of g\u0302i as\ng\u0302i = \u03b2 (i) 1 g1 + \u03b2 (i) 2 g2 + \u00b7 \u00b7 \u00b7+ \u03b2 (i) k gk, (3.4)\nand define \u2016\u03b2\u20162 = \u2211kj=1 ( \u03b2 (i) j )2 . Then, it holds that\n1 = \u2016fi\u20162 = k\u2211\n\u2113=1\n( \u03b2 (i) \u2113 )2 \u2225\u2225\u2225f\u0302\u2113 \u2225\u2225\u2225 2 + \u2211\n\u2113 6=\u2113\u2032 \u03b2 (i) \u2113 \u03b2 (i) \u2113\u2032\n\u2329 f\u0302\u2113, f\u0302\u2113\u2032 \u232a\n> \u2016\u03b2\u20162(1\u2212 1/\u03a5)\u2212 \u2211\n\u2113\n\u2223\u2223\u2223\u03b2(i)\u2113 \u2223\u2223\u2223 \u2211\n\u2113\u2032 6=\u2113\n\u2223\u2223\u2223\u03b2(i)\u2113\u2032 \u2223\u2223\u2223 \u2329 f\u0302\u2113, f\u0302\u2113\u2032 \u232a\n> \u2016\u03b2\u20162(1\u2212 1/\u03a5)\u2212 (\u221a k \u00b7 \u2016\u03b2\u2016 ) \u00b7 (\u221a k \u00b7 \u2016\u03b2\u2016 ) \u00b7 ( 3 \u00b7 \u221a 1/\u03a5 ) > ( 1\u2212 1/\u03a5 \u2212 3k/ \u221a \u03a5 ) \u2016\u03b2\u20162,\nwhere the second inequality holds by the Cauchy-Schwarz inequality. Since \u03a5 = \u2126(k2), we have that\n\u2016\u03b2\u20162 6 ( 1\u2212 1\n\u03a5 \u2212 3k\u221a\n\u03a5\n)\u22121 6 1.1.\nCombining this with Part 1 of Theorem 3.1 and the Cauchy-Schwarz inequality, we have that\n\u2016fi \u2212 g\u0302i\u2016 6 k\u2211\nj=1\n\u2223\u2223\u2223\u03b2(i)j \u2223\u2223\u2223 \u2225\u2225\u2225f\u0302j \u2212 gj \u2225\u2225\u2225 6 ( 1/ \u221a \u03a5 ) k\u2211\nj=1\n\u2223\u2223\u2223\u03b2(i)j \u2223\u2223\u2223 6 \u221a 1.1k/\u03a5,\nwhich proves Part 2 of the theorem.\nTheorem 3.1 shows a close connection between the first k eigenvectors and the indicator vectors of the clusters. We leverage this and the fact that the {g\u0302i}\u2019s are almost orthogonal between each other to show that, for any two different clusters Si and Sj, there exists an eigenvector having reasonably different values on the coordinates which correspond to Si and Sj.\nLemma 3.4. Let \u03a5 = \u2126(k3). For any 1 6 i 6 k, let g\u0302i = \u03b2 (i) 1 g1 + \u00b7 \u00b7 \u00b7 + \u03b2 (i) k gk be such that \u2016fi \u2212 g\u0302i\u2016 6 1.1k/\u03a5. Then, for any \u2113 6= j, there exists i \u2208 {1, . . . , k} such that \u2223\u2223\u2223\u03b2(i)\u2113 \u2212 \u03b2 (i) j \u2223\u2223\u2223 > \u03b6 , 1\n10 \u221a k . (3.5)\nProof. Let \u03b2(i) = ( \u03b2 (i) 1 , . . . , \u03b2 (i) k )\u22ba , for 1 6 i 6 k. Since g\u0304i \u22a5 g\u0304j for any i 6= j, we have by the orthonormality of g1, \u00b7 \u00b7 \u00b7 , gk that\n\u3008g\u0302i, g\u0302j\u3009 = \u2329 \u03b2 (i) 1 g1 + \u00b7 \u00b7 \u00b7+ \u03b2 (i) k gk, \u03b2 (j) 1 g1 + \u00b7 \u00b7 \u00b7+ \u03b2 (j) k gk \u232a\n=\nk\u2211\n\u2113=1\n\u03b2 (i) \u2113 \u03b2 (j) \u2113 \u2016g\u2113\u20162 =\n\u2329 \u03b2(i), \u03b2(j) \u232a ,\nand \u2223\u2223\u2223 \u2329 \u03b2(i), \u03b2(j)\n\u232a\u2223\u2223\u2223 = |\u3008g\u0302i, g\u0302j\u3009| = |\u3008fi \u2212 (fi \u2212 g\u0302i), fj \u2212 (fj \u2212 g\u0302j)\u3009| = |\u3008fi, fj\u3009 \u2212 \u3008fi \u2212 g\u0302i, fj\u3009 \u2212 \u3008fj \u2212 g\u0302j , fi\u3009+ \u3008fi \u2212 g\u0302i, fj \u2212 g\u0302j\u3009| 6 \u2016fi \u2212 g\u0302i\u2016+ \u2016fj \u2212 g\u0302j\u2016+ \u2016fi \u2212 g\u0302i\u2016\u2016fj \u2212 g\u0302j\u2016 6 2.2 \u221a k/\u03a5 + 1.1k/\u03a5.\nMoreover, it holds that\n\u2225\u2225\u2225\u03b2(i) \u2225\u2225\u2225 = \u2016g\u0302i\u2016 = \u2016fi + g\u0302i \u2212 fi\u2016 6 1 + \u2016g\u0302i \u2212 fi\u2016 6 1 + \u221a 1.1k/\u03a5,\nand \u2225\u2225\u2225\u03b2(i) \u2225\u2225\u2225 = \u2016g\u0302i\u2016 = \u2016fi + g\u0302i \u2212 fi\u2016 > 1\u2212 \u2016g\u0302i \u2212 fi\u2016 > 1\u2212 \u221a 1.1k/\u03a5,\nwhich implies that\n\u2225\u2225\u2225\u03b2(i) \u2225\u2225\u2225 2 \u2208 ( 1\u2212 (2.2 \u221a k/\u03a5 + 1.1k/\u03a5), 1 + 2.2 \u221a k/\u03a5+ 1.1k/\u03a5 ) . (3.6)\nIn other words, we showed that \u03b2(i)\u2019s are almost orthonormal. Now we construct a k by k matrix B, where the jth column of B is \u03b2(j). By the Gers\u030cgorin Circle Theorem (Theorem 3.2), all eigenvalues \u03bb of B\u22baB satisfies\n|\u03bb\u2212 (B\u22baB)i,i| 6 (k \u2212 1) \u00b7 (2.2 \u221a k/\u03a5+ 1.1k/\u03a5) (3.7)\nfor any i. Combing this with (3.6), we have that the eigenvalues of B\u22baB are close to 1.\nNow we show that \u03b2 (i) \u2113 and \u03b2 (i) j are far from each other by contradiction. Suppose there\nexist \u2113 6= j such that \u03b6 \u2032 , max\n16i6k\n\u2223\u2223\u2223\u03b2(i)\u2113 \u2212 \u03b2 (i) j \u2223\u2223\u2223 < 1\n10 \u221a k .\nThis implies that the jth row and \u2113th row of matrix B are somewhat close to each other. Let us now define matrix E \u2208 Rk\u00d7k, where\nE\u2113,i , \u03b2 (i) j \u2212 \u03b2 (i) \u2113 ,\nand Et,i = 0 for any t 6= \u2113 and 1 6 i 6 k. Moreover, let\nQ = B+E.\nNotice that Q has two identical rows, and rank at most k \u2212 1. Therefore, Q has an eigenvalue with value 0, and the spectral norm \u2016E\u2016 of E, the largest singular value of E, is at most \u221a k\u03b6 \u2032. By definition of matrix Q we have that\nQ\u22baQ = B\u22baB+B\u22baE+E\u22baB+E\u22baE.\nSince B\u22baB is symmetric and 0 is an eigenvalue of Q\u22baQ, by Theorem 3.3 we know that, if \u03bb\u0302 is an eigenvalue of Q\u22baQ, then there is an eigenvalue \u03bb of B\u22baB such that\n|\u03bb\u0302\u2212 \u03bb| 6 \u2016B\u22baE+E\u22baB+E\u22baE\u2016 6 \u2016B\u22baE\u2016+ \u2016E\u22baB\u2016+ \u2016E\u22baE\u2016 6 4 \u221a k\u03b6 \u2032 + k\u03b6 \u20322,\nwhich implies that\n\u03bb\u0302 > \u03bb\u2212 4 \u221a k\u03b6 \u2032 \u2212 k\u03b6 \u20322 > 1\u2212 k(2.2 \u221a k/\u03a5+ 1.1k/\u03a5) \u2212 4 \u221a k\u03b6 \u2032 \u2212 k\u03b6 \u20322,\ndue to (3.6) and (3.7). By setting \u03bb\u0302 = 0, we have that\n1\u2212 k(2.2 \u221a k/\u03a5 + 1.1k/\u03a5) \u2212 4 \u221a k\u03b6 \u2032 \u2212 k\u03b6 \u20322 6 0.\nBy the condition of \u03a5 = \u2126(k3), the inequality above implies that \u03b6 \u2032 > 1 10 \u221a k , which leads to a contradiction.\nWe point out that it was shown in [21] that the first k eigenvectors can be approximated by a (2k + 1)-step function. The quality of the approximation is the same as the one given by our structure theorem. However, a (2k + 1)-step approximation is not enough to show that most vertices belonging to the same cluster are mapped close to each other in the spectral embedding.\nWe further point out that standard matrix perturbation theorems cannot be applied in our setting. For instance, we look at a well-clustered graph G that contains a subset C of a cluster Si such that most neighbors of vertices in C are outside Si. In this case, the adjacency matrix representing crossing edges of G has high spectral norm, and hence standard matrix perturbation arguments could not give us a meaningful result. However, our structure theorem takes the fact that vol(C) has to be small into account, and that is why the structure theorem is needed to analyze the cut structure of a graph."}, {"heading": "4 Analysis of Spectral Clustering", "text": "In this section we analyze an algorithm based on the classical spectral clustering paradigm, and give an approximation guarantee of this method on well-clustered graphs. We will show that any k-means algorithm AlgoMean(X , k) with certain approximation guarantee can be used for the k-way partitioning problem. Furthermore, it suffices to call AlgoMean in a black-box manner with a point set X \u2286 Rk.\nThis section is structured as follows. We first give a quick overview of spectral and k-means clustering in Section 4.1. In Section 4.2, we use the structure theorem to analyze the spectral embedding. Section 4.3 gives a general result about k-means when applied to this embedding, and the proof of Theorem 1.2.\n4.1 k-Means Clustering\nGiven a set of points X \u2286 Rd, a k-means algorithm AlgoMean(X , k) seeks to find a set K of k centers c1, \u00b7 \u00b7 \u00b7 , ck to minimize the sum of the \u211322-distance between x \u2208 X and the center to which it is assigned. Formally, for any partition X1, \u00b7 \u00b7 \u00b7 ,Xk of the set X \u2286 Rd, we define the cost function by\nCOST(X1, . . . ,Xk) , min c1,...,ck\u2208Rd\nk\u2211\ni=1\n\u2211 x\u2208Xi \u2016x\u2212 ci\u20162,\ni.e., the COST function minimizes the total \u211322-distance between the points x\u2019s and their individually closest center ci, where c1, . . . , ck are chosen arbitrarily in R\nd. We further define the optimal clustering cost by\n\u22062k(X ) , min partition X1,...,Xk COST(X1, . . . ,Xk). (4.1)\nSpectral clustering can be described as follows: (i) Compute the bottom k eigenvectors f1, \u00b7 \u00b7 \u00b7 , fk of the normalized Laplacian matrix1 of graph G. (ii) Map every vertex u \u2208 V [G] to a point F (u) \u2208 Rk according to\nF (u) = 1\nNormalizationFactor(u) \u00b7 (f1(u), . . . , fk(u))\u22ba , (4.2)\nwith a proper normalization factor NormalizationFactor(u) \u2208 R for each u \u2208 V . (iii) Let X , {F (u) : u \u2208 V } be the set of the embedded points from vertices in G. Run AlgoMean(X , k), and group the vertices of G into k clusters according to the output of AlgoMean(X , k). This\n1Other graph matrices (e.g. the adjacency matrix, and the Laplacian matrix) are also widely used in practice. Notice that, with proper normalization, the choice of these matrices does not substantially influence the performance of k-means algorithms.\napproach that combines a k-means algorithm with a spectral embedding has been widely used in practice for a long time, although there was a lack of rigorous analysis of its performance prior to our result."}, {"heading": "4.2 Analysis of the Spectral Embedding", "text": "The first step of spectral clustering is to map vertices of a graph into points in Euclidean space, through the spectral embedding (4.2). This subsection analyzes the properties of this embedding. Let us define the normalization factor to be\nNormalizationFactor(u) , \u221a\ndu.\nWe will show that the embedding (4.2) with the normalization factor above has very nice properties: embedded points from the same cluster Si are concentrated around their center ci \u2208 Rk, and embedded points from different clusters of G are far from each other. These properties imply that a simple k-means algorithm is able to produce a good clustering2.\nWe first define k points p(i) \u2208 Rk (1 6 i 6 k), where\np(i) , 1\u221a\nvol (Si)\n( \u03b2 (1) i , . . . , \u03b2 (k) i )\u22ba (4.3)\nand the parameters {\u03b2(j)i }kj=1 are defined in Theorem 3.1. We will show in Theorem 4.1 that all embedded points Xi , {F (u) : u \u2208 Si} (1 6 i 6 k) are concentrated around p(i). Moreover, we bound the total \u211322-distance between points in Xi and p(i), which is proportional to 1/\u03a5: the bigger the value of \u03a5, the higher concentration the points within the same cluster have. Notice that we do not claim that p(i) is the actual center of Xi. However, these approximated points p(i)\u2019s suffice for our analysis.\nLemma 4.1. It holds that\nk\u2211\ni=1\n\u2211 u\u2208Si du \u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225 2 6 1.1k2/\u03a5.\nProof. Since g\u0302j(u) = \u221a\ndu vol(Si) \u03b2 (j) i and p (i) j = 1\u221a vol(Si) \u03b2 (j) i hold for any 1 6 j 6 k and u \u2208 Si by\ndefinition, we have that\nk\u2211\ni=1\n\u2211 u\u2208Si du ( F (u)j \u2212 p(i)j )2 = k\u2211 i=1 \u2211 u\u2208Si du ( 1\u221a du fj(u)\u2212 1\u221a vol(Si) \u03b2 (j) i )2\n=\nk\u2211\ni=1\n\u2211\nu\u2208Si\n( fj(u)\u2212 \u221a du\nvol(Si) \u03b2 (j) i\n)2\n=\nk\u2211\ni=1\n\u2211 u\u2208Si (fj(u)\u2212 g\u0302j(u))2\n= \u2016fj \u2212 g\u0302j\u20162\n6 1.1k/\u03a5,\nwhere the last inequality follows from Theorem 3.1. Summing over all j for 1 6 j 6 k implies that\nk\u2211\ni=1\n\u2211 u\u2208Si du \u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225 2 = k\u2211 i=1 k\u2211 j=1 \u2211 u\u2208Si du ( F (u)j \u2212 p(i)j )2 6 1.1k2/\u03a5.\n2Notice that this embedding is similar with the one used in [22], with the only difference that F (u) is not normalized and so it is not necessarily a unit vector. This difference, though, is crucial for our analysis.\nThe next lemma shows that the \u211322-norm of p (i) is inversely proportional to the volume of Si. This implies that embedded points from a big cluster are close to the origin, while embedded points from a small cluster are far from the origin.\nLemma 4.2. It holds for every 1 6 i 6 k that\n99\n100 vol(Si) 6\n\u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 6\n101\n100 vol(Si) .\nProof. By (4.3), we have that\n\u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 =\n1\nvol(Si)\n\u2225\u2225\u2225 ( \u03b2 (1) i , . . . , \u03b2 (k) i )\u22ba\u2225\u2225\u2225 2 .\nNotice that p(i) is just the ith row of the matrix B defined in the proof of Theorem 3.4, normalized by \u221a vol(Si). Since B and B\n\u22ba share the same singular values (this follows from the SVD decomposition), by (3.7) the eigenvalues of BB\u22ba are close to 1. But since (BB\u22ba)i,i is equal to the \u211322-norm of the ith row of B, we have that\n\u2225\u2225\u2225 ( \u03b2 (1) i , . . . , \u03b2 (k) i )\u22ba\u2225\u2225\u2225 2 \u2208 ( 1\u2212 (2.2 \u221a k/\u03a5 + 1.1k/\u03a5), 1 + 2.2 \u221a k/\u03a5+ 1.1k/\u03a5 ) , (4.4)\nwhich implies the statement.\nWe will further show in Theorem 4.3 that these points p(i)(1 6 i 6 k) exhibit another excellent property: the distance between p(i) and p(j) is inversely proportional to the volume of the smaller cluster between Si and Sj. Therefore, points in Si of smaller vol(Si) are far from points in Sj of bigger vol(Sj). Notice that, if this were not the case, a misclassification of a small fraction of points in Sj could introduce a large error to Si.\nLemma 4.3. For every i 6= j, it holds that \u2225\u2225\u2225p(i) \u2212 p(j) \u2225\u2225\u2225 2 >\n\u03b62\n10min {vol(Si), vol(Sj)} ,\nwhere \u03b6 is defined in (3.5).\nProof. Let Si and Sj be two arbitrary clusters. By Theorem 3.4, there exists 1 6 \u2113 6 k such that \u2223\u2223\u2223\u03b2(\u2113)i \u2212 \u03b2 (\u2113) j \u2223\u2223\u2223 > \u03b6.\nBy the definition of p(i) and p(j) it follows that\n\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 p(i) \u2016p(i)\u2016 \u2212 p(j) \u2016p(j)\u2016 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 >   \u03b2 (\u2113) i\u221a\n\u2211k t=1 ( \u03b2 (t) i\n)2 \u2212 \u03b2 (\u2113) j\u221a\n\u2211k t=1 ( \u03b2 (t) j )2\n  2 .\nBy (4.4), we know that\n\u221a\u221a\u221a\u221a k\u2211\n\u2113=1\n( \u03b2 (\u2113) j )2 = \u2225\u2225\u2225 ( \u03b2 (1) j , . . . , \u03b2 (k) j )\u22ba\u2225\u2225\u2225 \u2208 ( 1\u2212 \u03b6\n10 , 1 +\n\u03b6\n10\n) .\nTherefore, we have that\n\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 p(i) \u2016p(i)\u2016 \u2212 p(j) \u2016p(j)\u2016 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 > 1 2 \u00b7 ( \u03b2 (\u2113) i \u2212 \u03b2 (\u2113) j )2 > 1 2 \u00b7 \u03b62,\nand \u2329 p(i)\n\u2016p(i)\u2016 , p(j) \u2016p(j)\u2016\n\u232a 6 1\u2212 \u03b62/4.\nWithout loss of generality, we assume that \u2225\u2225p(i) \u2225\u22252 > \u2225\u2225p(j) \u2225\u22252. By Theorem 4.2, it holds that \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 > 9\n10 \u00b7 vol(Si) ,\nand \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 > \u2225\u2225\u2225p(j) \u2225\u2225\u2225 2 >\n9\n10 \u00b7 vol(Sj) .\nHence, it holds that \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 >\n9\n10min {vol(Si), vol(Sj)} .\nWe can now finish the proof by considering two cases based on \u2225\u2225p(i) \u2225\u2225. Case 1: Suppose that \u2225\u2225p(i) \u2225\u2225 > 4 \u2225\u2225p(j) \u2225\u2225. We have that\n\u2225\u2225\u2225p(i) \u2212 p(j) \u2225\u2225\u2225 > \u2225\u2225\u2225p(i) \u2225\u2225\u2225\u2212 \u2225\u2225\u2225p(j) \u2225\u2225\u2225 > 3\n4\n\u2225\u2225\u2225p(i) \u2225\u2225\u2225 ,\nwhich implies that\n\u2225\u2225\u2225p(i) \u2212 p(j) \u2225\u2225\u2225 2 > 9\n16\n\u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 >\n1\n2min {vol(Si), vol(Sj)} .\nCase 2: Suppose \u2225\u2225p(j) \u2225\u2225 = \u03b1 \u2225\u2225p(i) \u2225\u2225 for \u03b1 \u2208 (14 , 1]. In this case, we have that\n\u2225\u2225\u2225p(i) \u2212 p(j) \u2225\u2225\u2225 2 = \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 + \u2225\u2225\u2225p(j) \u2225\u2225\u2225 2 \u2212 2\n\u2329 p(i)\n\u2016p(i)\u2016 , p(j) \u2016p(j)\u2016\n\u232a\u2225\u2225\u2225p(i) \u2225\u2225\u2225 \u2225\u2225\u2225p(j) \u2225\u2225\u2225\n> \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 + \u2225\u2225\u2225p(j) \u2225\u2225\u2225 2 \u2212 2(1\u2212 \u03b62/4) \u00b7 \u2225\u2225\u2225p(i) \u2225\u2225\u2225 \u2225\u2225\u2225p(j) \u2225\u2225\u2225 = (1 + \u03b12) \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 \u2212 2(1\u2212 \u03b62/4)\u03b1 \u00b7 \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 = (1 + \u03b12 \u2212 2\u03b1 + \u03b1\u03b62/2) \u00b7 \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2\n> \u03b1\u03b62 2 \u00b7 \u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 > \u03b62 \u00b7 1 10min {vol(Si), vol(Sj)} ,\nand the lemma follows."}, {"heading": "4.3 Approximation Guarantees of Spectral Clustering", "text": "Now we analyze why spectral clustering performs well for solving the k-way partitioning problem. We assume that A1, . . . , Ak is any k-way partition returned by a k-means algorithm with an approximation ratio of APT.\nWe map every vertex u to du identical points in R k. This \u201ctrick\u201d allows us to bound the volume of the overlap between the clusters retrieved by a k-means algorithm and the optimal ones. For this reason we define the cost function of partition A1, . . . , Ak of V [G] by\nCOST(A1, . . . , Ak) , min c1,...,ck\u2208Rk\nk\u2211\ni=1\n\u2211\nu\u2208Ai du\u2016F (u)\u2212 ci\u20162,\nand the optimal clustering cost is defined by\n\u22062k , min partition A1,...,Ak COST(A1, . . . , Ak).\ni.e., we define the optimal clustering cost in the same way as in (4.1), except that we look at the embedded points from vertices of G in the definition. From now on, we always refer COST and \u22062k as the COST and optimal COST values of points {F (u)}u\u2208V , and for technical reasons every point is counted du times. The next lemma gives an upper bound to the cost of the optimal k-means clustering which depends on the gap \u03a5\nLemma 4.4. It holds that \u22062k 6 1.1k 2/\u03a5.\nProof. Since \u22062k is obtained by minimizing over all partitions A1, . . . , Ak and c1, . . . , ck, we have\n\u22062k 6 k\u2211\ni=1\n\u2211 u\u2208Si du \u2225\u2225\u2225F (u) \u2212 p(i) \u2225\u2225\u2225 2 . (4.5)\nHence the statement follows by applying Theorem 4.1.\nSince A1, \u00b7 \u00b7 \u00b7 , Ak is the output of a k-means algorithm with approximation ratio APT, by Theorem 4.4 we have that COST(A1, . . . , Ak) 6 APT \u00b7 1.1k2/\u03a5. We will show that this upper bound of APT \u00b7 1.1k2/\u03a5 suffices to show that this approximate clustering A1, . . . , Ak is close to the \u201cactual\u201d clustering S1, . . . , Sk, in the sense that (i) every Ai has low conductance, and (ii) under a proper permutation \u03c3 : {1, . . . , k} \u2192 {1, . . . , k}, the symmetric difference between Ai and S\u03c3(i) is small. The fact is proven by contradiction: If we could always find a set Ai with high symmetric difference with its correspondence S\u03c3(i), regardless of how we map {Ai} to their corresponding {S\u03c3(i)}, then the COST value will be high, which contradicts to the fact that COST(A1, . . . , Ak) 6 APT \u00b7 1.1k2/\u03a5. The core of of the whole contradiction arguments is the following technical lemma, whose proof will be presented in the next subsection.\nLemma 4.5. Let A1, . . . , Ak be a partition of V . Suppose that, for every permutation of the indices \u03c3 : {1, . . . , k} \u2192 {1, . . . , k}, there exists i such that vol ( Ai\u25b3S\u03c3(i) ) > 2\u03b5 vol ( S\u03c3(i) ) for \u03b5 > 105 \u00b7 k3/\u03a5, then COST(A1, . . . , Ak) > 10\u22124 \u00b7 \u03b5/k.\nProof of Theorem 1.2. Let A1, . . . , Ak be a k-way partition that achieves an approximation ratio of APT, and let\n\u03b5 = 2 \u00b7 105 \u00b7 k3 \u00b7 APT\n\u03a5 .\nWe first show that there exists a permutation \u03c3 of the indices such that\nvol ( Ai\u25b3S\u03c3(i) ) 6 \u03b5 vol(S\u03c3(i)), for any 1 6 i 6 k. (4.6)\nAssume for contradiction that for all permutation \u03c3 there is 1 6 i 6 k such that\nvol(Ai\u25b3S\u03c3(i)) > \u03b5 vol ( S\u03c3(i) ) .\nThis implies by Theorem 4.5 that\nCOST(A1, . . . , Ak) > 10 \u00b7 APT \u00b7 k2/\u03a5,\nwhich contradicts to the fact that A1, . . . , Ak is an APT-approximation to a k-way partition, whose corresponding k-means cost is at most 1.1 \u00b7 APT \u00b7 k2/\u03a5.\nNow we assume that \u03c3 : {1, \u00b7 \u00b7 \u00b7 , k} \u2192 {1, \u00b7 \u00b7 \u00b7 , k} is the permutation satisfying (4.6), and bound the conductance of every cluster Ai. For any 1 6 i 6 k, the number of leaving edges of Ai is upper bounded by\n|\u2202 (Ai)| 6 \u2223\u2223\u2202 ( Ai \\ S\u03c3(i) )\u2223\u2223+ \u2223\u2223\u2202 ( Ai \u2229 S\u03c3(i) )\u2223\u2223\n6 \u2223\u2223\u2202 ( Ai\u25b3S\u03c3(i) )\u2223\u2223+ \u2223\u2223\u2202 ( Ai \u2229 S\u03c3(i) )\u2223\u2223 .\nNotice that \u2223\u2223\u2202 ( Ai\u25b3S\u03c3(i) )\u2223\u2223 6 \u03b5 vol ( S\u03c3(i) ) by our assumption on \u03c3, and every node in \u2223\u2223\u2202 ( Ai \u2229 S\u03c3(i) )\u2223\u2223 either belongs to \u2202S\u03c3(i) \\ S\u03c3(i) or \u2202 ( Ai\u25b3S\u03c3(i) ) , hence\n|\u2202 (Ai)| 6 \u03b5 vol ( S\u03c3(i) ) + \u03c6G ( S\u03c3(i) ) vol ( S\u03c3(i) ) + \u03b5 vol ( S\u03c3(i) )\n= ( 2\u03b5 + \u03c6G ( S\u03c3(i) )) vol(S\u03c3(i)).\nOn the other hand, we have that\nvol (Ai) > vol ( Ai \u2229 S\u03c3(i) ) > (1\u2212 2\u03b5) vol(S\u03c3(i)).\nHence,\n\u03c6G(Ai) 6 (2\u03b5+ \u03c6G(S\u03c3(i))) vol(S\u03c3(i))\n(1\u2212 2\u03b5) vol(S\u03c3(i))\n= 2\u03b5+ \u03c6G(S\u03c3(i)) 1\u2212 2\u03b5 6 1.1 \u00b7 \u03c6G(S\u03c3(i)) +O(APT \u00b7 k3/\u03a5)."}, {"heading": "4.4 Proof of Theorem 4.5", "text": "It remains to show Theorem 4.5. Our proof is based on the following high-level idea: suppose by contradiction that there is a cluster Sj which is very different from every cluster A\u2113. Then there is a cluster Ai with significant overlap with two different clusters Sj and Sj\u2032 (Theorem 4.6). However, we already proved in Theorem 4.3 that any two clusters are far from each other. This implies that the COST value of A1, . . . , Ak is high, which leads to a contradiction.\nLemma 4.6. Suppose for every permutation \u03c0 : {1, . . . , k} \u2192 {1, . . . , k} there exists an index i such that vol ( Ai\u25b3S\u03c0(i) ) > 2\u03b5 vol ( S\u03c0(i) ) . Then, at least one of the following two cases holds:\n1. for any index i there are indices i1 6= i2 and \u03b5i > 0 such that\nvol(Ai \u2229 Si1) > vol(Ai \u2229 Si2) > \u03b5i min {vol(Si1), vol(Si2)},\nand \u2211k\ni=1 \u03b5i > \u03b5;\n2. there exist indices i, \u2113 and \u03b5j > 0 such that, for j 6= \u2113,\nvol(Ai \u2229 S\u2113) > \u03b5i vol(S\u2113), vol(Ai \u2229 Sj) > \u03b5i vol(S\u2113)\nand \u2211k\ni=1 \u03b5i > \u03b5.\nProof. Let \u03c3 : {1, . . . , k} \u2192 {1, . . . , k} be the function defined by\n\u03c3(i) = argmax 16j6k vol(Ai \u2229 Sj) vol(Sj) .\nWe first assume that \u03c3 is one-to-one, i.e. \u03c3 is a permutation. By the hypothesis of the lemma, there exists an index i such that vol(Ai\u25b3S\u03c3(i)) > 2\u03b5 vol(S\u03c3(i)). Without loss of generality, we assume that i = 1 and \u03c3(j) = j for j = 1, . . . , k. Notice that\nvol (A1\u25b3S1) = \u2211\nj 6=1 vol (Aj \u2229 S1) +\n\u2211 j 6=1 vol (A1 \u2229 Sj) . (4.7)\nHence, one of the summations on the right hand side of (4.7) is at least \u03b5 vol (S1). Now the proof is based on the case distinction.\nCase 1: Assume that \u2211\nj 6=1 vol (Aj \u2229 S1) > \u03b5 vol(S1). We define \u03c4j for 2 6 j 6 k to be\n\u03c4j = vol (Aj \u2229 S1)\nvol (S1) .\nWe have that \u2211\nj 6=1 \u03c4j > \u03b5,\nand by the definition of \u03c3 it holds that\nvol (Aj \u2229 Sj) vol (Sj) > vol (Aj \u2229 S1) vol (S1) = \u03c4j\nfor 2 6 j 6 k. Setting \u03b5j = \u03c4j for 2 6 j 6 k and \u03b51 = 0 finishes the proof of Case 1. Case 2: Assume that \u2211\nj 6=1 vol (A1 \u2229 Sj) > \u03b5 vol(S1). (4.8)\nLet us define \u03c4 \u2032j for 1 6 j 6 k, j 6= 1, to be\n\u03c4 \u2032j = vol(A1 \u2229 Sj)\nvol (S1) .\nThen, (4.8) implies that \u2211\nj 6=1 \u03c4 \u2032j > \u03b5.\nThe statement in this case holds by assuming vol (A1 \u2229 S1) > \u03b5 vol (S1), since otherwise we have\nvol (S1)\u2212 vol (A1 \u2229 S1) = \u2211\nj 6=1 vol (Aj \u2229 S1) > (1\u2212 \u03b5) vol (S1) > \u03b5 vol (S1) ,\nand this case was proven in Case 1. So it suffices to study the case in which \u03c3 defined earlier is not one-to-one. Then, there is j (1 6 j 6 k) such that j 6\u2208 {\u03c3(1), \u00b7 \u00b7 \u00b7 , \u03c3(k)}. For any 1 6 \u2113 6 k, let\n\u03c4 \u2032\u2032\u2113 = vol(A\u2113 \u2229 Sj)\nvol(Sj) .\nThen, \u2211k\n\u2113=1 \u03c4 \u2032\u2032 \u2113 = 1 > \u03b5 and it holds for any 1 6 \u2113 6 k that\nvol ( A\u2113 \u2229 S\u03c3(\u2113) )\nvol ( S\u03c3(\u2113) ) > vol(A\u2113 \u2229 Sj) vol(Sj) = \u03c4 \u2032\u2032\u2113 .\nProof of Theorem 4.5. We first consider the case when part 1 of Theorem 4.6 holds, i.e., for every i there exist i1 6= i2 such that\nvol(Ai \u2229 Si1) > \u03b5imin {vol(Si1), vol(Si2)}, vol(Ai \u2229 Si2) > \u03b5imin {vol(Si1), vol(Si2)},\n(4.9)\nfor some \u03b5 > 0, and \u2211k\ni=1 \u03b5i > \u03b5. Let ci be the center of Ai. Let us assume without loss of generality that \u2016ci \u2212 p(i1)\u2016 > \u2016ci\u2212p(i2)\u2016, which implies \u2016p(i1)\u2212ci\u2016 > \u2016p(i1)\u2212p(i2)\u2016/2. However, points in Bi = Ai\u2229Si1 are far away from ci, see fig. 2. We lower bound the value of COST(A1, . . . , Ak) by only looking at the\ncontribution of points in the Bis . Notice that by Theorem 4.1 the sum of the squared-distances between points in Bi and p\n(i1) is at most k2/\u03a5, while the distance between p(i1) and p(i2) is large (Theorem 4.3). Therefore, we have that\nCOST(A1, . . . , Ak) = k\u2211\ni=1\n\u2211\nu\u2208Ai du\u2016F (u)\u2212 ci\u20162 >\nk\u2211\ni=1\n\u2211\nu\u2208Bi du\u2016F (u) \u2212 ci\u20162\nBy applying the inequality a2 + b2 > (a\u2212 b)2/2, we have that\nCOST(A1, . . . , Ak) > k\u2211\ni=1\n\u2211\nu\u2208Bi du\n(\u2225\u2225p(i1) \u2212 ci \u2225\u22252\n2 \u2212\n\u2225\u2225\u2225F (u)\u2212 p(i1) \u2225\u2225\u2225 2 )\n> k\u2211\ni=1\n\u2211\nu\u2208Bi du\n\u2225\u2225p(i1) \u2212 ci \u2225\u22252\n2 \u2212\nk\u2211\ni=1\n\u2211\nu\u2208Bi du\n\u2225\u2225\u2225F (u)\u2212 p(i1) \u2225\u2225\u2225 2\n> k\u2211\ni=1\n\u2211\nu\u2208Bi du\n\u2225\u2225p(i1) \u2212 ci \u2225\u22252\n2 \u2212 1.1k\n2\n\u03a5 (4.10)\n> k\u2211\ni=1\n\u2211\nu\u2208Bi du\n\u2225\u2225p(i1) \u2212 p(i2) \u2225\u22252\n8 \u2212 1.1k\n2\n\u03a5\n> k\u2211\ni=1\n\u03b62 vol(Bi)\n80min {vol(Si1), vol(Si2)} \u2212 1.1k\n2\n\u03a5 (4.11)\n> k\u2211\ni=1\n\u03b62\u03b5i min {vol(Si1), vol(Si2)} 80min {vol(Si1), vol(Si2)} \u2212 1.1k 2 \u03a5\n> k\u2211\ni=1\n\u03b62\u03b5i 80 \u2212 1.1k 2 \u03a5\n> \u03b62\u03b5 80 \u2212 1.1k 2 \u03a5 > \u03b62\u03b5 100\nwhere (4.10) follows from Theorem 4.1, (4.11) follows from Theorem 4.3 and the last inequality follows from the assumption that \u03b5 > 105 \u00b7 k3/\u03a5.\nNow, suppose that part 2 of Theorem 4.6 holds, i.e. there are indices i, \u2113 such that, for any j 6= \u2113, it holds that\nvol(Ai \u2229 S\u2113) > \u03b5i vol(S\u2113), vol(Ai \u2229 Sj) > \u03b5i vol(S\u2113)\nfor some \u03b5 > 0, and \u2211k\ni=1 \u03b5i > \u03b5. In this case, we only need to repeat the proof by setting, for any j 6= i, Bj = Ai \u2229 Sj , Sj1 = S\u2113, and Sj2 = Sj."}, {"heading": "5 Partitioning Well-Clustered Graphs in Nearly-Linear Time", "text": "In this section we present a nearly-linear time algorithm for partitioning well-clustered graphs, and prove Theorem 1.3. At a high level, our algorithm follows the general framework of k-means algorithms, and consists of two steps: the seeding step, and the grouping step. The seeding step chooses k candidate centers such that each one is close to the actual center of a different\ncluster. The grouping step assigns the remaining vertices to their individual closest candidate centers.\nAll the proofs for the seeding and grouping steps assume that we have an embedding {x(u)}u\u2208V [G] satisfying the following two conditions:\n( 1\u2212 1\n10 log n\n) \u00b7 \u2016F (u)\u20162 6\u2016x(u)\u20162 6 \u2016F (u)\u20162 + 1\nn5 , (5.1)\n( 1\u2212 1\n10 log n\n) \u00b7 \u2016F (u) \u2212 F (v)\u20162 6\u2016x(u)\u2212 x(v)\u20162 6 \u2016F (u) \u2212 F (v)\u20162 + 1\nn5 (5.2)\nNotice that these two conditions hold trivially if {x(u)}u\u2208V [G] is the spectral embedding {F (u)}u\u2208V [G], or any embedding produced by good approximations of the first k eigenvectors. However, obtaining such embedding becomes non-trivial for a large value of k, as directly computing the first k eigenvectors takes super-linear time. We will present a nearly-linear time algorithm that computes an embedding satisfying (5.1) and (5.2). By using standard dimensionality reduction techniques that approximately preserve pairwise distances, such as the Johnson-Lindenstrauss transform (see e.g. [11]), we can also always assume that the dimension of the embedding {x(u)}u\u2208V [G] is d = O(log3 n). Throughout the whole section, we assume k = \u03c9(poly log n) and \u03a5 = \u2126\u0303(k5).\nThis section is organized as follows: Section 5.1 and Section 5.2 discuss the seeding and grouping steps, assuming that we have an embedding {x(u)}u\u2208V [G] that satisfies (5.1) and (5.2), and Section 5.3 analyzes the approximation guarantee of the partition returned by the grouping step. In Section 5.4, we present an algorithm that computes all required quantities in nearly-linear time, assuming that we know the value of \u03bbk. This assumption on \u03bbk will be finally removed in Section 5.5, and this leads to our final algorithm which corresponds to Theorem 1.3."}, {"heading": "5.1 The Seeding Step", "text": "We proved in Section 4.2 that the approximate center p(i) for every 1 6 i 6 k satisfies\n99\n100 vol(Si) 6\n\u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 6\n101\n100 vol(Si) ,\nand most embedded points F (u) are close to their approximate centers. Together with (5.1) and (5.2), these two properties imply that, when sampling points x(u) with probability proportional to du \u00b7 \u2016x(u)\u20162, vertices from different clusters will be approximately sampled with the same\nprobability. We will prove that, when sampling \u0398(k log k) points in this way, with constant probability there is at least one point sampled from each cluster.\nIn the next step remove the sampled points which are close to each other, and call this resulting set C\u22c6. We prove that with constant probability there is exactly one point in C\u22c6 from a cluster. Algorithm 1 below gives a formal description of the seeding step.\nAlgorithm 1 SeedAndTrim(k, {x(u)}u\u2208V [G]) 1: input: the number of clusters k, and the embedding {x(u)}u\u2208V [G]. 2: Let K = \u0398(k log k). 3: for i = 1, . . . ,K do 4: Set ci = u with probability proportional to du\u2016x(u)\u20162. 5: end for\n6: for i = 2, . . . ,K do 7: Delete all cj with j < i such that \u2016x(ci)\u2212 x(cj)\u20162 < \u2016x(ci)\u2016 2\n2\u00b7104k . 8: end for 9: return the remaining sampled vertices.\nNow we analyze Algorithm 1. For any 1 6 i 6 k, we define Ei to be the sum of the \u211322-distance between the embedded points from Si and p (i), i.e.,\nEi , \u2211\nu\u2208Si du\n\u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225 2 .\nFor any parameter \u03c1 > 0, we define the radius of Si with respect to \u03c1 to be\nR\u03c1i , \u03c1 \u00b7 Ei vol(Si) ,\nand define CORE\u03c1i \u2286 Si to be the set of vertices whose \u211322-distance to p(i) is at most R \u03c1 i , i.e.,\nCORE \u03c1 i , { u \u2208 Si : \u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225 2 6 R\u03c1i } . (5.3)\nBy the averaging argument it holds that\nvol(Si \\ CORE\u03c1i ) 6 \u2211 u\u2208Si du \u2225\u2225F (u)\u2212 p(i) \u2225\u22252\nR\u03c1i =\nvol(Si)\n\u03c1 ,\nand\nvol(CORE\u03c1i ) > max\n{( 1\u2212 1\n\u03c1\n) vol(Si), 0 } . (5.4)\nFrom now on, we set the parameter\n\u03b1 , \u0398(K logK).\nWe first show that most embedded points of the vertices in Si are contained in the cores CORE \u03b1 i , for 1 6 i 6 k.\nLemma 5.1. The following statements hold:\n1. \u2211\nu\u2208CORE\u03b1i du \u00b7 \u2016F (u)\u2016 2 > 1\u2212 1100K .\n2. \u2211k\ni=1 \u2211 u/\u2208CORE\u03b1i du \u00b7 \u2016F (u)\u2016 2 6 k100K .\nProof. By the definition of CORE\u03b1i , it holds that\n\u2211\nu\u2208CORE\u03b1i\ndu \u00b7 \u2016F (u)\u20162\n> 1\n\u03b1\n\u222b \u03b1\n0\n\u2211\nu\u2208CORE\u03c1i\ndu \u00b7 \u2016F (u)\u20162d\u03c1\n> 1\n\u03b1\n\u222b \u03b1\n0\n(\u2225\u2225\u2225p(i) \u2225\u2225\u2225\u2212 \u221a R\u03c1i )2 vol(CORE\u03c1i )d\u03c1 (5.5)\n> 1\n\u03b1\n\u222b \u03b1\n0\n(\u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 \u2212 2 \u221a R\u03c1i \u00b7 \u2225\u2225\u2225p(i) \u2225\u2225\u2225 ) max {( 1\u2212 1\n\u03c1\n) vol(Si), 0 } d\u03c1 (5.6)\n> 1\n\u03b1\n\u222b \u03b1\n0 max\n{( 1\u2212 (2.2 \u221a k/\u03a5+ 1.1k/\u03a5) \u2212 3 \u221a Ei\u03c1 )( 1\u2212 1\n\u03c1\n) , 0 } d\u03c1 (5.7)\nwhere (5.5) follows from the fact that for all u \u2208 CORE\u03c1i , \u2016F (u)\u2016 > \u2016p(i)\u2016 \u2212 \u221a\nR\u03c1i , (5.6) from (5.4), and (5.7) from the definition of R\u03c1i and the fact that\n\u2225\u2225\u2225p(i) \u2225\u2225\u2225 2 \u00b7 vol(Si) \u2208 ( 1\u2212 (2.2 \u221a k/\u03a5 + 1.1k/\u03a5), 1 + 2.2 \u221a k/\u03a5+ 1.1k/\u03a5 ) .\nSince Ei 6 1.1k2/\u03a5 by Theorem 4.1, it holds that \u2211\nu\u2208CORE\u03b1i\ndu \u00b7 \u2016F (u)\u20162 > 1\n\u03b1\n\u222b \u03b1\n0 max\n{( 1\u2212 (2.2 \u221a k/\u03a5 + 1.1k/\u03a5) \u2212 4 \u221a k2\u03c1/\u03a5 )( 1\u2212 1\n\u03c1\n) , 0 } d\u03c1\n> 1\n\u03b1\n\u222b \u03b1\n0 max\n{ 1\u2212 (2.2 \u221a k/\u03a5+ 1.1k/\u03a5) \u2212 4 \u221a k2\u03c1/\u03a5 \u2212 1\n\u03c1 , 0\n} d\u03c1\n> 1\u2212 (2.2 \u221a k/\u03a5 + 1.1k/\u03a5) \u2212 4k \u221a\n\u03b1/\u03a5 \u2212 ln\u03b1 \u03b1\n> 1\u2212 1 100K ,\nwhere the last inequality holds by the assumption on \u03b1 and \u03a5. The second statement follows by the fact that\nk\u2211\ni=1\n\u2211\nu\u2208CORE\u03b1i\ndu \u00b7 \u2016F (u)\u20162 > k ( 1\u2212 1\n100K\n)\nand \u2211\nu\u2208V [G] du \u00b7 \u2016F (u)\u20162 = k.\nThe next lemma shows that the embedded points from the same core are close to each other, while the embedded points from different cores are far from each other.\nLemma 5.2. The following statements hold:\n1. For any 1 6 i 6 k and any two vertices u, v \u2208 CORE\u03b1i , it holds that\n\u2016x(u)\u2212 x(v)\u20162 6 min { 11\u03b1k2\n\u03a5vol(Si) , \u2016x(u)\u20162 2 \u00b7 104 \u00b7 k\n} .\n2. For any i 6= j, and u \u2208 CORE\u03b1i , v \u2208 CORE\u03b1j , it holds that\n\u2016x(u)\u2212 x(v)\u20162 > 1 7000k vol(Si) > \u2016x(u)\u20162 104k .\nProof. By the definition of CORE\u03b1i , it holds for any u \u2208 CORE\u03b1i that \u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225 6 \u221a\nR\u03b1i .\nBy the triangle inequality, it holds for any u \u2208 CORE\u03b1i and v \u2208 CORE\u03b1i that \u2016F (u) \u2212 F (v)\u2016 6 2 \u221a\nR\u03b1i , and\n\u2016F (u)\u2212 F (v)\u20162 6 4R\u03b1i = 4\u03b1Ei vol(Si) 6 5\u03b1k2 \u03a5vol(Si) ,\nwhere the last inequality follows from Theorem 4.1. Hence, by (5.2) it holds that\n\u2016x(u)\u2212 x(v)\u20162 6 \u2016F (u) \u2212 F (v)\u20162 + 1 n5 6 5\u03b1k2 \u03a5vol(Si) + 1 n5 6 11\u03b1k2 \u03a5vol(Si) ,\nwhere we use the fact that 1 n5 \u226a 1vol(Si) . On the other hand, we have that\n\u2016F (u)\u20162 > (\u2225\u2225\u2225p(i) \u2225\u2225\u2225\u2212 \u221a\nR\u03b1i\n)2 >\n9\n10 vol(Si) ,\nwhere the last inequality follow from Theorem 4.2 and the definition of R\u03b1i . By (5.1) and the conditions on \u03b1, \u03a5, it also holds\n\u2016x(u)\u2212 x(v)\u20162 6 5\u03b1k 2\n\u03a5vol(Si) +\n1\nn5 6\n10\u03b1k2 \u03a5 \u2016F (u)\u20162 6 \u2016x(u)\u2016 2 2 \u00b7 104 \u00b7 k .\nWith these we proved the first statement. Now for the second statement. By the triangle inequality, it holds for any pair of u \u2208 CORE\u03b1i and v \u2208 CORE\u03b1j that\n\u2016F (u) \u2212 F (v)\u2016 > \u2225\u2225\u2225p(i) \u2212 p(j) \u2225\u2225\u2225\u2212 \u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225\u2212 \u2225\u2225\u2225F (v)\u2212 p(j) \u2225\u2225\u2225 .\nBy Theorem 4.3, we have for any i 6= j that \u2225\u2225\u2225p(i) \u2212 p(j) \u2225\u2225\u2225 2 >\n1\n103kmin {vol(Si), vol(Sj)} .\nCombining this with the fact that\n\u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225 6 \u221a R\u03b1i 6\n\u221a 1.1\u03b1k2\n\u03a5vol(Si) ,\nwe obtain that\n\u2016F (u) \u2212 F (v)\u2016 > \u2225\u2225\u2225p(i) \u2212 p(j) \u2225\u2225\u2225\u2212 \u2225\u2225\u2225F (u)\u2212 p(i) \u2225\u2225\u2225\u2212 \u2225\u2225\u2225F (v)\u2212 p(j) \u2225\u2225\u2225\n>\n\u221a 1\n103kmin {vol(Si), vol(Sj)} \u2212\n\u221a 1.1\u03b1k2\n\u03a5vol(Si) \u2212\n\u221a 1.1\u03b1k2\n\u03a5vol(Sj)\n>\n\u221a 1\n1.1 \u00b7 103kmin {vol(Si), vol(Sj)} .\nNotice that\n\u2016x(u)\u20162 6 \u2016F (u)\u20162 + 1 n5\n6 (\u2225\u2225\u2225p(i) \u2225\u2225\u2225+ \u221a\nR\u03b1i\n)2 + 1\nn5 6\n11\n10 vol(Si) +\n1\nn5 6\n11\n9 vol(Si) ,\ntherefore we have\n\u2016x(u)\u2212 x(v)\u20162 > ( 1\u2212 1\n10 log n\n) \u2016F (u)\u2212 F (v)\u20162 > 1\n7000k vol(Si) > \u2016x(u)\u20162 104k .\nWe next show that, after sampling \u0398(k log k) vertices, with constant probability the sampled vertices are in the cores \u22c3k i=1 CORE \u03b1 i , and every core contains at least one sampled vertex.\nLemma 5.3. Assume that K = \u2126(k log k) vertices are sampled, in which each vertex is sampled with probability proportional to du \u00b7 \u2016x(u)\u20162. Then, with constant probability the set C = {c1 . . . cK} of sampled vertices satisfies the following properties:\n1. Set C only contains vertices from the cores, i.e. C \u2286 \u22c3ki=1 CORE\u03b1i ;\n2. Set C contains at least one vertex from each cluster, i.e. C \u2229 Si 6= \u2205 for any 1 6 i 6 k.\nProof. By (5.1), it holds for every vertex u that\n( 1\u2212 1\n10 log n\n) \u00b7 \u2016F (u)\u20162 6 \u2016x(u)\u20162 6 \u2016F (u)\u20162 + 1\nn5 .\nSince \u2211\nu\u2208V [G] du\u2016F (u)\u20162 = k, it holds that\n\u2211\nu\u2208V [G] du\u2016x(u)\u20162 6\n\u2211\nu\u2208V [G] du \u00b7\n( \u2016F (u)\u20162 + 1\nn5\n) 6 k + 1,\nand \u2211\nu\u2208V [G] du\u2016x(u)\u20162 >\n\u2211\nu\u2208V [G] du \u00b7\n( 1\u2212 1\n10 log n\n) \u00b7 \u2016F (u)\u20162 > ( 1\u2212 1\n10 log n\n) \u00b7 k,\ni.e., the total probability mass that we use to sample vertices, i.e. \u2211\nu\u2208V [G] du\u2016x(u)\u20162, is between( 1\u2212 110 logn ) \u00b7 k and k + 1.\nWe first bound the probability that we sample at least one vertex from every core. For any fixed 1 6 i 6 k, the probability that a vertex from CORE\u03b1i gets sampled is at least\n\u2211 u\u2208CORE\u03b1i du \u00b7 \u2016x(u)\u2016 2\nk + 1 >\n\u2211 u\u2208CORE\u03b1i du \u00b7 \u2016F (u)\u2016 2\n3(k + 1) > 1\u2212 1100K 3 \u00b7 (k + 1) > 1 10k .\nTherefore, the probability that we never encounter a vertex from CORE\u03b1i after sampling K\nvertices is at most ( 1\u2212 110k )K 6 110k . Also, the probability that a sampled vertex is outside the cores of the clusters is at most\n\u2211k i=1 \u2211 u\u2208Si\\CORE\u03b1i du \u00b7 \u2016x(u)\u2016 2\n( 1\u2212 110 logn ) \u00b7 k\n6\n\u2211k i=1 \u2211 u\u2208Si\\CORE\u03b1i du \u00b7 ( \u2016F (u)\u20162 + n\u22125 )\nk/2\n6 k 100K + n \u22123\nk/2 6\n2\n100K +\n1\nn2 .\nTaking a union bound over all these events gives that the total probability of undesired events is at most\nk \u00b7 1 10k\n+K \u00b7 ( 1\nn2 +\n2\n100K\n) 6 1\n3 .\nBased on Theorem 5.2 and Theorem 5.3, we can simply delete one of the two vertices ci and cj whose distance is less than 10\n\u22124 \u00b7\u2016x(ci)\u20162/(2k). The following lemma presents the correctness and runtime of the procedure SeedAndTrim, i.e., Algorithm 1.\nLemma 5.4. Given the embedding {x(u)}u\u2208V [G] of dimension d = O(log3 n) that satisfies (5.1) and (5.2), with constant probability the procedure SeedAndTrim returns a set C\u22c6 of centers c1 . . . ck in O\u0303(n+ k 2) time, such that each CORE\u03b1i contains exactly one vertex in C \u22c6.\nProof. Since the sampled set C contains at least one vertex from each core CORE\u03b1i with constant probability, and only vertices from different cores will remain in C\u22c6 by Theorem 5.2 and the algorithm description, the SeedAndTrim procedure returns k centers with constant probability.\nNow we analyze the runtime. The procedure takes O\u0303(n) time to compute the norms of {x(u)}u\u2208V [G], since the embedding has dimension O(log3 n) by assumption. It takes O\u0303(k) time to sample O\u0303(k) vertices, and trimming the sampling vertices takes O\u0303(k2) time. Hence, the total runtime is O\u0303(n + k2) .\nAs the end of this subsection, we would like to mention that choosing good candidate centers is crucial for most k-means algorithms, and has been studied extensively in the literature (e.g. [6, 31]). Comparing with recent algorithms that obtain good initial centers by iteratively picking points from a non-uniform distribution and take \u2126(nk) time, our seeding step (Algorithm 1) runs in O\u0303(n+ k2) time."}, {"heading": "5.2 The Grouping Step", "text": "After the seeding step, with constant probability we obtain a set of k vertices C\u22c6 = {c1, \u00b7 \u00b7 \u00b7 , ck}, and these k vertices belong to k different clusters. Now we assign each remaining vertex u to a cluster Si if, comparing with all other points x(cj) with cj \u2208 C\u22c6, x(u) is closer to x(ci). A naive implementation of this step requires \u2126\u0303(nk) time. To speed it up, we apply \u03b5-approximate nearest neighbor data structures (\u03b5-NNS) [16], whose formal description is as follows:\nProblem 1 (\u03b5-approximate nearest neighbor problem). Given a set of point P \u2282 Rd and a point q \u2208 Rd, find a point p \u2208 P such that, for all p\u2032 \u2208 P , \u2016p \u2212 q\u2016 6 (1 + \u03b5)\u2016p\u2032 \u2212 q\u2016.\nTheorem 5.5 ([16]). Given a set P of points in Rd, there is an algorithm that solves the \u03b5-approximate nearest neighbor problem with\nO\u0303 ( |P |1+ 11+\u03b5 + d \u00b7 |P | )\npreprocessing time and O\u0303 ( d \u00b7 |P | 11+\u03b5 ) query time.\nNow we set P = {x(c1), . . . , x(ck)}, and apply the above \u03b5-approximate nearest neighbor data structures to assign the remaining vertices to k clusters A1, \u00b7 \u00b7 \u00b7 , Ak. By Theorem 5.5 and setting \u03b5 = log k \u2212 1, this step can be finished with O\u0303(k) preprocessing time and O\u0303(1) query time for each query. Hence, the runtime of the grouping step is O\u0303(n). Notice that, with our choice of \u03b5 = log k\u2212 1 and application of \u03b5-NNS, all the remaining vertices in V \\C\u22c6 might not assign to the cluster Ai with the closest center ci. We will prove in the next subsection that our choice of \u03b5 suffices to obtain a good approximation of the optimal partition. The runtime of the grouping step, and the properties of the returned clusters are summarized in the following lemma:\nLemma 5.6. Given a set of centers C\u22c6 = {c1, . . . , ck}, the grouping step runs in O\u0303(n) time and returns a partition A1, . . . , Ak of vertices, such that for any i \u2208 {1, . . . , k}, and every u \u2208 Ai, it holds for any j 6= i that\n\u2016x(u)\u2212 x(ci)\u2016 6 log k \u00b7 \u2016x(u) \u2212 x(cj)\u2016.\nProof. The statement follows from the definition of \u03b5-NNS with the choice of \u03b5 = log k\u2212 1, and Theorem 5.5."}, {"heading": "5.3 Approximation Analysis of the Algorithm", "text": "Now we study the approximation ratio of the k-way partition computed by the seeding and grouping steps. The next lemma analyzes the symmetric difference between the optimal partition and the output of the algorithm.\nLemma 5.7. Let A1, . . . , Ak be the output of the grouping procedure. Then, under a proper permutation of the indices, with constant probability for any 1 6 i 6 k it holds that (i) vol(Ai\u25b3Si) = O\u0303 ( k3/\u03a5 ) vol(Si), and (ii) \u03c6G(Ai) = 1.1 \u00b7 \u03c6G(Si) + O\u0303 ( k3/\u03a5 ) .\nProof. We assume that c1, . . . , ck \u2208 V are the centers returned by SeedAndTrim, and {x(u)}u\u2208V [G] is the embedding we used in the algorithm. Moreover, {x(u)}u\u2208V [G] satisfies (5.1) and (5.2). We further assume that these sampled c1, . . . , ck \u2286 \u22c3k i=1 CORE \u03b1 i . By Theorem 5.3, this holds with constant probability, and we assume that this event happens in the following analysis. Then, by the second statement of Theorem 5.2 it holds for any i 6= j that\n\u2016x(ci)\u2212 x(cj)\u20162 = \u2126 (\n1\nk \u00b7min{vol(Si), vol(Sj)}\n) . (5.8)\nBy Theorem 5.6, it holds for any 1 6 i 6 k that\nvol(Si \\ Ai) 6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2016x(ci)\u2212 x(v)\u2016 >\n\u2016x(cj)\u2212 x(v)\u2016 log k\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2016x(ci)\u2212 x(v)\u2016 >\n\u2016x(ci)\u2212 x(cj)\u2016 \u2212 \u2016x(ci)\u2212 x(v)\u2016 log k\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : 2\u2016x(ci)\u2212 x(v)\u2016 >\n\u2016x(ci)\u2212 x(cj)\u2016 log k\n})\n= \u2211\ni 6=j vol\n({ v \u2208 Si : \u2016x(ci)\u2212 x(v)\u2016 >\n\u2016x(ci)\u2212 x(cj)\u2016 2 log k\n}) .\nBy (5.2) and the triangle inequality, we have that\n\u2016x(ci)\u2212 x(v)\u2016 6 \u2016F (ci)\u2212 F (v)\u2016 + 1\nn2.5 6\n\u2225\u2225\u2225F (ci)\u2212 p(i) \u2225\u2225\u2225+ \u2225\u2225\u2225p(i) \u2212 F (v) \u2225\u2225\u2225 + 1\nn2.5 ,\nand hence\nvol(Si \\ Ai) 6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2225\u2225\u2225F (ci)\u2212 p(i) \u2225\u2225\u2225+ \u2225\u2225\u2225p(i) \u2212 F (v) \u2225\u2225\u2225+ 1\nn2.5 > \u2016x(ci)\u2212 x(cj)\u2016 2 log k\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2225\u2225\u2225p(i) \u2212 F (v) \u2225\u2225\u2225 >\n\u2016x(ci)\u2212 x(cj)\u2016 2 log k\n\u2212 \u2225\u2225\u2225F (ci)\u2212 p(i) \u2225\u2225\u2225\u2212 1 n2.5\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2225\u2225\u2225p(i) \u2212 F (v) \u2225\u2225\u2225 >\n\u2016x(ci)\u2212 x(cj)\u2016 2 log k \u2212 \u221a R\u03b1i \u2212 1 n2.5\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2225\u2225\u2225p(i) \u2212 F (v) \u2225\u2225\u2225 2 = \u2126 ( 1\nk log2 k \u00b7min{vol(Si), vol(Sj)}\n)})\n= O\u0303 ( k3/\u03a5 ) vol(Si),\nwhere the last equality follows from Theorem 4.1. For the same reason, we have\nvol(Ai \\ Si) 6 \u2211\ni 6=j vol\n({ v \u2208 Sj : \u2016x(cj)\u2212 x(v)\u2016 >\n\u2016x(ci)\u2212 x(v)\u2016 log k\n})\n= O\u0303 ( k3/\u03a5 ) vol(Si),\nand therefore\nvol(Si\u25b3Ai) = vol(Si \\Ai) + vol(Ai \\ Si) = O\u0303 ( k3/\u03a5 ) vol(Si).\nThis yields the first statement of the lemma. The second statement follows by the same argument used in proving Theorem 1.2."}, {"heading": "5.4 Fast computation of the required embedding", "text": "So far we assumed the existence of the embedding {x(u)}u\u2208V [G] satisfying (5.1) and (5.2), and analyzed the performance of the seeding and grouping steps. In this subsection, we will present a nearly-linear time algorithm to compute all the required distances used in the seeding and grouping steps. Our algorithm is based on the so-called heat kernal of a graph.\nFormally, the heat kernel of G with parameter t > 0 is defined by\nHt , e \u2212tL =\nn\u2211\ni=1\ne\u2212t\u03bbifif \u22ba i . (5.9)\nWe view the heat kernel as a geometric embedding from V [G] to Rn defined by\nxt(u) , 1\u221a du\n\u00b7 ( e\u2212t\u00b7\u03bb1f1(u), \u00b7 \u00b7 \u00b7 , e\u2212t\u00b7\u03bbnfn(u) ) , (5.10)\nand define the \u211322-distance between the points xt(u) and xt(v) by\n\u03b7t(u, v) , \u2016xt(u)\u2212 xt(v)\u20162. (5.11)\nThe following lemma shows that, when k = \u2126(log n) and \u03a5 = \u2126(k3), the values of \u03b7t(u, v) for all edges {u, v} \u2208 E[G] can be approximately computed in O\u0303(m) time.\nLemma 5.8. Let k = \u2126(log n) and \u03a5 = \u2126(k3). Then, there is t = O(poly(n)) such that the embedding {xt(u)}u\u2208V [G] defined in (5.10) satisfies (5.1) and (5.2). Moreover, the values of \u03b7t(u, v) for all {u, v} \u2208 E[G] can be approximately computed in O\u0303(m) time, such that with high probability the conditions (5.1) and (5.2) hold for all edges {u, v} \u2208 E[G].\nOur proof of Theorem 5.8 uses the algorithm for approximating the matrix exponential in [29] as a subroutine, whose performance is summarised in Theorem 5.9. Recall that any n\u00d7 n real and symmetric matrix A is diagonally dominant (SDD), if Aii > \u2211 j 6=i |Aij| for each i = 1, . . . , n. It is easy to see that the Laplacian matrix of any undirected graph is diagonally dominant.\nTheorem 5.9 ([29]). Given an n\u00d7n SDD matrix A with mA nonzero entries, a vector v and a parameter \u03b4 > 0, there is an algorithm that can compute a vector x such that \u2016e\u2212Av\u2212x\u2016 6 \u03b4\u2016v\u2016 in time O\u0303((mA + n) log(2 + \u2016A\u2016)), where the O\u0303(\u00b7) notation hides poly log n and poly log(1/\u03b4) factors.\nProof of Theorem 5.8. By the higher-order Cheeger inequality (1.2), we have that\n\u03a5 = \u03bbk+1 \u03c1(k) 6 2\u03bbk+1 \u03bbk .\nSince k = \u2126(log n) and \u03a5 = \u2126(k3), it holds that 400 \u00b7 log2 n 6 \u03bbk+1/\u03bbk, and there is t such that\nt \u2208 ( 10 \u00b7 log n \u03bbk+1 , 1 20 \u00b7 \u03bbk \u00b7 log n ) .\nWe first show that the embedding {xt(u)}u\u2208V [G] with this t satisfies (5.1) and (5.2). By the definition of \u03b7t(u, v), we have that\n\u03b7t(u, v) = n\u2211\ni=1\ne\u22122t\u03bbi ( fi(u)\u221a du \u2212 fi(v)\u221a dv )2\n=\nk\u2211\ni=1\ne\u22122t\u03bbi ( fi(u)\u221a du \u2212 fi(v)\u221a dv )2 + n\u2211\ni=k+1\ne\u22122t\u03bbi ( fi(u)\u221a du \u2212 fi(v)\u221a dv )2 . (5.12)\nNotice that it holds for 1 6 i 6 k that\n1\u2212 1 10 log n 6 e\u22121/(10 logn) 6 e\u2212\u03bbi/(10\u03bbk/ logn) 6 e\u22122t\u03bbi 6 1, (5.13)\nand it holds for k + 1 6 i 6 n that\ne\u22122t\u00b7\u03bbi 6 e\u22122\u03bbi\u00b710 logn/\u03bbk+1 6 e\u221210 logn\u03bbk+1/\u03bbk+1 = 1\nn20 . (5.14)\nCombining (5.12), (5.13), and (5.14), it holds for any {u, v} \u2208 E[G] that ( 1\u2212 1\n10 \u00b7 log n\n) \u00b7 \u2016F (u)\u2212 F (v)\u20162 6 \u03b7t(u, v) 6 \u2016F (u)\u2212 F (v)\u20162 + 1\nn5 ,\nwhich proves the first statement. Now we show that the distances of \u2016xt(u) \u2212 xt(v)\u2016 for all edges {u, v} \u2208 E[G] can be approximately computed in nearly-linear time. For any vertex u \u2208 V [G], we define \u03beu \u2208 Rn, where (\u03beu)v = 1/ \u221a du if v = u, and (\u03beu)v = 0 otherwise. Combining (5.9) with (5.10) and (5.11), we have that \u03b7t(u, v) = \u2016Ht (\u03beu \u2212 \u03bev)\u20162. We define Z to be the operator of error \u03b4 which corresponds to the algorithm described in Theorem 5.9, and replacing Ht with Z we get\n\u2223\u2223\u2223\u2016Z (\u03beu \u2212 \u03bev)\u2016 \u2212 \u03b71/2t (u, v) \u2223\u2223\u2223 6 \u03b4 \u2016\u03beu \u2212 \u03bev\u2016 6 \u03b4,\nwhere the last inequality follows from du, dv > 1. Hence, it holds that\n\u03b7 1/2 t (u, v) \u2212 \u03b4 6 \u2016Z (\u03beu \u2212 \u03bev)\u2016 6 \u03b7 1/2 t (u, v) + \u03b4. (5.15)\nBy applying the Johnson-Lindenstrauss transform in a way analogous to the computation of effective resistances (e.g. [20] and [37]), we obtain an O(\u03b5\u22122 \u00b7 log n) \u00d7 n Gaussian matrix Q, such that with high probability it holds for all u, v that\n(1\u2212 \u03b5) \u2016Z (\u03beu \u2212 \u03bev)\u2016 6 \u2016QZ (\u03beu \u2212 \u03bev)\u2016 6 (1 + \u03b5) \u2016Z (\u03beu \u2212 \u03bev)\u2016 . (5.16) Combining (5.15) and (5.16) gives us that\n(1\u2212 \u03b5) ( \u03b7 1/2 t (u, v) \u2212 \u03b4 ) 6 \u2016QZ (\u03beu \u2212 \u03bev)\u2016 6 (1 + \u03b5) ( \u03b7 1/2 t (u, v) + \u03b4 ) .\nSquaring both sides and invoking the inequality (1\u2212 \u03b5)\u03b12 \u2212 (1+ \u03b5\u22121)b2 6 (a+ b)2 6 (1+ \u03b5)\u03b12 + (1 + \u03b5\u22121)b2 gives\n(1\u2212 5\u03b5) \u03b7t(u, v) \u2212 2\u03b42\u03b5\u22121 6 \u2016QZ (\u03beu \u2212 \u03bev)\u20162 6 (1 + 5\u03b5) \u03b7t(u, v) + 2\u03b42\u03b5\u22121\nScaling QZ by a factor of (1 + 5\u03b5)\u22121, and appending an extra entry in each vector to create an additive distortion of 2\u03b4\u03b5\u22121 then gives the desired bounds when \u03b4 is set to \u03b5n\u22126. To satisfy the conditions (5.1) and (5.2) we just need to set \u03b5 = O(1/ log n).\nTo analyze the runtime of computing \u2016QZ (\u03beu \u2212 \u03bev)\u20162 for all edges {u, v} \u2208 E[G], notice that Q has only O(log3 n) rows. We can then run the approximate exponential algorithm from [29] O(log3 n) times, where each time we use a different row of Q as input. Since \u2016L\u2016 6 2, by Theorem 5.9 we can compute QZ in O\u0303(m) time. Notice that QZ\u03beu is some column of QZ after rescaling, therefore we can compute all the required distances in time O\u0303(m).\nWe remark that the proof above shows an interesting property about the embedding (5.10), i.e., for a large value of k and a certain condition on \u03a5, there is always a t such that the values of \u03b7t(u, v) gives a good approximation of \u2016F (u) \u2212 F (v)\u20162 for all edges {u, v} \u2208 E[G]. A similar intuition which views the heat kernel embedding as a weighted combination of multiple eigenvectors was discussed in [29]."}, {"heading": "5.5 Proof of Theorem 1.3", "text": "We proved in Section 5.4 that if k = \u2126(log n) and \u03a5 = \u2126(k3), there is a\nt \u2208 ( 10 log n\n\u03bbk+1 ,\n1\n20 \u00b7 \u03bbk \u00b7 log n\n) (5.17)\nsuch that {xt(u)}u\u2208V [G] satisfies the conditions (5.1) and (5.2). Moreover, the values of \u2016xt(u)\u2212 xt(v)\u2016 for {u, v} \u2208 E[G] can be approximately computed in nearly-linear time3. However, it is unclear how to approximate \u03bbk, and without this approximation. Furthermore, without this approximation of \u03bbk, obtaining the desired embedding {x(u)}u\u2208V [G] becomes highly non-trivial.\nTo overcome this obstacle, we run the seeding and grouping steps for all possible t of the form 2i, where t \u2208 N>0, as it allows us to run the seeding and grouping steps with the right values of t at some point. However, by (5.11) the distance between any pair of embedded vertices decreases when we increase the value of t. Moreover, all these embedded points {xt(u)}u\u2208V [G] tend to \u201cconcentrate\u201d around a single point for an arbitrary large value of t. To avoid this situation, for every possible t we compute the value of \u2211 v\u2208V [G] dv\u2016xt(v)\u20162, and the algorithm only moves to the next iteration if\n\u2211\nv\u2208V [G] dv\u2016xt(v)\u20162 > k\n( 1\u2212 2\nlog n\n) . (5.18)\nBy Theorem 5.1, (5.18) is satisfied for all values of t in the right range (5.17), and the algorithm will not terminate before t = \u230alog n/\u03bbk+1\u230b. See Algorithm 2 for the formal description of our final algorithm.\nAlgorithm 2 A nearly-linear time graph clustering algorithm, k = \u2126(log n)\n1: input: the input graph G, and the number of clusters k 2: Let t = 2. 3: repeat 4: Let (c1, . . . , ck) = SeedAndTrim ( k, {xt(u)}u\u2208V [G] ) . 5: if SeedAndTrim returns exactly k points then 6: Compute a partition A1, . . . , Ak of V [G]: for every v \u2208 V [G] assign v to its nearest center ci using the \u03b5-NNS algorithm with \u03b5 = log k \u2212 1. 7: end if 8: Let t = 2t 9: until t > n10 or \u2211 v\u2208V [G] dv\u2016xt\u20162 < k ( 1\u2212 2logn ) .\n10: return (A1, \u00b7 \u00b7 \u00b7 , Ak).\nLemma 5.10. Let t = \u2126(1/(\u03bbk \u00b7 log n)), and t satisfies (5.18). Suppose that SeedAndTrim uses the embedding {xt(u)}u\u2208V [G] and returns k centers c1, . . . , ck. Then, with constant probability, the following statements hold:\n3Theorem 5.8 shows that both of the embedding {xt(u)}u\u2208V [G] and the embedding that the algorithm computes in nearly-linear time satisfy the conditions (5.1) and (5.2) with high probability. For the ease of discussion, we use {xt(u)}u\u2208V [G] to express the embedding that the algorithm actually uses.\n1. It holds that\n{c1, . . . , ck} \u2286 k\u22c3\ni=1\nCORE\u03b1i .\n2. These k centers belong to different cores, and it holds for any different i, j that\n\u2016xt(ci)\u2212 xt(cj)\u20162 = \u2126\u0303 (\n1\nk \u00b7 vol(Si)\n) .\n3. For any i = 1, . . . , k, it holds that\nk\u2211\ni=1\n\u2211 u\u2208Si du \u00b7 \u2016x(u) \u2212 x(ci)\u20162 = O\u0303\n( k3\n\u03a5\n) .\nProof. Since \u2016xt(u)\u2016 is decreasing with respect to the value of t for any vertex u, by Lemma 5.1 for any t = \u2126(1/(\u03bbk \u00b7 log n)) we have:\nk\u2211\ni=1\n\u2211\nu/\u2208CORE\u03b1i\ndu \u00b7 \u2016xt(u)\u20162 6 k\u2211\ni=1\n\u2211\nu/\u2208CORE\u03b1i\ndu \u00b7 ( \u2016F (u)\u20162 + 1\nn5\n) 6 k\n100K +\nkn2\nn5 6\n1\nlog k .\nOn the other hand, we only consider values of t satisfying (5.18). Since every vertex u is sampled with probability proportional to du \u00b7 \u2016xt(u)\u20162, with constant probability it holds that\n{c1, . . . , ck} \u2286 k\u22c3\ni=1\nCORE\u03b1i ,\nwhich proves the first statement. Now we prove that these k centers belong to different cores. We fix an index i, and assume that ci \u2208 Si. We will prove that\n\u2016xt(ci)\u20162 = \u2126\u0303 ( 1\nvol(Si)\n) . (5.19)\nAssume by contradiction that (5.19) does not hold, i.e.,\n\u2016xt(ci)\u20162 = o (\n1\nlogc k vol(Si)\n)\nfor any constant c. Then, we have that\n\u2211\nu\u2208CORE\u03b1i\ndu \u00b7 \u2016xt(u)\u20162 6 \u2211\nu\u2208CORE\u03b1i\ndu \u00b7 ( \u2016xt(ci)\u2016+ \u221a R\u03b1i )2\n6 2 \u00b7 \u2211\nu\u2208CORE\u03b1i\n( du \u00b7 \u2016xt(ci)\u20162 + du \u00b7R\u03b1i )\n= o\n( 1\nlogc k\n) + o ( 1\nk2\n)\n= o\n( 1\nlogc k\n) .\nCombining this with (5.18), the probability that vertices get sampled from CORE\u03b1i is\n\u2211 u\u2208CORE\u03b1i du \u00b7 \u2016xt(u)\u2016 2\n\u2211 v\u2208V [G] dv\u2016xt(v)\u20162 = o\n( 1\nk \u00b7 logc k\n) .\nThis means if we sample K = \u0398(k log k) vertices, vertices in CORE\u03b1i will not get sampled with probability at least 1 \u2212 1/ log5 k. This contradicts the fact that ci \u2208 CORE\u03b1i . Therefore (5.19) holds.\nNow, by description of Algorithm 1, we have for any j 6= i:\n\u2016xt(ci)\u2212 xt(cj)\u20162 > \u2016x(ci)\u20162 2 \u00b7 104 \u00b7 k = \u2126\u0303\n( 1\nk \u00b7 vol(Si)\n) ,\nwhere the last equality follows from (5.19). Since any vertex in CORE\u03b1i has distance at most R\u03b1i from ci, cj and ci belong to different cores. Therefore, the second statement holds.\nFinally we turn our attention to the third statement. We showed in Theorem 5.8 that, when t = \u0398(1/(\u03bbk \u00b7 log n)), the embedding {xt(u)}u\u2208V [G] satisfies the conditions (5.1) and (5.2). Hence, it holds that\nk\u2211\ni=1\n\u2211 u\u2208Si du \u00b7 \u2016x(u)\u2212 x(ci)\u20162 6 k\u2211 i=1 \u2211 u\u2208Si\n( du \u00b7 \u2016F (u)\u2212 F (ci)\u20162 + 1\nn5\n)\n6 k\u2211\ni=1\n\u2211\nu\u2208Si\n( du \u00b7 (\u2016F (u)\u2212 pi\u2016+ \u2016F (ci)\u2212 pi\u2016)2 + 1\nn5\n)\n6 k\u2211\ni=1\n\u2211\nu\u2208Si\n( 2 \u00b7 du \u00b7 ( \u2016F (u)\u2212 pi\u20162 + \u2016F (ci)\u2212 pi\u20162 ) + 1\nn5\n) (5.20)\nNotice that by Theorem 4.1 we have\nk\u2211\ni=1\n\u2211 u\u2208Si du \u00b7 \u2016F (u)\u2212 pi\u20162 6 1.1k2/\u03a5. (5.21)\nOn the other hand, we have \u2016F (ci)\u2212 pi\u20162 6 R\u03b1i as ci \u2208 CORE\u03b1i , and\nk\u2211\ni=1\n\u2211 u\u2208Si 2 \u00b7 du \u00b7 \u2016F (ci)\u2212 pi\u20162 6 k\u2211 i=1 2 vol(Si) \u00b7 \u03b1 \u00b7 Ei vol(Si) = k\u2211 i=1\n2\u03b1 \u00b7 Ei = O\u0303 ( k3\n\u03a5\n) . (5.22)\nCombining (5.20) with (5.21) and (5.22), we have that\nk\u2211\ni=1\n\u2211 u\u2208Si du \u00b7 \u2016x(u) \u2212 x(ci)\u20162 6 O\u0303\n( k3\n\u03a5\n) + \u2211\nu\u2208V [G]\ndu n5 = O\u0303\n( k3\n\u03a5\n) .\nMoreover, by (5.10) and (5.11) it is straightforward to see that the distance between any embedded vertices decreases as we increase the value of t. Hence, the statement holds for any t = \u2126(1/(\u03bbk \u00b7 log n)).\nLemma 5.11. Let A1, . . . , Ak be a k-way partition returned by Algorithm 2. Then, under a proper permutation of the indices, with constant probability for any 1 6 i 6 k it holds that (i) vol(Ai\u25b3Si) = O\u0303 ( k4/\u03a5 ) vol(Si), and (ii) \u03c6G(Ai) = 1.1 \u00b7 \u03c6G(Si) + O\u0303 ( k4/\u03a5 ) .\nProof. We assume that c1, . . . , ck are the centers returned by SeedAndTrim when obtaining A1, . . . , Ak. By Theorem 5.10, with constant probability it holds that {c1, . . . , ck} \u2286\u22c3k\ni=1 CORE \u03b1 i , and ci and cj belong to different cores for i 6= j. Without loss of generality,\nwe assume that ci \u2208 CORE\u03b1i . Then, it holds that\nvol(Si \\ Ai) 6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2016x(ci)\u2212 x(v)\u2016 >\n\u2016x(cj)\u2212 x(v)\u2016 log k\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2016x(ci)\u2212 x(v)\u2016 >\n\u2016x(ci)\u2212 x(cj)\u2016 \u2212 \u2016x(ci)\u2212 x(v)\u2016 log k\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : 2\u2016x(ci)\u2212 x(v)\u2016 >\n\u2016x(ci)\u2212 x(cj)\u2016 log k\n})\n6 \u2211\ni 6=j vol\n({ v \u2208 Si : \u2016x(ci)\u2212 x(v)\u20162 = \u2126\u0303 ( 1\nkmin{vol(Sj), vol(Si)}\n)}) (5.23)\n= O\u0303 ( k4/\u03a5 ) vol(Si), (5.24)\nwhere (5.23) follows from the second statement of Theorem 5.10. Similarly, we also have that\nvol(Ai \\ Si) 6 \u2211\ni 6=j vol\n({ v \u2208 Sj : \u2016x(cj)\u2212 x(v)\u2016 >\n\u2016x(ci)\u2212 x(v)\u2016 log k\n})\n= O\u0303 ( k4/\u03a5 ) vol(Si).\nThis yields the first statement of the lemma. The second statement follows by the same argument used in proving Theorem 1.2.\nProof of Theorem 1.3. The approximation guarantee of the returned partition is shown in Theorem 5.11. For the runtime, notices that we enumerate at most O(poly log n) possible values of t. Furthermore, and for every such possible value of t the algorithm runs in O\u0303(m) time. This includes computing the distances of embedded points and the seeding / grouping steps. Hence, the total runtime is O\u0303(m)."}, {"heading": "Acknowledgements", "text": "Part of this work was done while He Sun and Luca Zanetti were at the Max Planck Institute for Informatics, and while He Sun was visiting the Simons Institute for the Theory of Computing at UC Berkeley. We are grateful to Luca Trevisan for insightful comments on an earlier version of our paper, and to Gary Miller for very helpful discussions about heat kernels on graphs. We also would like to thank Pavel Kolev, and Kurt Mehlhorn [19] for pointing out an omission in an early version of Lemma 4.6. This omission was fixed locally without effecting the statement of the main results."}], "references": [{"title": "A local algorithm for finding well-connected clusters", "author": ["Zeyuan Allen-Zhu", "Silvio Lattanzi", "Vahab S. Mirrokni"], "venue": "In 30th International Conference on Machine Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Subexponential algorithms for unique games and related problems", "author": ["Sanjeev Arora", "Boaz Barak", "David Steurer"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "A combinatorial, primal-dual approach to semidefinite programs", "author": ["Sanjeev Arora", "Satyen Kale"], "venue": "Annual ACM Symposium on Theory of Computing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["Sanjeev Arora", "Satish Rao", "Umesh V. Vazirani"], "venue": "J. ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In 18th Annual ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Information Processing Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A theoretical approach to the clustering selection problem. In Proceedings of the 4th MultiClust Workshop on Multiple Clusterings, Multi-view Data, and Multi-source Knowledge-driven Clustering, page", "author": ["Shai Ben-David"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "A local graph partitioning algorithm using heat kernel pagerank", "author": ["Fan R.K. Chung"], "venue": "Internet Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Image segmentation by clustering", "author": ["Guy B. Coleman", "Harry C. Andrews"], "venue": "Proceedings of the IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1979}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["Chandler Davis", "William M. Kahan"], "venue": "iii. SIAM Journal on Numerical Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1970}, {"title": "Spectral concentration, robust k-center, and simple clustering", "author": ["Tamal K. Dey", "Alfred Rossi", "Anastasios Sidiropoulos"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Community detection in graphs", "author": ["Santo Fortunato"], "venue": "Physics Reports,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Matrix Analysis", "author": ["Roger A. Horn", "Charles R. Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In 30th Annual ACM Symposium on Theory of Computing", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "On clusterings: Good, bad and spectral", "author": ["Ravi Kannan", "Santosh Vempala", "Adrian Vetta"], "venue": "Journal of the ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "An almostlinear-time algorithm for approximate max flow in undirected graphs, and its multicommodity generalizations", "author": ["Jonathan A. Kelner", "Yin Tat Lee", "Lorenzo Orecchia", "Aaron Sidford"], "venue": "In 25th Annual ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A note on spectral clustering", "author": ["Pavel Kolev", "Kurt Mehlhorn"], "venue": "CoRR, abs/1509.09188,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Improved Spectral Sparsification and Numerical Algorithms for SDD Matrices", "author": ["Ioannis Koutis", "Alex Levin", "Richard Peng"], "venue": "In 29th International Symposium on Theoretical Aspects of Computer Science", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Improved Cheeger\u2019s inequality: analysis of spectral partitioning algorithms through higher order spectral gap", "author": ["Tsz Chiu Kwok", "Lap Chi Lau", "Yin Tat Lee", "Shayan Oveis Gharan", "Luca Trevisan"], "venue": "In 45th Annual ACM Symposium on Theory of Computing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Multi-way spectral partitioning and higher-order Cheeger inequalities", "author": ["James R. Lee", "Shayan Oveis Gharan", "Luca Trevisan"], "venue": "In 44th Annual ACM Symposium on Theory of Computing", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms", "author": ["Frank T. Leighton", "Satish Rao"], "venue": "Journal of the ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Many sparse cuts via higher eigenvalues", "author": ["Anand Louis", "Prasad Raghavendra", "Prasad Tetali", "Santosh Vempala"], "venue": "In 44th Annual ACM Symposium on Theory of Computing", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Correlation clustering with noisy partial information", "author": ["Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan"], "venue": "In 28th Conference on Learning Theory (COLT\u201915),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Sparsest cuts and bottlenecks in graphs", "author": ["David W. Matula", "Farhad Shahrokhi"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "Spectral partitioning of random graphs", "author": ["Frank McSherry"], "venue": "In 42nd Annual IEEE Symposium on Foundations of Computer Science", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Approximating the exponential, the Lanczos method and an \u00d5(m)-time spectral algorithm for balanced separator", "author": ["Lorenzo Orecchia", "Sushant Sachdeva", "Nisheeth K Vishnoi"], "venue": "In 44th Annual ACM Symposium on Theory of Computing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "On partitioning graphs via single commodity flows", "author": ["Lorenzo Orecchia", "Leonard J. Schulman", "Umesh V. Vazirani", "Nisheeth K. Vishnoi"], "venue": "In 40th Annual ACM Symposium on Theory of Computing", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "Journal of the ACM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Partitioning into expanders", "author": ["Shayan Oveis Gharan", "Luca Trevisan"], "venue": "In 25th Annual ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Spectral clustering and the high-dimensional stochastic blockmodel", "author": ["Karl Rohe", "Sourav Chatterjee", "Bin Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1878}, {"title": "Lectures on finite markov chains", "author": ["L. Saloff-Coste"], "venue": "Lectures on Probability Theory and Statistics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Breaking the multicommodity flow barrier for O( \u221a log n)-approximations to sparsest cut", "author": ["Jonah Sherman"], "venue": "In 50th Annual IEEE Symposium on Foundations of Computer Science", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Graph sparsification by effective resistances", "author": ["Daniel A. Spielman", "Nikhil Srivastava"], "venue": "SIAM Journal on Computing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1913}, {"title": "Spectral sparsification of graphs", "author": ["Daniel A. Spielman", "Shang-Hua Teng"], "venue": "SIAM Journal on Computing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Approximation algorithms for unique games", "author": ["Luca Trevisan"], "venue": "Theory of Computing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "A simple SVD algorithm for finding hidden partitions", "author": ["Van Vu"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "This problem is known to be NP-hard [26], and the current best approximation algorithm achieves an approximation ratio of O (\u221a log n ) [5].", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "This problem is known to be NP-hard [26], and the current best approximation algorithm achieves an approximation ratio of O (\u221a log n ) [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": "In computer vision, most image segmentation procedures are based on region-based merge and split [10], which in turn rely on partitioning graphs into multiple subsets [36].", "startOffset": 97, "endOffset": 101}, {"referenceID": 35, "context": "In computer vision, most image segmentation procedures are based on region-based merge and split [10], which in turn rely on partitioning graphs into multiple subsets [36].", "startOffset": 167, "endOffset": 171}, {"referenceID": 38, "context": "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].", "startOffset": 196, "endOffset": 208}, {"referenceID": 22, "context": "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].", "startOffset": 196, "endOffset": 208}, {"referenceID": 37, "context": "On a theoretical side, decomposing vertex/edge sets into multiple disjoint subsets is used in designing approximation algorithms for Unique Games [39], and efficient algorithms for graph problems [18, 23, 38].", "startOffset": 196, "endOffset": 208}, {"referenceID": 21, "context": "Despite widespread use of various graph partitioning schemes over the past decades, the quantitative relationship between the k-way expansion constant and the eigenvalues of the graph Laplacians were unknown until a sequence of very recent results [22, 24].", "startOffset": 248, "endOffset": 256}, {"referenceID": 23, "context": "Despite widespread use of various graph partitioning schemes over the past decades, the quantitative relationship between the k-way expansion constant and the eigenvalues of the graph Laplacians were unknown until a sequence of very recent results [22, 24].", "startOffset": 248, "endOffset": 256}, {"referenceID": 21, "context": "[22] proved the following higher-order Cheeger inequality: \u03bbk 2 6 \u03c1(k) 6 O(k) \u221a \u03bbk, (1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 39, "context": "[40] and Section D in [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[40] and Section D in [14].", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "([2], Theorem 2.", "startOffset": 1, "endOffset": 4}, {"referenceID": 11, "context": "1), and can be considered as a stronger version of the well-known Davis-Kahan theorem [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "Specifically, it omits much of the machinery used in the proofs of higher-order and improved Cheeger inequalities [21, 22].", "startOffset": 114, "endOffset": 122}, {"referenceID": 21, "context": "Specifically, it omits much of the machinery used in the proofs of higher-order and improved Cheeger inequalities [21, 22].", "startOffset": 114, "endOffset": 122}, {"referenceID": 27, "context": "[28, 40].", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[28, 40].", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "The heat kernel of a graph is a well-studied mathematical concept and is related to, for example, the study of random walks [34].", "startOffset": 124, "endOffset": 128}, {"referenceID": 28, "context": "Since the heat kernel distances between vertices can be approximated in nearly-linear time [29], this approach avoids the computation of eigenvectors for a large value of k.", "startOffset": 91, "endOffset": 95}, {"referenceID": 35, "context": "For instance, it is easy to see that \u03c1(k) and the normalized cut [36] studied in machine learning, which is defined as the sum of the conductance of all returned clusters, differ by at most a factor of k, and the normalized cut value of a k-way partition from spectral clustering can be derived from our results.", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "[7, 8, 17, 25].", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "[7, 8, 17, 25].", "startOffset": 0, "endOffset": 14}, {"referenceID": 16, "context": "[7, 8, 17, 25].", "startOffset": 0, "endOffset": 14}, {"referenceID": 24, "context": "[7, 8, 17, 25].", "startOffset": 0, "endOffset": 14}, {"referenceID": 35, "context": "Among these, our work is most closely related to spectral clustering, which is closely related to normalized or low conductance cuts [36].", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "Oveis Gharan and Trevisan [32] formulate the notion of clusters with respect to the inner and outer conductance: a cluster S should have low outer conductance, and the conductance of the induced subgraph by S should be high.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "[13] studies the properties of the spectral embedding for graphs having a gap between \u03bbk and \u03bbk+1 and presents a k-way partition algorithm, which is based on k-center clustering and is similar in spirit to our work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The heat kernel has been used in previous algorithms on local partitioning [9], balanced separators [29].", "startOffset": 75, "endOffset": 78}, {"referenceID": 28, "context": "The heat kernel has been used in previous algorithms on local partitioning [9], balanced separators [29].", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": "It also plays a key role in current efficient approximation algorithms for finding low conductance cuts [30, 35].", "startOffset": 104, "endOffset": 112}, {"referenceID": 34, "context": "It also plays a key role in current efficient approximation algorithms for finding low conductance cuts [30, 35].", "startOffset": 104, "endOffset": 112}, {"referenceID": 2, "context": "However, most of these theoretical guarantees are through the matrix multiplicative weights update framework [3, 4].", "startOffset": 109, "endOffset": 115}, {"referenceID": 3, "context": "However, most of these theoretical guarantees are through the matrix multiplicative weights update framework [3, 4].", "startOffset": 109, "endOffset": 115}, {"referenceID": 26, "context": "For instance, in the Stochastic Block Model (SBM) [27], the input graph with k clusters is generated according to probabilities p and q with p > q: an edge between any two vertices within the same cluster is placed with probability p, and an edge between any two vertices from different clusters is placed with probability q.", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41].", "startOffset": 96, "endOffset": 108}, {"referenceID": 32, "context": "It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41].", "startOffset": 96, "endOffset": 108}, {"referenceID": 40, "context": "It is proven that spectral algorithms give the correct clustering for certain ranges of p and q [27, 33, 41].", "startOffset": 96, "endOffset": 108}, {"referenceID": 11, "context": "For this reason, standard perturbation theorems used in the analysis of algorithms for SBMs, such as the Davis-Kahan theorem [12], cannot be always applied, and ad-hoc arguments specific for graphs, like our structure theorem (Theorem 1.", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "2 of [2].", "startOffset": 5, "endOffset": 8}, {"referenceID": 14, "context": "4, [15]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "We point out that it was shown in [21] that the first k eigenvectors can be approximated by a (2k + 1)-step function.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "Notice that this embedding is similar with the one used in [22], with the only difference that F (u) is not normalized and so it is not necessarily a unit vector.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "[11]), we can also always assume that the dimension of the embedding {x(u)}u\u2208V [G] is d = O(log n).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6, 31]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 30, "context": "[6, 31]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "To speed it up, we apply \u03b5-approximate nearest neighbor data structures (\u03b5-NNS) [16], whose formal description is as follows: Problem 1 (\u03b5-approximate nearest neighbor problem).", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "5 ([16]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "8 uses the algorithm for approximating the matrix exponential in [29] as a subroutine, whose performance is summarised in Theorem 5.", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "9 ([29]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "[20] and [37]), we obtain an O(\u03b5\u22122 \u00b7 log n) \u00d7 n Gaussian matrix Q, such that with high probability it holds for all u, v that (1\u2212 \u03b5) \u2016Z (\u03beu \u2212 \u03bev)\u2016 6 \u2016QZ (\u03beu \u2212 \u03bev)\u2016 6 (1 + \u03b5) \u2016Z (\u03beu \u2212 \u03bev)\u2016 .", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[20] and [37]), we obtain an O(\u03b5\u22122 \u00b7 log n) \u00d7 n Gaussian matrix Q, such that with high probability it holds for all u, v that (1\u2212 \u03b5) \u2016Z (\u03beu \u2212 \u03bev)\u2016 6 \u2016QZ (\u03beu \u2212 \u03bev)\u2016 6 (1 + \u03b5) \u2016Z (\u03beu \u2212 \u03bev)\u2016 .", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "We can then run the approximate exponential algorithm from [29] O(log n) times, where each time we use a different row of Q as input.", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "A similar intuition which views the heat kernel embedding as a weighted combination of multiple eigenvectors was discussed in [29].", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "We also would like to thank Pavel Kolev, and Kurt Mehlhorn [19] for pointing out an omission in an early version of Lemma 4.", "startOffset": 59, "endOffset": 63}], "year": 2017, "abstractText": "In this paper we study variants of the widely used spectral clustering that partitions a graph into k clusters by (1) embedding the vertices of a graph into a low-dimensional space using the bottom eigenvectors of the Laplacian matrix, and (2) grouping the embedded points into k clusters via k-means algorithms. We show that, for a wide class of graphs, spectral clustering gives a good approximation of the optimal clustering. While this approach was proposed in the early 1990s and has comprehensive applications, prior to our work similar results were known only for graphs generated from stochastic models. We also give a nearly-linear time algorithm for partitioning well-clustered graphs based on computing a matrix exponential and approximate nearest neighbor data structures.", "creator": "LaTeX with hyperref package"}}}