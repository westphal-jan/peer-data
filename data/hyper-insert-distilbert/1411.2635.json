{"id": "1411.2635", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2014", "title": "A chain rule for the expected suprema of Gaussian processes", "abstract": "because the expected supremum of assessing a wiener gaussian process being indexed by the image of an autonomous index set enclosed under identifying a analytic function class definition is bounded in terms of separate properties of the entire index set and ignores the function class. the bound error is relevant to predicting the estimation of nonlinear transformations or the analysis framework of learning approximation algorithms by whenever hypotheses are always chosen accurately from composite optimization classes, being as latter is the case for multi - layer algebraic models.", "histories": [["v1", "Mon, 10 Nov 2014 21:41:32 GMT  (14kb)", "http://arxiv.org/abs/1411.2635v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andreas maurer"], "accepted": false, "id": "1411.2635"}, "pdf": {"name": "1411.2635.pdf", "metadata": {"source": "CRF", "title": "A chain rule for the expected suprema of Gaussian processes", "authors": ["Andreas Maurer"], "emails": ["am@andreas-maurer.eu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n26 35\nv1 [\ncs .L\nG ]\n1 0\nN ov"}, {"heading": "1 Introduction", "text": "Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.\nTo briefly describe the use of Gaussian averages (Rademacher averages will not concern us), let Y \u2286 Rn and let \u03b3 be a vector \u03b3 = (\u03b31, ..., \u03b3n) of independent standard normal variables. We define the (expected supremum of the) Gaussian average of Y as\nG (Y ) = E sup y\u2208Y\n\u3008\u03b3,y\u3009 , (1)\nwhere \u3008., .\u3009 denotes the inner product in Rn. Consider a loss class F of functions f : X \u2192 R, where X is some space of examples (such as input-output pairs), a sample x = (x1, ..., xn) \u2208 Xn of observations and write F (x) for the subset of R\nn given by F (x) = {(f (x1) , ..., f (xn)) : f \u2208 F}. Then we have the following result [1].\nTheorem 1. Let the members of F take values in [0, 1] and let X,X1, ..., Xn be iid random variables with values in X , X = (X1, ..., Xn). Then for \u03b4 > 0 with probability at least 1\u2212 \u03b4 we have for every f \u2208 F that\nEf (X) \u2264 1 n \u2211 f (Xi) +\n\u221a 2\u03c0\nn G (F (X)) +\n\u221a\n9 ln 2/\u03b4\n2n ,\nwhere the expectation in the definition (1) of G (F (X)) is conditional to the sample X.\n2 The utility of Gaussian averages is not limited to functions with values in [0, 1]. For real functions \u03c6 with Lipschitz constant L (\u03c6) we have G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)) (see also Slepian\u2019s Lemma, [6], [4]), where \u03c6 \u25e6 F is the class {x 7\u2192 \u03c6 (f (x)) : f \u2208 F}.\nThe inequality G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)), which in the above form holds also for Rademacher averages [10], is extremely useful and in part responsible for the success of these complexity measures. For Gaussian averages it holds in a more general sense: if \u03c6 : Rn \u2192 Rm has Lipschitz constant L (\u03c6) with respect to the Euclidean distances, then G (\u03c6 (Y )) \u2264 L (\u03c6)G (Y ). This is a direct consequence of Slepian\u2019s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).\nBut what if we also want some freedom in the choice of \u03c6 after seeing the data? If the class of Lipschitz functions considered has small cardinality, a union bound can be used. If it is very large one can try to use covering numbers, but the matter soon becomes quite complicated and destroys the elegant simplicity of the method.\nThese considerations lead to a more general question: given a set Y \u2282 Rn and a class F of functions f : Rn\u2192 Rm, how can we bound the Gaussian average G (F (Y )) = G ({f (y) : f \u2208 F, y \u2208 Y }) in terms of separate properties of Y and F , properties which should preferably very closely resemble Gaussian averages? If H is some class of functions mapping samples to Rn and Y = H (x), then the bound is on G (F (Y )) = G ((F \u25e6 H) (x)), so our question is relevant to the estimation of composite functions in general. Such estimates are necessary for multitask feature-learning, where H is a class of feature maps and F is vectorvalued, with components chosen independently for each task. Other potential applications are to the currently popular subject of deep learning, where we consider functional concatenations as in FM\u25e6FM\u22121\u25e6... \u25e6 F1.\nThe present paper gives a preliminary answer. To state it we introduce some notation. We will always take \u03b3 = (\u03b31, ...) to be a random vector whose components are independent standard normal variables, while \u2016.\u2016 and \u3008., .\u3009 denote norm and inner product in a Euclidean space, the dimension of which is determined by context, as is the dimension of the vector \u03b3.\nDefinition 1. If Y \u2286 Rn we set\nD (Y ) = sup y,y\u2032\u2208Y \u2016y \u2212 y\u2032\u2016 and G (Y ) = E sup y\u2208Y \u3008\u03b3,y\u3009 .\nIf F is a class of functions f : Y \u2192 Rm we set\nL (F, Y ) = sup y,y\u2032\u2208Y, y 6=y\u2032 sup f\u2208F \u2016f (y) \u2212 f (y\u2032)\u2016 \u2016y \u2212 y\u2032\u2016 and\nR (F, Y ) = sup y,y\u2032\u2208Y, y 6=y\u2032 E sup f\u2208F \u3008\u03b3, f (y) \u2212 f (y\u2032)\u3009 \u2016y \u2212 y\u2032\u2016 .\n3 We also write F (Y ) = {f (y) : f \u2208 F,y \u2208 Y }. When there is no ambiguity we write L (F ) = L (F, Y ) and R (F ) = R (F, Y ).\nThen D (Y ) is the diameter of Y , and G (Y ) is the Gaussian average already introduced above. L (F ) is the smallest Lipschitz constant acceptable for all f \u2208 F , and the more unusual quantity R (F ) can be viewed as a Gaussian average of Lipschitz quotients. In section 3.1 we give some properties of R (F ). Our main result is the following chain rule.\nTheorem 2. Let Y \u2282 Rn be finite, F a finite class of functions f : Y \u2192 Rm. Then there are universal constants C1 and C2 such that for any y0 \u2208 Y\nG (F (Y )) \u2264 C1L (F )G (Y ) + C2D (Y )R (F ) +G (F (y0)) . (2)\nWe make some general remarks on the implications of our result.\n1. The requirement of finiteness for Y and F is a simplification to avoid issues of measurability. The cardinality of these sets plays no role.\n2. The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16]. This is a major shortcoming and the reason why our result is regarded as preliminary. Is there another proof of a similar result, avoiding majorizing measures and resulting in smaller constants? This question is the subject of current research.\n3. The first term on the right hand side of (2) describes the complexity inherited from the bottom layer Y (which we may think of as H (x)), and it depends on the top layer F only through the Lipschitz constant L (F ). The other two terms represent the complexity of the top layer, depending on the bottom layer only through the diameter D (Y ) of Y . If Y has unit diameter and the functions in F are contractions, then the two layers are completely decoupled in the bound. This decoupling is the most attractive property of our result.\n4. Apart from the large constants the inequality is tight in at least two situations: first, if Y = {y0} is a singleton, then only the last term remains, and we recover the Gaussian average of F (y0). This also shows that the last term cannot be eliminated. On the other hand if F consists of a single Lipschitz function \u03c6, then we recover (up to a constant) the inequality G (\u03c6 (Y )) \u2264 L (\u03c6)G (Y ) above.\n5. The bound can be iterated to multiple layers by re-substitution of F (Y ) in place of Y . A corresponding formula is given in Section 3, where we also sketch applications to vector-valued function classes.\nThe next section gives a proof of Theorem 2, then we explain how our result can be applied to machine learning. The last section is devoted to the proof of a technical result encapsulating our use of majorizing measures.\n4"}, {"heading": "2 Proving the chain rule", "text": "To prove Theorem 2 we need the theory of majorizing measures and generic chaining. Our use of these techniques is summarized in the following theorem, which is also the origin of our large constants.\nTheorem 3. Let Xy be a random process indexed by a finite set Y \u2282 Rn. Suppose that there is a number K \u2265 1 such that for any distinct members y,y\u2032 \u2208 Y and any s > 0\nPr {Xy \u2212Xy\u2032 > s} \u2264 K exp (\n\u2212s2\n2 \u2016y \u2212 y\u2032\u20162\n)\n(3)\nThen for any y0 \u2208 Y\nE\n[\nsup y\u2208Y\nXy \u2212Xy0 ] \u2264 C\u2032G (Y ) + C\u2032\u2032D (Y ) \u221a lnK,\nwhere C\u2032 and C\u2032\u2032 are universal constants.\nThis is obtained from Talagrand\u2019s majorizing measure theorem (Theorem 6 below) combined with generic chaining [16]. An early version of a similar result is Theorem 15 in [13], where the author remarks that his method of proof (which we also use) is very indirect, and that a more direct proof would be desirable. In Section 4 we do supply a proof, largely because the dependence on K, which can often be swept under the carpet, plays a crucial role in our arguments below.\nWe also need the following Gaussian concentration inequality (TsirelsonIbragimov-Sudakov inequality, Theorem 5.6 in [4]).\nTheorem 4. Let F : Rn \u2192 R be L-Lipschitz. Then for any s > 0\nPr {F (\u03b3) > EF (\u03b3) + s} \u2264 e\u2212s2/(2L2).\nTo conclude the preparation for the proof of Theorem 2 we give a simple lemma.\nLemma 1. Suppose a random variable X satisfies Pr {X \u2212A > s} \u2264 e\u2212s2 , for any s > 0. Then\n\u2200s > 0 , Pr {X > s} \u2264 eA2e\u2212s2/2.\nProof. For s \u2264 A the conclusion is trivial, so suppose that s > A. From s2 = (s\u2212A+A)2 \u2264 2 (s\u2212A)2 + 2A2 we get (s\u2212A)2 \u2265 ( s2/2 ) \u2212A2, so\nPr {X > s} = Pr {X \u2212A > s\u2212A} \u2264 e\u2212(s\u2212A)2 \u2264 eA2e\u2212s2/2.\n5 Proof (of Theorem 2). The result is trivial if F consists only of constants, so we can assume that L (F ) > 0. For y,y\u2032 \u2208 Y define a function F : Rm \u2192 R by\nF (z) = sup f\u2208F\n\u3008z, f (y) \u2212 f (y\u2032)\u3009 .\nF is Lipschitz with Lipschitz constant bounded by supf\u2208F \u2016f (y) \u2212 f (y\u2032)\u2016 \u2264 L (F ) \u2016y \u2212 y\u2032\u2016. Writing Zy,y\u2032 = F (\u03b3), it then follows from Gaussian concentration (Theorem 4) that\nPr {Zy,y\u2032 > EZy,y\u2032 + s} \u2264 exp (\n\u2212s2\n2L (F ) 2 \u2016y \u2212 y\u2032\u20162\n)\n.\nSince by definition EZy,y\u2032 \u2264 R (F ) \u2016y \u2212 y\u2032\u2016, Lemma 1 gives\nPr {Zy,y\u2032 > s} \u2264 exp ( R (F ) 2\n2L (F ) 2\n)\nexp\n(\n\u2212s2\n4L (F ) 2 \u2016y \u2212 y\u2032\u20162\n)\n.\nNow define a process Xy, indexed by Y , as\nXy = 1\u221a\n2L (F ) sup f\u2208F\n\u3008\u03b3, f (y)\u3009 .\nSince Xy \u2212Xy\u2032 \u2264 Zy,y\u2032/ (\u221a 2L (F ) ) we have\nPr {Xy \u2212Xy\u2032 > s} \u2264 Pr { Zy,y\u2032 > \u221a 2L (F ) s }\n\u2264 exp ( R (F )2\n2L (F ) 2\n)\nexp\n(\n\u2212s2\n2 \u2016y \u2212 y\u2032\u20162\n)\nand by Theorem 3, with K = exp ( R (F ) 2 / ( 2L (F ) 2 )) \u2265 1,\nE sup y\u2208Y (Xy \u2212Xy0) \u2264 C\u2032G (Y ) + C\u2032\u2032D (Y ) R (F )\u221a 2L (F ) .\nMultiplication by \u221a 2L (F ) then gives\nE sup y\u2208Y\n(\nsup f\u2208F \u3008\u03b3, f (y)\u3009 \u2212 sup f\u2208F\n\u3008\u03b3, f (y0)\u3009 ) \u2264 C1L (F )G (Y ) + C2D (Y )R (F )\nwith C1 = \u221a 2C\u2032 and C2 = C \u2032\u2032."}, {"heading": "3 Applications", "text": "We first give some elementary properties of the quantity R (F, Y ) which appears in Theorem 2. Then we apply Theorem 2 to a two layer kernel machine and give a bound for multi-task learning of low-dimensional representations.\n6"}, {"heading": "3.1 Some properties of R (F )", "text": "Recall the definition of R (F, Y ). If Y \u2286 Rnand F consists of functions f : Y \u2192 R m\nR (F, Y ) = sup y,y\u2032\u2208Y, y 6=y\u2032 E sup f\u2208F \u3008\u03b3, f (y)\u2212 f (y\u2032)\u3009 \u2016y \u2212 y\u2032\u2016 .\nR (F ) is itself a supremum of Gaussian averages. For y,y\u2032 \u2208 Y let \u2206F (y,y\u2032) \u2286 R m be the set of quotients\n\u2206F (y,y\u2032) =\n{\nf (y)\u2212 f (y\u2032) \u2016y \u2212 y\u2032\u2016 : f \u2208 F\n}\n.\nIt follows from the definition that R (F, Y ) = supy,y\u2032\u2208Y, y 6=y\u2032 G (\u2206F (y,y \u2032)). We record some simple properties. Recall that for a set S in a real vector space the convex hull Co (S) is defined as\nCo (S) =\n{\nn \u2211\ni=1\n\u03b1izi : n \u2208 N, zi \u2208 S, \u03b1i \u2265 0, \u2211\ni\n\u03b1i = 1\n}\n.\nTheorem 5. Let Y \u2286 Rn and let F and H be classes of functions f : Y \u2192 Rm. Then\n(i) If F \u2286 H then R (F, Y ) \u2264 R (H, Y ). (ii) If Y \u2286 Y \u2032 then R (F, Y ) \u2264 R (F, Y \u2032). (iii) If c \u2265 0 then R (cF, Y ) = cR (F, Y ) . (iv) R (F +H, Y ) \u2264 R (F, Y ) +R (H, Y ). (v) R (F, Y ) = R (Co (F ) , Y ).\n(vi) If Z \u2286 RK and \u03c6 : Z \u2192 Rn has Lipschitz constant L (\u03c6) and the members of F are defined on \u03c6 (Z), then R (F \u25e6 \u03c6, Z) \u2264 L (\u03c6)R (F, \u03c6 (Z)).\n(vii) R (F ) \u2264 L (F ) \u221a 2 ln |F |.\nRemarks:\n1. From (ii) we get R (F, Y ) \u2264 R (F,Rn). In applications where Y = H (x) the quantity R (F,H (x)) is data-dependent, but R (F,Rn) is sometimes easier to bound.\n2. We see that the properties of R (F ) largely parallel the properties of the Gaussian averages themselves, except for the inequality G (\u03c6 (Y )) \u2264 L (\u03c6)G (Y ), for which there doesn\u2019t seem to be an analogous property of R (F ). Instead we have a \u2019backwards\u2019 version of it with (vi) above, with a rather trivial proof below.\n3. Of course (vii) is relevant only when ln |F | is reasonably small and serves the comparison of Theorem 2 to alternative bounds.\nProof. (i)-(iii) are obvious from the definition. (iv) follows from linearity of the inner product and the triangle inequality for the supremum. To see (v) first note that R (F ) \u2264 R (Co (F )) follows from (i), while the reverse inequality follows\n7 from\nsup \u03b1i\u22650, \u2211 \u03b1i=1 sup f1,f2,...\u2208F\n\u2329\n\u03b3, \u2211\ni\n\u03b1ifi (y)\u2212 \u2211\ni\n\u03b1ifi (y \u2032)\n\u232a\n= sup \u03b1i\u22650, \u2211 \u03b1i=1 sup f1,f2,...\u2208F\n\u2211\ni\n\u03b1i \u3008\u03b3, fi (y) \u2212 fi (y\u2032)\u3009\n\u2264 sup \u03b1i\u22650, \u2211 \u03b1i=1\n\u2211\ni\n\u03b1i sup f\u2208F\n\u3008\u03b3, f (y) \u2212 f (y\u2032)\u3009\n= sup f\u2208F\n\u3008\u03b3, f (y)\u2212 f (y\u2032)\u3009 .\nFor (vi) we may chose y and y\u2032 such that \u03c6 (y) 6= \u03c6 (y\u2032), since otherwise both sides of the inequality to be proved are zero. But then\nE sup f\u2208F\u25e6\u03c6 \u3008\u03b3, f (y) \u2212 f (y\u2032)\u3009 \u2016y \u2212 y\u2032\u2016 = \u2016\u03c6 (y) \u2212 \u03c6 (y\u2032)\u2016 \u2016y \u2212 y\u2032\u2016 E supf\u2208F \u3008\u03b3, f (\u03c6 (y))\u2212 f (\u03c6 (y\u2032))\u3009 \u2016\u03c6 (y) \u2212 \u03c6 (y\u2032)\u2016\n\u2264 L (\u03c6)E sup f\u2208F \u3008\u03b3, f (\u03c6 (y))\u2212 f (\u03c6 (y\u2032))\u3009 \u2016\u03c6 (y) \u2212 \u03c6 (y\u2032)\u2016 .\nTo see (vii) note that for every y and y\u2032 and every f \u2208 F it follows from Gaussian concentration (Theorem 4) that\nPr { \u3008\u03b3, f (y) \u2212 f (y\u2032)\u3009 \u2016y \u2212 y\u2032\u2016 > s } \u2264 e\u2212s2/2L2 .\nThe conclusion then follows from standard estimates (e.g. [4], section 2.5)."}, {"heading": "3.2 A double layer kernel machine", "text": "We use the chain rule to bound the complexity of a double-layer kernel machine. The corresponding optimization problem is clearly non-convex and we are not aware of an efficient optimization method. The model is chosen to illustrate the application of Theorem 2. It is defined as follows.\nAssume the data to lie in Rm0 and fix two real numbers \u22061 and B1. On R m0 \u00d7 Rm0 define a (Gaussian radial-basis-function) kernel \u03ba by\n\u03ba (z, z\u2032) = exp\n(\n\u2212\u2016z \u2212 z\u2032\u20162 2\u220621\n)\n, z, z\u2032 \u2208 Rm0 ,\nand let \u03c6 : Rm0 \u2192 H be the associated feature map, where H is the associated RKHS with inner product \u3008., .\u3009H and norm \u2016.\u2016H (for kernel methods see . Now we let H be the class of vector valued functions h : Rm0 \u2192 Rm1 defined by\nH = {\nz \u2208 Rm0 7\u2192 (\u3008w1, \u03c6 (z)\u3009H , ..., \u3008wm1 , \u03c6 (z)\u3009H) : \u2211\nk\n\u2016wk\u20162H \u2264 B21\n}\n.\n8 This can also be written as H = {z \u2208 Rm0 7\u2192 W\u03c6 (z) : \u2016W\u2016HS \u2264 B1}, where \u2016W\u2016HS is the Hilbert-Schmidt norm of an operator W : H \u2192 Rm1 .\nFor the function class F , which we wish to compose with H, we proceed in a similar way, defining an analogous kernel of width \u22062 on R\nm1 \u00d7 Rm1 , a corresponding feature map \u03c8 : Rm1 \u2192 H and a class of real valued functions\nF = {z \u2208 Rm1 7\u2192 \u3008v, \u03c8 (z)\u3009H : \u2016vl\u2016H \u2264 B2} .\nWe now want high probability bounds on the estimation error for functional compositions f \u25e6 h, uniform over F \u25e6 H. To apply our result we should really restrict to finite subsets of F and H a requirement which we simply ignore. In machine learning we could of course always restrict all representations to some fixed, very high but finite precision.\nFix a sample x \u2208 Rnm0 . Then Y = H (x) \u2282 Rnm1 . To use Theorem 2 we define a class F \u2032 of functions from Rnm1 to Rn by\nF \u2032 = {(y1, ..., yn) \u2208 Rnm1 7\u2192 (f (y1) , ..., f (yn)) \u2208 Rn : f \u2208 F} .\nSince the first feature map \u03c6 maps to the unit sphere of H we have\nD (H (x)) \u2264 2B1 \u221a n and\nG (H (x)) = E sup W\n\u2211\nik\n\u03b3ik \u3008wk, \u03c6 (xi)\u3009H \u2264 B1 \u221a nm1.\nThe feature map corresponding to the Gaussian kernel\u22062 has Lipschitz constant \u2206\u221212 . For y,y \u2032 \u2208 Rnm1 we obtain\nsup v\n(\n\u2211\ni\n(\u3008v, \u03c6 (yi)\u3009H \u2212 \u3008v, \u03c6 (y\u2032i)\u3009H) 2\n)1/2\n\u2264 B2 ( \u2211\ni\n\u2016\u03c6 (yi)\u2212 \u03c6 (y\u2032i)\u2016 2 H\n)1/2\n\u2264 B2\u2206\u221212 \u2016y \u2212 y\u2032\u2016 ,\nso we have L (F \u2032,Rnm1) \u2264 B2\u2206\u221212 . On the other hand\nE sup v\n\u2211\ni\n\u03b3i (\u3008v, \u03c6 (yi)\u3009H \u2212 \u3008v, \u03c6 (y\u2032i)\u3009H) \u2264 B2E \u2225 \u2225 \u2225 \u2225\n\u2225\nn \u2211\ni=1\n\u03b3i (\u03c6 (yi)\u2212 \u03c6 (y\u2032i)) \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2264 B2 ( \u2211\ni\n\u2016\u03c6 (yi)\u2212 \u03c6 (y\u2032i)\u2016 2 H\n)1/2\n\u2264 B2\u2206\u221212 \u2016y \u2212 y\u2032\u2016 ,\nso we have R (F \u2032,Rnm1) \u2264 B2\u2206\u221212 . Furthermore\nG (F \u2032 (h0 (x))) \u2264 B2 \u221a n,\nsimilar to the bound for G (H (x)).\n9 For the composite network Theorem 2 gives us the bound\nG (F \u2032 (H (x))) \u2264 C1B1B2\u2206\u221212 \u221a nm1 + 2C2B1B2 \u221a n\u2206\u221212 +B2 \u221a n.\nDividing by n and appealing to Theorem 1 one obtains the uniform bound: with probability at least 1\u2212 \u03b4 we have for every h \u2208 H and every f \u2208 F that\nEf (h (X)) \u2264 1 n \u2211 f (h (Xi)) +\n+\n\u221a\n2\u03c0\nn B2\n(\nB1\u2206 \u22121 2 (C1 \u221a m1 + 2C2) + 1 ) +\n\u221a\n9 ln 2/\u03b4\n2n .\nRemarks. 1. One might object that the result depends heavily on the intermediate dimension m1 so that only a very classical relationship between sample size and dimension is obtained. In this sense our result only works for intermediate representations of rather low dimension. The mapping stages ofH and F however include nonlinear maps to infinite dimensional spaces.\n2. Clearly the above choice of the Gaussian kernel is arbitrary. Any positive semidefinite kernel can be used for the first mapping stage, and the application of the chain rule requires only the Lipschitz property for the second kernel in the definition of F . The Gaussian kernel was only chosen for definiteness.\n3. Similarly the choice of the Hilbert-Schmidt norm as a regularizer for W in the first mapping stage is arbitrary, one could equally use another matrix norm. This would result in different bounds for G (H (x)) and D (H (x)), incurring a different dependency of our bound on m1."}, {"heading": "3.3 Multitask learning", "text": "As a second illustration we modify the above model to accommodate multitask learning [2][3]. Here one observes a T \u00d7n sample x =(xti : 1 \u2264 t \u2264 T, 1 \u2264 i \u2264 n) \u2208 XnT , where (xti : 1 \u2264 i \u2264 n) is the sample observed for the t-th task. We consider a two layer situation where the bottom-layer H consists of functions h : X \u2192 Rm, and the top layer function class is of the form\nFT = { x \u2208 Rm1 7\u2192 f (x) = (f1 (x) , ..., fT (x)) \u2208 RT : ft \u2208 F } ,\nwhere F is some class of functions mapping Rm1 to R. The functions (or representations) of the bottom layer H are optimized for the entire sample, in the top layer each function ft is optimized for the represented data corresponding to the t-th task. In an approach of empirical risk minimization one selects the composed function f\u0302 \u25e6 h\u0302 which minimizes the task-averaged empirical loss\nmin f\u2208Fn,h\u2208H\n1\nnT\nn \u2211\ni=1\nT \u2211\nt=1\nft (h (xit)) .\nWe wish to give a general explanation of the potential benefits of this method over the separate learning of functions from F \u25e6 H, as studied in the previous\n10\nsection. Clearly we must assume that the tasks are related in the sense that the above minimum is small, so any possible benefit can only be a benefit of improved estimation.\nFor the multitask model a result analogous to Theorem 1 is easily obtained (see e.g. [7]). Let X =(Xti) be a vector of independent random variables with values in X , where Xti is iid to Xtj for all ijt, and let Xt be iid to Xti. Then with probability at least 1\u2212 \u03b4 we have for every f \u2208 Fn and every h \u2208 H\n1\nT\n\u2211\nt\nEft (h (Xt)) \u2264 1\nnT\n\u2211\nti\nft (h (Xti)) +\n\u221a 2\u03c0\nnT G ( FT \u25e6 H (X) ) +\n\u221a\n9 ln 2/\u03b4\n2nT .\nHere the left hand side is interpreted as the task averaged risk and\nG ( FT \u25e6 H (x) )\n= E sup f\u2208FT ,h\u2208H\n\u2211\nti\n\u03b3tift (h (xti)) .\nFor a definite example we take H and F as in the previous section and observe that now there is an additional factor T on the sample size. This implies the modified bounds G (H (x)) \u2264 B1 \u221a Tnm1 and D (H (x)) \u2264 2B1 \u221a Tn. Also for y,y\u2032 \u2208 RTnm1 with yti, y\u2032ti \u2208 Rm1\nsup f\u2208FT\n\u2211\nti\n(ft (yti)\u2212 ft (y\u2032ti)) 2 \u2264\n\u2211\nt\nsup f\u2208F\n\u2211\ni\n(ft (yti)\u2212 ft (y\u2032ti)) 2\n\u2264 L2 (F,Rnm1 ) \u2211\nt\n\u2211\ni\n\u2016yti \u2212 y\u2032ti\u2016 2 ,\nso L ( FT ,RTnm1 ) = L (F,Rnm1) . (4)\nTherefore L ( FT ,RTnm1 ) \u2264 B2\u2206\u221212 . Similarly\nE sup f\u2208FT\n\u2211\nti\n\u03b3ti (ft (yti)\u2212 ft (y\u2032ti))\n= \u2211\nt\nE sup f\u2208F\n\u2211\ni\n\u03b3ti (ft (yti)\u2212 ft (y\u2032ti))\n\u2264 \u221a T\n\n\n\u2211\nt\n(\nE sup f\u2208F\n\u2211\ni\n\u03b3ti (ft (yti)\u2212 ft (y\u2032ti)) )2\n\n\n1/2\n\u2264 \u221a T\n(\n\u2211\nt\nR2 (F,Rnm1) \u2211\ni\n\u2016yti \u2212 y\u2032ti\u2016 2\n)1/2\n= \u221a TR (F,Rnm1) \u2016y \u2212 y\u2032\u2016 .\nWe conclude that R ( FT ,RnmT ) \u2264 \u221a TR (F,Rnm) , (5)\nin the given case R ( FT ,RnmT ) \u2264 \u221a TB2\u2206 \u22121 2 .\n11\nAlso\nG ( FT (h0 (x)) )\n= E sup f\u2208FT\n\u2211\nti\n\u03b3tift (h0 (xti))\n= \u2211\nt\nE sup f\u2208F\n\u2211\ni\n\u03b3tif (h0 (xti))\n\u2264 TG (F (h0 (x))) , (6)\nso that here G ( FT (h0 (x)) ) \u2264 B2T \u221a n. The chain rule then gives\nG (F \u25e6 H (x)) \u2264 C1B1B2\u2206\u221212 \u221a Tnm1 + ( 2C2B1\u2206 \u22121 2 + 1 ) B2T \u221a n,\nwhere the first term represents the complexity of H and the second that of FT . Dividing by nT we obtain as the dominant term for the estimation error\nC1B1B2\u2206 \u22121 2\n\u221a\nm1 nT +\n(\n2C2B1\u2206 \u22121 2 + 1\n)\nB2\u221a n .\nThis reproduces a general property of multitask learning [3]: in the limit T \u2192 \u221e the contribution of the common representation (including the intermediate dimension m1) to the estimation error vanishes. There remains only the cost of estimating the task specific functions in the top layer.\nWe have obtained this result for a very specific model. The relations (4), (5) and (6) for L ( FT ) , R ( FT ) and G ( FT (h0 (x)) )\nare nevertheless independent of the exact model, so the chain rule could be made the basis of a fairly general result about multitask feature learning."}, {"heading": "3.4 Iteration of the bound", "text": "We apply the chain rule to multi-layered or \u201ddeep\u201d learning machines, a subject which appears to be of some current interest. Here we have function classes F1, ..., FK , where Fk consists of functions f : R\nnk\u22121 \u2192 Rnk and we are interested in the generalization properties of the composite class\nFK \u25e6 ... \u25e6 F1 = {x \u2208 Rn0 7\u2192 fK (fK\u22121 (... (f1 (x)))) : fk \u2208 Fk} .\nTo state our bound we are given some sample x in Rn0 and introduce the notation\nY0 = x\nYk = Fk (Yk\u22121) = Fk \u25e6 ... \u25e6 F1 (x) \u2286 Rnk , for k > 0 Gk = min\ny\u2208Yk\u22121(x) G (Fk (y)) .\nUnder the convention that the product over an empty index set is 1, induction shows that\nG (YK) \u2264 K \u2211\nk=1\n\nCK\u2212k1\nK \u220f\nj=k+1\nL (Fj)\n\n (C2D (Yk\u22121)R (Fk) +Gk) .\n12\nClearly the large constants are prohibitive for any useful quantitative prediction of generalization, but qualitative statements are possible. Observe for example that, apart from C1 and the Lipschitz constants, each layer only makes an additive contribution to the overall complexity. More specifically, for machine learning with a sample of size n, we can make the assumptions nk = nmk, where mk is the dimension of the k-th intermediate representations, and it is reasonable to postulate max {Gk, D (Yk)R (Fk)} \u2264 Cnp, where C is some constant not depending on n and p is some exponent p < 1 (for multi-layered kernel machines with Lipschitz feature maps we would have p = 1/2 - see above). Then the above expression is of order np and Theorem 1 yields a uniform law of large numbers for the multi-layered class, with a uniform bound on the estimation error decreasing as np\u22121."}, {"heading": "4 Proof of Theorem 3", "text": "Talagrand has proved the following result ([14]).\nTheorem 6. There are universal constants r \u2265 2 and C such that for every finite Y \u2282 Rn there is an increasing sequence of partitions Ak of Y and a probability measure \u00b5 on Y , such that, whenever A \u2208 Ak then D (A) \u2264 2r\u2212k and\nsup y\u2208Y\n\u221e \u2211\nk>k0\nr\u2212k\n\u221a\nln 1\n\u00b5 (Ak (y)) \u2264 C G (Y ) ,\nwhere Ak (y) denotes the unique member of Ak which contains y, and k0 is the largest integer k satisfying\n2r\u2212k \u2265 D (Y ) = sup y,y\u2032\u2208Y \u2016y \u2212 y\u2032\u2016\nObserve that 2r\u2212k0 \u2265 D (Y ), so we can assume Ak0 = {Y }. As explained in [14], the above Theorem is equivalent to the existence of a measure \u00b5 on Y such that\nsup y\u2208Y\n\u222b \u221e\n0\n\u221a\nln 1\n\u00b5 (B (y,\u01eb)) d\u01eb \u2264 C G (Y ) ,\nwhere C is some other universal constant and B (y,\u01eb) is the ball of radius \u01eb centered at y. The latter is perhaps the more usual formulation of the majorizing measure theorem.\nWe will use Talagrand\u2019s theorem to prove Theorem 3, but before please note the inequality\nD (Y ) \u2264 \u221a 2\u03c0G (Y ) , (7)\nwhich follows from\nsup y,y\u2032\u2208Y\n\u2016y \u2212 y\u2032\u2016 = \u221a \u03c0\n2 sup y,y\u2032\nE |\u3008\u03b3,y \u2212 y\u2032\u3009|\n\u2264 \u221a \u03c0\n2 E sup y,y\u2032 |\u3008\u03b3,y \u2212 y\u2032\u3009| =\n\u221a\n\u03c0 2 E sup y,y\u2032 \u3008\u03b3,y \u2212 y\u2032\u3009 .\n13\nIn the first equality we used the fact that \u2016v\u2016 = \u221a \u03c0/2E |\u3008\u03b3, v\u3009| for any vector v.\nProof (of Theorem 3.). Let \u00b5 and Ak be as determined for Y by Theorem 6. First we claim that for any \u03b4 \u2208 (0, 1)\nPr\n{\n\u2203y \u2208 Y : Xy \u2212Xy0 > \u2211\nk>k0\nr\u2212k+1\n\u221a\n8 ln\n(\n2k\u2212k0K\n\u00b5 (A (y)) \u03b4\n)\n}\n< \u03b4. (8)\nFor every k > k0 and every A \u2208 Ak let \u03c0 (A) be some element chosen from A. We set \u03c0 (Y ) = y0. We denote \u03c0k (y) = \u03c0 (Ak (y)). This implies the chaining identity:\nXy \u2212Xy0 = \u2211\nk>k0\n( X\u03c0k(y) \u2212X\u03c0k\u22121(y) ) .\nFor k > k0 and A \u2208 Ak use A\u0302 to denote the unique member of Ak\u22121 such that A \u2286 A\u0302. Since for A \u2208 Ak both \u03c0 (A) and \u03c0 ( A\u0302 )\nare members of A\u0302 \u2208 Ak\u22121 we must have \u2225 \u2225 \u2225\u03c0 (A)\u2212 \u03c0 ( A\u0302 )\u2225 \u2225 \u2225 \u2264 2r\u2212k+1. Also note \u03c0k\u22121 (y) = \u03c0 ( A\u0302k (y) ) = \u03c0 ((Ak (\u03c0k (y))) \u02c6). For k \u2265 k0 we define a function \u03bek : Ak \u2192 R+ as follows:\n\u03bek (A) = r \u2212k+1\n\u221a\n8 ln\n(\n2k\u2212k0K\n\u00b5 (A) \u03b4\n)\n.\nTo prove the claim we have to show that\nPr\n{\n\u2203y \u2208 Y : Xy \u2212Xy0 \u2212 \u2211\nk>k0\n\u03bek (Ak (y)) > 0\n}\n< \u03b4.\nDenote the left hand side of this inequality with P . By the chaining identity\nP \u2264 Pr { \u2203y : \u2211\nk>k0\n( X\u03c0k(y) \u2212X\u03c0k\u22121(y) \u2212 \u03bek (Ak (y)) ) > 0\n}\n.\nIf the sum is positive, at least one of the terms has to be positive, so\nP \u2264 Pr { \u2203y, k > k0 : ( X\u03c0k(y) \u2212X\u03c0k\u22121(y) \u2212 \u03bek (Ak (y)) ) > 0 } .\nThe event on the right hand side can also be written as\n{ \u2203k > k0, \u2203A \u2208 Ak : X\u03c0(A) \u2212X\u03c0(A\u0302) > \u03bek (A) } ,\n14\nand a union bound gives\nP \u2264 \u2211\nk>k0\n\u2211\nA\u2208Ak\nPr { X\u03c0(A) \u2212X\u03c0(A\u0302) > \u03bek (A) }\n\u2264 \u2211\nk>k0\n\u2211\nA\u2208Ak\nK exp\n\n \n\u2212\u03bek (A)2\n2 \u2225 \u2225 \u2225\u03c0 (A)\u2212 \u03c0 ( A\u0302 )\u2225 \u2225 \u2225 2\n\n \n\u2264 \u2211\nk>k0\n\u2211\nA\u2208Ak\nK exp\n(\n\u2212\u03bek (A) 2\n2 (2r\u2212k+1) 2\n)\n,\nwhere we used the bound (3) in the second and the bound on \u2225 \u2225 \u2225\u03c0 (A)\u2212 \u03c0 ( A\u0302 )\u2225 \u2225 \u2225\nin the third inequality. Using the definition of \u03bek (A) the last expression is equal to\n\u03b4 \u2211\nk>k0\n1\n2k\u2212k0\n\u2211\nA\u2208Ak\n\u00b5 (A) = \u03b4 \u2211\nk>k0\n1\n2k\u2212k0 = \u03b4,\nbecause \u00b5 is a probability measure. This establishes the claim. Now, using \u221a a+ b \u2264 \u221aa+ \u221a b for a, b \u2265 0, with probability at least 1\u2212 \u03b4\nsup y\nXy \u2212Xy0 \u2264 r \u2211\nk>k0\nr\u2212k\n\u221a\n8 ln\n(\n1\n\u00b5 (Ak (y))\n)\n+ r\u2212k0+1 \u2211\nk>0\nr\u2212k+1\n\u221a\n8 ln\n(\n2kK\n\u03b4\n)\n\u2264 \u221a 8rC G (Y ) + \u221a 8r\u2212k0+1 \u2211\nk>0\n\u221a kr\u2212k+1\n\u221a\nln\n(\n2K\n\u03b4\n)\n,\nwhere we used Talagrand\u2019s theorem and the fact that K > 1. By the definition of k0 we have r \u2212k0+1 \u2264 r2D (Y ) /2, so this is bounded by\nC\u2032\u2032\u2032G (Y ) + C\u2032\u2032\u2032\u2032D (Y )\n\u221a\nln\n(\n2K\n\u03b4\n)\n,\nwith C\u2032\u2032\u2032 = \u221a 8rC and C\u2032\u2032\u2032\u2032 = \u221a 8 ( r2/2 ) \u2211\nk>0\n\u221a kr\u2212k+1. Converting the last\nbound into a tail bound and integrating we obtain\nE\n[\nsup y\nXy \u2212Xy0 ] \u2264 C\u2032\u2032\u2032G (Y ) + C\u2032\u2032\u2032\u2032D (Y ) (\u221a ln 2K +\n\u221a \u03c0\n2\n)\n\u2264 C\u2032\u2032\u2032G (Y ) + 3C\u2032\u2032\u2032\u2032D (Y ) \u221a ln 2K \u2264 ( C\u2032\u2032\u2032 + 3 \u221a 2\u03c0 ln 2C\u2032\u2032\u2032\u2032 ) G (Y ) + 3C\u2032\u2032\u2032\u2032D (Y ) \u221a lnK,\nwhere we again usedK \u2265 1 in the second inequality and (7) in the last inequality. This gives the conclusion with C\u2032 = C\u2032\u2032\u2032 + 3 \u221a 2\u03c0 ln 2C\u2032\u2032\u2032\u2032 and C\u2032\u2032 = 3C\u2032\u2032\u2032\u2032.\n15"}], "references": [{"title": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3: 463\u2013 482,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Theoretical Models of Learning to Learn, in Learning to Learn, S.Thrun, L.Pratt Eds", "author": ["J. Baxter"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "A Model of Inductive Bias Learning", "author": ["J. Baxter"], "venue": "Journal of Artificial Intelligence Research 12:149\u2013198,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Concentration Inequalities", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Rademacher processes and bounding the risk of function learning", "author": ["V.I. Koltchinskii", "D. Panchenko"], "venue": "In E. Gine, D. Mason, and J. Wellner, editors, High Dimensional Probability II, pages 443\u2013459.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "M", "author": ["M. Ledoux"], "venue": "Talagrand. Probability in Banach Spaces, Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Bounds for linear multi-task learning", "author": ["A. Maurer"], "venue": "Journal of Machine Learning Research, 7:117\u2013139,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Transfer bounds for linear feature learning", "author": ["A. Maurer"], "venue": "Machine Learning, 75(3): 327\u2013350,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "K-dimensional coding schemes in Hilbert spaces", "author": ["A. Maurer", "M. Pontil"], "venue": "IEEE Transactions on Information Theory, 56(11): 5839\u20135846,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalization error bounds for Bayesian mixture algorithms", "author": ["R. Meir", "T. Zhang"], "venue": "Journal of Machine Learning Research, 4: 839\u2013860,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "l -norm and its application to learning theory", "author": ["S. Mendelson"], "venue": "Positivity, 5:177\u2013191,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Regularity of Gaussian processes", "author": ["M. Talagrand"], "venue": "Acta Mathematica. 159: 99\u2013149,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1987}, {"title": "A simple proof of the majorizing measure theorem", "author": ["M. Talagrand"], "venue": "Geometric and Functional Analysis. Vol 2, No.1: 118\u2013125,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "Majorizing measures without measures", "author": ["M. Talagrand"], "venue": "Ann. Probab. 29: 411\u2013417,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The Generic Chaining", "author": ["M. Talagrand"], "venue": "Upper and Lower Bounds for Stochastic Processes. Springer, Berlin,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Then we have the following result [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "Let the members of F take values in [0, 1] and let X,X1, .", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "2 The utility of Gaussian averages is not limited to functions with values in [0, 1].", "startOffset": 78, "endOffset": 84}, {"referenceID": 5, "context": "For real functions \u03c6 with Lipschitz constant L (\u03c6) we have G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)) (see also Slepian\u2019s Lemma, [6], [4]), where \u03c6 \u25e6 F is the class {x 7\u2192 \u03c6 (f (x)) : f \u2208 F}.", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "For real functions \u03c6 with Lipschitz constant L (\u03c6) we have G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)) (see also Slepian\u2019s Lemma, [6], [4]), where \u03c6 \u25e6 F is the class {x 7\u2192 \u03c6 (f (x)) : f \u2208 F}.", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "The inequality G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)), which in the above form holds also for Rademacher averages [10], is extremely useful and in part responsible for the success of these complexity measures.", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "This is a direct consequence of Slepian\u2019s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "This is a direct consequence of Slepian\u2019s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 173, "endOffset": 176}, {"referenceID": 13, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 15, "context": "This is obtained from Talagrand\u2019s majorizing measure theorem (Theorem 6 below) combined with generic chaining [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "An early version of a similar result is Theorem 15 in [13], where the author remarks that his method of proof (which we also use) is very indirect, and that a more direct proof would be desirable.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "6 in [4]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[4], section 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "3 Multitask learning As a second illustration we modify the above model to accommodate multitask learning [2][3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "3 Multitask learning As a second illustration we modify the above model to accommodate multitask learning [2][3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "This reproduces a general property of multitask learning [3]: in the limit T \u2192 \u221e the contribution of the common representation (including the intermediate dimension m1) to the estimation error vanishes.", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "4 Proof of Theorem 3 Talagrand has proved the following result ([14]).", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "As explained in [14], the above Theorem is equivalent to the existence of a measure \u03bc on Y such that sup y\u2208Y \u222b \u221e 0 \u221a", "startOffset": 16, "endOffset": 20}], "year": 2014, "abstractText": "The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models.", "creator": "LaTeX with hyperref package"}}}