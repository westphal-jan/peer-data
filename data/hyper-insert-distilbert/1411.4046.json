{"id": "1411.4046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2014", "title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy", "abstract": "nowadays this theorem is very popular technique to use specific deep analytical architectures applications in machine powered learning. synthetic deep belief networks ( dbns ) are common deep architectures model that might use stack of restricted boltzmann machines ( termed rbm ) architecture to create a powerful generative model using training data. in building this paper we present an improvement presented in a somewhat common sorting method that is often usually used early in training samples of rbms. the new differentiation method method uses free energy as a simplified criterion to obtain elite samples estimated from generative simulations model. we argue that modelling these samples can more economically accurately theoretically compute gradient of log probability measures of training quantitative data. adding according to including the results, an interval error rate achieved of low 0. 99 % was often achieved on mnist test set. combining this promising result shows that the proposed method nearly outperforms giving the unification method is presented in the the first detailed paper introducing dbn ( 1. 65 25 % error rate ) and general classification methods concepts such similar as new svm ( 1. 4 % normal error rate ) method and knn ( with usually 1. 40 6 % error rate ). appearing in another numerical test using isolet dataset, simple letter rank classification error usually dropped estimated to 3. 86 59 % compared overall to 5. 59 % differential error rate achieved showed in those papers using this dataset. nowadays the widely implemented generic method text is easily available online below at \"", "histories": [["v1", "Fri, 14 Nov 2014 16:57:48 GMT  (715kb)", "http://arxiv.org/abs/1411.4046v1", "18 pages. arXiv admin note: substantial text overlap witharXiv:1408.3264"]], "COMMENTS": "18 pages. arXiv admin note: substantial text overlap witharXiv:1408.3264", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mohammad ali keyvanrad", "mohammad mehdi homayounpour"], "accepted": false, "id": "1411.4046"}, "pdf": {"name": "1411.4046.pdf", "metadata": {"source": "CRF", "title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy", "authors": ["Mohammad Ali Keyvanrad", "Mohammad Mehdi Homayounpour"], "emails": ["homayoun}@aut.ac.ir"], "sections": [{"heading": null, "text": "Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. In this paper we present an improvement in a common method that is usually used in training of RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can more accurately compute gradient of log probability of training data. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in those papers using this dataset. The implemented method is available online at \u201chttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html\u201d.\nKeywords: Deep Belief Network, Restricted Boltzmann Machine, Gibbs sampling, Contrastive Divergence (CD), Persistent Contrastive Divergence (PCD), Free energy"}, {"heading": "1. Introduction", "text": "Since many years ago, artificial neural networks have been used in artificial intelligence applications. Pattern recognition, voice and speech analysis and natural language processing are some of these applications that use artificial neural networks. Due to some theoretical and biological reasons, deep models and architectures with many nonlinear processing layers were suggested. These deep models have many layers and parameters that must be learnt. When the learning process is so complicated and a huge number of parameters are needed, artificial neural networks are rarely used. Problem of this number of\nlayers is that training is time consuming and training becomes trapped at local minima. Therefore we can\u2019t achieve acceptable results. One important tool for dealing with this problem is to use DBNs (Deep Neural Network) that can create neural networks including many hidden layers [1]. Deep Belief Networks can be used in classification and feature learning. Data representation is very important in machine learning. Therefore much work has been done for feature preprocessing, feature extraction and feature learning. In feature learning, we can create a feature extraction system and then use the extracted features in classification and other applications. Using unlabeled data in high level feature extraction [2] and also increasing discrimination between extracted features are the benefits of DBN for feature learning [3]. Layers of DBN are created from Restricted Boltzmann Machine (RBM) that is a generative and undirected probabilistic model. RBMs use a hidden layer to model the probability distribution of visible variables. Indeed we can create a DBN for hierarchical processing using stacking RBMs. Therefore most of improvements in DBNs are due to improvement in RBMs. This paper studies improvement in computing gradient of log probability of training data to train RBM model. Hinton presented DBNs and used it in the task of digit recognition on MNIST data set [4]. He used a DBN with 784-500-500-2000-10 structure, where the first layer possesses 784 features from 28*28 MNIST digit images. The last layer is related to 10 digit labels and other three layers are hidden layers with stochastic binary neurons. Finally this paper achieved 1.25% classification error rate on MNIST test data set. In another paper from this author [3], he used DBN as a nonlinear model for feature extraction and dimension reduction. Indeed the DBN may be considered as a model that can generate features in its last layer with the ability to reconstruct visible data from generated features. When a general Neural Network is used with many layers, the Neural Network becomes trapped in local minima and the performance will decrease. Therefore determining the initial values for NN weights is critical. Another paper proposed DDBN (Discriminative Deep Belief Network) is based on DBN as a new classifier [1]. This paper showed the power of DBN in using unlabeled data and also performance improvement by increasing layers (even by 50 hidden layers).\nDBN applications are not limited to image processing and can be used in voice processing [5]\u2013[8] with significant efficiency. Most of RBM improvements in this paper are related to model learning. Also the idea of this paper is improvement in computing the gradient of log probability of training data. In this new method, elite samples are obtained from DBN model using free energy, so gradient will be computed more accurately. According to the results, performance and training runtime are comparable with other sampling methods such as CD and PCD. The rest of this paper is organized as follows: in section 2, RBM and DBN are described. FEPCD (Free Energy in Persistent Contrastive Divergence) that is the proposed method in this paper is presented in section 3. In section 4, some experiments are conducted and the proposed method is compared to some other methods such as CD and PCD in the tasks of digit recognition on MNIST data set and prediction of which letter-name was spoken on ISOLET dataset. Finally, section 5 concludes the paper."}, {"heading": "2. Deep Belief Networks (DBNs) and Restricted", "text": "Boltzmann Machines (RBMs)\nDBNs are composed of multiple layers of RBMs. RBM is a Boltzmann machine where the connections between hidden visible layers are disjointed. Also the Boltzmann machine is an undirected graphical model (or Markov Random Field). In the Following section, the RBMs and some revised version of RBMs are discussed. It is explained how DBNs are constructed using Restricted Boltzmann Machines (RBMs). The Boltzmann Machine is a type of MRF. The Boltzmann Machine is a concurrent network with stochastic binary units. The network has a set of visible units \ud835\udc63 \u2208 {0,1}\ud835\udc54\ud835\udc63 and a set of hidden units \u210e \u2208 {0,1}\ud835\udc54\u210e where \ud835\udc54\ud835\udc63 and \ud835\udc54\u210e are the number of visible units and the number of hidden units respectively (left figure in Figure 1). The energy of the joint configuration {v, h} in Boltzmann machine is given as follows:\n(1) \ud835\udc38(\ud835\udc63, \u210e) = \u2212 1\n2 \ud835\udc63\ud835\udc47\ud835\udc3f\ud835\udc63 \u2212\n1 2 \u210e\ud835\udc47\ud835\udc3d\u210e \u2212 \ud835\udc63\ud835\udc47\ud835\udc4a\u210e\nThe bias is removed for simplicity of presentation. The term \ud835\udc4a is the concurrent weights between visible and hidden units, \ud835\udc3f is the concurrent weights between\nvisible and visible units and finally \ud835\udc3d is the concurrent weights between hidden and hidden units. Diagonal values of \ud835\udc3f and \ud835\udc3d are zero. Since Boltzmann machines have a complicated theory and formulations, therefore Restricted Boltzmann Machines are used for simplicity. If \ud835\udc3d = 0 and \ud835\udc3f = 0, the famous RBM model is introduced (the right hand figure in Figure 1).\nThe energy of the joint configuration {v, h} in restricted Boltzmann machine, with respect to adding bias is given by:\n(2)\n\ud835\udc38(\ud835\udc63, \u210e) = \u2212\ud835\udc63\ud835\udc47\ud835\udc4a\u210e \u2212 \ud835\udc4e\ud835\udc47\ud835\udc63 \u2212 \ud835\udc4f\ud835\udc47\u210e\n= \u2212 \u2211 \u2211 \ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc63\ud835\udc56\u210e\ud835\udc57 \u2212 \u2211 \ud835\udc4e\ud835\udc56\ud835\udc63\ud835\udc56 \u2212 \u2211 \ud835\udc4f\ud835\udc57\u210e\ud835\udc57\n\ud835\udc54\u210e\n\ud835\udc57=1\n\ud835\udc54\ud835\udc63\n\ud835\udc56=1\n\ud835\udc54\u210e\n\ud835\udc57=1\n\ud835\udc54\ud835\udc63\n\ud835\udc56=1\nWhere \ud835\udc4a\ud835\udc56\ud835\udc57 represents the symmetric interaction term between visible unit \ud835\udc56 and hidden unit \ud835\udc57, while \ud835\udc4f\ud835\udc56 and \ud835\udc4e\ud835\udc57 are bias terms for hidden units and visible units respectively. The network assigns a probability value with energy function to each state in visible and hidden units. Because potential functions in MRFs are strictly positive, it is convenient to express them as exponential and Boltzmann distribution [10]. The joint distribution is defined as the product of potentials, and so the total energy is obtained by adding the energies for potential functions. Therefore joint probability distribution for visible and hidden units can be defined as:\n(3) \ud835\udc43(\ud835\udc63, \u210e) = 1\n\ud835\udc4d exp(\u2212\ud835\udc38(\ud835\udc63, \u210e))\nWhere \ud835\udc4d as partition function or normalization constant, is obtained by summing over all possible pairs of visible and hidden vectors.\n(4) \ud835\udc4d = \u2211 \u2211 exp(\u2212\ud835\udc38(\ud835\udc63, \u210e))\n\u210e\ud835\udc63\nThe probability assigned to a visible vector \ud835\udc63 by the network, is obtained by marginalizing out hidden vector \u210e.\n(5) \ud835\udc43(\ud835\udc63) = \u2211 \ud835\udc43(\ud835\udc63, \u210e)\n\u210e\n= 1\n\ud835\udc4d \u2211 exp(\u2212\ud835\udc38(\ud835\udc63, \u210e))\n\u210e\nThe probability that the network assigns to a training image can be increased by adjusting the weights and biases to lower the energy of that image and to raise the energy of other images, especially those images that have low energies and therefore make a big contribution to the partition function [11]. Therefore, best value for each parameter can be found using the following objective function:\n(6) \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc67\ud835\udc52{\ud835\udc64\ud835\udc56\ud835\udc57,\ud835\udc4e\ud835\udc56,\ud835\udc4f\ud835\udc57} 1 \ud835\udc5a \u2211 log (\u2211 \ud835\udc43(\ud835\udc97(\ud835\udc59), \ud835\udc89(\ud835\udc59))\n\u210e\n)\n\ud835\udc5a\n\ud835\udc59=1\nWhere the parameter \ud835\udc5a is the number of training data samples and the aim is to increase the model probability for these training data. Therefore the partial derivative with respect to \ud835\udc64\ud835\udc56\ud835\udc57 of the above objective is given by [12] :\n(7)\n\ud835\udf15\n\ud835\udf15\ud835\udc64\ud835\udc56\ud835\udc57 (\n1 \ud835\udc5a \u2211 log (\u2211 \ud835\udc43(\ud835\udc97(\ud835\udc59), \ud835\udc89(\ud835\udc59))\n\u210e\n)\n\ud835\udc5a\n\ud835\udc59=1\n)\n= 1\n\ud835\udc5a \u2211 \u2211 \ud835\udc4b\ud835\udc56\ud835\udc59\u210e\ud835\udc57\ud835\udc43(\u210e|\ud835\udc63 = \ud835\udc65)\n\u210e\n\ud835\udc5a\n\ud835\udc59=1\n\u2212 \u2211 \u2211 \ud835\udc63\ud835\udc56 \u2032\u210e\ud835\udc57 \u2032\ud835\udc43(\ud835\udc63\u2032, \u210e\u2032)\n\u210e\u2032\ud835\udc63\u2032\nWhere \ud835\udc4b\ud835\udc56\ud835\udc59 refers to the \ud835\udc56 \ud835\udc61\u210e unit of the \ud835\udc59\ud835\udc61\u210e data instance. The sum on the left hand side can be computed exactly; however the expectation on the right hand side (also called the expectation under the model distribution) is intractable. Therefore other methods are used to estimate this partial derivative. The derivative of the log probability of a training vector with respect to a weight can be computed as follows:\n(8) \u2212 \ud835\udf15 log \ud835\udc43(\ud835\udc63)\n\ud835\udf15\ud835\udc64\ud835\udc56\ud835\udc57 = < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e \u2212< \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59\nWhere the angle brackets are used to denote expectations under the distribution specified by the subscript that follows. This leads to a very simple learning rule for performing stochastic steepest ascent in the log probability of the training data:\n(9) \u2206\ud835\udc64\ud835\udc56\ud835\udc57 = \ud835\udf16 (< \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e \u2212< \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59)\nWhere \ud835\udf16 parameter is a learning rate. Similarly the learning rule for the bias parameters is:\n(10) \u2206\ud835\udc4e\ud835\udc56 = \ud835\udf16 (< \ud835\udc63\ud835\udc56 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e \u2212< \ud835\udc63\ud835\udc56 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59)\n(11) \u2206\ud835\udc4f\ud835\udc57 = \ud835\udf16 (< \u210e\ud835\udc57 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e \u2212< \u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59)\nSince there are no direct connections between hidden units in an RBM, these hidden units are independent given visible units [11]. This fact is based on MRF properties [10]. Now Given a randomly selected training image \ud835\udc63, the binary state \u210e\ud835\udc57 of each hidden unit \ud835\udc57, is set to 1 where its probability is:\n(12) \ud835\udc43(\u210e\ud835\udc57 = 1|\ud835\udc97) = \u210a (\ud835\udc4f\ud835\udc57 + \u2211 \ud835\udc63\ud835\udc56\ud835\udc64\ud835\udc56\ud835\udc57 \ud835\udc56 )\nWhere \u210a(\ud835\udc65) is the logistic sigmoid function \u210a(\ud835\udc65) = 1/(1 + exp (\u2212\ud835\udc65)). Therefore <\n\ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e can be computed easily. Since there are no direct connections between visible units in an RBM, it is very easy to obtain an unbiased sample of the state of a visible unit, given a hidden vector\n(13) \ud835\udc43(\ud835\udc63\ud835\udc56 = 1|\ud835\udc89) = \u210a (\ud835\udc4e\ud835\udc56 + \u2211 \u210e\ud835\udc57\ud835\udc64\ud835\udc56\ud835\udc57 \ud835\udc57 )\nHowever computing < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 is so difficult. It can be done by starting from any random state of the visible units and performing sequential Gibbs sampling for a long time. Finally due to impossibility of this method and large run-times, Contrastive Divergence (CD) method is used [13]. RBM has many benefits and has been greatly used in recent years, especially in DBN\u2019s. Nowadays many papers wish to improve this model and its performance. In the following section these improvements on computing gradient of log probability of train data are discussed."}, {"heading": "2.1. Computing gradient of log probability of train data", "text": "According to equation (5), the log \ud835\udc43(\ud835\udc63) can be expressed as follows [14]:\n(14)\n\ud835\udf19 = log \ud835\udc43(\ud835\udc63) = \ud835\udf19+ \u2212 \ud835\udf19\u2212\n\ud835\udf19+ = log \u2211 exp(\u2212\ud835\udc38(\ud835\udc63, \u210e))\n\u210e\n\ud835\udf19\u2212 = log \ud835\udc4d = log \u2211 \u2211 exp(\u2212\ud835\udc38(\ud835\udc63, \u210e))\n\u210e\ud835\udc63\nThe gradient of \ud835\udf19+ according to model parameters is a positive gradient and similarly, the gradient of \ud835\udf19\u2212 according to model parameters is a negative gradient.\n(15)\n\ud835\udf15\ud835\udf19+ \ud835\udf15\ud835\udc64\ud835\udc56\ud835\udc57 = \ud835\udc63\ud835\udc56. \ud835\udc43(\u210e\ud835\udc57 = 1|\ud835\udc63)\n\ud835\udf15\ud835\udf19\u2212 \ud835\udf15\ud835\udc64\ud835\udc56\ud835\udc57 = \ud835\udc43(\ud835\udc63\ud835\udc56 = 1, \u210e\ud835\udc57 = 1)\nComputing the positive gradient is simple but computing the negative gradient is intractable and therefore inference methods using sampling are used to compute gradient. Based on the above sections, the gradient of log probability of training data is obtained from equation (8). We must compute < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e and < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 for computing gradient and adjusting parameters according to equation (9). Based on most of the literatures on RBMs, computing < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e is called positive phase, and computing < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 is called negative phase corresponding to positive gradient and negative gradient respectively.\nSince there is no interconnections between hidden units and they are independent, < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e can easily be computed by considering the visible units \ud835\udc63 (that their values have been determined by training data) and assigning the value 1 to each\nhidden unit with the probability of \ud835\udc43(\u210e\ud835\udc57 = 1|\ud835\udc63) regarding to equation (12). The main problem resides in the negative phase. In practice, the difference between different DBN learning methods (e.g. Contrastive Divergence or Persistent Contrastive Divergence) is in sampling in their negative phase [15]. To compute < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 , Gibbs sampling method may be used. This method starts with random values in visible units and Gibbs sampling steps should continue for a long time. Each Gibbs sampling step leads to updating of all hidden units according to equation (12) and then updating all visible units according to equation (13) (see Figure 2). Indeed, Gibbs sampling is a method for obtaining a good sample from joint distribution on \ud835\udc63 and \u210e in this model."}, {"heading": "2.1.1. Contrastive Divergence (CD)", "text": "Since Gibbs sampling method is slow, Contrastive Divergence (CD) algorithm is used [13]. In this method visible units are initialized using training data. Then binary hidden units are computed according to equation (12). After determining binary hidden unit states, \ud835\udc63\ud835\udc56 values are recomputed according to equation (13). Finally, probability of hidden unit activations is computed and using these values of hidden units and visible units, < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 is computed. The computation steps in CD1 method is graphically illustrated in Figure 3.\nAlthough CD1 method is not a perfect gradient computation method, but its\nresults are acceptable [13]. By repeating Gibbs sampling steps, CDk method is achieved. The k parameter is the number of repetitions of Gibbs sampling steps. This method has a higher performance and can compute gradient more exactly [16]."}, {"heading": "2.1.2. Persistent Contrastive Divergence (PCD)", "text": "Whereas CDk has some disadvantages and is not exact, other methods are proposed in RBM. One of these methods is PCD that is very popular [17]. Unlike CD method that uses training data as initial value for visible units, PCD method uses last chain state in the last update step. In other words, PCD uses successive Gibbs sampling runs to estimate < \ud835\udc63\ud835\udc56\u210e\ud835\udc57 >\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59. Although all model parameters are changed in each step, but can receive good samples from model distribution with a few Gibbs sampling steps because the model parameters change slightly [18]. Many persistent chains can be run in parallel and we will refer to the current state in each of these chains as new sample or a \u201cfantasy\u201d particle [9], [17]. Improvement in PCD method is the novelty of this paper that will be described in the section 3."}, {"heading": "2.2. Deep Belief Network", "text": "After an RBM has been learned, the activities of its hidden units (when they are being driven by data) can be used as the \u2018data\u2019 for learning a higher-level RBM [19]. The idea behind DBN is to allow each RBM model in the sequence to receive a different representation of the data. The model performs a nonlinear transformation on its input vectors and produces as output, the vectors that will be used as input for the next model in the sequence [4]."}, {"heading": "3. Free Energy in Persistent Contrastive", "text": "Divergence (FEPCD)\nOne of the main challenges in RBMs is training of their parameters. As discussed before, computing gradient of model is intractable; therefore sampling methods are used for gradient estimation. Sampling methods are used because gradient estimation needs samples from the model that has been trained. Since in an RBM each unit in a layer is independent from other units in other layers, therefore Gibbs sampling is a proper method. But in order to obtain appropriate samples from the model, Gibbs sampling needs to be run for many times and this is impossible. Therefore different methods as CD or PCD have been proposed. In this paper a new method for generating elite samples as described later has been proposed. In PCD method, as described before, many persistent chains can be run in parallel and we will refer to the current state in each of these chains as a \u201cfantasy\u201d particle. Chain selection in this method is blind and the best one may not be selected. If we can define a criterion for goodness of a chain, samples and therefore computing gradient will be more accurate.\nThe proposed criterion for selecting the best chain is the free energy of visible sample \ud835\udc63 which is defined as follows [11]:\n(16) \ud835\udc43(\ud835\udc63) = 1\n\ud835\udc4d \ud835\udc52\u2212\ud835\udc39(\ud835\udc63) =\n1 Z \u2211 \ud835\udc52\u2212\ud835\udc38(\ud835\udc63,\u210e)\n\u210e\nwhere \ud835\udc39(\ud835\udc63) is free energy. Therefore \ud835\udc39(\ud835\udc63) can be computed as follows [11]:\n(17)\n\ud835\udc39(\ud835\udc63) = \u2212 \u2211 \ud835\udc63\ud835\udc56\ud835\udc4e\ud835\udc56 \ud835\udc56 \u2212 \u2211 \ud835\udc5e\ud835\udc57\ud835\udc3c\ud835\udc57 \ud835\udc57\n+ \u2211(\ud835\udc5e\ud835\udc57 log \ud835\udc5e\ud835\udc57 + (1 \u2212 \ud835\udc5e\ud835\udc57) log(1 \u2212 \ud835\udc5e\ud835\udc57))\n\ud835\udc57\nWhere \ud835\udc3c\ud835\udc57 = \ud835\udc4f\ud835\udc57 + \u2211 \ud835\udc63\ud835\udc56\ud835\udc64\ud835\udc56\ud835\udc57\ud835\udc56 is equal to sum of inputs to hidden unit \ud835\udc57 and \ud835\udc5e\ud835\udc57 = \u210a(\ud835\udc3c\ud835\udc57) is equal to activation probability of hidden unit \u210e\ud835\udc57 given \ud835\udc63 and \u210a is logistic function. An equivalent and simpler equation for computing \ud835\udc39(\ud835\udc63) is as follows:\n(18) \ud835\udc39(\ud835\udc63) = \u2212 \u2211 \ud835\udc63\ud835\udc56\ud835\udc4e\ud835\udc56 \ud835\udc56\n\u2212 \u2211 log(1 + \ud835\udc52\ud835\udc3c\ud835\udc57)\n\ud835\udc57\nTo understand the criterion benefit, we must describe the training phase in more details. According to equation (3) and (5) , \ud835\udc43(\ud835\udc63) can be defined as follows:\n(19) \ud835\udc43(\ud835\udc63, \u210e) = e\u2212E(v,h)\n\u2211 e\u2212E(v \u2032,h\u2032) \ud835\udc63\u2032,\u210e\u2032 \u2192 \ud835\udc43(\ud835\udc63) =\n\u2211 e\u2212E(v,h)\u210e\n\u2211 e\u2212E(v \u2032,h\u2032)\n\ud835\udc63\u2032,\u210e\u2032\nThus the derivative of \ud835\udc43(\ud835\udc63) according to any parameter \u03b8 is as follows [16]:\n(20)\n\ud835\udf15 log \ud835\udc43(\ud835\udc63)\n\u2202\u03b8 = \ud835\udf15 log \u2211 e\u2212E(v,h)\u210e \u2202\u03b8 \u2212 \ud835\udf15 log \u2211 e\u2212E(v \u2032,h\u2032) \ud835\udc63\u2032,\u210e\u2032 \u2202\u03b8\n= \u2212 1\n\u2211 e\u2212E(v,h)\u210e \u2211 e\u2212E(v,h)\n\u210e\n\ud835\udf15\ud835\udc38(\ud835\udc63, \u210e)\n\u2202\u03b8\n+ 1\n\u2211 e\u2212E(v \u2032,h\u2032) \ud835\udc63\u2032,\u210e\u2032 \u2211 e\u2212E(\ud835\udc63\n\u2032,\u210e\u2032)\n\ud835\udc63\u2032,\u210e\u2032\n\ud835\udf15\ud835\udc38(\ud835\udc63\u2032, \u210e\u2032)\n\u2202\u03b8\n= \u2212 \u2211 \ud835\udc43(\u210e|\ud835\udc63) \ud835\udf15\ud835\udc38(\ud835\udc63, \u210e)\n\u2202\u03b8 \u210e\n+ \u2211 \ud835\udc43(\ud835\udc63\u2032, \u210e\u2032) \ud835\udf15\ud835\udc38(\ud835\udc63\u2032, \u210e\u2032)\n\u2202\u03b8 \ud835\udc63\u2032,\u210e\u2032\nAccording to equation (2), the energy function is very simple and \ud835\udf15\ud835\udc38(\ud835\udc63,\u210e)\n\u2202\u03b8 can be\neasily computed for any parameter \u03b8. In general, calculating expectation of arbitrary function \ud835\udc53 ( \ud835\udd3c(\ud835\udc53) = \u2211 \ud835\udc43(\ud835\udc67)\ud835\udc53(\ud835\udc67)\ud835\udc67 ) can be computed using sampling methods [10] (sampling \ud835\udc67 from \ud835\udc43(\ud835\udc67) and average on \ud835\udc53(\ud835\udc67), \ud835\udd3c(\ud835\udc53) = 1\n\ud835\udc5b \u2211 \ud835\udc53(\ud835\udc67)\ud835\udc5b\ud835\udc56=1 ).\nSo in equation (20), we need to sample from \ud835\udc43(\u210e|\ud835\udc63) and \ud835\udc43(\ud835\udc63\u2032, \u210e\u2032). In sampling for \ud835\udc43(\u210e|\ud835\udc63), \ud835\udc63 is clamped to the visible input vector, and we sample \u210e given \ud835\udc65. But sampling for \ud835\udc43(\ud835\udc63\u2032, \u210e\u2032) is more difficult and both \ud835\udc63 and \u210e are sampled. Due to this difficulty, Gibbs sampling method is used. Based on earlier description, Gibbs sampling is not practicable and other methods like CD or PCD are used instead. In these methods, sampling runs a few steps and therefore samples do not correspond to the model distribution exactly. Therefore in our FEPCD method, we try to find those better chains in PCD method that have more similarity to model distribution (or have greater \ud835\udc43(\ud835\udc63) ). By using these selected samples, gradient of \ud835\udc43(\ud835\udc63) will be more accurate and the error of samples obtained from the model will reduce. Finally we need a criterion that depicts the sampling chain goodness. This criterion can be \ud835\udc43(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) where \ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b are the samples generated from the model using sampling chains. Samples with greater \ud835\udc43(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) are then selected as elite samples obtained from the model. But computing \ud835\udc43(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) is intractable and therefore another criterion was proposed. Since the parameters of model are fixed for all generated samples, according to equation (16), the partition function Z is\nthe same for these samples and the \ud835\udc43(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) is only related to \ud835\udc39(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b). Therefore the generated samples with lower \ud835\udc39(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) are the best model samples in equation (20).\n(21) {\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b \u2208 \ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60 \u2236 \ud835\udc43(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) =\n1 \ud835\udc4d \ud835\udc52\u2212\ud835\udc39(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) > \ud835\udeff}\n= {\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b \u2208 \ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc60 \u2236 \ud835\udc39(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) < \ud835\udeff \u2032}\nwhere \ud835\udeff\u2032 is the threshold to determine good samples. In our experiments we select\nhalf of generated data with lower \ud835\udc39(\ud835\udc63\ud835\udc54\ud835\udc52\ud835\udc5b) as the elite samples and therefore computing the threshold is not necessary."}, {"heading": "4. Results", "text": "The method proposed in this paper was evaluated by applying it to the MNIST and ISOLET dataset. Also we used the DeeBNet toolbox [20] (the implemented toolbox with authors) in the experiments. In addition, our FEPCD method has been implemented and is available online1."}, {"heading": "4.1. MNIST dataset", "text": "MNIST dataset includes images of handwritten digits [21] (10 classes of digits 0- 9). Each digit was cared to be located in the center of each 28*28 image. The image pixels have discrete values between 0 and 255 that most of them have the values at the edge of this interval [22]. The image pixel values were normalized between 0 and 1. The dataset was divided to train and test parts including 60,000 and 10,000 images respectively2. In our experiments, these discrete values have been mapped to interval [0-1] using min-max normalization method. In the first experiment a discriminative RBM has been used. Structure of this RBM is 784-500. In other words this RBM has 784 visible units (images has 28*28 pixels) and 500 binary hidden units. Classification is done by computing \ud835\udc43(\ud835\udc66|\ud835\udc97) in each class [11], [23]. The results are presented in Table 1. The results have been obtained using 10 separate runs on MNIST.\nAccording to Table 1, the proposed FEPCD sampling method is more appropriate relating to other sampling methods such as CD or PCD. These results show that gradient is computed using better and more accurate samples. Another aspect to be considered is the training speed in each method. Naturally the FEPCD is more slowly than other methods since in this method we must\n1 Available online at \u201chttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html\u201d 2 Available online at \u201chttp://yann.lecun.com/exdb/mnist/\u201d\ncompute free energy that is partly time consuming. Since training is offline, so a long training time may be acceptable, but according to Figure 5 when PCD gets to its best results (after second 500), the FEPCD method reaches to its best results in similar time with fewer epochs. This figure shows that FEPCD is slower in each epoch, but it converges in fewer epochs, and its training time is close to that of CD and PCD methods.\nFinally in the main experiment, a DBN with an structure of 784-500-500-2000, similar to the structure proposed by Hinton [4] was trained. As described before, in many DBN papers, after training a DBN, it can be fine-tuned using BackPropagation (BP) method [1], [3]. The results are given in Table 2. In these results, each RBM in DBN (before back-propagation) was trained in 50 epochs. Then the DBN was fine-tuned in 200 epochs using back-propagation method.\nAccording to Table 2, FEPCD, can improve performance before and after using back-propagation. In another test, each RBM was trained in 200 epochs and the best result was obtained. In this experiment the test error rate decreased to 0.0099. These results are better than the best results obtained in Hinton [4] (0.0125,error rate), and other classification methods such as NN (0.016 error rate) or SVM (0.014 error rate) [3]."}, {"heading": "4.2. ISOLET dataset", "text": "In ISOLET data set, 150 subjects utter twice the name of each letter of the alphabet. Hence, there are 52 training examples from each speaker. The speakers are grouped into sets of 30 speakers each, and are referred to as isolet1, isolet2, isolet3, isolet4, and isolet5. The data appears in isolet1+2+3+4 data in a sequential order, i.e. first the speakers from isolet1, then isolet2, and so on. The test set, isolet5, is a separate file3. Due to missing three examples, there are 7797 examples in total referred to as isolet1-isolet5 (6238 training examples and 1559 test examples). The features vector has 617 features including spectral coefficients, contour features, sonorant features, pre-sonorant features, and post-sonorant features [24]. Since the features have real values, the Gaussian visible units is used [11]. Similar to MNIST test, in the first experiment a discriminative RBM has been applied. Structure of this RBM is 617-1000. The results are presented in Table 3. The results have been obtained using 10 separate runs on ISOLET dataset. In another test, a DBN with a structure of 617-1000-1000-2000 was trained. As described before, after training the DBN, fine tuning is done using Back-Propagation (BP) method. The results are presented in Table 3. In these results, each RBM in DBN (before back-propagation) was trained in 200 epochs. Then the DBN was finetuned in 200 epochs using back-propagation method. In these DBNs, first layer RBM is trained with different sampling methods as first layer feature extractor and the two other layers are trained with CD sampling method. The conducted experiments show again the capability of the proposed method to obtain more accurate gradients of log probability of training data and achieve smaller classification error. The best result was obtained with FEPCD method\n3 Available online at \u201chttps://archive.ics.uci.edu/ml/datasets/ISOLET\u201d\nwith 0.0353 classification error rate that is better than the best results obtained in new articles like [25] with 0.0559 classification error rate."}, {"heading": "5. Conclusion", "text": "In this paper we discussed one of the main problems in learning Deep Belief Networks. From the beginning of invention of DBN, since the gradient of log probability of training data is intractable, therefore the sampling methods are used to estimate this gradient. The main part of this estimation is sampling from the model that its parameters have been tuned using training data. In our new method, the elite samples are found in multiple sampling chains using free energy value, which is proportional to probability of training data. These selected samples can be used in computation of gradient with more accuracy. According to the results, this method improves performance on MNIST and ISOLET datasets and outperforms other general sampling methods such as CD or PCD. For future work, we would like to use free energy criterion as a fitness function in evolutionary algorithms. In this new idea, we can achieve new improved chains using evolutionary operations. Another improving idea is to use both CD and FEPCD sampling methods simultaneously, and to use their both advantages. Although the CD method has low performance, but in the first epochs, CD has better gradient computation. Now, we can improve the training speed of FEPCD by merging it to CD."}], "references": [{"title": "Discriminative deep belief networks for visual data classification", "author": ["Y. Liu", "S. Zhou", "Q. Chen"], "venue": "Pattern Recognition, vol. 44, no. 10\u201311, pp. 2287\u20132296, Oct. 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "Advances in neural information processing systems, vol. 20, pp. 873\u2013880, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": "Advances in neural information processing systems, vol. 22, pp. 1096\u20131104, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Comparing multilayer perceptron to Deep Belief Network Tandem features for robust ASR", "author": ["O. Vinyals", "S.V. Ravuri"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, 2011, pp. 4596\u20134599.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep belief networks for phone recognition", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "NIPS Workshop on Deep Learning for Speech Recognition and Related Applications, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning features from music audio with deep belief networks", "author": ["P. Hamel", "D. Eck"], "venue": "11th International Society for Music Information Retrieval Conference (ISMIR 2010), 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "Proceedings of the international conference on artificial intelligence and statistics, 2009, vol. 5, pp. 448\u2013455.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Machine Learning Group, University of Toronto, Technical report, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A tutorial on stochastic approximation algorithms for training Restricted Boltzmann Machines and Deep Belief Nets", "author": ["K. Swersky", "B. Chen", "B. Marlin", "N. de Freitas"], "venue": "Information Theory and Applications Workshop (ITA), 2010, 2010, pp. 1 \u201310.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G.E. Hinton"], "venue": "Artificial Intelligence and Statistics, 2005, vol. 2005, p. 17.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Using fast weights to improve persistent contrastive divergence", "author": ["T. Tieleman", "G. Hinton"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, New York, NY, USA, 2009, pp. 1033\u2013 1040.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Quickly Generating Representative Samples from an RBM-Derived Process", "author": ["O. Breuleux", "Y. Bengio", "P. Vincent"], "venue": "Neural Computation, vol. 23, no. 8, pp. 2058\u20132073, Apr. 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., vol. 2, no. 1, pp. 1\u2013127, Jan. 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "Proceedings of the 25th international  18  conference on Machine learning, New York, NY, USA, 2008, pp. 1064\u2013 1071.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "arXiv:1206.5538, Jun. 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in Cognitive Sciences, vol. 11, no. 10, pp. 428\u2013434, Oct. 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "A brief survey on deep belief networks and introducing a new object oriented MATLAB toolbox (DeeBNet)", "author": ["M.A. Keyvanrad", "M.M. Homayounpour"], "venue": "arXiv:1408.3264 [cs], Aug. 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "MNIST handwritten digit database", "author": ["Y. LeCun", "C. Cortes"], "venue": "AT&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Visual Object Recognition Using Generative Models of Images", "author": ["V. Nair"], "venue": "University of Toronto, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Detonation Classification from Acoustic Signature with the Restricted Boltzmann Machine", "author": ["Y. Bengio", "N. Chapados", "O. Delalleau", "H. Larochelle", "X. Saint-Mleux", "C. Hudon", "J. Louradour"], "venue": "Computational Intelligence, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Spoken Letter Recognition", "author": ["M.A. Fanty", "R.A. Cole"], "venue": "presented at the Advances in Neural Information Processing Systems, 1991, pp. 220\u2013226.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1991}, {"title": "Multi-class boosting with asymmetric binary weak-learners", "author": ["A. Fern\u00e1ndez-Baldera", "L. Baumela"], "venue": "Pattern Recognition, vol. 47, no. 5, pp. 2080\u20132090, May 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "One important tool for dealing with this problem is to use DBNs (Deep Neural Network) that can create neural networks including many hidden layers [1].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "Using unlabeled data in high level feature extraction [2] and also increasing discrimination between extracted features are the benefits of DBN for feature learning [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "Using unlabeled data in high level feature extraction [2] and also increasing discrimination between extracted features are the benefits of DBN for feature learning [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Hinton presented DBNs and used it in the task of digit recognition on MNIST data set [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "In another paper from this author [3], he used DBN as a nonlinear model for feature extraction and dimension reduction.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "Another paper proposed DDBN (Discriminative Deep Belief Network) is based on DBN as a new classifier [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "3 DBN applications are not limited to image processing and can be used in voice processing [5]\u2013[8] with significant efficiency.", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "3 DBN applications are not limited to image processing and can be used in voice processing [5]\u2013[8] with significant efficiency.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "the joints between hidden units and also between visible units are disconnected [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "The probability that the network assigns to a training image can be increased by adjusting the weights and biases to lower the energy of that image and to raise the energy of other images, especially those images that have low energies and therefore make a big contribution to the partition function [11].", "startOffset": 300, "endOffset": 304}, {"referenceID": 10, "context": "Therefore the partial derivative with respect to wij of the above objective is given by [12] :", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Since there are no direct connections between hidden units in an RBM, these hidden units are independent given visible units [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "Finally due to impossibility of this method and large run-times, Contrastive Divergence (CD) method is used [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "According to equation (5), the log P(v) can be expressed as follows [14]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Contrastive Divergence or Persistent Contrastive Divergence) is in sampling in their negative phase [15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The chain is initialized by setting the binary states of the visible units to be the same as a data vector [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 11, "context": "Since Gibbs sampling method is slow, Contrastive Divergence (CD) algorithm is used [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "Although CD1 method is not a perfect gradient computation method, but its results are acceptable [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "This method has a higher performance and can compute gradient more exactly [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "One of these methods is PCD that is very popular [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Although all model parameters are changed in each step, but can receive good samples from model distribution with a few Gibbs sampling steps because the model parameters change slightly [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 8, "context": "Many persistent chains can be run in parallel and we will refer to the current state in each of these chains as new sample or a \u201cfantasy\u201d particle [9], [17].", "startOffset": 147, "endOffset": 150}, {"referenceID": 15, "context": "Many persistent chains can be run in parallel and we will refer to the current state in each of these chains as new sample or a \u201cfantasy\u201d particle [9], [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "After an RBM has been learned, the activities of its hidden units (when they are being driven by data) can be used as the \u2018data\u2019 for learning a higher-level RBM [19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 3, "context": "The model performs a nonlinear transformation on its input vectors and produces as output, the vectors that will be used as input for the next model in the sequence [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "Pretraining helps generalization and the very limited information in the data is used only to slightly adjust the weights found by pretraining [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 9, "context": "11 The proposed criterion for selecting the best chain is the free energy of visible sample v which is defined as follows [11]:", "startOffset": 122, "endOffset": 126}, {"referenceID": 9, "context": "Therefore F(v) can be computed as follows [11]:", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "Thus the derivative of P(v) according to any parameter \u03b8 is as follows [16]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "Also we used the DeeBNet toolbox [20] (the implemented toolbox with authors) in the experiments.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "MNIST dataset includes images of handwritten digits [21] (10 classes of digits 09).", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "The image pixels have discrete values between 0 and 255 that most of them have the values at the edge of this interval [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "In our experiments, these discrete values have been mapped to interval [0-1] using min-max normalization method.", "startOffset": 71, "endOffset": 76}, {"referenceID": 9, "context": "Classification is done by computing P(y|v) in each class [11], [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "Classification is done by computing P(y|v) in each class [11], [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "Finally in the main experiment, a DBN with an structure of 784-500-500-2000, similar to the structure proposed by Hinton [4] was trained.", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "As described before, in many DBN papers, after training a DBN, it can be fine-tuned using BackPropagation (BP) method [1], [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "As described before, in many DBN papers, after training a DBN, it can be fine-tuned using BackPropagation (BP) method [1], [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "These results are better than the best results obtained in Hinton [4] (0.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "014 error rate) [3].", "startOffset": 16, "endOffset": 19}, {"referenceID": 22, "context": "The features vector has 617 features including spectral coefficients, contour features, sonorant features, pre-sonorant features, and post-sonorant features [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "Since the features have real values, the Gaussian visible units is used [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "0353 classification error rate that is better than the best results obtained in new articles like [25] with 0.", "startOffset": 98, "endOffset": 102}], "year": 2014, "abstractText": "Nowadays this is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. In this paper we present an improvement in a common method that is usually used in training of RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can more accurately compute gradient of log probability of training data. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in those papers using this dataset. The implemented method is available online at \u201chttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html\u201d.", "creator": "Microsoft\u00ae Word 2013"}}}