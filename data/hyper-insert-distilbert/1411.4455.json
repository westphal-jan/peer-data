{"id": "1411.4455", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Errata: Distant Supervision for Relation Extraction with Matrix Completion", "abstract": "the essence of distantly supervised relation extraction is that originally it is an incomplete multi - graph label type classification problem with correlated sparse and noisy labeled features. to tackle the sparsity ridge and sample noise challenges, we help propose solving explicitly the classification decision problem proposed using arrow matrix numerical completion on factorized matrix of minimized conditional rank. we formulate relation linear classification as implies completing just the unknown labels collection of testing items ( entity base pairs ) enclosed in a sparse matrix code that additionally concatenates many training elements and thus testing textual treatment features with estimated training labels. modeling our whole algorithmic framework is based on the assumption such that considering the rank of item - by - feature type and the item - by - tail label joint collection matrix is low. we apply two linked optimization models to recover the underlying estimated low - rank matrix leveraging the tree sparsity tree of feature - efficient label insertion matrix. the matrix completion algorithm problem : is then spontaneously solved by the functional fixed variable point continuation ( fpc ) algorithm, which consequently can consequently find the specified global optimum. experiments on first two, widely deployed used datasets supplemented with different dimensions matrix of assigned textual features demonstrate together that performing our experimental low - rank orthogonal matrix completion selection approach significantly outperforms the local baseline and the state - of - the - art algorithm methods.", "histories": [["v1", "Mon, 17 Nov 2014 12:43:30 GMT  (1872kb,D)", "http://arxiv.org/abs/1411.4455v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["miao fan", "deli zhao", "qiang zhou", "zhiyuan liu", "thomas fang zheng", "edward y chang"], "accepted": true, "id": "1411.4455"}, "pdf": {"name": "1411.4455.pdf", "metadata": {"source": "CRF", "title": "Errata: Distant Supervision for Relation Extraction with Matrix Completion", "authors": ["Miao Fan", "Deli Zhao", "Qiang Zhou", "Zhiyuan Liu", "Thomas Fang Zheng", "Edward Y. Chang"], "emails": ["fanmiao.cslt.thu@gmail.com"], "sections": [{"heading": null, "text": ""}, {"heading": "1 Introduction", "text": "Relation Extraction (RE) is the process of generating structured relation knowledge from unstructured natural language texts. Traditional supervised methods (Zhou et al., 2005; Bach and Badaskar, 2007) on small hand-labeled corpora, such as MUC1 and ACE2, can achieve high precision and recall. However, as producing handlabeled corpora is laborius and expensive, the supervised approach can not satisfy the increasing\n1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 2http://www.itl.nist.gov/iad/mig/tests/ace/\ndemand of building large-scale knowledge repositories with the explosion of Web texts. To address the lacking training data issue, we consider the distant (Mintz et al., 2009) or weak (Hoffmann et al., 2011) supervision paradigm attractive, and we improve the effectiveness of the paradigm in this paper.\nThe intuition of the paradigm is that one can take advantage of several knowledge bases, such as WordNet3, Freebase4 and YAGO5, to automatically label free texts, like Wikipedia6 and New York Times corpora7, based on some heuristic alignment assumptions. An example accounting for the basic but practical assumption is illustrated in Figure 1, in which we know that the two entities (<Barack Obama, U.S.>) are not only involved in the relation instances8 coming from knowledge bases (President-of(Barack Obama, U.S.) and Born-in(Barack Obama, U.S.)),\n3http://wordnet.princeton.edu 4http://www.freebase.com 5http://www.mpi-inf.mpg.de/yago-naga/yago 6http://www.wikipedia.org 7http://catalog.ldc.upenn.edu/LDC2008T19 8According to convention, we regard a structured triple r(ei, ej) as a relation instance which is composed of a pair of entities <ei, ej>and a relation name r with respect to them.\nbut also co-occur in several relation mentions9 appearing in free texts (Barack Obama is the 44th and current President of the U.S. and Barack Obama was born in Honolulu, Hawaii, U.S., etc.). We extract diverse textual features from all those relation mentions and combine them into a rich feature vector labeled by the relation names (President-of and Born-in) to produce a weak training corpus for relation classification.\nThis paradigm is promising to generate largescale training corpora automatically. However, it comes up against three technical challeges:\n\u2022 Sparse features. As we cannot tell what kinds of features are effective in advance, we have to use NLP toolkits, such as Stanford CoreNLP10, to extract a variety of textual features, e.g., named entity tags, part-of-speech tags and lexicalized dependency paths. Unfortunately, most of them appear only once in the training corpus, and hence leading to very sparse features.\n\u2022 Noisy features. Not all relation mentions express the corresponding relation instances. For example, the second relation mention in Figure 1 does not explicitly describe any relation instance, so features extracted from this sentence can be noisy. Such analogous cases commonly exist in feature extraction.\n\u2022 Incomplete labels. Similar to noisy fea9The sentences that contain the given entity pair are called\nrelation mentions. 10http://nlp.stanford.edu/downloads/corenlp.shtml\ntures, the generated labels can be incomplete. For example, the fourth relation mention in Figure 1 should have been labeled by the relation Senate-of. However, the incomplete knowledge base does not contain the corresponding relation instance (Senate-of(Barack Obama, U.S.)). Therefore, the distant supervision paradigm may generate incomplete labeling corpora.\nIn essence, distantly supervised relation extraction is an incomplete multi-label classification task with sparse and noisy features.\nIn this paper, we formulate the relationextraction task from a novel perspective of using matrix completion with low rank criterion. To the best of our knowledge, we are the first to apply this technique on relation extraction with distant supervision. More specifically, as shown in Figure 2, we model the task with a sparse matrix whose rows present items (entity pairs) and columns contain noisy textual features and incomplete relation labels. In such a way, relation classification is transformed into a problem of completing the unknown labels for testing items in the sparse matrix that concatenates training and testing textual features with training labels, based on the assumption that the item-by-feature and item-by-label joint matrix is of low rank. The rationale of this assumption is that noisy features and incomplete labels are semantically correlated. The low-rank factorization of the sparse feature-label matrix delivers the low-dimensional representation of de-correlation for features and labels.\nWe contribute two optimization models, DRMC11-b and DRMC-1, aiming at exploiting the sparsity to recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously. Moreover, the logistic cost function is integrated in our models to reduce the influence of noisy features and incomplete labels, due to that it is suitable for binary variables. We also modify the fixed point continuation (FPC) algorithm (Ma et al., 2011) to find the global optimum.\nExperiments on two widely used datasets demonstrate that our noise-tolerant approaches outperform the baseline and the state-of-the-art methods. Furthermore, we discuss the influence of feature sparsity, and our approaches consistently achieve better performance than compared methods under different sparsity degrees."}, {"heading": "2 Related Work", "text": "The idea of distant supervision was firstly proposed in the field of bioinformatics (Craven and Kumlien, 1999). Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date.\nAs we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-Pe\u0301rez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity\n11It is the abbreviation for Distant supervision for Relation extraction with Matrix Completion\npairs have more than one relation. They extended the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.\nOur work is more relevant to Riedel et al.\u2019s (2013) which considered the task as a matrix factorization problem. Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision."}, {"heading": "3 Model", "text": "We apply a new technique in the field of applied mathematics, i.e., low-rank matrix completion with convex optimization. The breakthrough work on this topic was made by Cande\u0300s and Recht (2009) who proved that most low-rank matrices can be perfectly recovered from an incomplete set of entries. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem. The new framework for classification enhances the robustness to data noise by penalizing different cost functions for features and labels."}, {"heading": "3.1 Formulation", "text": "Suppose that we have built a training corpus for relation classification with n items (entity pairs), d-dimensional textual features, and t labels (relations), based on the basic alignment assumption proposed by Mintz et al. (2009). Let Xtrain \u2208 Rn\u00d7d and Ytrain \u2208 Rn\u00d7t denote the feature matrix and the label matrix for training, respectively. The linear classifier we adopt aims to explicitly learn\nthe weight matrix W \u2208 Rd\u00d7t and the bias column vector b \u2208 Rt\u00d71 with the constraint of minimizing the loss function l,\narg min W,b\nl(Ytrain, [ 1 Xtrain ] [ bT\nW\n] ), (1)\nwhere 1 is the all-one column vector. Then we can predict the label matrix Ytest \u2208 Rm\u00d7t of m testing items with respect to the feature matrix Xtest \u2208 Rm\u00d7d. Let\nZ = [ Xtrain Ytrain Xtest Ytest ] .\nThis linear classification problem can be transformed into completing the unobservable entries in Ytest by means of the observable entries in Xtrain, Ytrain and Xtest, based on the assumption that the rank of matrix Z \u2208 R(n+m)\u00d7(d+t) is low. The model can be written as,\narg min Z\u2208R(n+m)\u00d7(d+t) rank(Z)\ns.t. \u2200(i, j) \u2208 \u2126X , zij = xij , (1 \u2264 i \u2264 n+m, 1 \u2264 j \u2264 d), \u2200(i, j) \u2208 \u2126Y , zi(j+d) = yij , (1 \u2264 i \u2264 n, 1 \u2264 j \u2264 t),\n(2)\nwhere we use \u2126X to represent the index set of observable feature entries in Xtrain and Xtest, and \u2126Y to denote the index set of observable label entries in Ytrain.\nFormula (2) is usually impractical for real problems as the entries in the matrix Z are corrupted by noise. We thus define\nZ = Z\u2217 + E,\nwhere Z\u2217 as the underlying low-rank matrix\nZ\u2217 = [ X\u2217 Y \u2217 ] = [ X\u2217train Y \u2217 train\nX\u2217test Y \u2217 test\n] ,\nand E is the error matrix\nE = [ EXtrain EYtrain EXtest 0 ] .\nThe rank function in Formula (2) is a non-convex function that is difficult to be optimized. The surrogate of the function can be the convex nuclear norm ||Z||\u2217 = \u2211 \u03c3k(Z) (Cande\u0300s and Recht, 2009), where \u03c3k is the k-th largest singular value of Z. To tolerate the noise entries in the error matrix E, we minimize the cost functions Cx and Cy for features and labels respectively, rather than using the hard constraints in Formula (2).\nAccording to Formula (1), Z\u2217 \u2208 R(n+m)\u00d7(d+t) can be represented as [X\u2217,WX\u2217] instead of [X\u2217, Y \u2217], by explicitly modeling the bias vector b. Therefore, this convex optimization model is called DRMC-b,\narg min Z,b\n\u00b5||Z||\u2217 + 1 |\u2126X | \u2211\n(i,j)\u2208\u2126X\nCx(zij , xij)\n+ \u03bb |\u2126Y | \u2211\n(i,j)\u2208\u2126Y\nCy(zi(j+d) + bj , yij),\n(3)\nwhere \u00b5 and \u03bb are the positive trade-off weights. More specifically, we minimize the nuclear norm ||Z||\u2217 via employing the regularization terms, i.e., the cost functions Cx and Cy for features and labels.\nIf we implicitly model the bias vector b, Z\u2217 \u2208 R(n+m)\u00d7(1+d+t) can be denoted by [1, X\u2217,W \u2032 X\u2217] instead of [X\u2217, Y \u2217], in which W \u2032 takes the role of [bT ; W] in DRMC-b. Then we derive another optimization model called DRMC1,\narg min Z\n\u00b5||Z||\u2217 + 1 |\u2126X | \u2211\n(i,j)\u2208\u2126X\nCx(zi(j+1), xij)\n+ \u03bb |\u2126Y | \u2211\n(i,j)\u2208\u2126Y\nCy(zi(j+d+1), yij)\ns.t. Z(:, 1) = 1, (4)\nwhere Z(:, 1) denotes the first column of Z. For our relation classification task, both features and labels are binary. We assume that the actual entry u belonging to the underlying matrix Z\u2217 is randomly generated via a sigmoid function (Jordan, 1995): Pr(u|v) = 1/(1 + e\u2212uv), given the observed binary entry v from the observed sparse matrix Z. Then, we can apply the log-likelihood cost function to measure the conditional probability and derive the logistic cost function for Cx and Cy,\nC(u, v) = \u2212 logPr(u|v) = log(1 + e\u2212uv),\nAfter completing the entries in Ytest, we adopt the sigmoid function to calculate the conditional probability of relation rj , given entity pair pi pertaining to yij in Ytest,\nPr(rj |pi) = 1\n1 + e\u2212yij , yij \u2208 Ytest.\nFinally, we can achieve Top-N predicted relation instances via ranking the values of Pr(rj |pi)."}, {"heading": "4 Algorithm", "text": "The matrix rank minimization problem is NPhard. Therefore, Cande\u0301s and Recht (2009) suggested to use a convex relaxation, the nuclear norm minimization instead. Then, Ma et al. (2011) proposed the fixed point continuation (FPC) algorithm which is fast and robust. Moreover, Goldfrab and Ma (2011) proved the convergence of the FPC algorithm for solving the nuclear norm minimization problem. We thus adopt and modify the algorithm aiming to find the optima for our noisetolerant models, i.e., Formulae (3) and (4)."}, {"heading": "4.1 Fixed point continuation for DRMC-b", "text": "Algorithm 1 describes the modified FPC algorithm for solving DRMC-b, which contains two steps for each iteration,\nGradient step: In this step, we infer the matrix gradient g(Z) and bias vector gradient g(b) as follows,\ng(zij) =  1 |\u2126X | \u2212xij 1+exijzij , (i, j) \u2208 \u2126X \u03bb |\u2126Y | \u2212yi(j\u2212d) 1+e yi(j\u2212d)(zij+bj) , (i, j \u2212 d) \u2208 \u2126Y\n0, otherwise\nand\ng(bj) = \u03bb |\u2126Y | \u2211\ni:(i,j)\u2208\u2126Y\n\u2212yij 1 + eyij(zi(j+d)+bj) .\nWe use the gradient descents A = Z \u2212 \u03c4zg(Z) and b = b \u2212 \u03c4bg(b) to gradually find the global minima of the cost function terms in Formula (3), where \u03c4z and \u03c4b are step sizes.\nShrinkage step: The goal of this step is to minimize the nuclear norm ||Z||\u2217 in Formula (3). We perform the singular value decomposition (SVD) (Golub and Kahan, 1965) for A at first, and then cut down each singular value. During the iteration, any negative value in \u03a3\u2212 \u03c4z\u00b5 is assigned by zero, so that the rank of reconstructed matrix Z will be reduced, where Z = Umax(\u03a3\u2212 \u03c4z\u00b5, 0)VT.\nTo accelerate the convergence, we use a continuation method to improve the speed. \u00b5 is initialized by a large value \u00b51, thus resulting in the fast reduction of the rank at first. Then the convergence slows down as \u00b5 decreases while obeying \u00b5k+1 = max(\u00b5k\u03b7\u00b5, \u00b5F ). \u00b5F is the final value of \u00b5, and \u03b7\u00b5 is the decay parameter.\nFor the stopping criteria in inner iterations, we define the relative error to measure the residual of matrix Z between two successive iterations,\nAlgorithm 1 FPC algorithm for solving DRMC-b Input:\nInitial matrix Z0, bias b0; Parameters \u00b5, \u03bb; Step sizes \u03c4z, \u03c4b.\nSet Z = Z0, b = b0. foreach \u00b5 = \u00b51 > \u00b52 > ... > \u00b5F do\nwhile relative error > \u03b5 do Gradient step: A = Z\u2212 \u03c4zg(Z),b = b\u2212 \u03c4bg(b). Shrinkage step: U\u03a3VT = SVD(A), Z = U max(\u03a3\u2212 \u03c4z\u00b5, 0) VT.\nend while end foreach\nOutput: Completed Matrix Z, bias b.\n||Zk+1 \u2212 Zk||F max(1, ||Zk||F ) \u2264 \u03b5,\nwhere \u03b5 is the convergence threshold."}, {"heading": "4.2 Fixed point continuation for DRMC-1", "text": "Algorithm 2 is similar to Algorithm 1 except for two differences. First, there is no bias vector b. Second, a projection step is added to enforce the first column of matrix Z to be 1. In addition, The matrix gradient g(Z) for DRMC-1 is\ng(zij) =  1 |\u2126X | \u2212xi(j\u22121) 1+e xi(j\u22121)zij , (i, j \u2212 1) \u2208 \u2126X \u03bb |\u2126Y | \u2212yi(j\u2212d\u22121) 1+e yi(j\u2212d\u22121)zij , (i, j \u2212 d\u2212 1) \u2208 \u2126Y 0, otherwise .\nAlgorithm 2 FPC algorithm for solving DRMC-1 Input:\nInitial matrix Z0; Parameters \u00b5, \u03bb; Step sizes \u03c4z .\nSet Z = Z0. foreach \u00b5 = \u00b51 > \u00b52 > ... > \u00b5F do\nwhile relative error > \u03b5 do Gradient step: A = Z\u2212 \u03c4zg(Z). Shrinkage step: U\u03a3VT = SVD(A), Z = U max(\u03a3\u2212 \u03c4z\u00b5, 0) VT. Projection step: Z(:, 1) = 1.\nend while end foreach\nOutput: Completed Matrix Z."}, {"heading": "5 Experiments", "text": "In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets."}, {"heading": "5.1 Dataset", "text": "The two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora. The first dataset12, NYT\u201910, was developed by Riedel et al. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT\u201913, was also released by Riedel et al. (2013), in which they only regarded the lexicalized dependency path between two entities as features. Table 1 shows that the two datasets differ in some main attributes. More specifically, NYT\u201910 contains much higher dimensional features than NYT\u201913, whereas fewer training and testing items."}, {"heading": "5.2 Parameter setting", "text": "In this part, we address the issue of setting parameters: the trade-off weights \u00b5 and \u03bb, the step sizes \u03c4z and \u03c4b, and the decay parameter \u03b7\u00b5.\nWe set \u03bb = 1 to make the contribution of the cost function terms for feature and label matrices equal in Formulae (3) and (4). \u00b5 is assigned by a series of values obeying \u00b5k+1 = max(\u00b5k\u03b7\u00b5, \u00b5F ).\n12http://iesl.cs.umass.edu/riedel/ecml/ 13http://iesl.cs.umass.edu/riedel/data-univSchema/\nWe follow the suggestion in (Goldberg et al., 2010) that \u00b5 starts at \u03c31\u03b7\u00b5, and \u03c31 is the largest singular value of the matrix Z. We set \u03b7\u00b5 = 0.01. The final value of \u00b5, namely \u00b5F , is equal to 0.01. Ma et al. (2011) revealed that as long as the nonnegative step sizes satisfy \u03c4z < min( 4|\u2126Y | \u03bb , |\u2126X |) and \u03c4b < 4|\u2126Y | \u03bb(n+m) , the FPC algorithm will guarantee to converge to a global optimum. Therefore, we set \u03c4z = \u03c4b = 0.5 to satisfy the above constraints on both two datasets."}, {"heading": "5.3 Rank estimation", "text": "Even though the FPC algorithm converges in iterative fashion, the value of \u03b5 varying with different datasets is difficult to be decided. In practice, we record the rank of matrix Z at each round of iteration until it converges at a rather small threshold \u03b5 = 10\u22124. The reason is that we suppose the optimal low-rank representation of the matrix Z conveys the truly effective information about underlying semantic correlation between the features and the corresponding labels.\nWe use the five-fold cross validation on the validation set and evaluate the performance on each fold with different ranks. At each round of iteration, we gain a recovered matrix and average the F114 scores from Top-5 to Top-all predicted relation instances to measure the performance. Figure 3 illustrates the curves of average F1 scores. After recording the rank associated with the highest F1 score on each fold, we compute the mean and the standard deviation to estimate the range of optimal rank for testing. Table 2 lists the range of optimal ranks for DRMC-b and DRMC-1 on NYT\u201910 and NYT\u201913.\n14F1 = 2\u00d7precision\u00d7recall precision+recall\nOn both two datasets, we observe an identical phenomenon that the performance gradually increases as the rank of the matrix declines before reaching the optimum. However, it sharply decreases if we continue reducing the optimal rank. An intuitive explanation is that the high-rank matrix contains much noise and the model tends to be overfitting, whereas the matrix of excessively low rank is more likely to lose principal information and the model tends to be underfitting."}, {"heading": "5.4 Method Comparison", "text": "Firstly, we conduct experiments to compare our approaches with Mintz-09 (Mintz et al., 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT\u201910 dataset. Surdeanu et al. (2012) released the open source code15 to reproduce the experimental results on those previous methods. Moreover, their programs can control the feature spar-\n15http://nlp.stanford.edu/software/mimlre.shtml\nsity degree through a threshold \u03b8 which filters the features that appears less than \u03b8 times. They set \u03b8 = 5 in the original code by default. Therefore, we follow their settings and adopt the same way to filter the features. In this way, we guarantee the fair comparison for all methods. Figure 4 (a) shows that our approaches achieve the significant improvement on performance.\nWe also perform the experiments to compare our approaches with the state-of-the-art NFE-1316 (Riedel et al., 2013) and its sub-methods (N-13, F-13 and NF-13) on NYT\u201913 dataset. Figure 4 (b) illustrates that our approaches still outperform the state-of-the-art methods. In practical applications, we also concern about the precision on Top-N predicted relation instances. Therefore, We compare the precision of Top-100s, Top-200s and\n16Readers may refer to the website, http://www.riedelcastro.org/uschema for the details of those methods. We bypass the description due to the limitation of space.\nTop-500s for DRMC-1, DRMC-b and the state-ofthe-art method NFE-13 (Riedel et al., 2013). Table 3 shows that DRMC-b and DRMC-1 achieve 24.0% and 26.6% precision increments on average, respectively."}, {"heading": "6 Discussion", "text": "We have mentioned that the basic alignment assumption of distant supervision (Mintz et al., 2009) tends to generate noisy (noisy features and incomplete labels) and sparse (sparse features) data. In this section, we discuss how our approaches tackle these natural flaws.\nDue to the noisy features and incomplete labels, the underlying low-rank data matrix with truly effective information tends to be corrupted and the rank of observed data matrix can be extremely high. Figure 5 demonstrates that the ranks of data matrices are approximately 2,000 for the initial optimization of DRMC-b and DRMC-1. However, those high ranks result in poor performance. As the ranks decline before approaching the optimum, the performance gradually improves, implying that our approaches filter the noise in data\nand keep the principal information for classification via recovering the underlying low-rank data matrix.\nFurthermore, we discuss the influence of the feature sparsity for our approaches and the stateof-the-art methods. We relax the feature filtering threshold (\u03b8 = 4, 3, 2) in Surdeanu et al.\u2019s (2012) open source program to generate more sparse features from NYT\u201910 dataset. Figure 6 shows that our approaches consistently outperform the baseline and the state-of-the-art methods with diverse feature sparsity degrees. Table 2 also lists the range of optimal rank for DRMC-b and DRMC1 with different \u03b8. We observe that for each approach, the optimal range is relatively stable. In other words, for each approach, the amount of truly effective information about underlying semantic correlation keeps constant for the same dataset, which, to some extent, explains the reason why our approaches are robust to sparse features."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we contributed two noise-tolerant optimization models17, DRMC-b and DRMC-1, for distantly supervised relation extraction task from a novel perspective. Our models are based on matrix completion with low-rank criterion. Experiments demonstrated that the low-rank representation of the feature-label matrix can exploit the underlying semantic correlated information for relation classification and is effective to overcome the difficulties incurred by sparse and noisy features and incomplete labels, so that we achieved significant improvements on performance.\nOur proposed models also leave open questions for distantly supervised relation extraction task. First, they can not process new coming testing items efficiently, as we have to reconstruct the data matrix containing not only the testing items but also all the training items for relation classification, and compute in iterative fashion again. Second, the volume of the datasets we adopt are rela-\n17The source code can be downloaded from https:// github.com/nlpgeek/DRMC/tree/master\ntively small. For the future work, we plan to improve our models so that they will be capable of incremental learning on large-scale datasets (Chang, 2011)."}, {"heading": "Acknowledgments", "text": "This work is supported by National Program on Key Basic Research Project (973 Program) under Grant 2013CB329304, National Science Foundation of China (NSFC) under Grant No.61373075 and HTC Laboratory."}], "references": [{"title": "Freebase: A shared database of structured general human knowledge", "author": ["Robert Cook", "Patrick Tufts"], "venue": "In Proceedings of the national conference on Artificial Intelligence,", "citeRegEx": "Bollacker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2007}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Matrix completion for multi-label image classification", "author": ["Fernando Torre", "Jo\u00e3o P Costeira", "Alexandre Bernardino"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cabral et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cabral et al\\.", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Cand\u00e8s", "Recht2009] Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2009}, {"title": "Foundations of Large-Scale Multimedia Information Management and Retrieval", "author": ["Edward Y Chang"], "venue": null, "citeRegEx": "Chang.,? \\Q2011\\E", "shortCiteRegEx": "Chang.", "year": 2011}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["Sanjoy Dasgupta", "Robert E Schapire"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven", "Kumlien1999] Mark Craven", "Johan Kumlien"], "venue": "In ISMB,", "citeRegEx": "Craven et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Craven et al\\.", "year": 1999}, {"title": "A rank minimization heuristic with application to minimum order system", "author": ["Fazel et al.2001] Maryam Fazel", "Haitham Hindi", "Stephen P Boyd"], "venue": null, "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "Transduction with matrix completion: Three birds with one stone", "author": ["Ben Recht", "Junming Xu", "Robert Nowak", "Xiaojin Zhu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goldberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2010}, {"title": "Convergence of fixed-point continuation algorithms for matrix rank minimization", "author": ["Goldfarb", "Ma2011] Donald Goldfarb", "Shiqian Ma"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Goldfarb et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldfarb et al\\.", "year": 2011}, {"title": "Calculating the singular values and pseudo-inverse of a matrix", "author": ["Golub", "Kahan1965] Gene Golub", "William Kahan"], "venue": "Journal of the Society for Industrial & Applied Mathematics, Series B: Numerical Analysis,", "citeRegEx": "Golub et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1965}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld"], "venue": "In Proceedings of the 49th Annual Meeting", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Why the logistic function? a tutorial discussion on probabilities and neural networks. Computational Cognitive Science Technical Report", "author": ["Michael Jordan"], "venue": null, "citeRegEx": "Jordan.,? \\Q1995\\E", "shortCiteRegEx": "Jordan.", "year": 1995}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Koren.,? \\Q2008\\E", "shortCiteRegEx": "Koren.", "year": 2008}, {"title": "Fixed point and bregman iterative methods for matrix rank minimization", "author": ["Ma et al.2011] Shiqian Ma", "Donald Goldfarb", "Lifeng Chen"], "venue": "Mathematical Programming,", "citeRegEx": "Ma et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2011}, {"title": "A framework for multiple-instance learning", "author": ["Maron", "Lozano-P\u00e9rez1998] Oded Maron", "Tom\u00e1s Lozano-P\u00e9rez"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Maron et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Maron et al\\.", "year": 1998}, {"title": "Distant supervision for relation extraction with an incomplete knowledge base", "author": ["Min et al.2013] Bonan Min", "Ralph Grishman", "Li Wan", "Chang Wang", "David Gondek"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Asso-", "citeRegEx": "Min et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Min et al\\.", "year": 2013}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["Rennie", "Srebro2005] Jasson DM Rennie", "Nathan Srebro"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Rennie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rennie et al\\.", "year": 2005}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Asso-", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Reducing wrong labels in distant supervision for relation extraction", "author": ["Issei Sato", "Hiroshi Nakagawa"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long", "citeRegEx": "Takamatsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Takamatsu et al\\.", "year": 2012}, {"title": "Filling knowledge base gaps for distant supervision of relation extraction", "author": ["Xu et al.2013] Wei Xu", "Raphael Hoffmann", "Le Zhao", "Ralph Grishman"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Towards accurate distant supervision for relational facts extraction", "author": ["Zhang et al.2013] Xingxing Zhang", "Jianwen Zhang", "Junyu Zeng", "Jun Yan", "Zheng Chen", "Zhifang Sui"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computa-", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] Guodong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "Traditional supervised methods (Zhou et al., 2005; Bach and Badaskar, 2007) on small hand-labeled corpora, such as MUC1 and ACE2, can achieve high precision and recall.", "startOffset": 31, "endOffset": 75}, {"referenceID": 11, "context": ", 2009) or weak (Hoffmann et al., 2011) supervision paradigm attractive, and we improve the effectiveness of the paradigm in this paper.", "startOffset": 16, "endOffset": 39}, {"referenceID": 14, "context": "We also modify the fixed point continuation (FPC) algorithm (Ma et al., 2011) to find the global optimum.", "startOffset": 60, "endOffset": 77}, {"referenceID": 1, "context": "(2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus.", "startOffset": 24, "endOffset": 72}, {"referenceID": 0, "context": "(2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus.", "startOffset": 24, "endOffset": 72}, {"referenceID": 15, "context": "Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date. As we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al.", "startOffset": 0, "endOffset": 522}, {"referenceID": 0, "context": "(2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P\u00e9rez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence.", "startOffset": 25, "endOffset": 617}, {"referenceID": 0, "context": "(2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P\u00e9rez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity", "startOffset": 25, "endOffset": 725}, {"referenceID": 18, "context": "They extended the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance.", "startOffset": 52, "endOffset": 73}, {"referenceID": 22, "context": "Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.", "startOffset": 18, "endOffset": 97}, {"referenceID": 16, "context": "Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.", "startOffset": 18, "endOffset": 97}, {"referenceID": 24, "context": "Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.", "startOffset": 18, "endOffset": 97}, {"referenceID": 23, "context": "Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance.", "startOffset": 18, "endOffset": 97}, {"referenceID": 17, "context": "They extended the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair.", "startOffset": 53, "endOffset": 130}, {"referenceID": 5, "context": "Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008).", "startOffset": 58, "endOffset": 80}, {"referenceID": 13, "context": ", 2001) and collaborative filtering (Koren, 2008).", "startOffset": 36, "endOffset": 49}, {"referenceID": 16, "context": "Our work is more relevant to Riedel et al.\u2019s (2013) which considered the task as a matrix factorization problem.", "startOffset": 29, "endOffset": 52}, {"referenceID": 2, "context": "This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al.", "startOffset": 107, "endOffset": 128}, {"referenceID": 7, "context": ", 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001).", "startOffset": 77, "endOffset": 97}, {"referenceID": 2, "context": "This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem.", "startOffset": 108, "endOffset": 327}, {"referenceID": 12, "context": "We assume that the actual entry u belonging to the underlying matrix Z\u2217 is randomly generated via a sigmoid function (Jordan, 1995): Pr(u|v) = 1/(1 + e\u2212uv), given the observed binary entry v from the observed sparse matrix Z.", "startOffset": 117, "endOffset": 131}, {"referenceID": 14, "context": "Then, Ma et al. (2011) proposed the fixed point continuation (FPC) algorithm which is fast and robust.", "startOffset": 6, "endOffset": 23}, {"referenceID": 14, "context": "Then, Ma et al. (2011) proposed the fixed point continuation (FPC) algorithm which is fast and robust. Moreover, Goldfrab and Ma (2011) proved the convergence of the FPC algorithm for solving the nuclear norm minimization problem.", "startOffset": 6, "endOffset": 136}, {"referenceID": 11, "context": "In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets.", "startOffset": 178, "endOffset": 265}, {"referenceID": 21, "context": "In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets.", "startOffset": 178, "endOffset": 265}, {"referenceID": 19, "context": "In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets.", "startOffset": 178, "endOffset": 265}, {"referenceID": 17, "context": "The first dataset12, NYT\u201910, was developed by Riedel et al. (2010), and also used by Hoffmann et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 11, "context": "(2010), and also used by Hoffmann et al. (2011) and Surdeanu et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 11, "context": "(2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions.", "startOffset": 25, "endOffset": 75}, {"referenceID": 11, "context": "(2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT\u201913, was also released by Riedel et al. (2013), in which they only regarded the lexicalized dependency path between two entities as features.", "startOffset": 25, "endOffset": 270}, {"referenceID": 8, "context": "edu/riedel/data-univSchema/ We follow the suggestion in (Goldberg et al., 2010) that \u03bc starts at \u03c31\u03b7\u03bc, and \u03c31 is the largest singular value of the matrix Z.", "startOffset": 56, "endOffset": 79}, {"referenceID": 8, "context": "edu/riedel/data-univSchema/ We follow the suggestion in (Goldberg et al., 2010) that \u03bc starts at \u03c31\u03b7\u03bc, and \u03c31 is the largest singular value of the matrix Z. We set \u03b7\u03bc = 0.01. The final value of \u03bc, namely \u03bcF , is equal to 0.01. Ma et al. (2011) revealed that as long as the nonnegative step sizes satisfy \u03c4z < min( 4|\u03a9Y | \u03bb , |\u03a9X |) and \u03c4b < 4|\u03a9Y | \u03bb(n+m) , the FPC algorithm will guarantee to converge to a global optimum.", "startOffset": 57, "endOffset": 244}, {"referenceID": 11, "context": ", 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 21, "context": ", 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT\u201910 dataset.", "startOffset": 42, "endOffset": 65}, {"referenceID": 11, "context": ", 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT\u201910 dataset. Surdeanu et al. (2012) released the open source code15 to reproduce the experimental results on those previous methods.", "startOffset": 20, "endOffset": 143}, {"referenceID": 19, "context": "We also perform the experiments to compare our approaches with the state-of-the-art NFE-1316 (Riedel et al., 2013) and its sub-methods (N-13, F-13 and NF-13) on NYT\u201913 dataset.", "startOffset": 93, "endOffset": 114}, {"referenceID": 19, "context": "Top-500s for DRMC-1, DRMC-b and the state-ofthe-art method NFE-13 (Riedel et al., 2013).", "startOffset": 66, "endOffset": 87}, {"referenceID": 21, "context": "We relax the feature filtering threshold (\u03b8 = 4, 3, 2) in Surdeanu et al.\u2019s (2012) open source program to generate more sparse features from NYT\u201910 dataset.", "startOffset": 58, "endOffset": 83}, {"referenceID": 4, "context": "For the future work, we plan to improve our models so that they will be capable of incremental learning on large-scale datasets (Chang, 2011).", "startOffset": 128, "endOffset": 141}], "year": 2014, "abstractText": "The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of featurelabel matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}