{"id": "1411.5007", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2014", "title": "A Unified View of Large-scale Zero-sum Equilibrium Computation", "abstract": "because the task procedure of computing approximate nash matrix equilibria in large scale zero - degree sum greedy extensive - form games has received a tremendous amount gain of scholarly attention annually due mainly thanks to the annual computer poker competition. immediately after its mathematical inception, its two competing and then seemingly partially different inference approaches originally emerged - - - one an application of no - regret online machine learning, the dominant other a sufficiently sophisticated backward gradient reasoning method applied explicitly to a proprietary convex - hill concave saddle - point pricing formulation. since exactly then, both approaches generally have grown in mutual relative increasing isolation with advancements on one side often not effecting overcoming the other. following in this paper, we independently rectify this by dissecting and, \" in partially a sense, finally unify the appropriate two views.", "histories": [["v1", "Tue, 18 Nov 2014 20:43:39 GMT  (23kb)", "http://arxiv.org/abs/1411.5007v1", "AAAI Workshop on Computer Poker and Imperfect Information"]], "COMMENTS": "AAAI Workshop on Computer Poker and Imperfect Information", "reviews": [], "SUBJECTS": "cs.AI cs.GT", "authors": ["kevin waugh", "j", "rew bagnell"], "accepted": false, "id": "1411.5007"}, "pdf": {"name": "1411.5007.pdf", "metadata": {"source": "CRF", "title": "A Unified View of Large-scale Zero-sum Equilibrium Computation", "authors": ["Kevin Waugh", "J. Andrew Bagnell"], "emails": ["waugh@cs.cmu.edu", "dbagnell@ri.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n50 07\nv1 [\ncs .A\nI] 1\n8 N\nov 2\n01 4"}, {"heading": "Introduction", "text": "The first annual Computer Poker Competition was held in 2007 providing a testbed for adversarial decision-making with imperfect information. Though incredible advancements have been made since then, the solution of an abstract game still remains a critical component of the top agents. The strength of such a strategy correlates with how well the abstract game models the intractably large full game. Algorithmic improvements in equilibrium computation enable teams to solve larger abstract games and thus improve the overall strength of their agents.\nThe first agents used off-the-shelf linear programming packages to solve the abstract games. Shortly after, two specialized equilibrium-finding techniques emerged drastically reducing resource requirements by allowing implicit game representations. One method, counterfactual regret minimization (CFR), combines many simple no-regret learners together to minimize overall regret and as a whole they converge to an equilibrium (Zinkevich et al. 2008). The other, an application of Nesterov\u2019s excessive gap technique (EGT, 2005), is a gradient method that attacks the convex-concave saddle-point formulation directly (Gilpin et al. 2007).\nCurrently, the two techniques are thought of as simply different and competing. Though both improved, the advancements and the methods remained isolated. At this point, CFR has more widespread adoption due to its simplicity and the power of the sampling schemes available to it.\nCopyright c\u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nIn this paper we connect CFR and EGT. Specifically, both define Bregman divergences with the same structure. This viewpoint allows us to make important connections between the two as well as current research on convex optimization and no-regret learning. In particular, we show that CFR can be thought of as smoothed fictitious play and its dual weights are a function of the opponent\u2019s average strategy; with the appropriate step-size the primal iterate converges to the solution (as opposed to the average); and that a convergence rate of O(1/T ) can be achieved while sampling."}, {"heading": "Zero-Sum Extensive-form Games", "text": "A extensive-form game is a tuple \u0393 = (N,H, p, \u03c3c, I, u) (see, e.g., Osborne and Rubinstein). The game has N players, the set of which is [N ]. The set of histories, H, form a tree rooted at \u03c6, the empty history. For a history h \u2208 H, we denote the set of available actions by A(h). For any action a \u2208 A(h), the history ha \u2208 H is a child of h. The tree\u2019s leaves, or terminal histories, are denoted by Z \u2286 H. At a non-terminal history the player choice function, p : H\\Z \u2192 [N ]\u222a{c}, determines who is to act, either a player or nature. Nature\u2019s policy is denoted \u03c3c defines a distribution over actions when it is to act \u03c3c(\u00b7|h) \u2208 \u2206A(h). The information partition, I = \u222ai\u2208[N ]Ii, is a partition the players\u2019 histories. All histories in an information set are indistinguishable to the player to act. We have \u2200I \u2208 I, h, h\u2032 \u2208 I , that p(h) = p(h\u2032) and A(h) = A(h\u2032). Finally, at a terminal history, z \u2208 Z , the utility for player i, ui : Z \u2192 R, determines the reward for player i reaching terminal z.\nEach player plays the game by means of a behavioral strategy. A behavioral strategy for player i, \u03c3i \u2208 \u03a3i, maps histories to distributions over actions, \u03c3i(\u00b7|h) \u2208 \u2206A(h). When the player is to act, their choice is drawn from this distribution. A strategy must respect the information partition, \u2200I \u2208 Ii, h, h\u2032 \u2208 I, \u03c3i(\u00b7|h) = \u03c3i(\u00b7|h\u2032). We call the tuple of N strategies, (\u03c31, . . . , \u03c3N ), a strategy profile.\nThere are two additional properties we require of the extensive-form games we examine. First, we consider twoplayer zero-sum games. That is, N = 2 and u2(z) = \u2212u1(z); what one player wins the other loses.\nSecond, we will consider games of perfect recall\u2014 neither player is forced forget any information they once knew. Mathematically this requirement is that all histories in an information set share the same sequence of information\nsets and actions from the point-of-view of the acting player. With these additional restrictions, we can conveniently represent a game in its sequence form, \u0393 = (A,E, F ). Following from perfect recall, any sequence of information set/action pairs, henceforth simply sequence, is uniquely identified by its final element. Consequently, we can represent behavioral strategies as vectors, called realization plans, such that the expected utility of the game is a bilinear product xTAy. In particular, a realization plan for the row player is a non-negative vector x indexed by sequences such that \u2211\na\u2208A(I) x(I, a) = x(parent(I)) for all I \u2208 Ii and where x(\u03c6) = 1. In words, the probability mass flowing out of an information set equals the probability of playing to reach that information set. The constraint on the empty sequence, \u03c6, normalizes the probability. We represent these linear equality constraints with the matrix E, i.e., Ex = e1, and thus \u03a31 = {x | Ex = e, x \u2265 0}. For the column player, we have corresponding concepts y, F , and \u03a32. The matrix A is the payoff matrix. Entry ai,j is the expected payoff to the row player over all terminals reach by sequences i and j.\nA pair of strategies, (x, y) is said to be an \u03b5-Nash equilibrium if neither player can benefit by more than \u03b5 by deviating to another strategy. In particular,\nxTAy + \u03b5 \u2265 x\u2032Ay, and \u2200x\u2032 \u2208 \u03a31 \u2212xTAy + \u03b5 \u2265 \u2212xTAy\u2032. \u2200y\u2032 \u2208 \u03a32\nRemarkably, a Nash equilibrium always exists and in our case we can efficiently find \u03b5-equilibria. In the next sections we will discuss and relate large-scale methods to do so."}, {"heading": "Counterfactual Regret Minimization", "text": "Online learning is a powerful framework for analyzing the performance of adaptive algorithms. At time t \u2208 [T ], an online algorithm chooses a policy xt \u2208 \u03a3 and then receives reward vector ut \u2208 K. It aims to minimize external regret,\nmax x\u2217\u2208\u03a3\nT \u2211\nt=1\nut \u00b7 x\u2217 \u2212 ut \u00b7 xt,\nits utility relative to the best fixed policy in hindsight. An algorithm is said to be no-regret if its regret grows sublinear in T for any sequence of u\u2019s from bounded set K. That is, if the bound on its time-averaged regret approaches zero in the worst-case (Cesa-Bianchi and Lugosi 2006).\nThere is an important connection between no-regret learning and zero-sum equilibrium computation. Two no-regret algorithms in self-play converge to a Nash equilibrium. Operationally, the row player gets reward reward ut = Ayt and the column player ut = \u2212ATxt.\nTheorem 1. If two no-regret algorithms play a zero-sum game against one and other for T iterations and have timeaveraged regret less than \u03b5, then their average strategies (x\u0304, y\u0304) are a 2\u03b5-Nash equilibrium. Here x\u0304 = \u2211T\nt=1 x t/T .\nProof. For any x\u2032 \u2208 \u03a31 and y\u2032 \u2208 \u03a32,\n1\nT\nT \u2211\nt=1\nx\u2032 \u00b7 Ayt \u2212 xt \u00b7 Ayt \u2264 \u03b5, and\n1\nT\nT \u2211\nt=1\n(\u2212xt \u00b7 Ay\u2032)\u2212 (\u2212xt \u00b7 Ayt) \u2264 \u03b5\nadding the two inequalities together\n1\nT\nT \u2211\nt=1\nx\u2032 \u00b7Ayt \u2212 xt \u00b7 Ay\u2032 \u2264 2\u03b5\nsubstituting in the definitions of x\u0304 and y\u0304\nx\u2032 \u00b7 Ay\u0304 \u2212 x\u0304 \u00b7 Ay\u2032 \u2264 2\u03b5 choosing y\u2032 = y\u0304, we get for all x\u2032 \u2208 \u03a31 x\u2032 \u00b7Ay\u0304 \u2264 x\u0304 \u00b7Ay\u0304 + 2\u03b5 Similarly, if instead we choose x\u2032 = x\u0304, we get the second inequality in the definition of a 2\u03b5-Nash.\nThe space of mixed strategies, \u03a31 and \u03a32, though structured, is complicated. Zinkevich et al. overcome this and describe a no-regret algorithm over the space of realization plans. Their approach minimizes a new notion of regret\u2014 counterfactual regret\u2014at each information set using simple no-regret algorithms for the probability simplex. They show that counterfactual regret bounds external regret, thus their approach computes an equilibrium in self-play.\nThe counterfactual utility for action a at information set I is the expected utility given that the player tries to and successfully plays action a. That is, we weight a terminal by the total probability of the opponent and chance reaching it, but only by the remaining probability for the acting player. Computing counterfactual utilities is done by traversing the game tree, or one sparse matrix-vector product.\nThere are numerous no-regret algorithms operating over the probability simplex, \u03a3 = \u2206. Let us present two. The first, regret-matching (Hart and Mas-Colell 2000), is also known as the polynomially-weighted forecaster. It is the most commonly used algorithm for zero-sum equilibrium computation. Notationally, we call rt = ut \u2212 ut \u00b7 xte the instantaneous regret and Rt = \u2211t\ni=1 r i the cumulative\nregret at time t. Let L = supu\u2208K \u2016u\u2016\u221e. Definition 1 (Regret-matching). Choose xt+1 \u221d (Rt)+.\nHere, (x)+ = max{0, x}. Theorem 2 (from (Hart and Mas-Colell 2000)). If xt\u2019s are chosen using regret-matching, then the external regret is no more than L \u221a NT .\nThe second algorithm is Hedge (Freund and Schapire 1997). It is also known as the exponentially-weighted forecaster, exponential weights, or weighted majority (Littlestone and Warmuth 1994).\nDefinition 2 (Hedge). Choose xt+1 \u221d exp(\u03b7Rt). Theorem 3 (from (Freund and Schapire 1997)). If xt\u2019s are chosen using Hedge with \u03b7 = \u221a\n2 log(N)/T/L, then the external regret is no more than L \u221a 2T logN .\nAlgorithm 1 CFR Update with Hedge function UPDATEREGRETI (R, g)\nfor a \u2208 A(I) do for I \u2032 \u2208 child(I, a) do\nu\u2032, RI\u2032 \u2190 UPDATEREGRETI\u2032 (RI\u2032 , gI\u2032) ga \u2190 ga + u\u2032\nend for end for xa \u221d exp(RI,a) for a \u2208 A(I) do\nRI,a \u2190 RI,a + ga \u2212 g \u00b7 x end for return g \u00b7 x,R\nend function function UPDATEREGRET(R, g)\nfor I \u2208 child(\u03a6) do , RI \u2190 UPDATEREGRETI\u2032 (RI , gI)\nend for end function\nFor equilibrium-finding, the regret-matching algorithm of Hart and Mas-Colell is common place. Though Hedge has a slightly better theoretical bound than regret-matching, it is computationally more expensive due to exp and choosing \u03b7 can be tricky in practice. As we will see, the use of Hedge here leads to an insightful connection between the two equilibrium-finding approaches in question. We show the counterfactual regret update in Algorithm 1."}, {"heading": "Connection to Convex Optimization", "text": "A different view from the no-regret learning approach is simply to write the equilibrium computation as a nonsmooth minimization. This minimization is convex, as both the objective and the set of realization plans are.\nTheorem 4. Let f(x) = maxy\u2208\u03a32 \u2212xTAy then f is convex on \u03a31. Furthermore, if x\u2217 \u2208 argminx\u2208\u03a31 f(x) then x\u2217 is a minimax optimal strategy for the row player.\nProof. First, let y\u2032 \u2208 argmaxy\u2208\u03a32 \u2212xTAy. Claim f \u2032(x) = \u2212Ay\u2032 \u2208 \u2202f(x). For any x\u2032 \u2208 \u03a31,\nf(x\u2032)\u2212 f(x) = max y\u2208\u03a32 \u2212x\u2032 \u00b7Ay \u2212 max y\u2208\u03a32 \u2212x \u00b7 Ay\n\u2265 \u2212x\u2032 \u00b7 Ay\u2032 \u2212 max y\u2208\u03a32 \u2212x \u00b7 Ay = \u2212x\u2032 \u00b7 Ay\u2032 + x \u00b7Ay\u2032\n= (\u2212Ay\u2032) \u00b7 (x\u2032 \u2212 x) This is an instantiation of Danskin\u2019s theorem (Bertsekas 1999). By the optimality of x\u2217, for any x\u0302 \u2208 \u03a31:\nx\u2217 \u00b7Ay\u2217 = \u2212f(x\u2217) \u2265 \u2212f(x\u2032) = x\u0302 \u00b7 Ay\u0302. where y\u2217 maximizes \u2212x\u2217 \u00b7Ay and y\u0302 maximizes\u2212x\u0302\u00b7Ay.\nNote, the subgradient computation is precisely y\u2019s best response. The CFR-BR technique is exactly CFR applied to this non-smooth optimization (Johanson et al. 2012).\nAs f is convex, and we can efficiently evaluate its subgradient via a best response calculation, we can use any subgradient method to find an \u03b5-equilibrium strategy. Unfortunately, the most basic approach, the projected subgradient method, requires a complicated and costly projection onto the set of realization plans. We can avoid this projection by employing the proper Bregman divergence. In particular, if h : D \u2192 R is a strongly convex such that we can quickly solve the minimization\nargmin x\u2208D\ng \u00b7 x+ h(x) (1)\nwe say h fits D. In these cases, we can often use h in place of the squared l2 distance and avoid any projections.\nHoda, Gilpin, and Pen\u0303a (2008) describe a family of diverences, or distance generating functions, for the set of realization plans. They construct their family of distance generating functions inductively. One such h is as follows:\nhI(x, y) = \u2211\na\u2208A(I) xI,a log xI,a+ \u2200I \u2208 Ii \u2211\nI\u2032\u2208child(I,a)\nxI,a [hI\u2032(yI\u2032/xI,a)]\nh(x) = \u2211\nI\u2208child(\u03c6)\nhI(xI)\nA few things worth noting. First, by child(x) we mean the set of information sets that are immediate children of sequence x. Second, we slightly abuse of notation above in that hI(x, y) depends on the immediate realization weights, x, and all child weights y. We denote the child weights belonging to information set I \u2032 as yI\u2032 . Second, due to perfect recall, this recursion does bottom out; there are information sets with no children and there are no cycles. At a terminal information set, hI is the negative entropy function. The recursion makes use of the dilation or perspective operator.\nGordon (2006) introduces the same function in his supplementary materials, but does not provide a closed-form solution to its minimization. The closed-form solution is shown in Algorithm 2. Note that this algorithm is the same as a best response calculation where we replace the max operator with the softmax operator. The normalization step afterwards restores the sequence form constraints, i.e., converts from a behavioral strategy to a realization plan.\nThe computational structure of Algorithm 1 and 2 are identical. CFR, too, must normalize when computing an equilibrium to properly average the strategies. Both use the softmax operator to on the expected utility to define the current policy. In particular, if R = 0, then the computation is equivalent with the exception of the initial sign of g."}, {"heading": "Dual Averaging", "text": "Nesterov\u2019s dual averaging (2009) is a subgradient method, and thus we can use to find equilibria. As we shall see, it has close connections to counterfactual regret when equipped with Hoda, Gilpin, and Pen\u0303a distance function.\nDual averaging defines two sequences, xt, the query points, and gt, the corresponding sequence of subgradients. The averages x\u0304 and g\u0304 converge to a primal-dual solution.\nAlgorithm 2 Smoothed Best Response function MINIMIZE hI (u)\nfor a \u2208 A(I) do for I \u2032 \u2208 child(I, a) do\nu\u2032, xI\u2032 \u2190 MINIMIZE hI\u2032 (uI\u2032) uI,a \u2190 uI,a + u\u2032\nend for end for xI,a \u221d exp(uI,a) \u22b2 softmax instead of max return uI \u00b7 x, x\nend function function NORMALIZEI (x, Z)\nfor a \u2208 A(I) do xI,a \u2190 xI,a/Z for I \u2032 \u2208 child(I, a) do\nNORMALIZEI\u2032(xI\u2032 , xI,a) end for\nend for end function function MINIMIZE h(g)\nfor I \u2208 child(\u03a6) do , xI \u2190 MINIMIZE hI\u2032 (\u2212gI)\nNORMALIZEI (x, 1) end for return x\nend function\nDefinition 3. Let \u03b2t > 0 be a sequence of step sizes and h : D \u2192 R be a strongly convex distance generating function. The sequence xt is given by\nxt+1 = argmin x\u2208D\n1\nt\nt \u2211\ni=1\n[ f(xi) + f \u2032(xi) \u00b7 (x\u2212 xi) ] + \u03b2th(x)\n= argmin x\u2208D\ng\u0304t \u00b7 x+ \u03b2th(x) (2)\nThe method convergences for step sizes O( \u221a t), or for an appropriately chosen constant step size when the number if iterations is known ahead of time.\nInterestingly, Hedge and regret-matching over the simplex are operationally equivalent to dual averaging.\nTheorem 5. Hedge is equivalent to Dual Averaging over the simplex with \u03c3(x) =\n\u2211n i=1 xi log xi, \u03b2 t = 1/(t\u03b7) and gt = \u2212ut. Proof. The dual averaging minimization can be written as the convex conjugate of the negative entropy (Boyd and Vandenberghe 2004):\nmin x\u2208\u2206\ng\u0304t \u00b7 x+ \u03b2t n \u2211\ni=1\nxi log xi\n= \u2212\u03b2t max x\u2208\u2206\n\u2212g\u0304t \u00b7 x/\u03b2t \u2212 n \u2211\ni=1\nxi log xi\n= \u2212\u03b2t log n \u2211\ni=1\nexp ( \u2212g\u0302i/\u03b2t )\nThe gradient of the conjugate minimizes the objective (Rockafeller 1970),\nxt+1 \u221d exp ( \u2212g\u0304t/\u03b2t ) = exp\n(\n\u2212\u03b7 t \u2211\ni=1\ngt\n)\n\u221d exp ( \u03b7 t \u2211\ni=1\nut \u2212 ut \u00b7 xte ) = exp ( \u03b7Rt )\nThe last step follows from exp(x) \u221d exp(b) exp(x) = exp(be+ x) for any vector x and constant b.\nIn particular, note subtracting the expected utility in the regret update does not at all alter the iterates. We need only accumulate counterfactual utility when using Hedge.\nTheorem 6. Regret-matching is equivalent to Dual Averaging over the simplex with \u03c3(x) = \u2016x+\u201622/2, \u03b2t = eTRt+/t and gt = ut \u00b7 xte\u2212 ut.\nProof. Consider the dual averaging minimization without the normalization constraint:\nxt+1 = argmin x\u22650 g\u0304t \u00b7 x+ \u03b2t\u2016x+\u201622/2\n= (\u2212g\u0304t)+\n\u03b2t =\nt(1 t \u2211t i=1 u i \u2212 ui \u00b7 xie)+ eTRt+\n= Rt+\neTRt+\nNote that by construction xt+1 sums to one, therefore the normalization constraint holds. In order for dual averaging to converge, we need 1/\u03b2t \u2208 O( \u221a T ). This follows from the no-regret bound on regret-matching, eTRT+ \u2264 L \u221a NT .\nFollowing from Theorem 6, we see that CFR with regretmatching is dual averaging with a Hoda, Gilpin, and Pen\u0303astyle divergence built on \u2016x+\u201622. Note that the step sizes must be chosen appropriately to avoid projection. That is, this divergence may not be appropriate for other gradient methods that rely on more stringent step size choices.\nLet us explicitly instantiate dual averaging for solving the convex-concave equilibrium saddle-point optimization.\nxt+1 = argmin x\u2208\u03a31\n1\nt\nt \u2211\ni=1\n\u2212Ayi + \u03b2th(x),\n= argmin x\u2208\u03a31\n\u2212Ay\u0304t + \u03b2th(x),\nyt+1 = argmin y\u2208\u03a32\n1\nt\nt \u2211\ni=1\nATxi + \u03b2th(y).\n= argmin y\u2208\u03a32\nAT x\u0304t + \u03b2th(y).\nIn words, dual averaging applied to the saddle-point problem can be thought of as fictitious play with a smoothed best response as opposed to an actual best response (Brown 1951).\nDual averaging and Hedge are operationally equivalent at terminal information sets, that is, ones where all sequences\nhave no children. At a non-terminal information set, dual averaging responds as if its current policy is played against the opponent\u2019s average strategy in future decisions. Counterfactual regret minimization, on the other hand, plays against the opponent\u2019s current policy. The no-regret property of the algorithm guarantees that these two quantities remain close. In rough mathematical terms, we have\n\u2211T t=1 x t \u00b7 Ayt \u2248 xT \u00b7\u2211Tt=1 Ayt within O( \u221a T ). Conceptually, we can conclude that counterfactual regret minimization, too, is roughly equivalent to smoothed fictitious play.\nOperationally, the difference is counterfactual regret minimization propagates and accumulates the expected utilities from the child information sets in the regrets. Dual averaging, in spirit, is lazy and re-propagates these utilities on each iteration. We note that this re-propagation is not algorithmically necessary when we have sparse stochastic updates as the expected value of an information set only changes if play goes through it. That is, we can memoize and update this value in a sparse fashion.\nThis understanding of CFR hints at why it outperforms its bound in practice and why unprincipled speedups may indeed be reasonable. In particular, we can achieve faster rates of convergence, O(L/T ) as opposed to O(L/ \u221a T ), when minimizing smooth functions with gradient descent and the appropriate step size. Two no-regret learners in self-play are essentially smoothing the non-smooth objective for one and other. The smooth objective itself is changing from iteration to iteration, but this suggests we can choose more aggressive step sizes than necessary. Further evidence of this is that the convergence behavior for CFR-BR, a minimization of a known non-smooth objective, is exhibits more volatile behavior that is closer to the true CFR regret bound.\nDual averaging with the Hoda, Gilpin, and Pen\u0303a divergence is itself a no-regret learner over the set of realization plans (Xiao 2010). The regret bound itself is a function of the strong convexity parameter of the distance function. The bound on which appears to be quite loose. The above analysis suggests that it should be similar, or perhaps in some cases better, than the counterfactual regret bound on external regret. This is not shocking as counterfactual regret minimization is agnostic to no-regret algorithm in use."}, {"heading": "Initialization with a Prior", "text": "When computing an equilibrium or in an online setting, typically the initial strategy is uniformly random. Though the initial error does drop quite rapidly, it is often the case that we have available good priors available. Particularly in an online setting, it is preferable start with a good strategy instead of essentially learning both how to play the game and how to exploit an opponent at the same time. From the optimization perspective, we now discuss sound approaches.\nFirst, let us investigate how to initialize the dual weights\u2014the cumulative counterfactual regrets. In dual averaging, the dual weights to x, g\u0304 = Ay\u0304, is the utilities to x of the opponent\u2019s optimal strategy. If we have guesses at x\u2217 and y\u2217, we can use those to initialize the dual weights. This view of the dual weights is a simplification what is being done by Brown and Sandholm (2014), where counterfactual\nregret minimization is started from a known good solution. From the convex optimization view-point this is immediate.\nAn appealing property of this is that the representation of the opponent\u2019s policy need not be known. That is, so long as we can play against it we can estimate the dual weights. In some cases, the variance of this estimate may be quite poor. With more structured representations, or domain knowledge it may be possible to greatly improve the variance.\nIt is quite common when considering convex optimization problems to recenter the problem. In particular, note that dual averaging starts with the policy x0 = argminx\u2208D h(x). By instead choosing h\u2032(x) = \u2207h(x) \u2212\u2207h(x\u2032) \u00b7 x, we shift the starting policy to x\u2032. In the case of the negative entropy over the simplex, this is equivalent to instead using a Kullback-Leibler divergence. Note, the step size schedule is an important practical factor that needs to be considered more carefully. In particular, smaller step sizes keep iterates closer too the initial policy."}, {"heading": "Convergence of the Current Policy", "text": "Some work has considered using CFR\u2019s final policy as opposed to the average policy. Experimentally, it is shown that the current policy works quite well in practice despite the lack of bounds on its performance.\nA large assortment of recent work on stochastic convex optimization has considered this problem in depth exploring different averaging and step size selection schemes. When we view the problem from the convex optimization viewpoint these results transfer without modification. In particular, it has been shown that using a step size decreasing like 1/ \u221a t will lead to convergence of the current iterate for nonsmooth optimization. That is, if the opponent plays a best response, like in CFR-BR, we need not store the average policy reducing the memory requirement by 50%."}, {"heading": "Acceleration and Sampling", "text": "An important reason that no-regret algorithms have emerged as the dominant approach for large-scale equilibrium computation is their amenability to a various forms of sampling. At a high level, by introducing stochasticity we can drastically reduce the computation on each iteration by introducing different types of sparsity while only marginally increasing the number of necessary iterations.\nTheorem 7 (from (Cesa-Bianchi and Lugosi 2006)). Let the sequence xt be chosen according to a no-regret algorithm with regret bound \u221a CT and let x\u0303t \u223c xt. For all \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 the regret of the sequence x\u0303t is bounded by \u221a CT + \u221a T/2 log 1/\u03b4.\nBy sampling y\u0303 from y, now we choose ut+1 = Ay\u0303t. That is, y\u0303t is a standard basis vector and ut+1 is just a column of A (Lanctot et al. 2009). This has a number of important computational consequences. First, we no longer require a matrix-vector multiplication reducing the complexity of each iteration to linear from quadratic. In addition to the asymptotic reduction in complexity, we improve the constants since selecting a column of A requires no arithmetic\noperations. In fact, we may not even need to store ut+1 if we can index directly into A.\nA second important computational improvement is we can now often use integral numbers in place of floatingpoint numbers (Gibson 2013). In particular, note that rt = ut\u2212ut \u00b7x\u0303te is integral so long as ut is. Furthermore, x\u0304 can be represented as a vector of counts\u2013the number of times each action is sampled. By using regret-matching, even computing xt can be done completely with integers as no floatingpoint math is required to convert RT to xt+1.\nAnother form of sampling is possible when nature participates in the game (Zinkevich et al. 2008). For example, imagine that A = \u2211p\ni=1 Ai and we can implicitly access each Ai. This is the case when nature rolls a die or draws a card from a shuffled deck. Instead of explicitly forming and storing A, or examining each Ai on every iteration, we can choose one randomly. That is, A\u0303t = pAit , where it \u223c Uniform([p]). When we cannot store A this form of sampling reduces each iteration\u2019s asymptotic complexity from linear in p to constant.\nNesterov\u2019s excessive gap technique (2005) cannot handle stochastic gradients. This is the primary reason that it and the Hoda, Gilpin, and Pen\u0303a divergence fell out of favor for equilibrium computation. As we note here, the divergence itself has nothing to do with the inability to handle stochasticity. It is a power tool enabling us to consider a wide variety of gradient methods. The stochastic mirror prox (SMP) algorithm of Juditsky, Nemirovski, and Tauvel (2011) is an extension of the extra-gradient method that achieves a rate of O(L/T + \u03c3/ \u221a T ) on saddle-point problems like ours. Here, \u03c3 is the variance of the gradient. This rate is optimal. Specifically, it enables us to trade off between the variance of the sampling technique and the slower 1/ \u221a T rate that CFR achieves. Current sampling procedures favors low computation with sparse updates. It is unlikely that using them along side SMP will work well out-of-the-box. Further investigation is needed to determine if a successful compromise can practically improve performance."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The task of computing approximate Nash equilibria in large zero-sum extensive-form games has received a tremendous amount of attention due mainly to the Annual Computer Poker Competition. Immediately after its inception, two competing and seemingly different approaches emerged\u2014one an application of noregret online learning, the other a sophisticated gradient method applied to a convex-concave saddle-point formulation. Since then, both approaches have grown in relative isolation with advancements on one side not effecting the other. In this paper, we rectify this by dissecting and, in a sense, unify the two views.", "creator": "LaTeX with hyperref package"}}}