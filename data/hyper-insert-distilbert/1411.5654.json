{"id": "1411.5654", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2014", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "abstract": "in presenting this working paper we particularly explore the bi - conceptual directional mapping between images and their generic sentence - based processing descriptions. we propose learning modeling this brain mapping readily using a recurrent classical neural network. unlike typically previous computation approaches that map both verbal sentences and images to a common embedding, there we merely enable the generation of novel sentences without given an image. already using mostly the generic same model, we can successfully also reconstruct the visual semantic features associated automatically with an image given its chosen visual description. we do use locally a novel recurrent visual memory that it automatically learns to incorrectly remember similar long - stay term visual process concepts to aid in both enabling sentence generation and its visual feature reconstruction. suppose we accordingly evaluate basically our approach on several tasks. these include sentence generation, the sentence data retrieval functions and image target retrieval. state - operation of - the - art results are shown for the task determination of generating novel image descriptions. when compared to classical human generated stored captions, our search automatically \u2010 generated captured captions rewards are preferred better by humans over $ 2 19. 8 \\ % $ 4 of the acquisition time. presentation results chosen are pronounced better than or comparable to state - use of - the - art results on mastering the original image design and sentence retrieval tasks produced for methods using similar auditory visual recall features.", "histories": [["v1", "Thu, 20 Nov 2014 19:50:27 GMT  (3923kb,D)", "http://arxiv.org/abs/1411.5654v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["xinlei chen", "c lawrence zitnick"], "accepted": false, "id": "1411.5654"}, "pdf": {"name": "1411.5654.pdf", "metadata": {"source": "CRF", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "authors": ["Xinlei Chen", "C. Lawrence Zitnick"], "emails": ["xinleic@cs.cmu.edu", "larryz@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "A good image description is often said to \u201cpaint a picture in your mind\u2019s eye.\u201d The creation of a mental image may play a significant role in sentence comprehension in humans [15]. In fact, it is often this mental image that is remembered long after the exact sentence is forgotten [29, 21]. What role should visual memory play in computer vision algorithms that comprehend and generate image descriptions?\nRecently, several papers have explored learning joint feature spaces for images and their descriptions [13, 32, 16]. These approaches project image features and sentence features into a common space, which may be used for image search or for ranking image captions. Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) [13], recursive neural networks [32], or deep neural networks [16]. While these approaches project both semantics and visual features to a\ncommon embedding, they are not able to perform the inverse projection. That is, they cannot generate novel sentences or visual depictions from the embedding.\nIn this paper, we propose a bi-directional representation capable of generating both novel descriptions from images and visual representations from descriptions. Critical to both of these tasks is a novel representation that dynamically captures the visual aspects of the scene that have already been described. That is, as a word is generated or read the visual representation is updated to reflect the new information contained in the word. We accomplish this using Recurrent Neural Networks (RNNs) [6, 24, 27]. One long-standing problem of RNNs is their weakness in remembering concepts after a few iterations of recurrence. For instance RNN language models often find difficultly in learning long distance relations [3, 24] without specialized gating units [12]. During sentence generation, our novel dynamically updated visual representation acts as a long-term memory of the concepts that have already been mentioned. This allows the network to automatically pick salient concepts to convey that have yet to be spoken. As we demonstrate, the same representation may be used to create a visual representation of a written description.\nWe demonstrate our method on numerous datasets. These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22]. When generating novel image descriptions, we demonstrate state-of-the-art results as measured by both BLEU [30] and METEOR [1] on PASCAL 1K. Surprisingly, we achieve performance only slightly below humans as measured by BLEU and METEOR on the MS COCO dataset. Qualitative results are shown for the generation of novel image captions. We also evaluate the bi-directional ability of our algorithm on both the image and sentence retrieval tasks. Since this does not require the ability to generate novel sentences, numerous previous papers have evaluated on this task. We show results that are better or comparable to previous state-of-the-art results using similar visual features.\n1\nar X\niv :1\n41 1.\n56 54\nv1 [\ncs .C\nV ]\n2 0\nN ov\n2 01\n4"}, {"heading": "2. Related work", "text": "The task of building a visual memory lies at the heart of two long-standing AI-hard problems: grounding natural language symbols to the physical world and semantically understanding the content of an image. Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10]. Viewing corresponding text and images as correlated, KCCA [13] is a natural option to discover the shared features spaces. However, given the highly non-linear mapping between the two, finding a generic distance metric based on shallow representations can be extremely difficult. Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].\nWith a good distance metric, it is possible to perform tasks like bi-directional image-sentence retrieval. However, in many scenarios it is also desired to generate novel image descriptions and to hallucinate a scene given a sentence description. Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17]. These papers use various approaches to generate text, such as using pre-trained object detectors with templatebased sentence generation [36, 7, 19]. Retrieved sentences may be combined to form novel descriptions [20]. Recently, purely statistical models have been used to generate sentences based on sampling [17] or recurrent neural networks [23]. While [23] also uses a RNN, their model is significantly different from our model. Specifically their RNN does not attempt to reconstruct the visual features, and is\nmore similar to the contextual RNN of [27]. For the synthesizing of images from sentences, the recent paper by Zitnick et al. [38] uses abstract clip art images to learn the visual interpretation of sentences. Relation tuples are extracted from the sentences and a conditional random field is used to model the visual scene.\nThere are numerous papers using recurrent neural networks for language modeling [2, 24, 27, 17]. We build most directly on top of [2, 24, 27] that use RNNs to learn word context. Several models use other sources of contextual information to help inform the language model [27, 17]. Despite its success, RNNs still have difficulty capturing long-range relationships in sequential modeling [3]. One solution is Long Short-Term Memory (LSTM) networks [12, 33, 17], which use \u201cgates\u201d to control gradient back-propagation explicitly and allow for the learning of long-term interactions. However, the main focus of this paper is to show that the hidden layers learned by \u201ctranslating\u201d between multiple modalities can already discover rich structures in the data and learn long distance relations in an automatic, data-driven manner."}, {"heading": "3. Approach", "text": "In this section we describe our approach using recurrent neural networks. Our goals are twofold. First, we want to be able to generate sentences given a set of visual observations or features. Specifically, we want to compute the probability of a word wt being generated at time t given the set of previously generated words Wt\u22121 = w1, . . . , wt\u22121 and the observed visual features V . Second, we want to enable the capability of computing the likelihood of the visual features V given a set of spoken or read words Wt for generating visual representations of the scene or for performing image search. To accomplish both of these tasks we introduce a\nset of latent variables Ut\u22121 that encodes the visual interpretation of the previously generated or read words Wt\u22121. As we demonstrate later, the latent variables U play the critical role of acting as a long-term visual memory of the words that have been previously generated or read.\nUsing U , our goal is to compute P (wt|V,Wt\u22121, Ut\u22121) and P (V |Wt\u22121, Ut\u22121). Combining these two likelihoods together our global objective is to maximize,\nP (wt, V |Wt\u22121, Ut\u22121) = P (wt|V,Wt\u22121, Ut\u22121)P (V |Wt\u22121, Ut\u22121). (1)\nThat is, we want to maximize the likelihood of the word wt and the observed visual features V given the previous words and their visual interpretation. Note that in previous papers [27, 23] the objective was only to compute P (wt|V,Wt\u22121) and not P (V |Wt\u22121)."}, {"heading": "3.1. Model structure", "text": "Our recurrent neural network model structure builds on the prior models proposed by [24, 27]. Mikolov [24] proposed a RNN language model shown by the green boxes in Figure 1(a). The word at time t is represented by a vector wt using a \u201cone hot\u201d representation. That is, wt is the same size as the word vocabulary with each entry having a value of 0 or 1 depending on whether the word was used. The output w\u0303t contains the likelihood of generating each word. The recurrent hidden state s provides context based on the previous words. However, s typically only models shortrange interactions due to the problem of vanishing gradients [3, 24]. This simple, yet effective language model was shown to provide a useful continuous word embedding for a variety of applications [25].\nFollowing [24], Mikolov et al. [27] added an input layer v to the RNN shown by the white box in Figure 1. This layer may represent a variety of information, such as topic models or parts of speech [27]. In our application, v represents the set of observed visual features. We assume the visual features v are constant. These visual features help inform the selection of words. For instance, if a cat was detected, the word \u201ccat\u201d is more likely to be spoken. Note that unlike [27], it is not necessary to directly connect v to w\u0303, since v is static for our application. In [27] v represented dynamic information such as parts of speech for which w\u0303 needed direct access. We also found that only connecting v to half of the s units provided better results, since it allowed different units to specialize on modeling either text or visual features.\nThe main contribution of this paper is the addition of the recurrent visual hidden layer u, blue boxes in Figure 1(a). The recurrent layer u attempts to reconstruct the visual features v from the previous words, i.e. v\u0303 \u2248 v. The visual hidden layer is also used by w\u0303t to help in predicting the next word. That is, the network can compare its visual memory\nof what it has already said u to what it currently observes v to predict what to say next. At the beginning of the sentence, u represents the prior probability of the visual features. As more words are observed, the visual feature likelihoods are updated to reflect the words\u2019 visual interpretation. For instance, if the word \u201csink\u201d is generated, the visual feature corresponding to sink will increase. Other features that correspond to stove or refrigerator might increase as well, since they are highly correlated with sink.\nA critical property of the recurrent visual features u is their ability to remember visual concepts over the long term. The property arises from the model structure. Intuitively, one may expect the visual features shouldn\u2019t be estimated until the sentence is finished. That is, u should not be used to estimate v until wt generates the end of sentence token. However, in our model we force u to estimate v at every time step to help in remembering visual concepts. For instance, if the word \u201ccat\u201d is generated, ut will increase the likelihood of the visual feature corresponding to cat. Assuming the \u201ccat\u201d visual feature in v is active, the network will receive positive reinforcement to propagate u\u2019s memory of \u201ccat\u201d from one time instance to the next. Figure 2 shows an illustrative example of the hidden units s and u. As can be observed, some visual hidden units u exhibit longer temporal stability.\nNote that the same network structure can predict visual features from sentences or generate sentences from visual features. For generating sentences (Fig. 1(b)), v is known and v\u0303 may be ignored. For predicting visual features from sentences (Fig. 1(c)), w is known, and s and v may be ignored. This property arises from the fact that the words units w separate the model into two halves for predicting words or visual features respectively. Alternatively, if the hidden units s were connected directly to u, this property would be lost and the network would act as a normal autoencoder [34]."}, {"heading": "3.2. Implementation details", "text": "In this section we describe the details of our language model and how we learn our network."}, {"heading": "3.3. Language Model", "text": "Our language model typically has between 3,000 and 20,000 words. While each word may be predicted independently, this approach is computationally expensive. Instead, we adopted the idea of word classing [24] and factorized the distribution into a product of two terms:\nP (wt|\u00b7) = P (ct|\u00b7)\u00d7 P (wt|ct, \u00b7). (2)\nP (wt|\u00b7) is the probability of the word, P (ct|\u00b7) is the probability of the class. The class label of the word is computed in an unsupervised manner, grouping words of similar frequencies together. Generally, this approach greatly accelerates the learning process, with little loss of perplexity. The predicted word likelihoods are computed using the standard soft-max function. After each epoch, the perplexity is evaluated on a separate validation set and the learning reduced (cut in half in our experiments) if perplexity does not decrease.\nIn order to further reduce the perplexity, we combine the RNN model\u2019s output with the output from a Maximum Entropy language model [26], simultaneously learned from the training corpus. For all experiments we fix how many words to look back when predicting the next word used by the Maximum Entropy model to three.\nFor any natural language processing task, pre-processing is crucial to the final performance. For all the sentences, we did the following two steps before feeding them into the RNN model. 1) Use Stanford CoreNLP Tool to tokenize the sentences. 2) Lower case all the letters."}, {"heading": "3.4. Learning", "text": "For learning we use the Backpropagation Through Time (BPTT) algorithm [35]. Specifically, the network is unrolled for several words and standard backpropagation is applied. Note that we reset the model after an End-ofSentence (EOS) is encountered, so that prediction does not cross sentence boundaries. As shown to be beneficial in [24], we use online learning for the weights from the recurrent units to the output words. The weights for the rest of the network use a once per sentence batch update. The activations for all units are computed using the sigmoid function \u03c3(z) = 1/(1 + exp(\u2212z)) with clipping, except the word predictions that use soft-max. We found that Rectified Linear Units (ReLUs) [18] with unbounded activations were numerically unstable and commonly \u201cblew up\u201d when used in recurrent networks.\nWe used the open source RNN code of [24] and the Caffe framework [14] to implement our model. A big advantage of combining the two is that we can jointly learn\nthe word and image representations: the error from predicting the words can be directly backpropagated to the imagelevel features. However, deep convolution neural networks require large amounts of data to train on, but the largest sentence-image dataset has only\u223c80K images [22]. Therefore, instead of training from scratch, we choose to finetune from the pre-trained 1000-class ImageNet model [4] to avoid potential over-fitting. In all experiments, we used the 4096D 7th full-connected layer output as the visual input v to our model."}, {"heading": "4. Results", "text": "In this section we evaluate the effectiveness of our bidirectional RNN model on multiple tasks. We begin by describing the datasets used for training and testing, followed by our baselines. Our first set of evaluations measure our model\u2019s ability to generate novel descriptions of images. Since our model is bi-directional, we evaluate its performance on both the sentence retrieval and image retrieval tasks. For addition results please see the supplementary material."}, {"heading": "4.1. Datasets", "text": "For evaluation we perform experiments on several standard datasets that are used for sentence generation and the sentence-image retrieval task:\nPASCAL 1K [31] The dataset contains a subset of images from the PASCAL VOC challenge. For each of the 20 categories, it has a random sample of 50 images with 5 descriptions provided by Amazon\u2019s Mechanical Turk (AMT).\nFlickr 8K and 30K [31] These datasets consists of 8,000 and 31,783 images collected from Flickr respectively. Most of the images depict humans participating in various activities. Each image is also paired with 5 sentences. These datasets have a standard training, validation, and testing splits.\nMS COCO [22] The Microsoft COCO dataset contains 82,783 training images and 40,504 validation images each with\u223c5 human generated descriptions. The images are collected from Flickr by searching for common object categories, and typically contain multiple objects with significant contextual information. We downloaded the version which contains \u223c40K annotated training images and \u223c10K validation images for our experiments."}, {"heading": "4.2. RNN Baselines", "text": "To gain insight into the various components of our model, we compared our final model with three RNN baselines. For fair comparison, the random seed initialization\nwas fixed for all experiments. The the hidden layers s and u sizes are fixed to 100. We tried increasing the number of hidden units, but results did not improve. For small datasets, more units can lead to overfitting.\nRNN based Language Model (RNN) This is the basic RNN language model developed by [24], which has no input visual features.\nRNN with Image Features (RNN+IF) This is an RNN model with image features feeding into the hidden layer inspired by [27]. As described in Section 3 v is only connected to s and not w\u0303. For the visual features v we used the 4096D 7th Layer output of BVLC reference Net [14] after ReLUs. This network is trained on the ImageNet 1000-way classification task [4]. We experimented with other layers (5th and 6th) but they do not perform as well.\nRNN with Image Features Fine-Tuned (RNN+FT) This model has the same architecture as RNN+IF, but the error is back-propagated to the Convolution Neural Network [9]. The CNN is initialized with the weights from the BVLC reference net. The RNN is initialized with the the pre-trained RNN language model. That is, the only randomly initialized weights are the ones from visual features v to hidden layers s. If the RNN is not pre-trained we found the initial gradients to be too noisy for the CNN. If the weights from v to hidden layers s are also pre-trained the\nsearch space becomes too limited. Our current implementation takes\u223c5 seconds to learn a mini-batch of size 128 on a Tesla K40 GPU. It is also crucial to keep track of the validation error and avoid overfitting. We observed this finetuning strategy is particularly helpful for MS COCO, but does not give much performance gain on Flickr Datasets before it overfits. The Flickr datasets may not provide enough training data to avoid overfitting.\nAfter fine-tuning, we fix the image features again and retrain our model on top of it."}, {"heading": "4.3. Sentence generation", "text": "Our first set of experiments evaluate our model\u2019s ability to generate novel sentence descriptions of images. We experiment on all the image-sentence datasets described previously and compare to the RNN baselines and other previous papers [28, 19]. Since PASCAL 1K has a limited amount of training data, we report results trained on MS COCO and tested on PASCAL 1K. We use the standard train-test splits for the Flickr 8K and 30K datasets. For MS COCO we train and validate on the training set (\u223c37K/\u223c3K), and test on the validation set, since the testing set is not available. To generate a sentence, we first sample a target sentence length from the multinomial distribution of lengths learned from the training data, then for this fixed length we sample 100 random sentences, and use the one with the lowest loss (negative likelihood, and in case of our model, also reconstruction error) as output.\nWe choose three automatic metrics for evaluating the quality of the generated sentences, perplexity, BLEU [30] and METEOR [1]. Perplexity measures the likelihood of generating the testing sentence based on the number of bits it would take to encode it. The lower the value the better. BLEU and METEOR were originally designed for automatic machine translation where they rate the quality of a translated sentences given several references sentences. We can treat the sentence generation task as the \u201ctranslation\u201d of images to sentences. For BLEU, we took the geometric mean of the scores from 1-gram to 4-gram, and used the ground truth length closest to the generated sentence to penalize brevity. For METEOR, we used the latest version1 (v1.5). For both BLEU and METEOR higher scores are better. For reference, we also report the consistency between human annotators (using 1 sentence as query and the rest as references)2.\n1http://www.cs.cmu.edu/\u02dcalavie/METEOR/ 2We used 5 sentences as references for system evaluation, but leave out 4 sentences for human consistency. It is a bit unfair but the difference is usually around 0.01\u223c0.02.\nResults are shown in Table 1. Our approach significantly improves over both Midge [28] and BabyTalk [19] on the PASCAL 1K dataset as measured by BLEU and METEOR. Several qualitative results for the three algorithms are shown in Figure 4. Our approach generally provides more naturally descriptive sentences, such as mentioning an image is black and white, or a bus is a \u201cdouble decker\u201d. Midge\u2019s descriptions are often shorter with less detail and BabyTalk provides long, but often redundant descriptions. Results on Flickr 8K and Flickr 30K are also provided.\nOn the MS COCO dataset that contains more images of high complexity we provide perplexity, BLEU and METEOR scores. Surprisingly our BLEU and METEOR scores (18.99 & 20.42) are just slightly lower than the human scores (20.19 & 24.94). The use of image features (RNN + IF) significantly improves performance over using just an RNN language model. Fine-tuning (FT) and our full approach provide additional improvements for all datasets. For future reference, our final model gives BLEU-1 to BLEU-4 (with penalty) as 60.4%, 26.4%, 12.6% and 6.5%, compared to human consistency 65.9%, 30.5%, 13.6% and\n6.0%. Qualitative results for the MS COCO dataset are shown in Figure 3. Note that since our model is trained on MS COCO, the generated sentences are generally better on MS COCO than PASCAL 1K.\nIt is known that automatic measures are only roughly correlated with human judgment [5], so it is also important to evaluate the generated sentences using human studies. We evaluated 1000 generated sentences on MS COCO by asking human subjects to judge whether it had better, worse or same quality to a human generated ground truth caption. 5 subjects were asked to rate each image, and the majority vote was recorded. In the case of a tie (2-2-1) the two winners each got half of a vote. We find 12.6% and 19.8% prefer our automatically generated captions to the human captions without (Our Approach) and with fine-tuning (Our Approach + FT) respectively. Less than 1% of the subjects rated the captions as the same. This is an impressive result given we only used image-level visual features for the complex images in MS COCO."}, {"heading": "4.4. Bi-Directional Retrieval", "text": "Our RNN model is bi-directional. That is, it can generate image features from sentences and sentences from image features. To evaluate its ability to do both, we measure its performance on two retrieval tasks. We retrieve images given a sentence description, and we retrieve a description given an image. Since most previous methods are capable of only the retrieval task, this also helps provide experimental comparison.\nFollowing other methods, we adopted two protocols for using multiple image descriptions. The first one is to treat each of the \u223c5 sentences individually. In this scenario, the rank of the retrieved ground truth sentences are used for evaluation. In the second case, we treat all the sentences as a single annotation, and concatenate them together for retrieval.\nFor each retrieval task we have two methods for ranking. First, we may rank based on the likelihood of the sentence\ngiven the image (T). Since shorter sentences naturally have higher probability of being generated, we followed [23] and normalized the probability by dividing it with the total probability summed over the entire retrieval set. Second, we could rank based on the reconstruction error between the image\u2019s visual features v and their reconstructed visual features v\u0303 (I). Due to better performance, we use the average reconstruction error over all time steps rather than just the error at the end of the sentence. In Tables 3, we report retrieval results on using the text likelihood term only (I) and its combination with the visual feature reconstruction error (T+I).\nThe same evaluation metrics were adopted from previous papers for both the tasks of sentence retrieval and image retrieval. They used R@K (K = 1, 5, 10) as the measurements, which are the recall rates of the (first) ground truth sentences (sentence retrieval task) or images (image retrieval task). Higher R@K corresponds to better retrieval performance. We also report the median/mean rank of the (first) retrieved ground truth sentences or images (Med/Mean r). Lower Med/Mean r implies better performance. For Flickr 8K and 30K several different evaluation methodologies have been proposed. We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13]\nand [23] respectively, and for Flickr 30K [8] and [23]. Measured by Mean r, we achieve state-of-the-art results on PASCAL 1K image and sentence retrieval (Table 2). As shown in Tables 3 and 4, for Flickr 8K and 30K our approach achieves comparable or better results than all methods except for the recently proposed DeepFE [16]. However, DeepFE uses a different set of features based on smaller image regions. If the same features are used (DeepFE+DECAF) as our approach, we achieve better results. We believe these contributions are complementary, and by using better features our approach may also show further improvement. In general ranking based on text and visual features (T + I) outperforms just using text (T). Please see the supplementary material for retrieval results on MS COCO."}, {"heading": "5. Discussion", "text": "Image captions describe both the objects in the image and their relationships. An area of future work is to examine the sequential exploration of an image and how it relates to image descriptions. Many words correspond to spatial relations that our current model has difficultly in detecting. As demonstrated by the recent paper of [16] better feature localization in the image can greatly improve the performance\nof retrieval tasks and similar improvement might be seen in the description generation task.\nIn conclusion, we describe the first bi-directional model capable of the generating both novel image descriptions and visual features. Unlike many previous approaches using RNNs, our model is capable of learning long-term interactions. This arises from using a recurrent visual memory that learns to reconstruct the visual features as new words are read or generated. We demonstrate state-of-the-art results on the task of sentence generation, image retrieval and sentence retrieval on numerous datasets."}, {"heading": "6. Acknowledgements", "text": "We thank Hao Fang, Saurabh Gupta, Meg Mitchell, Xiaodong He, Geoff Zweig, John Platt and Piotr Dollar for their thoughtful and insightful discussions in the creation of this paper."}], "references": [{"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Comparing automatic evaluation measures for image description", "author": ["D. Elliott", "F. Keller"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, pages 15\u201329. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121\u20132129", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Y. Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "ECCV, pages 529\u2013545", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Choosing linguistics over vision to describe images", "author": ["A. Gupta", "Y. Verma", "C. Jawahar"], "venue": "AAAI", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. J. Artif. Intell. Res.(JAIR), 47:853\u2013899", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagery in sentence comprehension: an fmri study", "author": ["M.A. Just", "S.D. Newman", "T.A. Keller", "A. McEleney", "P.A. Carpenter"], "venue": "Neuroimage, 21(1):112\u2013124", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1406.5679", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR, pages 1601\u20131608. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Words versus objects: Comparison of free verbal recall", "author": ["L.R. Lieberman", "J.T. Culpepper"], "venue": "Psychological Reports, 17(3):983\u2013988", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1965}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1410.1090", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "International Conference on Learning Representations: Workshops Track", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. Cernocky"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on, pages 196\u2013201. IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT, pages 234\u2013239", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Midge: Generating image descriptions from  computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "EACL, pages 747\u2013756. Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Why are pictures easier to recall than words? Psychonomic Science", "author": ["A. Paivio", "T.B. Rogers", "P.C. Smythe"], "venue": "11(4):137\u2013138", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1968}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Collecting image annotations using Amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT Workshop Creating Speech and Language Data with Amazon\u2019s Mechanical Turk", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q. Le", "C. Manning", "A. Ng"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Experimental analysis of the real-time recurrent learning algorithm", "author": ["R.J. Williams", "D. Zipser"], "venue": "Connection Science, 1(1):87\u2013111", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1989}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daum\u00e9 III", "Y. Aloimonos"], "venue": "EMNLP", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "I2T: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.-C. Zhu"], "venue": "Proceedings of the IEEE, 98(8):1485\u20131508", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3009\u20133016. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "\u201d The creation of a mental image may play a significant role in sentence comprehension in humans [15].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "In fact, it is often this mental image that is remembered long after the exact sentence is forgotten [29, 21].", "startOffset": 101, "endOffset": 109}, {"referenceID": 20, "context": "In fact, it is often this mental image that is remembered long after the exact sentence is forgotten [29, 21].", "startOffset": 101, "endOffset": 109}, {"referenceID": 12, "context": "Recently, several papers have explored learning joint feature spaces for images and their descriptions [13, 32, 16].", "startOffset": 103, "endOffset": 115}, {"referenceID": 30, "context": "Recently, several papers have explored learning joint feature spaces for images and their descriptions [13, 32, 16].", "startOffset": 103, "endOffset": 115}, {"referenceID": 15, "context": "Recently, several papers have explored learning joint feature spaces for images and their descriptions [13, 32, 16].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) [13], recursive neural networks [32], or deep neural networks [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) [13], recursive neural networks [32], or deep neural networks [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 15, "context": "Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) [13], recursive neural networks [32], or deep neural networks [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 5, "context": "We accomplish this using Recurrent Neural Networks (RNNs) [6, 24, 27].", "startOffset": 58, "endOffset": 69}, {"referenceID": 25, "context": "We accomplish this using Recurrent Neural Networks (RNNs) [6, 24, 27].", "startOffset": 58, "endOffset": 69}, {"referenceID": 2, "context": "For instance RNN language models often find difficultly in learning long distance relations [3, 24] without specialized gating units [12].", "startOffset": 92, "endOffset": 99}, {"referenceID": 11, "context": "For instance RNN language models often find difficultly in learning long distance relations [3, 24] without specialized gating units [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 28, "context": "When generating novel image descriptions, we demonstrate state-of-the-art results as measured by both BLEU [30] and METEOR [1] on PASCAL 1K.", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "When generating novel image descriptions, we demonstrate state-of-the-art results as measured by both BLEU [30] and METEOR [1] on PASCAL 1K.", "startOffset": 123, "endOffset": 126}, {"referenceID": 17, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 7, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 8, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 12, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 30, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 15, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 9, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 12, "context": "Viewing corresponding text and images as correlated, KCCA [13] is a natural option to discover the shared features spaces.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 193, "endOffset": 201}, {"referenceID": 9, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 193, "endOffset": 201}, {"referenceID": 6, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 34, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 18, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 35, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 26, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 10, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 19, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 16, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 34, "context": "These papers use various approaches to generate text, such as using pre-trained object detectors with templatebased sentence generation [36, 7, 19].", "startOffset": 136, "endOffset": 147}, {"referenceID": 6, "context": "These papers use various approaches to generate text, such as using pre-trained object detectors with templatebased sentence generation [36, 7, 19].", "startOffset": 136, "endOffset": 147}, {"referenceID": 18, "context": "These papers use various approaches to generate text, such as using pre-trained object detectors with templatebased sentence generation [36, 7, 19].", "startOffset": 136, "endOffset": 147}, {"referenceID": 19, "context": "Retrieved sentences may be combined to form novel descriptions [20].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "Recently, purely statistical models have been used to generate sentences based on sampling [17] or recurrent neural networks [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "Recently, purely statistical models have been used to generate sentences based on sampling [17] or recurrent neural networks [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 22, "context": "While [23] also uses a RNN, their model is significantly different from our model.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "Specifically their RNN does not attempt to reconstruct the visual features, and is more similar to the contextual RNN of [27].", "startOffset": 121, "endOffset": 125}, {"referenceID": 36, "context": "[38] uses abstract clip art images to learn the visual interpretation of sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "There are numerous papers using recurrent neural networks for language modeling [2, 24, 27, 17].", "startOffset": 80, "endOffset": 95}, {"referenceID": 25, "context": "There are numerous papers using recurrent neural networks for language modeling [2, 24, 27, 17].", "startOffset": 80, "endOffset": 95}, {"referenceID": 16, "context": "There are numerous papers using recurrent neural networks for language modeling [2, 24, 27, 17].", "startOffset": 80, "endOffset": 95}, {"referenceID": 1, "context": "We build most directly on top of [2, 24, 27] that use RNNs to learn word context.", "startOffset": 33, "endOffset": 44}, {"referenceID": 25, "context": "We build most directly on top of [2, 24, 27] that use RNNs to learn word context.", "startOffset": 33, "endOffset": 44}, {"referenceID": 25, "context": "Several models use other sources of contextual information to help inform the language model [27, 17].", "startOffset": 93, "endOffset": 101}, {"referenceID": 16, "context": "Several models use other sources of contextual information to help inform the language model [27, 17].", "startOffset": 93, "endOffset": 101}, {"referenceID": 2, "context": "Despite its success, RNNs still have difficulty capturing long-range relationships in sequential modeling [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": "One solution is Long Short-Term Memory (LSTM) networks [12, 33, 17], which use \u201cgates\u201d to control gradient back-propagation explicitly and allow for the learning of long-term interactions.", "startOffset": 55, "endOffset": 67}, {"referenceID": 31, "context": "One solution is Long Short-Term Memory (LSTM) networks [12, 33, 17], which use \u201cgates\u201d to control gradient back-propagation explicitly and allow for the learning of long-term interactions.", "startOffset": 55, "endOffset": 67}, {"referenceID": 16, "context": "One solution is Long Short-Term Memory (LSTM) networks [12, 33, 17], which use \u201cgates\u201d to control gradient back-propagation explicitly and allow for the learning of long-term interactions.", "startOffset": 55, "endOffset": 67}, {"referenceID": 25, "context": "Note that in previous papers [27, 23] the objective was only to compute P (wt|V,Wt\u22121) and not P (V |Wt\u22121).", "startOffset": 29, "endOffset": 37}, {"referenceID": 22, "context": "Note that in previous papers [27, 23] the objective was only to compute P (wt|V,Wt\u22121) and not P (V |Wt\u22121).", "startOffset": 29, "endOffset": 37}, {"referenceID": 25, "context": "Our recurrent neural network model structure builds on the prior models proposed by [24, 27].", "startOffset": 84, "endOffset": 92}, {"referenceID": 2, "context": "However, s typically only models shortrange interactions due to the problem of vanishing gradients [3, 24].", "startOffset": 99, "endOffset": 106}, {"referenceID": 23, "context": "This simple, yet effective language model was shown to provide a useful continuous word embedding for a variety of applications [25].", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "[27] added an input layer v to the RNN shown by the white box in Figure 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "This layer may represent a variety of information, such as topic models or parts of speech [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "Note that unlike [27], it is not necessary to directly connect v to w\u0303, since v is static for our application.", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": "In [27] v represented dynamic information such as parts of speech for which w\u0303 needed direct access.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "Alternatively, if the hidden units s were connected directly to u, this property would be lost and the network would act as a normal autoencoder [34].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "In order to further reduce the perplexity, we combine the RNN model\u2019s output with the output from a Maximum Entropy language model [26], simultaneously learned from the training corpus.", "startOffset": 131, "endOffset": 135}, {"referenceID": 33, "context": "For learning we use the Backpropagation Through Time (BPTT) algorithm [35].", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We found that Rectified Linear Units (ReLUs) [18] with unbounded activations were numerically unstable and commonly \u201cblew up\u201d when used in recurrent networks.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "We used the open source RNN code of [24] and the Caffe framework [14] to implement our model.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "However, deep convolution neural networks require large amounts of data to train on, but the largest sentence-image dataset has only\u223c80K images [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "Therefore, instead of training from scratch, we choose to finetune from the pre-trained 1000-class ImageNet model [4] to avoid potential over-fitting.", "startOffset": 114, "endOffset": 117}, {"referenceID": 29, "context": "PASCAL 1K [31] The dataset contains a subset of images from the PASCAL VOC challenge.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Flickr 8K and 30K [31] These datasets consists of 8,000 and 31,783 images collected from Flickr respectively.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "MS COCO [22] The Microsoft COCO dataset contains 82,783 training images and 40,504 validation images each with\u223c5 human generated descriptions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "Midge [28] - 2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "80 Baby Talk [19] - 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "Results are measured using perplexity (PPL), BLEU (%) [30] and METEOR (METR, %) [1].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Results are measured using perplexity (PPL), BLEU (%) [30] and METEOR (METR, %) [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 26, "context": "When available results for Midge [28] and BabyTalk [19] are provided.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "When available results for Midge [28] and BabyTalk [19] are provided.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "DeViSE [8] 17.", "startOffset": 7, "endOffset": 10}, {"referenceID": 30, "context": "5 SDT-RNN [32] 25.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "0 DeepFE [16] 39.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "The protocol of [32] was followed.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "RNN with Image Features (RNN+IF) This is an RNN model with image features feeding into the hidden layer inspired by [27].", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "For the visual features v we used the 4096D 7th Layer output of BVLC reference Net [14] after ReLUs.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "This network is trained on the ImageNet 1000-way classification task [4].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "RNN with Image Features Fine-Tuned (RNN+FT) This model has the same architecture as RNN+IF, but the error is back-propagated to the Convolution Neural Network [9].", "startOffset": 159, "endOffset": 162}, {"referenceID": 26, "context": "We experiment on all the image-sentence datasets described previously and compare to the RNN baselines and other previous papers [28, 19].", "startOffset": 129, "endOffset": 137}, {"referenceID": 18, "context": "We experiment on all the image-sentence datasets described previously and compare to the RNN baselines and other previous papers [28, 19].", "startOffset": 129, "endOffset": 137}, {"referenceID": 30, "context": "[32] 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "0 29 DeViSE [8] 4.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "6 29 DeepFE [16] 12.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "5 15 DeepFE+DECAF [16] 5.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "[13] 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "M-RNN [23] 14.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "The protocols of [32], [13] and [23] are used respectively in each row.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "The protocols of [32], [13] and [23] are used respectively in each row.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "The protocols of [32], [13] and [23] are used respectively in each row.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "DeViSE [8] 4.", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "7 25 DeepFE+FT [16] 16.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "M-RNN [23] 18.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "The protocols of [8] and [23] are used respectively in each row.", "startOffset": 17, "endOffset": 20}, {"referenceID": 22, "context": "The protocols of [8] and [23] are used respectively in each row.", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "We choose three automatic metrics for evaluating the quality of the generated sentences, perplexity, BLEU [30] and METEOR [1].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "We choose three automatic metrics for evaluating the quality of the generated sentences, perplexity, BLEU [30] and METEOR [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 26, "context": "Our approach significantly improves over both Midge [28] and BabyTalk [19] on the PASCAL 1K dataset as measured by BLEU and METEOR.", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "Our approach significantly improves over both Midge [28] and BabyTalk [19] on the PASCAL 1K dataset as measured by BLEU and METEOR.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "It is known that automatic measures are only roughly correlated with human judgment [5], so it is also important to evaluate the generated sentences using human studies.", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "Midge [28] (green) and BabyTalk [19] (blue).", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "Midge [28] (green) and BabyTalk [19] (blue).", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "Since shorter sentences naturally have higher probability of being generated, we followed [23] and normalized the probability by dividing it with the total probability summed over the entire retrieval set.", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 137, "endOffset": 140}, {"referenceID": 22, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "As shown in Tables 3 and 4, for Flickr 8K and 30K our approach achieves comparable or better results than all methods except for the recently proposed DeepFE [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 15, "context": "As demonstrated by the recent paper of [16] better feature localization in the image can greatly improve the performance", "startOffset": 39, "endOffset": 43}], "year": 2014, "abstractText": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-ofthe-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over 19.8% of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.", "creator": "LaTeX with hyperref package"}}}