{"id": "1412.0691", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2014", "title": "RoboBrain: Large-Scale Knowledge Engine for Robots", "abstract": "certainly in this paper we specifically introduce about a knowledge engine, which learns and shares knowledge relevant representations, facilitating for robots to collectively carry correctly out a variety purpose of useful tasks. with building nowadays such an abstraction engine brings with it possible the challenge of dealing with multiple data modalities including machine symbols, natural language, haptic senses, robot endurance trajectories, adaptive visual features and many others. also the knowledge representations stored in the this engine comes from witnessing multiple sources including human physical interactions that robots have while performing computer tasks ( perception, behavior planning devices and self control ), learning knowledge gained bases from www discussions and incorporating learned representations written from nearby leading robotics research groups.", "histories": [["v1", "Mon, 1 Dec 2014 21:22:46 GMT  (7935kb,D)", "http://arxiv.org/abs/1412.0691v1", null], ["v2", "Sun, 12 Apr 2015 06:16:39 GMT  (3064kb,D)", "http://arxiv.org/abs/1412.0691v2", "10 pages, 9 figures"]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["ashutosh saxena", "ashesh jain", "ozan sener", "aditya jami", "dipendra k misra", "hema s koppula"], "accepted": false, "id": "1412.0691"}, "pdf": {"name": "1412.0691.pdf", "metadata": {"source": "CRF", "title": "RoboBrain: Large-Scale Knowledge Engine for Robots", "authors": ["Ashutosh Saxena", "Ashesh Jain", "Ozan Sener", "Aditya Jami", "Dipendra K Misra", "Hema S Koppula"], "emails": ["asaxena@cs.stanford.edu", "ashesh@cs.stanford.edu", "ozansener@cs.stanford.edu", "adityaj@cs.stanford.edu", "dipendra@cs.stanford.edu", "hema@cs.stanford.edu"], "sections": [{"heading": null, "text": "We discuss various technical aspects and associated challenges such as modeling the correctness of knowledge, inferring latent information and formulating different robotic tasks as queries to the knowledge engine. We describe the system architecture and how it supports different mechanisms for users and robots to interact with the engine. Finally, we demonstrate its use in three important research areas: grounding natural language, perception, and planning, which are the key building blocks for many robotic tasks. This knowledge engine is a collaborative effort and we call it RoboBrain.\nI. INTRODUCTION\nOver the last decade, we have seen many applications that have redefined machine intelligence by successfully combining information at a large-scale. Examples include Google knowledge graph [15], IBM Watson [19], Wikipedia, Apple Siri, and many others. The very fact they know answers to many of our day-to-day questions, and not crafted for a specific task, makes them valuable for humans. Inspired by them, researchers have aggregated domain specific knowledge by mining data [4, 6], processing natural language [10], images [13] and speech [45]. The knowledge available from these sources is centered around humans, however their symbolic nature makes them of limited use for robots\u2014for example, imagine a robot querying a search engine for how to \u201cbring sweet tea from the kitchen\u201d (Figure 1).\nContrary to humans, for whom incomplete and ambiguous instructions may suffice, robots require access to a large variety of information with finer details for performing perception, planning, control and natural language understanding. Specifically, the robot would need access to knowledge for grounding the language symbols into physical entities, knowledge that sweet tea can either be on table or in fridge, and knowledge for inferring the appropriate plans for grasping and manipulating objects. Efficiently handling this joint knowledge representation across different tasks and modalities is still an open problem in robotics.\nIn this paper, we present RoboBrain, a knowledge engine that allows robots to learn and share such knowledge. We learn these knowledge representations from a variety of sources, including interactions that robots may have while performing perception, planning and control, as well as natural language and other unstructured knowledge and visual data from the Internet. Our representation considers several modalities including symbols, natural language, visual or shape features, haptic properties, and so on. We believe that by learning and sharing such large-scale knowledge, different robots can become adept faster at performing a variety of tasks in new situations.\nWe mention a few challenges that we need to address while designing RoboBrain:\n\u2022 Multi-modal data. Robots have a variety of sensors, ranging from visual (such as images, 3D point-clouds, and videos) to haptic. Our representations should be\nar X\niv :1\n41 2.\n06 91\nv1 [\ncs .A\nI] 1\nD ec\n2 01\n4\ncapable of encoding these different modalities. \u2022 Beliefs in the concepts. The concepts learned from the\ndata come from a variety of noisy sources. RoboBrain should be able to represent the belief in the correctness of learned concepts. \u2022 Never ending learning. Our knowledge engine should be able to incorporate new knowledge and update beliefs. The system should be scalable, allow real-time updates to existing knowledge, and allow knowledge sharing across robots and users. \u2022 Ability to answer queries. Many robotic tasks such as perception, reasoning and grounding natural language should be able to make use of the knowledge in RoboBrain. We provide an interface to RoboBrain as a query library for building various applications. \u2022 Knowledge acquisition. RoboBrain acquires knowledge from different sources, including physical robot interactions and partner projects. Thus we need an architecture that is compatible with most of the sources. Currently, the partners include Tell Me Dave [44], Tellex\u2019s group [56], PlanIt [26] and affordances [29]. \u2022 Crowd-sourcing. Incorporating knowledge from different sources may lead to significant inconsistencies. Therefore, we need to have some supervisory signal for learning correct concepts in RoboBrain. Hence, our system comprises several forms of feedback that allows users to give feedback at different levels of engagement.\nIn order to address the aforementioned challenges, we present an architecture that can store the large-scale knowledge, in a way that is efficient to retrieve and update during learning. Our knowledge is multi-modal and primarily contains relational information, therefore we use a graph representation of the knowledge (see Figure 1 and 2). We also present a query library that robots can use for performing different tasks.\nWe present use of RoboBrain on three applications in the area of grounding natural language, perception and planning. In first, we show how we can ground natural language commands into an action plan for robot controllers. In second, we consider human activity anticipation. Many perception tasks require different contextual information such as spatial context, affordances and others. In third, we consider path planning for mobile manipulators in context-rich environments. When planning a path, a robot needs to have the knowledge of the usage of different objects and human preferences. We show how we use RoboBrain to formulate a cost function for the planner to plan paths.\nOne nice property that emerges because RoboBrain being an integrated knowledge engine is that representations are now shared across research areas. For example, knowledge about how cups are placed on a table (upright, especially when they contain liquids) could have been learned from perception, but it would help in manipulation as well.\nAs more and more researchers contribute knowledge to RoboBrain, it will not only make their robots perform better but we also believe this will be beneficial for the robotics community at large. RoboBrain is under open, Creative Commons Attribution license (aka CC-BY),1 and is avalable at: http://robobrain.me"}, {"heading": "II. RELATED WORK", "text": "Knowledge bases. Collecting and representing a large amount of information in the form of a machine-readable knowledge base (KB) has been widely studied in the areas of artificial intelligence, data mining, natural language processing and machine learning. Early seminal works have manually created knowledge bases (KBs) with the study of common\n1http://creativecommons.org/licenses/by/2.5, Extra conditions may apply depending on the data sources, see robobrain.me for detailed information.\nsense knowledge (Cyc [40]) and lexical knowledge of english language (WordNet [18]). With the rapid growth of Wikipedia, KBs started to use crowdsourcing (DBPedia [4], Freebase [6]) and automatic information extraction methods (Yago [55, 24]) for mining knowledge.\nRecently, several approaches to KB generation from web data and other unstructured sources have been developed. Examples include NELL [10] and NEIL [11]. One of the limitations of these KBs is their strong dependence on a single modality that is the text modality. There have been few successful attempts to combine multiple modalities. Imagenet [13] enriched the wordnet synsets with images obtained from large-scale internet search and a crowdsourced approach was used to obtain the object labels. These object labels were further extended to object affordances [61].\nIn industry, we have seen many successful applications of the existing KBs within the modalities they covered, for example, Google Knowledge Graph and IBM Watson Jeopardy Challenge [20]. However, the existing KBs do not apply directly to robotics problems where the knowledge needs to be about entities in the physical world and of various modalities.\nRobotics Works. For robots to operate autonomously they should perceive our environments, plan paths, manipulate objects and interact with humans. This is very challenging because solution to each sub-problem varies with the task, human preference and the environment context. We now briefly describe previous work in each of these areas. Perceiving the environment. Perception is a key element of many robotic tasks. It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34]. Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41]. Sharing representations from other modalities such as sound [52] and haptics [21] would also improve perception. Path planning and manipulation. There exist a large class of algorithms which allow robots to move around and modify the environment. Broadly the planning algorithms can be categorized as motion planning [62, 53], task planning [1, 7] and symbolic planning [17, 51]. Bakebot [7], towel-folding [54], IkeaBot [32] and robots preparing pancakes [5] are few of the many successful planning applications. In order to execute complex manipulation tasks, cost functions have been learned in a data-driven manner [50, 26]. Most planning algorithms abstract out the perception details, however as explained earlier, access to perception and manipulation knowledge can allow robots to plan in dynamic real world environments. Interacting with humans. Another important aspect is humanrobot interaction. Previous works have focused on various aspects, such as human-robot collaboration for task com-\npletion [48, 33], generating safe robot motion near humans [42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc. All these applications require access to perception, manipulation, language understanding, etc., further demonstrating the need for large scale multi-modal data.\nPrevious efforts on connecting robots range from creating a common operating system (ROS) for developing robot applications [49] to sharing data acquired by various robots in the cloud [58, 3]. For example, RoboEarth [58] provides a platform for the robots to store and off-load computation to the cloud and communicate to other robots; and KIVA systems [3] use the cloud to coordinate motion and update tracking data for hundreds of mobile platforms. However, RoboBrain knowledge engine provides the knowledge representation layer on top of data storing, sharing and communication."}, {"heading": "III. OVERVIEW", "text": "We represent the knowledge in RoboBrain as a graph (see Section IV, Figure 1 and Figure 2). The concepts are the nodes in the graph, and the edges represent relations between them.\nRoboBrain is a never ending learning system in that it continuously incorporates new knowledge from different sources and modifies the graph. In other words, we look at every additional knowledge acquisition as a measurement event that dynamically modifies the graph. In the next section, we will describe the formal definition of the RoboBrain graph, how we update it, and the concept of latent graph.\nBefore we describe the technical specification of RoboBrain, we mention a few of the technical components:\nKnowledge acquisition. RoboBrain acquires knowledge from various sources, including physical robot interactions, knowledge bases from WWW, and from partner projects. One key knowledge resource for RoboBrain is by crawling knowledge sources on the Internet such as WordNet, ImageNet, Freebase and OpenCyc. These knowledge sources provide lexical knowledge, grounding of concepts into images, and common sense facts about the world. In detail,\n\u2022 WordNet provides the required lexical knowledge that allows us to group nouns and verbs into distinct cognitive concepts. \u2022 ImageNet grounds the noun category from WordNet into images. This allows our knowledge graph to associate an image with every noun. \u2022 OpenCyc is an ontology for everyday common sense knowledge.\nWe are also crawling many other sources such as Wikipedia to add knowledge to RoboBrain. The technical details of adding knowledge to RoboBrain is described in Section IV.\nDisambiguation. Incorporating new knowledge into the graph is challenging. We need to decide whether the incoming knowledge should create new nodes and edges or instead add information to the existing nodes and edges. Since our knowledge graph carries semantic meaning, it should resolve\npolysemy using the context associated with nodes. For example, a \u2018plant\u2019 could mean a \u2018tree\u2019 or an \u2018industrial plant\u2019 and merging them together will create errors in the graph. Therefore, RoboBrain needs to disambiguate the incoming information using the associated context and decide how to add it to the existing knowledge graph.\nBeliefs. Knowledge that we incorporate in the RoboBrain may not be fully reliable and there could be inconsistencies in the knowledge coming from different sources. We need a way to indicate beliefs in the correctness of the concepts and relations in the RoboBrain.\nOnline crowd-sourced interaction. We take inspiration from large-scale crowd-sourced projects such as Wikipedia. While previous works have focussed on using crowd-sourcing for data collection and labeling (e.g., [13]), our work focuses on taking crowd-sourced feedback at many levels: \u2022 Weak feedback. We have found that it is easiest for users\nto provide a binary \u201cApprove\u201d/\u201cDisapprove\u201d feedback while they are browsing online. Our RoboBrain system encourages users to provide such feedback, which is then used for several purposes such as estimating belief on the nodes and edges. \u2022 Feedback on graph. We present the graph to the users on the Internet and smartphones, where they can give feedback on the correctness of a node or an edge in the graph. Such feedback is stored for inferring the latent graph. The key novelty here is that the system shows the proposed \u201clearned\u201d concepts to the users \u2014 thus getting feedback not at the data level but at the knowledge level. \u2022 Feedback from partner projects. There are several partner projects to RoboBrain, such as Tell Me Dave [44], PlanIt [26], and so on. These projects have a crowd-sourced system where they take feedback, demonstrations or interaction data from users. Such data as well as the learned knowledge is shared with the RoboBrain.\nWe not only need to figure out how to present knowledge in a consumable form to users but also need to figure out how to store and use the user feedback in improving the quality and correctness of knowledge in the RoboBrain.\nQueries. The primary purpose of the RoboBrain is to allow queries by the users and robots. We present a RoboBrain Query Library (RaQueL) that is used to interact with the RoboBrain knowledge base. It comprises of functionalities for retrieving sub-graph, ranking relations between nodes, using beliefs, functions to work with multi-modal media and other application tailored functionalities that allow its use by the users and robots. Details of RaQueL along with usage examples are provided in Section VI."}, {"heading": "IV. KNOWLEDGE ENGINE: FORMAL DEFINITION", "text": "In this section, we present the formal definition of RoboBrain (RB) Knowledge Engine.\nOur RB is expressed as a directed graph G = (V,E), see Figure 2 for an example. The vertices V can be of a variety of types such as image, text, video, haptic data, or a learned\nentity such as affordances, deep features, parameters, etc. The edges E \u2286 V \u00d7 V \u00d7C link two nodes with a direction and a type, where C is the set of edge types. See Table I and II for a few examples.\nWe further define a set of auxiliary functions Xv(\u00b7), Xe(\u00b7), bv(\u00b7) and be(\u00b7) where Xv(v) and Xe(e) is the raw data associated with the vertex v and the edge e, and bv(v) and be(e) are the beliefs that indicate the correctness of the node v and the edge e respectively.\nAn edge (v1, v2, `) is an ordered set of two nodes v1 and v2 of type `. Few examples of such edges are: (StandingHuman, Shoe, CanUse), (StandingHuman, N (\u00b5,\u03a3), SpatiallyDistributedAs) and (Grasping, DeepFeature23, UsesFeature). These edges can be considered as (subject, object, predicate) triplets. Although we do not impose any constraints over the nodes in terms of content or modality, we require the edges to be consistent with the RB edge set."}, {"heading": "A. Creating the Graph", "text": "Graph creation consists of never ending cycle of two stages, namely, knowledge acquisition and inference. Within the knowledge acquisition stage, we collect data from various sources and during the inference stage we apply statistical techniques to infer the latent information in the aggregated data. These two stages are explained in more detail below.\nKnowledge acquisition: RoboBrain accepts new information in the form of an edge set. This information can either be from an automated algorithm or from our research collaborators. We call this edge set a feed.\nWe add the new information to the existing graph through a graph union of the existing graph and the feed followed by an inference. Specifically, given a new feed consisting of N edges (v11 , v 1 2 , ` 1) . . . (vN1 , v N 2 , `\nN ) and an existing graph G = (V,E), graph union stage gives us G\u2032 = (V \u2032, E\u2032) as:\nV \u2032 = v11 \u222a v12 \u222a . . . \u222a vN1 \u222a vN2 \u222a V E\u2032 = (v11 , v 1 2 , ` 1) \u222a . . . \u222a (vN1 , vN2 , `N ) \u222a E (1)\nInferring the Latent Graph: We treat the acquired knowledge as a noisy observation of the real knowledge-base that should contain the proper information about the real physical world. We may not be able to observe the real graph directly, therefore we call it the latent graph. For example, latent information \u201ccoffee is typically in a container\u201d is partially observed through many images of a container in media corresponding to coffee. This construction can be explained in a generative setting as having a latent graph G? of all the knowledge about physical word. The measurements through various channels of limited expressiveness (e.g., natural language, trajectories, etc.), in the form of feeds, create an ambiguity over the underlying latent graph.\nIn order to obtain the latent graph G?(V ?, E?), we can apply several operations over the aggregated data. We describe two of them here: split and merge. Split operation is defined as splitting a node into a set of two nodes. Edges having end point in the split node are connected to one of the\nresultant nodes using the disambiguation algorithm. A merge operation is defined as merging two nodes into a single node, while updated the connected edges with the merged node. These two operations create latent concepts by splitting nodes to semantically different subsets or inferring common latent concepts that exist in multiple nodes via merge operation. An example of such an update is shown in Figure 3. When a new information \u201csitting human can use a mug\u201d is added, it causes the split of Cup node into Cup and Mug nodes.\nThus we can define the inferred latent RoboBrain graph as:\nG? = splitvs1 \u25e6mergevm1 ,vm2 \u25e6 . . . \u25e6 splitvsM \u25e6G \u2032\nResulting Latent Graph: Since we do not expect the acquired knowledge to be complete, we treat the information probabilistically. In other words, we express the uncertainty over the missing information as a set of beliefs. These beliefs represent the confidence over the created nodes and edges. In other words, we apply an update operation on the belief functions as new knowledge is added to the RoboBrain. Ideally, the beliefs should converge to the latent graph of the physical world with more information and feedback. Practically, however, we never expect full convergence, and we design probabilistic query algorithms that use the beliefs for answering the input queries."}, {"heading": "V. SYSTEM ARCHITECTURE", "text": "We now describe the system architecture of RoboBrain, shown in Figure 4. The system consists of four interconnected\nlayers: (a) knowledge acquisition, (b) knowledge parser, (c) knowledge storage, and (d) knowledge inference. The principle behind our design is to efficiently process large amount of unstructured multi-modal knowledge and represent it using structured RoboBrain graph. In addition, our design also supports various mechanisms for users and robots to interact with the brain. Through these interactions users retrieve knowledge and also improve RoboBrain through feedback. Below we discuss each of the components.\nKnowledge acquisition layer is the interface between the RoboBrain and the different sources of multi-modal data. Through this layer RoboBrain gets access to new information which the upper layers process (see Figure 4). RoboBrain primarily collects knowledge through its partner projects and by crawling the existing knowledge bases such as Freebase, ImageNet and WordNet, etc., as well as unstructured sources such as Wikipedia and Youtube.\nKnowledge parser layer of RoboBrain processes the data acquired by the acquisition layer and converts it to a consistent format for the storage layer. It also marks the incoming data with appropriate meta-data such as timestamps, source\nversion number etc., for scheduling and managing future data processing. Further, since knowledge bases might change with time, it adds a back pointer to the original source.\nKnowledge storage layer of RoboBrain is responsible for storing different representations of the data. In particular, it consists of a NoSQL document storage database cluster \u2013 RB Knowledge Base (RB-KB) \u2013 to store \u201cfeeds\u201d parsed by the knowledge parser, crowd-sourcing feedback from users and parameters of different machine learning algorithms provided by RoboBrain project partners. RB-KB offloads large media content such as images, videos and 3D point clouds to a distributed object storage system built using Amazon Simple Storage Service (S3). The real power of RoboBrain comes through its graph database (RB-GD) which stores the structured knowledge. The data from RB-KB is refined through multiple learning algorithms and its graph representation is stored in RB-GD. One of the primary goal behind this design is to keep RB-KB as RoboBrain\u2019s single source of truth (SSOT). SSOT centric design allow us to re-build RB-GD in case of failures or malicious knowledge sources.\nKnowledge inference layer contains the key processing and machine learning components of RoboBrain. All the new and recently updated feeds go through a persistent replicated distributed queuing system which are consumed by some of our machine learning plugins (disambiguation, graph builder, etc.) and populates the graph database. These plugins along with other learning algorithms (that operates on entire knowledge graph) constitutes our Learning and Inference framework. This framework comprises a bunch of fault tolerant restful application servers that are scaled automatically based on load. Every application server also bookmarks the progress information (metadata) so that a new server can catch up instantly if one of the systems die.\nRoboBrain also support various interaction mechanisms to enable robots, cars and users to communicate with RoboBrain knowledge base. RoboBrain query library is the primary method for robots to interact with the RoboBrain. A set of public APIs allow information to be presented on WWW for enabling online learning mechanisms (crowd-sourcing).\nOur RoboBrain system is designed with several other desired properties such as security, overall throughput, monitoring and alerting. Our front end and API servers (querying and crowd-sourcing systems) are auto-scaled based on load and most of the data is served using a commercial content delivery network to reduce the end user latency. The detailed description of the system design is beyond the scope of this paper, and will be described in future publications.\nVI. ROBOBRAIN QUERY LIBRARY (RaQueL)\nFor robots to perform tasks (see Section VII for some examples), they not only need to retrieve the stored knowledge (e.g., trajectories, heatmaps, object affordance, natural language lexicons, etc.) but also need to perform certain types of operations specific to the applications. This situation is not usually encountered in traditional databases, and thus we need\nto design a RoboBrain Query Library (RaQueL) for making RoboBrain easily usable. RaQueL can be used for diverse tasks such as semantic labeling, cost functions and features for grasping and manipulation, grounding language, anticipating activities, and fetching trajectory representations. Typically, the queries involve finding a sub-graph matching a given pattern, retrieving beliefs over nodes and edges, and then performing reasoning on the extracted information. The structure of RaQueL admits the following three types of functions: \u2022 Graph retrieval function: fetch function for querying\na sub-graph of RoboBrain that satisfies a given pattern. A pattern defines relations between node and edge variables. E.g., the pattern given below defines two node variables u,v in parenthesis and one edge variable e in square brackets. The arrows represent the edge direction.\n(u)\u2192 [e]\u2192 (v)\nThis pattern is satisfied by all paths of length 1 and fetch function can then be used to retrieve all of them. \u2022 Integrating programming constructs: RaQueL integrates programming language constructs such as map, filter, find, etc. \u2022 Task specific functions: Functions for performing a specific type of task, such as ranking trajectories, semantic labeling of an image, grounding natural language, functions to work with heatmaps, etc.\nWe describe these functions in detail in the following."}, {"heading": "A. Graph retrieval function", "text": "The fetch function is used to retrieve a sub-graph of RoboBrain that matches an input pattern. The function fetch takes as input a pattern Pattern(arg1, arg2, \u00b7 \u00b7 \u00b7 ) where {arg1, arg2, \u00b7 \u00b7 \u00b7 } are the pattern variables and returns a list of tuples with instantiated values of variables.\nfetch : Pattern(arg1, arg2, \u00b7 \u00b7 \u00b7 )\u2192 [(arg(i)1 , arg (i) 2 , \u00b7 \u00b7 \u00b7 )]i\nThis pattern is matched against sub-graphs of RoboBrain and these matched sub-graphs are used to create initialized entries for the pattern variables. RaQueL uses Neo4j Cypher [47] to implement the fetch functionality. We now explain the usage of fetch in the following examples.\nExample 1: Retrieve all the activity nodes connected to nodes with name \u201ccup\u201d.\nfetch({name : \u2018cup\u2032})\u2192 [{istype : \u2018IsActivity\u2032}]\u2192 (v)\nThe parenthesis describes a node variable whose attribute specifications are given inside the curly brackets. In this query, the name attribute of the node should be \u2018cup\u2019. The edge variable also has a istype attribute specification. The pattern will match against paths of length one {u, e, v}; where node u has name \u2018cup\u2019 and edge e has type \u2018IsActivity\u2019. Since there is only one exposed variable v, the fetch will return a list of tuple with instantiated values of variable v only.\nFor brevity, we will directly write the attribute value for edges when attribute name is istype.\nWe can also fetch paths between nodes using the extension of the above example. These paths can be unbounded or bounded by a given length.\nExample 2: Return the path between the nodes with name \u201ccup\u201d and \u201cchair\u201d.\nfetch({name : \u2018cup\u2032})\u2192 [r\u2217]\u2192 ({name : \u2018chair\u2032}) fetch({name : \u2018cup\u2032})\u2192 [r \u2217 5]\u2192 ({name : \u2018chair\u2032})\nThe first query returns edges of all the paths between nodes with name \u2018cup\u2019 and \u2018chair\u2019. The second one only returns edges of all paths of length less than or equal to 5.\nThe fetch function in general can accept any pattern string allowed by Cypher. For more details we refer the readers to Cypher\u2019s documentation [47].\nB. Integrating Programming Constructs\nWhile fetch function is sufficient for queries where the pattern is known in advance. For other type of queries, we want to make a sequence of fetch queries depending upon the results from the previous queries. This is specially useful for greedy exploration of the graph e.g, by fetching only on those nodes, returned by a fetch query, which have a high belief. For this reason, RaQueL supports functional programming constructs such as map, filter, find, etc.2\nWe now present how these constructs can be combined with fetch to do a greedy exploration of RoboBrain. Consider the problem of exploring the sub-graph rooted at a given node u and finding its relations to all other nodes such that these relations have a certain minimum belief.\nFormally, by relation Ru of a given node u we refer to a tuple (u, P, v), where P is a path from node u to node v.\nIn such cases, exploring the entire graph is impractical. Hence, several greedy search heuristics can be employed. We now take up a specific example, to explain this further:\nExample 3: Find at least N number of relations of nodes named \u2018cup\u2032 which have a belief of atleast b.\nquery t := fetch (u{name : \u2018cup\u2032})\u2192 [P : \u2217 \u00b7 \u00b7 \u00b7 t]\u2192 (v) trustedPath t := filter (\u03bb (u, P, v) \u2192 belief P \u2265 b) query t find (\u03bbrel\u2192 len rel \u2265 N) map (\u03bb t\u2192 trustedPath t) [1 \u00b7 \u00b7 \u00b7 ]\nIn this query, we begin by defining two functions query and trustedPath. The query function takes a parameter t and returns all relations (u, P, v) such that u is a node with name \u2018cup\u2019 and P is a path of length less than t. The trustedPath function takes a parameter t and filters in only those relations (u, P, v) returned by calling query t, such that path P has a belief of at least b. Here, the function belief P is an user-defined function which returns a belief for path P . This could be as simple as addition/multiplication of beliefs over nodes and edges in path P . The last line gradually calls the trustedPath function on increasing values of t \u2208 {1, 2 \u00b7 \u00b7 \u00b7 } and each time returning a set of relations whose belief is at least b. The find function then returns the first element\n2map applies a function to list elements, filter returns list elements which satisfy a predicate, find returns the first element that satisfies the predicate.\n(which itself is a list) in this list whose length is at least N . Handling the corner case where there does not exist a set of N relations of belief at least b, is not shown in this example for brevity. Note that since these programming constructs are lazy-evaluated, the last line terminates as soon as it explores the sub-graphs rooted at \u2018cup\u2019 nodes until the depth required for finding N relations with belief at least b."}, {"heading": "C. Task Specific Functions", "text": "RaQueL provides functions for special tasks such as semantic labeling of an image, ranking a trajectory based on human preference, finding a set of affordances for a given object, grounding natural language expressions, etc. RaQueL also provides functionalities to work with media files of RoboBrain, such as functions to generate and work with heatmaps, trajectories, videos, etc. For full list of functions supported by RaQueL and more examples, please refer to the RoboBrain website."}, {"heading": "VII. APPLICATIONS", "text": "In this section, we show use of RoboBrain in following three applications: (a) grounding natural language into controllers, (b) anticipating human actions, and (c) planning in presence of humans."}, {"heading": "A. Grounding Natural Language", "text": "In the context of a given environment and natural language discourse, we define the problem of grounding a natural language command as coming up with an action plan for a robot that accomplishes close to the given command.\nIt is challenging to ground natural language for an open set of tasks. For example, grounding the sentence \u201cget me a cup of hot water\u201d requires that the robot must know the appearance of cup, it should be able to infer that cup has the containable affordance. The robot must be able to classify if the cup has water or not and in case the cup is empty then the robot must know that objects such as tap, fridge dispenser, etc. can potentially fill it with water. It must also know that objects such as stove, microwave can heat an object. Finally, robot should be able to manipulate these devices to accomplish the required sub-goals. Specifically, this consists of knowledge of environment such as manipulation features of objects, object affordances, prior constraints over state-action space etc.; of language such as synset relations, POS tags and knowledge that requires both such as grounding of noun-phrases like \u201cred mug\u201d. We now take specific examples to show how some of these queries are translated to a graph-query on RoboBrain.\nUse of RoboBrain. A common approach to language grounding problem is to maximize the likelihood P (I|E,L) of the grounded instruction sequence I given the environment E and the natural language command L. Figure 5 shows such a setting for the task of making recipes. The instruction sequences typically consist of action primitives such as pour, grasp, keep etc. that are suitable for high-level planning. For example, the instruction sequence corresponding the command \u201cfill a cup with water\u201d could be:\ncereal\nmoveto(cup01); grasp(cup01); moveto(sink01); keep(cup01, on, sink01); toggle(sink knob01); wait(); toggle(sink knob01);\nHowever not all instructions are meaningful (trying to pour from a book), and not all instruction sequences are valid (keeping cup inside microwave without opening the door). In order to come up with meaningful instruction sequence I requires knowing what actions can be performed with a given object and what sequences of instructions are permitted. Misra et al. [44] used strips language to represent pre-conditions for a given instruction.\nFor example, in Figure 5 the action primitive squeeze(syrup1,pot1) is meaningful only if it satisfies the following predicate: \u2022 grasping pr2 syrup1: Robot is grasping the syrup. \u2022 squeezeable syrup1: Syrup bottle should be squeezeable. \u2022 on syrup1 pot1: Syrup bottle should be placed directly above the pot. RoboBrain is used to compute satisfiability of these predicates which are challenging to compute for an arbitrary object. For example, the satisfiability of the second predicate, for text modality, is written as the following RaQueL query:\nsqueezeable syrup1 = len fetch (u{name : \u2018syrup\u2032})\u2192 [\u2018HasAffordance\u2032]\u2192 (v{name : \u2018squeezeable\u2032}) > 0\nOnce the robot finds the optimum instruction sequence, it needs to know how to translate action primitives such as pour(cup01) into trajectories. This is challenging since the trajectory would depend upon the object parameter (consider pour(cup01) and pour(kettle01)). These are translated to queries to RoboBrain as follows:\npour cup01 = fetch ({name : \u2018pour\u2032})\u2192 (v{name : \u2018HasTrajectory\u2032})\u2190 ({name : \u2018cup\u2032})\nThe above example returns all the trajectory nodes(containing trajectory parameters) that are connected to nodes with name cup and nodes with name pour. Note that since RoboBrain returns the result with beliefs that are continuously updated by user feedback, we further order them by rank and use the highest belief node or apply more specific filters, such as safety."}, {"heading": "B. Anticipating Human actions", "text": "Assistive robots working with humans need to reactively respond to the changes in their environments. In addition to understanding what can be done in the environment and detect the human activities, the robots also need to anticipate which activities will a human do next (and how). This enables an assistive robot to plan ahead for reactive responses [35].\nIn this application, our goal is to predict the future activities as well as the details of how a human is going to perform them in short-term (e.g., 1-10 seconds). For example, if a robot has seen a person move his hand to a coffee mug, it is possible he would move the coffee mug to a few potential places such as his mouth, to a kitchen sink or to a different location on the table. In order to predict this, we model the activities as a conditional random field which captures the rich spatial-temporal relations through object affordances [35]. However, the challenge is that there are a large number of objects and affordances that need to be stored and updated as new affordances are discovered. Use of RoboBrain. In this work, we use RoboBrain to store the learned activity, affordance and trajectory parameters and query for the appropriate knowledge as and when required by the anticipation algorithm. More specifically, there are two sets of queries issued to the RoboBrain. We first query for the RoboBrain for node and edge parameters of the conditional random field model as shown below:\nparents n := fetch (v)\u2192 [\u2018HasParameters\u2032]\u2192 ({handle : n}) parameters n := fetch ({name : n})\u2192 [\u2018HasParameters\u2032]\u2192 (v{src : \u2018Activity\u2032}) node parameters a = filter(\u03bbu\u2192 len parents u = 1)parameters a edge parameters a1 a2 = filter(\u03bbu\u2192 len parents u = 2 and\nu in parameters a2) parameters a1\nThe queries node parameters and edge parameters are issued with all activities of interest and pairs of them as parameters, respectively. The returned parameters are then used by the robot to infer activities and affordances in its environment. Once the activities of the past are predicted, the second set of queries, shown below, are issued to the RoboBrain for generating possible futures.\naffordances n := fetch ({name : n})\u2192 [\u2018HasAffordance\u2032]\u2192 (v{src : \u2018Affordance\u2032}) trajectories a := fetch ({handle : a})\u2192 [\u2018HasParameters\u2032]\u2192 (v{src : \u2018Affordance\u2032, type : \u2018Trajectory\u2032})\ntrajectory parameters o = map(\u03bba\u2192 trajectories a) affordances o\nThe affordances query is issued for each object of interest in the environment in order to obtain the possible future object affordances and hence the actions that can be performed with them. For each retrieved object affordance, the trajectories query is issued to fetch the corresponding motion trajectory parameters, using which the anticipation algorithm generates the most likely futures and ranks them (see Figure 6).\nQuerying the RoboBrain for activity and affordance parameters allows the anticipation algorithm to scale to new activities and affordances. For example, as other works on learning activities and affordances [38, 61] use RoboBrain, these will become available to the anticipation application as well.\nFeedback to RoboBrain. In addition to querying RoboBrain for affordance and trajectory parameters for predicting the future, the anticipation algorithm can also provide feedback on the quality of the information returned by the query. For example, when queried for the possible affordances of the object cup, if an incorrect affordance is returned, say writable. The anticipation algorithm gives the possibility of writing with a cup a very low score. This information is communicated back to RoboBrain which is useful for updating the beliefs in the knowledge graph."}, {"heading": "C. Planning in presence of humans", "text": "One key problem robots face in performing tasks in human environments is identifying trajectories desirable to the users. An appropriate trajectory not only needs to be valid from a geometric standpoint (i.e., feasible and obstacle-free), but it also needs to satisfy the user preferences [26, 27]. For\nexample, a household robot should move a glass of water in an upright position while maintaining a safe distance from nearby electronic devices. Similarly, robot should move sharp objects such as knife strictly away from nearby humans [25].\nThese preferences are commonly represented as cost functions, parameters of which are learned in a data-driven manner. The cost functions for generating good trajectories jointly model the environment, task and trajectories. This joint modeling is expensive in terms of resource requirement and data collection, and typically research groups have independently learned different cost functions over the years [26, 36, 31] that are not shared across the research groups. Use of RoboBrain. We now demonstrate use of RoboBrain for planning. Through RoboBrain, robots can share cost function parameters for different planning problems, such as indoor 2D-navigation, manipulation planning, high-level symbolic planning etc., as shown in Figure 7. In addition to this, robots can also query RoboBrain for expressive trajectories features to instantiate cost functions for predicting good trajectories. This kind of sharing will allow research groups to share their learned cost functions and trajectory features.\nFigure 7 shows how PlanIt planning system [27] queries RoboBrain for trajectory parameters for the task at hand. The robot\u2019s goal is to move an egg carton to the other end of\ntable. Since eggs are fragile users would prefer to move them slowly and close to table surface. In order to complete the task, robot first queries RoboBrain for labeling of objects in the environment. For this it uses task specific semanticLabeling function described in Section VI-C.\nobjects = semanticLabeling \u201cenvironment.png\u201d\nAfter object labels are obtained it locates the egg carton and queries for its attributes, which the RoboBrain returns as fragile. Finally it queries for the trajectory parameters of fragile objects. Below we show the RaQueL queries.\nattributes n := fetch ({name : n})\u2192 [\u2018HasAttribute\u2032]\u2192 (v) trajectories a := fetch ({handle : a})\u2192 [\u2018HasTrajectory\u2032]\u2192 (v) trajectory parameters = map(\u03bba\u2192 trajectories a) attributes \u2018egg\u2032\nAfter getting the cost function parameters robot samples trajectories and executes the top-ranked trajectory.\nFeedback to RoboBrain. Sometimes the robot may not be certain about the optimal trajectory, when it can ask the users for feedback. This feedback is directly registered in RoboBrain and trajectory parameters are updated for future queries, as shown in Figure 7. A user can provide various kinds of trajectory feedback. For example, in Figure 7 user provides coactive feedback: the robot displays top three ranked trajectories and user selects the best one [26]. In another example, RoboBrain also improves with feedback provided physically on the robot. In Figure 8, the user directly improves a trajectory by correcting one of the trajectory waypoints. Such feedback can thus help improve all the robots that use RoboBrain."}, {"heading": "VIII. DISCUSSION AND CONCLUSION", "text": "RoboBrain is a collaborative effort to create a knowledge engine that acquires and integrates knowledge about the physical world the robots live in from several sources with multiple modalities. We described some technical challenges we have addressed, such as the multi-modality of knowledge, never-ending learning, large-scale nature of the system, use of crowd-sourcing, and allowing queries with RaQueL for\nbuilding applications with RoboBrain. We have shown how to use RoboBrain in three different areas: perception, planning and natural language grounding. We showed that sharing knowledge across different robotic tasks allows them to scale their learning to new situations.\nRoboBrain is an ongoing effort, where we are constantly improving different aspects of the work. We are improving the system architecture and expanding RaQueL to support scaling to even larger knowledge sources (e.g., millions of videos). Furthermore, we have several ongoing research efforts that include achieving better disambiguation and improving neverending learning abilities. More importantly, we are constantly expanding the set of our RoboBrain research partners. This will not only improve the abilities of their robots, but also their contribution of knowledge to RoboBrain will help other researchers in the robotics community at large."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Arzav Jain for his help in building the graph infrastructure for the RoboBrain. We thank Bart Selman, Jitendra Malik and Ken Goldberg for their visionary ideas. We thank Emin Gun Sirer for help with databases, and thank Silvio Savarese, Stefanie Tellex, Fei-Fei Li, and Thorsten Joachims for useful discussions. We also thank Yun Jiang, Ian Lenz, Jaeyong Sung, Chenxia Wu, and Ayush Dubey for their contributions to the knowledge for the RoboBrain. We also thank Michela Meister, Hope Casey-Allen, and Gabriel Kho for their help and involvement with the RoboBrain project. We are also very thankful to Debarghya Das and Kevin Lee for their help with developing the front-end for the RoboBrain.\nThis work was supported in part by Army Research Office (ARO) award W911NF-12-1-0267, Office of Naval Research (ONR) award N00014-14-1-0156, NSF National Robotics Initiative (NRI) award IIS-1426744, Google Faculty Research award (to Saxena), and Qualcomm research award. This was also supported in part by Google PhD Fellowship to Koppula, and by Microsoft Faculty Fellowship, NSF CAREER Award and Sloan Fellowship to Saxena."}], "references": [{"title": "Toward human-aware robot task planning", "author": ["R. Alami", "A. Clodic", "V. Montreuil", "E.A. Sisbot", "R. Chatila"], "venue": "AAAI Spring Symposium: To Boldly Go Where No Human-Robot Team Has Gone Before,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Contextually guided semantic labeling and search for 3d point clouds", "author": ["A. Anand", "H.S. Koppula", "T. Joachims", "A. Saxena"], "venue": "IJRR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Guest editorial: A revolution in the warehouse: A retrospective on kiva systems and the grand challenges ahead", "author": ["R.D. Andrea"], "venue": "IEEE Tran. on Automation Science and Engineering (T-ASE), 9(4),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Robotic roommates making pancakes", "author": ["M. Beetz", "U. Klank", "I. Kresse", "A. Maldonado", "L. Mosenlechner", "D. Pangercic", "T. Ruhr", "M. Tenorth"], "venue": "Humanoids,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proc. ACM SIGMOD, pages 1247\u20131250,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Interpreting and executing recipes with a cooking robot", "author": ["M. Bollini", "S. Tellex", "T. Thompson", "M. Roy", "D. Rus"], "venue": "ISER,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic state estimation techniques for autonomous and decision support systems", "author": ["W. Burgard", "D. Fox", "S. Thrun"], "venue": "Informatik Spektrum, 34(5),", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Human preferences for robot-human hand-over configurations", "author": ["M. Cakmak", "S.S. Srinivasa", "M.K. Lee", "J. Forlizzi", "S.B. Kiesler"], "venue": "IROS,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Toward an architecture for never-ending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka Jr", "T. M Mitchell"], "venue": "AAAI, volume 5, page 3,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "NEIL: Extracting Visual Knowledge from Web Data", "author": ["X. Chen", "A. Shrivastava", "A. Gupta"], "venue": "ICCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Scene semantics from long-term observation of people", "author": ["V. Delaitre", "D. Fouhey", "I. Laptev", "J. Sivic", "A. Gupta", "A. Efros"], "venue": "Proc. ECCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L-J Li", "K. Li", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Path planning for autonomous vehicles in unknown semi-structured environments", "author": ["D. Dolgov", "S. Thrun", "M. Montemerlo", "J. Diebel"], "venue": "IJRR, 29(5),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X.L. Dong", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "KDD,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "RSS, June", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal symbolic planning with action costs and preferences", "author": ["S. Edelkamp", "P. Kissmann"], "venue": "IJCAI,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "WordNet", "author": ["C. Fellbaum"], "venue": "Wiley Online Library,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Building watson: An overview of the deepqa project", "author": ["D. Ferrucci", "E. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J. Prager"], "venue": "AI magazine,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Introduction to this is watson", "author": ["D.A. Ferrucci"], "venue": "IBM J. of RnD, 56(3.4): 1\u20131,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning haptic representation for manipulating deformable food objects", "author": ["M. Gemici", "A. Saxena"], "venue": "IROS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating human-like motion for robots", "author": ["M.J. Gielniak", "C. Karen Liu", "A.L. Thomaz"], "venue": "IJRR, 32(11),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning rich features from RGB-D images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbelaez", "J. Malik"], "venue": "Proc. ECCV,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Yago2: a spatially and temporally enhanced knowledge base from wikipedia", "author": ["J. Hoffart", "F. M Suchanek", "K. Berberich", "G. Weikum"], "venue": "Artificial Intelligence, 194:28\u201361,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Beyond geometric path planning: Learning context-driven trajectory preferences via sub-optimal feedback", "author": ["A. Jain", "S. Sharma", "A. Saxena"], "venue": "ISRR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning trajectory preferences for manipulators via iterative improvement", "author": ["A. Jain", "B. Wojcik", "T. Joachims", "A. Saxena"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Planit: A crowdsourced approach for learning to plan paths from large scale preference feedback", "author": ["A. Jain", "D. Das", "J.K. Gupta", "A. Saxena"], "venue": "arXiv preprint arXiv:1406.2616,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "3d reasoning from blocks to stability", "author": ["Z. Jia", "A. Gallagher", "A. Saxena", "T. Chen"], "venue": "IEEE PAMI,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Hallucinated humans as the hidden context for labeling 3d scenes", "author": ["Y. Jiang", "H. Koppula", "A. Saxena"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Perceiving, learning, and exploiting object affordances for autonomous pile manipulation", "author": ["D. Katz", "A. Venkatraman", "M. Kazemi", "J.A. Bagnell", "A. Stentz"], "venue": "Autonomous Robots, 37(4),", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Activity forecasting", "author": ["K. Kitani", "B.D. Ziebart", "J.A. Bagnell", "M. Hebert"], "venue": "Proc. ECCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Ikeabot: An autonomous multi-robot coordinated furniture assembly system", "author": ["R.A. Knepper", "T. Layton", "J. Romanishin", "D. Rus"], "venue": "ICRA,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Anticipatory planning for humanrobot teams", "author": ["H. Koppula", "A. Jain", "A. Saxena"], "venue": "ISER,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Physically grounded spatio-temporal object affordances", "author": ["H.S. Koppula", "A. Saxena"], "venue": "Proc. ECCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H.S. Koppula", "A. Saxena"], "venue": "RSS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-based prediction of trajectories for socially compliant navigation", "author": ["M. Kuderer", "H. Kretzschmar", "C. Sprunk", "W. Burgard"], "venue": "RSS,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A Large-Scale Hierarchical Multi- View RGB-D Object Dataset", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "ICRA,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical representation for future action prediction", "author": ["T. Lan", "T.C. Chen", "S. Savarese"], "venue": "Proc. ECCV,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward safe close-proximity human-robot interaction with standard industrial robots", "author": ["P.A. Lasota", "G.F. Rossano", "J.A. Shah"], "venue": "CASE,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Cyc: A large-scale investment in knowledge infrastructure", "author": ["D. B Lenat"], "venue": "Communications of the ACM, 38(11):33\u201338,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "RSS,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-robot collaborative manipulation planning using early prediction of human motion", "author": ["J. Mainprice", "D. Berenson"], "venue": "IROS,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Scene signatures: Localised and point-less features for localisation", "author": ["C. McManus", "B. Upcroft", "P. Newmann"], "venue": "RSS,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Tell me dave: Contextsensitive grounding of natural language to mobile manipulation instructions", "author": ["D.K. Misra", "J. Sung", "K. Lee", "A. Saxena"], "venue": "RSS,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["A.R. Mohamed", "T.N. Sainath", "G. Dahl", "B. Ramabhadran", "G.E. Hinton", "M.A. Picheny"], "venue": "(ICASSP), pages 5060\u20135063,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust visual robot localization across seasons using network flows", "author": ["T. Naseer", "L. Spinello", "W. Burgard", "C. Stachniss"], "venue": "AAAI,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Human-robot collaboration in manufacturing: Quantitative evaluation of predictable, convergent joint action", "author": ["S. Nikolaidis", "P.A. Lasota", "G.F. Rossano", "C. Martinez", "T.A. Fuhlbrigge", "J.A. Shah"], "venue": "ISR,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Ros: an open-source robot operating system", "author": ["M. Quigley", "B. Gerkey", "K. Conley", "J. Faust", "T. Foote", "J. Leibs", "E. Berger", "R. Wheeler", "A. Ng"], "venue": "ICRA Workshop on Open Source Software,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Imitation learning for locomotion and manipulation", "author": ["N.D. Ratliff", "J.A. Bagnell", "S.S. Srinivasa"], "venue": "Int. Conf. on Humanoid Robots,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Planning as satisfiability: Heuristics", "author": ["J. Rintanen"], "venue": "Artificial Intelligence, 193:45\u201386,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning sound location from a single microphone", "author": ["A. Saxena", "A.Y. Ng"], "venue": "ICRA,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding locally optimal, collision-free trajectories with sequential convex optimization", "author": ["J. Schulman", "J. Ho", "A.X. Lee", "I. Awwal", "H. Bradlow", "P. Abbeel"], "venue": "RSS,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Cloth grasp point detection based on multiple-view geometric cues with application to robotic towel folding", "author": ["J.M. Shepard", "M.C. Towner", "J. Lei", "P. Abbeel"], "venue": "ICRA,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2010}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "WWW, pages 697\u2013706,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S.J. Teller", "N. Roy"], "venue": "AAAI,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "Asking for help using inverse semantics", "author": ["S. Tellex", "R. Knepper", "A. Li", "D. Rus", "N. Roy"], "venue": "RSS,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Roboearth: A world wide web for robots", "author": ["M. Waibel", "M. Beetz", "R. D\u2019Andrea", "R. Janssen", "M. Tenorth", "J. Civera", "J. Elfring", "D. G\u00e1lvez-L\u00f3pez", "K. H\u00e4ussermann", "J. Montiel", "A. Perzylo", "B. Schie\u00dfle", "A. Zweigle", "R. van de Molengraft"], "venue": "IEEE Robotics & Automation Magazine,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "Patch to the future: Unsupervised visual prediction", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "CVPR,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical semantic labeling for taskrelevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "RSS,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Reasoning about object affordances in a knowledge base representation", "author": ["Y. Zhu", "A. Fathi", "Li Fei-Fei"], "venue": "In Proc. ECCV,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2014}, {"title": "CHOMP: covariant hamiltonian optimization for motion planning", "author": ["M. Zucker", "N.D. Ratliff", "A.D. Dragan", "M.K.M. Pivtoraiko", "C.M. Dellin", "J.A. Bagnell", "S.S. Srinivasa"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Examples include Google knowledge graph [15], IBM Watson [19], Wikipedia, Apple Siri, and many others.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "Examples include Google knowledge graph [15], IBM Watson [19], Wikipedia, Apple Siri, and many others.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Inspired by them, researchers have aggregated domain specific knowledge by mining data [4, 6], processing natural language [10], images [13] and speech [45].", "startOffset": 87, "endOffset": 93}, {"referenceID": 5, "context": "Inspired by them, researchers have aggregated domain specific knowledge by mining data [4, 6], processing natural language [10], images [13] and speech [45].", "startOffset": 87, "endOffset": 93}, {"referenceID": 9, "context": "Inspired by them, researchers have aggregated domain specific knowledge by mining data [4, 6], processing natural language [10], images [13] and speech [45].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "Inspired by them, researchers have aggregated domain specific knowledge by mining data [4, 6], processing natural language [10], images [13] and speech [45].", "startOffset": 136, "endOffset": 140}, {"referenceID": 44, "context": "Inspired by them, researchers have aggregated domain specific knowledge by mining data [4, 6], processing natural language [10], images [13] and speech [45].", "startOffset": 152, "endOffset": 156}, {"referenceID": 43, "context": "Currently, the partners include Tell Me Dave [44], Tellex\u2019s group [56], PlanIt [26] and affordances [29].", "startOffset": 45, "endOffset": 49}, {"referenceID": 54, "context": "Currently, the partners include Tell Me Dave [44], Tellex\u2019s group [56], PlanIt [26] and affordances [29].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "Currently, the partners include Tell Me Dave [44], Tellex\u2019s group [56], PlanIt [26] and affordances [29].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Currently, the partners include Tell Me Dave [44], Tellex\u2019s group [56], PlanIt [26] and affordances [29].", "startOffset": 100, "endOffset": 104}, {"referenceID": 39, "context": "sense knowledge (Cyc [40]) and lexical knowledge of english language (WordNet [18]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "sense knowledge (Cyc [40]) and lexical knowledge of english language (WordNet [18]).", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "KBs started to use crowdsourcing (DBPedia [4], Freebase [6]) and automatic information extraction methods (Yago [55, 24]) for mining knowledge.", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "KBs started to use crowdsourcing (DBPedia [4], Freebase [6]) and automatic information extraction methods (Yago [55, 24]) for mining knowledge.", "startOffset": 56, "endOffset": 59}, {"referenceID": 53, "context": "KBs started to use crowdsourcing (DBPedia [4], Freebase [6]) and automatic information extraction methods (Yago [55, 24]) for mining knowledge.", "startOffset": 112, "endOffset": 120}, {"referenceID": 23, "context": "KBs started to use crowdsourcing (DBPedia [4], Freebase [6]) and automatic information extraction methods (Yago [55, 24]) for mining knowledge.", "startOffset": 112, "endOffset": 120}, {"referenceID": 9, "context": "Examples include NELL [10] and NEIL [11].", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "Examples include NELL [10] and NEIL [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "Imagenet [13] enriched the wordnet synsets with images obtained from large-scale internet search and a crowdsourced approach was used to obtain the object labels.", "startOffset": 9, "endOffset": 13}, {"referenceID": 59, "context": "These object labels were further extended to object affordances [61].", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "In industry, we have seen many successful applications of the existing KBs within the modalities they covered, for example, Google Knowledge Graph and IBM Watson Jeopardy Challenge [20].", "startOffset": 181, "endOffset": 185}, {"referenceID": 36, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 39, "endOffset": 50}, {"referenceID": 1, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 39, "endOffset": 50}, {"referenceID": 58, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 39, "endOffset": 50}, {"referenceID": 22, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 71, "endOffset": 75}, {"referenceID": 42, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 96, "endOffset": 104}, {"referenceID": 45, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 96, "endOffset": 104}, {"referenceID": 29, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 155, "endOffset": 159}, {"referenceID": 27, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 11, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 227, "endOffset": 235}, {"referenceID": 33, "context": "It has been applied to object labeling [37, 2, 60], scene segmentation [23], robot localization [43, 46], feature extraction for planning and manipulation [30], understanding environment constraints [28] and object affordances [12, 34].", "startOffset": 227, "endOffset": 235}, {"referenceID": 13, "context": "Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41].", "startOffset": 203, "endOffset": 210}, {"referenceID": 7, "context": "Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41].", "startOffset": 203, "endOffset": 210}, {"referenceID": 30, "context": "Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41].", "startOffset": 225, "endOffset": 237}, {"referenceID": 34, "context": "Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41].", "startOffset": 225, "endOffset": 237}, {"referenceID": 57, "context": "Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41].", "startOffset": 225, "endOffset": 237}, {"referenceID": 35, "context": "Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41].", "startOffset": 263, "endOffset": 267}, {"referenceID": 40, "context": "Sharing representations from visual information not only improve the performance of each of the perception tasks, but also significantly help various applications such as autonomous or assistive driving [14, 8], anticipation [31, 35, 59], planning sociable paths [36] and for various household chores such as grasping and cutting [41].", "startOffset": 330, "endOffset": 334}, {"referenceID": 50, "context": "Sharing representations from other modalities such as sound [52] and haptics [21] would also improve perception.", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "Sharing representations from other modalities such as sound [52] and haptics [21] would also improve perception.", "startOffset": 77, "endOffset": 81}, {"referenceID": 60, "context": "Broadly the planning algorithms can be categorized as motion planning [62, 53], task planning [1, 7] and symbolic planning [17, 51].", "startOffset": 70, "endOffset": 78}, {"referenceID": 51, "context": "Broadly the planning algorithms can be categorized as motion planning [62, 53], task planning [1, 7] and symbolic planning [17, 51].", "startOffset": 70, "endOffset": 78}, {"referenceID": 0, "context": "Broadly the planning algorithms can be categorized as motion planning [62, 53], task planning [1, 7] and symbolic planning [17, 51].", "startOffset": 94, "endOffset": 100}, {"referenceID": 6, "context": "Broadly the planning algorithms can be categorized as motion planning [62, 53], task planning [1, 7] and symbolic planning [17, 51].", "startOffset": 94, "endOffset": 100}, {"referenceID": 16, "context": "Broadly the planning algorithms can be categorized as motion planning [62, 53], task planning [1, 7] and symbolic planning [17, 51].", "startOffset": 123, "endOffset": 131}, {"referenceID": 49, "context": "Broadly the planning algorithms can be categorized as motion planning [62, 53], task planning [1, 7] and symbolic planning [17, 51].", "startOffset": 123, "endOffset": 131}, {"referenceID": 6, "context": "Bakebot [7], towel-folding [54], IkeaBot [32] and robots preparing pancakes [5] are few of the many successful planning applications.", "startOffset": 8, "endOffset": 11}, {"referenceID": 52, "context": "Bakebot [7], towel-folding [54], IkeaBot [32] and robots preparing pancakes [5] are few of the many successful planning applications.", "startOffset": 27, "endOffset": 31}, {"referenceID": 31, "context": "Bakebot [7], towel-folding [54], IkeaBot [32] and robots preparing pancakes [5] are few of the many successful planning applications.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "Bakebot [7], towel-folding [54], IkeaBot [32] and robots preparing pancakes [5] are few of the many successful planning applications.", "startOffset": 76, "endOffset": 79}, {"referenceID": 48, "context": "in a data-driven manner [50, 26].", "startOffset": 24, "endOffset": 32}, {"referenceID": 25, "context": "in a data-driven manner [50, 26].", "startOffset": 24, "endOffset": 32}, {"referenceID": 46, "context": "aspects, such as human-robot collaboration for task completion [48, 33], generating safe robot motion near humans", "startOffset": 63, "endOffset": 71}, {"referenceID": 32, "context": "aspects, such as human-robot collaboration for task completion [48, 33], generating safe robot motion near humans", "startOffset": 63, "endOffset": 71}, {"referenceID": 41, "context": "[42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc.", "startOffset": 0, "endOffset": 8}, {"referenceID": 38, "context": "[42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc.", "startOffset": 35, "endOffset": 38}, {"referenceID": 21, "context": "[42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc.", "startOffset": 82, "endOffset": 90}, {"referenceID": 15, "context": "[42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc.", "startOffset": 82, "endOffset": 90}, {"referenceID": 55, "context": "[42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc.", "startOffset": 129, "endOffset": 137}, {"referenceID": 43, "context": "[42, 39], obeying user preferences [9], generating human like and legible motions [22, 16], interaction through natural language [57, 44], etc.", "startOffset": 129, "endOffset": 137}, {"referenceID": 47, "context": "Previous efforts on connecting robots range from creating a common operating system (ROS) for developing robot applications [49] to sharing data acquired by various robots in the cloud [58, 3].", "startOffset": 124, "endOffset": 128}, {"referenceID": 56, "context": "Previous efforts on connecting robots range from creating a common operating system (ROS) for developing robot applications [49] to sharing data acquired by various robots in the cloud [58, 3].", "startOffset": 185, "endOffset": 192}, {"referenceID": 2, "context": "Previous efforts on connecting robots range from creating a common operating system (ROS) for developing robot applications [49] to sharing data acquired by various robots in the cloud [58, 3].", "startOffset": 185, "endOffset": 192}, {"referenceID": 56, "context": "For example, RoboEarth [58] provides a platform for the robots to store and off-load computation to the cloud and communicate to other robots; and KIVA systems [3] use the cloud to coordinate motion and update tracking data for hundreds of mobile platforms.", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "For example, RoboEarth [58] provides a platform for the robots to store and off-load computation to the cloud and communicate to other robots; and KIVA systems [3] use the cloud to coordinate motion and update tracking data for hundreds of mobile platforms.", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": ", [13]), our work focuses on taking crowd-sourced feedback at many levels:", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "There are several partner projects to RoboBrain, such as Tell Me Dave [44], PlanIt [26], and so on.", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "There are several partner projects to RoboBrain, such as Tell Me Dave [44], PlanIt [26], and so on.", "startOffset": 83, "endOffset": 87}, {"referenceID": 43, "context": "[44] used strips language to represent pre-conditions for a given instruction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "This enables an assistive robot to plan ahead for reactive responses [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "In order to predict this, we model the activities as a conditional random field which captures the rich spatial-temporal relations through object affordances [35].", "startOffset": 158, "endOffset": 162}, {"referenceID": 37, "context": "For example, as other works on learning activities and affordances [38, 61] use RoboBrain, these will become available to the anticipation application as well.", "startOffset": 67, "endOffset": 75}, {"referenceID": 59, "context": "For example, as other works on learning activities and affordances [38, 61] use RoboBrain, these will become available to the anticipation application as well.", "startOffset": 67, "endOffset": 75}, {"referenceID": 25, "context": "it also needs to satisfy the user preferences [26, 27].", "startOffset": 46, "endOffset": 54}, {"referenceID": 26, "context": "it also needs to satisfy the user preferences [26, 27].", "startOffset": 46, "endOffset": 54}, {"referenceID": 24, "context": "Similarly, robot should move sharp objects such as knife strictly away from nearby humans [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "This joint modeling is expensive in terms of resource requirement and data collection, and typically research groups have independently learned different cost functions over the years [26, 36, 31] that", "startOffset": 184, "endOffset": 196}, {"referenceID": 35, "context": "This joint modeling is expensive in terms of resource requirement and data collection, and typically research groups have independently learned different cost functions over the years [26, 36, 31] that", "startOffset": 184, "endOffset": 196}, {"referenceID": 30, "context": "This joint modeling is expensive in terms of resource requirement and data collection, and typically research groups have independently learned different cost functions over the years [26, 36, 31] that", "startOffset": 184, "endOffset": 196}, {"referenceID": 26, "context": "Figure 7 shows how PlanIt planning system [27] queries RoboBrain for trajectory parameters for the task at hand.", "startOffset": 42, "endOffset": 46}, {"referenceID": 26, "context": "Robot queries RoboBrain, through PlanIt planning system [27], for trajectory parameters for planning fragile objects such as egg cartons.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "For example, in Figure 7 user provides coactive feedback: the robot displays top three ranked trajectories and user selects the best one [26].", "startOffset": 137, "endOffset": 141}], "year": 2014, "abstractText": "In this paper we introduce a knowledge engine, which learns and shares knowledge representations, for robots to carry out a variety of tasks. Building such an engine brings with it the challenge of dealing with multiple data modalities including symbols, natural language, haptic senses, robot trajectories, visual features and many others. The knowledge stored in the engine comes from multiple sources including physical interactions that robots have while performing tasks (perception, planning and control), knowledge bases from WWW and learned representations from leading robotics research groups. We discuss various technical aspects and associated challenges such as modeling the correctness of knowledge, inferring latent information and formulating different robotic tasks as queries to the knowledge engine. We describe the system architecture and how it supports different mechanisms for users and robots to interact with the engine. Finally, we demonstrate its use in three important research areas: grounding natural language, perception, and planning, which are the key building blocks for many robotic tasks. This knowledge engine is a collaborative effort and we call it RoboBrain.", "creator": "LaTeX with hyperref package"}}}