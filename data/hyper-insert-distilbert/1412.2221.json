{"id": "1412.2221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2014", "title": "Declarative Statistical Modeling with Datalog", "abstract": "adaptive formalisms for specifying statistical models, such as linear probabilistic - programming languages, typically routinely consist of two components : determining a specification representative of a stochastic process ( the generic prior ), and presenting a specification of observations that will restrict the probability procedure space to a conditional correlated subspace ( except the posterior ). use cases ahead of such formalisms include also the development of sophisticated algorithms in machine language learning and in artificial numerical intelligence. initially we might propose and continuously investigate building a sophisticated declarative computation framework for specifying statistical hardware models on top of matching a database, through an appropriate extension application of normal datalog. by virtue definition of extending datalog, our framework informally offers a weakly natural integration procedure with the template database, and instead has a robust declarative semantics. our datalog theoretical extension provides convenient conceptual mechanisms to include numerical smooth probability functions ; in particular, contingent conclusions of rules described may contain fewer values drawn from such discrete functions. the semantics defined of a program is a probability tree distribution over the possible outcomes diagram of the common input database with respect to the database program ; ensuring these predictable outcomes are minimal solutions while with limited respect also to modeling a related program with existentially meaningful quantified observed variables in conclusions. observations are clearly naturally incorporated by means of integrity constraints especially over the possibly extensional and intensional entropy relations. unfortunately we clearly focus somewhat on short programs products that use discrete fuzzy numerical inference distributions, independently but even right then computing the space of possible predictable outcomes therefore may simply be uncountable ( as a solution can be infinite ). we define accordingly a probability optimization measure over possible outcomes by naturally applying mainly the known validity concept of cylinder sets to a probabilistic chase algorithm procedure. where we moreover show that even the resulting semantics is robust even under no different chases. we also identify structural conditions guaranteeing that virtually all possible outcomes also are finite ( and through then perhaps the probability space definition is also discrete ). we argue precisely that the performance framework we actually propose for retains about the purely theoretical declarative or nature component of standard datalog, defined and ultimately allows for natural specifications of several statistical program models.", "histories": [["v1", "Sat, 6 Dec 2014 11:04:14 GMT  (46kb)", "https://arxiv.org/abs/1412.2221v1", "14 pages, 4 figures"], ["v2", "Mon, 5 Jan 2015 19:49:24 GMT  (48kb)", "http://arxiv.org/abs/1412.2221v2", "14 pages, 4 figures"]], "COMMENTS": "14 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.DB cs.AI cs.PL", "authors": ["vince barany", "balder ten cate", "benny kimelfeld", "dan olteanu", "zografoula vagena"], "accepted": false, "id": "1412.2221"}, "pdf": {"name": "1412.2221.pdf", "metadata": {"source": "CRF", "title": "Declarative Statistical Modeling with Datalog", "authors": ["Vince Barany"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n22 21\nv2 [\ncs .D\nB ]\n5 J\nan 2\n01 5"}, {"heading": "1. INTRODUCTION", "text": "Formalisms for specifying general statistical models are commonly used for developing machine learning and artificial intelligence algorithms for problems that involve inference under uncertainty. A substantial scientific effort has been made on developing such formalisms and corresponding system implementations. An intensively studied concept in that area is that of Probabilistic Programming [21] (PP), where the idea is that the programming language allows for building general random procedures, while the system executes the program not in the standard programming sense, but rather by means of inference. Hence, a PP system is built around a language and an inference engine (which is typically based on variants of Markov Chain Monte Carlo, most notably Metropolis-Hastings). An inference task is a probability-aware aggregate operation over all the possible worlds, such as finding the most likely possible world, or estimating the probability of an event (which\nis phrased over the outcome of the program). Recently, DARPA initiated the project of Probabilistic Programming for Advancing Machine Learning, aimed at advancing PP systems (with a focus on a specific collection of systems, e.g., [29,31,37]) towards facilitating the development of algorithms based on machine learning. In probabilistic programming, a statistical model is typically phrased by means of two components. The first component is a generative process that produces a random possible world by straightforwardly following instructions with randomness, and in particular, sampling from common numerical probability functions; this gives the prior distribution. The second component allows to phrase constraints that the relevant possible worlds should satisfy, and, semantically, transforms the prior distribution into the posterior distribution\u2014the subspace conditional on the constraints. As an example, in supervised text classification (e.g., spam detection) the goal is to classify a text document into one of several known classes (e.g., spam/nonspam). Training data consists of a collection of documents labeled with classes, and the goal of learning is to build a model for predicting the classes of unseen documents. One common approach to this task assumes a generative process that produces random parameters for every class, and then uses these parameters to define a generator of random words in documents of the corresponding class [30, 32]. So, the prior distribution generates parameters and documents for each class, and the posterior is defined by the actual documents of the training data. In unsupervised text classification the goal is to cluster a given set of documents, so that different clusters correspond to different topics (which are not known in advance). Latent Dirichlet Allocation [9] approaches this problem in a similar generative way as the above, with the addition that each document is associated with a distribution over topics. While the agenda of probabilistic programming is the deployment of programming languages to developing statistical models, in this framework paper we explore this agenda from the point of view of database programming. Specifically, we propose and investigate an extension of Datalog for declarative specification of statistical models on top of a database. We believe that Datalog can be naturally extended to a language for building statistical models, since its essence is the production of new facts from known (database) facts. Of course, traditionally these facts are deterministic, and our exten-\nsion enables the production of probabilistic facts that, in particular, involve numbers from available numerical distributions. And by virtue of extending Datalog, our framework offers a natural integration with the database, and has a robust declarative semantics: a program is a set of rules that is semantically invariant under transformations that retain logical equivalence. Moreover, the semantics of a program (i.e., the probability space it specifies) is fully determined by the satisfaction of rules, and does not depend on the specifics of any execution engine. In par with languages for probabilistic programming, our proposed extension consists of two parts: a generative Datalog program that specifies a prior probability space over (finite or infinite) sets of facts that we call possible outcomes, and a definition of the posterior probability by means of observations, which come in the form of an ordinary logical constraint over the extensional and intensional relations. The generative component of our Datalog extension provides convenient mechanisms to include conventional parameterized numerical probability functions (e.g., Poisson, geometrical, etc.). Syntactically, this extension allows to sample values in the conclusion of rules, according to specified parameterized distributions. As an example, consider the relation Client(ssn, branch, avgVisits) that represents clients of a service provider, along with their associated branch and average number of visits (say, per month). The following distributional rule models a random number of visits for that client in the branch.\nVisits(c, b,Poisson[\u03bb]) \u2190 Client(c, b, \u03bb) (1)\nNote, however, that a declarative interpretation of the above rule is not straightforward. Suppose that we have another rule of the following form:\nVisits(c, b,Poisson[\u03bb]) \u2190 PreferredClient(c, b, \u03bb) (2)\nThen, what would be the semantics if a person is both a client and a preferred client? Do we sample twice for that person? And what if the two \u03bbs of the two facts are not the same? Is sampling according to one rule considered a satisfaction of the other rule? What if we have also the following rule:\nVisits(c, b,Poisson[\u03bb]) \u2190 Client(c, b, \u03bb),Active(b) (3)\nFrom the viewpoint of Datalog syntax, Rule (3) is logically implied by Rule (1), since the premise of Rule (3) implies the premise of Rule (1). Hence, we would like the addition of Rule (3) to have no effect on the program. This means that some rule instantiations will not necessarily fire an actual sampling. To make sense of rules such as the above, we associate with every program G an auxiliary program G\u0302, such that G\u0302 does not use distributions, but is rather an ordinary Datalog program where a rule can have an existentially quantified variable in the conclusion. Intuitively, in our\nexample such a rule states that \u201cif the premise holds, then there exists a fact Visits(c, b, x) where x is associated with the distribution Poisson and the parameter \u03bb.\u201d In particular, if the program contains the aforementioned Rule (1), then Rule (3) has no effect; similarly, if the tuple (c, b, \u03bb) is in both Client and PreferredClient, then in the presence of Rule (2) the outcome does not change if one of these tuples is removed. In this paper we focus on numerical probability distributions that are discrete (e.g., the aforementioned ones). Our framework has a natural extension to continuous distributions (e.g., Gaussian or Pareto), but our analysis requires a nontrivial generalization that we defer to future work. When applying the program G to an input instance I, the probability space is over all the minimal solutions of I w.r.t. G\u0302, such that all the numerical samples have a positive probability. To define the probabilities of a sample in this probability space, we consider two cases. In the case where all the possible outcomes are finite, we get a discrete probability distribution, and the probability of a possible outcome can be defined immediately from its content. But in general, a possible outcome can be infinite, and moreover, the set of all possible outcomes can be uncountable. Hence, in the general case we define a probability measure space. To make the case for the coherence of our definitions (i.e., our definitions yield proper probability spaces), we define a natural notion of a probabilistic chase where existential variables are produced by invoking the corresponding numerical distributions. We use cylinder sets [6] to define a measure space based on a chase, and prove that this definition is robust, since one establishes the same probability measure no matter which chase is used.\nRelated Work. Our contribution is a marriage between probabilistic programming and the declarative specification of Datalog. The key features of our approach are the ability to express probabilistic models concisely and declaratively in a Datalog extension with probability distributions as first-class citizens. Existing formalisms that associate a probabilistic interpretation with logic are either not declarative (at least in the Datalog sense) or depart from the probabilistic programming paradigm (e.g., by lacking the support for numerical probability distributions). We next discuss representative related formalisms and contrast them with our work. They can be classified into three broad categories: (1) imperative specifications over logical structures, (2) logic over probabilistic databases, and (3) indirect specifications over the Herbrand base. (Some of these formalisms belong to more than one category.) The first category includes imperative probabilistic programming languages [43], such as BLOG [31], that can express probability distributions over logical structures, via generative stochastic models that can draw values at random from numerical distributions, and con-\ndition values of program variables on observations. In contrast with closed-universe languages such as SQL and logic programs, BLOG considers open-universe probability models that allow for uncertainty about the existence and identity of objects. Instantiations of this category also do not focus on a declarative specification, and indeed, their semantics is dependent on their particular imperative implementations. P-log [7] is a Prologbased language for specifying Bayesian networks. Although declarative in nature, the semantics inherently assumes a form of acyclicity that allows the rules to be executed serially. Here we are able to avoid such an assumption since our approach is based on the minimal solutions of an existential Datalog program. The formalisms in the second category view the generative part of the specification of a statistical model as a two-step process. In the first step, facts are being randomly generated by a mechanism external to the program. In the second step, a logic program, such as Prolog [27] or Datalog [1], is evaluated over the resulting random structure. This approach has been taken by PRISM [40], the Independent Choice Logic [38], and to a large extent by probabilistic databases [41] and their semistructured counterparts [26]. The focus of our work, in contrast, is on a formalism that completely defines the statistical model, without referring to external processes. One step beyond the second category and closer to our work is taken by uncertainty-aware query languages for probabilistic data such as TriQL [42], I-SQL, and world-set algebra [4, 5]. The latter two are natural analogs to SQL and relational algebra for the case of incomplete information and probabilistic data [4]. They feature constructs such as repair-key, choice-of, possible, certain, and group-worlds-by that can construct possible worlds representing all repairs of a relation with respect to (w.r.t.) key constraints, close the possible worlds by unioning or intersecting them, or group the worlds into sets with the same results to sub-queries. World-set algebra has been extended to (world-set) Datalog, fixpoint, and while-languages [14] to define Markov chains. While such languages cannot explicitly specify probability distributions, they may simulate a specific categorical distribution indirectly using non-trivial programs with specialized language constructs like repair-key on input tuples with weights representing samples from the distribution. MCDB [25] and SimSQL [11] propose SQL extensions (with for-loops and probability distributions) coupled with Monte Carlo simulations and parallel database techniques for stochastic analytics in the database. In contrast, our work focuses on existential Datalog with recursion and probability spaces over the minimal solutions of the data w.r.t. the Datalog program. Formalisms in the third category are indirect specifications of probability spaces over the Herbrand base, which is the set of all the facts that can be obtained\nusing the predicate symbols and the constants of the database. This category includes Markov Logic Networks (MLNs) [15, 33], where the logical rules are used as a compact and intuitive way of defining factors. In other words, the probability of a possible world is the product of all the numbers (factors) that are associated with the rules that the world satisfies. This approach is applied in DeepDive [34], where a database is used for storing relational data and extracted text, and database queries are used for defining the factors of a factor graph. We view this approach as indirect since a rule does not determine directly the distribution of values. Moreover, the semantics of rules is such that the addition of a rule that is logically equivalent to (or implied by, or indeed equal to) an existing rule changes the semantics and thus the probability distribution. A similar approach is taken by Probabilistic Soft Logic [10], where in each possible world every fact is associated with a weight (degree of truth). Further formalisms in this category are probabilistic Datalog [19], probabilistic Datalog+/- [22], and probabilistic logic programming (ProbLog) [27]. In these formalisms, every rule is associated with a probability. For ProbLog, the semantics is not declarative as the rules follow a certain evaluation order; for probabilistic Datalog, the semantics is purely declarative. Both semantics are different from ours and that of the other formalisms mentioned thus far. A Datalog rule is interpreted as a rule over a probability distribution over possible worlds, and it states that, for a given grounding of the rule, the marginal probability of being true is as stated in the rule. Probabilistic Datalog+/- uses MLNs as the underlying semantics. Besides our support for numerical probability distributions, our formalism is used for defining a single probability space, which is in par with the standard practice in probabilistic programming. As said earlier, the programs in our proposed formalism allow for recursion. As we show in the paper, the semantics is captured by Markov chains that may be infinite. Related formalisms are those of the Probabilistic Context-Free Grammar (PCFG) and the more general Recursive Markov Chain (RMC) [17], where the probabilistic specification is by means of a finite set of transition graphs that can call one another (in the sense of method call) in a possibly recursive fashion. In database research, PCFGs and RMCs have been explored in the context of probabilistic XML [8, 13]. Although these formalisms do not involve numerical distributions, in future work we plan to conduct a study of the relative expressive power between them and restrictions of our framework. Moreover, we plan to study whether and how inference techniques on PCFGs and RMCs can be adapted to our framework.\nOrganization. The remainder of the paper is organized as follows. In Section 2 we give basic definitions. The syntax and semantics of generative Datalog is intro-\nduced in Section 3, where we focus on the case where all solutions are finite. In Section 4 we present our adaptation of the chase. The general case of generative Datalog, where solutions can be infinite, is presented in Section 5. We complete our development in Section 6, where generative Datalog is extended with constraints (observations) to form Probabilistic-Programming Datalog (PPDL). Finally, we discuss extensions and future directions in Section 7 and conclude in Section 8."}, {"heading": "2. PRELIMINARIES", "text": "In this section we give some preliminary definitions that we will use throughout the paper.\nSchemas and instances. A (relational) schema is a collection S of relation symbols, where each relation symbol R is associated with an arity, denoted arity(R), which is a natural number. An attribute of a relation symbol R is any number in {1, . . . , arity(R)}. For simplicity, we consider here only databases over real numbers; our examples may involve strings, which we assume are translatable into real numbers. A fact over a schema S is an expression of the form R(c1, . . . , cn) where R is an n-ary relation in S and c1, . . . , cn \u2208 R. An instance I over S is a finite set of facts over S. We will denote by RI the set of all tuples (c1, . . . , cn) such that R(c1, . . . , cn) \u2208 I is a fact of I.\nDatalog programs. In this work we use Datalog with the option of having existential variables in the head [12]. Formally, an existential Datalog program, or just Datalog\u2203 program for short, is a triple D = (E , I,\u0398) where: (1) E is a schema, called the extensional database (EDB) schema, (2) I is a schema, called the intensional database (IDB) schema, and is disjoint from E , and (3) \u0398 is a finite set of Datalog\u2203 rules, i.e.,, first-order formulas of the form\n\u2200x [ \u2203y(\u03c8(x,y)) \u2190 \u03d5(x) ]\nwhere \u03d5(x) is a conjunction of atomic formulas over E \u222aI and \u03c8(x,y) is an atomic formula over I, such that each variable in x occurs in at least one atomic formula of \u03d5. Here, by an atomic formula (or, atom) we mean an expression of the form R(t1, . . . , tn) where R is an n-ary relation and t1, . . . , tn are either constants (i.e., real numbers) or variables. We usually omit the universal quantifiers for readability\u2019s sake. Datalog is the fragment of Datalog\u2203 where the conclusion (left-hand side) of each rule is a single atomic formula without existential quantifiers. Let D = (E , I,\u0398) be a Datalog\u2203 program. An input instance for D is an instance I over E . A solution of I w.r.t. D is a possibly-infinite set F of facts over E \u222a I, such that I \u2286 F and F satisfies all rules in \u0398 (viewed as first-order sentences). A minimal solution of I (w.r.t. D) is a solution F of I such that no proper subset of F is a solution of I. The set of all, finite\nand infinite, minimal solutions of I w.r.t. D is denoted by min-solD(I), and the set of all finite minimal solutions is denoted by min-solfinD (I). It is a well known fact that, if D is a Datalog program (that is, without existential quantifiers), then every input instance I has a unique minimal solution, which is finite, and therefore min-solfinD (I) = min-solD(I).\nProbability spaces. We separately consider discrete and continuous probability spaces. We initially focus on the discrete case; there, a probability space is a pair (\u2126, \u03c0), where \u2126 is a finite or countably infinite set, called the sample space, and \u03c0 : \u2126 \u2192 [0, 1] is such that\u2211\no\u2208\u2126 \u03c0(o) = 1. If (\u2126, \u03c0) is a probability space, then \u03c0 is a probability distribution over \u2126. We say that \u03c0 is a numerical probability distribution if \u2126 \u2286 R. In this work we focus on discrete numerical distributions. A parameterized probability distribution is a function \u03b4 : \u2126 \u00d7 Rk \u2192 [0, 1], such that \u03b4(\u00b7,p) : \u2126 \u2192 [0, 1] is a probability distribution for all p \u2208 Rk. We use pardim(\u03b4) to denote the number k, called the parameter dimensionality of \u03b4. For presentation\u2019s sake, we may write \u03b4(o|p) instead of \u03b4(o,p). Moreover, we denote the (non-parameterized) distribution \u03b4(\u00b7|p) by \u03b4[p]. Examples of (discrete) parameterized distributions follow.\n\u2022 Flip(x|p): \u2126 is {0, 1}, and for a parameter p \u2208 [0, 1] we have Flip(1|p) = p and Flip(0|p) = 1\u2212 p. \u2022 Poisson(x|\u03bb): \u2126 = N, and for a parameter \u03bb \u2208 (0,\u221e) we have Poisson(x|\u03bb) = \u03bbxe\u2212\u03bb/x!. \u2022 Geo(x|p): \u2126 = N, and for a parameter p \u2208 [0, 1] we have Geo(x|p) = (1\u2212 p)xp.\nIn Section 7 we will discuss the extension of our framework to models that have an unbounded number of parameters, and to continuous distributions."}, {"heading": "3. GENERATIVE DATALOG", "text": "A Datalog program without existential quantifiers specifies how to obtain a solution from an input EDB instance by producing the set of inferred IDB facts. In this section we present generative Datalog programs, which specify how to infer a distribution over possible outcomes given an input EDB instance."}, {"heading": "3.1 Syntax", "text": "We first define the syntax of a generative Datalog program, which we call a GDatalog[\u2206] program.\nDefinition 3.1 (GDatalog[\u2206]). Let \u2206 be a finite set of parametrized numerical distributions.\n1. A \u2206-term is a term of the form \u03b4[p1, . . . , pk] where \u03b4 \u2208 \u2206 is a parametrized distribution with pardim(\u03b4) = k, and p1, . . . , pk are variables and/or constants.\n2. A \u2206-atom in a schema S is an atomic formula R(t1, . . . , tn) with R \u2208 S an n-ary relation, such that exactly one term ti (1 \u2264 i \u2264 n) is a \u2206-term,\nand all other tj are constants and/or variables.\n3. A GDatalog[\u2206] rule over a pair of disjoint schemas E and I is a first-order sentence of the form \u2200x(\u03c8(x) \u2190 \u03c6(x)) where \u03c6(x) is a conjunction of atoms in E \u222aI and \u03c8(x) is either an atom in I or a \u2206-atom in I.\n4. A GDatalog[\u2206] program is a triple G = (E , I,\u0398), where E and I are disjoint schemas and \u0398 is a finite set of GDatalog[\u2206] rules over E and I.\nExample 3.2. Our example is based on the burglar example of Pearl [36] that has been frequently used for illustrating probabilistic programming (e.g., [35]). Consider the EDB schema E consisting of the following relations: House(h, c) represents houses h and their location cities c, Business(b, c) represents businesses b and their location cities c, City(c, r) represents cities c and their associated burglary rates r, and AlarmOn(x) represents units (houses or businesses) x where the alarm is on. Figure 3 shows an instance I over this schema. Now consider the GDatalog[\u2206] program G = (E , I,\u0398) of Figure 1. Here, \u2206 consists of only one distribution, namely Flip. The first rule above, intuitively, states that, for every fact of the form City(c, r), there must be a fact Earthquake(c, y) where y is drawn from the Flip (Bernoulli) distribution with the parameter 0.01."}, {"heading": "3.2 Possible Outcomes", "text": "To define the possible outcomes of a GDatalog[\u2206] program, we associate to each GDatalog[\u2206] program G = (E , I,\u0398) a corresponding Datalog\u2203 program G\u0302 = (E , I\u2206,\u0398\u2206). The possible outcomes of an input instance I w.r.t. G will then be minimal solutions of I w.r.t. G\u0302. Next, we describe I\u2206 and \u0398\u2206. The schema I\u2206 extends I with the following additional relation symbols: whenever a rule in \u0398 contains a \u2206-atom of the form R(. . . , \u03b4[. . .], . . .), and i \u2264 arity(R) is the argument position at which the \u03b4-term in question occurs, then we add to I\u2206 a corresponding relation symbol R\u03b4i , whose arity is arity(R) + pardim(\u03b4). These relation symbols R\u03b4i are called the distributional relation symbols of I\u2206, and the other relation symbols of\nI\u2206 (namely, those of I) are referred to as the ordinary relation symbols. Intuitively, a fact in R\u03b4i asserts the existence of a tuple in R and a sequence of parameters, such that the ith element of the tuple is sampled from \u03b4 using the parameters. The set \u0398\u2206 contains three kinds of rules:\n(i) All Datalog rules from \u0398 that contain no \u2206-terms;\n(ii) The rule \u2203yR\u03b4i (t, y, t \u2032,p) \u2190 \u03c6(x) for every rule of\nthe form R(t, \u03b4[p], t\u2032) \u2190 \u03c6(x) in \u0398, where i is the position of \u03b4[p] in R(t, \u03b4[p], t\u2032);\n(iii) The rule \u2200x,p(R(x) \u2190 R\u03b4i (x,p)) for every distributional relation symbol R\u03b4i \u2208 I \u2206. Note that in (ii), t and t\u2032 are the terms that occur before and after the \u2206-term \u03b4[p], respectively. A rule in (iii) states that every fact in R\u03b4i should be reflected in the relation R.\nExample 3.3. The GDatalog[\u2206] program G given in Example 3.2 gives rise to the corresponding Datalog\u2203 program G\u0302 of Figure 2. As an example of (ii), rule 6 of Figure 1 is replaced with rule 6 of Figure 2. Rules 8\u201310 of Figure 2 are examples of (iii).\nA possible outcome is defined as follows.\nDefinition 3.4 (Possible Outcome). Let I be an input instance for a GDatalog[\u2206] program G. A possible outcome for I w.r.t. G is a minimal solution F of I w.r.t. G\u0302, such that \u03b4(b|p) > 0 for every distributional fact R\u03b4i (a, b, c,p) \u2208 F with b in the ith position.\nWe denote the set of all possible outcomes of I w.r.t. G by \u2126G(I), and we denote the set of all finite possible outcomes by \u2126finG (I). The following proposition provides an insight into the possible outcomes of an instance, and will reappear later on in our study of the chase. For any distributional relation R\u03b4i \u2208 \u0398\n\u2206, the functional dependency associated to R\u03b4i is the functional dependency\nR\u03b4i : ({1, . . . , arity(R \u03b4 i )} \\ {i}) \u2192 i, expressing that the i-th attribute is functionally determined by the rest.\nProposition 3.5. Let I be any input instance for a GDatalog[\u2206] instance G. Then every possible outcome in \u2126G(I) satisfies all functional dependencies associated to distributional relations.\nThe proof of Proposition 3.5 is easy: if an instance J violates the funtional dependency associated to a distributional relation R\u03b4i , then one of the two facts involved in the violation can be removed, showing that J is, in fact, not a minimal solution w.r.t. G\u0302."}, {"heading": "3.3 Finiteness and Weak Acyclicity", "text": "Our presentation first focuses on the case where all the possible outcomes for the GDatalog[\u2206] program are finite. Before we proceed to defining the semantics of such a GDatalog[\u2206] program, we present the notion of weak acyclicity for a GDatalog[\u2206] program, as a natural syntactic property that guarantees finiteness of all possible outcomes. This draws on the notion of weak acyclicity for Datalog\u2203 [18]. Consider any GDatalog[\u2206] program G = (E , I,\u0398). A position of I is a pair (R, i) where R \u2208 I and i is an attribute of R. The dependency graph of G is the directed graph that has the attributes of I as the nodes, and the following edges:\n\u2022 A normal edge (R, i) \u2192 (S, j) whenever there is a rule \u03c8(x) \u2190 \u03d5(x) and a variable x at position (R, i) in \u03d5(x), and at position (S, j) in \u03c8(x).\n\u2022 A special edge (R, i) \u2192\u2217 (S, j) whenever there is a rule of the form\nS(t1, . . . , tj\u22121, \u03b4[p], tj+1, . . . , tn) \u2190 \u03d5(x)\nand an exported variable at position (R, i) in \u03d5(x). By an exported variable, we mean a variable that appears in both the premise and the conclusion.\nWe say that G is weakly acyclic if no cycle in the dependency graph of G contains a special edge.\nTheorem 3.6. If a GDatalog[\u2206] program G is weakly acyclic, then \u2126G(I) = \u2126 fin G (I) for all input instances I."}, {"heading": "3.4 Probabilistic Semantics", "text": "Intuitively, the semantics of a GDatalog[\u2206] program is a function that maps every input instance I to a probability distribution over \u2126G(I). We now make this precise. Let G be a GDatalog[\u2206] program, let I be an input for G. Again, we first consider the case where an input instance I only has finite possible outcomes (i.e., \u2126G(I) = \u2126 fin G (I)). Observe that, when all possible outcomes of I are finite, the set \u2126G(I) is countable, since we assume that all of our numerical distributions are discrete. In this case, we can define a discrete probability distribution over the possible outcomes of I w.r.t. G. We denote this probability distribution by PrG,I . For a distributional fact f = R\u03b4i (a1, . . . , an,p), we define the weight of f (notation: weight(f)) to be \u03b4(ai|p). For an ordinary (non-distributional) fact f , we set weight(f) = 1. For a finite set F of facts, we denote by P(F ) the product of the weights of all the facts in F .\nP(F ) def = \u220f\nf\u2208F\nweight(f)\nThe probability assigned to a possible outcome J \u2208 \u2126finP (I), denoted PrG,I(J), is simply P(J). If a possible outcome J does not contain any distributional facts, then PrG,I(J) = 1 by definition.\nExample 3.7. (continued) Let J be the instance that consists of all of the relations in Figures 3 and 4. Then J is a possible outcome of I w.r.t. G. For convenience, in the case of distributional relation symbols, we have added the weight of each fact to the corresponding row as the rightmost attribute. This weight is not part of our model (since it can be inferred from the rest of the attributes). For presentation\u2019s sake, the sampled values are under the attribute name draw (while attribute names are again external to our formal model). PrG,I(J) is the product of all of the numbers in the columns titled \u201cw(f),\u201d that is, 0.01\u00d7 0.99\u00d7 0.03\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 0.4.\nThe following theorem states that PrG,I is indeed a probability space over all the possible outcomes.\nTheorem 3.8. Let G be a GDatalog[\u2206] program, and I an input instance for G, such that \u2126G(I) = \u2126 fin G (I). Then PrG,I is a discrete probability function over \u2126G(I).\nWe prove Theorem 3.8 in Section 4. In Section 5 we consider the general case, and in particular the generalization of Theorem 3.8, where not all possible outcomes are guaranteed to be finite. There, if one considers only the (countable set of all) finite possible outcomes, then the sum of probabilities is not necessarily one. But still:\nTheorem 3.9. Let G be a GDatalog[\u2206] program, and I an input for G. Then \u03a3J\u2208\u2126fin\nG (I) PrG,I(J) \u2264 1.\nWe conclude this section with some comments. First, we note that the restriction of a conclusion of a rule to include a single \u2206-term significantly simplifies the presentation, but does not reduce the expressive power. In particular, we could simulate multiple \u2206-terms in the conclusion using a collection of predicates and rules. For example, if one wishes to have conclusion where a person gets both a random height and a random weight (possibly with shared parameters), then she can do so by deriving PersonHeight(p, h) and PersonWeight(p, w) separately, and using the rule PersonHW(p, h, w) \u2190 PersonHeight(p, h),PersonWeight(p, w). We also highlight the fact that our framework can easily simulate the probabilistic database model of independent tuples [41] with probabilities mentioned in the database, using the Flip distribution, as follows. Suppose that we have the EDB relation R(x, p) where p represents the probability of every tuple. Then we can obtain the corresponding probabilistic relation R\u2032 using the rules S(x,Flip[p]) \u2190 R(x, p) and R\u2032(x) \u2190 S(x, 1). Finally, we note that a disjunctive Datalog rule [16], where the conclusion can be a disjunction of atoms, can be simulated by our model (with probabilities ignored): If the conclusion has n disjuncts, then we construct a distributional rule with a probability distribution over {1, . . . , n}, and additional n deterministic rules corresponding to the atoms."}, {"heading": "4. CHASING GENERATIVE PROGRAMS", "text": "The chase [3,28] is a classic technique used for reasoning about tuple-generating dependencies and equalitygenerating dependencies. In the special case of full tuple-generating dependencies, which are syntactically isomorphic to Datalog rules, the chase is closely related to (a tuple-at-a-time version of) the naive bottom-up evaluation strategy for Datalog program (cf. [2]). In this section, we present a suitable variant of the chase for generative Datalog programs, and analyze some of its properties. The goal of that is twofold. First, as we will show, the chase provides an intuitive executional counterpart of the declarative semantics in Section 3. Second, we use the chase to prove Theorems 3.8 and 3.9. We note that, although the notions and results could arguably be phrased in terms of a probabilisitic extension of bottom-up Datalog evaluation strategy, the fact that a GDatalog[\u2206] rule can create new values makes it more convenient to phrase them in terms of a suitable adaptation of the chase procedure. To simplify the notation in this section, we fix a GDatalog[\u2206] program G = (E , I,\u0398). Let G\u0302 = (E , I\u2206,\u0398\u2206) be the associated Datalog\u2203 program. We define the notions of chase step and chase tree.\nChase step. Consider an instance J , a rule \u03c4 \u2208 \u0398\u2206 of the form \u03c8(x) \u2190 \u03d5(x), and a tuple a such that \u03d5(a) is satisfied in J but \u03c8(a) is not satisfied in J . If \u03c8(x) is a distributional atom of the form \u2203yR\u03b4i (t, y, t\n\u2032,p), then \u03c8 being \u201cnot satisfied\u201d is interpreted in the logical sense (regardless of probabilities): there is no y such that the tuple (t, y, t\u2032,p) is in R\u03b4i . In that case, let J be the set of all instances Jb obtained by extending J with \u03c8(a) for a specific value b of the existential variable y, such that \u03b4(b|p) > 0. Furthermore, let \u03c0 be the discrete probability distribution over J that assigns to Jb the probability mass \u03b4(b|p). If \u03c8(x) is an ordinary atom without existential quantifiers, J is simply defined as {J \u2032}, where J \u2032 extends J with the facts in \u03c8(a), and \u03c0(J \u2032) = 1. Then, we say that\nJ \u03c4(a) \u2212\u2212\u2212\u2192 (J , \u03c0)\nis a valid chase step.\nChase tree. Let I be an input instance for G. A chase tree for I w.r.t. G is a possibly infinite tree, whose nodes are labeled by instances over E\u222aI and where each edge is labeled by a real number r \u2208 [0, 1] such that\n1. The root is labeled by I;\n2. For each non-leaf node labeled J , if J is the set of labels of the children of the node, and if \u03c0 is the map assigning to each J \u2032 \u2208 J the label of the edge\nfrom J to J \u2032, then J \u03c4(a) \u2212\u2212\u2212\u2192 (J , \u03c0) is a valid chase step for some rule \u03c4 \u2208 \u0398\u2206 and tuple a.\n3. For each leaf node labeled J , there does not ex-\nist a valid chase step of the form J \u03c4(a) \u2212\u2212\u2212\u2192 (J , \u03c0). In other words, the tree cannot be extended to a larger chase tree.\nWe denote by L(v) the label (instance) of the node v. Each instance L(v) of a node of v of a chase tree is said to be an intermediate instance w.r.t. that chase tree. A chase tree is said to be injective if no intermediate instance is the label of more than one node; that is, for v1 6= v2 we have L(v1) 6= L(v2). As we will see shortly, due to the specific construction of \u0398\u2206, every chase tree turns out to be injective.\nProperties of the chase. We now state some properties of our chase procedure.\nProposition 4.1. Let I be any input instance, and consider any chase tree for I w.r.t. G. Then every intermediate instance satisfies all functional dependencies associated to distributional relations.\nProposition 4.2. Every chase tree w.r.t. G is injective.\nWe denote by leaves(T ) the set of leaves of a chase tree T , and we denote by L(leaves(T )) the set {L(v) | v \u2208 leaves(T )}.\nTheorem 4.3. Let T be a chase tree for an input instance I w.r.t. G. The following hold.\n1. Every intermediate instance is a subset of some possible outcome in \u2126G(I).\n2. If T does not have infinite directed paths, then L(leaves(T )) = \u2126finG (I).\nThis theorem is a special case of a more general result, Theorem 5.3, which we prove later."}, {"heading": "4.1 Proof of Theorems 3.8 and 3.9", "text": "By construction, for every node of a chase tree T , the weights of the edges that emanate from the node in question sum up to one. We can associate to each intermediate instance L(v) a weight, namely the product of the edge labels on the path from the root to v. This weight is well defined, since T is injective. We can then consider a random walk over the tree, where the probabilities are given by the edge labels. Then, for a node v, the weight of L(v) is equal to the probability of visiting v in this random world. From Theorem 4.3 we conclude that, if all the possible outcomes are finite, then T does not have any infinite paths, and moreover, the random walk defines a probability distribution over the labels of the leaves, which are the possible outcomes. This is precisely the probability distribution of Theorem 3.8. Moreover, in the general case, \u03a3J\u2208\u2126fin\nG (I) PrG,I(J) is the\nprobability that the random walk terminates (at a leaf), and hence, Theorem 3.9 follows from the fact that this probability (as is any probability) is a number between zero and one."}, {"heading": "5. INFINITE POSSIBLE OUTCOMES", "text": "In the general case of a GDatalog[\u2206] program, possible oucomes may be infinite, and moreover, the space of possible outcomes may be uncountable.\nExample 5.1. We now discuss examples that show what would happen if we straightforwardly extended our current definition of the probability PrG,I(J) of possible outcomes J to infinite possible outcomes (where, in the case where J is infinite, PrG,I(J) would be the limit of an infinite product of weights). Consider the GDatalog[\u2206] program defined by the rule R(y, \u03b4[y]) \u2190 R(x, y) where \u03b4 is a probability distribution with one parameter p and such that \u03b4(z|p) is equal to 1 if z = 2p and 0 otherwise. Then, I = {R(0, 1)} has no finite possible outcome. In fact, I has exactly one infinite possible outcome: {R(0, 1)} \u222a {R\u03b42(2\ni, 2i+1, 2i) | i \u2265 0} \u222a {R(2i, 2i+1) | i \u2265 0}. Now consider the previous program extended with the rule R(0,Flip[0.5]) \u2190 Q(x), and consider the input instance I \u2032 = {Q(0)}. Then, I \u2032 has one finite possible outcome J = {Q(0), R(0, 0), RFlip2 (0, 0, 0.5)} with PrG,I\u2032(J) = 0.5, and another infinite possible outcome J \u2032 = {R(0, 1), RFlip2 (0, 1, 0.5)} \u222a {R \u03b4 2(2\ni, 2i+1, 2i) | i \u2265 0} \u222a {R(2i, 2i+1) | i \u2265 0} with PrG,I\u2032(J\n\u2032) = 0.5. Next, consider the GDatalog[\u2206] program defined by R(y, \u03b4\u2032[y]) \u2190 R(x, y), where \u03b4\u2032 is a probability distribution with one parameter p, and \u03b4\u2032(z|p) is equal to 0.5 if z \u2208 {2p, 2p + 1} and 0 otherwise. Then, for I = {R(0, 1)}, every possible outcome is infinite, and would have the probability 0. Now consider the previous program extended with the rule R(0,Flip[0.5]) \u2190 Q(x), and consider again the input instance I \u2032 = {Q(0)}. Then I \u2032 would have exactly one possible outcome J with PrG,I\u2032(J) > 0, namely J = {Q(0), R(0, 0), RFlip2 (0, 0, .5), R \u03b4\u2032\n2 (0, 0, 0)} where PrG,I\u2032(J) = 0.25."}, {"heading": "5.1 Generalization of Probabilistic Semantics", "text": "To generalize our framework, we need to consider probability spaces over uncountable domains; those are defined by means of measure spaces, which are defined as follows. Let \u2126 be a set. A \u03c3-algebra over \u2126 is a collection F of subsets of \u2126, such that F contains \u2126 and is closed under complement and countable unions. (Implied properties include that F contains the empty set, and that F is closed under countable intersections.) If F \u2032 is a nonempty collection of subsets of \u2126, then the closure of F \u2032 under complement and countable unions is a \u03c3algebra, and it is said to be generated by F \u2032. A probability measure space is a triple (\u2126,F , \u03c0), where: (1) \u2126 is a set, called the sample space, (2) F is a \u03c3-algebra over \u2126, and (3) \u03c0 : F \u2192 [0, 1], called a probability measure, is such that \u03c0(\u2126) = 1, and \u03c0(\u222aE) = \u2211 e\u2208E \u03c0(e) for every countable set E of pairwise-disjoint measurable sets.\nLet G be a GDatalog[\u2206] program, and let I be an input for G. We say that a sequence f = (f1, . . . , fn) of facts is a derivation (w.r.t. I) if for all i = 1, . . . , n, the fact fi is the result of applying some rule of G that is not satisfied in I\u222a{f1, . . . , fi\u22121} (in the case of applying a rule with a \u2206-atom in the head, choosing a value randomly). If f1, . . . , fn is a derivation, then the set {f1, . . . , fn} is a derivation set. Hence, a finite set F of facts is a derivation set if and only if I \u222a F is an intermediate instance in some chase tree. Let G be a GDatalog[\u2206] program, let I be an input for G, and let F be a set of facts. We denote by \u2126F\u2286G (I) the set of all the possible outcomes J \u2286 \u2126G(I) such that F \u2286 J . The following theorem states how we determine the measure space defined by a GDatalog[\u2206] program.\nTheorem 5.2. Let G be a GDatalog[\u2206] program, and let I be an input for G. There exists a unique probability measure space (\u2126,F , \u03c0), denoted \u00b5G,I , that satisfies all of the following.\n(i) \u2126 = \u2126G(I);\n(ii) The \u03c3-algebra (\u2126,F) is generated from the sets of\nthe form \u2126F\u2286G (I) where F is finite;\n(iii) \u03c0(\u2126F\u2286G (I)) = P(F ) for every derivation set F . Moreover, if J is a finite possible outcome, then \u03c0({J}) is equal to P(F ).\nObserve that the items (i) and (ii) of Theorem 5.2 describe the unique properties of the probability measure space. The proof will be given in the next section. The last part of the theorem states that our discrete and continuous probability definitions coincide on finite possible outcomes; this is a simple consequence of item (ii), since for a finite possible outcome J , the set F = J \\ I is such that \u2126F\u2286G (I) = {J}, and F is itself a derivation set (e.g., due to Theorem 4.3)."}, {"heading": "5.2 Measure Spaces by Infinite Chase", "text": "We prove Theorem 5.2 by defining and investigating measure spaces that are defined in terms of the chase. Consider a GDatalog[\u2206] program G and an input I for G. A maximal path of a chase tree T is a path P that starts with the root, and either ends in a leaf or is infinite. Observe that the labels (instances) along a maximal path form a chain (w.r.t. the set-containment partial order). A maximal path P of a chase tree is fair if whenever the premise of a rule is satisfied by some tuple in some intermediate instance on P , then the conclusion of the rule is satisfied for the same tuple in some intermediate instance on P . A chase tree T is fair (or has the fairness property) if every maximal path is fair. Note that every finite chase tree is fair. We will restrict attention to fair chase trees. Fairness is a classic notion in the study of infinite computations; moreover, fair chase trees can easily be constructed, for examples, by maintaining a queue of \u201cactive rule firings\u201d (cf. any textbook on term rewriting systems or lambda calculus).\nLet G be a GDatalog[\u2206] program, let I be an input for G, and let T be a chase tree. We denote by paths(T ) the set of all the maximal paths of T . (Note that paths(T ) may be uncountably infinite.) For P \u2208 paths(T ), we denote by \u222aP the union of the (chain of) labels L(v) along P . The following generalizes Theorem 4.3.\nTheorem 5.3. Let G be a GDatalog[\u2206] program, I an input for G, and T a fair chase tree. The mapping P \u2192 \u222aP is a bijection between paths(T ) and \u2126G(I)."}, {"heading": "5.3 Chase Measures", "text": "Let G be a GDatalog[\u2206] program, let I be an input for G, and let T be a chase tree. Our goal is to define a probability measure over \u2126G(I). Given Theorem 5.3, we can do that by defining a probability measure over paths(T ). A random path in paths(T ) can be viewed as a Markov chain that is defined by a random walk over T , starting from the root. A measure space for such a Markov chain is defined by means of cylinderification [6]. Let v be a node of T . The v-cylinder of T , denoted CTv , is the subset of paths(T ) that consists of all the maximal paths that contain v. A cylinder of T is a subset of paths(T ) that forms a v-cylinder for some node v. We denote by C(T ) the set of all the cylinders of T . Recall that L(v) is a finite set of facts, and observe that P(L(v)) is the product of the weights along the path from the root to v. The following theorem is a special case of a classic result on Markov chains (cf. [6]).\nTheorem 5.4. Let G be a GDatalog[\u2206] program, let I be an input for G, and let T be a chase tree. There exists a unique probability measure (\u2126,F , \u03c0) that satisfies all of the following.\n1. \u2126 = paths(T ).\n2. (\u2126,F) is the \u03c3-algebra generated from C(T ).\n3. \u03c0(CTv ) = P(L(v)) for all nodes v of T .\nTheorems 5.3 and 5.4 suggest the following definition.\nDefinition 5.5 (Chase Probability Measure). Let G be a GDatalog[\u2206] program, let I be an input for G, let T be a chase tree, and let (\u2126,F , \u03c0) be the probability measure of Theorem 5.4. The probability measure \u00b5T over \u2126G(I) is the one obtained from (\u2126,F , \u00b5) by replacing every maximal path P with the possible outcome \u222aP .\nNext, we prove that the probability measure space represented by a chase tree is independent of the specific chase tree of choice. For that, we need some notation and a lemma. Let G be a GDatalog[\u2206] program, let I be an input for G, let T be a chase tree, and let v be a node of T . We denote by \u222aCTv the set {\u222aP | P \u2208 C T v }. The following lemma is a consequence of Proposition 4.1 and Theorem 5.3.\nLemma 5.6. Let G be a GDatalog[\u2206] program, let I be an input for G, and let T be a fair chase tree. Let v be a node of T and F = L(v). Then \u222aCTv = \u2126 F\u2286 G (I); that is, \u222aCTv is the set {J \u2208 \u2126G(I) | L(v) \u2286 J}.\nUsing Lemma 5.6 we can prove the following theorem.\nTheorem 5.7. Let G be a GDatalog[\u2206] program, let I be an input for G, and let T and T \u2032 be two fair chase trees. Then \u00b5T = \u00b5T \u2032 ."}, {"heading": "5.4 Proof of Theorem 5.2", "text": "We can now prove Theorem 5.2. Let G be a GDatalog[\u2206] program, let I be an input for G, and let T be a fair chase tree for I w.r.t. G. Let \u00b5T = (\u2126G(I),FT , \u03c0T ) be the probability measure on \u2126G(I) associated to T , as defined in Definition 5.5.\nLemma 5.8. The \u03c3-algebra (\u2126G(I),FT ) is generated\nby the sets of the form \u2126F\u2286G (I), where F is finite.\nProof. Let (\u2126G(I),F) be the \u03c3-algebra generated from the sets \u2126F\u2286G (I). We will show that every \u2126 F\u2286 G (I) is in FT , and that every \u222aC T v is in F . The second claim is due to Lemma 5.6, so we will prove the first. So, let \u2126F\u2286G (I) be given. Due to Lemma 5.6, the set \u2126 F\u2286 G (I) is the countable union \u222au\u2208U (\u222aC T u ) where U is the set of all the nodes u such that F \u2286 u. Hence, \u2126F\u2286G (I) \u2208 FT .\nLemma 5.9. For every derivation set F we have \u03c0T (\u2126 F\u2286 G (I)) = P(F ).\nProof. Let F be a derivation set. Due to Theorem 5.7, it suffices to prove that for some chase tree T \u2032 it is the case that \u03c0T \u2032(\u2126 F\u2286 G (I)) = P(F ). But since F is a derivation set, we can craft a chase tree T \u2032 that has a node v with L(v) = F . Then we have that \u03c0T \u2032(\u2126 F\u2286 G (I)) is the product of the weights along the path to v, which is exactly P(F ).\nLemma 5.10. Let \u00b5 = (\u2126,F , \u03c0) be any probability space that satisfies (i)\u2013(iii) of Theorem 5.2. Then \u00b5 = \u00b5T .\nProof. Let \u00b5T = (\u2126G(I),FT , \u03c0T ). Due to Lemma 5.8, we have that F = FT . So it is left to prove that \u03c0 = \u03c0T . Due to Lemmas 5.9 and 5.6, we have that \u03c0 agrees with \u03c0T on the cylinder sets of T . Due to Theorems 5.3 and 5.4 we get that \u03c0 must be equal to \u03c0T due to the uniqueness of \u03c0T .\nThe above lemmas show that \u00b5T = (\u2126G(I),FT , \u03c0T ) is a probability measure space that satisfies (i) and (ii) of Theorem 5.2, and moreover, that no other probability measure space satisfies (i) and (ii)."}, {"heading": "6. PROBABILISTIC-PROGRAMMING", "text": "DATALOG\nTo complete our framework, we define probabilisticprogramming Datalog, PPDL for short, wherein a program augments a generative Datalog program with constraints; these constraints unify the traditional integrity constraints of databases and the traditional observations of probabilistic programming.\nDefinition 6.1 (PPDL[\u2206]). Let \u2206 be a finite set of parametrized numerical distributions. A PPDL[\u2206] program is a quadruple (E , I,\u0398,\u03a6), where (E , I,\u0398) is a GDatalog[\u2206] program and \u03a6 is a finite set of logical constraints over E \u222a I.1\nExample 6.2. Consider again Example 3.2. Suppose that we have the EDB relations ReportHAlarm and ReportBAlarm that represent reported home and business alarms, respectively. We obtain from the program in the example a PPDL[\u2206]-program by adding the following constraints.\n1. ReportHAlarm(h) \u2192 Alarm(h)\n2. ReportBAlarm(b) \u2192 Alarm(b)\nNote that we use right (in contrast to left) arrows to distinguish constraints from ordinary Datalog rules.\nThe semantics of a PPDL[\u2206] program is the posterior distribution over the corresponding GDatalog[\u2206] program, conditioned on the satisfaction of the constraint. A formal definition follows. Let P = (E , I,\u0398,\u03a6) be a PPDL[\u2206] program, and let G be the GDatalog[\u2206] program (E , I,\u0398). An input instance for P is an input instance I for G. We say that I is a legal input instance for P if {J \u2208 \u2126G(I) | J |= \u03a6} is a measurable set in the probability space \u00b5G,I , and moreover, its measure is nonzero. Intuitively, an input instance I is legal if it is consistent with the observations (i.e., with the conjunction of the constraints in \u03a6), given G. The semantics of a PPDL[\u2206] program is defined as follows.\nDefinition 6.3. Let P = (E , I,\u0398,\u03a6) be a PPDL[\u2206] program, and let G be the GDatalog[\u2206] program (E , I,\u0398). Let I be a legal input instance for P, and let \u00b5G,I = (\u2126G(I),FG , \u03c0G). The probability space defined by P and I, denoted \u00b5P,I , is the triple (\u2126P(I),FP , \u03c0P) where:\n\u2022 \u2126P(I) = {J \u2208 \u2126G(I) | J |= \u03a6}\n\u2022 FP = {S \u2229 \u2126P(I) | S \u2208 FG}\n\u2022 \u03c0P(S) = \u03c0G(S)/\u03c0G(\u2126P(I)) for every S \u2208 FP ."}, {"heading": "In other words, \u00b5P,I is \u00b5G,I conditioned on \u03a6.", "text": "Example 6.4. Continuing Example 6.2, the semantics of this program is the posterior probability distribution for the prior of Example 3.2, under the conditions that the alarm is on whenever it is reported. 1The restriction of the language of constraints to a fragment with tractability (or other goodness) properties is beyond the scope of this paper.\nThen, one can ask various queries over the probability space defined, for example, the probability of the fact Earthquake(Napa, 1). Observe that, with negation, we could also phrase the condition that an alarm is off unless reported.\nWe note that a traditional integrity constraint on the input (e.g., there are no x, c1 and c2 such that both Home(x, c1) and Business(x, c2) hold) can be viewed as a constraint in \u03a6 that holds with either probability 0 (and then the input is illegal) or with probability 1 (and then the prior \u00b5G,I is the same as the posterior \u00b5P,I). An important direction for future work is to establish tractable conditions that guarantee that a given input is legal. Also, an interesting problem is to detect conditions under which the chase is a self conjugate [39], that is, the probability space \u00b5P,I is captured by a chase procedure without backtracking."}, {"heading": "7. EXTENSIONS AND FUTURE WORK", "text": "Our ultimate goal is to design a language for probabilistic programming that possesses the inherent declarative and logical nature of Datalog. To that end, extensions are required. In this section we discuss some of the important future directions and challenges to pursue. We focus on the semantic aspects of expressive power. (An obvious aspect for future work is a practical implementation, e.g., corresponding sampling techniques.)"}, {"heading": "7.1 Unbounded Number of Parameters", "text": "It is often the case that the probability distributions have a large number of parameters. A simple example is the categorical distribution where a single member of a finite domain of items is to be selected, each item with its own probability. In that case, the domain can be very large, and moreover, it can be determined dynamically from the database content and be unknown to the static program. To support such distributions, one can associate the distribution with a relation symbol in a schema, and here we illustrate it for the case of a categorical distribution. Let R be a relation symbol. A categorical distribution over R is associated with two attributes i and j of R that determine dynamic parameters : i represents a possible value, and j represents its probability. In addition, the distribution is associated with a tuple g of attributes of R that split R into groups with the semantics of SQL\u2019s GROUP BY. We denote this distribution by CatR\u3008i, j;g\u3009(x;q), where g and q have the same length. Given a relation RI over R and parameters q, let RI\ng be the sub-relation of RI that have the values in\nthe attributes vector g are equal to q. Suppose that the facts of RI\nq are f1, . . . , fn. We assume that fk[j] \u2208 [0, 1]\nfor all k = 1, . . . , n, and moreover, that \u2211n\nk=1 fk[j] = 1.\nThen we define CatR\u3008i, j|g\u3009(x|q) = s, where s is the sum of fk[j] over all k \u2208 {1, . . . , n} with fk[i] = x. As an example, consider the relation Cor(wc, we, q)\nthat provides, for every English word wc, a distribution over the possible misspelled words we; hence, the fact Cor(wc, we, q) means that wc is misspelled into we with probability q. In our notation, this distribution will be captured by the notation CatCor\u30082, 3; 1\u3009(we;wc). Hence, the following program contains, for each document, the set of words that the document can contain by replacing each word with a corresponding correction. The relation Doc(d, i, w) denotes that document d has the word w as its ith token. The relation CDoc is the same as Doc, except that each word is replaced with a random correction.\nCDoc(d, i,CatCor\u30082, 3; 1\u3009(wc;w)) \u2190 Doc(d, i;w) (4)\nThe above specification for the categorical distribution is similar to the repair-key operation of the worldset algebra [4, 5]. In the general case, we define an Rdistribution to be one of the form \u03b4R\u3008d;g\u3009(x;q), where R is a relation symbol, d is a tuple of attributes of R, representing the dynamic parameters (i.e., separate parameters for every row), g is a tuple of grouping attributes, and q has the same length as g. We could also add a tuple p of static parameters (i.e., ones that are shared by all the tuples in the group). Extending our semantics to support a program such as (4) is straightforward, since the relation Cor that defines the distribution is an EDB relation. The only difference from our previous definitions is that, now, the probability of a distributional fact is determined not only by the fact itself, but also by the content of the relation to which the fact refers to (e.g., Cor). When the relation R that is used for defining the distribution is an IDB, we need to be careful with the definition of the chase, since when we wish to sample from an R-distribution, it may be the case that some of the relevant facts of R have not been produced yet. We may even be in a cyclic situation where the facts of R are determined by facts derived (indirectly) from facts that are produced using R-distributions. We plan to devise a (testable) acyclicity condition that avoids such cycles, and then restrict the chase to behave accordingly; R-distributions are sampled from only after R is complete. Of course, it would be interesting to explore the semantics of programs without the acyclicity property."}, {"heading": "7.2 Multivariate Distributions", "text": "A natural and important extension is to support multivariate distributions, which are distributions with a support in Rk for k > 1. Examples of popular such distributions are multinomial, Dirichlet, and multivariate Gaussian distribution. When k is fixed, one can replace our single distributional term with multiple such terms. But when k is unbounded, such a distribution should be supported as an aggregate operation that implies in a set of facts (rather than a single one).\n7.3 Continuous Distributions\nA natural extension of our framework is the support of continuous probability distributions (e.g., continuous uniform, Pareto, Gaussian, Dirichlet, etc.). This is a very important extension, as such distributions are highly popular in defining statistical models. Syntactically, this extension is straightforward: we just need to include these distributions in \u2206. Likewise, extending the probabilistic chase is also straightforward. The challenge, though, is with the semantic analysis, and in particular, with the definition of the probability space implied by the chase. When a chase step involves a continuous numeric distribution, such as U(0, 1) (the uniform distribution between 0 and 1), then no chase step is measurable, and hence, we can no longer talk about the probability of a step or the probability of a cylinder set (but we can talk about the density of those). Note that our definition of the measure space in Section 5 is inherently based on the assumption that the set of possible outcomes that contains a given finite set of facts is measurable. But to support continuous distribution, the definition of measurable sets will need to be based on sets of paths. We refer this to future work, and we believe that our current framework will naturally extend to continuous distributions."}, {"heading": "8. CONCLUDING REMARKS", "text": "We proposed and investigated a declarative framework for specifying statistical models in the context of a database, based on an extension of Datalog with numerical distributions. The framework differs from traditional probabilistic programming languages not only due to the tight integration with a database, but also because of its fully declarative rule-based language: the interpretation of a program is independent under transformations (such as reordering or duplication of rules) that preserve the first-order semantics. This was achieved by treating a GDatalog[\u2206] program as a Datalog program with existentially quantified variables in the conclusion, and using the minimal solutions as the sample space of a (discrete or continuous) probability distribution. Using a suitable notion of chase that we introduced, we established that the resulting probability distributions are well-defined and robust. This work is done as part of the effort to extend the LogicBlox database [23], and its Datalog-based data management language LogiQL [24], to support the specification of statistical models. Through its purely declarative rule-based syntax, such an extension of LogiQL allows for natural specifications of statistical models. Moreover, there is a rich literature on (extensions of) Datalog, and we expect that, through our framework, techniques and insights from this active research area can be put to use to advance the state of the art in probabilistic programming."}, {"heading": "Acknowledgments", "text": "We are thankful to Molham Aref, Todd J. Green and Emir Pasalic for insightful discussions and feedback on this work. We also thank Michael Benedikt, Georg Gottlob and Yannis Kassios for providing useful comments and suggestions. Finally, we are grateful to Kathleen Fisher and Suresh Jagannathan for including us in DARPA\u2019s PPAML initiative; this work came from our efforts to design a principled approach to translating probabilistic programs into statistical solvers."}, {"heading": "9. REFERENCES", "text": "[1] S. Abiteboul, D. Deutch, and V. Vianu. Deduction with contradictions in Datalog. In ICDT, pages 143\u2013154, 2014. [2] S. Abiteboul, R. Hull, and V. Vianu. Foundations of Databases. 1995. [3] A. V. Aho, C. Beeri, and J. D. Ullman. The theory of joins in relational databases. ACM Trans. on Datab. Syst., (3):297\u2013314, 1979. [4] L. Antova, C. Koch, and D. Olteanu. From complete to incomplete information and back. In SIGMOD, pages 713\u2013724, 2007. [5] L. Antova, C. Koch, and D. Olteanu. Query language support for incomplete information in the MayBMS system. In VLDB, pages 1422\u20131425, 2007. [6] R. B. Ash and C. Doleans-Dade. Probability & Measure Theory. Harcourt Academic Press, 2000. [7] C. Baral, M. Gelfond, and N. Rushton. Probabilistic reasoning with answer sets. Theory Pract. Log. Program., 9(1):57\u2013144, 2009. [8] M. Benedikt, E. Kharlamov, D. Olteanu, and P. Senellart. Probabilistic XML via markov chains. PVLDB, 3(1):770\u2013781, 2010. [9] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. of Machine Learning Research, 3:993\u20131022, 2003.\n[10] M. Bro\u0308cheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. In UAI, pages 73\u201382, 2010. [11] Z. Cai, Z. Vagena, L. L. Perez, S. Arumugam, P. J. Haas, and C. M. Jermaine. Simulation of database-valued Markov chains using SimSQL. In SIGMOD, pages 637\u2013648, 2013. [12] A. Cal\u0300\u0131, G. Gottlob, T. Lukasiewicz, B. Marnette, and A. Pieris. Datalog+/-: A family of logical knowledge representation and query languages for new applications. In LICS, pages 228\u2013242, 2010. [13] S. Cohen and B. Kimelfeld. Querying parse trees of stochastic context-free grammars. In Database Theory - ICDT 2010, 13th International Conference, Lausanne, Switzerland, March 23-25, 2010, Proceedings, ACM International Conference Proceeding Series, pages 62\u201375. ACM, 2010. [14] D. Deutch, C. Koch, and T. Milo. On\nprobabilistic fixpoint and Markov chain query languages. In PODS, pages 215\u2013226, 2010.\n[15] P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on AI and Machine Learning. Morgan & Claypool Publishers, 2009. [16] T. Eiter, G. Gottlob, and H. Mannila. Disjunctive datalog. ACM Trans. Database Syst., 22(3):364\u2013418, 1997. [17] K. Etessami and M. Yannakakis. Recursive markov chains, stochastic grammars, and monotone systems of nonlinear equations. J. ACM, 56(1), 2009. [18] R. Fagin, P. G. Kolaitis, R. J. Miller, and L. Popa. Data exchange: Semantics and query answering. In ICDT, volume 2572 of LNCS, pages 207\u2013224. Springer, 2003. [19] N. Fuhr. Probabilistic datalog: Implementing logical information retrieval for advanced applications. JASIS, 51(2):95\u2013110, 2000. [20] A. Fuxman, P. G. Kolaitis, R. J. Miller, and W. C. Tan. Peer data exchange. ACM Trans. Datab. Syst., 31(4):1454\u20131498, 2006. [21] N. D. Goodman. The principles and practice of probabilistic programming. In POPL, pages 399\u2013402, 2013. [22] G. Gottlob, T. Lukasiewicz, M. Martinez, and G. Simari. Query answering under probabilistic uncertainty in Datalog+/ ontologies. Annals of Math.& AI, 69(1):37\u201372, 2013. [23] T. J. Green, M. Aref, and G. Karvounarakis. LogicBlox, platform and language: A tutorial. In Int Conf on Datalog in Academia and Industry, pages 1\u20138, 2012. [24] T. Halpin and S. Rugaber. LogiQL: A Query Language for Smart Databases. CRC Press, 2014. [25] R. Jampani, F. Xu, M. Wu, L. L. Perez, C. M. Jermaine, and P. J. Haas. MCDB: a monte carlo approach to managing uncertain data. In SIGMOD, pages 687\u2013700, 2008. [26] B. Kimelfeld and P. Senellart. Probabilistic XML: models and complexity. In Advances in Probabilistic Databases for Uncertain Information Management, volume 304 of Studies in Fuzziness and Soft Computing, pages 39\u201366. 2013. [27] A. Kimmig, B. Demoen, L. De Raedt, V. Santos Costa, and R. Rocha. On the implementation of the probabilistic logic programming language ProbLog. Theory and Practice of Logic Programming, 11:235\u2013262, 2011. [28] D. Maier, A. O. Mendelzon, and Y. Sagiv. Testing implications of data dependencies. ACM Trans. on Datab. Syst., (4):455\u2013469, 1979. [29] V. K. Mansinghka, D. Selsam, and Y. N. Perov. Venture: a higher-order probabilistic programming platform with programmable inference. CoRR, abs/1404.0099, 2014.\n[30] A. K. McCallum. Multi-label text classification with a mixture model trained by EM. 1999. [31] B. Milch and et al. BLOG: Probabilistic models with unknown objects. In IJCAI, pages 1352\u20131359, 2005. [32] K. Nigam, A. McCallum, S. Thrun, and T. M. Mitchell. Text classification from labeled and unlabeled documents using EM. Machine Learning, pages 103\u2013134, 2000. [33] F. Niu, C. Re\u0301, A. Doan, and J. W. Shavlik. Tuffy: Scaling up statistical inference in markov logic networks using an RDBMS. PVLDB, 4(6):373\u2013384, 2011. [34] F. Niu, C. Zhang, C. Re, and J. W. Shavlik. DeepDive: Web-scale knowledge-base construction using statistical learning and inference. In Proceedings of the Second International Workshop on Searching and Integrating New Web Data Sources, Istanbul, Turkey, August 31, 2012, volume 884 of CEUR Workshop Proceedings, pages 25\u201328. CEUR-WS.org, 2012. [35] A. V. Nori, C. Hur, S. K. Rajamani, and S. Samuel. R2: an efficient MCMC sampler for probabilistic programs. In AAAI, pages 2476\u20132482, 2014. [36] J. Pearl. Probabilistic reasoning in intelligent systems - networks of plausible inference. Morgan Kaufmann, 1989. [37] A. Pfeffer. Figaro: An object-oriented probabilistic programming language. Technical report, Charles River Analytics, 2009. [38] D. Poole. The independent choice logic and beyond. In Probabilistic Inductive Logic Programming - Theory and Applications, pages 222\u2013243, 2008. [39] H. Raiffa and R. Schlaifer. Applied Statistical Decision Theory. Harvard University Press, Harvard, 1961. [40] T. Sato and Y. Kameya. PRISM: A language for symbolic-statistical modeling. In IJCAI, pages 1330\u20131339, 1997. [41] D. Suciu, D. Olteanu, C. Re\u0301, and C. Koch. Probabilistic Databases. Synthesis Lectures on Data Management. Morgan & Claypool Publishers, 2011. [42] J. Widom. Trio: a system for data, uncertainty, and lineage. In C. Aggarwal, editor, Managing and Mining Uncertain Data, chapter 5. Springer-Verlag, 2008. [43] www.probabilistic-programming.org. Repository on probabilistic programming languages, 2014.\nAPPENDIX"}, {"heading": "A. ADDITIONAL PROOFS", "text": "A.1 Proof of Theorem 3.6\nTheorem 3.6.If a GDatalog[\u2206] program G is weakly acyclic, then \u2126G(I) = \u2126 fin G (I) for all input instances I.\nProof. It is easy to show that if G is weakly acyclic in our sense, then P\u0302 is weakly acyclic according to the classic definition of weak ayclicity given in [18]. We then apply the following result (restated here to match our notation), which was established in [20]: if a Datalog\u2203 program D is weakly acyclic, then there is a polynomial p(\u00b7) (depending only on D) such that for every input instance I and for every solution J of I w.r.t. D, there is a solution J \u2032 of I w.r.t. D with J \u2032 \u2286 J and |J \u2032| \u2264 p(|I|). In particular, all J \u2208 min-solD(I) are finite and have size at most p(|I|).\nA.2 Proof of Proposition 4.1\nProposition 4.1. Let I be any input instance, and consider any chase tree for I w.r.t. G. Then every intermediate instance satisfies all functional dependencies associated to distributional relations.\nProof. The proof proceeds by induction on the distance from the root of the chase tree. Suppose that in a chase step J \u03c4(a) \u2212\u2212\u2212\u2192 (J , \u03c0), some J \u2032 \u2208 J contains two R\u03b4i -facts that are identical except for the i-th attribute. Then either J already contains both atoms, in which case we can apply our induction hypothesis, or J \u2032 is obtained by extending J with one of the two facts in question, in which case, it is easy to see that the conclusion of \u03c4 was already satisfied for the tuple a, which is not possible in case of a valid chase step.\nA.3 Proof of Proposition 4.2\nProposition 4.2. Every chase tree w.r.t. G is injective.\nProof. For the sake of a contradiction, assume that two nodes n1 and n2 in a chase tree are labeled by the same instance J . Let n0 be the node that is the least common ancestor of n1 and n2 in the tree, and let n \u2032 1 and n\u20322 be the children of n0 that are ancestors of n1 and n2, respectively. By construction, n \u2032 1 and n \u2032 2 are labeled with distinct instances J1 6= J2, respectively. Consider the rule \u03c4 = \u03c8(x) \u2190 \u03d5(x) and tuple a constituting the chase step applied at node n0. Since n0 has more than one child, \u03c8(x) must be a distributional atom, say \u2203yR\u03b4i (t, y, t\n\u2032,p). Then each Jk (k = 1, 2) contains an R\u03b4i -fact. Moreover, the two R \u03b4 i -facts in question differ in the choice of value for the variable y, and are otherwise identical. Due to the monotonic nature of the chase, both atoms must belong J , and hence, J \u2032 violates the functional dependency of Proposition 4.1. Hence, we have reached a contradiction.\nA.4 Proof of Theorem 5.3\nTheorem 5.3. Let G be a GDatalog[\u2206] program, I an input for G, and T a fair chase tree. The mapping P \u2192 \u222aP is a bijection between paths(T ) and \u2126G(I).\nProof. We first prove that every \u222aP is in \u2126G(I). Let P \u2208 paths(T ) be given. We need to show that \u222aP \u2208 \u2126G(I). By definition it is the case that every distributional fact of \u222aP has a nonzero probability. It is also clear that \u222aP is consistent, due to the fairness property of T . Hence, it suffices to prove that \u222aP is a minimal solution, that is, no proper subset of \u222aP is a solution. So, letK be a strict subset of \u222aP and suppose, by way of contraction, that K is also a solution. Let (J, J \u2032) be the first edge in P such that \u222aP contains a fact that is not in K. Now, consider the chase step that leads from J to J \u2032. Let f be the unique fact in J \u2032 \\ J . Then J \u2286 K and f \u2208 J \u2032 \\ K. The selected rule \u03c4 in this step cannot be deterministic, or otherwise K must contain f as well. Hence, it is a distributional rule, and f has the form R\u03b4i (a|p). But then, K satisfies this rule, and hence, K must include a fact f \u2032 = R\u03b4i (a\n\u2032|p), where a\u2032 differs from a only in the ith element. And since some node in \u222aP contains both f and f \u2032, we get a violation of the fd of Proposition 4.1. Hence, a contraction. Next, we prove that every possible outcome J in \u2126G(I) is equal to \u222aP for some P \u2208 paths(T ). Let such J be given. We build the path P inductively, as follows. We start with the root, and upon every node v we select the next edge to be one that leads to a subset K of J ; note that K must exist since J resolves the rule violated in L(v) by some fact, and that fact must be in one of the children of v. Now, \u222aP is consistent since T is fair, and \u222aP \u2286 J by construction. And since J is a minimal solution, we get that \u222aP is in fact equal to J . Finally, we need to prove that if \u222aP1 = \u222aP2 then P1 = P2. We will prove the contrapositive statement. Suppose that P1, P2 \u2208 paths(T ) are such that P1 6= P2. The two paths agree on the root. Let J be the first node in the paths such that the two paths disagree on the outgoing edge of J . Suppose that P1 has the edge from J to J1 and P2 has an edge from J to J2. Then J1 \u222a J2 have a pair of facts that violate the functional dependency of Proposition 4.1, and in particular, J1 6\u2286 \u222aP2. We conclude that \u222aP1 = \u222aP2, as claimed.\nA.5 Proof of Theorem 5.7\nTheorem 5.7. Let G be a GDatalog[\u2206] program, let I be an input for G, and let T and T \u2032 be two fair chase trees. Then \u00b5T = \u00b5T \u2032 .\nProof. Let \u00b5T = (\u2126,F , \u03c0) and \u00b5T \u2032 = (\u2126 \u2032,F \u2032, \u03c0\u2032). We need to prove that \u2126 = \u2126\u2032, F = F \u2032 and \u03c0 = \u03c0\u2032. We have \u2126 = \u2126\u2032 due to Theorem 5.3. To prove that F = F \u2032, it suffices to prove that every \u222aCTv is F\n\u2032 and every \u222aCT \u2032\nv\u2032 is in F (since both \u03c3-algebras are generated by the cylinders). And due to symmetry, it suffices to prove that CT \u2032\nv\u2032 is in F . So, let v \u2032 be a node of T \u2032. Recall\nthat L(v\u2032) is a set of facts. Due to Lemma 5.6, we have that \u222aCT \u2032\nv\u2032 is precisely the set of all possible outcomes J in \u2126G(I) such that L(v\n\u2032) \u2286 J . Let U be the set of all the nodes of u of P with L(v\u2032) \u2286 L(u). Then, due\nto Theorem 5.3 we have that \u222aCT \u2032\nv\u2032 = \u222au\u2208U (\u222aC T u ). Ob-\nserve that U is countable, since T has only a countable number of nodes (as every node is identified by a finite path from the root). Moreover, (\u2126,F) is a closed under countable unions, and therefore, \u222au\u2208U (\u222aC T u ) is in F .\nIt remains to prove that \u03c0 = \u03c0\u2032. By now we know that the \u03c3-algebras (\u2126,F) and (\u2126\u2032,F \u2032) are the same. Due to Theorem 5.3, every measure space over (\u2126,F) can be translated into a measure space over the cylinder algebra of T and T \u2032. So, due to the uniqueness property of Theorem 5.4, it suffices to prove that every \u222aCT \u2032\nv\u2032 has\nthe same probability in \u00b5T and \u00b5T \u2032 . That is, \u03c0(\u222aC T \u2032\nv\u2032 ) = P(L(v\u2032)). We do so next. We assume that v\u2032 is not the root of T \u2032, or otherwise the claim is straightforward. Let U be the set of all the nodes u in T with the property that L(v\u2032) \u2286 L(u) but L(v\u2032) 6\u2286 L(p) for the parent p of u. Due to Lemma 5.6 we have the following:\n\u03c0(\u222aCT \u2032 v\u2032 ) = \u2211\nu\u2208U\nP(L(u)) (5)\nLet E be the set of all the edges (v1, u1) of T , such that L(u1) \\ L(v1) consists of a node in v\n\u2032. Let Q be the set of all the paths from the root of T to nodes in U . Due to Proposition 4.1, we have that every two paths P1 and P2 in Q and edges (v1, u1) and (v2, u2) in P1 and P2, respectively, if both edges are in E and v1 = v2, then u1 = u2. Let T\n\u2032\u2032 be the tree that is obtained from T by considering every edge (v1, u1) in E, changing its weight to 1, and changing the weights of remaining (v1, u \u2032 1) emanating from v1 to 0. Then we have the following for every node u \u2208 U .\nP(u) = wT \u2032\u2032(u) \u00b7P(L(v \u2032)) (6)\nwhere wT \u2032\u2032 (u) is the product of the weights along the path from the root of T \u2032\u2032 to u. Combining (5) and (6), we get the following.\n\u03c0(\u222aCT \u2032\nv\u2032 ) = P(L(v \u2032)) \u00b7\n\u2211\nu\u2208U\nwT \u2032\u2032 (u)\nLet p = \u2211\nu\u2208U wT \u2032\u2032 (u). We need to prove that p = 1. Observe that p is the probability of visiting a node of U in a random walk over T \u2032\u2032 (with the probabilities defined by the weights). Equivalently, p is the probability that random walk over T \u2032\u2032 eventually sees all of the facts in v\u2032. But due to the construction of T \u2032\u2032, every rule violation that arises due to facts in both L(v\u2032) and any node of T \u2032\u2032 is deterministically resolved exactly as in L(v\u2032). Moreover, since L(v\u2032) is obtained from a chase derivation (i.e., L(v\u2032) is a derivation set), solving all such rules repeatedly results in the containment of L(v\u2032). Finally, since T \u2032\u2032 is fair (because T is fair), we get that every random walk over T \u2032\u2032 eventually sees all of the facts in L(v\u2032). Hence, p = 1, as claimed."}], "references": [{"title": "Deduction with contradictions in Datalog", "author": ["S. Abiteboul", "D. Deutch", "V. Vianu"], "venue": "In ICDT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The theory of joins in relational databases", "author": ["A.V. Aho", "C. Beeri", "J.D. Ullman"], "venue": "ACM Trans. on Datab. Syst.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1979}, {"title": "From complete to incomplete information and back", "author": ["L. Antova", "C. Koch", "D. Olteanu"], "venue": "In SIGMOD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Query language support for incomplete information in the MayBMS system", "author": ["L. Antova", "C. Koch", "D. Olteanu"], "venue": "In VLDB,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Doleans-Dade. Probability & Measure Theory", "author": ["C.R.B. Ash"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Probabilistic reasoning with answer sets", "author": ["C. Baral", "M. Gelfond", "N. Rushton"], "venue": "Theory Pract. Log. Program.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Probabilistic XML via markov chains", "author": ["M. Benedikt", "E. Kharlamov", "D. Olteanu", "P. Senellart"], "venue": "PVLDB, 3(1):770\u2013781,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Probabilistic similarity logic", "author": ["M. Br\u00f6cheler", "L. Mihalkova", "L. Getoor"], "venue": "In UAI, pages 73\u201382,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Simulation of database-valued Markov chains using SimSQL", "author": ["Z. Cai", "Z. Vagena", "L.L. Perez", "S. Arumugam", "P.J. Haas", "C.M. Jermaine"], "venue": "In SIGMOD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Datalog+/-: A family of logical knowledge representation and query languages for new applications", "author": ["A. Cal\u0300\u0131", "G. Gottlob", "T. Lukasiewicz", "B. Marnette", "A. Pieris"], "venue": "In LICS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Querying parse trees of stochastic context-free grammars", "author": ["S. Cohen", "B. Kimelfeld"], "venue": "In Database Theory - ICDT 2010, 13th International Conference, Lausanne, Switzerland, March 23-25,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "On 12  probabilistic fixpoint and Markov chain query languages", "author": ["D. Deutch", "C. Koch", "T. Milo"], "venue": "In PODS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on AI and Machine Learning", "author": ["P. Domingos", "D. Lowd"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Disjunctive datalog", "author": ["T. Eiter", "G. Gottlob", "H. Mannila"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Recursive markov chains, stochastic grammars, and monotone systems of nonlinear equations", "author": ["K. Etessami", "M. Yannakakis"], "venue": "J. ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Data exchange: Semantics and query answering", "author": ["R. Fagin", "P.G. Kolaitis", "R.J. Miller", "L. Popa"], "venue": "In ICDT, volume 2572 of LNCS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Probabilistic datalog: Implementing logical information retrieval for advanced applications", "author": ["N. Fuhr"], "venue": "JASIS, 51(2):95\u2013110,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Peer data exchange", "author": ["A. Fuxman", "P.G. Kolaitis", "R.J. Miller", "W.C. Tan"], "venue": "ACM Trans. Datab. Syst.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "The principles and practice of probabilistic programming", "author": ["N.D. Goodman"], "venue": "In POPL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Query answering under probabilistic uncertainty in Datalog+/ ontologies", "author": ["G. Gottlob", "T. Lukasiewicz", "M. Martinez", "G. Simari"], "venue": "Annals of Math.& AI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "LogicBlox, platform and language: A tutorial", "author": ["T.J. Green", "M. Aref", "G. Karvounarakis"], "venue": "In Int Conf on Datalog in Academia and Industry,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "LogiQL: A Query Language for Smart Databases", "author": ["T. Halpin", "S. Rugaber"], "venue": "CRC Press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "MCDB: a monte carlo approach to managing uncertain data", "author": ["R. Jampani", "F. Xu", "M. Wu", "L.L. Perez", "C.M. Jermaine", "P.J. Haas"], "venue": "In SIGMOD,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Probabilistic XML: models and complexity. In Advances in Probabilistic Databases for Uncertain Information Management, volume 304 of Studies in Fuzziness and Soft Computing, pages 39\u201366", "author": ["B. Kimelfeld", "P. Senellart"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "On the implementation of the probabilistic logic programming language ProbLog", "author": ["A. Kimmig", "B. Demoen", "L. De Raedt", "V. Santos Costa", "R. Rocha"], "venue": "Theory and Practice of Logic Programming,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Testing implications of data dependencies", "author": ["D. Maier", "A.O. Mendelzon", "Y. Sagiv"], "venue": "ACM Trans. on Datab. Syst.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1979}, {"title": "Venture: a higher-order probabilistic programming platform with programmable inference", "author": ["V.K. Mansinghka", "D. Selsam", "Y.N. Perov"], "venue": "CoRR, abs/1404.0099,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Multi-label text classification with a mixture model trained by EM", "author": ["A.K. McCallum"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch"], "venue": "In IJCAI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A. McCallum", "S. Thrun", "T.M. Mitchell"], "venue": "Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Tuffy: Scaling up statistical inference in markov logic networks using an RDBMS", "author": ["F. Niu", "C. R\u00e9", "A. Doan", "J.W. Shavlik"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "DeepDive: Web-scale knowledge-base construction using statistical learning and inference", "author": ["F. Niu", "C. Zhang", "C. Re", "J.W. Shavlik"], "venue": "In Proceedings of the Second International Workshop on Searching and Integrating New Web Data Sources, Istanbul,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "R2: an efficient MCMC sampler for probabilistic programs", "author": ["A.V. Nori", "C. Hur", "S.K. Rajamani", "S. Samuel"], "venue": "In AAAI,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Probabilistic reasoning in intelligent systems - networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1989}, {"title": "Figaro: An object-oriented probabilistic programming language", "author": ["A. Pfeffer"], "venue": "Technical report, Charles River Analytics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "The independent choice logic and beyond", "author": ["D. Poole"], "venue": "In Probabilistic Inductive Logic Programming - Theory and Applications,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Applied Statistical Decision Theory", "author": ["H. Raiffa", "R. Schlaifer"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1961}, {"title": "PRISM: A language for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "In IJCAI,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1997}, {"title": "Probabilistic Databases", "author": ["D. Suciu", "D. Olteanu", "C. R\u00e9", "C. Koch"], "venue": "Synthesis Lectures on Data Management. Morgan & Claypool Publishers,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Trio: a system for data, uncertainty, and lineage", "author": ["J. Widom"], "venue": "Managing and Mining Uncertain Data,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}], "referenceMentions": [{"referenceID": 19, "context": "An intensively studied concept in that area is that of Probabilistic Programming [21] (PP), where the idea is that the programming language allows for building general random procedures, while the system executes the program not in the standard programming sense, but rather by means of inference.", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": ", [29,31,37]) towards facilitating the development of algorithms based on machine learning.", "startOffset": 2, "endOffset": 12}, {"referenceID": 29, "context": ", [29,31,37]) towards facilitating the development of algorithms based on machine learning.", "startOffset": 2, "endOffset": 12}, {"referenceID": 35, "context": ", [29,31,37]) towards facilitating the development of algorithms based on machine learning.", "startOffset": 2, "endOffset": 12}, {"referenceID": 28, "context": "One common approach to this task assumes a generative process that produces random parameters for every class, and then uses these parameters to define a generator of random words in documents of the corresponding class [30, 32].", "startOffset": 220, "endOffset": 228}, {"referenceID": 30, "context": "One common approach to this task assumes a generative process that produces random parameters for every class, and then uses these parameters to define a generator of random words in documents of the corresponding class [30, 32].", "startOffset": 220, "endOffset": 228}, {"referenceID": 7, "context": "Latent Dirichlet Allocation [9] approaches this problem in a similar generative way as the above, with the addition that each document is associated with a distribution over topics.", "startOffset": 28, "endOffset": 31}, {"referenceID": 4, "context": "We use cylinder sets [6] to define a measure space based on a chase, and prove that this definition is robust, since one establishes the same probability measure no matter which chase is used.", "startOffset": 21, "endOffset": 24}, {"referenceID": 29, "context": ") The first category includes imperative probabilistic programming languages [43], such as BLOG [31], that can express probability distributions over logical structures, via generative stochastic models that can draw values at random from numerical distributions, and con-", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "P-log [7] is a Prologbased language for specifying Bayesian networks.", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "In the second step, a logic program, such as Prolog [27] or Datalog [1], is evaluated over the resulting random structure.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "In the second step, a logic program, such as Prolog [27] or Datalog [1], is evaluated over the resulting random structure.", "startOffset": 68, "endOffset": 71}, {"referenceID": 38, "context": "This approach has been taken by PRISM [40], the Independent Choice Logic [38], and to a large extent by probabilistic databases [41] and their semistructured counterparts [26].", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "This approach has been taken by PRISM [40], the Independent Choice Logic [38], and to a large extent by probabilistic databases [41] and their semistructured counterparts [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 39, "context": "This approach has been taken by PRISM [40], the Independent Choice Logic [38], and to a large extent by probabilistic databases [41] and their semistructured counterparts [26].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "This approach has been taken by PRISM [40], the Independent Choice Logic [38], and to a large extent by probabilistic databases [41] and their semistructured counterparts [26].", "startOffset": 171, "endOffset": 175}, {"referenceID": 40, "context": "One step beyond the second category and closer to our work is taken by uncertainty-aware query languages for probabilistic data such as TriQL [42], I-SQL, and world-set algebra [4, 5].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": "One step beyond the second category and closer to our work is taken by uncertainty-aware query languages for probabilistic data such as TriQL [42], I-SQL, and world-set algebra [4, 5].", "startOffset": 177, "endOffset": 183}, {"referenceID": 3, "context": "One step beyond the second category and closer to our work is taken by uncertainty-aware query languages for probabilistic data such as TriQL [42], I-SQL, and world-set algebra [4, 5].", "startOffset": 177, "endOffset": 183}, {"referenceID": 2, "context": "The latter two are natural analogs to SQL and relational algebra for the case of incomplete information and probabilistic data [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 12, "context": "World-set algebra has been extended to (world-set) Datalog, fixpoint, and while-languages [14] to define Markov chains.", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "MCDB [25] and SimSQL [11] propose SQL extensions (with for-loops and probability distributions) coupled with Monte Carlo simulations and parallel database techniques for stochastic analytics in the database.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "MCDB [25] and SimSQL [11] propose SQL extensions (with for-loops and probability distributions) coupled with Monte Carlo simulations and parallel database techniques for stochastic analytics in the database.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "This category includes Markov Logic Networks (MLNs) [15, 33], where the logical rules are used as a compact and intuitive way of defining factors.", "startOffset": 52, "endOffset": 60}, {"referenceID": 31, "context": "This category includes Markov Logic Networks (MLNs) [15, 33], where the logical rules are used as a compact and intuitive way of defining factors.", "startOffset": 52, "endOffset": 60}, {"referenceID": 32, "context": "This approach is applied in DeepDive [34], where a database is used for storing relational data and extracted text, and database queries are used for defining the factors of a factor graph.", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "A similar approach is taken by Probabilistic Soft Logic [10], where in each possible world every fact is associated with a weight (degree of truth).", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "Further formalisms in this category are probabilistic Datalog [19], probabilistic Datalog+/- [22], and probabilistic logic programming (ProbLog) [27].", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Further formalisms in this category are probabilistic Datalog [19], probabilistic Datalog+/- [22], and probabilistic logic programming (ProbLog) [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "Further formalisms in this category are probabilistic Datalog [19], probabilistic Datalog+/- [22], and probabilistic logic programming (ProbLog) [27].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "Related formalisms are those of the Probabilistic Context-Free Grammar (PCFG) and the more general Recursive Markov Chain (RMC) [17], where the probabilistic specification is by means of a finite set of transition graphs that can call one another (in the sense of method call) in a possibly recursive fashion.", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "In database research, PCFGs and RMCs have been explored in the context of probabilistic XML [8, 13].", "startOffset": 92, "endOffset": 99}, {"referenceID": 11, "context": "In database research, PCFGs and RMCs have been explored in the context of probabilistic XML [8, 13].", "startOffset": 92, "endOffset": 99}, {"referenceID": 10, "context": "In this work we use Datalog with the option of having existential variables in the head [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "We initially focus on the discrete case; there, a probability space is a pair (\u03a9, \u03c0), where \u03a9 is a finite or countably infinite set, called the sample space, and \u03c0 : \u03a9 \u2192 [0, 1] is such that \u2211 o\u2208\u03a9 \u03c0(o) = 1.", "startOffset": 170, "endOffset": 176}, {"referenceID": 0, "context": "A parameterized probability distribution is a function \u03b4 : \u03a9 \u00d7 R \u2192 [0, 1], such that \u03b4(\u00b7,p) : \u03a9 \u2192 [0, 1] is a probability distribution for all p \u2208 R.", "startOffset": 67, "endOffset": 73}, {"referenceID": 0, "context": "A parameterized probability distribution is a function \u03b4 : \u03a9 \u00d7 R \u2192 [0, 1], such that \u03b4(\u00b7,p) : \u03a9 \u2192 [0, 1] is a probability distribution for all p \u2208 R.", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "\u2022 Flip(x|p): \u03a9 is {0, 1}, and for a parameter p \u2208 [0, 1] we have Flip(1|p) = p and Flip(0|p) = 1\u2212 p.", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "\u2022 Geo(x|p): \u03a9 = N, and for a parameter p \u2208 [0, 1] we have Geo(x|p) = (1\u2212 p)p.", "startOffset": 43, "endOffset": 49}, {"referenceID": 34, "context": "Our example is based on the burglar example of Pearl [36] that has been frequently used for illustrating probabilistic programming (e.", "startOffset": 53, "endOffset": 57}, {"referenceID": 33, "context": ", [35]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "This draws on the notion of weak acyclicity for Datalog [18].", "startOffset": 56, "endOffset": 60}, {"referenceID": 39, "context": "We also highlight the fact that our framework can easily simulate the probabilistic database model of independent tuples [41] with probabilities mentioned in the database, using the Flip distribution, as follows.", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "Finally, we note that a disjunctive Datalog rule [16], where the conclusion can be a disjunction of atoms, can be simulated by our model (with probabilities ignored): If the conclusion has n disjuncts, then we construct a distributional rule with a probability distribution over {1, .", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "The chase [3,28] is a classic technique used for reasoning about tuple-generating dependencies and equalitygenerating dependencies.", "startOffset": 10, "endOffset": 16}, {"referenceID": 26, "context": "The chase [3,28] is a classic technique used for reasoning about tuple-generating dependencies and equalitygenerating dependencies.", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "G is a possibly infinite tree, whose nodes are labeled by instances over E\u222aI and where each edge is labeled by a real number r \u2208 [0, 1] such that", "startOffset": 129, "endOffset": 135}, {"referenceID": 0, "context": "A probability measure space is a triple (\u03a9,F , \u03c0), where: (1) \u03a9 is a set, called the sample space, (2) F is a \u03c3-algebra over \u03a9, and (3) \u03c0 : F \u2192 [0, 1], called a probability measure, is such that \u03c0(\u03a9) = 1, and \u03c0(\u222aE) = \u2211 e\u2208E \u03c0(e) for every countable set E of pairwise-disjoint measurable sets.", "startOffset": 144, "endOffset": 150}, {"referenceID": 4, "context": "A measure space for such a Markov chain is defined by means of cylinderification [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "Also, an interesting problem is to detect conditions under which the chase is a self conjugate [39], that is, the probability space \u03bcP,I is captured by a chase procedure without backtracking.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "We assume that fk[j] \u2208 [0, 1] for all k = 1, .", "startOffset": 23, "endOffset": 29}, {"referenceID": 2, "context": "The above specification for the categorical distribution is similar to the repair-key operation of the worldset algebra [4, 5].", "startOffset": 120, "endOffset": 126}, {"referenceID": 3, "context": "The above specification for the categorical distribution is similar to the repair-key operation of the worldset algebra [4, 5].", "startOffset": 120, "endOffset": 126}, {"referenceID": 21, "context": "This work is done as part of the effort to extend the LogicBlox database [23], and its Datalog-based data management language LogiQL [24], to support the specification of statistical models.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "This work is done as part of the effort to extend the LogicBlox database [23], and its Datalog-based data management language LogiQL [24], to support the specification of statistical models.", "startOffset": 133, "endOffset": 137}], "year": 2015, "abstractText": "Formalisms for specifying general statistical models, such as probabilistic-programming languages, typically consist of two components: a specification of a stochastic process (the prior), and a specification of observations that restrict the probability space to a conditional subspace (the posterior). Use cases of such formalisms include the development of algorithms in machine learning and artificial intelligence. We propose and investigate a declarative framework for specifying statistical models on top of a database, through an appropriate extension of Datalog. By virtue of extending Datalog, our framework offers a natural integration with the database, and has a robust declarative semantics (that is, semantic independence from the algorithmic evaluation of rules, and semantic invariance under logical program transformations). Our proposed Datalog extension provides convenient mechanisms to include common numerical probability functions; in particular, conclusions of rules may contain values drawn from such functions. The semantics of a program is a probability distribution over the possible outcomes of the input database with respect to the program; these possible outcomes are minimal solutions with respect to a related program that involves existentially quantified variables in conclusions. Observations are naturally incorporated by means of integrity constraints over the extensional and intensional relations. We focus on programs that use discrete numerical distributions, but even then the space of possible outcomes may be uncountable (as a solution can be infinite). We define a probability measure over possible outcomes by applying the known concept of cylinder sets to a probabilistic chase procedure. We show that the resulting semantics is robust under different chases. We also identify conditions guaranteeing that all possible outcomes are finite (and then the probability space is discrete). We argue that the framework we propose retains the purely declarative nature of Datalog, and allows for natural specifications of statistical models.", "creator": "LaTeX with hyperref package"}}}