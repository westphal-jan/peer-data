{"id": "1412.4729", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2014", "title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "abstract": "troubles solving, the visual symbol grounding system problem inherently has long been a goal of teaching artificial intelligence. indeed the neuroscience field generally appears to be advancing closer to implementing this goal culminated with sufficiently recent breakthroughs assisting in discovering deep structured learning solutions for natural language encoding grounding in executing static language images. in this paper, we specifically propose to translate videos performed directly to sentences, using either a unified deep neural network inherently with both chaotic convolutional and randomly recurrent structure. described are video transformation datasets are uniformly scarce, and most existing training methods have been applied mechanically to toy domains with a small vocabulary of 12 possible key words. characterized by transferring input knowledge from 1. 2m + images with category labels presented and 100, 520 000 + images often with small captions, our method perspective is overwhelmingly able to partially create sentence graphic descriptions of open - domain videos with large vocabularies. consequently we compare up our structured approach with recent work by using language text generation metrics, citing subject, verb, and query object marking prediction accuracy, and a human evaluation.", "histories": [["v1", "Mon, 15 Dec 2014 19:21:50 GMT  (1859kb,D)", "https://arxiv.org/abs/1412.4729v1", null], ["v2", "Fri, 19 Dec 2014 00:58:38 GMT  (1859kb,D)", "http://arxiv.org/abs/1412.4729v2", "Corrected minor typos"], ["v3", "Thu, 30 Apr 2015 04:22:06 GMT  (6586kb,D)", "http://arxiv.org/abs/1412.4729v3", "NAACL-HLT 2015 camera ready"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["subhashini venugopalan", "huijuan xu", "jeff donahue", "marcus rohrbach", "raymond j mooney", "kate saenko"], "accepted": true, "id": "1412.4729"}, "pdf": {"name": "1412.4729.pdf", "metadata": {"source": "CRF", "title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "authors": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "emails": ["vsub@cs.utexas.edu", "hxu1@cs.uml.edu", "jdonahue@eecs.berkeley.edu", "rohrbach@eecs.berkeley.edu", "mooney@cs.utexas.edu", "saenko@cs.uml.edu"], "sections": [{"heading": "1 Introduction", "text": "For most people, watching a brief video and describing what happened (in words) is an easy task. For machines, extracting the meaning from video pixels and generating natural-sounding language is a very complex problem. Solutions have been proposed for narrow domains with a small set of known actions and objects, e.g., (Barbu et al., 2012; Rohrbach et al., 2013), but generating descriptions for \u201cin-thewild\u201d videos such as the YouTube domain (Figure 1) remains an open challenge.\nProgress in open-domain video description has been difficult in part due to large vocabularies and\nInput video:\nOur output: A cat is playing with a toy. Humans: A Ferret and cat fighting with each other. / A cat and a ferret are playing. / A kitten is playing with a ferret. / A kitten and a ferret are playfully wrestling.\nFigure 1: Our system takes a short video as input and outputs a natural language description of the main activity in the video.\nvery limited training data consisting of videos with associated descriptive sentences. Another serious obstacle has been the lack of rich models that can capture the joint dependencies of a sequence of frames and a corresponding sequence of words. Previous work has simplified the problem by detecting a fixed set of semantic roles, such as subject, verb, and object (Guadarrama et al., 2013; Thomason et al., 2014), as an intermediate representation. This fixed representation is problematic for large vocabularies and also leads to oversimplified rigid sentence templates which are unable to model the complex structures of natural language.\nIn this paper, we propose to translate from video pixels to natural language with a single deep neural network. Deep NNs can learn powerful features (Donahue et al., 2013; Zeiler and Fergus, 2014), but require a lot of supervised training data. We address the problem by transferring knowledge from auxiliary tasks. Each frame of the video is modeled by a convolutional (spatially-invariant) network pre-trained on 1.2M+ images with category labels (Krizhevsky et al., 2012). The meaning state\nar X\niv :1\n41 2.\n47 29\nv3 [\ncs .C\nV ]\n3 0\nA pr\nand sequence of words is modeled by a recurrent (temporally invariant) deep network pre-trained on 100K+ Flickr (Hodosh and Hockenmaier, 2014) and COCO (Lin et al., 2014) images with associated sentence captions. We show that such knowledge transfer significantly improves performance on the video task.\nOur approach is inspired by recent breakthroughs reported by several research groups in image-to-text generation, in particular, the work by Donahue et al. (2014). They applied a version of their model to video-to-text generation, but stopped short of proposing an end-to-end single network, using an intermediate role representation instead. Also, they showed results only on the narrow domain of cooking videos with a small set of pre-defined objects and actors. Inspired by their approach, we utilize a Long-Short Term Memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997) to model sequence dynamics, but connect it directly to a deep convolutional neural network to process incoming video frames, avoiding supervised intermediate representations altogether. This model is similar to their image-to-text model, but we adapt it for video sequences.\nOur proposed approach has several important advantages over existing video description work. The LSTM model, which has recently achieved state-ofthe-art results on machine translation tasks (French and English (Sutskever et al., 2014)), effectively models the sequence generation task without requiring the use of fixed sentence templates as in previous work (Guadarrama et al., 2013). Pre-training on image and text data naturally exploits related data to supplement the limited amount of descriptive video currently available. Finally, the deep convnet, the winner of the ILSVRC2012 (Russakovsky et al., 2014) image classification competition, provides a strong visual representation of objects, actions and scenes depicted in the video.\nOur main contributions are as follows: \u2022 We present the first end-to-end deep model for\nvideo-to-text generation that simultaneously learns a latent \u201cmeaning\u201d state, and a fluent grammatical model of the associated language.\n\u2022 We leverage still image classification and caption data and transfer deep networks learned on such data to the video domain.\n\u2022 We provide a detailed evaluation of our model on the popular YouTube corpus (Chen and Dolan, 2011) and demonstrate a significant improvement over the state of the art."}, {"heading": "2 Related Work", "text": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al. (2014) produce descriptions for videos of several people cooking in the same kitchen. These approaches generate sentences by first predicting a semantic role representation, e.g., modeled with a CRF, of high-level concepts such as the actor, action and object. Then they use a template or statistical machine translation to translate the semantic representation to a sentence.\nMost work on \u201cin-the-wild\u201d online video has focused on retrieval and predicting event tags rather than generating descriptive sentences; examples are tagging YouTube (Aradhye et al., 2009) and retrieving online video in the TRECVID competition (Over et al., 2012). Work on TRECVID has also included clustering both video and text features for video retrieval, e.g., (Wei et al., 2010; Huang et al., 2013).\nThe previous work on the YouTube corpus we employ (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate a grammatical sentence. They also utilize language models learned from large text corpora to aid visual interpretation as well as sentence generation. We compare our method to the best-performing method of Thomason et al. (2014). A recent paper by Xu et al. (2015) extracts deep features from video and a continuous vector from language, and projects both to a joint semantic space. They apply their joint embedding to SVO prediction and generation, but do not provide quantitative generation results. Our network learns a joint state vector implicitly, and additionally models sequence dynamics of the language.\nPredicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al. (2010), Yao et al. (2010), and Kulkarni et al. (2011) amongst others. Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).\nIn this work, we use deep recurrent nets (RNNs), which have recently demonstrated strong results for machine translation tasks using Long Short Term Memory (LSTM) RNNs (Sutskever et al., 2014; Cho et al., 2014). In contrast to traditional statistical MT (Koehn, 2010), RNNs naturally combine with vector-based representations, such as those for images and video. Donahue et al. (2014) and Vinyals et al. (2014) simultaneously proposed a multimodal analog of this model, with an architecture which uses a visual convnet to encode a deep state vector, and an LSTM to decode the vector into a sentence.\nOur approach to video to text generation is inspired by the work of Donahue et al. (2014), who also applied a variant of their model to video-to-text generation, but stopped short of training an end-toend model. Instead they converted the video to an intermediate role representation using a CRF, then decoded that representation into a sentence. In contrast, we bypass detection of high-level roles and use the output of a deep convolutional network directly as the state vector that is decoded into a sentence. This avoids the need for labeling semantic roles, which can be difficult to detect in the case of very large vocabularies. It also allows us to first pre-train the model on a large image and caption database, and transfer the knowledge to the video domain where the corpus size is smaller. While Donahue et al. (2014) only showed results on a narrow domain of cooking videos with a small set of pre-defined objects and actors, we generate sentences for opendomain YouTube videos with a vocabulary of thousands of words."}, {"heading": "3 Approach", "text": "Figure 2 depicts our model for sentence generation from videos. Our framework is based on deep image description models in Donahue et al. (2014);Vinyals\net al. (2014) and extends them to generate sentences describing events in videos. These models work by first applying a feature transformation on an image to generate a fixed dimensional vector representation. They then use a sequence model, specifically a Recurrent Neural Network (RNN), to \u201cdecode\u201d the vector into a sentence (i.e. a sequence of words). In this work, we apply the same principle of \u201ctranslating\u201d a visual vector into an English sentence and show that it works well for describing dynamic videos as well as static images.\nWe identify the most likely description for a given video by training a model to maximize the log likelihood of the sentence S, given the corresponding video V and the model parameters \u03b8,\n\u03b8\u2217 = argmax \u03b8 \u2211 (V,S) log p(S|V ; \u03b8) (1)\nAssuming a generative model of S that produces each word in the sequence in order, the log probability of the sentence is given by the sum of the log probabilities over the words and can be expressed as:\nlog p(S|V ) = N\u2211 t=0 log p(Swt |V, Sw1 , . . . , Swt\u22121)\nwhere Swi represents the i th word in the sentence and N is the total number of words. Note that we have dropped \u03b8 for convenience.\nA sequence model would be apt to model p(Swt |V, Sw1 , . . . , Swt\u22121), and we choose an RNN. An RNN, parameterized by \u03b8, maps an input xt, and the previously seen words expressed as a hidden state or memory, ht\u22121 to an output zt and an\nupdated state ht using a non-linear function f :\nht = f\u03b8(xt, ht\u22121) (2)\nwhere (h0 = 0). In our work we use the highly successful Long Short-Term Memory (LSTM) net as the sequence model, since it has shown superior performance on tasks such as speech recognition (Graves and Jaitly, 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014) and the more related task of generating sentence descriptions of images (Donahue et al., 2014; Vinyals et al., 2014). To be specific, we use two layers of LSTMs (one LSTM stacked atop another) as shown in Figure 2. We present details of the network in Section 3.1. To convert videos to a fixed length representation (input xt), we use a Convolutional Neural Network (CNN). We present details of how we apply the CNN model to videos in Section 3.2."}, {"heading": "3.1 LSTMs for sequence generation", "text": "A Recurrent Neural Network (RNN) is a generalization of feed forward neural networks to sequences. Standard RNNs learn to map a sequence of inputs (x1, . . . , xt) to a sequence of hidden states (h1, . . . , ht), and from the hidden states to a sequence of outputs (z1, . . . , zt) based on the following recurrences:\nht = f(Wxhxt +Whhht\u22121) (3)\nzt = g(Wzhht) (4)\nwhere f and g are element-wise non-linear functions such as a sigmoid or hyperbolic tangent, xt is a fixed length vector representation of the input, ht \u2208 RN is the hidden state with N units, Wij are the weights connecting the layers of neurons, and zt the output vector.\nRNNs can learn to map sequences for which the alignment between the inputs and outputs is known ahead of time (Sutskever et al., 2014) however it\u2019s unclear if they can be applied to problems where the inputs (xi) and outputs (zi) are of varying lengths. This problem is solved by learning to map sequences of inputs to a fixed length vector using one RNN, and then map the vector to an output sequence using another RNN. Another known problem with RNNs is that, it can be difficult to train them to learn longrange dependencies (Hochreiter et al., 2001). However, LSTMs (Hochreiter and Schmidhuber, 1997),\nwhich incorporate explicitly controllable memory units, are known to be able to learn long-range temporal dependencies. In our work we use the LSTM unit in Figure 3, described in Zaremba and Sutskever (2014), and Donahue et al. (2014).\nAt the core of the LSTM model is a memory cell c which encodes, at every time step, the knowledge of the inputs that have been observed up to that step. The cell is modulated by gates which are all sigmoidal, having range [0, 1], and are applied multiplicatively. The gates determine whether the LSTM keeps the value from the gate (if the layer evaluates to 1) or discards it (if it evaluates to 0). The three gates \u2013 input gate (i) controlling whether the LSTM considers its current input (xt), the forget gate (f ) allowing the LSTM to forget its previous memory (ct\u22121), and the output gate (o) deciding how much of the memory to transfer to the hidden state (ht), all enable the LSTM to learn complex long-term dependencies. The recurrences for the LSTM are then defined as: it = \u03c3(Wxixt +Whiht\u22121) (5)\nft = \u03c3(Wxfxt +Whfht\u22121) (6)\not = \u03c3(Wxoxt +Whoht\u22121) (7)\nct = ft ct\u22121 + it \u03c6(Wxcxt +Whcht\u22121) (8) ht = ot \u03c6(ct) (9)\nwhere \u03c3 is the sigmoidal non-linearity, \u03c6 is the hyperbolic tangent non-linearity, represents the\nproduct with the gate value, and the weight matrices denoted by Wij are the trained parameters."}, {"heading": "3.2 CNN-LSTMs for video description", "text": "We use a two layer LSTM model for generating descriptions for videos based on experiments by Donahue et al. (2014) which suggest two LSTM layers are better than four and a single layer for image to text tasks. We employ the LSTM to \u201cdecode\u201d a visual feature vector representing the video to generate textual output. The first step in this process is to generate a fixed-length visual input that effectively summarizes a short video. For this we use a CNN, specifically the publicly available Caffe (Jia et al., 2014) reference model, a minor variant of AlexNet (Krizhevsky et al., 2012). The net is pre-trained on the 1.2M image ILSVRC-2012 object classification subset of the ImageNet dataset (Russakovsky et al., 2014) and hence provides a robust initialization for recognizing objects and thereby expedites training. We sample frames in the video (1 in every 10 frames) and extract the output of the fc7 layer and perform a mean pooling over the frames to generate a single 4,096 dimension vector for each video. The resulting visual feature vector forms the input to the first LSTM layer. We stack another LSTM layer on top as in Figure 2, and the hidden state of the LSTM in the first layer is the input to the LSTM unit in the second layer. A word from the sentence forms the target of the output LSTM unit. In this work, we represent words using \u201cone-hot\u201d vectors (i.e 1-of-N coding, where is N is the vocabulary size).\nTraining and Inference: The two-layer LSTM model is trained to predict the next word Swt in the sentence given the visual features and the previous t \u2212 1 words, p(Swt |V, Sw1 , . . . , Swt\u22121). During training the visual feature, sentence pair (V, S) is provided to the model, which then optimizes the log-likelihood (Equation 1) over the entire training dataset using stochastic gradient descent. At each time step, the input xt is fed to the LSTM along with the previous time step\u2019s hidden state ht\u22121 and the LSTM emits the next hidden state vector ht (and a word). For the first layer of the LSTM xt is the concatenation of the visual feature vector and the previous encoded word (Swt\u22121 , the ground truth word during training and the predicted word during test\ntime). For the second layer of the LSTM xt is zt of the first layer. Accordingly, inference must also be performed sequentially in the order h1 = fW (x1, 0), h2 = fW (x2, h1), until the model emits the endof-sentence (EOS) token at the final step T . In our model the output (ht = zt) of the second layer LSTM unit is used to obtain the emitted word. We apply the Softmax function, to get a probability distribution over the words w in the vocabulary D.\np(w|zt) = exp(Wwzt)\u2211\nw\u2032\u2208D exp(Ww\u2032zt) (10)\nwhere Ww is a learnt embedding vector for word w. At test time, we choose the word w\u0302 with the maximum probability for each time step t until we obtain the EOS token."}, {"heading": "3.3 Transfer Learning from Captioned Images", "text": "Since the training data available for video description is quite limited (described in Section 4.1), we also leverage much larger datasets available for image captioning to train our LSTM model and then fine tune it on the video dataset. Our LSTM model for images is the same as the one described above for single video frames (in Section 3.1, and 3.2). As with videos, we extract fc7 layer features (4096 dimensional vector) from the network (Section 3.2) for the images. This forms the visual feature that is input to the 2-layer LSTM description model. The vocabulary is the combined set of words in the video and image datasets. After the model is trained on the image dataset, we use the weights of the trained model to initialize the LSTM model for the video description task. Additionally, we reduce the learning rate on our LSTM model to allow it to tune to the video dataset. This speeds up training and allows exploiting knowledge previously learned for image description."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "Video dataset. We perform all our experiments on the Microsoft Research Video Description Corpus (Chen and Dolan, 2011). This video corpus is a collection of 1970 YouTube snippets. The duration of each clip is between 10 seconds to 25 seconds, typically depicting a single activity or a short\nsequence. The dataset comes with several human generated descriptions in a number of languages; we use the roughly 40 available English descriptions per video. This dataset (or portions of it) have been used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks. For our task we pick 1200 videos to be used as training data, 100 videos for validation and 670 videos for testing, as used by the prior works on video description (Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015).\nDomain adaptation, image description datasets. Since the number of videos for the description task is quite small when compared to the size of the datasets used by LSTM models in other tasks such as translation (Sutskever et al., 2014) (12M sentences), we use data from the Flickr30k and COCO2014 datasets for training and learn to adapt to the video dataset by fine-tuning the image description models. The Flickr30k (Hodosh and Hockenmaier, 2014) dataset has about 30,000 images, each with 5 or more descriptions. We hold out 1000 images at random for validation and use the remaining for training. In addition to this, we use the recent COCO2014 (Lin et al., 2014) image description dataset consisting of 82,783 training images and 40,504 validation images, each with 5 or more sentence descriptions. We perform ablation experiments by training models on each dataset individually, and on the combination and report results on the YouTube video test dataset."}, {"heading": "4.2 Models", "text": "HVC This is the Highest Vision Confidence model described in (Thomason et al., 2014). The model uses strong visual detectors to predict confidence over 45 subjects, 218 verbs and 241 objects.\nFGM (Thomason et al., 2014) also propose a factor graph model (FGM) that combines knowledge mined from text corpora with visual confidences from the HVC model using a factor graph and performs probabilistic inference to determine the most likely subject, verb, object and scene tuple. They then use a simple template to generate a sentence from the tuple. In this work, we compare the output of our model to the subject, verb, object words\npredicted by the HVC and FGM models and the sentences generated from the SVO triple.\nOur LSTM models We present four main models. LSTM-YT is our base two-layer LSTM model trained on the YouTube video dataset. LSTMYTflickr is the model trained on the Flickr30k (Hodosh and Hockenmaier, 2014) dataset, and fine tuned on the YouTube dataset as descibed in Section 3.3. LSTM-YTcoco is first trained on the COCO2014 (Lin et al., 2014) dataset and then fine-tuned on the video dataset. Our final model, LSTM-YTcocoflickr is trained on the combined data of both the Flickr and COCO models and is tuned on YouTube. To compare the overlap in content between the image dataset and YouTube dataset, we use the model trained on just the Flickr images (LSTMflickr) and just the COCO images (LSTMcoco) and evaluate their performance on the test videos."}, {"heading": "4.3 Evaluation Metrics and Results", "text": "SVO accuracy. Earlier works (Krishnamoorthy et al., 2013; Guadarrama et al., 2013) that reported results on the YouTube dataset compared their method based on how well their model could predict the subject, verb, and object (SVO) depicted in the video. Since these models first predicted the content (SVO triples) and then generated the sentences, the S,V,O accuracy captured the quality of the content generated by the models. However, in our case the sequential LSTM directly outputs the sentence, so we extract the S,V,O from the dependency parse of the generated sentence. We present, in Table 1 and Table 2, the accuracy of S,V,O words comparing the performance of our model against any valid ground truth triple and the most frequent triple found in human description for each video. The latter evaluation was also reported by (Xu et al., 2015), so we include it here for comparison.\nSentence Generation. To evaluate the generated sentences we use the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores against all ground truth sentences. BLEU is the metric that is seen more commonly in image description literature, but a more recent study (Elliott and Keller, 2014) has shown METEOR to be a better evaluation metric. However, since both metrics have been shown to correlate well with human eval-\nuations, we compare the generated sentences using both and present our results in Table 3.\nHuman Evaluation. We used Amazon Mechanical Turk to also collect human judgements. We created a task which employed three Turk workers to watch each video, and rank sentences generated by the different models from \u201cMost Relevant\u201d (5) to \u201cLeast Relevant\u201d (1). No two sentences could have the same rank unless they were identical. We also evaluate sentences on grammatical correctness. We created a different task which required workers to rate sentences based on grammar. This task\n1They evaluate against a filtered set of groundtruth SVO words which provides a tiny boost to their scores.\ndisplayed only the sentences and did not show any video. Here, workers had to choose a rating between 1-5 for each sentence. Multiple sentences could have the same rating. We discard responses from workers who fail gold-standard items and report the mean ranking/rating for each of the evaluated models in Table 4.\nIndividual Frames. In order to evaluate the effectiveness of mean pooling, we performed experiments to train and test the model on individual frames from the video. Our first set of experiments involved testing how well the image description models performed on a randomly sampled frame in the video. Similar to Tables 1 and 2, the model trained on Flickr30k when tested on random frames from the video scored better on subjects and verbs with any valid accuracy of 75.16% and 11.65% respectively; and 9.01% on objects. The one trained on COCO did better on objects (12.54%, any valid accuracy) but very poorly on subjects and verbs. In our next experiment, we used image description models (trained on Flickr30k, COCO or a combination of both) and fine-tuned them on individual frames in the video by picking a different frame\nfor each description in the YouTube dataset. These models were tested on a random frame from the test video. The overall trends in the results were similar to those seen in Tables 1 and 2. The model trained on COCO and fine-tuned on individual video frames performed best with any valid S,V,O accuracies 84.8%, 38.98%, and 22.34% respectively. The one trained on both COCO and Flickr30k had any valid S,V,O accuracies of 85.67%, 38.83%, and 19.72%. We report the generation results for these models in Table 5."}, {"heading": "5 Discussion", "text": "Image only models. The models trained purely on the image description data LSTMflickr and LSTMcoco achieve lower accuracy on the verbs and objects (Tables 1, 2) since the YouTube videos encompass a wider domain and a variety of actions not detectable from static images.\nBase LSTM model. We note that in the SVO binary accuracy metrics (Tables 1 and 2), the base LSTM model (LSTM-YT) achieves a slightly lower accuracy compared to prior work. This is likely due to the fact that previous work explicitly optimizes to identify the best subject, verb and object for a video; whereas the LSTM model is trained on objects and actions jointly in a sentence and needs to learn to interpret these in different contexts. However, with regard to the generation metrics BLEU and METEOR, training based on the full sentence helps the LSTM model develop fluency and vocabulary similar to that seen in the training descriptions and allows it to outperform the template based generation.\nTransferring helps. From our experiments, it is\nclear that learning from the image description data improves the performance of the model in all criteria of evaluation. We present a few examples demonstrating this in Figure 4. The model that was pretrained on COCO2014 shows a larger performance improvement, indicating that our model can effectively leverage a large auxiliary source of training data to improve its object and verb predictions. The model pre-trained on the combined data of Flickr30k and COCO2014 shows only a marginal improvement, perhaps due to overfitting. Adding dropout as in (Vinyals et al., 2014) is likely to help prevent overfitting and improve performance.\nFrom the automated evaluation in Table 3 it is clear that the fully deep video-to-text generation models outperform previous work. As mentioned previously, training on the full sentences is probably the main reason for the improvements.\nTesting on individual frames. The experiments that evaluated models on individual frames (Section 4.3) from the video have trends similar to those seen on mean pooled frame features. Specifically, the model trained on Flickr30k, when directly evaluated on YouTube video frames performs better on subjects and verbs, whereas the one trained on COCO does better on objects. This is explained by the fact that Flickr30k images are more varied but COCO has more examples of a smaller collection of objects, thus increasing object accuracy. Amongst the models trained on images and individual video frames, the ones trained on COCO (and the combination of both) perform well, but are still a bit poorer compared to the models trained on mean-pooled features. One point to note however is that, these models were trained and evaluated on random frames from the video, and not necessarily a key-frame or most-representative frame. It\u2019s likely that choosing a representative frame from the video might result in a small improvement. But, on the whole, our experiments show that models trained on images alone do not directly perform well on video frames, and a better representation is required to learn from videos.\nMean pooling is significant. Our additional experiments that trained and tested on individual frames in the video, reported in section 4.3, suggest that mean pooling frame features gives significantly better results. This could potentially indicate that mean pooling features across all frames in the video\nis a reasonable representation for short video clips at least for the task of generating simple sentential descriptions.\nHuman evaluation. We note that the sentences generated by our model have been ranked more relevant (Table 4) to the content in the video than previous models. However, there is still a significant gap between the human ground truth sentence and the ones generated by the LSTM models. Additionally, when we ask Turkers to rate only the sentences (they are not provided the video) on grammatical correctness, the template based FGM (Thomason et al., 2014) achieves the highest ratings. This can be explained by the fact that their work uses a template technique to generate sentences from content, and is hence grammatically well formed. Our model sometimes predicts prepositions and articles more frequently, resulting in duplicates and hence incorrect grammar."}, {"heading": "6 Conclusion", "text": "In this paper we have proposed a model for video description which uses neural networks for the entire pipeline from pixels to sentences and can potentially allow for the training and tuning of the entire network. In an extensive experimental evaluation, we showed that our approach generates better sentences than related approaches. We also showed that exploiting image description data improves performance compared to relying only on video description data. However our approach falls short in better utilizing the temporal information in videos, which is a good direction for future work. We will release our Caffe-based implementation, as well as the model and generated sentences."}, {"heading": "Acknowledgments", "text": "The authors thank Trevor Darrell for his valuable advice. We would also like to thank reviewers for their comments and suggestions. Marcus Rohrbach was supported by a fellowship within the FITweltweitProgram of the German Academic Exchange Service (DAAD). This research was partially supported by ONR ATL Grant N00014-11-1-010, NSF Awards IIS-1451244 and IIS-1212798."}], "references": [{"title": "Generating image descriptions using dependency relational patterns", "author": ["Ahmet Aker", "Robert Gaizauskas."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Aker and Gaizauskas.,? 2010", "shortCiteRegEx": "Aker and Gaizauskas.", "year": 2010}, {"title": "Video2text: Learning to annotate video content", "author": ["H. Aradhye", "G. Toderici", "J. Yagnik."], "venue": "IEEE International Conference on Data Mining Workshops (ICDMW).", "citeRegEx": "Aradhye et al\\.,? 2009", "shortCiteRegEx": "Aradhye et al\\.", "year": 2009}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Satanjeev Banerjee", "Alon Lavie."], "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Sum-", "citeRegEx": "Banerjee and Lavie.,? 2005", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Video in sentences out. In Association for Uncertainty in Artificial Intelligence (UAI)", "author": ["lian Wei", "Yifan Yin", "Zhiqi Zhang"], "venue": null, "citeRegEx": "Wei et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2012}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L. Chen", "William B. Dolan."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Translating related words to videos and back through latent topics", "author": ["P. Das", "R.K. Srihari", "J.J. Corso."], "venue": "Proceedings of Sixth ACM International Conference on Web Search and Data Mining (WSDM).", "citeRegEx": "Das et al\\.,? 2013a", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R.F. Doell", "J.J. Corso."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Das et al\\.,? 2013b", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "Beyond audio and video retrieval: towards multimedia summarization", "author": ["D. Ding", "F. Metze", "S. Rawat", "P.F. Schulam", "S. Burger", "E. Younessian", "L. Bao", "M.G. Christel", "A. Hauptmann."], "venue": "Proceedings of the 2nd ACM International Conference on Multimedia", "citeRegEx": "Ding et al\\.,? 2012", "shortCiteRegEx": "Ding et al\\.", "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell."], "venue": "arXiv preprint arXiv:1310.1531.", "citeRegEx": "Donahue et al\\.,? 2013", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Kate Saenko", "Trevor Darrell."], "venue": "CoRR, abs/1411.4389.", "citeRegEx": "Saenko and Darrell.,? 2014", "shortCiteRegEx": "Saenko and Darrell.", "year": 2014}, {"title": "Comparing automatic evaluation measures for image description", "author": ["Desmond Elliott", "Frank Keller."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Elliott and Keller.,? 2014", "shortCiteRegEx": "Elliott and Keller.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig."], "venue": "CoRR,", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth."], "venue": "European Conference on Computer Vision (ECCV).", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Towards end-toend speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14).", "citeRegEx": "Graves and Jaitly.,? 2014", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot", "author": ["Sergio Guadarrama", "Niveda Krishnamoorthy", "Girish Malkarnenkar", "Subhashini Venugopalan", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young Alice Lai Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics (TACL).", "citeRegEx": "Hodosh and Hockenmaier.,? 2014", "shortCiteRegEx": "Hodosh and Hockenmaier.", "year": 2014}, {"title": "A multi-modal clustering method for web videos", "author": ["Haiqi Huang", "Yueming Lu", "Fangwei Zhang", "Songlin Sun."], "venue": "Trustworthy Computing and Services. Springer.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell."], "venue": "arXiv preprint arXiv:1408.5093.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Karpathy et al\\.,? 2014", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Describing video contents in natural language", "author": ["Muhammad Usman Ghani Khan", "Yoshihiko Gotoh"], "venue": null, "citeRegEx": "Khan and Gotoh.,? \\Q2012\\E", "shortCiteRegEx": "Khan and Gotoh.", "year": 2012}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhuditnov", "Richard. S Zemel."], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Natural language description of human activities from video images based on concept hierarchy of actions", "author": ["A. Kojima", "T. Tamura", "K. Fukunaga."], "venue": "International Journal of Computer Vision (IJCV), 50(2).", "citeRegEx": "Kojima et al\\.,? 2002", "shortCiteRegEx": "Kojima et al\\.", "year": 2002}, {"title": "Generating natural-language video descriptions using text-mined knowledge", "author": ["Niveda Krishnamoorthy", "Girish Malkarnenkar", "Raymond J. Mooney", "Kate Saenko", "Sergio Guadarrama."], "venue": "AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Krishnamoorthy et al\\.,? 2013", "shortCiteRegEx": "Krishnamoorthy et al\\.", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Tamara L Berg", "UNC Chapel Hill", "Yejin Choi."], "venue": "Transactions of the Association for Computational Linguistics, 2(10).", "citeRegEx": "Kuznetsova et al\\.,? 2014", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Save: A framework for semantic annotation of visual events", "author": ["M.W. Lee", "A. Hakeem", "N. Haering", "S.C. Zhu."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Microsoft COCO: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1405.0312.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille."], "venue": "arXiv preprint arXiv:1410.1090.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Improving video activity recognition using object recognition and text mining", "author": ["Tanvi S. Motwani", "Raymond J. Mooney."], "venue": "Proceedings of the 20th European Conference on Artificial Intelligence (ECAI).", "citeRegEx": "Motwani and Mooney.,? 2012", "shortCiteRegEx": "Motwani and Mooney.", "year": 2012}, {"title": "TRECVID 2012 \u2013 an overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["Paul Over", "George Awad", "Martial Michel", "Jonathan Fiscus", "Greg Sanders", "B Shaw", "Alan F. Smeaton", "Georges Qu\u00e9enot."], "venue": "Proceedings of TRECVID 2012.", "citeRegEx": "Over et al\\.,? 2012", "shortCiteRegEx": "Over et al\\.", "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele."], "venue": "IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Rohrbach et al\\.,? 2013", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["Anna Rohrbach", "Marcus Rohrbach", "Wei Qiu", "Annemarie Friedrich", "Manfred Pinkal", "Bernt Schiele."], "venue": "German Conference on Pattern Recognition (GCPR), September.", "citeRegEx": "Rohrbach et al\\.,? 2014", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R.J. Mooney."], "venue": "Proceedings of the 25th International Conference on Computational", "citeRegEx": "Thomason et al\\.,? 2014", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CoRR, abs/1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Multimodal fusion for video search reranking", "author": ["Shikui Wei", "Yao Zhao", "Zhenfeng Zhu", "Nan Liu."], "venue": "IEEE Transactions on Knowledge and Data Engineering,, 22(8).", "citeRegEx": "Wei et al\\.,? 2010", "shortCiteRegEx": "Wei et al\\.", "year": 2010}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso."], "venue": "AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "I2t: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu."], "venue": "Proceedings of the IEEE, 98(8).", "citeRegEx": "Yao et al\\.,? 2010", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Grounded language learning from videos described with sentences", "author": ["Haonan Yu", "Jeffrey Mark Siskind."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Yu and Siskind.,? 2013", "shortCiteRegEx": "Yu and Siskind.", "year": 2013}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1410.4615.", "citeRegEx": "Zaremba and Sutskever.,? 2014", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "European Conference on Computer Vision (ECCV). Springer.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 36, "context": ", (Barbu et al., 2012; Rohrbach et al., 2013), but generating descriptions for \u201cin-thewild\u201d videos such as the YouTube domain (Figure 1) remains an open challenge.", "startOffset": 2, "endOffset": 45}, {"referenceID": 15, "context": "a fixed set of semantic roles, such as subject, verb, and object (Guadarrama et al., 2013; Thomason et al., 2014), as an intermediate representation.", "startOffset": 65, "endOffset": 113}, {"referenceID": 40, "context": "a fixed set of semantic roles, such as subject, verb, and object (Guadarrama et al., 2013; Thomason et al., 2014), as an intermediate representation.", "startOffset": 65, "endOffset": 113}, {"referenceID": 9, "context": "Deep NNs can learn powerful features (Donahue et al., 2013; Zeiler and Fergus, 2014), but require a lot of supervised training data.", "startOffset": 37, "endOffset": 84}, {"referenceID": 47, "context": "Deep NNs can learn powerful features (Donahue et al., 2013; Zeiler and Fergus, 2014), but require a lot of supervised training data.", "startOffset": 37, "endOffset": 84}, {"referenceID": 27, "context": "2M+ images with category labels (Krizhevsky et al., 2012).", "startOffset": 32, "endOffset": 57}, {"referenceID": 18, "context": "and sequence of words is modeled by a recurrent (temporally invariant) deep network pre-trained on 100K+ Flickr (Hodosh and Hockenmaier, 2014) and COCO (Lin et al.", "startOffset": 112, "endOffset": 142}, {"referenceID": 31, "context": "and sequence of words is modeled by a recurrent (temporally invariant) deep network pre-trained on 100K+ Flickr (Hodosh and Hockenmaier, 2014) and COCO (Lin et al., 2014) images with associated sen-", "startOffset": 152, "endOffset": 170}, {"referenceID": 9, "context": "Our approach is inspired by recent breakthroughs reported by several research groups in image-to-text generation, in particular, the work by Donahue et al. (2014). They applied a version of their model to video-to-text generation, but stopped short of proposing an end-to-end single network, using an intermediate role representation instead.", "startOffset": 141, "endOffset": 163}, {"referenceID": 16, "context": "Inspired by their approach, we utilize a Long-Short Term Memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997) to", "startOffset": 96, "endOffset": 130}, {"referenceID": 39, "context": "The LSTM model, which has recently achieved state-ofthe-art results on machine translation tasks (French and English (Sutskever et al., 2014)), effectively", "startOffset": 117, "endOffset": 141}, {"referenceID": 15, "context": "models the sequence generation task without requiring the use of fixed sentence templates as in previous work (Guadarrama et al., 2013).", "startOffset": 110, "endOffset": 135}, {"referenceID": 38, "context": "Finally, the deep convnet, the winner of the ILSVRC2012 (Russakovsky et al., 2014) image classification competition, provides a strong visual representation of objects, actions and scenes depicted in the video.", "startOffset": 56, "endOffset": 82}, {"referenceID": 4, "context": "\u2022 We provide a detailed evaluation of our model on the popular YouTube corpus (Chen and Dolan, 2011) and demonstrate a significant improvement over the state of the art.", "startOffset": 78, "endOffset": 100}, {"referenceID": 25, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 30, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 22, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 8, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 22, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 7, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 6, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 36, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 45, "context": "Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013).", "startOffset": 133, "endOffset": 338}, {"referenceID": 6, "context": ", 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al.", "startOffset": 30, "endOffset": 150}, {"referenceID": 6, "context": ", 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al. (2014) produce descriptions for videos of several people cooking in the same kitchen.", "startOffset": 30, "endOffset": 174}, {"referenceID": 1, "context": "Most work on \u201cin-the-wild\u201d online video has focused on retrieval and predicting event tags rather than generating descriptive sentences; examples are tagging YouTube (Aradhye et al., 2009) and retriev-", "startOffset": 166, "endOffset": 188}, {"referenceID": 34, "context": "ing online video in the TRECVID competition (Over et al., 2012).", "startOffset": 44, "endOffset": 63}, {"referenceID": 42, "context": ", (Wei et al., 2010; Huang et al., 2013).", "startOffset": 2, "endOffset": 40}, {"referenceID": 19, "context": ", (Wei et al., 2010; Huang et al., 2013).", "startOffset": 2, "endOffset": 40}, {"referenceID": 33, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 26, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 15, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 40, "context": "ploy (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate", "startOffset": 5, "endOffset": 108}, {"referenceID": 40, "context": "We compare our method to the best-performing method of Thomason et al. (2014). A recent paper by Xu et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 40, "context": "We compare our method to the best-performing method of Thomason et al. (2014). A recent paper by Xu et al. (2015) extracts deep features from video and a continuous vector from language, and projects both to a joint semantic space.", "startOffset": 55, "endOffset": 114}, {"referenceID": 0, "context": "Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al.", "startOffset": 128, "endOffset": 155}, {"referenceID": 0, "context": "Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al. (2010), Yao et al.", "startOffset": 128, "endOffset": 178}, {"referenceID": 0, "context": "Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al. (2010), Yao et al. (2010), and", "startOffset": 128, "endOffset": 197}, {"referenceID": 32, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 21, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 12, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 23, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 41, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 29, "context": "Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014).", "startOffset": 111, "endOffset": 260}, {"referenceID": 24, "context": "In contrast to traditional statistical MT (Koehn, 2010), RNNs naturally combine with vector-based representations, such as those for images and video.", "startOffset": 42, "endOffset": 55}, {"referenceID": 9, "context": "Donahue et al. (2014) and Vinyals et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Donahue et al. (2014) and Vinyals et al. (2014) simultaneously proposed a multimodal", "startOffset": 0, "endOffset": 48}, {"referenceID": 9, "context": "Our approach to video to text generation is inspired by the work of Donahue et al. (2014), who also applied a variant of their model to video-to-text", "startOffset": 68, "endOffset": 90}, {"referenceID": 9, "context": "While Donahue et al. (2014) only showed results on a narrow domain of cooking videos with a small set of pre-defined objects and actors, we generate sentences for opendomain YouTube videos with a vocabulary of thousands of words.", "startOffset": 6, "endOffset": 28}, {"referenceID": 9, "context": "Our framework is based on deep image description models in Donahue et al. (2014);Vinyals m ea n p o o lin g Input Video Convolutional Net Recurrent Net Output", "startOffset": 59, "endOffset": 81}, {"referenceID": 14, "context": "In our work we use the highly successful Long Short-Term Memory (LSTM) net as the sequence model, since it has shown superior performance on tasks such as speech recognition (Graves and Jaitly, 2014), machine translation (Sutskever et al.", "startOffset": 174, "endOffset": 199}, {"referenceID": 39, "context": "In our work we use the highly successful Long Short-Term Memory (LSTM) net as the sequence model, since it has shown superior performance on tasks such as speech recognition (Graves and Jaitly, 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014) and the more related task of generating sentence descriptions of images (Donahue et al.", "startOffset": 221, "endOffset": 263}, {"referenceID": 5, "context": "In our work we use the highly successful Long Short-Term Memory (LSTM) net as the sequence model, since it has shown superior performance on tasks such as speech recognition (Graves and Jaitly, 2014), machine translation (Sutskever et al., 2014; Cho et al., 2014) and the more related task of generating sentence descriptions of images (Donahue et al.", "startOffset": 221, "endOffset": 263}, {"referenceID": 41, "context": ", 2014) and the more related task of generating sentence descriptions of images (Donahue et al., 2014; Vinyals et al., 2014).", "startOffset": 80, "endOffset": 124}, {"referenceID": 39, "context": "RNNs can learn to map sequences for which the alignment between the inputs and outputs is known ahead of time (Sutskever et al., 2014) however it\u2019s unclear if they can be applied to problems where the inputs (xi) and outputs (zi) are of varying lengths.", "startOffset": 110, "endOffset": 134}, {"referenceID": 17, "context": "range dependencies (Hochreiter et al., 2001).", "startOffset": 19, "endOffset": 44}, {"referenceID": 16, "context": "However, LSTMs (Hochreiter and Schmidhuber, 1997), xt", "startOffset": 15, "endOffset": 49}, {"referenceID": 45, "context": "In our work we use the LSTM unit in Figure 3, described in Zaremba and Sutskever (2014), and Donahue et al.", "startOffset": 59, "endOffset": 88}, {"referenceID": 9, "context": "In our work we use the LSTM unit in Figure 3, described in Zaremba and Sutskever (2014), and Donahue et al. (2014). At the core of the LSTM model is a memory cell c", "startOffset": 93, "endOffset": 115}, {"referenceID": 20, "context": "For this we use a CNN, specifically the publicly available Caffe (Jia et al., 2014) reference model, a minor variant of AlexNet (Krizhevsky et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 27, "context": ", 2014) reference model, a minor variant of AlexNet (Krizhevsky et al., 2012).", "startOffset": 52, "endOffset": 77}, {"referenceID": 9, "context": "We use a two layer LSTM model for generating descriptions for videos based on experiments by Donahue et al. (2014) which suggest two LSTM layers are better than four and a single layer for image to text tasks.", "startOffset": 93, "endOffset": 115}, {"referenceID": 4, "context": "We perform all our experiments on the Microsoft Research Video Description Corpus (Chen and Dolan, 2011).", "startOffset": 82, "endOffset": 104}, {"referenceID": 33, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 26, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 15, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 40, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 43, "context": "used in several prior works (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015) on action recognition and video description tasks.", "startOffset": 28, "endOffset": 148}, {"referenceID": 15, "context": "For our task we pick 1200 videos to be used as training data, 100 videos for validation and 670 videos for testing, as used by the prior works on video description (Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015).", "startOffset": 164, "endOffset": 229}, {"referenceID": 40, "context": "For our task we pick 1200 videos to be used as training data, 100 videos for validation and 670 videos for testing, as used by the prior works on video description (Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015).", "startOffset": 164, "endOffset": 229}, {"referenceID": 43, "context": "For our task we pick 1200 videos to be used as training data, 100 videos for validation and 670 videos for testing, as used by the prior works on video description (Guadarrama et al., 2013; Thomason et al., 2014; Xu et al., 2015).", "startOffset": 164, "endOffset": 229}, {"referenceID": 39, "context": "lation (Sutskever et al., 2014) (12M sentences), we use data from the Flickr30k and COCO2014 datasets for training and learn to adapt to the video dataset by fine-tuning the image description models.", "startOffset": 7, "endOffset": 31}, {"referenceID": 18, "context": "The Flickr30k (Hodosh and Hockenmaier, 2014) dataset", "startOffset": 14, "endOffset": 44}, {"referenceID": 31, "context": "In addition to this, we use the recent COCO2014 (Lin et al., 2014) image description dataset consisting of", "startOffset": 48, "endOffset": 66}, {"referenceID": 40, "context": "HVC This is the Highest Vision Confidence model described in (Thomason et al., 2014).", "startOffset": 61, "endOffset": 84}, {"referenceID": 40, "context": "FGM (Thomason et al., 2014) also propose a factor graph model (FGM) that combines knowledge mined from text corpora with visual confidences from the HVC model using a factor graph and performs probabilistic inference to determine the most likely subject, verb, object and scene tuple.", "startOffset": 4, "endOffset": 27}, {"referenceID": 18, "context": "LSTMYTflickr is the model trained on the Flickr30k (Hodosh and Hockenmaier, 2014) dataset, and fine tuned on the YouTube dataset as descibed in Section 3.", "startOffset": 51, "endOffset": 81}, {"referenceID": 31, "context": "LSTM-YTcoco is first trained on the COCO2014 (Lin et al., 2014) dataset and then fine-tuned on the video dataset.", "startOffset": 45, "endOffset": 63}, {"referenceID": 26, "context": "Earlier works (Krishnamoorthy et al., 2013; Guadarrama et al., 2013) that reported results on the YouTube dataset compared their method", "startOffset": 14, "endOffset": 68}, {"referenceID": 15, "context": "Earlier works (Krishnamoorthy et al., 2013; Guadarrama et al., 2013) that reported results on the YouTube dataset compared their method", "startOffset": 14, "endOffset": 68}, {"referenceID": 43, "context": "The latter evaluation was also reported by (Xu et al., 2015), so we include it here for comparison.", "startOffset": 43, "endOffset": 60}, {"referenceID": 35, "context": "To evaluate the generated sentences we use the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores against all ground truth sentences.", "startOffset": 52, "endOffset": 75}, {"referenceID": 2, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) scores against all ground truth sentences.", "startOffset": 19, "endOffset": 45}, {"referenceID": 11, "context": "BLEU is the metric that is seen more commonly in image description literature, but a more recent study (Elliott and Keller, 2014) has shown METEOR to be a better evaluation metric.", "startOffset": 103, "endOffset": 129}, {"referenceID": 40, "context": "HVC (Thomason et al., 2014) 86.", "startOffset": 4, "endOffset": 27}, {"referenceID": 40, "context": "09 FGM (Thomason et al., 2014) 88.", "startOffset": 7, "endOffset": 30}, {"referenceID": 40, "context": "HVC (Thomason et al., 2014) 76.", "startOffset": 4, "endOffset": 27}, {"referenceID": 40, "context": "94 FGM (Thomason et al., 2014) 76.", "startOffset": 7, "endOffset": 30}, {"referenceID": 43, "context": "39 JointEmbed1(Xu et al., 2015) 78.", "startOffset": 14, "endOffset": 31}, {"referenceID": 40, "context": "FGM (Thomason et al., 2014) 13.", "startOffset": 4, "endOffset": 27}, {"referenceID": 40, "context": "FGM (Thomason et al., 2014) 2.", "startOffset": 4, "endOffset": 27}, {"referenceID": 41, "context": "Adding dropout as in (Vinyals et al., 2014) is likely to help prevent overfitting and improve performance.", "startOffset": 21, "endOffset": 43}, {"referenceID": 40, "context": "Additionally, when we ask Turkers to rate only the sentences (they are not provided the video) on grammatical correctness, the template based FGM (Thomason et al., 2014) achieves the highest ratings.", "startOffset": 146, "endOffset": 169}], "year": 2015, "abstractText": "Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.", "creator": "TeX"}}}