{"id": "1412.5836", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Incorporating Both Distributional and Relational Semantics in Word Representations", "abstract": "we investigate the hypothesis that stretching word representations ought to incorporate almost both distributional and relational semantics. to explore this correct end, suppose we employ the same alternating direction repetition method of neural multipliers ( rv admm ), as which flexibly optimizes a distributional behavioral objective base on raw text imagery and a relational objective norm on traditional wordnet. preliminary results on knowledge chain base analysis completion, analogy tests, and semantics parsing evaluation show that expanding word linguistic representations if trained on encompass both internal objectives can give improvements independently in some cases.", "histories": [["v1", "Thu, 18 Dec 2014 12:30:55 GMT  (171kb,D)", "http://arxiv.org/abs/1412.5836v1", "Under review as a workshop contribution at ICLR2015 (short version of the paper). Long version at:arXiv:1412.4369"], ["v2", "Tue, 17 Feb 2015 17:18:34 GMT  (82kb,D)", "http://arxiv.org/abs/1412.5836v2", "Under review as a workshop contribution at ICLR2015 (short version of the paper). Long version at:arXiv:1412.4369"], ["v3", "Sat, 21 Mar 2015 13:27:28 GMT  (82kb,D)", "http://arxiv.org/abs/1412.5836v3", "Accepted as a workshop contribution at ICLR2015. Long version at:arXiv:1412.4369"]], "COMMENTS": "Under review as a workshop contribution at ICLR2015 (short version of the paper). Long version at:arXiv:1412.4369", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel fried", "kevin duh"], "accepted": true, "id": "1412.5836"}, "pdf": {"name": "1412.5836.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Daniel Fried"], "emails": ["dfried@email.arizona.edu", "kevinduh@is.naist.jp"], "sections": [{"heading": "1 INTRODUCTION", "text": "We are interested in algorithms for learning vector representations of words. Recent work has shown that such representations, also known as word embeddings, can successfully capture the semantic and syntactic regularities of words (Mikolov et al., 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al., 2013a), and semantic role labeling (Collobert et al., 2011).\nAlthough many kinds of representation learning algorithms have been proposed so far, they are all essentially based on the same premise of distributional semantics (Harris, 1954), embodied by J. R. Firth\u2019s dictum: \u201cYou shall know a word by the company it keeps.\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word. Intuitively, these algorithms learn to map words with similar context to nearby points in vector space.\nHowever, distributional semantics is by no means the only theory of word meaning. Relational semantics, exemplified by WordNet (Miller, 1995), defines a word by its relation with other words. Relations such as synonymy, hypernymy, and meronymy (Cruse, 1986) create a graph that links words in terms of our world knowledge and psychological predispositions. For example, stating a relation like \u201cdog is-a mammal\u201d gives a precise hierarchy between the two words, in a way that is very different from the distributional similarities observable from corpora. Arguably, the vector representation of \u201cdog\u201d ought be close to that of \u201cmammal\u201d, regardless of their distributional contexts.\nWe believe both distributional and relational semantics are valuable for word representations. Our goal is to explore how to combine these complementary approaches into a unified learning algorithm. We employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011). Its advantages include (a) flexibility in incorporating arbitrary objectives, and (b) relative ease of implementation. We show that ADMM effectively optimizes the joint objective and present preliminary results on several tasks. \u2217Currently at the University of Cambridge.\nar X\niv :1\n41 2.\n58 36\nv1 [\ncs .C\nL ]\n1 8"}, {"heading": "2 DISTRIBUTIONAL AND RELATIONAL OBJECTIVES", "text": "Distributional Semantics Objective: We implement distributional semantics using the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi \u2208 Rd, the word\u2019s embedding. An n-length sequence of words (i1, i2, . . . , in) is represented as a vector x by concatenating the vector embeddings for each word, x = [wi1 ;wi2 . . . ;win ]. This vector x is then scored by feeding it through a two-layer neural network with h hidden nodes: SNLM (x) = u>(f(Ax + b)), where A \u2208 Rh\u00d7(nd), b \u2208 Rh, u \u2208 Rh are network parameters and f is the sigmoid f(t) = 1/(1 + e\u2212t) applied element-wise. The model is trained using noise contrastive estimation (NCE) (Mnih & Kavukcuoglu, 2013), where training text is corrupted by random replacement of random words to provide an implicit negative training example, xc. The loss function, optimized via stochastic gradient descent (SGD), is:\nLNLM (x,xc) = max(0, 1\u2212 SNLM (x) + SNLM (xc)) (1)\nRelational Semantics Objective: We investigate three different objectives. The Graph Distance loss, LGD, is based on the idea that words close together in WordNet should have similar embeddings. First, for a word pair (i, j), we define WordSim(i, j) based on the shortest path between word synsets (Leacock & Chodorow, 1998). Then, we encourage the cosine similarity between their embeddings vi and vj to match that of WordSim(i, j):\nLGD(i, j) =\n( vi \u00b7 vj\n||vi||2||vj ||2 \u2212 [a\u00d7WordSim(i, j) + b]\n)2 (2)\nwhere a and b are parameters (learned during training) that scale WordSim(i, j) to be of the same range as the cosine.\nA different approach directly models WordNet relations as operations in vector space. These models score an input tuple (vl, R, vr) indicating a relation of type R between words vl and vr. The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr \u2208 Rd should be close after translating vl by a relation vector R \u2208 Rd: STransE(vl, R, vr) = \u2212||vl +R\u2212 vr||2. Socher et al. (2013b) introduce a Neural Tensor Network (NTN) that models interaction between embeddings using tensors. The\nscoring function for a relation R is: SNTN (vl, R, vr) = U>f ( v>l WRvr +VR [ vl vr ] + bR ) , where U \u2208 Rh, WR \u2208 Rd\u00d7d\u00d7h, VR \u2208 Rh\u00d72d and bR \u2208 Rk are parameters for relationship R. As in the NLM, parameters for these relational models are trained using NCE and SGD, using the hinge loss as defined in Eq. 1, with SNLM replaced by the STransE or SNTN scoring function.\nJoint Objective Optimization by ADMM: We now describe an ADMM formulation for joint optimization of the above objectives. Let w be the set of word embeddings {w1,w2, . . .wN \u2032} for the distributional objective, and v be the set of word embeddings {v1,v2, . . .vN \u2032\u2032} for the relational objective, where N \u2032 and N \u2032\u2032 are the vocabulary size of the corpus and WordNet, respectively. Let I be the set of N words that occur in both. Then we define a set of vectors y = {y1,y2, . . .yN}, which correspond to Lagrange multipliers, to penalize the difference (wi \u2212 vi) between sets of embeddings for each word i in the joint vocabulary I:\nLP (w,v) = \u2211 i\u2208I ( y>i (wi \u2212 vi) ) + \u03c1 2 (\u2211 i\u2208I ||wi \u2212 vi||22 ) (3)\nIn the first term, y has same dimensionality as w and v, so a scalar penalty is maintained for each entry in every embedding vector. The second residual penalty term with hyperparameter \u03c1 is added to avoid saddle points; \u03c1 can be viewed as a step-size during the update of y. Finally, this augmented Lagrangian term (Eq. 3) is added to the sum of the loss terms for each objective (Eq. 1 and Eq. 2). Let \u03b8 = (u,A,b) be the parameters of the distributional objective, and \u03c6 be the parameters of the relational objective. The final loss function we optimize becomes:\nL = LNLM (w, \u03b8) + LGD(v, \u03c6) + LP (w,v) (4)\nThe ADMM algorithm proceeds by repeating the following three steps until convergence: (1) minimizew,\u03b8LNLM + LP ; (2) minimizev,\u03c6LGD + LP ; (3) For all embeddings i in joint vocabulary I , update the constraint vector: yi := yi + \u03c1(wi \u2212 vi). Since LNLM and LGD share no parameters, Steps (1) and (2) can be optimized easily using the same NCE and SGD procedures as the single objective case, with additional regularization term \u03c1 (wi \u2212 vi)."}, {"heading": "3 PRELIMINARY EXPERIMENTS & DISCUSSIONS", "text": "The distributional objectiveLNLM is trained using 5-grams from the Google Books English corpus1, containing over 180 million 5-gram types. For training LGD, we sample 100k words and compute similarity to 5 other words per word in each ADMM iteration. For training LTransE and LNTN , we use the dataset of Socher et al. (2013b).\nWe first provide an analysis of the behavior of ADMM on the training set, to confirm that it effectively optimizes the joint objective. Fig. 1(left) plots the learning curve by training iteration for various values of the \u03c1 hyperparameter. We see that ADMM attains a reasonable objective value relatively quickly in 100 iterations. Fig. 1(right) shows the averaged difference between the resulting sets of embeddings w and v, which decreases as desired.2\nNext, we compare the embeddings learned with different objectives on three standard benchmark tasks (Table 1). The Knowledge Base Completion task (Socher et al., 2013b) evaluates the models\u2019 ability to classify relationship triplets from WordNet as correct or incorrect. The SemEval2012 Analogy Test is a relational word similarity task similar to SAT-style analogy questions (Jurgens et al., 2012). Given a set of word pairs, the model needs to select the pairs that most and least represent a particular relation. The Dependency Parsing task is based on the SANCL2012 data (Petrov & McDonald, 2012) and evaluates the accuracy of parsers trained on news domain adapted for web domain. We incorporate the embeddings as additional features in a standard MST parser to see whether embeddings improve generalization of out-of-domain words.\nFor both Knowledge Base and Parsing tasks, we observe that joint objective generally improves over single objectives: e.g. TransE+NLM (83.10%) > TransE (82.87%) for Knowledge Base, GD+NLM (76.18%) > GD (75.90%) for Parsing. The improvements are not large, but relatively consistent. For the Analogy Test, joint objectives did not improve over the single objective NLM baseline. We provide further analysis as well as extended descriptions of methods and experiments in a longer version of the paper here: http://arxiv.org/abs/1412.4369.\n1Berkeley distribution: tomato.banatao.berkeley.edu:8080/berkeleylm_binaries/ 2The reason for the peak around iteration 50 in Fig. 1 is that the embeddings begin with similar random initializations, so initially differences are small; as ADMM starts to see more data, w and v diverge, but converge eventually as y become large."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by a Microsoft Research CORE Grant and JSPS KAKENHI Grant Number 26730121. D.F. was supported by the Flinn Scholarship during the course of this work. We thank Haixun Wang, Jun\u2019ichi Tsujii, Tim Baldwin, Yuji Matsumoto, and several anonymous reviewers for helpful discussions at various stages of the project."}], "references": [{"title": "A neural probabilistic language models", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Lexical Semantics", "author": ["Cruse", "Alan D"], "venue": null, "citeRegEx": "Cruse and D.,? \\Q1986\\E", "shortCiteRegEx": "Cruse and D.", "year": 1986}, {"title": "Semeval-2012 task 2: Measuring degrees of relational similarity", "author": ["Jurgens", "David A", "Turney", "Peter D", "Mohammad", "Saif M", "Holyoak", "Keith J"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "Jurgens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["Leacock", "Claudia", "Chodorow", "Martin"], "venue": "WordNet: An Electronic Lexical Database,", "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tom\u00e1s", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["Mnih", "Andriy", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Continuous space language models", "author": ["Schwenk", "Holger"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk and Holger.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk and Holger.", "year": 2007}, {"title": "Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Socher", "Richard", "Bauer", "John", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: A simple and general method for semi-supervise learning", "author": ["Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Wang", "Mengqiu", "Manning", "Christopher D"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": ", 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al.", "startOffset": 118, "endOffset": 161}, {"referenceID": 3, "context": ", 2013a), and semantic role labeling (Collobert et al., 2011).", "startOffset": 37, "endOffset": 61}, {"referenceID": 0, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 3, "context": "\u201d For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.", "startOffset": 29, "endOffset": 138}, {"referenceID": 2, "context": "We employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011).", "startOffset": 118, "endOffset": 137}, {"referenceID": 3, "context": "Distributional Semantics Objective: We implement distributional semantics using the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi \u2208 R, the word\u2019s embedding.", "startOffset": 115, "endOffset": 139}, {"referenceID": 1, "context": "The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr \u2208 R should be close after translating vl by a relation vector R \u2208 R: STransE(vl, R, vr) = \u2212||vl +R\u2212 vr||2.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": "The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr \u2208 R should be close after translating vl by a relation vector R \u2208 R: STransE(vl, R, vr) = \u2212||vl +R\u2212 vr||2. Socher et al. (2013b) introduce a Neural Tensor Network (NTN) that models interaction between embeddings using tensors.", "startOffset": 20, "endOffset": 289}, {"referenceID": 5, "context": "The SemEval2012 Analogy Test is a relational word similarity task similar to SAT-style analogy questions (Jurgens et al., 2012).", "startOffset": 105, "endOffset": 127}, {"referenceID": 12, "context": "For training LTransE and LNTN , we use the dataset of Socher et al. (2013b). We first provide an analysis of the behavior of ADMM on the training set, to confirm that it effectively optimizes the joint objective.", "startOffset": 54, "endOffset": 76}], "year": 2017, "abstractText": "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.", "creator": "LaTeX with hyperref package"}}}