{"id": "1412.6277", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "N-gram-Based Low-Dimensional Representation for Document Classification", "abstract": "whereas the standard bag - value of - words ( bow ) model modeling is the common approach adapted for classifying documents, where words are used briefly as statistical feature coding for training a database classifier. this algorithm generally involves utilizing a huge generalized number scaling of new features. some data techniques, such as latent semantic analysis ( vs lsa ) modelling or latent dirichlet allocation ( lda ), might have been designed to completely summarize irrelevant documents in a lower dimension with the least expected semantic internal information loss. some intrinsic semantic information is nevertheless always lost, primarily since only words concerned are normally considered. instead, we aim better at using information coming from conventional n - grams to overcome them this limitation, while remaining in a low - dimension space. fairly many approaches, such as the skip - gram vocabulary model, provide one good interactive word definition vector scheme representations very remarkably quickly. generally we consequently propose to vastly average update these representations altogether to obtain richer representations of standard n - grams. all n -'grams systems are so thus embedded further in generate a same semantic model space. generating a k - means clustering can then mechanically group further them into semantic concepts. the number bag of features is therefore remarkably dramatically theoretically reduced and documents also can be represented as simply bag subdivisions of semantic meaningful concepts. thereby we show importantly that in this model outperforms lsa constraints and updates lda on detecting a fuzzy sentiment classification computing task, presents and yields similar fuzzy results than a given traditional bow - model with far less logical features.", "histories": [["v1", "Fri, 19 Dec 2014 10:29:33 GMT  (170kb,D)", "https://arxiv.org/abs/1412.6277v1", "ICLR 2015 conference track"], ["v2", "Fri, 10 Apr 2015 13:53:40 GMT  (37kb)", "http://arxiv.org/abs/1412.6277v2", "Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "ICLR 2015 conference track", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\\'emi lebret", "ronan collobert"], "accepted": true, "id": "1412.6277"}, "pdf": {"name": "1412.6277.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["remi@lebret.ch", "ronan@collobert.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n62 77\nv2 [\ncs .C\nL ]\n1 0\nA pr"}, {"heading": "1 INTRODUCTION", "text": "Text document classification aims at assigning a text document to one or more classes. Successful methods are traditionally based on bag-of-words (BOW). Finding discriminative keywords is, in general, good enough for text classification. Given a dictionary of words D to consider, documents are represented by a |D|-dimensional vector (the bag of its words). Each dimension is either a binary value (present or not in the document) or a word occurrence frequency. Some term weightings (e.g. the popular td-idf) have also been defined to reflect how discriminative a word is for a document. These are considered as features for training a classifier. Naive Bayes (NB) and Support Vector Machine (SVM) models are often the first choices. One limitation of the bag-of-words model is that the discriminative words are usually not the most frequent ones. A large dictionary of words needs to be defined to obtain a robust model. Classifiers then have to deal with a huge number of features, and thus become time-consuming and memory-hungry.\nSome techniques have been proposed to reduce the dimensionality and represent documents in a low-dimensional semantic space. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) uses the term-document matrix and a singular value decomposition (SVD) to represent terms and documents in a new low-dimensional space. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative probabilistic model of a corpus. Each document is represented as a mixture of latent topics, where each topic is characterized by a distribution over words. By defining K topics,\n\u2217All research was conducted at the Idiap Research Institute, before Ronan Collobert joined Facebook AI Research.\ndocuments can then be represented as K-dimensional vectors. Pessiot et al. (2010) also proposed probabilistic models for unsupervised dimensionality reduction in the context of document clustering. They make the hypothesis that words occuring with the same frequencies in the same document are semantically related. Based on this assumption, words are partioned into word topics. Document are then represented by a vector where each feature corresponds to a word-topic representing the number of occurrences of words from that word-topic in the document. Other techniques have tried to improve text document clustering by taking into account relationships between important terms. Some have enriched document representations by integrating core ontologies as background knowledge (Staab & Hotho, 2003), or with Wikipedia concepts and category information (Hu et al., 2009). Part-of-speech tags have also been used to disambiguate words (Sedding & Kazakov, 2004).\nAll these techniques are based on words alone, which raises another limitation. A collection of words cannot capture phrases or multi-word expressions, while n-grams have shown to be helpful features in several natural language processing tasks (Tan et al., 2002; Lin & Wu, 2009; Wang & Manning, 2012). N -gram features are not commonly used in text classification, probably because the dictionary Dn tends to grow exponentially with n. Phrase structure extraction can be used to identify only n-grams which are phrase patterns, and thus limit the dictionary size. However, this adds another step to the model, making it more complex. To overcome these barriers, we propose that documents be represented as a bag of semantic concepts, where n-grams are considered instead of only words. Good word vector representations are obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Lebret & Collobert, 2014; Pennington et al., 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king - man + woman \u2248 queen. By leveraging the ability of these word vector representations to compose, representations for n-grams are easily computed with an element-wise addition. Using a clustering algorithm such as K-means, those representations are grouped into K clusters which can be viewed as semantic concepts. Text documents are now represented as bag of semantic concepts, with each feature corresponding to the presence or not of n-grams from the resulting clusters. Therefore, more information is captured while remaining in a low-dimensional space. As Mikolov et al\u2019s Skip-gram model and K-means are highly parallelizable, this model is much faster to compute than LSA or LDA. The same classifiers as with BOW-based models are then applied on these bag of semantic concepts. We show that such model is a good alternative to LSA or LDA to represent documents and yields even better results on movie review tasks."}, {"heading": "2 A BAG OF SEMANTIC CONCEPTS MODEL", "text": "The model is divided into three steps: (1) vector representations of n-grams are obtained by averaging pre-trained representations of its individual words; (2) n-grams are gouped into K semantic concepts by performing K-means clustering on all n-gram representations; (3) documents are represented by a bag of K semantic concepts, where each entry depends on the presence of n-grams from the concepts defined in the previous step."}, {"heading": "2.1 N -GRAM REPRESENTATION", "text": "The first step of the model is to generate continuous vector representations xw for each word w within the dictionary D. Leveraging recent models, such as the Skip-gram (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) models, they are trained over a large corpus of unlabeled data in an efficient manner. These models are indeed highly parallelizable, which helps to obtain these representations very quickly. Word representations are then summed to generate n-gram representations:\n1\nn\nn \u2211\ni=1\nxwi . (1)\nThese representations are vectors which keep the semantic information of n-grams with different n in the same dimensionality. Distances between them are thus computable. It allows the use of a K-means clustering for grouping all n-grams into K classes."}, {"heading": "2.2 K -MEANS CLUSTERING", "text": "K-means is an unsupervised learning algorithm commonly used to automatically partition a data set into K clusters. Considering a set of n-gram representations xi \u2208 Rm, the algorithm will determine a set of K centroids \u03b3k \u2208 Rm, so as to minimize the average distance from each representation to its nearest centroid:\n\u2211\ni\n||xi \u2212 \u03b3\u03c3i || 2 , where \u03c3i = argmin\nk\n||xi \u2212 \u03b3k|| 2 . (2)\nThe limitation due to the size of the dictionary is therefore overcomed. By setting K to a low value, documents can also be represented by more compact vectors than with a bag-of-words model, while keeping all the meaningful information."}, {"heading": "2.3 DOCUMENT REPRESENTATION", "text": "Denoting D = (d1, d2, . . . , dL) a set of text documents, where each document di contains a set of n-grams. First, each n-gram is embedded into a common vector space by averaging its word vector representations. The resulting n-grams representations are assigned to clusters using the centroids \u03b3k defined by the K-means clustering. Documents di are then represented by a vector of K features, fi \u2208 RK . Each entry fki usually corresponds to the frequency of n-grams from the k th cluster within the document di. The set of text documents is then defined as D\u0302 = {(fi, yi)| fi \u2208 R\nK , yi \u2208 {\u22121, 1}} L i=1.\nWith NB features. For certain type of document, such as movie reviews, the use of Naive Bayes features can improve the general performance (Wang & Manning, 2012). Success in sentiment analysis relies mostly on the capability of the models to detect negative and positive n-grams in a document. A proper normalization is then calculated to determine how important each n-gram is for a given class y. We denote ngm = (ngm1, . . . , ngmN) a set of count vectors for all n-grams contained in D, ngmi \u2208 RL. ngmit represents the number of occurence of the n-gram t in the training document di. Defining count vectors as p = 1 + \u2211\ni:yi=1 ngmi and q = 1 +\n\u2211\ni:yi=\u22121 ngmi, a\nlog-count ratio is calculated to determine how important n-grams are for the classes y:\nr = log\n(\np/||p||1 q/||q||1\n)\n, with r \u2208 RN . (3)\nBecause n-grams are in clusters, we extract the maximum absolute log-count ratio for every cluster:\nf\u0303ki = argmax rt |rt| , \u2200t \u2208 k, ngm i t > 0 (4)\nThese document representations can then be used for several NLP tasks such as classification or information retrieval. As for BOW-based models, this model is particulary suitable for linear SVM."}, {"heading": "3 EXPERIMENTS WITH SENTIMENT ANALYSIS", "text": "Sentiments can have a completely different meaning if n-grams are considered instead of words. A classifier might leverage a bigram such as \u201cnot good\u201d to classify a document as negative, while this would probably fail if only unigrams (words) were considered. We thus benchmark the bag of semantic concepts model on sentiment analysis."}, {"heading": "3.1 IMDB MOVIE REVIEWS DATASETS", "text": "Datasets from IMDB have the nice property of containing long documents. It is thus valuable to considerer n-grams in such a framework. We did experiments with small and large collections of reviews. We can thus analyse how well our model compares against classical models, for different dataset sizes."}, {"heading": "3.1.1 PANG & LEE (2004)", "text": "The collection consists of 1,000 positive and 1,000 negative processed reviews1. So a random guess yields 50% accuracy. The authors selected only reviews where rating was expressed either with stars or some numerical value. To avoid domination of the corpus by a small number of prolific reviewers, they imposed a limit of fewer than 20 reviews per author per sentiment category. As there is no test set, we used 10-fold cross-validation."}, {"heading": "3.1.2 MAAS ET AL. (2011)", "text": "The collection consists of 100,000 reviews2. It has been divided into three datasets: training and test sets (25,000 labeled reviews each), and 50,000 unlabeled training reviews. It allows no more than 30 reviews per movie. It contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. Only highly polarized reviews have been considered. A negative review has a score \u2264 4 out of 10, and a positive review has a score \u2265 7 out of 10."}, {"heading": "3.2 EXPERIMENTAL SETUP", "text": "We first learn word vector representations over a large corpus of unlabeled text. This step could however be skipped by taking existing pre-trained word representations3 instead of learning them from scratch. By following the three steps described in Section 2, movie reviews are then represented as bags of semantic concepts. These representations are finally used for training a linear SVM to classify sentiment."}, {"heading": "3.2.1 LEARNING WORD REPRESENTATION OVER LARGE CORPORA", "text": "Our English corpus is composed of the entire English Wikipedia4, the Reuters corpus and the Wall Street Journal (WSJ) corpus. We consider lower case words and replace digits with a special token. The resulting text is tokenized using the Stanford tokenizer. The final data set contains about 2 billion words. Our dictionary D consists of all the words appearing at least one hundred times. This results in a 202,255 words dictionary. We then train a Skip-gram model to get word representation in a 100-dimensional vector. This dimension is intentionally quite low to speed up the clustering afterwards. As other hyperparameters, we use a fixed learning rate of 0.01, a context size of 5 phrases, Negative Sampling with 5 negative samples for each positive sample, and a subsampling approach with a threshold of 10\u22125"}, {"heading": "3.2.2 BAG OF SEMANTIC CONCEPTS FOR MOVIE REVIEWS", "text": "Computing n-gram representations. We consider n-grams up to n = 3. Only n-grams with words from our dictionary are considered for both datasets.5 This results in a set of 34,360 1- gram representations, 419,918 2-gram representations, and 921,837 3-gram representations for the Pang and Lee\u2019s dataset. And 67,847 1-gram representations, 1,842,461 2-gram representations, and 5,724,871 3-gram representations for the Maas et al.\u2019s dataset. Because n-gram representations are computed by averaging representations of its word, all n-grams are also represented in a 100- dimensional vector.\nPartitioning n-grams into semantic concepts. Because n-grams are represented in a common vector space, similarities between n-grams of different length can be computed. To evaluate the benefit of adding n-grams for sentiment analysis, we define semantic concepts with different combinations of n-grams: (1) only 1-grams (i.e. clusters of words), (2) only 2-grams, (3) only 3-grams, (4) with 1-grams and 2-grams, and (5) with 1-grams, 2-grams and 3-grams. Each of these five sets\n1Available at http://www.cs.cornell.edu/people/pabo/movie-review-data/. 2Available at http://www.andrew-maas.net/data/sentiment. 3Different pre-trained word vector representations are available at https://code.google.com/p/word2vec/, http://stanford.edu/\u02dcjpennin/ or http://lebret.ch/words/.\n4We took the January 2014 version. 5Our English corpus is not large enough to cover all the words present in the IMDB datasets. We thus use\nthe same 1-gram dictionary with the other methods.\nof n-gram representations are then partitioned in K = {100, 200, 300} clusters with the K-means clustering. The centroids \u03b3k \u2208 R100 are obtained after 10 iterations of the algorithm.\nMovie review representations. Movie reviews are then represented as bags of semantic concepts with naive bayes features as described in Section 2.3. The log-count ratio for each n-gram is calculated on the training set for both datasets."}, {"heading": "3.2.3 COMPARISON WITH OTHER METHODS", "text": "We compare our models with two classical techniques for representing text documents in a lowdimensional vector space: LSA and LDA. Both methods use the same 1-gram dictionaries than with the bag of semantic concepts model with K = {100, 200, 300}. In the framework of Maas et al.\u2019s dataset, LSA and LDA benefit from the large set of unlabeled reviews.\nLatent Sentiment Analysis (LSA) (Deerwester et al., 1990). Let X \u2208 R|D|\u00d7L be a matrix where each element Xi,j describes the log count ratio of words i in document j, with L the number of training documents and D the dictionary of words (i.e. 34,360 for Pang and Lee\u2019s dataset, 67,847 for Maas et al\u2019s dataset). By applying truncated SVD to the log-count ratio matrix X , we thus obtain semantic representations in a K-dimensional space for movie reviews.\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003). We train the K-topics LDA model using the code released by Blei et al. (2003)6. We leave the LDA hyperparameters at their default values. Like our model, LDA extracts K topics (i.e. semantic concepts) and assigns words to these topics. Considering only the words in documents, we thus apply the method described in Section 2.3 to get document representations. A movie review di is then represented in a K-dimensional vector, where each feature f\u0303ki is the maximum absolute log-count ratio for the k th topic."}, {"heading": "3.2.4 CLASSIFICATION USING SVM", "text": "Having representations of movie reviews in a K-dimensional vector, a classifier is trained to determine whether a given review is positive or negative. Given the set of training documents D\u0303 = {(\u0303fi, yi)| f\u0303i \u2208 R K , yi \u2208 {\u22121, 1}} L i=1, we picked a linear SVM as a classifier, trained using the LIBLINEAR library (Fan et al., 2008):\nmin w\n1 2 wTw + C \u2211\ni\nmax(0, 1\u2212 yiw T f\u0303i) 2 , (5)\nwith w the weight vector, and C a penalty parameter."}, {"heading": "3.3 RESULTS", "text": "The overall results summarized in Table 1 show that the bag of semantic concepts approach outperforms the traditionnal LDA and LSA approaches to represent documents in a low-dimensional space. Good performance is achieved even with only 100 clusters, where LSA needs more clusters to improve. We also denote that our approach performs well on a small dataset, where LDA fails. A significant increase is observed when using 2-grams instead of 1-grams. However, using only 3-grams hurts the performance. The best results are obtained using a combination of n-grams, which confirms the benefit of the method. That also means that word vector representations can be combined while keeping relevant semantic information. This is illustrated in Table 3 where semantically close n-grams are in the same cluster. We can see that the model is furthermore able to clearly separate antonyms, which is a good asset for sentiment classification. The results are also very competitive with a traditional BOW-model. Using the same 1-gram dictionary and a linear SVM classifier with the naive bayes features, BOW-model achieves 83% accuracy for Pang and Lee\u2019s dataset, and 88.58% for Maas et al\u2019s dataset. Our model therefore performs better with about 344 times less features for the first dataset, and yields similar result with about 678 times less features for the second one.\n6Available at http://www.cs.princeton.edu/\u02dcblei/lda-c/."}, {"heading": "3.4 COMPUTATION TIME", "text": "The bag of semantic concepts model can leverage information coming from n-grams to improve sentiment classification of documents. This model has also the nice property to build document representations in an efficient and timely manner. The most time-consuming and costly process step in the model is the K-means clustering, especially when dealing with millions of n-gram representations. However, this step can be done very quickly with low memory by using mini-batch K-means method. Computation times for generating 300-dimensional representations are reported in Table 2. All experiments have been run on single CPU core Intel i7 2600K 3.4 GHz. Despite the fact that single CPU has been used for this benchmark, the three steps of the model are highly parallelizable. The recorded times could thus be divided by the number of CPUs available. We see that representations can be computed in less than one minute with only 1-gram dictionary. About 10 minutes are necessary when adding 2-grams, and about 40 minutes by adding 3-grams. In comparison, LDA needs six hours for extracting 100 topics and three days for 300 topics. Our model is also very competitive with LSA which takes 540 seconds to generate 300-dimensional document representations. However, adding 2-grams and 3-grams to perform a LSA would be extremely time-consuming and memory-hungry while our model can handle it.\n3.5 INFERRING SEMANTIC CONCEPTS FOR UNSEEN n-GRAMS\nAnother drawback of classical models is that they cannot deal with unseen words. Only words present in the training documents are used to infer representation for a new text document. Unlike these models, our model can easily assign semantic concepts for new n-grams. Because n-gram representations are based on its word vector representations, a new n-gram vector representation can be calculated if a representation is available for each of its words. This new representations is then assigned to the nearest centroid \u03b3k, which determines its semantic concept. With a small training set, this is a valuable asset when compared to other models."}, {"heading": "4 CONCLUSION", "text": "Word vector representations can be quickly obtained with recent techniques such as the Skip-gram model. N -grams with different length n can then be embedded in a same dimensional vector space with a simple element-wise addition. This makes it possible to compute distances between n-grams, which can have many applications in natural language processing. We therefore proposed a bag of semantic concepts model to represent documents in a low-dimensional space. These semantic concepts are obtained by performing a K-means clustering which partition all n-grams into K clusters. This model has several advantages over classical approaches for representing documents in a low-dimensional space: it leverages semantic information coming from n-grams; it builds document representations with low resource consumption (time and memory); it can infer semantic concepts for unseen n-grams. Furthermore, we have shown that such model is suitable for document classification. Competitive performance has been reached on binary sentiment classification tasks, where this model outperforms traditional approaches. It also attained similar results to traditional bag-ofwords with considerably less features."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the HASLER foundation through the grant \u201cInformation and Communication Technology for a Better World 2020\u201d (SmartWorld)."}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Exploiting Wikipedia As External Knowledge for Document Clustering", "author": ["X. Hu", "X. Zhang", "C. Lu", "E.K. Park", "X. Zhou"], "venue": "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Word Embeddings through Hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the EACL,", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Phrase clustering for discriminative learning", "author": ["D. Lin", "X. Wu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lin and Wu,? \\Q2009\\E", "shortCiteRegEx": "Lin and Wu", "year": 2009}, {"title": "Learning Word Vectors for Sentiment Analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of ACL,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshp,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In NIPS", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity", "author": ["B. Pang", "L. Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang and Lee,? \\Q2004\\E", "shortCiteRegEx": "Pang and Lee", "year": 2004}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Improving Document Clustering in a Learned Concept Space", "author": ["Pessiot", "J.-F", "Kim", "Y.-M", "Amini", "M.-R", "P. Gallinari"], "venue": "Information Processing & Management,", "citeRegEx": "Pessiot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pessiot et al\\.", "year": 2010}, {"title": "WordNet-based Text Document Clustering", "author": ["J. Sedding", "D. Kazakov"], "venue": "In Proceedings of the 3rd Workshop on RObust Methods in Analysis of Natural Language Data,", "citeRegEx": "Sedding and Kazakov,? \\Q2004\\E", "shortCiteRegEx": "Sedding and Kazakov", "year": 2004}, {"title": "Ontology-based Text Document Clustering", "author": ["S. Staab", "A. Hotho"], "venue": "In Intelligent Information Processing and Web Mining, Proceedings of the International IIS: IIPWM\u201903 Conference held in Zakopane,", "citeRegEx": "Staab and Hotho,? \\Q2003\\E", "shortCiteRegEx": "Staab and Hotho", "year": 2003}, {"title": "The use of bigrams to enhance text categorization", "author": ["C. Tan", "Y. Wang", "C. Lee"], "venue": "Journal of Information Processing and Management,", "citeRegEx": "Tan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2002}, {"title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "author": ["S.I. Wang", "C.D. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Wang and Manning,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Latent Semantic Analysis (LSA) (Deerwester et al., 1990) uses the term-document matrix and a singular value decomposition (SVD) to represent terms and documents in a new low-dimensional space.", "startOffset": 31, "endOffset": 56}, {"referenceID": 0, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative probabilistic model of a corpus.", "startOffset": 34, "endOffset": 53}, {"referenceID": 3, "context": "Some have enriched document representations by integrating core ontologies as background knowledge (Staab & Hotho, 2003), or with Wikipedia concepts and category information (Hu et al., 2009).", "startOffset": 174, "endOffset": 191}, {"referenceID": 15, "context": "A collection of words cannot capture phrases or multi-word expressions, while n-grams have shown to be helpful features in several natural language processing tasks (Tan et al., 2002; Lin & Wu, 2009; Wang & Manning, 2012).", "startOffset": 165, "endOffset": 221}, {"referenceID": 11, "context": "Good word vector representations are obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Lebret & Collobert, 2014; Pennington et al., 2014).", "startOffset": 97, "endOffset": 197}, {"referenceID": 8, "context": "Pessiot et al. (2010) also proposed probabilistic models for unsupervised dimensionality reduction in the context of document clustering.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Some have enriched document representations by integrating core ontologies as background knowledge (Staab & Hotho, 2003), or with Wikipedia concepts and category information (Hu et al., 2009). Part-of-speech tags have also been used to disambiguate words (Sedding & Kazakov, 2004). All these techniques are based on words alone, which raises another limitation. A collection of words cannot capture phrases or multi-word expressions, while n-grams have shown to be helpful features in several natural language processing tasks (Tan et al., 2002; Lin & Wu, 2009; Wang & Manning, 2012). N -gram features are not commonly used in text classification, probably because the dictionary D tends to grow exponentially with n. Phrase structure extraction can be used to identify only n-grams which are phrase patterns, and thus limit the dictionary size. However, this adds another step to the model, making it more complex. To overcome these barriers, we propose that documents be represented as a bag of semantic concepts, where n-grams are considered instead of only words. Good word vector representations are obtained very quickly with many different recent approaches (Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013; Lebret & Collobert, 2014; Pennington et al., 2014). Mikolov et al. (2013a) also showed that simple vector addition can often produce meaningful results, such as king - man + woman \u2248 queen.", "startOffset": 175, "endOffset": 1290}, {"referenceID": 11, "context": ", 2013b) or GloVe (Pennington et al., 2014) models, they are trained over a large corpus of unlabeled data in an efficient manner.", "startOffset": 18, "endOffset": 43}, {"referenceID": 1, "context": "Latent Sentiment Analysis (LSA) (Deerwester et al., 1990).", "startOffset": 32, "endOffset": 57}, {"referenceID": 0, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003).", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We train the K-topics LDA model using the code released by Blei et al. (2003)6.", "startOffset": 35, "endOffset": 133}, {"referenceID": 2, "context": "Given the set of training documents D\u0303 = {(\u0303fi, yi)| f\u0303i \u2208 R K , yi \u2208 {\u22121, 1}} L i=1, we picked a linear SVM as a classifier, trained using the LIBLINEAR library (Fan et al., 2008):", "startOffset": 162, "endOffset": 180}], "year": 2015, "abstractText": "The bag-of-words (BOW) model is the common approach for classifying documents, where words are used as feature for training a classifier. This generally involves a huge number of features. Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize documents in a lower dimension with the least semantic information loss. Some semantic information is nevertheless always lost, since only words are considered. Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space. Many approaches, such as the Skip-gram model, provide good word vector representations very quickly. We propose to average these representations to obtain representations of n-grams. All n-grams are thus embedded in a same semantic space. A K-means clustering can then group them into semantic concepts. The number of features is therefore dramatically reduced and documents are then represented as bag of semantic concepts. We show that this model outperforms LSA and LDA on a sentiment classification task, and yields similar results than a traditional BOW-model with far less features.", "creator": "LaTeX with hyperref package"}}}