{"id": "1412.6596", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "abstract": "current state - of - the - art deep learning systems especially for visual object types recognition and detection use purely supervised process training parameters with regularization such as detection dropout to avoid repeated overfitting. hence the improvement performance depends critically on the amount recall of newly labeled graphic examples, knowing and otherwise in current practice the labels displayed are assumed to well be approximately unambiguous and appropriately accurate. however, this assumption now often does not hold ; e. g. objects in recognition, several class defined labels may yet be individually missing ; in detection, objects in the image may not be localized ; definitions and artifacts in general, acknowledging the labeling may be subjective. in this new work we propose finding a generic way to handle overly noisy inference and incomplete labeling by augmenting either the prediction objective logic with utilizing a notion measurement of consistent consistency. we may consider a prediction consistent if determining the given same prediction expression is made only given similar observed percepts, where the notion value of stimulus similarity is between distinctly deep network features computed strongly from the biased input target data. in initial experiments we demonstrate that our approach yields substantial robustness to label noise noise on several datasets. on evaluating mnist handwritten labeled digits, imaging we immediately show it that nowadays our model enhancement is robust then to label network corruption. speaking on completing the toronto face profile database, we show illustrates that while our model handles well the complicated case of subjective stimuli labels realized in emotion type recognition, achieving state - of - the - art results, efficacy and what can apparently also additionally benefit economically from improving unlabeled face images delivered with or no signal modification to our experimental method. on the ilsvrc2014 detection factor challenge data, interviews we show highlights that currently our approach extends to modelling very deep networks, high resolution feature images and clearly structured outputs, and mixed results shown in improved verbal scalable detection.", "histories": [["v1", "Sat, 20 Dec 2014 04:11:33 GMT  (432kb,D)", "https://arxiv.org/abs/1412.6596v1", null], ["v2", "Sat, 7 Feb 2015 22:30:39 GMT  (432kb,D)", "http://arxiv.org/abs/1412.6596v2", null], ["v3", "Wed, 15 Apr 2015 19:48:37 GMT  (432kb,D)", "http://arxiv.org/abs/1412.6596v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["scott reed", "honglak lee", "dragomir anguelov", "christian szegedy", "dumitru erhan", "rew rabinovich"], "accepted": true, "id": "1412.6596"}, "pdf": {"name": "1412.6596.pdf", "metadata": {"source": "CRF", "title": "TRAINING DEEP NEURAL NETWORKS ON NOISY LABELS WITH BOOTSTRAPPING", "authors": ["Scott E. Reed", "Honglak Lee"], "emails": ["reedscot@umich.edu", "honglak@umich.edu", "dragomir@google.com", "szegedy@google.com", "dumitru@google.com", "amrabino@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al., 2012) to avoid overfitting. These systems do not account for missing labels, subjective labeling or inexhaustivelyannotated images. However, this assumption often does not hold, especially for very large datasets and in high-resolution images with complex scenes. For example, in recognition, the class labels may be missing; in detection, the objects in the image may not all be localized; in subjective tasks such as facial emotion recognition, humans may not even agree on the class label. As training sets for deep networks become larger (as they should), the problem of missing and noisy labels becomes more acute, and so we argue it is a fundamental problem for scaling up vision.\nIn this work we propose a simple approach to hande noisy and incomplete labeling in weaklysupervised deep learning, by augmenting the usual prediction objective with a notion of perceptual consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity incorporates features learned by the deep network.\nOne interpretation of the perceptual consistency objective is that the learner makes use of its representation of the world (implicit in the network parameters) to match incoming percepts to known\nar X\niv :1\n41 2.\n65 96\nv3 [\ncs .C\nV ]\n1 5\nA pr\ncategories, or in general structured outputs. This provides the learner justification to \u201cdisagree\u201d with a perceptually-inconsistent training label, and effectively re-label the data while training. More accurate labels may lead to a better model, which allows further label clean-up, and the learner bootstraps itself in this way. Of course, too much skepticism of the labels carries the risk of ending up with a delusional agent, so it is important to balance the trade-off between prediction and the learner\u2019s perceptual consistency.\nIn our experiments we demonstrate that our approach yields substantial robustness to several types of label noise on several datasets. On MNIST handwritten digits (LeCun & Cortes, 1998) we show that our model is robust to label corruption. On the Toronto Face Database (Susskind et al., 2010) we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the-art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data (Russakovsky et al., 2014), we show that our approach improves single-shot person detection using a MultiBox network (Erhan et al., 2014), and also improves performance in full 200-way detection using MultiBox for region proposal and a deep CNN for post-classification.\nIn section 2 we discuss related work, in section 3 we describe our method along with a probabilistic interpretation and in section 4 we present our results."}, {"heading": "2 RELATED WORK", "text": "The literature on semi-supervised and weakly-supervised learning is vast (see Zhu (2005) for a survey), and so in this section we focus on the key previous papers that inspired this work and on other papers on weakly- and semi-supervised deep learning.\nThe notion of bootstrapping, or \u201cself-training\u201d a learning agent was proposed in (Yarowsky, 1995) as a way to do word-sense disambiguation with only unlabeled examples and a small list of seed example sentences with labels. The algorithm proceeds by building an initial classifier using the seed examples, and then iteratively classifying unlabeled examples, extracting new seed rules for the classifier using the now expanded training data, and repeating these steps until convergence. The algorithm was analyzed by Abney (Abney, 2004) and more recently by (Haffari & Sarkar, 2012).\nCo-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarlymotivated but used a pair of classifiers with separate views of the data to iteratively learn and generate additional training labels. Whitney & Sarkar (2012) proposed bootstrapping labeled training examples with graph-based label propagation. Brodley & Friedl (1999) developed statistical methods for identifying mislabeled training data.\nRosenberg et al. (2005) also trained an object detection system in a weakly-supervised manner using self-training, and demonstrated that their proposed model achieved comparable performance to models trained with a much larger set of labels. However, that approach works as a wrapper around an existing detection system, whereas in this work we integrate a consistency objective for bootstrapping into the training of the deep network itself.\nOur work shares a similar motivation to these earlier works, but instead of explicitly generating new training labels and adding new examples to the training set in an outer loop, we incorporate our consistency objective directly into the model. In addition, we consider not only the case of learning from unlabeled examples, but also from noisy labels and inexhaustively-annotated examples.\nMnih & Hinton (2012) developed deep neural networks for improved labeling of aerial images, with robust loss functions to handle label omission and registration errors. This work shares a similar motivation of robustness to noisy labels, but rather than formulating loss functions for specific types of noise, we add a generic consistency objective to the loss to achieve robustness.\nMinimum entropy regularization, proposed in (Grandvalet & Bengio, 2005; 2006), performs semisupervised learning by augmenting cross-entropy loss with a term encouraging the classifier to make predictions with high confidence on the unlabeled examples1. This is notable because in their approach training on unlabeled examples does not require a generative model, which is beneficial for training on high-resolution images and other sensory data. We take a similar approach by side-\n1see eq. 9.7 in (Grandvalet & Bengio, 2006)\nstepping the difficulty of fully-generative models of high-dimensional sensory data. However, we extend beyond shallow models to deep networks, and to structured output prediction.\nNever ending language learning (NELL) (Carlson et al., 2010) and never ending image learning (NEIL) (Chen et al., 2013; 2014) are lifelong-learning systems for language and image understanding, respectively. They continuously bootstrap themselves using a cycle of data collection, propagation of labels to the newly collected data, and self-improvement by training on the new data. Our work is complementary to these efforts, and focuses on building robustness to noisy and missing labels into the model for weakly-supervised deep learning.\nLarochelle & Bengio (2008) developed an RBM for classification that uses a hybrid generative and discriminative training objective. Deep Boltmann Machines (Salakhutdinov & Hinton, 2009) can also be trained in a semi-supervised manner with labels connected to the top layer. More recently, multi-prediction DBM training (Goodfellow et al., 2013) and Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) improved the performance and simplified the training of deep generative models, enabling training via backpropagation much like in standard deep supervised networks. However, fully-generative unsupervised training on high-dimensional sensory data, e.g. ImageNet images, is still far behind supervised methods in terms of performance, and so in this work we do not follow the generative approach directly. Instead, this work focuses on a way to benefit from unlabeled and weakly-labeled examples with minimal modification to existing deep supervised networks. We demonstrate increased robustness to label noise and performance improvements from unlabeled data for a minimal engineering effort.\nMore recently, the problem of deep learning from noisy labels has begun to receive attention. Lee (2013) also followed the idea of minimum entropy regularization, and proposed generating \u201cpseudolabels\u201d as training targets for unlabeled data, and showed improved performance on MNIST with few labeled examples. Sukhbaatar & Fergus (2014) developed two deep learning techniques for handling noisy labels, learning to model the noise distribution in a top-down and bottom-up fashion. In this work, we push further by extending beyond class labels to structured outputs, and we achieve state-of-the-art scalable detection performance on ILSVRC2014, despite the fact that our method does not require explicitly modeling the noise distribution."}, {"heading": "3 METHOD", "text": "In this section we describe two approaches: section 3.1 uses reconstruction error as a consistency objective and explicitly models the noise distribution as a matrix mapping model predictions to training labels. A reconstruction loss is added to promote top-down consistency of model predictions with the observations, which allows the model to discover the pattern of noise in the data.\nThe method presented in section 3.2 (bootstrapping) uses a convex combination of training labels and the current model\u2019s predictions to generate the training targets, and thereby avoids directly modeling the noise distribution. This property is well-suited to the case of structured outputs, for which modeling dense interactions among all pairs of output units may be neither practical nor useful. These two approaches are compared empirically in section 4.\nIn section 3.3 we show how to apply our bootstrapping approach to structured outputs by using the MultiBox (Erhan et al., 2014) region proposal network to handle the case of inexhaustive structured output labeling for single-shot person detection and for class-agnostic region proposal."}, {"heading": "3.1 CONSISTENCY IN MULTI-CLASS PREDICTION VIA RECONSTRUCTION", "text": "Let x \u2208 {0, 1}D be the data (or deep features computed from the data) and t \u2208 {0, 1}L, \u2211\nk tk = 1 the observed noisy multinomial labels. The standard softmax regresses x onto t without taking into account noisy or missing labels. In addition to optimizing the conditional log-likelihood logP (t|x), we add a regularization term encouraging the class prediction to be perceptually consistent.\nWe first introduce into our model the \u201ctrue\u201d class label (as opposed to the noisy label observations) as a latent multinomial variable q \u2208 {0, 1}L, \u2211 j qj = 1. Our deep feed-forward network models\nthe posterior over q using the usual softmax regression:\nP (qj = 1|x) = P\u0303 (qj = 1|x)\u2211L\nj\u2032=1 P\u0303 (qj\u2032 = 1|x) =\nexp( \u2211D\ni=1W (1) ij xi + b (1) i )\u2211L j\u2032=1 exp( \u2211D i=1W (1) ij\u2032 xi + b (1) i )\n(1)\nwhere P\u0303 denotes the unnormalized probability distribution. Given the true label q, the label noise can be modeled using another softmax with logits as follows:\nlog P\u0303 (tk = 1|q) = L\u2211\nj=1\nW (2) kj qj + b (2) k (2)\nRoughly, W (2)kj learns the log-probability of observing true label j as noisy label k. Given only an observation x, we can marginalize over q to compute the posterior of target t given x.\nP (tk = 1|x) = L\u2211\nj=1\nP (tk = 1, qj = 1|x) = L\u2211\nj=1\nP (tk = 1|qj = 1)P (qj = 1|x) (3)\nwhere the label noise distribution and posterior over true labels are defined above. We can perform discriminative training by gradient ascent on logP (t|x). However, this purely discriminative training does not yet incorporate perceptual consistency, and there is no explicit incentive for the model to treat q as the \u201ctrue\u201d label; it can be viewed as another hidden layer, with the multinomial constraint resulting in an information bottleneck.\nIn unpublished work2, Hinton & Mnih (2009) developed a Restricted Boltzmann Machine (RBM) (Smolensky, 1986) variant with hidden multinomial output unit q and observed noisy label unit t as described above. The associated energy function can be written as\nE(x, t,q) = \u2212 D\u2211 i=1 L\u2211 j=1 W (1) ij xiqj \u2212 L\u2211 k=1 L\u2211 j=1 W (2) kj tkqj \u2212 L\u2211 j=1 b (1) j qj \u2212 L\u2211 k=1 b (2) k tk \u2212 D\u2211 i=1 b (3) i xi (4)\nDue to the bipartite structure of the RBM, t and x are conditionally independent given q, and so the energy function in eq. (4) leads to a similar form of the posterior as in eq. (3), marginalizing out the hidden multinomial unit. The probability distribution arising from (4) is given by\nP (x, t) =\n\u2211 q exp(\u2212E(x, t,q))\nZ where Z = \u2211\nx,t,q exp(\u2212E(x, t,q)) is the partition function. The model can be trained with a generative objective, e.g. by approximate gradient ascent on logP (x, t) via contrastive divergence (Hinton, 2002). Generative training naturally provides a notion of consistency between observations x and predictions q because the model learns to draw sample observations via the conditional likelihood P (xi = 1|q) = \u03c3( \u2211L j=1W (1) ij qj + b (3) i ), assuming binary observations.\n2Known from personal correspondence.\nHowever, fully-generative training is complicated by the fact that the exact likelihood gradient is intractable due to computing the partition function Z, and in practice MCMC is used. Generative training is further complicated (though certainly still possible) in cases where the features x are nonbinary. To avoid these complications, and to make our approach rapidly applicable to existing deep networks using rectified linear activations, and trainable via exact gradient descent, we propose an analogous autoencoder version.\nFigure 1 compares the RBM and autoencoder approaches to the multiclass prediction problem with perceptual consistency. The overall objective in the feed-forward version is as follows:\nLrecon(x, t) = \u2212 L\u2211\nk=1\ntk logP (tk = 1|x) + \u03b2||x\u2212W (2)q(x)||22 (5)\nwhere q(x)j = P (qj = 1|x) as in equation 1. The parameter \u03b2 can be found via cross-validation. Experimental results using this method are presented in sections 4.1 and 4.2."}, {"heading": "3.2 CONSISTENCY IN MULTI-CLASS PREDICTION VIA BOOTSTRAPPING", "text": "In this section we develop a simple consistency objective that does not require an explicit noise distribution or a reconstruction term. The idea is to dynamically update the targets of the prediction objective based on the current state of the model. The resulting targets are a convex combination of (1) the noisy training label, and (2) the current prediction of the model. Intuitively, as the learner improves over time, its predictions can be trusted more. This mitigates the damage of incorrect labeling, because incorrect labels are likely to be eventually highly inconsistent with other stimuli predicted to have the same label by the model.\nBy paying less heed to inconsistent labels, the learner can develop a more coherent model, which further improves its ability to evaluate the consistency of noisy labels. We refer to this approach as \u201cbootstrapping\u201d, in the sense of pulling oneself up by one\u2019s own bootstraps, and also due to inspiration from the work of Yarowsky (1995) which is also referred to as bootstrapping.\nConcretely, we use a cross-entropy objective as before, but generate new regression targets for each SGD mini-batch based on the current state of the model. We empirically evaluated two types of bootstrapping. \u201cSoft\u201d bootstrapping uses predicted class probabilities q directly to generate regression targets for each batch as follows:\nLsoft(q, t) = L\u2211\nk=1\n[\u03b2tk + (1\u2212 \u03b2)qk] log(qk) (6)\nIn fact, it can be shown that the resulting objective is equivalent to softmax regression with minimum entropy regularization, which was previously studied in (Grandvalet & Bengio, 2006). Intuitively, minimum entropy regularization encourages the model to have a high confidence in predicting labels (even for the unlabeled examples, which enables semi-supervised learning).\n\u201cHard\u201d bootstrapping modifies regression targets using the MAP estimate of q given x, which we denote as zk := 1[k = argmax qi, i = 1...L]:\nLhard(q, t) = L\u2211\nk=1\n[\u03b2tk + (1\u2212 \u03b2)zk] log(qk) (7)\nWhen used with mini-batch stochastic gradient descent, this leads to an EM-like algorithm: In the E-step, estimate the \u201ctrue\u201d confidence targets as a convex combination of training labels and model predictions; in the M-step, update the model parameters to better predict those generated targets.\nBoth hard and soft bootstrapping can be viewed as instances of a more general approach in which model-generated regression targets are modulated by a softmax temperature parameter T ; i.e.\nP (qj = 1|x) = exp(T \u00b7 (\n\u2211D i=1W (1) ij xi + b\n(1) j ))\u2211L j\u2032=1 exp(T \u00b7 ( \u2211D i=1W (1) ij\u2032 xi + b (1) j\u2032 ))\n(8)\nSetting T = 1 recovers soft boostrapping, and T = \u221e recovers hard bootstrapping. We only use these two operating points in our experiments, but it may be worthwhile to explore other values for T , and learning T for each dataset."}, {"heading": "3.3 CONSISTENCY WITH STRUCTURED OUTPUT PREDICTION", "text": "Noisy labels also occur in structured output prediction problems such as object detection. Current state-of-the-art object detection systems train on images annotated with bounding box labels of the relevant objects in each image, and the class label for each box. However, it is expensive to exhaustively annotate each image, and for some commonly-appearing categories the data may be prone to missing annotations. In this section, we modify the training objective of the MultiBox (Erhan et al., 2014) network for object detection to incorporate a notion of perceptual consistency into the loss.\nIn the MultiBox approach, ground-truth bounding boxes are clustered and the resulting centroids are used as \u201cpriors\u201d for predicting object location. A deep neural network is trained to predict, for each groundtruth object in an image, a residual of that groundtruth bounding box to the best-matching bounding box prior. The network also outputs a logistic confidence score for each prior, indicating the model\u2019s belief of whether or not an object appears in the corresponding location. Because MultiBox gives proposals with confidence scores, it enables very efficient runtime-quality tradeoffs for detection via thresholding the top-scoring proposals within budget. Thus it is an attractive target for further quality improvements, as we pursue in this section.\nDenote the confidence score training targets as t \u2208 {0, 1}L and the predicted confidence scores as c \u2208 [0, 1]L. The objective for MultiBox 3 can be written as the following cross-entropy loss:\nLmultibox(c, t) = \u2212 L\u2211\nk=1\n(tk log(ck) + (1\u2212 tk) log(1\u2212 ck)) (9)\nNote that the sum here is over object locations, not class labels as in the case of sections 3.1 and 3.2.\nIf there is an object at location k, but tk = 0 due to inexhaustive annotation, the model pays a large cost for correctly predicting ck = 1. Training naively on the noisy labels leads to perverse learning situations such as the following: two objects of the same category (potentially within the same image) appear in the training data, but only one of them is labeled. To reduce the loss, the confidence prediction layer must learn to distinguish the two objects, which is exactly contrary to the objective of visual invariance to category-preserving differences.\nTo incorporate a notion of perceptual consistency into the loss, we follow the same approach as in the case of multi-class classification: augment the regression targets using the model\u2019s current state. In the \u201chard\u201d case, MAP estimates can be obtained by thresholding ck > 1/2.\nLmultibox\u2212hard(c, t) =\u2212 L\u2211\nk=1\n[\u03b2tk + (1\u2212 \u03b2)1ck>0.5] log(ck) (10)\n\u2212 L\u2211\nk=1\n[\u03b2(1\u2212 tk) + (1\u2212 \u03b2)(1\u2212 1ck>0.5)] log(1\u2212 ck)\nLmultibox\u2212soft(c, t) =\u2212 L\u2211\nk=1\n[\u03b2tk + (1\u2212 \u03b2)ck] log(ck) (11)\n\u2212 L\u2211\nk=1\n[\u03b2(1\u2212 tk) + (1\u2212 \u03b2)(1\u2212 ck)] log(1\u2212 ck)\nWith the bootstrap variants of the MultiBox objective, unlabeled positives pose less of a problem because penalties for large ck are down-scaled by factor \u03b2 in the first term and (1\u2212ck) in the second term. By mitigating penalties due to missing positives in the data, our approach allows the model to learn to predict ck with high confidence even if the objects at location k are often unlabeled."}, {"heading": "4 EXPERIMENTS", "text": "We perform experiments on three image understanding tasks: MNIST handwritten digits recognition, Toroto Faces Database facial emotion recognition, and ILSVRC2014 detection. In all tasks,\n3We omit the bounding box regression term for simplicity, see (Erhan et al., 2014) for full details.\nwe train a deep neural network with our proposed consistency objective. In our figures, \u201cbootstraprecon\u201d refers to training as described in section 3.1, using reconstruction as a consistency objective. \u201cbootstrap-soft\u201d and \u201cbootstrap-hard\u201d refer to our method described in sections 3.2 and 3.3."}, {"heading": "4.1 MNIST WITH NOISY LABELS", "text": "In this section we train using our reconstruction-based objective (detailed in section 3.1) on MNIST handwritten digits with varying degrees of noise in the labels. Specifically, we used a fixed random permutation of the labels as visualized in figure 2, and we perform control experiments while varying the probability of applying the label permutation to each training example.\nAll models were trained with mini-batch SGD, with the same architecture: 784-500-300-10 neural network with rectified linear units. We used L2 weight decay of 0.0001. We found that \u03b2 = 0.8 worked best for bootstrap-hard, 0.95 for bootstrap-soft, and 0.005 for bootstrap-recon. We initialize W (2) to the identity matrix.\nFor the network trained with our proposed consistency objective, we initialized the network layers from the baseline prediction-only model. It is also possible to initialize from scratch using our approach, but we found that with pre-training we could use a larger \u03b2 and more quickly converge to a good result. Intuitively, this is similar to the initial collection of \u201cseed\u201d rules in the original bootstrapping algorithm of (Yarowsky, 1995). During the fine-tuning training phase, all network weights are updated by backpropagating gradients through the layers.\nFigure 2 shows that our bootstrapping method provides a very significant benefit in the case of permuted labels. The bootstrap-recon method performs the best, and bootstrap-hard nearly as well. The bootstrap-soft method provides some benefit in the high-noise regime, but only slightly better than the baseline overall.\nFigure 2 also shows that bootstrap-recon effectively learns the noise distribution P (t|q) via the parameters W (2)kj . Intuitively, the loss from the reconstruction term provides the learner a basis on which predictions q may disagree with training labels t. Since x must be able to be reconstructed from q in bootstrap-recon, learning a non-identity W (2) allows q to flexibly vary from t to better reconstruct x without incurring a penalty from prediction error.\nHowever, it is interesting to note that bootstrap-hard achieves nearly equivalent performance without explicitly parameterizing the noise distribution. This is useful because reconstruction may be challenging in many cases, such as when x is drawn from a complicated, high-dimensional distribution, and bootstrap-hard is trivial to implement on top of existing deep supervised networks."}, {"heading": "4.2 TORONTO FACES DATABASE EMOTION RECOGNITION", "text": "In this section we present results on emotion recognition. The Toronto Faces Database has 112,234 images, 4,178 of which have emotion labels. In all experiments we first extracted spatial-pyramidpooled OMP-1 features as described in (Coates & Ng, 2011) to get 3200-dimensional features. We then trained a 3200-1000-500-7 network to predict the 1-of-7 emotion labels for each image.\nAs in the case for our MNIST experiments, we initialize our model from the network pre-trained with prediction only, and the fine-tuned all layers with our hybrid objective.\nFigure 3 summarizes our TFD results. As in the case of MNIST, bootstrap-recon and bootstrap-hard perform the best, significantly outperforming the softmax baseline, and bootstrap-soft provides a more modest improvement.\nTraining Accuracy (%) baseline 85.3 bootstrap-recon 86.8 bootstrap-hard 86.8 bootstrap-soft 85.6 disBM a 85.4 CDA+CCA b 85.0\nThere is significant off-diagonal weight in W (2) learned by bootstrap-recon on TFD, suggesting that the model learns to \u201chedge\u201d its emotion prediction during training by spreading probability mass from the predicted class to commonly-confused classes, such as \u201cafraid\u201d and \u201csurprised\u201d. The strongest off diagonals are in the \u201chappy\u201d column, which may be due to the fact that \u201chappy\u201d is the most common expression, or perhaps that happy expressions have a large visual diversity. Our method improves the emotion recognition performance, which to our knowledge is state-of-the-art. The performance improvement from all three bootstrap methods on TFD suggests that our approach can be useful not just for mistaken labels, but also for semi-supervised learning (missing labels) and learning from weak labels such as emotion categories."}, {"heading": "4.3 ILSVRC 2014 FAST SINGLE-SHOT PERSON DETECTION", "text": "In this section we apply our method to detecting persons using a MultiBox network built on top of the Inception architecture proposed in (Szegedy et al., 2014). We first pre-trained MultiBox on classagnostic localization using the full ILSVRC2014 training set, since there are only several thousand images labeled with persons in ILSVRC, and then fine-tuned on person images only.\nAn important point for comparison is the top-K bootstrapping heuristic introduced for MultiBox training in (Szegedy et al., 2014) for person detection in the presence of missing annotations. In that approach, the top-K largest confidence predictions are dropped from the loss (which we show here in eq. (9)), and the setting used was K = 4. In other words, there is no gradient coming from the top-K most confident location predictions. In fact, it can be viewed as a form of bootstrapping where only the top-K most confident locations modify their targets, which become the predictions themselves. In this work, we aim to achieve similar or better performance in a more general way that can be applied to MultiBox and other discriminative models.\nThe precision-recall curves in figure 4 show that our proposed bootstrapping improves substantially over the prediction-only baseline. At the high-precision end of the PR curve, the approaches introduced in this paper perform better, while the top-K heuristic is slightly better at high-recall."}, {"heading": "4.4 ILSVRC 2014 200-CATEGORY DETECTION", "text": "In this section we apply our method to the case of object detection on the large-scale ImageNet data. Our proposed method is applied in two ways: first to the MultiBox network for region proposal, and second to the classifier network that predicts labels for each cropped image region. We follow the approach in (Szegedy et al., 2014) and combine image crops from MultiBox region proposals with deep network context features as the input to the classifier for each proposed region.\nWe trained the MultiBox network as described in 3.3, and the post-classifier network as described in section 3.2. We found that \u201chard\u201d performed better than the \u201csoft\u201d form of bootstrapping.\nMultiBox Postclassifier mAP (%) Recall @ 60p baseline baseline 39.8 38.4 baseline bootstrap-hard 40.0 38.6 bootstrap-hard baseline 40.3 39.3 bootstrap-hard bootstrap-hard 40.3 39.1 - GoogLeNet single modela 38.8 - - DeepID-Net single modelb 40.1 -\nUsing our proposed bootstrapping method, we observe modest improvement on the ILSVRC2014 detection \u201cval2\u201d data4, mainly attributable to bootstrapping in MultiBox training."}, {"heading": "5 CONCLUSIONS", "text": "In this paper we developed novel training methods for weakly-supervised deep learning, and demonstrated the effectiveness of our approach on multi-class prediction and structured output prediction for several datasets. Our method is exceedingly simple and can be applied with very little engineering effort to existing networks trained using a purely-supervised objective. The improvements that we show even with very simple methods, suggest that moving beyond purely-supervised deep learning is worthy of further research attention. In addition to achieving better performance with the data we already have, our results suggest that performance gains may be achieved from collecting more data at a cheaper price, since image annotation need not be as exhasutive and mistaken labels are not as harmful to the performance.\nIn future work, it may be promising to consider learning a time-dependent policy for tuning \u03b2, the scaling factor between prediction and perceptual consistency objectives, and also to extend our approach to the case of a situated agent. Another promising direction is to augment large-scale training for detection (e.g. ILSVRC) with unlabeled and more weakly-labeled images, to further benefit from our proposed perceptual consistency objective.\n4In a previous version of this draft we included numbers on the full validation set. However, we discovered that context and post-classifier models used \u201cval1\u201d data, so we re-ran experiments only on the \u201cval2\u201d subset"}], "references": [{"title": "Understanding the yarowsky algorithm", "author": ["Abney", "Steven"], "venue": "Computational Linguistics,", "citeRegEx": "Abney and Steven.,? \\Q2004\\E", "shortCiteRegEx": "Abney and Steven.", "year": 2004}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Blum", "Avrim", "Mitchell", "Tom"], "venue": "In Proceedings of the eleventh annual conference on Computational learning theory,", "citeRegEx": "Blum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1998}, {"title": "Identifying mislabeled training data", "author": ["Brodley", "Carla E", "Friedl", "Mark A"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Brodley et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Brodley et al\\.", "year": 1999}, {"title": "Toward an architecture for never-ending language learning", "author": ["Carlson", "Andrew", "Betteridge", "Justin", "Kisiel", "Bryan", "Settles", "Burr", "Hruschka Jr.", "Estevam R", "Mitchell", "Tom M"], "venue": "In AAAI,", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Neil: Extracting visual knowledge from web data", "author": ["Chen", "Xinlei", "Shrivastava", "Abhinav", "Gupta"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Enriching visual knowledge bases via object discovery and segmentation", "author": ["Chen", "Xinlei", "Shrivastava", "Abhinav", "Gupta"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Scalable Object Detection Using Deep Neural Networks", "author": ["Erhan", "Dumitru", "Szegedy", "Christian", "Toshev", "Alexander", "Anguelov", "Dragomir"], "venue": "In CVPR, pp", "citeRegEx": "Erhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1311.2524,", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "Multi-prediction deep boltzmann machines", "author": ["Goodfellow", "Ian", "Mirza", "Mehdi", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Grandvalet", "Yves", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grandvalet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Grandvalet et al\\.", "year": 2005}, {"title": "Analysis of semi-supervised learning with the yarowsky algorithm", "author": ["Haffari", "Gholam Reza", "Sarkar", "Anoop"], "venue": "arXiv preprint arXiv:1206.5240,", "citeRegEx": "Haffari et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haffari et al\\.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "Restricted boltzmann machine with hidden multinomial output unit", "author": ["Hinton", "Geoffrey E", "Mnih", "Volodymyr"], "venue": "Unpublished work,", "citeRegEx": "Hinton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks", "author": ["Lee", "Dong-Hyun"], "venue": "In Workshop on Challenges in Representation Learning,", "citeRegEx": "Lee and Dong.Hyun.,? \\Q2013\\E", "shortCiteRegEx": "Lee and Dong.Hyun.", "year": 2013}, {"title": "Learning to label aerial images from noisy data", "author": ["Mnih", "Volodymyr", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Analyzing the effectiveness and applicability of co-training", "author": ["Nigam", "Kamal", "Ghani", "Rayid"], "venue": "In Proceedings of the ninth international conference on Information and knowledge management,", "citeRegEx": "Nigam et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "Text classification from labeled and unlabeled documents using em", "author": ["Nigam", "Kamal", "McCallum", "Andrew Kachites", "Thrun", "Sebastian", "Mitchell", "Tom"], "venue": "Machine learning,", "citeRegEx": "Nigam et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nigam et al\\.", "year": 2000}, {"title": "Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection", "author": ["Ouyang", "Wanli", "Luo", "Ping", "Zeng", "Xingyu", "Qiu", "Shi", "Tian", "Yonglong", "Li", "Hongsheng", "Yang", "Shuo", "Wang", "Zhe", "Xiong", "Yuanjun", "Qian", "Chen"], "venue": "arXiv preprint arXiv:1409.3505,", "citeRegEx": "Ouyang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2014}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["Reed", "Scott", "Sohn", "Kihyuk", "Zhang", "Yuting", "Lee", "Honglak"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal", "Mirza", "Mehdi"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Semi-supervised self-training of object detection models", "author": ["Rosenberg", "Chuck", "Hebert", "Martial", "Schneiderman", "Henry"], "venue": null, "citeRegEx": "Rosenberg et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2005}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Smolensky", "Paul"], "venue": null, "citeRegEx": "Smolensky and Paul.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1986}, {"title": "Learning from noisy labels with deep neural networks", "author": ["Sukhbaatar", "Sainbayar", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1406.2080,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2014}, {"title": "The toronto face database", "author": ["Susskind", "Josh M", "Anderson", "Adam K", "Hinton", "Geoffrey E"], "venue": "Department of Computer Science,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}, {"title": "Scalable, High-Quality Object Detection", "author": ["C. Szegedy", "S. Reed", "D. Erhan", "D. Anguelov"], "venue": "ArXiv e-prints, December 2014", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Bootstrapping via graph propagation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pp. 620\u2013628", "author": ["Whitney", "Max", "Sarkar", "Anoop"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Whitney et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Whitney et al\\.", "year": 2012}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["Yarowsky", "David"], "venue": "In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Yarowsky and David.,? \\Q1995\\E", "shortCiteRegEx": "Yarowsky and David.", "year": 1995}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "Semi-supervised learning literature survey", "author": ["Zhu", "Xiaojin"], "venue": null, "citeRegEx": "Zhu and Xiaojin.,? \\Q2005\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2005}], "referenceMentions": [{"referenceID": 16, "context": "Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al.", "startOffset": 78, "endOffset": 194}, {"referenceID": 9, "context": "Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al.", "startOffset": 78, "endOffset": 194}, {"referenceID": 28, "context": "Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al.", "startOffset": 78, "endOffset": 194}, {"referenceID": 32, "context": "Currently the predominant systems for visual object recognition and detection (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014) use purely supervised training with regularization such as dropout (Hinton et al.", "startOffset": 78, "endOffset": 194}, {"referenceID": 15, "context": ", 2014) use purely supervised training with regularization such as dropout (Hinton et al., 2012) to avoid overfitting.", "startOffset": 75, "endOffset": 96}, {"referenceID": 31, "context": "On the Toronto Face Database (Susskind et al., 2010) we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the-art results, and can also benefit from unlabeled face images with no modification to our method.", "startOffset": 29, "endOffset": 52}, {"referenceID": 8, "context": ", 2014), we show that our approach improves single-shot person detection using a MultiBox network (Erhan et al., 2014), and also improves performance in full 200-way detection using MultiBox for region proposal and a deep CNN for post-classification.", "startOffset": 98, "endOffset": 118}, {"referenceID": 21, "context": "Co-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarlymotivated but used a pair of classifiers with separate views of the data to iteratively learn and generate additional training labels.", "startOffset": 12, "endOffset": 76}, {"referenceID": 21, "context": "Co-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarlymotivated but used a pair of classifiers with separate views of the data to iteratively learn and generate additional training labels. Whitney & Sarkar (2012) proposed bootstrapping labeled training examples with graph-based label propagation.", "startOffset": 36, "endOffset": 249}, {"referenceID": 21, "context": "Co-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarlymotivated but used a pair of classifiers with separate views of the data to iteratively learn and generate additional training labels. Whitney & Sarkar (2012) proposed bootstrapping labeled training examples with graph-based label propagation. Brodley & Friedl (1999) developed statistical methods for identifying mislabeled training data.", "startOffset": 36, "endOffset": 358}, {"referenceID": 21, "context": "Co-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarlymotivated but used a pair of classifiers with separate views of the data to iteratively learn and generate additional training labels. Whitney & Sarkar (2012) proposed bootstrapping labeled training examples with graph-based label propagation. Brodley & Friedl (1999) developed statistical methods for identifying mislabeled training data. Rosenberg et al. (2005) also trained an object detection system in a weakly-supervised manner using self-training, and demonstrated that their proposed model achieved comparable performance to models trained with a much larger set of labels.", "startOffset": 36, "endOffset": 454}, {"referenceID": 21, "context": "Co-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarlymotivated but used a pair of classifiers with separate views of the data to iteratively learn and generate additional training labels. Whitney & Sarkar (2012) proposed bootstrapping labeled training examples with graph-based label propagation. Brodley & Friedl (1999) developed statistical methods for identifying mislabeled training data. Rosenberg et al. (2005) also trained an object detection system in a weakly-supervised manner using self-training, and demonstrated that their proposed model achieved comparable performance to models trained with a much larger set of labels. However, that approach works as a wrapper around an existing detection system, whereas in this work we integrate a consistency objective for bootstrapping into the training of the deep network itself. Our work shares a similar motivation to these earlier works, but instead of explicitly generating new training labels and adding new examples to the training set in an outer loop, we incorporate our consistency objective directly into the model. In addition, we consider not only the case of learning from unlabeled examples, but also from noisy labels and inexhaustively-annotated examples. Mnih & Hinton (2012) developed deep neural networks for improved labeling of aerial images, with robust loss functions to handle label omission and registration errors.", "startOffset": 36, "endOffset": 1286}, {"referenceID": 4, "context": "Never ending language learning (NELL) (Carlson et al., 2010) and never ending image learning (NEIL) (Chen et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 5, "context": ", 2010) and never ending image learning (NEIL) (Chen et al., 2013; 2014) are lifelong-learning systems for language and image understanding, respectively.", "startOffset": 47, "endOffset": 72}, {"referenceID": 10, "context": "More recently, multi-prediction DBM training (Goodfellow et al., 2013) and Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) improved the performance and simplified the training of deep generative models, enabling training via backpropagation much like in standard deep supervised networks.", "startOffset": 45, "endOffset": 70}, {"referenceID": 4, "context": "Never ending language learning (NELL) (Carlson et al., 2010) and never ending image learning (NEIL) (Chen et al., 2013; 2014) are lifelong-learning systems for language and image understanding, respectively. They continuously bootstrap themselves using a cycle of data collection, propagation of labels to the newly collected data, and self-improvement by training on the new data. Our work is complementary to these efforts, and focuses on building robustness to noisy and missing labels into the model for weakly-supervised deep learning. Larochelle & Bengio (2008) developed an RBM for classification that uses a hybrid generative and discriminative training objective.", "startOffset": 39, "endOffset": 568}, {"referenceID": 4, "context": "Never ending language learning (NELL) (Carlson et al., 2010) and never ending image learning (NEIL) (Chen et al., 2013; 2014) are lifelong-learning systems for language and image understanding, respectively. They continuously bootstrap themselves using a cycle of data collection, propagation of labels to the newly collected data, and self-improvement by training on the new data. Our work is complementary to these efforts, and focuses on building robustness to noisy and missing labels into the model for weakly-supervised deep learning. Larochelle & Bengio (2008) developed an RBM for classification that uses a hybrid generative and discriminative training objective. Deep Boltmann Machines (Salakhutdinov & Hinton, 2009) can also be trained in a semi-supervised manner with labels connected to the top layer. More recently, multi-prediction DBM training (Goodfellow et al., 2013) and Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) improved the performance and simplified the training of deep generative models, enabling training via backpropagation much like in standard deep supervised networks. However, fully-generative unsupervised training on high-dimensional sensory data, e.g. ImageNet images, is still far behind supervised methods in terms of performance, and so in this work we do not follow the generative approach directly. Instead, this work focuses on a way to benefit from unlabeled and weakly-labeled examples with minimal modification to existing deep supervised networks. We demonstrate increased robustness to label noise and performance improvements from unlabeled data for a minimal engineering effort. More recently, the problem of deep learning from noisy labels has begun to receive attention. Lee (2013) also followed the idea of minimum entropy regularization, and proposed generating \u201cpseudolabels\u201d as training targets for unlabeled data, and showed improved performance on MNIST with few labeled examples.", "startOffset": 39, "endOffset": 1753}, {"referenceID": 4, "context": "Never ending language learning (NELL) (Carlson et al., 2010) and never ending image learning (NEIL) (Chen et al., 2013; 2014) are lifelong-learning systems for language and image understanding, respectively. They continuously bootstrap themselves using a cycle of data collection, propagation of labels to the newly collected data, and self-improvement by training on the new data. Our work is complementary to these efforts, and focuses on building robustness to noisy and missing labels into the model for weakly-supervised deep learning. Larochelle & Bengio (2008) developed an RBM for classification that uses a hybrid generative and discriminative training objective. Deep Boltmann Machines (Salakhutdinov & Hinton, 2009) can also be trained in a semi-supervised manner with labels connected to the top layer. More recently, multi-prediction DBM training (Goodfellow et al., 2013) and Generative Stochastic Networks (Bengio & Thibodeau-Laufer, 2013) improved the performance and simplified the training of deep generative models, enabling training via backpropagation much like in standard deep supervised networks. However, fully-generative unsupervised training on high-dimensional sensory data, e.g. ImageNet images, is still far behind supervised methods in terms of performance, and so in this work we do not follow the generative approach directly. Instead, this work focuses on a way to benefit from unlabeled and weakly-labeled examples with minimal modification to existing deep supervised networks. We demonstrate increased robustness to label noise and performance improvements from unlabeled data for a minimal engineering effort. More recently, the problem of deep learning from noisy labels has begun to receive attention. Lee (2013) also followed the idea of minimum entropy regularization, and proposed generating \u201cpseudolabels\u201d as training targets for unlabeled data, and showed improved performance on MNIST with few labeled examples. Sukhbaatar & Fergus (2014) developed two deep learning techniques for handling noisy labels, learning to model the noise distribution in a top-down and bottom-up fashion.", "startOffset": 39, "endOffset": 1985}, {"referenceID": 8, "context": "3 we show how to apply our bootstrapping approach to structured outputs by using the MultiBox (Erhan et al., 2014) region proposal network to handle the case of inexhaustive structured output labeling for single-shot person detection and for class-agnostic region proposal.", "startOffset": 94, "endOffset": 114}, {"referenceID": 8, "context": "In this section, we modify the training objective of the MultiBox (Erhan et al., 2014) network for object detection to incorporate a notion of perceptual consistency into the loss.", "startOffset": 66, "endOffset": 86}, {"referenceID": 8, "context": "In all tasks, We omit the bounding box regression term for simplicity, see (Erhan et al., 2014) for full details.", "startOffset": 75, "endOffset": 95}, {"referenceID": 24, "context": "(Reed et al., 2014) (Rifai et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": ", 2014) (Rifai et al., 2012)", "startOffset": 8, "endOffset": 28}, {"referenceID": 32, "context": "In this section we apply our method to detecting persons using a MultiBox network built on top of the Inception architecture proposed in (Szegedy et al., 2014).", "startOffset": 137, "endOffset": 159}, {"referenceID": 32, "context": "An important point for comparison is the top-K bootstrapping heuristic introduced for MultiBox training in (Szegedy et al., 2014) for person detection in the presence of missing annotations.", "startOffset": 107, "endOffset": 129}, {"referenceID": 32, "context": "We follow the approach in (Szegedy et al., 2014) and combine image crops from MultiBox region proposals with deep network context features as the input to the classifier for each proposed region.", "startOffset": 26, "endOffset": 48}], "year": 2015, "abstractText": "Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-theart results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.", "creator": "LaTeX with hyperref package"}}}