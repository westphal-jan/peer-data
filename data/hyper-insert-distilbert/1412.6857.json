{"id": "1412.6857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Contour Detection Using Cost-Sensitive Convolutional Neural Networks", "abstract": "we address the problem of contour issue detection models via providing per - pixel textual classifications of edge edge access point. to help facilitate addressing the research process, specifically the proposed approach tentatively leverages communication with enhanced densenet, an efficient mapping implementation instead of multiscale structured convolutional neural matrix networks ( often cnn ), to extract an advanced informative video feature vector generator for each pixel and uses an explicit svm classifier to dramatically accomplish contour problem detection. the main challenge challenge lies fundamentally in the adapting a simplified pre - chosen trained per - image 2d cnn model for yielding per - pixel image features. we propose to base technique on eliminating the densenet architecture to achieve pixelwise fine - size tuning and then consider a cost - difference sensitive strategy possible to seek further strongly improve the distance learning properties with a less small scaled dataset of edge defects and transparent non - edge image patches. in adopting the experiment of contour detection, we go look deeper into the effectiveness of combining 3d per - 8 pixel local features from different color cnn layers and obtain comparable performances to the state - better of - perfect the - scene art on bsds500.", "histories": [["v1", "Mon, 22 Dec 2014 01:16:50 GMT  (1169kb,D)", "https://arxiv.org/abs/1412.6857v1", "9 pages, 3 figures"], ["v2", "Wed, 24 Dec 2014 14:37:27 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v2", "9 pages, 3 figures"], ["v3", "Thu, 15 Jan 2015 15:01:16 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v3", "9 pages, 3 figures"], ["v4", "Sat, 28 Feb 2015 07:37:54 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v4", "9 pages, 3 figures"], ["v5", "Tue, 12 May 2015 08:42:42 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v5", "9 pages, 3 figures"]], "COMMENTS": "9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jyh-jing hwang", "tyng-luh liu"], "accepted": false, "id": "1412.6857"}, "pdf": {"name": "1412.6857.pdf", "metadata": {"source": "CRF", "title": "CONTOUR DETECTION USING COST-SENSITIVE CON-", "authors": [], "emails": ["jyhjinghwang@iis.sinica.edu.tw", "liutyng@iis.sinica.edu.tw"], "sections": [{"heading": "1 INTRODUCTION", "text": "Contour detection is fundamental to a wide range of computer vision applications, including image segmentation (Malik et al., 2001; Arbelaez et al., 2011), object detection (Zitnick & Dolla\u0301r, 2014) and recognition (Shotton et al., 2008). The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Dolla\u0301r & Zitnick, 2014). Take, for example, that Dolla\u0301r & Zitnick (2014) use structured random forests to learn local edge patterns, and report current state-of-the-art results with impressive computation efficiency. More recently, object cues are also considered in Kivinen et al. (2014) and Ganin & Lempitsky (2014) to further boost the performance. Despite the constant evolvement of relevant techniques in better solving the problem, seeking an appropriate feature representation remains the cornerstone of such efforts. We are thus motivated to propose a new learning formulation that can generate suitable per-pixel features for more satisfactorily performing contour detection.\nWe consider deep neural networks to construct a desired per-pixel feature learner. In particular, since the underlying task is essentially a classification problem, we adopt deep convolutional neural networks (CNNs) to establish a discriminative approach. However, one subtle deviation from typical applications of CNNs should be emphasized. In our method, we intend to use the CNN architecture, e.g., AlexNet (Krizhevsky et al., 2012), to generate features for each image pixel, not just a single feature vector for the whole input image. Such a distinction would call for a different perspective of parameter fine-tuning so that a pre-trained per-image CNN on ImageNet (Deng et al., 2009) can be adapted into a new model for per-pixel edge classifications. To further investigate the property of the features from different convolutional layers and from various ensembles, we carry out a number of experiments to evaluate their effectiveness in performing contour detection on the benchmark BSDS Segmentation dataset (Martin et al., 2001).\nThe organization of the paper is as follows. Section 2 includes related work of contour detection and deep convolutional neural networks. In Section 3, we describe the overall model for learning per-pixel features and useful techniques for fine-tuning the parameters. Section 4 provides detailed experimental results and comparisons to demonstrate the advantages of our method. In Section 5 we discuss the key ideas of the proposed techniques and possible future research efforts.\nar X\niv :1\n41 2.\n68 57\nv5 [\ncs .C\nV ]\n1 2\nM ay\n2 01"}, {"heading": "2 RELATED WORK", "text": "As stated, we focus on using a deep convolutional neural network to achieve feature learning for improving contour detection. The survey of relevant work is thus presented to give an insightful picture of the recent progress in each of the two areas of emphasis."}, {"heading": "2.1 CONTOUR DETECTION", "text": "Early techniques for contour detection (Fram & Deutsch, 1975; Canny, 1986; Perona & Malik, 1990) mainly concern local image cues, such as intensity and color gradients. Amongst them, the Canny detector (Canny, 1986) stands out for its simplicity and accuracy owing to exploring the peak gradient magnitude orthogonal to the contour direction. Detailed discussions about these approaches can be found in, e.g., Bowyer et al. (1999). Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy.\nApart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Dolla\u0301r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Dolla\u0301r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Dolla\u0301r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011).\nLim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Dolla\u0301r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency.\nMore relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures. Kivinen et al. (2014) encode and decode contours using multilayer mean-and-covariance restricted Boltzmann machines. Ganin & Lempitsky (2014) establish a deep architecture, which composes of convolutional neural networks and nearest neighbor search, and obtain convincing results. Different from Ganin & Lempitsky (2014), we strive for designing fine-tuning mechanisms with a small dataset for adapting an ImageNet pre-trained convolutional neural network for producing per-pixel image features. As we will see later, this effort leads to the state-of-the-art results of contour detection on the benchmark testing."}, {"heading": "2.2 CONVOLUTIONAL NEURAL NETWORKS", "text": "Noticeably, CNNs are popularized by LeCun and colleagues who first apply CNNs to digit recognition (LeCun et al., 1989), OCR (LeCun et al., 1998) and generic object recognition (Jarrett et al., 2009). In contrast to using hand-crafted features, CNNs learn discriminative features and exhibit hierarchical semantic information along their deep architecture.\nThe AlexNet by Krizhevsky et al. (2012) is perhaps the most popular implementation of CNNs for generic object classification. The model has been shown to outperform competing approaches based on traditional features in solving a number of mainstream computer vision problems. In Turaga et al. (2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al. (2013) also consider features from a deep CNN in a region proposal framework to achieve state-of-the-art object detection results on the PASCAL VOC dataset.\nWhile CNNs thrives in generic object recognition and detection, less attention is paid to applications demanding per-pixel processing, such as contour detection and segmentation. Our method exploits the AlexNet model for contour detection and explores its per-pixel fine-tuning with a small dataset. Recently and independently from our work, generating per-pixel features based on CNNs can also be found in Hariharan et al. (2014) and Long et al. (2014)."}, {"heading": "3 PER-PIXEL CNN FEATURES", "text": "Learning features by employing a deep architecture of neural net has been shown to be effective, but most of the existing techniques focus on yielding a feature vector for an input image (or image patch). Such a design may not be appropriate for vision applications that require investigating image characteristics in pixel level. In the problem of contour detection, the central task is to decide whether an underlying pixel is an edge point or not. Thus, it would be convenient that the deep network could yield per-pixel features.\nWe propose to construct a multiscale CNN model for contour detection. To this end, we extract perpixel CNN features in AlexNet (Krizhevsky et al., 2012) using DenseNet (Iandola et al., 2014), and pixelwise concatenate them to feed into a support vector machine (SVM) classifier. In particular, DenseNet provides fast multiscale feature pyramid extraction of any Caffe convolutional neural networks (Jia et al., 2014) and the convenience of working with images of arbitrary size. To extract per-pixel features, we upsample the feature maps from the first convolutional layer (Conv1) to the fifth convolutional layer (Conv5) to the original size of the input image. We then pixelwise stack the features from differen convolutional layers to constitute the per-pixel features. Depending on the selection of the convolutional layers, the resulting feature vector at each pixel would encode different level of information about an underlying pixel. Figure 1 illustrates the case of concatenating features from all five convolutional layers to form a 1376-D feature vector at each pixel.\nTo decide a pixel, say, (i, j) is a contour point, one can now readily feed its corresponding feature vector to an SVM classifier. In practice, it is useful to include information from neighboring pixels so that local contour structures can be better distinguished. We consider the following eight neighboring pixels (i \u00b1 k, j), (i, j \u00b1 k), (i \u00b1 k, j \u00b1 k) and append, starting from (i \u2212 k, j \u2212 k), their respective feature vector to that of (i, j) in clockwise order. In our implementation, we have tested k = 1, 2, 3, which correspond to an image patch of size 3\u00d7 3, 5\u00d7 5 and 9\u00d7 9, respectively."}, {"heading": "3.1 DENSENET FEATURE PYRAMIDS", "text": "We use DenseNet for CNN feature extraction because of its efficiency, flexibility, and availability. DenseNet is an open source system that computes dense and multiscale features from the convolutional layers of a Caffe CNN based object classifier. The process of feature extraction proceeds as follows. Given an input image, DenseNet computes its multiscale versions and stitches them to a large plane. After processing the whole plane by CNNs, DenseNet would unstitch the descriptor planes and then obtain multiresolution CNN descriptors.\nThe dimensions of convolutional features are ratios of the image size, e.g., one-fourth for Conv1, and one-eighth for Conv2. We rescale feature maps of all the convolutional layers to the image size. That is, there is a feature vector in every pixel. As illustrated in Figure 1, the dimension of the resulting feature vector is 1376 \u00d7 1, which is concatenated by Conv1 (96 \u00d7 1), Conv2 (256 \u00d7 1), Conv3 (384\u00d7 1), Conv4 (384\u00d7 1), and Conv5 (256\u00d7 1). For classification, we first concatenate features from the surrounding eight pixels to incorporate information about the local contour structure, and then use the combined per-pixel feature vectors to train a binary linear SVM. Specifically, in our multiscale setting, we train the SVM based on only the original resolution. In test time, we classify test images using both the original and the double resolutions. We average the two resulting edge maps for the final output of contour detection."}, {"heading": "3.2 PER-PIXEL FINE-TUNING", "text": "To fine-tune parameters for per-pixel contour detection, we exclude the two fully-connected layers of the ImageNet pre-trained CNN model in that the two layers will cause to restrict the input image size and consequently the overall architecture. We keep only the five convolutional layers, and on top of Conv5, we add a new 2-way softmax layer for edge classification.\nSpecifically, the input image size of ImageNet pre-trained CNN model is 227 \u00d7 227, which is not suitable for our per-pixel design as each map in the Conv5 layer would still be 13\u00d7 13. In addition, we need to remove padding in CNN to conform to that DenseNet does not use padding (except the input plane). To carry out per-pixel fine-tuning, we first generate a set of edge and non-edge patches. The image (patch) size is set to 163\u00d7 163, and would reduce to 1\u00d7 1 in Conv5, at which the 2-way softmax layer can now properly compute the per-pixel probability of being a contour point. Note that the loss for back-propagation is computed by the label prediction and the ground truth of the center pixel of 163\u00d7 163 input patch."}, {"heading": "3.3 COST-SENSITIVE FINE-TUNING", "text": "Compared with the number of parameters in DenseNet, the size of the training set of edge and nonedge patches is relatively small. Using the aforementioned per-pixel fine-tuning alone is usually insufficient to achieve good performance. Still, when addressing edges, it is evident that there will be certain underlying features especially crucial for distinguishing edges from non-edges. To further learn these subtle features from a small database, we adopt the concept in cost-sensitive learning. The original 2-way softmax training cost is the negative log-likelihood cost:\n\u2212 \u2211 i logP (yi |xi,\u03b8) = \u2212 \u2211 i yi logP (yi = 1 |xi,\u03b8)\u2212 \u2211 i (1\u2212 yi) logP (yi = 0 |xi,\u03b8) (1)\nwhere xi is the input image patch, \u03b8 is the parameters of CNN, and yi is the binary (0 or 1) edge label prediction. This cost is computed above the 2-way softmax layer, and will be back-propagated to train all convolutional layers. To apply cost-sensitive fine-tuning, we consider a biased negative log-likelihood cost:\n\u2212 \u2211 i \u03b1 yi logP (yi = 1 |xi,\u03b8)\u2212 \u2211 i \u03b2 (1\u2212 yi) logP (yi = 0 |xi,\u03b8) (2)\nwhere \u03b1 and \u03b2 are respectively the bias for positive (edge) or negative (non-edge) training data. If \u03b1 = 1 and \u03b2 = 1, (2) is reduced to the original negative log-likelihood cost as in (1). In our approach, we set \u03b1 = 2\u03b2 for positive cost-sensitive fine-tuning, and 2\u03b1 = \u03b2 for negative costsensitive fine-tuning. Notice that, rather than directly back-propagating with (2), a convenient and alternative strategy is to create biased sampling for fine-tuning with (1). That is, for positive costsensitive fine-tuning, we sample twice more edge patches than non-edge ones, and vice versa, for negative cost-sensitive fine-tuning.\n(b) ODS OIS AP"}, {"heading": "3.4 FINAL FUSION MODEL", "text": "The overall framework is an ensemble model. We combine an ImageNet pre-trained model, a perpixel fine-tuned model, a positive cost-sensitive fine-tuned model, and a negative cost-sensitive finetuned model together. We use a heuristic branch-and-bound scheme to decide the fusion coefficients. The idea of fusing different training models is to capture different aspects of features. It is worthy mentioning that the improvements owing to the model fusion indicates that the various fine-tunings have their own merits on feature learning and are all useful in this respect."}, {"heading": "4 EXPERIMENT RESULTS", "text": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011). To better assess the effects of the various fine-tuning techniques, we report their respective performance of contour detection. Comparisons with other competitive methods are also included to demonstrate the effectiveness of the proposed model.\nThe BSDS500 dataset is the current de facto standard image collection for contour detection. The dataset contains 200 training, 100 validation, and 200 testing images. Boundaries in each image are labeled by several workers and are averaged to form the ground truth. The accuracy of contour detection is evaluated by three measures: the best F-measure on the dataset for a fixed threshold (ODS), the aggregate F-measure on the dataset for the best threshold in each image (OIS), and the average precision (AP) on the full recall range (Arbelaez et al., 2011). Prior to evaluation, we apply a standard non-maximal suppression technique to edge maps to obtain thinned edges (Canny, 1986)."}, {"heading": "4.1 ON FINE-TUNING", "text": "The parameter fine-tuning is done on a a server with a GeForce GTX Titan Black GPU card. We set the overall learning rate as tenth of the original ImageNet pre-trained learning rate, and the softmax learning rate as ten times of the overall learning rate. The modification to the proposed per-pixel fine-tuning speeds up the parameter fine-tuning process. It takes 3 days to finish 100, 000 iterations of per-pixel fine-tuning, while requiring more than 10 days for traditional fine-tuning. For both traditional fine-tuning and per-pixel fine-tuning, we sample 500 boundary (edge) and 500 nonboundary (non-edge) patches per training image. For positive cost-sensitive fine-tuning, we sample 1000 boundary patches and 500 non-boundary patches per training image, while 500 boundary patches and 1000 non-boundary patches per training image for negative cost-sensitive fine-tuning.\nWe report the results of the various fine-tuning techniques in Table 1(a). The experiments use only Conv5 features, and are carried out with SVM classifications. Since this setting is most similar to a softmax fine-tuning architecture, we can directly observe the effectiveness of fine-tuning. The experiment results show that, compared with the baseline (the pre-trained model), traditional fine-tuning,\nwhich is the original fine-tuning architecture with padding in every layer, degrades overall performance by 0.2 to 0.4. This implies that traditional per-image fine-tuning is not appropriate in learning per-pixel features for per-pixel applications. On the other hand, per-pixel fine-tuning improves the performance by about 0.15 in all measurements. Pertaining to cost-sensitive fine-tuning, when compared with the per-pixel fine-tuning, positive fine-tuning slightly degrades and negative fine-tuning slightly improves. One possible explanation is that there are relatively more non-boundary regions than boundary points, so features learned for non-boundary regions improve the overall performance. However, if we combine features of positive and negative fine-tuning, the performance is significantly boosted again by 0.2. The performance gain signifies the complementary property of positive and negative fine-tunings as expected.\nIn conclusion, per-pixel fine-tuning raises the performances of per-pixel applications. Also, the combination of positive and negative cost-sensitive fine-tunings improves the classification performance the most. Therefore, it supports the advantage of using an ensemble fine-tuning model."}, {"heading": "4.2 ON FEATURES IN DIFFERENT LAYERS", "text": "We next conduct experiments to show how features from different convolutional layers contribute to the performance. In Table 1(b), we see that features in the second convolutional layer contribute the most, and then the third and the fourth layer. These suggest that low- to mid-level features are most useful for contour detection, while the lowest- and higher-level features provide additional boost. Although features in the first and the fifth convolutional layer are less effective when employed alone, we achieve the best results by combining all five streams. It indicates that the local edge information in low-level features and the object contour information in higher-level features are both necessary for achieving high performance in contour detection tasks."}, {"heading": "4.3 CONTOUR DETECTION RESULTS AND COMPARISONS", "text": "Finally, we show the experimental results of our pre-trained model and final fusion model. In Table 2, we report the contour detection performances on BSDS500 by our methods and seven competitive techniques, including gPb (Arbelaez et al., 2011), Sketch Tokens (Lim et al., 2013), Sparse Code Gradients (Xiaofeng & Bo, 2012), DeepNet (Kivinen et al., 2014), Pointwise Mutual Information (Isola et al., 2014), N4-fields (Ganin & Lempitsky, 2014) and Structured Edges (Dolla\u0301r & Zitnick, 2014). While our 5-stream ImageNet pre-trained model (using features from all five convolutional layers) already achieves impressive results for contour detection on ODS and OIS measurements, the proposed fine-tuning techniques can further improve the performance. In particular, the final ensemble model improves from 0.75 to 0.76 on ODS measurement, and from 0.77 to 0.78 on OIS\nmeasurement. It also achieves state-of-the-art performance on the AP measurement. In Figure 2, we include a number of contour detection examples for qualitative visualization."}, {"heading": "5 DISCUSSION", "text": "In this work, we describe how to use the DenseNet architecture to tailor for per-pixel computer vision problems, such as contour detection. We propose fine-tuning techniques to more effectively carry out parameter learning with a per-pixel based cost function and to overcome the limitation of using a small training set. The resulting cost-sensitive model appears to be promising for generating useful per-pixel feature vectors and should be useful for computer vision applications requiring analyzing local image property. An interesting future research direction is to establish a proper dimensionality reduction framework for the resulting high-dimensional per-pixel feature vectors and to examine its effects on the performance of contour detection."}], "references": [{"title": "Contour detection and hierarchical image segmentation", "author": ["Arbelaez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Arbelaez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arbelaez et al\\.", "year": 2011}, {"title": "Edge detector evaluation using empirical roc curves", "author": ["Bowyer", "Kevin", "Kranenburg", "Christine", "Dougherty", "Sean"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Bowyer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bowyer et al\\.", "year": 1999}, {"title": "Maximin affinity learning of image segmentation", "author": ["Briggman", "Kevin", "Denk", "Winfried", "Seung", "Sebastian", "Helmstaedter", "Moritz N", "Turaga", "Srinivas C"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Briggman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Briggman et al\\.", "year": 2009}, {"title": "A computational approach to edge detection", "author": ["Canny", "John"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Canny and John.,? \\Q1986\\E", "shortCiteRegEx": "Canny and John.", "year": 1986}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Fast edge detection using structured forests", "author": ["Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "arXiv preprint arXiv:1406.5549,", "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doll\u00e1r et al\\.", "year": 2014}, {"title": "Supervised learning of edges and object boundaries", "author": ["Doll\u00e1r", "Piotr", "Tu", "Zhuowen", "Belongie", "Serge"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Doll\u00e1r et al\\.", "year": 2006}, {"title": "Learning hierarchical features for scene labeling", "author": ["Farabet", "Clement", "Couprie", "Camille", "Najman", "Laurent", "LeCun", "Yann"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Farabet et al\\.,? \\Q1915\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 1915}, {"title": "On the quantitative evaluation of edge detection schemes and their comparison with human performance", "author": ["Fram", "Jerry R", "Deutsch", "Edward S"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Fram et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Fram et al\\.", "year": 1975}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1311.2524,", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["Hariharan", "Bharath", "Arbel\u00e1ez", "Pablo", "Girshick", "Ross", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1411.5752,", "citeRegEx": "Hariharan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2014}, {"title": "Densenet: Implementing efficient convnet descriptor pyramids", "author": ["Iandola", "Forrest", "Moskewicz", "Matt", "Karayev", "Sergey", "Girshick", "Ross", "Darrell", "Trevor", "Keutzer", "Kurt"], "venue": "arXiv preprint arXiv:1404.1869,", "citeRegEx": "Iandola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2014}, {"title": "Crisp boundary detection using pointwise mutual information", "author": ["Isola", "Phillip", "Zoran", "Daniel", "Krishnan", "Dilip", "Adelson", "Edward H"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Isola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2014}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "M Ranzato", "LeCun", "Yann"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Visual boundary prediction: A deep neural prediction network and quality dissection", "author": ["Kivinen", "Jyri J", "Williams", "Christopher KI", "Heess", "Nicolas", "Technologies", "DeepMind"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kivinen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sketch tokens: A learned mid-level representation for contour and object detection", "author": ["Lim", "Joseph J", "Zitnick", "C Lawrence", "Doll\u00e1r", "Piotr"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1411.4038,", "citeRegEx": "Long et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "Discriminative sparse image models for class-specific edge detection and image interpretation", "author": ["Mairal", "Julien", "Leordeanu", "Marius", "Bach", "Francis", "Hebert", "Martial", "Ponce", "Jean"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Mairal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2008}, {"title": "Contour and texture analysis for image segmentation", "author": ["Malik", "Jitendra", "Belongie", "Serge", "Leung", "Thomas", "Shi", "Jianbo"], "venue": "International journal of computer vision,", "citeRegEx": "Malik et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Malik et al\\.", "year": 2001}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["Martin", "David", "Fowlkes", "Charless", "Tal", "Doron", "Malik", "Jitendra"], "venue": "In Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["Martin", "David R", "Fowlkes", "Charless C", "Malik", "Jitendra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Martin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2004}, {"title": "Scale-space and edge detection using anisotropic diffusion", "author": ["Perona", "Pietro", "Malik", "Jitendra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Perona et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Perona et al\\.", "year": 1990}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["Sermanet", "Pierre", "Kavukcuoglu", "Koray", "Chintala", "Soumith", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Multiscale categorical object recognition using contour fragments", "author": ["Shotton", "Jamie", "Blake", "Andrew", "Cipolla", "Roberto"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Shotton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shotton et al\\.", "year": 2008}, {"title": "Convolutional networks can learn to generate affinity graphs for image segmentation", "author": ["Turaga", "Srinivas C", "Murray", "Joseph F", "Jain", "Viren", "Roth", "Fabian", "Helmstaedter", "Moritz", "Briggman", "Kevin", "Denk", "Winfried", "Seung", "H Sebastian"], "venue": "Neural Computation,", "citeRegEx": "Turaga et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turaga et al\\.", "year": 2010}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["Xiaofeng", "Ren", "Bo", "Liefeng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Xiaofeng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiaofeng et al\\.", "year": 2012}, {"title": "Detecting object boundaries using low-, mid, and high-level information", "author": ["Zheng", "Songfeng", "Yuille", "Alan", "Tu", "Zhuowen"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Zheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2010}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["Zitnick", "C Lawrence", "Doll\u00e1r", "Piotr"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Zitnick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Contour detection is fundamental to a wide range of computer vision applications, including image segmentation (Malik et al., 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 0, "context": "Contour detection is fundamental to a wide range of computer vision applications, including image segmentation (Malik et al., 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 28, "context": ", 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008).", "startOffset": 67, "endOffset": 89}, {"referenceID": 24, "context": "The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014).", "startOffset": 125, "endOffset": 227}, {"referenceID": 21, "context": "The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014).", "startOffset": 125, "endOffset": 227}, {"referenceID": 0, "context": "The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014).", "startOffset": 125, "endOffset": 227}, {"referenceID": 16, "context": ", AlexNet (Krizhevsky et al., 2012), to generate features for each image pixel, not just a single feature vector for the whole input image.", "startOffset": 10, "endOffset": 35}, {"referenceID": 4, "context": "Such a distinction would call for a different perspective of parameter fine-tuning so that a pre-trained per-image CNN on ImageNet (Deng et al., 2009) can be adapted into a new model for per-pixel edge classifications.", "startOffset": 131, "endOffset": 150}, {"referenceID": 23, "context": "To further investigate the property of the features from different convolutional layers and from various ensembles, we carry out a number of experiments to evaluate their effectiveness in performing contour detection on the benchmark BSDS Segmentation dataset (Martin et al., 2001).", "startOffset": 260, "endOffset": 281}, {"referenceID": 0, "context": ", 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008). The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014). Take, for example, that Doll\u00e1r & Zitnick (2014) use structured random forests to learn local edge patterns, and report current state-of-the-art results with impressive computation efficiency.", "startOffset": 8, "endOffset": 391}, {"referenceID": 0, "context": ", 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008). The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014). Take, for example, that Doll\u00e1r & Zitnick (2014) use structured random forests to learn local edge patterns, and report current state-of-the-art results with impressive computation efficiency. More recently, object cues are also considered in Kivinen et al. (2014) and Ganin & Lempitsky (2014) to further boost the performance.", "startOffset": 8, "endOffset": 607}, {"referenceID": 0, "context": ", 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008). The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014). Take, for example, that Doll\u00e1r & Zitnick (2014) use structured random forests to learn local edge patterns, and report current state-of-the-art results with impressive computation efficiency. More recently, object cues are also considered in Kivinen et al. (2014) and Ganin & Lempitsky (2014) to further boost the performance.", "startOffset": 8, "endOffset": 636}, {"referenceID": 24, "context": "Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy.", "startOffset": 32, "endOffset": 97}, {"referenceID": 21, "context": "Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy.", "startOffset": 32, "endOffset": 97}, {"referenceID": 0, "context": "Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy.", "startOffset": 32, "endOffset": 97}, {"referenceID": 6, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 21, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 31, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 19, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 15, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 0, "context": ", Bowyer et al. (1999). Subsequent work along this line (Martin et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch.", "startOffset": 8, "endOffset": 432}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours.", "startOffset": 8, "endOffset": 554}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al.", "startOffset": 8, "endOffset": 690}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al.", "startOffset": 8, "endOffset": 779}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures.", "startOffset": 8, "endOffset": 798}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours.", "startOffset": 8, "endOffset": 933}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency.", "startOffset": 8, "endOffset": 1080}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency.", "startOffset": 8, "endOffset": 1105}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures.", "startOffset": 8, "endOffset": 1289}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures.", "startOffset": 8, "endOffset": 1318}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures. Kivinen et al. (2014) encode and decode contours using multilayer mean-and-covariance restricted Boltzmann machines.", "startOffset": 8, "endOffset": 1391}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures. Kivinen et al. (2014) encode and decode contours using multilayer mean-and-covariance restricted Boltzmann machines. Ganin & Lempitsky (2014) establish a deep architecture, which composes of convolutional neural networks and nearest neighbor search, and obtain convincing results.", "startOffset": 8, "endOffset": 1511}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures. Kivinen et al. (2014) encode and decode contours using multilayer mean-and-covariance restricted Boltzmann machines. Ganin & Lempitsky (2014) establish a deep architecture, which composes of convolutional neural networks and nearest neighbor search, and obtain convincing results. Different from Ganin & Lempitsky (2014), we strive for designing fine-tuning mechanisms with a small dataset for adapting an ImageNet pre-trained convolutional neural network for producing per-pixel image features.", "startOffset": 8, "endOffset": 1690}, {"referenceID": 17, "context": "Noticeably, CNNs are popularized by LeCun and colleagues who first apply CNNs to digit recognition (LeCun et al., 1989), OCR (LeCun et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 18, "context": ", 1989), OCR (LeCun et al., 1998) and generic object recognition (Jarrett et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 13, "context": ", 1998) and generic object recognition (Jarrett et al., 2009).", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": ", 1998) and generic object recognition (Jarrett et al., 2009). In contrast to using hand-crafted features, CNNs learn discriminative features and exhibit hierarchical semantic information along their deep architecture. The AlexNet by Krizhevsky et al. (2012) is perhaps the most popular implementation of CNNs for generic object classification.", "startOffset": 40, "endOffset": 259}, {"referenceID": 9, "context": ", 1998) and generic object recognition (Jarrett et al., 2009). In contrast to using hand-crafted features, CNNs learn discriminative features and exhibit hierarchical semantic information along their deep architecture. The AlexNet by Krizhevsky et al. (2012) is perhaps the most popular implementation of CNNs for generic object classification. The model has been shown to outperform competing approaches based on traditional features in solving a number of mainstream computer vision problems. In Turaga et al. (2010) and Briggman et al.", "startOffset": 40, "endOffset": 519}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation.", "startOffset": 11, "endOffset": 34}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation.", "startOffset": 11, "endOffset": 132}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al.", "startOffset": 11, "endOffset": 196}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al.", "startOffset": 11, "endOffset": 361}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al. (2013) also consider features from a deep CNN in a region proposal framework to achieve state-of-the-art object detection results on the PASCAL VOC dataset.", "startOffset": 11, "endOffset": 385}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al. (2013) also consider features from a deep CNN in a region proposal framework to achieve state-of-the-art object detection results on the PASCAL VOC dataset. While CNNs thrives in generic object recognition and detection, less attention is paid to applications demanding per-pixel processing, such as contour detection and segmentation. Our method exploits the AlexNet model for contour detection and explores its per-pixel fine-tuning with a small dataset. Recently and independently from our work, generating per-pixel features based on CNNs can also be found in Hariharan et al. (2014) and Long et al.", "startOffset": 11, "endOffset": 966}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al. (2013) also consider features from a deep CNN in a region proposal framework to achieve state-of-the-art object detection results on the PASCAL VOC dataset. While CNNs thrives in generic object recognition and detection, less attention is paid to applications demanding per-pixel processing, such as contour detection and segmentation. Our method exploits the AlexNet model for contour detection and explores its per-pixel fine-tuning with a small dataset. Recently and independently from our work, generating per-pixel features based on CNNs can also be found in Hariharan et al. (2014) and Long et al. (2014).", "startOffset": 11, "endOffset": 989}, {"referenceID": 16, "context": "To this end, we extract perpixel CNN features in AlexNet (Krizhevsky et al., 2012) using DenseNet (Iandola et al.", "startOffset": 57, "endOffset": 82}, {"referenceID": 11, "context": ", 2012) using DenseNet (Iandola et al., 2014), and pixelwise concatenate them to feed into a support vector machine (SVM) classifier.", "startOffset": 23, "endOffset": 45}, {"referenceID": 14, "context": "In particular, DenseNet provides fast multiscale feature pyramid extraction of any Caffe convolutional neural networks (Jia et al., 2014) and the convenience of working with images of arbitrary size.", "startOffset": 119, "endOffset": 137}, {"referenceID": 23, "context": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 0, "context": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 0, "context": "The accuracy of contour detection is evaluated by three measures: the best F-measure on the dataset for a fixed threshold (ODS), the aggregate F-measure on the dataset for the best threshold in each image (OIS), and the average precision (AP) on the full recall range (Arbelaez et al., 2011).", "startOffset": 268, "endOffset": 291}, {"referenceID": 0, "context": "Method ODS OIS AP gPb-owt-ucm (Arbelaez et al., 2011) .", "startOffset": 30, "endOffset": 53}, {"referenceID": 19, "context": "73 Sketch tokens (Lim et al., 2013) .", "startOffset": 17, "endOffset": 35}, {"referenceID": 15, "context": "77 DeepNet (Kivinen et al., 2014) .", "startOffset": 11, "endOffset": 33}, {"referenceID": 12, "context": "76 PMI+sPb, MS (Isola et al., 2014) .", "startOffset": 15, "endOffset": 35}, {"referenceID": 0, "context": "In Table 2, we report the contour detection performances on BSDS500 by our methods and seven competitive techniques, including gPb (Arbelaez et al., 2011), Sketch Tokens (Lim et al.", "startOffset": 131, "endOffset": 154}, {"referenceID": 19, "context": ", 2011), Sketch Tokens (Lim et al., 2013), Sparse Code Gradients (Xiaofeng & Bo, 2012), DeepNet (Kivinen et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 15, "context": ", 2013), Sparse Code Gradients (Xiaofeng & Bo, 2012), DeepNet (Kivinen et al., 2014), Pointwise Mutual Information (Isola et al.", "startOffset": 62, "endOffset": 84}, {"referenceID": 12, "context": ", 2014), Pointwise Mutual Information (Isola et al., 2014), N-fields (Ganin & Lempitsky, 2014) and Structured Edges (Doll\u00e1r & Zitnick, 2014).", "startOffset": 38, "endOffset": 58}], "year": 2015, "abstractText": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500.", "creator": "LaTeX with hyperref package"}}}