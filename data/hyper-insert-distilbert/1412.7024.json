{"id": "1412.7024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Training deep neural networks with low precision multiplications", "abstract": "we presently simulate employing the training of identifying a set of state of better the art neural networks, the augmented maxout knowledge networks ( daniel goodfellow et al., 2013a ), functioning on three benchmark global datasets : the standard mnist, nasa cifar10, and thomson svhn, combine with three important distinct semantic arithmetics : floating point, fixed point range and dynamic fixed link point. for determining each member of those datasets and for each purpose of assuming those arithmetics, we assess the impact of the temporal precision of the various computations on setting the calculated final computed error of employing the prior training. we certainly find conclude that very low turing precision computation is sufficient whilst not firstly just for now running conventional trained computing networks fast but also sufficiently for fully training them. for example, historically almost state - of - the - form art results were essentially obtained on most datasets with around 10 bits for subsequent computing activations measuring and calculating gradients, 17 and 12 bits for data storing updated parameters.", "histories": [["v1", "Mon, 22 Dec 2014 15:22:45 GMT  (183kb,D)", "http://arxiv.org/abs/1412.7024v1", "9 pages, 4 figures, ICLR 2015"], ["v2", "Thu, 25 Dec 2014 18:05:12 GMT  (183kb,D)", "http://arxiv.org/abs/1412.7024v2", "Few more details on our dynamic fixed point implementation compared with the previous version"], ["v3", "Thu, 26 Feb 2015 00:26:12 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7024v3", "9 pages, 4 figures, under review as conference paper at ICRL 2015"], ["v4", "Fri, 3 Apr 2015 22:52:43 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7024v4", "9 pages, 4 figures, Accepted as a workshop contribution at ICLR 2015"], ["v5", "Wed, 23 Sep 2015 01:00:44 GMT  (347kb,D)", "http://arxiv.org/abs/1412.7024v5", "10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "9 pages, 4 figures, ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["matthieu courbariaux", "yoshua bengio", "jean-pierre david"], "accepted": false, "id": "1412.7024"}, "pdf": {"name": "1412.7024.pdf", "metadata": {"source": "CRF", "title": "LOW PRECISION ARITHMETIC FOR DEEP LEARNING", "authors": ["Matthieu Courbariaux", "Yoshua Bengio"], "emails": ["matthieu.courbariaux@polymtl.ca", "jean-pierre.david@polymtl.ca", "yoshua.bengio@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep learning is very often limited by memory and computational power. Lots of previous works address the best exploitation of general-purpose hardware, typically CPU clusters (Dean et al., 2012) and GPUs (Coates et al., 2009; Krizhevsky et al., 2012a). Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).\nActually, such approaches always consist in adapting the algorithm to best exploit state of the art hardware. Nevertheless, some dedicated deep learning hardware is appearing as well. FPGA implementations claim a better power efficiency than general-purpose hardware (Farabet et al., 2011; Kim et al., 2009). The corresponding ASIC implementations are even more efficient (Pham et al., 2012). In contrast with general-purpose hardware, dedicated hardware such as ASIC and FPGA enables to build the hardware from the algorithm. In this context, it is important to know what is the minimum precision acceptable.\nActually, minimizing the size of the arithmetic operators and the size of the memories would lead to architectures with more operators and memories working in parallel. It would also drastically reduce the power consumption. For instance, using single precision (32 bits) instead of double precision (64 bits) for a floating point multiplier reduces its area by four on modern FPGAs (Govindu et al., 2004; Underwood, 2004).\nIn this paper, we simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results\n\u2217Yoshua Bengio is a CIFAR Senior Fellow.\nar X\niv :1\n41 2.\n70 24\nv1 [\ncs .L\nG ]\n2 2\nD ec\nwere obtained on each dataset with around 10 bits for computing activations and gradients, and 12 bits for storing updated parameters."}, {"heading": "2 MAXOUT NETWORKS", "text": "A Maxout network is a multi-layer neural network that uses maxout units in its hidden layers. A maxout unit outputs the maximum of a set of k dot products between between k weight vectors and the input vector of the unit (e.g., the output of the previous layer):\nhli = k\nmax j=1\n(bli,j + w l i,j \u00b7 hl\u22121)\nwhere hl is the vector of activations at layer l and weight vectors wli,j and biases b l i,j are the parameters of the j-th filter of unit i on layer l.\nA maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hli = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al., 2013a). Combined with dropout, a very effective regularization method (Hinton et al., 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al., 2013a), both as part of fully connected feedforward deep nets and as part of deep convolutional nets. The dropout technique is a good approximation of model averaging with shared parameters across an exponentially large number of networks that are formed by subsets of the units of the original noise-free deep network. In our article, we reproduce the experiments of Goodfellow et al. (2013a) on the MNIST, CIFAR10 and SVHN data sets while changing the arithmetic operations and the numerical precision of the computations."}, {"heading": "3 FLOATING POINT", "text": "Floating point formats are often used to represent real values. They consist in a sign, an exponent, and a mantissa. The exponent gives the floating point formats a wide range, and the mantissa gives them a good precision. You can compute the value of a floating point number using the following formula:\nvalue = (\u22121)sign \u00d7 ( 1 + mantissa\n223\n) \u00d7 2(exponent\u2212127)\nTable 1 shows the exponent and mantissa widths associated with each floating point format. In our experiments, we use single precision floating point format as our reference as it is the most widely used format in deep learning, especially for GPU computation. We show that the use of half precision floating point format has little to no impact on the training of neural networks. As of the moment of writing this article, no IEEE standard exists below the half precision floating point format."}, {"heading": "4 FIXED POINT", "text": "Fixed point formats consist in a signed mantissa and a global scaling factor shared between all fixed point variables. The scaling factor can be seen as the position of the radix point. The scaling factor is usually fixed, hence the name \u201dfixed point\u201d. Reducing the scaling factor reduces the range and augments the precision of the format. This is why in order to maximize the precision, we\nmust first find the minimum acceptable range. The scaling factor is typically a power of two for computational efficiency (the scaling multiplications are then replaced with shifts). As a result, fixed point arithmetic can also be seen as a floating point arithmetic with a shared fixed global exponent. Fixed point arithmetic is commonly found on embedded systems with no FPU (Floating Point Unit). It relies on integer operations. It is hardware-wise cheaper than its floating point counterpart, as the exponent is fixed.\nIn Vanhoucke et al. (2011), the authors show that the use of 8 bits fixed point arithmetic speeds up over three times the application of a neural network on CPU, compared to floating point. This assumes that training has been previously performed with single precision floating point arithmetic. In our article, we go further by actually training neural networks with fixed point arithmetic. The application of a trained neural network (computing outputs given inputs) is required while training a neural network but the reciprocal is false.\nTraining neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007). However, those works use very small models and datasets in today standards. For instance, in Savich et al. (2007), the model is a MultiLayer Perceptron with two hidden units and the task is to learn a XOR gate. One of our contributions is to update their results with some state of the art models, that is to say Maxout networks, and some of the latest benchmark datasets, that is to say MNIST, CIFAR10 and SVHN."}, {"heading": "5 DYNAMIC FIXED POINT", "text": "Dynamic fixed point arithmetic (Williamson, 1991) is a variant of fixed point arithmetic in which there are several scaling factors instead of a single global one. Those scaling factors are not fixed. As such, it can be seen as a compromise between floating point arithmetic - where each scalar variable owns its scaling factor which is updated during each operations - and fixed point arithmetic - where there is only one global scaling factor which is never updated. In dynamic fixed point, a few grouped variables share a scaling factor which is updated from time to time to reflect the statistics of values in the group.\nIn practice, we choose to associate each layer\u2019s weights, bias, weighted sum, outputs (postnonlinearity) and the respective gradients vectors and matrices with a different scaling factor. Those scaling factors are initialized with a global value at the beginning of the training. The scaling factors are updated at the end of each training epoch to the minimum value which does not cause the corresponding group of values to exceed a maximum overflow rate (which is a hyperparameter).\nDynamic fixed point arithmetic was introduced several decades ago (Williamson, 1991) and it was recently used for an SVM implementation (Wang et al., 2010). However, as far as we know, this is the first paper reporting on the use of dynamic fixed point arithmetic to train deep neural networks, so this is one of the contributions of this paper."}, {"heading": "6 TWO DIFFERENT BIT WIDTHS", "text": "We use two different bit widths in our fixed point and dynamic fixed point arithmetic experiments: one for the assignment of parameters and the other for the rest of the computations. The idea behind this is to be able to accumulate small changes in the parameters (which requires more precision) and thus spare a few bits for the computations, which can be done with lower precision because of the implicitly averaging performed via stochastic gradient descent during training:\n\u03b8t+1 = \u03b8t \u2212 \u2202Ct(\u03b8t)\n\u2202\u03b8t\nwhere Ct(\u03b8t) is the cost to minimize over the minibatch visited at iteration t using \u03b8t as parameters and the learning rate. We see that the resulting parameter is the sum\n\u03b8T = \u03b80 \u2212 T\u22121\u2211 t=1 \u2202Ct(\u03b8t) \u2202\u03b8t .\nThe terms of this sum are not statistically independent (because the value of \u03b8t depends on the value of \u03b8t\u22121) but the dominant variations come from the random sample of examples in the minibatch\n(\u03b8 moves slowly) so that a strong averaging effect takes place, and each contribution in the sum is relatively small, hence the demand for sufficient precision (when adding a small number with a large number)."}, {"heading": "7 SIMULATION", "text": "Since we do not have access to actual low-precision hardware, all of the results reported in this paper are obtained by simulating low-precision computation. We use single floating point format for computation and storage as it allows us to use existing GPU libraries, for instance Theano (Bergstra et al., 2010; Bastien et al., 2012). However, each time an activation, a gradient or a parameter is stored, we artificially reduce its precision.\nThe only limitation of our simulation is that it does not take into account the size of the accumulators. We somehow make the hypothesis that the accumulators are as precise as the single floating point format. That being said, in practice, accumulators are often more precise than single floats (Lee et al., 2008)."}, {"heading": "8 BASELINE RESULTS", "text": "We reproduce the results of Goodfellow et al. (2013a) on the MNIST, CIFAR10 and SVHN, using single precision floating point arithmetic. In the next section, LOW-PRECISION RESULTS, we assess the impact of low-precision arithmetic on the models of this section, BASELINE RESULTS."}, {"heading": "8.1 MNIST", "text": "The MNIST (LeCun et al., 1998) data set is described in Table 2. We do not use any dataaugmentation (e.g. distortions) nor any unsupervised pre-training. We simply use minibatch stochastic gradient descent (SGD). We use a linearly decaying learning rate and a linearly saturating momentum. We regularize the model with dropout and a constraint on the norm of each weight vector, as in (Srebro and Shraibman, 2005).\nWe train two different models on the MNIST. The first is a permutation invariant (PI) model which is unaware of the structure of the data. It consists in two fully connected maxout layers followed\nby a softmax layer. The second model consists in three convolutional maxout hidden layers (with spatial max pooling on top of the maxout layers) followed by a densely connected softmax layer.\nThis is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples because it requires to stop the training at an arbitrary epoch, which adds some randomness to the final test error, which blurs the impact of low-precision arithmetic. As a result, our test error is slightly bigger that the one reported in Goodfellow et al. (2013a). The final test error is in Table 3."}, {"heading": "8.2 CIFAR10", "text": "The CIFAR10 (Krizhevsky and Hinton, 2009) data set is described in Table 2. We preprocess the data using global contrast normalization and ZCA whitening. The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer. Otherwise, we follow a similar procedure as with the MNIST dataset. This is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples. The final test error is in Table 3."}, {"heading": "8.3 STREET VIEW HOUSE NUMBERS", "text": "The SVHN (Netzer et al., 2011) data set is described in Table 2. We applied local contrast normalization preprocessing the same way as Zeiler and Fergus (2013). The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer. Otherwise, we followed the same approach as on the MNIST dataset. This is the same procedure as in Goodfellow et al. (2013a). The final test error is in Table 3."}, {"heading": "9 LOW-PRECISION RESULTS", "text": "We assess the impact of low-precision arithmetic on the models of the previous section, BASELINE RESULTS. We made our code available 1."}, {"heading": "9.1 FLOATING POINT", "text": "The use of half precision floating point format has little to no impact on the test set error rate, as shown in Table 3.\n1 https://github.com/MatthieuCourbariaux/deep-learning-arithmetic"}, {"heading": "9.2 FIXED POINT", "text": "The optimal radix point position in fixed point is after the fifth most important bits, as illustrated in Figure 1. The corresponding range is approximately [-32,32]. The corresponding scaling factor depends on the bit-width we are using. The minimum bit-width for computations in fixed point is 19 (20 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 2. The minimum bit-width for parameters updates in fixed point is 19 (20 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 3. In the end, using 19 (20 with the sign) bits for both the computations and the parameters updates has little impact on the final test error, as shown in Table 3. Doubling the number of hidden units does not allow any further reduction of the bit-widths on the permutation invariant MNIST."}, {"heading": "9.3 DYNAMIC FIXED POINT", "text": "Augmenting the maximum overflow rate allows us to reduce the computations bit-width but it also significantly augments the final test error rate, as illustrated in Figure 4. As a result, we use a maximum overflow rate of 0% for the rest of the experiments. The minimum bit-width for the computations in dynamic fixed point is 9 (10 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 2. The minimum bit-width for the parameters updates in dynamic fixed point is 11 (12 with the sign). Below this bit-width, the test set error rate rises very sharply, as illustrated in Figure 3. In the end, using 9 (10 with the sign) bits for the computations and 11 (12 with the sign) bits for the parameters updates has little impact on the final test error, with the exception of the SVHN data set, as shown in Table 3. Doubling the number of hidden units does not allow any further reduction of the bit-widths on the permutation invariant MNIST."}, {"heading": "10 DISCUSSION", "text": "We think there are two main arithmetic difficulties in the training of deep neural networks:\n1. The activations, the gradients and the parameters typically have very different ranges 2. The gradients diminish during the training, so do their ranges\nHalf floating point format easily overcome those two difficulties at the cost of its exponent bit-width (5 bits). Fixed point format saves the exponent bit-width by sharing the exponent between all fixed point variables. However, the global exponent cannot simultaneously be adapted to the activations, the gradients, and the parameters. The resulting bit-width (20 bits) is wider than half precision floating point (16 bits). Dynamic fixed point format is an interesting compromise between floating and fixed point formats. The exponents are shared by groups of variables. The activations, the gradients, and the parameters have different exponents. The gradients exponents can be updated during the training. The resulting bit-width (12 bits) is narrower than half precision floating point (16 bits). Besides, using two bit-widths allows us to spare two additional bits on the computations (10 bits).\nIf very fast low-precision hardware is designed for taking advantage of these results, we conjecture that a high-precision fine-tuning could recover the small degradation brought by the low-precision training."}, {"heading": "11 CONCLUSION", "text": "We have simulated the training of a set of state-of-the-art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those\narithmetics, we have assessed the impact of the precision of the computations on the final error of the training. We have found that very low precision computation is sufficient not just for running trained networks but also for training them. This opens the door to new memory optimizations of deep learning algorithms on general-purpose hardware, and most importantly, this opens the door to very power-efficient training of neural networks on dedicated hardware."}, {"heading": "12 ACKNOWLEDGEMENT", "text": "We thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library which allowed us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow et al., 2013b), a Python library built on the top of Theano which allowed us to easily interface the data sets with our Theano code. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation.", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Scalable learning for object detection with gpu hardware", "author": ["A. Coates", "P. Baumstarck", "Q. Le", "A.Y. Ng"], "venue": "Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, pages 4287\u20134293. IEEE.", "citeRegEx": "Coates et al\\.,? 2009", "shortCiteRegEx": "Coates et al\\.", "year": 2009}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "NIPS\u20192012.", "citeRegEx": "Dean et al\\.,? 2012", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "E. Culurciello", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on, pages 109\u2013116. IEEE.", "citeRegEx": "Farabet et al\\.,? 2011", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS\u20192011.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "Technical report, Universit\u00e9 de Montr\u00e9al.", "citeRegEx": "Goodfellow et al\\.,? 2013a", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Pylearn2: a machine learning research library", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "arXiv preprint arXiv:1308.4214.", "citeRegEx": "Goodfellow et al\\.,? 2013b", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Analysis of high-performance floatingpoint arithmetic on fpgas", "author": ["G. Govindu", "L. Zhuo", "S. Choi", "V. Prasanna"], "venue": "Parallel and Distributed Processing Symposium, 2004. Proceedings. 18th International, page 149. IEEE.", "citeRegEx": "Govindu et al\\.,? 2004", "shortCiteRegEx": "Govindu et al\\.", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical report, arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Back propagation simulations using limited precision calculations", "author": ["J.L. Holt", "T.E. Baker"], "venue": "Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, volume 2, pages 121\u2013126. IEEE.", "citeRegEx": "Holt and Baker,? 1991", "shortCiteRegEx": "Holt and Baker", "year": 1991}, {"title": "What is the best multi-stage architecture for object recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "International Conference on Computer Vision (ICCV\u201909), pages 2146\u20132153. IEEE.", "citeRegEx": "Jarrett et al\\.,? 2009", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "A highly scalable restricted boltzmann machine fpga implementation", "author": ["S.K. Kim", "L.C. McAfee", "P.L. McMahon", "K. Olukotun"], "venue": "Field Programmable Logic and Applications, 2009. FPL 2009. International Conference on, pages 367\u2013372. IEEE.", "citeRegEx": "Kim et al\\.,? 2009", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, University of Toronto.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25 (NIPS\u20192012).", "citeRegEx": "Krizhevsky et al\\.,? 2012a", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS\u20192012.", "citeRegEx": "Krizhevsky et al\\.,? 2012b", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11), 2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Design and implementation of 16-bit fixed point digital signal processor", "author": ["D. Lee", "C. Ryu", "J. Park", "K. Kwon", "W. Choi"], "venue": "SoC Design Conference, 2008. ISOCC\u201908. International, volume 2, pages II\u201361. IEEE.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML\u20192010.", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Neuflow: dataflow vision processing system-on-a-chip", "author": ["Pham", "P.-H.", "D. Jelaca", "C. Farabet", "B. Martini", "Y. LeCun", "E. Culurciello"], "venue": "Circuits and Systems (MWSCAS), 2012 IEEE 55th International Midwest Symposium on, pages 1044\u20131047. IEEE.", "citeRegEx": "Pham et al\\.,? 2012", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "A fixed point implementation of the backpropagation learning algorithm", "author": ["R.K. Presley", "R.L. Haggard"], "venue": "Southeastcon\u201994. Creative Technology Transfer-A Global Affair., Proceedings of the 1994 IEEE, pages 136\u2013138. IEEE.", "citeRegEx": "Presley and Haggard,? 1994", "shortCiteRegEx": "Presley and Haggard", "year": 1994}, {"title": "The impact of arithmetic representation on implementing mlp-bp on fpgas: A study", "author": ["A.W. Savich", "M. Moussa", "S. Areibi"], "venue": "Neural Networks, IEEE Transactions on, 18(1), 240\u2013 252.", "citeRegEx": "Savich et al\\.,? 2007", "shortCiteRegEx": "Savich et al\\.", "year": 2007}, {"title": "Rank, trace-norm and max-norm", "author": ["N. Srebro", "A. Shraibman"], "venue": "Proceedings of the 18th Annual Conference on Learning Theory, pages 545\u2013560. Springer-Verlag.", "citeRegEx": "Srebro and Shraibman,? 2005", "shortCiteRegEx": "Srebro and Shraibman", "year": 2005}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Technical report, arXiv preprint arXiv:1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Fpgas vs", "author": ["K. Underwood"], "venue": "cpus: trends in peak floating-point performance. In Proceedings of the 2004 ACM/SIGDA 12th international symposium on Field programmable gate arrays, pages 171\u2013180. ACM.", "citeRegEx": "Underwood,? 2004", "shortCiteRegEx": "Underwood", "year": 2004}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop.", "citeRegEx": "Vanhoucke et al\\.,? 2011", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Dynamic fixed-point arithmetic design of embedded svm-based speaker identification system", "author": ["Wang", "J.-F.", "Kuan", "T.-W.", "Wang", "J.-C.", "Sun", "T.-W."], "venue": "volume 6064 LNCS, pages 524 \u2013 531, Shanghai, China. Embedded environment;Fixed point arithmetic;Linear prediction cepstral coefficients;Point design;Sequential minimal optimization;Speaker identification;Speaker identification systems;.", "citeRegEx": "Wang et al\\.,? 2010", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Dynamically scaled fixed point arithmetic. pages 315 \u2013 18, New York, NY, USA. dynamic scaling;iteration stages;digital filters;overflow probability;fixed point arithmetic;fixed-point filter", "author": ["D. Williamson"], "venue": null, "citeRegEx": "Williamson,? \\Q1991\\E", "shortCiteRegEx": "Williamson", "year": 1991}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Zeiler and Fergus,? 2013", "shortCiteRegEx": "Zeiler and Fergus", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point.", "startOffset": 91, "endOffset": 117}, {"referenceID": 3, "context": "Lots of previous works address the best exploitation of general-purpose hardware, typically CPU clusters (Dean et al., 2012) and GPUs (Coates et al.", "startOffset": 105, "endOffset": 124}, {"referenceID": 2, "context": ", 2012) and GPUs (Coates et al., 2009; Krizhevsky et al., 2012a).", "startOffset": 17, "endOffset": 64}, {"referenceID": 14, "context": ", 2012) and GPUs (Coates et al., 2009; Krizhevsky et al., 2012a).", "startOffset": 17, "endOffset": 64}, {"referenceID": 3, "context": "Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).", "startOffset": 64, "endOffset": 133}, {"referenceID": 14, "context": "Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).", "startOffset": 64, "endOffset": 133}, {"referenceID": 24, "context": "Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).", "startOffset": 64, "endOffset": 133}, {"referenceID": 4, "context": "FPGA implementations claim a better power efficiency than general-purpose hardware (Farabet et al., 2011; Kim et al., 2009).", "startOffset": 83, "endOffset": 123}, {"referenceID": 12, "context": "FPGA implementations claim a better power efficiency than general-purpose hardware (Farabet et al., 2011; Kim et al., 2009).", "startOffset": 83, "endOffset": 123}, {"referenceID": 20, "context": "The corresponding ASIC implementations are even more efficient (Pham et al., 2012).", "startOffset": 63, "endOffset": 82}, {"referenceID": 8, "context": "For instance, using single precision (32 bits) instead of double precision (64 bits) for a floating point multiplier reduces its area by four on modern FPGAs (Govindu et al., 2004; Underwood, 2004).", "startOffset": 158, "endOffset": 197}, {"referenceID": 25, "context": "For instance, using single precision (32 bits) instead of double precision (64 bits) for a floating point multiplier reduces its area by four on modern FPGAs (Govindu et al., 2004; Underwood, 2004).", "startOffset": 158, "endOffset": 197}, {"referenceID": 6, "context": "In this paper, we simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point.", "startOffset": 106, "endOffset": 132}, {"referenceID": 11, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 18, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 5, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 15, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 6, "context": ", 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al., 2013a).", "startOffset": 126, "endOffset": 152}, {"referenceID": 9, "context": "Combined with dropout, a very effective regularization method (Hinton et al., 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 6, "context": ", 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al., 2013a), both as part of fully connected feedforward deep nets and as part of deep convolutional nets.", "startOffset": 85, "endOffset": 111}, {"referenceID": 5, "context": ", 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al., 2013a). Combined with dropout, a very effective regularization method (Hinton et al., 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al., 2013a), both as part of fully connected feedforward deep nets and as part of deep convolutional nets. The dropout technique is a good approximation of model averaging with shared parameters across an exponentially large number of networks that are formed by subsets of the units of the original noise-free deep network. In our article, we reproduce the experiments of Goodfellow et al. (2013a) on the MNIST, CIFAR10 and SVHN data sets while changing the arithmetic operations and the numerical precision of the computations.", "startOffset": 31, "endOffset": 798}, {"referenceID": 10, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007).", "startOffset": 90, "endOffset": 160}, {"referenceID": 21, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007).", "startOffset": 90, "endOffset": 160}, {"referenceID": 22, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007).", "startOffset": 90, "endOffset": 160}, {"referenceID": 23, "context": "In Vanhoucke et al. (2011), the authors show that the use of 8 bits fixed point arithmetic speeds up over three times the application of a neural network on CPU, compared to floating point.", "startOffset": 3, "endOffset": 27}, {"referenceID": 10, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007). However, those works use very small models and datasets in today standards. For instance, in Savich et al. (2007), the model is a MultiLayer Perceptron with two hidden units and the task is to learn a XOR gate.", "startOffset": 91, "endOffset": 276}, {"referenceID": 28, "context": "Dynamic fixed point arithmetic (Williamson, 1991) is a variant of fixed point arithmetic in which there are several scaling factors instead of a single global one.", "startOffset": 31, "endOffset": 49}, {"referenceID": 28, "context": "Dynamic fixed point arithmetic was introduced several decades ago (Williamson, 1991) and it was recently used for an SVM implementation (Wang et al.", "startOffset": 66, "endOffset": 84}, {"referenceID": 27, "context": "Dynamic fixed point arithmetic was introduced several decades ago (Williamson, 1991) and it was recently used for an SVM implementation (Wang et al., 2010).", "startOffset": 136, "endOffset": 155}, {"referenceID": 1, "context": "We use single floating point format for computation and storage as it allows us to use existing GPU libraries, for instance Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 131, "endOffset": 176}, {"referenceID": 0, "context": "We use single floating point format for computation and storage as it allows us to use existing GPU libraries, for instance Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 131, "endOffset": 176}, {"referenceID": 17, "context": "That being said, in practice, accumulators are often more precise than single floats (Lee et al., 2008).", "startOffset": 85, "endOffset": 103}, {"referenceID": 6, "context": "We reproduce the results of Goodfellow et al. (2013a) on the MNIST, CIFAR10 and SVHN, using single precision floating point arithmetic.", "startOffset": 28, "endOffset": 54}, {"referenceID": 6, "context": "PI MNIST MNIST CIFAR10 SVHN Goodfellow et al. (2013a) 32 32 0.", "startOffset": 28, "endOffset": 54}, {"referenceID": 6, "context": "The single precision floating point line refers to the results of our experiments attempting to reproduce the results of Goodfellow et al. (2013a) without training on the validation samples.", "startOffset": 121, "endOffset": 147}, {"referenceID": 16, "context": "The MNIST (LeCun et al., 1998) data set is described in Table 2.", "startOffset": 10, "endOffset": 30}, {"referenceID": 23, "context": "We regularize the model with dropout and a constraint on the norm of each weight vector, as in (Srebro and Shraibman, 2005).", "startOffset": 95, "endOffset": 123}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples because it requires to stop the training at an arbitrary epoch, which adds some randomness to the final test error, which blurs the impact of low-precision arithmetic.", "startOffset": 33, "endOffset": 59}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples because it requires to stop the training at an arbitrary epoch, which adds some randomness to the final test error, which blurs the impact of low-precision arithmetic. As a result, our test error is slightly bigger that the one reported in Goodfellow et al. (2013a). The final test error is in Table 3.", "startOffset": 33, "endOffset": 390}, {"referenceID": 13, "context": "The CIFAR10 (Krizhevsky and Hinton, 2009) data set is described in Table 2.", "startOffset": 12, "endOffset": 41}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples.", "startOffset": 33, "endOffset": 59}, {"referenceID": 19, "context": "The SVHN (Netzer et al., 2011) data set is described in Table 2.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "The SVHN (Netzer et al., 2011) data set is described in Table 2. We applied local contrast normalization preprocessing the same way as Zeiler and Fergus (2013). The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer.", "startOffset": 10, "endOffset": 160}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a). The final test error is in Table 3.", "startOffset": 33, "endOffset": 59}, {"referenceID": 6, "context": "We have simulated the training of a set of state-of-the-art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point.", "startOffset": 97, "endOffset": 123}], "year": 2014, "abstractText": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with around 10 bits for computing activations and gradients, and 12 bits for storing updated parameters.", "creator": "LaTeX with hyperref package"}}}