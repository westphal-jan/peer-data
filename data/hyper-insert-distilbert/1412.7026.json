{"id": "1412.7026", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Language Recognition using Random Indexing", "abstract": "mathematical random indexing is a quantum simple implementation of random projections connected with a particular wide range suite of applications. it so can solve a variety of problems constructed with good accuracy together without introducing this much complexity. here we cannot use doing it solely for identifying the language strengths of text distribution samples. we present a now novel method of initially generating language representation composite vectors using letter blocks. further, we show that the decision method is seemingly easily implemented and requires indeed little computational effort power and space. combining experiments on a number ranges of model validity parameters will illustrate certain properties about high dimensional globally sparse mixed vector representations of data. proof mechanisms of recognizing statistically relevant minority language vectors are shown through the extremely high success factor of various language recognition tasks. emphasis on surveying a technically difficult data density set of 21, 000 diverse short sentences interpreted from 21 fundamentally different languages, testing our consensus model performs a limited language recognition task completely and actually achieves 97. 8 % accuracy, comparable tasks to prior state - in of - the - art modelling methods.", "histories": [["v1", "Mon, 22 Dec 2014 15:34:43 GMT  (345kb,D)", "http://arxiv.org/abs/1412.7026v1", "9 pages, 3 figures, 5 tables, ICLR 2015"], ["v2", "Fri, 27 Feb 2015 08:02:49 GMT  (443kb,D)", "http://arxiv.org/abs/1412.7026v2", "7 pages, 1 figures, 2 tables, ICLR 2015"]], "COMMENTS": "9 pages, 3 figures, 5 tables, ICLR 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["aditya joshi", "johan halseth", "pentti kanerva"], "accepted": false, "id": "1412.7026"}, "pdf": {"name": "1412.7026.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RANDOM INDEXING", "Aditya Joshi", "Johan T. Halseth"], "emails": ["adityajoshi@berkeley.edu", "halseth@berkeley.edu", "pkanerva@cberkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "As humans who communicate through language, we have the fascinating ability to recognize unknown languages in spoken or written form, using simple cues to distinguish one language from another. Some unfamiliar languages, of course, might sound very similar, especially if they come from the same language family, but we are often able to identify the language in question with very high accuracy. This is because embedded within each language are certain features that clearly distinguish one from another, whether it be accent, rhythm, or pitch patterns. The same can be said for written languages, as they all have features that are unique. Recognizing the language of a given text is the first step in all sorts of language processing, such as text analysis, categorization, translation and much more.\nAs popularized by Shannon (1948), most language models use distributional statistics to explain structural similarities in various specified languages. The traditional method of identifying languages consists of counting individual letters, letter bigrams, trigrams, etc., and comparing the frequency profiles of different text samples. As a general principle, the more accurate you want your detection method to be, the more data you have to store about the various languages. For example, Google\u2019s recently open-sourced program called Chromium Compact Language Detector uses large language profiles built from enormous corpora of data. As a result, the accuracy of their detection, as seen through large-scale testing and in practice, is near perfect McCandless (2011).\nar X\niv :1\n41 2.\n70 26\nv1 [\ncs .C\nL ]\nHigh-dimensional vector models are popular in natural-language processing and are used to capture word meaning from word-use statistics. The vectors are called semantic vectors or context vectors. Ideally, words with a similar meaning are represented by context vectors that are close to each other in the vector space, while dissimilar meanings are represented by context vectors far from each other. Latent Semantic Analysis is a well-known model that is explained in detail in Landauer & Dumais (1997). It produces 300-dimensional (more or less) semantic vectors from a singular value decomposition (SVD) of a matrix of word frequencies in a large collection of documents. An alternative to SVD based on Random Projections was proposed by Papadimitriou et al. and Kaski (1998). Random Indexing Kanerva et al. (2000); Sahlgren (2005) is a simple and effective implementation of the idea.\nIn this paper, we will present a way of doing language detection using Random Indexing, which is fast, highly scalable, and space efficient. We will also present some results regarding the accuracy of the method, even though this will not be the main goal of this paper and should be investigated further."}, {"heading": "2 RANDOM INDEXING", "text": "Random Indexing stores information by projecting data onto vectors in a hyperdimensional space. There exist a huge number of different, nearly orthogonal vectors in such a space (Kanerva, 1988, p. 19). This lets us combine two such vectors into a new vector using well-defined vector space operations, while keeping the information of the two with high probability. In our implementation of Random Indexing, we use a variant of the MAP (Multiply, Add, Permute) coding described in Levy & Gayler (2009) to define the hyperdimensional vector space. Vectors are initially taken to be from a D-dimensional space, where the vector elements are from the set {\u22121, 0, 1}. Such vectors are used to represent the basic elements of the system, which in our case are letters of the alphabet. The number of \u22121 elements and 1 elements are both k, so the total number of non-zero elements in such a vector is 2k. The smaller k is, the more sparse the vector is. High k leads to dense vectors.\nThe binary operations on such vectors are defined as follows. Elementwise addition of two vectorsA andB, is denoted byA+B. Similar, elementwise multiplication is denoted byA\u2217B. For maximally dense vectors (2k = D), a vector A will be its own multiplicative inverse, A \u2217 A = 1, where 1 is the D-dimensional identity vector consisting of only 1\u2019s. Cosine angles are used to measure the similarity of two vectors. It is defined as cos(A,B) = |A\u2032 \u2217 B\u2032|, where A\u2032 and B\u2032 are the normalized vectors of A and B, respectively, and |C| denotes the sum of the elements in C.\nInformation from both vectors A and B are stored and utilized by exploiting the summation operation. That is, the sum of two separate vectors naturally preserves unique information from each vector because of the mathematical properties of the hyperdimensional space. To see this, note that cos(A,A) = 1, while for all B 6= A, cos(A,B) < 1. For both sparse and dense vectors, the cosine of two random, uncorrelated vectors tend to be close to 0. Because of this, the vectorB can easily be found in the vectorA+B: cos(B,A+B) differs significantly from 0.\nFor storing sequences of vectors, we use a random (but fixed throughout all our computations) permutation operation \u03c1 of the vectors. Hence, the sequence A-B-C, is stored as the vector (\u03c1((\u03c1A) \u2217 B)) \u2217 C = \u03c1\u03c1A \u2217 \u03c1B \u2217 C. This efficiently distinguishes the sequence A-B-C from, say, A-C-B. This can be seen from looking at their cosine (here c is the normalization factor):\nV1 = \u03c1\u03c1A \u2217 \u03c1B \u2217 C V2 = \u03c1\u03c1A \u2217 \u03c1C \u2217B\n=\u21d2 cos(V1, V2) = c \u00b7 |(\u03c1\u03c1A \u2217 \u03c1B \u2217 C) \u2217 (\u03c1\u03c1A \u2217 \u03c1C \u2217B)| = c \u00b7 |\u03c1\u03c1A \u2217 \u03c1\u03c1A \u2217 \u03c1B \u2217 \u03c1C \u2217 C \u2217B)| = c \u00b7 |\u03c1\u03c1(A \u2217A) \u2217 \u03c1(B \u2217 C) \u2217 (B \u2217 C))| \u2248 c \u00b7 0\nsince a random permutation \u03c1V1 of a vector V1 is uncorrelated to V2."}, {"heading": "2.1 RANDOM INDEXING FOR TEXTS OF DIFFERENT LANGUAGES", "text": "We use the properties of hyperdimensional vectors to extract certain properties of text. Kanerva (2014) shows how Random Indexing can be used for efficiently storing a word as it appears in a text. We show how to use a similar strategy for recognizing a text\u2019s language from creating a Text Vector for the text, and comparing the similarity of this vector to precomputed Language Vectors.\nSimple language recognition can be done by comparing letter frequencies of a given text to known letter frequencies of languages. Given enough text, a text\u2019s letter distribution will approach the letter distribution of the language in which the text was written. The phenomenon is called an \u201dergodic\u201d process in Shannon (1948), as borrowed from similar ideas in physics and thermodynamics. This can be generalized to using letter blocks of different sizes. By a block of size n, we mean n consecutive letters in the text, ignoring spacing, so that a text of length m would have m\u2212 n+ 1 blocks. When the letters are taken in the order in which they appear in the text, they are referred to as a sequence (of length n) or an n-gram. By an \u201dunordered\u201d block we mean that the n letters are treated as a multiset.\nAs an example, the text \u201da dark night\u201d will have the letter blocks ad, da, ar, rk, kn, ni, ig, gh and ht, when we look at blocks of size 2. Still, frequencies of such letter blocks can be found for a text and compared to known frequencies for different languages. For texts in languages using the Latin alphabet of 26 letters, like English, this would lead to keeping track of 262 = 676 or( 25+2\n2\n) = 378 different block frequencies, for ordered and unordered blocks, respectively. For\narbitrary alphabets of l letters, there are ln ordered n-blocks and ( (l\u22121)+n\nn\n) unordered blocks. These\nnumbers grow quickly as the block size increases.\nThe Random Indexing approach for doing language recognition is similar. A text\u2019s Text Vector is first calculated by running over all the blocks of size n within the text, creating a Block Vector for each. A Block Vector is created by storing the sequence of letters in that block, as described earlier. If we want to look at unordered blocks (instead of sequences), we just sort the letters alphabetically before doing the same procedure. As an example, if we encounter the block \u201drab\u201d, its Block Vector is calculated by performing \u03c1\u03c1R + \u03c1A + B, where R, A and B are the Letter Vectors for a, b and r. A Letter Vector is a random D-dimensional vector having k 1\u2019s and k \u22121\u2019s. The Block Vector for the unordered block would be \u03c1\u03c1A+ \u03c1B +R.\nA text\u2019s Text Vector is now obtained from summing all the Block Vectors for the blocks in the text. This is still an D-dimensional vector and can be stored efficiently. Language Vectors are made in exactly the same way, making Text Vectors for sample texts of a known language. Determining the language of a text is done by finding the Language Vector that gives the highest cosine angle between the Text Vector of the unknown text. More precisely, the cosine angle measure dcos between a language vector X and an unknown text vector V is defined as follows.\ndcos(X,V ) = X \u00b7 V |X||V | = \u2211D i=1 xivi\u221a\u2211D\nj=1 x 2 j \u2211D k=1 v 2 k\nIf the cosine angle is high (close to 1), the block frequencies of the text are similar to the block frequencies of that language and thus, the text is likely to be written in the same language."}, {"heading": "2.2 COMPLEXITY", "text": "The outlined algorithm for Text Vector generation can be implemented efficiently. For generating a Block Vector for a block of size n, n \u2212 1 additions and permutations are performed. This takes time O(n \u00b7 D). Looping over a text of m letters, O(m) Block Vectors must be created and added together. This clearly implies an O(n \u00b7D \u00b7m) implementation. This can be improved to O(D \u00b7m) by noting that most of the information needed for creating the Block Vector for the next block is already contained in the previous Block Vector, and can be retrieved by removing the contribution from the letter that is now no longer in the block.\nSay we have the Block Vector A = \u03c1(n\u22121)V1 \u2217 \u03c1(n\u22122)V2 \u2217 . . . \u2217 \u03c1Vn\u22121 \u2217 Vn for block number i, and now want to find the Block Vector B for block i + 1. We remove from A the vector\n\u03c1(n)V1 by multiplying with it\u2019s inverse (which is the vector itself), which we can do in O(D) time since \u03c1(n\u22121) is just another (pre-calculated) permutation. Then we permute the result once using \u03c1 and multiply that with the Letter Vector Vn+1 for the new letter in the block. This gives us the new Block Vector\nB = \u03c1(\u03c1(n\u22121)V1 \u2217A) \u2217 Vn+1 = \u03c1(\u03c1(n\u22122)V2 \u2217 . . . \u2217 \u03c1Vn\u22121 \u2217 Vn) \u2217 Vn+1 = \u03c1(n\u22121)V2 \u2217 . . . \u2217 \u03c1(2)Vn\u22121 \u2217 \u03c1Vn \u2217 Vn+1\nwhich let\u2019s us create Block Vectors for arbitrary size blocks without adding complexity. It should be noted that to be able to do this safely (that is, not multiply to a zero vector with high probability), the vectors need to be dense."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "The algorithm outlined in the previous section was implemented Joshi & Halseth (2014), and used to create Language Vectors for 21 languages. Depending on the task, text samples were taken from Project Gutenberg Hart, where texts in a number of languages can be found, or the Wortschatz CorporaQuastoff et al. (2006), where large numbers of sentences in select languages can be easily downloaded. One randomly chosen text or set of sentences of each language was used to create the Language Vector of that language.\nTo determine the algorithm\u2019s ability to distinguish two languages, we define the similarity function of two Language Vectors, fcorr. The similarity is defined as the arcsine of the cosine angles between two Language Vectors. That is, fcorr(LA, LB) = arcsin(dcos(LA, LB)), where LA and LB are the two Language Vectors. The arcsine function further spreads the cosine angle data near 1 where small differences in cosine are more significant than near 0. Thus, for the algorithm\u2019s overall ability to cluster languages, we look at the variance of the similarity data. If the variance of the data is high, the similarity between pairs of languages is spread out, with highly correlated languages forming clusters.\nTwo key parameters chosen for the algorithm are the D, the dimensionality of the Text Vectors, and k, a value denoting half the sparsity of the letter vectors used to build the text vectors. Using the variance as a heuristic measure of the algorithm\u2019s performance, we test over varying values of D and k and determine that vectors in higher dimensions that are more sparse are better. Results of this can be seen in Figure 1.\nWith algorithms to automatically detect languages, methods to separate languages should also preserve certain relations between languages. Intuitively, Language Vectors from languages of related language families should be closer than languages from unrelated language families. Indeed, the hyperdimensional Language Vectors roughly preserve such clustering amongst languages, as seen in Figure 2. This result shows that the relatively small amount of information used to generate Language Vectors well represents the distributional statistics of a language.\nTo get a rough estimate of how well the actual detection algorithm worked, we ran tests on the Europarl Parallel Corpus, described in Nakatani. This corpus has 1000 samples of text for each language (21 of them), and each such sample is a sentence. Our Language Vectors were built from sentences in the Wortschatz corpora by the University of Leipzig Quastoff et al. (2006). The results with different parameters are shown in Tables 1-4.\nFrom the results we can clearly see that using dense vectors is important when encoding blocks of more than one letter. This makes sense, since multiplying two sparse vectors results in an even sparser vector. For large block sizes, this means that the encoding of the block most likely will end up as a zero-vector. Also, we see that vectors of size D = 5, 000 are enough to mostly capture the block frequencies of the languages, and going to D = 10, 000 dimensions does not make much of a difference.\nIn Table 5, we show the results of running the exact same experiment retaining the spacing between words in the input text. That means we are now looking at text represented using 27 different characters. As we can see this improves the accuracy of the method, and we are able to guess the correct language with 97.8% accuracy. This should be explored further, as explained in the Future Work section.\nAnother interesting observation is that a high variance in the pairwise Language Vector cosines does not necessarily mean that the algorithm will have high success in language detection accuracy. This is better shown in Figures 2, where sparse and dense vectors were used to generate Language Vectors, and t-sne van der Maaten (2008) was used to project the data from the hyperdimensional space to a 2 dimensional image. More accurate clustering is seen in the visualization made with dense vectors.\nA final observation is that because more accurate clustering is shown with dense hyperdimensional vectors, mistakes made by the language detection algorithm are not bad. By this, we mean that wrongly guessed languages are very often from the same language family as the correct language. This is further elaborated in Figure 3."}, {"heading": "4 FUTURE WORK", "text": "Many improvements can be made to refine the efficacy of Random Indexing on language detection, from small tweaks to large, new applications. Inspired by the improved accuracy achieved by retaining spaces in the input text, one could try to retain even more of the characters in the input. Another approach could be to encode words or pairs of words to capture characteristics of the text using the same idea as when creating Block Vectors. Additionally, one could encode the sequential data using a combination of the MAP operations.\nBecause of the generality of Random Indexing on texts, if sequential information is encoded successfully, one could try to generate text using Language Vectors. Further, one does not have to stick to text. Any time series data with well distributed statistics can be well encoded with Random Indexing using this scheme. In this way, we propose that our method, with a well-made \u201dalphabet\u201d, can be used to do language detection in speech data, addressing our original problem."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Bruno Olshausen, Mayur Mudigonda, and many others at the Redwood Center for Theoretical Neuroscience for insightful discussions and feedback."}], "references": [{"title": "Github: Random indexing for languages python implementation, 2014. URL https://github.com/halseth/vs265_project_f14", "author": ["A. Joshi", "J.T. Halseth"], "venue": null, "citeRegEx": "Joshi and Halseth,? \\Q2014\\E", "shortCiteRegEx": "Joshi and Halseth", "year": 2014}, {"title": "Sparse Distributed Memory", "author": ["P. Kanerva"], "venue": null, "citeRegEx": "Kanerva,? \\Q1988\\E", "shortCiteRegEx": "Kanerva", "year": 1988}, {"title": "Computing with 10,000-bit", "author": ["P. Kanerva"], "venue": "words. Proc. 52nd Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "Kanerva,? \\Q2014\\E", "shortCiteRegEx": "Kanerva", "year": 2014}, {"title": "Random indexing of text samples for latent semantic analysis. pp. \u201d1036", "author": ["P. Kanerva", "J. Kristoferson", "A. Holst"], "venue": "Proc. 22nd Annual Conference of the Cognitive Science Society,", "citeRegEx": "Kanerva et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kanerva et al\\.", "year": 2000}, {"title": "Dimensionality reduction by random mapping: Fast similarity computation for clustering", "author": ["S. Kaski"], "venue": "Proc. IJCNN\u201998, International Joint Converence on Neural Networks,", "citeRegEx": "Kaski,? \\Q1998\\E", "shortCiteRegEx": "Kaski", "year": 1998}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge", "author": ["T. Landauer", "S. Dumais"], "venue": "Psychology Review,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "lateral inhibition in a fully distributed connectionist architecture", "author": ["S.D. Levy", "R.W. Gayler"], "venue": "Proceedings of the Ninth International Conference on Cognitive Modeling,", "citeRegEx": "Levy and Gayler,? \\Q2009\\E", "shortCiteRegEx": "Levy and Gayler", "year": 2009}, {"title": "Accuracy and performance of google\u2019s compact language detector", "author": ["M. McCandless"], "venue": null, "citeRegEx": "McCandless,? \\Q2011\\E", "shortCiteRegEx": "McCandless", "year": 2011}, {"title": "langdetect is updated(added profiles of Estonian / Lithuanian / Latvian / Slovene, and so on. http://shuyo.wordpress.com/2011/09/29/langdetect-is-updatedadded-profiles-of-estonianlithuanian-latvian-slovene-and-so-on/. [Online; accessed 16-December-2014", "author": ["S. Nakatani"], "venue": null, "citeRegEx": "Nakatani,? \\Q2014\\E", "shortCiteRegEx": "Nakatani", "year": 2014}, {"title": "Corpus portal for search in monolingual corpora", "author": ["U. Quastoff", "M. Richter", "C. Biemann"], "venue": "Proceedings of the fifth international conference on Language Resources and Evaluation,", "citeRegEx": "Quastoff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Quastoff et al\\.", "year": 2006}, {"title": "An introduction to random indexing", "author": ["M. Sahlgren"], "venue": "Methods and Applications of Semantic Indexing Workshop at the 7th international conference on Terminology and Knowledge Engineering,", "citeRegEx": "Sahlgren,? \\Q2005\\E", "shortCiteRegEx": "Sahlgren", "year": 2005}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal,", "citeRegEx": "Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L. van der Maaten"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten,? \\Q2008\\E", "shortCiteRegEx": "Maaten", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "As popularized by Shannon (1948), most language models use distributional statistics to explain structural similarities in various specified languages.", "startOffset": 18, "endOffset": 33}, {"referenceID": 7, "context": "As a result, the accuracy of their detection, as seen through large-scale testing and in practice, is near perfect McCandless (2011).", "startOffset": 115, "endOffset": 133}, {"referenceID": 1, "context": "and Kaski (1998). Random Indexing Kanerva et al.", "startOffset": 4, "endOffset": 17}, {"referenceID": 1, "context": "Random Indexing Kanerva et al. (2000); Sahlgren (2005) is a simple and effective implementation of the idea.", "startOffset": 16, "endOffset": 38}, {"referenceID": 1, "context": "Random Indexing Kanerva et al. (2000); Sahlgren (2005) is a simple and effective implementation of the idea.", "startOffset": 16, "endOffset": 55}, {"referenceID": 1, "context": "There exist a huge number of different, nearly orthogonal vectors in such a space (Kanerva, 1988, p. 19). This lets us combine two such vectors into a new vector using well-defined vector space operations, while keeping the information of the two with high probability. In our implementation of Random Indexing, we use a variant of the MAP (Multiply, Add, Permute) coding described in Levy & Gayler (2009) to define the hyperdimensional vector space.", "startOffset": 83, "endOffset": 406}, {"referenceID": 1, "context": "Kanerva (2014) shows how Random Indexing can be used for efficiently storing a word as it appears in a text.", "startOffset": 0, "endOffset": 15}, {"referenceID": 11, "context": "The phenomenon is called an \u201dergodic\u201d process in Shannon (1948), as borrowed from similar ideas in physics and thermodynamics.", "startOffset": 49, "endOffset": 64}, {"referenceID": 9, "context": "Depending on the task, text samples were taken from Project Gutenberg Hart, where texts in a number of languages can be found, or the Wortschatz CorporaQuastoff et al. (2006), where large numbers of sentences in select languages can be easily downloaded.", "startOffset": 152, "endOffset": 175}, {"referenceID": 12, "context": "Hyperdimensional Language Vectors were projected onto a 2 dimensional space using t-sne van der Maaten (2008). Language Vectors were generated with random sentences from the Wortschatz Corpora.", "startOffset": 96, "endOffset": 110}, {"referenceID": 8, "context": "To get a rough estimate of how well the actual detection algorithm worked, we ran tests on the Europarl Parallel Corpus, described in Nakatani. This corpus has 1000 samples of text for each language (21 of them), and each such sample is a sentence. Our Language Vectors were built from sentences in the Wortschatz corpora by the University of Leipzig Quastoff et al. (2006). The results with different parameters are shown in Tables 1-4.", "startOffset": 134, "endOffset": 374}, {"referenceID": 12, "context": "This is better shown in Figures 2, where sparse and dense vectors were used to generate Language Vectors, and t-sne van der Maaten (2008) was used to project the data from the hyperdimensional space to a 2 dimensional image.", "startOffset": 124, "endOffset": 138}], "year": 2017, "abstractText": "Random Indexing is a simple implementation of Random Projections with a wide range of applications. It can solve a variety of problems with good accuracy without introducing much complexity. Here we use it for identifying the language of text samples. We present a novel method of generating language representation vectors using letter blocks. Further, we show that the method is easily implemented and requires little computational power and space. Experiments on a number of model parameters illustrate certain properties about high dimensional sparse vector representations of data. Proof of statistically relevant language vectors are shown through the extremely high success of various language recognition tasks. On a difficult data set of 21,000 short sentences from 21 different languages, our model performs a language recognition task and achieves 97.8% accuracy, comparable to state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}