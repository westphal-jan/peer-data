{"id": "1412.7091", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets", "abstract": "yet an important computing class 3 of problems involves explicitly training deep neural filter networks complete with normal sparse word prediction targets of and very rare high theoretical dimension d. alternatively these occur naturally in e. frank g. neural language models or the learning of word - embeddings, often better posed as predicting the probability of next words written among a vocabulary of relatively size d ( e. g. 200 000 ). computing the equally large, but typically weighted non - sparse d - dimensional database output vector input from a last hidden layer of unusually reasonable dimension d ( e. | g. 500 ) incurs a prohibitive weighted o ( dd ) computational design cost requirements for each experimental example, taking as does optimization updating the d x d output weight ; matrix overhead and computing the predicted gradient time needed for backpropagation to previous layers. while sufficiently efficient handling testing of large vector sparse network resource inputs is invariably trivial, consequently this extreme case of ultra large sparse predict targets is not, systems and hardware has thus thus so now far been sidestepped with fewer approximate alternatives such as hierarchical filter softmax implementations or sampling - based complexity approximations during candidate training. in this work methodology we develop an original algorithmic computation approach that, for adding a family basis of intrinsic loss functions strategies that includes squared error and also spherical biased softmax, can continuously compute the exact loss, gradient update search for the minimum output weights, and gradient query for scaled backpropagation, all in 1000 o ( d ^ 2 ) per example chosen instead of o ( 610 dd ), improving remarkably slowly without iteration ever computing fully the hypothetical d - dimensional estimated output. the proposed comparison algorithm is apparently expected to gradually yield an actual speedup of at quite least d / 4d, but i. by e. yields two orders of great magnitude for typical component sizes, not for predicting that critical percentage part accuracy of the matrix computations that appears often dominates below the training performance time allocated in pursuing this 3 kind section of traditional network architecture.", "histories": [["v1", "Mon, 22 Dec 2014 18:51:08 GMT  (14kb)", "http://arxiv.org/abs/1412.7091v1", null], ["v2", "Sat, 11 Apr 2015 04:02:12 GMT  (14kb)", "http://arxiv.org/abs/1412.7091v2", null], ["v3", "Tue, 14 Jul 2015 01:27:13 GMT  (1498kb,D)", "http://arxiv.org/abs/1412.7091v3", "15 pages technical report version"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["pascal vincent", "alexandre de br\u00e9bisson", "xavier bouthillier"], "accepted": true, "id": "1412.7091"}, "pdf": {"name": "1412.7091.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vincentp@iro.umontreal.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n70 91\nv1 [\ncs .N\nE ]\n2 2\nD ec"}, {"heading": "1 INTRODUCTION", "text": "Many modern applications of neural networks have to deal with data represented, or representable, as very large sparse vectors. Such representations arise in natural language related tasks, where the dimensionD of that vector is typically (a multiple of) the size of the vocabulary, but also in the sparse user-item matrices of collaborative-filtering applications. It is trivial to handle very large sparse inputs to a neural network in a computationally efficient manner: the forward propagation and update to the input weight matrix after backpropagation are correspondingly sparse. By contrast, training with very large sparse prediction targets is problematic: even if the target is sparse, the computation of the equally large network output and the corresponding gradient update to the huge output weight matrix are not sparse and thus computationally prohibitive. This has been a practical problem ever since Bengio et al. (2001) first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications. Several approaches have been proposed that attempt to address this difficulty essentially by sidestepping it. They fall in two categories:\n\u2022 Sampling based approximations consider and compute only a tiny fraction of the output\u2019s dimensions sampled at random. The reconstruction sampling of Dauphin et al. (2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al. (2013) fall under this category.\n\u2022 Hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013) imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.\nCompared to the initial problem of considering all D output dimensions, both kinds of approaches are crude approximations. In the present work, we will instead investigate a way to actually perform the exact gradient update that corresponds to considering all D outputs, but does so implicitly, in a computationally efficient manner, without actually computing the outputs. This approach works for a relatively restricted class of loss functions, the simplest of which is linear output with squared error (a natural choice for sparse real-valued regression targets). The most common choice for multiclass classification, the softmax loss is not part of that class, but we may use an alternative spherical softmax, which will also yield normalized class probabilities. For simplicity, our presentation from now on will focus on squared error and on an online setting, and we will only later discuss how this can be extended to minibatches and to a larger class of loss functions."}, {"heading": "2 THE PROBLEM", "text": ""}, {"heading": "2.1 PROBLEM DEFINITION AND SETUP", "text": "We are concerned with gradient-descent based training of a deep feed-forward neural network with target vectors of very high dimension D (e.g. D = 200 000) but that are sparse, i.e. a comparatively small number, at most K \u226a D, of the elements of the target vector are non-zero. Such a K-sparse vector will typically be stored and represented compactly as 2K numbers correpsonding to pairs (index, value). A network to be trained with such targets will naturally have an equally large output layer of dimensionD. We can also optionally allow for the input to the network to be a similarly high dimensional sparse vector of dimension Din. Besides the large sparse target, output, and (optionally large sparse) input, we suppose the network\u2019s intermediate hidden layers will be of a smaller, more typically manageable, dimension d \u226a D (e.g. d = 500)1.\nNotation Vectors are denoted using lower-case letters, e.g. x, y, u, v\nVectors are considered column-vectors; corresponding row vectors are denoted with a transpose, e.g. uT\nMatrices are denoted using upper-case letters, e.g. U , V , W\nWT is the transpose of W .\nWi is the ith column of W\nW:i is the ith row of W (viewed as a column vector).\nU\u2212T = ( U\u22121 )T denotes the transpose of the inverse of a square matrix U .\nId is the d\u00d7 d identity matrix.\nArchitecture\n\u2022 An input vector x \u2208 RDin is linearly transformed into a linear activation a(1) = W (1)Tx+ b(1) through a Din \u00d7 d input weight matrix W (1) (and an optional bias vector b(1) \u2208 Rd). This is typically followed by a non-linear transformation s to yield the representation of the first hidden layer h(1) = s(a(1)).\n\u2022 This first hidden layer representation is then similarly transformed through a number of subsequent non-linear layers (that can be of any usual kind amendable to backpropagation)\n1Our approach does not impose any restriciton on the architecture nor size of the hidden layers, as long as they are amendable to usual gradient backpropagation. In particular hidden layers may differ in the specific operation they perform and in their size. However, only to simplify notations, in our presentation all hidden layers will be considered to have size d.\ne.g. h(k) = s(W (k)Th(k\u22121) + b(k) \ufe38 \ufe37\ufe37 \ufe38\na(k)\n) until we obtain last hidden layer representation h =\nh(m).\n\u2022 We obtain the final D-dimensional network output as o = Wh where W is a D\u00d7 d output weight matrix, which will be our main focus in this work2\n\u2022 The network\u2019s D-dimensional output o is compared to the D-dimensional target vector y associated with input x using squared error, yielding loss L = \u2016o\u2212 y\u20162.\nTraining procedure The above presented architecture is a typical (possibly deep) multi-layer feed forward neural network architectrue with a linear output layer and squared error loss. Its parameters (weight matrices and bias vectors) will usually be trained by gradient descent, using gradient backpropagation to efficiently compute the gradients . Given an example from the training set as an (input,target) pair (x,t), a pass of forward propagation proceeds as outlined above, computing the hidden representation of each hidden layer in turn based on the previous one, and finally the network\u2019s predicted output o and associated loss L. A pass of gradient backpropagation then works in the opposite direction, starting from \u2207o = \u2202L\u2202o = 2(o \u2212 y) and propagating back the gradients \u2207h(k) = \u2202L \u2202h(k) and \u2207a(k) = \u2202L \u2202a(k) upstream through the network. The corresponding gradient contributions on parameters (weights and biases), collected along the way, are straightforward once we have the associated \u2207a(k) . Specifically they are \u2207b(k) = \u2207a(k) and \u2207W (k) = h (k\u22121)(\u2207a(k)) T . Similarly for the input layer \u2207W (1) = x(\u2207a(1) ) T , and for the output layer\u2207W = (o\u2212y)hT . Parameters are then updated through a gradient descent stepW (k) \u2190 W (k)\u2212\u03b7\u2207W (k) and b (k) \u2190 b(k)\u2212\u03b7\u2207b(k) , where \u03b7 is a positive learning-rate. Similarly for the output layer whch will be our main focus here: W \u2190 W \u2212 \u03b7\u2207W ."}, {"heading": "2.2 THE EASY PART: INPUT LAYER FORWARD PROPAGATION AND WEIGHT UPDATE", "text": "It is easy and straightforward to efficiently compute the forward propagation, and the backpropagation and weight update part for the input layer when we have a very large Din-dimensional but K\u2212sparse input vector x with appropriate sparse representation. Specifically we suppose x is represented as a pair of vectors u, v of length (at most) K , where u contains integer indexes and v the associated real values of the elements of x such that xi = 0 if i /\u2208 u, and xuk = vk .\nForward propagation through the input layer The sparse representation of x as the positions of K elements together with their value makes it cheap to compute W (1)Tx. Even though W (1) may be a huge full Din \u00d7 d matrix, only K of its rows (those corresponding to the non-zero entries of x) need to be visited and summed to compute W (1)Tx. Precisely, with our (u, v) sparse representaiton of x this operation can be written as\nW (1)Tx =\nK\u2211\nk=1\nvkW (1) :uk\nwhere each W (1):uk is a d-dimensional vector, making this an O(Kd) operation rather than O(Dd).\nGradient and update through input layer Let us for now suppose that we were able to get gradients (through backpropagation) up to the first hidden layer activations a(1) \u2208 Rd in the form of gradient vector \u2207a(1) = \u2202L \u2202a(1)\n. The corresponding gradient-based update to input layer weights W (1) is simply:\nW (1) \u2190 W (1) \u2212 \u03b7x(\u2207a(1)) T\n2It is on purpose that we defined our output as o = Wh rather than o = W Th with a transpose, as we did for all previous layer activations; in that way if we have a large sparse input and equally large sparse target, then input and output weight matrices will have the same dimension D \u00d7 d. This makes thinking about the special case of tied input and output weights easier. Also note that we did not explicitly specify an output bias vector. If we want to learn a traditionally placed output bias i.e. added. after the last linear transformation, it is possible to use the old trick of clamping the fist component of last hidden layer h to 1 so that the first column of W plays the role of the output bias vector.\nThis is a rank-one update to W (1). Here again, we see that only the K rows of W (1) associated to the (at most) K non-zero entries of x need to be modified. Precisely this operation can be written as:\nW (1):uk \u2190 W (1) :uk \u2212 \u03b7vk\u2207a(1) \u2200k \u2208 {1, . . . ,K}\nmaking this again a O(Kd) operation rather than O(Dd)."}, {"heading": "2.3 THE HARD PART: OUTPUT LAYER PROPAGATION AND WEIGHT UPDATE", "text": "Given some network input x we suppose we can compute without difficuly through forward propagation the associated last hidden layer representation h \u2208 Rd. From then on:\n\u2022 Computing the final output o = Wh incurs a prohibitive computational cost of O(Dd) since W is a full D\u00d7dmatrix. Note that there is a-priori no reason for representation h to be sparse (e.g. with a sigmoid non-lienarity) but even if it was, this would not fundamentally change the problem since it is D that is extremely large, and we supposed d reasonably sized already.\n\u2022 Computing the residual (o \u2212 t) and associated squared error loss \u2016o \u2212 t\u20162 incurs an additional O(D) cost.\n\u2022 The gradient on h that we need to backpropagate to lower layers is \u2207h = \u2202L\u2202h = 2W T (o\u2212\ny) which is another O(Dd) matrix-vector product.\n\u2022 Finally, when performing the corresponding output weight update W \u2190 W \u2212 \u03b7(o\u2212 y)hT\nwe see that it is a rank-one update that updates all D\u00d7d elements of W which again incurs a prohibitive O(Dd) computational cost.\nFor very large D all these three O(Dd) operations are prohibitive, and the fact that y is sparse, seen from this perspective, doesn\u2019t help, since neither o nor o\u2212 y will be sparse."}, {"heading": "3 A COMPUTATIONALLY EFFICIENT ALGORITHM FOR PERFORMING THE EXACT ONLINE GRADIENT UPDATE", "text": "Previously proposed workarounds are approximate or use stochastic sampling. We propose a different approach that results in the exact same, yet efficient gradient update, remarkably without ever having to compute large output o. The gist of our approach is that, rather than working with W directly, we will instead work with a factorized view of it W = V U where V is a D \u00d7 d matrix (same shape as W ) and U is a smaller d \u00d7 d invertible matrix that we will initialize to the identity. This will allow us, instead of updating W explicitly, to update it implicitly by changing V and/or U , which as we will see, we can achieve in a computationally much more efficient manner. We will also need to maintain up-to-date versions of two d \u00d7 d matrices: U\u22121 and Q = WTW = UTV TV U . This, as we shall see, can be achieved cheaply following rank-one updates to V and/or U ."}, {"heading": "3.1 COMPUTING THE SQUARED ERROR LOSS EFFICIENTLY", "text": "Suppose we have, for a network input example x, computed last hidden representation h \u2208 Rd through forward propagation. The network\u2019s D dimensional output o = Wh is then in principle compared to high dimensional target y \u2208 RD. The corresponding squared error loss is L = \u2016Wh\u2212 y\u2016\n2. As we have seen in Section 2.3, computing it in the direct naive way would have a prohibitive computational complexity of O(Dd +D) = O(Dd) because computing output Wh with a full D \u00d7 d matrix W and a typically non-sparse h is O(Dd). Note however that we can rewrite this as:\nL = \u2016Wh\u2212 y\u2016 2\n= (Wh\u2212 y) T (Wh\u2212 y)\n= hTWTWh\u2212 yTWh\u2212 hTWT y + yT y\n= hTQh\u2212 2hT (WT y) + yT y\n= hTQh\u2212 2hTUTV T y + yT y\n= hT (Qh)\u2212 2hT (UT (V T y)) + yT y\n= hT (Qh \ufe38\ufe37\ufe37\ufe38\nh\u0302\n\u22122(UT (V T y) \ufe38 \ufe37\ufe37 \ufe38\ny\u0302\n) + yT y (1)\nwith Q = WTW\nSupposing we have maintained an up-to-date Q = WTW , which is a compact d \u00d7 d matrix (we will see how we update Q cheaply in section 7), computing h\u0302 = Qh has a complexity of O(d2). Thanks to the K\u2212sparsity and sparse representation of y, computing V T y is O(Kd) and results in a d\u2212dimensional vector, so that computing y\u0302 = UT (V T y) is O(Kd+d2) . The last term is O(K). So the overall computational complexity for computing L in this way is O(Kd+ d2) = O((K + d)d). With K \u226a D and d \u226a D this can be several orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach.\nIf we define intermediate vectors h\u0302 = Qh and y\u0302 = WT y = UT (V T y) the computation of L can be rewritten a little more compactly as\nL = hT (h\u0302\u2212 2y\u0302) + \u2016y\u20162\n3.2 COMPUTING THE GRADIENT ON h EFFICIENTLY\nTo backpropagate the gradient through the network, we need to compute the gradient of loss L with respect to last hidden layer representation h. This is \u2207h = \u2202L\u2202h = \u2202\u2016Wh\u2212y\u20162 \u2202h = 2WT (Wh \u2212 y). Again, if we were to compute it directly in this manner the computational complexity would be a prohibitive O(Dd). But we can instead rewrite it as\n\u2207h = \u2202L\n\u2202h =\n\u2202 \u2016Wh\u2212 y\u2016 2\n\u2202h\n= 2WT (Wh\u2212 y) = 2 ( WTWh\u2212WT y ) = 2 ( Qh\u2212 UTV T y ) = 2 ( Qh\u2212 UT (V T y) ) (2)\n= 2(h\u0302\u2212 y\u0302)\nAgain, supposing we have maintained an up-to-date Q (we will see how we update Q cheaply in section 7) computing \u2202L\n\u2202h this way is O(Kd+ d2) = O((K + d)d), much cheaper than the O(Dd)\nof the direct approach."}, {"heading": "3.3 EFFICIENT GRADIENT UPDATE OF W", "text": "The gradient of the squared error loss with respect to output layer weight matrix W is \u2202L \u2202W = \u2202\u2016Wh\u2212y\u20162\n\u2202W = 2(Wh \u2212 y)hT . And the corresponding gradient descent update to W would be Wnew \u2190 W \u2212 2\u03b7(Wh\u2212 y)h T where \u03b7 is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residue Wh\u2212 y, and then to update all the Dd elements of W (since generally neither Wh\u2212 y nor h will be sparse). To overcome this difficulty let us first rewrite the update as\nWnew = W \u2212 2\u03b7(Wh\u2212 y)h T (3)\n= W \u2212 2\u03b7WhhT + 2\u03b7yhT\nNote that we can decompose this update into two consecutive update steps:\na) W \u2190 W \u2212 2\u03b7WhhT\nb) W \u2190 W + 2\u03b7yhT\nNotice that we can perform each of these updates implicitly by updating only U and V respectively.:\na) Unew = U \u2212 2\u03b7(Uh)h T (4)\nb) Vnew = V + 2\u03b7y(U \u2212T newh) T (5)\nThis results in implicitly updating W as we did explicitly in the naive approach of Eq. 3.\nProof:\nVnewUnew = (V + 2\u03b7y(U \u2212T newh) T )Unew\n= V Unew + 2\u03b7y(U \u2212T newh) TUnew = V Unew + 2\u03b7yh TU\u22121newUnew = V (U \u2212 2\u03b7(Uh)hT ) + 2\u03b7yhT (U\u22121newUnew) = V U \u2212 2\u03b7V UhhT + 2\u03b7yhT\n= V U \u2212 2\u03b7(V Uh\u2212 y)hT\n= W \u2212 2\u03b7(Wh\u2212 y)ThT\n= Wnew\nWe see that the update of U in Eq. 4is a simple O(d2) operation. Following this simple rank-one update to U , we can use the Sherman-Morrison formula to derive the corresponding rank-one update to U\u2212T which will also be O(d2):\nU\u2212Tnew = U \u2212T +\n2\u03b7\n1\u2212 2\u03b7 \u2016h\u2016 2 (U\n\u2212Th)hT (6)\nIt is then easy to compute the U\u2212Tnewh, an O(d 2) operation needed in Eq. 5, and the ensuing rank-one update of V , thanks to the K-sparsity of y is only O(Kd).\nThanks to the K\u2212sparsity and sparse representation of y, computing y\u0302 = V T y is O(Kd) and \u2016t\u2016\n2 is O(K). Computation of h\u0302 = U\u2212Th is O(d2). Given these, the update of Q is O(d2) and the rank-one update of V , thanks to the K-sparsity of y is O(Kd). So these operations together have computational complexity of O(Kd + d2) = O((K + d)d), which is much cheaper than the prohibitive O(Dd) of the direct approach."}, {"heading": "3.4 BOOKKEEPING: KEEPING AN UP-TO-DATE Q AND U\u2212T", "text": "We have already seen, in Eq. 6, how we can cheaply maintain an up-to-date U\u2212T following our update of U .\nSimilarly, following our updates to U and V , we need to keep an up-to-date Q = WTW which is needed to efficiently compute the loss L (Eq. 1) and gradient \u2207h (Eq. 2).\nThe updates to U and V in Equations 4 and 5 are equivalent to implicitly updating W as in Eq. 3, and this translates into the following update to Q = WTW :\nz\u0302 = Qh\u2212 UT (V T y)\nQnew = Q\u2212 2\u03b7 ( hz\u0302T + z\u0302hT ) + (4\u03b72L)hhT (7)\nProof is straightforward but not provided here due to space constraints."}, {"heading": "3.5 PUTTING IT ALL TOGETHER: ALGORITHM FOR COMPUTING THE COST L, GRADIENT ON", "text": "h, AND UPDATING U AND V\nEfficient computation of cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping for U\u2212T and Q. The following table describes the algorithmic steps that we put together from the equations derived above.\nStep # Operation Computational complexity Number of multiply-adds\n1: h\u0302 = Qh O(d2) d2 2: y\u0302 = UT (V T y) O(Kd+ d2) Kd+ d2\n3: z\u0302 = h\u0302\u2212 y\u0302 O(d) d 4: \u2207h = 2z\u0302 O(d) d 5: L = hT h\u0302\u2212 2hT y\u0302 + yT y O(2d+K) 2d+K + 1 6: Unew = U \u2212 2\u03b7(Uh)hT O(d2) 2d2 + d 7: U\u2212Tnew =\nU\u2212T + 2\u03b7 1\u22122\u03b7\u2016h\u20162\n(U\u2212Th)hT O(d2) 2d2 + 2d+ 3\n8: Vnew = V + 2\u03b7y(U\u2212Tnewh) T O(d2 +Kd) d2 +K +Kd 9: Qnew = Q\u2212 2\u03b7 ( hz\u0302T + z\u0302hT ) +\n(4\u03b72L)hhT\nO(d2) 4 + 2d+ 3d2"}, {"heading": "4 DISCUSSION: EXPECTED BENEFITS, EXTENSIONS AND LIMITATIONS", "text": "Having K \u226a d \u226a D we see that the proposed algorithm requires O(d2) operations whereas the standard approach required O(Dd) operations. If we take K \u2248 d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will requires roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overall the proposed algorithm change corresponds to a computational speedup by a factor of D4d . For D = 200 000 and d = 500 the expected speedup is thus 100.\nNote that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and change all D \u00d7 d elements of matrix W , whereas the proposed approach only accesses the much smaller number K\u00d7d element of V as well as the three d\u00d7 d matrices U , U\u2212T , and Q.\nSo overall we have a much faster algorithm, which while doing so implicitly, will however perform the exact same gradient update as the standard approach. We want to emphasize here that what we are doing is not at all the same as simply chaining 2 linear layers U and V and performing ordinary gradient descent updates on these: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to Uand V would not be equivalent to the ordinary gradient update to W = V U .\nOur algorithm can be straightforwardly extended to the minibatch case, and is expected to yield the same speedup factor compared to the standard approach. But one needs to be careful in order to keep the computation of U\u2212Th reasonably efficient. Indeed, depending on the size of the minibatch m, it may be more efficient to resolve the correpsonding linear equation for each minibatch from scratch rather than updating U\u2212T with the Woodbury equation (which generalizes the Sheman-Morrison formula for m > 1). This approach that we detailed for linear output and squared error can easily be extended to slightly more exotic loss functions: basically any loss function that can be expressed using only the oc associated to non-zero yc and \u2016o\u20162 = \u2211 j o 2 j the squared norm of the whole output vector, which we can compute cheaply. This family of loss functions does not include the standard softmax, but includes the so-called spherical softmax: log o 2 c\u2211 j o2 j (where c is the correct class label). It remains to be seen in practice how this approach performs computationally, and whether we lose something due to using this more limited family of loss functions."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research is supported by NSERC."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "In NIPS\u201900,", "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Large-scale learning of embeddings with reconstruction sampling", "author": ["Y. Dauphin", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine learning,", "citeRegEx": "Dauphin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2011}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen"], "venue": "In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201910)", "citeRegEx": "Gutmann and Hyvarinen,? \\Q2010\\E", "shortCiteRegEx": "Gutmann and Hyvarinen", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In International Conference on Learning Representations: Workshops Track", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Morin and Bengio,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "This has been a practical problem ever since Bengio et al. (2001) first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications.", "startOffset": 45, "endOffset": 66}, {"referenceID": 2, "context": "(2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al.", "startOffset": 50, "endOffset": 79}, {"referenceID": 1, "context": "The reconstruction sampling of Dauphin et al. (2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 1, "context": "The reconstruction sampling of Dauphin et al. (2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al.", "startOffset": 31, "endOffset": 157}, {"referenceID": 1, "context": "The reconstruction sampling of Dauphin et al. (2011)and the use of Noise Contrastive Estimation (Gutmann and Hyvarinen, 2010) in Mnih and Kavukcuoglu (2013) and Mikolov et al. (2013) fall under this category.", "startOffset": 31, "endOffset": 183}, {"referenceID": 5, "context": "\u2022 Hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013) imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 69}, {"referenceID": 3, "context": "\u2022 Hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013) imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 69}], "year": 2017, "abstractText": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D \u00d7 d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, this case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach that, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm is expected to yield an actual speedup of at least D 4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "creator": "LaTeX with hyperref package"}}}