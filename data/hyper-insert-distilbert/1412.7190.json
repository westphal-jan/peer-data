{"id": "1412.7190", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Convolutional Neural Networks for joint object detection and pose estimation: A comparative study", "abstract": "in this analytical paper we actually study the application of convolutional neural computation networks for jointly detecting objects depicted in still images and estimating together their 3d pose. recently we thoroughly identify different distinguishing feature representations of image oriented reference objects, capacities and energies that lead a network to learn this specific representations. comparing the choice of incorporating the representation is sometimes crucial since the internal pose domain of modelling an object perception has primarily a natural, continuous facial structure basis while its category is effectively a general discrete state variable. together we routinely evaluate the different empirical approaches on the joint object detection problem and pose error estimation task task of locating the pascal3d + ff benchmark using average viewpoint precision. we show something that aiming a classification quantitative approach on predicting discretized viewpoints partially achieves state - of - the - art modeling performance for designing joint joint object detection and pose estimation, and significantly noticeably outperforms existing baselines on assessing this quantitative benchmark.", "histories": [["v1", "Mon, 22 Dec 2014 22:26:26 GMT  (61kb,D)", "https://arxiv.org/abs/1412.7190v1", null], ["v2", "Fri, 2 Jan 2015 16:43:41 GMT  (61kb,D)", "http://arxiv.org/abs/1412.7190v2", null], ["v3", "Sat, 7 Feb 2015 05:27:24 GMT  (65kb,D)", "http://arxiv.org/abs/1412.7190v3", null], ["v4", "Sat, 28 Feb 2015 05:15:45 GMT  (71kb,D)", "http://arxiv.org/abs/1412.7190v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["francisco massa", "mathieu aubry", "renaud marlet"], "accepted": false, "id": "1412.7190"}, "pdf": {"name": "1412.7190.pdf", "metadata": {"source": "CRF", "title": "A COMPARATIVE STUDY", "authors": ["Francisco Massa", "Mathieu Aubry", "Renaud Marlet"], "emails": ["francisco-vitor.suzano-massa@polytechnique.edu", "mathieu.aubry@imagine.enpc.fr", "renaud.marlet@enpc.fr"], "sections": [{"heading": "1 INTRODUCTION", "text": "Convolutional Neural Networks (CNNs) have long shown impressive and practical results on specialized vision tasks, such as optical character recognition (LeCun et al., 1998; Simard et al., 2003). Thanks to the availability of large, annotated image datasets (Deng et al., 2009) and the increased power of GPU computing, CNNs have recently outperformed other techniques in less constrained computer vision tasks. Following the seminal work of Krizhevsky et al. (2012) on ImageNet classification many methods have been developed to apply CNNs to other tasks. In this work, we explore the potential of CNNs in a specialized task on real world images, namely joint detection and pose estimation in images from the difficult Pascal VOC dataset (Everingham et al., 2012) extended with 3D annotations (Xiang et al., 2014).\nDetecting and estimating the pose of a known object instance is one of the oldest problem of computer vision (Roberts, 1963). In early works, object contours were detected explicitly in images and matched to the contours of a 3D model of the object instance (Mundy, 2006). As the field moved towards object category recognition, methods focused on photographs of the objects and performed detection without explicitly taking into account the 3D structure of the object. These approaches are typically based on manually designed low-level features, such as HOG (Dalal & Triggs, 2005) and SIFT (Lowe, 2004). These features are then aggregated and used as the input of a classifier such as a SVM. Recently, the problem of pose estimation for object categories has drawn attention, and many techniques have been developed (see section 2 for an overview).\nJoint object detection and orientation estimation in real images is difficult because it requires features with two conflicting properties. First, estimating orientation requires a high level of discriminative power, since for example the rotation of an airplane by 10 degrees will not change its appearance much. Second, the features need to cope with strong appearance changes since objects may be occluded or their appearance may change, e.g. because of the intra-class variability of the categories or illumination effects. Moreover, the amount of training data for this task is very limited because\nar X\niv :1\n41 2.\n71 90\nv4 [\ncs .C\nV ]\n2 8\nFe b\n20 15\nof both the difficulty of annotating object orientation and the fact that some viewpoints are less common (for example a TV monitor is rarely seen from behind).\nTo deal with the lack of training data, following Oquab et al. (2014) and He et al. (2014), we use a CNN pre-trained on ImageNet (Deng et al., 2009) and only learn or fine-tune the last layers. Based on this idea, we choose to use the Spatial Pyramid Pooling (SPP) framework (He et al., 2014). It consists of computing densely over the image the convolutional layers of a pre-trained CNN in a manner similar to Sermanet et al. (2013), and re-scaling the output to compute the fully connected layers which are fine-tuned for the task. This framework allows efficient training and testing by taking advantage of shared computation in the first layers, and provides state-of-the-art performance in both classification and detection. We adapt and fine-tune the last layers of the SPP architecture to predict different features which identify both the presence of an object and its orientation. In all our experiments, the same network performs both detection and pose estimation, the only difference being the feature used and the associated error function.\nOur goal can be related to an emerging trend in computer vision, which attempts to predict more detailed information about objects in addition to their category, such as 3D information. To achieve this goal, the question of the representation of an object category, which is not seen as a single entity anymore, is central. For the special case of orientation, we identify and test several natural representations.\nContributions. In this paper, we compare several feature representations, learned with a single CNN, for joint detection and pose estimation. We show that handling the continuous nature of object poses is possible, but that a method which discretizes object poses and classify each image patch as both an object and a pose performs well. Moreover it significantly outperforms existing methods, providing a new baseline for the task. We also demonstrate the benefit of continuous representations"}, {"heading": "2 RELATED WORK", "text": "Pose estimation. Computer vision work focusing on pose estimation can be roughly separated in two classes: those which aim exclusively at instance alignment and those which target alignment for an entire category. Work on instance alignment includes both very early results such as (Huttenlocher & Ullman, 1990; Lowe, 1987) and more recent ones aiming at difficult or specialized tasks (Lim et al., 2013; Aubry et al., 2014). To perform category-level pose estimation, it is necessary to handle the large intra-class variation of the categories. That may be difficult to do in 3D, and for this reason most work either focuses on classes with a simple 3D geometry, such as cars, or consider simplified models of objects, for example approximated with planes (Xiang & Savarese, 2012) or cuboids (Xiao et al., 2012; Fidler et al., 2012). These hypotheses clearly limit the applicability of the methods for general objects and real images.\nMost of these approaches were demonstrated only on a restricted dataset. Recently, Xiang et al. (2014) introduced an extension of the classic and challenging Pascal VOC dataset (Everingham et al., 2012) by aligning a set of CAD models for 12 object classes. They also introduced the Average Viewpoint Precision as a standard metric for the evaluation of joint detection and viewpoint estimation. They provided baseline evaluations using a DPM detector for each orientation class as well as using the method of Pepik et al. (2012) which uses an adaptation of DPM with 3D constraints and usually performs better. We show that we can clearly outperform this baseline using CNNs.\nCNNs. Convolutional neural networks (LeCun et al., 1989) are architectures designed to learn robust features over images. They are formed by a succession of layers such as convolutions or spatial poolings. They have been successfully applied to many specialized tasks, such as digit recognition (LeCun et al., 1998; Simard et al., 2003) or, more related to our work, joint detection and pose estimation for faces (Osadchy et al., 2007). However they have attracted much more attention in computer vision after the impressive advances made on the classification task for the 1000 categories of ILSVRC 2012 by Krizhevsky et al. (2012). Since then, they improved dramatically results on many vision problems, being either adapted to the specific task or used as generic features (Sermanet et al., 2013).\nObject detection is of special interest to us and can be easily achieved by classifying a set of bounding boxes provided by a selective search algorithm (van de Sande et al., 2011). This strategy was\nfirst used by Girshick et al. (2013) and led to a dramatic improvement of Pascal VOC detection results. The drawback of this algorithm is that the CNN must be applied independently to each candidate bounding box. For this reason, He et al. (2014) proposed to apply selective search after the convolutional layers, leading to a more efficient algorithm and achieving slightly better performance.\nMany methods have recently been proposed to use neural networks with less training data than is available in ImageNet. In particular Oquab et al. (2014) performed classification on Pascal VOC dataset by adding and learning two fully connected layers on top of a network already trained on ImageNet, and He et al. (2014) obtained state-of-the-art results in both detection and classification by learning new fully connected layers on top of convolutional layers trained on ImageNet. It remains however unclear how much invariance is encoded in the first layers of the network, and if such a strategy could be applied to a task such as pose estimation, which requires to be more discriminative."}, {"heading": "3 PREDICTING ORIENTATION WITH CONVOLUTIONAL NEURAL NETWORKS", "text": "Except when stated otherwise, we consider the problem of detection and pose estimation for N object classes. Given that the pitch and roll angles vary only slightly for Pascal VOC images, we restrict ourselves to the estimation of the azimuth angle in this paper. However, our model could be extended to account for other angles.\nThe key elements to define in order to learn a CNN that predicts both object class and pose are:\n1. a feature space where each object and pose is associated to a given point or set of points, 2. a cost function defining the loss for each prediction to be minimized during training, 3. a network architecture.\nThe choice of features sometimes seems obvious, for example having a N -dimensional vector as output to classify an image into N discrete classes. However, it is not the case for our problem,\nbecause our output has a continuous structure. For example, it seems more natural to represent the features associated to chairs of different orientations as a circle rather than a set of separated points. For this reason, we tested several alternative representations, which are summarized in figure 1.\nIn section 4.1 we adapted directly the successful algorithms developed for detection by discretizing the different object poses into P orientations for N different classes, predicting a N \u00d7P +1 vector with the probabilities for each class and orientation, plus the background class.\nOn the contrary, in section 4.2 we focus on the continuous aspect of angle prediction. For a single class, we associate to each orientation of the object a point on a circle, and force background patches to have features far from this circle. This way, detection and pose estimation are completely joined: small distances from the features to the circle are associated to the class, and the angle predicts orientation.\nSection 4.3 can be seen as an intermediate between the two previous approaches. For each patch, we predict jointly the probability of the class as a N \u00d7 1 vector and the angle. The manifold corresponding to the angle feature can be a single circle for all classes, a circle per class, or an hyper-cylinder per class: we evaluate each option.\nThe error function we use for each case is different and adapted to the feature we want to learn, our choices for each approach are detailed in section 4.\nFinally, the choice of the CNN architecture has of course an important influence on the results. To provide comparable results, we choose to base all our models on the Spatial Pyramid Pooling framework developed by He et al. (2014), which is efficient for training and testing. We used the \u201dZeiler5\u201d architecture which provides good results for detection. Dense features are computed over the image at multiple scales using five convolutional layers and the two max-pooling layers similar to those of Krizhevsky et al. (2012). This first part of the network is trained on ImageNet and we used the network provided by He et al. (2014). The features corresponding to windows picked up by selective search over the image (van de Sande et al., 2011) are then rescaled and used as input for the three fully-connected layers. As in the work of He et al. (2014), we fine tune only the fully-connected layers for our different tasks."}, {"heading": "4 SELECTED FRAMEWORKS FOR POSE ESTIMATION", "text": "In this section, we describe in more details the different approaches that are evaluated in this paper and the corresponding choices of error functions. They were selected to cover the most different approaches to the problem, corresponding to the different cases presented in figure 1.\nIn all the section, we write y(x,w) the output of a network y of parameters w for the input x. We suppose we are given M training images (xk)1\u2264k\u2264M , with ground truth labels tk and orientation \u03b8k for each k. As explained in section 3, labels tk have different dimensions and interpretations depending on the choice of architecture."}, {"heading": "4.1 DISCRETIZED POSE CLASSIFICATION", "text": "The simplest way to solve the problem of pose estimation based on a classification algorithm is to discretize the orientations into P bins and to consider each bin as a separate class. This leads to a classification problem with N \u00d7 P + 1 classes. We write (t0, t1,1, t1,2, ..., t1,P , t2,1, ..., tN,P ) the coordinates of a vector t in RN\u00d7P+1. We define tk = (tk0 , tk1,1, tk1,2, ..., tkN,P ) by setting tki,j = 1 if xk is of object category i and orientation j and 0 otherwise, setting tk0 = 1 if the object is not from any category, and 0 otherwise. We use exactly the same SPP architecture as He et al. (2014) for detection. The first part of the network trained on ImageNet is fixed. Windows corresponding to patches extracted by selective search on the image are extracted from the feature maps and rescaled to a fixed size input to three fully connected layers, followed by a softmax. The error minimized by the learning algorithm is the negative log-likelihood, which can be written:\nE(w) = \u2212 M\u2211 k=1 tk0 log (y(xk,w))0 + N\u2211 i=1 P\u2211 j=1 tki,j log ( y(xk,w) ) i,j  (1)\nBecause of the softmax, y(x,w) has all its coordinates between 0 and 1 and sums to 1. Thus the error is minimal for y(xk,w) = tk. For a test input x, (y(x,w))i,j can be interpreted as the probability for the input to be of class i and orientation j."}, {"heading": "4.2 CONTINUOUS REGRESSION", "text": "The discrete framework does not consider the natural continuous structure of the object space. Indeed, the appearance varies continuously with the viewpoint while the output of the discretized network is supposed to \u201cjump\u201d from one point to another when the viewpoint varies, not taking advantage of the continuity of the appearance. For this reason, we present in this section a different view of the problem, similar to the one developed by Osadchy et al. (2007) for face detection and orientation prediction. We consider a single class and we associate to each positive example a point on the unit circle, with its angle corresponding to the orientation. We do not enforce any specific prediction for the negative examples, but we force them to be far away from the circle describing the positives. A problem that arises if one simply considers a circle in the 2D space is that an error function defined to push symmetrically the negatives away from the circle will have a local minimum inside the circle for the negatives. Thus, some negative samples may be trapped into the circle instead of being pushed far away. To avoid this effect, we add a third dimension to our features t and let the 2D unit circle live in a 3D space. More formally, the circle defining the positives is defined as \u03c7 = {(cos(\u03b8), sin(\u03b8), 0), \u03b8 \u2208 R}. Since the problem is not a classification problem, we use the network of He et al. (2014), without the final softmax.\nWe separate the indices of the training examples into the sets Spos corresponding to positive examples and the set Sneg corresponding to negative examples. For k \u2208 Spos , the desired output of the network is tk = (cos(\u03b8k), sin(\u03b8k), 0). An error function that attracts positive examples to their target position on the network can be written as:\nE(w) = 1 |Spos | \u2211\nk\u2208Spos\nLpos(xk, tk,w) +K 1 |Sneg | \u2211\nk\u2208Sneg\nLneg(xk,w) (2)\nwhere K weighs the respective contributions of the positive and negative losses Lpos , Lneg in the global error. The most natural properties to require from the losses Lpos and Lneg are that:\n\u2022 Lpos(x, t,w) should have a local minimum only if y(x,w) = t. \u2022 Lneg(x,w) should be a decreasing function of \u2016y(x,w) \u2212 \u03c0(y(x,w))\u2016, where \u03c0 is the\nprojection on \u03c7.\nFollowing Osadchy et al. (2007), we choose the following losses, with the above properties:\nLpos(x, t,w) = \u2016y(x,w)\u2212 t\u201622 (3)\nLneg(x, t,w) = exp\n( \u2212\u2016y(x,w)\u2212 \u03c0(y(x,w))\u20162\n\u03b4\n) (4)\nThe parameter \u03b4 was not introduced by Osadchy et al. (2007) but we found it important and complementary to K. Indeed, huge values of K would be needed to compensate the exponential decrease of the negative weights, and push the negative examples at a distance clearly larger than the radius of the positive circle. With this network, the probability of a test sample x to correspond to a positive example is a decreasing function of \u2016y(x,w)\u2212\u03c0(y(x,w))\u2016 and its most probable orientation if it is a positive sample is the angle defined by its projection on \u03c7. From a computed feature y(x,w), we obtain the angle corresponding to the pose by \u03b8\u0303k = arctan(y(x,w)1, y(x,w)2), where y(x,w)i is the i-th coordinate of feature y(x,w)."}, {"heading": "4.3 JOINT CLASSIFICATION AND CONTINUOUS POSE ESTIMATION", "text": "The network described in the previous paragraph treats the classification problem implicitly and is not straightforward to extend to several mutually exclusive classes. One would need for example to define parameters such as the ratio of the distance between the categories and the distance between different orientations of objects of the same category. For this reason, we considered an alternative\ninspired by the work of Penedones et al. (2011), which consists of dividing the last layer of the network in two, one part devoted to the classification that we call y class and the other to orientation prediction, y pose . The classification layer is followed by a softmax while the pose prediction layer is not. For the rest of the network, we keep the same architecture as in the previous sections. This approach can be seen as an intermediate between the other two.\nThe target values for this network output can be decomposed as t = (t class , t pose), each part corresponding to the associated last layer. As in a standard classification network, t class is a N + 1 dimensional vector, each dimension corresponding to the probability that the input is an instance of the corresponding class. For t pose , two natural choices are possible:\n(a) t pose is a point in 2D whose angle is associated to the viewpoint of an object of any class.\n(b) t pose is a point in a 2N dimensional space, the angle of the pair (t pose2i\u22121, t pose 2i ) corresponding to\nthe angle prediction for an object of the object class i.\nThe training labels associated to the first case are tk, pose = (cos(\u03b8k), sin(\u03b8k)) for the positive examples and the negative examples are not constrained to have any value. For the second case, (tk, pose2i\u22121 , t k, pose 2i ) = (cos(\u03b8 k), sin(\u03b8k)) if tk, classi = 1 and (0, 0) if not.\nWe define the error of the network by the sum of a classification term and a pose estimation term:\nE(w) = \u03bbE class(w) + E pose(w) (5)\nwhere \u03bb balances the contribution of the two terms. We choose for E class(w) the log-likelihood similarly to equation 1 using the classification feature. In case (a), the definition of E pose(w) is based on a loss similar to equation 3. In case (b), we consider two possibilities to define E pose(w): (b1) penalizes the distance to t pose ; (b2) penalizes only the distance of the two components corresponding to the relevant class. (b1) is similar to the approach of Penedones et al. (2011). An approach similar to (b2) was used in Williams et al. (1997) for control points localization.\nSince performing jointly detection and pose estimation requires optimizing \u03bb, we provide experiments on pose estimation only using \u03bb = 0, using the detection score of a reference classifier and only learning the pose regression, and compare several variants of penalization for Lpos . We experimented with the L1, L2 and squared L2 losses.\nWith this network, the probability of a test sample to belong to each object class is given by the classification part of the output, while the orientation prediction can be made from the pose estimation part in a way similar to section 4.2."}, {"heading": "5 EXPERIMENTS", "text": ""}, {"heading": "5.1 DATASET, EVALUATION MEASURE AND BASELINES", "text": "We trained and evaluated our methods using the Pascal3D+ dataset introduced by Xiang et al. (2014), which extends the annotation of the challenging Pascal VOC 2012 dataset (Everingham et al., 2012). In particular it provides orientation annotations for 12 object classes. The orientation annotation were obtained by aligning CAD models to the images. Xiang et al. (2014) also introduced a standard measure for evaluating joint detection and pose estimation and provided several baselines.\nTheir accuracy measure, called Average Viewpoint Precision (AVP), is very similar to the standard Average Precision (AP) used for detection and obtained by computing the area under the precisionrecall curve, except that it considers a detection to be positive only if the associated orientation prediction is accurate with an error below a given threshold. In the case of discrete viewpoint prediction, the threshold can be associated to a bin size, and thus to the number of different viewpoints that are predicted. For this reason, following Xiang et al. (2014), we report our results with tolerance thresholds corresponding to 4, 8, 16 and 24 views. Since the test set orientation annotations are not available, all the networks were finetuned using only the Pascal training set, augmented with the flipped images, and evaluated on Pascal validation set, as in Xiang et al. (2014). We did not use the data from Imagenet for fine-tunning (except in a single comparison experiment). Moreover, we considered all instances for evaluation, except those marked as difficult; in particular, occluded and truncated objects are evaluated.\nThe best baseline provided by Xiang et al. (2014) corresponds to the method of Pepik et al. (2012). To this baseline, we add two others which consist first in using the detection method of He et al. (2014) with the same \u201cZeiler5\u201d architecture as used in the rest of the paper. Then, after detection, either (1) we predict the most probable viewpoint for each class, or (2) we predict the orientation for each class using a linear regression on the fc7 features computed for each bounding box. Note that baseline (2) corresponds to the limit of the method (b2) described in section 4.3 when \u03bb tends to +\u221e. The corresponding results are provided in tables 1 and 2. Detailed results for the CNN baselines can be found in the appendix. As expected, the AP results are clearly better using CNNs, but one can notice that the AVP is still in favor of the method of Pepik et al. (2012). When comparing baselines (1) and (2), we see that the regression on the fc7 features is not able to recover any valuable orientation information."}, {"heading": "5.2 TRAINING DETAILS", "text": "All the networks were trained using Stochastic Gradient Descent, with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 128 patches. We balanced the proportion of positive and negative patches in the following manner: for the discretized method (sec. 4.1) and for the joint classification and regression models (sec. 4.3), we use 32 positive patches and 96 negative patches per batch; for the continuous regression (sec. 4.2), as there is only one positive class, we consider 8 positive and 120 negative patches per batch. We used an adaptive strategy to vary the learning rate and the number of required iterations in our different experiments. For this, we randomly selected a validation set of 6400 patches, keeping the same positive and negative proportions as during training (to be compared to the 127K positive and 13M negative patches generated by the selective search) on which we evaluate every 500 iterations. We started with a learning rate of 10\u22123 (except for the continuous regression case for which we used 5 \u00d7 10\u22125 to avoid divergence). We then divided the learning rate by 2 when the error on our small validation set stopped decreasing for 10 successive evaluations (5K iterations). This strategy allowed a fast training of our networks, although it is not optimal. All the models were implemented using Torch7 (Collobert et al., 2011)."}, {"heading": "5.3 RESULTS", "text": "Discrete method. For experiments with the discretized method (section 4.1), we trained a different model for each discretization variant (4, 8, 16 or 24 poses). The results are presented in table 3. They clearly outperform the previous state-of-the-art results presented in table 1, both in term of AP and AVP. We can see that the detection results are comparable to the CNN baseline presented in table 2 and that they decrease with the number of viewpoints. Another interesting fact is that the margin by which our method outperforms the baseline decreases when the number of viewpoints augments. It can be explained by the fact that less training examples per orientation class are available when the number of discrete viewpoints is increased. To test this interpretation, we trained the 24-views model using both Pascal training and Imagenet annotations. The results are presented in the last row of Table 3. As expected, adding more training data increases considerably the mAP and mAVP.\nContinuous regression. To study the method of 4.2 and determine the best set of parameters (\u03bb, \u03b4), we selected the bicycle class, for which all orientations are well represented in the database, and explored different values of the parameters (see table 8 in the appendix). We found that the model performs best for \u03b4 = 1 and K = 640. However, we were not able to study yet higher values of K because of stability issues. With this set of parameters, we conducted experiments on other classes. We report the results in table 4. The detection performances are better than the DPM baseline, but worse than the CNN baseline and the discrete method. This is not surprising since the error function is not optimized for detection. The viewpoint predictions are clearly better than the CNN baseline, showing that pose information is effectively learned.\nJoint classification and continuous pose estimation. We first test the regression method (b2) without detection (\u03bb = 0) with different loss functions, using the same detector as in the CNN baseline to score the candidates. The results are presented on the left side of Table 5. Using the squared L2 norm as loss, we did not observe any improvement over the CNN baseline, showing that no real pose estimation is learned. The results obtained with L1 and L2 norms are much better,"}, {"heading": "24V I 56.4 18.2 49.9 17.0 25.5 4.0 23.0 22.1 57.0 37.8 41.0 25.3 13.5 4.5 29.8 10.0 56.7 20.1 33.7 16.1 45.2 31.0 48.1 19.8 40.0 18.8", "text": "really learning pose estimation and improving over both baselines. However, they are slightly lower than the results of the discrete method. We then investigate the interest of jointly optimizing detection and pose prediction. We test the three variants presented in section 4.3 using the L1 norm and \u03bb = 10, similar to Penedones et al. (2011). The results are presented on the right side of Table 5. The approach (b1) fails, probably because of its lack of flexibility. The most surprising result is the clear improvement of the detection performance for method (a) and (b2). This result corroborates the initial findings of Penedones et al. (2011) on a much more challenging dataset. A possible explanation is that separating the different orientations helps the network to find better representations of the object classes. Interestingly, despite this improvement in detection, the orientation predictions are not as good as those obtained by learning the pose independently from the detection. A possible explanation would be that the relative weight of the pose loss is too small. We thus repeated the experiment for method (b2) with \u03bb = 1. We observe only a limited improvement of the fine pose estimation performance, while the detection performance drops.\nSummary. Our experiments demonstrate that it is possible to perform joint detection and pose estimation with CNNs both in a continuous and discrete set-up. The discrete set-up provides the best results for orientation prediction, but it needs more data to avoid a decrease of the detection performance, because it treats each orientation as a separate class. On the contrary, it is possible in a continuous set-up to take advantage of the additional orientation annotations to improve detection performances."}, {"heading": "6 CONCLUSION", "text": "In this paper, we presented a unified view of several CNN approaches for joint detection and orientation prediction. We compared these approaches and showed that there is a benefit in performing the two tasks jointly. Finally, we used CNNs to outperform state-of-the-art results and provide a new baseline for joint detection and pose estimation."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was carried out in IMAGINE, a joint research project between Ecole des Ponts ParisTech (ENPC) and the Scientific and Technical Centre for Building (CSTB). It was partly supported by ANR project Semapolis ANR-13-CORD-0003."}, {"heading": "A SUPPLEMENTARY RESULTS", "text": ""}], "references": [{"title": "Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models", "author": ["M. Aubry", "D. Maturana", "A.A. Efros", "B.C. Russell", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "Aubry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Aubry et al\\.", "year": 2014}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In CVPR,", "citeRegEx": "Dalal and Triggs,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs", "year": 2005}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "3D object detection and viewpoint estimation with a deformable 3D cuboid model", "author": ["S. Fidler", "S. Dickinson", "R. Urtasun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fidler et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fidler et al\\.", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In European Conf. on Comp. Vision (ECCV),", "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Recognizing solid objects by alignment with an image", "author": ["D.P. Huttenlocher", "S. Ullman"], "venue": null, "citeRegEx": "Huttenlocher and Ullman,? \\Q1990\\E", "shortCiteRegEx": "Huttenlocher and Ullman", "year": 1990}, {"title": "ImageNet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural comp.,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Parsing ikea objects: Fine pose estimation", "author": ["J.J. Lim", "H. Pirsiavash", "A. Torralba"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": null, "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Three-dimensional object recognition from single two-dimensional", "author": ["D.G. Lowe"], "venue": "images. AIJ,", "citeRegEx": "Lowe,? \\Q1987\\E", "shortCiteRegEx": "Lowe", "year": 1987}, {"title": "Object recognition in the geometric era: A retrospective", "author": ["J.L. Mundy"], "venue": "In Toward category-level object recognition,", "citeRegEx": "Mundy,? \\Q2006\\E", "shortCiteRegEx": "Mundy", "year": 2006}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "Synergistic face detection and pose estimation with energy-based models", "author": ["M. Osadchy", "Y. LeCun", "M.L. Miller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Osadchy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Osadchy et al\\.", "year": 2007}, {"title": "Improving object classification using pose information", "author": ["H. Penedones", "R. Collobert", "F. Fleuret", "D. Grangier"], "venue": "Technical report, Idiap Research Institute,", "citeRegEx": "Penedones et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Penedones et al\\.", "year": 2011}, {"title": "Teaching 3D geometry to deformable part models", "author": ["B. Pepik", "M. Stark", "P. Gehler", "B. Schiele"], "venue": "In CVPR, pp", "citeRegEx": "Pepik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pepik et al\\.", "year": 2012}, {"title": "Machine perception of three-dimensional solids", "author": ["L.G. Roberts"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Roberts,? \\Q1963\\E", "shortCiteRegEx": "Roberts", "year": 1963}, {"title": "OverFeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In 7th International Conference on Document Analysis and Recognition (ICDAR),", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Segmentation as selective search for object recognition", "author": ["K.E.A. van de Sande", "J.R.R. Uijlings", "T. Gevers", "A.W.M. Smeulders"], "venue": "In ICCV,", "citeRegEx": "Sande et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sande et al\\.", "year": 2011}, {"title": "Instantiating deformable models with a neural net", "author": ["Williams", "Christopher KI", "Revow", "Michael", "Hinton", "Geoffrey E"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Williams et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1997}, {"title": "Beyond Pascal: A benchmark for 3D object detection in the wild", "author": ["Y. Xiang", "R. Mottaghi", "S. Savarese"], "venue": "In IEEE Winter Conference on Applications of Computer Vision (WACV),", "citeRegEx": "Xiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2014}, {"title": "Estimating the aspect layout of object categories", "author": ["Xiang", "Yu", "Savarese", "Silvio"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Xiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2012}, {"title": "Localizing 3D cuboids in single-view images", "author": ["J. Xiao", "B. Russell", "A. Torralba"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xiao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Convolutional Neural Networks (CNNs) have long shown impressive and practical results on specialized vision tasks, such as optical character recognition (LeCun et al., 1998; Simard et al., 2003).", "startOffset": 153, "endOffset": 194}, {"referenceID": 21, "context": "Convolutional Neural Networks (CNNs) have long shown impressive and practical results on specialized vision tasks, such as optical character recognition (LeCun et al., 1998; Simard et al., 2003).", "startOffset": 153, "endOffset": 194}, {"referenceID": 3, "context": "Thanks to the availability of large, annotated image datasets (Deng et al., 2009) and the increased power of GPU computing, CNNs have recently outperformed other techniques in less constrained computer vision tasks.", "startOffset": 62, "endOffset": 81}, {"referenceID": 24, "context": ", 2012) extended with 3D annotations (Xiang et al., 2014).", "startOffset": 37, "endOffset": 57}, {"referenceID": 3, "context": "Thanks to the availability of large, annotated image datasets (Deng et al., 2009) and the increased power of GPU computing, CNNs have recently outperformed other techniques in less constrained computer vision tasks. Following the seminal work of Krizhevsky et al. (2012) on ImageNet classification many methods have been developed to apply CNNs to other tasks.", "startOffset": 63, "endOffset": 271}, {"referenceID": 19, "context": "Detecting and estimating the pose of a known object instance is one of the oldest problem of computer vision (Roberts, 1963).", "startOffset": 109, "endOffset": 124}, {"referenceID": 14, "context": "In early works, object contours were detected explicitly in images and matched to the contours of a 3D model of the object instance (Mundy, 2006).", "startOffset": 132, "endOffset": 145}, {"referenceID": 12, "context": "These approaches are typically based on manually designed low-level features, such as HOG (Dalal & Triggs, 2005) and SIFT (Lowe, 2004).", "startOffset": 122, "endOffset": 134}, {"referenceID": 3, "context": "(2014), we use a CNN pre-trained on ImageNet (Deng et al., 2009) and only learn or fine-tune the last layers.", "startOffset": 45, "endOffset": 64}, {"referenceID": 6, "context": "Based on this idea, we choose to use the Spatial Pyramid Pooling (SPP) framework (He et al., 2014).", "startOffset": 81, "endOffset": 98}, {"referenceID": 13, "context": "To deal with the lack of training data, following Oquab et al. (2014) and He et al.", "startOffset": 50, "endOffset": 70}, {"referenceID": 5, "context": "(2014) and He et al. (2014), we use a CNN pre-trained on ImageNet (Deng et al.", "startOffset": 11, "endOffset": 28}, {"referenceID": 3, "context": "(2014), we use a CNN pre-trained on ImageNet (Deng et al., 2009) and only learn or fine-tune the last layers. Based on this idea, we choose to use the Spatial Pyramid Pooling (SPP) framework (He et al., 2014). It consists of computing densely over the image the convolutional layers of a pre-trained CNN in a manner similar to Sermanet et al. (2013), and re-scaling the output to compute the fully connected layers which are fine-tuned for the task.", "startOffset": 46, "endOffset": 350}, {"referenceID": 13, "context": "Work on instance alignment includes both very early results such as (Huttenlocher & Ullman, 1990; Lowe, 1987) and more recent ones aiming at difficult or specialized tasks (Lim et al.", "startOffset": 68, "endOffset": 109}, {"referenceID": 11, "context": "Work on instance alignment includes both very early results such as (Huttenlocher & Ullman, 1990; Lowe, 1987) and more recent ones aiming at difficult or specialized tasks (Lim et al., 2013; Aubry et al., 2014).", "startOffset": 172, "endOffset": 210}, {"referenceID": 0, "context": "Work on instance alignment includes both very early results such as (Huttenlocher & Ullman, 1990; Lowe, 1987) and more recent ones aiming at difficult or specialized tasks (Lim et al., 2013; Aubry et al., 2014).", "startOffset": 172, "endOffset": 210}, {"referenceID": 26, "context": "That may be difficult to do in 3D, and for this reason most work either focuses on classes with a simple 3D geometry, such as cars, or consider simplified models of objects, for example approximated with planes (Xiang & Savarese, 2012) or cuboids (Xiao et al., 2012; Fidler et al., 2012).", "startOffset": 247, "endOffset": 287}, {"referenceID": 4, "context": "That may be difficult to do in 3D, and for this reason most work either focuses on classes with a simple 3D geometry, such as cars, or consider simplified models of objects, for example approximated with planes (Xiang & Savarese, 2012) or cuboids (Xiao et al., 2012; Fidler et al., 2012).", "startOffset": 247, "endOffset": 287}, {"referenceID": 23, "context": "Recently, Xiang et al. (2014) introduced an extension of the classic and challenging Pascal VOC dataset (Everingham et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 18, "context": "They provided baseline evaluations using a DPM detector for each orientation class as well as using the method of Pepik et al. (2012) which uses an adaptation of DPM with 3D constraints and usually performs better.", "startOffset": 114, "endOffset": 134}, {"referenceID": 9, "context": "Convolutional neural networks (LeCun et al., 1989) are architectures designed to learn robust features over images.", "startOffset": 30, "endOffset": 50}, {"referenceID": 10, "context": "They have been successfully applied to many specialized tasks, such as digit recognition (LeCun et al., 1998; Simard et al., 2003) or, more related to our work, joint detection and pose estimation for faces (Osadchy et al.", "startOffset": 89, "endOffset": 130}, {"referenceID": 21, "context": "They have been successfully applied to many specialized tasks, such as digit recognition (LeCun et al., 1998; Simard et al., 2003) or, more related to our work, joint detection and pose estimation for faces (Osadchy et al.", "startOffset": 89, "endOffset": 130}, {"referenceID": 16, "context": ", 2003) or, more related to our work, joint detection and pose estimation for faces (Osadchy et al., 2007).", "startOffset": 84, "endOffset": 106}, {"referenceID": 20, "context": "Since then, they improved dramatically results on many vision problems, being either adapted to the specific task or used as generic features (Sermanet et al., 2013).", "startOffset": 142, "endOffset": 165}, {"referenceID": 8, "context": "However they have attracted much more attention in computer vision after the impressive advances made on the classification task for the 1000 categories of ILSVRC 2012 by Krizhevsky et al. (2012). Since then, they improved dramatically results on many vision problems, being either adapted to the specific task or used as generic features (Sermanet et al.", "startOffset": 171, "endOffset": 196}, {"referenceID": 5, "context": "first used by Girshick et al. (2013) and led to a dramatic improvement of Pascal VOC detection results.", "startOffset": 14, "endOffset": 37}, {"referenceID": 5, "context": "first used by Girshick et al. (2013) and led to a dramatic improvement of Pascal VOC detection results. The drawback of this algorithm is that the CNN must be applied independently to each candidate bounding box. For this reason, He et al. (2014) proposed to apply selective search after the convolutional layers, leading to a more efficient algorithm and achieving slightly better performance.", "startOffset": 14, "endOffset": 247}, {"referenceID": 14, "context": "In particular Oquab et al. (2014) performed classification on Pascal VOC dataset by adding and learning two fully connected layers on top of a network already trained on ImageNet, and He et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "(2014) performed classification on Pascal VOC dataset by adding and learning two fully connected layers on top of a network already trained on ImageNet, and He et al. (2014) obtained state-of-the-art results in both detection and classification by learning new fully connected layers on top of convolutional layers trained on ImageNet.", "startOffset": 157, "endOffset": 174}, {"referenceID": 6, "context": "To provide comparable results, we choose to base all our models on the Spatial Pyramid Pooling framework developed by He et al. (2014), which is efficient for training and testing.", "startOffset": 118, "endOffset": 135}, {"referenceID": 6, "context": "To provide comparable results, we choose to base all our models on the Spatial Pyramid Pooling framework developed by He et al. (2014), which is efficient for training and testing. We used the \u201dZeiler5\u201d architecture which provides good results for detection. Dense features are computed over the image at multiple scales using five convolutional layers and the two max-pooling layers similar to those of Krizhevsky et al. (2012). This first part of the network is trained on ImageNet and we used the network provided by He et al.", "startOffset": 118, "endOffset": 429}, {"referenceID": 6, "context": "To provide comparable results, we choose to base all our models on the Spatial Pyramid Pooling framework developed by He et al. (2014), which is efficient for training and testing. We used the \u201dZeiler5\u201d architecture which provides good results for detection. Dense features are computed over the image at multiple scales using five convolutional layers and the two max-pooling layers similar to those of Krizhevsky et al. (2012). This first part of the network is trained on ImageNet and we used the network provided by He et al. (2014). The features corresponding to windows picked up by selective search over the image (van de Sande et al.", "startOffset": 118, "endOffset": 537}, {"referenceID": 6, "context": "To provide comparable results, we choose to base all our models on the Spatial Pyramid Pooling framework developed by He et al. (2014), which is efficient for training and testing. We used the \u201dZeiler5\u201d architecture which provides good results for detection. Dense features are computed over the image at multiple scales using five convolutional layers and the two max-pooling layers similar to those of Krizhevsky et al. (2012). This first part of the network is trained on ImageNet and we used the network provided by He et al. (2014). The features corresponding to windows picked up by selective search over the image (van de Sande et al., 2011) are then rescaled and used as input for the three fully-connected layers. As in the work of He et al. (2014), we fine tune only the fully-connected layers for our different tasks.", "startOffset": 118, "endOffset": 758}, {"referenceID": 6, "context": "We use exactly the same SPP architecture as He et al. (2014) for detection.", "startOffset": 44, "endOffset": 61}, {"referenceID": 15, "context": "For this reason, we present in this section a different view of the problem, similar to the one developed by Osadchy et al. (2007) for face detection and orientation prediction.", "startOffset": 109, "endOffset": 131}, {"referenceID": 6, "context": "Since the problem is not a classification problem, we use the network of He et al. (2014), without the final softmax.", "startOffset": 73, "endOffset": 90}, {"referenceID": 16, "context": "Following Osadchy et al. (2007), we choose the following losses, with the above properties:", "startOffset": 10, "endOffset": 32}, {"referenceID": 16, "context": "The parameter \u03b4 was not introduced by Osadchy et al. (2007) but we found it important and complementary to K.", "startOffset": 38, "endOffset": 60}, {"referenceID": 17, "context": "inspired by the work of Penedones et al. (2011), which consists of dividing the last layer of the network in two, one part devoted to the classification that we call y class and the other to orientation prediction, y pose .", "startOffset": 24, "endOffset": 48}, {"referenceID": 17, "context": "(b1) is similar to the approach of Penedones et al. (2011). An approach similar to (b2) was used in Williams et al.", "startOffset": 35, "endOffset": 59}, {"referenceID": 17, "context": "(b1) is similar to the approach of Penedones et al. (2011). An approach similar to (b2) was used in Williams et al. (1997) for control points localization.", "startOffset": 35, "endOffset": 123}, {"referenceID": 24, "context": "We trained and evaluated our methods using the Pascal3D+ dataset introduced by Xiang et al. (2014), which extends the annotation of the challenging Pascal VOC 2012 dataset (Everingham et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 24, "context": "We trained and evaluated our methods using the Pascal3D+ dataset introduced by Xiang et al. (2014), which extends the annotation of the challenging Pascal VOC 2012 dataset (Everingham et al., 2012). In particular it provides orientation annotations for 12 object classes. The orientation annotation were obtained by aligning CAD models to the images. Xiang et al. (2014) also introduced a standard measure for evaluating joint detection and pose estimation and provided several baselines.", "startOffset": 79, "endOffset": 371}, {"referenceID": 24, "context": "For this reason, following Xiang et al. (2014), we report our results with tolerance thresholds corresponding to 4, 8, 16 and 24 views.", "startOffset": 27, "endOffset": 47}, {"referenceID": 24, "context": "For this reason, following Xiang et al. (2014), we report our results with tolerance thresholds corresponding to 4, 8, 16 and 24 views. Since the test set orientation annotations are not available, all the networks were finetuned using only the Pascal training set, augmented with the flipped images, and evaluated on Pascal validation set, as in Xiang et al. (2014). We did not use the data from Imagenet for fine-tunning (except in a single comparison experiment).", "startOffset": 27, "endOffset": 367}, {"referenceID": 22, "context": "The best baseline provided by Xiang et al. (2014) corresponds to the method of Pepik et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 17, "context": "(2014) corresponds to the method of Pepik et al. (2012). To this baseline, we add two others which consist first in using the detection method of He et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 6, "context": "To this baseline, we add two others which consist first in using the detection method of He et al. (2014) with the same \u201cZeiler5\u201d architecture as used in the rest of the paper.", "startOffset": 89, "endOffset": 106}, {"referenceID": 6, "context": "To this baseline, we add two others which consist first in using the detection method of He et al. (2014) with the same \u201cZeiler5\u201d architecture as used in the rest of the paper. Then, after detection, either (1) we predict the most probable viewpoint for each class, or (2) we predict the orientation for each class using a linear regression on the fc7 features computed for each bounding box. Note that baseline (2) corresponds to the limit of the method (b2) described in section 4.3 when \u03bb tends to +\u221e. The corresponding results are provided in tables 1 and 2. Detailed results for the CNN baselines can be found in the appendix. As expected, the AP results are clearly better using CNNs, but one can notice that the AVP is still in favor of the method of Pepik et al. (2012). When comparing baselines (1) and (2), we see that the regression on the fc7 features is not able to recover any valuable orientation information.", "startOffset": 89, "endOffset": 778}, {"referenceID": 1, "context": "All the models were implemented using Torch7 (Collobert et al., 2011).", "startOffset": 45, "endOffset": 69}, {"referenceID": 18, "context": "Table 1: Baseline - DPM-VOC+VP (Pepik et al., 2012) - [AP |AVP]", "startOffset": 31, "endOffset": 51}, {"referenceID": 17, "context": "3 using the L norm and \u03bb = 10, similar to Penedones et al. (2011). The results are presented on the right side of Table 5.", "startOffset": 42, "endOffset": 66}, {"referenceID": 17, "context": "3 using the L norm and \u03bb = 10, similar to Penedones et al. (2011). The results are presented on the right side of Table 5. The approach (b1) fails, probably because of its lack of flexibility. The most surprising result is the clear improvement of the detection performance for method (a) and (b2). This result corroborates the initial findings of Penedones et al. (2011) on a much more challenging dataset.", "startOffset": 42, "endOffset": 372}], "year": 2015, "abstractText": "In this paper we study the application of convolutional neural networks for jointly detecting objects depicted in still images and estimating their 3D pose. We identify different feature representations of oriented objects, and energies that lead a network to learn this representations. The choice of the representation is crucial since the pose of an object has a natural, continuous structure while its category is a discrete variable. We evaluate the different approaches on the joint object detection and pose estimation task of the Pascal3D+ benchmark using Average Viewpoint Precision. We show that a classification approach on discretized viewpoints achieves state-of-the-art performance for joint object detection and pose estimation, and significantly outperforms existing baselines on this benchmark. We also show that performing the two tasks jointly can improve significantly the detection performances.", "creator": "LaTeX with hyperref package"}}}