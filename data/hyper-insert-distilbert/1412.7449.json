{"id": "1412.7449", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Grammar as a Foreign Language", "abstract": "syntactic parsing query is a fundamental fundamental complex problem relevant in computational linguistics modelling and particularly natural language processing. traditional approaches to parsing are highly heavily complex and many problem domains specific. recently, sutskever et al. ( until 2014 ) presented a domain - type independent sampling method for learning to map simultaneous input sequences to its output sequences that achieved strong synthesis results critically on a pretty large scale cellular machine translation problem. fortunately in modeling this key work, we repeatedly show that precisely creating the same primitive sequence - to - sequence method achieves powerful results independently that theoretically are close to state - of - above the - art operations on syntactic processing constituency parsing, obtained whilst making almost no factual assumptions either about the structure implications of the same problem.", "histories": [["v1", "Tue, 23 Dec 2014 17:16:24 GMT  (114kb)", "http://arxiv.org/abs/1412.7449v1", null], ["v2", "Sat, 28 Feb 2015 03:16:54 GMT  (115kb)", "http://arxiv.org/abs/1412.7449v2", null], ["v3", "Tue, 9 Jun 2015 22:41:07 GMT  (113kb,D)", "http://arxiv.org/abs/1412.7449v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["oriol vinyals", "lukasz kaiser", "terry koo", "slav petrov", "ilya sutskever", "geoffrey e hinton"], "accepted": true, "id": "1412.7449"}, "pdf": {"name": "1412.7449.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vinyals@google.com", "lukaszkaiser@google.com", "terrykoo@google.com", "slav@google.com", "ilyasu@google.com", "geoffhinton@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n74 49\nv1 [\ncs .C\nL ]\n2 3\nD ec\n2 01\n4 Under review as a conference paper at ICLR 2015"}, {"heading": "1 INTRODUCTION", "text": "It has recently been shown that a recurrent neural network can learn complex sequence-to-sequence mappings directly from raw data. This was first demonstrated on the English-to-French translation task (Sutskever et al., 2014), but the same approach also works for evaluating short python programs (Zaremba & Sutskever, 2014). In this work, we use the same type of recurrent neural net, called Long Short-Term Memory (Hochreiter & Schmidhuber, 1997, LSTM). The LSTM model directly maps a variable-length input sequence to a large but fixed-sized vector, which is then mapped to a variable-length output. It can therefore be used as a general function learning mechanism: Given example inputs x and corresponding outputs y of any serializable type, to learn a function f such that f(x) = y, just serialize each x and y and apply the sequence-to-sequence learning model.\nThe procedure described above will not work for arbitrary functions f , as the sequence-to-sequence network has inherent limitations: it uses a memory of constant size and runs in linear time. It does, however, achieve high performance on a large scale machine translation task (Luong et al., 2014). In this paper we show that it also works well for syntactic constituency parsing, even though this task requires modeling complex relations between input words and producing trees as the output.\nOur main results are as follows: We train a deep LSTM model with 34M parameters on a dataset consisting of 90K sentences obtained from various treebanks and 7M sentences from the web that are automatically parsed with the BerkeleyParser (Petrov et al., 2006). This model achieves an F1 score of 90.5 on section 23 of the Penn Treebank. For comparison, the BerkeleyParser achieves an F1 score of 90.2 when trained on the same treebank data. A model combination of 10 such LSTMs achieves an F1 score of 91.6.\nAs we will demonstrate in the experimental section, the automatically parsed data is crucial for our model. Since the LSTM lacks prior task-specific knowledge, it needs many examples to learn to parse accurately. However, in the presence of sufficient training examples, it is able to automatically learn the complex syntactic relationships between the input and output pairs, which are typically manually engineered into parsing models. In particular, we do not binarize the parse trees and do not need any special handling for unary productions or unknown words, which are simply mapped to a single unknown word token. Despite the simplicity of our approach, our final results are competitive and close to the state of the art.\nA common criticism of the sequence-to-sequence approach of Sutskever et al. (2014) is that it is fundamentally incapable of dealing with long inputs and outputs, due to its need to store the entire input sequence in its short-term memory. Despite this potential concern, the deep LSTM model (which in our experiments used a 4,000-dimensional state) had little trouble with fairly long sequences. Indeed, the average sentence in the dataset has 22 words and the average parse tree annotation has 66 symbols, which did not pose a challenge to our sequence-to-sequence LSTM model.\n\u2217Equal contribution\nUnder review as a conference paper at ICLR 2015"}, {"heading": "2 LSTM PARSING MODEL", "text": "Let us first recall the sequence-to-sequence LSTM model. The Long Short-Term Memory model of Hochreiter & Schmidhuber (1997) is defined as follows. Let xt, ht, and mt be the input, control state, and memory state at timestep t. Then, given a sequence of inputs (x1, . . . , xT ), the LSTM computes the h-sequence (h1, . . . , hT ) and the m-sequence (m1, . . . ,mT ) as follows\nit = sigm(W1xt +W2ht\u22121)\ni\u2032 t = tanh(W3xt +W4ht\u22121)\nft = sigm(W5xt +W6ht\u22121)\not = sigm(W7xt +W8ht\u22121)\nmt = mt\u22121 \u2299 ft + it \u2299 i \u2032\nt\nht = mt \u2299 ot\nThe operator \u2299 denotes element-wise multiplication, the matrices W1, . . . ,W8 and the vector h0 are the parameters of the model, and all the nonlinearities are computed element-wise. Note that the LSTM these equations and their derivatives need to be implemented only once.\nIn a deep LSTM, each subsequent layer uses the h-sequence of the previous layer for its input sequence x. The deep LSTM defines a distribution over output sequences given an input sequence:\nP (B|A) =\nTB\u220f\nt=1\nP (Bt|A1, . . . , ATA , B1, . . . , Bt\u22121)\n\u2261\nTB\u220f\nt=1\nsoftmax(Wo \u00b7 ht\u22121+TA) \u22a4\u03b4Bt\nThe above equation assumes a deep LSTM whose input sequence is x = (A1, . . . , ATA , B1, . . . , BTB ), so ht denotes t-th element of the h-sequence of topmost LSTM, which is a function of (A1, . . . , ATA , B1, . . . , Bt\u2212TA). The matrix Wo consists of the vector representations of each output symbol and the symbol \u03b4b is a Kronecker delta with a dimension for each output symbol, so softmax(Wo \u00b7 ht\u22121+TA)\n\u22a4\u03b4Bt is precisely the Bt\u2019th element of the distribution defined by the softmax. Every output sequence terminates with a special end-of-sequence token which is necessary in order to define a distribution over sequences of variable lengths. We may sometimes use two different sets of LSTM parameters, one for the input sequence and one for the output sequence, as shown in Figure 1. When we use two sets of LSTM parameters, we say that the parameters are untied. Otherwise, when LSTMin = LSTMout, we say that they are tied. Stochastic gradient descent is used to maximize the training objective which is the average over the training set of the log probability of the correct output sequence given the input sequence."}, {"heading": "2.1 ADAPTATIONS FOR PARSING", "text": "To apply the model described above to parsing, we need to design an invertible way of converting the parse tree into a sequence (linearization). We experimented with two ways of using the network\nUnder review as a conference paper at ICLR 2015\nwhich both rely on linearizing the tree in a very simple way following a depth-first traversal order, as depicted in Figure 2.\nThe most basic way of using the above model for parsing works as follows. First, the network consumes the full sentence in a linear left-to-right sweep, creating a vector in memory. Then, it outputs the linearized parse tree using only the information in this memory vector. We call this \u201cbasic encoding\u201d in the remainder of the paper. An example run of this model on the sentence \u201cGo.\u201d with a 2-layer network looks as follows.\nGo .\nLSTM1in\nLSTM2in\nEND (S (VP VB )VP . )S\nLSTM1out\nLSTM2out\n(S (VP VB )VP . )S END\nWhile the above method is the most straightforward application of the translation model, we experimented with ways of improving the input format for the network. Inspired by shift-reduce transition-based parsers (Nivre, 2008; Zhu et al., 2013), we introduce a stack where the words are provided to the LSTM as additional inputs during decoding.\nFirst, as before, the network consumes the full sentence in a linear left-to-right sweep, creating a vector in memory. Then, while outputing the linearized parse tree, we maintain a stack of words. The word on the top of the stack is provided to the network as an additional feature at each time step. When the network produces the symbol \u22a5, we pop a word from the stack. We train the network to pop words after reaching the common ancestor of the previous and next word in the depth-first tree traversal (cf. Figure 2). Note that since we do not employ complex transition strategies our model only has access to the linear ordering of the words. Furthermore, we provide only the current word as an input. We call this \u201cstack encoding\u201d in the remainder of the paper. An example run of this model on the sentence \u201cGo.\u201d looks as follows.\n\u3008Go|\u3009 \u3008.|\u3009\nLSTM1in\nLSTM2in\n\u3008Go|END\u3009\u3008Go|(S\u3009\u3008Go|(VP\u3009\u3008Go|VB\u3009\u3008Go|)VP\u3009 \u3008.|\u22a5\u3009 \u3008.|.\u3009 \u3008.|)S\u3009 \u3008.|\u22a5\u3009\nLSTM1out\nLSTM2out\n(S (VP VB )VP \u22a5 . )S \u22a5 END\nUnder review as a conference paper at ICLR 2015"}, {"heading": "3 EXPERIMENTS", "text": "We performed a number of experiments with both the basic encoding and the stack encoding model, as described above. We experimented with tied and untied input and output LSTMs and measured the influence of using pre-trained word embeddings and fine-tuning on in-domain data."}, {"heading": "3.1 TRAINING DATA", "text": "In our experiments we focus on English, but the model could easily be applied to other languages. Our goal is to build a robust and domain-independent parser, that can be used to process text from various genres. To this end, we train and evaluate on the union of a number of publicly available treebanks. We use the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov & McDonald, 2012) and the updated and corrected Question Treebank (Judge et al., 2006).1 Note that the popular Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) as part of the OntoNotes corpus. In total, we train on \u223c90K sentences and evaluate on \u223c13K sentences. We term the 90K set the Treebank union. We chose this setup since it allows us to train and evaluate on a more diverse set of sentences, rather than overfitting the WSJ evaluation set which has been in use for 20 years and is not representative for text encountered on the web (Petrov & McDonald, 2012). To compare to an established baseline parser, we also train and test the publicly available BerkeleyParser (Petrov et al., 2006) on the Treebank union.\nAdditionally, we use a corpus of \u223c7 million unlabeled sentences sampled from the web. These sentences are parsed with an in-house reimplementation of the BerkeleyParser trained on the Treebank union. We include our reimplementation as an additional baseline, and also add a self-training experiment where it is trained on its own output, similar to Huang & Harper (2009). These automatically parsed sentences are used as additional training data in our experiments and result in large performance gains (see Table 1). We will release this data to facilitate replication of our experiments.\nWe use EVALB for evaluation and report F1 scores on three data sets: (1) WSJ 22: section 22 of the Penn Treebank, (2) Questions: 1000 sentences from the Question Treebank, (3) Web: the first half of each domain from Web Treebank. We also test our best model on section 23 of the Penn Treebank. More details on the experimental setup can be found in the Appendix.\nWe do not apply any special preprocessing to the data. In particular, we do not binarize the parse trees or handle unaries in any specific way. We also treat unknown words in a naive way: we map all words beyond our 50K vocabulary to a single UNK token. This potentially underestimates our final results, but keeps our framework task-independent."}, {"heading": "3.2 MODELS AND PARAMETERS", "text": "In our experiments we used a deep LSTM model with either 3 layers with 640 units, or 4 layers with 512 units per layer. All our architectures have about 4000 dimensions for representing the input sentence dimensions. The LSTM has an input vocabulary of 50K and an output vocabulary of 100 symbols, which results in 34M parameters. The exact architecture and optimization parameters are further discussed in the Appendix. We used a beam of size 20 during decoding, but a beam size of 2 achieved almost identical results (see below). We also found it useful to reverse the input sentences but not their parse trees, similarly to Sutskever et al. (2014), and we did it in all of our experiments. Not reversing the input had a negative impact on our development set of up to 2% absolute F1.\nA basic encoding LSTM produces malformed trees in 10% of the test cases. However, a simple change to the decoder, in which we do not consider outputs that do not consume all the input words, eliminates all the malformed trees. The stack encoding LSTM implicitly enforces the consumption of all the input words, so no further modification of the decoder is needed. In the few cases where the LSTM outputs a malformed tree, we simply add brackets to either the beginning or the end of the tree in order to make it balanced.\nTable 1 presents some baseline numbers for the BerkeleyParser and our reimplementation, as well as several LSTM versions. The main observation is that, despite the lack of prior knowledge encoded\n1 All treebanks are available through the Linguistic Data Consortium (LDC): OntoNotes (LDC2013T19), English Web Treebank (LDC2012T13) and Question Treebank (LDC2012R121).\nUnder review as a conference paper at ICLR 2015\nin the model, both LSTM with basic encoding and stack encoding perform as well as the BerkeleyParser on WSJ section 22 (which was used as a development set), as well as in the questions dataset. On the more challenging web data, the basic encoding suffers \u2013 likely as a result of early mistakes propagating forward through the left to right naive decoder. Using stack encoding, these early mistakes are likely less of an issue. We trained a stack encoding LSTM only on Treebank data as well. This model (cf. first line of Table 1) does not achieve good results, so a large dataset parsed with BerkeleyParser is indeed crucial for this method. We also report an ensemble of 10 basic encoding LSTMs, which is an effective way to improve the performance of neural networks. Indeed, all the scores substantially improve over the in-house BerkeleyParser \u2013 used to generate the training set \u2013 on WSJ 22 and questions, and closes most of the gap on the web dataset. Lastly, untying the parameters was effective for the basic encoding scheme, but unnecessary with stack encoding."}, {"heading": "3.3 EFFECT OF SENTENCE LENGTH AND BEAM SIZE", "text": "An important concern with the sequence-to-sequence LSTM is that it may not be able to handle long sentences well. We determine the extent of this problem by partitioning the validation set by length, and evaluating the LSTM and the BerkeleyParser on sentences of each length. The results, which are presented in Figure 3, clearly show that the LSTM\u2019s performance does not deteriorate on long sentences, as compared to the performance of the BerkeleyParser.\nAs for the effects of beam size in the decoder, using no beam search at all (i.e., beam size of 1) lowers the score significantly. The numbers below show the F1 scores on WSJ 22 for different beam sizes for the stack encoding 3x640 tied and fine-tuned model.\nBeam size 1 2 3 6 9 20 F1 score 90.7 91.0 91.0 91.1 91.2 91.3"}, {"heading": "3.4 EFFECT OF PRE-TRAINING AND FINE-TUNING", "text": "In addition to the results above, where we train only on the joint set of 7M+90K sentences and with a vocabulary of size 50K, we experimented with two additional variations.\nFor one, instead of feeding the network with tokens from a 50K vocabulary and learning an embedding for them, we tried to provide the network with already embedded words. These skip-gram\nUnder review as a conference paper at ICLR 2015\nembeddings of size 512 were pre-trained using word2vec (Mikolov et al., 2013) on a 10B-word corpus, and kept fix while training the other parameters of the network.\nIn addition to pre-training, we also experimented with fine-tuning the model only on the 40K set of WSJ training sentences. This gives the network a chance to correct certain errors, but it must be stopped early to prevent overfitting.\nWe measured the influence of these factors on a stack-driven model with 3 LSTM layers of size 640. The version with no pre-training but with fine-tuning corresponds to a row in the table above. The influence of pre-training and fine-tuning on the F1 score on our development set (section 22 of WSJ) is summarized below. We write PT to stand for \u201cpre-training\u201d and FT to stand for \u201cfine-tuning\u201d.\nModel No PT, No FT PT, No FT No PT, FT PT, FT F1 score 90.9 90.5 91.3 91.2\nOne can see that fine-tuning brings moderate gains while pre-training has almost no influence. It is worth noting that pre-training significantly reduces the number of trainable parameters of the model."}, {"heading": "3.5 FINAL RESULTS", "text": "It is difficult to directly compare our results to those reported in previous work since our training setup differs. To compare to a publicly available state-of-the-art parser, we trained the BerkeleyParser (Petrov et al., 2006) on our experimental setup. Table 2 shows performance on section 23 from the Penn Treebank when training on our setup on the left, and results from other papers on the right. Additionally, we compare to variants of the BerkeleyParser that use self-training on unlabeled data (Huang & Harper, 2009), or built an ensemble of multiple parsers (Petrov, 2010), or combine both techniques. It is interesting to see that additional treebank data does not help much on Section 23 of WSJ, but it helps a lot for parsing out-of-domain text. Finally, we include the best linear-time parser in the literature, the transition-based parser of Zhu et al. (2013).\nIt is encouraging to see that our LSTM models are competitive with these highly optimized parsers that have received a lot of task specific tuning. Moreover, when running on GPUs the LSTM model has a significant speed advantage. Using batches of 128 sentences on a generic, unoptimized decoder on a GPU we decode about 160 sentences from WSJ per second. This is better than the speed reported in Figure 4 of (Hall et al., 2014) even though we run on sentences of all lengths (not only under 40), our model achieves better accuracy, and their code is highly optimized."}, {"heading": "4 ANALYSIS", "text": "To shed some light on what the LSTM is learning, we examined some of the induced latent representations. We looked at individual words, non-terminal labels and also entire sentences.\nTable 3 shows some selected examples that exhibit interesting patterns for words and their closest neighbors by cosine similarity. The table contains the closest three neighbors for the embeddings learnt by the LSTM and contrasts them to the embeddings learnt by a skip-gram model by Mikolov et al. (2013). It is interesting to observe that the neighbors differ quite significantly between the two methods. While the skip-gram neighbors are very topical and semantic in nature, the LSTM neighbors are a lot more syntactic. For example, the neighbors of give and wait are different forms of the same verbs for the skip-grams, but are different verbs that take the same number of arguments for the LSTM. Similarly, book is grouped with other words that can be both nouns and\nUnder review as a conference paper at ICLR 2015\nverbs by the LSTM, while the skip-gram embedded it with its noun synonyms. It is also interesting to see that Angeles is grouped with the second part of other noun noun compound city names by the LSTM, but with LA and Los by the skip-gram.\nSince the non-terminals of the parse trees are also embedded we can examine their neighbors as well. In general, the distance between the various non-terminal labels is much larger than between different words. Only a few non-terminals are close to each other, for example (S is close to (SQ, (SINV and (SBARQ, but most others are pretty far apart. For the part of speech tags, only NNP has close neighbors, namely NN, NNPS, and CD.\nFinally, we can examine full sentence embeddings and their neighbors. For the sentences in the development set, we find that the LSTM groups sentences of similar lengths. Furthermore there is some syntactic and semantic resemblance, but since the sentences are quite diverse there are few interesting neighbors. We therefore supplemented the development set with a few manually generated sentences. In particular, we generated sentences with identical syntactic structure and only lexical differences, e.g. I have a {brother, sister}, who has a {brother, sister}. The LSTM grouped these sentences in close proximity and there was very little difference due to the different lexical choices. Some of the other manually generated sentences included prepositional phrase attachment ambiguities: I ate the pasta with the {tomatoes, cheese, pesto, fork, spoon}. It was exciting to see that the sentences where the prepositional phrase modifies the noun pasta were grouped closer to each other than the ones where the prepositional phrase modifies the verb ate. Unfortunately, the predicted parse trees all had the prepositional phrase attaching low and modifying the noun.\nTo see how well the LSTM can handle embedded clauses and nested structures, we tried parsing sentences of the form I have a (brother who has a)n brother. The LSTM can parse such sentences for n up to 12 without forgetting to close all clauses, which suggests that it has learned to model context-free structures for a reasonably deep stack. This further confirms the finding that LSTMs are capable of handling long and complex sentences in a robust way."}, {"heading": "5 RELATED WORK", "text": "The task of syntactic constituency parsing has received a tremendous amount of attention in the last 20 years. Traditional approaches to constituency parsing rely on probabilistic context-free grammars (CFGs). The focus in these approaches is on devising appropriate smoothing techniques for highly lexicalized and thus rare events (Collins, 1997) or carefully crafting the model structure (Klein & Manning, 2003). Petrov et al. (2006) partially alleviate the heavy reliance on manual modeling of linguistic structure by using latent variables to learn a more articulated model. However, their model still depends on a CFG backbone and is thereby potentially restricted in its capacity.\nEarly neural network approaches to parsing, for example by Henderson (2003; 2004) also relied on strong linguistic insights. Titov & Henderson (2007) and Henderson & Titov (2010) introduced Incremental Sigmoid Belief Networks for syntactic parsing. By constructing the model structure incrementally, they are able to avoid making strong independence assumptions but inference becomes intractable. To avoid complex inference methods, Collobert (2011) propose a recurrent neural network where parse trees are decomposed into a stack of independent levels. Unfortunately, this decomposition breaks for long sentences and their accuracy on longer sentences falls quite significantly behind the state of the art. Socher et al. (2011) used a tree-structured neural network to score\nUnder review as a conference paper at ICLR 2015\ncandidate parse trees. Their model however relies again on the CFG assumption and furthermore can only be used to score candidate trees rather than for full inference.\nOur LSTM model significantly differs from all these models, as it makes no assumptions about the task. As a sequence-to-sequence prediction model it is somewhat related to the incremental parsing models, pioneered by Ratnaparkhi (1997) and extended by Collins & Roark (2004). Such linear time parsers however typically need some task-specific constraints and might build up the parse in multiple passes. Relatedly, Zhu et al. (2013) present excellent parsing results with a single left-toright pass, but require a stack to explicitly delay making decisions and a parsing-specific transition strategy in order to achieve good parsing accuracies. The LSTM in contrast uses its short term memory to model the complex underlying structure that connects the input-output pairs.\nLSTMs might not be the only models capable of such modeling. Recently, researchers have developed a number of neural network models that can be applied to general sequence-to-sequence problems. Graves (2013) was the first to propose a differentiable attention mechanism for the general problem of handwritten text synthesis, although his approach assumed a monotonic alignment between the input and the output sequences. Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence. Essentially the same model has been used by Vinyals et al. (2014) to successfully learn to generate image captions. Even though most of these models could be applied to parsing, we chose the model of Sutskever et al. (2014) because it is the simplest architecture that can solve general sequence-tosequence problems and because it achieves the best performance on a large scale machine translation task (Luong et al., 2014). It is also able to embed entire sentences in a continuous vector space. Finally, Ghahramani (1990) applied a similar recurrent neural network to the problem of syntactic parsing 20 years ago."}, {"heading": "6 CONCLUSIONS", "text": "In this work, we have shown that the generic sequence-to-sequence approach of Sutskever et al. (2014) can achieve competitive results on syntactic constituent parsing with relatively little effort or tuning. Our results highlight the importance of large datasets when using large deep neural networks that do not contain domain-specific, hand-engineered knowledge. Lacking prior knowledge, our system was unable to learn an accurate parser from the treebank union alone (cf. first line of Table 1). Fortunately, there is a very simple way to benefit from the hand-engineering that goes into more conventional parsers: We use these parsers to create additional training data. This allows us to benefit from the prior knowledge in other parsing systems without putting any constraints on the form of the internal representations used by the LSTM. The fact that the LSTM was able to match and even outperform the BerkeleyParser that was used to annotate the 7M sentences suggests that this simple way of stealing prior knowledge is very effective, though computationally expensive. In the long run, however, generic learning algorithms for deep recurrent neural networks would be much more useful if they could learn from smaller datasets.\nAcknowledgement. We would like to thank Amin Ahmad, Dan Bikel and Jonni Kanerva."}, {"heading": "7 DETAILED EXPERIMENTAL SETUP", "text": "In this section we present details regarding our experimental setup. We use the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov & McDonald, 2012) and the updated and corrected Question Treebank (Judge et al., 2006). All treebanks are available through the Linguistic Data Consortium (LDC): OntoNotes (LDC2013T19), English Web Treebank (LDC2012T13) and Question Treebank (LDC2012R121). Table 7 presents the splits into training, development and test data that we used for each treebank. We followed standard splits whenever possible, but had to devise also our own splits when no prior split was established. This was the case for the non-WSJ portions of the OntoNotes corpus, where we divided the data into shards of 10 sentences and selected the first 8 for training, while reserving the 9th for development and the 10th for test (neither of which we used in our experiments).\nWe trained a deep LSTM model with Stochastic Gradient Descent (without momentum). We initialize all the weights following a random uniform distribution between -0.08 and 0.08, and use a learning rate of 0.4 with an exponential decay which drops the learning rate by half every 1.5 epochs, and learning typically stops after 5 epochs. We also constrain our gradients by clipping them to be inside a sphere of radius 5. Our model has 512 cells, and a stack of 4 LSTMs, which with a 50K input vocabulary, and 100 output softmax, yields 34M parameters when the parameters of the encoder and decoder have tied parameters, and 42M when they are untied. We also experimented with a different architecture (which was used for the stack encoding LSTM model), which has 640 cells and a stack of 3 LSTMs. The two architectures make no difference in terms of performance.\nFor evaluation we used EVALB with new.prm as the configuration for the WSJ and Question Treebank. For the Web Treebank we followed the SANCL shared task evaluation and used sancl.prm, which removes some of the special handling around punctuation part-of-speech tags.\nThis figure \"lenchart.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1412.7449v1\nThis figure \"lstm.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1412.7449v1"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["Collins", "Michael"], "venue": "In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Collins and Michael.,? \\Q1997\\E", "shortCiteRegEx": "Collins and Michael.", "year": 1997}, {"title": "Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904)", "author": ["Collins", "Michael", "Roark", "Brian"], "venue": "Main Volume,", "citeRegEx": "Collins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Deep learning for efficient discriminative parsing", "author": ["Collobert", "Ronan"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Collobert and Ronan.,? \\Q2011\\E", "shortCiteRegEx": "Collobert and Ronan.", "year": 2011}, {"title": "A neural network for learning how to parse tree adjoining grammar", "author": ["Ghahramani", "Zoubin"], "venue": "B.S.Eng Thesis, University of Pennsylvania,", "citeRegEx": "Ghahramani and Zoubin.,? \\Q1990\\E", "shortCiteRegEx": "Ghahramani and Zoubin.", "year": 1990}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Sparser, better, faster gpu parsing", "author": ["Hall", "David", "Berg-Kirkpatrick", "Taylor", "Canny", "John", "Klein", "Dan"], "venue": "In ACL,", "citeRegEx": "Hall et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["Henderson", "James"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Henderson and James.,? \\Q2003\\E", "shortCiteRegEx": "Henderson and James.", "year": 2003}, {"title": "Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904)", "author": ["Henderson", "James"], "venue": "Main Volume,", "citeRegEx": "Henderson and James.,? \\Q2004\\E", "shortCiteRegEx": "Henderson and James.", "year": 2004}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["Henderson", "James", "Titov", "Ivan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Henderson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: The 90% solution", "author": ["Hovy", "Eduard", "Marcus", "Mitchell", "Palmer", "Martha", "Ramshaw", "Lance", "Weischedel", "Ralph"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL, Short Papers,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Self-training PCFG grammars with latent annotations across languages", "author": ["Huang", "Zhongqiang", "Harper", "Mary"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Self-training with products of latent variable grammars", "author": ["Huang", "Zhongqiang", "Harper", "Mary", "Petrov", "Slav"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Questionbank: Creating a corpus of parse-annotated questions", "author": ["Judge", "John", "Cahill", "Aoife", "van Genabith", "Josef"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Judge et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Judge et al\\.", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In EMNLP, pp. 1700\u20131709,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Dan", "Manning", "Christopher D"], "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong", "Thang", "Sutskever", "Ilya", "Le", "Quoc V", "Vinyals", "Oriol", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Santorini", "Beatrice", "Marcinkiewicz", "Mary Ann"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Nivre", "Joakim"], "venue": "Comput. Linguist.,", "citeRegEx": "Nivre and Joakim.,? \\Q2008\\E", "shortCiteRegEx": "Nivre and Joakim.", "year": 2008}, {"title": "Products of random latent variable grammars", "author": ["Petrov", "Slav"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Petrov and Slav.,? \\Q2010\\E", "shortCiteRegEx": "Petrov and Slav.", "year": 2010}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "Slav", "McDonald", "Ryan"], "venue": "Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov", "Slav", "Barrett", "Leon", "Thibaux", "Romain", "Klein", "Dan"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "A linear observed time statistical parser based on maximum entropy models", "author": ["Ratnaparkhi", "Adwait"], "venue": "In Second Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ratnaparkhi and Adwait.,? \\Q1997\\E", "shortCiteRegEx": "Ratnaparkhi and Adwait.", "year": 1997}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Manning", "Chris", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Constituent parsing with incremental sigmoid belief networks", "author": ["Titov", "Ivan", "Henderson", "James"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Zhu", "Muhua", "Zhang", "Yue", "Chen", "Wenliang", "Min", "Jingbo"], "venue": "In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers),", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 27, "context": "Recently, Sutskever et al. (2014) presented a domainindependent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem.", "startOffset": 10, "endOffset": 34}, {"referenceID": 27, "context": "This was first demonstrated on the English-to-French translation task (Sutskever et al., 2014), but the same approach also works for evaluating short python programs (Zaremba & Sutskever, 2014).", "startOffset": 70, "endOffset": 94}, {"referenceID": 18, "context": "It does, however, achieve high performance on a large scale machine translation task (Luong et al., 2014).", "startOffset": 85, "endOffset": 105}, {"referenceID": 24, "context": "Our main results are as follows: We train a deep LSTM model with 34M parameters on a dataset consisting of 90K sentences obtained from various treebanks and 7M sentences from the web that are automatically parsed with the BerkeleyParser (Petrov et al., 2006).", "startOffset": 237, "endOffset": 258}, {"referenceID": 27, "context": "A common criticism of the sequence-to-sequence approach of Sutskever et al. (2014) is that it is fundamentally incapable of dealing with long inputs and outputs, due to its need to store the entire input sequence in its short-term memory.", "startOffset": 59, "endOffset": 83}, {"referenceID": 27, "context": "Figure 1: A schematic outline of the sequence-to-sequence model of Sutskever et al. (2014). A deep input LSTM reads the input sequence A1, A2, A3 one token at a time and encodes it as its final hidden state vector.", "startOffset": 67, "endOffset": 91}, {"referenceID": 31, "context": "Inspired by shift-reduce transition-based parsers (Nivre, 2008; Zhu et al., 2013), we introduce a stack where the words are provided to the LSTM as additional inputs during decoding.", "startOffset": 50, "endOffset": 81}, {"referenceID": 12, "context": "We use the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov & McDonald, 2012) and the updated and corrected Question Treebank (Judge et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": ", 2006), the English Web Treebank (Petrov & McDonald, 2012) and the updated and corrected Question Treebank (Judge et al., 2006).", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "1 Note that the popular Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) as part of the OntoNotes corpus.", "startOffset": 73, "endOffset": 94}, {"referenceID": 24, "context": "To compare to an established baseline parser, we also train and test the publicly available BerkeleyParser (Petrov et al., 2006) on the Treebank union.", "startOffset": 107, "endOffset": 128}, {"referenceID": 27, "context": "We also found it useful to reverse the input sentences but not their parse trees, similarly to Sutskever et al. (2014), and we did it in all of our experiments.", "startOffset": 95, "endOffset": 119}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.", "startOffset": 2, "endOffset": 23}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.4 Ensemble of 10 basic encoding LSTMs 91.6 Petrov (2010) WSJ only ensemble 91.", "startOffset": 2, "endOffset": 92}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.4 Ensemble of 10 basic encoding LSTMs 91.6 Petrov (2010) WSJ only ensemble 91.8 Petrov et al. (2006) Treebank union only 90.", "startOffset": 2, "endOffset": 136}, {"referenceID": 21, "context": "5 Petrov et al. (2006) WSJ only 90.4 Ensemble of 10 basic encoding LSTMs 91.6 Petrov (2010) WSJ only ensemble 91.8 Petrov et al. (2006) Treebank union only 90.4 Huang & Harper (2009) semi-supervised 91.", "startOffset": 2, "endOffset": 183}, {"referenceID": 13, "context": "3 Huang et al. (2010) semi-supervised ensemble 92.", "startOffset": 2, "endOffset": 22}, {"referenceID": 13, "context": "3 Huang et al. (2010) semi-supervised ensemble 92.4 Zhu et al. (2013) WSJ only 90.", "startOffset": 2, "endOffset": 70}, {"referenceID": 13, "context": "3 Huang et al. (2010) semi-supervised ensemble 92.4 Zhu et al. (2013) WSJ only 90.4 Zhu et al. (2013) semi-supervised 91.", "startOffset": 2, "endOffset": 102}, {"referenceID": 20, "context": "embeddings of size 512 were pre-trained using word2vec (Mikolov et al., 2013) on a 10B-word corpus, and kept fix while training the other parameters of the network.", "startOffset": 55, "endOffset": 77}, {"referenceID": 24, "context": "To compare to a publicly available state-of-the-art parser, we trained the BerkeleyParser (Petrov et al., 2006) on our experimental setup.", "startOffset": 90, "endOffset": 111}, {"referenceID": 7, "context": "This is better than the speed reported in Figure 4 of (Hall et al., 2014) even though we run on sentences of all lengths (not only under 40), our model achieves better accuracy, and their code is highly optimized.", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "To compare to a publicly available state-of-the-art parser, we trained the BerkeleyParser (Petrov et al., 2006) on our experimental setup. Table 2 shows performance on section 23 from the Penn Treebank when training on our setup on the left, and results from other papers on the right. Additionally, we compare to variants of the BerkeleyParser that use self-training on unlabeled data (Huang & Harper, 2009), or built an ensemble of multiple parsers (Petrov, 2010), or combine both techniques. It is interesting to see that additional treebank data does not help much on Section 23 of WSJ, but it helps a lot for parsing out-of-domain text. Finally, we include the best linear-time parser in the literature, the transition-based parser of Zhu et al. (2013). It is encouraging to see that our LSTM models are competitive with these highly optimized parsers that have received a lot of task specific tuning.", "startOffset": 91, "endOffset": 758}, {"referenceID": 20, "context": "The table contains the closest three neighbors for the embeddings learnt by the LSTM and contrasts them to the embeddings learnt by a skip-gram model by Mikolov et al. (2013). It is interesting to observe that the neighbors differ quite significantly between the two methods.", "startOffset": 153, "endOffset": 175}, {"referenceID": 23, "context": "Petrov et al. (2006) partially alleviate the heavy reliance on manual modeling of linguistic structure by using latent variables to learn a more articulated model.", "startOffset": 0, "endOffset": 21}, {"referenceID": 26, "context": "Socher et al. (2011) used a tree-structured neural network to score", "startOffset": 0, "endOffset": 21}, {"referenceID": 31, "context": "Relatedly, Zhu et al. (2013) present excellent parsing results with a single left-toright pass, but require a stack to explicitly delay making decisions and a parsing-specific transition strategy in order to achieve good parsing accuracies.", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "(2014) because it is the simplest architecture that can solve general sequence-tosequence problems and because it achieves the best performance on a large scale machine translation task (Luong et al., 2014).", "startOffset": 186, "endOffset": 206}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al.", "startOffset": 7, "endOffset": 30}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition.", "startOffset": 7, "endOffset": 183}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence.", "startOffset": 7, "endOffset": 259}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence. Essentially the same model has been used by Vinyals et al. (2014) to successfully learn to generate image captions.", "startOffset": 7, "endOffset": 486}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence. Essentially the same model has been used by Vinyals et al. (2014) to successfully learn to generate image captions. Even though most of these models could be applied to parsing, we chose the model of Sutskever et al. (2014) because it is the simplest architecture that can solve general sequence-tosequence problems and because it achieves the best performance on a large scale machine translation task (Luong et al.", "startOffset": 7, "endOffset": 644}, {"referenceID": 0, "context": "Later, Bahdanau et al. (2014) introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and Chorowski et al. (2014) applied the same model to speech recognition. Kalchbrenner & Blunsom (2013) used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used an RNN to produce the output sentence. Essentially the same model has been used by Vinyals et al. (2014) to successfully learn to generate image captions. Even though most of these models could be applied to parsing, we chose the model of Sutskever et al. (2014) because it is the simplest architecture that can solve general sequence-tosequence problems and because it achieves the best performance on a large scale machine translation task (Luong et al., 2014). It is also able to embed entire sentences in a continuous vector space. Finally, Ghahramani (1990) applied a similar recurrent neural network to the problem of syntactic parsing 20 years ago.", "startOffset": 7, "endOffset": 944}], "year": 2014, "abstractText": "Syntactic parsing is a fundamental problem in computational linguistics and natural language processing. Traditional approaches to parsing are highly complex and problem specific. Recently, Sutskever et al. (2014) presented a domainindependent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem. In this work, we show that precisely the same sequence-to-sequence method achieves results that are close to state-of-the-art on syntactic constituency parsing, whilst making almost no assumptions about the structure of the problem.", "creator": "LaTeX with hyperref package"}}}