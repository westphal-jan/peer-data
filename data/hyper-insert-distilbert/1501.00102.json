{"id": "1501.00102", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2014", "title": "ModDrop: adaptive multi-modal gesture recognition", "abstract": "we present pioneering a method for achieving gesture detection technique and localisation synthesis based today on multi - scale and multi - modal deep learning. usually each visual modality captures significant spatial perception information at locating a particular spatial spatial scale ( variously such is as directional motion of actually the lower upper body or a hand ), and where the whole cognitive system visibly operates distinctly at three narrower temporal scales. key support to undertaking our pioneering technique is a training strategy which exploits : i ) careful statistical initialization knowledge of individual semantic modalities ; and ii ) gradual fusion involving random event dropping activation of partially separate channels ( dubbed moddrop ) providing for multiple learning cross - linking modality correlations while preserving every uniqueness and of each modality - like specific point representation. we present experiments notably on the 2001 chalearn 2014 participant looking pointedly at people challenge gesture recognition track, in question which we essentially placed them first out of the 17 teams. fusing highly multiple modalities focused at possessing several spatial and unique temporal geographic scales leads to observing a significant predicted increase in recognition rates, vastly allowing the naive model to thereby compensate significantly for errors of introducing the same individual classifiers as examples well as false noise in the separate channels. futhermore, the conceptual proposed moddrop visual training synthesis technique ensures robustness of the classifier vulnerable to missing signals in particular one or several channels constructed to produce meaningful effect predictions from executing any number such of available modalities. in physical addition, we demonstrate the applicability ratios of the proposed software fusion scheme to modalities of this arbitrary descriptive nature by using experiments resulting on only the same dataset augmented with audio.", "histories": [["v1", "Wed, 31 Dec 2014 09:55:43 GMT  (7779kb)", "http://arxiv.org/abs/1501.00102v1", "14 pages, 7 figures"], ["v2", "Sat, 6 Jun 2015 14:46:33 GMT  (3382kb,D)", "http://arxiv.org/abs/1501.00102v2", "14 pages, 7 figures"]], "COMMENTS": "14 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CV cs.HC cs.LG", "authors": ["natalia neverova", "christian wolf", "graham w taylor", "florian nebout"], "accepted": false, "id": "1501.00102"}, "pdf": {"name": "1501.00102.pdf", "metadata": {"source": "CRF", "title": "ModDrop: adaptive multi-modal gesture recognition", "authors": ["Natalia Neverova", "Christian Wolf", "Graham Taylor", "Florian Nebout"], "emails": ["firstname.surname@liris.cnrs.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n00 10\n2v 1\n[ cs\n.C V\n] 3\n1 D\nec 2\n01 4\nIndex Terms\u2014Gesture Recognition, Convolutional Neural Networks, Multi-modal Learning, Deep Learning\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "G ESTURE RECOGNITION is one of the central problemsin the rapidly growing fields of human-computer and human-robot interaction. Effective gesture detection and classification is challenging due to several factors: cultural and individual differences in tempos and styles of articulation, variable observation conditions, the small size of fingers in images taken in typical scenarios, noise in camera channels, infinitely many kinds of out-of-vocabulary motion, and real-time performance constraints.\nRecently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5]. Convolutional neural networks (ConvNets) [6] have excelled on several scientific competitions such as ILSVRC [3], Emotion Recognition in the Wild [7], Kaggle Dogs vs. Cats [2] and Galaxy Zoo. Taigman et al. [8] recently claimed to have reached human-level performance using ConvNets for face recognition. On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12]. It can be partially explained by lack of sufficiently large datasets and the high cost of data labeling in many practical areas, as well as increased modeling complexity brought about by the additional temporal dimension and the interdependencies it implies.\n\u2022 N. Neverova and C. Wolf are with INSA-Lyon, LIRIS, UMR5205, F69621, Universite\u0301 de Lyon, CNRS, France. E-mail: firstname.surname@liris.cnrs.fr \u2022 G. Taylor is with School of Engineering of University of Guelph, Canada. E-mail: gwtaylor@uoguelph.ca \u2022 F. Nebout is with Awabot, France. E-mail: florian.nebout@awabot.com\nThe first gesture-oriented dataset containing a sufficient amount of training samples for deep learning methods was proposed for the ChaLearn 2013 Challenge on Multi-modal Gesture Recognition. The deep learning method described in this paper placed first in the 2014 version of this competition [13].\nA core aspect of our approach is employing a multimodal convolutional neural network for classification of so-called dynamic poses of varying duration (i.e. temporal scales). Visual data modalities integrated by our algorithm include intensity and depth video, as well as articulated pose information extracted from depth maps (see Fig. 1). We make use of different data channels to decompose each gesture at multiple scales not only temporally, but also spatially, to provide context for upper-body motion and more fine-grained hand/finger articulation.\nIn this work, we pay special attention to developing an effective and efficient learning algorithm since learning large-scale multi-modal networks on a limited amount of labeled data is a formidable challenge. We also introduce an advanced training strategy, ModDrop, that makes the network\u2019s predictions robust to missing or corrupted channels.\nWe demonstrate that the proposed scheme can be augmented with more data channels of arbitrary nature by introducing audio into the classification framework.\nThe major contributions of the present work are the following: We (i) develop a deep learning-based multimodal and multi-scale framework for gesture detection, localization and recognition, which can be augmented with channels of an arbitrary nature (demonstrated by inclusion of audio); (ii) propose ModDrop for effective fusion of multiple modality channels, which targets learning crossmodality correlations while prohibiting false co-adaptations between data representations and ensuring robustness of the\n2\nclassifier to missing signals; and (iii) introduce an audioenhanced version of the ChaLearn 2014 LAP dataset."}, {"heading": "2 RELATED WORK", "text": "While having an immediate application in gesture recognition, this work addresses more general aspects of learning representations from raw data and multimodal fusion.\nGesture recognition Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].\nNear-range applications may require more accurate reconstruction of hand shapes. In this case, fitting a 3D hand model, as well as appearance-based algorithms provide more appropriate solutions. A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24]. In parallel, tracking-based approaches are advancing quickly [25], [26]. On the other hand, in [27] the authors proposed the Latent Regression Forest for coarse-to-fine search of joint positions. Finally, graphical models, exploring spatial relationships between body and hand parts, have recently attracted close attention [28], [29].\nMulti-modal aspects are of relevance in this domain. In [30], a combination of skeletal features and local occupancy patterns (LOP) were calculated from depth maps to describe hand joints. In [31], skeletal information was integrated in two ways for extracting HoG features from RGB and depth images: either from global bounding boxes containing a whole body or from regions containing an arm, a torso and a head. Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.\nRepresentation learning Various multi-layer architectures have been proposed in the context of motion analysis for learning (as opposed to\nhandcrafting) representations directly from data, either in a supervised or unsupervised way. Independent subspace analysis (ISA) [36] as well as autoencoders [37], [9] are examples of efficient unsupervised methods for learning hierarchies of invariant spatio-temporal features. Spacetime deep belief networks [38] produce high-level representations of video sequences using convolutional RBMs.\nVanilla supervised convolutional networks have also been explored in this context. A method proposed in [39] is based on low-level preprocessing of the video input and employs a 3D convolutional network for learning of midlevel spatio-temporal representations and classification. Recently, Karpathy et al. [10] have proposed a convolutional architecture for large-scale video classification operating at two spatial resolutions (fovea and context streams).\nMulti-modal fusion\nWhile in most practical applications, late fusion of scores output by several models offers a cheap and surprisingly effective solution [7], both late and early fusion of either final or intermediate data representations remain under active investigation.\nA significant amount of work on early combining of diverse feature types has been applied to object and action recognition. Multiple Kernel Learning (MKL) [40] has been actively discussed in this context. At the same time, as shown by [41], simple additive or multiplicative averaging of kernels may reach the same level of performance while being orders of magnitude faster.\nYe et al. [42] proposed a late fusion strategy compensating for errors of individual classifiers by minimising the rank of a score matrix. In follow-up work [43], they identified sample-specific optimal fusion weights by enforcing similarity in fusion scores for visually similar labeled and unlabeled samples. Xu et al. [44] introduced the Feature Weighting via Optimal Thresholding (FWOT) algorithm jointly optimising feature weights and thresholds. Nataranjan et al. [45] employed multiple strategies, including MKL-based combinations of features, Bayesian model combination, and weighted average fusion of scores from multiple systems.\nA number of deep architectures have recently been proposed specifically for multi-modal data. Ngiam et al. [46] employed sparse RBMs and bimodal deep antoencoders to learn cross-modality correlations in the context of audiovisual speech classification of isolated letters and digits. Srivastava et al. [47] used a multi-modal deep Boltzmann machine in a generative fashion to tackle the problem of integrating images and text annotations. Kahou et al. [7] won the 2013 Emotion Recognition in the Wild Challenge by training convolutional architectures on several modalities, such as facial expressions from video, audio, scene context and features extracted around mouth regions. Finally, in [48] the authors proposed a multi-modal convolutional network for gesture detection and classification from a combination of depth, skeletal information and audio.\n3"}, {"heading": "3 GESTURE CLASSIFICATION", "text": "On a dataset such as ChaLearn 2014 LAP, we face several key challenges: learning representations at multiple spatial and temporal scales, integrating the various modalities, and training a complex model when the number of labeled examples is not at web-scale like static image datasets (e.g. [3]). We start by describing how the first two challenges are overcome at an architectural level. Our training strategy addressing the last issue is described in Sec. 4.\nOur proposed multi-scale deep neural network consists of a combination of single-scale paths connected in parallel (see Fig. 2). Each path independently learns a representation and performs gesture classification at its own temporal scale given input from RGBD video and pose signals (an audio channel can be also added, if available). Predictions from all paths are aggregated through additive late fusion.\nTo differentiate among temporal scales, a notion of dynamic pose is introduced, meaning a sequence of video frames, synchronized across modalities, sampled with a given temporal stride s and concatenated to form a spatiotemporal 3D volume. Varying the value of s allows the model to leverage multiple temporal scales for prediction, accommodating differences in tempos and styles of articulation. Our model is therefore different from the one proposed in [4], where by \u201cmulti-scale\u201d Farabet et al. imply a multiresolution spatial pyramid rather than a fusion of temporal sampling strategies. Regardless of the stride s, we use the same number of frames (5) at each scale. Fig. 2 shows the paths used in this work. At each scale and for each dynamic pose, the classifier outputs a per-class score.\nAll available modalities, such as depth, gray scale video, articulated pose, and eventually audio, contribute to the network\u2019s prediction. Global appearance of each gesture instance is captured by the skeleton descriptor, while video streams convey additional information about hand shapes and their dynamics which are crucial for discriminating between gesture classes performed in similar body poses.\nDue to the high dimensionality of the data and the nonlinear nature of cross-modality structure, an immediate con-\ncatenation of raw skeleton and video signals is sub-optimal. However, initial discriminative learning of individual data representations from each isolated channel followed by fusion has proven to be efficient in similar tasks [46]. Therefore, we first learn discriminative data representations within each separate channel, followed by joint fine tuning and fusion by a meta-classifier independently at each scale. More details are given in Sec. 4. A shared set of hidden layers is employed at different levels for, first, fusing of \u201csimilar by nature\u201d gray scale and depth video streams and, second, combining the obtained joint video representation with the transformed articulated pose descriptor (and audio signal, if available)."}, {"heading": "3.1 Articulated pose", "text": "The full body skeleton provided by modern consumer depth cameras and associated middleware consists of 20 or fewer joints identified by their coordinates in a 3D coordinate system aligned with the depth sensor. For our purposes we exploit only 11 joints corresponding to the upper body.\nWe formulate a pose descriptor consisting of 7 logical subsets as described in [49]. Following [50], we first calculate normalized joint positions, as well as their velocities and accelerations, and then augment the descriptor with a set of characteristic angles and pairwise distances.\nThe skeleton is represented as a tree structure with the HipCenter joint playing the role of a root node. Its coordinates are subtracted from the rest of the vectors to eliminate the influence of position of the body in space. To compensate for differences in body sizes, proportions and shapes, we start from the top of the tree and iteratively normalize each skeleton segment to a corresponding average \u201cbone\u201d length estimated from all available training data. Once the normalized joint positions are obtained, we perform Gaussian smoothing along the temporal dimension (\u03c3=1, filter 5\u00d71) to decrease the influence of skeleton jitter.\nJoint velocities and joint accelerations are calculated as first and second derivatives of normalized joint positions.\nInclination angles are formed by all triples of anatomically connected joints plus two \u201cvirtual\u201d angles [49].\n4\nAzimuth angles provide additional information about the pose in the coordinate space associated with the body. We apply PCA on the positions of 6 torso joints. Then for each pair of connected bones, we calculate angles between projections of the second bone and the vector on the plane perpendicular to the orientation of the first bone.\nBending angles are a set of angles between a basis vector, perpendicular to the torso, and joint positions.\nFinally, we include pairwise distances between all normalized joint positions.\nCombined together, this produces a 183-dimensional pose descriptor for each video frame. Finally, each feature is normalized to zero mean and unit variance.\nA set of consequent 5 frame descriptors sampled with a given stride s are concatenated to form a 915-dimensional dynamic pose descriptor which is further used for gesture classification. The two subsets of features involving derivatives contain dynamic information and for dense sampling may be partially redundant as several occurrences of the same frame are stacked when a dynamic pose descriptor is formulated. Although theoretically unnecessary, this is beneficial when the amount of training data is limited."}, {"heading": "3.2 Depth and intensity video", "text": "Two video streams serve as a source of information about hand pose and finger articulation. Bounding boxes containing images of hands are cropped around positions of the RightHand and LeftHand joints. To eliminate the influence of the person\u2019s position with respect to the camera and keep the hand size approximately constant, the size of each bounding box is normalized by the distance between the hand and the sensor.\nWithin each set of frames forming a dynamic pose, hand position is stabilized by minimizing inter-frame squareroot distances calculated as a sum over all pixels, and corresponding frames are concatenated to form a single spatio-temporal volume. The color stream is converted to gray scale, and both depth and intensity frames are normalized to zero mean and unit variance. Left hand videos are flipped about the vertical axis and combined with right hand instances in a single training set.\nDuring modality-wise pre-training, video pathways are adapted to produce predictions for each hand, rather than for the whole gesture. Therefore, we introduce an additional step to eliminate possible noise associated with switching from one active hand to another. For one-handed gesture classes, we detect the active hand and adjust the class label for the inactive one. In particular, we estimate the motion trajectory length of each hand using the respective joints provided by the skeleton stream (summing lengths of hand trajectories projected to the x and y axes):\n\u2206 =\n5 \u2211\nt=2\n(|x(t)\u2212 x(t \u2212 1)|+ |y(t)\u2212 y(t\u2212 1)|), (1)\nwhere x(t) is the x-coordinate of a hand joint (either left or right) and y(t) is its y-coordinate. Finally, the hand with a greater value of \u2206 is assigned the label class, while the other hand is assigned the zero-class \u201cno action\u201d label.\nFor each channel and each hand, we perform 2-stage convolutional learning of data representations independently (first in 3D, then in 2D, see Fig. 3) and fuse the two streams with a set of fully connected hidden layers. Parameters of the convolutional and fully-connected layers at this step are shared between the right hand and left hand pathways. Our experiments have demonstrated that relatively early fusion of depth and intensity features leads to a significant increase\n5\nin performance, even though the quality of predictions obtained from each channel alone is unsatisfactory."}, {"heading": "3.3 Audio stream", "text": "Recent advances in the field of speech processing have demonstrated that using weakly preprocessed raw audio data in combination with deep learning leads to higher performance relative to state-of-the-art systems based on hand crafted features (typically from the family of Melfrequency cepstral coefficients, or MFCC). Deng et al. [51] demonstrated the advantage of using primitive spectral features, such as 2D spectrograms, in combination with deep autoencoders. Ngiam et al. [46] applied the same strategy to the task of multi-modal speech recognition while augmenting the audio signal with visual features. Further experiments from Microsoft [51] have shown that ConvNets appear to be especially efficient in this context since they allow the capture and modeling of structure and invariances that are typical for speech.\nComparative analysis of our previous approach [48] based on phoneme recognition from sequences of MFCC features and a deep learning framework has demonstrated that the latter strategy allows us to obtain significantly better performance on the ChaLearn dataset (see Sec. 7 for more details). Therefore, in this work, the audio signal is processed in the same manner as video data, i.e. by feature learning within a convolutional architecture.\nTo preprocess, we perform basic noise filtering and speech detection by thresholding the raw signal along the absolute value of the amplitude (\u03c41). Short, isolated peaks of duration less than \u03c42 are also ignored during training. We apply a short-time Fourier transform on the raw audio signal to obtain a 2D local spectrogram which is further transformed to the Mel-scale to produce 40 log filterbanks on the frequency range from 133.3 to 6855.5 Hz, i.e. the zero-frequency component is eliminated. In order to synchronize the audio and visual signals, the size of the Hamming window is chosen to correspond to the duration of L1 frames with half-frame overlap. A typical output is illustrated in Fig. 4. As it was experimentally demonstrated by [51], the step of the scale transform is important. Even state-of-the-art deep architectures have difficulty learning these kind of non-linear transformations.\nA one-layer convolutional network in combination with two fully-connected layers form the corresponding path which we, as before, pretrain for preliminary gesture classification from short utterances. The output of the penultimate layer provides audio features for data fusion and modeling temporal dependencies (see Sec. 4)."}, {"heading": "4 TRAINING PROCEDURE", "text": "In this section we describe the most important architectural solutions that were critical for our multi-modal setting: per-modality pre-training and aspects of fusion such as the initialization of shared layers. Also, we introduce the concept of multi-modal dropout (ModDrop), which makes the network less sensitive to loss of one or more channels.\nPretraining Depending on the source and physical nature of a signal, input representation of any modality is characterized by its dimensionality, information density, and associated correlated and uncorrelated noise. Accordingly, a monolithic network taking as an input a combined collection of features from all channels is suboptimal, since a uniform distribution of parameters over the input is likely to overfit one subset of features and underfit the others. Here, performance-based optimization of hyper-parameters may resolve in cumbersome architectures requiring sufficiently larger amounts of training data and computational resources at training and test times. Furthermore, blind fusion of fundamentally different signals at early stages has a high risk of learning false cross-modality correlations and dependencies among them (see Sec. 7). To capture complexity within each channel, separate pretraining of input layers and optimization of hyper parameters for each subtask are required.\nRecall Fig. 3 illustrating a single-scale deep multi-modal convolutional network. Initially it starts with six separate pathways: depth and intensity video channels for right (V1) and left (V2) hands, a mocap stream (M) and an audio stream (A). From our observations, inter-modality fusion is effective at early stages if both channels are of the same nature and convey complementary information. On the other hand, mixing modalities which are weekly correlated, is rarely beneficial until the final stage. Accordingly, in our architecture, two video channels corresponding to each hand (layers HLV1 and HLV2) are fused immediately after feature extraction. We postpone any attempt to capture cross-modality correlations of complementary skeleton motion, hand articulation and audio until the shared layer HLS.\nInitialization of the fusion process Assuming the weights of the modality-specific paths are pre-trained, the next important issue is determining a fusion strategy. Pre-training solves some of the problems related to learning in deep networks with many parameters. However, direct fully-connected wiring of pre-trained paths to the shared layer in large-scale networks is not effective, as the high degrees of freedom afforded by the fusion process may lead to a quick degradation of pre-trained connections. We therefore proceed by initializing the shared layer such that a given hard-wired fusion strategy is performed, and then gradually relax it to more powerful fusion strategies.\nA number of works have shown that among fusion strategies, the weighted arithmetic mean of per-model outputs is the least sensitive to errors of individual classifiers [52]. It is often used in practice, outperforming more complex fusion algorithms. Considering the weighted mean as a\n6\nsimple baseline, we aim to initialize the fusion process with this starting point and proceed with gradient descent optimization towards an improved solution.\nUnfortunately, implementing the arithmetic mean in the case of early fusion and non-linear shared layers is not straightforward [53]. It has been shown though [54], that in dropout-like [55] systems activation units of complete models produce a weighted normalized geometric mean of per-model outputs. This kind of average approximates the arithmetic mean better than the geometric mean and the quality of this approximation depends on consistency in the neuron activation. We therefore initialize the fusion process to a normalized geometric mean of of per-model outputs.\nData fusion is implemented at two different layers: the shared hidden layer (HLS) and the output layer. The weight matrices of these two layers, denoted respectively as W1 and W2, are block-wise structured and initialized in a specific way, as illustrated in Fig. 5. The left figure shows the architecture in a conventional form as a diagram of connected neurons. The weights of the connections are indicated by matrices. On the right we introduce a less conventional notation, which allows one to better visualize and interpret the block structure. Note that the image scale is chosen for clarity of description and the real aspect ratio between dimensions of W1 (1600\u00d784) is not preserved, the ratio between vertical sizes of matrix blocks corresponding to different modalities is 9:9:7:7.\nWe denote the number of hidden units in the modalityspecific hidden layers on each path as Fk, where k=1. . .K and K is the number of modality-specific paths. We set the number of units of the shared hidden layer equal to K\u00b7N , where N=21 is the number of target gesture classes.\nAs a consequence, the matrix W1 of the shared hidden layer is of size F\u00d7(N \u00b7K), where F= \u2211\nk Fk, and the weight matrix W2 of the output layer is of size (N \u00b7K)\u00d7N . Weight matrix W1 can be thought of as a matrix of K\u00d7K blocks, where each block k is of size Fk\u00d7N . This imposes a certain meaning on the units and weights of the network. Each column in a block (and each unit in the shared\nlayer) is therefore related to a specific gesture class. Note that this block structure (and meaning) is forced on the weight matrix during initialization and in the early phases of training. If only the diagonal blocks are non-zero, which is forced at the beginning of the training procedure, then individual modalities are trained independently, and no cross correlations between modalities are captured. During the final phases of training, no structure is imposed and the weights can evolve freely. Formally, the activation of each hidden unit hkl in the shared layer can be expressed as:\nh (k) l =\u03c3\n[ Fk \u2211\ni=1\nw (k,k) i,l x (k) i +\u03b3\nK \u2211\nm=1 m 6=k\nFn \u2211\ni=1\nw (m,k) i,l x (m) i + b (k) l\n]\n(2)\nwhere h(k)l is unit l initially related to modality k, and all w are from weight matrix W1. Notation w (m,k) i,l stands for a weight between non-shared hidden unit i from the output layer of modality channel m and the given shared hidden unit l related to modality k. Accordingly, x(m)i is input number i from channel m, \u03c3 is an activation function. Finally, b(k)l is a bias of the shared hidden unit h (k) l . The first term contains the diagonal blocks and the second term contains the off-diagonal weights. Setting \u03b3=0 freezes learning of the off-diagonal weights responsible for intermodality correlations.\nThis initial meaning forced onto both weight matrices W1 and W2 produces a setting where the hidden layer is organized into K subsets of units h(k)l , one for each modality k, and where each subset comprises N units, one for each gesture class. The weight matrix W2 is initialized in a way such that these units are interpreted as posterior probabilities for gesture classes, which are averaged over modalities by the output layer controlled by weight matrix W2. In particular, each of the N\u00d7N blocks of the matrix W2 (denoted as v(k)) is initialized as an identity matrix, which results in the following expression for the output units, which are softmax activated:\noj = e \u2211 K k=1 \u2211 N c=1 v (k) j,c h(k)c\n\u2211N i=1 e \u2211 K k=1 \u2211 N c=1 v (k) i,c h (k) c\n= e \u2211 K k=1 h (k) j\n\u2211N i=1 e \u2211 K k=1 h (k) i\n(3)\n7 where we used that v(k)j,c =1/K if j=c and 0 else. From (3) we can see that the diagonal initialization of W2 forces the output layer to perform modality fusion as a normalized geometric mean over modalities, as motivated in the initial part of this section. Again, this setting is forced in the early stages of training and relaxed later, freeing the output layer to more complex fusion strategies.\nModDrop: multimodal dropout Inspired by the concept of dropout [55] as the normalized geometric mean of an exponential number of weakly trained models, we aim on exploiting a priori information about groupings in the feature set. We initiate a similar process but with a fixed number of models corresponding to separate modalities and pre-trained to convergence. We have two main motivations: (i) to learn a shared model while preserving uniqueness of per-channel features and avoiding false co-adaptations between modalities; (ii) to handle missing data in one or more of the channels at test time. The key idea is to train the shared model in a way that it would be capable of producing meaningful predictions from an arbitrary number of available modalities (with an expected loss in precision when some signals are missing).\nFormally, let us consider a set of M(k), k=1. . .K modality-specific models. During pretraining, the joint learning objective can be generally formulated as follows:\nLpretraining =\nK \u2211\nk=1\nL [ M(k) ] + \u03b1\nH \u2211\nh=1\n||Wh|| 2, (4)\nwhere each term in the first sum represents a loss of the corresponding modality-specific model (in our case, negative log likelihood, summarized over all samples xd for the given modality k from the training set |D|):\nL [ M(k) ] = \u2212 \u2211\nd\u2208D\nlog o (k) Y (Y = yd|x (k) d ), (5)\nwhere o(k)Y is output probability distribution over classes of the network corresponding to modality k and yd is a ground truth label for a given sample d.\nThe second term in Eq. 4 is L2 regularization on all weights Wh from all hidden layers h=1. . .H in the network (with weight \u03b1). At this pretraining stage, all loss terms in the first sum are minimized independently.\nOnce the weight matrices W1 and W2 are initialized with pre-trained diagonal elements and initially zeroed out offdiagonal blocks of weights are relaxed (i.e. \u03b3=1 in Eq. 2), fusion is learned from the training data. The desired training objective during the fusion process can be formulated as a combination of losses of all possible combinations of modality-specific models:\nL\u03a3=\nK \u2211\nk=1\nL [ M(k) ] + \u2211\nk 6=m\nL [ M(k,m) ] + \u2211\nk 6=m 6=n\nL [ M(k,m,n) ] +. . .\n+ \u03b1\nH \u2211\nh=1\n||Wh|| 2=\n2K \u2211\nm=1\nL [Sm] + \u03b1\nH \u2211\nh=1\n||Wh|| 2, (6)\nwhere M(k,m) indicates fusion of models M(k) and M(m), and Sm is an element of the power set of all models corresponding to all possible combinations of modalities.\nFig. 6. Toy network architecture and notations used for derivation of ModDrop regularization properties.\nThe loss function formulated in (6) reflects the objective of the training procedure but in practice we approximate this objective by ModDrop as iterative interchangeable training of one term at a time. In particular, the fusion process starts by joint training through back propagation over the shared layers and fine tuning all modality specific paths. As this step, the network takes as an input multimodal training samples {\u03b4(k)x(k)d }, k = 1. . .K from the training set |D| where for each sample each modality component x(k)d is dropped (set to 0) with a certain probability q(k)=1\u2212p(k) indicated by Bernoulli selector \u03b4(k):P (\u03b4(k)=1)=p(k). Accordingly, one step of gradient descent given an input with a certain number of non-zero modality components minimizes the loss of a corresponding multi-modal subnetwork denoted as {\u03b4(k)M(k)}. This aligns well with the initialization process described above which ensures that modality-specific subnetworks that are being removed or added by ModDrop are well pre-trained in advance.\nRegularization properties In the following we will study the regularization properties of modality-wise dropout on inputs (ModDrop) on a simpler network architecture, namely a one-layer shared network with K modality specific paths and sigmoid activation units. Input i for modality k is denoted as x(k)i and we assume that there are Fk inputs coming from each modality k (see Fig. 6). Output unit l related to modality n is denoted as o(n)l . Finally, a weight coefficient connecting input unit x(k)i with output unit o (n) l is denoted as w (k,n) i,l .\nIn our example, output units are sigmoidal, i.e. for each output unit ol related to modality n, o (n) l = \u03c3(s (n) l ) = 1/(1+ e\u2212\u03bbs (n) l ), where s(n)l = \u2211K k=1 \u2211Fk i w (k,n) i,l x (k) i is the input to the activation function coming to the given output unit from the previous layer, and \u03bb is a coefficient.\nWe minimize cross-entropy error calculated from the targets y (indices are dropped for simplicity)\nE = \u2212(y log o+ (1\u2212 y) log (1\u2212 o)), (7) whose partial derivatives can be given as follows:\n\u2202E \u2202w = \u2202E \u2202o \u2202o \u2202s \u2202s \u2202w , \u2202E \u2202o = \u2212y 1 o + (1\u2212 o) 1 1\u2212 o ,\n\u2202o \u2202s = \u03bbo(1 \u2212 o), \u2202E \u2202w = \u2212\u03bb(y \u2212 o) \u2202s \u2202w . (8)\n8 Along the lines of [54], we consider two situations corresponding to two different loss functions: E\u03a3, corresponding to the \u201ccomplete network\u201d where all modalities are present, and E\u0303 where ModDrop is performed. In our case, we assume that whole modalities (sets of units corresponding to a given modality k) are either dropped or preserved. In a ModDrop network, this can be formulated such that the input to the activation function of a given output unit l related to modality n (denoted as s\u0303(n)l ) involves a Bernoulli selector variable \u03b4(k) for each modality k which can take on values in {0, 1} and is activated with probablity p(k):\ns\u0303 (n) l =\nK \u2211\nk=1\n\u03b4(k) Fk \u2211\ni=1\nw (k,n) i,l x (k) i (9)\nAs a reminder, in the case of the complete network (all channels are present) the output activation it the following:\ns (n) l =\nK \u2211\nk=1\nFk \u2211\ni=1\nw (k,n) i,l x (k) i (10)\nAs the following reasoning always concerns a single output unit l related to modality n, from now on these indices will be dropped for simplicity of notation. Therefore, we denote s = s\n(n) l , s\u0303 = s\u0303 (n) l and w (k) i = w (k,n) i,l .\nGradients of corresponding complete and ModDrop sums with respect to weights can be expressed as follows:\n\u2202s\u0303\n\u2202w (k) i\n= \u03b4(k)x (k) i ,\n\u2202s\n\u2202w (k) i\n= x (k) i (11)\nUsing the gradient of the error E \u2202E\n\u2202w (k) i\n= \u2212\u03bb [y \u2212 \u03c3(s)] \u2202s\n\u2202w (k) i\n, (12)\nthe gradient of the error for the complete network is:\n\u2202E\u03a3\n\u2202w (k) i\n= \u2212\u03bbx (k) i\n[ y \u2212 \u03c3 (\nK \u2211\nm=1\nFm \u2211\nj=1\nw (m) j x (m) j\n)]\n(13)\nIn the case of ModDrop, for one realization of the network where a modality is dropped with corresponding probability q(k)=1\u2212p(k), indicated by the means of Bernoulli selectors \u03b4(k), i.e. P (\u03b4(k)=1)=p(k), we get:\n\u2202E\u0303\n\u2202w (k) i\n=\u2212\u03bb\u03b4(k)x (k) i\n[ y \u2212 \u03c3 (\nK \u2211\nm=1\n\u03b4(m) Fm \u2211\nj=1\nw (m) j x (m) j\n)]\n(14)\nTaking the expectation of this expression requires an expression introduced in [54], which approximates E[\u03c3(x)] by \u03c3(E[x]). We take the expectation over the \u03b4(m) with the exception of \u03b4(k)=1, which is the Bernouilli selector of the modality k for which the derivative is calculated:\nE\n[\n\u2202E\u0303\n\u2202w (k) i\n]\n\u2248\u2212\u03bbp(k)x (k) i\n[ y \u2212 \u03c3 (\nK \u2211\nm 6=k\np(m) Fm \u2211\nj=1\nw (m) j x (m) j\n+\nFk \u2211\nj=1\nw (k) j x (k) j\n)]\n=\u2212\u03bbp(k)x (k) i\n[ y\u2212\u03c3 (\nK \u2211\nm 6=k\nFm \u2211\nj=1\nw (m) j x (m) j\n\u2212\nK \u2211\nm 6=k\n(1\u2212p(m))\nFm \u2211\nj=1\nw (m) j x (m) j +\nFk \u2211\nj=1\nw (k) j x (k) j\n)]\n=\u2212\u03bbp(k)x (k) i\n\u00d7 [ y\u2212\u03c3 (\nK \u2211\nm=1\nFm \u2211\nj=1\nw (m) j x (m) j \u2212\nK \u2211\nm 6=k\n(1\u2212p(m))\nFm \u2211\nj=1\nw (m) j x (m) j\n)]\nTaking the first-order Taylor expansion of the activation function \u03c3 around s = \u2211\nm\n\u2211 j w (m) j x (m) j gives\nE\n[\n\u2202E\u0303\n\u2202w (k) i\n]\n\u2248\u2212\u03bbp(k)x (k) i\n[\ny\u2212\u03c3s+\u03c3 \u2032 s\nK \u2211\nm 6=k\n(1\u2212p(m))\nFm \u2211\nj=1\nw (m) j x (m) j\n]\nwhere \u03c3\u2032s=\u03c3 \u2032(s)=\u03c3(s)/(1\u2212\u03c3(s)). Plugging in equation (13),\nE\n[\n\u2202E\u0303\n\u2202w (k) i\n]\n\u2248p(k) \u2202E\u03a3\n\u2202w (k) i\n\u2212\u03bb\u03c3\u2032sx (k) i p\n(k) K \u2211\nm 6=k\n(1\u2212p(m))\nFk \u2211\nj=1\nw (m) j x (m) j\nIf p(k)=p(m)=p then p(1\u2212p)=Var(\u03b4). From the gradient, we can calculate the error E\u0303 integrating out the partial derivatives and summing over the weights i:\nE\u0303\u2248pE\u03a3\u2212\u03bb\u03c3 \u2032 s Var(\u03b4)\nK \u2211\nk=1\nK \u2211\nm 6=k\nFk \u2211\ni=1\nFm \u2211\nj=1\nw (k) i w (m) j x (k) i x (m) j (15)\nAs it can be seen, the error of the network with ModDrop is approximately equal to the error of the complete model (up to a coefficient) minus an additional term including a sum of products of inputs and weights corresponding to different modalities in all possible combinations. We need to stress here that this second term reflects exclusively cross-modality correlations and does not involve multiplications of inputs from the same channel. To understand what influence the cross-product term has on the training process, we analyse two extreme cases depending on whether or not signals in different channels are correlated.\nLet us consider two input units x(k)i and x (m) j coming from different modalities and first assume that they are independent and therefore uncorrelated. Since each network input is normalized to zero mean, the expectation is also equal to zero:\nE[x (k) i x (m) j ]=E[x (k) i ]E[x (m) j ] =0. (16)\nWeights in a single layer of a neural network typically obey a unimodal distribution with zero expectation [56]. It can be shown [57] that under these assumptions, Lyapunov\u2019s condition is satisfied and that Lyapunov\u2019s central mean theorem holds; in this case the sum of products of inputs and weights will tend to a normal distribution given that the number of training samples is sufficiently large. As both the input and weight distributions have zero mean, the resulting law is also centralized and its variance is defined by the magnitudes of the weights (assuming inputs are fixed).\nWe conclude that, assuming independence of inputs in different channels, the second term in equation (15) tends to vanish if the number of training samples in a batch is sufficiently large. In practice, additional regularization on weights is required to prevent weights from exploding.\nNow let us consider a more interesting scenario when two inputs x(k)i and x (m) j belonging to different modalities are positively correlated. In this case, given zero mean distributions on each input, their product is expected to be positive:\nE[x (k) i x (m) j ] =E[x (k) i ]E[x (m) j ] + Cov[x (k) i , x (k) j ]. (17)\nTherefore, on each step of gradient descent this term enforces the product w(k)i w (m) j to be positive and therefore introduces correlations between these weights (given,\n9 again, the additional regularization term preventing one of the multipliers from growing significantly faster than the other). The same logic applies if inputs are negatively correlated, which would enforce negative correlations on corresponding weights. Accordingly, for correlated modalities this additional term in the error function introduced by ModDrop acts as a cross-modality regularizer forcing the network to generalize by discovering similarities between different signals and \u201caligning\u201d them with each other by introducing soft ties on the corresponding weights.\nFinally, as has been shown by [54] for dropout, the multiplier proportional to the derivative of the sigmoid activation makes the regularization effect adaptive to the magnitude of the weights. As a result, it is strong in the mid-range of weights, plays a less significant role when weights are small and gradually weakens with saturation.\nOur experiments have shown that ModDrop achieves the best results if combined with dropout, which introduces an adaptive L2 regularization term E\u0302 in the error function [54]:\nE\u0302 \u2248 \u03bb\u03c3\u2032sVar(\u03b4\u0302) K \u2211\nk=1\nFk \u2211\ni=1\n[\nw (k) i x (k) i\n]2\n, (18)\nwhere \u03b4\u0302 is a Bernoulli selector variable, P (\u03b4\u0302=1)=p\u0302 and p\u0302 is the probability that a given input unit is present."}, {"heading": "5 INTER-SCALE FUSION DURING TEST TIME", "text": "Once individual single-scale predictions are obtained, we employ a simple voting strategy for fusion with a single weight per model. We note here that introducing additional per-class per-model weights and training meta-classifiers (such as an MLP) on this step quickly leads to overfitting.\nAt each given frame t, per-class network outputs ok are obtained via per-frame aggregation and temporal filtering of predictions at each scale with corresponding weights \u00b5s defined empirically:\nok(t) = 4 \u2211\ns=2\n\u00b5s\n0 \u2211\nj=\u22124s\nos,k(t+ j), (19)\nwhere os,k(t + j) is the score of class k obtained for a spatio-temporal block sampled starting from the frame t+j at step s. Finally, the frame is assigned the class label l(t) having the maximum score: l(t) = argmaxk ok(t)."}, {"heading": "6 GESTURE LOCALIZATION", "text": "With increasing duration of a dynamic pose, recognition rates of the classifier increase at a cost of loss in precision in gesture localization. Using wider sliding windows leads to noisy predictions at pre-stroke and post-stroke phases due to the overlap of several gesture instances at once. On the other hand, too short dynamic poses are not discriminative either, as most gesture classes at their initial and final stages have a similar appearance (e.g. raising or lowering hands).\nTo address this issue, we introduce an additional binary classifier to distinguish resting moments from periods of activity. Trained on dynamic poses at the finest temporal resolution s=1, this classifier is able to precisely localize starting and ending points of each gesture.\nThe module is a two-layer fully connected network taking as an input the articulated pose descriptor. All training frames having a gesture label are used as positive examples, while a set of frames right before and after such gesture are considered as negatives. Each frame is thus assigned with a label \u201cmotion\u201d or \u201cno motion\u201d with accuracy of 98%.\nTo combine the classification and localization modules, frame-wise gesture class predictions are first obtained as described in Section 5. Output predictions at the beginning and at the end of each gesture are typically noisy. Therefore, for each spotted gesture, its boundaries are extended or shrunk towards the closest switching point produced by the binary classifier."}, {"heading": "7 EXPERIMENTS", "text": "The Chalearn 2014 Looking at People Challenge (track 3) dataset [13] consists of 13,858 instances of Italian conversational gestures performed by different people and recorded with a consumer RGB-D sensor. It includes color, depth video and mocap streams. The gestures are drawn from a large vocabulary, from which 20 categories are identified to detect and recognize and the rest are considered as arbitrary movements. Each gesture in the training set is accompanied by a ground truth label as well as information about its startand end-points. For the challenge, the corpus was split into development, validation and test sets. The test data was released to participants after submitting their source code.\nTo further explore the dynamics of learning in multimodal systems, we augmented the data with audio recordings extracted from a dataset released under the framework of the Chalearn 2013 Multi-modal Challenge on Gesture Recognition. Differences between the 2014 and 2013 versions are mainly permutations in sequence ordering, improved quality of gesture annotations, and a different metric used for evaluation: the Jaccard index in 2014 instead of the Levenhstein distance in 2013. As a result, each gesture in a video sequence is accompanied by a corresponding vocal phrase bearing the same meaning. Due to dialectical and personal differences in pronunciation and vocabulary, gesture recognition from the audio channel alone was surprisingly challenging.\nTo summarize, we report results for two settings: i) the original dataset used for the ChaLearn 2014 Looking at People (LAP) Challenge (track 3), ii) an extended version of the dataset augmented with audio recordings taken from the Chalearn 2013 Multi-modal Gesture Recognition dataset."}, {"heading": "7.1 Experimental setup", "text": "Hyper-parameters of the multi-modal neural network for classification are provided in Table 1. The architecture is identical for each temporal scale. Gesture localization is performed with another MLP with 300 hidden units (see Section 6). All hidden units in the classification and localization modules have hyperbolic tangent activations. Hyper-parameters were optimized on the validation data with early stopping to prevent the models from overfitting and without additional regularization. For simplicity, fusion\n10\nweights for the different temporal scales are set to \u00b5s=1, as well as the weight of the baseline model (see Section 5). The deep learning architecture is implemented with the Theano library. A single-scale predictor operates at frame rates close to real time (24 fps on GPU).\nWe followed the evaluation procedure proposed by the challenge organizers and adopted the Jaccard Index to quantify model performance:\nJs,n = As,n \u2229Bs,n As,n \u222aBs,n , (20)\nwhere As,n is the ground truth label of gesture n in sequence s, and Bs,n is the obtained prediction for the given gesture class in the same sequence. Here As,n and Bs,n are binary vectors where the frames in which the given gesture is being performed are marked with 1 and the rest with 0. Overall performance was calculated as the mean Jaccard index among all gesture categories and all sequences, with equal weight for all gesture classes."}, {"heading": "7.2 Baseline models", "text": "In addition to the main pipeline, we have implemented a baseline model based on an ensemble classifier trained in a similar iterative fashion but on purely handcrafted descriptors. The purpose of this comparison was to explore relative advantages and disadvantages of using learned representations as well as the nuances of fusion. We also found it beneficial to combine the proposed deep network with the baseline method in a hybrid model (see Table 5).\nThe baseline used for visual models is described in detail in [49]. We use depth and intensity hand images and extract three sets of features. HoG features describe the hand pose in the image plane. Histograms of depths describe pose along the third spatial dimension. The third set of features is comprised of derivatives of HOGs and depth histograms, which reflect temporal dynamics of hand shape.\nExtremely randomized trees (ERT) [58] are adopted for data fusion and gesture classification. During training, we followed the same iterative strategy as in the case of the neural architecture (see [49] for more details).\nA baseline has also been created for the audio channel, where we compare the proposed deep learning approach to a traditional phoneme recognition framework, as described in [48], and implemented with the Julius engine [59]. In this approach, each gesture is associated with a pre-defined vocabulary of possible ordered sequences of phonemes that can correspond to a single word or a phrase. After spotting and segmenting periods of voice activity, each utterance is assigned a n-best list of gesture classes with corresponding scores. Finally, frequencies of appearances of each gesture class in the list are treated as output class probabilities."}, {"heading": "7.3 Results on the ChaLearn 2014 LAP dataset", "text": "The top 10 scores of the ChaLearn 2014 LAP Challenge (track 3) are reported in Table 2. Our winning entry [49] corresponding to a hybrid model (i.e. a combination of the proposed deep neural architecture and the ERT baseline model) surpasses the second best score by a margin of 1.61 percentage points. We also note that the multi-scale neural architecture still achieves the best performance, as well as the top one-scale neural model alone (see Tables 3 and 5). In post-challenge work we were able to further improve the score by 2.0 percentage points to 0.870 by introducing additional capacity into the model, optimizing the architectures of the video and skeleton paths and employing a more advanced training and fusion procedure (ModDrop) which was not used for the challenge submission.\nDetailed information on the performance of the neural architectures for each modality and at each scale is provided in Table 3, including both the multi-modal setting and permodality tests. Our experiments have proven that useful information can be extracted at any scale given sufficient model capacity (which is typically higher for small temporal steps). Trained independently, articulated pose models corresponding to different temporal scales demonstrate similar performance if predictions are refined by the gesture localization module. Video streams, containing information about hand shape and articulation, are also insensitive to\n11\nthe sampling step and demonstrate good performance even for short spatio-temporal blocks.\nThe overall highest score is nevertheless obtained in the case of a dynamic pose with duration roughly corresponding to the length of an average gesture (s=4, i.e. 17 frames).\nTable 4 illustrates performance of the proposed modalityspecific architectures compared to results reported by other participants of the challenge. For both visual channels: articulated pose and video, our method significantly outperforms the proposed alternatives.\nThe comparative performance of the baseline and hybrid models for visual modalities are reported in Table 5. In spite of the low scores of the isolated ERT baseline model, fusing its outputs with those provided by the neural architecture is still slightly beneficial, mostly due to differences in feature formulation in the video channel (adding ERT to mocap alone did not result in a significant gain).\nFor each combination, we provide results obtained with a classification module alone (without additional gesture localization) and coupled with the binary motion detector. The experiments demonstrate that the localization module contributes significantly to overall performance."}, {"heading": "7.4 Results on the ChaLearn 2014 LAP dataset augmented with audio", "text": "To demonstrate how the proposed model can be further extended with arbitrary data modalities, we introduce speech to the existing setup. In this setting, each gesture in the dataset is accompanied by a word or a short phrase expressing the same meaning and pronounced by each actor while performing the gesture. As expected, introducing a new data channel resulted in significant gain in classification performance (1.3 points on the Jaccard index, see Table 3).\nAs with the other modalities, an audio-specific neural network was first pretrained discriminatively on the audio data alone. Next, the same fusion procedure was employed without any change. In this case, the quality of predictions\nproduced by the audio path depends on the temporal sampling frequency: the best performance was achieved for dynamic poses of duration \u223c0.5 s (see Table 3).\nAlthough the overall score after adding the speech channel is improved significantly, the audio modality alone does not perform so well. This can be partly explained by natural gesture-speech desynchronisation resulting in poor audiobased gesture localization. In this dataset, gestures are annotated based on video recordings, while pronounced words and phrases are typically shorter in time than movements. Moreover, depending on the style of each actor, vocalisation can be either slightly delayed to coincide with gesture culmination, or can be slightly ahead of time announcing the gesture. Therefore, the audio signal alone does not allow the model to robustly predict the start- and end-points of a gesture, which results in poor Jaccard scores.\nTable 6 compares the performance of the proposed solution based on learning representations from mel-frequency spectrograms with the baseline model involving traditional phoneme recognition [48]. Here, we report the values of Jaccard indices for the reference, but, as it was mentioned above, accurate gesture localization based exclusively on the audio channel is not possible for reasons outside of the model\u2019s control. To make a more meaningful comparison of the classification performance, we report recall, precision and F-measure for each model. In this case we assume that the gesture was correctly detected and recognised if temporal overlap between predicted and ground truth gestures is at least 20%.\nOur results show that, in the given context, employing the deep learning approach drastically improves performance in comparison with the traditional framework based on phoneme recognition."}, {"heading": "7.5 Impact of the different fusion strategies", "text": "We explore the relative advantages of different training strategies, starting with preliminary experiments on the MNIST dataset [66] and then a more extensive analysis on the ChaLearn 2014 dataset augmented with audio."}, {"heading": "7.5.1 Preliminary experiments on MNIST dataset", "text": "As a sanity check of ModDrop fusion, we transform the MNIST dataset [66] to imitate multi-modal data. A classic deep learning benchmark, MNIST consists of 28\u00d728 grayscale images of handwritten digits, where 60k examples are used for training and 10k images are used for testing. We use the original version with no data augmentation. We also avoid any data preprocessing and apply a simple architecture: a multi-layer perceptron with two hidden layers (i.e. no convolutional layers).\n12\nWe cut each digit image into 4 quarters and assume that each quarter corresponds to one modality (see Fig. 7). In spite of the apparent simplicity of this formulation, we show that the obtained results accurately reflect the dynamics of a real multi-modal setup.\nThe multi-signal training objective is two-fold: first, we optimize the architecture and the training procedure to obtain the best overall performance on the full set of modalities. The second goal is to make the model robust to missing signals or a high level of noise in the separate channels. To explore the latter aspect, during test time we occlude one or more image quarters or add pepper noise to one or more image parts.\nCurrently, the state-of-the-art for a fully-connected 782- 1200-1200-10 network with dropout regularization (50% for hidden units and 20% for the input) and tanh activations [55] is 110 errors on the MNIST test set (see Table 7). In this case, the number of units in the hidden layer is unnecessarily large, which is exploited by dropout-like strategies. When real-time performance is a constraint, this redundancy in the number of operations becomes a serious limitation. Instead, switching to our tree-structured network (i.e. a network with separated modality-specific input layers connected to a set of shared layers) is helpful for independent modality-wise tuning of model capacity, which in this case does not have to be uniformly distributed over the input units. For this multi-modal setting we optimized the number of units (125) for each channel and do not apply dropout to the hidden units (which in this case turns out to be harmful due to the compactness of the model), limiting ourselves to dropping out the inputs at a rate of 20%. In addition, we apply ModDrop on the input, where the probability of each segment to be dropped is 10%.\nThe results in Table 7 show that separate pretraining of modality-specific paths generally yields better performance and leads to a significant decrease in the number of parameters due to the capacity restriction placed on each channel. This is apparent in the 4th row of Table 7b: with pretraining, better performance (102 errors) is obtained with 20 times less parameters.\nMNIST results under occlusion and noise are presented in Table 8. We see that ModDrop, while not affecting the overall performance on MNIST, makes the model significantly less sensitive to occlusion and noise."}, {"heading": "7.5.2 Experiments on ChaLearn 2014 LAP with audio", "text": "In a real multi-modal setting, optimizing and balancing a tree-structured architecture is an extremely difficult task as its separated parallel paths vary in complexity and operate on different feature spaces. The problem becomes even harder under the constraint of real-time performance and, consequently, the limited capacity of the network.\nOur experiments have shown that insufficient modelling capacity of one of the modality-specific subnetworks leads to a drastic degradation in performance of the whole system due to the multiplicative nature of the fusion process. Those bottlenecks are typically difficult to find without thorough per-channel testing.\nWe propose to start by optimizing the architecture and hyper-parameters for each modality separately through discriminative pretraining. During fusion, input paths are initialized with pretrained values and fine tuned while training the output shared layers.\nFurthermore, the shared layers can also be initialized with pretrained diagonal blocks as described in Section 4, which results in a significant speed up in the training process. We have observed that in this case, setting the biases of the shared hidden layer is critical in converging to a better solution.\n13\nAs in the case of the MNIST experiments, we apply 20% dropout on the input signal and ModDrop with probability of 10% (optimized on the validation set). As before, dropping hidden units during training led to degradation in performance of our architecture due to its compactness.\nA comparative analysis of the efficiency of various training strategies is reported in Table 9. Here, we provide validation error of per dynamic pose classification as a direct indicator of convergence of training. The \u201cPretraining\u201d column corresponds to modality-specific paths while \u201cInitial.\u201d indicates whether or not the shared layers have also been pre-initialized with pretrained diagonal blocks. In all cases, dropout (20%) and ModDrop (10%) are applied to the input signal. Accuracy corresponds to per-block classification on the validation set.\nDifferences in effectiveness of different strategies agree well with what we have observed previously on MNIST. Modality-wise pretraining and regularization of the input have a strong positive effect on performance. Interestingly, in this case ModDrop resulted in further improvement in scores even for the complete set of modalities (while increasing the dropout rate did not have the same effect).\nAnalysis of the network behaviour in conditions of noisy or missing signals in one or several channels is provided in Table 10. Once again, ModDrop regularization resulted in much better network stability with respect to signal corruption and loss."}, {"heading": "8 CONCLUSION", "text": "We have described a generalized method for gesture and near-range action recognition from a combination of range video data and articulated pose. Each of the visual modalities captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at two temporal scales.\nThe model can be further extended and augmented with arbitrary channels (depending on available sensors) by introducing additional parallel pathways without significant changes in the general structure. We illustrate this concept by augmenting video with speech. Multiple spatial and temporal scales per channel can easily be integrated.\nFinally, we have explored various aspects of multi-modal fusion in terms of joint performance on a complete set\nof modalities as well as robustness of the classifier with respect to noise and dropping of one or several data channels. As a result, we have proposed a modality-wise regularisation strategy (ModDrop) allowing our model to obtain stable predictions even when inputs are corrupted."}, {"heading": "Acknowledgement", "text": "This work has been partly financed through the French grant Interabot, a project of type \u201cInvestissement\u2019s d\u2019Avenir / Briques Ge\u0301ne\u0301riques du Logiciel Embarque\u0301\u201d."}], "references": [{"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "ICLR, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Hierarchical Features for Scene Labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Indoor Semantic Segmentation using depth information", "author": ["C. Couprie", "F. Cl\u00e9ment", "L. Najman", "Y. LeCun"], "venue": "ICLR, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "c. G\u00fcl\u00e7ehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio"], "venue": "ICMI, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Y. Taigman", "M. Yang", "M.A. Ranzato", "L. Wolf"], "venue": "CVPR, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatio-Temporal Convolutional Sparse Auto-Encoder for Sequence Classification", "author": ["M. Baccouche", "F. Mamalet", "C. Wolf", "C. Garcia", "A. Baskurt"], "venue": "BMVC, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale Video Classification with Convolutional Neural Networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "F.- F. Li"], "venue": "CVPR, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-Stream Convolutional Networks for Action Recognition in Videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "inarXiv:1406.2199v1, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation", "author": ["A. Jain", "J. Tompson", "Y. LeCun", "C. Bregler"], "venue": "ACCV, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "ChaLearn Looking at People Challenge 2014: Dataset and Results", "author": ["S. Escalera", "X. Bar\u00f3", "J. Gonz\u00e0lez", "M. Bautista", "M. Madadi", "M. Reyes", "V. Ponce", "H. Escalante", "J. Shotton", "I. Guyon"], "venue": "ECCVW, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Dense trajectories and motion boundary descriptors for action recognition", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "IJCV, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation of local spatio-temporal features for action recognition", "author": ["H. Wang", "M.M. Ullah", "A. Klaser", "I. Laptev", "C. Schmid"], "venue": "BMVC, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Behavior Recognition via Sparse Spatio-Temporal Features", "author": ["P. Doll\u00e1r", "V. Rabaud", "G. Cottrell", "S. Belongie"], "venue": "VS-PETS, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marsza\u0142ek", "C. Schmid", "B. Rozenfeld"], "venue": "CVPR, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A spatio-temporal descriptor based on 3D-gradients", "author": ["A. Kl\u00e4ser", "M. Marsza\u0142ek", "C. Schmid"], "venue": "BMVC, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "AnEfficientDenseand Scale- Invariant Spatio-Temporal Interest Point Detector", "author": ["G. Willems", "T. Tuytelaars", "L. Gool"], "venue": "ECCV, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "A. Fitzgibbon", "M. Cook", "T. Sharp", "M. Finocchio", "R. Moore", "A. Kipman", "A. Blake"], "venue": "CVPR, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Real time hand pose estimation using depth sensors", "author": ["C. Keskin", "F. Kira\u00e7", "Y. Kara", "L. Akarun"], "venue": "ICCV Workshop, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time Articulated Hand Pose Estimation using Semi-supervised Transductive Regression Forests", "author": ["D. Tang", "T.-H. Yu", "T.-K. Kim"], "venue": "ICCV, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks", "author": ["J. Tompson", "M. Stein", "Y. LeCun", "K. Perlin"], "venue": "ACM Transaction on Graphics, 2014.  14", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Hand segmentation with structured convolutional learning", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "ACCV, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient model-based 3D tracking of hand articulations using Kinect", "author": ["I. Oikonomidis", "N. Kyriazis", "A. Argyros"], "venue": "BMVC, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Realtime and Robust Hand Tracking from Depth", "author": ["C. Qian", "X. Sun", "Y. Wei", "X. Tang", "J. Sun"], "venue": "CVPR, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture", "author": ["D. Tang", "H.J. Chang", "A. Tejani", "T.-K. Kim"], "venue": "CVPR, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond Physical Connections: Tree Models in Human Pose Estimation", "author": ["F. Wang", "Y. Li"], "venue": "CVPR, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining actionlet ensemble for action recognition with depth cameras", "author": ["J. Wang", "Z. Liu", "Y. Wu", "J. Yuan"], "venue": "CVPR, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Unstructured Human Activity Detection from RGBD Images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "ICRA, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Online RGB-D gesture recognition with extreme learning machines", "author": ["X. Chen", "M. Koskela"], "venue": "ICMI, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A Multi-scale Boosted Detector for Efficient and Robust Gesture Recognition", "author": ["C. Monnier", "S. German", "A. Ost"], "venue": "ECCVW, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric Gesture Labeling from Multi-modal Data", "author": ["J.Y. Chang"], "venue": "ECCV Workshop, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "A Multi-modal Gesture Recognition System Using Audio, Video, and Skeletal Joint Data Categories and Subject Descriptors", "author": ["K. Nandakumar", "W.K. Wah", "C.S.M. Alice", "N.W.Z. Terence", "W.J. Gang", "Y.W. Yun"], "venue": "ICMI Workshop, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "CVPR, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "CVPR, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep learning of invariant Spatio-Temporal Features from Video", "author": ["B. Chen", "J.-A. Ting", "B. Marlin", "N. de Freitas"], "venue": "NIPSW, 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "3D Convolutional Neural Networks for Human Action Recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "PAMI, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiple Kernel Learning, Conic Duality, and the SMO Algorithm", "author": ["F. Bach", "G. Lanckriet", "M. Jordan"], "venue": "ICML, 2004.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "On Feature Combination for Multiclass Object Classification", "author": ["P. Gehler", "S. Nowozin"], "venue": "ICCV, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust Late Fusion With Rank Minimization", "author": ["G. Ye", "D. Liu", "I.-H. Jhuo", "S.-F. Chang"], "venue": "CVPR, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Sample-Specific Late Fusion for Visual Category Recognition", "author": ["D. Liu", "K.-T. Lai", "G. Ye", "M. Chen", "S. Chang"], "venue": "CVPR, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "FeatureWeighting via Optimal Thresholding for Video Analysis", "author": ["Z. Xu", "Y. Yang", "I. Tsang", "N. Sebe", "A. Hauptmann"], "venue": "ICCV, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal Feature Fusion for Robust Event Detection in Web Videos", "author": ["P. Natarajan", "S. Wu", "S. Vitaladevuni", "X. Zhuang", "S. Tsakalidis", "U. Park", "R. Prasad", "P. Natarajan"], "venue": "CVPR, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kin", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "A multi-scale approach to gesture detection and recognition", "author": ["N. Neverova", "C. Wolf", "G. Paci", "G. Sommavilla", "G.W. Taylor", "F. Nebout"], "venue": "ICCV Workshop, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-scale deep learning for gesture detection and localization", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "ECCVW, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection", "author": ["M. Zanfir", "M. Leordeanu", "C. Sminchisescu"], "venue": "ICCV, 2013.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Recent advances in deep learning for speech recognition at Microsoft", "author": ["L. Deng", "J. Li", "J. Huang", "K. Yao", "D. Yu", "F. Seide", "M. Seltzer", "G. Zweig", "X. He", "J. Williams", "Y. Gong", "A. Acero"], "venue": "ICASSP, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "On combining classifiers using sum and product rules", "author": ["L.A. Alexandre", "A.C. Campilho", "M. Kamel"], "venue": "Pattern Recognition Letters, no. 22, 2001, pp. 1283\u20131289.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2001}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv:1302.4389v4, 2013.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "The dropout learning algorithm", "author": ["P. Baldi", "P. Sadowski"], "venue": "Journal of Artificial Intelligence, vol. 210, pp. 78\u2013122, 2014.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580, 2012.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast dropout training", "author": ["S. Wang", "C. Manning"], "venue": "ICML, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Elements of Large-Sample Theory", "author": ["E.L. Lehmann"], "venue": "ICML, 1998.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1998}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine learning, 63(1), 3-42, 2006.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "Julius - an open source realtime large vocabulary recognition engine", "author": ["A. Lee", "T. Kawahara", "K. Shikano"], "venue": "Interspeech, 2001.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2001}, {"title": "Gesture Recognition using Template Based Random Forest Classifiers", "author": ["N. Camgoz", "A. Kindiroglu", "L. Akarun"], "venue": "ECCVW, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous gesture recognition from articulated poses", "author": ["G. Evangelidis", "G. Singh", "R. Horaud"], "venue": "ECCV Workshop, 2014.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Action and Gesture Temporal Spotting with Super Vector Representation", "author": ["X. Peng", "L. Wang", "Z. Cai"], "venue": "ECCVW, 2014.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-modality Gesture Detection and Recognition With Unsupervision, Randomization and Discrimination", "author": ["G. Chen", "D. Clarke", "M. Giuliani", "D. Weikersdorfer", "A. Knoll"], "venue": "ECCVW, 2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Sign Language Recognition Using Convolutional Neural Networks", "author": ["L. Pigou", "S. Dieleman", "P.-J. Kindermans"], "venue": "ECCVW, 2014.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Dynamic Neural Networks for Gesture Segmentation and Recognition", "author": ["D. Wu"], "venue": "ECCV Workshop, 2014.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 191, "endOffset": 194}, {"referenceID": 2, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 235, "endOffset": 238}, {"referenceID": 4, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 240, "endOffset": 243}, {"referenceID": 5, "context": "Convolutional neural networks (ConvNets) [6] have excelled on several scientific competitions such as ILSVRC [3], Emotion Recognition in the Wild [7], Kaggle Dogs vs.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "Convolutional neural networks (ConvNets) [6] have excelled on several scientific competitions such as ILSVRC [3], Emotion Recognition in the Wild [7], Kaggle Dogs vs.", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Convolutional neural networks (ConvNets) [6] have excelled on several scientific competitions such as ILSVRC [3], Emotion Recognition in the Wild [7], Kaggle Dogs vs.", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "Cats [2] and Galaxy Zoo.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "[8] recently claimed to have reached human-level performance using ConvNets for face recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 194, "endOffset": 197}, {"referenceID": 9, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 10, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 205, "endOffset": 209}, {"referenceID": 11, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "The deep learning method described in this paper placed first in the 2014 version of this competition [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 201, "endOffset": 205}, {"referenceID": 14, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 213, "endOffset": 217}, {"referenceID": 16, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 219, "endOffset": 223}, {"referenceID": 17, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 225, "endOffset": 229}, {"referenceID": 18, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 231, "endOffset": 235}, {"referenceID": 19, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 182, "endOffset": 186}, {"referenceID": 21, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 188, "endOffset": 192}, {"referenceID": 22, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 194, "endOffset": 198}, {"referenceID": 23, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 200, "endOffset": 204}, {"referenceID": 24, "context": "In parallel, tracking-based approaches are advancing quickly [25], [26].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "In parallel, tracking-based approaches are advancing quickly [25], [26].", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "On the other hand, in [27] the authors proposed the Latent Regression Forest for coarse-to-fine search of joint positions.", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "Finally, graphical models, exploring spatial relationships between body and hand parts, have recently attracted close attention [28], [29].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "Finally, graphical models, exploring spatial relationships between body and hand parts, have recently attracted close attention [28], [29].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "In [30], a combination of skeletal features and local occupancy patterns (LOP) were calculated from depth maps to describe hand joints.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [31], skeletal information was integrated in two ways for extracting HoG features from RGB and depth images: either from global bounding boxes containing a whole body or from regions containing an arm, a torso and a head.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 115, "endOffset": 119}, {"referenceID": 35, "context": "Independent subspace analysis (ISA) [36] as well as autoencoders [37], [9] are examples of efficient unsupervised methods for learning hierarchies of invariant spatio-temporal features.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "Independent subspace analysis (ISA) [36] as well as autoencoders [37], [9] are examples of efficient unsupervised methods for learning hierarchies of invariant spatio-temporal features.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "Independent subspace analysis (ISA) [36] as well as autoencoders [37], [9] are examples of efficient unsupervised methods for learning hierarchies of invariant spatio-temporal features.", "startOffset": 71, "endOffset": 74}, {"referenceID": 37, "context": "Spacetime deep belief networks [38] produce high-level representations of video sequences using convolutional RBMs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 38, "context": "A method proposed in [39] is based on low-level preprocessing of the video input and employs a 3D convolutional network for learning of midlevel spatio-temporal representations and classification.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "[10] have proposed a convolutional architecture for large-scale video classification operating at two spatial resolutions (fovea and context streams).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "While in most practical applications, late fusion of scores output by several models offers a cheap and surprisingly effective solution [7], both late and early fusion of either final or intermediate data representations remain under active investigation.", "startOffset": 136, "endOffset": 139}, {"referenceID": 39, "context": "Multiple Kernel Learning (MKL) [40] has been actively discussed in this context.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "At the same time, as shown by [41], simple additive or multiplicative averaging of kernels may reach the same level of performance while being orders of magnitude faster.", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "[42] proposed a late fusion strategy compensating for errors of individual classifiers by minimising the rank of a score matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "In follow-up work [43], they identified sample-specific optimal fusion weights by enforcing similarity in fusion scores for visually similar labeled and unlabeled samples.", "startOffset": 18, "endOffset": 22}, {"referenceID": 43, "context": "[44] introduced the Feature Weighting via Optimal Thresholding (FWOT) algorithm jointly optimising feature weights and thresholds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] employed multiple strategies, including MKL-based combinations of features, Bayesian model combination, and weighted average fusion of scores from multiple systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46] employed sparse RBMs and bimodal deep antoencoders to learn cross-modality correlations in the context of audiovisual speech classification of isolated letters and digits.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] used a multi-modal deep Boltzmann machine in a generative fashion to tackle the problem of integrating images and text annotations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] won the 2013 Emotion Recognition in the Wild Challenge by training convolutional architectures on several modalities, such as facial expressions from video, audio, scene context and features extracted around mouth regions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 47, "context": "Finally, in [48] the authors proposed a multi-modal convolutional network for gesture detection and classification from a combination of depth, skeletal information and audio.", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "[3]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Our model is therefore different from the one proposed in [4], where by \u201cmulti-scale\u201d Farabet et al.", "startOffset": 58, "endOffset": 61}, {"referenceID": 45, "context": "However, initial discriminative learning of individual data representations from each isolated channel followed by fusion has proven to be efficient in similar tasks [46].", "startOffset": 166, "endOffset": 170}, {"referenceID": 48, "context": "We formulate a pose descriptor consisting of 7 logical subsets as described in [49].", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": "Following [50], we first calculate normalized joint positions, as well as their velocities and accelerations, and then augment the descriptor with a set of characteristic angles and pairwise distances.", "startOffset": 10, "endOffset": 14}, {"referenceID": 48, "context": "Inclination angles are formed by all triples of anatomically connected joints plus two \u201cvirtual\u201d angles [49].", "startOffset": 104, "endOffset": 108}, {"referenceID": 50, "context": "[51] demonstrated the advantage of using primitive spectral features, such as 2D spectrograms, in combination with deep autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46] applied the same strategy to the task of multi-modal speech recognition while augmenting the audio signal with visual features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Further experiments from Microsoft [51] have shown that ConvNets appear to be especially efficient in this context since they allow the capture and modeling of structure and invariances that are typical for speech.", "startOffset": 35, "endOffset": 39}, {"referenceID": 47, "context": "Comparative analysis of our previous approach [48] based on phoneme recognition from sequences of MFCC features and a deep learning framework has demonstrated that the latter strategy allows us to obtain significantly better performance on the ChaLearn dataset (see Sec.", "startOffset": 46, "endOffset": 50}, {"referenceID": 50, "context": "As it was experimentally demonstrated by [51], the step of the scale transform is important.", "startOffset": 41, "endOffset": 45}, {"referenceID": 51, "context": "A number of works have shown that among fusion strategies, the weighted arithmetic mean of per-model outputs is the least sensitive to errors of individual classifiers [52].", "startOffset": 168, "endOffset": 172}, {"referenceID": 52, "context": "Unfortunately, implementing the arithmetic mean in the case of early fusion and non-linear shared layers is not straightforward [53].", "startOffset": 128, "endOffset": 132}, {"referenceID": 53, "context": "It has been shown though [54], that in dropout-like [55] systems activation units of complete models produce a weighted normalized geometric mean of per-model outputs.", "startOffset": 25, "endOffset": 29}, {"referenceID": 54, "context": "It has been shown though [54], that in dropout-like [55] systems activation units of complete models produce a weighted normalized geometric mean of per-model outputs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 54, "context": "Inspired by the concept of dropout [55] as the normalized geometric mean of an exponential number of weakly trained models, we aim on exploiting a priori information about groupings in the feature set.", "startOffset": 35, "endOffset": 39}, {"referenceID": 53, "context": "Along the lines of [54], we consider two situations corresponding to two different loss functions: E\u03a3, corresponding to the \u201ccomplete network\u201d where all modalities are present, and \u1ebc where ModDrop is performed.", "startOffset": 19, "endOffset": 23}, {"referenceID": 53, "context": "Taking the expectation of this expression requires an expression introduced in [54], which approximates E[\u03c3(x)] by \u03c3(E[x]).", "startOffset": 79, "endOffset": 83}, {"referenceID": 55, "context": "(16) Weights in a single layer of a neural network typically obey a unimodal distribution with zero expectation [56].", "startOffset": 112, "endOffset": 116}, {"referenceID": 56, "context": "It can be shown [57] that under these assumptions, Lyapunov\u2019s condition is satisfied and that Lyapunov\u2019s central mean theorem holds; in this case the sum of products of inputs and weights will tend to a normal distribution given that the number of training samples is sufficiently large.", "startOffset": 16, "endOffset": 20}, {"referenceID": 53, "context": "Finally, as has been shown by [54] for dropout, the multiplier proportional to the derivative of the sigmoid activation makes the regularization effect adaptive to the magnitude of the weights.", "startOffset": 30, "endOffset": 34}, {"referenceID": 53, "context": "Our experiments have shown that ModDrop achieves the best results if combined with dropout, which introduces an adaptive L2 regularization term \u00ca in the error function [54]:", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "The Chalearn 2014 Looking at People Challenge (track 3) dataset [13] consists of 13,858 instances of Italian conversational gestures performed by different people and recorded with a consumer RGB-D sensor.", "startOffset": 64, "endOffset": 68}, {"referenceID": 48, "context": "The baseline used for visual models is described in detail in [49].", "startOffset": 62, "endOffset": 66}, {"referenceID": 57, "context": "Extremely randomized trees (ERT) [58] are adopted for data fusion and gesture classification.", "startOffset": 33, "endOffset": 37}, {"referenceID": 48, "context": "During training, we followed the same iterative strategy as in the case of the neural architecture (see [49] for more details).", "startOffset": 104, "endOffset": 108}, {"referenceID": 48, "context": "1 Ours [49] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 59, "context": "[60] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[61] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "745 3 Chang [34] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 61, "context": "[62] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[63] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[64] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "6 Wu [65] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "A baseline has also been created for the audio channel, where we compare the proposed deep learning approach to a traditional phoneme recognition framework, as described in [48], and implemented with the Julius engine [59].", "startOffset": 173, "endOffset": 177}, {"referenceID": 58, "context": "A baseline has also been created for the audio channel, where we compare the proposed deep learning approach to a traditional phoneme recognition framework, as described in [48], and implemented with the Julius engine [59].", "startOffset": 218, "endOffset": 222}, {"referenceID": 48, "context": "Our winning entry [49] corresponding to a hybrid model (i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 60, "context": "[61], submitted entry 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[60] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[61], after competition 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "768 \u2013 Wu and Shao [65] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": "[33] (validation set) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "791 \u2013 Chang [34] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 63, "context": "[64] \u2013 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62] \u2013 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Ours, submitted entry [49] 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 48, "context": "781 (6) Ours [49] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 48, "context": "849 (1) Ours [49] + ERT 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 47, "context": "Phoneme recognition [48] 64.", "startOffset": 20, "endOffset": 24}, {"referenceID": 47, "context": "Table 6 compares the performance of the proposed solution based on learning representations from mel-frequency spectrograms with the baseline model involving traditional phoneme recognition [48].", "startOffset": 190, "endOffset": 194}, {"referenceID": 54, "context": "Dropout, 784-1200-1200-10 [55] 107 2395210 N Dropout, 784-500-40-10 (ours) 119 412950 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 54, "context": "Currently, the state-of-the-art for a fully-connected 7821200-1200-10 network with dropout regularization (50% for hidden units and 20% for the input) and tanh activations [55] is 110 errors on the MNIST test set (see Table 7).", "startOffset": 172, "endOffset": 176}], "year": 2017, "abstractText": "We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.", "creator": "LaTeX with hyperref package"}}}