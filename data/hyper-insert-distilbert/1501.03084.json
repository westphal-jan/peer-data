{"id": "1501.03084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2015", "title": "Deep Learning with Nonparametric Clustering", "abstract": "clustering is an essential essential early problem in machine adaptive learning and data mining. one vital factor element that often impacts reliable clustering performance is how carefully to learn or design the statistical data representation ( or features ). fortunately, recent advances in challenging deep learning contexts can learn unsupervised features effectively, randomly and have simultaneously yielded exemplary state of the half art performance in many mathematical classification problems, such as robust character sequence recognition, quantitative object recognition and document categorization. however, little attention nowadays has not been paid recently to advance the theoretical potential of deep software learning for potentially unsupervised clustering problems. quoted in this paper, alternatively we suggest propose a feasible deep belief network with numerical nonparametric probability clustering. given as an unsupervised systematic method, our model first carefully leverages the proven advantages of deep conditional learning practices for general feature model representation learning and topological dimension chain reduction. quite then, periodically it performs nonparametric clustering task under a maximum level margin framework - - a discriminative clustering model and can be trained further online efficiently starting in the objective code sampling space. lastly model parameters are gradually refined in the deep mathematical belief network. thereby thus, simultaneously this model paradigm can learn features responsible for clustering memory and infer model complexity in introducing an innovative unified test framework. simultaneously the experimental empirical results show the advantage result of our hierarchical approach effectively over socially competitive mathematical baselines.", "histories": [["v1", "Tue, 13 Jan 2015 17:26:26 GMT  (274kb,D)", "http://arxiv.org/abs/1501.03084v1", "14 pages, 6 figures"]], "COMMENTS": "14 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen"], "accepted": false, "id": "1501.03084"}, "pdf": {"name": "1501.03084.pdf", "metadata": {"source": "CRF", "title": "Deep Learning with Nonparametric Clustering", "authors": ["Gang Chen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Clustering methods, such as k-means, Gaussian mixture model (GMM), spectral clustering and non-parametrical Bayesian methods, have been widely used in machine learning and data mining. Among various clustering methods, nonparametric Bayesian model is one of promising approaches for data clustering, because of its ability to infer the model complexity from the data automatically. To mine clusters or patterns from data, we can group them based on some notion of similarity. In general, calculating the clustering similarity is dependent on the features describing data. Thus, feature representation is vital for successful clustering. Just as common for other clustering methods, the presence of noisy and irrelevant features can degrade clustering performance, making feature representation an important factor in cluster analysis. Moreover, different features may be relevant or irrelevant in the high dimensional data, suggesting the need for feature learning.\nRecent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23]. The advantages of deep learning are that they give mappings which can capture meaningful structure information in the code space and introduce bias towards configurations of the parameter space that are helpful for unsupervised learning [6]. More specifically, it learns the composition of multiple non-linear transformations (such as stacked\nar X\niv :1\n50 1.\n03 08\n4v 1\n[ cs\n.L G\n] 1\n3 Ja\nn 20\nrestricted Boltzmann machines), with the purpose to yield more abstract and ultimately more useful representations [3]. In addition, deep learning with gradient descent scales linearly in time and space with the number of train cases, which makes it possible to apply to large scale data sets [9].\nUnfortunately, little work has been done to leverage the advantages of deep learning for unsupervised clustering problems. Moreover, unsupervised clustering also presents a challenge in the deep learning framework, compared to supervised methods in the final fine-tuning process. Another important research topic in clustering analysis is how to adapt model complexity for increasing volumes in the era of big data [21, 4, 24]. However, most approaches are generative models and have restrictions on the prior base measures.\nIn this paper, we are interested in clustering problems and propose a deep belief network (DBN) with nonparametric clustering. This approach is an unsupervised clustering method, inspired by the advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models [1, 7, 4]. On the one hand, clustering performance depends heavily on data representation, which implies the need for feature learning in clustering. On the other hand, while the nonparametric Bayesian model can perform model selection and data clustering, it is intractable for non-conjugate prior; furthermore, it may not perform well on high-dimensional data, especially in terms of space and time complexity. Thus, we propose the deep learning with nonparametric maximum margin model for clustering analysis. Essentially, we first pre-train DBN for feature learning and dimension reduction. Then, we will learn the clustering weights discriminatively with nonparametric maximum margin clustering (NMMC), which can be updated online efficiently. Finally, we fine-tune the model parameters in the deep belief network. Refer to Fig. (1) for visual understanding to our model. Hence, our framework can handle high-dimensional input features with nonlinear mapping, and cluster large scale data sets with model selection using the online nonparametric clustering method.\nOur contributions can be mainly summarized as: (1) leveraging unsupervised feature learning with DBN for clustering analysis; (2) a discriminative approach for nonparametric clustering under maximum margin framework. The experimental results show advantages of our model over competitive baselines."}, {"heading": "2 Related work", "text": "Clustering has been an interesting research topic for decades, including a wide range of techniques, such as generative/discriminative and parametric/nonparametric approaches. As an discriminative method, maximum margin clustering (MMC) treats the label of each instance as a latent variable and uses SVM for clustering with large margins. However, they [2, 28] either cannot learn parameters online efficiently or need to define the number of clusters like other clustering approaches, such as k-means, Gaussian mixture model (GMM) and spectral clustering. Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems. One of the widely used nonparametric models for clustering is Dirichlet process mixture (DPM) [1, 7]. DPM can learn the number of mixture components without specified in advance, which can grow as new data come in. However, the behavior of the model is sensitive to the choice of prior base measure G0. In addition, DPM of Gaussians need to calculate mean and covariance for each component, and update covariance with Cholesky decomposition, which may lead to high space and time complexity in high-dimensional\ndata. Unsupervised feature learning with deep structures was first proposed in [9] for dimension reduction. Later, this unsupervised approach was developed into semi-supervised embedding [27] and supervised mapping [16] scenarios. Many other supervised approaches also exploit deep learning for feature extraction and then learn a discriminative classifier with objectives, e.g., square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space. The success behind deep learning is that it can learn useful information for data visualization and classification [6, 3]. Thus, it is desirable to leverage deep learning for clustering analysis, because the performance for clustering depends heavily on data representation. Unfortunately, little attention has been paid to leveraging deep learning for unsupervised clustering problems.\nA recent interesting approach is the implicit mixture of RBMs [17]. Instead of modeling each component with Gaussian distribution, it models each component with RBM. It is formulated as a third-order Boltzmann machine with cluster label as the hidden variable for each instance. However, it also requires the number of clusters specified as input.\nIn this paper, we are interested in deep learning for unsupervised clustering problems. In our framework, we take advantage of deep learning for representation learning, which is helpful for clustering analysis. Moreover, we take an discriminative approach, namely nonparametric maximum margin clustering to infer model complexity online, without the prior measure assumption as DPM."}, {"heading": "3 Deep learning with nonparametric maximum margin clustering", "text": "In this section, we will first review RBM and DBN for feature learning. Then, we will introduce nonparametric maximum margin clustering (NMMC) method given the feature learned from DBN.\nFinally, we will fine-tune our model given the clustering labels for the data."}, {"heading": "3.1 Feature learning with deep belief network", "text": "Assume that we have a training set D = {vi}Ni=1, where vi \u2208 Rd. An RBM with n hidden units is a parametric model of the joint distribution between a layer of hidden variables h = (h1, ..., hn) and the observations v = (v1, ..., vd). The RBM joint likelihood takes the form:\np(v,h) \u221d e\u2212E(v,h) (1)\nwhere the energy function is\nE(v,h) = \u2212hTW1v \u2212 bTv \u2212 cTh (2)\nAnd we can compute the following conditional likelihood: p(v|h) = \u220f i p(vi|h) (3a)\np(vi = 1|h) = logistic(bi + \u2211 j W1(i, j)hj) (3b)\np(hi = 1|v) = logistic(ci + \u2211 j W1(j, i)vj) (3c)\nwhere logistic(x) = 1/(1 + e\u2212x). To learn RBM parameters, we need to optimize the negative log likelihood \u2212logp(v) on training data D, the parameters updating can be calculated with a efficient stochastic descent method, namely contrastive divergence (CD) [10].\nA Deep Belief Network (DBN) is composed of stacked RBMs [9] learned layer by layer greedily, where the top layer is an RBM and the lower layers can be interpreted as a directed sigmoid belief network [3], shown in Fig. (1). Suppose the DBN used here has L layers, and the weight for each layer is indicated as Wi for i = {1, ..,L}. Specifically, we think RBM is a 1-layer DBN, with weight W1. Thus, DBN can learn parametric nonlinear mapping from input v to output x, f : v \u2192 x. For example, for 1-layer DBN, we have x = logistic(W1\nTv + c). After we learn the representation for the data, we use NMCC for clustering analysis to model the data distribution."}, {"heading": "3.2 Nonparametric maximum margin clustering", "text": "Nonparametric maximum margin clustering (NMMC) is a discriminative clustering model for clustering analysis. Given the nonlinear mapping with DBN, we can first map the original training data D = {vi}Ni=1 into codes X = {xi}Ni=1 in the embedding space. Then, with X = {xi}Ni=1 and its the cluster indicators z = {zi}Ni=1, we propose the following conditional probability for nonparametric clustering:\nP (z, {\u03b8k}Kk=1|X ) \u221d p(z) [ N\u220f i=1 p(xi|\u03b8zi) ] K\u220f k=1 p(\u03b8k) (4)\nwhere K is the number of clusters, p(xi|\u03b8zi) is the likelihood term defined in Sec. 3.2.1 and p(\u03b8k) can be thought as the Gaussian prior for k = [1, ...,K]. Note that the prior p(\u03b8k) will be used in the maximum margin learning in Eq. (12). p(z) = \u0393(\u03b1) \u220fK k=1 \u0393(nk+\u03b1/K)\n\u0393(n+\u03b1)\u0393(\u03b1/K)K is the symmetric Dirichlet\nprior, where nk is the number of element in the cluster k, and \u03b1 is the concentration parameter.\nRecall that Dirichlet process mixture (DPM) [1, 7] is the widely used nonparametric Bayesian approach for clustering analysis and model learning, specified with DP prior measure G0 and \u03b1. As a joint likelihood model, it has to model p(X ), which is intractable for non-conjugate prior. The essential difference between our model and DPM is that we maximize a conditional probability, instead of joint probability as in DPM [14]. Moreover, our approach is a discriminative clustering model with component parameters learned under maximum margin framework.\nTo maximizing the objective function in Eq. (4), we hope the higher within-cluster correlation and lower correlation between different clusters. Given z, we will need to learn {\u03b8k}Kk=1 to keep each cluster as compact as possible, which in turn will help infer better K. In other words, to keep the objective climbing, we need higher likelihood p(xi|\u03b8zi) with higher correlation within-cluster, which can be addressed with discriminative clustering. Given the component parameters, {\u03b8k}Kk=1, we need to decide the label for each element for better K. For each round (on the instance level), we use Gibbs sampling to infer zi for each instance xi, which in turn can be used to estimate {\u03b8k}Kk=1 with online maximum margin learning. For each iteration (on the whole dataset), we also update \u03b1 with adaptive rejection sampling [18]."}, {"heading": "3.2.1 Gibbs sampling", "text": "Given the data points X = {xi}Ni=1 and its the cluster indicators z = {zi}Ni=1, the Gibbs sampling involves iterations that alternately draw samples from conditional probability while keeping other variables fixed. For each indicator variable zi, we can derive its conditional posterior as follows:\np(zi = k|z\u2212i,xi, {\u03b8k}Kk=1, \u03b1, \u03bb) (5) = p(zi = k|xi, z\u2212i, {\u03b8k}Kk=1) (6) \u221d p(zi = k|z\u2212i, {\u03b8k}Kk=1)p(xi|zi = k, {\u03b8k}Kk=1) (7) = p(zi = k|z\u2212i, \u03b1)p(xi|\u03b8k) (8)\nwhere the subscript \u2212i indicates all indices except for i, p(zi = k|z\u2212i, \u03b1) is determined by Chinese restaurant process, and p(xi|\u03b8k) is the likelihood for the current observation xi. For DPM, we need to maximize the conditional posterior to compute \u03b8k, which depends on observations belonging to this cluster and prior G0.\nIn our conditional likelihood model, we define the following likelihood for instance xi\np(xi|\u03b8k) \u221d exp(xTi \u03b8k \u2212 \u03bb||\u03b8k||2) (9)\nwhere \u03bb is a regularization constant to control weights between the two terms above. By default, the prediction function should be proportional to arg maxk(x T i \u03b8k), for k \u2208 [1,K]. In other words, higher correlation between xi and \u03b8k indicates higher probability that xi belongs to cluster k, which further leads to higher objective in Eq. (4). In our likelihood definition, we also subtract \u03bb||\u03b8k||2\nin Eq. (9), which can keep the maximum margin beneficial properties in the model to separate clusters as far away as possible. Another understanding for the above likelihood is that Eq. (9) satisfies the general form of exponential families, which are functions solely of the chosen sufficient statistics [22]. Thus, such probability assumption in Eq. (9) make it general to real applications.\nPlug Eq. (9) into Eq. (8), we get the final Gibbs sampling strategy for our model\np(zi = k|z\u2212i,xi, {\u03b8k}Kk=1, \u03b1, \u03bb) \u221d p(zi = k|z\u2212i, \u03b1)exp(xTi \u03b8k \u2212 \u03bb||\u03b8k||2) (10)\nWe will introduce online maximum margin learning for component parameters {\u03b8k}Kk=1 in Sec 3.2.2. For the newly created cluster, we assume \u03b8K+1 is sampled from multivariate t-distribution."}, {"heading": "3.2.2 Online maximum margin learning", "text": "We follow the passive aggressive algorithm (PA) [5] below in order to learn component parameters in our discriminative model with maximum margins [25].\nWe denote the instance presented to the algorithm on round t by xt \u2208 Rn, which is associated with a unique label zt \u2208 [1,K]. Note that the label zt is determined by the above Gibbs sampling algorithm in Eq. (10). We shall define \u0398 = [\u03b81, ...,\u03b8K ] a parameter vector by concatenating all the parameters {\u03b8k}Kk=1 (that means \u0398zt is zt-th block in \u0398, or says \u0398zt = \u03b8zt), and \u03a6(xt, zt) is a feature vector relating input xt and output zt, which is composed of K blocks, and all blocks but the zt-th are set to be the zero vector while the zt-th block is set to be xt. We denote by \u0398t the weight vector used by the algorithm on round t, and refer to the term \u03b3(\u0398t; (xt, zt)) = \u0398t \u00b7\u03a6(xt, zt) \u2212\u0398t \u00b7\u03a6(xt, z\u0302t) as the (signed) margin attained on round t. In this paper, we use the hinge-loss function, which is defined by the following,\n`(\u0398; (xt, zt)) = { 0 if \u03b3(\u0398t; (xt, zt)) \u2265 1 1\u2212 \u03b3(\u0398t; (xt, zt)) otherwise\n(11)\nFollowing the passive aggressive (PA) algorithm [5], we optimize the objective function:\n\u0398t+1 = arg min \u0398\n1 2 ||\u0398\u2212\u0398t||2 + C\u03be\ns.t. `(\u0398; (xt, zt)) \u2264 \u03be (12)\nwhere the l2 norm of \u0398 on the right hand size can be thought as Gaussian prior in Eq. (4). If there\u2019s loss, then the updates of PA-1 has the following closed form\n\u0398ztt+1 = \u0398 zt t + \u03c4txt, \u0398z\u0302tt+1 = \u0398 z\u0302t t \u2212 \u03c4txt,\n(13)\nwhere z\u0302t is the label prediction for xt, and \u03c4t = min{C, `(\u0398t;(xt,zt))||xt||2 }. Note that the Gibbs sampling step can decide the indicator variable zt for xt. Given the cluster label (the ground truth assignment) for xt, we update our parameter \u0398 using the above Eq. (13). For convergence analysis and time complexity, refer to [5]."}, {"heading": "3.3 Fine-tuning the model", "text": "Having determined the number of clusters and labels for all training data, we can take the fine-tuning process to refine the DBN parameters. Note that the objective function in Eq. (12) takes the l1 hinge loss as in [23]. Thus, one possible way is that we can take the sub-gradient and backpropagate the error to update DBN parameters. In our approach, we employ another method and only update the top layer weights WL and \u0398 in the deep structures. This fine-tuning process is inspired by the classification RBM [15] for model refining. Basically, we assume the top DBN layer weight WL and SVM weight \u0398 can be combined into a classification RBM as in [15] by maximizing the joint likelihood p(x, z) after we infer the cluster labels for all instances with NMMC. Note that there is mapping from SVM\u2019s scores to probabilistic outputs with logistic function [19], which can maintain label consistency between the SVM classifier and the softmax function. Thus, the SVM weight \u0398 can be used to initialize the weight of the softmax function in the classification RBM. After the fine-tuning process, we can maxz p(z|v) for z \u2208 [1,K] to label the unknown data v. For 1-layer DBN, we can get the following classification probability:\np(z|v) = edz \u220fn j=1 ( 1 + ecj+\u0398jz+ \u2211 iW1(i,j)vi )\u2211 z\u2217 e dz\u2217 \u220fn j=1 ( 1 + ecj+\u0398jz\u2217+ \u2211 iW1(i,j)vi\n) (14) where dz for z \u2208 [1,K] is the bias of clustering labels, and cj for j \u2208 [1, n] are biases of the hidden units. Note that \u0398 has been reshaped into n\u00d7K matrix before updating in the fine-tuning process. For the deep neural network with more than one layer, we first project v into the coding space x, then use the above equation for classification.\nIn our algorithm, we only fine-tune in the top layer because of the following reasons: (1) the objective function in Eq. (4) with deep feature learning is non-convex, which can be easily trapped into local minimum with L-BFGS [9]; (2) if there was clustering error in the top layer, it could be easily propagated in the backpropagation stage; (3) To only update the top layer can effectively handle the overfitting problem."}, {"heading": "4 Experimental Results", "text": "In order to analyze our model, we performed clustering analysis on two types of data: images and documents, and compared our results to competitive baselines. For all experiments, including pretraining and fine-tuning, we set the learning rate as 0.1, the maximum epoch to be 100, and used CD-1 to learn the weights and biases in the deep belief network. We used the adjusted Rand Index [11, 20] to evaluate all the clustering results.\nClustering on MNIST dataset: The MNIST dataset1 consists of 28 \u00d7 28-size images of handwriting digits from 0 through 9 with a training set of 60,000 examples and a test set of 10,000 examples, and has been widely used to test character recognition methods. In the experiment, we randomly sample 5000 images from the training sets for parameter learning and 1000 examples from the testing sets to test our model. After learning the features with DBN in the pre-training stage, we used NMMC for clustering, with setting \u03b1 = 4, \u03bb = 15 and C = 0.001. In the experiment, \u03bb\n1http://yann.lecun.com/exdb/mnist/\nplays a vital role on the final number of clusters. Higher \u03bb, larger number of clusters generated. To make an fair comparison, we basically tuned parameters to keep the number of generated clusters close to the groundtruth in the training stage. For example, in the MNIST experiment, we keep it around 5 to 20 in the training set for both NMMC and DPM. The results from baselines such as k-means and GMM should be conceived as upper bound (specify the number of clusters K = 10).\nThe clustering performance of our method (DBN+NMMC) is shown in Table (1), where \u201cpretrain\u201d and \u201cfine-tune\u201d indicate how the accuracy changes before and after the fine-tuning process for the same parameter setting on the same dataset. The results with 2-layer DBN in Table (1)\ndemonstrate that our method significantly outperforms baselines. It also shows that fine-tuning process can greatly improve accuracy, especially on the testing data. In Table (1), we think the largest train/test difference for the least complex model is caused by biases between before and after finetuning. In other words, the fine-tuning step can learn better biases via classification RBM and improve testing performance. We also visualize how the weights change before and after the fine-tuning process in Fig. (2).\nWe also evaluate how the depth and dimensionality of deep structures influence clustering accuracy. Fig. 3(a) shows how adjusted Rand Index changes with the number of dimensions for 1-layer DBN (or RBM), and it demonstrates that higher dimensionality does not mean higher performance. In Fig. 3(a), we can see fine-tuning severely hurt performance on the training set on higher dimension coding space, we guess it is caused by overfitting problem in the complex model. In other words, the wrong clustering prediction will deteriorate the clustering performance even further through fine-tuning. That makes sense because we treat the wrong labeling as the correct one in the finetuning stage. It also verifies that it is reasonable by just fine-tuning the model in the top layer, instead of the whole network, with the purpose to reduce the overfitting problem. Fig. 3(b) shows that given the 100 hidden nodes in the top layer, how the performance changes with the depth of DBN structure. It seems that the deeper complex model cannot guarantee better performance.\nTo verify whether our NMMC is effective for data clustering and model selection, we also compare our NMMC to DPM given the same DBN for feature learning. The results in Fig. (4) demonstrates that NMMC outperforms DPM significantly and also shows that our NMMC can always converge after 100 iterations. The time complexity comparison between our method and DPM is shown in Fig. 5 in the DBN projection space. It shows that our method is significantly efficient, compared to DPM. To manifest how effective our method is, we also show the upper bound DBN+GMM, with 2 layers n = [400, 100] in Table (1). It shows that features learned with DBN are helpful for clustering, compared to raw data. It also shows that our method yields better clustering results than the upper bound.\nClustering on 20 newsgroup: We also evaluated our model on 20 newsgroup datasets for document categorization. This document dataset has 20 categories, which has been widely used in text categorization and document classification. In the experiment, we tested our model on the binary version of the 20 newsgroup dataset2. We used the training set for training and tested the model on the testing dataset. After we learned features in the DBN, we used NMMC for clustering, with setting \u03b1 = 4, \u03bb = 30 and C = 0.001. To make an fair comparison, we basically took a similar setting as in the MNIST dataset, for both NMMC and DPM in order to generate the number of clusters which is comparable for both methods. Baselines such as k-means and GMM should be thought of as upper bound because they need to specify the number of clusters K = 20.\nThe clustering performance of our method (DBN+NMMC) on 20 newsgroups is shown in Table. (2). It also demonstrates that the fine-tuning process can greatly improve accuracy, especially on the testing data. Although our model cannot beat baselines on the training set, our model can achieve better evaluation performance on the testing set (better than GMM and k-means on the raw data clustering). To verify whether our NMMC is effective for data clustering and model selection, we also compare our NMMC to DPM given the same DBN for feature learning. The results in Fig.\n2http://www.cs.toronto.edu/~larocheh/public/datasets/20newsgroups/20newsgroups_{train,valid,\ntest}_binary_5000_voc.txt\n(6) demonstrate that NMMC outperforms DPM remarkably. To test how time complexity changes with respect to the number of dimensions in the projected space, we tried different coding spaces and compared our method with DPM, with results shown in Fig. 5. Again, it demonstrates our\nmethod is more efficient in practice.\nTo sum up, our model can converge well after 100 iterations from the experiments above. Moreover, the fine-tuning process in our model can greatly improve the performance on the test sets. Thus, it also shows that the parameters learned with NMMC can be embedded well in the deep structures.\nConclusion\nClustering is an important problem in machine learning and its performance highly depends on data representation. And, how to adapt the model complexity with data also pose a challenge. In this paper, we propose a deep belief network with nonparametric maximum margin clustering. This approach is inspired by recent advances of deep learning for representation learning. As an unsupervised method, our model leverages deep learning for feature learning and dimension reduction. Moreover, our approach with nonparametric maximum margin clustering (NMMC) is a discriminative clustering method, which can adapt model size automatically when data grows. In addition, the fine-tuning process can incorporate NMMC well in the deep structures. Thus, our approach can learn features for clustering and infer model complexity in an unified framework. We currently use DBN [10] instead of deep autoencoders [9] for fast feature learning because the latter is time-consuming for dimension reduction. In future work, we will explore deep autoencoders to learn better feature representation for clustering analysis. Another interesting topic to be explored is how to optimize the depth of deep learning structures in order to improve clustering performance."}], "references": [{"title": "Mixtures of dirichlet processes with applications to bayesian nonparametric problems", "author": ["C.E. Antoniak"], "venue": "Annals of Statistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "Support vector clustering", "author": ["A. Ben-Hur", "D. Horn", "H.T. Siegelmann", "V. Vapnik"], "venue": "J. Mach. Learn. Res", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "PAMI pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Variational inference for dirichlet process mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "JMLR pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "J. Mach. Learn. Res", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1973}, {"title": "Dirichlet process mixtures of generalized linear models", "author": ["L.A. Hannah", "D.M. Blei", "W.B. Powell"], "venue": "J. Mach. Learn. Res. pp", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science 313(5786),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Comput", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification 2(1),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1985}, {"title": "A nonparametric variable clustering model", "author": ["D.A. Knowles", "K. Palla", "Z. Ghahramani"], "venue": "In: NIPS. pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Collapsed variational Dirichlet process mixture models", "author": ["K. Kurihara", "M. Welling", "Y. Teh"], "venue": "Proc. Int. Jt. Conf. Artif. Intell", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "J. Mach. Learn. Res", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Deep supervised tdistributed embedding", "author": ["M.R. Min", "L. van der Maaten", "Z. Yuan", "A.J. Bonner", "Z. Zhang"], "venue": "Omnipress", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Implicit mixtures of restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Markov chain sampling methods for dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In: ADVANCES IN LARGE MARGIN CLASSIFIERS", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W. Rand"], "venue": "Journal of the American Statistical Association 66(336),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1971}, {"title": "The infinite gaussian mixture model. In: NIPS12", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Graphical models for visual object recognition and tracking", "author": ["E.B. Sudderth"], "venue": "Ph.D. thesis,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Deep learning using support vector machines", "author": ["Y. Tang"], "venue": "Workshop on Representational Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Dirichlet processes. In: Encyclopedia of Machine Learning", "author": ["Y.W. Teh"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "J. Mach. Learn. Res", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle"], "venue": "In: International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "In: NIPS17", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 33, "endOffset": 44}, {"referenceID": 25, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 33, "endOffset": 44}, {"referenceID": 2, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 33, "endOffset": 44}, {"referenceID": 8, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 99, "endOffset": 106}, {"referenceID": 26, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 99, "endOffset": 106}, {"referenceID": 9, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 135, "endOffset": 147}, {"referenceID": 14, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 135, "endOffset": 147}, {"referenceID": 22, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 135, "endOffset": 147}, {"referenceID": 5, "context": "The advantages of deep learning are that they give mappings which can capture meaningful structure information in the code space and introduce bias towards configurations of the parameter space that are helpful for unsupervised learning [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 2, "context": "restricted Boltzmann machines), with the purpose to yield more abstract and ultimately more useful representations [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "In addition, deep learning with gradient descent scales linearly in time and space with the number of train cases, which makes it possible to apply to large scale data sets [9].", "startOffset": 173, "endOffset": 176}, {"referenceID": 20, "context": "Another important research topic in clustering analysis is how to adapt model complexity for increasing volumes in the era of big data [21, 4, 24].", "startOffset": 135, "endOffset": 146}, {"referenceID": 3, "context": "Another important research topic in clustering analysis is how to adapt model complexity for increasing volumes in the era of big data [21, 4, 24].", "startOffset": 135, "endOffset": 146}, {"referenceID": 23, "context": "Another important research topic in clustering analysis is how to adapt model complexity for increasing volumes in the era of big data [21, 4, 24].", "startOffset": 135, "endOffset": 146}, {"referenceID": 0, "context": "This approach is an unsupervised clustering method, inspired by the advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models [1, 7, 4].", "startOffset": 161, "endOffset": 170}, {"referenceID": 6, "context": "This approach is an unsupervised clustering method, inspired by the advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models [1, 7, 4].", "startOffset": 161, "endOffset": 170}, {"referenceID": 3, "context": "This approach is an unsupervised clustering method, inspired by the advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models [1, 7, 4].", "startOffset": 161, "endOffset": 170}, {"referenceID": 1, "context": "However, they [2, 28] either cannot learn parameters online efficiently or need to define the number of clusters like other clustering approaches, such as k-means, Gaussian mixture model (GMM) and spectral clustering.", "startOffset": 14, "endOffset": 21}, {"referenceID": 27, "context": "However, they [2, 28] either cannot learn parameters online efficiently or need to define the number of clusters like other clustering approaches, such as k-means, Gaussian mixture model (GMM) and spectral clustering.", "startOffset": 14, "endOffset": 21}, {"referenceID": 3, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 13, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 7, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 11, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 0, "context": "One of the widely used nonparametric models for clustering is Dirichlet process mixture (DPM) [1, 7].", "startOffset": 94, "endOffset": 100}, {"referenceID": 6, "context": "One of the widely used nonparametric models for clustering is Dirichlet process mixture (DPM) [1, 7].", "startOffset": 94, "endOffset": 100}, {"referenceID": 8, "context": "Unsupervised feature learning with deep structures was first proposed in [9] for dimension reduction.", "startOffset": 73, "endOffset": 76}, {"referenceID": 26, "context": "Later, this unsupervised approach was developed into semi-supervised embedding [27] and supervised mapping [16] scenarios.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "Later, this unsupervised approach was developed into semi-supervised embedding [27] and supervised mapping [16] scenarios.", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 14, "endOffset": 17}, {"referenceID": 14, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 76, "endOffset": 84}, {"referenceID": 22, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 76, "endOffset": 84}, {"referenceID": 5, "context": "The success behind deep learning is that it can learn useful information for data visualization and classification [6, 3].", "startOffset": 115, "endOffset": 121}, {"referenceID": 2, "context": "The success behind deep learning is that it can learn useful information for data visualization and classification [6, 3].", "startOffset": 115, "endOffset": 121}, {"referenceID": 16, "context": "A recent interesting approach is the implicit mixture of RBMs [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "To learn RBM parameters, we need to optimize the negative log likelihood \u2212logp(v) on training data D, the parameters updating can be calculated with a efficient stochastic descent method, namely contrastive divergence (CD) [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "A Deep Belief Network (DBN) is composed of stacked RBMs [9] learned layer by layer greedily, where the top layer is an RBM and the lower layers can be interpreted as a directed sigmoid belief network [3], shown in Fig.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "A Deep Belief Network (DBN) is composed of stacked RBMs [9] learned layer by layer greedily, where the top layer is an RBM and the lower layers can be interpreted as a directed sigmoid belief network [3], shown in Fig.", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "Recall that Dirichlet process mixture (DPM) [1, 7] is the widely used nonparametric Bayesian approach for clustering analysis and model learning, specified with DP prior measure G0 and \u03b1.", "startOffset": 44, "endOffset": 50}, {"referenceID": 6, "context": "Recall that Dirichlet process mixture (DPM) [1, 7] is the widely used nonparametric Bayesian approach for clustering analysis and model learning, specified with DP prior measure G0 and \u03b1.", "startOffset": 44, "endOffset": 50}, {"referenceID": 13, "context": "The essential difference between our model and DPM is that we maximize a conditional probability, instead of joint probability as in DPM [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "For each iteration (on the whole dataset), we also update \u03b1 with adaptive rejection sampling [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "(9) satisfies the general form of exponential families, which are functions solely of the chosen sufficient statistics [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "We follow the passive aggressive algorithm (PA) [5] below in order to learn component parameters in our discriminative model with maximum margins [25].", "startOffset": 48, "endOffset": 51}, {"referenceID": 24, "context": "We follow the passive aggressive algorithm (PA) [5] below in order to learn component parameters in our discriminative model with maximum margins [25].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Following the passive aggressive (PA) algorithm [5], we optimize the objective function:", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "For convergence analysis and time complexity, refer to [5].", "startOffset": 55, "endOffset": 58}, {"referenceID": 22, "context": "(12) takes the l1 hinge loss as in [23].", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "This fine-tuning process is inspired by the classification RBM [15] for model refining.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "Basically, we assume the top DBN layer weight WL and SVM weight \u0398 can be combined into a classification RBM as in [15] by maximizing the joint likelihood p(x, z) after we infer the cluster labels for all instances with NMMC.", "startOffset": 114, "endOffset": 118}, {"referenceID": 18, "context": "Note that there is mapping from SVM\u2019s scores to probabilistic outputs with logistic function [19], which can maintain label consistency between the SVM classifier and the softmax function.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "(4) with deep feature learning is non-convex, which can be easily trapped into local minimum with L-BFGS [9]; (2) if there was clustering error in the top layer, it could be easily propagated in the backpropagation stage; (3) To only update the top layer can effectively handle the overfitting problem.", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "We used the adjusted Rand Index [11, 20] to evaluate all the clustering results.", "startOffset": 32, "endOffset": 40}, {"referenceID": 19, "context": "We used the adjusted Rand Index [11, 20] to evaluate all the clustering results.", "startOffset": 32, "endOffset": 40}, {"referenceID": 16, "context": "02 IMRBM [17] (n = 100, K = 10) 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "010 IMRBM [17] (n = 200, K = 20) 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "We currently use DBN [10] instead of deep autoencoders [9] for fast feature learning because the latter is time-consuming for dimension reduction.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "We currently use DBN [10] instead of deep autoencoders [9] for fast feature learning because the latter is time-consuming for dimension reduction.", "startOffset": 55, "endOffset": 58}], "year": 2015, "abstractText": "Clustering is an essential problem in machine learning and data mining. One vital factor that impacts clustering performance is how to learn or design the data representation (or features). Fortunately, recent advances in deep learning can learn unsupervised features effectively, and have yielded state of the art performance in many classification problems, such as character recognition, object recognition and document categorization. However, little attention has been paid to the potential of deep learning for unsupervised clustering problems. In this paper, we propose a deep belief network with nonparametric clustering. As an unsupervised method, our model first leverages the advantages of deep learning for feature representation and dimension reduction. Then, it performs nonparametric clustering under a maximum margin framework \u2013 a discriminative clustering model and can be trained online efficiently in the code space. Lastly model parameters are refined in the deep belief network. Thus, this model can learn features for clustering and infer model complexity in an unified framework. The experimental results show the advantage of our approach over competitive baselines.", "creator": "LaTeX with hyperref package"}}}