{"id": "1501.05352", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2015", "title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "abstract": "in algebraic binary hashing, one wants to learn a stream function that maps to a high - dimensional feature indicator vector to a vector mesh of binary phase codes, for continuous application to analyze fast image element retrieval. this typically also results in a difficult optimization method problem, nonconvex compression and nearly nonsmooth, because mainly of changing the simple discrete fourier variables involved. much work methodology has simply relaxed the original problem during thorough training, besides solving a continuous optimization, designing and truncating the constituent codes a posteriori. this adjustment gives seemingly reasonable results but retention is suboptimal. recent work involves has applied alternating optimization to the objective scheme over introducing the binary codes parameters and achieved better results, but previously the hash function was still intentionally learned a posteriori, which which remains frequently suboptimal. we propose a general framework variant for naturally learning additive hash functions independently using affinity - based loss cancel functions arithmetic that closes periodically the block loop ; and optimizes jointly over the hash termination functions function and the binary codes. the accelerated resulting sequential algorithm complexity can be seen as designing a simultaneous corrected, iterated version cipher of the procedure responsible of appropriately optimizing first identities over both the codes and outputs then learning approximately the intrinsic hash generating function. compared to changing this, our primal optimization is guaranteed to obtain better functional hash functions while being technically not systematically much slower, properties as demonstrated now experimentally demonstrate in various supervised and biased unsupervised datasets. integration in addition, within the framework facilitates successfully the design of partial optimization algorithms for arbitrary pulse types representation of loss flows and hash functions.", "histories": [["v1", "Wed, 21 Jan 2015 23:53:47 GMT  (64kb)", "https://arxiv.org/abs/1501.05352v1", "18 pages, 7 figures"], ["v2", "Fri, 5 Feb 2016 01:25:26 GMT  (230kb)", "http://arxiv.org/abs/1501.05352v2", "22 pages, 12 figures; added new experiments and references"]], "COMMENTS": "18 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV math.OC stat.ML", "authors": ["ramin raziperchikolaei", "miguel \u00e1 carreira-perpi\u00f1\u00e1n"], "accepted": true, "id": "1501.05352"}, "pdf": {"name": "1501.05352.pdf", "metadata": {"source": "CRF", "title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "authors": ["Ramin Raziperchikolaei", "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n05 35\n2v 2\n[ cs\n.L G\n] 5\nF eb\nIn supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions."}, {"heading": "1 Introduction", "text": "Information retrieval arises in several applications, most obviously web search. For example, in image retrieval, a user is interested in finding similar images to a query image. Computationally, this essentially involves defining a high-dimensional feature space where each relevant image is represented by a vector, and then finding the closest points (nearest neighbors) to the vector for the query image, according to a suitable distance (Shakhnarovich et al., 2006). For example, one can use features such as SIFT (Lowe, 2004) or GIST (Oliva and Torralba, 2001) and the Euclidean distance for this purpose. Finding nearest neighbors in a dataset of N images (where N can be millions), each a vector of dimension D (typically in the hundreds) is slow, since exact algorithms run essentially in time O(ND) and space O(ND) (to store the image dataset). In practice, this is approximated, and a successful way to do this is binary hashing (Grauman and Fergus, 2013). Here, given a high-dimensional vector x \u2208 RD, the hash function h maps it to a b-bit vector z = h(x) \u2208 {\u22121,+1}b, and the nearest neighbor search is then done in the binary space. This now costs O(Nb) time and space, which is orders of magnitude faster because typically b < D and, crucially, (1) operations with binary vectors (such as computing Hamming distances) are very fast because of hardware support, and (2) the entire dataset can fit in (fast) memory rather than slow memory or disk.\nThe disadvantage is that the results are inexact, since the neighbors in the binary space will not be identical to the neighbors in the original space. However, the approximation error can be controlled by using sufficiently many bits and by learning a good hash function. This has been the topic of much work in recent years. The general approach consists of defining a supervised objective that has a small value for good hash functions and minimizing it. Ideally, such an objective function should be minimal when the neighbors of any given image are the same in both original and binary spaces. Practically in information retrieval, this is often evaluated using precision and recall. However, this ideal objective cannot be easily optimized over hash functions, and one uses approximate objectives instead. Many such objectives have been proposed in\nthe literature. We focus here on affinity-based loss functions, which directly try to preserve the original similarities in the binary space. Specifically, we consider objective functions of the form\nminL(h) =\nN \u2211\nn,m=1\nL(h(xn),h(xm); ynm) (1)\nwhere X = (x1, . . . ,xN ) is the high-dimensional dataset of feature vectors, minh means minimizing over the parameters of the hash function h (e.g. over the weights of a linear SVM), and L(\u00b7) is a loss function that compares the codes for two images (often through their Hamming distance \u2016h(xn)\u2212 h(xm)\u2016) with the ground-truth value ynm that measures the affinity in the original space between the two images xn and xm (distance, similarity or other measure of neighborhood; Grauman and Fergus, 2013). The sum is often restricted to a subset of image pairs (n,m) (for example, within the k nearest neighbors of each other in the original space), to keep the runtime low. Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpin\u0303a\u0301n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) or Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al., 2012).\nIf the hash function h was a continuous function of its input x and its parameters, one could simply apply the chain rule to compute derivatives over the parameters of h of the objective function (1) and then apply a nonlinear optimization method such as gradient descent. This would be guaranteed to converge to an optimum under mild conditions (for example, Wolfe conditions on the line search), which would be global if the objective is convex and local otherwise (Nocedal and Wright, 2006). Hence, optimally learning the function h would be in principle doable (up to local optima), although it would still be slow because the objective can be quite nonlinear and involve many terms.\nIn binary hashing, the optimization is much more difficult, because in addition to the previous issues, the hash function must output binary values, hence the problem is not just generally nonconvex, but also nonsmooth. In view of this, much work has sidestepped the issue and settled on a simple but suboptimal solution. First, one defines the objective function (1) directly on the b-dimensional codes of each image (rather than on the hash function parameters) and optimizes it assuming continuous codes (in Rb). Then, one binarizes the codes for each image. Finally, one learns a hash function given the codes. Optimizing the affinity-based loss function (1) can be done using spectral methods or nonlinear optimization as described above. Binarizing the codes has been done in different ways, from simply rounding them to {\u22121,+1} using zero as threshold (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011, 2012), to optimally finding a threshold (Liu et al., 2011; Strecha et al., 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al., 2013). Finally, learning the hash function for each of the b output bits can be considered as a binary classification problem, where the resulting classifiers collectively give the desired hash function, and can be solved using various machine learning techniques. Several works (e.g. Zhang et al., 2010; Lin et al., 2013, 2014) have used this approach, which does produce reasonable hash functions (in terms of retrieval measures such as precision and recall).\nIn order to do better, one needs to take into account during the optimization (rather than after the optimization) the fact that the codes are constrained to be binary. This implies attempting directly the discrete optimization of the affinity-based loss function over binary codes. This is a daunting task, since this is usually an NP-complete problem with Nb binary variables altogether, and practical applications could make this number as large as millions or beyond. Recent works have applied alternating optimization (with various refinements) to this, where one optimizes over a usually small subset of binary variables given fixed values for the remaining ones (Lin et al., 2013, 2014), and this did result in very competitive precision/recall compared with the state-of-the-art. This is still slow and future work will likely improve it, but as of now it provides an option to learn better binary codes.\nOf the three-step suboptimal approach mentioned (learn continuous codes, binarize them, learn hash function), these works manage to join the first two steps and hence learn binary codes. Then, one learns the hash function given these binary codes. Can we do better? Indeed, in this paper we show that all elements of the problem (binary codes and hash function) can be incorporated in a single algorithm that optimizes\njointly over them. Hence, by initializing it from binary codes from the previous approach, this algorithm is guaranteed to achieve a lower error and learn better hash functions. In fact, our framework can be seen as an iterated, corrected version of the two-step approach: learn binary codes given the current hash function, learn hash functions given codes, iterate (note the emphasis). The key to achieve this in a principled way is to use a recently proposed method of auxiliary coordinates (MAC) for optimizing \u201cnested\u201d systems, i.e., consisting of the composition of two or more functions or processing stages. MAC introduces new variables and constraints that cause decoupling between the stages, resulting in the mentioned alternation between learning the hash function and learning the binary codes. Section 2 reviews affinity-based loss functions, section 3 describes our MAC-based proposed framework, section 4 evaluates it in several supervised datasets, using linear and nonlinear hash functions, and section 5 discusses implications of this work.\nRelated work Although one can construct hash functions without training data (Andoni and Indyk, 2008; Kulis and Grauman, 2012), we focus on methods that learn the hash function given a training set, since they perform better, and our emphasis is in optimization. The learning can be unsupervised, which attempts to preserve distances in the original space, or supervised, which in addition attempts to preserve label similarity. Many objective functions have been proposed to achieve this and we focus on affinity-based ones. These create an affinity matrix for a subset of training points based on their distances (unsupervised) or labels (supervised) and combine it with a loss function (Liu et al., 2012; Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Lin et al., 2013, 2014). Some methods optimize this directly over the hash function. For example, Binary Reconstructive Embeddings (Kulis and Darrell, 2009) use alternating optimization over the weights of the hash functions. Supervised Hashing with Kernels (Liu et al., 2012) learns hash functions sequentially by considering the difference between the inner product of the codes and the corresponding element of the affinity matrix. Although many approaches exist, a common theme is to apply a greedy approach where one first finds codes using an affinity-based loss function, and then fits the hash functions to them (usually by training a classifier). The codes can be found by relaxing the problem and binarizing its solution (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al., 2013, 2014; Ge et al., 2014), or by using relaxation in other ways (Liu et al., 2012; Norouzi and Fleet, 2011)."}, {"heading": "2 Nonlinear embedding and affinity-based loss functions for bi-", "text": "nary hashing\nThe dimensionality reduction literature has developed a number of objective functions of the form (1) (often called \u201cembeddings\u201d) where the low-dimensional projection zn \u2208 R\nb of each high-dimensional data point xn \u2208 R\nD is a free, real-valued parameter. The neighborhood information is encoded in the ynm values (using labels in supervised problems, or distance-based affinities in unsupervised problems). A representative example is the elastic embedding (Carreira-Perpin\u0303a\u0301n, 2010), where L(zn, zm; ynm) has the form:\ny+nm \u2016zn \u2212 zm\u2016 2 + \u03bby\u2212nm exp (\u2212\u2016zn \u2212 zm\u2016 2 ), \u03bb > 0 (2)\nwhere the first term tries to project true neighbors (having y+nm > 0) close together, while the second repels all non-neighbors\u2019 projections (having y\u2212nm > 0) from each other. Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000) result from replacing the second term above with a constraint that fixes the scale of Z, which results in an eigenproblem rather than a nonlinear optimization, but also produces more distorted embeddings. Other objectives exist, such as t-SNE (van der Maaten and Hinton, 2008), that do not separate into functions of pairs of points. Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpin\u0303a\u0301n, 2010; Vladymyrov and Carreira-Perpin\u0303a\u0301n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpin\u0303a\u0301n, 2014). Although these models were developed to produce continuous projections, they have been successfully used for binary hashing too by truncating their codes (Weiss et al., 2009; Zhang et al., 2010) or using the two-step approach of (Lin et al., 2013, 2014).\nOther loss functions have been developed specifically for hashing, where now zn is a b-bit vector (where binary values are in {\u22121,+1}). For example (see a longer list in Lin et al., 2013), for Supervised Hashing\nwith Kernels (KSH) L(zn, zm; ynm) has the form\n(zTnzm \u2212 bynm) 2 (3)\nwhere ynm is 1 if xn, xm are similar and \u22121 if they are dissimilar. Binary Reconstructive Embeddings (Kulis and Darrell, 2009) uses (1\nb \u2016zn \u2212 zm\u2016\n2 \u2212 ynm) 2 where ynm = 1 2 \u2016xn \u2212 xm\u2016 2 . The exponential variant\nof SPLH (Wang et al., 2012) proposed by Lin et al. (2013) (which we call eSPLH) uses exp(\u2212 1 b ynmz T nzn). Our approach can be applied to any of these loss functions, though we will mostly focus on the KSH loss for simplicity. When the variables Z are binary, we will call these optimization problems binary embeddings, in analogy to the more traditional continuous embeddings for dimension reduction."}, {"heading": "3 Learning codes and hash functions using auxiliary coordinates", "text": "The optimization of the loss L(h) in eq. (1) is difficult because of the thresholded hash function, which appears as the argument of the loss function L. We use the recently proposed method of auxiliary coordinates (MAC) (Carreira-Perpin\u0303a\u0301n and Wang, 2012, 2014), which is a meta-algorithm to construct optimization algorithms for nested functions. This proceeds in 3 stages. First, we introduce new variables (the \u201cauxiliary coordinates\u201d) as equality constraints into the problem, with the goal of unnesting the function. We can achieve this by introducing one binary vector zn \u2208 {\u22121,+1} for each point. This transforms the original, unconstrained problem into the following, constrained problem:\nmin h,Z\nN \u2211\nn=1\nL(zn, zm; ynm) s.t.\n{\nz1 = h(x1) \u00b7 \u00b7 \u00b7\nzN = h(xN ) (4)\nwhich is seen to be equivalent to (1) by eliminating Z. We recognize as the objective function the \u201cembedding\u201d form of the loss function, except that the \u201cfree\u201d parameters zn are in fact constrained to be the deterministic outputs of the hash function h.\nSecond, we solve the constrained problem using a penalty method, such as the quadratic-penalty or augmented Lagrangian (Nocedal and Wright, 2006). We discuss here the former for simplicity. We solve the following minimization problem (unconstrained again, but dependent on \u00b5) while progressively increasing \u00b5, so the constraints are eventually satisfied:\nminLP (h,Z;\u00b5) =\nN \u2211\nn,m=1\nL(zn, zm; ynm) + \u00b5\nN \u2211\nn=1\n\u2016zn \u2212 h(xn)\u2016 2\ns.t. z1, . . . , zN \u2208 {\u22121, 1} b. (5)\nThe quadratic penalty \u2016zn \u2212 h(xn)\u2016 2 is proportional to the Hamming distance between the binary vectors zn and h(xn). Third, we apply alternating optimization over the binary codes Z and the hash function parameters h. This results in iterating the following two steps (described in detail later):\n\u2022 Optimize the binary codes z1, . . . , zN given h (hence, given the output binary codes h(x1), . . . ,h(xN ) for each of the N images). This can be seen as a regularized binary embedding, because the projections Z are encouraged to be close to the hash function outputs h(X). Here, we try two different approaches (Lin et al., 2013, 2014) with some modifications.\n\u2022 Optimize the hash function h given binary codes Z. This reduces to training b binary classifiers using X as inputs and Z as targets.\nThis is very similar to the two-step (TSH) approach of Lin et al. (2013), except that the latter learns the codes Z in isolation, rather than given the current hash function, so iterating the two-step approach would change nothing, and it does not optimize the loss L. More precisely, TSH corresponds to optimizing LP for \u00b5 \u2192 0+. In practice, we start from a very small value of \u00b5 (hence, initialize MAC from the result of TSH), and increase \u00b5 slowly while optimizing LP , until the equality constraints are satisfied, i.e., zn = h(xn) for n = 1, . . . , N .\nFig. 1 gives the overall MAC algorithm to learn a hash function by optimizing an affinity-based loss function. We now describe the steps over h and Z, and the path followed by the iterates as a function of \u00b5."}, {"heading": "3.1 Stopping criterion, schedule over \u00b5 and path of optimal values", "text": "It is possible to prove that once Z = h(X) after a Z step (regardless of the value of \u00b5), the MAC algorithm will make no further changes to Z or h, since then the constraints are satisfied. This gives us a reliable stopping criterion that is easy to check, and the MAC algorithm will stop after a finite number of iterations (see below).\nIt is also possible to prove that the path of minimizers of LP over the continuous penalty parameter \u00b5 \u2208 [0,\u221e) is in fact discrete, with changes to (Z,h) happening only at a finite number of values 0 < \u00b51 < \u00b7 \u00b7 \u00b7 < \u00b5\u221e < \u221e. Based on this and on our practical experience, we have found that the following approach leads to good schedules for \u00b5 with little effort. We use exponential schedules, of the form \u00b5i = \u00b51\u03b1 i\u22121 for i = 1, 2, . . . , so the user has to set only two parameters: the initial \u00b51 and the multiplier \u03b1 > 1. We choose exponential schedules because typically the algorithm makes most progress at the beginning, and it is important to track a good minimum there. The upper value \u00b5\u221e past which no changes occur will be reached by our exponential schedule in a finite number of iterations, and our stopping criterion will detect that. We set the multiplier to a value 1 < \u03b1 < 2 that is as small as computationally convenient. If \u03b1 is too small, the algorithm will take many iterations, some of which may not even change Z or h (because the path of minima is discrete). If \u03b1 is too big, the algorithm will reach too quickly a stopping point, without having had time to find a better minimum. As for the initial \u00b51, we estimate it by trying values (exponentially spaced) until we find a \u00b5 for which changes to Z from its initial value (for \u00b5 = 0) start to occur. (It is also possible to find lower and upper bounds for \u00b51 and \u00b5\u221e, respectively, for a particular loss function, such as KSH, eSPH or EE.) Overall, the computational time required to estimate \u00b51 and \u03b1 is comparable to running a few extra iterations of the MAC algorithm.\nFinally, in practice we use a form of early stopping in order to improve generalization. We use a small validation set to evaluate the precision achieved by the hash function h along the MAC optimization. If the precision decreases over that of the previous step, we ignore the step and skip to the next value of \u00b5. Besides helping to avoid overfitting, this saves computation, by avoid such extra optimization steps. Since the validation set is small, it provides a noisy estimate of the generalization ability at the current iterate, and this occasionally leads to skipping a valid \u00b5 value. This is not a problem because the next \u00b5 value, which is close to the one we skipped, will likely work. At some point during the MAC optimization, we do reach an overfitting region and the precision stops increasing, so the algorithm will skip all remaining \u00b5 values until it stops. In summary, using this validation procedure guarantees that the precision (in the validation set) is greater or equal than that of the initial Z, thus resulting in a better hash function."}, {"heading": "3.2 h step", "text": "Given the binary codes z1, . . . , zN , since h does not appear in the first term of LP , this simply involves finding a hash function h that minimizes\nmin h\nN \u2211\nn=1\n\u2016zn \u2212 h(xn)\u2016 2 =\nb \u2211\ni=1\nmin hi\nN \u2211\nn=1\n(zni \u2212 hi(xn)) 2\nwhere zni \u2208 {\u22121,+1} is the ith bit of the binary vector zn. Hence, we can find b one-bit hash functions in parallel and concatenate them into the b-bit hash function. Each of these is a binary classification problem using the number of misclassified patterns as loss. This allows us to use a regular classifier for h, and even to use a simpler surrogate loss (such as the hinge loss), since this will also enforce the constraints eventually (as \u00b5 increases). For example, we can fit an SVM by optimizing the margin plus the slack and using a high penalty for misclassified patterns. We discuss other classifiers in the experiments."}, {"heading": "3.3 Z step", "text": "Although the MAC technique has significantly simplified the original problem, the step over Z is still complex. This involves finding the binary codes given the hash function h, and it is an NP-complete problem in Nb binary variables. Fortunately, some recent works have proposed practical approaches for this problem based on alternating optimization: a quadratic surrogate method (Lin et al., 2013), and a GraphCut method (Lin et al., 2014). In both cases, this would correspond to the first step in the two-step hashing of Lin et al. (2013).\nIn both the quadratic surrogate and the GraphCut method, the starting point is to apply alternating optimization over the ith bit of all points given the remaining bits are fixed for all points (for i = 1, . . . , b), and to solve the optimization over the ith bit approximately. We describe this next for each method. We start by describing each method in their original form (which applies to the loss function over binary codes, i.e., the first term in LP ), and then we give our modification to make it work with our Z step objective (the regularized loss function over binary codes, i.e., the complete LP ).\nSolution using a quadratic surrogate method (Lin et al., 2013) This is based on the fact that any loss function that depends on the Hamming distance of two binary variables can be equivalently written as a quadratic function of those two binary variables (Lin et al., 2013). Since this is the case for every term L(zn, zm; ynm) (because only the ith bit in each of zn and zm is free), we can write the first term in LP as a binary quadratic problem. We now consider the second term (on \u00b5) as well. (We use a similar notation as that of Lin et al., 2013.) The optimization for the ith bit can be written as:\nmin z(i)\nN \u2211\nn,m=1\nli(zni, zmi) + \u00b5\nN \u2211\nn=1\n(zni \u2212 hi(xn)) 2 (6)\nwhere li = L(zni, zmi, z\u0304n, z\u0304m; ynm) is the loss function defined on the ith bit, zni is the ith bit of the nth point, z\u0304n is a vector containing the binary codes of the nth point except the ith bit, and hi(xn) is the ith bit of the binary code of the nth point generated by the hash function h. Lin et al. (2013) show that l(z1, z2) can be replaced by a binary quadratic function\nl(z1, z2) = 1 2z1z2\n( l(11) \u2212 l(\u221211) ) + constant (7)\nas long as l(1, 1) = l(\u22121,\u22121) = l(11) and l(1,\u22121) = l(\u22121, 1) = l(\u221211), where z1, z2 \u2208 {\u22121, 1}. Equation (7) helps us to rewrite the optimization (6) as the following:\nmin z(i)\nN \u2211\nn,m=1\n1 2 znizmi ( l(11) \u2212 l(\u221211) )\n+ \u00b5 N \u2211\nn=1\n(zni \u2212 hi(xn)) 2. (8)\nBy defining anm = ( l (11) (inm) \u2212 l (\u221211) (inm) ) as the (n,m) element of a matrix A \u2208 RN\u00d7N and ignoring the coefficients, we have the following optimization problem:\nmin z(i)\nzT(i)Az(i) + \u00b5 \u2225 \u2225z(i) \u2212 hi(X) \u2225 \u2225 2 s.t. z(i) \u2208 {\u22121,+1} N\nwhere hi(X) = (hi(x1), . . . , hi(xN )) T is a vector of length N (one bit per data point). Both terms in the above minimization are quadratic on binary variables. This is still an NP-complete problem (except in special cases), and we approximate it by relaxing it to a continuous quadratic program (QP) over z(i) \u2208 [\u22121, 1]\nN and binarizing its solution. In general, the matrix A is not positive definite and the relaxed QP is not convex, so we need an initialization. (However, the term on \u00b5 adds \u00b5I to A, so even if A is not positive definite, A+\u00b5I will be positive definite for large enough \u00b5, and the QP will be convex.) We construct an initialization by converting the binary QP into a binary eigenproblem:\nmin \u03b1\n\u03b1 TB\u03b1 s.t. \u03b10 = 1, z(i) \u2208 {\u22121, 1} N , \u03b1 = ( z(i) \u03b10 )\n, B = (\nA \u2212 \u00b5\n2 hi(X)\n\u2212 \u00b5 2 hi(X) T 0\n)\n. (9)\nTo solve this problem we use spectral relaxation, where the constraints z(i) \u2208 {\u22121,+1} N and zi+1 = 1 are relaxed to \u2016\u03b1\u2016 = N + 1. The solution to this problem is the eigenvector corresponding to the smallest eigenvalue of B. We use the truncated eigenvector as the initialization for minimizing the relaxed, boundconstrained QP:\nmin z(i)\nzT(i)Az(i) + \u00b5 \u2225 \u2225z(i) \u2212 hi(X) \u2225 \u2225 2 s.t. z(i) \u2208 [\u22121, 1] N .\nwhich we solve using L-BFGS-B (Zhu et al., 1997). As noted above, the Z step is an NP-complete problem in general, so we cannot expect to find the global optimum. It is even possible that the approximate solution could increase the objective over the previous iteration\u2019s Z (this is likely to happen as the overall MAC algorithm converges). If that occurs, we simply skip the update, in order to guarantee that we decrease monotonically on LP , and avoid oscillating around a minimum.\nSolution using a GraphCut algorithm (Lin et al., 2014) To optimize over the ith bit (given all the other bits are fixed), we have to minimize eq. (8). In general, this is an NP-complete problem over N bits (the ith bit for each image), with the form of a quadratic function on binary variables. We can apply the GraphCut algorithm (Boykov and Kolmogorov, 2003, 2004; Kolmogorov and Zabih, 2003), as proposed by the FastHash algorithm of Lin et al. (2014). This proceeds as follows. First, we assign all the data points to different, possibly overlapping groups (blocks). Then, we minimize the objective function over the binary codes of the same block, while all the other binary codes are fixed, then proceed with the next block, etc. (that is, we do alternating optimization of the bits over the blocks). Specifically, to optimize over the bits in block B, we define anm = ( l (11) (inm) \u2212 l (\u221211) (inm) ) and, ignoring the constants, we can rewrite equation (8) as:\nmin z(i,B)\n\u2211\nn\u2208B\n\u2211\nm\u2208B\nanmznizmi + 2 \u2211\nn\u2208B\n\u2211\nm 6\u2208B\nanmznizmi \u2212 \u00b5 \u2211\nn\u2208B\nznihi(xn).\nWe then rewrite this equation in the standard form for the GraphCut algorithm:\nmin z(i,B)\n\u2211\nn\u2208B\n\u2211\nm\u2208B\nvnmznizmi + \u2211\nn\u2208B\nunmzni\nwhere vnm = anm, unm = 2 \u2211\nm 6\u2208B anmzmi\u2212\u00b5hi(xn). To minimize the objective function using the GraphCut algorithm, the blocks have to define a submodular function. For the objective functions that we explained in the paper, this can be easily achieved by putting points with the same label in one block (Lin et al., 2014 give a simple proof of this).\nUnlike in the quadratic surrogate method, using the GraphCut algorithm with alternating optimization on blocks defining submodular functions is guaranteed to find a Z that has a lower or equal objective value that the initial one, and therefore to decrease monotonically LP ."}, {"heading": "4 Experiments", "text": "We have tested our framework with several combinations of loss function, hash function, number of bits, datasets, and comparing with several state-of-the-art hashing methods (appendix A contains additional experiments). We report a representative subset to show the flexibility of the approach. We use the KSH (3) (Liu et al., 2012) and eSPLH (Wang et al., 2012) loss functions. We test quadratic surrogate and GraphCut methods for the Z step in MAC. As hash functions (for each bit), we use linear SVMs (trained with LIBLINEAR; Fan et al., 2008) and kernel SVMs1\nWe use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes. We use D = 320 GIST features (Oliva and Torralba, 2001) from each image. We use 58 000 images for training and 2 000 for test. (2) Infinite MNIST (Loosli et al., 2007). We generated, using elastic deformations of the original MNIST handwritten digit dataset, 1 000 000 images for training and 2 000 for test, in 10 classes. We represent each image by a D = 784 vector of raw pixels. Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013). We train the hash functions in a subset of 10 000 points of the training set, and report precision and recall by searching for a test query on the entire dataset (the base set).\nWe report precision and precision/recall for the test set queries using as ground truth (set of true neighbors in original space) all the training points with the same label. In precision curves, the retrieved set contains the k nearest neighbors of the query point in the Hamming space. We report precision for different values of k to test the robustness of different algorithms. In precision/recall curves, the retrieved set contains the points inside Hamming distance r of the query point. These curves show the precision and recall at different Hamming distances r = 0 to r = L. We report zero precision when there is no neighbor inside Hamming distance r of a query. This happens most of the time when L is large and r is small. In most of our precision/recall curves, the precision drops significantly for very small and very large values of r. For small values of r, this happens because most of the query points do not retrieve any neighbor. For large values of r, this happens because the number of retrieved points becomes very large.\nThe main comparison point are the quadratic surrogate and GraphCut methods of Lin et al. (2013, 2014), which we denote in this section as quad and cut, respectively, regardless of the hash function that fits the resulting codes. Correspondingly, we denote the MAC version of these as MACquad and MACcut, respectively. We use the following schedule for the penalty parameter \u00b5 in the MAC algorithm (regardless of the hash function type or dataset). We initialize Z with \u00b5 = 0, i.e., the result of quad or cut. Starting from \u00b51 = 0.3 (MACcut) or 0.01 (MACquad), we multiply \u00b5 by 1.4 after each iteration (Z and h step).\nOur experiments show that the MAC algorithm indeed finds hash functions with a significantly and consistently lower objective function value than rounding or two-step approaches (in particular, cut and quad); and that it outperforms other state-of-the-art algorithms on different datasets, with MACcut beating MACquad most of the time. The improvement in precision makes using MAC well worth the relatively small extra runtime and minimal additional implementation effort it requires. In all our plots, the vertical arrows indicate the improvement of MACcut over cut and of MACquad over quad."}, {"heading": "4.1 The MAC algorithm finds better optima", "text": "The goal of this paper is not to introduce a new affinity-based loss or hash function, but to describe a generic framework to construct algorithms that optimize a given combination thereof. We illustrate its effectiveness here with the CIFAR dataset, with different sizes of retrieved neighbor sets, and using 16 to 48 bits. We optimize two affinity-based loss functions (KSH from eq. (3) and eSPLH), and two hash functions (linear and kernel SVM). In all cases, the MAC algorithm achieves a better hash function both in terms of the loss and of the precision/recall. We compare 4 ways of optimizing the loss function: quad (Lin et al., 2013), cut (Lin et al., 2014), MACquad and MACcut.\nFor each point xn in the training set, we use \u03ba+ = 100 positive (similar) and \u03ba\u2212 = 500 negative\n1To train a kernel SVM, we use 500 radial basis functions with centers given by a random subset of the training points, and apply a linear SVM to their output. Computationally, this is fast because we can use a constant Gram matrix. Using as hash function a kernel SVM trained with LIBSVM gave similar results, but is much slower because the support vectors change when the labels change. We set the RBF bandwidth to the average Euclidean distance of the first 300 points.\n(dissimilar) neighbors, chosen at random to have the same or a different label as xn, respectively. Fig. 2(top panel) shows the KSH loss function for all the methods (including the original KSH method in Liu et al., 2012) over iterations of the MAC algorithm (KSH, quad and cut do not iterate), as well as precision and recall. It is clear that MACcut (red lines) and MACquad (magenta lines) reduce the loss function more than cut (blue lines) and quad (black lines), respectively, as well as the original KSH algorithm (cyan), in all cases: type of hash function (linear: dashed lines, kernel: solid lines) and number of bits b = 16 to 48. Hence, applying MAC is always beneficial. Reducing the loss nearly always translates into better precision and recall (with a larger gain for linear than for kernel hash functions, usually). The gain of MACcut/MACquad over cut/quad is significant, often comparable to the gain obtained by changing from the linear to the kernel hash function within the same algorithm.\nWe usually find cut outperforms quad (in agreement with Lin et al., 2014), and correspondingly MACcut outperforms MACquad. Interestingly, MACquad and MACcut end up being very similar even though they started very differently. This suggests it is not crucial which of the two methods to use in the MAC Z step, although we still prefer cut, because it usually produces somewhat better optima. Finally, fig. 2(bottom panel) shows the MACcut results using the eSPLH loss. All settings are as in the first KSH experiment. As before, MACcut outperforms cut in both loss function and precision/recall using either a linear or a kernel SVM."}, {"heading": "4.2 Why does MAC learn better hash functions?", "text": "In both the two-step and MAC approaches, the starting point are the \u201cfree\u201d binary codes obtained by minimizing the loss over the codes without them being the output of a particular hash function. That is, minimizing (4) without the \u201czn = h(xn)\u201d constraints:\nmin Z E(Z) =\nN \u2211\nn=1\nL(zn, zm; ynm), z1, . . . , zN \u2208 {\u22121,+1} b. (10)\nThe resulting free codes try to achieve good precision/recall independently of whether a hash function can actually produce such codes. Constraining the codes to be realizable by a specific family of hash functions (say, linear), means the loss E(Z) will be larger than for free codes. How difficult is it for a hash function to produce the free codes? Fig. 3 plots the loss function for the free codes, the two-step codes from cut, and the codes from MACcut, for both linear and kernel hash functions in the same experiment as in fig. 2. It is clear that the free codes have a very low loss E(Z), which is far from what a kernel function can produce, and even farther from what a linear function can produce. Both of these are relatively smooth functions that cannot represent the presumably complex structure of the free codes. This could be improved by using a very flexible hash function (e.g. using a kernel function with many centers), which could better approximate the free codes, but 1) a very flexible function would likely not generalize well, and 2) we require fast hash functions for fast retrieval anyway. Given our linear or kernel hash functions, what the two-step cut optimization does is fit the hash function directly to the free codes. This is not guaranteed to find the best hash function in terms of the original problem (1), and indeed it produces a pretty suboptimal function. In contrast, MAC gradually optimizes both the codes and the hash function so they eventually match, and finds a better hash function for the original problem (although it is still not guaranteed to find the globally optimal function of problem (1), which is NP-complete).\nFig. 4 illustrates this conceptually. It shows the space of all possible binary codes, the contours of E(Z) (green) and the set of codes that can be produced by (say) linear hash functions h (gray), which is the feasible set {Z \u2208 {\u22121,+1}b\u00d7N : Z = h(X) for linear h}. The two-step codes \u201cproject\u201d the free codes onto the feasible set, but these are not the codes for the optimal hash function h."}, {"heading": "4.3 Runtime", "text": "The runtime per iteration for our 10 000-point training sets with b = 48 bits and \u03ba+ = 100 and \u03ba\u2212 = 500 neighbors in a laptop is 2\u2019 for both MACcut and MACquad. They stop after 10\u201320 iterations. Each iteration is comparable to a single cut or quad run, since the Z step dominates the computation. The iterations after the first one are faster because they are warm-started."}, {"heading": "4.4 Comparison with binary hashing methods", "text": "Fig. 5 shows results on CIFAR and Infinite MNIST. We create affinities ynm for all methods using the dataset labels as before, with \u03ba+ = 100 similar neighbors and \u03ba\u2212 = 500 dissimilar neighbors. We compare MACquad and MACcut with Two-Step Hashing (quad) (Lin et al., 2013), FastHash (cut) (Lin et al., 2014), Hashing with Kernels (KSH) (Liu et al., 2012), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) and Self-Taught Hashing (STH) (Zhang et al., 2010). MACquad, MACcut, quad and cut all use the KSH loss function (3). The results show that MACcut (and MACquad) generally outperform all other methods, often by a large margin, in nearly all situations (dataset, number of bits, size of retrieved set). In particular, MACcut and MACquad are the only ones to beat ITQ, as long as one uses sufficiently many bits."}, {"heading": "5 Discussion", "text": "Two-step approaches vs the MAC algorithm for affinity-based loss functions The two-step approach of Two-Step Hashing (Lin et al., 2013) and FastHash (Lin et al., 2014) is a significant advance in finding good codes for binary hashing, but it also causes a maladjustment between the codes and the hash function, since the codes were learned without knowledge of what hash function would use them. Ignoring the interaction between the loss and the hash function limits the quality of the results. For example, a linear hash function will have a harder time than a nonlinear one at learning such codes. In our algorithm, this tradeoff is enforced gradually (as \u00b5 increases) in the Z step as a regularization term (eq. (5)): it finds the best codes according to the loss function, but makes sure they are close to being realizable by the current hash function. Our experiments demonstrate that significant, consistent gains are achieved in both the loss function value and the precision/recall in image retrieval over the two-step approach.\nA similar, well-known situation arises in feature selection for classification (Kohavi and John, 1998). The best combination of classifier and features will result from jointly minimizing the classification error with respect to both classifier and features (the \u201cwrapper\u201d approach), rather than first selecting features according to some criterion and then using them to learn a particular classifier (the \u201cfilter\u201d approach). From this point\nof view, the two-step approaches of (Lin et al., 2013, 2014) are filter approaches that first optimize the loss function over the codes Z (equivalently, optimize LP with \u00b5 = 0) and then fit the hash function h to those codes. Any such filter approach is then equivalent to optimizing LP over (Z,h) for \u00b5 \u2192 0\n+. The method of auxiliary coordinates algorithmically decouples (within each iteration) the two elements that make up a binary hashing model: the hash function and the loss function. Both elements act in combination to produce a function that maps input patterns to binary codes so that they represent neighborhood in input space, but they play distinct roles. The hash function role is to map input patterns to binary codes. The loss function role is to assign binary codes to input patterns in order to preserve neighborhood relations, regardless of how easy it is for a mapping to produce such binary codes. By itself, the loss function would produce a nonparametric hash function for the training set with the form of a table of (image,code) pairs. However, the hash function and the loss function cannot act independently, because the objective function depends on both. The optimal combination of hash and loss is difficult to obtain, because of the nonlinear and discrete nature of the objective. Several previous optimization attempts for binary hashing first find codes that optimize the loss, and then fit a hash function to them, thus imposing a strict, suboptimal separation between loss function and hash function. In MAC, both elements are decoupled within each iteration, while still optimizing the correct objective: the step over the hash function does not involve the loss, and the step over the codes does not involve the hash function, but both are iterated. The connection between both steps occurs through the auxiliary coordinates, which are the binary codes themselves. The penalty regularizes the loss so that its optimal codes are progressively closer to what a hash function from the given class (e.g. linear) can achieve.\nWhat is the best type of hash function to use? The answer to this is not unique, as it depends on application-specific factors: quality of the codes produced (to retrieve the correct images), time to compute the codes on high-dimensional data (since, after all, the reason to use binary hashing is to speed up retrieval), ease of implementation within a given hardware architecture and software libraries, etc. Our MAC framework facilitates considerably this choice, because training different types of hash functions simply involves reusing an existing classification algorithm within the h step, with no changes to the Z step.\nIn terms of runtime, the resulting MAC algorithm is not much slower than the two-step approach; it is comparable to iterating the latter a few times. Besides, since all iterations except the first are warm-started, the average cost of one iteration is lower than for the two-step approach.\nFinally, note that the method of auxiliary coordinates can be used also to learn an out-of-sample mapping for a continuous embedding (Carreira-Perpin\u0303a\u0301n and Vladymyrov, 2015), such as the elastic embedding (Carreira-Perpin\u0303a\u0301n, 2010) or t-SNE (van der Maaten and Hinton, 2008)\u2014rather than to learn hash functions for a discrete embedding, as is our case in binary hashing. The resulting MAC algorithm optimizes\nover the out-of-sample mapping and the auxiliary coordinates (which are the data points\u2019 low-dimensional projections), by alternating two steps. One step optimizes the out-of-sample mapping that projects highdimensional points to the continuous, latent space, given the auxiliary coordinates Z. This is a regression problem, while in binary hashing this is a classification problem (per hash function). The other step optimizes the auxiliary coordinates Z given the mapping and is a regularized continuous embedding problem. Both steps can be solved using existing algorithms. In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpin\u0303a\u0301n, 2010; Vladymyrov and Carreira-Perpin\u0303a\u0301n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpin\u0303a\u0301n, 2014). In binary hashing the Z step is a combinatorial optimization and, at present, far more challenging to solve. However, with continuous embeddings one must drive the penalty parameter \u00b5 to infinity for the constraints to be satisfied and so the solution follows a continuous path over \u00b5 \u2208 R, while with binary hashing the solution follows a discretized, piecewise path which terminates at a finite value of \u00b5.\nBinary autoencoder vs affinity-based loss, trained with MAC The method of auxiliary coordinates has also been applied in the context of binary hashing to a different objective function, the binary autoencoder (BA) (Carreira-Perpin\u0303a\u0301n and Raziperchikolaei, 2015):\nEBA(h, f) =\nN \u2211\nn=1\n\u2016xn \u2212 f(h(xn))\u2016 2\n(11)\nwhere h is the hash function, or encoder (which outputs binary values), and f is a decoder. (ITQ, Gong et al., 2013, can be seen as a suboptimal way to optimize this.) As with the affinity-based loss function, the MAC algorithm alternates between fitting the hash function (and the decoder) given the codes, and optimizing over the codes. However, in the binary autoencoder the optimization over the codes decouples over every data point (since the objective function involves one term per data point). This has an important computational advantage in the Z step: rather than having to solve one large optimization problem {z1, . . . , zN} over Nb binary variables, it has to solve N small optimization problems {z1}, . . . , {zN} each over b variables, which is much faster and easier to solve (since b is relatively small in practice), and to parallelize. Also, the BA objective does not require any neighborhood information (e.g. the affinity between pairs of neighbors) and scales linearly with the dataset. Computing these affinity values, or even finding pairs of neighbors in the first place, is computationally costly. For these reasons, the BA can scale to training on larger datasets than affinity-based loss functions.\nThe BA objective function does have the disadvantage of being less directly related to the goals that are desirable from an information retrieval point of view, such as precision and recall. Neighborhood relations are only indirectly preserved by autoencoders (Carreira-Perpin\u0303a\u0301n and Raziperchikolaei, 2015), whose direct aim is to reconstruct its inputs and thus to learn the data manifold (imperfectly, because of the binary projection layer). Affinity-based loss functions of the form (1) allow the user to specify more complex neighborhood relations, for example based on class labels, which may significantly differ from the actual distances in image feature space. Still, finding more efficient and scalable optimization methods for binary embeddings (in the Z step of the MAC algorithm), that are able to handle larger numbers of training and neighbor points, would improve the quality of the loss function. This is an important topic of future research."}, {"heading": "6 Conclusion", "text": "We have proposed a general framework for optimizing binary hashing using affinity-based loss functions. It improves over previous, two-step approaches based on learning binary codes first and then learning the hash function. Instead, it optimizes jointly over the binary codes and the hash function in alternation, so that the binary codes eventually match the hash function, resulting in a better local optimum of the affinitybased loss. This was possible by introducing auxiliary variables that conditionally decouple the codes from the hash function, and gradually enforcing the corresponding constraints. Our framework makes it easy to design an optimization algorithm for a new choice of loss function or hash function: one simply reuses existing software that optimizes each in isolation. The resulting algorithm is not much slower than the suboptimal two-step approach\u2014it is comparable to iterating the latter a few times\u2014and well worth the improvement in precision/recall.\nThe step over the hash function is essentially a solved problem if using a classifier, since this can be learned in an accurate and scalable way using machine learning techniques. The most difficult and timeconsuming part in our approach is the optimization over the binary codes, which is NP-complete and involves many binary variables and terms in the objective. Although some techniques exist (Lin et al., 2013, 2014) that produce practical results, designing algorithms that reliably find good local optima and scale to large training sets is an important topic of future research.\nAnother direction for future work involves learning more sophisticated hash functions that go beyond mapping image features onto output binary codes using simple classifiers such as SVMs. This is possible because the optimization over the hash function parameters is confined to the h step and takes the form of a supervised classification problem, so we can apply an array of techniques from machine learning and computer vision. For example, it may be possible to learn image features that work better with hashing than standard features such as SIFT, or to learn transformations of the input to which the binary codes should be invariant, such as translation, rotation or alignment."}, {"heading": "A Additional experiments", "text": "A.1 Unsupervised dataset\nAlthough affinity-based hashing is intended to work with supervised datasets, it can also be used with unsupervised ones, and our MAC approach applies just as well. We use the SIFT1M dataset (Je\u0301gou et al., 2011), which contains N = 1 000 000 training high-resolution color images and 10 000 test images, each represented by D = 128 SIFT features. The experiments and conclusions are generally the same as with supervised datasets, with small differences in the settings of the experiments. In order to construct an affinity-based objective function, we define neighbors as follows. For each point in the training set we use the \u03ba+ = 100 nearest neighbors as positive (similar) neighbors, and \u03ba\u2212 = 500 points chosen randomly among the remaining points as negative (dissimilar) neighbors. We report precision and precision/recall for the test set queries using as ground truth (set of true neighbors in original space) the K nearest neighbors in unsupervised datasets, and all the training points with the same label in supervised datasets.\nFig. 7 shows results using KSH and eSLPH loss functions, respectively, with different sizes of retrieved neighbor sets and using 8 to 32 bits. As with the supervised datasets, it is clear that the MAC algorithm finds better optima and that MACcut is generally better than MACquad. Fig. 8 shows one case, using \u03ba+ = 50, \u03ba\u2212 = 1 000 and K = 10 000 (1% of the base set), where quad outperforms cut and correspondingly MACquad outperforms MACcut, although both MAC results are very close, particularly in precision and recall.\nFig. 10 shows results comparing with binary hashing methods. All methods are trained on a subset of 5 000 points. We consider two types of methods. In the first type, we create pseudolabels for each point and then apply supervised methods as in CIFAR (in particular, cut/quad andMACcut/MACquad, using the KSH loss function). The pseudolabels ynm for each training point xn are obtained by declaring as similar points its \u03ba+ = 100 true nearest neighbors and as dissimilar points a random subset of \u03ba\u2212 = 500 points among the remaining points. In the second type, we use purely unsupervised methods (not based on similar/dissimilar affinities): thresholded PCA (tPCA), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpin\u0303a\u0301n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al., 2009), AnchorGraph Hashing (AGH) (Liu et al., 2011), and Spherical Hashing (SPH) (Heo et al., 2012). The results are again in general agreement with the conclusions in the main paper.\nComparison using code utilization Fig. 12 shows the results (for all methods on SIFT1M) in effective number of bits beff. This is a measure of code utilization of a hash function introduced by Carreira-Perpin\u0303a\u0301n and Raziperchikolaei (2015), defined as the entropy of the code distribution. That is, given the N codes z1, . . . , zN \u2208 {0, 1}\nb for the training set, we consider them as samples of a distribution over the 2b possible codes. The entropy of this distribution, measured in bits, is between 0 (when all N codes are equal) and min(b, log2 N) (when all N codes are distributed as uniformly as possible). We do the same for the test set. Although code utilization correlates to some extent with precision/recall when ranking different methods, a large beff does not guarantee a good hash function, and indeed, tPCA (which usually achieves a low precision compared to the state-of-theart) typically achieves the largest beff; see the discussion in Carreira-Perpin\u0303a\u0301n and Raziperchikolaei (2015).\nHowever, a large beff does indicate a better use of the available codes (and fewer collisions if N < 2 b), and beff has the advantage over precision/recall that it does not depend on any user parameters (such as ground truth size or retrieved set size), so we can compare all binary hashing methods with a single number beff (for a given number of bits b). It is particularly useful to compare methods that are optimizing the same objective function. With this in mind, we can compare MACcut with cut and MACquad with quad because these pairs of methods optimize the same objective function."}, {"heading": "Acknowledgments", "text": "Work supported by NSF award IIS\u20131423515. We thank Ming-Hsuan Yang, Yi-Hsuan Tsai and Mehdi Alizadeh (UC Merced) for useful discussions."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Comm. ACM,", "citeRegEx": "Andoni and Indyk.,? \\Q2008\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2008}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Computing geodesics and minimal surfaces via graph cuts", "author": ["Y. Boykov", "V. Kolmogorov"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Boykov and Kolmogorov.,? \\Q2003\\E", "shortCiteRegEx": "Boykov and Kolmogorov.", "year": 2003}, {"title": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision", "author": ["Y. Boykov", "V. Kolmogorov"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov and Kolmogorov.,? \\Q2004\\E", "shortCiteRegEx": "Boykov and Kolmogorov.", "year": 2004}, {"title": "The elastic embedding algorithm for dimensionality reduction", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n.,? \\Q2010\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n.", "year": 2010}, {"title": "Hashing with binary autoencoders", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "R. Raziperchikolaei"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.,? \\Q2015\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.", "year": 2015}, {"title": "A fast, universal algorithm to learn parametric nonlinear embeddings", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "M. Vladymyrov"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Vladymyrov.,? \\Q2015\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Vladymyrov.", "year": 2015}, {"title": "Distributed optimization of deeply nested systems", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "W. Wang"], "venue": "Proc. of the 17th Int. Conf. Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.", "year": 2014}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Graph cuts for supervised binary coding", "author": ["T. Ge", "K. He", "J. Sun"], "venue": "In Proc. 13th European Conf. Computer Vision (ECCV\u201914),", "citeRegEx": "Ge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2014}, {"title": "Iterative quantization: A Procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Learning binary hash codes for large-scale image search", "author": ["K. Grauman", "R. Fergus"], "venue": "Machine Learning for Computer Vision,", "citeRegEx": "Grauman and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Grauman and Fergus.", "year": 2013}, {"title": "Spherical hashing", "author": ["J.-P. Heo", "Y. Lee", "J. He", "S.-F. Chang", "S.-E. Yoon"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Heo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Heo et al\\.", "year": 2012}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "J\u00e9gou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "J\u00e9gou et al\\.", "year": 2011}, {"title": "The wrapper approach", "author": ["R. Kohavi", "G.H. John"], "venue": "Feature Extraction, Construction and Selection. A Data Mining Perspective. Springer-Verlag,", "citeRegEx": "Kohavi and John.,? \\Q1998\\E", "shortCiteRegEx": "Kohavi and John.", "year": 1998}, {"title": "What energy functions can be minimized via graph cuts", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kolmogorov and Zabih.,? \\Q2003\\E", "shortCiteRegEx": "Kolmogorov and Zabih.", "year": 2003}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Dept. of Computer Science, University of Toronto, Apr", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kulis and Darrell.,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell.", "year": 2009}, {"title": "Kernelized locality-sensitive hashing", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kulis and Grauman.,? \\Q2012\\E", "shortCiteRegEx": "Kulis and Grauman.", "year": 2012}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "D. Suter", "A. van den Hengel"], "venue": "In Proc. 14th Int. Conf. Computer Vision (ICCV\u201913),", "citeRegEx": "Lin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In Proc. of the 2014 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Training invariant support vector machines using selective sampling", "author": ["G. Loosli", "S. Canu", "L. Bottou"], "venue": "Large Scale Kernel Machines, Neural Information Processing Series,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Computer Vision,", "citeRegEx": "Lowe.,? \\Q2004\\E", "shortCiteRegEx": "Lowe.", "year": 2004}, {"title": "Numerical Optimization. Springer Series in Operations Research and Financial Engineering", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D. Fleet"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Norouzi and Fleet.,? \\Q2011\\E", "shortCiteRegEx": "Norouzi and Fleet.", "year": 2011}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. Computer Vision,", "citeRegEx": "Oliva and Torralba.,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba.", "year": 2001}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science,", "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Nearest-Neighbor Methods in Learning and Vision. Neural Information Processing Series", "author": ["G. Shakhnarovich", "P. Indyk", "T. Darrell", "editors"], "venue": null, "citeRegEx": "Shakhnarovich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2006}, {"title": "LDAHash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A.M. Bronstein", "M.M. Bronstein", "P. Fua"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Strecha et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Strecha et al\\.", "year": 2012}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "J. Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Partial-Hessian strategies for fast learning of nonlinear embeddings", "author": ["M. Vladymyrov", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.,? \\Q2012\\E", "shortCiteRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.", "year": 2012}, {"title": "Linear-time training of nonlinear low-dimensional embeddings", "author": ["M. Vladymyrov", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 17th Int. Conf. Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.,? \\Q2014\\E", "shortCiteRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.", "year": 2014}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}, {"title": "Scalable optimization for neighbor embedding for visualization", "author": ["Z. Yang", "J. Peltonen", "S. Kaski"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Yu and Shi.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Shi.", "year": 2003}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. of the 33rd ACM Conf. Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Algorithm 778: L-BFGS-B: FORTRAN subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Trans. Mathematical Software,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 29, "context": "Computationally, this essentially involves defining a high-dimensional feature space where each relevant image is represented by a vector, and then finding the closest points (nearest neighbors) to the vector for the query image, according to a suitable distance (Shakhnarovich et al., 2006).", "startOffset": 263, "endOffset": 291}, {"referenceID": 24, "context": "For example, one can use features such as SIFT (Lowe, 2004) or GIST (Oliva and Torralba, 2001) and the Euclidean distance for this purpose.", "startOffset": 47, "endOffset": 59}, {"referenceID": 27, "context": "For example, one can use features such as SIFT (Lowe, 2004) or GIST (Oliva and Torralba, 2001) and the Euclidean distance for this purpose.", "startOffset": 68, "endOffset": 94}, {"referenceID": 11, "context": "In practice, this is approximated, and a successful way to do this is binary hashing (Grauman and Fergus, 2013).", "startOffset": 85, "endOffset": 111}, {"referenceID": 11, "context": "over the weights of a linear SVM), and L(\u00b7) is a loss function that compares the codes for two images (often through their Hamming distance \u2016h(xn)\u2212 h(xm)\u2016) with the ground-truth value ynm that measures the affinity in the original space between the two images xn and xm (distance, similarity or other measure of neighborhood; Grauman and Fergus, 2013).", "startOffset": 270, "endOffset": 351}, {"referenceID": 1, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al.", "startOffset": 151, "endOffset": 176}, {"referenceID": 28, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al.", "startOffset": 206, "endOffset": 229}, {"referenceID": 4, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al.", "startOffset": 274, "endOffset": 299}, {"referenceID": 22, "context": "Examples of these objective functions (described below) include models developed for dimension reduction, be they spectral such as Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) or Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 463, "endOffset": 481}, {"referenceID": 17, "context": ", 2012), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) or Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 34, "context": ", 2012), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) or Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al., 2012).", "startOffset": 139, "endOffset": 158}, {"referenceID": 25, "context": "This would be guaranteed to converge to an optimum under mild conditions (for example, Wolfe conditions on the line search), which would be global if the objective is convex and local otherwise (Nocedal and Wright, 2006).", "startOffset": 194, "endOffset": 220}, {"referenceID": 35, "context": "Binarizing the codes has been done in different ways, from simply rounding them to {\u22121,+1} using zero as threshold (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011, 2012), to optimally finding a threshold (Liu et al.", "startOffset": 115, "endOffset": 179}, {"referenceID": 38, "context": "Binarizing the codes has been done in different ways, from simply rounding them to {\u22121,+1} using zero as threshold (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011, 2012), to optimally finding a threshold (Liu et al.", "startOffset": 115, "endOffset": 179}, {"referenceID": 21, "context": ", 2011, 2012), to optimally finding a threshold (Liu et al., 2011; Strecha et al., 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al.", "startOffset": 48, "endOffset": 88}, {"referenceID": 30, "context": ", 2011, 2012), to optimally finding a threshold (Liu et al., 2011; Strecha et al., 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al.", "startOffset": 48, "endOffset": 88}, {"referenceID": 37, "context": ", 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al., 2013).", "startOffset": 85, "endOffset": 122}, {"referenceID": 10, "context": ", 2012), to rotating the continuous codes so that thresholding introduces less error (Yu and Shi, 2003; Gong et al., 2013).", "startOffset": 85, "endOffset": 122}, {"referenceID": 0, "context": "Related work Although one can construct hash functions without training data (Andoni and Indyk, 2008; Kulis and Grauman, 2012), we focus on methods that learn the hash function given a training set, since they perform better, and our emphasis is in optimization.", "startOffset": 77, "endOffset": 126}, {"referenceID": 18, "context": "Related work Although one can construct hash functions without training data (Andoni and Indyk, 2008; Kulis and Grauman, 2012), we focus on methods that learn the hash function given a training set, since they perform better, and our emphasis is in optimization.", "startOffset": 77, "endOffset": 126}, {"referenceID": 22, "context": "These create an affinity matrix for a subset of training points based on their distances (unsupervised) or labels (supervised) and combine it with a loss function (Liu et al., 2012; Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Lin et al., 2013, 2014).", "startOffset": 163, "endOffset": 255}, {"referenceID": 17, "context": "These create an affinity matrix for a subset of training points based on their distances (unsupervised) or labels (supervised) and combine it with a loss function (Liu et al., 2012; Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Lin et al., 2013, 2014).", "startOffset": 163, "endOffset": 255}, {"referenceID": 26, "context": "These create an affinity matrix for a subset of training points based on their distances (unsupervised) or labels (supervised) and combine it with a loss function (Liu et al., 2012; Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Lin et al., 2013, 2014).", "startOffset": 163, "endOffset": 255}, {"referenceID": 17, "context": "For example, Binary Reconstructive Embeddings (Kulis and Darrell, 2009) use alternating optimization over the weights of the hash functions.", "startOffset": 46, "endOffset": 71}, {"referenceID": 22, "context": "Supervised Hashing with Kernels (Liu et al., 2012) learns hash functions sequentially by considering the difference between the inner product of the codes and the corresponding element of the affinity matrix.", "startOffset": 32, "endOffset": 50}, {"referenceID": 35, "context": "The codes can be found by relaxing the problem and binarizing its solution (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al.", "startOffset": 75, "endOffset": 133}, {"referenceID": 38, "context": "The codes can be found by relaxing the problem and binarizing its solution (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al.", "startOffset": 75, "endOffset": 133}, {"referenceID": 21, "context": "The codes can be found by relaxing the problem and binarizing its solution (Weiss et al., 2009; Zhang et al., 2010; Liu et al., 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al.", "startOffset": 75, "endOffset": 133}, {"referenceID": 9, "context": ", 2011), or by approximately solving for the binary codes using some form of alternating optimization (possibly combined with GraphCut), as in two-step hashing (Lin et al., 2013, 2014; Ge et al., 2014), or by using relaxation in other ways (Liu et al.", "startOffset": 160, "endOffset": 201}, {"referenceID": 22, "context": ", 2014), or by using relaxation in other ways (Liu et al., 2012; Norouzi and Fleet, 2011).", "startOffset": 46, "endOffset": 89}, {"referenceID": 26, "context": ", 2014), or by using relaxation in other ways (Liu et al., 2012; Norouzi and Fleet, 2011).", "startOffset": 46, "endOffset": 89}, {"referenceID": 4, "context": "A representative example is the elastic embedding (Carreira-Perpi\u00f1\u00e1n, 2010), where L(zn, zm; ynm) has the form: y nm \u2016zn \u2212 zm\u2016 2 + \u03bby nm exp (\u2212\u2016zn \u2212 zm\u2016 2 ), \u03bb > 0 (2) where the first term tries to project true neighbors (having y nm > 0) close together, while the second repels all non-neighbors\u2019 projections (having y nm > 0) from each other.", "startOffset": 50, "endOffset": 75}, {"referenceID": 1, "context": "Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000) result from replacing the second term above with a constraint that fixes the scale of Z, which results in an eigenproblem rather than a nonlinear optimization, but also produces more distorted embeddings.", "startOffset": 20, "endOffset": 45}, {"referenceID": 28, "context": "Laplacian Eigenmaps (Belkin and Niyogi, 2003) and Locally Linear Embedding (Roweis and Saul, 2000) result from replacing the second term above with a constraint that fixes the scale of Z, which results in an eigenproblem rather than a nonlinear optimization, but also produces more distorted embeddings.", "startOffset": 75, "endOffset": 98}, {"referenceID": 4, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 32, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 36, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 33, "context": "Optimizing nonlinear embeddings is quite challenging, but much progress has been done recently (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 95, "endOffset": 241}, {"referenceID": 35, "context": "Although these models were developed to produce continuous projections, they have been successfully used for binary hashing too by truncating their codes (Weiss et al., 2009; Zhang et al., 2010) or using the two-step approach of (Lin et al.", "startOffset": 154, "endOffset": 194}, {"referenceID": 38, "context": "Although these models were developed to produce continuous projections, they have been successfully used for binary hashing too by truncating their codes (Weiss et al., 2009; Zhang et al., 2010) or using the two-step approach of (Lin et al.", "startOffset": 154, "endOffset": 194}, {"referenceID": 17, "context": "Binary Reconstructive Embeddings (Kulis and Darrell, 2009) uses ( b \u2016zn \u2212 zm\u2016 2 \u2212 ynm) 2 where ynm = 1 2 \u2016xn \u2212 xm\u2016 2 .", "startOffset": 33, "endOffset": 58}, {"referenceID": 34, "context": "The exponential variant of SPLH (Wang et al., 2012) proposed by Lin et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 15, "context": "Binary Reconstructive Embeddings (Kulis and Darrell, 2009) uses ( b \u2016zn \u2212 zm\u2016 2 \u2212 ynm) 2 where ynm = 1 2 \u2016xn \u2212 xm\u2016 2 . The exponential variant of SPLH (Wang et al., 2012) proposed by Lin et al. (2013) (which we call eSPLH) uses exp(\u2212 1 b ynmz T nzn).", "startOffset": 34, "endOffset": 201}, {"referenceID": 25, "context": "Second, we solve the constrained problem using a penalty method, such as the quadratic-penalty or augmented Lagrangian (Nocedal and Wright, 2006).", "startOffset": 119, "endOffset": 145}, {"referenceID": 19, "context": "Here, we try two different approaches (Lin et al., 2013, 2014) with some modifications. \u2022 Optimize the hash function h given binary codes Z. This reduces to training b binary classifiers using X as inputs and Z as targets. This is very similar to the two-step (TSH) approach of Lin et al. (2013), except that the latter learns the codes Z in isolation, rather than given the current hash function, so iterating the two-step approach would change nothing, and it does not optimize the loss L.", "startOffset": 39, "endOffset": 296}, {"referenceID": 19, "context": "Fortunately, some recent works have proposed practical approaches for this problem based on alternating optimization: a quadratic surrogate method (Lin et al., 2013), and a GraphCut method (Lin et al.", "startOffset": 147, "endOffset": 165}, {"referenceID": 20, "context": ", 2013), and a GraphCut method (Lin et al., 2014).", "startOffset": 31, "endOffset": 49}, {"referenceID": 19, "context": "Solution using a quadratic surrogate method (Lin et al., 2013) This is based on the fact that any loss function that depends on the Hamming distance of two binary variables can be equivalently written as a quadratic function of those two binary variables (Lin et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 19, "context": ", 2013) This is based on the fact that any loss function that depends on the Hamming distance of two binary variables can be equivalently written as a quadratic function of those two binary variables (Lin et al., 2013).", "startOffset": 200, "endOffset": 218}, {"referenceID": 19, "context": "Fortunately, some recent works have proposed practical approaches for this problem based on alternating optimization: a quadratic surrogate method (Lin et al., 2013), and a GraphCut method (Lin et al., 2014). In both cases, this would correspond to the first step in the two-step hashing of Lin et al. (2013). In both the quadratic surrogate and the GraphCut method, the starting point is to apply alternating optimization over the ith bit of all points given the remaining bits are fixed for all points (for i = 1, .", "startOffset": 148, "endOffset": 309}, {"referenceID": 19, "context": "Lin et al. (2013) show that l(z1, z2) can be replaced by a binary quadratic function l(z1, z2) = 1 2z1z2 ( l \u2212 l )", "startOffset": 0, "endOffset": 18}, {"referenceID": 39, "context": "which we solve using L-BFGS-B (Zhu et al., 1997).", "startOffset": 30, "endOffset": 48}, {"referenceID": 20, "context": "Solution using a GraphCut algorithm (Lin et al., 2014) To optimize over the ith bit (given all the other bits are fixed), we have to minimize eq.", "startOffset": 36, "endOffset": 54}, {"referenceID": 15, "context": "We can apply the GraphCut algorithm (Boykov and Kolmogorov, 2003, 2004; Kolmogorov and Zabih, 2003), as proposed by the FastHash algorithm of Lin et al.", "startOffset": 36, "endOffset": 99}, {"referenceID": 2, "context": "We can apply the GraphCut algorithm (Boykov and Kolmogorov, 2003, 2004; Kolmogorov and Zabih, 2003), as proposed by the FastHash algorithm of Lin et al. (2014). This proceeds as follows.", "startOffset": 37, "endOffset": 160}, {"referenceID": 22, "context": "We use the KSH (3) (Liu et al., 2012) and eSPLH (Wang et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 34, "context": ", 2012) and eSPLH (Wang et al., 2012) loss functions.", "startOffset": 18, "endOffset": 37}, {"referenceID": 8, "context": "As hash functions (for each bit), we use linear SVMs (trained with LIBLINEAR; Fan et al., 2008) and kernel SVMs We use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes.", "startOffset": 53, "endOffset": 95}, {"referenceID": 16, "context": ", 2008) and kernel SVMs We use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes.", "startOffset": 125, "endOffset": 143}, {"referenceID": 27, "context": "We use D = 320 GIST features (Oliva and Torralba, 2001) from each image.", "startOffset": 29, "endOffset": 55}, {"referenceID": 23, "context": "(2) Infinite MNIST (Loosli et al., 2007).", "startOffset": 19, "endOffset": 40}, {"referenceID": 17, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 26, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 22, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 19, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Norouzi and Fleet, 2011; Liu et al., 2012; Lin et al., 2013).", "startOffset": 131, "endOffset": 217}, {"referenceID": 19, "context": "We compare 4 ways of optimizing the loss function: quad (Lin et al., 2013), cut (Lin et al.", "startOffset": 56, "endOffset": 74}, {"referenceID": 20, "context": ", 2013), cut (Lin et al., 2014), MACquad and MACcut.", "startOffset": 13, "endOffset": 31}, {"referenceID": 19, "context": "We compare MACquad and MACcut with Two-Step Hashing (quad) (Lin et al., 2013), FastHash (cut) (Lin et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 20, "context": ", 2013), FastHash (cut) (Lin et al., 2014), Hashing with Kernels (KSH) (Liu et al.", "startOffset": 24, "endOffset": 42}, {"referenceID": 22, "context": ", 2014), Hashing with Kernels (KSH) (Liu et al., 2012), Iterative Quantization (ITQ) (Gong et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 10, "context": ", 2012), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) and Self-Taught Hashing (STH) (Zhang et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 17, "context": ", 2013), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) and Self-Taught Hashing (STH) (Zhang et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 38, "context": ", 2013), Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009) and Self-Taught Hashing (STH) (Zhang et al., 2010).", "startOffset": 104, "endOffset": 124}, {"referenceID": 19, "context": "5 Discussion Two-step approaches vs the MAC algorithm for affinity-based loss functions The two-step approach of Two-Step Hashing (Lin et al., 2013) and FastHash (Lin et al.", "startOffset": 130, "endOffset": 148}, {"referenceID": 20, "context": ", 2013) and FastHash (Lin et al., 2014) is a significant advance in finding good codes for binary hashing, but it also causes a maladjustment between the codes and the hash function, since the codes were learned without knowledge of what hash function would use them.", "startOffset": 21, "endOffset": 39}, {"referenceID": 14, "context": "A similar, well-known situation arises in feature selection for classification (Kohavi and John, 1998).", "startOffset": 79, "endOffset": 102}, {"referenceID": 6, "context": "Finally, note that the method of auxiliary coordinates can be used also to learn an out-of-sample mapping for a continuous embedding (Carreira-Perpi\u00f1\u00e1n and Vladymyrov, 2015), such as the elastic embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008)\u2014rather than to learn hash functions for a discrete embedding, as is our case in binary hashing.", "startOffset": 133, "endOffset": 173}, {"referenceID": 4, "context": "Finally, note that the method of auxiliary coordinates can be used also to learn an out-of-sample mapping for a continuous embedding (Carreira-Perpi\u00f1\u00e1n and Vladymyrov, 2015), such as the elastic embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008)\u2014rather than to learn hash functions for a discrete embedding, as is our case in binary hashing.", "startOffset": 205, "endOffset": 230}, {"referenceID": 4, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 32, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 36, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 33, "context": "In particular, solving the Z step can be done efficiently with large datasets by using N -body methods and efficient optimization techniques (Carreira-Perpi\u00f1\u00e1n, 2010; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2012; van der Maaten, 2013; Yang et al., 2013; Vladymyrov and Carreira-Perpi\u00f1\u00e1n, 2014).", "startOffset": 141, "endOffset": 287}, {"referenceID": 5, "context": "Binary autoencoder vs affinity-based loss, trained with MAC The method of auxiliary coordinates has also been applied in the context of binary hashing to a different objective function, the binary autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015): EBA(h, f) = N", "startOffset": 214, "endOffset": 260}, {"referenceID": 5, "context": "Neighborhood relations are only indirectly preserved by autoencoders (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), whose direct aim is to reconstruct its inputs and thus to learn the data manifold (imperfectly, because of the binary projection layer).", "startOffset": 69, "endOffset": 115}, {"referenceID": 13, "context": "We use the SIFT1M dataset (J\u00e9gou et al., 2011), which contains N = 1 000 000 training high-resolution color images and 10 000 test images, each represented by D = 128 SIFT features.", "startOffset": 26, "endOffset": 46}, {"referenceID": 10, "context": "In the second type, we use purely unsupervised methods (not based on similar/dissimilar affinities): thresholded PCA (tPCA), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al.", "startOffset": 154, "endOffset": 173}, {"referenceID": 5, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al.", "startOffset": 33, "endOffset": 79}, {"referenceID": 35, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al., 2009), AnchorGraph Hashing (AGH) (Liu et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 21, "context": ", 2009), AnchorGraph Hashing (AGH) (Liu et al., 2011), and Spherical Hashing (SPH) (Heo et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 12, "context": ", 2011), and Spherical Hashing (SPH) (Heo et al., 2012).", "startOffset": 37, "endOffset": 55}, {"referenceID": 4, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al., 2009), AnchorGraph Hashing (AGH) (Liu et al., 2011), and Spherical Hashing (SPH) (Heo et al., 2012). The results are again in general agreement with the conclusions in the main paper. Comparison using code utilization Fig. 12 shows the results (for all methods on SIFT1M) in effective number of bits beff. This is a measure of code utilization of a hash function introduced by Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei (2015), defined as the entropy of the code distribution.", "startOffset": 34, "endOffset": 541}, {"referenceID": 4, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), Spectral Hashing (SH) (Weiss et al., 2009), AnchorGraph Hashing (AGH) (Liu et al., 2011), and Spherical Hashing (SPH) (Heo et al., 2012). The results are again in general agreement with the conclusions in the main paper. Comparison using code utilization Fig. 12 shows the results (for all methods on SIFT1M) in effective number of bits beff. This is a measure of code utilization of a hash function introduced by Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei (2015), defined as the entropy of the code distribution. That is, given the N codes z1, . . . , zN \u2208 {0, 1} b for the training set, we consider them as samples of a distribution over the 2 possible codes. The entropy of this distribution, measured in bits, is between 0 (when all N codes are equal) and min(b, log2 N) (when all N codes are distributed as uniformly as possible). We do the same for the test set. Although code utilization correlates to some extent with precision/recall when ranking different methods, a large beff does not guarantee a good hash function, and indeed, tPCA (which usually achieves a low precision compared to the state-of-theart) typically achieves the largest beff; see the discussion in Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei (2015). 15", "startOffset": 34, "endOffset": 1301}], "year": 2016, "abstractText": "In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.", "creator": "LaTeX with hyperref package"}}}