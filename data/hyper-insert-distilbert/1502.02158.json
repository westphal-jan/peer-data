{"id": "1502.02158", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2015", "title": "Learning Parametric-Output HMMs with Two Aliased States", "abstract": "in various applications while involving hidden generalized markov entropy models ( hmms ), some of the least hidden states are deeply aliased, having identical output distributions. the minimality, parameter identifiability behavior and nonlinear learnability of such aliased dependency hmms have inevitably been long point standing problems, with only potentially partial solutions obviously provided thus far. in processing this 2009 paper we critically focus on parametric - output hmms, galaxies whose output _ distributions always come from only a parametric family, and networks that have exactly two aliased states. for exploring this derivation class, we present a computational complete modeling characterization of their minimality hierarchy and identifiability. furthermore, for extending a large family of parametric complex output mp distributions, we uniquely derive computationally advanced efficient and complex statistically dynamic consistent network algorithms looking to positively detect the constant presence dependence of aliasing and learn clearly the dominant aliased mode hmm matching transition and emission cycle parameters. we presently illustrate as our intricate theoretical network analysis simulations by several simulations.", "histories": [["v1", "Sat, 7 Feb 2015 16:21:28 GMT  (128kb,D)", "http://arxiv.org/abs/1502.02158v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roi weiss", "boaz nadler"], "accepted": true, "id": "1502.02158"}, "pdf": {"name": "1502.02158.pdf", "metadata": {"source": "CRF", "title": "Learning Parametric-Output HMMs with Two Aliased States", "authors": ["Roi Weiss", "Boaz Nadler"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "HMMs are a fundamental tool in the analysis of time series. A discrete time HMM with n hidden states is characterized by a n \u00d7 n transition matrix, and by the emissions probabilities from these n states. In several applications, the HMMs, or more general processes such as partially observable Markov decision processes, are aliased, with some states having identical output distributions. In modeling of ion channel gating, for example, one postulates that at any given time an ion channel can be in only one of a finite number of hidden states, some of which are open and conducting current while others are closed, see e.g. Fredkin & Rice [1992]. Given electric current measurements, one fits an aliased HMM and infers important biological insight regarding the gating process. Other examples appear in the fields of reinforcement learning [Chrisman, 1992, McCallum, 1995, Brafman & Shani, 2004, Shani et al., 2005] and robot navigation [Jefferies & Yeap, 2008, Zatuchna & Bagnall, 2009]. In the latter case, aliasing occurs whenever different spatial locations appear (statistically) identical to the robot, given its limited sensing devices. As a last example, HMMs with several silent states that do not emit any output [Leggetter & Woodland, 1994, Stanke & Waack, 2003, Brejova et al., 2007], can also be viewed as aliased.\nar X\niv :1\n50 2.\n02 15\n8v 1\n[ cs\n.L G\n] 7\nKey notions related to the study of HMMs, be them aliased or not, are their minimality, identifiability and learnability:\nMinimality. Is there an HMM with fewer states that induces the same distribution over all output sequences?\nIdentifiability. Does the distribution over all output sequences uniquely determines the HMM\u2019s parameters, up to a permutation of its hidden states?\nLearning. Given a long output sequence from a minimal and identifiable HMM, efficiently learn its parameters.\nFor non-aliased HMMs, these notions have been intensively studied and by now are relatively well understood, see for example Petrie [1969], Finesso [1990], Leroux [1992], Allman et al. [2009] and Cappe\u0301 et al. [2005]. The most common approach to learn the parameters of an HMM is via the Baum-Welch iterative algorithm [Baum et al., 1970]. Recently, tensor decompositions and other computationally efficient spectral methods have been developed to learn non-aliased HMMs [Hsu et al., 2009, Siddiqi et al., 2010, Anandkumar et al., 2012, Kontorovich et al., 2013].\nIn contrast, the minimality, identifiability and learnability of aliased HMMs have been long standing problems, with only partial solutions provided thus far. For example, Blackwell & Koopmans [1957] characterized the identifiability of a specific aliased HMM with 4 states. The identifiability of deterministic output HMMs, where each hidden state outputs a deterministic symbol, was partially resolved by Ito et al. [1992]. To the best of our knowledge, precise characterizations of the minimality, identifiability and learnability of probabilistic output HMMs with aliased states are still open problems. In particular, the recently developed tensor and spectral methods mentioned above, explicitly require the HMM to be non-aliasing, and are not directly applicable to learning aliased HMMs.\nMain results. In this paper we study the minimality, identifiability and learnability of parametric-output HMMs that have exactly two aliased states. This is the simplest possible class of aliased HMMs, and as shown below, even its analysis is far from trivial. Our main contributions are as follows: First, we provide a complete characterization of their minimality and identifiability, deriving necessary and sufficient conditions for each of these notions to hold. Our identifiability conditions are easy to check for any given 2-aliased HMM, and extend those of Ito et al. [1992] for the case of deterministic outputs. Second, we solve the problem of learning a possibly aliased HMM, from a long sequence of its outputs. To this end, we first derive an algorithm to detect whether an observed output sequence corresponds to a non-aliased HMM or to an aliased one. In the former case, the HMM can be learned by various methods, such as Anandkumar et al. [2012], Kontorovich et al. [2013]. In the latter case we show how the aliased states can be identified and present a method to recover the HMM parameters. Our approach is applicable to any family of output distributions whose mixtures are efficiently learnable. Examples include high dimensional Gaussians and products distributions, see Feldman et al. [2008], Belkin & Sinha [2010], Anandkumar et al. [2012] and references\ntherein. After learning the output mixture parameters, our moment-based algorithm requires only a single pass over the data. It is possibly the first statistically consistent and computationally efficient scheme to handle 2-aliased HMMs. While our approach may be extended to more complicated aliasing, such cases are beyond the scope of this paper. We conclude with some simulations illustrating the performance of our suggested algorithms."}, {"heading": "2 Definitions & Problem Setup", "text": "Notation. We denote by In the n\u00d7n identity matrix and 1n = (1, . . . , 1)T \u2208 Rn. For v \u2208 Rn, diag(v) is the n\u00d7 n diagonal matrix with entries vi on its diagonal. The i-th row and column of a matrix A \u2208 Rn\u00d7n are denoted by A[i,\u00b7] and A[\u00b7,i], respectively. We also denote [n] = {1, 2, . . . , n}. For a discrete random variable X we abbreviate P (x) for Pr(X = x). For a second random variable Z, the quantity P (z |x) denotes either Pr(Z = z |X = x), or the conditional density p(Z = z|X = x), depending on whether Z is discrete or continuous.\nHidden Markov Models. Consider a discrete-time HMM with n hidden states {1, . . . , n}, whose output alphabet Y is either discrete or continuous. Let F\u03b8 = {f\u03b8 : Y \u2192 R | \u03b8 \u2208 \u0398} be a family of parametric probability density functions where \u0398 is a suitable parameter space. A parametric-output HMM is defined by a tupleH = (A,\u03b8,\u03c00) where A is the n\u00d7 n transition matrix of the hidden states\nAij = Pr(Xt+1 = i |Xt = j) = P (i | j),\n\u03c00 \u2208 Rn is the distribution of the initial state, and the vector of parameters \u03b8 = (\u03b81, \u03b82, . . . , \u03b8n) \u2208 \u0398n determines the n probability density functions (f\u03b81 , f\u03b82 , . . . , f\u03b8n).\nThe output sequence of the HMM is generated as follows. First, an unobserved Markov sequence of hidden states x = (xt)T\u22121t=0 is generated according to the distribution\nP (x) = \u03c00x0 T\u22121\u220f t=1 P (xt |xt\u22121).\nThe output yt \u2208 Y at time t depends only on the hidden state xt via P (yt |xt) \u2261 f\u03b8xt (yt). Hence the conditional distribution of an output sequence y = (yt) T\u22121 t=0 is\nP (y |x) = T\u22121\u220f t=0 P (yt |xt) = T\u22121\u220f t=0 f\u03b8xt (yt).\nWe denote by PH,k : Yk \u2192 R the joint distribution of the first k consecutive outputs of the HMM H . For y = (y0, . . . , yk\u22121) \u2208 Yk this distribution is given by\nPH,k(y) = \u2211 x\u2208[n]k P (y |x)P (x).\nFurther we denote by PH = {PH,k | k \u2265 1} the set of all these distributions.\n2-Aliased HMMs. For an HMM H with output parameters \u03b8 = (\u03b81, \u03b82, . . . , \u03b8n) \u2208 \u0398n we say that states i and j are aliased if \u03b8i = \u03b8j . In this paper we consider the special case where H has exactly two aliased states, denoted as 2A-HMM. Without loss of generality, we assume the aliased states are the two last ones, n\u22121 and n. Thus,\n\u03b81 6= \u03b82 6= \u00b7 \u00b7 \u00b7 6= \u03b8n\u22122 6= \u03b8n\u22121 and \u03b8n\u22121 = \u03b8n.\nWe denote the vector of the n\u22121 unique output parameters ofH by \u03b8\u0304 = (\u03b81, \u03b82, . . . , \u03b8n\u22122, \u03b8n\u22121) \u2208 \u0398n\u22121. For future use, we define the aliased kernel K\u0304 \u2208 R(n\u22121)\u00d7(n\u22121) as the matrix of inner products between the n\u22121 different f\u03b8i \u2019s,\nK\u0304ij \u2261 \u3008f\u03b8i , f\u03b8j \u3009 = \u222b Y f\u03b8i(y)f\u03b8j (y)dy, i, j \u2208 [n\u22121]. (1)\nAssumptions. As in previous works [Leroux, 1992, Kontorovich et al., 2013], we make the following standard assumptions:\n(A1) The parametric family F\u03b8 of the output distributions is linearly independent of order n: for any distinct {\u03b8i}ni=1, \u2211n i=1 aif\u03b8i \u2261 0 iff ai = 0 for all i \u2208 [n].\n(A2) The transition matrix A is ergodic and its unique stationary distribution \u03c0 = (\u03c01, \u03c02, . . . , \u03c0n) is positive.\nNote that assumption (A1) implies that the parametric familyF\u03b8 is identifiable, namely f\u03b8 = f\u03b8\u2032 iff \u03b8 = \u03b8\u2032. It also implies that the kernel matrix K\u0304 of (1) is full rank n\u22121."}, {"heading": "3 Decomposing the transition matrix A", "text": "The main tool in our analysis is a novel decomposition of the 2A-HMM\u2019s transition matrix into its non-aliased and aliased parts. As shown in Lemma 1 below, the aliased part consists of three rank-one matrices, that correspond to the dynamics of exit from, entrance to, and within the two aliased states. This decomposition is used to derive the conditions for minimality and identifiability (Section 4), and plays a key role in learning the HMM (Section 5).\nTo this end, we introduce a pseudo-state n\u0304, combining the two aliased states n\u22121 and n. We define\n\u03c0n\u0304 = \u03c0n\u22121 + \u03c0n and \u03b2 = \u03c0n\u22121/\u03c0n\u0304. (2)\nWe shall make extensive use of the following two matrices:\nB =  0 0 In\u22122 ... ... 0 0\n0 . . . 0 1 1\n \u2208 R(n\u22121)\u00d7n,\nC\u03b2 =  0 In\u22122 ... 0\n0 . . . 0 \u03b2 0 . . . 0 1\u2212\u03b2  \u2208 Rn\u00d7(n\u22121). As explained below, these matrices can be viewed as projection and lifting operators, mapping between non-aliased and aliased quantities.\nNon-aliased part. The non-aliased part ofA is a stochastic matrix A\u0304 \u2208 R(n\u22121)\u00d7(n\u22121), obtained by merging the two aliased states n\u22121 and n into the pseudo-state n\u0304. Its entries are given by\nA\u0304=  P (1 | n\u0304) A[1:n\u22122]\u00d7[1:n\u22122] ... P (n\u22122 | n\u0304)\nP (n\u0304 | 1) . . . P (n\u0304 |n\u22122) P (n\u0304 | n\u0304)  , (3) where the transition probabilities into the pseudo-state are\nP (n\u0304 | j) = P (n\u22121 | j) + P (n | j), \u2200j \u2208 [n],\nthe transition probabilities out of the pseudo-state are defined with respect to the stationary distribution by\nP (i | n\u0304) = \u03b2P (i |n\u22121) + (1\u2212\u03b2)P (i |n), \u2200i \u2208 [n]\nand lastly, the probability to stay in the pseudo-state is\nP (n\u0304 | n\u0304) = \u03b2P (n\u0304 |n\u22121) + (1\u2212\u03b2)P (n\u0304 |n).\nIt is easy to check that the unique stationary distribution of A\u0304 is \u03c0\u0304 = (\u03c01, \u03c02, . . . , \u03c0n\u22122, \u03c0n\u0304) \u2208 Rn\u22121. Finally, note that A\u0304 = BAC\u03b2 , \u03c0\u0304 = B\u03c0 and \u03c0 = C\u03b2\u03c0\u0304, justifying the lifting and projection interpretation of the matrices B,C\u03b2 .\nAliased part. Next we present some key quantities that distinguish between the two aliased states. Let suppin = {j \u2208 [n] |P (n\u0304 | j) > 0} be the set of states that can move into either one of the aliased states. We define\n\u03b1j =\n{ P (n\u22121 | j) P (n\u0304 | j) j \u2208 suppin\n0 otherwise, (4)\nas the relative probability of moving from state j to state n\u22121, conditional on moving to either n\u22121 or n. We define the two vectors \u03b4out, \u03b4in \u2208 Rn\u22121 as follows: \u2200i, j \u2208 [n\u22121],\n\u03b4outi = { P (i |n\u22121)\u2212 P (i |n) i < n\u2212 1 P (n\u0304 |n\u22121)\u2212 P (n\u0304 |n) i = n\u2212 1\n(5)\n\u03b4inj =  (\u03b1j\u2212\u03b2)P (n\u0304 | j) j < n\u22121\n\u03b2(\u03b1n\u22121\u2212\u03b2)P (n\u0304 |n\u22121) + (1\u2212\u03b2)(\u03b1n\u2212\u03b2)P (n\u0304 |n) j = n\u22121.\n(6)\nIn other words, \u03b4out captures the differences in the transition probabilities out of the aliased states. In particular, if \u03b4out = 0 then starting from either one of the two aliased states, the Markov chain evolution is identical. Intuitively such an HMM is not minimal, as its two aliased states can be lumped together, see Theorem 1 below.\nSimilarly, \u03b4in compares the relative probabilities into the aliased states \u03b1j , to the stationary relative probability \u03b2 = \u03c0n\u22121/\u03c0n\u0304. This quantity also plays a role in the minimality of the HMM.\nLastly, for our decomposition, we define the scalar\n\u03ba = (\u03b1n\u22121 \u2212 \u03b2)P (n\u0304 |n\u22121)\u2212 (\u03b1n \u2212 \u03b2)P (n\u0304 |n). (7)\nDecomposing A. The following lemma provides a decomposition of the transition matrix in terms of A\u0304, \u03b4out, \u03b4in, \u03ba and \u03b2 (all omitted proofs are given in the Appendix).\nLemma 1. The transition matrix A of a 2A-HMM can be written as\nA = C\u03b2A\u0304B + C\u03b2\u03b4 outcT\u03b2 + b(\u03b4 in)TB + \u03ba bcT\u03b2 , (8)\nwhere c\u03b2T = (0, . . . , 0, 1\u2212\u03b2,\u2212\u03b2) \u2208 Rn and b = (0, . . . , 0, 1,\u22121)T \u2208 Rn.\nIn (8), the first term is the merged transition matrix A\u0304 \u2208 R(n\u22121)\u00d7(n\u22121) lifted back into Rn\u00d7n. This term captures all of the non-aliased transitions. The second matrix is zero except in the last two columns, accounting for the exit transition probabilities from the two aliased states. Similarly, the third matrix is zero except in the last two rows, differentiating the entry probabilities. The fourth term is non-zero only on the lower right 2 \u00d7 2 block involving the aliased states n\u22121, n. This term corresponds to the internal dynamics between them. Note that each of the last three terms is at most a rank-1 matrix, which together can be seen as a perturbation due to the presence of aliasing.\nIn section 5 we shall see that given a long output sequence from the HMM, the presence of aliasing can be detected and the quantities A\u0304, \u03b4out, \u03b4in, \u03ba and \u03b2 can all be estimated from it. An estimate for A is then obtained via Eq. (8)."}, {"heading": "4 Minimality and Identifiability", "text": "Two HMMs H and H \u2032 are said to be equivalent if their observed output sequences are statistically indistinguishable, namely PH\u2032 = PH . Similarly, an HMM H is minimal if\nthere is no equivalent HMM with fewer number of states. Note that if H is non-aliased then Assumptions (A1-A2) readily imply that it is also minimal [Leroux, 1992]. In this section we present necessary and sufficient conditions for a 2A-HMM to be minimal, and for two minimal 2A-HMMs to be equivalent. Finally, we derive necessary and sufficient conditions for a minimal 2A-HMM to be identifiable."}, {"heading": "4.1 Minimality", "text": "The minimality of an HMM is closely related to the notion of lumpability: can hidden states be merged without affecting the distribution PH [Fredkin & Rice, 1986, White et al., 2000, Huang et al., 2014]. Obviously, an HMM is minimal iff no subset of hidden states can be merged. In the following theorem we give precise conditions for the minimality of a 2A-HMM.\nTheorem 1. Let H be a 2A-HMM satisfying Assumptions (A1-A2) whose initial state X0 is distributed according to \u03c00 = (\u03c001 , \u03c0 0 2 , . . . , \u03b2 0\u03c00n\u0304, (1\u2212\u03b20)\u03c00n\u0304). Then,\n(i) If \u03c00n\u0304 6= 0 and \u03b20 6= \u03b2 then H is minimal iff \u03b4 out 6= 0.\n(ii) If \u03c00n\u0304 = 0 or \u03b2 0 = \u03b2 then H is minimal iff both \u03b4out 6= 0 and \u03b4in 6= 0.\nBy Theorem 1, a necessary condition for minimality of a 2A-HMM is that the two aliased states have different exit probabilities, \u03b4out 6= 0. Namely, there exists a nonaliased state i \u2208 [n\u22122] such that P (i |n\u22121) 6= P (i |n). Otherwise the two aliased states can be merged. If the 2A-HMM is started from its stationary distribution, then an additional necessary condition is \u03b4in 6= 0. This last condition implies that there is a non-aliased state j \u2208 suppin \\{n\u22121, n} with relative entrance probability \u03b1j 6= \u03b2."}, {"heading": "4.2 Identifiability", "text": "Recall that an HMMH is (strictly) identifiable ifPH uniquely determines the transition matrix A and the output parameters \u03b8, up to a permutation of the hidden states. We establish the conditions for identifiability of a 2A-HMM in two steps. First we derive a novel geometric characterization of the set of all minimal HMMs that are equivalent to H , up to a permutation of the hidden states (Theorem 2). Then we give necessary and sufficient conditions for H to be identifiable, namely for this set to be the singleton set, consisting of onlyH itself (Appendix C). In the process, we provide a simple procedure (Algorithm 1) to determine whether a given minimal 2A-HMM is identifiable or not.\nEquivalence between minimal 2A-HMMs. Necessary and sufficient conditions for the equivalence of two minimal HMMs were studied in several works [Finesso, 1990, Ito et al., 1992, Vanluyten et al., 2008]. We now provide analogous conditions for parametric output 2A-HMMs. Toward this end, we define the following 2-dimensional\nfamily of matrices S(\u03c4n\u22121, \u03c4n) \u2208 Rn\u00d7n given by\nS(\u03c4n\u22121, \u03c4n) =  0 0 In\u22122 ... ... 0 0\n0 . . . 0 \u03c4n\u22121 \u03c4n 0 . . . 0 1\u2212\u03c4n\u22121 1\u2212\u03c4n\n .\nClearly, for \u03c4n\u22121 6= \u03c4n, S is invertible. As in Ito et al. [1992], consider then the following similarity transformation of the transition matrix A,\nAH(\u03c4n\u22121, \u03c4n) = S(\u03c4n\u22121, \u03c4n) \u22121AS(\u03c4n\u22121, \u03c4n). (9)\nIt is easy to verify that 1TnAH = 1 T n. However, AH is not necessarily stochastic, as depending on \u03c4n\u22121, \u03c4n it may have negative entries. The following lemma resolves the equivalence of 2A-HMMs, in terms of this transformation.\nLemma 2. LetH = (A,\u03b8,\u03c0) be a minimal 2A-HMM satisfying Assumptions (A1-A2). Then a minimal HMM H \u2032 = (A\u2032,\u03b8\u2032,\u03c0\u2032) with n\u2032 states is equivalent to H iff n\u2032 = n and there exists a permutation matrix \u03a0 \u2208 Rn\u00d7n and \u03c4n\u22121 > \u03c4n such that \u03b8\u2032 = \u03a0\u03b8 and\n\u03c0\u2032 = \u03a0S(\u03c4n\u22121, \u03c4n) \u22121\u03c0 A\u2032 = \u03a0AH(\u03c4n\u22121, \u03c4n) \u03a0 \u22121 \u2265 0.\nThe feasible region. By Lemma 2, any matrix AH(\u03c4n\u22121, \u03c4n) whose entries are all non-negative yields an HMM equivalent to the original one. We thus define the feasible region of H by\n\u0393H = {(\u03c4n\u22121, \u03c4n) \u2208 R2 |AH(\u03c4n\u22121, \u03c4n) \u2265 0, \u03c4n\u22121>\u03c4n}. (10)\nBy definition, \u0393H is non-empty, since (\u03c4n\u22121, \u03c4n) = (1, 0) recover the original matrixA. As we show below, \u0393H is determined by three simpler regions \u03931,\u03932,\u03933 \u2282 R2. The region \u03931 ensures that all entries of AH are non-negative except possibly in the lower right 2\u00d7 2 block corresponding to the two aliased states. The regions \u03932 and \u03933 ensure non-negativity of the latter, depending on whether the aliased relative probabilities of (4) satisfy \u03b1n\u22121 \u2265 \u03b1n or \u03b1n\u22121 < \u03b1n, respectively. For ease of exposition we assume as a convention that P (n\u0304 |n\u22121) \u2265 P (n\u0304 |n).\nTheorem 2. Let H be a minimal 2A-HMM satisfying Assumptions (A1-A2). There exist (\u03c4minn\u22121 , \u03c4 min n ), (\u03c4 max n\u22121 , \u03c4 max n ), (\u03c4\n\u2212, \u03c4+) \u2208 R2, and convex monotonic decreasing functions f, g : R\u2192 R such that\n\u0393H = { \u03931 \u2229 \u03932 \u03b1n\u22121 \u2265 \u03b1n \u03931 \u2229 \u03933 \u03b1n\u22121 < \u03b1n,\nwhere the regions \u03931,\u03932,\u03933 \u2282 R2 are given by\n\u03931 = [\u03c4 min n\u22121 , \u03c4 max n\u22121 ]\u00d7 [\u03c4maxn , \u03c4minn ]\n\u03932 = [\u03c4 +,\u221e)\u00d7 [\u03c4\u2212, \u03c4+]\n\u03933 = {(\u03c4n\u22121, \u03c4n) \u2208 \u03931 | f(\u03c4n\u22121) \u2264 \u03c4n \u2264 g(\u03c4n\u22121) }."}, {"heading": "In addition, the set \u0393H is connected.", "text": "The feasible regions in the two possible cases (\u03b1n\u22121 \u2265 \u03b1n or \u03b1n\u22121 < \u03b1n) are depicted in Appendix C, Fig.6.\nStrict Identifiability. By Lemma 2, for strict identifiability of H , \u0393H should be the singleton set \u0393H = {(1, 0)}. Due to lack of space, sufficient and necessary conditions for this to hold, as well as a corresponding simple procedure to determine whether a 2A-HMM is identifiable, are given in Appendix C.2.\nRemark. While beyond the scope of this paper, we note that instead of strict identifiability of a given HMM, several works studied a different concept of generic identifiability [Allman et al., 2009], proving that under mild conditions the class of HMMs is generically identifiable. In contrast, if we restrict ourselves to the class of 2A-HMMs, then our Theorem 2 implies that this class is generically non-identifiable. The reason is that by Theorem 2, for any 2A-HMM whose matrix A has all its entries positive, there are an infinite number of equivalent 2A-HMMs, implying non-identifiability."}, {"heading": "5 Learning a 2A-HMM", "text": "Let (Yt)T\u22121t=0 be an output sequence generated by a parametric-output HMM that satisfies Assumptions (A1-A2) and initialized with its stationary distribution, X0 \u223c \u03c0. We assume the HMM is either non-aliasing, with n\u22121 states, or 2-aliasing with n states. We further assume that the HMM is minimal and identifiable, as otherwise its parameters cannot be uniquely determined.\nIn this section we study the problems of detecting whether the HMM is aliasing and recovering its output parameters \u03b8 and transition matrix A, all in terms of (Yt)T\u22121t=0 .\nHigh level description. The proposed learning procedure consists of the following steps (see Fig.1):\n(i) Determine the number of output components n\u22121 and estimate the n\u22121 unique output distribution parameters \u03b8\u0304 and the projected stationary distribution \u03c0\u0304.\n(ii) Detect if the HMM is 2-aliasing.\n(iii) In case of a non-aliased HMM, estimate the (n\u22121) \u00d7 (n\u22121) transition matrix A\u0304, as for example in Kontorovich et al. [2013] or Anandkumar et al. [2012].\n(iv) In case of a 2-aliased HMM, identify the component \u03b8n\u22121 corresponding to the two aliased states, and estimate the n\u00d7 n transition matrix A.\nWe now describe in detail each of these steps. As far as we know, our learning procedure is the first to consistently learn a 2A-HMM in a computationally efficient way. In particular, the solutions for problems (ii) and (iv) are new.\nEstimating the output distribution parameters. As the HMM is stationary, each observable Yt is a random realization from the following parametric mixture model,\nY \u223c n\u22121\u2211 i=1 \u03c0\u0304if\u03b8\u0304i(y). (11)\nHence, the number of unique output components n \u2212 1, the corresponding output parameters \u03b8\u0304 and the projected stationary distribution \u03c0\u0304 can be estimated by fitting a mixture model (11) to the observed output sequence (Yt)T\u22121t=0 .\nConsistent methods to determine the number of components in a mixture are well known in the literature [Titterington et al., 1985]. The estimation of \u03b8\u0304 and \u03c0\u0304 is typically done by either applying an EM algorithm, or any recently developed spectral method [Dasgupta, 1999, Achlioptas & McSherry, 2005, Anandkumar et al., 2012]. As our focus is on the aliasing aspects of the HMM, in what follows we assume that the number of unique output components n\u22121, the output parameters \u03b8\u0304 and the projected stationary distribution \u03c0\u0304 are exactly known. As in Kontorovich et al. [2013], it is possible to show that our method is robust to small perturbations in these quantities (not presented)."}, {"heading": "5.1 Moments", "text": "To solve problems (ii), (iii) and (iv) above, we first introduce the moment-based quantities we shall make use of. Given \u03b8\u0304 and \u03c0\u0304 or estimates of them, for any i, j \u2208 [n\u22121], we define the second order moments with time lag t by\nM(t)ij = E[f\u03b8i(Y0)f\u03b8j (Yt)], t \u2208 {1, 2, 3}. (12)\nThe consecutive in time third order moments are defined by\nG(c)ij = E[f\u03b8i(Y0)f\u03b8c(Y1)f\u03b8j (Y2)], \u2200c \u2208 [n\u22121]. (13)\nWe also define the lifted kernel, K = BTK\u0304B \u2208 Rn\u00d7n. One can easily verify that for a 2A-HMM,\nM(t) = K\u0304BAtC\u03b2 diag(\u03c0\u0304)K\u0304 (14) G(c) = K\u0304BAdiag(K[\u00b7,c])AC\u03b2 diag(\u03c0\u0304)K\u0304. (15)\nNext we define the kernel free moments M (t), G(c) \u2208 R(n\u22121)\u00d7(n\u22121) as follows:\nM (t) = K\u0304\u22121M(t)K\u0304\u22121 diag(\u03c0\u0304)\u22121 (16) G(c) = K\u0304\u22121G(c)K\u0304\u22121 diag(\u03c0\u0304)\u22121. (17)\nNote that by Assumption (A1), the kernel K\u0304 is full rank and thus K\u0304\u22121 exists. Similarly, by (A2) \u03c0\u0304 > 0, so diag(\u03c0\u0304)\u22121 also exists. Thus, (16,17) are well defined.\nLet R(2), R(3), F (c) \u2208 R(n\u22121)\u00d7(n\u22121) be given by\nR(2) = M (2) \u2212 (M (1))2 (18) R(3) = M (3)\u2212M (2)M (1)\u2212M (1)M (2) + (M (1))3 (19) F (c) = G(c) \u2212M (1) diag(K\u0304[\u00b7,c])M (1). (20)\nThe following key lemma relates the moments (18, 19, 20) to the decomposition (8) of the transition matrix A.\nLemma 3. Let H be a minimal 2A-HMM with aliased states n\u22121 and n. Let A\u0304, \u03b4out, \u03b4in and \u03ba be defined in (3,5,6,7) respectively. Then the following relations hold:\nM (1) = A\u0304 (21) R(2) = \u03b4out(\u03b4in)T (22) R(3) = \u03baR(2) (23) F (c) = K\u0304n\u22121,cR (2), \u2200c \u2208 [n\u22121]. (24)\nIn the following, these relations will be used to detect aliasing, identify the aliased states and recover the aliased transition matrix A.\nEmpirical moments. In practice, the unknown moments (12,13) are estimated from the output sequence (Yt)T\u22121t=0 by\nM\u0302(t)ij = 1\nT \u2212 t T\u2212t\u22121\u2211 l=0 f\u03b8i(Yl)f\u03b8j (Yl+t),\nG\u0302(c)ij = 1\nT \u2212 2 T\u22123\u2211 l=0 f\u03b8i(Yl)f\u03b8c(Yl+1)f\u03b8j (Yl+2).\nWith K\u0304, \u03c0\u0304 known, the corresponding empirical kernel free moments are given by\nM\u0302 (t) = K\u0304\u22121M\u0302(t)K\u0304\u22121 diag(\u03c0\u0304)\u22121 (25) G\u0302(c) = K\u0304\u22121G\u0302(c)K\u0304\u22121 diag(\u03c0\u0304)\u22121. (26)\nThe empirical estimates for (18,19,20) similarly follow. To analyze the error between the empirical and population quantities, we make the following additional assumption: (A3) The output distributions are bounded. Namely there exists L > 0 such that \u2200i \u2208 [n] and \u2200y \u2208 Y , f\u03b8i(y) \u2264 L.\nLemma 4. Let (Yt)T\u22121t=0 be an output sequence generated by an HMM satisfying Assumptions (A1-A3). Then, as T \u2192 \u221e, for any t \u2208 {1, 2, 3} and c \u2208 [n\u22121], all error terms M\u0302 (t) \u2212M (t), R\u0302(t) \u2212R(t) and F\u0302 (c) \u2212 F (c) are OP (T\u2212 1 2 ).\nIn fact, due to strong mixing, all of the above quantities are asymptotically normally distributed [Bradley, 2005]."}, {"heading": "5.2 Detection of aliasing", "text": "We now proceed to detect if the HMM is aliased (step (ii) in Fig.1). We pose this as a hypothesis testing problem:\nH0 : H is non-aliased with n\u22121 states vs. H1 : H is 2-aliased with n states.\nWe begin with the following simple observation:\nLemma 5. LetH be a minimal non-aliased HMM with n\u22121 states, satisfying Assumptions (A1-A3). Then R(2) = 0.\nIn contrast, if H is 2-aliasing then according to (22) we have R(2) = \u03b4out(\u03b4in)T. In addition, since the HMM is assumed to be minimal and started from the stationary distribution, Theorem 1 implies that both \u03b4out 6= 0 and \u03b4in 6= 0. Thus R(2) is exactly a rank-1 matrix, which we write as\nR(2) = \u03c3uvT with \u2016u\u20162 = \u2016v\u20162 = 1, \u03c3 > 0, (27)\nwhere \u03c3 is the unique non-zero singular value of R(2). Hence, our hypothesis testing problem takes the form:\nH0 : R(2) = 0 vs. H1 : R(2) = \u03c3uvT with \u03c3 > 0.\nIn practice, we only have the empirical estimate R\u0302(2). Even if \u03c3 = 0, this matrix is typically full rank with n\u22121 non-zero singular values. Our problem is thus detecting the rank of a matrix from a noisy version of it. There are multiple methods to do so. In this paper, motivated by Kritchman & Nadler [2009], we adopt the largest singular value \u03c3\u03021 of R\u0302(2) as our test statistic. The resulting test is\nif \u03c3\u03021 \u2265 hT returnH1, otherwise returnH0, (28)\nwhere hT is a predefined threshold. By Lemma 4, as T \u2192 \u221e the singular values of R\u0302(2) converge to those of R(2). Thus, as the following lemma shows, with a suitable threshold this test is asymptotically consistent.\nLemma 6. Let H be a minimal HMM satisfying Assumptions (A1-A3) which is either non-aliased or 2-aliased. Then for any 0< < 12 , the test (28) with hT = \u2126(T\n\u2212 12 + ) is consistent: as T \u2192\u221e, with probability one, it will correctly detect whether the HMM is non-aliased or 2-aliased.\nEstimating the non-aliased transition matrix A\u0304. If the HMM was detected as nonaliasing, then its (n\u22121)\u00d7 (n\u22121) transition matrix A\u0304 can be estimated for example by the spectral methods given in Kontorovich et al. [2013] or Anandkumar et al. [2012]. It is shown there, that these methods are (strongly) consistent. Moreover, as T \u2192\u221e,\n\u02c6\u0304A = A\u0304+OP (T \u2212 12 ). (29)"}, {"heading": "5.3 Identifying the aliased component \u03b8n\u22121", "text": "Assuming the HMM was detected as 2-aliasing, our next task, step (iv), is to identify the aliased component. Recall that if the aliased component is \u03b8n\u22121, then by (24)\nF (c) = K\u0304n\u22121,cR (2), \u2200c \u2208 [n\u22121].\nWe thus estimate the index i \u2208 [n\u22121] of the aliased component by solving the following least squares problem:\ni\u0302 = argmin i\u2208[n\u22121] \u2211 c\u2208[n\u22121] \u2225\u2225\u2225F\u0302 (c) \u2212 K\u0304i,cR\u0302(2)\u2225\u2225\u22252 F . (30)\nThe following result shows this method is consistent.\nLemma 7. For a minimal 2A-HMM satisfying Assumptions (A1-A3) with aliased states n\u22121 and n,\nlim T\u2192\u221e\nPr(\u0302i 6= n\u22121) = 0."}, {"heading": "5.4 Learning the aliased transition matrix A", "text": "Given the aliased component, we estimate the n \u00d7 n transition matrix A using the decomposition (8). First, recall that by (22), R(2) = \u03b4out(\u03b4in)T = \u03c3uvT. As singular vectors are determined only up to scaling, we have that\n\u03b4out = \u03b3u and \u03b4in = \u03c3\n\u03b3 v,\nwhere \u03b3 \u2208 R is a yet undetermined constant. Thus, the decomposition (8) of A takes the form:\nA = C\u03b2A\u0304B + \u03b3C\u03b2uc T \u03b2 +\n\u03c3 \u03b3 bvTB + \u03ba bcT\u03b2 . (31)\nGiven that A\u0304, \u03c3,u and v are known from previous steps, we are left to determine the scalars \u03b3, \u03b2 and \u03ba of Eq. (7).\nAs for \u03ba, according to (23) we have R(3) = \u03baR(2). Thus, plugging the empirical versions, \u03ba\u0302 is estimated by\n\u03ba\u0302 = argmin r\u2208R \u2225\u2225\u2225R\u0302(3) \u2212 rR\u0302(2)\u2225\u2225\u22252 F . (32)\nTo determine \u03b3 and \u03b2 we turn to the similarity transformation AH(\u03c4n\u22121, \u03c4n), given in (9). As shown in Section 3, this transformation characterizes all transition matrices equivalent to A. To relate AH to the form of the decomposition (31), we reparametrize \u03c4n\u22121 and \u03c4n as follows:\n\u03b3\u2032 = \u03b3(\u03c4n\u22121 \u2212 \u03c4n), \u03b2\u2032 = \u03b2 \u2212 \u03c4n \u03c4n\u22121 \u2212 \u03c4n .\nReplacing \u03c4n\u22121, \u03c4n with \u03b3\u2032, \u03b2\u2032 we find that AH is given by\nAH = C\u03b2\u2032A\u0304B + \u03b3 \u2032C\u03b2\u2032uc T \u03b2\u2032 +\n\u03c3 \u03b3\u2032 bvTB + \u03ba bcT\u03b2\u2032 . (33)\nNote that putting \u03b3\u2032 = \u03b3 and \u03b2\u2032 = \u03b2 recovers the decomposition (31) for the original transition matrix A.\nNow, since H is assumed identifiable, the constraint AH(\u03c4n\u22121, \u03c4n) \u2265 0 has the unique solution (\u03c4n\u22121, \u03c4n) = (1, 0), or equivalently (\u03b3\u2032, \u03b2\u2032) = (\u03b3, \u03b2). Thus, with exact knowledge of the various moments, only a single pair of values (\u03b3\u2032, \u03b2\u2032) will yield a non-negative matrix (33). This perfectly recovers \u03b3, \u03b2 and the original transition matrix A.\nIn practice we plug into (33) the empirical versions \u02c6\u0304A, \u03ba\u0302, \u03c3\u03021, u\u03021 and v\u03021, where u\u03021, v\u03021 are the left and right singular vectors of R\u0302(2), corresponding to the singular value \u03c3\u03021. As described in Appendix D.5, the values (\u03b3\u0302, \u03b2\u0302) are found by maximizing a simple two dimensional smooth function. The resulting estimate for the aliased transition matrix is\nA\u0302 = C\u03b2\u0302 \u02c6\u0304AB + \u03b3\u0302C\u03b2\u0302u\u03021c T \u03b2\u0302 + \u03c3\u03021 \u03b3\u0302 bv\u0302T1B + \u03ba\u0302 bc T \u03b2\u0302 .\nThe following theorem proves our method is consistent.\nTheorem 3. Let H be a 2A-HMM satisfying assumption (A1-A3) with aliased states n\u22121 and n. Then as T \u2192\u221e,\nA\u0302 = A+ oP (1)."}, {"heading": "6 Numerical simulations", "text": "We present simulation results, illustrating the consistency of our methods for the detection of aliasing, identifying the aliased component and learning the transition matrix\nA. As our focus is on the aliasing, we assume for simplicity that the output parameters \u03b8\u0304 and the projected stationary distributions \u03c0\u0304 are exactly known.\nMotivated by applications in modeling of ion channel gating [Crouzy & Sigworth, 1990, Rosales et al., 2001, Witkoskie & Cao, 2004], we consider the following HMM H with n = 4 hidden states (see Fig.2, left). The output distributions are univariate Gaussians N (\u00b5i, \u03c32i ) . Its matrix A and (f\u03b8i)4i=1 are given by\nA =  0.3 0.25 0.0 0.8 0.6 0.25 0.2 0.0 0.0 0.5 0.1 0.1 0.1 0.0 0.7 0.1  , f\u03b81 = N (3, 1) f\u03b82 = N (6, 1) f\u03b83 = N (0, 1) f\u03b84 = N (0, 1).\nStates 3 and 4 are aliased and by Procedure 1 in Appendix C.3 this 2A-HMM is identifiable. The rank-1 matrix R(2) has a singular value \u03c3 = 0.33. Fig.2 (right) shows its non-aliased version H\u0304 with states 3 and 4 merged.\nTo illustrate the ability of our algorithm to detect aliasing, we generated T outputs from the original aliased HMM and from its non-aliased version H\u0304 . Fig.3 (left) shows the empirical densities (averaged over 1000 independent runs) of the largest singular value of R\u0302(2), for both H and H\u0304 . In Fig.3 (right) we show similar results for a 2A-HMM with \u03c3 = 0.22. When \u03c3 = 0.33, already T = 1000 outputs suffice for essentially perfect detection of aliasing. For the smaller \u03c3 = 0.22, more samples are required.\nFig.4 (left) shows the false alarm and misdetection probability vs. sample size T of the aliasing detection test (28) with threshold hT = 2T\u2212 1 3 . The consistency of our method is evident. Fig.4 (right) shows the probability of misidentifying the aliased component \u03b83\u0304. We considered the same 2A-HMM H but with different means for the Gaussian output distribution of the aliased states, \u00b53\u0304 = {0, 1, 2}. As expected, when f\u03b83\u0304 is closer to the output distribution of the non-aliased state f\u03b81 (with mean \u00b51 = 3), identifying the aliased component is more difficult.\nFinally, to estimate A we considered the following methods: The Baum-Welch algorithm with random initial guess of the HMM parameters (BW); our method of moments with exactly known \u03b8\u0304 (MoM+Exact); BW initialized with the output of our method (BW+MoM+Exact); and BW with exactly known output distributions but random initial guess of the transition matrix (BW+Exact).\nFig.5 (left) shows on a logarithmic scale the mean square error E||A\u0302 \u2212 A||2F vs. sample size T , averaged over 100 independent realizations. Fig.5 (right) shows the running time as a function of T . In these two figures, the number of iterations of the BW was set to 20.\nThese results show that with a random initial guess of the HMM parameters, BW requires far more than 20 iterations to converge. Even with exact knowledge of the output distributions but a random initial guess for the transition matrix, BW still fails to converge after 20 iterations. In contrast, our method yields a relatively accurate estimator in only a fraction of run-time. For an improved accuracy, this estimator can further be used as an initial guess for A in the BW algorithm."}, {"heading": "A Proofs for Section 3 (Decomposing A)", "text": "Proof of Lemma 1. Writing each term in the decomposition (8) explicitly and summing these together we find a match between all entries to those of A.\nAs a representative example let us consider the last entry An,n = P (n |n). The first term gives\n(C\u03b2A\u0304B)[n,n] = (1\u2212 \u03b2)P (n\u0304 |n).\nThe second term gives\n(C\u03b2\u03b4 outcT\u03b2)[n,n] = \u2212\u03b2(1\u2212\u03b2)\u03b4outn\u22121\n= \u2212\u03b2(1\u2212\u03b2)(P (n\u0304 |n\u22121)\u2212P (n\u0304 |n)).\nThe third term,\n(b(\u03b4in)TB)[n,n] = \u2212\u03b4inn\u22121 = \u2212\u03b2(\u03b1n\u22121\u2212\u03b2)P (n\u0304 |n\u22121) \u2212 (1\u2212\u03b2)(\u03b1n\u2212\u03b2)P (n\u0304 |n).\nAnd lastly, the fourth term gives\n(\u03babcT\u03b2)[n,n] = \u03b2(\u03b1n\u22121 \u2212 \u03b2)P (n\u0304 |n\u22121) \u2212\u03b2(\u03b1n \u2212 \u03b2)P (n\u0304 |n).\nPutting P (n\u0304 |n) = P (n\u22121 |n) + P (n |n) and P (n\u0304 |n\u22121) = P (n\u22121 |n\u22121) + P (n |n\u22121), and summing all these four terms we obtain P (n |n) as needed. The other entries of A are obtained similarly."}, {"heading": "B Proofs for Section 4.1 (Minimality)", "text": "Let H = (A,\u03b8,\u03c00) be a 2A-HMM. For any k \u2265 1 the distribution PH,k \u2208 PH can be cast in an explicit matrix form. Let o \u2208 Y . The observable operator T\u03b8(o) \u2208 Rn\u00d7n is defined by\nT\u03b8(o) = diag (f\u03b81(o), f\u03b82(o), . . . , f\u03b8n(o)) .\nLet y = (y0, y1, . . . , yk\u22121) \u2208 Yk be a sequence of k \u2265 1 initial consecutive observations. Then the distribution PH,k(y) is given by Jaeger [2000],\nPH,k(y) = 1 T nT\u03b8(yk\u22121)A . . . AT\u03b8(y1)AT\u03b8(y0)\u03c0 0. (34)\nProof of Theorem 1. Let us first show that \u03b4out 6= 0 is necessary for minimality, namely if \u03b4out = 0 then H is not minimal, regardless of the initial distribution \u03c00. The nonminimality will be shown by explicitly constructing a n\u22121 state HMM equivalent to H . Let us denote the lifting of the merged transition matrix by\nA\u0303 = C\u03b2A\u0304B \u2208 Rn\u00d7n.\nAssume that \u03b4out = 0. We will shortly see that for any \u03c00 and for any k \u2265 2 consecutive observations y = (y0, y1, . . . , yk\u22121) \u2208 Yk we have that\nT\u03b8(yk\u22121)A . . . T\u03b8(y1)AT\u03b8(y0)\u03c0 0 (35)\n\u2212 T\u03b8(yk\u22121)A\u0303 . . . T\u03b8(y1)A\u0303T\u03b8(y0)\u03c00 \u221d b.\nCombining (35) with (34), and the fact that 1Tnb = 0, we have that P(A,\u03b8,\u03c00) = P(A\u0303,\u03b8,\u03c00). Since A\u0303 has identical (n\u22121)-th and n-th columns, and f\u03b8n\u22121 = f\u03b8n we have that P(A\u0303,\u03b8,\u03c00) = P(A\u0304,\u03b8\u0304,\u03c0\u03040). Thus H \u2032 = (A\u0304, \u03b8\u0304, \u03c0\u03040) is an equivalent (n\u22121)-state HMM and H is not minimal, proving the claim. We prove (35) by induction on the sequence length k \u2265 2. First note that since \u03b4out = 0, by Lemma 1 we have that\nA = A\u0303+ b((\u03b4in)TB + \u03bacT\u03b2).\nSince for any y \u2208 Y , T\u03b8(y)b = f\u03b8n\u0304(y)b, we have that\nT\u03b8(y)A\u2212 T\u03b8(y)A\u0303 = f\u03b8n\u0304(y)b((\u03b4 in)TB + \u03bacT\u03b2) \u221d b.\nThis proves the case k = 2. Next, assume (35) holds for all sequences of length at least 2 and smaller than k, namely, for some a \u2208 R\nT\u03b8(yk\u22122)A . . . T\u03b8(y1)AT\u03b8(y0)\u03c0 0\n= ab+ T\u03b8(yk\u22122)A\u0303 . . . T\u03b8(y1)A\u0303T\u03b8(y0)\u03c0 0.\nUsing the fact that Bb = 0 we have T\u03b8(yk\u22121)A\u0303b = 0. Inserting the expansion of A in the l.h.s of (35) we get\nf\u03b8n\u0304(yk\u22121)b ( (\u03b4in)TB + \u03bacT\u03b2 ) \u00d7 ( ab+ A\u0303T\u03b8(yk\u22122) . . . T\u03b8(y1)A\u0303T\u03b8(y0)\u03c0 0 ) .\nSince this last expression is proportional to b we are done.\n(ii) The case \u03c00n\u0304 = 0 or \u03b20 = \u03b2. As we just saw, having \u03b4 out = 0 implies that the HMM is not minimal. We now show that if \u03b4in = 0 then H is not minimal either. By contraposition this will prove the first direction of (ii).\nSo assume that \u03b4in = 0. Lemma 1 implies\nA = A\u0303+ (C\u03b2(\u03b4 out) + \u03bab)cT\u03b2 . (36)\nNow note that for all y \u2208 Y , cT\u03b2T\u03b8(y) = f\u03b8n\u22121(y)cT\u03b2 and since either \u03c00n\u0304 = 0 or \u03b20 = \u03b2 we have that cT\u03b2\u03c0 0 = 0. Thus c\u03b2T\u03b8(y)\u03c00 = 0 and we find that\nT\u03b8(yk\u22121)A . . . AT\u03b8(y1)AT\u03b8(y0)\u03c0 0 (37)\n= T\u03b8(yk\u22121)A . . . AT\u03b8(y1)A\u0303T\u03b8(y0)\u03c0 0. (38)\nNow since cT\u03b2C\u03b2 = 0 we have that for any y \u2208 Y , cT\u03b2T\u03b8(y)A\u0303 = 0 and thus expanding A by (36) we find that for any y \u2208 Y ,\nAT\u03b8(y)A\u0303 = ( A\u0303+ (C\u03b2(\u03b4 out) + \u03bab)cT\u03b2 ) T\u03b8(y)A\u0303 = A\u0303T\u03b8(y)A\u0303.\nThus each A in the right hand side of (37) can be replaced by A\u0303 and we conclude that P(A,\u03b8,\u03c00) = P(A\u0303,\u03b8n,\u03c00). Similarly to the case \u03b4\nout = 0 we have that H \u2032 = (A\u0304, \u03b8\u0304, \u03c0\u03040) is an equivalent (n\u22121)-state HMM and thus H is not minimal.\nIn order to prove the other direction we will show that if H is not minimal then either \u03b4out = 0 or \u03b4in = 0. This is equivalent to the condition \u03b4out\u03b4in T = 0.\nAssuming H is not minimal, there exists an HMM H \u2032 with n\u2032 < n states such that PH\u2032 = PH . Assumptions (A1-A3) readily imply that H \u2032 must have n\u2032 = n\u22121 states and that the unique n\u22121 output components are identical for H and H \u2032. Since PH\u2032 is invariant to permutations, we may assume that \u03b8\u0304\u2032 = \u03b8\u0304 and consequently the kernel matrices in (1) for both H and H \u2032 are equal K\u0304 = K\u0304 \u2032.\nLetA\u2032 \u2208 R(n\u22121)\u00d7(n\u22121) be the transition matrix ofH \u2032 and defineH \u2032\u2032 = (A\u2032\u2032,\u03b8\u2032\u2032, \u03c0\u2032\u2032) as the equivalent n-state HMM to H \u2032 by setting \u03b2\u2032\u2032 = \u03b2, A\u2032\u2032 = C\u03b2A\u2032B, \u03b8\u2032\u2032 = \u03b8\u0304 \u2032 B and \u03c0\u2032\u2032 = C\u03b2\u03c0\u0304\u2032. Note that for H \u2032\u2032, by construction we have \u03b4out \u2032\u2032 (\u03b4in \u2032\u2032 )T = 0.\nNow, by the equivalence of the two models H and H \u2032\u2032, we have that the second order momentsM(2) given in (12) are the same for both. By the fact that K\u0304 \u2032\u2032 = K\u0304, \u03c0\u0304\u2032\u2032 = \u03c0\u0304 and by (22) in Lemma 3 we must have that \u03b4out(\u03b4in) T = \u03b4out \u2032\u2032 (\u03b4in \u2032\u2032 )T. Thus \u03b4out(\u03b4in) T = 0 and the claim is proved.\n(i) The case \u03c00n\u0304 6= 0 and \u03b20 6= \u03b2. We saw above that if H is minimal then \u03b4 out 6= 0. Thus, in order to prove the claim we are left to show that if H is not minimal then \u03b4out = 0.\nSo assume H is not minimal and let H \u2032\u2032 be constructed as above. By way of contradiction assume \u03b4out 6= 0. As we just saw, sinceH is not minimal then \u03b4out(\u03b4in)T = 0. Thus by the assumption \u03b4out 6= 0 we must have \u03b4in = 0. This implies that A is in the form (36). Since PH = PH\u2032\u2032 we have PH,2 = PH\u2032\u2032,2 where:\nPH,2 = 1 T nT\u03b8(y2)AT\u03b8(y1)\u03c0 0 = 1TnT\u03b8(y2) ( A\u0303+ (C\u03b2\u03b4 out + \u03bab)cT\u03b2 ) T\u03b8(y1)\u03c0 0\nPH\u2032\u2032,2 = 1 T nT\u03b8(y2)A \u2032\u2032T\u03b8(y1)\u03c0 0.\nIn addition, by the fact that K\u0304 \u2032\u2032 = K\u0304, \u03c0\u0304\u2032\u2032 = \u03c0\u0304 we must have that M (1) = M \u2032\u2032(1), where M (1) is defined in (16) and M \u2032\u2032(1) is defined similarly with the parameters of H \u2032\u2032 instead of H . By (21) in Lemma 3 we thus have\nM \u2032\u2032 (1) = A\u2032 = A\u0304 = M (1).\nHence A\u2032\u2032 = A\u0303 and PH,2 = PH\u2032\u2032,2 is equivalent to\n1TnT\u03b8(y2) ( C\u03b2\u03b4 out + \u03bab ) cT\u03b2T\u03b8(y1)\u03c0 0 = 0. (39)\nNow, note that \u2200y1, y2 \u2208 Y we have\n1TnT\u03b8(y2)b = 0\ncT\u03b2T\u03b8(y1)\u03c0 0 = (\u03b20 \u2212 \u03b2)\u03c00n\u0304f\u03b8n\u0304(y1) 1TnT\u03b8(y2)C\u03b2 = (f\u03b81(y2), . . . , f\u03b8n\u22121(y2)).\nThus, (39) is given by (\u03b20 \u2212 \u03b2)\u03c00n\u0304f\u03b8n\u22121(y1) ( f\u03b81(y2), . . . , f\u03b8n\u22121(y2) ) \u00b7 \u03b4out = 0.\nSince by assumption (\u03b20 \u2212 \u03b2)\u03c00n\u0304 6= 0 we have \u2200y1, y2 \u2208 Y f\u03b8n\u22121(y1) ( f\u03b81(y2), . . . , f\u03b8n\u22121(y2) ) \u00b7 \u03b4out = 0.\nFor each i \u2208 [n\u22121], multiplying by f\u03b8i(y2) and integrating over y1, y2 \u2208 Y we get\nK\u0304\u03b4out = 0.\nSince K\u0304 is full rank we must have \u03b4out = 0 in contradiction to the assumption \u03b4out 6= 0. This concludes the proof of the Theorem."}, {"heading": "C Proofs for Section 4.2 (Identifiability)", "text": ""}, {"heading": "C.1 Proof of Theorem 2", "text": "Before characterizing \u0393H let us first give some intuition on the role of (\u03c4n\u22121, \u03c4n). Consider the n\u22121 dimensional columns {a\u0304i | i \u2208 [n]} of the matrix BA. These can be\nplotted on the n\u22121 dimensional simplex, as shown in Fig.7 (top), for n = 4 and aliased states {3, 4}. Recall that\nAH(\u03c4n\u22121, \u03c4n) = S(\u03c4n\u22121, \u03c4n) \u22121AS(\u03c4n\u22121, \u03c4n)\nand let {a\u0304H,i | i \u2208 [n]} be the columns of the matrix BAH \u2208 Rn\u22121\u00d7n. Since BS(\u03c4n\u22121, \u03c4n)\u22121 = B we have BAH = BAS(\u03c4n\u22121, \u03c4n). So the non-aliased columns of BAH are unaltered from these of BA, i.e. for all i \u2208 [n\u22122], a\u0304i = a\u0304H,i. The new aliased columns of BAH are\na\u0304H,n\u22121 = a\u0304n + \u03c4n\u22121\u03b4 out\na\u0304H,n = a\u0304n + \u03c4n\u03b4 out.\nThus \u03c4n\u22121 (\u03c4n) determines the position of the vector a\u0304H,n\u22121 (a\u0304H,n) along the ray passing through a\u0304n\u22121 and a\u0304n (dashed line in Fig.7).\nHence a necessary condition forAH to be a valid transition matrix is that a\u0304H,n\u22121 \u2265 0 and a\u0304H,n \u2265 0, and one cannot take \u03c4n\u22121 and \u03c4n arbitrarily. In particular, there are \u03c4maxn\u22121 and \u03c4maxn such that a\u0304H,n\u22121 and a\u0304H,n are as \u201cfar\u201d apart as possible by putting them on\nthe opposite sides of the ray connecting them, such that both sit on the simplex boundary. This is achieved by taking\na\u0304maxH,n\u22121 = a\u0304n + \u03c4 max n\u22121 \u03b4 out\na\u0304maxH,n = a\u0304n + \u03c4 max n \u03b4 out,\nwhere\n\u03c4maxn\u22121 = minj\u2208X\\{n\u22121,n} 1 2 (1+sign(\u03b4 out j ))\u2212(a\u0304n)j\n\u03b4outj \u2265 0\n\u03c4maxn = maxj\u2208X\\{n\u22121,n} 1 2 (1\u2212sign(\u03b4 out j ))\u2212(a\u0304n)j\n\u03b4outj \u2264 0.\n(see Fig.7, bottom). Since we assumed as a convention that \u03c4n\u22121 > \u03c4n we have that any \u03c4n\u22121 \u2264 \u03c4maxn\u22121 and \u03c4n \u2265 \u03c4maxn results in a non negative matrix BAH . Note that BAH \u2265 0 implies AH [1:n\u22122,1:n] \u2265 0.\nNext, consider the new relative probabilities \u03b1H,i as defined by (4) with AH replacing A. One can verify that these satisfy\n\u03b1H,i = \u03b1i \u2212 \u03c4n \u03c4n\u22121 \u2212 \u03c4n , i \u2208 suppin \\{n\u22121, n}.\nObviously, a necessary condition for AH to be a valid transition matrix is that\n0 \u2264 \u03b1H,i = \u03b1i \u2212 \u03c4n \u03c4n\u22121 \u2212 \u03c4n \u2264 1, i \u2208 suppin \\{n\u22121, n}. (40)\nDefine the minimal and maximal relative probabilities of the non-aliased states by\n\u03b1min = min {\u03b1i | i \u2208 suppin \\{n\u22121, n}} \u03b1max = max {\u03b1i | i \u2208 suppin \\{n\u22121, n}}.\nLet \u03b1minH and \u03b1 max H be defined similarly. Taking\n\u03c4minn\u22121 = \u03b1 max \u03c4minn = \u03b1 min,\nwe have \u03b1minH = 0 and \u03b1 max H = 1. Hence, for any \u03c4n\u22121 \u2265 \u03c4minn\u22121 and \u03c4n \u2264 \u03c4minn the constraint (40) holds and consequently AH [1:n,1:n\u22122] is non-negative. The corresponding columns a\u0304minH,n\u22121 = a\u0304n + \u03c4 min n\u22121 \u03b4 out and a\u0304minH,n = a\u0304n + \u03c4 min n \u03b4\nout are depicted in Fig.7 (bottom).\nCombining the above constraints we have that the four parameters \u03c4minn\u22121 , \u03c4 min n , \u03c4 max n\u22121 , \u03c4 max n\ndefine the rectangle\n\u03931 = [\u03c4 min n\u22121 , \u03c4 max n\u22121 ]\u00d7 [\u03c4maxn , \u03c4minn ], (41)\nwhich characterize the equivalent matrices AH having all entries non-negative except of possibly in the 2\u00d7 2 aliased block (see Fig.6). Thus we must have \u0393H \u2282 \u03931.\nWe are left to find the conditions under which the 2 \u00d7 2 aliased block is nonnegative. Writing AH explicitly we have that these conditions are\nAH,n,n\u22121 = \u03c4n\u22121(\u03c4n\u22121 \u2212 \u03b1n\u22121)P (n\u0304 |n\u22121) (42) + (1\u2212 \u03c4n\u22121)(\u03c4n\u22121 \u2212 \u03b1n)P (n\u0304 |n) \u2265 0\nAH,n\u22121,n = \u03c4n(\u03b1n\u22121 \u2212 \u03c4n)P (n\u0304 |n\u22121) (43) + (1\u2212 \u03c4n)(\u03b1n \u2212 \u03c4n)P (n\u0304 |n) \u2265 0\nAH,n\u22121,n\u22121 = \u03c4n\u22121(\u03b1n\u22121\u2212\u03c4n)P (n\u0304 |n\u22121) (44) + (1\u2212\u03c4n\u22121)(\u03b1n \u2212 \u03c4n)P (n\u0304 |n) \u2265 0\nAH,n,n = \u03c4n(\u03c4n\u22121 \u2212 \u03b1n\u22121)P (n\u0304 |n\u22121) (45) + (1\u2212 \u03c4n)(\u03c4n\u22121 \u2212 \u03b1n)P (n\u0304 |n) \u2265 0.\nAs the case P (n\u0304 |n\u22121) = P (n\u0304 |n) = 0 is trivial, we assume that at least one of P (n\u0304 |n\u22121), P (n\u0304 |n) is nonzero (and since by convention P (n\u0304 |n\u22121) \u2265 P (n\u0304 |n), this is equivalent to P (n\u0304 |n\u22121) > 0).\nRecall that by definition \u03b4outn\u22121 = P (n\u0304 |n\u22121)\u2212P (n\u0304 |n) (see (5)). We now consider the cases \u03b4outn\u22121 = 0 and \u03b4 out n\u22121 > 0 separately.\nThe case \u03b4outn\u22121 = 0. Consider first the off-diagonal constraint (43) for AH,n\u22121,n \u2265 0, taking the form\n\u03c4n(1\u2212 (\u03b1n\u22121 \u2212 \u03b1n))) \u2264 \u03b1n.\nDenote \u03c40 = \u03b1n/(1\u2212 (\u03b1n\u22121 \u2212 \u03b1n)).\nSince \u03b1n\u22121 \u2212 \u03b1n \u2264 1 we need \u03c4n \u2264 \u03c40. Similarly, (42) is satisfied if and only if \u03c4n\u22121 \u2265 \u03c40. Thus in order for the off-diagonal entries AH,n,n\u22121, AH,n\u22121,n to be nonnegative we need (\u03c4n\u22121, \u03c4n) \u2208 \u039302 where\n\u039302 = [\u03c4 0,\u221e]\u00d7 [\u2212\u221e, \u03c40]. (46)\nNext, the on-diagonal constraint (44) for AH,n\u22121,n\u22121 \u2265 0 is equivalent to\n\u03c4n \u2264 \u03b1n + \u03c4n\u22121(\u03b1n\u22121 \u2212 \u03b1n). (47)\nSimilarly, the on-diagonal constrain (45) for AH,n,n \u2265 0 is\n\u03c4n(\u03b1n\u22121 \u2212 \u03b1n) \u2264 \u03c4n\u22121 \u2212 \u03b1n. (48)\nDefine the two linear functions g0, f0 : R\u2192 R by\ng0(\u03c4n\u22121) = \u03b1n + \u03c4n\u22121(\u03b1n\u22121 \u2212 \u03b1n) f0(\u03c4n\u22121) = \u03c4n\u22121 \u2212 \u03b1n \u03b1n\u22121 \u2212 \u03b1n .\nNote that \u03c40 is a fixed point of both g0 and f0,\n\u03c40 = g0(\u03c40) = f0(\u03c40).\nNote also that for \u03b1n\u22121 \u2265 \u03b1n the functions g0 and f0 are increasing, while for \u03b1n\u22121 < \u03b1n they are decreasing. Thus, if \u03b1n\u22121 \u2265 \u03b1n the constrains (47,48) are automatically satisfied for (\u03c4n\u22121, \u03c4n) \u2208 \u039302, so in this case AH,n\u22121,n\u22121, AH,n,n are also guaranteed to be non-negative.\nIf \u03b1n\u22121 < \u03b1n then with \u03c4n\u22121 \u2265 \u03c4n (as we assume here) we have f0(\u03c4n\u22121) \u2264 g0(\u03c4n\u22121) \u2264 \u03c40 and the constraints (47,48) take the form f0(\u03c4n\u22121) \u2264 \u03c4n \u2264 g0(\u03c4n\u22121). Thus, in order for the on-diagonal entries AH,n\u22121,n\u22121 and AH,n,n to be non-negative we must have (\u03c4n\u22121, \u03c4n) \u2208 \u039303, where\n\u039303 = {(\u03c4n\u22121, \u03c4n) \u2208 \u03931 | f0(\u03c4n\u22121) \u2264 \u03c4n \u2264 g0(\u03c4n\u22121)}. (49)\nWe are left to ensure that for \u03b1n\u22121 < \u03b1n the off diagonal entries are also nonnegative. Indeed, since \u03c4n \u2264 \u03c4n\u22121, \u03c40 is a fixed point and g(\u03c4n\u22121), f(\u03c4n\u22121) are decreasing, for any (\u03c4n\u22121, \u03c4n) \u2208 \u039303 we automatically have that \u03c40 \u2264 \u03c4n\u22121 and \u03c4n \u2264 \u03c40, so (\u03c4n\u22121, \u03c4n) \u2208 \u039303 implies (\u03c4n\u22121, \u03c4n) \u2208 \u039302. Thus all entries of the aliasing block are guaranteed to be non-negative.\nTo conclude, we have shown that for \u03b4outn\u22121 = 0 the feasible region (10) is given by\n\u03930H = { \u03931 \u2229 \u039302 \u03b1n\u22121 \u2265 \u03b1n \u03931 \u2229 \u039303 \u03b1n\u22121 < \u03b1n.\nThe case \u03b4outn\u22121 > 0. This case has the same characteristics as for the \u03b4outn\u22121 = 0 case, but it is a bit more complex to analyze. Define \u03c4\u00b1 (as the analogues of \u03c40) by\n\u03c4\u00b1 = 1\n2\u03b4outn\u22121\n( \u03b1n\u22121P (n\u0304 |n\u22121) (50)\n\u2212(1 + \u03b1n)P (n\u0304 |n)\u00b1 \u221a \u2206 ) ,\nwhere\n\u2206 = ( \u03b1n\u22121P (n\u0304 |n\u22121)\u2212 (1 + \u03b1n)P (n\u0304 |n) )2 + 4\u03b1nP (n\u0304 |n)\u03b4outn\u22121 \u2265 0.\nAnd define the regions\n\u03932 = [\u03c4 +,\u221e]\u00d7 [\u03c4\u2212, \u03c4+] (51)\n\u03933 = {(\u03c4n\u22121, \u03c4n) \u2208 \u03931 | f(\u03c4n\u22121) \u2264 \u03c4n \u2264 g(\u03c4n\u22121)} (52)\nwhere the functions g, f : R\u2192 R are given by\ng(\u03c4n\u22121) =\n( \u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n)\n\u03b4outn\u22121\n) (53)\n\u2212 P (n\u0304 |n\u22121)P (n\u0304 |n)(\u03b1n\u22121 \u2212 \u03b1n) (\u03b4outn\u22121) 2\n\u00d7 ( \u03c4n\u22121 \u2212 (\u2212P (n\u0304 |n) \u03b4outn\u22121 ))\u22121 and\nf(\u03c4n\u22121) = ( \u2212P (n\u0304 |n) \u03b4outn\u22121 ) (54)\n\u2212 P (n\u0304 |n\u22121)P (n\u0304 |n)(\u03b1n\u22121 \u2212 \u03b1n) (\u03b4outn\u22121) 2\n\u00d7 ( \u03c4n\u22121 \u2212 (\u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n) \u03b4outn\u22121 ))\u22121 .\nLemma 8. Let \u03931 be defined in (41) and let \u03932 and \u03933 be defined according to whether \u03b4outn\u22121 = 0 (46,49) or not (51,52). Then the feasible region \u0393H satisfies\n\u0393H = { \u03931 \u2229 \u03932 \u03b1n\u22121 \u2265 \u03b1n \u03931 \u2229 \u03933 \u03b1n\u22121 < \u03b1n.\nProof. As the case \u03b4outn\u22121 = 0 was treated above, we consider the case \u03b4 out n\u22121 > 0. Consider first the off diagonal constraint (43). Multiplying by (\u22121), we need to solve the following inequality for \u03c4 \u2208 R,\n\u03c42\u03b4outn\u22121 \u2212 \u03c4(\u03b1n\u22121P (n\u0304 |n\u22121) (55) \u2212(1 + \u03b1n)P (n\u0304 |n))\u2212 \u03b1nP (n\u0304 |n) \u2264 0.\nWe first solve with equality to find the solutions \u03c4\u2212, \u03c4+ given in (50). Thus, since \u2206 \u2265 0 we have that any feasible \u03c4n must satisfy \u03c4\u2212 \u2264 \u03c4n \u2264 \u03c4+. Note that the constraint (42) for \u03c4n\u22121 is the complement of (43), and by assumption \u03c4n\u22121 \u2265 \u03c4n, so (42) is satisfied iff \u03c4+ \u2264 \u03c4n\u22121. Thus, the region \u03932 given in (51) indeed characterize the non-negativity of both An\u22121,n and An,n\u22121. With some algebra, \u03c4+ and \u03c4\u2212 can be shown to satisfy the following useful relations:\n\u2022 If \u03b1n\u22121 \u2265 \u03b1n then\n\u2212P (n\u0304 |n\u22121) \u03b4outn\u22121 \u2264 \u03c4\u2212 \u2264 0 (56)\nand\n0 \u2264 \u03c4+ \u2264 \u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n) \u03b4outn\u22121 . (57)\n\u2022 If \u03b1n\u22121 < \u03b1n then\n\u03c4\u2212 \u2264 \u2212P (n\u0304 |n) \u03b4outn\u22121\n(58)\n\u2264 \u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n) \u03b4outn\u22121 \u2264 \u03c4+.\nWe proceed to handle the constraints (44) and (45) corresponding to the region \u03933. We begin by solving the inequality (44):\n\u2212\u03c4n(P (n\u0304 |n) + \u03c4n\u22121\u03b4outn\u22121) + \u03b1nP (n\u0304 |n) + \u03c4n\u22121(\u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n)) \u2265 0.\nNote that for (\u03c4n\u22121, \u03c4n) \u2208 \u03931 we have (P (n\u0304 |n) + \u03c4n\u22121\u03b4outn\u22121) \u2265 0. Rearranging we get that in order for AH,n\u22121,n\u22121 to be non-negative we must have that\nif \u03c4n\u22121 \u2265 \u2212 P (n\u0304 |n) \u03b4outn\u22121 then \u03c4n \u2264 g(\u03c4n\u22121), (59)\nwhere g is the function given in (53). Similarly, consider the condition (45),\n\u03c4n ( \u03b1nP (n\u0304 |n)\u2212 \u03b1n\u22121P (n\u0304 |n\u22121) + \u03c4n\u22121\u03b4outn\u22121 ) +P (n\u0304 |n)(\u03b1n \u2212 \u03c4n\u22121) \u2265 0.\nRearranging we find that in order for AH,n,n \u2265 0 we must have{ \u03c4n \u2264 f(\u03c4n\u22121) \u03c4n\u22121 \u2264 \u03b1n\u22121P (n\u0304 |n\u22121)\u2212\u03b1nP (n\u0304 |n)P (n\u0304 |n\u22121)\u2212P (n\u0304 |n) \u03c4n \u2265 f(\u03c4n\u22121) otherwise,\n(60)\nwhere the function f is given in (54). Note that g (res. f ) defines the boundary where (44) (res. (45)) changes sign, namely any pair (\u03c4n\u22121, \u03c4n) = (\u03c4n\u22121, g(\u03c4n\u22121)) is on the curve making Equation (44) equal zero, and similarly f(\u03c4n\u22121) is such that (\u03c4n\u22121, \u03c4n) = (\u03c4n\u22121, f(\u03c4n\u22121)) is on the curve making (45) equal zero. Having the boundaries g, f in our disposal let us first consider the case \u03b1n\u22121 \u2265 \u03b1n.\nThe sub-case \u03b1n\u22121 \u2265 \u03b1n. We show that in this case, having (\u03c4n\u22121, \u03c4n) \u2208 \u03931 \u2229 \u03932 already ensures that conditions (59) and (60) are trivially met, which in turn implies the non-negativity of both AH,n\u22121,n\u22121 and AH,n,n. This is done by showing that for any (\u03c4n\u22121, \u03c4n) \u2208 \u03931 \u2229 \u03932 the curve (\u03c4n\u22121, g(\u03c4n\u22121)) is above (\u03c4n\u22121, \u03c4+). Similarly, for \u03c4n\u22121 < (\u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n))/\u03b4outn\u22121 the curve (\u03c4n\u22121, f(\u03c4n\u22121)) is above (\u03c4n\u22121, \u03c4\n+) and for \u03c4n\u22121 > (\u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n))/\u03b4outn\u22121 the curve (\u03c4n\u22121, f(\u03c4n\u22121)) is below (\u03c4n\u22121, \u03c4\u2212), thus making conditions (59) and (60) true. Toward this end consider the equality g(\u03c4) = f(\u03c4) given by\n0 = ( \u03b1n\u22121P (n\u0304 |n\u22121) + (1\u2212 \u03b1n)P (n\u0304 |n) ) \u00d7(\n\u03c42\u03b4outn\u22121 + \u03c4((1 + \u03b1n)P (n\u0304 |n) \u2212\u03b1n\u22121P (n\u0304 |n\u22121))\u2212 \u03b1nP (n\u0304 |n) ) .\nThus if (\u03b1n\u22121P (n\u0304 |n\u22121) + (1\u2212 \u03b1n)P (n\u0304 |n)) = 0 we have that g = f identically. Otherwise we need to solve again (55) so the solutions are \u03c4+, \u03c4\u2212 with g(\u03c4+) = f(\u03c4+) and g(\u03c4\u2212) = f(\u03c4\u2212). In addition one can show that \u03c4+ and \u03c4\u2212 are in fact fixed points of both g and f , so together we have\n\u03c4+ = g(\u03c4+) = f(\u03c4+) \u03c4\u2212 = g(\u03c4\u2212) = f(\u03c4\u2212).\nInspecting g(\u03c4n\u22121) one can see that for \u03c4n\u22121 \u2265 \u2212P (n\u0304 |n)/\u03b4outn\u22121, g(\u03c4n\u22121) is monotonic increasing and concave. Since by (57) we have \u03c4+ \u2265 \u2212P (n\u0304 |n)/\u03b4outn\u22121 we get that for \u03c4n\u22121 \u2265 \u03c4+ we must have g(\u03c4n\u22121) \u2265 \u03c4+ as needed. Similarly, for \u03c4+ \u2264 \u03c4n\u22121 < (\u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n))/\u03b4outn\u22121 the function f(\u03c4n\u22121) is increasing and convex and thus above \u03c4+, while for (\u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n))/\u03b4outn\u22121 < \u03c4n\u22121 it is increasing but always below \u03c4\u2212. Thus, for \u03b1n\u22121 \u2265 \u03b1n we have that \u03931 \u2229 \u03932 also characterize the non-negativity of AH,n\u22121,n\u22121 and AH,n,n as claimed.\nThe sub-case\u03b1n\u22121 < \u03b1n. Note that by (58) we have \u03c4+ \u2265 (\u03b1n\u22121P (n\u0304 |n\u22121)\u2212 \u03b1nP (n\u0304 |n))/\u03b4outn\u22121. Thus for \u03c4+ \u2264 \u03c4n\u22121 both g and f are decreasing and convex and f(\u03c4n\u22121) \u2264 g(\u03c4n\u22121). Thus in order to ensure (59, 60) we need f(\u03c4n\u22121) \u2264 \u03c4n \u2264 g(\u03c4n\u22121). Thus, \u03933 as defined in (52) characterize the non-negativity of AH,n,n, AH,n\u22121,n\u22121. Finally we need to show that having (\u03c4n\u22121, \u03c4n) \u2208 \u03933 also ensures the non-negativity of AH,n,n\u22121 and AH,n\u22121,n. But by (58) we have that for \u03c4n\u22121 \u2265 \u03c4+ both g(\u03c4n\u22121), f(\u03c4n\u22121) \u2265 \u03c4\u2212 and thus \u03933 \u2282 \u03932. Hence we have shown that \u0393H is characterized as claimed.\nLemma 9. The set \u0393H is connected.\nProof. If \u03b1n\u22121 \u2265 \u03b1n then \u0393H = \u03931 \u2229\u03932 is a rectangle and thus connected. In the case \u03b1n\u22121 < \u03b1n we have that f(\u03c4n\u22121) \u2264 g(\u03c4n\u22121) and are both decreasing and convex thus the region \u03933 with intersection with a rectangle is a connected set."}, {"heading": "C.2 Conditions for |\u0393H | = 1", "text": "Let us first write\n(\u03c4n\u22121, \u03c4n) = (1, 0) + (\u2206\u03c4n\u22121,\u2206\u03c4n).\nWe characterize the conditions for |\u0393H | = 1 by determining the geometrical constraints the entries of the transition matrix A pose on (\u2206\u03c4n\u22121,\u2206\u03c4n) in order to ensure AH(1 + \u2206\u03c4n\u22121,\u2206\u03c4n) \u2265 0. Note that |\u0393H | = 1 iff these constraints imply that (\u2206\u03c4n\u22121,\u2206\u03c4n) = (0, 0) is the unique feasible pair.\nAs a first example, consider a 2A-HMM H having a transition matrix with all entries being strictly positive, A \u2265 > 0. Since the mapping (9) is continuous in \u2206\u03c4n\u22121,\u2206\u03c4n, there exists a neighborhood N \u2282 R2 of (\u2206\u03c4n\u22121,\u2206\u03c4n) = (0, 0), such that for any (\u2206\u03c4n\u22121,\u2206\u03c4n) \u2208 N the matrix AH(1 + \u2206\u03c4n\u22121,\u2206\u03c4n) is non-negative, and thus N \u2282 \u0393H . This condition can be represented in the (\u2206\u03c4n\u22121,\u2206\u03c4n) plane (i.e. R2) as the \u201dfull\u201d diagram Fig.8. On the other hand, the condition that (\u2206\u03c4n\u22121,\u2206\u03c4n) = (0, 0) is the unique feasible pair can be represented by a point like diagram as in Fig.8.\nIn general, the entries of the transition matrix A put constraints on the feasible (\u2206\u03c4n\u22121,\u2206\u03c4n) only when (\u03c4n\u22121, \u03c4n) = (1, 0) is on the boundary of \u0393H . These constraints can be explicitly determined in terms of A\u2019s entries by considering the exact characterization of \u0393H given in Theorem 2. Note however that by the fact that \u0393H is connected, and as far as the condition |\u0393H | = 1 is concerned, we only need to consider the shape of these constraints in a small neighborhood of (\u03c4n\u22121, \u03c4n) = (1, 0), i.e for |\u2206\u03c4n\u22121| , |\u2206\u03c4n| << 1. Any such neighborhood can be represented on the R2 plane (as in Fig.8). The shape of this neighborhood for a given H is called the effective feasible\nregion of \u0393H . Now, as the example with A \u2265 > 0 shows, a non-trivial constraint on the (effective) feasible region must results from A having some zeros entries. Each such a zero entry, as determined by its position in A, put a boundary constraint on (\u2206\u03c4n\u22121,\u2206\u03c4n). These in turn corresponds to a suitable diagram in R2 (as the diagram for \u2206\u03c4n \u2265 0 in Fig.8). The effective feasible region of A is obtained by taking the intersection of all these diagrams. The exact correspondence between A\u2019s entries and the corresponding diagrams is given in Tables 1,2,3,4. The procedure for determining the effective feasible region of a 2A-HMM is given in Algorithm 1. The correctness of the algorithm is demonstrated in the proof of Lemma 8.\nAlgorithm 1 determining the effective feasible region for minimal 2A-HMM H 1: permute aliased states so that P (n\u0304 |n\u22121) \u2265 P (n\u0304 |n) 2: collect the following diagrams:\n- \u2200i \u2208 [n\u22122] with Ai,n \u2208 {0, 1} or Ai,n\u22121 \u2208 {0, 1} pick the relevant diagram in Table 1\n- \u2200j \u2208 suppin \\{n\u22121, n} with \u03b1j \u2208 {0, 1} pick corresponding diagram in Table 2\n- if \u03b1n\u22121 \u2265 \u03b1n pick relevant diagram in table 3 and if \u03b1n\u22121 < \u03b1n pick relevant diagram in Table 4\n3: Return the intersection of all the regions obtained in previous step"}, {"heading": "C.3 Examples.", "text": "We demonstrate our Algorithm 1 for determining the identifiability of 2A-HMMs on the 2A-HMM given in Section 6, shown in Fig 2 (left). Going through the steps of Algorithm 1 we get the following diagrams for the effective feasible region:\n\ufe38 \ufe37\ufe37 \ufe38 from Table 1:\nA1,3=0\u2227A1,4 6=0\n\u2229 \ufe38 \ufe37\ufe37 \ufe38 from Table 1:\nA2,4=0\u2227A2,3 6=0 \u2229 \ufe38 \ufe37\ufe37 \ufe38 from Table 2: \u03b11=1 \u2229 \ufe38 \ufe37\ufe37 \ufe38 from Table 2: \u03b12=0 = .\nSince their intersection results in a point like diagram, this 2A-HMM is identifiable. More generally, for a minimal stationary 2A-HMM satisfying Assumptions (A1A2) with aliased states n and n\u22121, a sufficient condition for uniqueness is the following constraints on the allowed transitions between the hidden states: \u2203in\u22121, jn\u22121, in, jn \u2208\n[n\u22122] such that\nX in\u22121 \u2192 n\u22121 \u2192 jn\u22121 X in\u22121 \u2192 n\u2192 \u2217 X in \u2192 n\u2192 jn X in \u2192 n\u22121 \u2192 \u2217\nX \u2217 \u2192 n\u22121 \u2192 jn X \u2217 \u2192 n\u2192 jn\u22121.\nOne can check that these conditions give the same set of diagrams as above."}, {"heading": "D Proofs for Section 5 (Learning)", "text": ""}, {"heading": "D.1 Proof of Lemma 3", "text": "The claim in (21) that M (1) = BAC\u03b2 = A\u0304 follows directly from (14) and (16). Next, we have\nR(2) = BAAC\u03b2 \u2212BAC\u03b2BAC\u03b2 = BA(In \u2212 C\u03b2B)AC\u03b2 = BAbcT\u03b2AC\u03b2 ,\nwhere the last equality is by the fact that (In \u2212 C\u03b2B) = bcT\u03b2 . Since BAb = \u03b4 out and cT\u03b2AC\u03b2 = (\u03b4 in)T we have R(2) = \u03b4out(\u03b4in)T as claimed in (22).\nAs for (23) we have,\nR(3) = BAAAC\u03b2 \u2212BAAC\u03b2BAC\u03b2 \u2212BAC\u03b2BAAC\u03b2 +BAC\u03b2BAC\u03b2BAC\u03b2\n= BA(In \u2212 C\u03b2B)A(In \u2212 C\u03b2B)AC\u03b2 = \u03b4outcT\u03b2Ab(\u03b4 in)T.\nSince by definition \u03ba = cT\u03b2Ab we have R (3) = \u03baR(2) and the claim in (23) is proved.\nFinally, F (c) = BA ( diag(K[\u00b7,c])\u2212 C\u03b2 diag(K\u0304[\u00b7,c])B ) AC\u03b2 .\nSince\ndiag(K[\u00b7,c])\u2212 C\u03b2 diag(K\u0304[\u00b7,c])B = K\u0304n\u22121,cb(c\u03b2)T\nwe have that F (c) = K\u0304n\u22121,cR(2) as claimed in (24)."}, {"heading": "D.2 Proof of Lemma 4", "text": "Assumption (A2) combined with the fact that the HMM has a finite number of states imply that the HMM is geometrically ergodic: there exist parameters G < \u221e and \u03c8 \u2208 [0, 1) such that from any initial distribution \u03c00,\u2225\u2225At\u03c00 \u2212 \u03c0\u2225\u2225\n1 \u2264 2G\u03c8t, \u2200t \u2208 N. (61)\nThus, we may apply the following concentration bound, given in Kontorovich & Weiss [2014]:\nTheorem 4. Let Y = Y0, . . . , YT\u22121 \u2208 YT be the output of a HMM with transition matrixA and output parameters \u03b8. Assume thatA is geometrically ergodic with constants G,\u03c8. Let F : (Y0, . . . , YT\u22121) 7\u2192 R be any function that is Lipschitz wit constatnt l with respect to the Hamming metric on YT . Then, for all > 0,\nPr(|F (Y )\u2212 EF | > T ) \u2264 2 exp ( \u2212T (1\u2212 \u03c8) 2 2\n2l2G2\n) . (62)\nIn order to apply the theorem note that \u2200t \u2208 {1, 2, 3}, E[M\u0302(t)ij ] = M (t) ij for any i, j \u2208 [n\u22121]. In addition, following Assumption (A3), (T \u2212 t)M\u0302(t)ij is (t + 1)L2Lipschitz with respect to the Hamming metric on YT . Thus, taking \u2248 T\u2212 12 in Theorem 4 and applying a union bound on i, j readily gives\nM\u0302(t) = M(t) +OP ( T\u2212 1 2 ) .\nThe kernel-free moments M\u0302 (t) given in (25) incur additional error which results in a factor of at most 1/(\u03c3min(K\u0304)2 mini \u03c0i) hidden in theOP notation. SinceR(t) are (low order) polynomials of M (t), the asymptotics OP ( T\u2212 1 2 ) carry on to the error in R\u0302(t).\nA similar argument yields the claim for F (c)."}, {"heading": "D.3 Proof of Lemma 6", "text": "Let \u03c31 and \u03c3\u03021 be the largest singular values of R(2) and R\u0302(2), respectively. Combining Weyl\u2019s Theorem [Stewart & Sun, 1990] with Lemma 4 gives\n|\u03c31 \u2212 \u03c3\u03021| \u2264 \u2225\u2225\u2225R(2) \u2212 R\u0302(2)\u2225\u2225\u2225\nF = OP (T\n\u2212 12 ),\nRecall that under the null hypothesis H0, we have \u03c31 = 0. Thus, with high probability \u03c3\u03021 < \u03be0T\u2212 1 2 , for some \u03be0 > 0. In contrast, underH1 we have \u03c31 = \u03c3 > 0, thus for some \u03be1 > 0, \u03c3\u03021 > \u03c3 \u2212 \u03be1T\u2212 1 2 . Hence, taking T sufficiently large, we have that for any ch > 0 and 0 < < 12 , with hT = chT \u2212 12 + ,\nin caseH0 : \u03c3\u03021 < hT in caseH1 : \u03c3\u03021 > hT ,\nwith high probability. Thus, the correct detection of aliasing is with high probability."}, {"heading": "D.4 Proof of Lemma 7", "text": "Let us define the following score function for any i \u2208 [n\u22121],\nscore(i) = \u2211\nj\u2208[n\u22121]\n\u2225\u2225\u2225F\u0302 (j) \u2212 K\u0304i,jR\u0302(2)\u2225\u2225\u22252 F .\nAccording to Eq. (30) the chosen aliased component is the index with minimal score. Hence, in order to prove the Lemma we need to show that\nlim T\u2192\u221e\nPr(\u2203i 6= n\u22121 : score(i) < score(n\u22121)) = 0.\nBy Lemma 4 and (20) we have\nF\u0302 (j) = F (j) + \u03be (j) F\u221a T = K\u0304n\u22121,jR (2) + \u03be (j) F\u221a T\nR\u0302(2) = R(2) + \u03beR\u221a T ,\nfor some \u03beR, \u03be (j) F \u2208 R(n\u22121)\u00d7(n\u22121) with OP (\u03beR) = 1 and OP (\u03be (j) F ) = 1. Thus,\nscore(n\u22121) = 1\u221a T \u2211 j\u2208[n\u22121] \u2225\u2225\u2225\u03be(j)F \u2212 K\u0304j,n\u22121\u03beR\u2225\u2225\u22252 F P\u2212\u2192 0.\nIn contrast, for any i 6= n\u22121 we may write score(i) as\n\u2211 j\u2208[n\u22121] \u2225\u2225\u2225\u2225(K\u0304j,i \u2212 K\u0304j,n\u22121)R(2) + 1\u221aT (\u03be(j)F \u2212 K\u0304j,n\u22121\u03beR) \u2225\u2225\u2225\u22252 F .\nApplying the (inverse) triangle inequality we have\nscore(i) \u2265 \u03c32 \u2225\u2225K\u0304[\u00b7,n\u22121] \u2212 K\u0304[\u00b7,i]\u2225\u22252 \u2212OP (T\u2212 12 ).\nSince K\u0304 is full rank, \u03c32 \u2225\u2225K\u0304[\u00b7,n\u22121] \u2212 K\u0304[\u00b7,i]\u2225\u22252 > 0. Thus, for any i 6= n\u22121 as T \u2192 \u221e, w.h.p score(i) > score(n\u22121). Taking a union bound over i yields the claim."}, {"heading": "D.5 Estimating \u03b3 and \u03b2", "text": "We now show how to estimate \u03b3 and \u03b2. As discussed in Section 5.4, this is done by searching for \u03b3\u2032, \u03b2\u2032 ensuring the non-negativity of (33), namely, A\u2032H(\u03b3\n\u2032, \u03b2\u2032) \u2265 0, where\nA\u2032H(\u03b3 \u2032, \u03b2\u2032) \u2261 C\u03b2\u2032A\u0304B + \u03b3\u2032C\u03b2\u2032uc\u03b2\u2032 T +\n\u03c3 \u03b3\u2032 bvTB + \u03ba bc\u03b2\u2032 T.\nWe pose this as a non-linear two dimensional optimization problem. For any \u03b3\u2032 \u2265 0 and 0 \u2264 \u03b2\u2032 \u2264 1 define the objective function h : R2 \u2192 R by\nh(\u03b3\u2032, \u03b2\u2032) = min i,j\u2208[n] {\u03b3\u2032A\u2032H(\u03b3\u2032, \u03b2\u2032)ij}.\nNote that h(\u03b3\u2032, \u03b2\u2032) \u2265 0 iff A\u2032H(\u03b3\u2032, \u03b2\u2032) does not have negative entries. Recall that by the identifiability of H , if we constrain \u03b3\u2032 \u2265 0 then the constraint A\u2032H(\u03b3\u2032, \u03b2\u2032) \u2265 0 has the unique solution (\u03b3, \u03b2) (this is the equivalent to the convention \u03c4n\u22121 \u2265 \u03c4n made in Section 4.2). Namely, any (\u03b3\u2032, \u03b2\u2032) 6= (\u03b3, \u03b2) results in at least one negative entry in\nA\u2032H(\u03b3 \u2032, \u03b2\u2032). Hence, h(\u03b3\u2032, \u03b2\u2032) has a unique maximum, obtained at the true (\u03b3, \u03b2). In addition, since \u2016u\u20162 = \u2016v\u20162 = 1, a feasible solution must have \u03b3\u2032 \u2264 2/\u03c3. So our optimization problem is:\n(\u03b3\u0302, \u03b2\u0302) = argmax (\u03b3\u2032,\u03b2\u2032)\u2208[0, 2\u03c3 ]\u00d7[0,1]\nh(\u03b3\u2032, \u03b2\u2032) (63)\nThis two dimensional optimization problem can be solved by either brute force or any non-linear problem solver.\nIn practice, we solve the optimization problem (63) with the empirical estimates plugged in, that is\nA\u0302\u2032H(\u03b3 \u2032, \u03b2\u2032) = C\u03b2\u0302 \u02c6\u0304AB + \u03b3\u2032C\u03b2\u2032u\u03021c T \u03b2\u2032 + \u03c3\u03021 \u03b3\u2032 bv\u0302T1B + \u03ba\u0302 bc T \u03b2\u2032 .\nThe empirical objective function h\u0302(\u03b3\u2032, \u03b2\u2032) is defined similarly. Such a perturbation may results in a problem with many feasible solutions, or worse, with no feasible solutions at all. Nevertheless, as shown in the proof of Theorem 3, this method is consistent. Namely, as T \u2192\u221e, the above method will return an arbitrarily close solution (in \u2016\u00b7\u2016F) to the true transition matrix A, with high probability."}, {"heading": "D.6 Proof of Theorem 3", "text": "Recall the definitions of A\u2032H(\u03b3 \u2032, \u03b2\u2032) and its empirical version A\u0302\u2032H(\u03b3 \u2032, \u03b2\u2032), given in the previous Section D.5. To prove the theorem we show that\u2225\u2225\u2225A\u0302\u2032H(\u03b3\u0302, \u03b2\u0302)\u2212A\u2032H(\u03b3, \u03b2)\u2225\u2225\u2225 F P\u2212\u2192 0.\nToward this goal we bound the l.h.s by\u2225\u2225\u2225A\u0302\u2032H(\u03b3\u0302, \u03b2\u0302)\u2212A\u2032H(\u03b3\u0302, \u03b2\u0302)\u2225\u2225\u2225 F + \u2225\u2225\u2225A\u2032H(\u03b3\u0302, \u03b2\u0302)\u2212A\u2032H(\u03b3, \u03b2)\u2225\u2225\u2225 F , (64)\nand show that each term converges to 0 in probability. We shall need the following lemma, establishing the pointwise convergence in probability of A\u0302H to AH :\nLemma 10. For any 0 < \u03b3\u2032 and 0 \u2264 \u03b2\u2032 \u2264 1,\u2225\u2225\u2225A\u0302H(\u03b3\u2032, \u03b2\u2032)\u2212AH(\u03b3\u2032, \u03b2\u2032)\u2225\u2225\u2225 F = oP (1).\nProof. By (29), \u02c6\u0304A P\u2212\u2192 A\u0304. In addition, in Section D.3 we saw \u03c3\u03021 P\u2212\u2192 \u03c3 and one can easily show that \u03ba\u0302 P\u2212\u2192 \u03ba. Thus, in order to prove the claim it suffices to show that u\u03021 P\u2212\u2192 u and v\u03021 P\u2212\u2192 v. By Wedin\u2019s Theorem [Stewart & Sun, 1990]:\n\u2016u\u03021 \u2212 u\u20162 \u2264 C\n\u2225\u2225\u2225R\u0302(2) \u2212R(2)\u2225\u2225\u2225 2\n\u03c3 ,\nfor some C > 0. Combining this with Lemma 4 gives that \u2016u\u03021 \u2212 u\u20162 = OP (T\u2212 1 2 ). The same argument goes for \u2016v\u03021 \u2212 v\u20162.\nWe begin with the second term in (64). The first step is showing that the estimated parameters \u03b3\u0302, \u03b2\u0302 in (63) converge with probability to the true parameters \u03b3, \u03b2. We first need to following lemma, establishing the convergence of h\u0302 to h uniformly in probability:\nLemma 11. For any > 0,\nPr ( sup\n(\u03b3\u2032,\u03b2\u2032)\u2208[0,2]\u00d7[0,1] \u2223\u2223\u2223h\u0302(\u03b3\u2032, \u03b2\u2032)\u2212 h(\u03b3\u2032, \u03b2\u2032)\u2223\u2223\u2223 > ) = o(1). Proof. Note that h\u0302(\u03b3\u2032, \u03b2\u2032) is the value of the minimal entry of a matrix with all entries being polynomials of \u03b3\u2032, \u03b2\u2032 with bounded coefficients. Thus h\u0302 is Lipschitz. In addition [0, 2]\u00d7 [0, 1] is compact and, similarly to Lemma 10, h\u0302(\u03b3\u2032, \u03b2\u2032) converges in probability pointwise to h(\u03b3\u2032, \u03b2\u2032). Hence, the claim follows by Newey [1991, Corollary 2.2].\nLemma 12. (\u03b3\u0302, \u03b2\u0302) P\u2212\u2192 (\u03b3, \u03b2).\nProof. Recall that (\u03b3\u0302, \u03b2\u0302) are the maximizers of h\u0302(\u03b3\u2032, \u03b2\u2032) and (\u03b3, \u03b2) are the maximizers of h(\u03b3\u2032, \u03b2\u2032), over (\u03b3\u2032, \u03b2\u2032) \u2208 [0, 2]\u00d7 [0, 1]. To prove the claim we need to show that for any \u03b4 > 0,\nPr (\u2225\u2225\u2225(\u03b3\u0302, \u03b2\u0302)\u2212 (\u03b3, \u03b2)\u2225\u2225\u2225 > \u03b4) = o(1).\nToward this end define\n(\u03b4) \u2261 h(\u03b3, \u03b2)\u2212 max \u2016(\u03b3\u2032,\u03b2\u2032)\u2212(\u03b3,\u03b2)\u2016>\u03b4\nh(\u03b3\u2032, \u03b2\u2032).\nNote that (\u03b4) > 0 since h(\u03b3\u2032, \u03b2\u2032) has the unique maximum (\u03b3, \u03b2). Now,by Lemma 11, we have that\nPr ( sup \u03b3\u2032,\u03b2\u2032 \u2223\u2223\u2223h\u0302(\u03b3\u2032, \u03b2\u2032)\u2212 h(\u03b3\u2032, \u03b2\u2032)\u2223\u2223\u2223 > (\u03b4)/4) = o(1). (65) Thus, if we show that sup\n\u2223\u2223\u2223h\u0302\u2212 h\u2223\u2223\u2223 \u2264 (\u03b4)/4 implies \u2225\u2225\u2225(\u03b3\u0302, \u03b2\u0302)\u2212 (\u03b3, \u03b2)\u2225\u2225\u2225 \u2264 \u03b4 then the claim is proved. So assume\nsup \u03b3\u2032,\u03b2\u2032 \u2223\u2223\u2223h\u0302(\u03b3\u2032, \u03b2\u2032)\u2212 h(\u03b3\u2032, \u03b2\u2032)\u2223\u2223\u2223 \u2264 (\u03b4)/4. Toward getting a contradiction let us assume that\n\u2225\u2225\u2225(\u03b3\u0302, \u03b2\u0302)\u2212 (\u03b3, \u03b2)\u2225\u2225\u2225 > \u03b4. Then the following relations hold:\nh(\u03b3\u0302, \u03b2\u0302) \u2264 h(\u03b3, \u03b2)\u2212 (\u03b4) h\u0302(\u03b3\u0302, \u03b2\u0302) \u2264 h(\u03b3\u0302, \u03b2\u0302) + (\u03b4)/4 h\u0302(\u03b3, \u03b2) \u2265 h(\u03b3, \u03b2)\u2212 (\u03b4)/4.\nThus,\nh\u0302(\u03b3\u0302, \u03b2\u0302) \u2264 h\u0302(\u03b3, \u03b2)\u2212 (\u03b4)/2,\nin contradiction to the optimality of (\u03b3\u0302, \u03b2\u0302).\nBy Lemma 12, (\u03b3\u0302, \u03b2\u0302) P\u2212\u2192 (\u03b3, \u03b2). Since H is minimal, Theorem 1 implies \u03b3 > 0 and thus \u03b3\u0302 \u2265P \u03b3/2. In addition, AH is continuous in the compact set [\u03b3/2, 2]\u00d7 [0, 1]. Thus, by the continuous mapping theorem we have\u2225\u2225\u2225AH(\u03b3\u0302, \u03b2\u0302)\u2212AH(\u03b3, \u03b2)\u2225\u2225\u2225\nF\nP\u2212\u2192 0.\nThis proves the case for the right term of (64). The convergence in probability of the left term of (64) to zero is a direct consequence of the following uniform convergence lemma:\nLemma 13.\nsup (\u03b3\u2032,\u03b2\u2032)\u2208[ \u03b32 ,2]\u00d7[0,1]\n\u2225\u2225\u2225A\u0302H(\u03b3\u2032, \u03b2\u2032)\u2212AH(\u03b3\u2032, \u03b2\u2032)\u2225\u2225\u2225 F = oP (1).\nProof. Since \u03b3\u2032 \u2265 \u03b3/2 we have that for any i, j \u2208 [n], A\u0302H(\u03b3\u2032, \u03b2\u2032)ij is Lipschitz. In addition, by Lemma 10, for any (\u03b3\u2032, \u03b2\u2032) \u2208 [\u03b32 , 2] \u00d7 [0, 1], each entry A\u0302H(\u03b3\n\u2032, \u03b2\u2032)ij converge pointwise in probability to AH(\u03b3\u2032, \u03b2\u2032)ij . Finally, [\u03b32 , 2] \u00d7 [0, 1] is compact. Thus, the claim follows from Newey [1991, Corollary 2.2] with an application of a union bound over i, j \u2208 [n]."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In various applications involving hidden Markov models (HMMs), some of<lb>the hidden states are aliased, having identical output distributions. The minimal-<lb>ity, identifiability and learnability of such aliased HMMs have been long standing<lb>problems, with only partial solutions provided thus far. In this paper we focus<lb>on parametric-output HMMs, whose output distributions come from a parametric<lb>family, and that have exactly two aliased states. For this class, we present a com-<lb>plete characterization of their minimality and identifiability. Furthermore, for a<lb>large family of parametric output distributions, we derive computationally efficient<lb>and statistically consistent algorithms to detect the presence of aliasing and learn<lb>the aliased HMM transition and emission parameters. We illustrate our theoretical<lb>analysis by several simulations.", "creator": "TeX"}}}