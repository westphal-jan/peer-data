{"id": "1502.03302", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Using Distance Estimation and Deep Learning to Simplify Calibration in Food Calorie Measurement", "abstract": "high calorie intake in judging the human body available on the the one hand, has actually proved harmful in serving numerous nutritional occasions such leading to several devastating diseases and on the generally other hand, a reasonable standard amount level of calorie intake measuring has been deemed essential precisely by normal dieticians to maintain up the baseline right balance of calorie components content in human body. as such, researchers have proposed proposing a variety of automatic calculating tools and communication systems to assist users significantly measure their calorie in - tank take. in this first paper, we consider the technical category of those tools proposed that use image processing specifically to recognize the food, and we propose basically a straightforward method explanation for evaluating fully automatic modelling and user - friendly calibration of the dimension of the food grams portion tract sizes, which is why needed better in quality order to measure food portion scale weight range and its overall ensuing amount usage of calories. those experimental marketing results show that our method, also which uses deep context learning, ubiquitous mobile cloud array computing, euclidean distance estimation and size meter calibration ratios inside to a universal mobile teller device, leads to an optimal accuracy improvement to up 95 % on absolute average compared to prior previous work", "histories": [["v1", "Wed, 11 Feb 2015 13:27:01 GMT  (1044kb)", "http://arxiv.org/abs/1502.03302v1", "7 pages"], ["v2", "Mon, 23 Mar 2015 11:45:11 GMT  (0kb,I)", "http://arxiv.org/abs/1502.03302v2", "This paper has been withdrawn due to errors in equation 1"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CY cs.HC cs.LG", "authors": ["pallavi kuhad", "abdulsalam yassine", "shervin shirmohammadi"], "accepted": false, "id": "1502.03302"}, "pdf": {"name": "1502.03302.pdf", "metadata": {"source": "META", "title": "Using Distance Estimation and Deep Learning to Simplify Calibration in Food Calorie Measurement", "authors": ["Pallavi Kuhad", "Abdulsalam Yassine", "Shervin Shirmohammadi"], "emails": ["pkuha009@uottawa.ca,", "ayassine@discover.uottawa.ca", "shervin@discover.uottawa.ca"], "sections": [{"heading": "Keywords\u2014 Calorie Measurement; Deep Learning; Distance", "text": ""}, {"heading": "Estimation; Mobile Cloud Computing.", "text": "I. INTRODUCTION\nDue the unprecedented rise in overweightness and obesity in the world and the diseases they cause[1][2][3][4], an important issue for people to maintain a good quality of life today is to watch their daily eating routines in order to avoid excess calorie intake. To assist people with such a task, efficient and convenient applications that can track and alert users of their calorie intake are becoming popular and are being developed.\nIn our previous work [5]-[8], we have developed a mobile system that can measure the calories of the food from that food\u2019s image taken by the user\u2019s smartphone. Once the user captures the image of the food item on the plate, the image is sent to the cloud for food recognition and calorie computation. Food recognition is done by deep learning running in the cloud: the image is recognised and the calorie details matching the image are fetched from the database that also exists in the cloud. The result is then prompted back to the user\u2019s phone. In our system, for calibration, the user has to put his/her thumb near the food when the food picture is taken. The components of our system are shown in Fig. 1, where we have used Support Vector Machine (SVM) for image processing, MapReduce for the cloud model, Volume calculation and finger calibration for calorie calculation. The user with an Android smartphone connects to a server through http request and\nresponse, which further connects to the cloud server (in our case an Amazon EC2 instance). We then use elastic mapreduce for parallelcomputing in the Amazon cloud.\nWhile our system has achieved excellent results, the issue of calibration has been bothersome, because it is difficult for the user to take the photo with one hand on the phone and other hand\u2019s thumb near the plate. In this work, we have addressed this issue by proposing a distance calculation method between the object on the plate and the person taking the image. By using distance calculation method, the user now does not have to keep the finger in the plate for calibration.\nWe use deep learning to accurately train and classify the food object to its corresponding label. With the help of mobile sensors, the system in real-time, can gauge the distance that the user is standing from the food object. The system will then record this value and send it to the cloud along with the food photo captured. The image will then be processed in the cloud with the help of virtualization, and the results (including the calorie value and the food object label) will be sent back to the user on the mobile device. We have also proposed a new method whereby the application will assist the user in realtime for determining the ideal distance from which the user must capture the photo. If the user fails to capture the photo from the mentioned distance, the system will smartly recalibrate the block size based on the distance measure. Hence calibration will always remain accurate, irrespective of the distance from which the user captures the photo.\nThe rest of the paper is organized as follows: In the next section, we provide the related work followed by the proposed\nsystem in Section III. In section IV, we present the implementation of the system. The experimental results are presented in section V. Finally, in section VI we conclude the paper and provide our plan for future work.\nII. RELATED WORK\nA detailed review of existing calorie measurement systems was shown in [8] and will not be repeated here due to shortage of space. Instead, here we cover works that use mobile sensors and/or papers that try to estimate the dimensions of the food.\nCalorie count, with the help of a mobile phone was shown in [9]. They proposed a system where user\u2019s posture was monitored with the help of mobile accelerometers, to determine the activity performed by the user, further enabling them to estimate the calorie spent on those activities. Although, we make use of accelerometers sensors in our mobile application, we use the mobile device to measure the calorie of the food portion consumed by the user. The work in [10] also makes use of the mobile sensors to detect the human activities like walking, jogging and running. The mobile sensors are powerful tools that have helped many researchers in analysing the activity of the user, further allowing them to calculate the calories burnt. We make use of the mobile sensors (accelerometers) in our application to estimate the distance between the food object and the user taking the photo of the food. In [11], the authors measure the dimension of the food portions using 2D image based on the assumption of known dimensions of the circular plate. But this is not a practical assumption as different plates have different sizes and the user cannot be expected to measure the size of each plate on which food is served.\nOur work therefore makes a novel contribution compared to the above, in that it automatically calibrates the food dimensions without imposing extra workload on the user.\nIII. PROPOSED SYSTEM\nIn this section, we discuss the various methodologies we have adopted for our system. As explained above, our primarily goal is firstly to recognize the image and then measure the calorie. For classifying the food object, we used deep learning, which helped us achieve high accuracy. For measuring the calorie of the food object, we have proposed a new methodology wherein we used a unique approach for estimating the distance from the food object followed by size calibration of the image for measuring calories."}, {"heading": "A. Deep Learning Method", "text": "We have implemented a deep learning technique which enables the system to ascertain the food features based on color, contour, texture and size to classify the food object accurately. We were hence able to achieve higher accuracy results during the classification stage as compared to our previous approaches (such as Support Vector Machine) [8]. The Deep Learning approach is in continuation to what has been achieved as part of that previous work. With the deep learning method, first we train our food images with a deep neural network, then we generate the model file. Every time the user submits a food picture for calorie measurement, the\nsystem performs segmentation and extracts features, which are further written into hidden layers in the deep neural network. The libraries and the model files of the deep neural network here are based on the work of Hinton [12] and Srivastava [13]. After customizing the top-level feature layer, we are able to generate the classification results. The details are described next.\n1) Training the Deep Neural Network.\nA neural network computes a differentiable function of its input. For example, our application computes the probability, p, of the match of the input image with the corresponding label set [12]:\nThe standard way to model a neuron\u2019s output f as an activation function of its input x is either a hyperbolic or sigmoid function [12]:\nBut we use the term rectified linear unit (ReLU) to refer to unit in a neural net that use the activation function max(0;x) [14]. As compared to the above mentioned activation functions (hyperbolic or sigmoid), the training of deep convolutional neural networks with ReLUs is relatively faster [12]. For understanding the deep neural network that we built, we can refer to this example. Consider we choose the greyscale images which are 28 by 28 pixels in size as input. If we will use the notation x to denote the training input, then there would be 28*28= 784 input neurons. Each entry in the neuron represents the grey value for a single pixel in the image. We will denote the corresponding desired output by y=y(x). What we would like is an algorithm which lets us find the weights and biases so that the output from the network approximates y(x) for all training inputs x. So if we have a training image set of food images and we want to classify the food type to Apple during the learning phase, we could possibly achieve that by tweaking the weight and bias values.\nWe will do that using an algorithm known as stochastic gradient descent as described in [15]. Hence we will able to tweak the weights and bias to get the output closer to the desired output, during the learning phase. The idea is to use gradient descent to find the weights and biases which minimize the cost . The gradient vector C has corresponding components and . The stochastic gradient descent works by randomly picking out a small number m of randomly chosen training inputs (for example 10 images from the original set of 100 images). We will label those random training inputs X1,X2,\u2026,Xm. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs [16], and training with those, where the weights and bias is computed by: ( 3 )\n( 4 )\nWhere the sums are over all the training examples Xj in the current mini-batch and \u03b7 is the learning rate. Then we pick out another randomly chosen mini-batch and train with those, until we have exhausted the training inputs.\nThe back-propagation algorithm, is the fast way of computing the gradient of the cost function [12]. Training the deep neural network in this way, will allow us to make the necessary changes to the weight and bias, without majorly aaffecting the output of the network and giving us the desired results. For example, this algorithm will help us tweak the weights (w) and bias (b) during the learning phase, in a way we can finally determine the output as one of the two (apple or cherry), without effecting the rest of the food classes. Delta changes in either the weights or the bias will change the result from one food class to the other. As shown in the Fig. 2, considering we have taken the color feature into account, any changes in the weight of w or bias b would make the small changes to the final results, which in this case between apple and cherry (having almost same color features) will alter the eventual result. If the probability of the image (p>0.5 towards Apple), it would be classified as Apple and same is the case with any food type. 2) Food Object Classification based on Deep Neural"}, {"heading": "Network", "text": "Once we have trained the deep neural network with the food images pertaining to certain classes, the system then generates a model file. While integrating this system to mobile, we made sure the image processing steps are performed in the cloud. This was primarily done to remove the overhead of processing, from the mobile device of the user. We used the concept of cloud virtualization, which enabled us to create a replica of our Android application to be run in cloud. Computing in cloud was performed on Amazon EC2 instances. Once the user clicked the photo of the food object, the image along with other details (distance information and user information) will be sent to cloud for processing. Here, the image is firstly segmented to obtain the food portion and the features are extracted from the target object. Once features are extracted, it is then mapped against the model file which provides with the probability and food tag associated with the food object."}, {"heading": "B. Distance Measurement", "text": "The application, calculates the distance from the food object during the live camera feed, when the user is about to capture the photo. During the registration phase, the user is prompted to enter his height details (in feet or cms), which is later used during the measure of the distance of food object from his phone. We then calculate the mobile phone\u2019s orientation using the rotation matrix. The values of the rotation matrix are obtained from the Accelerometer sensor and Magnetic Field sensor values. The values returned by the rotation matrix contain the Azimuth (rotation around the zaxis), Pitch (rotation around the x axis), Roll (rotation around the y axis) [17]. For getting the sensor values, firstly we invoke the Accelerometer sensor instance using the Sensor Manager in Android. Similarly we obtain the values from Magnetic Field sensor. For our application, Eat Healthy Stay Healthy (EHSH), we are capturing the food object images from the phone\u2019s camera, keeping it in the landscape mode. The landscape mode gives a detailed overview of the food object in the plate, further helping us in accurately calculating the dimensions of the food object. The coordinate-system is measured with respect to the phone\u2019s screen in its default orientation. The Android accelerometer sensor is used to measure the acceleration applied to the mobile device. We hence are able to obtain the angle (radian) based on orientation of the phone, to the target food object. We convert the value of angle, originally in radian, to degrees (theta \u03b8). Based on the angle value and the height entered by the user (example 165 cm), we are able to obtain the distance of the target food object from the mobile phone.\nAfter obtaining the angle, the system calculates the distance of the food object to the phone\u2019s camera as explained in the Error! Reference source not found.. We obtained the orientation angle (the angle at which the phone is placed in space) from the accelerometer and the magnetic field sensors (as explained above), based on the Rotation matrix. Once we calculate the orientation angle, we use the height of the person (h) to measure the distance (d). The height of the person (h) as shown in Error! Reference source not found., is mentioned by the user while creating his user account. We assume that the user, in a normal scenario would capture the image from the phone\u2019s camera, of a plate which is placed on a stool, at height (h/2), i.e. half the height of the person.\n( 5 )\nIV. IMPLEMENTATION\nAs soon as the user captures the food object photo, the system measures the distance of the food object from the phones camera and records it in the database. Once he/she clicks on the submit button for calorie estimation in the EHSH app, the system sends the photo and the distance (d) information to the cloud for image processing and calorie measurement steps. During the image processing stage, deep learning methodology (as explained in the previous section) is applied for the feature extraction and classification process. Once the food object is recognized and classified, the system then performs the calorie measurement step. As shown in the Fig. 4, the first step of calorie measurement is to access the food content and its quantity, present on the plate. We do this by obtaining contour around the food object with the use of Canny edge detection [18]. Once we obtain the contour, we\nare able to dimension the food object and further obtain the closed food object in plate (in grams or milligrams) and further obtain its calorie value. One of the main challenges in this method would be the assumption that the user would always follow the instructions (like in our Android Application, the user needs to click the photo from a specific distance e.g. 50 cm). This might mitigate the interest of the usability of this application.\nHence to handle this problem, we propose a distance calibration method, which will enable the system to accurately calculate the calorie value, irrespective of the changes in the distance from which the food object is being clicked. For doing so, our primary goal was to determine a constant as a reference point, with respect to the varying distance (d). We hence, created, equal sized blocks on the image with the help of grid lines as shown in the Fig. 5. The blocks size would remain the same for all images (irrespective of the distance from which the images were clicked). Fig. 5 illustrates an image of bread when taken from a distance of 45 cm. The same bread is\ncontour area of the food object. The Arclength and contour are obtained from opencv functions [19][20], help us determine the dimension of the food object in pixels. We then performed linear regression analysis on the contour area and the perimeter (arclength) against the weight. Normally we would have used a weighing machine, to calculate the food quantity. As asking the user to weight the food object on the plate is not feasible in our scenario, we have proposed a novel idea to access the weight of the food object present on the plate and further determine the calorie content of the food object. Based on the image of a known food object, if we can somehow calculate the dimension of this food object, given a reference point, we will be surely be able to obtain a quantifiable value\nof the food object, the unit for which would not be in grams or milligrams but rather in centimeters or feet. We can then calibrate the weight of the clicked from 80 cm distance in Fig. 6. Since the block size will remain constant, the system then calculates the total number of blocks from images that was taken from varying distance. From the TABLE I. , we can determine that with the increasing distance, the total number of blocks decreased.\nThe total number of blocks for Bread was 1620, when image clicked from a distance of 25 cm. This reduced to 84 blocks when image was clicked from 90 cm. The lower value for the same food object (in this case it is Bread), would further impact the final calorie measurement, as the weight of the food object would be equated to the dimension of the food objects.\nThis should not be the case as the food object has remained the same throughout. Hence we calibrated the scale of the block with the real dimension. The real dimension of the block length (l) and width (w) were increased with the increasing distance (d). This helped us achieve a scaled image, or in other words, it helped us to recreate the image, as if it was shot from a specific distance. The graph (shown in Fig. 7) helps us determine, the block area with respect to the distance from which the photo is being taken. The graph shows an exponential growth of the real area (cm2) when the distance increases. If an image were taken from a distance of 60 cm, then the area of the blocks would be 0.328 cm2 and the dimension of the block in that case would be 0.478 cm wide and 0.687 cm long. Similarly, if an image was captured from a distance of 80 cm, then the area of the blocks would increase to 0.855 cm2 and the dimension of the block would also increase to 0.687 cm wide and 1.1 cm long.\nThus in this manner the proposed method allows us to standardize the dimensions of the target image, irrespective of the distance from which the photo is being taken. Once we calibrate the target image and resize it, we then process the image to obtain the weight of the food item. As part of processing the image, our goal was to obtain the area (in terms of pixel size) and the perimeter (also in term of pixel size) of the food portion. For acquiring the area of the food portion, we detect the edges of the food portion. We use the Canny Edge Detector [18], for calculating the edges of the food portion, further helping us to compute the closed area and the arc length (perimeter) of the food portion.\nV. EXPERIMENTAL RESULTS\nIn this section, we describe the results for the methods described above. We performed a linear regression on the area and perimeter against the real weights of the food objects. This test enables us to predict the best fitting slope or the regression line. Hence given the area of the food object, it will predict the corresponding weight (in grams) followed by the calorie measurement (Cal). Based on the tests, we were able to predict the calorie in both scenarios (with perimeter and area) as shown in Fig. 8 and Fig. 9.\nWhen we compared the calorie computed with area, we achieved the R square value or the coefficient correlation square value to be 0.9709 which was higher than the square of the coefficient correlation value of 0.8412, when obtained from the comparison of perimeter and calorie. The R square"}, {"heading": "Block Area v/s Distance Photo is clicked", "text": "value amounting to accuracy of 97.09% when the comparison was made with respect to the contour area computed from the edge.\nWe were also able to estimate the error in computing the calorie of different sized breads. From this table we were able to determine the error percentage in estimated Calorie (C\u2019\u2019) from linear regression on area is minimal when compared to estimated calorie when obtained from perimeter. From TABLE II. , we can see that in some scenario the calorie difference has been as less 0.65 when compared with the actual calorie.\nSimilar computations were performed on every food class to obtain the best fit slope for estimating the calorie. The reason for considering different base values for each food class is because the area computed of bread might be greater than area obtained from banana, but it does not necessarily corroborate the linear relation between area and calorie. Hence each food class was scaled individually with different base values for area.\nVI. CONCLUSION AND FUTURE WORK\nIn this paper, we have proposed a new method for measuring the calories of the food object. With the right combination of mobile and cloud computing we were able to obtain the estimated distance from the mobile device and used it to process the image in the cloud. This enabled us to process all the images from the same scale and further enabling us to determine the calorie value of the food object. Using deep learning we were able to extract the features of the food object and further classify it accurately. We also studied the relation between the area and perimeter of the food object against the calorie value, with area being more accurate in determining the calorie value.\nAs part of the future work, we believe the real dimensions of the food object could be obtained in real time using the mobile device. Currently we are able to obtain the distance from the mobile device in real-time. We will also analyze more food classes and determine the accuracy of this methodology. Using the concepts of distance estimation discussed in this paper, we could also obtain the width and the height of the food object, further helping us to calculate the area and perimeter of the food object in real-time. This would allow us to accurately determine the calorie of the food object."}], "references": [{"title": "Energy intake, physical activity, energy balance, and cancer: epidemiologic evidence", "author": ["SY Pan", "M. DesMeules"], "venue": "Methods Mol Biol", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Obesity and cancer.\" Experimental and clinical endocrinology & diabetes", "author": ["R. Percik", "M. Stumvoll"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "et al", "author": ["D. Craig"], "venue": "\"The Okinawan diet: health implications of a low-calorie, nutrient-dense, antioxidant-rich dietary pattern low in glycemic load.\" Journal of the American College of Nutrition 28.sup4 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Prevalence of the metabolic syndrome among US adults: findings from the third National Health and Nutrition Examination Survey", "author": ["ES Ford", "WH Giles", "WH Dietz"], "venue": "JAMA 287:356\u2013359,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Cloud-Based SVM for Food Categorization", "author": ["P. Pouladzadeh", "S. Shirmohammadi", "A. Bakirov", "A. Bulut", "Abdulsalam Yassine"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Using graph cut segmentation for food calorie measurement.", "author": ["Pouladzadeh", "Parisa", "Shervin Shirmohammadi", "Abdulsalam Yassine"], "venue": "Medical Measurements and Applications (MeMeA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Mobile cloud based food calorie measurement", "author": ["P. Pouladzadeh", "P. Kuhad", "S.V.B. Peddi", "A. Yassine", "Shirmohammadi", "July"], "venue": "In Multimedia and Expo Workshops (ICMEW),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "and T", "author": ["N. Ryu", "Y. Kawahawa"], "venue": "Asami, \"A Calorie Count Application for a Mobile Phone Based on METS Value,\" Sensor, Mesh and Ad Hoc Communications and Networks", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "The statistical recognition of walking", "author": ["E. Thammasat"], "venue": "jogging, and running using smartphone accelerometers,\u201d in Biomedical Engineering International Conference (BMEiCON), 2013 6th", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Measurement of food volume based on single 2-D image without conventional camera calibration", "author": ["Y Yue", "W Jia", "M. Sun"], "venue": "Proceedings of IEEE 34th Annual Conference on Engineering in Medicine and Biology;", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks.", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": " ldquo", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "\u201cMultimodal Learning with Deep Boltzmann Machines\u201d, Proc. Neural Information and Processing System", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "OnRectied Linear Units for Speech Processing", "author": ["M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "Vanhoucke V", "Dean J", "G.E. Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines,", "author": ["G.E. Hinton"], "venue": "Technical report 2010-003,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "A computational approach to edge detection", "author": ["John Canny"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}], "referenceMentions": [{"referenceID": 0, "context": "Due the unprecedented rise in overweightness and obesity in the world and the diseases they cause[1][2][3][4], an important issue for people to maintain a good quality of life today is to watch their daily eating routines in order to avoid excess calorie intake.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Due the unprecedented rise in overweightness and obesity in the world and the diseases they cause[1][2][3][4], an important issue for people to maintain a good quality of life today is to watch their daily eating routines in order to avoid excess calorie intake.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "Due the unprecedented rise in overweightness and obesity in the world and the diseases they cause[1][2][3][4], an important issue for people to maintain a good quality of life today is to watch their daily eating routines in order to avoid excess calorie intake.", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Due the unprecedented rise in overweightness and obesity in the world and the diseases they cause[1][2][3][4], an important issue for people to maintain a good quality of life today is to watch their daily eating routines in order to avoid excess calorie intake.", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "In our previous work [5]-[8], we have developed a mobile system that can measure the calories of the food from that food\u2019s image taken by the user\u2019s smartphone.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "A detailed review of existing calorie measurement systems was shown in [8] and will not be repeated here due to shortage of space.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "Calorie count, with the help of a mobile phone was shown in [9].", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "The work in [10] also makes use of the mobile sensors to detect the human activities like walking, jogging and running.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "In [11], the authors measure the dimension of the food portions using 2D image based on the assumption of known dimensions of the circular plate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "We were hence able to achieve higher accuracy results during the classification stage as compared to our previous approaches (such as Support Vector Machine) [8].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "The libraries and the model files of the deep neural network here are based on the work of Hinton [12] and Srivastava [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "The libraries and the model files of the deep neural network here are based on the work of Hinton [12] and Srivastava [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "For example, our application computes the probability, p, of the match of the input image with the corresponding label set [12]:", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "The standard way to model a neuron\u2019s output f as an activation function of its input x is either a hyperbolic or sigmoid function [12]:", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "But we use the term rectified linear unit (ReLU) to refer to unit in a neural net that use the activation function max(0;x) [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "As compared to the above mentioned activation functions (hyperbolic or sigmoid), the training of deep convolutional neural networks with ReLUs is relatively faster [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs [16], and training with those, where the weights and bias is computed by:", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "The back-propagation algorithm, is the fast way of computing the gradient of the cost function [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "We do this by obtaining contour around the food object with the use of Canny edge detection [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "We use the Canny Edge Detector [18], for calculating the edges of the food portion, further helping us to compute the closed area and the arc length (perimeter) of the food portion.", "startOffset": 31, "endOffset": 35}], "year": 2015, "abstractText": "High calorie intake in the human body on the one hand, has proved harmful in numerous occasions leading to several diseases and on the other hand, a standard amount of calorie intake has been deemed essential by dieticians to maintain the right balance of calorie content in human body. As such, researchers have proposed a variety of automatic tools and systems to assist users measure their calorie in-take. In this paper, we consider the category of those tools that use image processing to recognize the food, and we propose a method for fully automatic and user-friendly calibration of the dimension of the food portion sizes, which is needed in order to measure food portion weight and its ensuing amount of calories. Experimental results show that our method, which uses deep learning, mobile cloud computing, distance estimation and size calibration inside a mobile device, leads to an accuracy improvement to 95% on average compared to previous work. Keywords\u2014 Calorie Measurement; Deep Learning; Distance Estimation; Mobile Cloud Computing.", "creator": "PDFCreator Version 1.6.2"}}}