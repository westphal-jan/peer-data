{"id": "1502.03409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Large-Scale Deep Learning on the YFCC100M Dataset", "abstract": "we present a strong work - in - progress snapshot of learning processes with evaluating a 15 ~ billion pixels parameter collaborative deep learning network on 28 hpc architectures applied positively to the 23rd largest publicly made available human natural image raw and video dataset released to - today date. recent advancements underway in providing unsupervised applied deep model neural networks technique suggest that scaling \u2011 up such communication networks in both model and training dataset size can yield significant educational improvements in mastering the lifetime learning of learned concepts produced at the highest layers. here we essentially train our three - dimension layer deep memory neural network on the new yahoo! flickr search creative entertainment commons research 100m prototype dataset. computing the main dataset site comprises approximately 99. 2 quarter million internal images rendered and 800, 05 000 user - created videos from yahoo's flickr image and discovery video record sharing platform. peak training of adopting our true network concepts takes eight days on 98 gpu nodes at the high performance computing center suite at at lawrence livermore national laboratory. encouraging preliminary results and future research directions are as presented before and discussed.", "histories": [["v1", "Wed, 11 Feb 2015 19:24:36 GMT  (2202kb,D)", "http://arxiv.org/abs/1502.03409v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["karl ni", "roger pearce", "kofi boakye", "brian van essen", "damian borth", "barry chen", "eric wang"], "accepted": false, "id": "1502.03409"}, "pdf": {"name": "1502.03409.pdf", "metadata": {"source": "CRF", "title": "LARGE-SCALE DEEP LEARNING ON THE YFCC100M DATASET", "authors": ["Karl Ni", "Roger Pearce", "Eric Wang", "Kofi Boakye", "Brian Van Essen", "Damian Borth", "Barry Chen", "Lawrence Livernmore"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Deep Learning, Autoencoders, High Performance Computing"}, {"heading": "1. INTRODUCTION", "text": "The field of deep learning via stacked neural networks has received renewed interest in the last decade [1, 2, 3]. Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].\nDeep neural networks applied to natural images have demonstrated state-of-the-art performance in supervised object recognition tasks [10, 1] as well as unsupervised neural networks [2, 3]. The classical approach to training neural networks for computer vision is via a large dataset of labeled data. However, sufficiently large and accurately labeled data is difficult and expensive to acquire. Motivated by this, [3] explored the application of deep neural networks in unsupervised deep learning and discovered that sufficiently large deep networks are capable of learning highly complex concept level features at the top level without labels.\nSpurred by this advancement, [2] set out to construct very large networks on the order of 109 to 1010 parameters. A key advancement was the highly efficient multi-GPU architecture of their model. [2] employed a high degree of model parallelism and was able to process 10 million YouTube thumbnails in a few days processing time on a medium sized cluster. A notable result was the unsupervised learning of various faces, including those of humans and cats. Ultimately, improved feature learning at larger scales can improve\nThis work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory in part under Contract W-7405-Eng-48 and in part under Contract DE-AC52-07NA27344.\ndownstream capabilities such as scene or object classification, additional unsupervised learning (i.e. via topic modeling [11] or natural language processing algorithms [12]).\nIn collaboration with the authors of [2], we have scaled a similar model and architecture to over 15 billion parameters on the Lawrence Livermore National Laboratory\u2019s (LLNL) Edge High Performance Computing (HPC) system. Our long-term goal is two-fold: (1) explore at-the-limit performance of massive networks (> 10 billion parameters) and (2) train on and analyze datasets on the order of 100 million images.\nAs the number of network parameters grow, datasets need to be scaled accordingly to avoid overfitting the models. We take advantage of a brand-new dataset released jointly by Yahoo!, LLNL and the International Computer Science Institute (ICSI) called the Yahoo! Flickr Creative Commons 100M (YFCC100M) dataset10. The dataset is, to the authors\u2019 knowledge, the largest single publicly available image and video dataset ever published. In addition to the raw images and video, the YFCC100M also contains metadata for each entry including locations, camera types, keywords, titles, etc. Although beyond the scope of this paper, this rich associated metadata potentially offers researchers additional avenues of semantic multi-modality learning to explore.\nWorking with the large-scale datasets, models and computing architectures considered in this paper presents several daunting engineering challenges. For example, the significantly greater number of GPUs and compute nodes used in our system versus [2] creates communication issues in MPI. In addition, a typical model takes up over 40 GB of memory, making simple offline analysis tasks such as visualization challenging. Various network architectures were tested, balancing performance and computational constraints, before we arrived at our current model. Finally, as in [2], data throughput presents a bottleneck to model training. We present a novel pipeline approach to address this problem.\nThe rest of this paper is organized as follows. In Section 2 we give a brief overview of the YFCC100M dataset. The network architecture and computational framework being employed is described in Section 3. We present preliminary results and visualizations of our network in Section 4. Finally, we summarize and discuss future research directions in Section 5."}, {"heading": "2. OVERVIEW OF THE YFCC100M DATASET", "text": "In late June 2014, Yahoo! released the Yahoo! Flickr Creative Commons dataset (YFCC100M). This dataset consists of 100 million Flickr user-uploaded images and videos (99,206,564 images and 793,436 videos) along with their corresponding metadata including title, description, camera type, tags, and geotags when available. All of the data is under Creative Commons licensing and is freely pro-\nar X\niv :1\n50 2.\n03 40\n9v 1\n[ cs\n.L G\n] 1\n1 Fe\nb 20\n15\nvided to scientists for the advancement of multimedia research 1. In addition to the raw images, videos, and metadata, Yahoo! in collaboration with the ICSI and LLNL will be computing and providing standard computer vision and audio features using LLNL\u2019s supercomputing resources.\nWang et al. [13] have used YFCC100M data to build systems that associate images with more natural annotations like those found in user-generated captions. Others are interested in using the YFCC100M imagery and audio to geolocate where the photo or video was taken [14]. In fact, the 2014 MediaEval Placing Task is using YFCC100M as the source of benchmark data [15]. We are interested in using YFCC100M as our sandbox dataset for learning image features using massive unsupervised neural networks, repeating the experiment by [3] on an order of magnitude more data and neural network parameters. In particular, we want to see what other \u201cgrandmother neurons\u201d [3] our network would automatically learn from YFCC100M.\nThe 99,206,564 images were created and posted by 578,268 different Flickr users. 76%, 20%, and 4% of the images have titles, auto-titles, or no titles, respectively. The average number of words per title is 3.08. 32% of the images have descriptions with an average of 22.52 words per description. Finally, 69% of the images have on average 7.07 tags per image. The top 60 tags are shown in Table 1. In Fig. 1 we show example images and associated meta-data for several YFCC100M images."}, {"heading": "3. ANALYSIS WITH LARGE SCALE NEURAL NETWORKS", "text": ""}, {"heading": "3.1. Network Architecture", "text": "For the large set of image data, we employed a three-layer, largescale deep neural network with a reconstruction independent component analysis (RICA) cost function,\nmin W,\u03b1,b \u2211 i \u2225\u2225\u2225WT (\u03b1Wx(i)) + b\u2212 x(i)\u2225\u2225\u22252 2 + \u03bb \u221a (\u03b1Wx(i))2\nsubject to \u2016W (k)\u20162 = 1, \u2200k,\nwhere as in [2],W is a weighting matrix, \u03b1 is a scaling value and x(i) are the data points at the beginning of each layer. In addition, we introduce an offset, b, for increased model flexibility. The parameter \u03bb controls the relative sparsity, and is set to 0.1 at the first two layers and 0.01 at the final layer. Unlike [2], we do not presently include a pooling layer, as we believe the scale of the network and training data allows a similar translational invariance to be automatically learned. A particular advantage conferred by the RICA construction is that the sparseness term \u03bb \u221a (\u03b1Wx(i))2 can be computed in-situ with the rest of the model parameters. This is in contrast to the conventional\n1Available at http://research.yahoo.com/Academic Relations\nsparse autoencoder construction that requires a second pass through the data to compute a sparseness-specific gradient contribution.\nFig. 2 illustrates the structure of our network. The three layers are composed of two untied convolutional layers, and a third fullyconnected layer. The first convolutional layer utilizes 5184 filters 2 of input size 16 \u00d7 16 \u00d7 3 with stride 4 and output size 4 \u00d7 4 \u00d7 24. The second layer takes 16 spatially contiguous 3 4\u00d7 4\u00d7 24 outputs of the first layer and connects them fully to a 4 \u00d7 4 \u00d7 24 output. The stride length of the second layer is 4. The third layer is dense, and fully connects the 62 \u00d7 62 \u00d7 24 outputs of the second layer to 4096 top-level neurons. The total number of parameters trained is 15 billion. After each layer, local contrast normalization (LCN) is applied prior to continuing onto the next layer. Though no pooling is applied, the window sizes at the next layer are large enough to incorporate spatial information from neighboring blocks.\nTraining data is arranged into 99,207 data blocks of 960 images. Each data block consists of 5 mini-batches, where each mini-batch contains 192 images. Due to the scale of the data, the proposed algorithm reduces training time by employing a pipeline technique where the next layer begins training before the previous layer has finished. Analogous to the example shown in Fig. 3, after a layer L has trained an initial set of data blocks (in our case, 1000), the next layer, L + 1, starts training. To accomplish this, two instances of the layer L are run simultaneously: one which continues training and one that uses up-to-date parameters to forward propagate data from Block 0 to the layer L + 1. The parameters of the forwardpropagating layer L instance are periodically synchronized with the layerL instance that continued training. We observed that our model was not sensitive to the choice of synchronization frequency. As a\n2Arranged in a 72\u00d7 72 grid 3Arranged in a 4\u00d7 4 grid\nrule of thumb, we wait to train layer L+1 until the objective of layer L stabilizes, which typically occurs after approximately one million images."}, {"heading": "3.2. HPC Architecture", "text": "To train the neural network at scale, we used 98 nodes of the Edge HPC cluster at Lawrence Livermore National Laboratory. The Edge cluster consists of 206 nodes with 12 core Intel Xeon EP X5660 running at 2.8 GHz. Each node has 96 GB of DRAM and a Tesla M2050 (Fermi) NVIDIA GPU with 3 GB of GDDR5. The training algorithm is model parallel as described in [2], with the nodes and GPUs processing each mini-batch across the system and distributing the model across the GPUs. Communication was provided by MPI over Mellanox QDR Infiniband cards. The GPU accelerators were used with CUDA 5.5 and MPI-direct communication and the operating system was a 2.6.32 kernel RHEL 6 derivative.\nThe dataset was stored in a Lustre file system with a peak bandwidth of 10 GB/s. Each mini-batch was copied from Lustre into memory and then streamed into the GPU\u2019s memory. Each GPU is responsible for computing its section of the model parameters for the current mini-batch. Communication within the algorithm occurs when a layer\u2019s input (or output) field spans multiple GPUs. The\ncommunication is handled by a distributed array data structure (using MPI) within the training algorithm. Global communication is minimized by using untied local receptive fields, and allowing receptive fields to be trained independently."}, {"heading": "4. PRELIMINARY RESULTS", "text": "We trained the network using all images from the YFCC100M dataset. Images were preprocessed as in [2], and subsequently resized to 300 x 300 pixels by first centering, then scaling the smallest dimension to 300 pixels, and finally cropping. After training all three layers, we forward propagated 2 million images through the network in order to obtain activation values for visualization. Note that in this\npaper, the test set is significantly noisier than the benchmark Labeled Faces In the Wild [16] and ImageNet [17] datasets considered in previous works such as [3].\nIn Fig. 5, we show the top 5 stimuli for some example neurons. We observe that our network is capable of learning significant structure, identifying buildings, aircraft, text, cityscapes, and tower-like buildings, among many others. The network seems to cue in on distinctive textures such as the edges of text, sides of buildings and the sharp edge of airplanes against the smooth gradation of the sky. Moreover, the network seems to activate on large-scale structures within an image rather than local features. We believe that a significant contributor to our networks\u2019 performance is due to its large size being able to capture complex concepts.\nWhile our results are encouraging, we believe that significant improvements in learning can be achieved through improved network architecture and increased depth. As was demonstrated in [1], network architecture has a significant impact on the performance of deep networks. While the networks described in [3] were able to learn complex features in just three layers, our results suggest that extremely large datasets such as the YFCC100M can support (and possibly benefit from) deeper networks with improved high-level concept learning."}, {"heading": "5. SUMMARY AND FUTURE WORK", "text": "The results discussed in this paper present a snapshot of the work in progress at Lawrence Livermore National Laboratory in scaling up deep neural networks. Such networks offer enormous potential to researchers in both supervised and unsupervised computer vision tasks, from object recognition and classification to unsupervised feature extraction.\nTo date, we see highly encouraging results from training our large 15 billion parameter three-layer neural network on the YFCC100M dataset in an unsupervised manner. The results suggest that the network is capable of learning highly complex concepts such as cityscapes, aircraft, buildings, and text, all without labels or other guidance. That this structure is visible upon examination is made all the more remarkable due to the noisiness of our test set (taken at random from the YFCC100M dataset itself).\nFuture work on our networks will focus on two main thrusts: (1) improve the high-level concept learning by increasing the depth of our network, and (2) scaling our network\u2019s width in the middle layers. On the first thrust, we aim for improved high-level summarization and scene understanding. Challenges on this front include careful tuning of parameters to combat the \u201cvanishing gradient\u201d problem and design of the connectivity structure of the higher-level layers to maximize learning. On the second thrust, our challenges are primarily engineering focused. Memory and message passing constraints become a serious concern, even on the large HPC systems fielded by LLNL. As we move beyond our current large neural network, we plan to explore the use of memory hierarchies for staging intermediate/input data to minimize the amount of node-to-node communication, enabling the efficient training and analysis of even larger networks."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "We would like to thank Adam Coates, Brody Huval and Andrew Ng for providing their COTS HPC Deep Learning software and helpful advice. This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344."}, {"heading": "7. REFERENCES", "text": "[1] A. Krizhevsky, I. Sutskever, and G. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems, 2012.\n[2] A. Coates, B. Huval, T. Wang, D. J. Wu, A. Y. Ng, and B. Catanzaro., \u201cDeep learning with cots hpc,\u201d in International Conference on Machine Learning, 2013.\n[3] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng, \u201cBuilding high-level features using large scale unsupervised learning,\u201d in International Conference on Machine Learning, 2012.\n[4] T. Mikolov, K. Chen, G. Corrado, and J. Dean., \u201cEfficient estimation of word representations in vector space,\u201d in Proceedings of Workshop at ICLR, 2013.\n[5] O. Abdel-Hamid, L. Deng, and D. Yu, \u201cExploring convolutional neural network structures and optimization techniques for speech recognition,\u201d in Interspeech 2013, 2013.\n[6] G. Hinton, L. Deng, D. Yu, A.-R. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, G. Dahl, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.\n[7] H. Bourlard and N. Morgan, Connectionist Speech Recognition: A Hybrid Approach, Kluwer Academic Publishers, 1993.\n[8] D. Claudiu Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber, \u201cConvolutional neural network committees for handwritten character classification,\u201d in International Conference on Document Analysis and Recognition, 2011.\n[9] D. Reby, S. Lek, I. Dimopoulos, J. Joachim, J. Lauga, and S. Aulagnier, \u201cArtificial neural networks as a classification method in the behavioural sciences,\u201d Behavioural Processes, vol. 40, pp. 3543, 1997.\n[10] R Uetz and S. Behnke, \u201cLarge-scale object recognition with cuda-accelerated hierarchical neural networks,\u201d in IEEE International Conference on Intelligent Computing and Intelligent Systems, 2009.\n[11] L. Cao and L. Fei-Fei, \u201cSpatially coherent latent topic model for concurrent object segmentation and classification,\u201d in Proceedings of International Conference on Computer vision, 2007.\n[12] R. Socher and M. Ganjoo and C. D. Manning and A. Y. Ng, \u201cZero Shot Learning Through Cross-Modal Transfer,\u201d in Advances in Neural Information Processing Systems 26. 2013.\n[13] J. K. Wang, F. Yan, A. Aker, and R. Gaizauskas, \u201cA poodle or a dog? Evaluating automatic image annotation using human descriptions at different levels of granularity,\u201d in Proceedings of the Workshop on Vision and Language, 2014.\n[14] James Hays and Alexei A. Efros, \u201cim2gps: estimating geographic information from a single image,\u201d in Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2008.\n[15] J. Choi, B. Thomee, G. Friedland, L. Cao, K. Ni, D. Borth, B. Elizalde, L. Gottlieb, C. Carrano, R. Pearce, D. Poland, \u201c The Placing Task: A Large Scale Geo-Estimation Challenge for Social-Media Videos and Images,\u201d 3rd ACM Multimedia Workshop On GeoTagging and Its Applications in Multimedia.\n[16] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, \u201cLabeled faces in the wild: A database for studying face recognition in unconstrained environments,\u201d .\n[17] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in In CVPR, 2009."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in neural information processing systems, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning with cots hpc", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro."], "venue": "International Conference on Machine Learning, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "International Conference on Machine Learning, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean."], "venue": "Proceedings of Workshop at ICLR, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring convolutional neural network structures and optimization techniques for speech recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "Interspeech 2013, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "A.-R. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "G. Dahl", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist Speech Recognition: A Hybrid Approach", "author": ["H. Bourlard", "N. Morgan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Convolutional neural network committees for handwritten character classification", "author": ["D. Claudiu Ciresan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "International Conference on Document Analysis and Recognition, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Artificial neural networks as a classification method in the behavioural sciences", "author": ["D. Reby", "S. Lek", "I. Dimopoulos", "J. Joachim", "J. Lauga", "S. Aulagnier"], "venue": "Behavioural Processes, vol. 40, pp. 3543, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Large-scale object recognition with cuda-accelerated hierarchical neural networks", "author": ["R Uetz", "S. Behnke"], "venue": "IEEE International Conference on Intelligent Computing and Intelligent Systems, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Spatially coherent latent topic model for concurrent object segmentation and classification", "author": ["L. Cao", "L. Fei-Fei"], "venue": "Proceedings of International Conference on Computer vision, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Zero Shot Learning Through Cross-Modal Transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems 26. 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A poodle or a dog? Evaluating automatic image annotation using human descriptions at different levels of granularity", "author": ["J.K. Wang", "F. Yan", "A. Aker", "R. Gaizauskas"], "venue": "Proceedings of the Workshop on Vision and Language, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "im2gps: estimating geographic information from a single image", "author": ["James Hays", "Alexei A. Efros"], "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "The Placing Task: A Large Scale Geo-Estimation Challenge for Social-Media Videos and Images", "author": ["J. Choi", "B. Thomee", "G. Friedland", "L. Cao", "K. Ni", "D. Borth", "B. Elizalde", "L. Gottlieb", "C. Carrano", "R. Pearce", "D. Poland"], "venue": "3rd ACM Multimedia Workshop On GeoTagging and Its Applications in Multimedia.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 0}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": ".", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-fei"], "venue": "In CVPR, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The field of deep learning via stacked neural networks has received renewed interest in the last decade [1, 2, 3].", "startOffset": 104, "endOffset": 113}, {"referenceID": 1, "context": "The field of deep learning via stacked neural networks has received renewed interest in the last decade [1, 2, 3].", "startOffset": 104, "endOffset": 113}, {"referenceID": 2, "context": "The field of deep learning via stacked neural networks has received renewed interest in the last decade [1, 2, 3].", "startOffset": 104, "endOffset": 113}, {"referenceID": 3, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 124, "endOffset": 133}, {"referenceID": 5, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 124, "endOffset": 133}, {"referenceID": 6, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 124, "endOffset": 133}, {"referenceID": 7, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 164, "endOffset": 170}, {"referenceID": 8, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 164, "endOffset": 170}, {"referenceID": 0, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 253, "endOffset": 262}, {"referenceID": 1, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 253, "endOffset": 262}, {"referenceID": 2, "context": "Neural networks have been shown to perform well in a wide variety of tasks, including text analysis [4], speech recognition [5, 6, 7], various classification tasks [8, 9], and most notably unsupervised and supervised feature learning on natural imagery [1, 2, 3].", "startOffset": 253, "endOffset": 262}, {"referenceID": 9, "context": "Deep neural networks applied to natural images have demonstrated state-of-the-art performance in supervised object recognition tasks [10, 1] as well as unsupervised neural networks [2, 3].", "startOffset": 133, "endOffset": 140}, {"referenceID": 0, "context": "Deep neural networks applied to natural images have demonstrated state-of-the-art performance in supervised object recognition tasks [10, 1] as well as unsupervised neural networks [2, 3].", "startOffset": 133, "endOffset": 140}, {"referenceID": 1, "context": "Deep neural networks applied to natural images have demonstrated state-of-the-art performance in supervised object recognition tasks [10, 1] as well as unsupervised neural networks [2, 3].", "startOffset": 181, "endOffset": 187}, {"referenceID": 2, "context": "Deep neural networks applied to natural images have demonstrated state-of-the-art performance in supervised object recognition tasks [10, 1] as well as unsupervised neural networks [2, 3].", "startOffset": 181, "endOffset": 187}, {"referenceID": 2, "context": "Motivated by this, [3] explored the application of deep neural networks in unsupervised deep learning and discovered that sufficiently large deep networks are capable of learning highly complex concept level features at the top level without labels.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Spurred by this advancement, [2] set out to construct very large networks on the order of 10 to 10 parameters.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "[2] employed a high degree of model parallelism and was able to process 10 million YouTube thumbnails in a few days processing time on a medium sized cluster.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "via topic modeling [11] or natural language processing algorithms [12]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "via topic modeling [11] or natural language processing algorithms [12]).", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "In collaboration with the authors of [2], we have scaled a similar model and architecture to over 15 billion parameters on the Lawrence Livermore National Laboratory\u2019s (LLNL) Edge High Performance Computing (HPC) system.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "For example, the significantly greater number of GPUs and compute nodes used in our system versus [2] creates communication issues in MPI.", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "Finally, as in [2], data throughput presents a bottleneck to model training.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "[13] have used YFCC100M data to build systems that associate images with more natural annotations like those found in user-generated captions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Others are interested in using the YFCC100M imagery and audio to geolocate where the photo or video was taken [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "In fact, the 2014 MediaEval Placing Task is using YFCC100M as the source of benchmark data [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "We are interested in using YFCC100M as our sandbox dataset for learning image features using massive unsupervised neural networks, repeating the experiment by [3] on an order of magnitude more data and neural network parameters.", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "In particular, we want to see what other \u201cgrandmother neurons\u201d [3] our network would automatically learn from YFCC100M.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "where as in [2],W is a weighting matrix, \u03b1 is a scaling value and x are the data points at the beginning of each layer.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "Unlike [2], we do not presently include a pooling layer, as we believe the scale of the network and training data allows a similar translational invariance to be automatically learned.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "The training algorithm is model parallel as described in [2], with the nodes and GPUs processing each mini-batch across the system and distributing the model across the GPUs.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "Images were preprocessed as in [2], and subsequently resized to 300 x 300 pixels by first centering, then scaling the smallest dimension to 300 pixels, and finally cropping.", "startOffset": 31, "endOffset": 34}, {"referenceID": 15, "context": "Note that in this paper, the test set is significantly noisier than the benchmark Labeled Faces In the Wild [16] and ImageNet [17] datasets considered in previous works such as [3].", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "Note that in this paper, the test set is significantly noisier than the benchmark Labeled Faces In the Wild [16] and ImageNet [17] datasets considered in previous works such as [3].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Note that in this paper, the test set is significantly noisier than the benchmark Labeled Faces In the Wild [16] and ImageNet [17] datasets considered in previous works such as [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 0, "context": "As was demonstrated in [1], network architecture has a significant impact on the performance of deep networks.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "While the networks described in [3] were able to learn complex features in just three layers, our results suggest that extremely large datasets such as the YFCC100M can support (and possibly benefit from) deeper networks with improved high-level concept learning.", "startOffset": 32, "endOffset": 35}], "year": 2015, "abstractText": "We present a work-in-progress snapshot of learning with a 15 billion parameter deep learning network on HPC architectures applied to the largest publicly available natural image and video dataset released to-date. Recent advancements in unsupervised deep neural networks suggest that scaling up such networks in both model and training dataset size can yield significant improvements in the learning of concepts at the highest layers. We train our three-layer deep neural network on the Yahoo! Flickr Creative Commons 100M dataset. The dataset comprises approximately 99.2 million images and 800, 000 user-created videos from Yahoo\u2019s Flickr image and video sharing platform. Training of our network takes eight days on 98 GPU nodes at the High Performance Computing Center at Lawrence Livermore National Laboratory. Encouraging preliminary results and future research directions are presented and discussed.", "creator": "LaTeX with hyperref package"}}}