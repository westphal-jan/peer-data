{"id": "1502.04623", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "DRAW: A Recurrent Neural Network For Image Generation", "abstract": "together this paper introduces the deep neural recurrent attentive writer ( semantic draw ) computational neural network network architecture for image generation. \u2022 draw networks combine a novel spatial multiple attention mechanism that loosely mimics from the foveation scheme of primarily the human transparent eye, with a comprehensive sequential data variational auto - encoding design framework that allows for the easy iterative construction of complex pictorial images. the system integration substantially improves how on in the state is of the art for mounting generative models on mnist, map and, when trained on both the street view house catalogue numbers online dataset, gradually it generates images profiles that normally cannot be distinguished from their real data synchronized with the virtual naked eye.", "histories": [["v1", "Mon, 16 Feb 2015 16:48:56 GMT  (2705kb,D)", "http://arxiv.org/abs/1502.04623v1", null], ["v2", "Wed, 20 May 2015 15:29:42 GMT  (4693kb,D)", "http://arxiv.org/abs/1502.04623v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["karol gregor", "ivo danihelka", "alex graves", "danilo jimenez rezende", "daan wierstra"], "accepted": true, "id": "1502.04623"}, "pdf": {"name": "1502.04623.pdf", "metadata": {"source": "META", "title": "DRAW: A Recurrent Neural Network For Image Generation", "authors": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "emails": ["KAROLG@GOOGLE.COM", "DANIHELKA@GOOGLE.COM", "GRAVESA@GOOGLE.COM", "WIERSTRA@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "A person asked to draw, paint or otherwise recreate a visual scene will naturally do so in a sequential, iterative fashion, reassessing their handiwork after each modification. Rough outlines are gradually replaced by precise forms, lines are sharpened, darkened or erased, shapes are altered, and the final picture emerges. Most approaches to automatic image generation, however, aim to generate entire scenes at once. In the context of generative neural networks, this typically means that all the pixels are conditioned on a single latent distribution (Dayan et al., 1995; Hinton & Salakhutdinov, 2006; Larochelle & Murray, 2011). As well as precluding the possibility of iterative self-correction, the \u201cone shot\u201d approach is fundamentally difficult to scale to large images. The Deep Recurrent Attentive Writer (DRAW) architecture represents a shift towards a more natural form of image construction, in which parts of a scene are created independently from others, and approximate sketches are successively refined.\nThe core of the DRAW architecture is a pair of recurrent neural networks: an encoder network that compresses the real images presented during training, and a decoder that reconstitutes images after receiving codes. The combined system is trained end-to-end with stochastic gradient de-\nscent, where the loss function is a variational upper bound on the log-likelihood of the data. It therefore belongs to the family of variational auto-encoders, a recently emerged hybrid of deep learning and variational inference that has led to significant advances in generative modelling (Gregor et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014; Salimans et al., 2014). Where DRAW differs from its siblings is that, rather than generating images in a single pass, it iteratively constructs scenes through an accumulation of modifications emitted by the decoder, each of which is observed by the encoder.\nAn obvious correlate of generating images step by step is the ability to selectively attend to parts of the scene while ignoring others. A wealth of results in the past few years suggest that visual structure can be better captured by a se-\nar X\niv :1\n50 2.\n04 62\n3v 1\n[ cs\n.C V\n] 1\n6 Fe\nb 20\nquence of partial glimpses, or foveations, than by a single sweep through the entire image (Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014). The main challenge faced by sequential attention models is learning where to look, which can be addressed with reinforcement learning techniques such as policy gradients (Mnih et al., 2014). The attention model in DRAW, however, is fully differentiable, making it possible to train with standard backpropagation. In this sense it resembles the selective read and write operations developed for the Neural Turing Machine (Graves et al., 2014).\nThe following section defines the DRAW architecture, along with the loss function used for training and the procedure for image generation. Section 3 presents the selective attention model and shows how it is applied to reading and modifying images. Section 4 provides experimental results on the MNIST, Street View House Numbers and CIFAR-10 datasets, with examples of generated images; and concluding remarks are given in Section 5. Lastly, we would like to direct the reader to the video accompanying this paper (https://www.youtube. com/watch?v=Zt-7MI9eKEo) which contains examples of DRAW networks reading and generating images."}, {"heading": "2. The DRAW Network", "text": "The basic structure of a DRAW network is similar to that of other variational auto-encoders: an encoder network determines a distribution over latent codes that capture salient information about the input data; a decoder network receives samples from the code distribuion and uses them to condition its own distribution over images. However there are three key differences. Firstly, both the encoder and decoder are recurrent networks in DRAW, so that a sequence of code samples is exchanged between them; moreover the encoder is privy to the decoder\u2019s previous outputs, allowing it to tailor the codes it sends according to the decoder\u2019s behaviour so far. Secondly, the decoder\u2019s outputs are successively added to the distribution that will ultimately generate the data, as opposed to emitting this distribution in a single step. And thirdly, a dynamically updated attention mechanism is used to restrict both the input region observed by the encoder, and the output region modified by the decoder. In simple terms, the network decides at each timestep \u201cwhere to read\u201d and \u201cwhere to write\u201d as well as \u201cwhat to write\u201d. The architecture is sketched in Fig. 2, alongside a conventional, feedforward variational auto-encoder."}, {"heading": "2.1. Network Architecture", "text": "Let RNN enc be the function enacted by the encoder network at a single time-step. The output of RNN enc at time\nx\nx\nx\nt is the encoder hidden vector henct . Similarly the output of the decoder RNN dec at t is the hidden vector hdect . In general the encoder and decoder may be implemented by any recurrent neural network. In our experiments we use the Long Short-Term Memory architecture (LSTM; Hochreiter & Schmidhuber (1997)) for both, in the extended form with forget gates (Gers et al., 2000). We favour LSTM due to its proven track record for handling long-range dependencies in real sequential data (Graves, 2013; Sutskever et al., 2014). Throughout the paper, we use the notation b = L(a) to denote a linear weight matrix from the vector a to the vector b.\nAt each time-step t, the encoder receives input from both the image x and from the previous decoder hidden vector hdect\u22121. The precise form of the encoder input depends on a read operation, which will be defined in the next section. The output henct of the encoder is used to parameterise a distribution Q(Zt|henct ) over the latent vector zt. In our experiments the latent distribution is a diagonal Gaussian N (Zt|\u00b5t, \u03c3t):\n\u00b5t = L(h enc t ) (1) \u03c3t = exp (L(h enc t )) (2)\nBernoulli distributions are more common than Gaussians\nfor latent variables in auto-encoders (Dayan et al., 1995; Gregor et al., 2014); however a great advantage of Gaussian latents is that the gradient of a function of the samples with respect to the distribution parameters can be easily obtained using the so-called reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014). This makes it straightforward to back-propagate unbiased, low variance stochastic gradients of the loss function through the latent distribution.\nAt each time-step a sample zt \u223c Q(Zt|henct ) drawn from the latent distribution is passed as input to the decoder. The output hdect of the decoder is added (via a write operation, defined in the sequel) to a cumulative canvas matrix ct, which is ultimately used to reconstruct the image. The total number of time-steps T consumed by the network before performing the reconstruction is a free parameter that must be specified in advance.\nFor each image x presented to the network, c0, henc0 , h dec 0 are initialised to learned biases, and the DRAW network iteratively computes the following equations for t = 1 . . . , T :\nx\u0302t = x\u2212 \u03c3(ct\u22121) (3) rt = read(xt, x\u0302t, h dec t\u22121) (4)\nhenct = RNN enc(henct\u22121, [rt, h dec t\u22121]) (5)\nzt \u223c Q(Zt|henct ) (6) hdect = RNN dec(hdect\u22121, zt) (7)\nct = ct\u22121 + write(h dec t ) (8)\nwhere x\u0302t is the error image, [v, w] is the concatenation of vectors v and w into a single vector, and \u03c3 denotes the logistic sigmoid function: \u03c3(x) = 11+exp(\u2212x) . Note that henct , and hence Q(Zt|henct ), depends on both x and the history z1:t\u22121 of previous latent samples. We will sometimes make this dependency explicit by writing Q(Zt|x, z1:t\u22121), as shown in Fig. 2."}, {"heading": "2.2. Loss Function", "text": "The final canvas matrix cT is used to parameterise a model D(X|cT ) of the input data. If the input is binary, the natural choice for D is a Bernoulli distribution with means given by \u03c3(cT ). The reconstruction loss Lx is defined as the negative log probability of x under D:\nLx = \u2212 logD(x|cT ) (9)\nThe latent loss Lz for a sequence of latent distributions Q(Zt|henct ) is defined as the summed Kullback-Leibler divergence of some latent prior P (Zt) from Q(Zt|henct ):\nLz = T\u2211\nt=1\nKL ( Q(Zt|henct )||P (Zt) ) (10)\nNote that this loss depends upon the latent samples zt drawn from Q(Zt|henct ), which depend in turn on the input x. If the latent distribution is a diagonal Gaussian with \u00b5t, \u03c3t as defined in Eqs 1 and 2, a simple choice for P (Zt) is a standard Gaussian with mean zero and standard deviation one, in which case Eq. 10 becomes\nLz = 1 2\n( T\u2211\nt=1\n\u00b52t + \u03c3 2 t \u2212 log \u03c32t\n) \u2212 T/2 (11)\nThe total loss L for the network is the expectation over the latent samples of the sum of the reconstruction and latent losses: L = \u3008Lx + Lz\u3009z\u223cQ (12) which we optimise using a single sample of z for each stochastic gradient descent step.\nLz can be interpreted as the number of nats required to transmit the latent sample sequence z1:T to the decoder from the prior, and (if x is discrete) Lx is the number of nats required for the decoder to reconstruct x given z1:T . The total loss is therefore equivalent to the expected compression of the data by the decoder and prior."}, {"heading": "2.3. Stochastic Data Generation", "text": "An image x\u0303 can be generated by a DRAW network by iteratively picking latent samples z\u0303t from the prior P , then running the decoder to update the canvas matrix c\u0303t. After T repetitions of this process the generated image is a sample from D(X|c\u0303T ):\nz\u0303t \u223c P (Zt) (13) h\u0303dect = RNN dec(h\u0303dect\u22121, z\u0303t) (14)\nc\u0303t = c\u0303t\u22121 + write(h\u0303 dec t ) (15)\nx\u0303 \u223c D(X|c\u0303T ) (16) Note that the encoder is not involved in image generation."}, {"heading": "3. Read and Write Operations", "text": "The DRAW network described in the previous section is not complete until the read and write operations in Eqs. 4 and 8 have been defined. This section describes two ways to do so, one with selective attention and one without."}, {"heading": "3.1. Reading and Writing Without Attention", "text": "In the simplest instantiation of DRAW the entire input image is passed to the encoder at every timestep, and the decoder modifies the entire canvas matrix at every timestep. In this case the read and write operations reduce to\nread(x, x\u0302t, h dec t\u22121) = [x, x\u0302t] (17)\nwrite(hdect ) = L(h dec t ) (18)\nHowever this approach does not allow the encoder to focus on only part of the input when creating the latent distribution; nor does it allow the decoder to modify only a part of the canvas vector. In other words it does not provide the network with a selective attention mechanism, which we believe to be crucial to large scale image generation. We refer to the above configuration as \u201cDRAW without attention\u201d."}, {"heading": "3.2. Selective Attention Model", "text": "To endow the network with selective attention, without sacrificing the benefits of gradient descent training, we take inspiration from the differentiable attention mechanisms recently used in handwriting synthesis (Graves, 2013) and Neural Turing Machines (Graves et al., 2014). Unlike the aforementioned works, we consider an explicitly twodimensional form of attention, where an array of 2D Gaussian filters is applied to the image, yielding an image \u2018patch\u2019 of smoothly varying location and zoom. We refer to this configuration simply as \u201cDRAW\u201d.\nAs illustrated in Fig. 3, theN\u00d7N grid of Gaussian filters is positioned on the image by specifying the co-ordinates of the grid centre and the stride distance between adjacent filters. The stride controls the \u2018zoom\u2019 of the patch; that is, the larger the stride, the larger an area of the original image will be visible in the attention patch, but the lower the effective resolution of the patch will be. The grid centre (gX , gY ) and stride \u03b4 (both of which are real-valued) determine the mean location \u00b5iX , \u00b5 j Y of the filter at row i, column j in the\npatch as follows:\n\u00b5iX = gX + \u03b4 (i\u2212N/2\u2212 0.5) (19) \u00b5jY = gY + \u03b4 (j \u2212N/2\u2212 0.5) (20)\nTwo more parameters are required to fully specify the attention model: the isotropic variance \u03c32 of the Gaussian filters, and a scalar intensity \u03b3 that multiplies the filter response. Given an A \u00d7 B input image x, all five attention parameters are dynamically determined at each time step via a linear transformation of the decoder output hdec :\n(g\u0303X , g\u0303Y , log \u03c3 2, log \u03b4\u0303, log \u03b3) = L(hdec) (21)\ngX = A+ 1\n2 (g\u0303X + 1) (22)\ngY = B + 1\n2 (g\u0303Y + 1) (23)\n\u03b4 = max(A,B)\u2212 1\nN \u2212 1 \u03b4\u0303 (24)\nwhere the variance, stride and intensity are emitted in the log-scale to ensure positivity. The scaling of gX , gY and \u03b4 is chosen to ensure that the initial patch (with a randomly initialised network) roughly covers the whole input image.\nGiven the attention parameters emitted by the decoder, the horizontal and vertical filterbank matrices FX and FY (dimensions N \u00d7 A and N \u00d7 B respectively) are defined as follows:\nFX [i, a] = 1\nZX exp\n( \u2212 (a\u2212 \u00b5 i X) 2\n2\u03c32\n) (25)\nFY [j, b] = 1\nZY exp\n( \u2212 (b\u2212 \u00b5 j Y ) 2\n2\u03c32\n) (26)\nwhere (i, j) is a point in the attention patch, (a, b) is a point in the input image, and Zx, Zy are normalisation constants that ensure that \u2211 a FX [i, a] = 1 and \u2211 b FY [j, b] = 1."}, {"heading": "3.3. Reading and Writing With Attention", "text": "Given FX , FY and intensity \u03b3 determined by hdect\u22121, along with an input image x and error image x\u0302t, the read operation returns the concatenation of two N \u00d7N patches from the image and error image:\nread(x, x\u0302t, h dec t\u22121) = \u03b3[FY xF T X , FY x\u0302F T X ] (27)\nNote that the same filterbanks are used for both the image and error image. For the write operation, a distinct set of attention parameters \u03b3\u0302, F\u0302X and F\u0302Y are extracted from hdect , the order of transposition is reversed, and the intensity is inverted:\nwt = L(h dec t ) (28)\nwrite(hdect ) = 1\n\u03b3\u0302 F\u0302TY wtF\u0302X (29)\nwhere wt is the N \u00d7N writing patch emitted by hdect . For colour images each point in the input and error image (and hence in the reading and writing patches) is an RGB triple. In this case the same reading and writing filters are used for all three channels."}, {"heading": "4. Experimental Results", "text": "We assess the ability of DRAW to generate realisticlooking images by training on three datasets of progressively increasing visual complexity: MNIST (LeCun et al., 1998), Street View House Numbers (SVHN) (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2009). The images generated by the network are always novel (not simply copies of training examples), and are virtually indistinguishable from real data for MNIST and SVHN; the generated CIFAR images are somewhat blurry, but still contain recognisable structure from natural scenes. The log-loss for binarized MNIST is substantially lower than the state of the art. As a preliminary exercise, we also evaluate the 2D attention module of the DRAW network on cluttered MNIST classification.\nNetwork hyper-parameters for all the experiments are presented in Table 3. The Adam optimisation algorithm (Kingma & Ba, 2014) was used throughout. Examples of generation sequences for MNIST and SVHN are provided in the accompanying video (https://www. youtube.com/watch?v=Zt-7MI9eKEo)."}, {"heading": "4.1. Cluttered MNIST Classification", "text": "To test the classification efficacy of the DRAW attention mechanism (as opposed to its ability to aid in image generation), we evaluate its performance on the 100 \u00d7 100 cluttered translated MNIST task (Mnih et al., 2014). Each image in cluttered MNIST contains many digit-like fragments of visual clutter that the network must distinguish from the true digit to be classified. As illustrated in Fig. 5, having an iterative attention model allows the network to progressively zoom in on the relevant region of the image, and ignore the clutter outside it.\nOur model consists of an LSTM recurrent network that receives a 12 \u00d7 12 \u2018glimpse\u2019 from the input image at each timestep, using the selective read operation defined in Section 3.2. After a fixed number of glimpses the network uses a softmax layer to classify the MNIST digit. The network is similar to the recently introduced Recurrent Attention Model (RAM) (Mnih et al., 2014), except that our at-\ntention method is differentiable; we therefore refer to it as \u201cDifferentiable RAM\u201d.\nThe results in Table 1 demonstrate a significant improvement in test error over the original RAM network. Moreover our model had only a single attention patch at each time-step, whereas RAM used four, at different levels of zoom."}, {"heading": "4.2. MNIST Generation", "text": "We trained the full DRAW network as a generative model on the binarized MNIST dataset (Salakhutdinov & Murray, 2008). This dataset has been widely studied in the literature, allowing us to compare the numerical performance (measured in average nats per image on the test set) of DRAW with existing methods. Table 2 shows that DRAW without selective attention performs comparably to other recent generative models such as DARN, NADE and DBMs, and that DRAW with attention considerably improves on the state of the art.\nOnce the DRAW network was trained, we generated MNIST digits following the method in Section 2.3, exam-\nples of which are presented in Fig. 6. Fig. 7 illustrates the image generation sequence for a DRAW network without selective attention (see Section 3.1). It is interesting to compare this with the generation sequence for DRAW with attention, as depicted in Fig. 1. Whereas without attention it progressively sharpens a blurred image in a global way,\nwith attention it constructs the digit by tracing the lines\u2014 much like a person with a pen."}, {"heading": "4.3. MNIST Generation with Two Digits", "text": "The main motivation for using an attention-based generative model is that large images can be built up iteratively, by adding to a small part of the image at a time. To test this capability in a controlled fashion, we trained DRAW to generate images with two 28 \u00d7 28 MNIST images chosen at random and placed at random locations in a 60\u00d7 60 black background. In cases where the two digits overlap, the pixel intensities were added together at each point and clipped to be no greater than one. Examples of generated data are shown in Fig. 8. The network typically generates one digit and then the other, suggesting an ability to recreate composite scenes from simple pieces."}, {"heading": "4.4. Street View House Number Generation", "text": "MNIST digits are very simplistic in terms of visual structure, and we were keen to see how well DRAW performed on natural images. Our first natural image generation experiment used the multi-digit Street View House Numbers dataset (Netzer et al., 2011). We used the same preprocessing as (Goodfellow et al., 2013), yielding a 64 \u00d7 64 house number image for each training example. The network was then trained using 54\u00d7 54 patches extracted at random locations from the preprocessed images. The SVHN training set contains 231,053 images, and the validation set contains\n4,701 images.\nA major challenge with natural image generation is how to model the pixel colours. In this work we applied a simple approximation where the normalised intensity of each of the RGB channels was treated as an independent Bernoulli probability. This approach has the advantage of being easy to implement and train; however it does mean that the loss function used for training does not match the true compression cost of the data.\nThe house number images generated by the network are highly realistic, as shown in Figs. 9 and 10. Fig. 11 reveals that, despite the long training time, the DRAW network underfit the SVHN training data."}, {"heading": "4.5. Generating CIFAR Images", "text": "The most challenging dataset we applied DRAW to was the CIFAR-10 collection of natural images (Krizhevsky, 2009). CIFAR-10 is very diverse, and with only 50,000 training examples it is very difficult to generate realisticlooking objects without overfitting (in other words, without copying from the training set). Nonetheless the images in Fig. 12 demonstrate that DRAW is able to capture much of the shape, colour and composition of real photographs.\nTable 3. Experimental Hyper-Parameters.\nTask #glimpses LSTM #h #z Read Size Write Size 100\u00d7 100 MNIST Classification 8 256 - 12\u00d7 12 - MNIST Model 64 256 100 2\u00d7 2 5\u00d7 5 SVHN Model 32 800 100 12\u00d7 12 12\u00d7 12 CIFAR Model 64 400 200 5\u00d7 5 5\u00d7 5\nTime\nFigure 10. SVHN Generation Sequences. The red rectangle indicates the attention patch. Notice how the network draws the digits one at a time, and how it moves and scales the writing patch to produce numbers with different slopes and sizes.\n5060 5080 5100 5120 5140 5160 5180 5200 5220\n0 50 100 150 200 250 300 350\nco st\np er\ne xa\nm pl\ne\nminibatch number (thousands)\ntraining validation\nFigure 11. Training and validation cost on SVHN. The validation cost is consistently lower because the validation set patches were extracted from the image centre (rather than from random locations, as in the training set). The network was never able to overfit on the training data.\nFigure 12. Generated CIFAR images. The rightmost column shows the nearest training examples to the column beside it."}, {"heading": "5. Conclusion", "text": "This paper introduced the Deep Recurrent Attentive Writer (DRAW) neural network architecture, and demonstrated its ability to generate highly realistic natural images such as photographs of house numbers, as well as improving on the best known results for binarized MNIST generation. We also established that the two-dimensional differentiable attention mechanism embedded in DRAW is beneficial not only to image generation, but also to cluttered image classification."}, {"heading": "Acknowledgments", "text": "Of the many who assisted in creating this paper, we are especially thankful to Koray Kavukcuoglu, Volodymyr Mnih, Jimmy Ba, Yaroslav Bulatov, Greg Wayne, Andrei Rusu, Danilo Jimenez Rezende and Shakir Mohamed."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "The helmholtz machine", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E", "Neal", "Radford M", "Zemel", "Richard S"], "venue": "Neural computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural computation,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["Goodfellow", "Ian J", "Bulatov", "Yaroslav", "Ibarz", "Julian", "Arnoud", "Sacha", "Shet", "Vinay"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Mnih", "Andriy", "Blundell", "Charles", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": null, "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["Murray", "Iain", "Salakhutdinov", "Ruslan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Murray et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2009}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Iterative neural autoregressive distribution estimator nade-k", "author": ["Raiko", "Tapani", "Li", "Yao", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "On learning where to look", "author": ["Ranzato", "Marc\u2019Aurelio"], "venue": "arXiv preprint arXiv:1405.5488,", "citeRegEx": "Ranzato and Marc.Aurelio.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato and Marc.Aurelio.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "On the quantitative analysis of Deep Belief Networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In Proceedings of the 25th Annual International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1410.6460,", "citeRegEx": "Salimans et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2014}, {"title": "Attention for fine-grained categorization", "author": ["Sermanet", "Pierre", "Frome", "Andrea", "Real", "Esteban"], "venue": "arXiv preprint arXiv:1412.7054,", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning generative models with visual attention", "author": ["Tang", "Yichuan", "Srivastava", "Nitish", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1312.6110,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "A deep and tractable density estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Zheng", "Yin", "Zemel", "Richard S", "Zhang", "Yu-Jin", "Larochelle", "Hugo"], "venue": "International Journal of Computer Vision, pp", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "In the context of generative neural networks, this typically means that all the pixels are conditioned on a single latent distribution (Dayan et al., 1995; Hinton & Salakhutdinov, 2006; Larochelle & Murray, 2011).", "startOffset": 135, "endOffset": 212}, {"referenceID": 7, "context": "It therefore belongs to the family of variational auto-encoders, a recently emerged hybrid of deep learning and variational inference that has led to significant advances in generative modelling (Gregor et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014; Salimans et al., 2014).", "startOffset": 195, "endOffset": 306}, {"referenceID": 22, "context": "It therefore belongs to the family of variational auto-encoders, a recently emerged hybrid of deep learning and variational inference that has led to significant advances in generative modelling (Gregor et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014; Salimans et al., 2014).", "startOffset": 195, "endOffset": 306}, {"referenceID": 25, "context": "It therefore belongs to the family of variational auto-encoders, a recently emerged hybrid of deep learning and variational inference that has led to significant advances in generative modelling (Gregor et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014; Salimans et al., 2014).", "startOffset": 195, "endOffset": 306}, {"referenceID": 2, "context": "quence of partial glimpses, or foveations, than by a single sweep through the entire image (Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014).", "startOffset": 91, "endOffset": 251}, {"referenceID": 28, "context": "quence of partial glimpses, or foveations, than by a single sweep through the entire image (Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014).", "startOffset": 91, "endOffset": 251}, {"referenceID": 30, "context": "quence of partial glimpses, or foveations, than by a single sweep through the entire image (Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014).", "startOffset": 91, "endOffset": 251}, {"referenceID": 16, "context": "quence of partial glimpses, or foveations, than by a single sweep through the entire image (Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014).", "startOffset": 91, "endOffset": 251}, {"referenceID": 0, "context": "quence of partial glimpses, or foveations, than by a single sweep through the entire image (Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014).", "startOffset": 91, "endOffset": 251}, {"referenceID": 26, "context": "quence of partial glimpses, or foveations, than by a single sweep through the entire image (Larochelle & Hinton, 2010; Denil et al., 2012; Tang et al., 2013; Ranzato, 2014; Zheng et al., 2014; Mnih et al., 2014; Ba et al., 2014; Sermanet et al., 2014).", "startOffset": 91, "endOffset": 251}, {"referenceID": 16, "context": "The main challenge faced by sequential attention models is learning where to look, which can be addressed with reinforcement learning techniques such as policy gradients (Mnih et al., 2014).", "startOffset": 170, "endOffset": 189}, {"referenceID": 6, "context": "In this sense it resembles the selective read and write operations developed for the Neural Turing Machine (Graves et al., 2014).", "startOffset": 107, "endOffset": 128}, {"referenceID": 3, "context": "In our experiments we use the Long Short-Term Memory architecture (LSTM; Hochreiter & Schmidhuber (1997)) for both, in the extended form with forget gates (Gers et al., 2000).", "startOffset": 155, "endOffset": 174}, {"referenceID": 27, "context": "We favour LSTM due to its proven track record for handling long-range dependencies in real sequential data (Graves, 2013; Sutskever et al., 2014).", "startOffset": 107, "endOffset": 145}, {"referenceID": 1, "context": "for latent variables in auto-encoders (Dayan et al., 1995; Gregor et al., 2014); however a great advantage of Gaussian latents is that the gradient of a function of the samples with respect to the distribution parameters can be easily obtained using the so-called reparameterization trick (Kingma & Welling, 2014; Rezende et al.", "startOffset": 38, "endOffset": 79}, {"referenceID": 7, "context": "for latent variables in auto-encoders (Dayan et al., 1995; Gregor et al., 2014); however a great advantage of Gaussian latents is that the gradient of a function of the samples with respect to the distribution parameters can be easily obtained using the so-called reparameterization trick (Kingma & Welling, 2014; Rezende et al.", "startOffset": 38, "endOffset": 79}, {"referenceID": 22, "context": ", 2014); however a great advantage of Gaussian latents is that the gradient of a function of the samples with respect to the distribution parameters can be easily obtained using the so-called reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 217, "endOffset": 263}, {"referenceID": 6, "context": "To endow the network with selective attention, without sacrificing the benefits of gradient descent training, we take inspiration from the differentiable attention mechanisms recently used in handwriting synthesis (Graves, 2013) and Neural Turing Machines (Graves et al., 2014).", "startOffset": 256, "endOffset": 277}, {"referenceID": 15, "context": "We assess the ability of DRAW to generate realisticlooking images by training on three datasets of progressively increasing visual complexity: MNIST (LeCun et al., 1998), Street View House Numbers (SVHN) (Netzer et al.", "startOffset": 149, "endOffset": 169}, {"referenceID": 19, "context": ", 1998), Street View House Numbers (SVHN) (Netzer et al., 2011) and CIFAR-10 (Krizhevsky, 2009).", "startOffset": 42, "endOffset": 63}, {"referenceID": 16, "context": "To test the classification efficacy of the DRAW attention mechanism (as opposed to its ability to aid in image generation), we evaluate its performance on the 100 \u00d7 100 cluttered translated MNIST task (Mnih et al., 2014).", "startOffset": 201, "endOffset": 220}, {"referenceID": 16, "context": "The network is similar to the recently introduced Recurrent Attention Model (RAM) (Mnih et al., 2014), except that our at-", "startOffset": 82, "endOffset": 101}, {"referenceID": 29, "context": "The previous results are from [1] (Salakhutdinov & Hinton, 2009), [2] (Murray & Salakhutdinov, 2009), [3] (Uria et al., 2014), [4] (Raiko et al.", "startOffset": 106, "endOffset": 125}, {"referenceID": 20, "context": ", 2014), [4] (Raiko et al., 2014), [5] (Rezende et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 22, "context": ", 2014), [5] (Rezende et al., 2014), [6] (Salimans et al.", "startOffset": 13, "endOffset": 35}, {"referenceID": 25, "context": ", 2014), [6] (Salimans et al., 2014), [7] (Gregor et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 7, "context": ", 2014), [7] (Gregor et al., 2014).", "startOffset": 13, "endOffset": 34}, {"referenceID": 19, "context": "Our first natural image generation experiment used the multi-digit Street View House Numbers dataset (Netzer et al., 2011).", "startOffset": 101, "endOffset": 122}, {"referenceID": 4, "context": "We used the same preprocessing as (Goodfellow et al., 2013), yielding a 64 \u00d7 64 house number image for each training example.", "startOffset": 34, "endOffset": 59}], "year": 2015, "abstractText": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.", "creator": "LaTeX with hyperref package"}}}