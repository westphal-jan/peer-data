{"id": "1502.04956", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2015", "title": "The Linearization of Belief Propagation on Pairwise Markov Networks", "abstract": "belief intensity propagation ( bp ) then allows agents to quickly approximate exact direct probabilistic inference in graphical models, such collections as quantum markov networks ( also unofficially called markov networks random fields, intrinsic or undirected graphical models ). further however, traditionally no exact linear convergence guarantees for bp are explicitly known, mainly in general. recent work research has dramatically proposed way to approximate bp conditional by linearizing the update procedure equations around alternating default absolute values for the special case when merging all edges lying in the markov network vertices carry above the same instant symmetric, doubly discrete stochastic potential. this unusual linearization has led repeatedly to exact convergence guarantees, needing considerable scaling speed - up, while maintaining increasingly high quality efficiency results in network - based state classification ( both i. here e. when analyzed we only care about the most precisely likely label or class value for defining each node component and overall not the original exact specific probabilities ). the present paper generalizes our prior work on utilizing linearized simultaneously belief mode propagation ( linbp ) with implementing an approach that approximates loopy simultaneous belief propagation on applying any continuous pairwise markov serial network with underlying the actual problem of solving fitting a linear equation scoring system.", "histories": [["v1", "Tue, 17 Feb 2015 16:49:23 GMT  (707kb,D)", "https://arxiv.org/abs/1502.04956v1", "14 pages, 7 figures"], ["v2", "Tue, 27 Dec 2016 15:01:40 GMT  (797kb,D)", "http://arxiv.org/abs/1502.04956v2", "Full version of AAAI 2017 paper with same title (23 pages, 9 figures)"]], "COMMENTS": "14 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.SI", "authors": ["wolfgang gatterbauer"], "accepted": false, "id": "1502.04956"}, "pdf": {"name": "1502.04956.pdf", "metadata": {"source": "CRF", "title": "The Linearization of Belief Propagation on Pairwise Markov Random Fields", "authors": ["Wolfgang Gatterbauer"], "emails": [], "sections": [{"heading": null, "text": "1 Introduction Belief Propagation (BP) is an iterative message-passing algorithm for performing inference in graphical models (GMs), such as Markov Random Fields (MRFs). BP calculates the marginal distribution for each unobserved node, conditional on any observed nodes (Pearl 1988). It achieves this by propagating the information from a few observed nodes throughout the network by iteratively passing information between neighboring nodes. It is known that when the graphical model has a tree structure, then BP converges to the true marginals (according to exact probabilistic inference) after a finite number of iterations. In loopy graphs, convergence to the correct marginals is not guaranteed; in fact, it is not guaranteed at all, and using BP can lead to welldocumented convergence problems (Sen et al. 2008). While there is a lot of research on convergence of BP (Elidan, McGraw, and Koller 2006; Ihler, Fisher III, and Willsky 2005; Mooij and Kappen 2007), exact criteria for convergence are not known (Murphy 2012), and most existing bounds for BP\nThis paper is a significantly extended version of a paper with the same title presented at the 31st AAAI Conference on Artificial Intelligence (AAAI-17). The present paper contains all proofs and details on the experimental results. Possible future updates will be made available on CORR at http://arxiv.org/abs/1502.04956.\non general pairwise MRFs give only sufficient convergence criteria, or are for restricted cases, such as when the underlying distributions are Gaussians (Malioutov, Johnson, and Willsky 2006; Su and Wu 2015; Weiss and Freeman 2001).\nSemi-supervised node classification. BP is also a versatile formalism for semi-supervised learning; i.e., assigning classes to unlabeled nodes while maximizing the number of correctly labeled nodes (Koller and Friedman 2009, ch. 4). The goal is to predict the most probable class for each node in a network independently, which corresponds to the Maximum Marginal (MM) assignment (Domke 2013; Weiss 2000). Let P be a probability distribution over a set of random variables X\u222aY. MM-inference (or \u201cMM decoding\u201d) searches for the most probable assignment yi for each unlabeled node Yi independently, given evidence X = x:\nMM(y|x) = {arg max yi P(Yi=yi|X=x)|Yi \u2208 Y}\nNotice that this problem is simpler than finding the actual marginal distribution. It is also different from finding the Maximum A-Posteriori (MAP) assignment (the \u201cmost probable configuration\u201d), which is the mode or the most probable joint classification of all non-evidence variables:1\nMAP(y|x) = arg max y P(Y=y|X=x)\nConvergent message-passing algorithms. There has been much research on finding variations to the update equations of BP that guarantee convergence. These algorithms are often similar in structure to the non-convergent algorithms, yet it can be proven that the value of the variational problem (or its dual) improves at each iteration (Hazan and Shashua 2008; Heskes 2006; Meltzer, Globerson, and Weiss 2009). Another body of recent papers have suggested to solve the convergence problems of MM-inference by linearizing the update equations. Krzakala et al. study a form of linearization for unsupervised classification called \u201cspectral redemption\u201d in the stochastic block model. That model\n1See (Murphy 2012, ch. 5.2.1) for a detailed discussion on why MAP has some undesirable properties and is not necessarily a \u201crepresentative\u201d assignment. While in theory it is arguably preferable to compute marginal probabilities, in practice researchers often use MAP inference due to the availability of efficient discrete optimization algorithms (Korc\u030c, Kolmogorov, and Lampert 2012).\nar X\niv :1\n50 2.\n04 95\n6v 2\n[ cs\n.A I]\n2 7\nD ec\n2 01\n6\nis unsupervised and has no obvious way to include supervision in its setup (i.e., it is not clear how to leverage labeled nodes). Donoho, Maleki, and Montanari propose \u201capproximate message-passing\u201d (AMP) as an iterative thresholding algorithm for compressed sensing that is largely inspired by BP. Koutra et al. linearize BP for the case of two classes and proposed \u201cFast Belief Propagation\u201d (FaBP) as a method to propagate existing knowledge of homophily or heterophily to unlabeled data. This framework allows one to specify a homophily factor h (h > 0 for homophily or h < 0 for heterophily) and to then use this algorithm with exact convergence criteria for binary classification. Gatterbauer et al. derive a multivariate (\u201cpolytomous\u201d) generalization of FaBP from binary to multiple labels called \u201cLinearized Belief Propagation\u201d (LinBP). Both aforementioned papers show considerable speed-ups for the application of node classification and relational learning by transforming the update equations of BP into an efficient matrix formulation. However, those papers solve only special cases: FaBP is restricted to two classes per node (de facto, one single score). LinBP can handle multiple classes, but is restricted to one single node type, one single edge type, and a potential that is symmetric and doubly stochastic (see Fig. 1).2\nContributions. This paper derives a linearization of BP for arbitrary pairwise MRFs, which transforms the parameters of an MRF into an equation system that replaces multiplication with addition. In contrast to standard BP, the derived update equations (i) come with exact convergence guarantees, (ii) allow a closed-form solution, (iii) keep the derived beliefs normalized at each step, and (iv) can thus be put into an efficient linear algebra framework. We also show empirically that this approach \u2013 in addition to its compelling computational advantages \u2013 performs comparably to Loopy BP for a large part of the parameter space. In contrast to prior work on linearizing BP, we remove any restriction on the potentials and solve the most general case for pairwise MRFs (see Fig. 1). Since it is known that any higher-order MRF can be converted to a pairwise MRF (Wainwright and Jordan 2008, Appendix E.3), the approach can be also be used for higher-order potentials. Our formalism can thus model arbitrary heterogeneous networks; i.e., such that have directed edges or have different types of nodes.3 This generalization is not obvious and required us to solve several new algebraic problems: (i) Non-symmetric potentials modulate messages differently across both directions of an edge; each direction then requires different centering points (this is particularly pronounced for non-quadratic potentials; i.e., when nodes adjacent to an edge have different numbers of classes). (ii) Multiplying belief vectors with non-stochastic\n2A potential is \u201cdoubly stochastic\u201d if all rows and columns sum up to 1. As potentials can be scaled without changing the semantics of BP, this definition also extends to any potential where the rows and columns sum to the same value.\n3Notice that an underlying directed network is still modeled as an undirected Graphical Model (GM). For example, while the \u201cfriendship\u201d relation on Facebook is undirected, the \u201cfollower\u201d relation on Twitter is directed and has different implications on the two nodes adjacent to a directed \u201clinks to\u201d-edge. Yet, the resulting GM is still undirected, but now has asymmetric potentials.\nBP FaBP LinBP this work # node types arbitrary 1 1 arbitrary # node classes arbitrary 2 const k arbitrary # edge types arbitrary 1 1 arbitrary edge symmetry arbitrary required required arbitrary edge potential arbitrary doubly stoch. doubly stoch. arbitrary closed form no yes yes yes\nHere, we write Zs for a normalizer that makes the elements of ys sum up to 1. Thus, the posterior belief ys(j) is computed by multiplying the prior belief xs(j) with the incoming messagesmus(j) from all neighbors u \u2208 N(s), and then normalizing so that the beliefs in all ks classes sum to 1. In parallel, each node sends messages to each of its neighbors:\nmst(i)\u2190 1\nZst\n\u2211\nj\n\u03c8st(j, i) xs(j) \u220f\nu\u2208N(s)\\t mus(j) (2)\nHere,\u03c8st(j, i) is a proportional \u201ccoupling weight\u201d (or \u201ccompatibility,\u201d \u201caffinity,\u201d \u201cmodulation\u201d) that indicates the relative influence of class j of node s on class i of node t. Thus, the message mst(i) is computed by multiplying together all incoming messages at node s \u2013 except the one sent by the recipient t \u2013 and then passing through the \u03c8st edge potential. Notice that we use Zst in Eq. (2) as a normalizer that makes the elements of mst sum up to kt at each iteration. As pointed out by Murphy, Weiss, and Jordan; Pearl, normalizing the messages has no effect on the final beliefs; however, this intermediate normalization of messages will become crucial in our derivations. BP then repeatedly computes the above update equations for each node until the values (hopefully) converge. At iteration r of the algorithm, ys(j) represents the posterior belief of j conditioned on the evidence that is r steps away in the network.\n3 Linearizing BP over any pairwise MRF This section gives a closed form description for the final beliefs after convergence of BP in arbitrary pairwise MRFs under a certain limit consideration of all parameters. This is a strict and non-trivial generalization of recent works (Fig. 1). The difficulty of our generalization lies in technical details: non-symmetric potentials require different centering points for messages across different directions of an edge; non-stochastic potentials require different normalizers for different iterations (and for different potentials in the networks) which does not easily lead to a simple matrix formulation; and the full heterogenous case (e.g., different number of classes k for different nodes) requires a considerably more general derivation and final formulation.\nOur approach is conceptually simple: we center all matrix entries around well-chosen default values and then focus only on the deviations from these defaults using Maclaurin series at several steps in our derivation. The resulting equations replace multiplication with addition and can thus be put into the framework of matrix-vector multiplication, which can leverage existing highly-optimized code. It also allows us to give exact convergence criteria for the resulting update equations and a closed form solution (that would require the inversion of a large matrix). The approach is similar in spirit to the idea of writing any MRF (with strictly positive density) as log-linear model. However, by starting from the update equations for loopy BP, we solve the intractability problem by ignoring all dependencies between messages that have traveled over a path of length 2 or more. Definition 1 (Centering). We call a vector x or matrix X \u201ccentered around cwith standard deviation v\u201d if the average entry \u00b5(x) = c and standard deviation \u03c3(x) = v.\nDefinition 2 (Residual vector/matrix). If a vector x is centered around c, then the \u201cresidual vector\u201d x\u0302 around c is defined as x\u0302 = [x1\u2212 c, x2\u2212 c, . . .]\u1d40. Accordingly, we denote a matrix X\u0302 as a \u201cresidual matrix\u201d if each entry is the residual after centering around c.\nFor example, the vector x = [1.1, 1.2, 0.7]\u1d40 is centered around c = 1, and the residuals from 1 form the residual vector x\u0302 = [0.1, 0.2,\u22120.3]\u1d40; i.e., x = 13 + x\u0302, where 13 is the 3-dimensional vector with all entries equal to 1. By definition of a normalized vector, beliefs for any node s are centered around 1ks , and the residuals for prior beliefs have non-zero elements (i.e., x\u0302s 6= 0ks ) only for nodes with local evidence (nodes \u201cwith explicit beliefs\u201d). Further notice that the entries in a residual vector or matrix always sum up to 0 (i.e., \u2211 i x\u0302(i) = 0). This is done by construction and will become important in the derivations of our results. The main idea of our derivation relies then on the following observation: if we start with messages and potentials with rows and columns centered around 1 with small enough standard deviations, then the normalizer of the update equation Eq. (2) is independent of the beliefs and remains constant as Zst = k\u22121t . Importantly, the resulting equations do not require further normalization. The derivation further makes use of certain linearizing approximations that result in a well-behaved linear equation system. We show that the MM solutions implied by this equation system are identical to those from the original BP update equations in case of nearly uniform priors and potentials. For strong priors and potentials (e.g., [ 1 100100 1 ]), the resulting solutions are not identical anymore, yet serve as reasonable approximations in a wide range of problem parameters (see Section 4). WLOG, we start with potentials that are centered around 1 and then re-center the potentials before using them:4\nDefinition 3 (Row-recentered residual matrix). Let \u03c8 \u2208 R`\u00d7k be centered around 1 and \u03c8\u0302 be the residual matrix around 1. Furthermore, let r\u0302(j) := \u2211 i \u03c8\u0302(j, i) be the sum of the residuals of row j. Then the \u201crow-recentered residual matrix\u201d \u03c8\u0302\n\u2032 has entries \u03c8\u0302\u2032(j, i) := 1k ( \u03c8\u0302(j, i)\u2212 r\u0302(j)k ) .\nBefore we can state our main result, we need some additional notation. WLOG, let [n] be the set of all nodes. For each node s \u2208 [n], let ks be the number of its possible classes. Let ks := 1ks1ks , i.e., the ks-dimensional uniform stochastic column vector. Furthermore, let ktot :=\u2211 s\u2208[n] ks be the sum of classes across nodes. To write all our resulting equations as one large equation system, we stack the individual explicit (x\u0302) and implicit (y\u0302) residual belief vectors together with the ks-vectors one underneath the other to form three ktot-dimensional stacked column vectors. We also combine all row-recentered residual matrices into one large but sparse [ktot \u00d7 ktot]-square block matrix\n4Without changing the joint probability distribution, every potential in a MRF can be scaled so that the average entry is 1. For\nexample, given \u03c8 = [ 4 6 56 8 7 ], we scale by 1 6\nto get \u03c8 = [ 2 3 1 5 6\n1 4 3 7 6\n] ,\nwhich has the identical semantics but is now centered around 1.\n(notice that all entries for non-existing edges remain empty):\ny\u0302 :=   y\u03021 ... y\u0302n  , x\u0302 :=   x\u03021 ... x\u0302n  , k :=   k1 ... kn  , \u03c8\u0302 \u2032 :=   \u03c8\u0302 \u2032 11 . . . \u03c8\u0302 \u2032 1n ... . . . ... \u03c8\u0302 \u2032 n1 . . . \u03c8\u0302 \u2032 nn  \nWe can now state our main theorem:\nTheorem 4 (Linearizing Belief Propagation). Let y\u0302, x\u0302, k\u0302, and \u03c8\u0302 \u2032 be the above defined residual vectors and matrix. Let be a bound on the standard deviation of all non-zero entries of \u03c8\u0302 \u2032 and x\u0302, \u03c3(\u03c8\u0302 \u2032 ) < and \u03c3(x\u0302) < . Let yBPv be the final belief assignment for any node v after convergence of BP. Then, for lim \u21920+ , arg maxi yBPv (i) = arg maxi y\u0302 Lin v (i), where y\u0302v results from solving the following system of ktot linear equations in y\u0302:\ny\u0302 = x\u0302\ufe38\ufe37\ufe37\ufe38 1st\n+ \u03c8\u0302 \u2032\u1d40 k\ufe38 \ufe37\ufe37 \ufe38\n2nd\n+ \u03c8\u0302 \u2032\u1d40 y\u0302\ufe38 \ufe37\ufe37 \ufe38\n3rd \u2212 \u03c8\u0302\u2032\u1d402y\u0302\ufe38 \ufe37\ufe37 \ufe38 4th\n(3)\nIn other words, the MM node labeling from BP can be approximated by solving a linear equation system if each of the potentials and each of the beliefs are reasonably tightly centered around their average values. Notice that the 2nd term \u03c8\u0302 \u2032\u1d40 k is a \u201cbias\u201d vector that depends only on the structure of the network and the potentials, but not the beliefs. We thus sometimes prefer to write c\u0302\u2032\u2217 := \u03c8\u0302 \u2032\u1d40 k to emphasize that it remains constant during the iterations. This term vanishes if all potentials are doubly stochastic. Also notice that the 4th term is what was called the \u201cecho cancellation\u201d in (Gatterbauer et al. 2015).5 Simple algebraic manipulations then lead a closed-form solution by solving Eq. (3) for y\u0302:\ny\u0302 = ( Iktot \u2212 \u03c8\u0302 \u2032\u1d40 + \u03c8\u0302 \u2032\u1d402)\u22121( x\u0302 + c\u0302\u2032\u2217 ) (4)"}, {"heading": "Iterative updates and convergence", "text": "The complexity of inverting a matrix is cubic in the number of variables, which makes direct application of Eq. (4) difficult. Instead, we use Eq. (3), which gives an implicit definition of the final beliefs, iteratively. Starting with an arbitrary initialization of y\u0302 (e.g., all values zero), we repeatedly compute the right hand side of the equations and update the values of y\u0302 until the process converges:6\n5Notice that the BP update equations send a message across an edge that excludes information received across the same edge from the other direction: \u201cu \u2208 N(s)\\ t\u201d in Eq. (2). In a probabilistic scenario on tree-based graphs, this echo cancellation is required for correctness. In loopy graphs (without well-justified semantics), this term still compensates for the message a node t would otherwise send to itself via a neighbor s, i.e., via the path t\u2192 s\u2192 t.\n6Interestingly, our linearized update equations, Eq. (5), are reminiscent of the update equations for the mean beliefs in Gaussian MRFs (Malioutov, Johnson, and Willsky 2006; Su and Wu 2015; Weiss and Freeman 2001). Notice however, that whereas the update equations are exact in the case of continuous Gaussian MRFs, our equations are approximations for the general discrete case.\nProposition 5 (Update equations). The positive fix points for Eq. (3) can be calculated iteratively with the following update equations starting from y\u0302(0) = 0:\ny\u0302(r+1) \u2190 ( x\u0302 + c\u0302\u2032\u2217 ) + ( \u03c8\u0302 \u2032\u1d40 \u2212 \u03c8\u0302\u2032\u1d402 ) y\u0302(r) (5)\nThese particular update equations allow us to give a sufficient and necessary criterium for convergence via the spectral radius \u03c1 of a matrix.7\nCorollary 6 (Convergence). The update Eq. (5) converges if and only if \u03c1 ( \u03c8\u0302 \u2032 \u2212 \u03c8\u0302\u20322 ) < 1.\nThus, the updates converge towards the closed-form solution, and the final beliefs of each node can be computed via efficient matrix operations with optimized packages, while the implicit form gives us guarantees for the convergence of this process.8 In order to apply our approach to problem settings with spectral radius bigger than one (and thus direct application of Eq. (5) would not work), we propose to modify the model by weakening the potentials. In other words, we multiply \u03c8\u0302 \u2032 with a factor that guarantees convergence. We call the multiplicative factor which exactly separates convergence from divergence, the \u201cconvergence boundary\u201d \u2217. Choosing any with s := \u2217 and s < 1 guarantees convergence. We call any choice of s the \u201cconvergence parameter.\u201d\nDefinition 7 (Convergence boundary \u2217). For any \u03c8\u0302 \u2032 , the convergence boundary \u2217 > 0 is defined implicitly by \u03c1 ( \u2217\u03c8\u0302 \u2032 \u2212 2\u2217\u03c8\u0302 \u20322) = 1."}, {"heading": "Computational complexity", "text": "Naively materializing \u03c8\u0302 \u2032\nwould lead to a space requirement of O(n2k2max) where n is the number of nodes and kmax the max number of classes per node. However, by using a sparse matrix implementation, both the space requirement and the computational complexity of each iteration are only proportional to the number of edges: O(mk2max). The time complexity is identical to the one of message-passing with division, which avoids redundant calculations and is faster than standard BP on graphs with high node degrees (Koller and Friedman 2009). However, the ability to use existing highly-optimized packages for efficient matrix-vector multiplication will considerably speed-up the actual calculations.\n4 Experiments Questions. Our experiments will answer the following 3 questions: (1) What is the effect of the convergence parameter s on accuracy and number of required iterations until convergence? (2) How accurate is our approximation under\n7The \u201cspectral radius\u201d \u03c1(\u00b7) of a matrix is the supremum among the absolute values of its eigenvalues.\n8The intuition behind these equivalences can be illustrated by comparing to the geometric series S = 1 + x + x2 + . . . and its closed form S = (1 \u2212 x)\u22121. Whereas for |x| < 1, the series converges to its closed-form, for |x| > 1, it diverges, and the closed-form is meaningless.\nvarying conditions: (i) the density of the network, (ii) the strength on the interaction, and (iii) the fraction of labeled nodes? (3) How fast is the linearized approximation as compared to standard Loopy BP?\nExperimental protocol. We define \u201caccuracy\u201d as the fraction of unlabeled nodes that receive correct labels. In order to evaluate the accuracy of a method, we need to use graphs with known label ground truth (GT). As we are interested in the accuracy as a function of various parameters, we need graphs with controlled GT. We thus decided to compare BP against its linearization on synthetic graphs with known GT, which allows us to measure the accuracy as result of systematic parameter changes. The well-studied stochastic block-model (Airoldi et al. 2008) leads to networks with degree distributions that are not similar to those found in most empirical network data. Our synthetic graph generator is thus a variant thereof with two important differences: (1) we actively control the degree distributions in the resulting graph; and (2) we \u201cplant\u201d exact graph properties (instead of fixing a property only in expectation). In other words, our generator preserves desired degree distribution and compatibilities between classes. The online appendix (Gatterbauer 2015) contains all details. We focus on the scenario of a network with one non-symmetric potential along each edge. The generator creates a graph using a tuple of parameters (n,m,\u03b1,\u03c8,dist), where n is the number of nodes, m is the number of edges, \u03b1 is the node label distribution with \u03b1(i) being the fraction of nodes of class i, \u03c8 is the edge potential, and dist is a chosen degree distribution (e.g., uniform or power law with chosen coefficient).\nParameter choices. Throughout our experiments, we use k = 3 classes and the potential \u03c8 = [ 1 h 1 1 1 h h 1 1 ] , parameterized by a value h representing the ratio between min and max entries. Dividing by (2 + h) centers it around 1. Thus parameter h models the strength of the potential, and we expect higher values of h to make our approximation less suitable. Notice that this matrix is not symmetric and shows very different modulation behavior across both directions of an edge. We create graphs with n nodes and assign the same fraction of nodes to one of the 3 classes: \u03b1 = [ 13 , 1 3 , 1 3 ]. We also vary the parameters m and d = mn as the average inand outdegree in the graph, and we assume a power law distribution with coefficient 0.5. We then keep a fraction f of node labels and measure accuracy on the remainder.\nComputational setup. All methods are implemented in Python and use the optimized SciPy library (Jones et al. 2001) to handle sparse matrix operations. The experiments are run on a 2.5 Ghz Intel Core i5 with 16G of main memory and a 1TB SSD hard drive. To allow comparability across implementations, we limit evaluation to one processor. For timing BP, we use message-passing with division which is faster than standard BP on graphs with high node degree (Koller and Friedman 2009). To calculate the approximate spectral radius of a matrix, we use a method from the PyAMG library (Bell, Olson, and Schroder 2011) that implements a technique described in (Bai et al. 2000). Our code, including the data generator, is inspired by Scikit-learn (Pedregosa et al. 2011) and is available on Github to encour-\nage reproducible research (SSLH 2015).\nQuestion 1. What is the effect of scaling parameter s on accuracy and number of iterations for convergence?\nResult 1. Our scaling parameter s gives an exact criterion for our approach to converge. In contrast, BP often does not converge and requires a lot of fine-tuning; e.g., damping or even scaling of the potential. The accuracy of the linearization is highest for s close or slightly above 1 and by not iterating until convergence.\nFigure 2a shows the number of required iterations to reach convergence and confirms our theoretical results from Corollary 6. In case the convergence condition does not hold, we scale the centered potential by a value , resulting from = s \u00b7 \u2217 with s < 1. This action weakens the potentials, but preserves the relative affinities (we also use the same approach to help BP find a fixed point if it does not converge within 200 iterations). Figure 2b shows what happens to accuracy if we run the iterative updates a fixed number of times as a function of s. Notice that even considerably scaling a potential does not entirely change the model and still gives reasonable approximations. The figure fixes a number of iterations, but then varies again via s. Also interestingly, almost all of the performance gains from the linearized update equations come from running just a few iterations, and convergence for optimal labeling is not necessary; instead, by choosing s \u2248 1 (at the exact boundary of convergence) or even s > 1 and iterating only a few times, we can maximize the expected accuracy. For the remaining accuracy experiments, we use s = 0.5 and run our algorithm to convergence.\nQuestion 2. How accurate is our approximation, and under which conditions is it reasonable?\nResult 2. The linearization gives comparable labeling accuracy as LBP for graphs with weak potentials. The performance deteriorates the most in dense networks with strong potentials.\nWe found that h, d and f have important influence on the labeling accuracy of BP and its linearization (whereas n, dist and \u03b1 influence only to a lesser extent). Figures 2c and 2d show accuracy as a function of the fraction f of labeled nodes. Notice that we chose the best BP was able to perform (over several choices of and damping factors to make it converge) whereas for LinBP we consistently chose s = 0.5 as proposed in (Gatterbauer et al. 2015). Figures 2e to 2g show labeling quality as a function the strength h of the potential. For strong potentials (h > 3), BP gives better accuracy if it converges. In practice, BP often did not converge within 200 iterations even for weak potentials (bordered data points required dampening; red crosses required additional entry-wise scaling of the potential with our convergence boundary \u2217). In our experiments, BP often did not converge despite using damping, surprisingly often when h is not big. It is known that if the potentials are close to indifference then loopy BP usually converges. In this case, our formalism is equivalent to loopy BP (this follows from our linearization). Thus, whenever loopy BP did not converge,\nwe simply exponentiated the entries of the potential with a varying factor until BP converged. Thus for high h, BP can perform better than the linearization, but only after a lot of fine-tuning of parameters. In contrast, for our formulation we know exactly the boundary of convergence.\nOverall, the linearization gives comparable results to the original BP for small potentials, and BP performance is better than the linearization only either for strong potentials with h \u2265 3 and dampening (see a few yellow dots without borders as exceptions) or after fine-tuning BP after using our own convergence boundary and scaling the potentials, or after a lot of manual fine-tuning.\nQuestion 3. How fast is the linearized approximation as compared to BP?\nResult 3. The linearization is around 100 times faster than BP per iteration and often needs 10 times fewer iterations until convergence. In practice, this can lead to a speed-up of 1000 times.\nA key advantage of the linearization is that it has predictable convergence and comes with considerable speedups. Figure 2h shows that our approach scales linearly in the number of edges and is 50 times faster than regular loopy BP per iteration; an iteration on a graph with 3 million nodes and 30 million edges takes less than 2 sec. Calculating the exact convergence boundary via a spectral radius calcula-\ntion can take more time (approx. 1000 sec for the same graph). Notice that any dampening strategy for BP results in increased number of iterations and needs to overcome the additional slow-down of further iterations. Also recall that on each circled point in Figs. 2e to 2g, BP did not converge within 200 iterations and required dampening; each red cross required additional scaling of the potentials with our calculated \u2217 in order to make BP converge.\n5 Conclusions We have derived a linearization of BP for arbitrary pairwise MRFs for the purpose of node labeling with MM-inference. The approach transforms the parameters of an MRF into a linear equation system that can be solved with simple iterative updates. These updates come with exact convergence guarantees, allow a closed-form solution, keep the derived beliefs normalized at each step, and can thus be put into an efficient linear algebra framework that does not require normalization at each step. Experiments on carefully controlled synthetic data with known ground truth show that our approach performs comparably with Loopy BP for weak potentials and comes with a predictable behavior, compelling computational advantages, and an easy implementation with only few lines of code. An unexplored application of the linearization may be speeding-up convergence of regular BP by starting from good approximations of its fixed points.\nAcknowledgements. This work was supported in part by NSF grant IIS-1553547. I would like to thank Christos Faloutsos for very convincingly persuading me of the power of linear algebra and continued support. I am also grateful to Stephan Gu\u0308nneman, Vladimir Kolmogorov, and Christoph Lampert for a number of insightful comments.\nReferences Airoldi, E. M.; Blei, D. M.; Fienberg, S. E.; and Xing, E. P. 2008. Mixed membership stochastic blockmodels. Journal of Machine Learning Research 9:1981\u20132014. Bai, Z.; Demmel, J.; Dongarra, J.; Ruhe, A.; and van der Vorst, H. 2000. Templates for the solution of algebraic eigenvalue problems. SIAM. Bell, W. N.; Olson, L. N.; and Schroder, J. B. 2011. PyAMG: Algebraic multigrid solvers in Python v2.0. Domke, J. 2013. Learning graphical model parameters with approximate marginal inference. IEEE Trans. Pattern Anal. Mach. Intell. 35(10):2454\u20132467. Donoho, D. L.; Maleki, A.; and Montanari, A. 2009. Message-passing algorithms for compressed sensing. PNAS 106(45):18914\u201318919. Elidan, G.; McGraw, I.; and Koller, D. 2006. Residual belief propagation: Informed scheduling for asynchronous message passing. In UAI, 165\u2013173. Gatterbauer, W.; Gu\u0308nnemann, S.; Koutra, D.; and Faloutsos, C. 2015. Linearized and single-pass belief propagation. PVLDB 8(5):581\u2013592. Gatterbauer, W. 2015. The linearization of belief propagation on pairwise markov random fields. CoRR abs/1502.04956. (http://arxiv.org/abs/1502.04956). Hazan, T., and Shashua, A. 2008. Convergent messagepassing algorithms for inference over general graphs with convex free energies. In UAI, 264\u2013273. Heskes, T. 2006. Convexity arguments for efficient minimization of the Bethe and Kikuchi free energies. J. Artif. Intell. Res. (JAIR) 26:153\u2013190. Ihler, A. T.; Fisher III, J. W.; and Willsky, A. S. 2005. Loopy belief propagation: Convergence and effects of message errors. Journal of Machine Learning Research 6:905\u2013936. Jones, E.; Oliphant, T.; Peterson, P.; et al. 2001. SciPy: Open source scientific tools for Python. Koller, D., and Friedman, N. 2009. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning. MIT Press. Korc\u030c, F.; Kolmogorov, V.; and Lampert, C. 2012. Approximating marginals using discrete energy minimization. In ICML Workshop on Inferning: Interactions between Inference and Learning. Koutra, D.; Ke, T.-Y.; Kang, U.; Chau, D. H.; Pao, H.-K. K.; and Faloutsos, C. 2011. Unifying guilt-by-association approaches: Theorems and fast algorithms. In ECML/PKDD (2), 245\u2013260.\nKrzakala, F.; Moore, C.; Mossel, E.; Neeman, J.; Sly, A.; Zdeborova\u0301, L.; and Zhang, P. 2013. Spectral redemption in clustering sparse networks. PNAS 110(52):20935\u201320940. Malioutov, D. M.; Johnson, J. K.; and Willsky, A. S. 2006. Walk-sums and belief propagation in Gaussian graphical models. Journal of Machine Learning Research 7:2031\u2013 2064. Meltzer, T.; Globerson, A.; and Weiss, Y. 2009. Convergent message passing algorithms \u2013 a unifying view. In UAI, 393\u2013 401. Mooij, J. M., and Kappen, H. J. 2007. Sufficient conditions for convergence of the sum-product algorithm. IEEE Transactions on Information Theory 53(12):4422\u20134437. Murphy, K. P.; Weiss, Y.; and Jordan, M. I. 1999. Loopy belief propagation for approximate inference: An empirical study. In UAI, 467\u2013475. Murphy, K. P. 2012. Machine learning: a probabilistic perspective. Adaptive computation and machine learning series. MIT Press. Pearl, J. 1988. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V.; Vanderplas, J.; Passos, A.; Cournapeau, D.; Brucher, M.; Perrot, M.; and Duchesnay, E. 2011. Scikitlearn: Machine learning in Python. Journal of Machine Learning Research 12:2825\u20132830. Sen, P.; Namata, G.; Bilgic, M.; Getoor, L.; Gallagher, B.; and Eliassi-Rad, T. 2008. Collective classification in network data. AI Magazine 29(3):93\u2013106. SSLH. 2015. A Python package for Semi-Supervised Learning with Heterophily. http://github.com/sslh/sslh/. Su, Q., and Wu, Y. 2015. On convergence conditions of Gaussian belief propagation. IEEE Transactions on Signal Processing 63(5):1144\u20131155. Wainwright, M. J., and Jordan, M. I. 2008. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning 1(1-2):1\u2013305. Weiss, Y., and Freeman, W. T. 2001. Correctness of belief propagation in Gaussian graphical models of arbitrary topology. Neural Computation 13(10):2173\u20132200. Weiss, Y. 2000. Correctness of local probability propagation in graphical models with loops. Neural Computation 12(1):1\u201341.\nGiven a matrix X, we write X(i, j) for one scalar entry, X(i, :) for the i-th row vector, and X(:, j) for the j-th column vector. We also write \u2211 j as short form for \u2211 j\u2208[k] whenever k is clear from the context.\nB Derivation of the linearization of BP over any pairwise MRFs This section contains the derivation of Theorem 4. We will center the elements of all message and belief vectors around their \u201cnatural default values,\u201d i.e., the elements of mst around 1, and the elements of xs, and ys around 1 ks\n(Lemma 10 will provide some intuition why our chosen center points are the natural choice to simplify all later derivations). We are interested in the residual values defined by m\u0302(i) := m(i)\u2212 1, x\u0302s(j) := xs(j)\u2212 1ks , and y\u0302s(j) := ys(j)\u2212 1 ks\n. WLOG, we start from a potential \u03c8 \u2208 R`\u00d7k that is centered around 1 (Recall that we can scale any potential with a positive real number without changing the semantics of the MRF). We then appropriately recenter a potential differently across both directions of an edge as to make it singly stochastic for either direction and most of the residual terms for the belief update equations cancel each other out, leading to simplified equations. Definition 3 provided the definition for the residual matrix in one direction, row-recentering. Adding to that definition, the row-recentered stochastic matrix\u03c8\u2032 is centered around 1k and has entries \u03c8\u2032(j, i) := \u03c8\u0302\u2032(j, i) + 1k . Both matrices are indicated with a single apostrophe \u2032.\nAnalogously, let c\u0302(i) = \u2211 i \u03c8\u0302(j, i) be the residual sum of column i. Then, a column-recentered residual matrix \u03c8\u0302 \u2032\u2032 has\nentries \u03c8\u0302\u2032\u2032(j, i) := 1` (\u03c8\u0302(j, i) \u2212 c\u0302(i) ` ) and the column-recentered stochastic matrix \u03c8 \u2032\u2032 has entries \u03c8\u2032\u2032(j, i) := 1` + \u03c8\u0302 \u2032\u2032(j, i). Notice that both matrices are indicated with a double apostrophe \u2032\u2032. The resulting recentered residual potentials are coupling matrices that make explicit the relative attraction and repulsion of neighboring nodes. For example, the sign of \u03c8\u0302\u2032(j, i) tells\n\u03c8\u0302 \u2032 and \u03c8\u0302 \u2032\u2032 (and stochastic matrices \u03c8\u2032 and \u03c8\u2032\u2032).\nus if the class j attracts or repels class i in a neighbor, and the magnitude of \u03c8\u0302\u2032(j, i) indicates the extent. Subsequently, this centering allows us to rewrite belief propagation in terms of the residuals.\nNotice that column-recentering and row-recentering are connected via the transpose. However, message modulation across one direction of an edge is is not simply the transpose of the modulation across the other direction:\nCorollary 8 (Row-recentering vs. column-recentering). (\u03c8\u0302 \u2032\u2032 )\u1d40 = (\u03c8\u0302 \u1d40 )\u2032. In particular,\n\u03c8\u0302 \u2032\u2032 st = (\u03c8\u0302 \u2032 ts) \u1d40\nWe also write the `-dimensional vector r := \u03c81k for the row sums, the k-dimensional vector c := \u03c8 \u1d40 1` for the column\nsums, and s := \u2211 j r(j) = 1 \u1d40 ` r = 1 \u1d40 ` \u03c81k for the sum of all entries in a matrix. We illustrate recentering next with a detailed example.\nExample 9 (Recentering). Figure 4 shows the 3\u00d7 2 matrix \u03c8 that is centered around 1 (i.e., each entry is close to 1 and the average is exactly 1) together with the row sums r(j) and the column sums c(i). \u03c8\u0302 is then the residual matrix. Notice that the recentered residual matrices \u03c8\u0302 \u2032 and \u03c8\u0302 \u2032\u2032 have zero row sums r\u0302(j)\u2032 or column sums c\u0302(i)\u2032\u2032, respectively. As consequence, the row-recentered matrix \u03c8\u2032 and column-recentered matrix \u03c8\u2032\u2032 are row-stochastic or column-stochastic, respectively.\nWe will further make use of the linearizing Maclaurin series approximations shown in Fig. 5 to derive a well-behaved linear equation system."}, {"heading": "Recentering", "text": "The following lemma provides the mathematical justification for our particular choice of recentering:\nLemma 10 (Recentering). Consider the update equation\ny\u2190 1 Z \u03c8\u1d40x (6)\nwith x being a `-dimensional stochastic vector, \u03c8 \u2208 R`\u00d7k being centered around 1, and Z a normalizer that makes the elements of the resulting k-dimensional vector y sum up to k. Then, the update equation can be approximated with the row-recentered stochastic matrix \u03c8\u2032 by\ny\u2190 k\u03c8\u2032\u1d40x (7)\nProof Lemma 10. Our proof will express both equations (Eq. (6) with \u03c8 and Eq. (7) with \u03c8\u2032) in terms of the residual matrix \u03c8\u0302 \u2032 , and show that they lead to the same equation. From Definition 3 and the definitions at the beginning of Appendix B, we know that \u03c8(j, i) = 1 + \u03c8\u0302(j, i) and \u03c8\u0302(j, i) = k \u03c8\u0302\u2032(j, i) + r\u0302(j)k . Therefore, \u03c8(j, i) = 1 + k \u03c8\u0302 \u2032(j, i) + r\u0302(j)k . Similarly, \u03c8\u2032(j, i) = 1k + \u03c8\u0302 \u2032(j, i).\nIn the following, we are going to use matrix notation that allows us to express the above identities very compactly as: \u03c8\u1d40 = 1k1 \u1d40 ` + 1 k1kr\u0302 \u1d40 + k \u03c8\u0302 \u2032\u1d40 , \u03c8\u2032\u1d40 = 1k1k1 \u1d40 ` + \u03c8\u0302 \u2032\u1d40 , and x = 1`1` + x\u0302. 9\n(i) Equation (6): We calculate y in two steps that treat the normalization separately: first z = \u03c8\u1d40x, and then y = 1Z z.\nz = \u03c8\u1d40x\n= ( 1k1 \u1d40 ` + 1\nk 1kr\u0302\n\u1d40 + k\u03c8\u0302 \u2032\u1d40) \u00b7 (1 ` 1` + x\u0302 )\n= 1k + 1\nk` 1k s\u0302\ufe38\ufe37\ufe37\ufe38\n=0\n+ k ` \u03c8\u0302 \u2032\u1d40 1`\ufe38 \ufe37\ufe37 \ufe38\n=c\u0302\u2032 +1k 1 \u1d40 ` x\u0302\ufe38\ufe37\ufe37\ufe38 =0 + 1 k 1kr\u0302 \u1d40x\u0302 + k\u03c8\u0302 \u2032\u1d40 x\u0302\n= 1k + 1\nk 1kr\u0302\n\u1d40x\u0302 + k ` c\u0302\u2032 + k \u03c8\u0302 \u2032\u1d40 x\u0302\nWe next calculate the value of the normalizer. Recall that the normalizer makes the entries of the vector z sum up to k.\nZ = 1 k 1\u1d40kz = 1 k\n( k + r\u0302\u1d40x\u0302 + k\n` 1\u1d40k c\u0302 \u2032\n\ufe38\ufe37\ufe37\ufe38 =0\n+k 1\u1d40k\u03c8\u0302 \u2032\u1d40 x\u0302\ufe38 \ufe37\ufe37 \ufe38\n=0\n)\n= 1 + r\u0302\u1d40x\u0302 k\nWe see that the normalizer is not a constant but also depends on \u03c8 and x. However, notice that if each row of \u03c8 is centered around 1 (not just the matrix as a whole), then r\u0302(j) = 0 for all rows, and thus Z = 1. In the following, we approximate 1/(1 + ) \u2248 (1\u2212 ) and (1 + 1)(1 + 2) \u2248 1 + 1 \u2212 2.\ny = ( 1k + 1\nk 1kr\u0302\n\u1d40x\u0302 + k ` c\u0302\u2032 + k\u03c8\u0302\n\u2032\u1d40 x\u0302 )( 1 + r\u0302\u1d40x\u0302 k )\u22121\n\u2248 ( 1k + 1\nk 1kr\u0302\n\u1d40x\u0302 + k ` c\u0302\u2032 + k\u03c8\u0302\n\u2032\u1d40 x\u0302 )( 1\u2212 r\u0302 \u1d40x\u0302 k )\n\u2248 1k + 1\nk 1kr\u0302\n\u1d40x\u0302 + k ` c\u0302\u2032 + k\u03c8\u0302 \u2032\u1d40 x\u0302\u2212 1 k 1kr\u0302 \u1d40x\u0302\n= 1k + k ` c\u0302\u2032 + k\u03c8\u0302 \u2032\u1d40 x\u0302\nNotice that the above equation is exact if r\u0302(j) = 0 for all rows. (ii) Equation (7): Here we get the same result much faster:\ny = k\u03c8\u2032\u1d40x\n= ( 1k1 \u1d40 ` + k\u03c8\u0302 \u2032\u1d40) \u00b7 (1 ` 1` + x\u0302 ) = 1k + k \u03c8\u0302 \u2032\u1d40 1`\ufe38 \ufe37\ufe37 \ufe38\nc\u0302\u2032\n1 ` + 1k 1 \u1d40 ` x\u0302\ufe38\ufe37\ufe37\ufe38 0 +k\u03c8\u0302 \u2032\u1d40 x\u0302\n= 1k + k ` c\u0302\u2032 + k\u03c8\u0302 \u2032\u1d40 x\u0302\n9A quick illustration: 1 k 1kr\u0302 \u1d40 = 1 1 [1, 1]\u1d40[\u22120.06, 0, 0.06] = 1 2 [\u22120.06 0 0.06 \u22120.06 0 0.06 ] for \u03c8 in Fig. 4.\nIt follows that Eq. (7) is an approximation of Eq. (6), in general, and both equations are equivalent if each row in \u03c8 is centered around 1.\nNotice that, since y(j) = 1 + y\u0302(j), we can express the update equation in terms of residuals as\ny\u0302 = k ` c\u0302\u2032 + k\u03c8\u0302 \u2032\u1d40 x\u0302\nFurther notice that if each column in the original potential is centered around 1, then the term c\u0302\u2032 disappears. Overall, Lemma 10 implies that by recentering the coupling matrix, we can replace the normalizer with a constant, which considerably simplifies our later derivations. The proof also showed that the approximation becomes exact if each row in \u03c8 is centered around 1.\nExample 11 (Recentering (Example 9 continued)). Consider again matrix \u03c8 \u2208 R3\u00d72 in Fig. 4: The matrix is centered around 1 as the sum of its entries is s = 6. However, row 1 is not centered around 1 as its row sum r(1) = 1.94 instead of 2. Next assume x = [0.1, 0.1, 0.8]\u1d40. Then y = [0.99021, 1.00979]\u1d40 for Eq. (6), but y = [0.99, 1.01]\u1d40 with Eq. (7). Thus, the residuals are \u00b10.00979 and \u00b10.01, respectively, and the relative difference \u2248 2%."}, {"heading": "Centered BP", "text": "By using the previous lemma and focusing on the residuals only, we can next transform the belief update equations from multiplication into addition:\nLemma 12 (Centered BP). By appropriately centering the coupling matrix, beliefs and messages, the equations for belief propagation can be approximated by:\ny\u0302s(j)\u2190 x\u0302s(j) + 1\nks\n\u2211\nu\u2208N(s) m\u0302us(j) (8)\nm\u0302st(i)\u2190 kt ks c\u0302st(i)\n\u2032 + kt \u2211\nj\n\u03c8\u0302\u2032st(j, i) ( y\u0302s(j)\u2212 1\nks m\u0302ts(j)\n) (9)\nProof Lemma 12. (i) Equation (8): Substituting the expansions into the belief updates Eq. (1) leads to\n1\nks + y\u0302s(j)\u2190\n1 Zs \u00b7 ( 1 ks + x\u0302s(j) ) \u00b7 \u220f\nu\u2208N(s)\n( 1 + m\u0302us(j) )\nln ( 1 + ksy\u0302s(i) ) \u2190 \u2212 lnZs + ln ( 1 + ksx\u0302s(j) ) + \u2211 u\u2208N(s) ln ( 1 + m\u0302us(j) )\nWe then use the approximation ln(1 + ) \u2248 for small :\nky\u0302s(j)\u2190 \u2212 lnZs + kx\u0302s(j) + \u2211\nu\u2208N(s) m\u0302us(j) (10)\nSumming both sides over j gives us:\nks \u2211\nj\ny\u0302s(j)\n\ufe38 \ufe37\ufe37 \ufe38 =0\n\u2190 \u2212ks lnZs + ks \u2211\nj\nx\u0302s(j)\n\ufe38 \ufe37\ufe37 \ufe38 =0\n+ \u2211\nj\n\u2211\nu\u2208N(s) m\u0302us(j) \ufe38 \ufe37\ufe37 \ufe38 =0\nHence, we see that lnZs needs to be 0, and therefore our normalizer is actually a normalization constant and for all nodes Zs = 1. Plugging Zs = 1 back into Eq. (10) leads to Eq. (8):\ny\u0302s(j)\u2190 x\u0302s(j) + 1\nks\n\u2211\nu\u2208N(s) m\u0302us(j)\n(ii) Equation (9): Using Lemma 10, we can write Eq. (2) as follows (recall that kt and \u03c8\u2032st take care of the normalization):\nmst(i)\u2190 kt \u2211\nj\n\u03c8\u2032st(j, i)xs(j) \u220f\nu\u2208N(s)\\t mus(j)\nBy further using Eq. (8), we get:\n\u2190 kt \u2211\nj\n\u03c8\u2032st(j, i) xs(j)\n\u220f u\u2208N(s)mus(j)\nmts(j)\n\u2190 kt \u2211\nj\n\u03c8\u2032st(j, i) ys(j)\nmts(j)\nThen, using the centering together with the approximation 1 k+ 1 1+ 2 \u2248 1k + 1 \u2212 1k 2, we get:\n1 + m\u0302st(i)\u2190 kt \u2211\nj\n( 1 kt + \u03c8\u0302\u2032st(j, i) ) 1 ks + y\u0302s(j) 1 + m\u0302ts(j)\n\u2190 kt \u2211\nj\n( 1 kt + \u03c8\u0302\u2032st(j, i) )( 1 ks + y\u0302s(j)\u2212 1 ks m\u0302ts(j) )\n\u2190 kt ( 1 kt + 1 ks \u2211\nj\n\u03c8\u0302\u2032st(j, i)\n\ufe38 \ufe37\ufe37 \ufe38 =c\u0302\u2032st(i)\n+ 1\nkt\n\u2211\nj\ny\u0302s(j)\n\ufe38 \ufe37\ufe37 \ufe38 =0\n+ \u2211\nj\n\u03c8\u0302\u2032st(j, i) y\u0302s(j)\n\u2212 1 kskt\n\u2211\nj\nm\u0302ts(j)\n\ufe38 \ufe37\ufe37 \ufe38 0\n\u2212 1 ks\n\u2211\nj\n\u03c8\u0302\u2032st(j, i) m\u0302ts(j) )\nm\u0302st(i)\u2190 kt ks c\u0302\u2032i + kt\n\u2211\nj\n\u03c8\u0302\u2032st(j, i) y\u0302s(j)\u2212 kt ks\n\u2211\nj\n\u03c8\u0302\u2032st(j, i) m\u0302ts(j)\nEquations (8) and (9) can be written in matrix notation as:\ny\u0302s \u2190 ( x\u0302s + 1 ks \u00b7 \u2211\nu\u2208N(s) m\u0302us\n)\nm\u0302st \u2190 kt ks c\u0302\u2032st + kt\u03c8\u0302 \u2032\u1d40 st\n( y\u0302s \u2212 1\nks m\u0302ts\n)\n(11)\n(12)\nAn alternative way to write the message updates is\nm\u0302st \u2190 kt ks c\u0302\u2032st + kt\u03c8\u0302 \u2032\u1d40 st\n( x\u0302s + 1 ks \u00b7 \u2211\nu\u2208N(s)\\t m\u0302us\n) (13)\nIt is instructive to compare the above derived linearized update equations against the matrix formulations of the original BP update equations Eq. (1) and Eq. (2): by using the symbol for the Hadamard product10, those can be written compactly in matrix notation, as:\nys \u2190 1\nZs\n( xs\n\u2299\nu\u2208N(s) mus\n) (14)\nmst \u2190 1 Zst \u03c8\u1d40st ( xs\n\u2299\nu\u2208N(s)\\t mus\n) (15)\n10The Hadamard product is defined by: Z=X Y \u21d4 Z(i, j)=X(i, j) \u00b7 Y (i, j).\nNotice that the potential \u03c8st is represented by a ks \u00d7 kt-dimensional \u201ccompatibility matrix\u201d and that the transpose \u03c8\u1d40st = \u03c8ts (see Fig. 6). This follows from the definition of a potential in a pairwise MRF and the resulting derivation of belief propagation (Yedidia, Freeman, and Weiss 2003). Also notice that we could reduce the amount of necessary calculation by first multiplying all incoming messages at a node, and then dividing through the message that a node sends to itself via a neighbor (we call this compensation \u201cecho cancellation\u201d). This approach is also called \u201cmessage-passing with division\u201d (Koller and Friedman 2009) (or \u201cbelief-update message passing\u201d) and can be made precise by defining a component-wise division operator by: Z=X Y \u21d4 Z(i, j)=X(i, j)/Y (i, j) where 0/0 = 0. Equation (15) can then be written more concisely as:\nmst \u2190 1 Zst \u03c8\u1d40st ( ys mts ) (16)\nWe invite the reader to carefully compare Eqs. (11) and (12) with the original BP update equations Eqs. (11) and (13). Notice that the first term ktks c\u0302 \u2032 st in Eq. (12) vanishes in the case of doubly stochastic potentials (or more generally, potentials with equal column and row sums). For non-doubly stochastic potentials, this term captures the prior probabilities of node classes resulting from non-equivalent row or column sums."}, {"heading": "Steady state messages", "text": "From Lemma 12, we can derive a closed-form equation for the message in steady-state of belief propagation.\nLemma 13 (Steady state messages). After convergence of belief propagation, message propagation can be approximated in terms of the steady centered beliefs as:\nm\u0302st = kt ks c\u0302\u2032st + kt\u03c8\u0302 \u2032\u1d40 st ( y\u0302s \u2212 \u03c8\u0302 \u2032\u2032 sty\u0302t ) (17)\nProof Lemma 13. To increase the readability of this proof, we ignore the subscripts in \u03c8st, cst, rst, and write instead \u03c8, c, r, respectively. We start by writing Eq. (9) for the messages in each of the two directions across the same edge s\u2212 t:\nm\u0302st(i)\u2190 kt ks c\u0302(i)\u2032 + kt\nks\u2211\nj=1\n\u03c8\u0302\u2032(j, i) ( y\u0302s(j)\u2212 1\nks m\u0302ts(j)\n)\nm\u0302ts(j)\u2190 ks kt r\u0302(j)\u2032\u2032 + ks\nkt\u2211\ng=1\n\u03c8\u0302\u2032\u2032(g, j) ( y\u0302t(g)\u2212 1\nkt m\u0302st(g)\n)\nWe then simply combine both equations into one:\nm\u0302st(i)\u2190 kt ks c\u0302(i)\u2032 + kt\nks\u2211\nj=1\n\u03c8\u0302\u2032(j, i) y\u0302s(j)\u2212 kt ks\nks\u2211\nj=1\n\u03c8\u0302\u2032(j, i) \u00b7\n(ks kt r\u0302(j)\u2032\u2032+ ks kt\u2211\ng=1\n\u03c8\u0302\u2032\u2032(g, j) y\u0302t(g)\u2212 ks kt\nkt\u2211\ng=1\n\u03c8\u0302\u2032\u2032(g, j) m\u0302st(g) )\nNow, if the equations converge, then m\u0302st(g) on both the left and right side of the equation need to be equivalent. We can, therefore, replace the update symbol with equality and group related terms together:\nm\u0302st(i)\u2212 ktks kskt\nks\u2211\nj=1\n\u03c8\u0302\u2032(j, i) kt\u2211\ng=1\n\u03c8\u0302\u2032\u2032(g, j) m\u0302st(g) =\nkt ks c\u0302(i)\u2032 \u2212 ktks kskt\nks\u2211\nj=1\n\u03c8\u0302\u2032(j, i) r\u0302(j)\u2032\u2032 + kt\nks\u2211\nj=1\n\u03c8\u0302\u2032(j, i) y\u0302s(j)\u2212 kt ks\nks\nks\u2211\nj=1\n\u03c8\u0302\u2032(j, i) kt\u2211\ng=1\n\u03c8\u0302\u2032\u2032(g, j) y\u0302t(g)\nWith Ikt as the kt-dimensional identity matrix, this can then be written in matrix notation as:\n(Ikt \u2212 \u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 )m\u0302st = kt ks c\u0302\u2032 \u2212 \u03c8\u0302\u2032\u1d40r\u0302\u2032\u2032 + kt\u03c8\u0302 \u2032\u1d40 y\u0302s \u2212 kt\u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 y\u0302t\nRecall that c\u0302\u2032 = \u03c8\u0302 \u2032\u1d40 1ks and r\u0302 \u2032\u2032 = \u03c8\u0302 \u2032\u2032 1kt .\n(Ikt \u2212 \u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 )m\u0302st = ( kt ks \u03c8\u0302 \u2032\u1d40 1ks \u2212 \u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 1kt ) + kt\u03c8\u0302 \u2032\u1d40 y\u0302s \u2212 kt\u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 y\u0302t\n= \u03c8\u0302 \u2032\u1d40( kt\nks 1ks + kty\u0302s\n) \u2212 \u03c8\u0302\u2032\u1d40\u03c8\u0302\u2032\u2032 ( 1kt + kty\u0302t )\nIf all entries of \u03c8\u0302 are appropriately small (so that the spectral radius \u03c1(\u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 ) < 1), then the inverse of (Ikt \u2212 \u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 ) exists. Thus, by further substituting \u03c8\u0302 \u2032\u1d40 4 := (Ikt \u2212 \u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 )\u22121\u03c8\u0302 \u2032\u1d40 , we can write:\nm\u0302st = \u03c8\u0302 \u2032\u1d40 4 ( kt ks 1ks + kty\u0302s ) \u2212 \u03c8\u0302\u2032\u1d404\u03c8\u0302 \u2032\u2032( 1kt + kty\u0302t ) (18)\n= \u03c8\u0302 \u2032\u1d40 4 ( kt ks 1ks \u2212 \u03c8\u0302 \u2032\u2032 1kt ) + kt\u03c8\u0302 \u2032\u1d40 4y\u0302s \u2212 kt\u03c8\u0302 \u2032\u1d40 4\u03c8\u0302 \u2032\u2032 y\u0302t\nBy further substituting h\u0302 \u2032 := ktks \u03c8\u0302 \u2032\u1d40 4 1ks \u2212 \u03c8\u0302 \u2032\u1d40 4\u03c8\u0302 \u2032\u2032 1kt , we get the following equation for the message updates Eq. (12) after convergence:\nm\u0302st = h\u0302 \u2032 + kt\u03c8\u0302 \u2032\u1d40 4 ( y\u0302s \u2212 \u03c8\u0302 \u2032\u2032 y\u0302t )\n(19)\nNext notice that \u03c8\u2032\u1d404 \u2248 \u03c8\u2032\u1d40, since Ikt |\u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032|, and therefore (Ikt \u2212 \u03c8\u0302 \u2032\u1d40 \u03c8\u0302 \u2032\u2032 )\u22121 \u2248 Ikt . From this, we can now also approximate h\u0302 \u2032 \u2248 ktks ( \u03c8\u0302 \u2032\u1d40 1ks ) \u2212 \u03c8\u0302\u2032\u1d40 ( \u03c8\u0302 \u2032\u2032 1kt ) = ktks c\u0302 \u2032 \u2212 \u03c8\u0302\u2032\u1d40r\u0302\u2032\u2032. Further ignoring the second term, we get h\u0302\u2032 \u2248 ktks c\u0302\n\u2032. Plugging back into Eq. (19) finally gives us Eq. (17).\nAlso notice that we can alternatively write Eq. (18) as function of the uncentered beliefs, which results in a very intuitive equation:\nm\u0302st = kt\u03c8\u0302 \u2032\u1d40 4 ( 1 ks 1ks + y\u0302s ) \u2212 kt\u03c8\u0302 \u2032\u1d40 4\u03c8\u0302 \u2032\u2032( 1 kt 1kt + y\u0302t )\n= kt\u03c8\u0302 \u2032\u1d40 4 ( ys ) \u2212 kt\u03c8\u0302 \u2032\u1d40 4\u03c8\u0302 \u2032\u2032( yt ) = kt\u03c8\u0302 \u2032\u1d40 4 ( ys \u2212 \u03c8\u0302 \u2032\u2032 yt )\nTheorem 4: The actual linearization Finally, by using matrix notation, we can transform and write Eq. (17) for all nodes and edges together as one large equation system and get Theorem 4.\nProof Theorem 4. For steady-state, we can write Eq. (8) in vector form as:\ny\u0302s = x\u0302s + 1\nks\n\u2211\nu\u2208N(s) m\u0302us\nBy permuting subscripts, we can also write Eq. (17) as\nm\u0302us = ks ku c\u0302\u2032us + ks\u03c8\u0302 \u1d40 us(y\u0302u \u2212 \u03c8\u0302 \u2032\u2032 usy\u0302s)\nCombining the last two equations, we get\ny\u0302s = x\u0302s\n\ufe38\ufe37\ufe37\ufe38 1st\n+ \u2211\nu\u2208N(s)\nc\u0302\u2032us ku\n\ufe38 \ufe37\ufe37 \ufe38 2nd\n+ \u2211\nu\u2208N(s) \u03c8\u0302 \u2032\u1d40 usy\u0302u\n\ufe38 \ufe37\ufe37 \ufe38 3rd\n\u2212 \u2211\nu\u2208N(s) \u03c8\u0302 \u2032\u1d40 us\u03c8\u0302 \u2032\u2032 usy\u0302s\n\ufe38 \ufe37\ufe37 \ufe38 4th\n(20)\nBy using our combined new vectors and matrices y\u0302, x\u0302, k, and \u03c8\u0302 (and analogously for the column-recentered residual matrix \u03c8\u0302 \u2032\u2032 ), we can write Eq. (20) in matrix form as:\ny\u0302 = X\u0302 + \u03c8\u0302 \u2032\u1d40 k + \u03c8\u0302 \u2032\u1d40 y\u0302 \u2212 \u03c8\u0302\u2032\u1d40\u03c8\u0302\u2032\u2032y\u0302\nFrom Corollary 8, we know \u03c8\u0302 \u2032\u1d40 ij = \u03c8\u0302 \u2032\u2032 ji. Therefore, from our construction we also have \u03c8\u0302 \u2032\u1d40 = \u03c8\u0302 \u2032\u2032 . We thus get\ny\u0302 = x\u0302 + \u03c8\u0302 \u2032\u1d40 k + \u03c8\u0302 \u2032\u1d40 y\u0302 \u2212 \u03c8\u0302\u2032\u1d402y\u0302 (21)\nEquation (21) is now a straight-forward linear equation system that can be solved for y\u0302. To finish the proof, first notice that each of our approximations become exact for lim \u21920+ . Second, notice that higher order deltas vanish and our equation simplify to y\u0302 = x\u0302 + \u03c8\u0302 \u2032\u1d40 k + \u03c8\u0302 \u2032\u1d40 y\u0302. While the individual beliefs go to zero during this limit consideration, their relative sizes stay the same, and thus the Maximum Marginal for each node stays the same.\nProposition 5: Update equations and convergence Proof Proposition 5. From the Jacobi method for solving linear systems (Saad 2003) \u2013 also known as the Neumann series expansion of the matrix inverse \u2013 we know that the solution for y = (I\u2212M)\u22121x can be calculated (under certain conditions) via the iterative update equation\ny(r+1) \u2190 x + My(r)\nThese updates are known to converge for any choice of initial values for y(0), as long as M has a spectral radius \u03c1(M) < 1. The same convergence guarantees carry over to Eq. (5). We thus know that the update equation Eq. (5) converges if and only if the spectral radius of the matrix \u03c8\u0302 \u2032\u1d40 \u2212 \u03c8\u0302\u2032\u1d402 is smaller than 1.\nC Special formulations In this section, we derive alternative formulations of Theorem 4 for increasingly specialized cases: Graphs with node and edge types (Appendix C), graphs with equal number of classes for all nodes (Appendix C), graphs with one single directed edge potential (Appendix C), and the special case described in prior work of one single symmetric, doubly stochastic potential (Appendix C).\nNode types and edge types with repeated potentials In many realistic scenarios, the number of edges is usually larger than the number of different edge types (or edge potentials). For example, assume a set Q of different node types.11 We then have a |Q|-partite network and each node with type q \u2208 Q can be one of k(q) classes. Further assume that the couplings along an edge only depend on the types at both ends of the edge. Then there are maximal |Q|2 different row-recentered potentials irrespective of the size of the network, whereas the most general formulation of Theorem 4 would redundantly store in \u03c8\u0302 \u2032 one full potential for each edge (Recall that we have one rowrecentered potential for every edge direction, thus two for every edge type). In the following, we transform the update equations so that every different row-recentered edge potential appears only once in the equations. Notice that the ensuing formulation allows for more than one potential between any pair of node types.\nA complication in deriving a compact matrix formulation is that different types of nodes may have different numbers of classes. We address this issue by creating separate matrices that contain the beliefs of nodes with the same number of classes. Concretely, let N = {1, 2, . . . , n} be the set of all nodes, q(s) be the type of node s, k(q) be the number of classes for type q, andK = {k(1), k(2), . . . , k(|Q|)} be the set of numbers of classes across all nodes.12 LetNk \u2286 N denote the set of nodes with k \u2208 K classes so that all nodes are partitioned into groups Nk1 , Nk2 , . . . , Nk|K| . Let nk = |Nk| denote the number of nodes with k classes. We assume a numbering of nodes such that Nk1 = {1, 2, . . . , nk1}, Nk2 = {nk1 + 1, nk1 + 2, . . . , nk1 + nk2}, and so on. Given this convention, each node s has a unique order o(s) within the set Nk(q(s)). For each k \u2208 K, we create two nk \u00d7 k matrices Y\u0302k and X\u0302k that contain the posterior and prior residual beliefs of all nodes with k classes.\nFor each potential \u03c8 \u2208 R`\u00d7k, we create two centered residual potentials \u03c8\u0302\u2032 \u2208 R`\u00d7k and \u03c8\u0302\u2032\u2032\u1d40 = (\u03c8\u0302\u1d40)\u2032 \u2208 Rk\u00d7` that correspond to the two modulations across the two directions of an edge. For notational convenience, we treat them as two distinct potentials and ignore their common ancestry. For example,\u03c812 \u2208 R3\u00d72 leads to \u03c8\u0302 \u2032 12 \u2208 R3\u00d72 and \u03c8\u0302 \u2032 21 = \u03c8\u0302 \u2032\u2032\u1d40 12 \u2208 R2\u00d73. For each newly created row-recentered residual potential \u03c8\u0302 \u2032 \u2208 R`\u00d7k, we create two new matrices: (i) the adjacency matrix W \u03c8\u0302 \u2032 \u2208 Rn`\u00d7nk with W \u03c8\u0302 \u2032(o(s), o(t)) = 1 if node s with ` classes is connected to node t with k classes via an edge potential \u03c8\u0302 \u2032 ; and (ii) the diagonal in-degree matrix Din\n\u03c8\u0302 \u2032 \u2208 Rnk\u00d7nk with Din \u03c8\u0302 \u2032(o(t), o(t)) = d if there are d different nodes s that are\nconnected to t via an edge potential \u03c8\u0302 \u2032 (notice that \u03c8\u0302 \u2032\nmodulates along the direction s \u2192 t, therefore \u201cin-degree\u201d at node t with k classes).\n11Notice our use of vocabulary: the \u201ctype\u201d of a node in a network is observed and known a priori (e.g., whether the node represents a user or a product), whereas the \u201cclass\u201d of a node is the label that we are trying to learn (e.g., whether the user is male or female).\n12In a slight abuse of set notation, we allow here e.g. {1, 1, 2} to stand for a set {1, 2}.\nProposition 14 (Edge types). Let P\u0302 \u2032 bet the set of all row-recentered potentials, P\u0302 \u2032`\u00d7k \u2286 P \u2032 be the subset with dimensions ` \u00d7 k, and Y\u0302k, X\u0302k, W\u03c8\u0302\u2032 , D in \u03c8\u0302 \u2032 be the above defined partitioned matrices for all k \u2208 K. For each \u03c8\u0302\u2032 \u2208 P\u0302 \u2032 let \u03c8\u0302\u2032\u2032 be the corresponding column-recentered potential. The update equation Eq. (5) can then be written as follows:\n\u2200k \u2208 K : Y\u0302k \u2190 X\u0302k + C\u0302 \u2032 k\u2217 +\n\u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\n( W\u1d40 \u03c8\u0302 \u2032Y\u0302`\u03c8\u0302 \u2032 \u2212Din \u03c8\u0302 \u2032Y\u0302k\u03c8\u0302 \u2032\u2032\u1d40 \u03c8\u0302 \u2032)\n(22)\nwith\nC\u0302 \u2032 k\u2217 :=\n\u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\n1 ` W\u1d40 \u03c8\u0302 \u20321n`1 \u1d40 ` \u03c8\u0302 \u2032\nProof Proposition 14. We are going to derive Eq. (22) from Eq. (20). For convenience, we repeat here both equations:\nY\u0302k = X\u0302k + C\u0302 \u2032 k\u2217+\n\u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\n( W\u1d40 \u03c8\u0302 \u2032Y\u0302`\u03c8\u0302 \u2032\u2212Din \u03c8\u0302 \u2032Y\u0302k\u03c8\u0302 \u2032\u2032\u1d40 \u03c8\u0302 \u2032)\ny\u0302s = x\u0302s\n\ufe38\ufe37\ufe37\ufe38 1st\n+ \u2211\nu\u2208N(s)\nc\u0302\u2032us ku\n\ufe38 \ufe37\ufe37 \ufe38 2nd\n+ \u2211\nu\u2208N(s) \u03c8\u0302 \u2032\u1d40 usy\u0302u\n\ufe38 \ufe37\ufe37 \ufe38 3rd\n\u2212 \u2211\nu\u2208N(s) \u03c8\u0302 \u2032\u1d40 us\u03c8\u0302 \u2032\u2032 usy\u0302s\n\ufe38 \ufe37\ufe37 \ufe38 4th\nWe need to show that any vector y\u0302\u1d40s for a node s with k classes is equivalent to the o(s)-th row of Y\u0302k, for which we are going to write Y\u0302k(o(s), :) from now on (recall that o(s) is the order of node s within Nk). We show the equivalence for each the 4 terms separately:\n1st term: x\u0302\u1d40s = X\u0302k(o(s), :) by construction. 2nd term: For the following, recall that c\u0302\u2032us = \u03c8\u0302 \u2032\u1d40 us1ku :\n(\u2211\nu\u2208N(s)\n1 ku c\u0302\u2032us )\u1d40 = \u2211\nu\u2208N(s)\n1\nku c\u0302\u2032\u1d40us\n= \u2211\nu\u2208N(s)\n1\nku 1\u1d40ku\u03c8\u0302 \u2032 us\n= \u2211\nu\u2208N(s)\n1\nku W \u1d40 \u03c8\u0302 \u2032 us (o(s), o(u))1\u1d40ku\u03c8\u0302 \u2032 us\n= \u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\n1 ` W\u1d40 \u03c8\u0302 \u2032(o(s), :)1n`1 \u1d40 ` \u03c8\u0302 \u2032\n= C\u0302 \u2032 k\u2217(o(s), :)\n3rd term: (\u2211\nu\u2208N(s) \u03c8\u0302 \u2032\u1d40 usy\u0302u\n)\u1d40 = \u2211\nu\u2208N(s) y\u0302\u1d40u\u03c8\u0302 \u2032 us\n= \u2211\nu\u2208N(s) Y\u0302ku(o(u), :)\u03c8\u0302\n\u2032 us\n= \u2211\nu\u2208N(s) W \u1d40 \u03c8\u0302 \u2032 us (o(s), o(u))Y\u0302ku(o(u), :)\u03c8\u0302 \u2032 us\n= \u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\nW\u1d40 \u03c8\u0302 \u2032(o(s), :)Y\u0302`\u03c8\u0302\n\u2032\n= ( \u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\nW\u1d40 \u03c8\u0302 \u2032Y\u0302`\u03c8\u0302\n\u2032) (o(s), :)\n4th term: (\u2211\nu\u2208N(s) \u03c8\u0302 \u2032\u1d40 us\u03c8\u0302 \u2032\u2032 usy\u0302s\n)\u1d40 = \u2211\nu\u2208N(s) y\u0302\u1d40s \u03c8\u0302 \u2032\u2032\u1d40 us\u03c8\u0302 \u2032 us\n= \u2211\nu\u2208N(s) Y\u0302k(o(s), :)\u03c8\u0302\n\u2032\u2032\u1d40 us\u03c8\u0302 \u2032 us\n= \u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\nDin \u03c8\u0302 \u2032(o(s), o(s)) Y\u0302k(o(s), :)\u03c8\u0302\n\u2032\u2032\u1d40 \u03c8\u0302 \u2032\n= ( \u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032`\u00d7k,`\u2208K\nDin \u03c8\u0302 \u2032Y\u0302k\u03c8\u0302\n\u2032\u2032\u1d40 \u03c8\u0302 \u2032) (o(s), :)\nNodes with same number of classes k Proposition 14 simplifies considerably when all nodes have the same number of classes k:\nCorollary 15 (Same k). Let k be the number of classes for each node in the graph, P\u0302 \u2032 the set of row-recentered residual edge potentials (all with k\u00d7 k dimensions), Y\u0302 and X\u0302 the n\u00d7 k dimensional final and explicit belief matrices, and W\n\u03c8\u0302 \u2032 and Din \u03c8\u0302 \u2032\nthe adjacency and in-degree matrices for each potential \u03c8\u0302 \u2208 P\u0302 \u2032. The update equations can then be simplified to:\nY\u0302 \u2190 X\u0302 + C\u0302\u2032\u2217 + \u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032\n( W\u1d40 \u03c8\u0302 \u2032Y\u0302\u03c8\u0302 \u2032 \u2212Din \u03c8\u0302 \u2032Y\u0302\u03c8\u0302 \u2032\u2032\u1d40 \u03c8\u0302 \u2032)\n(23)\nwith\nC\u0302 \u2032 \u2217 := 1\nk\n\u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032\nW\u1d40 \u03c8\u0302 \u20321n1 \u1d40 k\u03c8\u0302 \u2032\nAlso the convergence criterium and the closed-form solution allow very concise formulations. For this step we need to introduce two new notations: Let xj denote the j-th column of matrix X (i.e., X = {xij} = [x1 . . .xn]) and let X and Y be matrices of order m \u00d7 n and p \u00d7 q, respectively. First, the vectorization of a matrix X stacks its columns one underneath the other to form a single column vector:\nvec (X) =   x1 ... xn  \nSecond, the Kronecker product (\u2297) of X and Y results in a mp\u00d7 nq block matrix:\nX\u2297Y =   x11Y x12Y . . . x1nY ... ... . . . ...\nxm1Y xm2Y . . . xmnY\n \nWith these notations, Corollary 6 now becomes\nProposition 16 (Convergence with same k). Update Eq. (23) converges if and only if the spectral radius \u03c1(M) < 1 for\nM := \u2211\n\u03c8\u0302 \u2032\u2208P\u0302\u2032\n( W\u1d40 \u03c8\u0302 \u2032 \u2297 \u03c8\u0302 \u2032\u1d40\u2212Din \u03c8\u0302 \u2032 \u2297 (\u03c8\u0302\u2032\u1d40\u03c8\u0302\u2032\u2032) )\nFurthermore, let y\u0302 := vec ( Y\u0302 \u1d40) , x\u0302 := vec ( X\u0302 \u1d40) , and c\u0302\u2032\u2217 := vec ( C\u0302 \u2032\u1d40 \u2217 ) . The closed-form solution of Eq. (23) is given by:\ny\u0302 = ( Ink \u2212M )\u22121( x\u0302 + c\u0302\u2032\u2217 ) (24)\nNotice that Eq. (24) is a special case of Eq. (4) that factors out repeated edge potentials. This concise factorization with the Kronecker product is only possible if all nodes have the same number of classes k.\nProof Proposition 16. If all nodes have the same number of classes k then all final and explicit beliefs form single n\u00d7k matrices\nY\u0302 and X\u0302. Furthermore, all potentials have k \u00d7 k dimensions. Hence, Eq. (23) can be written as a single matrix equation:\nY\u0302 = X\u0302 + C\u0302\u2217 + \u2211\n\u03c8\u0302 \u2032\u2208P\u2032\n( W\u1d40 \u03c8\u0302 \u2032Y\u0302\u03c8\u0302 \u2032 \u2212D \u03c8\u0302 \u2032Y\u0302\u03c8\u0302 \u2032\u2032\u1d40 \u03c8\u0302 \u2032)\nY\u0302 \u1d40 = X\u0302 \u1d40 + C\u0302 \u1d40 \u2217 +\n\u2211\n\u03c8\u0302 \u2032\u2208P\u2032\n( \u03c8\u0302 \u2032\u1d40 Y\u0302 \u1d40 W \u03c8\u0302 \u2032 \u2212 \u03c8\u0302\u2032\u1d40\u03c8\u0302\u2032\u2032Y\u0302\u1d40D \u03c8\u0302 \u2032 )\nwith C\u0302 \u1d40 \u2217 := 1 k \u2211 \u03c8\u0302 \u2032\u2208P\u2032 \u03c8\u0302 \u2032\u1d40 W \u03c8\u0302 \u2032 . We used the transpose in order for the later vectorization vec to create vectors where the\ndifferent beliefs of a node are adjacent (otherwise vec(Y\u0302) results in a vector where all beliefs in the same class from different nodes are adjacent). We next use Roth\u2019s column lemma (Henderson and Searle 1981; Roth 1934) that states that\nvec (XYZ) = (Z\u1d40\u2297X) vec (Y) to rewrite this equation as\ny\u0302 = x\u0302 + c\u0302\u2032\u2217 + ( \u2211\n\u03c8\u0302 \u2032\u2208P\u2032\n( W\u1d40 \u03c8\u0302 \u2032 \u2297 \u03c8\u0302 \u2032\u1d40 \u2212D \u03c8\u0302 \u2032 \u2297 (\u03c8\u0302\u2032\u1d40\u03c8\u0302\u2032\u2032) )) y\u0302\nfor y\u0302 = vec ( Y\u0302 \u1d40) , x\u0302 = vec ( X\u0302 \u1d40) , and c\u0302\u2032\u2217 = vec ( C\u0302 \u2032\u1d40 \u2217 ) . Using the substitution\nM := \u2211\n\u03c8\u0302 \u2032\u2208P\u2032\n( W\u1d40 \u03c8\u0302 \u2032 \u2297 \u03c8\u0302 \u2032\u1d40\u2212D \u03c8\u0302 \u2032 \u2297 (\u03c8\u0302\u2032\u1d40\u03c8\u0302\u2032\u2032) )\nand reforming the equation leads to the closed-form solution:\ny\u0302 = ( Ink \u2212M )\u22121( x\u0302 + c\u0302\u2032\u2217 )\nwhich is defined if the spectral radius \u03c1 of M is smaller than 1.\nOne single directed edge type Equation (23) simplifies further when we have just one single edge potential. In other words, we have a directed network and assume only one single type of edge whose meaning changes across the two directions (e.g., who follows some type of person on Twitter has a different meaning from who is followed by same type).\nCorollary 17 (One potential). Let k be the number of classes for each node in the graph, \u03c8 the k \u00d7 k-dimensional potential across an edge in the direction from source to target, Y\u0302 and X\u0302 the n \u00d7 k dimensional final and explicit belief matrices, W the directed adjacency matrix, and Din (Dout) the in-degree (out-degree) matrices. Then the update equations simplify to:\nY\u0302 \u2190 X\u0302 + C\u0302\u2032\u2217 + W\u1d40Y\u0302\u03c8\u0302 \u2032 + WY\u0302\u03c8\u0302 \u2032\u2032\u1d40 \u2212DinY\u0302\u03c8\u0302\u2032\u2032\u1d40\u03c8\u0302\u2032 \u2212DoutY\u0302\u03c8\u0302\u2032\u03c8\u0302\u2032\u2032\u1d40 (25) with\nC\u0302 \u2032 \u2217 := 1\nk\n( W\u1d401n1 \u1d40 k\u03c8\u0302 \u2032 + W1n1 \u1d40 k\u03c8\u0302 \u2032\u2032\u1d40)\nCorollary 18 (Convergence). Update Eq. (25) converges if and only if \u03c1(M) < 1 for\nM = W\u1d40 \u2297 \u03c8\u0302\u2032\u1d40 + W \u2297 \u03c8\u0302\u2032\u2032 \u2212Din \u2297 (\u03c8\u0302\u2032\u1d40\u03c8\u0302\u2032\u2032)\u2212Dout \u2297 (\u03c8\u0302\u2032\u2032\u03c8\u0302\u2032\u1d40)\nOne symmetric, doubly stochastic potential Recent work (Gatterbauer et al. 2015) derived a linearization of BP for the special case of one single symmetric, doubly stochastic edge potential that is used throughout the network (Recall that for such a potential all residual row and column sums are 0, and that by multiplying it by the number of classes it will be centered around 1). We can recover this special case from Corollary 15 and Proposition 16 with a slightly updated notation.\nProposition 19 (One symmetric, doubly stochastic potential). If the MRF contains only one single edge type with a symmetric doubly stochastic potential \u03c8, then the update equations simplify to:\nY\u0302 \u2190 X\u0302 + WY\u0302\u03c8\u0302\u2032 \u2212DY\u0302\u03c8\u0302\u20322\nAt the same time, the closed form solution simplifies to:\ny\u0302 = ( Ink \u2212W \u2297 \u03c8\u0302 \u2032 + D\u2297 \u03c8\u0302\u20322 )\u22121 x\u0302 (26)\nProof Proposition 19. First, notice that for any symmetric potential \u03c8 \u2208 Rk\u00d7k, \u03c8\u0302\u2032 = \u03c8\u0302\u2032\u2032 = \u03c8\u0302/k, and hence W\u1d40 \u03c8\u0302 \u2032Y\u0302\u03c8\u0302 \u2032 + W\u1d40 \u03c8\u0302 \u2032\u2032Y\u0302\u03c8\u0302 \u2032\u2032 = (W\u1d40 \u03c8\u0302 \u2032 +W\u03c8\u0302\u2032)Y\u0302\u03c8\u0302 \u2032 . Thus, its adjacency matrix becomes symmetric. Since we only have one potential, we also have only one adjacency matrix W. Furthermore, \u03c8\u0302 \u2032\u1d40 = \u03c8\u0302 \u2032 and hence, \u03c8\u0302 \u2032\u2032\u1d40 \u03c8\u0302 \u2032 = \u03c8\u0302 \u20322\n. Second, the constant term C\u0302 \u2032 \u2217 disappears for doubly stochastic potentials. This follows from the proof of Lemma 12 and the\nfact that in any doubly stochastic matrix \u03c8 \u2208 Rk\u00d7k, each column is centered around 1k . This allows us to simplify Eq. (23) to\nY\u0302 = X\u0302 + WY\u0302\u03c8\u0302 \u2032 \u2212DY\u0302\u03c8\u0302\u20322 (27)\nSimilarly, applying above assumptions to our closed-form solution Eq. (24) leads to:\ny\u0302 = ( Ink \u2212W \u2297 \u03c8\u0302 \u2032 + D\u2297 \u03c8\u0302\u20322 )\u22121 x\u0302\nNotice that Eq. (27) and Eq. (26) are exactly the ones given by (Gatterbauer et al. 2015), except for slightly different notation. In particular, the authors choose to center the potential\u03c8 around 1/k, which is possible in the case that all nodes have the same number of classes k (and thus all potentials are quadratic with k \u00d7 k dimensions).13 We also chose here to formulate Eq. (26) as y\u0302 = vec(Y\u0302 \u1d40 ) instead of vec(Y\u0302) to keep the beliefs of same nodes adjacent in the resulting stacked vectors. Vectorizing instead the transpose, we get the exact original formulation:\nvec(Y\u0302) = ( Ink \u2212 \u03c8\u0302 \u2032 \u2297W+ \u03c8\u0302\u20322 \u2297D )\u22121 vec(X\u0302) (28)\nNotice that in a slight abuse of notation, we used \u03c8\u0302 \u2032\nin Theorem 4 for the sparse ktot \u00d7 ktot-square matrix, whereas we use it here for the single k \u00d7 k recentered residual potential.\nD Illustrating examples Example 20 (Linearization). Consider the network Fig. 7a consisting of nodes N = {1, 2, 3}. Node 1 has three classes, whereas nodes 2 and 3 have two classes. We have two edges, e.g., the edge between nodes 1 and 2 with a 3\u00d7 2 potential\u03c812. Fig. 7b illustrates Eq. (3). Notice that c\u0302\u2032\u2217 = \u03c8\u0302 \u2032\u1d40 k with k = [ 13 , 1 3 , 1 3 , 1 2 , 1 2 , 1 2 , 1 2 ] \u1d40 and ktot = 3 + 2 + 2. Further notice that the matrix \u03c8\u0302 \u2032\u1d402\nis block-diagonal (entries represent the echo modulation that a node receives through all its neighbors). In the following, we write \u3008\u00b7\u30092 for the projection of a stacked vector on the entries for node 2, e.g., \u3008y\u0302\u30092 = y\u03022:\n\u3008x\u0302\u30092 = x\u03022 \u3008c\u0302\u2032\u2217\u30092 = 13\u03c8\u0302 \u2032\u1d40 1213 + 1 2\u03c8\u0302 \u2032\u1d40 3212\n\u3008\u03c8\u0302\u2032\u1d40y\u0302\u30092 = \u03c8\u0302 \u2032\u1d40 12y\u03021 + \u03c8\u0302 \u2032\u1d40 32y\u03023\n\u3008\u03c8\u0302\u2032\u1d402y\u0302\u30092 = ( \u03c8\u0302 \u2032\u1d40 12\u03c8\u0302 \u2032\u1d40 21 + \u03c8\u0302 \u2032\u1d40 32\u03c8\u0302 \u2032\u1d40 23 ) \ufe38 \ufe37\ufe37 \ufe38\n\u03c8\u03022\u2217\ny\u03022\nThen, the single update equation could also be written as several simultaneous update equations:\ny\u03021 \u2190 x\u03021 + \u3008c\u0302\u2032\u2217\u30091 + \u03c8\u0302 \u2032\u1d40 21y\u03022 \u2212 \u03c8\u03021\u2217y\u03021 y\u03022 \u2190 x\u03022 + \u3008c\u0302\u2032\u2217\u30092 + \u03c8\u0302 \u2032\u1d40 12y\u03021 + \u03c8\u0302 \u2032\u1d40 32y\u03023 \u2212 \u03c8\u03022\u2217y\u03022 y\u03023 \u2190 x\u03023 + \u3008c\u0302\u2032\u2217\u30093 + \u03c8\u0302 \u2032\u1d40 23y\u03021 \u2212 \u03c8\u03023\u2217y\u03023\nExample 21 (Repeating potentials). We use Example 20 to also illustrate Proposition 14. Let ysj be the belief of node s in class j. We create two belief matrices \u2200k \u2208 K = {2, 3}: Y\u03022 = [ y21 y22y31 y32 ], and Y\u03023 = [ y11 y12 y13 ]. Thus, for example, the\n13\u201cRow-recentering\u201d and \u201ccolumn-recentering\u201d as proposed in the present paper are more general forms of the centerings proposed in earlier work, which is necessary in order to deal with the general case of a non-quadratic and non-doubly stochastic potentials.\n1 2\n=\n3 b2 b3b1\n= 23 12\n(a)\n= + + \u2212\n321\n1\n2 3  \u030203\u21e4\n \u0302 0 2\u21e4\n \u0302 0 1\u21e4\n \u0302 0| 12\n \u0302 0| 21\n \u0302 0| 23\n \u0302 0| 32\nb\u03021\nb\u03022\nb\u03023\ny\u0302 = x\u0302 + c\u0302\u2032\u2217 + (\n\u03c8\u0302 \u2032\u1d40 \u2212 \u03c8\u0302\u2032\u1d402\n) y\u0302\n(b)\nE Weighted edges in MRFs The notion of \u201cweights\u201d on edges in MRFs is not defined and it is not immediately clear what an appropriate semantics would be. Here we give an a natural interpretation of edge weights in MRFs and derive a modification of the linearization to handle such weighted edges. We derive this interpretation starting from one single axiom:\nAxiom 22 (Edge weights). An edge with weight w \u2208 N behaves identically as w parallel edges with the same potential but weight 1.\nFrom the original BP update equations Eq. (1) and Eq. (2), we see that two parallel edges carry the same messages, and that these two messages need to be multiplied to calculate the resulting messages and beliefs. It follows that these parallel edges behave identically to having one single edge with a new potential \u03c8st \u03c8st, i.e., the result of element-wise multiplying the entries of the original potential. More generally, an edge with a potential \u03c8 and weight w is the same as an unweighted edge with a new potential \u03c8w with entries entries \u03c8w(j, i) = \u03c8(j, i)w.\nTo see how weights affect our linearized formulation in terms of residuals, recall that \u03c8(j, i) = 1 + \u03c8\u0302(j, i). Therefore, \u03c8(j, i)w = ( 1 + \u03c8\u0302(j, i) )w = 1 + w\u03c8\u0302(j, i) + O(\u03c8\u0302(j, i)2). Under the assumption of small deviations from the center, we thus\nget: \u03c8\u0302w(j, i) = w\u03c8\u0302(j, i). Hence, weights on edges simply multiply the residual potentials in our linearized formulation. In other words, weights on an edges simply scale the coupling strengths between two neighbors.\nIt follows that Proposition 14 can be generalized to weighted networks by using weighted adjacency matrices W \u03c8\u0302 \u2032 with\nelements A \u03c8\u0302 \u2032(o(s), o(t)) = w > 0 if the edge s \u2192 t with potential \u03c8\u0302\u2032 and weight w exists, and W \u03c8\u0302 \u2032(o(s), o(t)) = 0\notherwise. In addition, each entry Din \u03c8\u0302 \u2032(o(t), o(t)) in the block-diagonal matrix Din \u03c8\u0302 \u2032 is now the sum of the squared weights of\nedges to neighbors that are connected to t via edge potential \u03c8\u0302, instead of just the number of neighbors (recall that the echo cancellation goes back and forth, and notice again that the potentials work along the direction s \u2192 t). After this modification, Proposition 14 can immediately be used for weighted graphs as well.\nExample 23 (Edge weights). We give here a small detailed example that shows the effects of weights for a potential whose entries are not really close to each other (i.e., the average entry is 1, however entries can diverge considerably from 1). We start with the potential\u03c8 = [ 4 6 56 8 7 ]. By dividing all entries by 6, we get an equivalent potential that is centered around 1; and from this we get the residual and the row-recentered residual matrices:\n\u03c8 = 16 [ 4 6 5 6 8 7 ] , \u03c8\u0302 = 1 6 [\u22122 0 \u22121 0 2 1 ] , \u03c8\u0302 \u2032 = 118 [ 1 \u22121 0 1 \u22121 0 ]\nThe squared potential centered around 1 is then: \u03c82 = 3\n113\n[ 42 62 52\n62 82 72\n] . And the residual and row-recentered residual matri-\nces: \u03c8\u03022 \u2248 [ 0.575 0.044 0.336 0.044 \u22120.699 \u22120.300 ] , \u03c8\u0302 \u2032 2 \u2248 [ 0.085 \u22120.091 0.006 0.121 \u22120.127 0.006 ]\nWe can now compare the potential we get by multiplying the residual by 2, or by squaring the original potential and then recentering:\n2\u03c8\u0302 \u2248 [ 0.111 \u22120.111 0 0.111 \u22120.111 0 ] , \u03c8\u0302 \u2032 2 \u2248 [ 0.085 \u22120.091 0.006 0.121 \u22120.127 0.006 ]\nWe see that the overall direction is correct, but there are considerable differences (e.g., \u2248 30% relative difference for the first matrix entry: 0.111 vs. 0.085). We next bring each entry in the potential closer to the center. Concretely, we reduce the deviation by one order of magnitude:\n\u03c8 = 16 [ 5.8 6.0 5.9 6.0 6.2 6.1 ] , \u03c8\u0302 = 1 60 [\u22122 0 \u22121 0 2 1 ] , \u03c8\u0302 \u2032 = 1180 [ 1 \u22121 0 1 \u22121 0 ]\nNow both versions are very close (e.g., \u2248 2% relative difference for the first matrix entry: 0.0111 vs. 0.0109):\n2\u03c8\u0302 \u2248 [ 0.0111 \u22120.0111 0 0.0111 \u22120.0111 0 ] , \u03c8\u0302 \u2032 2 \u2248 [ 0.0109 \u22120.0110 0.00005 0.0113 \u22120.0113 0.00005 ]\nF Illustrating examples Example 24 (Convergence). We illustrate the different convergence behaviors of BP and our formalism in a graph with several different potentials. The scenario is a variant of an example given by (Heskes 2002) of a 4-clique graph with weights on the edges (see Fig. 8a). The weights are used to entry-wise exponentiate the entries of the potential \u03c8 = [ 4 11 4 ] before normalization: \u03c8(w)(i, j) = ( \u03c8(i, j) )w . For example, a weight 2 leads to\u03c8(2) = [ 16 11 16 ], and a weight \u22123 leads to\u03c8(\u22123) =[\n1 64 1 1 164\n] . The graph has 2 nodes A and B with prior beliefs: xA = xC = [0.8, 0.2]\u1d40.\nFigure 8b shows the beliefs for nodes A and C for various iterations of BP. The dashed lines show the maximum marginal (MM) distribution as determined by complete iteration. We see that BP has a somewhat erratic cyclic behavior and does not converge. Figure 8c shows a variant that uses damping (Koller and Friedman 2009, ch. 11.1), a method that is often used to make BP converge when it otherwise would not. Damping with 0.1 (meaning an updated message is a linear combination of 90% the prior message and 10% of newly calculated values) is able to dampen the the behavior, but convergence happens only after 1,000s of iterations, and even then the maximum marginal for node A is wrong (0.48 leads to choosing class 2, vs. 0.51 leads to choosing class 1). Furthermore, after replacing \u03c8 in our example with [ 8 11 8 ], BP will not converge anymore for any damping factor and the fixed points of BP are unstable.\nA B\nC\nD\n(a) Network (b) Parameter space\nG Existing graph generators and hardness of node labeling There is a large body of work that proposes various realistic synthetic generative graph models. However, almost all of this work assumes unlabeled graphs. While one could use these existing graph generators to have realistic graphs, one cannot easily take a graph and then label the nodes according to some desired compatibility matrix. In fact, this problem is NP-hard.\nProposition 26 (Labeling with potentials). Given a graph G(V,E). Finding labels ` : v \u2208 V \u2192 [k] so that the labels follow a given stochastic affinity matrix\u03c8 (where \u03c8(i, j) determines the average fraction of nodes of class j connected to a node of class i) is NP-hard.\nProof Proposition 26. Membership in NP follows from the fact that we can easily verify a solution by calculating the average neighbor-to-neighbor relations in a labeled graph. We use a simple reduction from the problem of Graph 3-colorability. Graph 3-colorability is the question of whether there exists a labeling function k : v \u2192 {1, 2, 3} such that k(u) 6= k(v) whenever (u, v) \u2208 E for a given graph G = (V,E) and is well known to be NP-hard (Stockmeyer 1973). Assume now that we have a method that allows us to label any graph G(V,E) following the heterophily matrix \u03c8 = 12 [ 0 1 1 1 0 1 1 1 0 ] , i.e., neighboring nodes never have the same label. It follows immediately that such a solution would also be solution to graph 3-colorability.\nWe thus need graph generators which generate both the graph topology (i.e., W) and the node labels (i.e., X) in the same process. We know of only two papers that have proposed graph generators that generate labeled data in the process: the early work by (Sen and Getoor 2007) and the very recent work by (Lee et al. 2015). Neither graph generator is available. In addition,\nneither of the papers gives a way to know the \u201cground truth\u201d actual potential matrix that was used to label data (e.g., (Lee et al. 2015) suggests this as future work).\nWe therefore had to implement our own synthetic graph generator with the additional design decision that any potential matrix can be \u201cplanted\u201d as exact graph property. This allows us to separate the concern between (1) how does our method work on graphs with certain properties, (2) what is the variation in properties of a given generative model. By planting exact properties (instead of expected properties) we can focus on question (1) only. The random graph generator is described in detail in (Gatterbauer 2016).\nReferences Gatterbauer, W. 2016. Semi-supervised learning with heterophily. (http://arxiv.org/abs/1412.3100CoRR abs/1412.3100). Henderson, H. V., and Searle, S. R. 1981. The vec-permutation matrix, the vec operator and Kronecker products: a review. Linear and Multilinear Algebra 9(4):271\u2013288. Heskes, T. 2002. Stable fixed points of loopy belief propagation are local minima of the bethe free energy. In NIPS, 343\u2013350. Lee, J.; Zaheer, M.; Gu\u0308nnemann, S.; and Smola, A. J. 2015. Preferential attachment in graphs with affinities. In AISTATS, 571\u2013580. Roth, W. E. 1934. On direct product matrices. Bull. Amer. Math. Soc. 40:461\u2013468. Saad, Y. 2003. Iterative methods for sparse linear systems. SIAM, 2nd ed edition. Sen, P., and Getoor, L. 2007. Link-based classification. Technical report, University of Maryland Technical Report CS-TR4858. Stockmeyer, L. 1973. Planar 3-colorability is polynomial complete. SIGACT News 5(3):19\u201325. Yedidia, J. S.; Freeman, W. T.; and Weiss, Y. 2003. Understanding belief propagation and its generalizations. In Exploring artificial intelligence in the new millennium. Morgan Kaufmann Publishers. 239\u2013269."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Belief Propagation (BP) is a widely used approximation for exact probabilistic inference in graphical models, such as Markov Random Fields (MRFs). In graphs with cycles, however, no exact convergence guarantees for BP are known, in general. For the case when all edges in the MRF carry the same symmetric, doubly stochastic potential, recent works have proposed to approximate BP by linearizing the update equations around default values, which was shown to work well for the problem of node classification. The present paper generalizes all prior work and derives an approach that approximates loopy BP on any pairwise MRF with the problem of solving a linear equation system. This approach combines exact convergence guarantees and a fast matrix implementation with the ability to model heterogenous networks. Experiments on synthetic graphs with planted edge potentials show that the linearization has comparable labeling accuracy as BP for graphs with weak potentials, while speeding-up inference by orders of magnitude.", "creator": "LaTeX with hyperref package"}}}